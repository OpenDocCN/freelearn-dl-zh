- en: 2\. Machine Learning versus Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 机器学习与深度学习
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概览
- en: In this chapter, we will begin creating Artificial Neural Networks (ANNs) using
    the Keras library. Before utilizing the library for modeling, we will get an introduction
    to the mathematics that comprise ANNs—understanding linear transformations and
    how they can be applied in Python. You'll build a firm grasp of the mathematics
    that make up ANNs. By the end of this chapter, we will have applied that knowledge
    by building a logistic regression model with Keras.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始使用Keras库创建人工神经网络（ANNs）。在利用该库进行建模之前，我们将介绍组成ANNs的数学知识——理解线性变换以及如何在Python中应用它们。你将牢固掌握构成ANNs的数学基础。到本章结束时，我们将通过使用Keras构建一个逻辑回归模型来应用这些知识。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we discussed some applications of machine learning
    and even built models with the scikit-learn Python package. The previous chapter
    covered how to preprocess real-world datasets so that they can be used for modeling.
    To do this, we converted all the variables into numerical data types and converted
    `categorical` variables into `dummy` variables. We used the `logistic regression`
    algorithm to classify users of a website by their purchase intention from the
    `online shoppers purchasing intention` dataset. We advanced our model-building
    skills by adding `regularization` to the dataset to improve the performance of
    our models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了机器学习的一些应用，甚至使用scikit-learn Python包构建了模型。上一章讲解了如何预处理现实世界的数据集，以便用于建模。为此，我们将所有变量转换为数值数据类型，并将`categorical`变量转换为`dummy`变量。我们使用`logistic
    regression`算法，根据`online shoppers purchasing intention`数据集中的购买意图对网站用户进行分类。通过将`regularization`添加到数据集中以提高模型的性能，我们进一步提升了模型构建技能。
- en: In this chapter, we will continue learning how to build machine learning models
    and extend our knowledge so that we can build an `Artificial Neural Network` (`ANN`)
    with the Keras package. (Remember that `ANNs` represent a large class of machine
    learning algorithms that are so-called because their architecture resembles the
    neurons in the human brain.)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续学习如何构建机器学习模型，并扩展我们的知识，使我们能够使用Keras包构建一个`人工神经网络`（`ANN`）。(记住，`ANNs`代表一类大型的机器学习算法，因为它们的结构类似于人类大脑中的神经元。)
- en: '`Keras` is a machine learning library designed specifically for building neural
    networks. While scikit-learn''s functionality spans a broader area of machine
    learning algorithms, the functionality of `scikit-learn` for neural networks is
    minimal.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '`Keras`是一个专门用于构建神经网络的机器学习库。虽然scikit-learn的功能涵盖了更广泛的机器学习算法，但`scikit-learn`在神经网络方面的功能较为有限。'
- en: '`ANNs` can be used for the same machine learning tasks that other algorithms
    can perform, such as `logistic regression` for `classification` tasks, `linear
    regression` for `regression` problems, and `k-means` for `clustering`. Whenever
    we begin any machine learning problem, to determine what kind of task it is (`regression`,
    `classification`, or `clustering`), we need to ask the following questions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`ANNs`可以用于与其他算法相同的机器学习任务，例如将`logistic regression`用于`classification`任务，将`linear
    regression`用于`regression`问题，将`k-means`用于`clustering`。每当我们开始解决任何机器学习问题时，要确定它属于哪种任务（`regression`、`classification`或`clustering`），我们需要问以下问题：'
- en: '`classification` task) or you could predict the value itself (which would be
    a `regression` problem). Each may lead to a different subsequent action or trading
    strategy.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （`classification`任务）或者你也可以预测值本身（这将是一个`regression`问题）。每种情况可能导致不同的后续操作或交易策略。
- en: The following plot shows a `candlestick chart`. It describes the price movements
    in financial data and is depicting a stock price. The colors represent whether
    the stock price increased (green) or decreased (red) in value over each period,
    and each candlestick shows the open, close, high, and low values of the data—important
    pieces of information for stock prices.
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了一个`candlestick chart`。它描述了金融数据中的价格波动，并展示了一只股票的价格。颜色表示股票价格在每个时间段内是上涨（绿色）还是下跌（红色），每根蜡烛图展示了数据的开盘、收盘、最高和最低值——这些都是股票价格的重要信息。
- en: Note
  id: totrans-10
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the high-quality color images for this chapter at: [https://packt.live/38nenXS](https://packt.live/38nenXS).'
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在此处找到本章的高质量彩色图片：[https://packt.live/38nenXS](https://packt.live/38nenXS)。
- en: 'One goal of modeling this data would be to predict what happens the following
    day. A `classification` task might predict a positive or negative change in the
    stock price and since there are only two possible values, this would be a binary
    classification task. Another option would be to predict the value of the stock
    the following day. Since the predicted value would be a `continuous` variable,
    this would be a `regression` task:'
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 建模这些数据的一个目标是预测第二天发生的情况。一个`分类`任务可能会预测股价是上涨还是下跌，由于只有两个可能的值，这将是一个二元分类任务。另一种选择是预测第二天股价的具体数值。由于预测的数值是一个`连续`变量，这将是一个`回归`任务：
- en: '![Figure 2.1: A candlestick chart indicating the movement of a stock index
    over the span of a month'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：一个烛台图，显示了一个月内股票指数的变化]'
- en: '](img/B15777_02_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_01.jpg)'
- en: 'Figure 2.1: A candlestick chart indicating the movement of a stock index over
    the span of a month'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：一个烛台图，显示了一个月内股票指数的变化
- en: '**Do we have the appropriately labeled data to train a model?** For a supervised
    learning task, we must have at least some labeled data in order to train a model.
    For example, if we want to build a model to classify images into dog images and
    cat images, we would need training data, the images themselves, and labels for
    the data indicating whether they are dog images or cat images. ANNs often need
    a lot of data. For image classification, this can be millions of images to develop
    accurate, robust models. This may be a determining factor when deciding which
    algorithm is appropriate for a given task.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们是否有合适的标注数据来训练模型？** 对于监督学习任务，我们必须至少拥有一些标注数据才能训练模型。例如，如果我们想建立一个模型来将图像分类为狗的图像和猫的图像，我们需要训练数据、图像本身以及数据的标签，指示它们是狗的图像还是猫的图像。人工神经网络通常需要大量的数据。对于图像分类，这可能需要数百万张图像来开发准确、强大的模型。在决定哪种算法适用于特定任务时，这可能是一个决定性因素。'
- en: ANNs are a type of machine learning algorithm that can be used to solve a task.
    They excel in certain aspects and have drawbacks in others, and these pros and
    cons should be considered before choosing this type of algorithm. Deep learning
    networks are distinguished from single-layer ANNs by their depth—the total number
    of hidden layers within the network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）是一种机器学习算法，可以用于解决任务。它们在某些方面表现优秀，但在其他方面也有缺点，因此在选择该算法之前，应该考虑这些优缺点。深度学习网络与单层人工神经网络的区别在于它们的“深度”——即网络中隐藏层的总数。
- en: So, deep learning is really just a specific subgroup of machine learning that
    relies on ANNs with multiple layers. We encounter the results of deep learning
    on a regular basis, whether it's in image classification models such as the friend
    recognition models that help tag friends in your Facebook photos, or the recommendation
    algorithms that help suggest your next favorite songs on Spotify. Deep learning
    models are becoming more prevalent over traditional machine learning models for
    a variety of reasons, including the growing sizes of unstructured data that deep
    learning models excel at and lower computational costs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，深度学习实际上只是机器学习的一个特定子组，依赖于具有多层的人工神经网络。我们经常接触到深度学习的结果，无论是图像分类模型，例如帮助在 Facebook
    照片中标记朋友的朋友识别模型，还是推荐算法，例如帮助在 Spotify 上推荐你下一首最喜欢的歌曲。由于多种原因，深度学习模型正在逐渐取代传统的机器学习模型，包括深度学习模型擅长处理的非结构化数据规模的不断扩大和计算成本的降低。
- en: Choosing whether to use ANNs or traditional machine learning algorithms such
    as linear regression and decision trees for a particular task is a matter of experience
    and an understanding of the inner workings of the algorithm itself. As such, the
    benefits of using traditional machine learning algorithms or ANNs will be mentioned
    in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 是否选择使用人工神经网络还是传统机器学习算法，如线性回归和决策树，取决于经验以及对算法本身工作原理的理解。因此，使用传统机器学习算法或人工神经网络的好处将在下一节中提到。
- en: Advantages of ANNs over Traditional Machine Learning Algorithms
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经网络相对于传统机器学习算法的优势
- en: '`ImageNet challenge` (a large-scale visual recognition challenge for classifying
    images into `1000 classes`), ANNs can attain greater accuracy than humans.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageNet挑战赛`（一个大规模的视觉识别挑战，旨在将图像分类为`1000个类别`），人工神经网络在准确性上可以超过人类。'
- en: '`logistic regression` and `decision trees`, plateau in performance, whereas
    the ANN architecture is able to learn higher-level features—nonlinear combinations
    of the input features that may be important for classification or regression tasks.
    This allows ANNs to perform better when provided with large amounts of data -
    especially those ANNs with a deep architecture. For example, ANNs that perform
    well in the ImageNet challenge are provided with `14 million images` for training.
    The following figure shows the performance scaling with the amount of data for
    both deep learning algorithms and traditional machine learning algorithms:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`逻辑回归` 和 `决策树` 在性能上存在平台期，而人工神经网络架构能够学习更高层次的特征——即输入特征的非线性组合，这些特征可能对分类或回归任务至关重要。这使得人工神经网络在提供大量数据时表现更好——尤其是那些具有深度架构的人工神经网络。例如，参加
    ImageNet 挑战的人工神经网络需要提供 `1400万图像` 进行训练。下图展示了深度学习算法和传统机器学习算法在数据量增加时的性能扩展：'
- en: '![Figure 2.2: Performance scaling with the amount of data for both deep learning
    algorithms and traditional machine learning algorithms'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：深度学习算法和传统机器学习算法在数据量增加时的性能扩展'
- en: '](img/B15777_02_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_02.jpg)'
- en: 'Figure 2.2: Performance scaling with the amount of data for both deep learning
    algorithms and traditional machine learning algorithms'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：深度学习算法和传统机器学习算法在数据量增加时的性能扩展
- en: '**No need for feature engineering**: ANNs are able to identify which features
    are important in modeling so that they are able to model directly from raw data.
    For example, in the binary classification of dog and cat images into their respective
    classes, there is no need to define features such as the color size or weight
    of the animal. The images themselves are sufficient for the ANN to successfully
    determine classification. In traditional machine learning algorithms, these features
    must be engineered in an iterative process that is manual and can be time-consuming.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无需特征工程**：人工神经网络能够识别出在建模过程中哪些特征是重要的，从而可以直接从原始数据进行建模。例如，在对狗和猫图像进行二分类时，不需要定义像动物的颜色、大小或体重等特征。图像本身就足以让人工神经网络成功地进行分类。在传统的机器学习算法中，这些特征必须通过手动和耗时的迭代过程进行工程化。'
- en: '`16-layer deep learning model` that''s used by `ImageNet` to classify `1000
    random objects`. The weights that are learned in the model can be transferred
    to classify other objects in significantly less time.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`16层深度学习模型` 被 `ImageNet` 用来对 `1000种随机物体` 进行分类。模型中学到的权重可以转移到其他物体的分类任务上，且所需时间大大减少。'
- en: However, there are some advantages of using traditional machine learning algorithms
    over ANNs, as explained in the following section.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用传统机器学习算法相对于人工神经网络（ANN）仍有一些优势，如下节所述。
- en: Advantages of Traditional Machine Learning Algorithms over ANNs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统机器学习算法相较于人工神经网络（ANN）的优势
- en: '`VGG-16` has over `138 million parameters` and required `14 million hand-labeled
    images` to train and learn all the parameters.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VGG-16` 拥有超过 `138百万参数`，并且需要 `1400万手工标注的图像` 来训练并学习所有的参数。'
- en: '**Cost-effective**: Both financially and computationally, deep networks can
    take a lot of computing power and time to train. This demands a lot of resources
    that may not be available to all. Moreover, these models are time-consuming to
    tune effectively and require a domain expert who''s familiar with the inner workings
    of the model to achieve optimal performance.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有成本效益**：无论是财务上还是计算上，深度网络训练通常需要大量的计算能力和时间。这需要大量的资源，而这些资源可能并不是每个人都能获得的。此外，这些模型的调优过程非常耗时，并且需要领域专家熟悉模型的内部机制，才能达到最佳性能。'
- en: '`black box`, in that while they are successful in classifying images and other
    tasks, the understanding behind how the predictions are made is unintuitive and
    buried in layers of computations. As such, interpreting the results requires more
    effort.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`黑盒`，虽然它们在分类图像和其他任务时表现成功，但理解预测是如何产生的却是直觉上难以理解的，并且埋藏在层层计算中。因此，解释结果需要更多的努力。'
- en: Hierarchical Data Representation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次化数据表示
- en: One reason that ANNs are able to perform so well is that a large number of layers
    allows the network to learn representations of the data at many different levels.
    This is illustrated in the following diagram, in which the representation of an
    ANN being used to identify faces is shown. At lower levels of the model, simple
    features are learned, such as edges and gradients, as can be seen by looking at
    the features that were learned in the initial layers. As the model progresses,
    combinations of lower-level features activate to form face parts, and at later
    layers of the model, generic faces are learned. This is known as feature hierarchy
    and illustrates the power that this layered representation has for model building
    and interpretation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络之所以能够表现如此出色的原因之一是，网络中的大量层次允许它在许多不同层次上学习数据的表示。以下图示说明了这一点，图中展示了用于识别人脸的ANN的表示。在模型的低层，学习到的是简单的特征，如边缘和梯度，可以通过观察初始层中学习到的特征看出。当模型逐步推进时，低层特征的组合激活，形成面部部分，在模型的更高层，学习到的是通用的人脸。这就是特征层次结构，它展示了这种层级表示在模型构建和解释中的强大作用。
- en: Many examples of input for real-world applications of deep neural networks involve
    images, video, and natural language text. The feature hierarchy that is learned
    by deep neural networks allows them to discover latent structures within unlabeled,
    unstructured data, such as images, video, and natural language text, which makes
    them useful for processing real-world data—most often raw and unprocessed.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在实际应用中的输入示例通常包括图像、视频和自然语言文本。深度神经网络所学习的特征层次结构使得它们能够发现未标注、未结构化数据中的潜在结构，如图像、视频和自然语言文本，这使得它们在处理实际数据时非常有用——这些数据通常是未经处理的原始数据。
- en: 'The following diagram shows an example of the learned representation of a deep
    learning model—lower features such as the `edges` and `gradients` activate together
    to form generic face shapes, which can be seen in the deeper layers:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了深度学习模型学习到的表示的一个例子——低层特征如`边缘`和`梯度`会共同激活，形成通用的面部形状，这些可以在更深层的网络中看到：
- en: '![Figure 2.3: Learned representation at various parts of a deep learning model'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3：深度学习模型各部分学习到的表示'
- en: '](img/B15777_02_03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_03.jpg)'
- en: 'Figure 2.3: Learned representation at various parts of a deep learning model'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：深度学习模型各部分学习到的表示
- en: 'Since deep neural networks have become more accessible, various companies have
    started exploiting their applications. The following are some examples of some
    companies that use ANNs:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度神经网络变得更加易于使用，许多公司开始利用它们的应用。以下是一些使用人工神经网络（ANNs）的公司示例：
- en: '**Yelp**: Yelp uses deep neural networks to process, classify, and label their
    images more efficiently. Since photos are one important aspect of Yelp reviews,
    the company has placed an emphasis on classifying and categorizing them. This
    is achieved more efficiently with deep neural networks.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Yelp**：Yelp使用深度神经网络更高效地处理、分类和标注其图片。由于照片是Yelp评论的一个重要方面，Yelp公司特别注重对照片的分类和归类。通过深度神经网络，这一过程得以更加高效地实现。'
- en: '**Clarifai**: This cloud-based company is able to classify images and videos
    using deep neural network-based models.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Clarifai**：这家基于云计算的公司能够利用深度神经网络模型对图片和视频进行分类。'
- en: '**Enlitic**: This company uses deep neural networks to analyze medical image
    data such as X-rays or MRIs. The use of such networks in this application increases
    diagnostic accuracy and decreases diagnostic time and cost.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Enlitic**：这家公司使用深度神经网络来分析医学影像数据，如X光或MRI。使用这种网络技术可以提高诊断准确性，减少诊断时间和成本。'
- en: Now that we understand the potential applications of using ANNs, we can understand
    the mathematics behind how they work. While they may seem intimidating and complex,
    they can be broken down into a series of linear and nonlinear transformations,
    which themselves are simple to understand. An ANN is created by sequentially combining
    a series of linear and nonlinear transformations. The next section discusses the
    basic components and operations involved in linear transformations that comprise
    the mathematics of ANNs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了使用人工神经网络（ANNs）的潜在应用后，我们可以理解它们工作的背后数学原理。尽管它们看起来可能令人生畏且复杂，但它们可以分解为一系列线性和非线性变换，而这些变换本身是容易理解的。一个ANN是通过按顺序组合一系列线性和非线性变换创建的。下一部分将讨论线性变换的基本组件和操作，这些变换构成了ANN的数学基础。
- en: Linear Transformations
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性变换
- en: In this section, we will introduce linear transformations. Linear transformations
    are the backbone of modeling with ANNs. In fact, all the processes of ANN modeling
    can be thought of as a series of linear transformations. The working components
    of linear transformations are scalars, vectors, matrices, and tensors. Operations
    such as `addition`, `transposition`, and `multiplication` are performed on these components.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍线性变换。线性变换是使用人工神经网络（ANN）进行建模的核心。事实上，所有的ANN建模过程都可以被看作是一系列的线性变换。线性变换的工作组件包括标量、向量、矩阵和张量。像`加法`、`转置`和`乘法`这样的操作会作用于这些组件。
- en: Scalars, Vectors, Matrices, and Tensors
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量、向量、矩阵和张量
- en: '`Scalars`, `vectors`, `matrices`, and `tensors` are the actual components of
    any deep learning model. Having a fundamental understanding of how to utilize
    these components, as well as the operations that can be performed on them, is
    key to understanding how ANNs operate. `Scalars`, `vectors`, and `matrices` are
    examples of the general entity known as a `tensor`, so the term `tensors` may
    be used throughout this chapter but may refer to any component. `Scalars`, `vectors`,
    and `matrices` refer to `tensors` with a specific number of dimensions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`标量`、`向量`、`矩阵`和`张量`是任何深度学习模型的实际组件。理解如何使用这些组件，并理解可以对它们执行的操作，对于理解ANN的工作原理至关重要。`标量`、`向量`和`矩阵`是`张量`这一通用实体的例子，因此，在本章中可能会使用`张量`这个术语，但它可以指代任何组件。`标量`、`向量`和`矩阵`是指具有特定维度数的`张量`。'
- en: 'The rank of a `tensor` is an attribute that determines the number of dimensions
    the `tensor` spans. The definitions of each are listed here:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`张量`的秩是一个属性，用来确定`张量`所跨越的维度数。每个概念的定义如下：'
- en: '`tensors`. For instance, the temperature at any given point is a `scalar` field.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`张量`。例如，某一给定点的温度是一个`标量`场。'
- en: '`tensors`. The `velocity` of a given object is an example of a `vector` field
    since it will have a speed in the `two (x,y)` or `three (x,y,z)` dimensions.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`张量`。给定物体的`速度`是一个`向量`场的例子，因为它会在`二维 (x,y)` 或 `三维 (x,y,z)` 空间中具有一个速度。'
- en: '`Matrices` are rectangular arrays that span over two dimensions that consist
    of single numbers. They are an example of second-order `tensors`. An example of
    where `matrices` might be used is to store the `velocity` of a given object over
    time. One dimension of the `matrix` comprises the speed in the given directions,
    while the other `matrix` dimension is comprised of each given time point.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`矩阵`是跨越二维的矩形数组，包含单一的数字。它们是二阶`张量`的例子。`矩阵`的一个应用实例是用于存储给定物体随时间变化的`速度`。`矩阵`的一个维度包含给定方向上的速度，而另一个`矩阵`维度则包含每个给定的时间点。'
- en: '`Tensors` are the general entities that encapsulate `scalars`, `vectors`, and
    `matrices`. In general, the name is reserved for `tensors` of order `3` or more.
    An example of where `tensors` might be used is to store the `velocity` of many
    objects over time. One dimension of the `matrix` comprises the speed in the given
    directions, another `matrix` dimension is given for each given time point, and
    a third dimension describes the various objects.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`张量`是封装`标量`、`向量`和`矩阵`的通用实体。通常，该名称用于秩为`3`或以上的`张量`。一个应用实例是存储多个物体随时间变化的`速度`。`矩阵`的一个维度包含给定方向上的速度，另一个`矩阵`维度包含每个给定的时间点，第三个维度描述各种物体。'
- en: 'The following diagram shows some examples of a `scalar`, a `vector`, a `matrix`,
    and a `three-dimensional tensor`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了`标量`、`向量`、`矩阵`和`三维张量`的一些示例：
- en: '![Figure 2.4: A visual representation of scalars, vectors, matrices, and tensors'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4：标量、向量、矩阵和张量的可视化表示'
- en: '](img/B15777_02_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_04.jpg)'
- en: 'Figure 2.4: A visual representation of scalars, vectors, matrices, and tensors'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：标量、向量、矩阵和张量的可视化表示
- en: Tensor Addition
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量加法
- en: '`Tensors` can be added together to create new `tensors`. We will use the example
    of matrices in this chapter, but this concept can be extended to `tensors` with
    any rank. `Matrices` may be added to `scalars`, `vectors`, and other `matrices`
    under certain conditions.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`张量`可以相加以创建新的`张量`。我们将在本章中使用矩阵作为例子，但这一概念可以扩展到任何秩的`张量`。在特定条件下，`矩阵`可以与`标量`、`向量`以及其他`矩阵`进行相加。'
- en: 'Two matrices may be added (or subtracted) together if they have the same shape.
    For such matrix-matrix addition, the resultant matrix is determined by the element-wise
    addition of the input matrices. The resultant matrix will, therefore, have the
    same shape as the two input matrices. We can define the matrix C = [cij] as the
    matrix sum **C = A + B**, where cij = aij + bij and each element in **C** is the
    sum of the same element in **A** and **B**. Matrix addition is commutative, which
    means that the order of **A** and **B** does not matter – **A + B = B + A**. Matrix
    addition is also associative, which means that the same result is achieved, even
    when the order of additions is different or even if the operation is applied more
    than once: **A + (B + C) = (A + B) + C**.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个矩阵形状相同，它们可以相加（或相减）。对于这样的矩阵加法，结果矩阵是通过输入矩阵按元素相加得出的。因此，结果矩阵将与两个输入矩阵具有相同的形状。我们可以定义矩阵
    C = [cij] 为矩阵和 **C = A + B**，其中 cij = aij + bij，并且 **C** 中的每个元素都是 **A** 和 **B**
    中相同元素的和。矩阵加法是交换律的，这意味着 **A + B = B + A**，顺序无关。矩阵加法也是结合律的，这意味着即使加法顺序不同，或者操作应用多次，结果也会相同：**A
    + (B + C) = (A + B) + C**。
- en: 'The same matrix addition principles apply for `scalars`, `vectors`, and `tensors`.
    An example of this is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的矩阵加法原则适用于 `标量`、`向量` 和 `张量`。以下是一个示例：
- en: '![Figure 2.5: An example of matrix-matrix addition'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：矩阵与矩阵相加的示例'
- en: '](img/B15777_02_05.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_05.jpg)'
- en: 'Figure 2.5: An example of matrix-matrix addition'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：矩阵与矩阵相加的示例
- en: '`Scalars` can also be added to `matrices`. Here, each element of the `matrix`
    is added to the `scalar` individually, as is shown in the below figure:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`标量`也可以加到`矩阵`中。在这里，`矩阵`的每个元素都会单独加上`标量`，如下图所示：'
- en: '![Figure 2.6: An example of matrix-scalar addition'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：矩阵与标量相加的示例'
- en: '](img/B15777_02_06.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_06.jpg)'
- en: 'Figure 2.6: An example of matrix-scalar addition'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：矩阵与标量相加的示例
- en: It is possible to add vectors to matrices if the number of columns between the
    two matches each other. This is known as broadcasting.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个矩阵的列数相同，可以将向量添加到矩阵中。这就是所谓的广播（broadcasting）。
- en: 'Exercise 2.01: Performing Various Operations with Vectors, Matrices, and Tensors'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：对向量、矩阵和张量进行各种操作
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For the exercises and activities within this chapter, you will need to have
    Python 3.7, Jupyter, and NumPy installed on your system. All the exercises and
    activities will be primarily developed in Jupyter notebooks. It is recommended
    to keep a separate notebook for different assignments unless advised not to. Use
    the following link to download them from this book''s GitHub repository: [https://packt.live/2vpc9rO](https://packt.live/2vpc9rO).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的练习和活动需要你在系统上安装 Python 3.7、Jupyter 和 NumPy。所有的练习和活动将主要在 Jupyter Notebook
    中进行开发。除非有特别指示，否则建议为不同的任务保持独立的 Notebook。使用以下链接从本书的 GitHub 仓库下载它们：[https://packt.live/2vpc9rO](https://packt.live/2vpc9rO)。
- en: In this exercise, we are going to demonstrate how to create and work with `vectors`,
    `matrices`, and `tensors` within Python. We will assume that you have some familiarity
    with scalars. This can all be achieved with the NumPy library using the `array`
    and `matrix` functions. Tensors of any rank can be created with the NumPy `array`
    function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将演示如何在 Python 中创建并处理`向量`、`矩阵`和`张量`。我们假设你对标量已有一定了解。所有这些操作都可以通过 NumPy
    库中的`array`和`matrix`函数来实现。任何秩的张量都可以通过 NumPy 的`array`函数创建。
- en: Before you begin, you should set up the files and folders for this chapter in
    your working directory using a similar structure and naming convention as you
    did in the previous chapter. You can verify your folder structure by comparing
    it to the GitHub repository, linked above.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，你应在工作目录中设置本章的文件和文件夹，使用类似于上一章的结构和命名约定。你可以通过与上面的 GitHub 仓库进行对比来验证你的文件夹结构。
- en: 'Follow these steps to perform this exercise:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行此练习：
- en: 'Open Jupyter Notebook to implement this exercise. Import the necessary dependency.
    Create a `one-dimensional array`, or a `vector`, as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Jupyter Notebook 实现这个练习。导入必要的依赖项。创建一个`一维数组`，或`向量`，如下所示：
- en: '[PRE0]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code produces the following output:'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码产生了以下输出：
- en: '[PRE1]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a `two-dimensional array`, or `matrix`, with the `array` function:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`array`函数创建一个`二维数组`，或`矩阵`：
- en: '[PRE2]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code produces the following output:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码产生了以下输出：
- en: '[PRE3]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Use the `matrix` function to create matrices, which will show a similar output:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`matrix`函数创建矩阵，这将显示类似的输出：
- en: '[PRE4]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a `three-dimensional array`, or `tensor`, using the `array` function:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`array`函数创建一个`三维数组`或`张量`：
- en: '[PRE5]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code produces the following output:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE6]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Determining the `shape` of a given `vector`, `matrix`, or `tensor` is important
    since certain operations, such as `addition` and `multiplication`, can only be
    applied to components of certain shapes. The shape of an n-dimensional array can
    be determined using the `shape` method. Write the following code to determine
    the `shape` of `vec1`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定给定`向量`、`矩阵`或`张量`的`形状`非常重要，因为某些操作（如`加法`和`乘法`）只能应用于特定形状的组件。可以使用`shape`方法来确定n维数组的形状。编写以下代码以确定`vec1`的`形状`：
- en: '[PRE7]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code produces the following output:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE8]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Write the following code to determine the `shape` of `mat1`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写以下代码以确定`mat1`的`形状`：
- en: '[PRE9]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code produces the following output:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE10]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Write the following code to determine the `shape` of `ten1`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写以下代码以确定`ten1`的`形状`：
- en: '[PRE11]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code produces the following output:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE12]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create a `matrix` with `four rows` and `three columns` with whichever numbers
    you like. Print the resulting matrix to verify its `shape`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个具有`四行`和`三列`的`矩阵`，可以使用任意数字。打印结果矩阵以验证其`形状`：
- en: '[PRE13]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code produces the following output:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE14]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create another matrix with `four rows` and `three columns` with whichever numbers
    you like. Print the resulting matrix to verify its `shape`:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个具有`四行`和`三列`的矩阵，可以使用任意数字。打印结果矩阵以验证其`形状`：
- en: '[PRE15]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code produces the following output:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE16]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add `matrix 1` and `matrix 2`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`矩阵 1`和`矩阵 2`相加：
- en: '[PRE17]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code produces the following output:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE18]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Add `scalars` to the `arrays` with the following code:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码将`标量`加到`数组`中：
- en: '[PRE19]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code produces the following output:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE20]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this exercise, we learned how to perform various operations with `vectors`,
    `matrices`, and `tensors`. We also learned how to determine the `shape` of the `matrix`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们学习了如何执行`向量`、`矩阵`和`张量`的各种操作。我们还学会了如何确定`矩阵`的`形状`。
- en: Note
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2NNQ7VA](https://packt.live/2NNQ7VA).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/2NNQ7VA](https://packt.live/2NNQ7VA)。
- en: You can also run this example online at [https://packt.live/3eUDtQA](https://packt.live/3eUDtQA).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在线运行这个示例，访问[https://packt.live/3eUDtQA](https://packt.live/3eUDtQA)。
- en: Reshaping
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重塑
- en: A `tensor` of any size can be reshaped as long as the number of total elements
    remains the same. For example, a `(4x3) matrix` can be reshaped into a `(6x2)
    matrix` since they both have a total of `12` elements. The `rank`, or `number
    of dimensions`, can also be changed in the `reshaping` process. For example, a
    `(4x3) matrix` can be reshaped into a `(3x2x2) tensor`. Here, the `rank` has changed
    from `2` to `3`. The `(4x3) matrix` can also be reshaped into a `(12x1) vector`,
    in which the `rank` has changed from `2` to `1`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大小的`张量`都可以被重塑，只要总元素数量保持不变。例如，`(4x3)矩阵`可以被重塑为`(6x2)矩阵`，因为它们的元素总数都是`12`。`秩`（即`维度数`）也可以在`重塑`过程中发生变化。例如，`(4x3)矩阵`可以重塑为`(3x2x2)张量`，在这里，`秩`从`2`变为`3`。`(4x3)矩阵`还可以重塑为`(12x1)向量`，此时，`秩`从`2`变为`1`。
- en: 'The following diagram illustrates tensor reshaping—on the left is a tensor
    with `shape (4x1x3)`, which can be reshaped to a tensor of `shape (4x3)`. Here,
    the number of elements (`12`) has remained constant, though the `shape` and `rank`
    of the tensor have changed:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了张量重塑的过程——左侧是一个`形状(4x1x3)`的张量，它可以被重塑为`形状(4x3)`的张量。在这里，元素的数量（`12`）保持不变，尽管张量的`形状`和`秩`发生了变化：
- en: '![Figure 2.7: Visual representation of reshaping a (4x1x3) tensor into a (4x3)
    tensor'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7：将(4x1x3)张量重塑为(4x3)张量的可视化表示'
- en: '](img/B15777_02_07.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_07.jpg)'
- en: 'Figure 2.7: Visual representation of reshaping a (4x1x3) tensor into a (4x3)
    tensor'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：将(4x1x3)张量重塑为(4x3)张量的可视化表示
- en: Matrix Transposition
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵转置
- en: 'The `transpose` of a matrix is an operator that flips the matrix over its diagonal.
    When this occurs, the rows become the columns and vice versa. The transpose operation
    is usually denoted as a `T` superscript upon the matrix. Tensors of any rank can
    also be transposed:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的 `转置` 是一种操作，它使矩阵沿其对角线翻转。当发生这种情况时，行变为列，反之亦然。转置操作通常用矩阵上方的 `T` 上标表示。任何秩的张量也可以转置：
- en: '![Figure 2.8: A visual representation of matrix transposition'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：矩阵转置的可视化表示'
- en: '](img/B15777_02_08.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_08.jpg)'
- en: 'Figure 2.8: A visual representation of matrix transposition'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：矩阵转置的可视化表示
- en: 'The following figure shows the matrix transposition properties of matrices
    `A` and `B`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了矩阵 `A` 和 `B` 的转置特性：
- en: '![Figure 2.9: Matrix transposition properties where A and B are matrices'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：矩阵转置特性，其中 A 和 B 是矩阵'
- en: '](img/B15777_02_09.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_09.jpg)'
- en: 'Figure 2.9: Matrix transposition properties where A and B are matrices'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：矩阵转置特性，其中 A 和 B 是矩阵
- en: A square matrix (that is, a matrix with an equivalent number of rows and columns)
    is said to be symmetrical if the transpose of a matrix is equivalent to the original
    matrix.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一个方阵（即行数和列数相等的矩阵）被称为对称的，如果矩阵的转置等于原矩阵。
- en: 'Exercise 2.02: Matrix Reshaping and Transposition'
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：矩阵重塑和转置
- en: 'In this exercise, we are going to demonstrate how to reshape and transpose
    matrices. This will become important since some operations can only be applied
    to components if certain tensor dimensions match. For example, tensor multiplication
    can only be applied if the inner dimensions of the two tensors match. Reshaping
    or transposing tensors is one way to modify the dimensions of the tensor to ensure
    that certain operations can be applied. Follow these steps to complete this exercise:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将演示如何重塑和转置矩阵。这个过程非常重要，因为某些操作只有在张量的维度匹配时才能应用。例如，张量乘法只有在两个张量的内维度匹配时才能应用。重塑或转置张量是修改张量维度的一种方式，以确保可以应用某些操作。按照以下步骤完成此练习：
- en: 'Open a Jupyter notebook from the start menu to implement this exercise. Create
    a `two-dimensional array` with `four rows` and `three columns`, as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从开始菜单打开一个 Jupyter notebook 来实现这个练习。创建一个 `二维数组`，具有 `四行` 和 `三列`，如下所示：
- en: '[PRE21]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This gives the following output:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can confirm its shape by looking at the shape of the matrix:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以通过查看矩阵的形状来确认它的维度：
- en: '[PRE23]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE24]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Reshape the array so that it has `three rows` and `four columns` instead, as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数组重塑为 `三行`和 `四列`，如下所示：
- en: '[PRE25]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code produces the following output:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE26]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Confirm this by printing the `shape` of the array:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印数组的 `shape` 来确认这一点：
- en: '[PRE27]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code produces the following output:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE28]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Reshape the matrix into a `three-dimensional array`, as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将矩阵重塑为一个 `三维数组`，如下所示：
- en: '[PRE29]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code produces the following output:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE30]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Print the `shape` of the array to confirm its dimensions:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数组的 `shape` 来确认其维度：
- en: '[PRE31]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code produces the following output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE32]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Reshape the matrix into a `one-dimensional array`, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将矩阵重塑为一个 `一维数组`，如下所示：
- en: '[PRE33]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code produces the following output:'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE34]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Confirm this by printing the `shape` of the array:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打印数组的 `shape` 来确认这一点：
- en: '[PRE35]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code produces the following output:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE36]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Taking the transpose of an array will flip it across its diagonal. For a one-dimensional
    array, a row-vector will be converted into a column vector and vice versa. For
    a two-dimensional array or matrix, each row becomes a column and vice versa. Call
    the transpose of an array using the `T` method:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数组进行转置会使其沿对角线翻转。对于一维数组，行向量会转换为列向量，反之亦然。对于二维数组或矩阵，每一行变为一列，反之亦然。使用 `T` 方法调用数组的转置：
- en: '[PRE37]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following figure shows the output of the preceding code:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图显示了上述代码的输出：
- en: '![Figure 2.10: Visual demonstration of the transpose function'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.10：转置函数的可视化演示'
- en: '](img/B15777_02_10.jpg)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15777_02_10.jpg)'
- en: 'Figure 2.10: Visual demonstration of the transpose function'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.10：转置函数的可视化演示
- en: 'Check the `shape` of the matrix and its transpose to verify that the dimensions
    have changed:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查矩阵及其转置的 `shape`，以验证维度是否已改变：
- en: '[PRE38]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code produces the following output:'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE39]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Check the `shape` of the transposed matrix:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查转置矩阵的`shape`：
- en: '[PRE40]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code produces the following output:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '[PRE41]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Verify the matrix elements do not match when a matrix is reshaped, and a matrix
    is transposed:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证当矩阵被重塑和转置时，矩阵元素不匹配：
- en: '[PRE42]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code produces the following output:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '[PRE43]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, we can see that only the first and last elements match.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到只有第一个和最后一个元素匹配。
- en: In this section, we introduced some of the basic components of linear algebra,
    including scalars, vectors, matrices, and tensors. We also covered some basic
    manipulation of linear algebra components, such as addition, transposition, and
    reshaping. By doing so, we learned how to put these concepts into action by using
    functions in the `NumPy` library to perform these operations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们介绍了线性代数的一些基本组成部分，包括标量、向量、矩阵和张量。我们还介绍了一些基本的线性代数组件操作，如加法、转置和重塑。通过这些操作，我们学习了如何通过使用`NumPy`库中的函数将这些概念付诸实践。
- en: Note
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gqBlR0](https://packt.live/3gqBlR0).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3gqBlR0](https://packt.live/3gqBlR0)。
- en: You can also run this example online at [https://packt.live/3eYCChD](https://packt.live/3eYCChD).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，访问[https://packt.live/3eYCChD](https://packt.live/3eYCChD)。
- en: In the next section, we will extend our understanding of linear transformations
    by covering one of the most important transformations related to ANNs—matrix multiplication.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将通过讲解与人工神经网络（ANN）相关的最重要的变换之一——矩阵乘法，来扩展我们对线性变换的理解。
- en: Matrix Multiplication
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: Matrix multiplication is fundamental to neural network operations. While the
    rules for addition are simple and intuitive, the rules for multiplication for
    matrices and tensors are more complex. Matrix multiplication involves more than
    simple element-wise multiplication of the elements. Instead, a more complicated
    procedure is implemented that involves the entire row of one matrix and an entire
    column of the other. In this section, we will explain how multiplication works
    for two-dimensional tensors or matrices; however, tensors of higher orders can
    also be multiplied.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法是神经网络运算的基础。虽然加法的规则简单直观，但矩阵和张量的乘法规则则更为复杂。矩阵乘法不仅仅是简单的逐元素相乘，而是实现了一个更复杂的过程，涉及一个矩阵的整个行和另一个矩阵的整个列。在这一部分中，我们将解释二维张量或矩阵的乘法如何运作；然而，较高阶的张量也可以进行乘法运算。
- en: Given a matrix, A = [aij]m x n, and another matrix, B = [bij]n x p , the product
    of the two matrices is C = AB = [Cij]m x p, and each element, cij, is defined
    element-wise as ![formula](img/B15777_02_10a.png). Note that the shape of the
    resultant matrix is the same as the outer dimensions of the matrix product or
    the number of rows of the first matrix and the number of columns of the second
    matrix. For the multiplication to work, the inner dimensions of the matrix product
    must match, or the number of columns of the first matrix and the number of columns
    of the second matrix.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个矩阵，A = [aij]m x n，另一个矩阵，B = [bij]n x p，两个矩阵的乘积是C = AB = [Cij]m x p，其中每个元素cij按元素定义为
    ![公式](img/B15777_02_10a.png)。注意，结果矩阵的形状与矩阵乘积的外维度相同，或者说是第一个矩阵的行数和第二个矩阵的列数。为了使乘法成立，矩阵乘积的内维度必须匹配，或者说第一个矩阵的列数和第二个矩阵的列数必须相同。
- en: 'The concept of inner and outer dimensions of matrix multiplication can be seen
    in the following figure:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的内外维度概念可以从下图中看到：
- en: '![Figure 2.11: A visual representation of the inner and outer dimensions in
    matrix multiplication'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11: 矩阵乘法中内外维度的可视化表示'
- en: '](img/B15777_02_11.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_11.jpg)'
- en: 'Figure 2.11: A visual representation of the inner and outer dimensions in matrix
    multiplication'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.11: 矩阵乘法中内外维度的可视化表示'
- en: 'Unlike matrix addition, matrix multiplication is not commutative, which means
    that the order of the matrices in the product matters:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 与矩阵加法不同，矩阵乘法不是交换律的，这意味着矩阵在乘积中的顺序很重要：
- en: '![Figure 2.12: Matrix multiplication is non-commutative'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12: 矩阵乘法不是交换律的'
- en: '](img/B15777_02_12.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_12.jpg)'
- en: 'Figure 2.12: Matrix multiplication is non-commutative'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.12: 矩阵乘法不是交换律的'
- en: 'For example, let''s say we have the following two matrices:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有以下两个矩阵：
- en: '![Figure 2.13: Two matrices, A and B'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13: 两个矩阵，A 和 B'
- en: '](img/B15777_02_13.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_13.jpg)'
- en: 'Figure 2.13: Two matrices, A and B'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：两个矩阵 A 和 B
- en: 'One way to construct the product is to have matrix **A** first, multiplied
    by **B**:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 构建乘积的一种方法是先将矩阵**A**与**B**相乘：
- en: '![Figure 2.14: Visual representation of matrix A multiplied by B'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14：矩阵 A 与 B 相乘的可视化表示'
- en: '](img/B15777_02_14.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_14.jpg)'
- en: 'Figure 2.14: Visual representation of matrix A multiplied by B'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：矩阵 A 与 B 相乘的可视化表示
- en: 'This results in a `2x2` matrix. Another way to construct the product is to
    have **B** first, multiplied by **A**:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到一个`2x2`矩阵。构建乘积的另一种方法是先将**B**与**A**相乘：
- en: '![Figure 2.15: Visual representation of matrix B multiplied by A'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：矩阵 B 与 A 相乘的可视化表示'
- en: '](img/B15777_02_15.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_15.jpg)'
- en: 'Figure 2.15: Visual representation of matrix B multiplied by A'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15：矩阵 B 与 A 相乘的可视化表示
- en: Here, we can see that the matrix that was formed from the product `3x3` matrix
    and is very different from the matrix that was formed from the product **AB**.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到由`3x3`矩阵的乘积形成的矩阵与由**AB**乘积形成的矩阵非常不同。
- en: Scalar-matrix multiplication is much more straightforward and is simply the
    product of every element in the matrix multiplied by the scalar so that λA = [λaij]m
    x n, where λ is a scalar and **A** is a matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 标量矩阵乘法要简单得多，它只是矩阵中每个元素与标量相乘的结果，因此 λA = [λaij]m x n，其中 λ 是标量，**A** 是矩阵。
- en: In the following exercise, we will put our understanding into practice by performing
    matrix multiplication in Python utilizing the `NumPy` library.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将通过使用`NumPy`库在 Python 中执行矩阵乘法来实践我们的理解。
- en: 'Exercise 2.03: Matrix Multiplication'
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.03：矩阵乘法
- en: 'In this exercise, we are going to demonstrate how to multiply matrices together.
    Follow these steps to complete this exercise:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将展示如何将矩阵相乘。请按照以下步骤完成此练习：
- en: Open a Jupyter notebook from the start menu to implement this exercise.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从开始菜单打开一个 Jupyter notebook 来实现此练习。
- en: 'To demonstrate the fundamentals of matrix multiplication, begin with two matrices
    of the same shape:'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了演示矩阵乘法的基本原理，从两个相同形状的矩阵开始：
- en: '[PRE44]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Since both matrices have the same shape and they are not square, they cannot
    be multiplied as is, otherwise, the inner dimensions of the product won''t match.
    One way we could resolve this is to take the transpose of one of the matrices;
    then, we would be able to perform the multiplication. Take the transpose of the
    second matrix, which would mean that a `(4x3) matrix` is multiplied by a `(3x4)
    matrix`. The result would be a `(4x4) matrix`. Perform the multiplication using
    the `dot` method:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这两个矩阵形状相同且不是方阵，因此不能直接相乘，否则，乘积的内维度将不匹配。我们可以通过对其中一个矩阵进行转置来解决这个问题；然后，就可以进行乘法运算。取第二个矩阵的转置，这意味着一个`(4x3)矩阵`与一个`(3x4)矩阵`相乘。结果将是一个`(4x4)矩阵`。使用`dot`方法进行乘法运算：
- en: '[PRE45]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The preceding code produces the following output:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生了以下输出：
- en: '[PRE46]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Take the transpose of the first matrix, which would mean that a `(3x4) matrix`
    is multiplied by a `(4x3) matrix`. The result would be a `(3x3) matrix`:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取第一个矩阵的转置，这意味着一个`(3x4)矩阵`与一个`(4x3)矩阵`相乘。结果将是一个`(3x3)矩阵`：
- en: '[PRE47]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code produces the following output:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生了以下输出：
- en: '[PRE48]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Reshape one of the arrays to make sure the inner dimension of the matrix multiplication
    matches. For example, we can reshape the first array to make it a `(3x4) matrix`
    instead of transposing. Note that the result is not the same as it is when transposing:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整其中一个数组的形状，以确保矩阵乘法的内维度匹配。例如，我们可以将第一个数组调整为`(3x4)矩阵`，而不是转置。请注意，结果与转置时的结果不同：
- en: '[PRE49]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code produces the following output:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生了以下输出：
- en: '[PRE50]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this exercise, we learned how to multiply two matrices together. The same
    concept can be applied to tensors of all ranks, not just second-order tensors.
    Tensors of different ranks can even be multiplied together if their inner dimensions
    match.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们学习了如何将两个矩阵相乘。这个概念不仅适用于二阶张量，还可以应用于所有阶张量。如果不同阶的张量的内维度匹配，它们甚至可以进行乘法运算。
- en: Note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/38p0RD7](https://packt.live/38p0RD7).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见[https://packt.live/38p0RD7](https://packt.live/38p0RD7)。
- en: You can also run this example online at [https://packt.live/2VYI1xZ](https://packt.live/2VYI1xZ).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在线运行此示例，网址是[https://packt.live/2VYI1xZ](https://packt.live/2VYI1xZ)。
- en: The next exercise demonstrates how to multiply three-dimensional tensors together.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 下一道练习展示了如何将三维张量相乘。
- en: 'Exercise 2.04: Tensor Multiplication'
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.04：张量乘法
- en: 'In this exercise, we are going to apply our knowledge of matrix multiplication
    to higher-order tensors. Follow these steps to complete this exercise:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将应用矩阵乘法知识到高阶张量。请按照以下步骤完成此练习：
- en: 'Open a Jupyter notebook from the start menu to implement this exercise. Begin
    by creating a three-dimensional tensor using the NumPy library and the `array`
    function. Import all the necessary dependencies:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开开始菜单中的Jupyter笔记本来实现此练习。首先使用NumPy库和`array`函数创建一个三维张量。导入所有必要的依赖：
- en: '[PRE51]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The preceding code produces the following output:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE52]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Confirm the shape using the `shape` method:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`shape`方法确认形状：
- en: '[PRE53]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This tensor has the shape (2x2x3).
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该张量的形状为(2x2x3)。
- en: 'Create a new `three-dimensional tensor` that we will be able to multiply the
    tensor by. Take the transpose of the original matrix:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`三维张量`，我们可以用它来与张量相乘。对原始矩阵取转置：
- en: '[PRE54]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code produces the following output:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE55]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Confirm the shape using the `shape` method:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`shape`方法确认形状：
- en: '[PRE56]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: This tensor has the shape (3x2x2).
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该张量的形状为(3x2x2)。
- en: 'Take the `dot` product of the `two matrices`, as follows:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取`两个矩阵`的`点积`，如下所示：
- en: '[PRE57]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding code produces the following output:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE58]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Look at the `shape` of this resultant tensor:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看此结果张量的`形状`：
- en: '[PRE59]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The preceding code produces the following output:'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE60]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now, we have a four-dimensional tensor.
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们有了一个四维张量。
- en: In this exercise, we learned how to perform matrix multiplication using the
    NumPy library in Python. While we do not have to perform matrix multiplication
    directly when we create ANNs with Keras, it is still useful to understand the
    underlying mathematics.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们学习了如何使用Python中的NumPy库执行矩阵乘法。尽管在使用Keras创建ANN时，我们不需要直接执行矩阵乘法，但理解其底层数学原理仍然是有用的。
- en: Note
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: To access the source code for this specific section, please refer to [https://packt.live/31G1rLn](https://packt.live/31G1rLn).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/31G1rLn](https://packt.live/31G1rLn)。
- en: You can also run this example online at [https://packt.live/2AriZjn](https://packt.live/2AriZjn).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，网址为[https://packt.live/2AriZjn](https://packt.live/2AriZjn)。
- en: Introduction to Keras
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras简介
- en: 'Building ANNs involves creating layers of nodes. Each node can be thought of
    as a tensor of weights that are learned in the training process. Once the ANN
    has been fitted to the data, a prediction is made by multiplying the input data
    by the weight matrices layer by layer, applying any other linear transformation
    when needed, such as activation functions, until the final output layer is reached.
    The size of each weight tensor is determined by the size of the shape of the input
    nodes and the shape of the output nodes. For example, in a single-layer ANN, the
    size of our single hidden layer can be thought of as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 构建ANN涉及创建节点的层。每个节点可以被视为在训练过程中学习的权重张量。一旦ANN被拟合到数据，预测是通过逐层将输入数据与权重矩阵相乘来进行的，当需要时应用任何其他线性变换，如激活函数，直到到达最终输出层。每个权重张量的大小由输入节点的形状和输出节点的形状决定。例如，在一个单层ANN中，我们的单一隐藏层的大小可以如下所示：
- en: '![Figure 2.16: Solving the dimensions of the hidden layer of a single-layer
    ANN'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.16：解决单层ANN的隐藏层维度'
- en: '](img/B15777_02_16.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_16.jpg)'
- en: 'Figure 2.16: Solving the dimensions of the hidden layer of a single-layer ANN'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16：解决单层ANN的隐藏层维度
- en: 'If the input matrix of features has `n` rows, or observations, and `m` columns,
    or features, and we want our predicted target to have `n` rows (one for each observation)
    and one column (the predicted value), we can determine the size of our hidden
    layer by what is needed to make the matrix multiplication valid. Here is the representation
    of a single-layer ANN:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入的特征矩阵有`n`行，或观测值，和`m`列，或特征，并且我们希望预测的目标有`n`行（每个观测值一个行）和一列（预测值），那么我们可以通过需要的方式来确定隐藏层的大小，以使矩阵乘法有效。以下是单层人工神经网络（ANN）的表示：
- en: '![Figure 2.17: Representation of a single-layer ANN'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17：单层ANN的表示'
- en: '](img/B15777_02_17.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_17.jpg)'
- en: 'Figure 2.17: Representation of a single-layer ANN'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17：单层ANN的表示
- en: Here, we can determine that the weight matrix will be of size (`mx1`) to ensure
    the matrix multiplication is valid.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以确定权重矩阵的大小将是（`mx1`），以确保矩阵乘法是有效的。
- en: If we have more than one hidden layer in an ANN, then we have much more freedom
    with the size of these weight matrices. In fact, the possibilities are endless,
    depending on how many layers there are and how many nodes we want in each layer.
    In practice, however, certain architecture designs work better than others, as
    we will be learning throughout this book.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在一个ANN中有多个隐藏层，那么我们在这些权重矩阵的大小上将有更多自由。事实上，可能性是无穷的，具体取决于层数和每层的节点数。然而，在实践中，某些架构设计比其他设计更有效，正如我们将在本书中学到的那样。
- en: In general, Keras abstracts much of the linear algebra out of building neural
    networks so that users can focus on designing the architecture. For most networks,
    only the input size, output size, and the number of nodes in each hidden layer
    are needed to create networks in Keras.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，Keras将神经网络构建中的大部分线性代数进行了抽象，使用户能够专注于设计架构。对于大多数网络，只需要输入大小、输出大小和每个隐藏层中的节点数即可在Keras中创建网络。
- en: 'The simplest model structure in Keras is the `Sequential` model, which can
    be imported from `keras.models`. The model of the `Sequential` class describes
    an ANN that consists of a linear stack of layers. A `Sequential` model can be
    instantiated as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中最简单的模型结构是`Sequential`模型，可以从`keras.models`导入。`Sequential`类的模型描述了一个由一系列层组成的人工神经网络（ANN）。可以如下实例化`Sequential`模型：
- en: '[PRE61]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Layers can be added to this model instance to create the structure of the model.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 可以向该模型实例添加层，以创建模型的结构。
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before initializing your model, it is helpful to set a seed using the `seed`
    function in NumPy's random library and the `set_seed` function from TensorFlow's
    random library.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化模型之前，使用NumPy的随机库中的`seed`函数和TensorFlow的随机库中的`set_seed`函数设置种子是很有帮助的。
- en: Layer Types
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层类型
- en: 'The notion of layers is part of the Keras core API. A layer can be thought
    of as a composition of nodes, and at each node, a set of computations happen.
    In Keras, all the nodes of a layer can be initialized by simply initializing the
    layer itself. The individual operation of a generalized layer node can be seen
    in the following diagram. At each node, the input data is multiplied by a set
    of weights using matrix multiplication, as we learned earlier in this chapter.
    The sum of the product between the weights and the input is applied, which may
    or may not include a bias, as shown by the input node equal to `1` in the following
    diagram. Further functions may be applied to the output of this matrix multiplication,
    such as activation functions:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 层的概念是Keras核心API的一部分。可以将层看作是节点的组合，在每个节点上发生一系列计算。在Keras中，层的所有节点都可以通过初始化该层本身来初始化。通用层节点的单独操作可以通过以下图示看到。在每个节点上，输入数据通过矩阵乘法与一组权重相乘，正如我们在本章之前所学到的那样。权重与输入的乘积之和会被应用，这可能包括或不包括偏置，如下图中输入节点等于`1`所示。还可以对该矩阵乘法的输出应用进一步的函数，例如激活函数：
- en: '![Figure 2.18: A depiction of a layer node'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18：层节点的示意图'
- en: '](img/B15777_02_18.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15777_02_18.jpg)'
- en: 'Figure 2.18: A depiction of a layer node'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18：层节点的示意图
- en: 'Some common layer types in Keras are as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中一些常见的层类型如下：
- en: '**Dense**: This is a fully connected layer in which all the nodes of the layer
    are directly connected to all the inputs and all the outputs. ANNs for classification
    or regression tasks on tabular data usually have a large percentage of their layers
    with this type in the architecture.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：这是一个完全连接的层，其中层的所有节点都直接连接到所有输入和所有输出。用于分类或回归任务的人工神经网络（ANNs）通常在其架构中有很大一部分层采用这种类型。'
- en: '**Convolutional**: This layer type creates a convolutional kernel that is convolved
    with the input layer to produce a tensor of outputs. This convolution can occur
    in one or multiple dimensions. ANNs for the classification of images usually feature
    one or more convolutional layers in their architecture.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**：这种层类型创建一个卷积核，并将其与输入层进行卷积，产生一个输出张量。此卷积可以发生在一个或多个维度上。用于图像分类的ANNs通常在其架构中包含一个或多个卷积层。'
- en: '**Pooling**: This type of layer is used to reduce the dimensionality of an
    input layer. Common types of pooling include max pooling, in which the maximum
    value of a given window is passed through to the output, or average pooling, in
    which the average value of a window is passed through. These layers are often
    used in conjunction with a convolutional layer, and their purpose is to reduce
    the dimensions of the subsequent layers, allowing for fewer training parameters
    to be learned with little information loss.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pooling**：这种类型的层用于减少输入层的维度。常见的池化类型包括最大池化，其中给定窗口的最大值被传递到输出，或平均池化，其中窗口的平均值被传递到输出。这些层通常与卷积层结合使用，目的是减少后续层的维度，从而减少训练参数的数量，同时尽可能减少信息损失。'
- en: '**Recurrent**: Recurrent layers learn patterns from sequences, so each output
    is dependent on the results from the previous step. ANNs that model sequential
    data such as natural language or time-series data often feature one or more recurrent
    layer types.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Recurrent**：递归层从序列中学习模式，因此每个输出依赖于前一步的结果。模拟序列数据（如自然语言或时间序列数据）的人工神经网络（ANNs）通常会包含一个或多个递归层类型。'
- en: There are other layer types in Keras; however, these are the most common types
    when it comes to building models using Keras.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中有其他层类型；然而，在使用 Keras 构建模型时，这些是最常见的层类型。
- en: 'Let''s demonstrate how to add layers to a model by instantiating a model of
    the `Sequential` class and adding a `Dense` layer to the model. Successive layers
    can be added to the model in the order in which we wish the computation to be
    performed and can be imported from `keras.layers`. The number of units, or nodes,
    needs to be specified. This value will also determine the shape of the result
    from the layer. A `Dense` layer can be added to a `Sequential` model in the following
    way:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实例化一个 `Sequential` 类的模型并向模型中添加一个 `Dense` 层来演示如何添加层。可以按我们希望执行计算的顺序，将后续层添加到模型中，这些层可以从
    `keras.layers` 导入。需要指定单元或节点的数量。这个值也会决定该层输出结果的形状。可以通过以下方式将 `Dense` 层添加到 `Sequential`
    模型中：
- en: '[PRE62]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Note
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: After the first layer, the input dimension does not need to be specified since
    it is determined from the previous layer.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层之后，不需要指定输入维度，因为它由前一层决定。
- en: Activation Functions
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'An activation function is generally applied to the output of a node to limit
    or bound its value. The value from each node is unbounded and may have any value,
    from negative to positive infinity. These can be troublesome within neural networks
    where the values of the weights and losses have been calculated and can head toward
    infinity and produce unusable results. Activation functions can help in this regard
    by bounding the value. Often, these activation functions push the value to two
    limits. Activation functions are also useful for deciding whether the node should
    be "fired" or not. Common activation functions are as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数通常应用于节点输出，以限制或限定其值。每个节点的值是没有界限的，可能具有任何值，从负无穷大到正无穷大。在神经网络中，这可能会带来问题，因为权重和损失的计算结果可能会趋向无穷大，产生无法使用的结果。激活函数在这方面可以提供帮助，通过限定值的范围。通常，这些激活函数将值推向两个极限。激活函数还用于决定节点是否应“激活”。常见的激活函数如下：
- en: 'The **Step** function: The value is nonzero if it is above a certain threshold;
    otherwise, it is zero.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Step** 函数：如果值超过某个阈值，则为非零值；否则为零。'
- en: 'The **Linear** function: ![formula](img/B15777_02_18a.png), which is a scalar
    multiplication of the input value.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linear** 函数：![公式](img/B15777_02_18a.png)，即输入值的标量乘法。'
- en: 'The **Sigmoid** function: ![formula](img/B15777_02_18b.png), such as a smoothed-out
    step function with smooth gradients. This activation function is useful for classification
    since the values are bound from zero to one.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid** 函数：![公式](img/B15777_02_18b.png)，例如平滑的阶跃函数，具有平滑的梯度。由于其值被限制在 0 到
    1 之间，这个激活函数对于分类任务非常有用。'
- en: The `x=0`.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x=0`。'
- en: 'The **ReLU** function: ![formula](img/B15777_02_18d.png), otherwise 0.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU** 函数：![公式](img/B15777_02_18d.png)，否则为 0。'
- en: Now that we have looked at some of the main components, we can begin to see
    how we might create useful neural networks out of these components. In fact, we
    can create a logistic regression model with all the concepts we have learned about
    in this chapter. A logistic regression model operates by taking the sum of the
    product of an input and a set of learned weights, followed by the output being
    passed through a logistic function. This can be achieved with a single-layer neural
    network with a sigmoid activation function.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些主要组件，我们可以开始看看如何利用这些组件创建有用的神经网络。事实上，我们可以用本章学到的所有概念来创建一个逻辑回归模型。逻辑回归模型通过计算输入与一组学习到的权重的乘积之和，然后将输出通过一个逻辑函数来操作。可以通过一个具有Sigmoid激活函数的单层神经网络来实现这一点。
- en: 'Activation functions can be added to models in the same manner that layers
    are added to models. The activation function will be applied to the output of
    the previous step in the model. A `tanh` activation function can be added to a
    `Sequential` model as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数可以以与添加层相同的方式添加到模型中。激活函数将应用于模型中前一步的输出。可以按如下方式向`Sequential`模型添加`tanh`激活函数：
- en: '[PRE63]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Note
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Activation functions can also be added to a model by including them as an argument
    when defining the layers.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数也可以通过将其作为参数添加到定义层时来添加到模型中。
- en: Model Fitting
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型拟合
- en: 'Once a model''s architecture has been created, the model must be compiled.
    The compilation process configures all the learning parameters, including which
    optimizer to use, the loss function to minimize, as well as optional metrics,
    such as accuracy, to calculate at various stages of the model training. Models
    are compiled using the `compile` method, as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型的架构创建完成，必须对模型进行编译。编译过程配置所有学习参数，包括使用的优化器、要最小化的损失函数，以及可选的指标（例如准确度），这些指标将在模型训练的各个阶段进行计算。模型使用`compile`方法进行编译，如下所示：
- en: '[PRE64]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'After the model has been compiled, it is ready to be fit to the training data.
    This is achieved with an instantiated model using the `fit` method. Useful arguments
    when using the `fit` method are as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型编译后，就可以将其拟合到训练数据上。通过使用`fit`方法实例化模型来实现这一点。使用`fit`方法时有以下有用的参数：
- en: '**X**: The array of the training feature data to fit the data to.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**：用于拟合数据的训练特征数据的数组。'
- en: '**y**: The array of the training target data.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**y**：训练目标数据的数组。'
- en: '**epochs**: The number of epochs to run the model for. An epoch is an iteration
    over the entire training dataset.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**epochs**：运行模型的训练周期数。一个训练周期是对整个训练数据集的一次迭代。'
- en: '**batch_size**: The number of training data samples to use per gradient update.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size**：每次梯度更新时使用的训练数据样本数。'
- en: '**validation_split**: The proportion of the training data to be used for validation
    that is evaluated after each epoch.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**validation_split**：用于验证的训练数据的比例，该数据将在每个训练周期后进行评估。'
- en: '**shuffle**: Indicates whether to shuffle the training data before each epoch.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**shuffle**：指示是否在每个训练周期前打乱训练数据。'
- en: 'The `fit` method can be used on a model in the following way:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法可以按以下方式在模型上使用：'
- en: '[PRE65]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'It is beneficial to save the output of calling the `fit` method of the model
    since it contains information on the model''s performance throughout training,
    including the loss, which is evaluated after each epoch. If a validation split
    is defined, the loss is evaluated after each epoch on the validation split. Likewise,
    if any metrics are defined in training, they are also calculated after each epoch.
    It is useful to plot such loss and evaluation metrics to determine model performance
    as a function of the epoch. The model''s loss as a function of the epoch can be
    visualized as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 调用模型的`fit`方法时，保存其输出是有益的，因为它包含了模型在整个训练过程中的性能信息，包括每个周期后的损失值。如果定义了验证分割，损失将在每个周期后对验证集进行评估。同样，如果在训练中定义了任何指标，这些指标也会在每个周期后进行计算。绘制这些损失和评估指标有助于确定模型在各个训练周期上的性能。模型的损失函数随训练周期变化的图形可以如下可视化：
- en: '[PRE66]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Keras models can be evaluated by utilizing the `evaluate` method of the model
    instance. This method returns the loss and any metrics that were passed to the
    model for training. The method can be called as follows when evaluating an out-of-sample
    test dataset:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Keras模型可以通过利用模型实例的`evaluate`方法进行评估。该方法返回损失值以及传递给模型用于训练的任何指标。当评估一个样本外的测试数据集时，可以按以下方式调用该方法：
- en: '[PRE67]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: These model-fitting steps represent the basic steps that need to be followed
    to build, train, and evaluate models using the Keras package. From here, there
    are an infinite number of ways to build and evaluate a model, depending on the
    task you wish to accomplish. In the following activity, we will create an ANN
    to perform the same task that we completed in *Chapter 1*, *Introduction to Machine
    Learning with Keras.* In fact, we will recreate the logistic regression algorithm
    with ANNs. As such, we expect there to be similar performance between the two
    models.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型拟合步骤代表了使用 Keras 包构建、训练和评估模型时需要遵循的基本步骤。从这里开始，构建和评估模型的方式有无数种，具体取决于您希望完成的任务。在接下来的活动中，我们将创建一个人工神经网络（ANN）来执行与*第一章*《使用
    Keras 的机器学习入门》相同的任务。事实上，我们将用 ANN 重现逻辑回归算法。因此，我们预计这两个模型之间的性能会相似。
- en: 'Activity 2.01: Creating a Logistic Regression Model Using Keras'
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：使用 Keras 创建逻辑回归模型
- en: In this activity, we are going to create a basic model using the Keras library.
    We will perform the same classification task that we did in *Chapter 1*, *Introduction
    to Machine Learning with Keras*. We will use the same online shopping purchasing
    intention dataset and attempt to predict the same variable.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将使用 Keras 库创建一个基本模型。我们将执行与*第一章*《使用 Keras 的机器学习入门》相同的分类任务。我们将使用相同的在线购物购买意图数据集，并尝试预测相同的变量。
- en: In the previous chapter, we used a logistic regression model to predict whether
    a user would purchase a product from a website when given various attributes about
    the online session's behavior and the attributes of the web page. In this activity,
    we will introduce the Keras library, though we'll continue to utilize the libraries
    we introduced previously, such as `pandas`, for easily loading in the data, and
    `sklearn`, for any data preprocessing and model evaluation metrics.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了逻辑回归模型来预测用户在给定关于在线会话行为和网页属性的各种特征时，是否会从网站购买产品。在本活动中，我们将介绍 Keras 库，尽管我们将继续使用之前介绍的库，如
    `pandas`，以便轻松加载数据，和 `sklearn`，用于任何数据预处理和模型评估指标。
- en: Note
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Preprocessed datasets have been provided for you to use for this activity. You
    can download them from [https://packt.live/2ApIBwT](https://packt.live/2ApIBwT).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 已为您提供了预处理过的数据集，可以用于本次活动。您可以从 [https://packt.live/2ApIBwT](https://packt.live/2ApIBwT)
    下载它们。
- en: 'The steps to complete this activity are as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Load in the processed feature and target datasets.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载处理后的特征数据和目标数据集。
- en: Split the training and target data into training and test datasets. The model
    will be fit to the training dataset and the test dataset will be used to evaluate
    the model.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据和目标数据分割为训练集和测试集。模型将拟合训练集，测试集将用于评估模型。
- en: Instantiate a model of the `Sequential` class from the `keras.models` library.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `keras.models` 库中实例化一个 `Sequential` 类的模型。
- en: Add a single layer of the `Dense` class from the `keras.layers` package to the
    model instance. The number of nodes should be equal to the number of features
    in the feature dataset.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型实例中添加一个 `keras.layers` 包中的 `Dense` 类单层。节点数应等于特征数据集中的特征数量。
- en: Add a sigmoid activation function to the model.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型添加一个 sigmoid 激活函数。
- en: Compile the model instance by specifying the optimizer to use, the loss metric
    to evaluate, and any other metrics to evaluate after each epoch.
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定要使用的优化器、评估损失指标以及每个 epoch 后要评估的其他指标来编译模型实例。
- en: Fit the model to the training data, specifying the number of epochs to run for
    and the validation split to use.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到训练数据，指定要运行的 epoch 数量和使用的验证集划分比例。
- en: Plot the loss and other evaluation metrics with respect to the epoch that will
    be evaluated on the training and validation datasets.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失和其他评估指标与 epoch 之间的关系，这些指标将在训练集和验证集上进行评估。
- en: Evaluate the loss and other evaluation metrics on the test dataset.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据集上评估损失和其他评估指标。
- en: 'After implementing these steps, you should get the following expected output:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，您应该会得到以下预期输出：
- en: '[PRE68]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 356.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 356 页找到。
- en: In this activity, we looked at some of the fundamental concepts of creating
    ANNs in Keras, including various layer types and activation functions. We used
    these components to create a simple logistic regression model using a package
    that gives us similar results to the logistic regression model we used in *Chapter
    1*, *Introduction to Machine Learning with Keras*. We learned how to build the
    model with the Keras library, train the model with a real-world dataset, and evaluate
    the performance of the model on a test dataset to provide an unbiased evaluation
    of the performance of the model.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们探讨了在 Keras 中创建人工神经网络（ANN）的一些基本概念，包括各种层类型和激活函数。我们使用这些组件创建了一个简单的逻辑回归模型，该模型给出的结果与我们在*第一章*《Keras与机器学习入门》中使用的逻辑回归模型相似。我们学习了如何使用
    Keras 库构建模型，使用实际数据集训练模型，并在测试数据集上评估模型的表现，以便提供对模型表现的公正评估。
- en: Summary
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the various types of linear algebra components and
    operations that pertain to machine learning. These components include scalars,
    vectors, matrices, and tensors. The operations that were applied to these tensors
    included addition, transposition, and multiplication—all of which are fundamental
    for understanding the underlying mathematics of ANNs.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了与机器学习相关的各种线性代数组件和操作。这些组件包括标量、向量、矩阵和张量。对这些张量应用的操作包括加法、转置和乘法——这些操作都是理解
    ANN 底层数学的基础。
- en: We also learned some of the basics of the Keras package, including the mathematics
    that occurs at each node. We replicated the model from the previous chapter, in
    which we built a logistic regression model to predict the same target from the
    online shopping purchasing intention dataset. However, in this chapter, we used
    the Keras library to create the model using an ANN instead of the scikit-learn
    logistic regression model. We achieved a similar level of accuracy using ANNs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了 Keras 包的一些基础知识，包括每个节点上发生的数学运算。我们复现了上一章中的模型，在该模型中，我们构建了一个逻辑回归模型来预测来自在线购物购买意图数据集的相同目标。然而，在这一章中，我们使用
    Keras 库创建了一个基于人工神经网络（ANN）的模型，而不是使用 scikit-learn 的逻辑回归模型。我们通过使用 ANN 实现了相似的准确性。
- en: The upcoming chapters of this book will use the same concepts we learned about
    in this chapter; however, we will continue building ANNs with the Keras package.
    We will extend our ANNs to more than a single layer by creating models that have
    multiple hidden layers. By adding multiple hidden layers to our ANNs, we will
    put the "deep" into "deep learning". We will also tackle the issues of underfitting
    and overfitting since they are related to training models with ANNs.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 本书接下来的章节将继续使用我们在本章中学到的相同概念；然而，我们将继续使用 Keras 包构建更复杂的 ANN。我们将通过创建具有多个隐藏层的模型，将
    ANN 扩展到超过单层的结构。通过向我们的 ANN 中添加多个隐藏层，我们将把“深度”带入“深度学习”。我们还将解决欠拟合和过拟合的问题，因为它们与使用 ANN
    训练模型相关。
