- en: Imitation and Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿与迁移学习
- en: At the time of writing, a new AI called AlphaStar, a **deep reinforcement learning**
    (**DRL**) agent, used **imitation learning** (**IL**) to beat a human opponent
    five-nil playing the real-time strategy game StarCraft II. AlphaStar was the continuation
    of David Silver and Google DeepMind's work to build a smarter and more intelligent
    AI. The specific techniques AlphaStar used to win could fill a book, and IL and
    the use of learning to copy human play is now of keen interest. Fortunately, Unity
    has already implemented IL in the form of offline and online training scenarios.
    While we won't make it to the level of AlphaStar in this chapter, we still will
    learn about the underlying technologies of IL and other forms of transfer learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文撰写时，一种名为 AlphaStar 的新 AI，**深度强化学习**（**DRL**）代理，使用**模仿学习**（**IL**）在实时战略游戏《星际争霸
    II》中以五比零击败了人类对手。AlphaStar 是 David Silver 和 Google DeepMind 为建立更智能、更强大 AI 的工作的延续。AlphaStar
    使用的具体技术可以写成一本书，而 IL 和模仿人类游戏的学习方法如今受到高度关注。幸运的是，Unity 已经在离线和在线训练场景的形式中实现了 IL。尽管我们在这一章中不会达到
    AlphaStar 的水平，但我们仍将了解 IL 和其他形式的迁移学习的基础技术。
- en: 'In this chapter, we will look at the implementation of IL in ML-Agents and
    then look to other applications of transfer learning. We will cover the following
    topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将研究 ML-Agents 中 IL 的实现，然后探讨其他迁移学习的应用。我们将在本章中涵盖以下内容：
- en: IL or behavioral cloning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IL 或行为克隆
- en: Online training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线训练
- en: Offline training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线训练
- en: Transfer Learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Imitation Transfer Learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿迁移学习
- en: While AlphaStar performed a stunning tactical victory against a human pro player
    in an RTS game, it has still come under scrutiny for the type of play and actions
    it used. Many human players stated that the AI's tactical abilities were clearly
    superior, but the overall strategy and planning were abysmal. It should be interesting
    to see how Google DeepMind approaches this criticism.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AlphaStar 在一场实时战略游戏中对人类职业玩家取得了惊人的战术胜利，但它仍然因所使用的游戏方式和动作类型而受到审视。许多玩家表示，AI 的战术能力显然优于人类，但整体的战略规划则非常糟糕。看看
    Google DeepMind 如何应对这一批评应该会很有趣。
- en: This will be an exciting chapter, and will provide you with plenty of training
    possibilities for your future developments, which all starts in the next section.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个令人兴奋的章节，并将为你的未来开发提供大量的训练可能性，一切从下一节开始。
- en: IL, or behavioral cloning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IL 或行为克隆
- en: IL, or behavioral cloning, is the process by which observations and actions
    are captured from a human, or perhaps another AI, and used as input into training
    an agent. The agent essentially becomes guided by the human and learns by their
    actions and observations. A set of learning observations can be received by real-time
    play (online) or be extracted from saved games (offline). This provides the ability
    to capture play from multiple agents and train them in tandem or individually.
    IL provides the ability to train or, in effect, program agents for tasks you may
    find impossible to train for using regular RL, and because of this, it will likely
    become a key RL technique that we use for most tasks in the near future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: IL 或行为克隆是通过从人类或可能是另一种 AI 捕捉观察和动作，并将其作为输入用于训练代理的过程。代理本质上由人类引导，并通过他们的动作和观察进行学习。一组学习观察可以通过实时游戏（在线）接收，或者从已保存的游戏中提取（离线）。这提供了捕捉多个代理的游戏并同时或单独训练它们的能力。IL
    提供了训练代理的能力，或者实际上是为你可能无法通过常规 RL 训练的任务编程代理，正因如此，它很可能成为我们在不久的将来用于大多数任务的关键 RL 技术。
- en: 'It is hard to gauge the value something gives you until you see what things
    are like without it. With that in mind, we will first start by looking at an example
    that uses no IL, but certainly could benefit from it. Open up the Unity editor
    and follow this exercise:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 很难评估某个东西给你带来的价值，直到你看到没有它的情况。考虑到这一点，我们首先将通过一个没有使用 IL，但显然可以受益于 IL 的例子开始。打开 Unity
    编辑器并按照以下步骤进行练习：
- en: Open up the Tennis scene from the Assets | ML-Agents | Examples | Tennis | Scenes
    folder.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 Assets | ML-Agents | Examples | Tennis | Scenes 文件夹中的 Tennis 场景。
- en: Select and disable the extra agent training areas, TennisArea(1) to TennisArea(17).
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并禁用额外的代理训练区域，TennisArea(1) 到 TennisArea(17)。
- en: Select AgentA and make sure Tennis Agent | Brain is set to TennisLearning. We
    want each agent to be against the other agent in this example.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 AgentA，并确保 Tennis Agent | Brain 设置为 TennisLearning。我们希望每个代理在这个例子中互相对抗。
- en: Select AgentB and make sure Tennis Agent | Brain is set to TennisLearning.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择AgentB并确保Tennis Agent | Brain设置为TennisLearning。
- en: In this example, for a brief instance, we are training multiple agents in the
    same environment. We will cover more scenarios where agents play other agents
    as a way of learning in [Chapter 11](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml),
    *Building Multi-Agent Environments*.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个示例中，短时间内我们正在同一环境中训练多个代理。我们将在[第11章](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml)《构建多代理环境》中讨论更多代理与其他代理进行学习的场景。
- en: 'Select Academy and make sure that Tennis Academy | Brains is set to TennisLearning
    and the Control option is enabled, as shown in the following screenshot:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Academy并确保Tennis Academy | Brains设置为TennisLearning，并且控制选项已启用，如下图所示：
- en: '![](img/cd61436d-3edf-44b8-9966-13f9ab48b633.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd61436d-3edf-44b8-9966-13f9ab48b633.png)'
- en: Setting Control to enabled on Academy
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Academy上将控制设置为启用
- en: 'Open a Python/Anaconda window and prepare it for training. We will launch training
    with the following command:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Python/Anaconda窗口并为训练做准备。我们将通过以下命令启动训练：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Watch the training for several thousand iterations, enough to convince yourself
    the agents are not going to learn this task easily. When you are convinced, stop
    the training and move on.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观看训练过程几千次，足以让你确信代理不会轻易学会这个任务。当你确信之后，停止训练并继续进行。
- en: You can see by just looking at this first example that ordinary training and
    the other advanced methods we looked at, such as Curriculum and Curiosity Learning,
    would be difficult to implement, and in this case could be counterproductive.
    In the next section, we look at how to run this example with IL in online training
    mode.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过查看这个第一个示例，你就可以发现普通训练以及我们所讨论的其他高级方法，如课程学习和好奇心学习，都会很难实现，而且在这种情况下可能会适得其反。在接下来的部分中，我们将展示如何在在线训练模式下使用IL运行这个示例。
- en: Online training
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线训练
- en: 'Online Imitation Learning is where you teach the agent to learn the observations
    of a player or another agent in real time. It also is one of the most fun and
    engaging ways to train agents or bots. Let''s jump in and set up the tennis environment
    for online Imitation Learning in the next exercise:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在线模仿学习是指你教代理实时学习一个玩家或另一个代理的观察。它也是训练代理或机器人最有趣且吸引人的方式之一。接下来，我们将跳入并为在线模仿学习设置网球环境：
- en: Select the TennisArea | AgentA object and set Tennis Agent | Brain to TennisPlayer.
    In this IL scenario, we have one brain acting as a teacher, the player, and a
    second brain acting as the student, the learner.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择TennisArea | AgentA对象，并将Tennis Agent | Brain设置为TennisPlayer。在这个IL场景中，我们有一个大脑作为教师，即玩家，另一个大脑作为学生，即学习者。
- en: Select the AgentB object and make sure Tennis Agent | Brain is set to TennisLearning.
    This will be the student brain.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择AgentB对象并确保Tennis Agent | Brain设置为TennisLearning。这将是学生大脑。
- en: Open the `online_bc_config.yaml` file from the `ML-Agents/ml-agents/config`
    folder. IL does not use the same configuration as PPO so the parameters will have
    similar names but may not respond to what you have become used to.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`ML-Agents/ml-agents/config`文件夹中的`online_bc_config.yaml`文件。IL使用的配置与PPO不同，因此这些参数的名称可能相似，但可能不会像你所习惯的那样响应。
- en: 'Scroll down in the file to the **`TennisLearning`** brain configuration as
    shown in the following code snippet:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件中向下滚动，找到**`TennisLearning`**大脑配置，如以下代码片段所示：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Looking over the hyperparameters, we can see there are two new parameters of
    interest. A summary of those parameters is as follows:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仔细查看超参数，我们可以看到有两个新的参数需要关注。以下是这些参数的总结：
- en: '`trainer`: `online_` *or* `offline_bc`—using online or offline Behavioral Cloning.
    In this case, we are performing online.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainer`: `online_` *或* `offline_bc`—使用在线或离线行为克隆。在这种情况下，我们正在进行在线训练。'
- en: '`brain_to_imitate`: `TennisPlayer`—this sets the brain that the learning brain
    should attempt to imitate.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`brain_to_imitate`: `TennisPlayer`—这设置了学习大脑应尝试模仿的目标大脑。'
- en: '*We won''t make any changes to the file at this point.*'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*此时我们不会对文件进行任何更改。*'
- en: 'Open your prepared Python/Anaconda window and launch training with the following
    command:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你准备好的Python/Anaconda窗口，并通过以下命令启动训练：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After you press Play in the editor, you will be able to control the left paddle
    with the *W*, *A*, *S*, *D* keys. Play the game, and you may be surprised at how
    quickly the agent learns and can get quite good. The following is an example of
    the game being played:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中按下播放按钮后，你将能够用*W*、*A*、*S*、*D*键控制左边的挡板。玩游戏时，你可能会惊讶于代理学习的速度，它可能变得相当优秀。以下是游戏进行中的一个示例：
- en: '![](img/77df8273-6f46-4876-aa07-4564b8092696.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77df8273-6f46-4876-aa07-4564b8092696.png)'
- en: Playing and teaching the agent with IL
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用IL进行代理的播放和教学
- en: Keep playing the example until completion if you like. It can also be interesting
    to switch players during a game, or even train the brain and use the trained model
    to play against later. You do remember how to run a trained model, right?
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果愿意，继续玩完示例。也可以有趣的是在游戏过程中切换玩家，甚至训练大脑并使用训练好的模型进行后续对战。你记得怎么运行训练好的模型吗？
- en: At some point while playing through the last exercise, you may have wondered
    why we don't we train all RL agents this way. A good question, but as you can
    imagine, it depends. While IL is very powerful, and quite a capable learner, it
    doesn't always do what we expect it to do. Also, an IL agent is only going to
    learn the search space (observations) it is shown and remain within those limitations.
    In the case of AlphaStar, IL was the main input for training, but the team also
    mentioned that the AI did have plenty of time to self-play, which likely accounted
    for many of its winning strategies. So, while IL is cool and powerful, it is not
    the golden goose that will solve all our RL problems. However, you are likely
    to have a new and greater appreciation for RL, and in particular IL, after this
    exercise. In the next section, we explore using offline IL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成上一个练习的过程中，您可能会想，为什么我们不以这种方式训练所有的RL代理。这是一个很好的问题，但正如您可以想象的那样，这取决于情况。虽然IL非常强大，且是一个相当能干的学习者，但它并不总是会按预期工作。而且，IL代理仅会学习它所看到的搜索空间（观察），并且只会停留在这些限制内。以AlphaStar为例，IL是训练的主要输入，但团队也提到，AI确实有很多时间进行自我对战，这可能是它许多获胜策略的来源。所以，虽然IL很酷且强大，但它并不是解决所有RL问题的“金蛋”。然而，在完成本练习后，您很可能会对RL，尤其是IL，产生新的更深的理解。在下一部分，我们将探索使用离线IL。
- en: Offline training
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线训练
- en: Offline training is where a recorded gameplay file is generated from a player
    or agent playing a game or performing a task, and is then fed back as training
    observations to help an agent learn later on. While online learning certainly
    is more fun, and in some ways more applicable to the Tennis scene or other multiplayer
    games, it is less practical. After all, you generally need to play an agent in
    real time for several hours before an agent will become good. Likewise, in online
    training scenarios, you are typically limited to single agent training, whereas
    in offline training a demo playback can be fed to multiple agents for better overall
    learning. This also allows us to perform interesting training scenarios, similar
    to AlphaStar training, where we can teach an agent so that it can teach other
    agents.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 离线训练是通过玩家或代理在游戏中进行游戏或执行任务时生成的录制游戏文件，然后将其作为训练观察反馈给代理，帮助代理进行后续学习。虽然在线学习当然更有趣，在某些方面更适用于网球场景或其他多人游戏，但它不太实际。毕竟，通常您需要让代理实时玩好几个小时，代理才会变得优秀。同样，在在线训练场景中，您通常只能进行单代理训练，而在离线训练中，可以将演示播放提供给多个代理，以实现更好的整体学习。这还允许我们执行有趣的训练场景，类似于AlphaStar的训练，我们可以教一个代理，让它去教其他代理。
- en: We will learn more about multi-agent gameplay in [Chapter 11](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml), *Building
    Multi-Agent Environments*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[第11章](15e7adeb-8b67-4b93-81d4-5f129772cd97.xhtml)中深入学习多代理游戏玩法，*构建多代理环境*。
- en: 'For this next exercise, we are going to revisit our old friend the Hallway/VisualHallway
    example. Again, we are doing this so we can compare our results to the previous
    sample exercises we ran with this environment. Follow this exercise to set up
    a new offline training session:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将重新访问我们老朋友“Hallway/VisualHallway”示例。再次这么做是为了将我们的结果与之前使用该环境运行的示例练习进行比较。请按照本练习的步骤设置一个新的离线训练会话：
- en: Clone and download the ML-Agents code to a new folder, perhaps choosing `ml-agents_b`, `ml-agents_c`,
    or some other name. The reason we do this is to make sure that we run these new
    exercises with a clean environment. Also, it can sometimes help to go back to
    old environments and recall settings or configuration that you may forget to update.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆并下载ML-Agents代码到一个新的文件夹，可能选择`ml-agents_b`、`ml-agents_c`或其他名称。这样做的原因是为了确保我们在干净的环境中运行这些新练习。此外，有时返回到旧环境并回忆可能忘记更新的设置或配置也能有所帮助。
- en: Launch Unity and open the **UnitySDK** project and the Hallway or VisualHallway
    scene, your choice.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Unity并打开**UnitySDK**项目以及Hallway或VisualHallway场景，您可以选择其中之一。
- en: The scene should be set to run in Player mode. Just confirm this. If you need
    to change it, then do so.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 场景应设置为以播放器模式运行。只需确认这一点。如果需要更改，请进行更改。
- en: Disable any additional agent training environments in the scene if others are
    active.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果场景中有其他活动的代理训练环境，请禁用它们。
- en: Select HallwayArea | Agent in the Hierarchy window.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图中选择HallwayArea | Agent。
- en: 'Click the Add Component button at the bottom of the Inspector window, type
    `demo`, and select the Demonstration Recorder component as shown in the following
    screenshot:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击Inspector窗口底部的Add Component按钮，输入`demo`，并选择演示录制组件，如下图所示：
- en: '![](img/76cb16eb-4858-4a5c-b6a5-a95d32b15a5f.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76cb16eb-4858-4a5c-b6a5-a95d32b15a5f.png)'
- en: Adding a Demonstration Recorder
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 添加演示录制器
- en: Click Record on the new Demonstration Recorder component, as shown in the preceding
    screenshot, check throughout. Also, fill in the Demonstration Name property of
    the recording, which is also shown.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前面的截图所示，点击新建的演示录制组件上的“Record”按钮，确保检查所有选项。同时，填写录制的“演示名称”属性，如图所示。
- en: Save the scene and project.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: Press Play and play the scene for a fair amount of time, more than a few minutes
    but perhaps less than hours. Of course, how well you play will also determine
    how well the agent learns. If you play poorly, so will the agent.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下Play按钮并播放场景一段时间，至少几分钟，可能不到几个小时。当然，你的游戏表现也会决定代理的学习效果。如果你玩的不好，代理也会学得差。
- en: After you think enough time has passed, and you have played as well as you could,
    stop the game.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你认为足够的时间已经过去，并且你已经尽力完成了游戏后，停止游戏。
- en: After playing the game, you should see a new folder called Demonstrations created
    in the Assets root folder in your Project window. Inside the folder will be your
    demonstration recording. This is the recording we will feed the agent in the next
    section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏播放结束后，你应该会看到一个名为Demonstrations的新文件夹，在项目窗口的Assets根文件夹中创建。文件夹内将包含你的演示录制。这就是我们在下一部分将喂入代理的数据。
- en: Setting up for training
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置训练环境
- en: 'Now that we have our demonstration recording, we can do more on the training
    part. This time, however, we will play back our observation file to multiple agents
    in multiple environments. Open the Hallway/VisualHallway sample scene and follow
    the next exercise to set up for training:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了演示录制，可以继续进行训练部分。然而，这一次，我们将把观察文件回放给多个代理，在多个环境中进行训练。打开Hallway/VisualHallway示例场景，并按照以下练习设置训练：
- en: Select and enable all the HallwayArea training environments HallwayArea(1) to
    HallwayArea(15)
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并启用所有HallwayArea训练环境，HallwayArea(1)到HallwayArea(15)。
- en: 'Select HallwayArea | Agent in the Hierarchy and then switch Hallway Agent |
    Brain to HallwayLearning, as shown in the following screenshot:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图中选择HallwayArea | Agent，然后将Hallway Agent | Brain切换为HallwayLearning，如下图所示：
- en: '![](img/984fc851-9110-491a-83b8-37da2b240c42.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/984fc851-9110-491a-83b8-37da2b240c42.png)'
- en: Setting the agent components
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 设置代理组件
- en: Also, select and disable the Demonstration Recording component as shown in the
    preceding screen excerpt
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，选择并禁用演示录制组件，如前面的屏幕截图所示。
- en: Make sure all the agents in the scene are using HallwayLearning brains
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保场景中的所有代理都使用HallwayLearning大脑。
- en: 'Select Academy in the Hierarchy and then enable the Hallway Academy | Brains
    | Control option as shown in the following screenshot:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图中选择Academy，然后启用Hallway Academy | Brains | Control选项，如下图所示：
- en: '![](img/85138784-78f7-4990-ab15-0c0b38439ae5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85138784-78f7-4990-ab15-0c0b38439ae5.png)'
- en: Enabling Academy to Control the Brains
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 启用Academy控制大脑
- en: Save the scene and project
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目
- en: Now that we have the scene configured for agent learning, we can move on to
    feeding the agent in the next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经为代理学习配置了场景，可以进入下一部分，开始喂入代理数据。
- en: Feeding the agent
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喂入代理数据
- en: When we performed online IL, we only fed one agent at a time in the tennis scene.
    This time, however, we are going to train multiple agents from the same demonstration
    recording in order to improve training performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行在线IL时，我们每次只给一个代理喂入数据，场景是网球场。然而，这次我们将从同一个演示录制中训练多个代理，以提高训练效果。
- en: 'We have already set up for training, so let''s start feeding the agent in the
    following exercise:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为训练做好了准备，现在开始在接下来的练习中喂入代理数据：
- en: Open a Python/Anaconda window and set it up for training from the new `ML-Agents`
    folder. You did reclone the source, right?
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Python/Anaconda窗口，并从新的`ML-Agents`文件夹中设置训练环境。你已经重新克隆了源代码，对吧？
- en: 'Open the `offline_bc_config.yaml` file from the `ML-Agents/ml-agents_b/config`
    folder. The contents of the file are as follows for reference:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`ML-Agents/ml-agents_b/config`文件夹中打开`offline_bc_config.yaml`文件。文件内容如下，供参考：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Change the last line of the `HallwayLearning` or `VisualHallwayLearning` brain
    to the following:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`HallwayLearning`或`VisualHallwayLearning`大脑的最后一行更改为以下内容：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that if you are using the `VisualHallwayLearning` brain, you will need
    to also change the name in the preceding config script.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，如果你使用的是`VisualHallwayLearning`大脑，你还需要在前面的配置脚本中更改相应的名称。
- en: Save your changes when you are done editing.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成编辑后，保存你的更改。
- en: 'Go back to your Python/Anaconda window and launch training with the following
    command:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回你的Python/Anaconda窗口，使用以下命令启动训练：
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When prompted, press Play in the editor and watch the training unfold. You will
    see the agent play using very similar moves to yourself, and if you played well,
    the agent will quickly start learning and you should see some impressive training,
    all thanks to IL.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时，在编辑器中按下Play并观看训练过程。你会看到代理使用与自己非常相似的动作进行游戏，如果你玩的不错，代理将很快开始学习，你应该会看到一些令人印象深刻的训练成果，这一切都得益于模仿学习。
- en: RL can be thought of as the brute-force approach to learning, while the refinement
    of Imitation Learning and training by observation will clearly dominate the future
    of agent training. Of course, is it really any wonder? After all, we simple humans
    learn that way.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以被看作是一种蛮力学习方法，而模仿学习和通过观察训练的改进无疑将主导未来的代理训练。当然，难道这真的是令人惊讶的事情吗？毕竟，我们这些简单的人类就是这样学习的。
- en: In the next section, we look at another exciting area of deep learning, transfer
    learning, and how it applies to games and DRL.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将探讨深度学习的另一个令人兴奋的领域——迁移学习，以及它如何应用于游戏和深度强化学习（DRL）。
- en: Transfer learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Imitation Learning, by definition, falls into a category of **Transfer Learning**
    (**TL**). We can define Transfer Learning as the process by which an agent or
    DL network is trained by transference of experiences from one to the other. This
    could be as simple as the observation training we just performed, or as complex
    as swapping layers/layer weights in an agent's brain, or just training an agent
    on a similar task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习，按定义属于**迁移学习**（**TL**）的一种类型。我们可以将迁移学习定义为一个代理或深度学习网络通过将经验从一个任务转移到另一个任务来进行训练的过程。这可以像我们刚才进行的观察训练那样简单，或者像在代理的大脑中交换层/层权重，或者仅仅在一个相似的任务上训练代理那样复杂。
- en: Intransfer learningwe need to make sure the experiences or previous weights
    we use are generalized. Through the foundational chapters in this book (chapters
    1-3), we learned the value of generalization using techniques such as dropout
    and batch normalization. We learned that these techniques are important for more
    general training; the form of training that allows the agent/network better inference
    on test data. This is no different than if we were to use an agent trained on
    one task to learn on another task. A more general agent will, in effect, be able
    to transfer knowledge more readily than a specialist agent could, if at all.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们需要确保我们使用的经验或先前的权重是可以泛化的。通过本书的基础章节（第1-3章），我们学习了使用诸如Dropout和批量归一化等技术进行泛化的价值。我们了解到，这些技术对于更通用的训练非常重要；这种训练方式使得代理/网络能够更好地推理测试数据。这与我们使用一个在某个任务上训练的代理去学习另一个任务是一样的。一个更通用的代理，实际上比一个专门化的代理更容易转移知识，甚至可能完全不同。
- en: 'We can demonstrate this in a quick example starting with training the following
    simple exercise:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个快速的示例来演示这一点，开始训练以下简单的练习：
- en: Open up the VisualHallway scene in the Unity editor.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Unity编辑器中打开VisualHallway场景。
- en: Disable any additional training areas.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用任何额外的训练区域。
- en: Confirm that Academy is in Control of the Brain.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认Academy控制大脑。
- en: 'Select the VisualHallwayLearning brain from the Hallway/Brains folder and set Vector
    Action | Branches Size | Branch 0 Size to `7`, as shown in the following screenshot:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Hallway/Brains文件夹中选择VisualHallwayLearning大脑，并将Vector Action | Branches Size
    | Branch 0 Size设置为`7`，如下面的截图所示：
- en: '![](img/4d4cb9a9-503a-43d5-8c7f-bdb2c1970824.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d4cb9a9-503a-43d5-8c7f-bdb2c1970824.png)'
- en: Increasing the vector action space of the agent
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 增加代理的向量动作空间
- en: We increase the action space for the brain so that it is compatible with the
    required action space for our transfer learning environment, which we will get
    to later.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们增加了大脑的动作空间，使其与我们的迁移学习环境所需的动作空间兼容，稍后我们会详细介绍。
- en: Save the scene and project.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: Open a Python/Anaconda window that is prepared for training.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个准备好的Python/Anaconda窗口以进行训练。
- en: 'Launch a training session with the following code:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码启动训练会话：
- en: '[PRE6]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we have introduced a new parameter that controls the frequency at which
    model checkpoints are created. The default is currently set to 50,000, but we
    just don't want to wait that long.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们引入了一个新的参数，用于控制模型检查点创建的频率。目前，默认值设置为50,000，但我们不想等这么久。
- en: 'Run the agent in training in the editor for at least one model checkpoint save,
    as shown in the following screen excerpt:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中运行代理进行训练，至少保存一个模型检查点，如下图所示：
- en: '![](img/32d4ca69-78e1-4a52-b2a4-f386d69cf508.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32d4ca69-78e1-4a52-b2a4-f386d69cf508.png)'
- en: The ML-Agents trainer creating a checkpoint
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents训练器正在创建一个检查点。
- en: Checkpoints are a way of taking snapshots of a brain and saving them for later.
    This allows you to go back and continue training where you left off.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查点是一种获取大脑快照并将其保存以供后用的方法。这允许你返回并继续从上次停止的地方进行训练。
- en: Let the agent train to a checkpoint and then terminate training by pressing
    *Ctrl *+ *C *or c*ommand *+ *C* on Mac in the Python/Anaconda window.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理训练到一个检查点，然后通过按*Ctrl* + *C*（在Python/Anaconda窗口中）或在Mac上按*command* + *C*来终止训练。
- en: When you have terminated training, it is time to try this saved brain on another
    learning environment in the next section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当你终止训练后，是时候在下一个部分尝试将这个已保存的大脑应用到另一个学习环境中了。
- en: Transferring a brain
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转移大脑。
- en: We now want to take the brain we have just been training and reuse it in a new,
    but similar, environment. Since our agent uses visual observations, this makes
    our task easier, but you could try and perform this example with other agents
    as well.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在想将刚刚训练过的大脑带入一个新的、但相似的环境中重新使用。由于我们的代理使用视觉观察，这使得任务变得更简单，但你也可以尝试用其他代理执行这个示例。
- en: 'Let''s open Unity and navigate to the VisualPushBlock example scene and follow
    this exercise:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Unity，进入VisualPushBlock示例场景并按照这个练习操作：
- en: Select Academy and enable it for Control of the Brains.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Academy并启用它来控制大脑。
- en: Select the Agent and set it to use the VisualPushBlockLearning brain. You should
    also confirm that this brain is configured in the same way as the VisualHallwayLearning
    brain we just ran, meaning that the Visual Observation and Vector Action spaces
    match.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择代理并设置它使用VisualPushBlockLearning大脑。你还应该确认这个大脑的配置与我们刚才运行的VisualHallwayLearning大脑相同，即视觉观察和向量动作空间相匹配。
- en: Open the `ML-Agents/ml-agents_b/models/vishall-0` folder in File Explorer or
    another file explorer.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件资源管理器或其他文件浏览器中打开`ML-Agents/ml-agents_b/models/vishall-0`文件夹。
- en: 'Change the name of the file and folder from `VisualHallwayLearning` to `VisualPushBlockLearning` as
    shown in the following screenshot:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件和文件夹的名称从`VisualHallwayLearning`更改为`VisualPushBlockLearning`，如以下截图所示：
- en: '![](img/d71f90f1-f500-4065-bc70-994600137b07.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d71f90f1-f500-4065-bc70-994600137b07.png)'
- en: Changing the model path manually
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 手动更改模型路径。
- en: By changing the name of the folder, we are essentially telling the model loading
    system to restore our VisualHallway brain as VisualPushBlockBrain. The trick here
    is making sure that both brains have all the same hyperparameters and configuration
    settings.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过更改文件夹的名称，我们实际上是在告诉模型加载系统将我们的VisualHallway大脑恢复为VisualPushBlockBrain。这里的技巧是确保两个大脑具有相同的超参数和配置设置。
- en: 'Speaking of hyperparameters, open the `trainer_config.yaml` file and make sure
    that the VisualHallwayLearning and VisualPushBlockLearning parameters are the
    same. The configuration for both is shown in the following code snippet for reference:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 说到超参数，打开`trainer_config.yaml`文件，确保VisualHallwayLearning和VisualPushBlockLearning参数相同。以下代码片段显示了这两个配置的参考示例：
- en: '[PRE7]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Save the configuration file when you are done editing.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑完成后，保存配置文件。
- en: 'Open your Python/Anaconda window and launch training with the following code:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你的Python/Anaconda窗口，使用以下代码启动训练：
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The previous code is not a misprint; it is the exact same command we used to
    run the VisualHallway example, except with `--load` appended on the end. This
    should launch the training and prompt you to run the editor.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前的代码不是打印错误；它是我们用来运行VisualHallway示例的完全相同的命令，只是在末尾加上了`--load`。这应该会启动训练并提示你运行编辑器。
- en: Feel free to run the training for as long as you like, but keep in mind that
    we barely trained the original agent.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随时可以运行训练，只要你喜欢，但请记住，我们几乎没有训练原始代理。
- en: Now, in this example, even if we had trained the agent to complete VisualHallway,
    this likely would not have been very effective in transferring that knowledge
    to VisualPushBlock. For the purposes of this example, we chose both since they
    are quite similar, and transferring one trained brain to the other was less complicated.
    For your own purposes, being able to transfer trained brains may be more about
    retraining agents on new or modified levels, perhaps even allowing the agents
    to train on progressively more difficult levels.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个示例中，即使我们已经训练了代理完成 VisualHallway，这可能也不太有效地将知识转移到 VisualPushBlock。为了这个示例，我们选择了这两个，因为它们非常相似，将一个训练好的大脑转移到另一个上要简单一些。对于你自己，能够转移训练过的大脑可能更多的是关于在新的或修改过的关卡上重新训练代理，甚至允许代理在逐渐更难的关卡上进行训练。
- en: Depending on your version of ML-Agents, this example may or may not work so
    well. The particular problem is the complexity of the model, number of hyperparameters,
    input space, and reward system that we are running. Keeping all of these factors
    the same also requires keen attention to detail. In the next section, we will
    take a short diversion to explore how complex these models are.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的 ML-Agents 版本，这个示例可能效果不一。具体问题在于模型的复杂性、超参数的数量、输入空间以及我们正在运行的奖励系统。保持这些因素一致也需要非常注意细节。在接下来的章节中，我们将稍作偏离，探讨这些模型的复杂性。
- en: Exploring TensorFlow checkpoints
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 TensorFlow 检查点
- en: 'TensorFlow is quickly becoming the underlying graph calculation engine that
    is powering most deep learning infrastructure. While we haven''t covered how these
    graph engines are constructed in much detail, it can be helpful to review these
    TensorFlow models visually. Not only can we start to appreciate the complexity
    of these systems better, but a good visual is often worth a thousand words. Let''s
    open up a web browser and follow the next exercise:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 正迅速成为支撑大多数深度学习基础设施的底层图计算引擎。尽管我们没有详细介绍这些图引擎是如何构建的，但从视觉上查看这些 TensorFlow
    模型是很有帮助的。我们不仅能更好地理解这些系统的复杂性，而且一个好的图像往往胜过千言万语。让我们打开浏览器，进行下一个练习：
- en: Search for the phrase `netron tensorflow` in your browser with your favorite
    search engine. Netron is an OpenSource TensorFlow model viewer that is perfect
    for our needs.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你最喜欢的搜索引擎在浏览器中搜索短语`netron tensorflow`。Netron 是一个开源的 TensorFlow 模型查看器，完美符合我们的需求。
- en: Find a link to the GitHub page and on the page the links to download the binary
    installers. Select the installer for your platform and click Download. This will
    take you to another download page where you can select the file for download.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到指向 GitHub 页面的链接，在页面中找到下载二进制安装程序的链接。选择适合你平台的安装程序并点击下载。这将带你到另一个下载页面，你可以选择下载的文件。
- en: Use the installer for your platform to install the Netron application. On Windows,
    this is as simple as downloading the exe installer and running it.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用适合你平台的安装程序安装 Netron 应用程序。在 Windows 上，下载安装 exe 安装程序并运行即可。
- en: 'Run the Netron application, and after it launches, you will see the following:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 Netron 应用程序，启动后，你将看到以下内容：
- en: '![](img/95bc6be0-a29e-4677-9a94-c3686ce9f21f.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95bc6be0-a29e-4677-9a94-c3686ce9f21f.png)'
- en: The Netron application
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Netron 应用程序
- en: Click the Open Model... button in the middle of the window
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击窗口中间的“打开模型...”按钮
- en: 'Use File Explorer to locate the `ML-Agents/ml-agents/models/vishall-0\VisualHallwayLearning`
    folder, and locate the `raw_graph.def` file as shown in the following screenshot:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件资源管理器定位到 `ML-Agents/ml-agents/models/vishall-0\VisualHallwayLearning` 文件夹，并找到
    `raw_graph.def` 文件，如下图所示：
- en: '![](img/ef7efa51-c1a1-4b3d-b189-619ab4a4f0c4.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef7efa51-c1a1-4b3d-b189-619ab4a4f0c4.png)'
- en: Selecting the model graph definition to load
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 选择要加载的模型图定义
- en: 'After loading the graph, use the - button in the top-right to zoom the view
    as far out as you can, similar to the following screenshot:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图形后，使用右上角的 - 按钮将视图缩放到最大，类似于以下截图：
- en: '![](img/091ce181-d2c3-428c-bc00-a6ce885bb62c.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/091ce181-d2c3-428c-bc00-a6ce885bb62c.png)'
- en: The TensorFlow graph model of our agent's brain
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们代理大脑的 TensorFlow 图模型
- en: As the inset shows, this graph is beyond complex, and not something we would
    be easily able to make sense of. However, it can be interesting to look through
    and see how the model/graph is constructed.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如插图所示，这个图形极其复杂，我们不太可能轻易理解它。然而，浏览并观察这个模型/图形是如何构建的还是很有趣的。
- en: 'Scroll to the top of the graph and find a node called advantages, then select
    the node and note the Graph and Inputs, model properties as shown in the following
    screenshot:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向图表顶部滚动，找到一个名为 advantages 的节点，然后选择该节点，并查看图表和输入的模型属性，如下图所示：
- en: '![](img/366e58fa-35bf-4cb4-be05-4b6c39f238fa.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/366e58fa-35bf-4cb4-be05-4b6c39f238fa.png)'
- en: Properties of the advantages graph model
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 优势图模型的属性
- en: Within the properties view of this model, you should be able to see some very
    familiar terms and settings, such as visual_observation_0, for instance, which
    shows the model input is a tensor of shape [84,84,3].
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个模型的属性视图中，你应该能够看到一些非常熟悉的术语和设置，例如 visual_observation_0，它显示该模型输入是形状为 [84,84,3]
    的张量。
- en: When you are done, feel free to look over other models, and perhaps even explore
    with other models even outside Unity. While this tool isn't quite capable of summarizing
    a complex model like we have, it does show how powerful these types of tools are
    becoming. What's more, if you can find your way around, you can even export variables
    for later inspection or use.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，可以随意查看其他模型，甚至探索Unity之外的其他模型。虽然这个工具还不完全能够总结像我们这样的复杂模型，但它展示了这些工具变得越来越强大的潜力。更重要的是，如果你能找到合适的方式，你甚至可以导出变量以供以后检查或使用。
- en: Imitation Transfer Learning
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿迁移学习
- en: One of the problems with Imitation Learning is that it often focuses the agent
    down a path that limits its possible future moves. This isn't unlike you being
    shown the improper way to perform a task and then doing it that way, perhaps without
    thinking, only to find out later that there was a better way. Humanity, in fact,
    has been prone to this type of problem over and over again throughout history.
    Perhaps you learned as a child that swimming right after eating was dangerous,
    only to learn later in life through your own experimentation, or just common knowledge,
    that that was just a myth, a myth that was taken as fact for a very long time.
    Training an agent through observation is no different you limit the agent's vision in
    many ways to a narrow focus that is limited by what it was taught. However, there
    is a way to allow an agent to revert back to the partial brute-force or trial-and
    error exploration in order to expand its training.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习的一个问题是，它常常将代理引导到一个限制其未来可能行动的路径上。这和你被教导错误的方式执行任务，然后按照那个方式做，可能没有多想，最后才发现其实有更好的方法并无太大区别。事实上，人类在历史上一直容易犯这种问题。也许你小时候学到吃完饭后游泳很危险，但后来通过自己的实验，或只是凭常识，你才知道那只是个神话，这个神话曾经被认为是事实很长时间。通过观察训练代理也没有什么不同，它会限制代理的视野，将其狭隘化，只能局限于它所学的内容。然而，有一种方法可以让代理回到部分的蛮力或试错探索，从而扩展它的训练。
- en: 'With ML-Agents we can combine IL with a form oftransfer learningin order to
    allow an agent to learn first from observation, then by furthering its training
    by learning from the once student. This form of IL chaining, if you will, allows
    you to train an agent to auto-train multiple agents. Let''s open up Unity to the
    TennisIL scene and follow the next exercise:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ML-Agents，我们可以将IL与某种形式的迁移学习结合起来，让代理先通过观察学习，然后通过从曾经的学生身上继续学习进一步训练。这种IL链式学习，若你愿意，可以让你训练一个代理来自动训练多个代理。让我们打开
    Unity，进入 TennisIL 场景，并按照下一个练习操作：
- en: 'Select the TennisArea | Agent object and in the Inspector, disable the BC Teacher
    Helper component, and then add a new Demonstration Recorder as shown in the following
    screenshot:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 TennisArea | Agent 对象，在检查器中禁用 BC Teacher Helper 组件，然后添加一个新的演示记录器，如下图所示：
- en: '![](img/65d19e63-93e6-436c-85c1-02a528d4c683.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65d19e63-93e6-436c-85c1-02a528d4c683.png)'
- en: Checking that the BC Teacher is attached to the Agent
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 BC Teacher 是否附加到代理上
- en: BC Teacher Helper is a recorder that works just like the Demonstration Recorder.
    The BC recorder allows you to turn the recording on and off as the agent runs,
    which is perfect for online training, but at the time of writing, the component
    was not working.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: BC Teacher Helper 是一个记录器，功能与演示记录器类似。BC记录器允许你在代理运行时打开和关闭录制，非常适合在线训练，但在编写本文时，该组件无法使用。
- en: Make sure Academy is set to Control the TennisLearning brain.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保 Academy 设置为控制 TennisLearning 大脑。
- en: Save the scene and project.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Open a Python/Anaconda window and launch training with the following command:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Python/Anaconda窗口，并使用以下命令启动训练：
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Press Play when prompted to run the game in the editor. Control the blue paddle
    with the *W*, *A*, *S*, *D* keys and play for a few seconds to warm up.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当提示时点击播放，在编辑器中运行游戏。使用 *W*, *A*, *S*, *D* 键控制蓝色球拍，并玩几秒钟来热身。
- en: After you are warmed up, press the *R* key to begin recording a demo observation.
    Play the game for several minutes and let the agent become capable. After the
    agent is able to return the ball, stop the training session.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 热身后，按 *R* 键开始录制演示观察。玩几分钟游戏，让智能体变得更有能力。在智能体能够回球后，停止训练。
- en: This will not only train the agent, which is fine, but it will also create a
    demo recording playback we can use to further train the agents to learn how to
    play each other in a similar way to how AlphaStar was trained. We will set up
    our tennis scene to now run in offline training mode with multiple agents in the
    next section.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅会训练智能体，效果很好，而且还会创建一个演示录制回放，我们可以用它进一步训练智能体，让它们学习如何像 AlphaStar 训练一样互相对战。接下来，我们将在下一个部分设置我们的网球场景，以便在离线训练模式下运行多个智能体。
- en: Training multiple agents with one demonstration
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用一个演示训练多个智能体
- en: 'Now, with the recording of us playing tennis, we can use this to feed into
    the training of multiple agents all feeding back into one policy. Open Unity to
    the tennis scene, the one with the multiple environments, and follow the next
    exercise:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过录制我们打网球的过程，我们可以将此录制用于训练多个智能体，所有智能体的反馈都汇入一个策略。打开 Unity 到网球场景，即那个拥有多个环境的场景，并继续进行下一个练习：
- en: 'Type `agent` into the Filter bar at the top of the Hierarchy window as shown
    in the following screenshot:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口的过滤栏中输入 `agent`，如以下截图所示：
- en: '![](img/88535449-155c-425c-911b-f57a448b2b5d.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88535449-155c-425c-911b-f57a448b2b5d.png)'
- en: Searching for all the agents in the scene
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索场景中的所有智能体
- en: Select all the agent objects in the scene and bulk change their Brain to use
    TennisLearning and not TennisPlayer.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择场景中所有智能体对象，并批量更改它们的大脑，使用 TennisLearning 而不是 TennisPlayer。
- en: Select Academy and make sure to enable it to control the brains.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 Academy 并确保启用它以控制智能体大脑。
- en: Open the `config/offline_bc_config.yaml` file.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `config/offline_bc_config.yaml` 文件。
- en: 'Add the following new section for the `TennisLearning` brain at the bottom:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在底部为 `TennisLearning` 大脑添加以下新部分：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Save the scene and the project.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Open the Python/Anaconda window and run training with the following code:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Python/Anaconda 窗口并使用以下代码进行训练：
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You may want to add the `--slow` switch in order to watch the training, but
    it should not be required.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能希望添加 `--slow` 参数来观看训练过程，但这不是必需的。
- en: Let the agents train for some time and notice its improved progress. Even with
    a short observation recording input, the agent becomes a capable player rather
    quickly.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让智能体训练一段时间，并注意其进步。即使只有短暂的观察录制输入，智能体也能很快成为一个有能力的玩家。
- en: There are multiple ways to perform this type of IL andtransfer learningchaining
    that will allow your agent some flexibility in training. You could even use the
    trained model's checkpoint without IL and run the agents with transfer learning
    as we did earlier. The possibilities are limitless, and it remains to be seen
    what will emerge as best practices.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以执行这种类型的 IL 和迁移学习链，使智能体在训练时具有一定的灵活性。你甚至可以不使用 IL，而是直接使用已训练的模型的检查点，并像我们之前那样通过迁移学习来运行智能体。可能性是无限的，最终哪些做法会成为最佳实践仍然有待观察。
- en: In the next section, we'll provide some exercises that you can use for your
    own personal learning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个部分，我们将提供一些练习，你可以用它们来进行个人学习。
- en: Exercises
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The exercises at the end of this chapter could likely provide several hours
    of fun. Try and only complete one or two exercises, as we still need to finish
    the book:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结尾的练习可能会提供数小时的乐趣。尽量只完成一到两个练习，因为我们还需要完成本书：
- en: Set up and run the PyramidsIL scene to run online IL.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行 PyramidsIL 场景以进行在线 IL 训练。
- en: Set up and run the PushBlockIL scene to run online IL.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行 PushBlockIL 场景以进行在线 IL 训练。
- en: Set up and run the WallJump scene to run with online IL. This requires you to
    modify the scene.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行 WallJump 场景以进行在线 IL 训练。这需要你修改场景。
- en: Set up and run the VisualPyramids scene to use offline recording. Record a training
    session then train an agent.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行 VisualPyramids 场景以使用离线录制。录制训练过程，然后训练智能体。
- en: Set up and run the VisualPushBlock scene to use offline recording. Use offline
    IL to train the agent.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行 VisualPushBlock 场景以使用离线录制。使用离线 IL 训练智能体。
- en: Set up the PushBlockIL scene to record an observation demo. Then use this offline
    training to train multiple agents in the regular PushBlock scene.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 PushBlockIL 场景以录制观察演示。然后，使用此离线训练来训练多个代理在常规的 PushBlock 场景中。
- en: Set up the PyramidsIL scene to record a demo recording. Then use this for offline
    training to train multiple agents in the regular Pyramids scene.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 PyramidsIL 场景以录制演示。然后，使用该数据进行离线训练，以训练多个代理在常规的 Pyramids 场景中。
- en: Train an agent in the VisualHallway scene using any form of learning you like.
    After training, modify the VisualHallway scene to use different materials on the
    walls and floor. Changing materials on Unity objects is quite easy. Then, use
    the technique of swapping model checkpoints as a way of transfer learning the
    previously trained brain into a new environment.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 VisualHallway 场景中训练一个代理，使用任何你喜欢的学习方式。训练后，修改 VisualHallway 场景，改变墙壁和地板的材质。在
    Unity 中更改物体材质非常容易。然后，使用交换模型检查点的技术，将之前训练好的大脑迁移到新环境中。
- en: Do exercise eight, but using the VisualPyramids scene. You could also add other
    objects or blocks in this scene.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成第八个练习，但使用 VisualPyramids 场景。你也可以在此场景中添加其他物体或方块。
- en: Do exercise eight, but using the VisualPushBlock scene. Try adding other blocks
    or other objects that the agent may have to work around.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成第八个练习，但使用 VisualPushBlock 场景。尝试添加其他方块或代理可能需要避开的其他物体。
- en: Just remember that, if you are attempting any of the Transfer Learning exercises,
    attention to detail is important when matching the complex graphs. In the next
    section, we summarize what we have covered in this chapter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住，如果你正在尝试任何迁移学习练习，匹配复杂图表时要特别注意细节。在下一节中，我们将总结本章所涵盖的内容。
- en: Summary
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered an emerging technique in RL called Imitation Learning
    or Behavioral Cloning. This technique, as we learned, takes the captured observations
    of a player playing a game and then uses those observations in an online or offline
    setting to further train the agent. We further learned that IL is just a form
    of Transfer Learning. We then covered a technique with ML-Agents that will allow
    you to transfer brains across environments. Finally, we looked at how to chain
    IL andtransfer learningas a way of stimulating the agent's training into developing
    new strategies on its own.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种新兴的强化学习技术，叫做模仿学习（Imitation Learning）或行为克隆（Behavioral Cloning）。正如我们所学，这种技术通过捕捉玩家玩游戏时的观察数据，然后在在线或离线环境中使用这些观察数据进一步训练代理。我们还了解到，IL
    只是迁移学习的一种形式。接着，我们介绍了使用 ML-Agents 的一种技术，它可以让你在不同的环境中迁移大脑。最后，我们探讨了如何将 IL 和迁移学习结合起来，作为激励代理自主开发新策略的训练方法。
- en: In the next chapter, we will further our understanding of DRL in games by looking
    at multiple agent training scenarios.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过研究多个代理训练场景，进一步加深对游戏中深度强化学习（DRL）的理解。
