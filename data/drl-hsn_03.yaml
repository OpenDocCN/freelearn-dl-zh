- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Deep Learning with PyTorch
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行深度学习
- en: In the previous chapter, you became familiar with open source libraries, which
    provided you with a collection of reinforcement learning (RL) environments. However,
    recent developments in RL, and especially its combination with deep learning (DL),
    now make it possible to solve much more challenging problems than ever before.
    This is partly due to the development of DL methods and tools. This chapter is
    dedicated to one such tool, PyTorch, which enables us to implement complex DL
    models with just a bunch of lines of Python code.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你已经熟悉了开源库，它们为你提供了一系列强化学习（RL）环境。然而，强化学习的最新发展，特别是与深度学习（DL）结合后，使得现在可以解决比以往更具挑战性的问题。这在某种程度上归功于深度学习方法和工具的发展。本章专门介绍了其中一个工具——PyTorch，它使我们能够用少量的Python代码实现复杂的深度学习模型。
- en: 'The chapter doesn’t pretend to be a complete DL manual, as the field is very
    wide and dynamic; however, we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章并不假设自己是一本完整的深度学习手册，因为这一领域非常广泛且动态；然而，我们将涵盖：
- en: The PyTorch library specifics and implementation details (assuming that you
    are already familiar with DL fundamentals)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch库的具体细节和实现方式（假设你已经熟悉深度学习的基础）
- en: Higher-level libraries on top of PyTorch, with the aim of simplifying common
    DL problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于PyTorch的高级库，旨在简化常见的深度学习问题
- en: The PyTorch Ignite library, which will be used in some examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章示例中将使用PyTorch Ignite库
- en: All of the examples in this chapter were updated for the latest(at the time
    of writing) PyTorch 2.3.1, which has changes in comparison to version 1.3.0, which
    was used in the second edition of this book. If you are using the old PyTorch,
    consider upgrading. Throughout this chapter, we will discuss the differences that
    are present in the latest version.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有示例都已更新为最新的（在写作时）PyTorch 2.3.1，相较于第二版书中使用的1.3.0版本有所变化。如果你还在使用旧版PyTorch，建议升级。在本章中，我们将讨论最新版本中的差异。
- en: Tensors
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: A tensor is the fundamental building block of all DL toolkits. The name sounds
    rather mystical, but the underlying idea is that a tensor is just a multi-dimensional
    array. Using the analogy of school math, one single number is like a point, which
    is zero-dimensional, while a vector is one-dimensional like a line segment, and
    a matrix is a two-dimensional object. Three-dimensional number collections can
    be represented by a cuboid of numbers, but they don’t have a separate name in
    the same way as a matrix. We can keep the term “tensor” for collections of higher
    dimensions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是所有深度学习工具包的基本构建块。这个名字听起来有些神秘，但其背后的基本思想是，张量只是一个多维数组。借用学校数学的类比，一个数字像一个点，是零维的；向量像一个线段，是一维的；矩阵是一个二维对象。三维的数字集合可以通过一个立方体的数字表示，但它们不像矩阵那样有一个独立的名称。我们可以保留“张量”这个术语来表示更高维度的集合。
- en: '![19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx
    aaa i,ij,,jk,k,...ii,j ](img/B22150_03_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![19473 3nD-t8267218391527931650-ten1181171216134951es2341506nosror 3nvmueaamctabtreoirrx
    aaa i,ij,,jk,k,...ii,j ](img/B22150_03_01.png)'
- en: 'Figure 3.1: Going from a single number to an n-dimensional tensor'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：从一个数字到n维张量的转换
- en: Another thing to note about tensors used in DL is that they are only partially
    related to tensors used in tensor calculus or tensor algebra. In DL, a tensor
    is any multi-dimensional array, but in mathematics, a tensor is a mapping between
    vector spaces, which might be represented as a multi-dimensional array in some
    cases, but has much more semantical payload behind it. Mathematicians usually
    frown at anybody who uses well-established mathematical terms to name different
    things, so be warned!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习中使用的张量，还有一个需要注意的点是，它们与张量微积分或张量代数中使用的张量仅部分相关。在深度学习中，张量是任何多维数组，但在数学中，张量是向量空间之间的映射，在某些情况下可能表现为多维数组，但其背后有更丰富的语义负载。数学家通常会对那些用已建立的数学术语命名不同事物的人表示不满，因此需要警惕！
- en: The creation of tensors
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量的创建
- en: As we’ll deal with tensors everywhere in this book, we need to be familiar with
    basic operations on them, and the most basic is how to create one. There are several
    ways to do this, and your choice might influence code readability and performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书中会到处使用张量，我们需要熟悉它们的基本操作，而最基本的操作就是如何创建一个张量。创建张量有几种方式，你的选择可能会影响代码的可读性和性能。
- en: 'If you are familiar with the NumPy library (and you should be), then you already
    know that its central purpose is the handling of multi-dimensional arrays in a
    generic way. Even though in NumPy, such arrays aren’t called tensors, they are,
    in fact, tensors. Tensors are used very widely in scientific computations as generic
    storage for data. For example, a color image could be encoded as a 3D tensor with
    the dimensions of width, height, and color plane. Apart from dimensions, a tensor
    is characterized by the type of its elements. There are 13 types supported by
    PyTorch:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 NumPy 库（而且你应该熟悉），那么你已经知道它的主要目的是以通用方式处理多维数组。尽管在 NumPy 中，这些数组没有被称为张量，但它们实际上就是张量。张量在科学计算中被广泛使用，作为数据的通用存储方式。例如，一张彩色图像可以被编码为一个三维张量，维度分别是宽度、高度和颜色通道。除了维度，张量还由其元素的类型来表征。PyTorch
    支持 13 种类型：
- en: 'Four float types: 16-bit, 32-bit, and 64-bit. 16-bit float has two variants:
    float16 has more bits for precision while bfloat16 has larger exponent part'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四种浮点类型：16 位、32 位和 64 位。16 位浮点数有两种变体：`float16` 提供更多的精度位，而 `bfloat16` 具有更大的指数部分。
- en: 'Three complex types: 32-bit, 64-bit, and 128-bit'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种复杂类型：32 位、64 位和 128 位
- en: 'Five integer types: 8-bit signed, 8-bit unsigned, 16-bit signed, 32-bit signed,
    and 64-bit signed'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 五种整数类型：8 位有符号、8 位无符号、16 位有符号、32 位有符号和 64 位有符号
- en: Boolean type
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔类型
- en: There are also four ”quantized number” types, but they are using the preceding
    types, just with different bit representation and interpretation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 也有四种“量化数值”类型，但它们使用的是前面提到的类型，只是采用不同的位表示和解释方式。
- en: Tensors of different types are represented by different classes, with the most
    commonly used being torch.FloatTensor (corresponding to a 32-bit float), torch.ByteTensor
    (an 8-bit unsigned integer), and torch.LongTensor (a 64-bit signed integer). You
    can find names of other tensor types in the documentation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的张量由不同的类表示，最常用的有 `torch.FloatTensor`（对应 32 位浮点数）、`torch.ByteTensor`（8 位无符号整数）和
    `torch.LongTensor`（64 位有符号整数）。你可以在文档中查找其他张量类型的名称。
- en: 'There are three ways to create a tensor in PyTorch:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，有三种创建张量的方法：
- en: By calling a constructor of the required type.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过调用所需类型的构造函数来创建。
- en: By asking PyTorch to create a tensor with specific data for you. For example,
    you can use the torch.zeros() function to create a tensor filled with zero values.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过让 PyTorch 为你创建一个包含特定数据的张量。例如，你可以使用 `torch.zeros()` 函数创建一个填充零值的张量。
- en: By converting a NumPy array or a Python list into a tensor. In this case, the
    type will be taken from the array’s type.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将 NumPy 数组或 Python 列表转换为张量。在这种情况下，张量的类型将取决于数组的类型。
- en: 'To give you examples of these methods, let’s look at a simple session:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你展示这些方法的例子，让我们看一个简单的会话：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we imported both PyTorch and NumPy and created a new float tensor tensor
    of size 3 × 2\. As you can see, PyTorch initializes memory with zeros, which is
    a different behaviour from previous versions. Before, it just allocated memory
    and kept it uninitialized, which is slightly faster but less safe (as it might
    introduce tricky bugs and security issues). But you shouldn’t rely on this behaviour,
    as it might change again (or behave differently on different hardware backends)
    and always initialize the contents of the tensor. To do so, you can either use
    one of the tensor construct operators:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们导入了 PyTorch 和 NumPy，并创建了一个新的大小为 3 × 2 的浮点张量。正如你所看到的，PyTorch 会用零来初始化内存，这与以前的版本不同。之前，它只是分配了内存并保持未初始化状态，虽然这样更快，但不太安全（因为可能会引入棘手的
    bug 和安全问题）。不过，你不应该依赖这种行为，因为它可能会发生变化（或在不同硬件后端上表现不同），所以始终应该初始化张量的内容。为此，你可以使用其中一种张量构造操作符：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or you can call the tensor modification method:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以调用张量修改方法：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There are two types of operation for tensors: inplace and functional. Inplace
    operations have an underscore appended to their name and operate on the tensor’s
    content. After this, the object itself is returned. The functional equivalent
    creates a copy of the tensor with the performed modification, leaving the original
    tensor untouched. Inplace operations are usually more efficient from a performance
    and memory point of view, but modification of an existing tensor (especially if
    it is shared in different pieces of code) might lead to hidden bugs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 张量有两种操作类型：原地操作和函数式操作。原地操作会在名称后附加一个下划线，并对张量的内容进行操作。操作完成后，返回的是原始对象本身。函数式操作则会创建张量的一个副本，并进行修改，原始张量保持不变。从性能和内存角度看，原地操作通常更高效，但修改现有张量（尤其是当它在不同代码片段中共享时）可能会引发潜在的
    bug。
- en: 'Another way to create a tensor by its constructor is to provide a Python iterable
    (for example, a list or tuple), which will be used as the contents of the newly
    created tensor:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构造函数创建张量的另一种方法是提供一个Python可迭代对象（例如，列表或元组），该对象将作为新创建的张量的内容：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, we are creating the same tensor with zeros from the NumPy array:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过NumPy数组创建相同的零张量：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The torch.tensor method accepts the NumPy array as an argument and creates
    a tensor of appropriate shape from it. In the preceding example, we created a
    NumPy array initialized by zeros, which created a double (64-bit float) array
    by default. So, the resulting tensor has the DoubleTensor type (which is shown
    in the example with the dtype value). Usually, in DL, double precision is not
    required and it adds an extra memory and performance overhead. Common practice
    is to use the 32-bit float type, or even the 16-bit float type, which is more
    than enough. To create such a tensor, you need to specify explicitly the type
    of NumPy array:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: torch.tensor方法接受NumPy数组作为参数，并从中创建一个适当形状的张量。在前面的示例中，我们创建了一个初始化为零的NumPy数组，默认创建了一个双精度（64位浮动）数组。因此，生成的张量具有DoubleTensor类型（在示例中通过dtype值显示）。通常，在深度学习中，不需要双精度，并且它会增加额外的内存和性能开销。常见做法是使用32位浮动类型，甚至16位浮动类型，这已经足够。要创建这样的张量，您需要明确指定NumPy数组的类型：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As an option, the type of the desired tensor could be provided to the torch.tensor
    function in the dtype argument. However, be careful, since this argument expects
    to get a PyTorch type specification and not the NumPy one. PyTorch types are kept
    in the torch package, for example, torch.float32, torch.uint8, and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种选择，所需张量的类型可以通过dtype参数提供给torch.tensor函数。然而，请小心，因为此参数期望的是PyTorch类型规范，而不是NumPy类型规范。PyTorch类型存储在torch包中，例如torch.float32、torch.uint8等。
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: A note on compatibility
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性说明
- en: The torch.tensor() method and explicit PyTorch type specification were added
    in the 0.4.0 release, and this is a step toward the simplification of tensor creation.
    In previous versions, the torch.from_numpy() function was a recommended way to
    convert NumPy arrays, but it had issues with handling the combination of the Python
    list and NumPy arrays. This from_numpy() function is still present for backward
    compatibility, but it is deprecated in favor of the more flexible torch.tensor()
    method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: torch.tensor()方法和显式的PyTorch类型指定功能是在0.4.0版本中添加的，这是简化张量创建的一个步骤。在之前的版本中，推荐使用torch.from_numpy()函数来转换NumPy数组，但它在处理Python列表和NumPy数组组合时存在问题。为了向后兼容，这个from_numpy()函数仍然存在，但它已被弃用，推荐使用更灵活的torch.tensor()方法。
- en: Scalar tensors
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量张量
- en: Since the 0.4.0 release, PyTorch has supported zero-dimensional tensors that
    correspond to scalar values (on the left of Figure [3.1](#x1-54002r1)). Such tensors
    can be the result of some operations, such as summing all values in a tensor.
    Previously, such cases were handled by the creation of a one-dimensional tensor
    (also known as vector) with a single dimension equal to one.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从0.4.0版本开始，PyTorch支持零维张量，这些张量对应标量值（如图[3.1](#x1-54002r1)左侧所示）。这类张量可以是某些操作的结果，例如对张量中所有值的求和。此前，此类情况通过创建一个维度为1的单维张量（也称为向量）来处理。
- en: 'This solution worked, but it wasn’t very simple, as extra indexation was needed
    to access the value. Now, zero-dimensional tensors are natively supported and
    returned by the appropriate functions, and they can be created by the torch.tensor()
    function. To access the actual Python value of such a tensor, we can use the special
    item() method:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案有效，但并不简单，因为需要额外的索引才能访问值。现在，零维张量已被原生支持，并由相应的函数返回，可以通过torch.tensor()函数创建。要访问此类张量的实际Python值，可以使用特殊的item()方法：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Tensor operations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量操作
- en: 'There are lots of operations that you can perform on tensors, and there are
    too many to list them all. Usually, it’s enough to search in the PyTorch documentation
    at [http://pytorch.org/docs/](http://pytorch.org/docs/). I need to mention that
    there are two places to look for operations:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对张量执行许多操作，操作种类太多，无法一一列举。通常，只需在PyTorch文档中搜索[http://pytorch.org/docs/](http://pytorch.org/docs/)即可。我需要提到的是，有两个地方可以查找操作：
- en: 'The torch package: The function usually accepts the tensor as an argument.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torch包：该函数通常接受张量作为参数。
- en: 'The tensor class: The function operates on the called tensor.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensor类：该函数操作于被调用的张量。
- en: Most of the time, tensor operations in PyTorch are trying to correspond to their
    NumPy equivalent, so if there is some not-very-specialized function in NumPy,
    then there is a good chance that PyTorch will also have it. Examples are torch.stack(),
    torch.transpose(), and torch.cat(). This is very convenient, as NumPy is a very
    widely used library (especially in the scientific community), so your PyTorch
    code becomes readable by anyone familiar with NumPy without looking into the documentation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，PyTorch 中的张量操作都是试图与其 NumPy 对应的功能相匹配，因此，如果 NumPy 中有一些不太特殊的函数，那么很有可能 PyTorch
    也会有类似的函数。比如 torch.stack()、torch.transpose() 和 torch.cat()。这非常方便，因为 NumPy 是一个广泛使用的库（尤其在科学界），因此你的
    PyTorch 代码可以被任何熟悉 NumPy 的人读取，而无需查阅文档。
- en: GPU tensors
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 张量
- en: PyTorch transparently supports CUDA GPUs, which means that all operations have
    two versions — CPU and GPU — that are automatically selected. The decision is
    made based on the type of tensors that you are operating on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 透明地支持 CUDA GPU，这意味着所有操作都有两个版本——CPU 和 GPU——并且会自动选择。这个选择是基于你正在操作的张量类型来决定的。
- en: Every tensor type that I mentioned is for CPU and has its GPU equivalent. The
    only difference is that GPU tensors reside in the torch.cuda package, instead
    of just torch. For example, torch.FloatTensor is a 32-bit float tensor that resides
    in CPU memory, but torch.cuda.FloatTensor is its GPU counterpart.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我提到的每种张量类型都是针对 CPU 的，并且都有其 GPU 对应版本。唯一的区别是，GPU 张量位于 torch.cuda 包中，而不是仅仅在 torch
    中。例如，torch.FloatTensor 是一个 32 位浮动张量，驻留在 CPU 内存中，但 torch.cuda.FloatTensor 是它的 GPU
    对应张量。
- en: In fact, under the hood, PyTorch supports not just CPU and CUDA; it has a notion
    of backend, which is an abstract computation device with memory. Tensors could
    be allocated in the backend’s memory and computations could be performed on them.
    For example, on Apple hardware, PyTorch supports Metal Performance Shaders (MPS)
    as a backend called mps. In this chapter, we focus on CPU and GPU as the mostly
    used backends, but your PyTorch code could be executed on much more fancier hardware
    without major modifications.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在 PyTorch 的底层，不仅支持 CPU 和 CUDA，还引入了后端的概念，这是一种带有内存的抽象计算设备。张量可以分配到后端的内存中，并且可以在其上进行计算。例如，在苹果硬件上，PyTorch
    支持作为名为 mps 的后端的 Metal 性能着色器（MPS）。在本章中，我们将重点讨论 CPU 和 GPU 作为最常用的后端，但你的 PyTorch 代码也可以在更高级的硬件上执行，而无需做重大修改。
- en: To convert from CPU to GPU, there is a tensor method, to(device), that creates
    a copy of the tensor to a specified device (this could be CPU or GPU). If the
    tensor is already on the device, nothing happens and the original tensor will
    be returned. The device type can be specified in different ways. First of all,
    you can just pass a string name of the device, which is "cpu" for CPU memory or
    "cuda" for GPU. A GPU device could have an optional device index specified after
    the colon; for example, the second GPU card in the system could be addressed by
    "cuda:1" (the index is zero-based).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 CPU 转换到 GPU，可以使用张量方法 to(device)，该方法会将张量的副本创建到指定的设备（可以是 CPU 或 GPU）。如果张量已经在该设备上，则什么也不发生，原始张量将被返回。设备类型可以通过不同方式指定。首先，你可以直接传递设备的字符串名称，对于
    CPU 内存是 "cpu"，对于 GPU 是 "cuda"。GPU 设备可以在冒号后面指定一个可选的设备索引；例如，系统中的第二张 GPU 卡可以通过 "cuda:1"
    来表示（索引是从零开始的）。
- en: 'Another slightly more efficient way to specify a device in the to() method
    is by using the torch.device class, which accepts the device name and optional
    index. To access the device that your tensor is currently residing in, it has
    a device property:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 to() 方法中，指定设备的另一种略微更高效的方式是使用 torch.device 类，它接受设备名称和可选的索引。要访问张量当前所在的设备，可以使用设备属性：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we created a tensor on the CPU, then copied it to GPU memory. Both copies
    can be used in computations, and all GPU-specific machinery is transparent to
    the user:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个位于 CPU 上的张量，然后将其复制到 GPU 内存中。两个副本都可以用于计算，并且所有与 GPU 相关的机制对用户是透明的：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The to() method and torch.device class were introduced in 0.4.0\. In previous
    versions, copying between CPU and GPU was performed by separate tensor methods,
    cpu() and cuda(), respectively, which required adding the extra lines of code
    to explicitly convert tensors into their CUDA versions. In newer PyTorch versions,
    you can create a desired torch.device object at the beginning of the program and
    use to(device) on every tensor that you’re creating. The old methods in the tensor,
    cpu() and cuda(), are still present and might be handy if you want to ensure that
    a tensor is in CPU or GPU memory regardless of its original location.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`to()` 方法和 `torch.device` 类在 0.4.0 版本中引入。在早期版本中，CPU 和 GPU 之间的复制是通过单独的张量方法 `cpu()`
    和 `cuda()` 来完成的，这需要添加额外的代码行来显式地将张量转换为它们的 CUDA 版本。在新的 PyTorch 版本中，你可以在程序开始时创建一个所需的
    `torch.device` 对象，并在每个创建的张量上使用 `to(device)`。旧的张量方法 `cpu()` 和 `cuda()` 仍然存在，并且如果你希望确保张量在
    CPU 或 GPU 内存中，不管它原来的位置在哪里，它们仍然可能会派上用场。'
- en: Gradients
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度
- en: Even with transparent GPU support, all of this dancing with tensors isn’t worth
    bothering without one “killer feature” — the automatic computation of gradients.
    This functionality was originally implemented in the Caffe toolkit and then became
    the de facto standard in DL libraries.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有透明的 GPU 支持，所有这些与张量的“跳舞”也毫无意义，除非有一个“杀手级功能” —— 自动计算梯度。这个功能最早在 Caffe 工具包中实现，后来成为了深度学习库中的事实标准。
- en: Earlier, computing gradients manually was extremely painful to implement and
    debug, even for the simplest neural network (NN). You had to calculate derivatives
    for all your functions, apply the chain rule, and then implement the result of
    the calculations, praying that everything was done right. This could be a very
    useful exercise for understanding the nuts and bolts of DL, but it wasn’t something
    that you wanted to repeat over and over again by experimenting with different
    NN architectures.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 早期，手动计算梯度是一个非常痛苦的过程，甚至对于最简单的神经网络（NN）来说也是如此。你需要为所有的函数计算导数，应用链式法则，然后实现计算结果，祈祷一切都能正确完成。这可能是理解深度学习核心机制的一个有用练习，但它绝对不是你愿意通过不断尝试不同的神经网络架构来反复做的事。
- en: Luckily, those days have gone now, much like programming your hardware using
    a soldering iron and vacuum tubes! Now, defining an NN of hundreds of layers requires
    nothing more than assembling it from predefined building blocks or, in the extreme
    case of you doing something fancy, defining the transformation expression manually.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，那些日子已经过去了，就像用烙铁和真空管编程硬件一样！现在，定义一个有数百层的神经网络，仅需要将它从预定义的构建块中组装起来，或者在你做一些特别的事情时，手动定义变换表达式。
- en: 'All gradients will be carefully calculated for you, backpropagated, and applied
    to the network. To be able to achieve this, you need to define your network architecture
    using DL library primitives. In Figure [3.2](#x1-59002r2), I have outlined the
    direction of the data and gradients flow during the optimization process:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的梯度将会被仔细计算、反向传播，并应用到网络中。为了实现这一点，你需要使用深度学习库的基本组件来定义你的网络架构。在图 [3.2](#x1-59002r2)
    中，我概述了数据和梯度在优化过程中的流动方向：
- en: '![IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts ](img/B22150_03_02.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![IOTDLGnuaaorpLLLtrtsauaaapgasdtyyyueieeetterrrn 1 2 3ts ](img/B22150_03_02.png)'
- en: 'Figure 3.2: Data and gradients flowing through the NN'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：数据和梯度流经神经网络
- en: 'What can make a fundamental difference is how your gradients are calculated.
    There are two approaches:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 产生根本性差异的因素可能是你计算梯度的方式。这里有两种方法：
- en: 'Static graph: In this method, you need to define your calculations in advance
    and it won’t be possible to change them later. The graph will be processed and
    optimized by the DL library before any computation is made. This model is implemented
    in TensorFlow (versions before 2.0), Theano, and many other DL toolkits.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态图：在这种方法中，你需要提前定义你的计算过程，并且之后无法更改它们。图形将在任何计算执行之前由深度学习库处理和优化。这种模型在 TensorFlow（2.0
    之前的版本）、Theano 和许多其他深度学习工具包中实现。
- en: 'Dynamic graph: You don’t need to define your graph in advance exactly as it
    will be executed; you just need to execute operations that you want to use for
    data transformation on your actual data. During this, the library will record
    the order of the operations performed, and when you ask it to calculate gradients,
    it will unroll its history of operations, accumulating the gradients of the network
    parameters. This method is also called notebook gradients and it is implemented
    in PyTorch, Chainer, and some others.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态图：你不需要提前精确定义你的图形如何执行；你只需要在实际数据上执行你希望用于数据转换的操作。在此过程中，库会记录执行操作的顺序，当你要求它计算梯度时，它会展开其操作历史，累积网络参数的梯度。这个方法也叫做笔记本梯度，它在
    PyTorch、Chainer 和其他一些框架中得到了实现。
- en: Both methods have their strengths and weaknesses. For example, a static graph
    is usually faster, as all computations can be moved to the GPU, minimizing the
    data transfer overhead. Additionally, in a static graph, the library has much
    more freedom in optimizing the order that computations are performed in or even
    removing parts of the graph.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法各有优缺点。例如，静态图通常更快，因为所有计算可以移动到 GPU 上，从而最小化数据传输开销。此外，在静态图中，库在优化计算顺序，甚至删除图形的一部分时，拥有更多的自由度。
- en: On the other hand, although a dynamic graph has a higher computation overhead,
    it gives a developer much more freedom. For example, they can say, “For this piece
    of data, I can apply this network two times, and for this piece of data, I’ll
    use a completely different model with gradients clipped by the batch mean”. Another
    very appealing strength of the dynamic graph model is that it allows you to express
    your transformation more naturally and in a more “Pythonic” way. In the end, it’s
    just a Python library with a bunch of functions, so just call them and let the
    library do the magic.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管动态图具有更高的计算开销，但它为开发者提供了更多的自由度。例如，开发者可以说，“对于这一块数据，我可以应用这个网络两次，而对于另一块数据，我会使用完全不同的模型，并且对梯度进行批均值裁剪”。动态图模型的另一个非常吸引人的优点是，它允许你以更自然、更“Pythonic”的方式表达转换。最终，这不过是一个包含一堆函数的
    Python 库，所以只需要调用它们，让库来完成魔法。
- en: Since version 2.0, PyTorch introduced the torch.compile function, which speeds
    up PyTorch code by JIT-compiling the code into optimized kernels. This is an evolution
    of the TorchScript and FX Tracing compiling methods from earlier versions.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2.0 版本以来，PyTorch 引入了 torch.compile 函数，通过 JIT 编译将代码转化为优化后的内核，从而加速 PyTorch 代码的执行。这是早期版本中
    TorchScript 和 FX Tracing 编译方法的演变。
- en: From a historical perspective, this is highly amusing how originally radically
    different approaches of TensorFlow (static graph) and PyTorch (dynamic graph)
    are fusing into each other over time. Nowadays, PyTorch supports compile() and
    TensorFlow has “eager execution mode”.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史角度来看，这非常有趣，最初完全不同的 TensorFlow（静态图）和 PyTorch（动态图）方法如何随着时间推移逐渐融合在一起。如今，PyTorch
    支持 compile()，而 TensorFlow 则有了“急切执行模式”。
- en: Tensors and gradients
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量与梯度
- en: PyTorch tensors have a built-in gradient calculation and tracking machinery,
    so all you need to do is convert the data into tensors and perform computations
    using the tensor methods and functions provided by torch. Of course, if you need
    to access underlying low-level details, you always can, but most of the time,
    PyTorch does what you’re expecting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 张量具有内建的梯度计算和跟踪机制，所以你只需要将数据转换为张量，并使用 torch 提供的张量方法和函数进行计算。当然，如果你需要访问底层的细节，也可以，但大多数情况下，PyTorch
    会按照你的预期工作。
- en: 'There are several attributes related to gradients that every tensor has:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个张量都有几个与梯度相关的属性：
- en: 'grad: A property that holds a tensor of the same shape containing computed
    gradients.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grad：一个属性，保存一个形状相同的张量，包含计算出的梯度。
- en: 'is_leaf: Equals True if this tensor was constructed by the user and False if
    the object is a result of function transformation (in other words, have a parent
    in the computation graph).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: is_leaf：如果该张量是用户构造的，则为 True；如果该对象是函数转换的结果（换句话说，计算图中有父节点），则为 False。
- en: 'requires_grad: Equals True if this tensor requires gradients to be calculated.
    This property is inherited from leaf tensors, which get this value from the tensor
    construction step (torch.zeros() or torch.tensor() and so on). By default, the
    constructor has requires_grad=False, so if you want gradients to be calculated
    for your tensor, then you need to explicitly say so.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: requires_grad：如果这个张量需要计算梯度，则为 True。这个属性从叶张量继承而来，叶张量在构造时就会得到这个值（如 torch.zeros()
    或 torch.tensor() 等）。默认情况下，构造函数的 requires_grad=False，因此如果你希望为张量计算梯度，你需要明确指定。
- en: 'To make all of this gradient-leaf machinery clearer, let’s consider this session:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让所有这些梯度-叶节点机制更加清晰，让我们考虑一下这个会话：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we created two tensors. The first requires gradients to be calculated
    and the second doesn’t.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了两个张量。第一个需要计算梯度，第二个则不需要。
- en: 'Next, we have added both vectors element-wise (which is vector [3, 3]), doubled
    every element, and summed them together:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对两个向量按元素加法（即向量 [3, 3]）进行了操作，随后将每个元素乘以 2 并相加：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result is a zero-dimensional tensor with the value 12\. Okay, so far this
    is just a simple math. Now let’s look at the underlying graph that our expressions
    created:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个零维张量，其值为 12。好的，到目前为止这只是一个简单的数学运算。现在，让我们来看看我们表达式所创建的底层图：
- en: '![vv+v×Σv2 12sruesm ](img/B22150_03_03.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![vv+v×Σv2 12sruesm ](img/B22150_03_03.png)'
- en: 'Figure 3.3: Graph representation of the expression'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：表达式的图表示
- en: 'If we check the attributes of our tensors, then we will find that v1 and v2
    are the only leaf nodes and every variable, except v2, requires gradients to be
    calculated:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查张量的属性，就会发现 v1 和 v2 是唯一的叶节点，并且除 v2 外的每个变量都需要计算梯度：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see, the property requires_grad is sort of “sticky”: if one of the
    variables involved in computations has it set to True, all subsequent nodes also
    have it. This is logical behaviour, as we normally need gradients to be calculated
    for all intermediate steps in our computation. But “calculation” doesn’t mean
    they will be preserved in the .grad field. For memory efficiency, gradients are
    stored only for leaf nodes with requires_grad=True. If you want to keep gradients
    in the non-leaf nodes, you need to call their retain_grad() method, which tells
    PyTorch to keep the gradients for non-leaf node.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，属性 requires_grad 是有“粘性”的：如果参与计算的变量之一将其设置为 True，那么所有后续节点也将继承这个属性。这是合乎逻辑的行为，因为我们通常需要对计算过程中的所有中间步骤计算梯度。但是，“计算”并不意味着它们会被保留在
    .grad 字段中。为了内存效率，只有要求计算梯度的叶节点会保存梯度。如果你希望在非叶节点中保留梯度，你需要调用它们的 retain_grad() 方法，这样
    PyTorch 就会告诉它们保留梯度。
- en: 'Now, let’s tell PyTorch to calculate the gradients of our graph:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们告诉 PyTorch 计算我们图的梯度：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By calling the backward function, we asked PyTorch to calculate the numerical
    derivative of the v_res variable with respect to any variable that our graph has.
    In other words, what influence do small changes to the v_res variable have on
    the rest of the graph? In our particular example, the value of 2 in the gradients
    of v1 means that by increasing any element of v1 by one, the resulting value of
    v_res will grow by two.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 backward 函数，我们让 PyTorch 计算 v_res 变量相对于图中其他变量的数值导数。换句话说，v_res 变量的小幅变化对图中其他部分的影响是什么？在我们的这个例子中，v1
    梯度中的值 2 表示通过将 v1 的任何元素增加 1，v_res 的结果值将增加 2。
- en: 'As mentioned, PyTorch calculates gradients only for leaf tensors with requires_grad=True.
    Indeed, if we try to check the gradients of v2, we get nothing:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PyTorch 只计算要求计算梯度的叶张量的梯度。事实上，如果我们尝试检查 v2 的梯度，我们将不会得到任何结果：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The reason for that is efficiency in terms of computations and memory. In real
    life, our network can have millions of optimized parameters, with hundreds of
    intermediate operations performed on them. During gradient descent optimization,
    we are not interested in gradients of any intermediate matrix multiplication;
    the only things we want to adjust in the model are gradients of loss with respect
    to model parameters (weights). Of course, if you want to calculate the gradients
    of input data (it could be useful if you want to generate some adversarial examples
    to fool the existing NN or adjust pretrained word embeddings), then you can easily
    do so by passing requires_grad=True on tensor creation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的原因是为了提高计算和内存的效率。在实际应用中，我们的网络可能会有数百万个优化参数，并对它们执行数百次中间操作。在梯度下降优化过程中，我们并不关心任何中间矩阵乘法的梯度；我们只关心模型中损失函数相对于模型参数（权重）的梯度。当然，如果你想计算输入数据的梯度（如果你想生成一些对抗样本来欺骗现有的神经网络或调整预训练的词嵌入，这可能是有用的），那么你可以通过在创建张量时传递`requires_grad=True`来轻松实现。
- en: Basically, you now have everything needed to implement your own NN optimizer.
    The rest of this chapter is about extra, convenient functions, which will provide
    you with higher-level building blocks of NN architectures, popular optimization
    algorithms, and common loss functions. However, don’t forget that you can easily
    reimplement all of these bells and whistles in any way that you like. This is
    why PyTorch is so popular among DL researchers — for its elegance and flexibility.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，你现在已经具备了实现自己神经网络优化器所需的一切。本章的剩余部分将介绍一些额外的、便捷的功能，它们将为你提供更高层次的神经网络架构模块、流行的优化算法和常见的损失函数。然而，不要忘记你可以轻松地以任何方式重新实现所有这些花里胡哨的功能。这就是为什么PyTorch在深度学习研究人员中如此受欢迎——因为它的优雅与灵活性。
- en: Compatibility
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 兼容性
- en: Support of gradients calculation in tensors is one of the major changes in PyTorch
    0.4.0\. In previous versions, graph tracking and gradients accumulation were done
    in a separate, very thin class, Variable. This worked as a wrapper around the
    tensor and automatically saved the history of computations in order to be able
    to backpropagate. This class is still present in 2.2.0 (available in torch.autograd),
    but it is deprecated and will go away soon, so new code should avoid using it.
    From my perspective, this change is great, as the Variable logic was really thin,
    but it still required extra code and the developer’s attention to wrap and unwrap
    tensors. Now, gradients are a built-in tensor property, which makes the API much
    cleaner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 张量中梯度计算的支持是PyTorch 0.4.0版本的重大变化之一。在之前的版本中，图追踪和梯度积累是在一个独立且非常薄的类——Variable中完成的。它作为张量的包装器，自动保存计算历史，以便能够进行反向传播。这个类在2.2.0版本中仍然存在（可在torch.autograd中找到），但它已被弃用，并将很快被移除，因此新代码应避免使用它。从我的角度来看，这个变化非常好，因为Variable的逻辑非常薄弱，但它仍然需要额外的代码以及开发者的注意来包装和解包装张量。现在，梯度已成为张量的内建属性，这使得API变得更加简洁。
- en: NN building blocks
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络构建模块
- en: 'In the torch.nn package, you will find tons of predefined classes providing
    you with the basic functionality blocks. All of them are designed with practice
    in mind (for example, they support mini-batches, they have sane default values,
    and the weights are properly initialized). All modules follow the convention of
    callable, which means that the instance of any class can act as a function when
    applied to its arguments. For example, the Linear class implements a feed-forward
    layer with optional bias:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在torch.nn包中，你会发现许多预定义的类，为你提供了基本的功能模块。所有这些类都是从实践出发设计的（例如，它们支持小批量处理，拥有合理的默认值，并且权重得到了适当初始化）。所有模块遵循可调用的约定，这意味着任何类的实例在应用于其参数时可以充当函数。例如，Linear类实现了一个前馈层，带有可选的偏置：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we created a randomly initialized feed-forward layer, with two inputs
    and five outputs, and applied it to our float tensor. All classes in the torch.nn
    packages inherit from the nn.Module base class, which you can use to implement
    your own higher-level NN blocks. You will see how you can do this in the next
    section, but, for now, let’s look at useful methods that all nn.Module children
    provide. They are as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个随机初始化的前馈层，具有两个输入和五个输出，并将其应用于我们的浮动张量。torch.nn包中的所有类都继承自nn.Module基类，你可以使用它来实现自己的更高层次的神经网络模块。你将在下一节中看到如何做到这一点，但现在，让我们先来看看所有nn.Module子类提供的有用方法。它们如下：
- en: 'parameters(): This function returns an iterator of all variables that require
    gradient computation (that is, module weights).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: parameters()：此函数返回一个迭代器，包含所有需要计算梯度的变量（即模块权重）。
- en: 'zero_grad(): This function initializes all gradients of all parameters to zero.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: zero_grad()：此函数将所有参数的梯度初始化为零。
- en: 'to(device): This function moves all module parameters to a given device (CPU
    or GPU).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: to(device)：此函数将所有模块参数移动到给定设备（CPU 或 GPU）。
- en: 'state_dict(): This function returns the dictionary with all module parameters
    and is useful for model serialization.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: state_dict()：此函数返回包含所有模块参数的字典，对于模型序列化非常有用。
- en: 'load_state_dict(): This function initializes the module with the state dictionary.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: load_state_dict()：此函数使用状态字典初始化模块。
- en: The whole list of available classes can be found in the documentation at [http://pytorch.org/docs](http://pytorch.org/docs).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所有可用类的完整列表可以在文档中找到，网址是[http://pytorch.org/docs](http://pytorch.org/docs)。
- en: 'Now, I should mention one very convenient class that allows you to combine
    other layers into the pipe: Sequential. The best way to demonstrate Sequential
    is through an example:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我应该提到一个非常方便的类，它允许你将其他层组合到管道中：Sequential。通过一个示例展示Sequential的最佳方式如下：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, we defined a three-layer NN with softmax on output, applied along dimension
    1 (dimension 0 is batch samples), rectified linear unit (ReLU) nonlinearities,
    and dropout. Let’s push something through it:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们定义了一个三层神经网络，输出使用 softmax，沿维度 1 进行应用（维度 0 是批样本），使用修正线性单元（ReLU）非线性激活函数，以及
    dropout。让我们通过它推送一些数据：
- en: '[PRE17]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, our mini-batch of one vector successfully traversed through the network!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的一个向量的迷你批次成功地通过了网络！
- en: Custom layers
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义层
- en: In the previous section, I briefly mentioned the nn.Module class as a base parent
    for all NN building blocks exposed by PyTorch. It’s not just a unifying parent
    for the existing layers — it’s much more than that. By subclassing the nn.Module
    class, you can create your own building blocks, which can be stacked together,
    reused later, and integrated into the PyTorch framework flawlessly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分，我简要提到过nn.Module类，它是PyTorch暴露的所有神经网络构建块的基类。它不仅仅是现有层的统一父类——它远不止于此。通过子类化nn.Module类，你可以创建自己的构建块，这些构建块可以被堆叠在一起，稍后可以重复使用，并无缝地集成到PyTorch框架中。
- en: 'At its core, the nn.Module provides quite rich functionality to its children:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，nn.Module为其子类提供了非常丰富的功能。
- en: It tracks all submodules that the current module includes. For example, your
    building block can have two feed-forward layers used somehow to perform the block’s
    transformation. To keep track of (register) the submodule, you just need to assign
    it to the class’s field.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它跟踪当前模块包含的所有子模块。例如，你的构建块可能有两个前馈层，用于某种方式执行该块的变换。为了跟踪（注册）子模块，你只需将其分配给类的字段。
- en: It provides functions to deal with all parameters of the registered submodules.
    You can obtain a full list of the module’s parameters (parameters() method), zero
    its gradients (zero_grads() method), move to CPU or GPU (to(device) method), serialize
    and deserialize the module (state_dict() and load_state_dict()), and even perform
    generic transformations using your own callable (apply() method).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了处理已注册子模块所有参数的功能。你可以获取模块参数的完整列表（parameters() 方法）、将其梯度归零（zero_grads() 方法）、移动到
    CPU 或 GPU（to(device) 方法）、序列化和反序列化模块（state_dict() 和 load_state_dict() 方法），甚至可以使用你自己的可调用函数执行通用变换（apply()
    方法）。
- en: It establishes the convention of Module application to data. Every module needs
    to perform its data transformation in the forward() method by overriding it.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它建立了模块应用于数据的约定。每个模块都需要通过重写forward()方法来执行数据变换。
- en: There are some more functions, such as the ability to register a hook function
    to tweak module transformation or gradients flow, but they are more for advanced
    use cases.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一些其他功能，比如注册钩子函数以调整模块的变换或梯度流，但这些更多用于高级用例。
- en: These functionalities allow us to nest our submodels into higher-level models
    in a unified way, which is extremely useful when dealing with complexity. It could
    be a simple one-layer linear transformation or a 1001-layer residual NN (ResNet)
    monster, but if they follow the conventions of nn.Module, then both of them could
    be handled in the same way. This is very handy for code reusability and simplification
    (by hiding non-relevant implementation details).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能使我们能够以统一的方式将子模型嵌套到更高级别的模型中，这在处理复杂性时非常有用。无论是简单的一层线性变换，还是一个1001层的残差神经网络（ResNet）怪兽，只要它们遵循nn.Module的约定，那么这两者就可以用相同的方式处理。这对于代码的重用和简化（通过隐藏不相关的实现细节）非常方便。
- en: To make our life simpler, when following the above conventions, PyTorch authors
    simplified the creation of modules through careful design and a good dose of Python
    magic. So, to create a custom module, we usually have to do only two things —
    register submodules and implement the forward() method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化我们的工作，遵循上述约定时，PyTorch的作者通过精心设计和大量Python魔法简化了模块的创建。所以，创建自定义模块时，通常只需要做两件事——注册子模块和实现forward()方法。
- en: 'Let’s look at how this can be done for our Sequential example from the previous
    section, but in a more generic and reusable way (the full sample is Chapter03/01_modules.py).
    The following is our module class that inherits nn.Module:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何以更通用和可重用的方式来完成之前章节中我们用到的Sequential示例（完整示例见Chapter03/01_modules.py）。以下是我们的模块类，它继承自nn.Module：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the constructor, we pass three parameters: the input size, the output size,
    and the optional dropout probability. The first thing we need to do is call the
    parent’s constructor to let it initialize itself.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们传入三个参数：输入大小、输出大小和可选的dropout概率。我们需要做的第一件事是调用父类的构造函数，让它初始化自己。
- en: In the second step in the preceding code, we create an already familiar nn.Sequential
    with a bunch of layers and assign it to our class field named pipe. By assigning
    a Sequential instance to our object’s field, we will automatically register this
    module (nn.Sequential inherits from nn.Module, as does everything in the nn package).
    To register it, we don’t need to call anything, we just need to assign our submodules
    to fields. After the constructor finishes, all those fields will be registered
    automatically. If you really want to, there is a function in nn.Module to register
    submodules called add_module(). It might be useful if your module can have variable
    number of layers and they need to be created programmatically.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码的第二步中，我们创建了一个已经熟悉的nn.Sequential，并用一堆层来初始化它，然后将其赋值给我们名为pipe的类字段。通过将Sequential实例赋值给对象的字段，我们将自动注册这个模块（nn.Sequential继承自nn.Module，就像nn包中的所有模块一样）。为了注册它，我们不需要调用任何东西，只需要将子模块赋值给字段。构造函数完成后，所有这些字段将自动注册。如果你真的需要，也可以通过nn.Module中的add_module()函数来注册子模块。如果你的模块有可变数量的层，且需要通过编程方式创建这些层，这个函数可能会非常有用。
- en: 'Next, we must override the forward function with our implementation of data
    transformation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须用数据转换的实现重写forward函数：
- en: '[PRE19]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As our module is a very simple wrapper around the Sequential class, we just
    need to ask self.pipe to transform the data. Note that to apply a module to the
    data, we need to call the module as a callable (that is, pretend that the module
    instance is a function and call it with the arguments) and not use the forward()
    function of the nn.Module class. This is because nn.Module overrides the __call__()
    method, which is being used when we treat an instance as callable. This method
    does some nn.Module magic and calls our forward() method. If we call forward()
    directly, we will intervene with the nn.Module duty, which can give wrong results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模块只是Sequential类的一个非常简单的封装，我们只需要让self.pipe来转换数据。请注意，要将一个模块应用于数据，我们需要像调用函数一样调用模块（也就是说，将模块实例当作函数来调用，并传入参数），而不是使用nn.Module类的forward()方法。这是因为nn.Module重载了__call__()方法，当我们将实例当作可调用对象时，这个方法会被使用。这个方法做了一些nn.Module的魔法，并调用了我们的forward()方法。如果直接调用forward()，我们会干扰nn.Module的职责，可能会得到错误的结果。
- en: 'So, that’s what we need to do to define our own module. Now, let’s use it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是我们定义自己模块所需要做的事情。现在，让我们使用它：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create our module, providing it with the desired number of inputs and outputs,
    then we create a tensor and ask our module to transform it, following the same
    convention of using it as callable. After that, we print our network’s structure
    (nn.Module overrides __str__() and __repr__()) to represent the inner structure
    in a nice way. The last thing we show is the result of the network’s transformation.
    The output of our code should look like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建我们的模块，提供所需数量的输入和输出，然后创建一个张量并要求我们的模块对其进行转换，按照将其作为可调用对象的相同约定进行操作。之后，我们打印网络的结构（nn.Module
    重写了 __str__() 和 __repr__()），以以一种清晰的方式表示内部结构。最后我们展示的是网络转换的结果。我们代码的输出应如下所示：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Of course, everything that was said about the dynamic nature of PyTorch is still
    true. The forward() method is called for every batch of data, so if you want to
    do some complex transformations based on the data you need to process, like hierarchical
    softmax or a random choice of network to apply, then nothing can stop you from
    doing so. The count of arguments to your module is also not limited by one parameter.
    So, if you want, you can write a module with multiple required parameters and
    dozens of optional arguments, and it will be fine.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，关于 PyTorch 动态特性的所有说法仍然适用。每处理一批数据，都会调用 forward() 方法，所以如果您想根据需要处理的数据执行一些复杂的转换，例如层次化
    Softmax 或随机选择应用的网络，那么没有什么能阻止您这样做。您模块的参数个数也不局限于一个参数。因此，如果您愿意，您可以编写一个需要多个必需参数和数十个可选参数的模块，它也完全没问题。
- en: 'Next, we need to get familiar with two important pieces of the PyTorch library
    that will simplify our lives: loss functions and optimizers.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要熟悉 PyTorch 库中的两个重要部分，这将简化我们的工作：损失函数和优化器。
- en: Loss functions and optimizers
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数和优化器
- en: The network that transforms input data into output is not the only thing we
    need for training. We also define our learning objective, which has to be a function
    that accepts two arguments — the network’s output and the desired output. Its
    responsibility is to return to us a single number — how close the network’s prediction
    is from the desired result. This function is called the loss function, and its
    output is the loss value. Using the loss value, we calculate gradients of network
    parameters and adjust them to decrease this loss value, which pushes our model
    to better results in the future. Both the loss function and the method of tweaking
    a network’s parameters by gradient are so common and exist in so many forms that
    both of them form a significant part of the PyTorch library. Let’s start with
    loss functions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入数据转换为输出的网络并不是我们训练所需的唯一部分。我们还需要定义学习目标，该目标必须是一个接受两个参数的函数——网络的输出和期望的输出。它的职责是返回一个单一的数值——网络的预测与期望结果的差距。这个函数称为损失函数，它的输出即为损失值。通过损失值，我们计算网络参数的梯度，并调整这些参数以减少损失值，从而推动模型未来取得更好的结果。损失函数和通过梯度调整网络参数的方法如此常见，且以多种形式存在，以至于它们成为
    PyTorch 库的重要组成部分。我们从损失函数开始。
- en: Loss functions
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Loss functions reside in the nn package and are implemented as an nn.Module
    subclass. Usually, they accept two arguments: output from the network (prediction)
    and desired output (ground-truth data, which is also called the label of the data
    sample). At the time of writing, PyTorch 2.3.1 contains over 20 different loss
    functions and, of course, nothing stops you from writing any custom function you
    want to optimize.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数位于 nn 包中，并作为 nn.Module 的子类实现。通常，它们接受两个参数：来自网络的输出（预测值）和期望的输出（真实数据，也称为数据样本的标签）。截至本文编写时，PyTorch
    2.3.1 包含了超过 20 种不同的损失函数，当然，您也可以编写任何自定义的函数来进行优化。
- en: 'The most commonly used standard loss functions are:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的标准损失函数有：
- en: 'nn.MSELoss: The mean square error between arguments, which is the standard
    loss for regression problems.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nn.MSELoss：计算两个参数之间的均方误差，这是回归问题的标准损失。
- en: 'nn.BCELoss and nn.BCEWithLogits: Binary cross-entropy loss. The first version
    expects a single probability value (usually it’s the output of the Sigmoid layer),
    while the second version assumes raw scores as input and applies Sigmoid itself.
    The second way is usually more numerically stable and efficient. These losses
    (as their names suggest) are frequently used in binary classification problems.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nn.BCELoss 和 nn.BCEWithLogits：二元交叉熵损失。第一种版本期望一个单一的概率值（通常是 Sigmoid 层的输出），而第二种版本假设原始分数作为输入并自行应用
    Sigmoid。第二种方式通常在数值上更稳定且更高效。这些损失函数（如其名称所示）通常用于二元分类问题。
- en: 'nn.CrossEntropyLoss and nn.NLLLoss: Famous “maximum likelihood” criteria that
    are used in multi-class classification problems. The first version expects raw
    scores for each class and applies LogSoftmax internally, while the second expects
    to have log probabilities as the input.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nn.CrossEntropyLoss 和 nn.NLLLoss：在多类分类问题中使用的著名“最大似然”标准。第一个版本期望每个类的原始得分，并在内部应用LogSoftmax，而第二个版本期望输入的是对数概率。
- en: There are other loss functions available and you are always free to write your
    own Module subclass to compare the output and target. Now, let’s look at the second
    piece of the optimization process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他损失函数可供选择，您可以随时编写自己的模块子类来比较输出和目标。现在，让我们看看优化过程的第二部分。
- en: Optimizers
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: 'The responsibility of the basic optimizer is to take the gradients of model
    parameters and change these parameters in order to decrease the loss value. By
    decreasing the loss value, we are pushing our model toward the desired output,
    which can give us hope for better model performance in the future. Changing parameters
    may sound simple, but there are lots of details here and the optimizer procedure
    is still a hot research topic. In the torch.optim package, PyTorch provides lots
    of popular optimizer implementations, and the most widely known are as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基本优化器的职责是获取模型参数的梯度，并更改这些参数以减少损失值。通过减少损失值，我们将模型推向期望的输出，这为未来模型表现的提升带来希望。改变参数听起来很简单，但这里有很多细节，优化过程仍然是一个热门的研究课题。在torch.optim包中，PyTorch提供了许多流行的优化器实现，其中最广为人知的如下：
- en: 'SGD: A vanilla stochastic gradient descent algorithm with an optional momentum
    extension'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD：一种常规的随机梯度下降算法，带有可选的动量扩展
- en: 'RMSprop: An optimizer proposed by Geoffrey Hinton'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSprop：Geoffrey Hinton提出的优化器
- en: 'Adagrad: An adaptive gradients optimizer'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad：一种自适应梯度优化器
- en: 'Adam: A quite successful and popular combination of both RMSprop and Adagrad'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam：RMSprop和Adagrad的成功且流行的组合
- en: All optimizers expose the unified interface, which makes it easy to experiment
    with different optimization methods (sometimes the optimization method can really
    make a difference in convergence dynamics and the final result). On construction,
    you need to pass an iterable of tensors, which will be modified during the optimization
    process. The usual practice is to pass the result of the params() call of the
    upper-level nn.Module instance, which will return an iterable of all leaf tensors
    with gradients.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 所有优化器都公开统一接口，这使得尝试不同的优化方法变得更加容易（有时候，优化方法确实会对收敛动态和最终结果产生影响）。在构造时，您需要传递一个张量的可迭代对象，这些张量将在优化过程中被修改。通常做法是传递上层nn.Module实例的params()调用结果，该调用将返回所有叶张量（包含梯度）的可迭代对象。
- en: 'Now, let’s discuss the common blueprint of a training loop:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论训练循环的常见蓝图：
- en: '[PRE22]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Usually, you iterate over your data over and over again (one iteration over
    a full set of examples is called an epoch). Data is usually too large to fit into
    CPU or GPU memory at once, so it is split into batches of equal size. Every batch
    includes data samples and target labels, and both of them have to be tensors (lines
    2 and 3).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您需要反复遍历数据（对整个示例集进行一次迭代称为一个epoch）。数据通常过大，无法一次性加载到CPU或GPU内存中，因此它被拆分成大小相等的小批次。每个小批次包含数据样本和目标标签，它们都必须是张量（第2行和第3行）。
- en: You pass data samples to your network (line 4) and feed its output and target
    labels to the loss function (line 5). The result of the loss function shows the
    “badness” of the network result relative to the target labels. As input to the
    network and the network’s weights are tensors, all transformations of your network
    are nothing more than a graph of operations with intermediate tensor instances.
    The same is true for the loss function — its result is also a tensor of one single
    loss value.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您将数据样本传递给网络（第4行），并将网络的输出和目标标签传递给损失函数（第5行）。损失函数的结果显示了网络结果相对于目标标签的“差距”。由于网络的输入和权重都是张量，网络的所有变换无非是一个包含中间张量实例的操作图。损失函数也是如此——其结果也是一个单一损失值的张量。
- en: Every tensor in this computation graph remembers its parent, so to calculate
    gradients for the whole network, all you need to do is call the backward() function
    on a loss function result (line 6). The result of this call will be the unrolling
    of the graph of the performed computations and the calculating of gradients for
    every leaf tensor with require_grad=True. Usually, such tensors are our model’s
    parameters, such as the weights and biases of feed-forward networks, and convolution
    filters. Every time a gradient is calculated, it is accumulated in the tensor.grad
    field, so one tensor can participate in a transformation multiple times and its
    gradients will be properly summed together. For example, one single recurrent
    neural network (RNN) cell could be applied to multiple input items.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图中的每个张量都会记住它的父节点，因此，要计算整个网络的梯度，您只需要对损失函数的结果调用 `backward()` 函数（第6行）。这个调用的结果是展开已执行计算的图并为每个
    `require_grad=True` 的叶子张量计算梯度。通常，这些张量是我们模型的参数，比如前馈网络的权重和偏置，以及卷积滤波器。每次计算梯度时，梯度都会累积到
    `tensor.grad` 字段中，因此一个张量可以参与多次变换，并且它的梯度会被正确地加总。例如，一个单独的递归神经网络（RNN）单元可能会应用于多个输入项。
- en: After the loss.backward() call is finished, we have the gradients accumulated,
    and now it’s time for the optimizer to do its job — it takes all gradients from
    the parameters we have passed to it on construction and applies them. All this
    is done with the method step() (line 7).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 `loss.backward()` 后，我们已经积累了梯度，现在该轮到优化器发挥作用了——它会从构造时传入的参数中获取所有梯度并应用它们。所有这些操作都通过
    `step()` 方法完成（第7行）。
- en: The last, but not least, piece of the training loop is our responsibility to
    zero gradients of parameters. This can be done by calling zero_grad() on our network,
    but, for our convenience, the optimizer also exposes such a call, which does the
    same thing (line 8). Sometimes, zero_grad() is placed at the beginning of the
    training loop, but it doesn’t matter much.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环中的最后一步，但并非最不重要的一步，是我们需要将参数的梯度归零。这可以通过在我们的网络上调用 `zero_grad()` 来完成，但为了方便起见，优化器也提供了这样一个调用，完成相同的操作（第8行）。有时，`zero_grad()`
    会被放在训练循环的开始，但这其实并没有太大关系。
- en: The preceding scheme is a very flexible way to perform optimization and it can
    fulfill the requirements even in sophisticated research. For example, you can
    have two optimizers tweaking the options of different models on the same data
    (and this is a real-life scenario from generative adversarial network (GAN) training).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方案是一种非常灵活的优化方法，即使在复杂的研究中也能满足需求。例如，您可以让两个优化器在相同的数据上调整不同模型的选项（这是生成对抗网络（GAN）训练中的一个真实场景）。
- en: So, we are done with the essential functionality of PyTorch required to train
    NNs. This chapter ends with a practical medium-size example to demonstrate all
    the concepts covered, but before we get to it, we need to discuss one important
    topic that is essential for an NN practitioner — monitoring the learning process.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经完成了 PyTorch 中训练神经网络所需的基本功能。本章最后将通过一个实际的中等规模的示例，来展示所有涵盖的概念，但在此之前，我们需要讨论一个对神经网络实践者至关重要的话题——监控学习过程。
- en: Monitoring with TensorBoard
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 进行监控
- en: If you have ever tried to train an NN on your own, then you will know how painful
    and uncertain it can be. I’m not talking about following the existing tutorials
    and demos, when all the hyperparameters are already tuned for you, but about taking
    some data and creating something from scratch. Even with modern DL high-level
    toolkits, where all best practices, such as proper weights initialization; optimizers’
    betas, gammas, and other options set to sane defaults; and tons of other stuff
    hidden under the hood, there are still lots of decisions that you have to make,
    hence lots of things that could go wrong. As a result, your code almost never
    works from the first run, and this is something that you should get used to.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾尝试过自己训练神经网络（NN），那么你一定知道这有多么痛苦和不确定。我并不是说在跟随现有的教程和示范时，那时所有的超参数已经为你调好，而是说从一些数据开始，创造一些全新的东西。即使使用现代深度学习（DL）高层工具包，在这些工具包中，所有最佳实践（如适当的权重初始化；优化器的β、γ及其他选项设置为合理的默认值；以及大量其他隐藏的配置）都已做好准备，但你仍然需要做出许多决策，因此仍有许多可能出错的地方。结果是，你的代码几乎总是在第一次运行时就不工作，这是你必须习惯的事情。
- en: Of course, with practice and experience, you will develop a strong understanding
    of the possible causes of problems, but this needs input data about what’s going
    on inside your network. So, you need to be able to peek inside your training process
    somehow and observe its dynamics. Even small networks (such as tiny MNIST tutorial
    networks) could have hundreds of thousands of parameters with quite nonlinear
    training dynamics.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，随着实践和经验的积累，你会对问题的可能原因有深入的理解，但这需要有关网络内部情况的输入数据。所以，你需要能够以某种方式窥视你的训练过程，并观察其动态。即使是小型网络（如微型MNIST教程网络）也可能拥有数十万参数，且训练动态相当非线性。
- en: 'DL practitioners have developed a list of things that you should observe during
    your training, which usually includes the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习从业者已经开发出了一份你在训练过程中应该观察的事项清单，通常包括以下内容：
- en: Loss value, which normally consists of several components like base loss and
    regularization losses. You should monitor both the total loss and the individual
    components over time.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失值，通常由几个组件组成，如基础损失和正则化损失。你应该随时间监控总损失和各个组成部分。
- en: Results of validation on training and test datasets.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集和测试集上的验证结果。
- en: Statistics about gradients and weights.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于梯度和权重的统计信息。
- en: Values produced by the network. For example, if you are solving a classification
    problem, you definitely want to measure the entropy of predicted class probabilities.
    In the case of a regression problem, raw predicted values can give tons of data
    about the training.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络产生的值。例如，如果你在解决分类问题，肯定希望衡量预测类别概率的熵。如果是回归问题，原始的预测值可以提供大量关于训练的数据。
- en: Learning rates and other hyperparameters, if they are adjusted over time.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率和其他超参数，如果它们随时间调整的话。
- en: The list could be much longer and include domain-specific metrics, such as word
    embedding projections, audio samples, and images generated by GANs. You also may
    want to monitor values related to training speed, like how long an epoch takes,
    to see the effect of your optimizations or problems with hardware.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个清单可以更长，包含领域特定的度量指标，比如词嵌入投影、音频样本和GAN生成的图像。你也可能想要监控与训练速度相关的值，比如每个epoch的时间，以查看优化效果或硬件问题。
- en: To cut a long story short, you need a generic solution to track lots of values
    over time and represent them for analysis, preferably developed especially for
    DL (just imagine looking at such statistics using an Excel spreadsheet). Luckily,
    such tools exist, and we will explore them next.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说，你需要一个通用的解决方案，来跟踪大量的值，并将它们表示出来以供分析，最好是专门为深度学习开发的（想象一下用Excel电子表格查看这些统计数据）。幸运的是，这样的工具是存在的，我们接下来将对它们进行探讨。
- en: TensorBoard 101
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorBoard 101
- en: When the first edition of this book was written, there wasn’t too much choice
    for NN monitoring. As time has passed by and new people and companies have become
    involved with the pursuit of ML and DL, more new tools have appeared, for example,
    MLflow [https://mlflow.org/](https://mlflow.org/). In this book, we will still
    focus on the TensorBoard utility from TensorFlow, but you might consider trying
    other alternatives.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当本书的第一版写作时，神经网络监控的选择并不多。随着时间的推移，越来越多的人和公司投入到机器学习和深度学习的追求中，出现了更多的新工具，例如MLflow
    [https://mlflow.org/](https://mlflow.org/)。在本书中，我们仍然会聚焦于TensorFlow的TensorBoard工具，但你可能会考虑尝试其他替代方案。
- en: 'From the first public version, TensorFlow included a special tool called TensorBoard,
    which was developed to solve the problem we are talking about — how to observe
    and analyze various NN characteristics during and after the training. TensorBoard
    is a powerful, generic solution with a large community and it looks quite pretty:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一个公开版本开始，TensorFlow就包含了一个名为TensorBoard的特别工具，旨在解决我们正在讨论的问题——如何在训练过程中及训练后观察和分析各种神经网络特征。TensorBoard是一个功能强大的通用解决方案，拥有庞大的社区，界面也相当漂亮：
- en: '![PIC](img/file9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file9.png)'
- en: 'Figure 3.4: The TensorBoard web interface (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：TensorBoard的网页界面（为了更好的可视化效果，请参考 https://packt.link/gbp/9781835882702）
- en: From the architecture point of view, TensorBoard is a Python web service that
    you can start on your computer, passing it the directory where your training process
    will save values to be analyzed. Then, you point your browser to TensorBoard’s
    port (usually 6006), and it shows you an interactive web interface with values
    updated in real time, as shown in Figure [3.4](#x1-67002r4). It’s nice and convenient,
    especially when your training is performed on a remote machine somewhere in the
    cloud.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构的角度来看，TensorBoard 是一个 Python Web 服务，你可以在自己的计算机上启动它，传递包含训练过程保存的值的目录。然后，你可以将浏览器指向
    TensorBoard 的端口（通常是 6006），它会显示一个交互式的 Web 界面，实时更新显示数值，如图 [3.4](#x1-67002r4) 所示。这非常方便，尤其是在你的训练是在云中的远程机器上进行时。
- en: Originally, TensorBoard was deployed as a part of TensorFlow, but after some
    time, it has been moved to a separate project (it’s still being maintained by
    Google) and it has its own package name. However, TensorBoard still uses the TensorFlow
    data format, so we will need to write this data from our PyTorch program. Several
    years ago, it required third-party libraries to be installed, but nowadays, PyTorch
    already comes with support of this data format (available in the torch.utils.tensorboard
    package).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，TensorBoard 是作为 TensorFlow 的一部分发布的，但经过一段时间后，它被移到了一个独立的项目中（仍由 Google 维护），并且拥有了自己的包名。不过，TensorBoard
    仍然使用 TensorFlow 的数据格式，因此我们需要从 PyTorch 程序中写入这些数据。几年前，这需要安装第三方库，但现在，PyTorch 已经原生支持这种数据格式（可以在
    torch.utils.tensorboard 包中找到）。
- en: Plotting metrics
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制指标
- en: To give you an impression of how simple is to use TensorBoard, let’s consider
    a small example that is not related to NNs, but is just about writing values into
    TensorBoard (the full example code is in Chapter03/02_tensorboard.py).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你了解使用 TensorBoard 有多简单，让我们考虑一个与神经网络无关的小例子，主要目的是将数值写入 TensorBoard（完整的示例代码在
    Chapter03/02_tensorboard.py 中）。
- en: 'In the following code, we import the required packages, create a writer of
    data, and define functions that we are going to visualize:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们导入所需的包，创建数据写入器，并定义我们要可视化的函数：
- en: '[PRE23]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: By default, SummaryWriter will create a unique directory in the runs directory
    for every launch, to be able to compare different rounds of training. The name
    of the new directory includes the current date and time, and the hostname. To
    override this, you can pass the log_dir argument to SummaryWriter. You can also
    add a suffix to the name of the directory by passing a comment argument, for example,
    to capture different experiments’ semantics, such as dropout=0.3 or strong_regularisation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SummaryWriter 会为每次启动在 runs 目录中创建一个唯一的目录，以便比较不同轮次的训练。新目录的名称包括当前日期、时间和主机名。要覆盖此行为，你可以将
    log_dir 参数传递给 SummaryWriter。你还可以通过传递 comment 参数来为目录名称添加后缀，例如捕获不同实验的语义，如 dropout=0.3
    或 strong_regularisation。
- en: 'Next, we loop over angle ranges in degrees:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们循环遍历角度范围（以度为单位）：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, we convert the angle ranges into radians and calculate our functions’
    values. Every value is added to the writer using the add_scalar function, which
    takes three arguments: the name of the parameter, its value, and the current iteration
    (which has to be an integer). The last thing we need to do after the loop is close
    the writer. Note that the writer does a periodical flush (by default, every two
    minutes), so even in the case of a lengthy optimization process, you will still
    see your values. If you need to flush SummaryWriter data explicitly, it has the
    flush() method.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将角度范围转换为弧度并计算函数值。每个值都会通过 add_scalar 函数添加到写入器中，该函数需要三个参数：参数名称、值和当前迭代（必须是整数）。在循环结束后，我们需要做的最后一件事是关闭写入器。请注意，写入器会定期刷新（默认情况下，每两分钟一次），因此即使在优化过程很长的情况下，你也能看到你的数值。如果你需要显式刷新
    SummaryWriter 数据，它有 flush() 方法。
- en: 'The result of running this will be zero output on the console, but you will
    see a new directory created inside the runs directory with a single file. To look
    at the result, we need to start TensorBoard:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码的结果是控制台没有输出，但你会看到在 runs 目录内创建了一个新目录，其中包含一个文件。要查看结果，我们需要启动 TensorBoard：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you are running TensorBoard on a remote server, you will need to add the
    --bind_all command-line option to make it accessible from other machines. Now
    you can open http://localhost:6006 in your browser to see something like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在远程服务器上运行 TensorBoard，你需要添加 --bind_all 命令行选项，以便从其他机器访问它。现在你可以在浏览器中打开 http://localhost:6006
    来查看类似的内容：
- en: '![PIC](img/file10.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file10.png)'
- en: 'Figure 3.5: Plots produced by the example (for better visualization, refer
    to https://packt.link/gbp/9781835882702 )'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：示例生成的图表（欲获得更好的可视化效果，请参考 [https://packt.link/gbp/9781835882702](https://packt.link/gbp/9781835882702)）
- en: The graphs are interactive, so you can hover over them with your mouse to see
    the actual values and select regions to zoom in and look at details. To zoom out,
    double-click inside the graph. If you run your program several times, then you
    will see several items in the Runs list on the left, which can be enabled and
    disabled in any combination, allowing you to compare the dynamics of several optimizations.
    TensorBoard allows you to analyze not only scalar values but also images, audio,
    text data, and embeddings, and it can even show you the structure of your network.
    Refer to the documentation of TensorBoard for all those features. Now, it’s time
    to combine everything you learned in this chapter and look at a real NN optimization
    problem using PyTorch.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图表是交互式的，因此你可以用鼠标悬停在图表上查看实际值，并选择区域进行放大查看细节。要缩小视图，可以在图表内双击。如果你多次运行程序，你会在左侧的“运行”列表中看到多个项目，可以任意组合启用和禁用，方便你比较多个优化过程的动态。TensorBoard
    允许你分析不仅是标量值，还包括图像、音频、文本数据和嵌入，并且它甚至可以显示你的网络结构。有关所有这些功能的详细信息，请参阅 TensorBoard 的文档。现在，是时候将本章学到的所有内容结合起来，使用
    PyTorch 查看一个真实的神经网络优化问题了。
- en: GAN on Atari images
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari 图像上的 GAN
- en: Almost every book about DL uses the MNIST dataset to show you the power of DL,
    which, over the years, has made this dataset extremely boring, like a fruit fly
    for genetic researchers. To break this tradition, and add a bit more fun to the
    book, I’ve tried to avoid well-beaten paths and illustrate PyTorch using something
    different. I briefly referred to generative adversarial networks (GANs) earlier
    in the chapter. In this example, we will train a GAN to generate screenshots of
    various Atari games.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每本关于深度学习的书籍都会使用 MNIST 数据集来展示深度学习的强大，而多年来，这个数据集已经变得极其乏味，像是遗传学研究者眼中的果蝇。为了打破这一传统，并给书籍增添一些趣味，我尝试避免老生常谈的路径，并用一些不同的内容来展示
    PyTorch。我在本章早些时候简要提到了生成对抗网络（GAN）。在这个例子中，我们将训练一个 GAN 来生成各种 Atari 游戏的屏幕截图。
- en: 'The simplest GAN architecture is this: we have two NNs where the first works
    as a ”cheater” (it is also called the generator), and the other as a ”detective”
    (another name is the discriminator). Both networks compete with each other — the
    generator tries to generate fake data, which will be hard for the discriminator
    to distinguish from your dataset, and the discriminator tries to detect the generated
    data samples. Over time, both networks improve their skills — the generator produces
    more and more realistic data samples, and the discriminator invents more sophisticated
    ways to distinguish the fake items.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的 GAN 架构是这样的：我们有两个神经网络，其中第一个充当“作弊者”（也称为生成器），另一个充当“侦探”（另一个名字是判别器）。两个网络相互竞争——生成器试图生成伪造数据，判别器则很难将其与数据集中的真实数据区分开，而判别器则尝试检测生成的数据样本。随着时间的推移，两个网络都在提高它们的技能——生成器生成的伪造数据越来越逼真，判别器则发明了更复杂的方法来区分假数据。
- en: Practical usage of GANs includes image quality improvement, realistic image
    generation, and feature learning. In our example, practical usefulness is almost
    zero, but it will be a good showcase about everything we learned about PyTorch
    so far.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 的实际应用包括图像质量提升、逼真图像生成和特征学习。在我们的示例中，实际的实用性几乎为零，但它将是一个很好的展示，展示我们迄今为止学到的关于 PyTorch
    的所有内容。
- en: 'So, let’s get started. The whole example code is in the file Chapter03/03_atari_gan.py.
    Here, we will look at only the most significant pieces of code, without the import
    section and constants declaration. The following class is a wrapper around a Gym
    game:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们开始吧。整个示例代码在文件 Chapter03/03_atari_gan.py 中。在这里，我们只看代码中最重要的部分，省略了导入部分和常量声明。以下类是对
    Gym 游戏的封装：
- en: '[PRE26]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding class includes several transformations:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述类包括几个转换：
- en: Resize the input image from 210×160 (the standard Atari resolution) to a square
    size of 64 × 64
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输入图像从 210×160（标准 Atari 分辨率）调整为 64 × 64 的正方形大小
- en: Move the color plane of the image from the last position to the first, to meet
    the PyTorch convention of convolution layers that input a tensor with the shape
    of the channels, height, and width
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像的颜色平面从最后的位置移到第一个位置，以符合 PyTorch 卷积层的惯例，这要求输入张量的形状为通道、高度和宽度
- en: Cast the image from bytes to float
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像从字节转换为浮动类型
- en: 'Then, we define two nn.Module classes: Discriminator and Generator. The first
    takes our scaled color image as input and, by applying five layers of convolutions,
    converts it into a single number passed through a Sigmoid nonlinearity. The output
    from Sigmoid is interpreted as the probability that Discriminator thinks our input
    image is from the real dataset.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了两个 nn.Module 类：判别器和生成器。第一个类将我们缩放后的彩色图像作为输入，并通过五层卷积将其转换为一个通过 Sigmoid
    非线性函数的单一数字。Sigmoid 的输出被解读为判别器认为输入图像来自真实数据集的概率。
- en: Generator takes as input a vector of random numbers (latent vector) and, by
    using the “transposed convolution” operation (it is also known as deconvolution),
    converts this vector into a color image of the original resolution. We will not
    look at those classes here as they are lengthy and not very relevant to our example;
    you can find them in the complete example file.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器则接受一个随机数向量（潜在向量）作为输入，并通过“反卷积”操作（也称为转置卷积），将该向量转换为原始分辨率的彩色图像。由于这些类较长且与我们的示例不太相关，这里我们不再详细介绍；你可以在完整的示例文件中找到它们。
- en: As input, we will use screenshots from several Atari games played simultaneously
    by a random agent. Figure [3.6](#x1-69029r6) is an example of what the input data
    looks like.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入，我们将使用几个 Atari 游戏的截图，这些截图由一个随机代理同时播放。图 3.6 展示了输入数据的样子。
- en: '![PIC](img/file11.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file11.png)'
- en: 'Figure 3.6: Sample screenshots from three Atari games'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：来自三款 Atari 游戏的截图样本
- en: 'Images are combined in batches that are generated by the following function:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通过以下函数按批次进行组合：
- en: '[PRE27]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This function infinitely samples the environment from the provided list, issues
    random actions, and remembers observations in the batch list. When the batch becomes
    of the required size, we normalize the image, convert it to a tensor, and yield
    from the generator. The check for the non-zero mean of the observation is required
    due to a bug in one of the games to prevent the flickering of an image.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数会从提供的列表中无限地采样环境，发出随机动作，并将观察结果保存在批次列表中。当批次达到所需大小时，我们对图像进行归一化，将其转换为张量，并从生成器中输出。由于某个游戏中的一个
    bug，检查观察值的非零均值是必需的，以防止图像闪烁。
- en: 'Now, let’s look at our main function, which prepares models and runs the training
    loop:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的主函数，它准备了模型并运行训练循环：
- en: '[PRE28]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we process the command-line arguments (which could be only one optional
    argument, --dev, which specifies the device to use for computations) and create
    our environment pool with a wrapper applied. This environment array will be passed
    to the iterate_batches function later to generate training data.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们处理命令行参数（可能只有一个可选参数 `--dev`，它指定用于计算的设备），并创建我们的环境池，应用了包装器。这个环境数组稍后会传递给 `iterate_batches`
    函数来生成训练数据。
- en: 'In the following piece, we create our classes — a summary writer, both networks,
    a loss function, and two optimizers:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们创建了我们的类——一个总结写入器、两个网络、一个损失函数和两个优化器：
- en: '[PRE29]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Why do we need two optimizers? It’s because that’s the way that GANs get trained:
    to train the discriminator, we need to show it both real and fake data samples
    with appropriate labels (1 for real and 0 for fake). During this pass, we update
    only the discriminator’s parameters.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要两个优化器？这是因为 GANs 的训练方式：训练判别器时，我们需要给它展示真实和虚假的数据样本，并附上适当的标签（真实为1，虚假为0）。在这一过程中，我们只更新判别器的参数。
- en: After that, we pass both real and fake samples through the discriminator again,
    but this time, the labels are 1s for all samples and we update only the generator’s
    weights. The second pass teaches the generator how to fool the discriminator and
    confuse real samples with the generated ones.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们再次将真实和虚假样本传入判别器，但这一次，所有样本的标签都是1，我们只更新生成器的权重。第二次传递教会生成器如何欺骗判别器，并将真实样本与生成的样本混淆。
- en: 'We then define arrays, which will be used to accumulate losses, iterator counters,
    and variables with the true and fake labels. We also store the current timestamp
    to report the time elapsed after 100 iterations of training:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义数组，用来累积损失、迭代器计数器和带有真实与虚假标签的变量。我们还存储当前的时间戳，以便在训练100次迭代后报告经过的时间：
- en: '[PRE30]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'At the beginning of the following training loop, we generate a random vector
    and pass it to the Generator network:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的训练循环开始时，我们生成一个随机向量，并将其传递给生成器网络：
- en: '[PRE31]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then train the discriminator by applying it two times, once to the true
    data samples in our batch and once to the generated ones:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过对判别器应用两次训练来训练它，一次用于批次中的真实数据样本，一次用于生成的数据样本：
- en: '[PRE32]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the preceding code, we need to call the detach() function on the generator’s
    output to prevent gradients of this training pass from flowing into the generator
    (detach() is a method of tensor, which makes a copy of it without connection to
    the parent’s operation, i.e., detaching the tensor from the parent’s graph).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们需要在生成器的输出上调用detach()函数，以防止这一轮训练的梯度流入生成器（detach()是tensor的一个方法，它会创建一个副本，但不与父操作关联，也就是将tensor从父图中分离出来）。
- en: 'Now it’s the generator’s training time:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是生成器的训练时间：
- en: '[PRE33]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We pass the generator’s output to the discriminator, but now we don’t stop
    the gradients. Instead, we apply the objective function with True labels. It will
    push our generator in the direction where the samples that it generates make the
    discriminator confuse them with the real data. That was the code related to training,
    and the next couple of lines report losses and feed image samples to TensorBoard:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成器的输出传递给判别器，但现在我们不再停止梯度传播。相反，我们应用带有真实标签的目标函数。这会推动我们的生成器朝着一个方向发展，使它生成的样本能让判别器混淆为真实数据。以上是与训练相关的代码，接下来的几行则报告损失并将图像样本传输到TensorBoard：
- en: '[PRE34]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The training of this example is quite a lengthy process. On a GTX 1080Ti GPU,
    100 iterations take about 2.7 seconds. At the beginning, the generated images
    are completely random noise, but after 10k–20k iterations, the generator becomes
    more and more proficient at its job and the generated images become more and more
    similar to the real game screenshots.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的训练过程相当漫长。在一块GTX 1080Ti GPU上，100次迭代大约需要2.7秒。刚开始时，生成的图像完全是随机噪声，但在经过10k到20k次迭代后，生成器变得越来越熟练，生成的图像也越来越像真实的游戏截图。
- en: It also worth noting the performance improvement in software libraries. In the
    first and second editions of the book, exactly the same example ran much slower
    on the same hardware I have. On GTX 1080Ti, 100 iterations took around 40 seconds.
    Now, with PyTorch 2.2.0 on exactly the same GPU, 100 iterations take 2.7 seconds.
    So, instead of 3–4 hours, it now takes about 30 minutes to get good generated
    images.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，软件库的性能改进。在本书的第一版和第二版中，完全相同的示例在我拥有的相同硬件上运行速度要慢得多。在GTX 1080Ti上，100次迭代大约需要40秒。而现在，使用PyTorch
    2.2.0在相同的GPU上，100次迭代仅需2.7秒。因此，从原本需要3-4小时的时间，现在只需要大约30分钟就能获得良好的生成图像。
- en: 'My experiments gave the following images after 40k–50k of training iterations
    (about half an hour on a 1080 GPU):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我的实验在40k到50k次训练迭代后（大约半小时，在1080 GPU上）产生了以下图像：
- en: '![PIC](img/file12.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file12.png)'
- en: 'Figure 3.7: Sample images produced by the generator network'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：生成器网络生成的示例图像
- en: As you can see, our network was able to reproduce the Atari screenshots quite
    well. In the next section, we’ll look at how we can simplify our code by using
    the add-on PyTorch library, Ignite.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的网络能够很好地再现Atari的截图。在接下来的部分，我们将探讨如何通过使用PyTorch的附加库Ignite来简化代码。
- en: PyTorch Ignite
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch Ignite
- en: 'PyTorch is an elegant and flexible library, which makes it a favorite choice
    for thousands of researchers, DL enthusiasts, industry developers, and others.
    But flexibility has its own price: too much code to be written to solve your problem.
    Sometimes, this is very beneficial, such as when implementing some new optimization
    method or DL trick that hasn’t been included in the standard library yet. Then
    you just implement the formulas using Python and PyTorch magic will do all the
    gradient and backpropagation machinery for you. Another example is in situations
    when you have to work on a very low level, fiddling with gradients, optimizer
    details, and the way your data is transformed by the NN.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch是一个优雅且灵活的库，这使得它成为成千上万的研究人员、深度学习爱好者、行业开发者等的首选。但灵活性也有其代价：需要编写大量代码来解决你的问题。有时，这种灵活性是非常有益的，比如当你实现一些尚未包含在标准库中的新优化方法或深度学习技巧时。那时，你只需使用Python实现公式，而PyTorch魔法会为你处理所有的梯度和反向传播机制。另一个例子是当你需要在非常低层次工作时，需要调试梯度、优化器的细节，或者调整神经网络处理数据的方式。
- en: 'However, sometimes you don’t need this flexibility, which happens when you
    work on routine tasks, like the simple supervised training of an image classifier.
    For such tasks, standard PyTorch might be at too low a level when you need to
    deal with the same code over and over again. The following is a non-exhaustive
    list of topics that are an essential part of any DL training procedure, but require
    some code to be written:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时你不需要这种灵活性，特别是当你处理常规任务时，比如简单的图像分类器的监督训练。对于这类任务，标准的 PyTorch 可能过于底层，特别是当你需要一遍又一遍地处理相同代码时。以下是一些常见的深度学习（DL）训练过程中必不可少的话题，但需要编写一些代码：
- en: Data preparation and transformation, and the generation of batches
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备和转换，以及批次的生成
- en: Calculation of training metrics, like loss values, accuracy, and F1-scores
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算训练度量指标，如损失值、准确率和 F1 值
- en: Periodical testing of the model being trained on the test and validation datasets
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期在测试集和验证集上对正在训练的模型进行测试
- en: Model checkpointing after some number of iterations or when a new best metric
    is achieved
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在若干迭代后，或者当达到新的最佳度量时，进行模型检查点保存
- en: Sending metrics into a monitoring tool like TensorBoard
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将度量数据发送到像 TensorBoard 这样的监控工具中
- en: Hyperparameters change over time, like a learning rate decrease/increase schedule
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数随着时间变化，如学习率的下降/上升计划
- en: Writing training progress messages on the console
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在控制台上写出训练进度信息
- en: They are all doable using only PyTorch, of course, but it might require you
    to write a significant amount of code. As those tasks occur in any DL project,
    it quickly becomes cumbersome to write the same code over and over again. The
    normal approach to solving the issue is to write the functionality once, wrap
    it into a library, and reuse it later. If the library is open source and of good
    quality (easy to use, provides a good level of flexibility, written properly,
    and so on), it will become popular as more and more people use it in their projects.
    This process is not DL-specific; it happens everywhere in the software industry.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用 PyTorch 完全可以实现这些任务，但可能需要编写大量代码。由于这些任务出现在任何 DL 项目中，重复编写相同的代码很快就会变得繁琐。解决这个问题的常见方法是一次性编写功能，将其封装成库，之后再重用。如果这个库是开源且高质量的（易于使用、提供良好的灵活性、编写得当等），它将随着越来越多的人在项目中使用而变得流行。这个过程不仅仅是深度学习特有的；它在软件行业的各个领域都在发生。
- en: 'There are several libraries for PyTorch that simplify the solving of common
    tasks: ptlearn, fastai, ignite, and some others. The current list of “PyTorch
    ecosystem projects” can be found here: [https://pytorch.org/ecosystem](https://pytorch.org/ecosystem).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个 PyTorch 库可以简化常见任务的解决方案：ptlearn、fastai、ignite 等。当前的“PyTorch 生态系统项目”列表可以在这里找到：[https://pytorch.org/ecosystem](https://pytorch.org/ecosystem)。
- en: It might be appealing to start using those high-level libraries from the beginning,
    as they allow you to solve common problems with just a couple of lines of code,
    but there is some danger here. If you only know how to use high-level libraries
    without understanding low-level details, you might get stuck on problems that
    can’t be solved solely by standard methods. In the very dynamic field of ML, this
    happens very often.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始就使用这些高级库可能很有吸引力，因为它们可以通过几行代码解决常见问题，但这里存在一定的风险。如果你只知道如何使用高级库，而不了解底层细节，可能会在遇到无法仅通过标准方法解决的问题时陷入困境。在机器学习这个高度动态的领域中，这种情况非常常见。
- en: 'The main focus of this book is to ensure that you understand RL methods, their
    implementation, and their applicability, so we will use an incremental approach.
    In the beginning, we will implement methods using only PyTorch code, but with
    more progress, examples will be implemented using high-level libraries. For RL,
    this will be the small library written by me: PTAN ([https://github.com/Shmuma/ptan/](https://github.com/Shmuma/ptan/)),
    and it will be introduced in Chapter [7](ch011.xhtml#x1-1070007).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要重点是确保你理解强化学习（RL）方法、它们的实现和应用性，因此我们将采用逐步推进的方式。最开始，我们将仅使用 PyTorch 代码来实现方法，但随着进展，示例将使用高级库进行实现。对于
    RL，我们将使用我编写的小型库：PTAN（[https://github.com/Shmuma/ptan/](https://github.com/Shmuma/ptan/)），并将在第[7](ch011.xhtml#x1-1070007)章介绍。
- en: 'To reduce the amount of DL boilerplate code, we will use a library called PyTorch
    Ignite: [https://pytorch-ignite.ai](https://pytorch-ignite.ai). In this section,
    a small overview of Ignite will be given, then we will check the Atari GAN example
    once it has been rewritten using Ignite.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少深度学习的样板代码，我们将使用一个名为 PyTorch Ignite 的库：[https://pytorch-ignite.ai](https://pytorch-ignite.ai)。在本节中，我们将简要介绍
    Ignite，然后我们会查看重写为 Ignite 的 Atari GAN 示例。
- en: Ignite concepts
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ignite 的概念
- en: 'At a high level, Ignite simplifies the writing of the training loop in PyTorch
    DL. Earlier in this chapter (in the Loss functions and optimizers section, you
    saw that the minimal training loop consists of:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，Ignite 简化了 PyTorch 深度学习训练循环的编写。在本章的前面部分（在损失函数和优化器部分），你看到最小的训练循环包括：
- en: Sampling a batch of training data
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练数据中采样一个批次
- en: Applying an NN to this batch to calculate the loss function—the single value
    we want to minimize
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经网络应用于该批次以计算损失函数——我们想要最小化的单一值
- en: Running backpropagation of the loss to get gradients on the network’s parameters
    in respect to the loss
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行反向传播以获取网络参数相对于损失函数的梯度
- en: Asking the optimizer to apply the gradients to the network
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求优化器将梯度应用到网络中
- en: Repeating until we are happy or bored of waiting
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复进行，直到我们满意或厌烦等待为止
- en: 'The central piece of Ignite is the Engine class, which loops over the data
    source, applying the processing function to the data batch. In addition to that,
    Ignite offers the ability to provide functions to be called at specific conditions
    of the training loop. Those conditions are called Events and could be at the:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: Ignite 的核心部分是 Engine 类，它循环遍历数据源，将处理函数应用于数据批次。除此之外，Ignite 还提供了在训练循环的特定条件下调用函数的功能。这些条件被称为事件（Events），可能发生在以下几个时刻：
- en: Beginning/end of the whole training process
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个训练过程的开始/结束
- en: Beginning/end of a training epoch (iteration over the data)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单次训练周期的开始/结束（对数据的迭代）
- en: Beginning/end of a single batch processing
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单次批次处理的开始/结束
- en: In addition to that, custom events exist and allow you to specify your function
    to be called every N events, for example, if you want to do some calculations
    every 100 batches or every second epoch.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有自定义事件，它们允许你指定在每 N 次事件时调用你的函数。例如，如果你希望每 100 个批次或每个第二个周期进行一些计算，可以使用自定义事件。
- en: 'A very simplistic example of Ignite in action is shown in the following code
    block:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: Ignite 在实际应用中的一个非常简单的例子如下所示：
- en: '[PRE35]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code is not runnable as it misses lots of details, like the data source,
    model, and optimizer creation, but it shows the basic idea of Ignite usage. The
    main benefit of Ignite is in the ability it provides to extend the training loop
    with existing functionality. You want the loss value to be smoothed and written
    in TensorBoard every 100 batches? No problem! Add two lines and it will be done.
    You want to run model validation every 10 epochs? Okay, write a function to run
    a test and attach it to the Engine instance, and it will be called.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码不能直接运行，因为缺少很多细节，比如数据源、模型和优化器的创建，但它展示了 Ignite 使用的基本思想。Ignite 的主要优势在于它提供了通过现有功能扩展训练循环的能力。你希望每
    100 个批次平滑损失值并写入 TensorBoard？没问题！加两行代码就能完成。你希望每 10 个周期运行模型验证？好吧，写一个函数来运行测试并将其附加到
    Engine 实例，它就会被调用。
- en: 'A description of the full Ignite functionality is beyond the scope of the book,
    but you can read the documentation on the official website: [https://pytorch-ignite.ai](https://pytorch-ignite.ai).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Ignite 功能的完整描述超出了本书的范围，但你可以在官方网站上阅读文档：[https://pytorch-ignite.ai](https://pytorch-ignite.ai)。
- en: GAN training on Atari using Ignite
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ignite 在 Atari 上进行 GAN 训练
- en: To give you an illustration of Ignite, let’s change the example of GAN training
    on Atari images. The full example code is available in Chapter03/04_atari_gan_ignite.py;
    here, I will just show code that differs from the previous section.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个 Ignite 的示例，我们将改变 Atari 图像上的 GAN 训练示例。完整的示例代码在 Chapter03/04_atari_gan_ignite.py
    中；在这里，我只会展示与前一部分不同的代码。
- en: 'First, we import several Ignite classes:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入几个 Ignite 类：
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The Engine and Events classes have already been outlined. The package ignite.metrics
    contains classes related to working with the performance metrics of the training
    process, such as confusion matrices, precision, and recall. In our example, we
    will use the class RunningAverage, which provides a way to smooth time series
    values. In the previous example, we did this by calling np.mean() on an array
    of losses, but RunningAverage provides a more convenient (and mathematically more
    correct) way of doing this. In addition, we import the TensorBoard logger from
    the Ignite contrib package (the functionality of which is contributed by others).
    We’ll also use the Timer handler, which provides a simple way to calculate time
    elapsed between certain events.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`Engine`和`Events`类已经概述过。`ignite.metrics`包包含与训练过程性能指标相关的类，如混淆矩阵、精确度和召回率。在我们的示例中，我们将使用`RunningAverage`类，它提供了一种平滑时间序列值的方法。在之前的示例中，我们通过对损失数组调用`np.mean()`来实现这一点，但`RunningAverage`提供了一种更方便（且在数学上更正确）的方法。此外，我们还从Ignite贡献包中导入了TensorBoard日志记录器（其功能由其他人贡献）。我们还将使用`Timer`处理程序，它提供了一种简单的方式来计算某些事件之间经过的时间。'
- en: 'As a next step, we need to define our processing function:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们需要定义我们的处理函数：
- en: '[PRE37]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This function takes the data batch and does an update of both the discriminator
    and generator models on this batch. This function can return any data to be tracked
    during the training process; in our case, it will be two loss values for both
    models. In this function, we can also save images to be displayed in TensorBoard.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接收数据批次，并对该批次中的判别器和生成器模型进行更新。此函数可以返回任何在训练过程中需要跟踪的数据；在我们的例子中，它将返回两个模型的损失值。在这个函数中，我们还可以保存图像，以便在TensorBoard中显示。
- en: 'After this is done, all we need to do is create an Engine instance, attach
    the required handlers, and run the training process:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步后，我们需要做的就是创建一个引擎实例，附加所需的处理程序，并运行训练过程：
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding code, we create our engine, passing our processing function
    and attaching two RunningAverage transformations for our two loss values. Being
    attached, every RunningAverage produces a so-called “metric” — a derived value
    kept around during the training process. The names of our smoothed metrics are
    avg_loss_gen for smoothed loss from the generator, and avg_loss_dis for smoothed
    loss from the discriminator. Those two values will be written in TensorBoard after
    every iteration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们创建了引擎，传入了处理函数并附加了两个`RunningAverage`变换，用于计算两个损失值。每次附加时，`RunningAverage`会产生一个所谓的“指标”——在训练过程中保持的派生值。我们平滑后的指标名称分别为`avg_loss_gen`（来自生成器的平滑损失）和`avg_loss_dis`（来自判别器的平滑损失）。这两个值将在每次迭代后写入到TensorBoard中。
- en: We also attach the timer, which, being created without any constructor arguments,
    acts as a simple manually-controlled timer (we call its reset() method manually),
    but can work in a more flexible way with different configuration options.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还附加了定时器，定时器在没有构造函数参数的情况下创建，作为一个简单的手动控制定时器（我们手动调用它的`reset()`方法），但也可以通过不同的配置选项以更灵活的方式工作。
- en: 'The last piece of code attaches another event handler, which will be our function,
    and is called by the Engine on every iteration completion:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一段代码附加了另一个事件处理程序，这将是我们的函数，并且在每次迭代完成时由引擎调用：
- en: '[PRE39]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: It will write a log line with an iteration index, time taken and values of smoothed
    metrics. The final line starts our engine, passing the already defined function
    as the data source (the iterate_batches function is a generator, returning the
    normal iterator over batches, so, it will be perfectly fine to pass its output
    as a data argument). And that’s it. If you run the Chapter03/04_atari_gan_ignite.py
    example, it will work the same way as our previous example, which might not be
    very impressive for such a small example, but in real projects, Ignite usage normally
    pays off by making your code cleaner and more extensible.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 它将记录一行日志，包含迭代索引、所用时间以及平滑后的指标值。最后一行启动了我们的引擎，将已定义的函数作为数据源传入（`iterate_batches`函数是一个生成器，返回正常的批次迭代器，因此，将其输出作为数据参数传入是完全可以的）。就这样。如果你运行`Chapter03/04_atari_gan_ignite.py`示例，它将像我们之前的示例一样工作，这对于这么一个小示例可能不太令人印象深刻，但在实际项目中，Ignite的使用通常能通过使代码更简洁、更具可扩展性而带来回报。
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you saw a quick overview of PyTorch’s functionality and features.
    We talked about basic fundamental pieces, such as tensors and gradients, and you
    saw how an NN can be made from the basic building blocks, before learning how
    to implement those blocks yourself.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你看到了 PyTorch 功能和特性的快速概览。我们讨论了基本的基础知识，如张量和梯度，并且你了解了如何利用这些基础构建块构建一个神经网络，接着学习了如何自己实现这些构建块。
- en: We discussed loss functions and optimizers, as well as the monitoring of training
    dynamics. Finally, you were introduced to PyTorch Ignite, a library used to provide
    a higher-level interface for training loops. The goal of the chapter was to give
    a very quick introduction to PyTorch, which will be used later in the book.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了损失函数和优化器，以及如何监控训练动态。最后，你还了解了 PyTorch Ignite，这是一个用于提供更高层次训练循环接口的库。本章的目标是对
    PyTorch 做一个非常快速的介绍，这将在书中的后续章节中使用。
- en: 'In the next chapter, we are ready to start dealing with the main subject of
    this book: RL methods.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始处理本书的主题：强化学习方法。
