- en: Autoencoders – Feature Extraction and Denoising
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器 – 特征提取与去噪
- en: An autoencoder network is nowadays one of the widely used deep learning architectures.
    It's mainly used for unsupervised learning of efficient decoding tasks. It can
    also be used for dimensionality reduction by learning an encoding or a representation
    for a specific dataset. Using autoencoders in this chapter, we'll show how to
    denoise your dataset by constructing another dataset with the same dimensions
    but less noise. To use this concept in practice, we will extract the important
    features from the MNIST dataset and try to see how the performance will be significantly
    enhanced by this.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器网络如今是广泛使用的深度学习架构之一。它主要用于无监督学习高效解码任务。它也可以通过学习特定数据集的编码或表示来进行降维。在本章中使用自编码器，我们将展示如何通过构建另一个具有相同维度但噪声较少的数据集来去噪你的数据集。为了将这一概念付诸实践，我们将从
    MNIST 数据集中提取重要特征，并尝试查看如何通过这一方法显著提高性能。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器简介
- en: Examples of autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器示例
- en: Autoencoder architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器架构
- en: Compressing the MNIST dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 压缩 MNIST 数据集
- en: Convolutional autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积自编码器
- en: Denoising autoencoders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: Applications of autoencoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的应用
- en: Introduction to autoencoders
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器简介
- en: An autoencoder is yet another deep learning architecture that can be used for
    many interesting tasks, but it can also be considered as a variation of the vanilla
    feed-forward neural network, where the output has the same dimensions as the input.
    As shown in *Figure 1*, the way autoencoders work is by feeding data samples *(x[1],...,x[6]) *to
    the network. It will try to learn a lower representation of this data in layer
    *L2*, which you might call a way of encoding your dataset in a lower representation.
    Then, the second part of the network, which you might call a decoder, is responsible
    for constructing an output from this representation ![](img/f1bff437-7017-445b-93d4-7886dd3f1623.png).
    You can think of the intermediate lower representation that the network learns
    from the input data as a compressed version of it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是另一种深度学习架构，可以用于许多有趣的任务，但它也可以被看作是普通前馈神经网络的一种变体，其中输出与输入具有相同的维度。如 *图 1* 所示，自编码器的工作原理是将数据样本
    *(x[1],...,x[6])* 输入网络。它将在 *L2* 层学习该数据的较低表示，你可以把它看作是将数据集编码为较低表示的一种方式。然后，网络的第二部分（你可以称之为解码器）负责根据这个表示构建输出！[](img/f1bff437-7017-445b-93d4-7886dd3f1623.png)。你可以将网络从输入数据中学习到的中间较低表示看作是它的压缩版本。
- en: Not very different from all the other deep learning architectures that we have
    seen so far, autoencoders use backpropagation algorithms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 和我们迄今为止见过的其他深度学习架构并没有太大不同，自编码器使用反向传播算法。
- en: 'An autoencoder neural network is an unsupervised learning algorithm that applies
    backpropagation, setting the target values to be equal to the inputs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器神经网络是一种无监督学习算法，应用反向传播，将目标值设为与输入相同：
- en: '![](img/e447a3cc-4dd7-4ae1-bba0-fd6c845ebbe5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e447a3cc-4dd7-4ae1-bba0-fd6c845ebbe5.png)'
- en: 'Figure 1: General autoencoder architecture'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一般自编码器架构
- en: Examples of autoencoders
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器示例
- en: In this chapter, we will demonstrate some examples of different variations of
    autoencoders using the MNIST dataset. As a concrete example, suppose the inputs *x*
    are the pixel intensity values from a 28 x 28 image (784 pixels); so the number
    of input data samples is *n=784*. There are *s2=392* hidden units in layer *L2*.
    And since the output will be of the same dimensions as the input data samples,
    *y ∈ R784*. The number of neurons in the input layer will be *784*, followed by
    *392* neurons in the middle layer *L2*; so the network will be a lower representation,
    which is a compressed version of the output. The network will then feed this compressed
    lower representation of the input *a(L2) ∈ R392* to the second part of the network,
    which will try hard to reconstruct the input pixels *784* from this compressed
    version.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过使用 MNIST 数据集演示一些不同类型的自编码器变体。作为一个具体例子，假设输入 *x* 是来自 28 x 28 图像的像素强度值（784
    个像素）；因此，输入数据样本的数量为 *n=784*。在 *L2* 层中有 *s2=392* 个隐藏单元。由于输出将与输入数据样本的维度相同，*y ∈ R784*。输入层中的神经元数量为
    *784*，中间层 *L2* 中有 *392* 个神经元；因此，网络将是一个较低的表示，这是输出的压缩版本。然后，网络将把这个压缩的较低表示 *a(L2)
    ∈ R392* 输入到网络的第二部分，后者将尽力从这个压缩版本中重建输入像素 *784*。
- en: Autoencoders rely on the fact that the input samples represented by the image
    pixels will be somehow correlated and then it will use this fact to reconstruct
    them. So autoencoders are a bit similar to dimensionality reduction techniques,
    because they learn a lower representation of the input data as well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器依赖于输入样本由图像像素表示，这些像素在某种程度上是相关的，然后它将利用这一点来重建它们。因此，自编码器有点类似于降维技术，因为它们也学习输入数据的低维表示。
- en: 'To sum up, a typical autoencoder will consist of three parts:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，典型的自编码器将由三个部分组成：
- en: The encoder part, which is responsible for compressing the input into a lower
    representation
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器部分，负责将输入压缩为低维表示
- en: The code, which is the intermediate result of the encoder
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码部分，即编码器的中间结果
- en: The decoder, which is responsible for reconstructing the the original input
    using this code
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器，负责使用该代码重建原始输入
- en: 'The following figure shows the three main components of a typical autoencoder:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了典型自编码器的三个主要组成部分：
- en: '![](img/3f678096-4330-4dfb-b907-c4f22b8c3971.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f678096-4330-4dfb-b907-c4f22b8c3971.png)'
- en: 'Figure 2: How encoders function over an image'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：编码器如何在图像上发挥作用
- en: As we mentioned, autoencoders part learn a compressed representation of the
    input that are then fed to the third part, which tries to reconstruct the input.
    The reconstructed input will be similar to the output but it won't be exactly
    the same as the original output, so autoencoders can't be used for compression
    tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所提到的，自编码器部分学习输入的压缩表示，然后将其馈送给第三部分，后者尝试重建输入。重建后的输入将类似于输出，但不会完全与原始输出相同，因此自编码器不能用于压缩任务。
- en: Autoencoder architectures
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器架构
- en: 'As we mentioned, a typical autoencoder consists of three parts. Let''s explore
    these three parts in more detail. To motivate you, we are not going to reinvent
    the wheel here in this chapter. The encoder-decoder part is nothing but a fully
    connected neural network, and the code part is another neural network but it''s
    not fully connected. The dimensionality of this code part is controllable and
    we can treat it as a hyperparameter:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所提到的，典型的自编码器由三个部分组成。让我们更详细地探索这三部分。为了激励你，我们在本章中不会重新发明轮子。编码器-解码器部分不过是一个完全连接的神经网络，而代码部分是另一个神经网络，但它不是完全连接的。这个代码部分的维度是可控的，我们可以把它当作超参数来处理：
- en: '![](img/552fb5f6-0459-4e13-887b-d55d288e0f72.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/552fb5f6-0459-4e13-887b-d55d288e0f72.png)'
- en: 'Figure 3: General encoder-decoder architecture of autoencoders'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：自编码器的一般编码器-解码器架构
- en: 'Before diving into using autoencoders for compressing the MNIST dataset, we
    are going to list the set of hyperparameters that we can use to fine-tune the
    autoencoder model. There are mainly four hyperparameters:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入使用自编码器压缩 MNIST 数据集之前，我们将列出可以用于微调自编码器模型的一组超参数。主要有四个超参数：
- en: '**Code part size**: This is the number of units in the middle layer. The lower
    the number of units we have in this layer, the more compressed the representation
    of the input we get.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码部分大小**：这是中间层的单元数。中间层的单元数越少，我们得到的输入压缩表示越多。'
- en: '**Number of layers in the encoder and decoder**: As we mentioned, the encoder
    and decoder are nothing but a fully connected neural network that we can make
    as deep as we can by adding more layers.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器和解码器的层数**：正如我们所提到的，编码器和解码器不过是一个完全连接的神经网络，我们可以通过添加更多层使其尽可能深。'
- en: '**Number of units per layer**: We can also use a different number of units
    in each layer. The shape of the encoder and decoder is very similar to DeconvNets,
    where the number of layers in the encoders decreases as we approach the code part
    and then starts to increase as we approach the final layer of the decoder.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**每层单元数**：我们也可以在每一层使用不同的单元数。编码器和解码器的形状与 DeconvNets 非常相似，其中编码器的层数在接近代码部分时减少，然后在接近解码器的最终层时开始增加。'
- en: '**Model loss function**: We can use different loss functions as well, such
    as MSE or cross-entropy.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型损失函数**：我们也可以使用不同的损失函数，例如 MSE 或交叉熵。'
- en: After defining these hyperparameters and giving them initial values, we can
    train the network using a backpropagation algorithm.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义这些超参数并赋予它们初始值后，我们可以使用反向传播算法来训练网络。
- en: Compressing the MNIST dataset
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩 MNIST 数据集
- en: In this part, we'll build a simple autoencoder that can be used to compress
    the MNIST dataset. So we will feed the images of this dataset to the encoder part,
    which will try to learn a lower compressed representation for them; then we will
    try to construct the input images again in the decoder part.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将构建一个简单的自动编码器，用于压缩 MNIST 数据集。因此，我们将把该数据集中的图像输入到编码器部分，编码器将尝试为它们学习一个压缩的低维表示；然后，我们将在解码器部分尝试重新构建输入图像。
- en: The MNIST dataset
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据集
- en: We will start the implementation by getting the MNIST dataset, using the helper
    functions of TensorFlow.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用 TensorFlow 的辅助函数获取 MNIST 数据集来开始实现。
- en: 'Let''s import the necessary packages for this implementation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们导入实现所需的必要包：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s start off by plotting some examples from the MNIST dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从绘制一些 MNIST 数据集中的示例开始：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/9ca739a8-c53b-48ee-b8b3-18f455abca36.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ca739a8-c53b-48ee-b8b3-18f455abca36.png)'
- en: 'Figure 4: Example image from the MNIST dataset'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：来自 MNIST 数据集的示例图像
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/25c86e3f-00cd-4eda-a9de-ed311292e709.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25c86e3f-00cd-4eda-a9de-ed311292e709.png)'
- en: 'Figure 5: Example image from the MNIST dataset'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：来自 MNIST 数据集的示例图像
- en: Building the model
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: In order to build the encoder, we need to figure out how many pixels each MNIST
    image will have so that we can figure out the size of the input layer of the encoder.
    Each image from the MNIST dataset is 28 by 28 pixels, so we will reshape this
    matrix to a vector of 28 x 28 = 784 pixel values. We don't have to normalize the
    images of MNIST because they are already normalized.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建编码器，我们需要弄清楚每张 MNIST 图像包含多少像素，这样我们就能确定编码器输入层的大小。每张来自 MNIST 数据集的图像是 28 x 28
    像素，因此我们将把该矩阵重塑为一个包含 28 x 28 = 784 个像素值的向量。我们不需要对 MNIST 图像进行归一化处理，因为它们已经是归一化的。
- en: 'Let''s start off building our three components of the model. In this implementation,
    we will use a very simple architecture of a single hidden layer followed by ReLU
    activation, as shown in the following figure:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建模型的三个组件。在此实现中，我们将使用一个非常简单的架构，即单个隐藏层后接 ReLU 激活函数，如下图所示：
- en: '![](img/a887c40b-4430-4ce7-95c3-8ebf56ad2b69.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a887c40b-4430-4ce7-95c3-8ebf56ad2b69.png)'
- en: 'Figure 6: Encoder-decoder architecture for MNIST implementation'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：MNIST 实现的编码器-解码器架构
- en: 'Let''s go ahead and implement this simple encoder-decoder architecture according
    to the preceding explanation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的解释，接下来我们将实现这个简单的编码器-解码器架构：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now we have defined our model and also used a binary cross-entropy since the
    images, pixels are already normalized.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，并且使用了二元交叉熵，因为图像像素已经进行了归一化处理。
- en: Model training
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: In this section, we'll kick off the training process. We'll use the helper function
    of the `mnist_dataset` object in order to get a random batch from the dataset
    with a specific size; then we'll run the optimizer on this batch of images.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将启动训练过程。我们将使用 `mnist_dataset` 对象的辅助函数来从数据集中获取指定大小的随机批次；然后我们将在这一批图像上运行优化器。
- en: 'Let''s start this section by creating the session variable, which will be responsible
    for executing the computational graph that we defined earlier:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建会话变量来开始本节内容，该变量将负责执行我们之前定义的计算图：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next up, let''s kick off the training process:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们启动训练过程：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After running the preceding code snippet for 20 epochs, we will get a trained
    model that is able to generate or reconstruct images from the test set of the
    MNIST data. Bear in mind that if we feed images that are not similar to the ones
    that the model was trained on, then the reconstruction process just won't work
    because autoencoders are data-specific.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码片段 20 个 epoch 后，我们将得到一个训练好的模型，它能够从 MNIST 数据的测试集中生成或重建图像。请记住，如果我们输入的图像与模型训练时使用的图像不相似，那么重建过程将无法正常工作，因为自动编码器是针对特定数据的。
- en: 'Let''s test the trained model by feeding some images from the test set and
    see how the model is able to reconstruct them in the decoder part:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过输入一些来自测试集的图像来测试训练好的模型，看看模型在解码器部分如何重建这些图像：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](img/7167052e-0b8f-4049-9738-4cca4e2c0e0f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7167052e-0b8f-4049-9738-4cca4e2c0e0f.png)'
- en: 'Figure 7: Examples of the original test images (first row) and their constructions
    (second row)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：原始测试图像（第一行）及其重建（第二行）的示例
- en: As you can see, the reconstructed images are very close to the input ones, but
    we can probably get better images using convolution layers in the encoder-decoder
    part.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，重建后的图像与输入图像非常接近，但我们可能可以通过在编码器-解码器部分使用卷积层来获得更好的图像。
- en: Convolutional autoencoder
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自动编码器
- en: The previous simple implementation did a good job while trying to reconstruct
    input images from the MNIST dataset, but we can get a better performance through
    a convolution layer in the encoder and the decoder parts of the autoencoder. The
    resulting network of this replacement is called **convolutional autoencoder**
    (**CAE**). This flexibility of being able to replace layers is a great advantage
    of autoencoders and makes them applicable to different domains.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的简单实现很好地完成了从MNIST数据集中重建输入图像的任务，但通过在自编码器的编码器和解码器部分添加卷积层，我们可以获得更好的性能。这个替换后得到的网络称为**卷积自编码器**（**CAE**）。这种能够替换层的灵活性是自编码器的一个巨大优势，使它们能够应用于不同的领域。
- en: The architecture that we'll be using for the CAE will contain upsampling layers
    in the decoder part of the network to get the reconstructed version of the image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于CAE的架构将在网络的解码器部分包含上采样层，以获取图像的重建版本。
- en: Dataset
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: 'In this implementation, we can use any kind of imaging dataset and see how
    the convolutional version of the autoencoder will make a difference. We will still
    be using the MNIST dataset for this, so let''s start off by getting the dataset
    using the TensorFlow helpers:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们可以使用任何类型的图像数据集，看看卷积版本的自编码器会带来什么变化。我们仍然将使用MNIST数据集，因此让我们开始使用TensorFlow辅助函数获取数据集：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s show one digit from the dataset:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展示数据集中的一个数字：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '![](img/2105032e-0770-41d3-af6a-85e6f825f43f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2105032e-0770-41d3-af6a-85e6f825f43f.png)'
- en: 'Figure 8: Example image from the MNIST dataset'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：来自MNIST数据集的示例图像
- en: Building the model
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: In this implementation, we will be using convolution layers with stride 1, and
    the padding parameter is set to be the same. By this, we won't change the height
    or width of the image. Also, we are using a set of max pooling layers to reduce
    the width and height of the image and hence building a compressed lower representation
    of the image.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们将使用步幅为 1 的卷积层，并将填充参数设置为相同。这样，我们不会改变图像的高度或宽度。同时，我们使用了一组最大池化层来减少图像的宽度和高度，从而构建图像的压缩低维表示。
- en: 'So let''s go ahead and build the core of our network:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们继续构建网络的核心部分：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we are good to go. We've built the decoder-decoder part of the convolutional
    neural network while showing how the dimensions of the input image will be reconstructed
    in the decoder part.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始了。我们已经构建了卷积神经网络的解码器-解码器部分，并展示了输入图像在解码器部分如何被重建。
- en: Model training
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: Now that we have the model built, we can kick off the learning process by generating
    random batches form the MNIST dataset and feed them to the optimizer defined earlier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了模型，我们可以通过生成来自MNIST数据集的随机批次并将它们输入到之前定义的优化器中来启动学习过程。
- en: 'Let''s start off by creating the session variable; it will be responsible for
    executing the computational graph that we defined earlier:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建会话变量开始；它将负责执行我们之前定义的计算图：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After running the preceding code snippet for 20 epochs, we''ll get a trained
    CAE, so let''s go ahead and test this model by feeding similar images from the
    MNIST dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码片段 20 个周期后，我们将得到一个训练好的CAE，因此我们可以继续通过输入来自MNIST数据集的相似图像来测试这个模型：
- en: '[PRE16]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/38895773-80a4-4f5d-b572-2f728dac20dd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38895773-80a4-4f5d-b572-2f728dac20dd.png)'
- en: 'Figure 9: Examples of the original test images (first row) and their constructions
    (second row) using the convolution autoencoder'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：原始测试图像（第一行）及其重建图像（第二行），使用卷积自编码器
- en: Denoising autoencoders
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'We can take the autoencoder architecture further by forcing it to learn more
    important features about the input data. By adding noise to the input images and
    having the original ones as the target, the model will try to remove this noise
    and learn important features about them in order to come up with meaningful reconstructed
    images in the output. This kind of CAE architecture can be used to remove noise
    from input images. This specific variation of autoencoders is called **denoising
    autoencoder**:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步优化自编码器架构，迫使它学习关于输入数据的重要特征。通过向输入图像添加噪声，并以原始图像作为目标，模型将尝试去除这些噪声，并学习关于它们的重要特征，以便在输出中生成有意义的重建图像。这种CAE架构可以用于去除输入图像中的噪声。这种自编码器的特定变种称为**去噪自编码器**：
- en: '![](img/5100d26b-63c5-4c69-93cd-4fb8df5ddcb2.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5100d26b-63c5-4c69-93cd-4fb8df5ddcb2.png)'
- en: 'Figure 10: Examples of original images and the same images after adding a bit
    of Gaussian noise'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：原始图像及添加少量高斯噪声后的相同图像示例
- en: 'So let''s start off by implementing the architecture in the following figure.
    The only extra thing that we have added to this denoising autoencoder architecture
    is some noise in the original input image:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们从实现下图中的架构开始。我们在这个去噪自编码器架构中唯一添加的额外内容就是在原始输入图像中加入了一些噪声：
- en: '![](img/5a04b580-1641-457a-8609-cf2392f35c28.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a04b580-1641-457a-8609-cf2392f35c28.png)'
- en: 'Figure 11: General denoising architecture of autoencoders'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：自编码器的通用去噪架构
- en: Building the model
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: In this implementation, we will be using more layers in the encoder and decoder
    part, and the reason for this is the new complexity that we have added to the
    input.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们将在编码器和解码器部分使用更多的层，原因在于我们给输入增加了新的复杂度。
- en: The next model is exactly the same as the previous CAE but with extra layers
    that will help us to reconstruct a noise-free image from a noisy one.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模型与之前的CAE完全相同，只是增加了额外的层，这将帮助我们从有噪声的图像中重建出无噪声的图像。
- en: 'So let''s go ahead and build this architecture:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们继续构建这个架构：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now we have a more complex or deeper version of the convolutional model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个更复杂或更深的卷积模型版本。
- en: Model training
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: It's time to start training this deeper network, which in turn will take more
    time to converge by reconstructing noise-free images from the noisy input.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候开始训练这个更深的网络了，这将需要更多的时间来收敛，通过从有噪声的输入中重建无噪声的图像。
- en: 'So let''s start off by creating the session variable:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们先创建会话变量：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next up, we will kick off the training process but for more number of epochs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将启动训练过程，但使用更多的训练轮次：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now we have trained the model to be able to produce noise-free images, which
    makes autoencoders applicable to many domains.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了模型，能够生成无噪声的图像，这使得自编码器适用于许多领域。
- en: In the next snippet of code, we will not feed the row images of the MNIST test
    set to the model as we need to add noise to these images first to see how the
    trained model will be able to produce noise-free images.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码片段中，我们不会直接将MNIST测试集的原始图像输入到模型中，因为我们需要先给这些图像添加噪声，看看训练好的模型如何生成无噪声的图像。
- en: 'Here I''m adding noise to the test images and passing them through the autoencoder.
    It does a surprisingly great job of removing the noise, even though it''s sometimes
    difficult to tell what the original number is:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我给测试图像加入噪声，并通过自编码器传递它们。即使有时很难分辨原始的数字是什么，模型仍然能够出色地去除噪声：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/14c7f119-bf36-4b37-aa0a-777b02b5703a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14c7f119-bf36-4b37-aa0a-777b02b5703a.png)'
- en: 'Figure 12: Examples of original test images with some Gaussian noise (top row)
    and their construction based on the trained denoising autoencoder'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：原始测试图像中加入一些高斯噪声（顶部行）以及基于训练好的去噪自编码器重建后的图像示例
- en: Applications of autoencoders
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器的应用
- en: In the previous example of constructing images from a lower representation,
    we saw it was very similar to the original input, and also we saw the benefits
    of CANs while denoising the noisy dataset. This kind of example we have implemented
    above is really useful for the image construction applications and dataset denoising.
    So you can generalize the above implementation to any other example of interest
    to you.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前从较低表示构建图像的示例中，我们看到它与原始输入非常相似，而且我们还看到在去噪噪声数据集时，卷积自编码网络（CANs）的优势。我们上面实现的这种示例对于图像构建应用和数据集去噪非常有用。因此，你可以将上述实现推广到你感兴趣的任何其他示例。
- en: Also, throughout this chapter, we have seen how flexible the autoencoder architecture is and
    how we can make different changes to it. We have even tested it to solve harder
    problems of removing noise from input images. This kind of flexibility opens the
    door to many more applications that auoencoders will be a great fit for.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在本章中，我们还展示了自编码器架构的灵活性，以及我们如何对其进行各种更改。我们甚至测试了它在解决更复杂的图像去噪问题中的表现。这种灵活性为自编码器的更多应用开辟了大门。
- en: Image colorization
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像上色
- en: 'Autoencoders—especially the convolutional version—can be used for harder tasks
    such as image colorization. In the following example, we feed the model with an
    input image without any colors, and the reconstructed version of this image will
    be colorized by the autoencoder model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器——尤其是卷积版本——可以用于更难的任务，例如图像上色。在接下来的示例中，我们给模型输入一张没有颜色的图像，经过自编码器模型重建后的图像将会被上色：
- en: '![](img/4d095b47-f875-4eed-89ec-378f9c56067d.jpeg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d095b47-f875-4eed-89ec-378f9c56067d.jpeg)'
- en: 'Figure 13: The CAE is trained to colorize the image'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：CAE模型训练进行图像上色
- en: '![](img/8e56cfc3-2c87-4af8-b844-081f5549e6ff.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e56cfc3-2c87-4af8-b844-081f5549e6ff.jpg)'
- en: 'Figure 14: Colorization paper architecture'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：上色论文架构
- en: Now that our autoencoder is trained, we can use it to colorize pictures we have
    never seen before!
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的自编码器已经训练完成，我们可以用它给我们之前从未见过的图片上色！
- en: This kind of application can be used to color very old images that were taken
    in the early days of the camera.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种应用可以用于给拍摄于早期相机时代的非常老旧图像上色。
- en: More applications
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多应用
- en: Another interesting application can be producing images with higher resolution,
    or neural image enhancement, like the following figures show.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的应用是生成更高分辨率的图像，或者像下图所示的神经图像增强。
- en: 'These figures show more realistic versions of image colorization by Richard
    Zhang:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图展示了理查德·张提供的更真实的图像上色版本：
- en: '![](img/737b6da4-0bd7-437c-8a6b-e79adbe19b63.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/737b6da4-0bd7-437c-8a6b-e79adbe19b63.png)'
- en: 'Figure 15: Colorful image colorization by Richard Zhang, Phillip Isola, and
    Alexei A. Efros'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：理查德·张（Richard Zhang）、菲利普·伊索拉（Phillip Isola）和亚历克塞·A·埃夫罗斯（Alexei A. Efros）提供的彩色图像上色
- en: 'This figure shows another application of autoencoders to make image enhancements:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了自编码器在图像增强中的另一个应用：
- en: '![](img/1eae3e42-4da8-4cea-b961-30bbe5229382.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1eae3e42-4da8-4cea-b961-30bbe5229382.png)'
- en: 'Figure 16: Neural enhancement by Alexjc (https://github.com/alexjc/neural-enhance)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：亚历克西（Alexjc）提供的神经增强（[https://github.com/alexjc/neural-enhance](https://github.com/alexjc/neural-enhance)）
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a totally new architecture that can be used for
    many interesting applications. Autoencoders are very flexible, so feel free to
    come up with your own problem in the area of image enhancement, colorization,
    or construction. Also, there are more variations of autoencoders, called **variational
    autoencoders**. They are also used for very interesting applications, such as
    image generation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种全新的架构，可以用于许多有趣的应用。自编码器非常灵活，所以可以随意提出你自己在图像增强、上色或构建方面的问题。此外，还有自编码器的更多变体，称为**变分自编码器**。它们也被用于非常有趣的应用，比如图像生成。
