- en: Chapter 7. Other Important Deep Learning Libraries
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：其他重要的深度学习库
- en: 'In this chapter, we''ll talk about other deep learning libraries, especially
    libraries with programming languages other than Java. The following are the most
    famous, well-developed libraries:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们将讨论其他深度学习库，特别是使用非Java编程语言的库。以下是一些最著名、最成熟的库：
- en: Theano
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theano
- en: TensorFlow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Caffe
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caffe
- en: You'll briefly learn about each of them. Since we'll mainly implement them using
    Python here, you can skip this chapter if you are not a Python developer. All
    the libraries introduced in this chapter support GPU implementations and have
    other special features, so let's dig into them.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你将简要了解每个库。由于我们将在这里主要使用Python来实现它们，如果你不是Python开发者，可以跳过本章。本章介绍的所有库都支持GPU实现，并具有其他特殊功能，所以让我们深入了解它们。
- en: Theano
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Theano
- en: Theano was developed for deep learning, but it is not actually a deep learning
    library; it is a Python library for scientific computing. The documentation is
    available at [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/).
    There are several characteristics introduced on the page such as the use of a
    GPU, but the most striking feature is that Theano supports **computational differentiation**
    or **automatic differentiation**, which ND4J, the Java scientific computing library,
    doesn't support. This means that, with Theano, we don't have to calculate the
    gradients of model parameters by ourselves. Theano automatically does this instead.
    Since Theano undertakes the most complicated parts of the algorithm, implementations
    of math expressions can be less difficult.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Theano是为深度学习开发的，但它实际上不是一个深度学习库；它是一个用于科学计算的Python库。文档可在[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)找到。页面上介绍了几个特性，如GPU的使用，但最显著的特点是Theano支持**计算微分**或**自动微分**，而Java科学计算库ND4J不支持这一功能。这意味着，使用Theano时，我们无需自己计算模型参数的梯度，Theano会自动完成这项工作。由于Theano处理了算法中最复杂的部分，数学表达式的实现变得不那么困难。
- en: 'Let''s see how Theano computes gradients. To begin with, we need to install
    Theano on the machine. Installation can be done just by using `pip install Theano`
    or `easy_install Theano`. Then, the following are the lines to import and use
    Theano:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Theano如何计算梯度。首先，我们需要在机器上安装Theano。安装可以通过`pip install Theano`或`easy_install
    Theano`命令完成。然后，以下是导入和使用Theano的代码：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With Theano, all variables are processed as tensors. For example, we have `scalar`,
    `vector`, and `matrix`, `d` for double, `l` for long, and so on. Generic functions
    such as `sin`, `cos`, `log`, and `exp` are also defined under `theano.tensor`.
    Therefore, as shown previously, we often use the alias of tensor, `T`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在Theano中，所有变量都作为张量进行处理。例如，我们有`scalar`、`vector`和`matrix`，`d`代表双精度，`l`代表长整型，等等。像`sin`、`cos`、`log`和`exp`这样的通用函数也在`theano.tensor`下定义。因此，如前所示，我们经常使用张量的别名`T`。
- en: 'As a first step to briefly grasp Theano implementations, consider the very
    simple parabola curve. The implementation is saved in `DLWJ/src/resources/theano/1_1_parabola_scalar.py`
    so that you can reference it. First, we define `x` as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简要了解Theano实现的第一步，考虑非常简单的抛物线曲线。实现代码保存在`DLWJ/src/resources/theano/1_1_parabola_scalar.py`中，你可以参考它。首先，我们定义`x`如下：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This definition is unique with Python because `x` doesn''t have a value; it''s
    just a symbol. In this case, `x` is `scalar` of the type `d` (double). Then we
    can define `y` and its gradient very intuitively. The implementation is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义在Python中是独特的，因为`x`没有值，它只是一个符号。在这种情况下，`x`是类型为`d`（双精度）的`scalar`。然后我们可以非常直观地定义`y`及其梯度。实现如下：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'So, `dy` should have `2x` within it. Let''s check whether we can get the correct
    answers. What we need to do additionally is to register the `math` function with
    Theano:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`dy`应该包含`2x`。让我们检查一下是否可以得到正确的答案。我们需要额外做的是将`math`函数注册到Theano中：
- en: '[PRE3]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then you can easily compute the value of the gradients:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以轻松地计算梯度的值：
- en: '[PRE4]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Very simple! This is the power of Theano. We have `x` of scalar here, but you
    can easily implement vector (and matrix) calculations as well just by defining
    `x` as:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单！这就是Theano的强大之处。这里我们有一个`scalar`类型的`x`，但是你也可以通过简单地将`x`定义为以下形式，轻松实现向量（甚至矩阵）计算：
- en: '[PRE5]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We won't go further here, but you can find the completed codes in `DLWJ/src/resources/theano/1_2_parabola_vector.py`
    and `DLWJ/src/resources/theano/1_3_parabola_matrix.py`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里深入探讨，但你可以在`DLWJ/src/resources/theano/1_2_parabola_vector.py`和`DLWJ/src/resources/theano/1_3_parabola_matrix.py`中找到完整的代码。
- en: 'When we consider implementing deep learning algorithms with Theano, we can
    find some very good examples on GitHub in *Deep Learning Tutorials* ([https://github.com/lisa-lab/DeepLearningTutorials](https://github.com/lisa-lab/DeepLearningTutorials)).
    In this chapter, we''ll look at an overview of the standard MLP implementation
    so you understand more about Theano. The forked repository as a snapshot is available
    at [https://github.com/yusugomori/DeepLearningTutorials](https://github.com/yusugomori/DeepLearningTutorials).
    First, let''s take a look at `mlp.py`. The model parameters of the hidden layer
    are the weight and bias:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑使用Theano实现深度学习算法时，可以在GitHub的*Deep Learning Tutorials*（[https://github.com/lisa-lab/DeepLearningTutorials](https://github.com/lisa-lab/DeepLearningTutorials)）找到一些非常好的示例。在本章中，我们将概述标准的MLP实现，让你更好地理解Theano。作为快照，fork后的仓库可以在[https://github.com/yusugomori/DeepLearningTutorials](https://github.com/yusugomori/DeepLearningTutorials)查看。首先，我们来看看`mlp.py`。隐藏层的模型参数是权重和偏置：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Both parameters are defined using `theano.shared` so that they can be accessed
    and updated through the model. The activation can be represented as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 两个参数都使用`theano.shared`定义，以便可以通过模型访问和更新。激活函数可以表示为如下：
- en: '![Theano](img/B04779_07_04.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![Theano](img/B04779_07_04.jpg)'
- en: 'This denotes the activation function, that is, the hyperbolic tangent in this
    code. Therefore, the corresponding code is written as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示激活函数，即代码中的双曲正切函数。因此，相应的代码写成如下：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, linear activation is also supported. Likewise, parameters `W` and `b`
    of the output layer, that is, logistic regression layer, are defined and initialized
    in `logistic_sgd.py`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也支持线性激活。同样，输出层的参数`W`和`b`，即逻辑回归层，在`logistic_sgd.py`中定义并初始化：
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The activation function of multi-class logistic regression is the `softmax`
    function and we can just write and define the output as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别逻辑回归的激活函数是`softmax`函数，我们可以直接写出并定义输出，如下所示：
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can write the predicted values as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将预测值写为：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In terms of training, since the equations of the backpropagation algorithm
    are computed from the loss function and its gradient, what we need to do is just
    define the function to be minimized, that is, the negative log likelihood function:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练方面，由于反向传播算法的方程是从损失函数及其梯度中计算得出的，我们需要做的就是定义要最小化的函数，即负对数似然函数：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, the mean values, not the sum, are computed to evaluate across the mini-batch.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算的是均值，而不是和，用于评估整个小批量（mini-batch）。
- en: 'With these preceding values and definitions, we can implement MLP. Here again,
    what we need to do is define the equations and symbols of MLP. The following is
    an extraction of the code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些前面的值和定义，我们可以实现MLP。再次强调，我们需要做的就是定义MLP的方程和符号。以下是代码的提取：
- en: '[PRE12]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then you can build and train the model. Let''s look at the code in `test_mlp()`.
    Once you load the dataset and construct MLP, you can evaluate the model by defining
    the cost:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以构建并训练模型。让我们来看一下`test_mlp()`中的代码。一旦加载数据集并构建MLP，你可以通过定义代价函数来评估模型：
- en: '[PRE13]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With this cost, we get the gradients of the model parameters with just a single
    line of code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个代价函数，我们只需一行代码就能获得模型参数的梯度：
- en: '[PRE14]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the equation to update the parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更新参数的公式：
- en: '[PRE15]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code in the first bracket follows this equation:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个括号中的代码遵循此公式：
- en: '![Theano](img/B04779_07_05.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![Theano](img/B04779_07_05.jpg)'
- en: 'Then, finally, we define the actual function for the training:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，最后我们定义实际的训练函数：
- en: '[PRE16]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each indexed input and label corresponds to `x`, `y` in *givens*, so when `index`
    is given, the parameters are updated with `updates`. Therefore, we can train the
    model with iterations of training epochs and mini-batches:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个索引的输入和标签对应于*givens*中的`x`，`y`，因此当给定`index`时，参数会通过`updates`进行更新。因此，我们可以通过训练周期和小批量的迭代来训练模型：
- en: '[PRE17]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The original code has the test and validation part, but what we just mentioned
    is the rudimentary structure. With Theano, equations of gradients will no longer
    be derived.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 原始代码中有测试和验证部分，但我们刚才提到的是最基本的结构。使用Theano时，梯度的方程将不再需要推导。
- en: TensorFlow
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow
- en: 'TensorFlow is the library for machine learning and deep learning developed
    by Google. The project page is [https://www.tensorflow.org/](https://www.tensorflow.org/)
    and all the code is open to the public on GitHub at [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    TensorFlow itself is written with C++, but it provides a Python and C++ API. We
    focus on Python implementations in this book. The installation can be done with
    `pip`, `virtualenv`, or `docker`. The installation guide is available at [https://www.tensorflow.org/versions/master/get_started/os_setup.html](https://www.tensorflow.org/versions/master/get_started/os_setup.html).
    After the installation, you can import and use TensorFlow by writing the following
    code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是由Google开发的机器学习和深度学习库。项目页面在[https://www.tensorflow.org/](https://www.tensorflow.org/)，所有代码都公开在GitHub上，地址为[https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)。TensorFlow本身是用C++编写的，但它提供了Python和C++的API。我们在本书中关注的是Python实现。安装可以通过`pip`、`virtualenv`或`docker`完成。安装指南可以在[https://www.tensorflow.org/versions/master/get_started/os_setup.html](https://www.tensorflow.org/versions/master/get_started/os_setup.html)找到。安装完成后，你可以通过编写以下代码导入并使用TensorFlow：
- en: '[PRE18]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'TensorFlow recommends you implement deep learning code with the following three
    parts:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow建议你将深度学习代码实现为以下三个部分：
- en: '`inference()`: This makes predictions using the given data, which defines the
    model structure'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inference()`：它使用给定的数据进行预测，定义了模型的结构'
- en: '`loss()`: This returns the error values to be optimized'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss()`：它返回要优化的误差值'
- en: '`training()`: This applies the actual training algorithms by computing gradients'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training()`：它通过计算梯度来应用实际的训练算法'
- en: We'll follow this guideline. A tutorial on MNIST classifications for beginners
    is introduced on [https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)
    and the code for this tutorial can be found in `DLWJ/src/resources/tensorflow/1_1_mnist_simple.py`.
    Here, we consider refining the code introduced in the tutorial. You can see all
    the code in `DLWJ/src/resources/tensorflow/1_2_mnist.py`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循这个指南。一个适合初学者的MNIST分类教程介绍在[https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)，这个教程的代码可以在`DLWJ/src/resources/tensorflow/1_1_mnist_simple.py`找到。在这里，我们考虑优化教程中介绍的代码。你可以在`DLWJ/src/resources/tensorflow/1_2_mnist.py`查看完整代码。
- en: 'First, what we have to consider is fetching the MNIST data. Thankfully, TensorFlow
    also provides the code to fetch the data in [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py)
    and we put the code into the same directory. Then, by writing the following code,
    you can import the MNIST data:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要考虑的是获取MNIST数据。幸运的是，TensorFlow也提供了获取数据的代码，地址为[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py)，我们将代码放入同一目录。然后，通过编写以下代码，你可以导入MNIST数据：
- en: '[PRE19]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'MNIST data can be imported using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下代码导入MNIST数据：
- en: '[PRE20]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Similar to Theano, we define the variable with no actual values as the placeholder:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Theano，我们定义变量时不赋予实际值，而是作为占位符：
- en: '[PRE21]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, `784` is the number of units in the input layer and `10` is the number
    in the output layer. We do this because the values in the placeholder change in
    accordance with the mini-batches. Once you define the placeholder you can move
    on to the model building and training. We set the non-linear activation with the
    `softmax` function in `inference()` here:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`784`是输入层的单元数，`10`是输出层的单元数。这样做是因为占位符中的值会随着小批量数据的变化而变化。一旦定义了占位符，你就可以继续进行模型的构建和训练。在`inference()`中，我们用`softmax`函数设置了非线性激活：
- en: '[PRE22]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, `W` and `b` are the parameters of the model. The `loss` function, that
    is, the `cross_entropy` function, is defined in `loss()` as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`W`和`b`是模型的参数。`loss`函数，即`cross_entropy`函数，在`loss()`中定义如下：
- en: '[PRE23]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With the definition of `inference()` and `loss()`, we can train the model by
    writing the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有了`inference()`和`loss()`的定义，我们可以通过编写以下代码来训练模型：
- en: '[PRE24]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`GradientDescentOptimizer()` applies the gradient descent algorithm. But be
    careful, as this method just defines the method of training and the actual training
    has not yet been executed. TensorFlow also supports `AdagradOptimizer()`, `MemontumOptimizer()`,
    and other major optimizing algorithms.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradientDescentOptimizer()` 应用梯度下降算法。但要小心，因为这个方法只是定义了训练的方法，实际的训练尚未执行。TensorFlow
    还支持 `AdagradOptimizer()`、`MomentumOptimizer()` 以及其他主要的优化算法。'
- en: 'The code and methods explained previously are to define the model. To execute
    the actual training, you need to initialize a session of TensorFlow:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 之前解释的代码和方法用于定义模型。要执行实际的训练，你需要初始化一个 TensorFlow 会话：
- en: '[PRE25]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we train the model with mini-batches. All the data in a mini-batch is
    stored in `feed_dict` and then used in `sess.run()`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们用小批量数据训练模型。所有小批量数据会存储在 `feed_dict` 中，然后在 `sess.run()` 中使用：
- en: '[PRE26]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'That''s it for the model training. It''s very simple, isn''t it? You can show
    the result by writing the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是模型训练的全部内容。非常简单，对吧？你可以通过编写以下代码来展示结果：
- en: '[PRE27]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`TensorFlow` makes it super easy to implement deep learning and it is very
    useful. Furthermore, `TensorFlow` has another powerful feature, `TensorBoard`,
    to visualize deep learning. By adding a few lines of code to the previous code
    snippet, we can use this useful feature.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlow` 使得实现深度学习变得非常简单，而且非常有用。此外，`TensorFlow` 还有一个强大的功能——`TensorBoard`，可以用来可视化深度学习。只需在之前的代码片段中添加几行代码，我们就可以使用这个有用的功能。'
- en: 'Let''s see how the model is visualized first. The code is in `DLWJ/src/resources/tensorflow/1_3_mnist_TensorBoard.py`,
    so simply run it. After you run the program, type the following command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看模型是如何可视化的。代码位于 `DLWJ/src/resources/tensorflow/1_3_mnist_TensorBoard.py`，所以只需运行它。在你运行程序后，输入以下命令：
- en: '[PRE28]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Here, `<ABSOLUTE_PATH>` is the absolute path of the program. Then, if you access
    `http://localhost:6006/` in your browser, you can see the following page:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`<ABSOLUTE_PATH>` 是程序的绝对路径。然后，如果你在浏览器中访问 `http://localhost:6006/`，你会看到以下页面：
- en: '![TensorFlow](img/B04779_07_01.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow](img/B04779_07_01.jpg)'
- en: 'This shows the process of the value of `cross_entropy`. Also, when you click
    **GRAPH** in the header menu, you see the visualization of the model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了 `cross_entropy` 的值变化过程。此外，当你点击头部菜单中的 **GRAPH** 时，你可以看到模型的可视化：
- en: '![TensorFlow](img/B04779_07_02.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow](img/B04779_07_02.jpg)'
- en: 'When you click on **inference** on the page, you can see the model structure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当你点击页面中的 **inference** 时，你可以看到模型结构：
- en: '![TensorFlow](img/B04779_07_03.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![TensorFlow](img/B04779_07_03.jpg)'
- en: 'Now let''s look inside the code. To enable visualization, you need to wrap
    the whole area with the scope: *with* `tf.Graph().as_default()`. By adding this
    scope, all the variables declared in the scope will be displayed in the graph.
    The displayed name can be set by including the `name` label as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看代码内部。为了启用可视化，你需要用作用域将整个区域包裹起来：*with* `tf.Graph().as_default()`。通过添加这个作用域，作用域中声明的所有变量将显示在图中。显示的名称可以通过以下方式设置
    `name` 标签：
- en: '[PRE29]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Defining other scopes will create nodes in the graph and this is where the
    division, `inference()`, `loss()`, and `training()` reveal their real values.
    You can define the respective scope without losing any readability:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 定义其他作用域会在图中创建节点，这里就是 `inference()`、`loss()` 和 `training()` 展现其实际价值的地方。你可以定义各自的作用域，而不会失去任何可读性：
- en: '[PRE30]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`tf.scalar_summary()` in `loss()` makes the variable show up in the **EVENTS**
    menu. To enable visualization, we need the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss()` 中的 `tf.scalar_summary()` 会使变量显示在 **EVENTS** 菜单中。为了启用可视化，我们需要以下代码：'
- en: '[PRE31]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then the process of variables can be added with the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以通过以下代码添加变量的处理过程：
- en: '[PRE32]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This feature of visualization will be much more useful when we're using more
    complicated models.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用更复杂的模型时，这种可视化功能会变得更加有用。
- en: Caffe
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Caffe
- en: Caffe is a library famous for its speed. The official project page is [http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)
    and the GitHub page is [https://github.com/BVLC/caffe](https://github.com/BVLC/caffe).
    Similar to TensorFlow, Caffe has been developed mainly with C++, but it provides
    a Python and MATLAB API. In addition, what is unique to Caffe is that you don't
    need any programming experience, you just write the configuration or protocol
    files, that is `.prototxt` files, to perform experiments and research with deep
    learning. Here, we focus on the protocol-based approach.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Caffe 是一个以速度著称的库。官方项目页面是[http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/)，GitHub
    页面是[https://github.com/BVLC/caffe](https://github.com/BVLC/caffe)。类似于 TensorFlow，Caffe
    主要使用 C++ 开发，但它提供了 Python 和 MATLAB 的 API。此外，Caffe 的独特之处在于，您不需要任何编程经验，您只需编写配置或协议文件，即
    `.prototxt` 文件，就可以进行深度学习的实验和研究。在这里，我们专注于基于协议的方法。
- en: 'Caffe is a very powerful library that enables quick model building, training,
    and testing; however, it''s a bit difficult to install the library to get a lot
    of benefits from it. As you can see from the installation guide at [http://caffe.berkeleyvision.org/installation.html](http://caffe.berkeleyvision.org/installation.html),
    you need to install the following in advance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Caffe 是一个非常强大的库，能够快速构建、训练和测试模型；然而，安装该库以获得其许多优势有些困难。如您从[http://caffe.berkeleyvision.org/installation.html](http://caffe.berkeleyvision.org/installation.html)的安装指南中看到的，您需要提前安装以下内容：
- en: CUDA
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA
- en: BLAS (ATLAS, MKL, or OpenBLAS)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BLAS（ATLAS、MKL 或 OpenBLAS）
- en: OpenCV
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV
- en: Boost
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boost
- en: 'Others: snappy, leveldb, gflags, glog, szip, lmdb, protobuf, and hdf5'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他：snappy、leveldb、gflags、glog、szip、lmdb、protobuf 和 hdf5
- en: 'Then, clone the repository from the GitHub page and create the `Makefile.config`
    file from `Makefile.config.example`. You may need Anaconda, a Python distribution,
    beforehand to run the `make` command. You can download this from [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    After you run the `make`, `make test`, and `make runtest` commands (you may want
    to run the commands with a `-jN` option such as `make -j4` or `make -j8` to speed
    up the process) and pass the test, you''ll see the power of Caffe. So, let''s
    look at an example. Go to `$CAFFE_ROOT`, the path where you cloned the repository,
    and type the following commands:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从 GitHub 页面克隆仓库，并从 `Makefile.config.example` 创建 `Makefile.config` 文件。您可能需要提前安装
    Anaconda（一个 Python 发行版）来运行 `make` 命令。您可以从[https://www.continuum.io/downloads](https://www.continuum.io/downloads)下载它。运行
    `make`、`make test` 和 `make runtest` 命令后（您可能想使用 `-jN` 选项，如 `make -j4` 或 `make -j8`
    来加速过程），如果测试通过，您将看到 Caffe 的强大功能。那么，让我们来看一个示例。进入 `$CAFFE_ROOT`，即您克隆仓库的路径，并输入以下命令：
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'That''s all you need to solve the standard MNIST classification problem with
    CNN. So, what happened here? When you have a look at `train_lenet.sh`, you will
    see the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是解决标准 MNIST 分类问题所需的全部内容，使用 CNN。那么，发生了什么呢？当您查看 `train_lenet.sh` 时，您会看到以下内容：
- en: '[PRE34]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It simply runs the `caffe` command with the protocol file `lenet_solver.prototxt`.
    This file configures the hyper parameters of the model such as the learning rate
    and the momentum. The file also references the network configuration file, in
    this case, `lenet_train_test.prototxt`. You can define each layer with a JSON-like
    description:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它简单地使用协议文件 `lenet_solver.prototxt` 运行 `caffe` 命令。此文件配置了模型的超参数，例如学习率和动量。该文件还引用了网络配置文件，在这种情况下是
    `lenet_train_test.prototxt`。您可以使用类似 JSON 的描述定义每一层：
- en: '[PRE35]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'So, basically, the protocol file is divided into two parts:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，协议文件分为两个部分：
- en: '**Net**: This defines the detailed structure of the model and gives a description
    of each layer, hence whole neural networks'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Net**：此项定义了模型的详细结构，并描述了每一层，从而构成了整个神经网络'
- en: '**Solver**: This defines the optimization settings such as the use of a CPU/GPU,
    the number of iterations, and the hyper parameters of the model such as the learning
    rate'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Solver**：此项定义了优化设置，如使用 CPU/GPU、迭代次数以及模型的超参数，如学习率'
- en: Caffe can be a great tool when you need to apply deep learning to a large dataset
    with principal approaches.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要将深度学习应用于大数据集并采用主要方法时，Caffe 是一个很好的工具。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to implement deep learning algorithms and models
    using Theano, TensorFlow, and Caffe. All of them have special and powerful features
    and each of them is very useful. If you are interested in other libraries and
    frameworks, you can have *Chainer* ([http://chainer.org/](http://chainer.org/)),
    *Torch* ([http://torch.ch/](http://torch.ch/)), *Pylearn2* ([http://deeplearning.net/software/pylearn2/](http://deeplearning.net/software/pylearn2/)),
    *Nervana* ([http://neon.nervanasys.com/](http://neon.nervanasys.com/)), and so
    on. You can also reference some benchmark tests ([https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)
    and [https://github.com/soumith/convnet-benchmarks/issues/66](https://github.com/soumith/convnet-benchmarks/issues/66))
    when you actually consider building your application with one of the libraries
    mentioned earlier.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用 Theano、TensorFlow 和 Caffe 实现深度学习算法和模型。它们都有各自独特且强大的功能，每一个都非常有用。如果你对其他库和框架感兴趣，可以参考
    *Chainer* （[http://chainer.org/](http://chainer.org/)）、*Torch* （[http://torch.ch/](http://torch.ch/)）、*Pylearn2*
    （[http://deeplearning.net/software/pylearn2/](http://deeplearning.net/software/pylearn2/)）、*Nervana*
    （[http://neon.nervanasys.com/](http://neon.nervanasys.com/)）等。你还可以参考一些基准测试（[https://github.com/soumith/convnet-benchmarks](https://github.com/soumith/convnet-benchmarks)
    和 [https://github.com/soumith/convnet-benchmarks/issues/66](https://github.com/soumith/convnet-benchmarks/issues/66)），当你实际考虑使用前面提到的库之一来构建应用时。
- en: Throughout this book, you learned the fundamental theories and algorithms of
    machine learning and deep learning and how deep learning is applied to study/business
    fields. With the knowledge and techniques you've acquired here, you should be
    able to cope with any problems that confront you. While it is true that you still
    need more steps to realize AI, you now have the greatest opportunity to achieve
    innovation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，你学习了机器学习和深度学习的基本理论和算法，以及深度学习在研究/商业领域中的应用。凭借你在这里获得的知识和技术，你应该能够应对任何面临的问题。虽然实现人工智能仍然需要更多的步骤，但你现在拥有了实现创新的最佳机会。
