- en: Machine Learning Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: '**Artificial Int****elligence **(**AI**) is rooted in mathematics and statistics.
    When creating an **Artificial Neural Network** (**ANN**), we''re conducting mathematical
    operations on data represented in linear space; it is, by nature, applied mathematics
    and statistics. Machine learning algorithms are nothing but function approximations;
    they try and find a mapping between an input and a correct corresponding output.
    We use algebraic methods to create algorithms that learn these mappings.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）根植于数学和统计学。在创建**人工神经网络**（**ANN**）时，我们在表示线性空间的数据上进行数学运算；从本质上讲，它是应用数学和统计学。机器学习算法不过是函数逼近；它们试图找到输入与正确输出之间的映射关系。我们使用代数方法来创建能够学习这些映射关系的算法。'
- en: Almost all machine learning can be expressed in a fairly straight-forward formula;
    bringing together a dataset and model, along with a loss function and optimization
    technique that are applicable to the dataset and model. This section is intended
    as a review of the basic mathematical tools and techniques that are essential
    to understanding *what's under the hood* in AI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有机器学习都可以通过一个相对简单的公式来表示；将数据集和模型结合起来，再加上适用于数据集和模型的损失函数与优化技术。本节旨在回顾理解*AI 底层原理*所必需的基本数学工具和技术。
- en: In this chapter, we'll review linear algebra and probability, and then move
    on to the construction of basic and fundamental machine learning algorithms and
    systems, before touching upon optimization techniques that can be used for all
    of your methods going forward. While we will utilize mathematical notation and
    expressions in this chapter and the following chapters, we will focus on translating
    each of these concepts into Python code. In general, Python is easier to read
    and comprehend than mathematical expressions, and allows readers to get off the
    ground quicker.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾线性代数和概率内容，然后进入基本和基础机器学习算法与系统的构建，最后介绍可以在未来所有方法中使用的优化技术。虽然我们在本章及后续章节中将使用数学符号和表达式，但我们将重点将这些概念转化为
    Python 代码。通常，Python 比数学表达式更易读易懂，能够帮助读者更快入门。
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Applied math basics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用数学基础
- en: Probability theory
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率论
- en: Constructing basic machine learning algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基础机器学习算法
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be working in Python 3 with the scikit-learn scientific
    computing package. You can install the package, you can run `pip install sklearn` in
    your terminal or command line.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 Python 3 和 scikit-learn 科学计算包。你可以通过在终端或命令行中运行 `pip install sklearn`
    来安装该包。
- en: Applied math basics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用数学基础
- en: When we talk about mathematics as related to deep learning and AI, we're often
    talking about linear algebra. Linear algebra is a branch of continuous mathematics
    that involves the study of vector space and operations performed in vector space.
    If you remember back to grade-school algebra, algebra in general deals with unknown
    variables. With linear algebra, we're extending this study into linear systems
    that have an arbitrary number of dimensions, which is what makes this a form of
    continuous mathematics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈到与深度学习和 AI 相关的数学时，我们通常指的是线性代数。线性代数是一个连续数学的分支，涉及到向量空间的研究以及在向量空间中进行的运算。如果你还记得小学时学过的代数，代数通常处理未知变量。线性代数则是将这种研究扩展到具有任意维度的线性系统，这使得它成为一种连续数学的形式。
- en: AI relies on the basic building block of the tensor. Within AI, these mathematical
    objects store information throughout ANNs that allow them to operate; they are
    data structures that are utilized throughout AI. As we will see, a tensor has
    a **rank**,which essentially tells us about the **indices** of the data (how many
    rows and columns the data has).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: AI 依赖于张量这一基本构建块。在 AI 中，这些数学对象存储着在人工神经网络（ANN）中运作所需的信息；它们是 AI 中广泛使用的数据结构。正如我们将看到的，张量有一个**秩**，它本质上告诉我们数据的**索引**（即数据的行列数）。
- en: While many problems in deep learning are not formally linear problems*, *the
    basic building blocks of matrices and tensors are the primary data structures
    for solving, optimizing, and approximating within an ANN.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习中的许多问题并不是严格的线性问题，*但矩阵和张量的基本构建块仍然是 ANN 中解决、优化和逼近问题的主要数据结构*。
- en: 'Want to see how linear algebra can help us from a programmatic standpoint?
    Take a look at the following code block:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想看看线性代数如何从编程角度帮助我们吗？看看下面的代码块：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We can eliminate strenuous loops by simply utilizing NumPy's built-in linear
    algebra functions. When you think of AI, and the thousands upon thousands of operations
    that have to be computed at the runtime of an application, the building blocks
    of linear algebra can also help us out programmatically. In the following sections,
    we'll be reviewing these fundamental concepts in both mathematical notation and
    Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单利用 NumPy 内建的线性代数函数来消除繁琐的循环。当你想到 AI 时，成千上万的操作必须在应用程序的运行时进行计算，线性代数的构建块也能帮助我们在编程上更高效。在接下来的部分中，我们将回顾这些基本概念，包括数学符号和
    Python 代码。
- en: Each of the following examples will use the Python package NumPy; `import numpy
    as np`
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将使用 Python 包 NumPy；`import numpy as np`
- en: The building blocks – scalars, vectors, matrices, and tensors
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建块 —— 标量、向量、矩阵和张量
- en: In the following section, we'll introduce the fundamental types of linear algebra
    objects that are used throughout AI applications; **scalars**, **vectors**, **matrices**,
    and **tensors**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将介绍在 AI 应用中广泛使用的线性代数基本对象类型；**标量**、**向量**、**矩阵**和**张量**。
- en: Scalars
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标量
- en: '**Scalars** are nothing but singular, **real numbers** that can take the form
    of an integer or floating point. In Python, we create a scalar by simply assigning
    it:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**标量**只是单一的**实数**，可以是整数或浮动小数。在 Python 中，我们通过简单地赋值来创建标量：'
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Vectors
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量
- en: '**Vectors** are one-dimensional arrays of integers. Geometrically, they store
    the direction and magnitude of change from a point. We''ll see how this works
    in machine learning algorithms when we discuss **principal component analysis**
    (**PCA**) in the next few pages. Vectors in Python are created as `numpy array`
    objects:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**向量**是单维的整数数组。从几何学角度来看，它们存储的是从一个点出发的变化方向和大小。当我们在接下来的几页中讨论**主成分分析**（**PCA**）时，您将看到向量如何在机器学习算法中起作用。在
    Python 中，向量作为 `numpy array` 对象创建：'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Vectors can be written in several ways:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 向量可以通过几种方式来书写：
- en: '![](img/7f77e9e8-5267-45e7-9950-b6cb37c453ff.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f77e9e8-5267-45e7-9950-b6cb37c453ff.png)'
- en: Matrices
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵
- en: '**Matrices** are two-dimensional lists of numbers that contain rows and columns. Typically,
    rows in a matrix are denoted by *i*, while columns are denoted by *j*.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵**是包含行和列的二维数字列表。通常，矩阵中的行用 *i* 表示，列用 *j* 表示。'
- en: 'Matrices are represented as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵表示为：
- en: '![](img/73f016a2-d87e-481b-9229-394b7661d3b6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73f016a2-d87e-481b-9229-394b7661d3b6.png)'
- en: 'We can easily create matrices in Python as NumPy arrays, much like we can with
    vectors:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像创建向量一样，轻松在 Python 中创建矩阵，作为 NumPy 数组：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The only different is that we are adding an additional vector to the array to
    create the matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是，我们向数组中添加了一个额外的向量来创建矩阵。
- en: Tensors
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: While you may have heard of vectors and matrices before, the name **t****ensor**may
    be new. A tensor is a generalized matrix, and they have different sizes, or ranks*,* which
    measure their dimensions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能以前听说过向量和矩阵，但**张量**这个名称可能对你来说是新的。张量是广义的矩阵，它们有不同的大小或*秩*，用来衡量它们的维度。
- en: 'Tensors are three (or more)-dimensional lists; you can think of them as a sort
    of multi-dimensional object of numbers, such as a cube. Tensors have a unique
    transitive property and form; if a tensor transforms another entity, it too must
    transform. Any rank 2 tensor can be represented as a matrix, but not all matrices
    are automatically rank 2 tensors. A tensor must have this transitive property.
    As we''ll see, this will come into play with neural networks in the next chapter.
    We can create tensors in Python such as the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是三维（或更多）列表；你可以将它们视为一种多维数字对象，例如立方体。张量具有独特的传递性和形式；如果一个张量变换了另一个实体，它自己也必须发生变化。任何秩为
    2 的张量都可以表示为矩阵，但并非所有矩阵都是自动的秩为 2 的张量。一个张量必须具备这种传递性。正如我们接下来会看到的，这在下一章的神经网络中将会发挥作用。我们可以在
    Python 中创建如下所示的张量：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Matrix math
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵运算
- en: The basic operations of an ANN are based on matrix math. In this section, we'll
    be reviewing the basic operations that you need to know to understand the mechanics
    of ANNs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）的基本运算基于矩阵运算。在这一部分，我们将回顾理解 ANN 机制所需掌握的基本运算。
- en: Scalar operations
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标量运算
- en: 'Scalar operations involve a vector (or matrix) and a scalar. To perform an
    operation with a scalar on a matrix, simply apply to the scalar to every item
    in the matrix:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 标量操作涉及向量（或矩阵）和标量。要对矩阵执行标量操作，只需将标量应用于矩阵中的每个元素：
- en: '![](img/034243e4-25c0-4081-b381-7a6c87ad45f2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/034243e4-25c0-4081-b381-7a6c87ad45f2.png)'
- en: 'In Python, we would simply do the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们只需要做以下操作：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Element–wise operations
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐元素操作
- en: In element-wise operations, position matters. Values that correspond positionally are
    combined to create a new value.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐元素操作中，位置非常重要。对应位置的值会被结合起来形成一个新的值。
- en: 'To add to and/or subtract matrices or vectors:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 要加法和/或减法矩阵或向量：
- en: '![](img/e07114f4-e679-4e3e-a997-747773c84017.png)![](img/5f919d62-e0c8-4c0a-9ffc-c19a2b915b6a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e07114f4-e679-4e3e-a997-747773c84017.png)![](img/5f919d62-e0c8-4c0a-9ffc-c19a2b915b6a.png)'
- en: 'And in Python:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are two forms of multiplication that we may perform with vectors: the** Dot product**, and
    the **Hadamard product**.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在向量上执行两种形式的乘法操作：**点积** 和 **哈达玛积**。
- en: 'The dot product is a special case of multiplication, and is rooted in larger
    theories of geometry that are used across the physical and computational sciences.
    It is a special case of a more general mathematical principle known as an **inner
    product**. When utilizing the dot product of two vectors, the output is a scalar:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 点积是乘法的一个特例，根植于更大的几何学理论中，这些理论被广泛应用于物理学和计算科学中。它是一个更一般的数学原理的特例，称为**内积**。在使用两个向量的点积时，输出是一个标量：
- en: '![](img/6d2dd8e6-671e-4644-93b3-1e8c056760ee.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d2dd8e6-671e-4644-93b3-1e8c056760ee.png)'
- en: 'Dot products are a workhorse in machine learning. Think about a basic operation:
    let''s say we''re doing a simple classification problem where we want to know
    if an image contains a cat or a dog. If we did this with a neural network, it
    would look as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 点积是机器学习中的重要工具。想象一个基本操作：假设我们在做一个简单的分类问题，想要判断一张图像是包含猫还是狗。如果我们用神经网络来做这个分类，它的表现将如下：
- en: '![](img/3cc5fe55-f9ae-4e86-becb-f86437c3d3b3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3cc5fe55-f9ae-4e86-becb-f86437c3d3b3.png)'
- en: Here, *y* is our classification cat or dog. We determine *y* by utilizing a
    network represented by *f*, where the input is *x*, while *w* and *b* represent
    a weight and bias factor (don't worry, we'll explain this in more detail in the
    coming chapter!). Our *x* and *w* are both matrices, and we need to output a scalar
    ![](img/77024446-eb78-43c0-8166-b29644cd0086.png), which represents either cat
    or dog. We can only do this by taking the dot product of *w* and ![](img/ea06997c-67a5-4c3f-96e1-ec7f15945c3a.png).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y* 是我们的分类结果，表示猫或狗。我们通过利用表示为 *f* 的网络来确定 *y*，其中输入是 *x*，而 *w* 和 *b* 分别表示权重和偏置因素（不用担心，我们将在接下来的章节中更详细地解释这些内容！）。我们的
    *x* 和 *w* 都是矩阵，我们需要输出一个标量 ![](img/77024446-eb78-43c0-8166-b29644cd0086.png)，它代表猫或狗。我们只能通过对
    *w* 和 ![](img/ea06997c-67a5-4c3f-96e1-ec7f15945c3a.png) 进行点积来实现这一点。
- en: 'Relating back to our example, if this function were presented with an unknown
    image, taking the dot product will tell us how similar in direction the new vector
    is to the cat vector (*a*) or dog vector (*b*) by the measure of the angle (![](img/fe9988d5-fb5b-406c-bac6-3f9b490e57af.png))
    between them:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，如果这个函数输入的是一张未知的图像，计算点积可以告诉我们新向量与猫向量 (*a*) 或狗向量 (*b*) 的方向相似度，通过它们之间的角度来衡量（![](img/fe9988d5-fb5b-406c-bac6-3f9b490e57af.png)）：
- en: '![](img/e2ac2c9f-cbe9-4b2e-a3f1-58daae581e1e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2ac2c9f-cbe9-4b2e-a3f1-58daae581e1e.png)'
- en: If the vector is closer to the direction of the cat vector (a), we'll classify
    the image as containing a cat. If it's closer to the dog vector (b), we'll classify
    it as containing a dog. In deep learning, a more complex version of this scenario
    is performed over and over; it's the core of how ANNs work.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果向量更接近猫向量 (a) 的方向，我们会将图像分类为包含猫。如果它更接近狗向量 (b) 的方向，我们会将图像分类为包含狗。在深度学习中，这种情境的更复杂版本会反复执行；这是人工神经网络（ANNs）工作的核心。
- en: 'In Python, we can take the dot product of two vectors by using a built-in function
    from `numpy`, `np.dot()`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以通过使用 `numpy` 中的内建函数 `np.dot()` 来计算两个向量的点积：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The Hadamard product, on the other hand, outputs a vector:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，哈达玛积输出的是一个向量：
- en: '![](img/ae8c61da-3bff-49b9-bd6d-0853315777c5.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae8c61da-3bff-49b9-bd6d-0853315777c5.png)'
- en: 'The Hadamard product is element-wise, meaning that the individual numbers in
    the new matrix are the scalar multiples of the numbers from the previous matrices.
    Looking back to Python, we can easily perform this operation in Python with a
    simple `*` operator:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Hadamard积是逐元素运算，这意味着新矩阵中的每个数字都是来自前一个矩阵中数字的标量倍数。回到Python，我们可以使用简单的`*`操作符轻松执行此操作：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we've scratched the surface of basic matrix operations, let's take
    a look at how probability theory can aid us in the artificial intelligence field.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了基本的矩阵运算，接下来让我们看看概率论如何在人工智能领域帮助我们。
- en: Basic statistics and probability theory
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本统计学和概率理论
- en: '**Probability**, the mathematical method for modeling uncertain scenarios,
    underpins the algorithms that make AI intelligent, helping to tell us how our
    systems should reason. So, what is probability? We''ll define it as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率**，即用于建模不确定场景的数学方法，是支撑人工智能算法的基础，帮助我们理解系统应该如何推理。那么，什么是概率呢？我们将其定义如下：'
- en: '*Probability is a frequency expressed as a fraction of the sample size, n*
    [1].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率是一个频率，表示为样本大小n的分数* [1]。'
- en: Simply said, probability is the mathematical study of uncertainty. In this section,
    we'll cover the basics of probability space and probability distributions, as
    well as helpful tools for solving simple problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，概率是对不确定性的数学研究。在这一部分，我们将介绍概率空间和概率分布的基础知识，以及解决简单问题的有用工具。
- en: The probability space and general theory
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率空间与一般理论
- en: 'When probability is discussed, it''s often referred to in terms of the probability
    of a certain **event** happening. Is it going to rain? Will the price of apples
    go up or down? In the context of machine learning, probabilities tell us the likelihood
    of events such as a comment being classified as positive vs. negative, or whether
    a fraudulent transaction will happen on a credit card. We measure probability
    by defining what we refer to as the **probability space**. A probability space
    is a measure of *how* and *why* of the probabilities of certain events. Probability
    spaces are defined by three characteristics:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论概率时，通常会提到某个**事件**发生的概率。例如，今天会下雨吗？苹果的价格会上涨还是下跌？在机器学习的背景下，概率告诉我们诸如评论被分类为正面或负面，或者信用卡是否会发生欺诈交易等事件的可能性。我们通过定义所谓的**概率空间**来衡量概率。概率空间是对某些事件概率的*如何*与*为什么*的度量。概率空间由三个特征定义：
- en: The sample space, which tells us the possible outcomes or a situation
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本空间，告诉我们可能的结果或情况
- en: A defined set of events; such as two fraudulent credit card transactions
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一组定义明确的事件；例如两个欺诈性的信用卡交易
- en: The measure of probability of each of these events
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个事件的概率度量
- en: While probability spaces are a subject worthy of studying in their own right,
    for our own understanding, we'll stick to this basic definition.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然概率空间是一个值得单独研究的主题，但为了更好地理解，我们将坚持这个基本定义。
- en: In probability theory, the idea of **independence** is essential. Independence is
    a state where a random variable does not change based on the value of another
    random variable. This is an important assumption in deep learning, as non–independent
    features can often intertwine and affect the predictive power of our models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，**独立性**的概念至关重要。独立性是指一个随机变量的值不受另一个随机变量值的影响。这在深度学习中是一个重要假设，因为非独立的特征往往会相互交织，并影响我们模型的预测能力。
- en: In statistical terms, a collection of data about an event is a **sample, **which
    is drawn from a theoretical superset of data called a **population** that represents
    everything that is known about a grouping or event. For instance, if we were poll
    people on the street about whether they believe in Political View A or Political
    View B, we would be generating a **random sample** from the population, which
    would be entire population of the city, state, or country where we are polling.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，关于事件的数据集合称为**样本**，它是从一个称为**总体**的理论数据超集中抽取的，整体代表了关于某个分组或事件的所有已知信息。例如，如果我们在街头对人们进行政治观点A或政治观点B的调查，我们将从总体中生成一个**随机样本**，总体是我们进行调查的城市、州或国家的所有人。
- en: Now let's say we wanted to use this sample to predict the likelihood of a person
    having one of the two political views, but we mostly polled people who were at
    an event supporting Political View A. In this case, we may have a **biased sample**.
    When sampling, it is important to take a random sample to decrease bias, otherwise
    any statistical analysis or modeling that we do with sample will be biased as
    well.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用这个样本来预测一个人持有两种政治观点之一的可能性，但我们主要对支持政治观点A的事件参与者进行了调查。在这种情况下，我们可能会有一个**偏倚样本**。在抽样时，重要的是要采取随机样本，以减少偏倚，否则我们使用样本进行的任何统计分析或建模也将存在偏倚。
- en: Probability distributions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率分布
- en: 'You''ve probably seen a chart such as the following one; it''s showing us the
    values that appear in a dataset, and how many times those values appear. This
    is called a **distribution** of a variable. In this particular case, we''re displaying
    the distribution with the help of a **histogram**, which shows the **frequency**
    of the variables:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能见过类似下面的图表；它显示了数据集中的值以及这些值出现的次数。这被称为**变量的分布**。在这个特定的例子中，我们通过使用**直方图**来显示分布，直方图显示了变量的**频率**：
- en: '![](img/b4d6dc2a-a29b-4cbf-ad33-b1c538ea3618.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4d6dc2a-a29b-4cbf-ad33-b1c538ea3618.png)'
- en: In this section, we're interested in a particular type of distribution, called
    a **probability** **distribution**. When we talk about probability distributions,
    we're talking about the likelihood of a random variable taking on a certain value,
    and we create one by dividing the frequencies in the preceding ...
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们关注一种特定类型的分布，称为**概率** **分布**。当我们讨论概率分布时，我们在谈论随机变量取某个特定值的可能性，我们通过将前面频率的值进行划分来创建一个分布……
- en: Probability mass functions
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率质量函数
- en: '**Probability mass functions** (**PMFs**)are discrete distributions. The random
    variables of the distribution can take on a finite number of values:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率质量函数** (**PMFs**)是离散分布。该分布的随机变量可以取有限个值：'
- en: '![](img/f44d2684-9c3f-40cb-b887-ef093f87eaba.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f44d2684-9c3f-40cb-b887-ef093f87eaba.png)'
- en: PMFs look a bit different from our typical view of a distribution, and that
    is because of their finite nature.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: PMF与我们通常看到的分布有些不同，这是因为它们的有限性。
- en: Probability density functions
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率密度函数
- en: '**Probability density functions** (**PDFs**) are continuous distributions;
    values from the distribution can take on infinitely many values. For example,
    take the image as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率密度函数** (**PDFs**)是连续分布；来自该分布的值可以取无限多个值。例如，参见以下图像：'
- en: '![](img/5c89482b-887c-40d8-b880-a7f612bca80a.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c89482b-887c-40d8-b880-a7f612bca80a.png)'
- en: You've probably seen something like this before; it's a probability density
    function of a **standard normal**, or **Gaussian distribution**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能见过类似的东西；它是**标准正态分布**或**高斯分布**的概率密度函数。
- en: Conditional and joint probability
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率与联合概率
- en: '**Conditional probability** is the probability that *x* happens, given that
    *y* happens. It''s one of the key tools for reasoning about uncertainty in probability
    theory. Let''s say we are talking about your winning the lottery, given that it''s
    a sunny day. Maybe you''re feeling lucky! How would we write that in a probability
    statement? It would be the probability of your lottery win, *A*, given the probability
    of it being sunny, *B*, so *P(A|B)*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件概率**是指在已知*y*发生的情况下，*x*发生的概率。它是概率论中推理不确定性的关键工具之一。假设我们在谈论你中奖的概率，假设今天是晴天。也许你觉得今天很幸运！我们如何将这个写成概率表达式呢？它将是你中奖的概率*A*，给定今天是晴天的概率*B*，因此是*P(A|B)*。'
- en: '**Joint probability** is the probability of two things happening simultaneously:
    what is the probability of you winning the lottery *and* it being a sunny day?'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**联合概率**是指两件事同时发生的概率：你中奖*并且*那天是晴天的概率是多少？'
- en: Chain rule for joint probability
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联合概率链式法则
- en: 'Joint probability is important in the AI space; it''s what underlies the mechanics
    of **generative models**, which are able to replicate voice, pictures, and other
    unstructured information. These models learn the joint probability distribution
    of a phenomenon. They generate all possible values for a given object or event.
    A chain rule is a technique by which to evaluate the join probability of two variables.
    Formally, it is written as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 联合概率在人工智能领域中非常重要；它是**生成模型**背后的机制，这些模型能够复制声音、图片和其他非结构化信息。这些模型学习现象的联合概率分布。它们生成给定对象或事件的所有可能值。链式法则是一种评估两个变量联合概率的技术。正式写作如下：
- en: '![](img/a223b2fa-6759-4f86-b295-482060293233.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a223b2fa-6759-4f86-b295-482060293233.png)'
- en: Bayes' rule for conditional probability
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率的贝叶斯定理
- en: '**Bayes**'' **rule** is another essential tenet of probability theory in the
    machine learning sphere. It allows us to calculate the conditional probability
    of an event happening by inverting the conditions of the events. Bayes'' rule
    is formally written as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯**定理是机器学习领域中概率论的另一个基本原则。它允许我们通过反转事件的条件来计算某个事件发生的条件概率。贝叶斯定理的正式表达式为：'
- en: '![](img/dcfd8314-ce15-4407-a1b0-468f855c74be.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcfd8314-ce15-4407-a1b0-468f855c74be.png)'
- en: 'Let''s use Bayes'' rule to look at a simple conditional probability problem.
    In the following table, we see the likelihood of a patient contacting a disease:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用贝叶斯定理来解决一个简单的条件概率问题。在下表中，我们看到一个患者接触到疾病的可能性：
- en: '|  | **Disease (1%)** | **No Disease (99%)** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | **疾病 (1%)** | **无疾病 (99%)** |'
- en: '| Test Positive | 80% | 9.6% |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 测试阳性 | 80% | 9.6% |'
- en: '| Test Negative | 20% | 90.4% |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 测试阴性 | 20% | 90.4% |'
- en: How do we interpret this table? The *x *axis tells us the percentage of the
    population who have the disease; if you have it, you are firmly in the Disease
    column. Based on that condition, the *y *axis is the likelihood of you testing
    positive or negative, based on whether you actually have the disease or not.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读这个表格？*x*轴告诉我们患病人群的百分比；如果你得了病，你就明确地位于“疾病”列。基于这个条件，*y*轴则表示你测试为阳性或阴性的可能性，这取决于你是否真正患病。
- en: 'Now, let''s say that we have a positive test result; what is the chance that
    we actually have the disease? We can use Bayes'' formula to figure solve:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们得到了一个阳性测试结果；那么我们实际患病的概率是多少呢？我们可以使用贝叶斯公式来解答：
- en: '![](img/b4469294-7f8e-4add-8f71-f3c50db0bfc2.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4469294-7f8e-4add-8f71-f3c50db0bfc2.png)'
- en: 'Our answer comes out to 7.8%, the actual probability of having the disease
    given a positive test:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的答案是7.8%，即在阳性测试结果的情况下，实际患病的概率为：
- en: 'In the following code, we can see how Bayes'' formula can model these conditional
    events based on likelihood. In machine learning and AI in general, this comes
    in handy when modeling situations or perhaps classifying objects. Conditional
    probability problems also play into discriminative models, which we will discuss
    in our section on **Generative adversarial networks**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们可以看到如何基于可能性来运用贝叶斯公式建模这些条件事件。在机器学习和人工智能中，这在建模情境或分类物体时非常有用。条件概率问题也涉及到判别模型，我们将在**生成对抗网络**一节中讨论这些模型：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Remember: when conducting multiplication, the type of operation matters. We
    can use the Hadamard product to multiply two equally-sized vectors or matrices
    where the output will be another equally-sized vector or matrix. We use the dot
    product in situations where we need a single number as an output. The dot product
    is essential in machine learning and deep learning; with neural networks, inputs
    are passed to each layer as a matrix or vector, and these are then multiplied
    with another matrix of weights, which forms the core of basic network operations.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 记住：进行乘法运算时，操作类型是非常重要的。我们可以使用哈达玛积（Hadamard积）来乘以两个大小相等的向量或矩阵，输出将是另一个大小相等的向量或矩阵。在需要单个数字输出的情况下，我们使用点积（dot
    product）。点积在机器学习和深度学习中至关重要；在神经网络中，输入作为矩阵或向量传递到每一层，然后与另一个权重矩阵相乘，这构成了基础网络操作的核心。
- en: Probability distributions and the computations based on them rely on Bayesian
    thinking in the machine learning realm. As we'll see in later chapters, some of
    the most innovative networks in AI directly rely on these distributions and the
    core concepts of Baye's theorem. Recall that there are two primary forms of probability
    distribution: PMFsfor discrete variables, and probability density functions PDFs
    for continuous variables; CDF, which apply to any random variables, also exist.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布及其基于这些分布的计算在机器学习领域依赖于贝叶斯思维。正如我们在后续章节中将看到的，人工智能中一些最具创新性的网络直接依赖于这些分布和贝叶斯定理的核心概念。回想一下，概率分布主要有两种形式：离散变量的概率质量函数（PMFs），以及连续变量的概率密度函数（PDFs）；累计分布函数（CDF）也适用于任何随机变量。
- en: Baye's rule, in fact, has inspired an entire branch of statistics known as **Bayesian
    statistics**.Thus far, we have discussed frequent statistics**,** which measure
    probability based on an observed space of repeatable events**. **Bayesian probability,
    on the other hand, measures degrees of belief; how likely is an event to happen
    based on the information that is currently available? This will become important
    as we delve into ANNs in the following chapters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理实际上启发了一个独立的统计学分支，称为**贝叶斯统计**。到目前为止，我们讨论的是**频率统计**，它通过可重复事件的观测空间来衡量概率。另一方面，**贝叶斯概率**衡量信念的程度；即根据当前可用的信息，一个事件发生的可能性有多大？随着我们在接下来的章节中深入探讨人工神经网络（ANNs），这将变得非常重要。
- en: Constructing basic machine learning algorithms
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基本的机器学习算法
- en: As mentioned in the last chapter, machine learning is a term that was developed
    as a reaction to the first AI winter. Today, we generally consider machine learning
    to be the overarching subject area for deep learning and ANNs in general.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一章所提到的，机器学习这个术语是在第一次AI寒冬之后提出的。如今，我们通常认为机器学习是深度学习和人工神经网络（ANNs）等领域的总称。
- en: Most machine learning solutions can be broken down into either a **classification**
    problem or a **regression** problem. A classification problem is when the output
    variables are categorical, such as fraud or not fraud. A regression problem is
    when the output is continuous, such as dollars or site visits. Problems with numerical
    output can be categorical, but are typically transformed to have a categorical
    output such as first class and second class.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习解决方案可以分为**分类**问题或**回归**问题。分类问题是指输出变量是类别性的，例如欺诈与非欺诈。回归问题是指输出是连续的，例如金额或网站访问量。具有数值输出的问题可以是类别性的，但通常会转化为类别输出，如头等舱和二等舱。
- en: Within machine ...
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中...
- en: Supervised learning algorithms
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习算法
- en: 'Supervised algorithms rely on human knowledge to complete their tasks. Let''s
    say we have a dataset related to loan repayment that contains several demographic
    indicators, as well as whether a loan was paid back or not:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法依赖于人工知识来完成任务。假设我们有一个与贷款偿还相关的数据集，包含多个人口统计指标，以及贷款是否还款的信息：
- en: '| **Income** | **Age** | **Marital Status** | **Location** | **Savings** |
    **Paid** |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| **收入** | **年龄** | **婚姻状况** | **地点** | **存款** | **是否已还款** |'
- en: '| $90,000 | 28 | Married | Austin, Texas | $50,000 | y |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| $90,000 | 28 | 已婚 | 德克萨斯州奥斯汀 | $50,000 | y |'
- en: 'The Paid column, which tells us if a loan was paid back or not, is called the
    **target** - it''s what we would like to predict. The data that contains information
    about the applicants background is known as the **features** of the datasets.
    In supervised learning, algorithms learn to predict the target based on the features,
    or in other words, what indicators give a high probability that an applicant will
    pay back a loan or not? Mathematically, this process looks as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为**目标**的“已还款”列告诉我们贷款是否已经还清——它是我们想要预测的内容。包含申请人背景信息的数据被称为数据集的**特征**。在监督学习中，算法会根据特征来学习如何预测目标，换句话说，哪些指标能大概率地预测申请人是否会还款？在数学上，这一过程表现如下：
- en: '![](img/99f325bf-11d3-4441-b8ae-b87042a3fc7a.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99f325bf-11d3-4441-b8ae-b87042a3fc7a.png)'
- en: Here, we are saying that our label ![](img/c855e39b-a2d0-41eb-b053-60907ec03d63.png)
    *is a function of* the input features ![](img/390b3c7c-cf71-47df-8996-42f6d8b2bc99.png),
    plus some amount of error ![](img/9b202d7a-42f7-4dfe-933e-2a62c9b4deb6.png) that
    it caused naturally by the dataset. We know that a certain set of features will
    likely produce a certain outcome. In supervised learning, we set up an algorithm
    to *learn* what function will produce the correct mapping of a set of features
    to an outcome.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以说我们的标签 ![](img/c855e39b-a2d0-41eb-b053-60907ec03d63.png) *是输入特征* ![](img/390b3c7c-cf71-47df-8996-42f6d8b2bc99.png)
    的一个函数，再加上一些由于数据集自然产生的误差 ![](img/9b202d7a-42f7-4dfe-933e-2a62c9b4deb6.png)。我们知道某一组特征很可能会产生某种结果。在监督学习中，我们设置一个算法来*学习*什么样的函数能够正确地将特征与结果进行映射。
- en: 'To illustrate how supervised learning works, we are going to utilize a famous
    example toy dataset in the machine learning field, the Iris Dataset. It shows
    four features: Sepal Length, Sepal Width, Petal Length, and Petal Width. In this
    dataset, our target variable (sometimes called a **label**) is *Name. *The dataset
    is available in the GitHub repository that corresponds with this chapter:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明有监督学习是如何工作的，我们将使用机器学习领域中的一个著名示例玩具数据集——鸢尾花数据集。它显示了四个特征：花萼长度、花萼宽度、花瓣长度和花瓣宽度。在这个数据集中，我们的目标变量（有时称为**标签**）是*名称*。该数据集可以在与本章对应的GitHub代码库中找到：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code generates the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/313467ac-81ee-4e6d-8953-97e7abc893ba.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/313467ac-81ee-4e6d-8953-97e7abc893ba.png)'
- en: Now that we have our data ready to go, let's jump into some supervised learning!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据，让我们开始一些有监督学习吧！
- en: Random forests
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: '**Random forests** are one of the most commonly utilized supervised learning
    algorithms. While they can be used for both classification and regression tasks,
    we''re going to focus on the former. Random forests are an example of an **ensemble
    method**, which works by aggregating the outputs of multiple models in order to
    construct a stronger performing model. Sometimes, you''ll hear this being referred
    to as a grouping of **weak learners** to create a **strong learner**.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**是最常用的有监督学习算法之一。虽然它们既可以用于分类任务，也可以用于回归任务，但我们将专注于前者。随机森林是**集成方法**的一个例子，它通过聚合多个模型的输出，从而构建一个性能更强的模型。有时，你会听到它被称为将**弱学习器**组合成**强学习器**。'
- en: Setting up a random forest classifier in Python is quite simple with the help
    of scikit-learn. First, we import the modules and set up our data. We do not have
    to perform any data cleaning here, as the Iris dataset comes pre-cleaned.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中设置一个随机森林分类器非常简单，只需要借助scikit-learn库。首先，我们导入模块并设置数据。我们在这里不需要进行数据清洗，因为鸢尾花数据集已经经过预处理。
- en: Before training machine learning algorithms, ...
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习算法之前，...
- en: Unsupervised learning algorithms
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习算法
- en: Unsupervised learning algorithms learn the properties of data on their own without
    explicit human intervention or labeling. Typically within the AI field, unsupervised
    learning technique learn the probability distribution that generated a dataset. These
    algorithms, such as **autoencoders** (we will visit these later in the book),
    are useful for a variety of tasks where we simply don't know important information
    about our data that would allow us to use traditional supervised techniques.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法在没有明确人工干预或标注的情况下，自主学习数据的特性。在人工智能领域，无监督学习技术通常学习生成数据集的概率分布。这些算法，如**自编码器**（我们将在本书后面讲解），对于许多任务非常有用，尤其是当我们对数据缺乏足够的信息，无法使用传统的有监督学习方法时。
- en: PCA is an unsupervised method for feature extraction. It combines input variables
    in such a way that we can drop those variables that provide the least amount of
    information to us. Afterwards, we are left with new variables that are independent
    of each other, making them easy to utilize in a basic linear model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种无监督的特征提取方法。它通过组合输入变量，使我们能够剔除那些对我们提供信息量最少的变量。之后，我们会得到一组彼此独立的新变量，这使得它们在基础线性模型中变得易于使用。
- en: AI applications are fundamentally hampered by the **curse of dimensionality**.
    This phenomenon, which occurs when the number of **dimensions** in your data is
    high, makes it incredibly difficult for learning algorithms to perform well. PCA
    can help alleviate this problem for us. PCA is one of the primary examples of
    what we call **dimensionality reduction**, which helps us take high-feature spaces
    (lots of data attributes) and transform them into lower-feature spaces (only the
    important features).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能应用的根本问题之一是**维度灾难**。这种现象发生在数据中的**维度**数量很高时，使得学习算法难以有效执行。主成分分析（PCA）可以帮助缓解这个问题。PCA是我们所说的**降维**的主要示例之一，它帮助我们将高维特征空间（大量数据属性）转化为低维特征空间（仅保留重要特征）。
- en: 'Dimensionality reduction can be conducted in two primary ways: **feature elimination**
    and **feature extraction**. Whereas feature elimination may involve the arbitrary
    cutting of features from the dataset, feature extraction (PCA is a form of) this
    gives us a more intuitive way to reduce our dimensionality. So, how does it work?
    In a nutshell:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 降维可以通过两种主要方式进行：**特征消除**和**特征提取**。而特征消除可能涉及从数据集中任意去除特征，特征提取（PCA就是一种形式）则为我们提供了更直观的降维方式。那么，它是如何工作的呢？简而言之：
- en: We create a matrix (correlation or covariance) that describes how all of our
    data relates to each other
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建一个描述所有数据之间关系的矩阵（相关矩阵或协方差矩阵）
- en: We decompose this matrix into separate components, called the **eigenvalues**
    and the **eigenvectors**, which describe the direction and magnitude of our data
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将这个矩阵分解为独立的成分，称为**特征值**和**特征向量**，它们描述了我们数据的方向和大小。
- en: We then transform or project our original data onto these components
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将原始数据转换或投影到这些成分上。
- en: 'Let''s break this down in Python manually to illustrate the process. We''ll
    use the same Iris dataset that we used for the supervised learning illustration.
    First, we''ll create the correlation matrix:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动分解这个过程，以在Python中说明这个过程。我们将使用与监督学习示例中相同的Iris数据集。首先，我们将创建相关矩阵：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output should look as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![](img/6b9f2573-871b-4295-abe0-f9e104b9e3d5.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b9f2573-871b-4295-abe0-f9e104b9e3d5.png)'
- en: 'Our correlation matrix contains information on how every element of the matrix
    relates to each other element. This record of association is essential in providing
    the algorithm with information. Lots of variability typically indicates a signal,
    whereas a lack of variability indicates noise. The more variability that is present
    in a particular direction, the more there is to detect. Next, we''ll create our
    `eigen_values` and `eigen_vectors`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的相关矩阵包含了每个矩阵元素之间相互关系的信息。这种关联记录在为算法提供信息时至关重要。较大的变异性通常表示有信号，而缺乏变异性则表示噪音。某一方向上的变异性越大，越能被检测到。接下来，我们将创建我们的`eigen_values`和`eigen_vectors`：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output should look as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '![](img/faf4a54f-4dc7-48a6-aef8-12465565c874.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/faf4a54f-4dc7-48a6-aef8-12465565c874.png)'
- en: 'Eigenvectors and Eigenvalues come in pairs ; each eigenvectors are directions
    in of data, and eigenvalues tell us how much variance exists within that direction.
    In PCA, we want to understand which inputs account for the most variance in the
    data (that is: how much do they explain the data). By calculating eigenvectors
    and their corresponding eigenvalues, we can begin to understand what is most important
    in our dataset.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量和特征值成对出现；每个特征向量代表数据的方向，特征值告诉我们在该方向上存在多少变异。在PCA中，我们希望理解哪些输入对数据的变异性贡献最大（即：它们解释了多少数据）。通过计算特征向量及其对应的特征值，我们可以开始理解数据集中最重要的内容。
- en: We now want to sort the pairs of eigenvectors/eigenvalues from highest to lowest.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在希望按从高到低的顺序对特征向量/特征值对进行排序。
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Lastly, we need to project these pairs into a *lower dimensional spac*e. This
    is dimensionality reduction aspect of PCA:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将这些对投影到*低维空间*。这是PCA的降维部分：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We''ll then conduct this transformation on the original data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对原始数据进行这一转换：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can then plot the components against each other:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将成分相互绘制出来：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should see the plot as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到如下图所示：
- en: '![](img/0223443c-4129-4038-9661-00e32a31e296.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0223443c-4129-4038-9661-00e32a31e296.png)'
- en: 'So when should we use PCA? Use PCA when the following are true:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们什么时候应该使用PCA呢？当以下情况成立时使用PCA：
- en: Do you have high dimensionality (too many variables) and want a logical way
    to reduce them?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否有高维数据（太多的变量）并且想要一种逻辑的方式来减少它们？
- en: Do you need to ensure that your variables are independent of each other?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否需要确保你的变量彼此独立？
- en: 'One of the downsides of PCA, however, is that it makes the underlying data
    more opaque, thus hurting it''s interpretability. Besides PCA and the k–means
    clustering model that we precedingly described, other commonly seen non-deep learning
    unsupervised learning algorithms are:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，PCA的一个缺点是它使得底层数据变得更加不透明，从而影响了它的可解释性。除了PCA和我们前面描述的k-means聚类模型之外，其他常见的非深度学习无监督学习算法包括：
- en: K-means clustering
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: Hierarchical clustering
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Mixture models
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合模型
- en: Basic tuning
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本调优
- en: So you've built a model, now what? Can you call it a day? Chances are, you'll
    have some optimization to do on your model. A key part of the machine learning
    process is the optimization of our algorithms and methods. In this section, we'll
    be covering the basic concepts of optimization, and will be continuing our learning
    of tuning methods throughout the following chapters.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你已经建立了一个模型，现在该怎么办？可以收工了吗？很可能，你的模型仍然需要一些优化。机器学习过程的关键部分是优化我们的算法和方法。在本节中，我们将介绍优化的基本概念，并将在接下来的章节中继续学习调优方法。
- en: Sometimes, when our models do not perform well with new data it can be related
    to them **overfitting** or **underfitting**. Let's cover some methods that we
    can use to prevent this from happening. First off, let's look at the random forest
    classifier that we trained earlier. In your notebook, call the `predict` method
    on it and pass the `x_test` data in to receive some ...
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当我们的模型在新数据上的表现不好时，可能与它们**过拟合**或**欠拟合**有关。让我们介绍一些可以用来防止这种情况发生的方法。首先，我们来看一下之前训练的随机森林分类器。在你的笔记本中，调用
    `predict` 方法，并将 `x_test` 数据传递进去以获得一些 ...
- en: Overfitting and underfitting
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: '**Overfitting** is a phenomenon that happens when an algorithm learns it''s
    training data *too well* to the point where it cannot accurately predict on new
    data. Models that overfit learn the small, intricate details of their training
    set and don''t generalize well. For analogy, think about it as if you were learning
    a new language. Instead of learning the general form of the language, say Spanish,
    you''ve learned to perfect a local version of it from a remote part of South America,
    including all of the local slang. If you went to Spain and tried to speak that
    version of Spanish, you would probably get some puzzled looks from the locals! Underfitting
    would be exact opposite of this; you didn''t study enough Spanish, and so you
    do not have enough knowledge to communicate effectively. From a modeling standpoint,
    an underfit model is not complex enough to generalize to new data.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合** 是当算法将训练数据学得*过于完美*，以至于无法准确预测新数据时发生的现象。过拟合的模型学习了训练集中的小细节，并且无法很好地进行泛化。举个例子，可以把它想象成你正在学习一门新语言。你不是学习这门语言的一般形式，比如西班牙语，而是从南美的一个偏远地区学会了它的本地方言，包括所有当地的俚语。如果你去西班牙，试图说这种西班牙语，可能会让当地人感到困惑！
    欠拟合则是完全相反的情况；你没有学够西班牙语，无法有效沟通。从建模的角度来看，欠拟合的模型不够复杂，无法很好地泛化到新数据。'
- en: 'Overfitting and underfitting are tried to a machine learning phenomenon known
    as the **bias**/**variance** tradeoff:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合是与机器学习中的**偏差**/**方差**权衡现象相关的：
- en: '**Bias** is the error that your model learns as it tries to approximately predict
    things. Understanding that models are simplified versions of reality, bias in
    a model is the error that develops from trying to create this simplified version.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差** 是你的模型在尝试近似预测时学习到的误差。理解模型是现实世界的简化版本，模型中的偏差是从尝试创建这一简化版本时所产生的误差。'
- en: '**Variance** is the degree to which your error changes based on variations
    in the input data. It measures your model''s sensitivity to the intricacies of
    the input data.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**方差** 是你的误差如何根据输入数据的变化而变化的程度。它衡量了你的模型对输入数据复杂性的敏感度。'
- en: The way to mitigate bias is to increase the complexity of a model, although
    this will increase variance and will lead to overfitting. To mitigate variance
    on the other hand, we could make our model to generalize well by reducing complexing,
    although this would lead to higher bias. As you can see, we cannot have a both
    low bias and low variance at the same time! A good model will be balanced between
    it's bias and variance. There are two ways to combat overfitting; cross-validation
    and regularization. We will touch upon cross-validation methods now, and come
    back to regularization in [Chapter 4](8c724645-08d4-4a08-af9e-45bf607f8a88.xhtml), *Your
    First Artificial Neural Network* when we begin to build our first ANNs.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 减少偏差的方法是增加模型的复杂性，尽管这样会增加方差并导致过拟合。另一方面，为了减少方差，我们可以通过简化模型来让其更好地进行泛化，尽管这会导致更高的偏差。如你所见，我们无法同时实现低偏差和低方差！一个好的模型应该在偏差和方差之间取得平衡。对抗过拟合有两种方法：交叉验证和正则化。我们现在将简要介绍交叉验证方法，并在[第4章](8c724645-08d4-4a08-af9e-45bf607f8a88.xhtml)，*你的第一个人工神经网络*中，回到正则化部分，当我们开始构建第一个ANN时。
- en: K-fold cross-validation
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: You've already seen a form of cross-validation before; holding out a portion
    of our data is the simplest form of cross- validation that we can have. While
    this is generally a good practice, it can sometimes leave important features out
    of the training set that can create poor performance when it comes time to test.
    To remedy this, we can take standard cross validation a step further with a technique
    called **k**-**fold cross validation**.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经见过一种交叉验证的形式；将一部分数据隔离出来是我们可以使用的最简单形式的交叉验证。虽然这通常是一种良好的实践，但有时它可能会将一些重要特征排除在训练集之外，从而在测试时导致性能不佳。为了解决这个问题，我们可以通过一种叫做**k**-**折交叉验证**的技术进一步优化标准的交叉验证。
- en: In k-fold cross validation, our dataset is evenly divided in *k* event parts,
    chosen by the user. As a rule of thumb, generally you should stick to k = 5 or
    k = 10 for best performance. The model is then trained and tested *k* times over.
    During each training episode, one *k* segment of the data ...
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在k折交叉验证中，我们的数据集被平均划分为*k*个部分，由用户选择。作为经验法则，通常你应该选择k = 5或k = 10，以获得最佳性能。然后模型将被训练并测试*k*次。在每次训练过程中，其中一个*k*数据段将作为验证集...
- en: Hyperparameter optimization
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Aside from protecting against overfitting, we can optimize models by searching
    for the best combination of **model hyperparameters**. Hyperparameters are configuration
    variables that tell the model what methods to use, as opposed to **model parameters**
    which are learned during training - we'll learn more about these in upcoming chapter.
    They are programmatically added to a model, and are present in all modeling packages
    in Python. In the random forest model that we built precedingly, for instance,
    `n_estimators` is a hyperparameter that tells the model how many trees to build.
    The process of searching for the combination of hyperparameters that leads to
    the best model performance is called **hyperparameter tuning**.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了防止过拟合，我们还可以通过搜索最佳的**模型超参数**组合来优化模型。超参数是配置变量，它告诉模型使用哪些方法，而**模型参数**则是在训练过程中学习到的——我们将在接下来的章节中详细了解这些内容。超参数是程序化地添加到模型中的，并且在Python中的所有建模包中都可以找到。在我们之前构建的随机森林模型中，例如，`n_estimators`是一个超参数，它告诉模型构建多少棵树。搜索导致最佳模型性能的超参数组合的过程被称为**超参数调优**。
- en: 'In Python, we can tune hyperparameter with an exhaustive search over their
    potential values, called a **Grid Search**. Let''s use our random forest model
    to see how we can do this in Python by import `GrisSearchCV`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，我们可以通过对超参数的潜在值进行穷举搜索来调优超参数，这种方法称为**网格搜索**。让我们通过导入`GrisSearchCV`，使用我们的随机森林模型来看如何在Python中实现这一点：
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this case, we are going to pass the Grid Search a few different hyperparameters
    to check; you can read about what they do in the documentation for the classifier
    ([http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将向网格搜索传递一些不同的超参数进行检查；你可以在分类器的文档中阅读它们的作用（[http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)）。
- en: 'To create the search, we simply have to initialize it:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建搜索，我们只需初始化它：
- en: '[PRE18]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can then apply it to the data:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将其应用于数据：
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we then want to check the performance of the best combination of parameters,
    we can easily do that in sklearn by evaluating it on the test data:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要检查最佳参数组合的性能，我们可以通过在测试数据上评估它，轻松地在sklearn中实现：
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Hyperparameter tuning searches can be applied to the neural network models that
    we'll be utilizing in the coming chapters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优搜索可以应用于我们将在接下来的章节中使用的神经网络模型。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine learning, and by extension, deep learning, relies on the building blocks
    of linear algebra and statistics at its core. Vectors, matrices, and tensors provide
    the means by which we represent input data and parameters in machine learning
    algorithms, and the computations between these are the core operations of these
    algorithms. Likewise, distributions and probabilities help us model data and events
    in machine learning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，进一步说，深度学习，其核心依赖于线性代数和统计学的基本构件。向量、矩阵和张量为我们在机器学习算法中表示输入数据和参数提供了手段，而它们之间的计算则是这些算法的核心操作。同样，分布和概率帮助我们在机器学习中对数据和事件进行建模。
- en: 'We also covered two classes of algorithms that will inform how we think about
    ANNs in further chapters: supervised learning methods and unsupervised learning
    methods. With supervised learning, we provide the algorithm with a set of features
    and labels, and it learns how to appropriately map certain feature combinations
    ...'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了两类算法，这些算法将影响我们在后续章节中如何看待人工神经网络（ANNs）：监督学习方法和无监督学习方法。在监督学习中，我们为算法提供一组特征和标签，它会学习如何适当地映射某些特征组合...
