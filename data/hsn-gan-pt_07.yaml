- en: Generating Images Based on Label Information
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于标签信息生成图像
- en: In the previous chapter, we got the first taste of the potential of GANs to
    learn the connections between latent vectors and generated images and made a vague
    observation that latent vectors somehow manipulate the attributes of images. In
    this chapter, we will officially make use of the label and attribute information
    commonly seen in open datasets to properly establish the bridge between latent
    vectors and image attributes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们初步体验了GAN（生成对抗网络）学习潜在向量与生成图像之间关系的潜力，并模糊地观察到潜在向量以某种方式操控图像的属性。在本章中，我们将正式利用开放数据集中常见的标签和属性信息，正确地建立潜在向量与图像属性之间的桥梁。
- en: In this chapter, you will learn how to use **conditional GANs** (**CGANs**)
    to generate images based on a given label and how to implement adversarial learning
    with autoencoders and age human faces from young to old. Following this, you will
    be shown how to efficiently organize your source code for easy adjustments and
    extensions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习如何使用**条件GAN**（**CGAN**）基于给定标签生成图像，以及如何通过自动编码器实现对抗性学习，并让人类面部从年轻到衰老。接下来，您将学习如何高效地组织源代码，便于调整和扩展。
- en: After reading this chapter, you will have learned both supervised and unsupervised
    approaches to improve the quality of the images generated by GANs with label and
    attribute information. This chapter also introduces the basic source code hierarchy
    throughout this book, which can be very useful for your own projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将学习到使用监督和无监督方法，通过标签和属性信息来提高GAN生成图像的质量。本章还介绍了本书中使用的基本源代码结构，这对您的项目将非常有用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: CGANs – how are labels used?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CGAN —— 标签是如何使用的？
- en: Generating images from labels with CGANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标签生成图像的CGAN
- en: Working with Fashion-MNIST
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fashion-MNIST
- en: InfoGAN – unsupervised attribute extraction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InfoGAN —— 无监督的属性提取
- en: References and useful reading list
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考文献和有用的阅读书单
- en: CGANs – how are labels used?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CGAN —— 标签是如何使用的？
- en: In the previous chapter, we learned that a relation between the latent vector
    and the generated images can be established by the training process of GANs and
    certain manipulation of the latent vectors is reflected by the changes in the
    generated images. But we have no control over what part or what kinds of latent
    vectors would give us images with the attributes we want. To address this issue,
    we will use a CGAN to add label information in the training process so that we
    can have a say in what kinds of images the model will generate.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到，潜在向量与生成图像之间的关系可以通过GAN的训练过程建立，并且潜在向量的某些操作会通过生成图像的变化反映出来。但我们无法控制哪些部分或哪种类型的潜在向量会生成我们想要的属性图像。为了解决这个问题，我们将使用CGAN在训练过程中添加标签信息，这样我们就可以决定模型生成什么样的图像。
- en: The idea of CGANs was proposed by Mehdi Mirza and Simon Osindero in their paper,
    *Conditional Generative Adversarial Nets*. The core idea was to integrate the
    label information into both generator and discriminator networks so that the label
    vector would alter the distribution of latent vectors, which leads to images with
    different attributes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN的概念由Mehdi Mirza和Simon Osindero在他们的论文《Conditional Generative Adversarial
    Nets》中提出。其核心思想是将标签信息融入生成器和判别器网络，以使标签向量改变潜在向量的分布，从而生成具有不同属性的图像。
- en: 'Compared to the vanilla GAN model, CGAN makes a small change to the objective
    function to make it possible to include extra information by replacing the real
    data, ![](img/eaca6776-3f02-440c-9a80-4356f4487348.png), and generated data, ![](img/6326aae7-0bc5-4054-903b-0b2495f591cf.png), with
    ![](img/c4248bfa-8bfe-4af4-ad14-746e0d5d2dd5.png) and ![](img/98915d08-f8bc-4f40-af11-71b0a59b2cb0.png),
    respectively, in which ![](img/a6657c7b-fa05-41b0-a223-a17d531ff638.png) represents auxiliary
    information such as label and attribute:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的GAN模型相比，CGAN在目标函数上做了小的改动，使得通过将真实数据 ![](img/eaca6776-3f02-440c-9a80-4356f4487348.png)
    和生成数据 ![](img/6326aae7-0bc5-4054-903b-0b2495f591cf.png) 分别替换为 ![](img/c4248bfa-8bfe-4af4-ad14-746e0d5d2dd5.png)
    和 ![](img/98915d08-f8bc-4f40-af11-71b0a59b2cb0.png) 后，可以包含额外的信息，其中 ![](img/a6657c7b-fa05-41b0-a223-a17d531ff638.png)
    表示辅助信息，如标签和属性：
- en: '![](img/32861aa6-4c2c-4fcc-ba65-145237f1815b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32861aa6-4c2c-4fcc-ba65-145237f1815b.png)'
- en: In this equation, ![](img/61b7d781-89a5-497d-8fbc-3a376b09e790.png) borrows
    the form of conditional probability that describes how data ![](img/1f85531f-4c3a-4308-ae51-fa820cb8a117.png) is
    distributed under the condition of ![](img/61e3a5c4-e687-4d28-be72-5bb31c3fc309.png).
    To calculate the new object function, we need the generator network to be able
    to generate data given certain conditions and the discriminator network to tell
    whether the input image obeys the given condition. Therefore, in this section,
    we will talk about how to design the generator and discriminator to achieve this
    purpose.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/61b7d781-89a5-497d-8fbc-3a376b09e790.png)借用了描述数据在条件![](img/61e3a5c4-e687-4d28-be72-5bb31c3fc309.png)下如何分布的条件概率形式。为了计算新的目标函数，我们需要生成器网络能够根据某些条件生成数据，而判别器网络则判断输入的图像是否符合给定的条件。因此，在本节中，我们将讨论如何设计生成器和判别器来实现这一目标。
- en: We will create two different models in this chapter and, in order to write reusable
    code, we will put our source codes in separate files instead of putting all the code
    in to one single file as we did in previous chapters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将创建两个不同的模型，为了编写可重用的代码，我们将把源代码放在不同的文件中，而不是像以前一样将所有代码放在一个文件里。
- en: Combining labels with the generator
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将标签与生成器结合
- en: 'The architecture of the generator network of the CGAN is illustrated as follows. As
    described in the original paper, all data is generated through an MLP-like network.
    Unlike in the original paper, however, we use a much deeper structure and techniques
    such as batch normalization and LeakyReLU to ensure better-looking results:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN的生成器网络架构如下所示。如原始论文所述，所有数据都是通过类似MLP的网络生成的。然而，与原始论文不同的是，我们使用了一个更深的结构，并采用了批量归一化和LeakyReLU等技术，以确保获得更好的结果：
- en: '![](img/360f39cd-eb70-4a81-af55-0f39dec63c23.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/360f39cd-eb70-4a81-af55-0f39dec63c23.png)'
- en: The generator network architecture of the CGAN
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN的生成器网络架构
- en: The label value is transformed into a vector with a length of 10, which is concatenated
    with the latent vector *z*. All data in the generator network is stored in the
    form of a vector. The length of the output vector equals the multiplication of
    width and height of the generated image, which is ![](img/c674568b-845e-4ceb-8acd-3dc0a733c1da.png) for
    the MNIST dataset. We can, of course, change the size of the output image to other
    values we want (we will set the image size to 64 x 64 later in the source code).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 标签值被转换为长度为10的向量，并与潜在向量*z*拼接。生成器网络中的所有数据都以向量的形式存储。输出向量的长度等于生成图像的宽度和高度的乘积，MNIST数据集的输出为
    ![](img/c674568b-845e-4ceb-8acd-3dc0a733c1da.png)。当然，我们可以将输出图像的大小更改为我们想要的其他值（稍后我们将在源代码中将图像大小设置为64
    x 64）。
- en: Let's organize the codes differently from previous chapters and create a `cgan.py`
    file for model definition.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将代码组织方式与之前的章节不同，并创建一个`cgan.py`文件来定义模型。
- en: 'First, we import the PyTorch and NumPy modules at the beginning of the source
    file:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在源代码文件的开始部分导入PyTorch和NumPy模块：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we define the `Generator` network:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义`Generator`网络：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The generator network consists of 5 linear layers, 3 of which are connected
    to batch normalization layers, and the first 4 linear layers have `LeakyReLU`
    activation functions while the last has a `Tanh` activation function. The label
    information is processed by the `nn.Embedding` module, which behaves as a lookup
    table. Say we have 10 labels at hand for training samples. The embedding layer
    transforms the 10 different labels into 10 pre-defined embedding vectors, which
    are initialized based on normal distribution by default. The embedding vector
    of labels is then concatenated with the random latent vector to serve as the input
    vector of the first layer. Finally, we need to reshape the output vector into
    2D images as the final results.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络由5个线性层组成，其中3个连接到批量归一化层，前4个线性层使用`LeakyReLU`激活函数，最后一个使用`Tanh`激活函数。标签信息通过`nn.Embedding`模块处理，它充当查找表。假设我们手头有10个用于训练样本的标签。嵌入层将这10个不同的标签转换为10个预定义的嵌入向量，这些向量默认通过正态分布初始化。标签的嵌入向量然后与随机潜在向量拼接，作为第一层的输入向量。最后，我们需要将输出向量重塑为2D图像作为最终结果。
- en: Integrating labels into the discriminator
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将标签集成到判别器中
- en: 'The architecture of the discriminator network of the CGAN is illustrated as
    follows. Again, the discriminator architecture is different from the one used
    in the original paper. You are, of course, more than welcome to make adjustments
    to the networks and see whether your models can generate better results:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN 判别器网络的架构如下所示。同样，判别器的架构与原始论文中使用的架构不同。当然，你可以根据自己的需求调整网络结构，看看你的模型是否能生成更好的结果：
- en: '![](img/65c76439-518e-45bd-b9b8-f83c697cc7ba.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65c76439-518e-45bd-b9b8-f83c697cc7ba.png)'
- en: Discriminator network architecture of CGAN
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CGAN 的判别器网络架构
- en: Similar to the generator network, the label value is also part of the input
    of the discriminator network of the CGAN. The input image (with a size of 28 x
    28) is transformed into a vector with a length of 784, therefore, the total length
    of the input vector of the discriminator network is 794\. There are 4 hidden layers
    in the discriminator network. Unlike common CNN models for image classification, the
    discriminator network outputs a single value instead of a vector with the length
    as the number of classes. It is because we already include the label information
    in the network input and we only want the discriminator network to tell us how close an
    image is to the real images, given the label condition.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成器网络类似，标签值也是 CGAN 判别器网络的输入部分。输入图像（大小为 28 x 28）被转换为一个长度为 784 的向量，因此判别器网络的输入向量的总长度为
    794。判别器网络中有 4 个隐藏层。与常见的用于图像分类的 CNN 模型不同，判别器网络输出的是一个单一的值，而不是一个长度等于类别数的向量。这是因为我们已经将标签信息包含在网络输入中，且我们只希望判别器网络根据给定的标签条件告诉我们，图像与真实图像的相似度有多高。
- en: 'Now, let''s define the discriminator network in the `cgan.py` file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 `cgan.py` 文件中定义判别器网络：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similarly, the labels are passed by another `nn.Embedding` module before being
    concatenated with the image vector. The discriminator network consists of 5 linear
    layers, 2 of which are connected to `Dropout` layers to enhance the generalization
    capability. Since we cannot always guarantee that the output values of the last
    layer lie within a range of [0, 1], we need a `Sigmoid` activation function to
    make sure of that.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，标签通过另一个 `nn.Embedding` 模块传递，在与图像向量连接之前。判别器网络由 5 个线性层组成，其中 2 个层连接到 `Dropout`
    层，以增强泛化能力。由于我们不能总是保证最后一层的输出值位于 [0, 1] 的范围内，因此我们需要一个 `Sigmoid` 激活函数来确保这一点。
- en: A `Dropout` layer with a dropout rate of 0.4 means that, at each iteration during
    training, each neuron has a probability of 0.4 of not participating in the calculation
    of the final results. Therefore, different submodels are trained at different
    training steps, which makes it harder for the whole model to overfit the training
    data compared to the one without `Dropout` layers. `Dropout` layers are often
    deactivated during evaluation.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dropout` 层的丢弃率为 0.4 意味着，在训练的每一次迭代中，每个神经元有 0.4 的概率不参与最终结果的计算。因此，在不同的训练步骤中，训练出的是不同的子模型，这使得整个模型相较于没有
    `Dropout` 层的模型，更不容易过拟合训练数据。在评估过程中，`Dropout` 层通常会被停用。'
- en: The choice of which layer has a `Dropout` or `LeakyReLU` activation function
    is rather subjective. You can try out other combinations and find out which configuration
    yields the best results.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 哪一层使用 `Dropout` 或 `LeakyReLU` 激活函数的选择是相当主观的。你可以尝试其他组合，并找出哪个配置产生了最好的结果。
- en: Generating images from labels with the CGAN
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CGAN 从标签生成图像
- en: In the previous section, we defined the architecture of both generator and discriminator
    networks of the CGAN. Now, let's write the code for model training. In order to
    make it easy for you to reproduce the results, we will use MNIST as the training
    set to see how the CGAN performs in image generation. What we want to accomplish
    here is that, after the model is trained, it can generate the correct digit image
    we tell it to, with extensive variety.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们定义了 CGAN 的生成器和判别器网络的架构。现在，让我们编写模型训练的代码。为了方便你重现结果，我们将使用 MNIST 作为训练集，看看
    CGAN 在图像生成方面的表现。我们希望完成的目标是，在模型训练完成后，它能够根据我们指定的数字生成正确的数字图像，并且具有丰富的多样性。
- en: One-stop model training API
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一站式模型训练 API
- en: 'First, let''s create a new `Model` class that serves as a wrapper for different
    models and provides the one-stop training API. Create a new file named `build_gan.py`
    and import the necessary modules:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个新的 `Model` 类，它作为不同模型的封装，并提供一站式训练 API。创建一个名为 `build_gan.py` 的新文件，并导入必要的模块：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, let''s create the `Model` class. In this class, we will initialize the
    `Generator` and `Discriminator` modules and provide `train` and `eval` methods
    so that users can simply call `Model.train()` (or `Model.eval()`) somewhere else
    to complete the model training (or evaluation):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们创建`Model`类。在这个类中，我们将初始化`Generator`和`Discriminator`模块，并提供`train`和`eval`方法，以便用户可以简单地在其他地方调用`Model.train()`（或`Model.eval()`）来完成模型的训练（或评估）：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, the generator network, `netG`, and the discriminator network, `netD`,
    are initialized based on the class number (`classes`), image channel (`channels`),
    image size (`img_size`), and length of the latent vector (`latent_dim`). These
    arguments will be given later. For now, let''s assume that these values are already
    known. Since we need to initialize all tensors and functions in this class, we
    need to define the `device` our model is running on (`self.device`). The `optim_G`
    and `optim_D` objects are optimizers for the two networks. They are initialized
    with the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，生成器网络`netG`和判别器网络`netD`是基于类数（`classes`）、图像通道（`channels`）、图像大小（`img_size`）和潜在向量的长度（`latent_dim`）进行初始化的。这些参数稍后会给出。目前，我们假设这些值已经是已知的。由于我们需要初始化该类中的所有张量和函数，我们需要定义模型运行的`device`（`self.device`）。`optim_G`和`optim_D`对象是两个网络的优化器，它们会用以下方式进行初始化：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first argument of the `Adam` optimizer, `filter(lambda p: p.requires_grad,
    self.netG.parameters())`, is to grab all `Tensor` whose `requires_grad` flag is
    set to `True`. It is pretty useful when part of the model is untrained (for example,
    fine-tuning the last layer after transferring a trained model to a new dataset),
    even though it''s not necessary in our case.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Adam`优化器的第一个参数，`filter(lambda p: p.requires_grad, self.netG.parameters())`，用于获取所有`requires_grad`标记为`True`的`Tensor`。这在模型的一部分未训练时非常有用（例如，将一个训练好的模型迁移到新数据集后对最后一层进行微调），尽管在我们的情况下并不必要。'
- en: 'Next, let''s define a method called `train` for model training. Arguments of
    `train` include the number of training epochs (`epochs`), the iteration interval
    between logging messages (`log_interval`), the output directory for results (`out_dir`),
    and whether to print training messages to the Terminal (`verbose`):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义一个名为`train`的方法用于模型训练。`train`的参数包括训练的轮数（`epochs`）、日志消息之间的迭代间隔（`log_interval`）、结果输出目录（`out_dir`）以及是否将训练消息打印到终端（`verbose`）：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In `train`, we first switch the networks to train mode (for example, `self.netG.train()`).
    It mostly affects the behaviors of `Dropout` and  the batch normalization layers.
    Then, we define a set of fixed latent vectors (`viz_noise`) and labels (`viz_label`).
    They are used to occasionally produce images during training so that we can track
    how the model is trained, otherwise, we may only realize the training has gone
    south after the training is done:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train`中，我们首先将网络切换到训练模式（例如，`self.netG.train()`）。这主要影响`Dropout`和批量归一化层的行为。然后，我们定义一组固定的潜在向量（`viz_noise`）和标签（`viz_label`）。它们用于在训练过程中偶尔生成图像，以便我们能够跟踪模型的训练进度，否则我们可能只有在训练完成后才意识到训练出现了问题：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we omitted some parts of the code (including the evaluation API and model
    exporting and loading). You can get the full source code from the code repository
    for this chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们省略了一些代码部分（包括评估API、模型导出和加载）。你可以从本章的代码库中获取完整的源代码。
- en: Argument parsing and model training
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数解析和模型训练
- en: 'Now, the only thing left for us to do is to create and define the main entry
    for the project. In this file, we will need to define the arguments we previously
    have assumed to be known. These hyper-parameters are essential when we create
    any network, and we will elegantly parse these values. Let''s create a new file
    called `main.py` and import the necessary modules:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要做的唯一事情就是为项目创建并定义主入口。在这个文件中，我们需要定义我们之前假定为已知的参数。这些超参数在创建任何网络时都至关重要，我们将优雅地解析这些值。让我们创建一个名为`main.py`的新文件，并导入必要的模块：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Have you noticed that the only Python module that's related to our model is `build_gan.Model`?
    We can easily create another project and copy most of the content in this file
    without major revisions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有注意到，唯一与我们模型相关的Python模块是`build_gan.Model`？我们可以轻松创建另一个项目，并在没有重大修改的情况下复制这个文件中的大部分内容。
- en: 'Then, let''s define the `main` function:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们定义`main`函数：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Since we have already defined the networks and training schedule in separate
    files, the initialization and training of the model are accomplished with only
    3 lines of codes: `model = Model()`, `model.create_optim()`, and `model.train()`.
    This way, our code is easy to read, modify, and maintain, and we can easily use
    most of the code in other projects.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在单独的文件中定义了网络和训练计划，因此模型的初始化和训练只需要3行代码：`model = Model()`、`model.create_optim()`和`model.train()`。这样，我们的代码易于阅读、修改和维护，而且我们可以轻松地在其他项目中使用大部分代码。
- en: The `FLAGS` object stores all the arguments and hyper-parameters needed for
    model definition and training. To make the configuration of the arguments more
    user-friendly, we will use the `argparse` module provided by Python.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`FLAGS`对象存储了定义和训练模型所需的所有参数和超参数。为了使参数配置更加用户友好，我们将使用Python提供的`argparse`模块。'
- en: Note that if you would like to use a different dataset, you can change the definition
    of the `dataset` object in the same way as in the previous chapter.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你想使用不同的数据集，可以像上一章一样修改`dataset`对象的定义。
- en: 'The `main` entry of the source code and the definitions of arguments are as
    follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码的`main`入口和参数定义如下：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: A new argument is created by `parser.add_argument(ARG_NAME, ARG_TYPE, DEFAULT_VALUE,
    HELP_MSG)`, in which `ARG_NAME` is the argument name, `ARG_TYPE` is the value
    type of argument (for example, `int`, `float`, `bool`, or `str`), `DEFAULT_VALUE`
    is the default argument value when none is given, and `HELP_MSG` is the message
    printed when running `python main.py --help` in the Terminal. The argument value
    is specified by `python main.py --ARG_NAME ARG_VALUE`, or you can change the default
    value in the source code and simply run `pythin main.py`. Here, our model is to
    be trained for 200 epochs with a batch size of 128\. The learning rate is set
    to 0.0002,  because a small learning rate value is suitable for the `Adam` method.
    The length of the latent vector is 100 and the size of the generated image is
    set to 64\. We also set the random seed to 1 so that you can produce the exact
    same results as in this book.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`parser.add_argument(ARG_NAME, ARG_TYPE, DEFAULT_VALUE, HELP_MSG)`创建一个新参数，其中`ARG_NAME`是参数名称，`ARG_TYPE`是参数的值类型（例如，`int`、`float`、`bool`或`str`），`DEFAULT_VALUE`是没有给定值时的默认参数值，`HELP_MSG`是在终端中运行`python
    main.py --help`时打印的消息。可以通过`python main.py --ARG_NAME ARG_VALUE`来指定参数值，或者你可以在源代码中更改默认值并直接运行`python
    main.py`。在这里，我们的模型将训练200个epoch，批次大小为128，学习率设置为0.0002，因为较小的学习率值适合`Adam`方法。潜在向量的长度为100，生成图像的大小设置为64。我们还将随机种子设置为1，以便你可以得到与本书中相同的结果。
- en: '`boolean_string` is defined in the `utils.py` file, which is as follows (reference
    visit [https://stackoverflow.com/a/44561739/3829845](https://stackoverflow.com/a/44561739/3829845) for
    more information). Otherwise, passing `--train False` in the Terminal will not
    affect the script:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`boolean_string`在`utils.py`文件中定义，内容如下（更多信息请参考[https://stackoverflow.com/a/44561739/3829845](https://stackoverflow.com/a/44561739/3829845)）。否则，在终端中传递`--train
    False`将不会影响脚本：'
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We still need to do some preprocessing on the arguments:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要对参数进行一些预处理：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we first make sure that CUDA is indeed available to PyTorch. Then, we
    manually set the random seed to the NumPy, PyTorch, and CUDA backend. We need
    to clear the output directory each time we retrain the model and all output messages
    are redirected to an external file, `log.txt`. Finally, we print all of the arguments
    taken before running the `main` function so that we may have a chance to check
    whether we have configured the model correctly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先确保CUDA确实可用于PyTorch。然后，我们手动设置NumPy、PyTorch和CUDA后端的随机种子。每次重新训练模型时，我们需要清理输出目录，并将所有输出消息重定向到外部文件`log.txt`。最后，我们在运行`main`函数之前打印所有的参数，以便检查是否正确配置了模型。
- en: 'Now, open a Terminal and run the following script. Remember to change `DATA_DIRECTORY`
    to the path of the MNIST dataset on your machine:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，打开终端并运行以下脚本。记得将`DATA_DIRECTORY`更改为你机器上MNIST数据集的路径：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output message may look like this (the order of the arguments might be
    different):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出信息可能如下所示（参数的顺序可能不同）：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It takes about 22 minutes to train for 200 epochs on a GTX 1080Ti graphics
    card and costs about 729 MB of GPU memory. The generated images from the MNIST
    dataset are shown here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在GTX 1080Ti显卡上训练200个epoch大约需要22分钟，并消耗约729 MB的GPU内存。从MNIST数据集中生成的图像如下所示：
- en: '![](img/02600168-55c5-4106-b548-bce59d22737c.png)![](img/2532795f-15e5-4362-acd0-3f62db82621f.png)![](img/9a207fa4-3801-43a6-81ae-105e4e27de51.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02600168-55c5-4106-b548-bce59d22737c.png)![](img/2532795f-15e5-4362-acd0-3f62db82621f.png)![](img/9a207fa4-3801-43a6-81ae-105e4e27de51.png)'
- en: 'Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch;
    right: 200th epoch)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过CGAN生成的MNIST图像（左：第1个epoch；中：第25个epoch；右：第200个epoch）
- en: We can see that the digit images are correctly generated for the corresponding
    labels while maintaining realistic variety in appearance. Because we treat the
    images as very long vectors in the model, it is hard to generate smoothness in
    both vertical and horizontal directions and it is easy to spot speckle noise in
    the generated images after only 25 epochs of training. However, the quality of
    the images gets a lot better after 200 epochs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对于相应的标签，数字图像被正确生成，同时保持外观上的逼真多样性。由于我们将图像视为模型中的非常长的向量，因此很难在垂直和水平方向上生成平滑度，而且在训练了仅25个epoch后，生成的图像容易出现斑点噪声。然而，在训练200个epoch后，图像质量有了显著提高。
- en: Working with Fashion-MNIST
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Fashion-MNIST
- en: So you know by now that the MNIST dataset is comprised of a bunch of handwritten
    numbers. It is the defacto standard for the Machine Learning community, and it
    is often used to validate processes. Another group has decided to create another
    dataset that could be a better replacement. This project is named **Fashion-MNIST** and
    is designed to be a simple drop-in replacement. You can get a deeper understanding
    of the project at [https://www.kaggle.com/zalando-research/fashionmnist/data#](https://www.kaggle.com/zalando-research/fashionmnist/data#).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你现在应该知道，MNIST数据集由一堆手写数字组成。它是机器学习领域的事实标准，常用于验证流程。另一个小组决定创建一个可能是更好的替代数据集。这个项目被命名为**Fashion-MNIST**，旨在作为一个简单的替代品。您可以通过[https://www.kaggle.com/zalando-research/fashionmnist/data#](https://www.kaggle.com/zalando-research/fashionmnist/data#)深入了解该项目。
- en: '**Fashion-MNIST** consists of a training set of 60,000 images and labels and
    a test set of 10,000 images and labels. All images are grayscale and set to 28x28
    pixels, and there are 10 classes of images, namely: T-shirt/top, Trouser, Pullover,
    Dress, Coat, Saldal, Shirt, Sneaker, Bag, and Ankle boot. You can already begin
    to see that this replacement dataset should work the algorithms harder.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**Fashion-MNIST** 包含一个包含60,000张图像和标签的训练集，以及一个包含10,000张图像和标签的测试集。所有图像都是灰度图，大小为28x28像素，共有10个类别的图像，分别是：T恤/上衣、裤子、套头衫、裙子、大衣、凉鞋、衬衫、运动鞋、包和
    ankle boot。你可以开始看到，这个替代数据集应该能让算法更加复杂。'
- en: To demonstrate the use of the dataset, we will use the program that we just
    created for the standard MNIST dataset, and make a few changes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示数据集的使用，我们将使用刚才为标准MNIST数据集创建的程序，并做一些修改。
- en: 'Copy the main.py file to `fashion-main.py` to keep the original safe. Now in
    the `fashion-main.py` file find the following portion of code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将main.py文件复制到`fashion-main.py`以保护原文件。现在，在`fashion-main.py`文件中找到以下代码部分：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It's the fourth line in the `main()` function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`main()`函数中的第四行。
- en: 'Now, all you need to change is the the `dset.MNIST(` to `dset.FashionMNIST(`
    like this:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您只需将`dset.MNIST(`更改为`dset.FashionMNIST(`，就像这样：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Luckily, torchvision already has a built-in class for Fashion-MNIST. We'll point
    out a few others in a few minutes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，torchvision已经为Fashion-MNIST提供了一个内置类。稍后我们会指出其他一些类。
- en: Now save your source file.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在保存您的源文件。
- en: Now, to make sure that your dataset from the first example is safe, rename the
    Data folder that was used last time. The new dataset will automatically be downloaded
    for you. One other thing you should do is to rename your output folder, again
    to keep that safe.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了确保您的第一个示例中的数据集安全，重新命名上次使用的Data文件夹。新数据集将自动下载给您。另一件事是，您应该重新命名输出文件夹，以确保它的安全。
- en: 'As we did with the last program, we''ll start it with a command line entry:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们在上一个程序中所做的，我们将通过命令行输入来启动它：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output in the Terminal will look pretty much like that of the last program,
    except for the lines showing the download of the new dataset information:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 终端中的输出将与上一个程序非常相似，除了显示下载新数据集信息的几行：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is an example of the output images you can expect:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是你可以期待的输出图像示例：
- en: '![](img/8b484efe-7cf1-4bbb-adcf-9cf7b26bd6f1.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b484efe-7cf1-4bbb-adcf-9cf7b26bd6f1.png)'
- en: On the left is the real sample data, in the middle is the result from epoch
    0, and finally on the right is the result from epoch 199.  While not perfect,
    you can see that the output is getting quite good.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是实际的样本数据，中间是第0轮的结果，最后右侧是第199轮的结果。尽管结果并不完美，但你可以看到输出已经变得相当不错。
- en: 'Earlier, I said that we would look at other classes that torchvision supports.
    There are too many to discuss here, but if you go to: [https://pytorch.org/docs/stable/torchvision/datasets.html](https://pytorch.org/docs/stable/torchvision/datasets.html),
    you can see the large list of each supported class and the API parameters. Many
    of them can be used as-is with our code, with the exception of modifying the dataset
    line in your code, and even allow the program to download the dataset for you.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我提到过我们将会看看torchvision支持的其他类。这里无法讨论所有内容，但如果你访问：[https://pytorch.org/docs/stable/torchvision/datasets.html](https://pytorch.org/docs/stable/torchvision/datasets.html)，你可以看到每个支持类及其API参数的详细列表。许多类可以直接与我们的代码一起使用，唯一需要修改的是代码中的数据集行，甚至可以让程序为你下载数据集。
- en: InfoGAN – unsupervised attribute extraction
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfoGAN – 无监督属性提取
- en: 'In the previous sections, we have learned how to use auxiliary information
    such as the labels of data to improve the image quality generated by GANs. However,
    it is not always possible to prepare accurate labels of training samples beforehand.
    Sometimes, it is even difficult for us to accurately describe the labels of extremely
    complex data. In this section, we will introduce another excellent model from
    the GAN family, **InfoGAN**, which is capable of extracting data attributes during
    training in an unsupervised manner. InfoGAN was proposed by Xi Chen, Yan Duan,
    Rein Houthooft, et. al. in their paper, *InfoGAN: Interpretable Representation
    Learning by Information Maximizing Generative Adversarial Nets*. It showed that
    GANs could not only learn to generate realistic samples but also learn semantic
    features, which are essential to sample generation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的章节中，我们学习了如何使用数据的标签等辅助信息来提高GAN生成图像的质量。然而，准备训练样本的准确标签并不总是可行的。有时候，准确描述极其复杂数据的标签甚至是非常困难的。在本节中，我们将介绍来自GAN家族的另一个优秀模型——**InfoGAN**，它能够在训练过程中以无监督的方式提取数据的属性。InfoGAN由Xi
    Chen、Yan Duan、Rein Houthooft等人在他们的论文《*InfoGAN: Interpretable Representation Learning
    by Information Maximizing Generative Adversarial Nets*》中提出。它展示了GAN不仅能够学习生成逼真的样本，还能够学习对样本生成至关重要的语义特征。'
- en: 'Similar to CGANs, InfoGAN also replaces the original distribution of data with
    conditional distribution (with auxiliary information as conditions). The main
    difference is that InfoGAN does not need to feed label and attribute information
    into the discriminator network; instead, it uses another classifier, Q, to measure
    how auxiliary features are learned. The objective function of InfoGAN is as follows.
    You may notice that it adds another objective, ![](img/79d47314-c5a5-43d0-aba0-ba44df6de818.png), at
    the end of the formula:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与CGAN类似，InfoGAN也用条件分布（将辅助信息作为条件）替换了数据的原始分布。主要的区别在于，InfoGAN不需要将标签和属性信息输入到判别器网络中；而是使用另一个分类器Q来衡量辅助特征的学习情况。InfoGAN的目标函数如下所示。你可能会注意到它在公式末尾添加了另一个目标，![](img/79d47314-c5a5-43d0-aba0-ba44df6de818.png)：
- en: '![](img/bcb9dc84-25a6-41b6-8ebe-bdcfa1679a8f.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcb9dc84-25a6-41b6-8ebe-bdcfa1679a8f.png)'
- en: In this formula, ![](img/cbf2563c-2567-41b1-8fb9-d9378d5acbb3.png) is the generated
    sample, ![](img/9cfdd18b-8977-4117-85a1-cc2a0c46c587.png) is the latent vector,
    and ![](img/654bfdde-2fef-4d3e-91a0-4f17c16d495b.png) represent auxiliary information. ![](img/a35772fe-0a4e-4ad1-8b0b-be86133e22e5.png) describes
    the actual distribution of ![](img/509e60c1-6cca-4ace-a755-73b3061c0b63.png), which
    is rather hard to find. Therefore, we use the posterior distribution, ![](img/475265f4-181b-4049-81ec-0b07128db5a0.png), to
    estimate ![](img/6bc1c28f-753d-4dfd-940b-a0228b0ad023.png), and this process is
    done with a neural network classifier.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，![](img/cbf2563c-2567-41b1-8fb9-d9378d5acbb3.png)是生成的样本，![](img/9cfdd18b-8977-4117-85a1-cc2a0c46c587.png)是潜在向量，![](img/654bfdde-2fef-4d3e-91a0-4f17c16d495b.png)代表辅助信息。![](img/a35772fe-0a4e-4ad1-8b0b-be86133e22e5.png)描述了![](img/509e60c1-6cca-4ace-a755-73b3061c0b63.png)的实际分布，而这通常很难找到。因此，我们使用后验分布，![](img/475265f4-181b-4049-81ec-0b07128db5a0.png)，来估计![](img/6bc1c28f-753d-4dfd-940b-a0228b0ad023.png)，这一过程通过神经网络分类器完成。
- en: In the preceding formula, ![](img/8136ddf4-79c1-4755-805c-944529966d0b.png) is,
    in fact, an approximation of **mutual information,** ![](img/1d81edb9-a9f6-437e-bddb-d2e0a675c288.png), between
    the auxiliary vector and generated sample. Mutual information, ![](img/ec123496-34f5-4261-bba5-46cc2d607f0c.png), describes
    how much we know about random variable *X* based on knowledge of *Y*—![](img/3afa941c-e215-4e7c-aab4-eb1c3da11316.png),
    in which ![](img/fb07f42f-3d25-44b5-9082-949730ee7887.png) is **entropy** and ![](img/6eae0ac6-7c94-429d-b139-c01f56f40ef6.png) is
    **conditional entropy**. It can also be described by the **Kullback–Leibler divergence**, ![](img/410d5388-e62a-4b72-906d-0d136f5d3614.png),
    which describes the information loss when we use marginal distributions to approximate
    the joint distribution of *X* and *Y*. You can refer to the original InfoGAN paper
    for detailed mathematical derivation. For now, you only need to know that ![](img/ffd1b3f6-526a-4e87-9b42-7a447788c35c.png) tells
    us whether the generation of ![](img/88503b5b-daaa-443d-899c-9b1e282c37b3.png) based
    on ![](img/ad5450c3-c36f-43c1-9a2b-bd936e9b1e73.png) goes as desired.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述公式中， ![](img/8136ddf4-79c1-4755-805c-944529966d0b.png) 实际上是 **互信息**的近似值，
    ![](img/1d81edb9-a9f6-437e-bddb-d2e0a675c288.png)，用于表示辅助向量与生成样本之间的关系。互信息， ![](img/ec123496-34f5-4261-bba5-46cc2d607f0c.png)，描述了我们基于
    *Y* 的知识能了解关于随机变量 *X* 的多少—— ![](img/3afa941c-e215-4e7c-aab4-eb1c3da11316.png)，其中
    ![](img/fb07f42f-3d25-44b5-9082-949730ee7887.png) 是 **熵**，而 ![](img/6eae0ac6-7c94-429d-b139-c01f56f40ef6.png)
    是 **条件熵**。它也可以通过 **Kullback-Leibler 散度**来描述， ![](img/410d5388-e62a-4b72-906d-0d136f5d3614.png)，该散度描述了我们使用边缘分布来近似
    *X* 和 *Y* 的联合分布时的信息损失。你可以参考原版 InfoGAN 论文来了解详细的数学推导。目前，你只需要知道， ![](img/ffd1b3f6-526a-4e87-9b42-7a447788c35c.png)
    告诉我们生成的 ![](img/88503b5b-daaa-443d-899c-9b1e282c37b3.png) 是否如预期那样基于 ![](img/ad5450c3-c36f-43c1-9a2b-bd936e9b1e73.png)
    生成。
- en: Network definitions of InfoGAN
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfoGAN 的网络定义
- en: 'The architecture of the generator network of InfoGAN is illustrated as follows.
    The reproduction of results from the original paper is rather tricky to handle.
    Therefore, we present a model architecture based on this GitHub repository, [https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN 生成器网络的架构如下所示。复现原论文中的结果相对复杂。因此，我们基于此 GitHub 仓库提供的模型架构， [https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/infogan/infogan.py)：
- en: '![](img/96400795-ee88-4aea-8602-ff8f394dcf9c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96400795-ee88-4aea-8602-ff8f394dcf9c.png)'
- en: Generator architecture of InfoGAN
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN 的生成器架构
- en: The generator network of InfoGAN consists of 4 hidden layers. The first hidden
    layer transforms the input vector with a length of 74 (*62+10+2*) into a length
    of 8,192 (*128*8*8*), which is then directly turned into tensor with the dimensionality
    of *128*8*8.* The feature maps are then gradually up-scaled to 32*32 images. The
    scaling of feature maps is done with `torch.nn.functional.interpolate`. We need
    to define a derived `Module` class for upsampling so that we can treat it as other
    `torch.nn` layers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN 的生成器网络由 4 层隐藏层组成。第一层隐藏层将长度为 74 (*62+10+2*) 的输入向量转换为长度为 8,192 (*128*8*8*)，然后直接转换为具有
    *128*8*8* 维度的张量。特征图随后逐渐放大到 32*32 的图像。特征图的缩放通过 `torch.nn.functional.interpolate`
    完成。我们需要定义一个派生的 `Module` 类来进行上采样，以便将其视为其他 `torch.nn` 层。
- en: 'Let''s create a new source file called `infogan.py` and import the same Python
    modules as in `cgan.py` and define the `Upsample` class as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 `infogan.py` 的新源文件，并导入与 `cgan.py` 中相同的 Python 模块，然后按如下方式定义 `Upsample`
    类：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We use the `bilinear` method to up-scale the images because it's the best fit
    compared to other choices. Since we derive this class from `torch.nn.Module` and
    only use the functions from `torch` to perform the calculation in the forward
    pass, our custom class will have no trouble performing the gradient back-propagation
    in training.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `bilinear` 方法来上采样图像，因为它相比其他选择最为合适。由于我们从 `torch.nn.Module` 派生这个类，并且仅在前向传播过程中使用
    `torch` 中的函数进行计算，我们的自定义类在训练时不会出现反向传播梯度计算的问题。
- en: In PyTorch 1.0, calling `nn.Upsample` will give a deprecated warning and it
    is now, in fact, implemented with `torch.nn.functional.interpolate`. Therefore,
    our custom `Upsample` layer is the same as `nn.Upsample`, but without warning
    message.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 1.0 中，调用 `nn.Upsample` 会给出一个弃用警告，实际上它现在是通过 `torch.nn.functional.interpolate`
    来实现的。因此，我们自定义的 `Upsample` 层与 `nn.Upsample` 相同，但没有警告信息。
- en: 'The generator network is defined as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络定义如下：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this class, we use a helper function, `_create_deconv_layer`, to create the
    convolutional hidden layers. Since we will use the custom `Upsample` layer to
    increase the size of feature maps, we only need to use `nn.Conv2d`, whose input
    size equals, output size, rather than `nn.ConvTranspose2d` as in the DCGAN in
    the last chapter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类中，我们使用了一个辅助函数`_create_deconv_layer`来创建卷积隐藏层。由于我们将使用自定义的`Upsample`层来增加特征图的大小，因此我们只需要使用`nn.Conv2d`，其输入大小等于输出大小，而不是像上一章中的DCGAN那样使用`nn.ConvTranspose2d`。
- en: In our configuration of InfoGAN, `torch.nn.functional.interpolate` combined
    with `nn.Conv2d` performs better than `nn.ConvTranspose2d` with stride. Although
    you are welcome to try out different configurations and see whether they produce
    better results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的InfoGAN配置中，`torch.nn.functional.interpolate`与`nn.Conv2d`组合比使用步幅的`nn.ConvTranspose2d`效果更好。尽管如此，您仍然可以尝试不同的配置，看看是否能得到更好的结果。
- en: 'The architecture of the discriminator network of InfoGAN is illustrated as
    follows. Again, we use a different structure than in the original paper:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN判别器网络的架构如下所示。再次强调，我们使用了与原始论文中不同的结构：
- en: '![](img/780dbcbf-9c2c-443f-b87e-1c14b91e065e.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/780dbcbf-9c2c-443f-b87e-1c14b91e065e.png)'
- en: Discriminator architecture of InfoGAN
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN的判别器架构
- en: The discriminator network consists of 4 hidden layers. As explained before,
    InfoGAN uses an additional classifier network to measure the validity of the auxiliary
    vector. In fact, this additional classifier shares most of its weights (the first
    4 hidden layers) with the discriminator. Therefore, the quality measure on the
    image is represented by a 1 x 1 tensor, which is a result of a linear layer at
    the end of the 4 hidden layers. The measurement of auxiliary information, which
    includes class fidelity and style fidelity, is obtained from two different groups
    of linear layers, in which the *128*2*2* feature maps are first mapped to 128-length
    vectors then mapped to output vectors with lengths of 10 and 2, respectively.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络由4个隐藏层组成。如前所述，InfoGAN使用了一个额外的分类器网络来衡量辅助向量的有效性。事实上，这个额外的分类器与判别器共享大部分权重（前4个隐藏层）。因此，图像的质量度量由一个1
    x 1的张量表示，这是4个隐藏层末尾线性层的结果。辅助信息的度量，包括类保真度和样式保真度，是通过两组不同的线性层得到的，其中*128*2*2*特征图首先映射到128长度的向量，然后分别映射到长度为10和2的输出向量。
- en: 'The definition of the discriminator in PyTorch code is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器在PyTorch代码中的定义如下：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, we treat the quality fidelity (`self.adv_loss`) as in an ordinary GAN
    model, the class fidelity (`self.class_loss`) as a classification problem (because
    label values are hard-coded, often in one-hot codes) and the style fidelity (`self.style_loss`)
    as an expectation maximization problem (because we want style vectors to obey
    certain random distribution). Therefore, cross-entropy (`torch.nn.CrossEntropyLoss`)
    and mean squared (`torch.nn.MSELoss`) loss functions are used for them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将质量保真度（`self.adv_loss`）视为普通GAN模型中的损失，类保真度（`self.class_loss`）视为分类问题（因为标签值是硬编码的，通常是独热编码），样式保真度（`self.style_loss`）则视为期望最大化问题（因为我们希望样式向量服从某种随机分布）。因此，交叉熵（`torch.nn.CrossEntropyLoss`）和均方误差（`torch.nn.MSELoss`）损失函数分别用于它们。
- en: We'd like explain why mean squared error is used for expectation maximization.
    We assume that the style vectors obey a normal distribution with a mean of 0 and
    a standard deviation of 1\. In the calculation of entropy, the logarithm of the
    probability of random variable is taken. The logarithm of **probability density
    function** (**pdf**) of the normal distribution ![](img/47a22187-f8e0-4124-a3e9-9f5ef9993390.png) is
    deduced to ![](img/1d1d3b79-700b-48b4-9935-48c9ffb365ed.png). Therefore, mean
    squared error is suited for such a purpose.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想解释为什么均方误差用于期望最大化。我们假设样式向量服从均值为0，标准差为1的正态分布。在计算熵时，取随机变量的概率的对数。正态分布的**概率密度函数**（**pdf**）的对数推导为![](img/47a22187-f8e0-4124-a3e9-9f5ef9993390.png)，因此，均方误差适用于此类目的。
- en: Training and evaluation of InfoGAN
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: InfoGAN的训练与评估
- en: We need to make some adjustments to the training API so that we can make use
    of the class and style vectors for attribute extraction and image generation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对训练API做一些调整，以便可以利用类向量和样式向量进行属性提取和图像生成。
- en: 'First, we add several imported modules in the `build_gan.py` file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在`build_gan.py`文件中添加几个导入的模块：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The default weight initialization provided by PyTorch easily leads to saturation,
    so we need a custom `weight` initializer:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供的默认权重初始化容易导致饱和，因此我们需要一个自定义的`weight`初始化器：
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s add the following lines in the definition of the `Model` class:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在`Model`类的定义中添加以下几行：
- en: '[PRE24]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And we need to change the definitions of `self.netG` and `self.netD`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要修改`self.netG`和`self.netD`的定义：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we add an optimizer for mutual information at the end of the `create_optim`
    method:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在`create_optim`方法的末尾添加一个用于互信息的优化器：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, we need to make some adjustments to `train` method, in which we first
    train the generator and discriminator networks and update the two networks again
    based on auxiliary information. Here, we omit all of the `if self.infogan` statements
    and only show the training procedure for InfoGAN. Full source code can be referred
    to in the code repository for this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对`train`方法进行一些调整，其中我们首先训练生成器和判别器网络，并基于辅助信息再次更新这两个网络。在这里，我们省略了所有的`if
    self.infogan`语句，只展示了InfoGAN的训练过程。完整的源代码可以参考本章的代码仓库。
- en: 'Initialize the fixed latent vectors for result visualization:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化固定的潜在向量以进行结果可视化：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, the `self._to_onehot` method is responsible for transforming the label
    values to one-hot coding:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`self._to_onehot`方法负责将标签值转换为one-hot编码：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A training iteration for InfoGAN includes the following:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN的一个训练迭代包括以下内容：
- en: Training the generator with fake data and letting the discriminator see them
    as real ones
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用伪数据训练生成器，并让判别器将它们视为真实数据
- en: Training the discriminator with both real and fake data to increase its ability
    to distinguish them
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用真实和伪数据训练判别器，以提高其区分能力
- en: 'Training both generator and discriminator so that the generator can produce
    samples with good quality based on given auxiliary information and the discriminator
    can tell whether the generated samples obey the distribution of given auxiliary
    information:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时训练生成器和判别器，以便生成器可以基于给定的辅助信息生成高质量的样本，而判别器可以判断生成的样本是否符合给定辅助信息的分布：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We don''t need to change anything in the `main.py` file at all, and we can simply
    run the following script in the Terminal:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完全不需要修改`main.py`文件，直接在终端运行以下脚本即可：
- en: '[PRE30]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It takes about 2 hours to finish 200 epochs of training and costs about 833
    MB GPU memory on a GTX 1080Ti graphics card. The results produced during training
    are shown here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 完成200个epoch的训练大约需要2小时，并且在GTX 1080Ti显卡上消耗约833MB的GPU内存。训练过程中生成的结果如下所示：
- en: '![](img/8fa92731-6e17-4dfe-83a9-b2aa5fd986bb.png)![](img/d9b0754d-2e98-42c8-b71e-f8e6c754a753.png)![](img/dae5ea62-3aff-4a3e-8c19-b32c49227292.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fa92731-6e17-4dfe-83a9-b2aa5fd986bb.png)![](img/d9b0754d-2e98-42c8-b71e-f8e6c754a753.png)![](img/dae5ea62-3aff-4a3e-8c19-b32c49227292.png)'
- en: 'Generated images from MNIST by the CGAN (left: 1st epoch; middle: 25th epoch;
    right: 200th epoch)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由CGAN生成的MNIST图像（左：第1个epoch；中：第25个epoch；右：第200个epoch）
- en: 'After the training is done, run the following script to perform the model evaluation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，运行以下脚本进行模型评估：
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Calling `model.eval()` with `mode=0` or `mode=1` will tell us what the two
    values in the style vector are responsible for, as shown here:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`model.eval()`并设置`mode=0`或`mode=1`将告诉我们风格向量中的两个值分别负责什么，如下所示：
- en: '![](img/7d1c962e-7cd5-43de-bf24-47691f80c5c6.png)![](img/25924586-1b46-4b84-b7b2-d4439f2f37a5.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d1c962e-7cd5-43de-bf24-47691f80c5c6.png)![](img/25924586-1b46-4b84-b7b2-d4439f2f37a5.png)'
- en: The first style bit (mode=0) controls the digit angle, and the second style
    bit (mode=1) controls the width of strokes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个风格位（mode=0）控制数字的角度，第二个风格位（mode=1）控制笔画的宽度。
- en: One of the style vector values is responsible for the angle of digits, and the
    other is responsible for the width of strokes, just as the original InfoGAN paper
    proclaims. Imagine what this technique is capable of on complex datasets and an
    elaborate training configuration.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 风格向量中的一个值负责数字的角度，另一个值负责笔画的宽度，就像原始的InfoGAN论文所述。想象一下，这项技术在复杂数据集和精心配置的训练下能够发挥什么作用。
- en: We can do a lot more with CGANs and similar. For example, the labels can be
    more than for images. An individual pixel in the image can certainly have its
    own label. In the next chapter, we will look into how GANs perform on pixel-wise
    labels and we can do interesting things, more than hand-written digits and human
    faces.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用CGAN及类似方法做更多的事情。例如，标签不仅仅限于图像，图像中的每个像素也可以有自己的标签。在下一章中，我们将探讨GAN在像素级标签上的表现，做一些更有趣的事情，不仅限于手写数字和人脸。
- en: References and useful reading list
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和有用的阅读书单
- en: Mirza M and Osindero S. (2014). *Conditional Generative Adversarial Nets*. arXiv
    preprint arXiv:1411.1784.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 米尔扎·M 和奥辛德罗·S。 (2014)。*条件生成对抗网络*。arXiv预印本arXiv:1411.1784。
- en: Hui J. (Jun 3, 2018). *GAN — CGAN & InfoGAN (using labels to improve GAN)*.
    Retrieved from [https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d](https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 会杰。 (2018年6月3日)。*GAN — CGAN与InfoGAN（使用标签提升GAN性能）*。取自[https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d](https://medium.com/@jonathan_hui/gan-cgan-infogan-using-labels-to-improve-gan-8ba4de5f9c3d)。
- en: Zhang Z and Song Y and Qi H. (2017). *Age Progression/Regression by Conditional
    Adversarial Autoencoder*. CVPR.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 张泽、宋颖和齐宏。 (2017)。*基于条件对抗自编码器的年龄进展/回归*。CVPR。
- en: 'Chen X, Duan Y, Houthooft R. (2016). *InfoGAN: Interpretable Representation
    Learning by Information Maximizing Generative Adversarial Nets*. arXiv preprint
    arXiv:1606.03657.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 陈欣、段宇、侯福特。 (2016)。*InfoGAN：通过信息最大化生成对抗网络进行可解释的表示学习*。arXiv预印本arXiv:1606.03657。
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discovered **Conditional Generative Adversarial Networks**
    (**CGANs**), which worked with MNIST and Fashion-MNIST, and we learned about using
    the InfoGAN model, which again, worked with MNIST.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们发现了**条件生成对抗网络**（**CGANs**），它可以与MNIST和Fashion-MNIST数据集一起使用，我们还了解了使用InfoGAN模型，它同样可以与MNIST数据集一起使用。
- en: In our next chapter, we will learn about image-to-image translation, which I
    truly believe you will find exciting and very relevant in today's world.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下一章中，我们将学习图像到图像的翻译技术，我相信你一定会觉得这非常令人兴奋，并且在今天的世界中非常相关。
