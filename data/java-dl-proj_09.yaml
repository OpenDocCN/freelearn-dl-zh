- en: Playing GridWorld Game Using Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度强化学习玩 GridWorld 游戏
- en: As human beings, we learn from experiences. We have not become so charming overnight
    or by accident. Years of compliments as well as criticism have all helped shape
    who we are today. We learn how to ride a bike by trying out different muscle movements
    until it just clicks. When you perform actions, you are sometimes rewarded immediately,
    and this is known as **reinforcement learning** **(****RL****).**
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们通过经验学习。我们并非一夜之间或偶然变得如此迷人。多年的赞美与批评都帮助塑造了今天的我们。我们通过反复尝试不同的肌肉运动，直到学会骑自行车。当你执行动作时，有时会立刻获得奖励，这就是
    **强化学习**（**RL**）。
- en: 'This chapter is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to develop a demo GridWorld game using **Deeplearning4j**
    (**DL4J**), **reinforcement learning 4j** (**RL4J**), and Neural Q-learning that
    acts as the Q function. We will start from reinforcement learning and its theoretical
    background so that the concept is easier to grasp. In summary, the following topics
    will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于设计一个由批评和奖励驱动的机器学习系统。我们将看到如何使用 **Deeplearning4j**（**DL4J**）、**reinforcement
    learning 4j**（**RL4J**）和作为 Q 函数的神经网络 Q 学习开发一个示范性 GridWorld 游戏。我们将从强化学习及其理论背景开始，以便更容易理解这个概念。简而言之，本章将涵盖以下主题：
- en: Notation, policy, and utility in reinforcement learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习中的符号表示、策略和效用
- en: Deep Q-learning algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 Q 学习算法
- en: Developing a GridWorld game using deep Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度 Q 学习开发 GridWorld 游戏
- en: Frequently asked questions (FAQs)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）
- en: Notation, policy, and utility for RL
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的符号表示、策略和效用
- en: Whereas supervised and unsupervised learning appear at opposite ends of the
    spectrum, RL exists somewhere in the middle. It is not supervised learning, because
    the training data comes from the algorithm deciding between exploration and exploitation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习看似位于两个极端，而强化学习则介于二者之间。它不是监督学习，因为训练数据来自于算法在探索与利用之间做出的决策。
- en: In addition, it is not unsupervised, because the algorithm receives feedback
    from the environment. As long as you are in a situation where performing an action
    in a state produces a reward, you can use reinforcement learning to discover a
    good sequence of actions to take the maximum expected rewards. The goal of an
    RL agent will be to maximize the total reward that it receives in the end. The
    third main sub-element is the value function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它也不是无监督学习，因为算法会从环境中获取反馈。只要你处于一个执行某个动作后能够获得奖励的状态，你就可以使用强化学习来发现一系列能够获得最大期望奖励的动作。强化学习代理的目标是最大化最终获得的总奖励。第三个主要子元素是价值函数。
- en: While rewards determine an immediate desirability of states, values indicate
    the long-term desirability of states, taking into account the states that may
    follow and the available rewards in those states. The value function is specified
    with respect to the chosen policy. During the learning phase, an agent tries actions
    that determine the states with the highest value, because these actions will get
    the best amount of reward in the end.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励决定了状态的即时可取性，而价值则表示了状态的长期可取性，考虑到可能跟随的状态和这些状态中的可用奖励。价值函数是相对于所选策略来指定的。在学习阶段，代理会尝试那些能够确定具有最高价值状态的动作，因为这些动作最终会带来最好的奖励。
- en: Reinforcement learning techniques are being used in many areas. A general idea
    that is being pursued right now is creating an algorithm that does not need anything
    apart from a description of its task. When this kind of performance is achieved,
    it will be applied virtually everywhere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习技术已经被应用到许多领域。目前正在追求的一个总体目标是创建一个只需要任务描述的算法。当这种表现得以实现时，它将被广泛应用于各个领域。
- en: Notations in reinforcement learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的符号表示
- en: You may notice that reinforcement learning jargon involves incarnating the algorithm
    into taking actions in situations to receive rewards. In fact, the algorithm is
    often referred to as an agent that acts with the environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，强化学习的术语涉及将算法化身为在特定情境中采取动作以获取奖励。事实上，算法通常被称为一个与环境互动的代理。
- en: 'You can just think it is an intelligent hardware agent sensing with sensors
    and interact with the environment using its actuators. Therefore, it should not
    be a surprise that much of RL theory is applied in robotics. Now, to prolong our
    discussion further, we need to know a few terminologies:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其视为一个通过传感器感知并通过执行器与环境互动的智能硬件代理。因此，强化学习理论在机器人技术中的广泛应用也就不足为奇了。现在，为了进一步展开讨论，我们需要了解一些术语：
- en: '**Environment**: This is the system having multiple states and mechanisms to
    transition in between states. For example, for a GridWorld game playing agent''s
    the environment is the grid space itself that defines the states and the way the
    agent gets rewarded to reach the goal.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：这是一个具有多个状态和在状态之间转换机制的系统。例如，在 GridWorld 游戏中，代理的环境就是网格空间本身，定义了状态以及代理如何通过奖励到达目标。'
- en: '**Agent**: This is an autonomus system that interacts with the environment.
    For example, in our GridWorld game, an agent is the player.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：这是一个与环境互动的自主系统。例如，在我们的 GridWorld 游戏中，代理就是玩家。'
- en: '**State**: A state in an environment is a set of variables that fully describe
    the environment.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：环境中的状态是一组完全描述环境的变量。'
- en: '**Goal**: It is also a state, which provides a higher discounted cumulative
    reward than any other state. For our GridWorld game, the goal is the state where
    the player wants to reach ultimately, but by accumulating the highest possible
    rewards.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：它也是一个状态，提供比任何其他状态更高的折扣累计奖励。在我们的 GridWorld 游戏中，目标状态是玩家最终希望达到的状态，但通过积累尽可能高的奖励。'
- en: '**Action**: Actions define the transition between different states. Thus, upon
    execution of an action, an agent can be rewarded or punished from the environment.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：动作定义了不同状态之间的转换。因此，在执行一个动作后，代理可以从环境中获得奖励或惩罚。'
- en: '**Policy**: This defines a set of rules based on actions to be performed and
    executed for a given state in the environment.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：它定义了一组基于动作的规则，用于在给定状态下执行和实施动作。'
- en: '**Reward**: This is a positive or negative quantity (that is score) for good
    and bad action/move respectively. Ultimately, the learning goal is reaching the
    goal with maximum score (reward). This way, rewards are essentially the training
    set for an agent.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励**：这是对好坏动作/移动的正负量度（即得分）。最终，学习的目标是通过最大化得分（奖励）来达到目标。因此，奖励本质上是训练代理的训练集。'
- en: '**Episode (also known as trials)**: This is the number of steps necessary to
    reach the goal state from the initial state (that is, position of an agent).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回合（也称为试验）**：这是从初始状态（即代理的位置）到达目标状态所需的步骤数。'
- en: 'We will discuss more on policy and utility later in this section. The following
    diagram demonstrates the interplay between states, actions, and rewards. If you
    start at state *s[1]*, you can perform action *a[1]* to obtain a reward *r (s[1],
    a[1])*. Arrows represent actions, and states are represented by circles:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节稍后讨论更多关于策略和效用的内容。下图展示了状态、动作和奖励之间的相互作用。如果你从状态 *s[1]* 开始，可以执行动作 *a[1]* 来获得奖励
    *r (s[1], a[1])*。箭头表示动作，状态由圆圈表示：
- en: '![](img/9ff64270-027c-4839-b06b-01795d189ff3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ff64270-027c-4839-b06b-01795d189ff3.png)'
- en: When an agent performs an action, the state produces a reward
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理执行一个动作时，状态会产生奖励。
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it is all about using a different or a concrete
    policy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人执行动作以在不同状态之间转换。但它如何决定采取哪种动作呢？实际上，这完全依赖于使用不同的或具体的策略。
- en: Policy
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 政策
- en: 'In reinforcement learning, a policy is a set of rules or a strategy. Therefore,
    one of the learning outcomes is to discover a good strategy that observes the
    long-term consequences of actions in each state. So, technically, a policy defines
    an action to be taken in a given state. The following diagram shows the optimal
    action given any state:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，策略是一组规则或一种战略。因此，学习的一个目标是发现一种良好的策略，能够观察到每个状态下动作的长期后果。所以，从技术上讲，策略定义了在给定状态下要采取的行动。下图展示了在任何状态下的最优动作：
- en: '![](img/2762d760-cfcf-4ac5-9d6b-fba6784b50fd.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2762d760-cfcf-4ac5-9d6b-fba6784b50fd.png)'
- en: A policy defines an action to be taken in a given state
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 策略定义了在给定状态下要采取的动作。
- en: 'The short-term consequence is easy to calculate:It is just the reward. Although
    performing an action yields an immediate reward, it is not always a good idea
    to choose the action greedily with the best reward. There may be different types
    of policies dependeing upon your RL problem formulation, as outlined here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 短期后果很容易计算：它只是奖励。尽管执行一个动作会得到即时奖励，但贪心地选择最大奖励的动作并不总是最好的选择。根据你的强化学习问题的定义，可能会有不同类型的策略，如下所述：
- en: When an agent always try to achieve the highest immediate reward by performing
    an action, we call this **greedy policy**.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个代理总是通过执行某个动作来追求最高的即时奖励时，我们称之为**贪心策略**。
- en: If an action is performed arbitrarily, the policy is called **random policy**.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个动作是任意执行的，则该策略称为**随机策略**。
- en: When a neural network learns a policy for picking actions by updating weights
    through backpropopagation and explicit feedback from the environment, we call
    this **policy gradients**.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当神经网络通过反向传播和来自环境的明确反馈更新权重来学习选择动作的策略时，我们称之为**策略梯度**。
- en: If we want to come up with a robust a policy to solve an RL problem, we have
    to find the optimal one that performs better than that of random and greedy policies.
    In this chapter, we will see why policy gradient is more direct and optimistic.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要制定一个健壮的策略来解决强化学习问题，我们必须找到一个在表现上优于随机策略和贪心策略的最优策略。在这一章中，我们将看到为什么策略梯度更加直接和乐观。
- en: Utility
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 效用
- en: 'The long-term reward is the **utility**. To decide which action to take, an
    agent can the action that produces the highest utility in a greedy way. The utility
    of performing an action *a* at a state *s* is written as a function *Q(s, a)*,
    called the utility function. The utility function predicts the immediate and final
    rewards based on an optimal policy generated by the input consisting of state
    and action, as shown in the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 长期奖励是**效用**。为了决定采取什么动作，代理可以选择产生最高效用的动作，并以贪心方式进行选择。执行一个动作 *a* 在状态 *s* 时的效用表示为函数
    *Q(s, a)*，称为效用函数。效用函数根据由状态和动作组成的输入，预测即时奖励和最终奖励，正如以下图所示：
- en: '![](img/b01b6793-f371-4a12-96a4-7a9f74c661e4.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b01b6793-f371-4a12-96a4-7a9f74c661e4.png)'
- en: Using a utility function
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用效用函数
- en: Neural Q-learning
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络Q学习
- en: 'Most reinforcement learning algorithms boil down to just three main steps:
    infer, do, and learn. During the first step, the algorithm selects the best action
    *a* in a given state *s* using the knowledge it has so far. Next, it performs
    an action to find the reward *r* as well as the next state *s''*.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习算法可以归结为三个主要步骤：推断、执行和学习。在第一步中，算法使用到目前为止所获得的知识，从给定的状态 *s* 中选择最佳动作 *a*。接下来，它执行该动作，以找到奖励
    *r* 和下一个状态 *s'*。
- en: Then it improves its understanding of the world using the newly acquired knowledge
    *(s, r, a, s')*. These steps can be formulated even better using QLearning algorithms,
    which is more or less at the core of Deep Reinforcement Learning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用新获得的知识 *(s, r, a, s')* 来改进对世界的理解。这些步骤甚至可以通过Q学习算法进行更好的公式化，Q学习算法或多或少是深度强化学习的核心。
- en: Introduction to QLearning
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q学习简介
- en: 'Computing the acquired knowledge using *(s, r, a, s'')* is just a naive way
    to calculate the utility. So, we need to find a more robust way to compute it
    in such that we calculate the utility of a particular state-action pair *(s, a)*
    by recursively considering the utilities of future actions. The utility of your
    current action is influenced by not only the immediate reward but also the next
    best action, as shown in the following formula, called **Q-function**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *(s, r, a, s')* 计算获得的知识只是计算效用的一种简单方法。因此，我们需要找到一种更健壮的方式来计算它，使得我们通过递归地考虑未来动作的效用来计算特定状态-动作对
    *(s, a)* 的效用。当前动作的效用不仅受到即时奖励的影响，还受到下一个最佳动作的影响，如下式所示，这称为**Q函数**：
- en: '![](img/54fe0db0-e6ab-426a-a24a-fbe697d49bcf.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54fe0db0-e6ab-426a-a24a-fbe697d49bcf.png)'
- en: In the previous formula, *s'* denotes the next state, *a'* denotes the next
    action, and the reward of taking action *a* in state *s* is denoted by *r(s, a).*
    Whereas*,* γ is a hyperparameter called the **discount factor**. If *γ* is *0*,
    then the agent chooses a particular action that maximizes the immediate reward.
    Higher values of *γ* will make the agent put more importance on considering long-term
    consequences.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*s'* 表示下一个状态，*a'* 表示下一个动作，执行动作 *a* 在状态 *s* 时的奖励表示为 *r(s, a)*。其中，*γ*
    是一个超参数，称为**折扣因子**。如果 *γ* 为 *0*，则代理选择一个特定的动作，最大化即时奖励。较高的 *γ* 值将使代理更重视考虑长期后果。
- en: In practice, we have more such hyperparameters to be considered. For example,
    if a vacuum cleaner robot is expected to learn to solve tasks quickly but not
    necessarily optimally, we may want to set a faster learning rate.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们需要考虑更多这样的超参数。例如，如果期望吸尘机器人快速学习解决任务，但不一定要求最优解，那么我们可能会设置一个更高的学习速率。
- en: 'Alternatively, if a robot is allowed more time to explore and exploit, we might
    tune down the learning rate. Let us call the learning rate *α*, and change our
    utility function as follows (note that when *α = 1*, both the equations are identical):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果允许机器人有更多的时间去探索和利用，我们可能会降低学习速率。我们将学习速率称为*α*，并将我们的效用函数更改如下（请注意，当*α = 1*时，这两个方程是相同的）：
- en: '![](img/52146aac-6bcb-412c-b6f9-b5d38208f74a.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52146aac-6bcb-412c-b6f9-b5d38208f74a.jpg)'
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    This motivates researchers to propose a more advanced **QLearning** algorithm
    called **neural QLearning**, which is a type of algorithm used to calculate state-action
    values. It falls under the class of **temporal difference** (**TD**) algorithms,
    which suggests that time differences between actions taken and rewards received
    are involved.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，一个RL问题可以通过了解这个*Q(s, a)*函数来解决。这促使研究人员提出了一种更先进的**QLearning**算法，称为**神经Q学习**，它是一种用于计算状态-动作值的算法，属于**时序差分**（**TD**）算法类别，意味着它涉及到动作执行和奖励获得之间的时间差异。
- en: Neural networks as a Q-function
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络作为Q函数
- en: 'Now we know the state and the action to perform. However, the `QLearning` agent
    needs to know the search space of the form (states x actions). The next step consists
    of creating the graph or search space, which is the container responsible for
    any sequence of states. The `QLSpace` class defines the search space (states x
    actions) for the `QLearning` algorithm, as shown in the following diagram:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了状态和需要执行的动作。然而，`QLearning`智能体需要了解形如（状态 x 动作）的搜索空间。下一步是创建图形或搜索空间，它是负责任何状态序列的容器。`QLSpace`类定义了`QLearning`算法的搜索空间（状态
    x 动作），如下面的图所示：
- en: '![](img/20fae0f1-19b4-47ff-a3f5-b8a228aec422.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20fae0f1-19b4-47ff-a3f5-b8a228aec422.png)'
- en: State transition matrix with QLData (Q-value, reward, probability)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移矩阵与QLData（Q值、奖励、概率）
- en: 'The end user with a list of states and actions can provide the search space.
    Alternatively, it is automatically created by providing the number of states,
    by taking the following parameters:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有状态和动作列表的最终用户可以提供搜索空间。或者，它可以通过提供状态数来自动创建，具体通过以下参数：
- en: '**States**: The sequence of all possible states defined in the Q-learning search
    space'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：Q学习搜索空间中定义的所有可能状态的序列'
- en: '**Goals**: A list of identifiers of states that are goals'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：一系列表示目标状态的标识符'
- en: However, classical notation of such a search space (or lookup table) is sometimes
    not efficient; as in most interesting problems, our state-action space is much
    too large to store in a table, for example, the *Pac-Man* game. Rather we need
    to generalize and pattern-match between states anyway. In other words, we need
    our Q-learning algorithm to say, *The value of this kind of state is X* instead
    of saying, *the value of this exact, super-specific state is X*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，传统的这种搜索空间（或查找表）表示方式有时效率不高；因为在大多数有趣的问题中，我们的状态-动作空间太大，无法存储在表中，例如*吃豆人*游戏。相反，我们需要进行泛化，并在状态之间进行模式匹配。换句话说，我们需要我们的Q学习算法能够说，*这种状态的值是X*，而不是说，*这个特定、超具体的状态的值是X*。
- en: Here neural-network-based Q-learning can be used instead of a lookup table as
    our *Q*(*s*, *a*) such that it accepts a state *s* and an action *a* and spits
    out the value of that state-action. However, as I alluded to earlier, an NN sometimes
    has millions of parameters associated with it. These are the weights. Therefore,
    our *Q* function actually looks like *Q*(*s*, *a*, *θ*), where *θ* is a vector
    of parameters.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里可以使用基于神经网络的Q学习，而不是查找表作为我们的*Q*(*s*, *a*)，它接受状态*s*和动作*a*，并输出该状态-动作对的值。然而，正如我之前提到的，神经网络有时包含数百万个与之相关的参数，这些参数就是权重。因此，我们的*Q*函数实际上看起来像*Q*(*s*,
    *a*, *θ*)，其中*θ*是一个参数向量。
- en: Instead of iteratively updating values in a table, we will iteratively update
    the *θ* parameters of our neural network so that it learns to provide us with
    better estimates of state-action values. By the way, we can use gradient descent
    (backpropagation) to train such a deep Q-learning network just like any other
    neural networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过反复更新神经网络的*θ*参数来代替反复更新表格中的值，从而使其学会为我们提供更好的状态-动作值估计。顺便提一下，我们可以像训练其他神经网络一样，使用梯度下降（反向传播）来训练这样的深度Q学习网络。
- en: For example, if the state (search space) is represented by an image, a neural
    network can rank the possible actions made by the agent such that it can predict
    the possible reward. For example, running left returns five points, jumping up
    returns seven, and jumping down returns two points, but running left returns none.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果状态（搜索空间）通过图像表示，神经网络可以对智能体的可能动作进行排名，从而预测可能的奖励。例如，向左跑返回五分，向上跳返回七分，向下跳返回两分，而向左跑则不返回任何奖励。
- en: '![](img/2dd9f59b-8fc8-4f23-a414-c19e938a5bc4.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dd9f59b-8fc8-4f23-a414-c19e938a5bc4.png)'
- en: Using a neural network for reinforcement learning-based gaming
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行基于强化学习的游戏
- en: To make this happen, instead of having to run our network forward for every
    action, we can run it forward once we just need to get the *max Q*(*s*′,*a*′),
    that is, *max Q* values for every possible action in the new state *s'*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们不需要为每个动作都运行网络，而是只需在我们需要获取*max Q*（*s*′，*a*′）时运行它，也就是在新状态*s'*下对每个可能的动作获取*max
    Q*值。
- en: We will see how to create a deep Q-learning network like this with `MultiLayerNetwork`
    and the `MultiLayerConfiguration` configuration of DL4J. Therefore, the neural
    network will serve as our Q*-*function. Now that we have minimal theoretical knowing
    about RL and Q-learning, it is time to get to coding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何使用`MultiLayerNetwork`和DL4J的`MultiLayerConfiguration`配置创建这样的深度Q学习网络。因此，神经网络将充当我们的Q*-*函数。现在，我们已经对强化学习（RL）和Q学习有了最基本的理论了解，是时候开始编写代码了。
- en: Developing a GridWorld game using a deep Q-network
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度Q网络开发GridWorld游戏
- en: We will now start diving into **Deep Q-Network** (**DQN**) to train an agent
    to play GridWorld, which is a simple text-based game. There is a 4 x 4 grid of
    tiles and four objects are placed. There is an agent (a player), a pit, a goal,
    and a wall.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入了解**深度Q网络**（**DQN**），以训练一个智能体玩GridWorld，这是一个简单的基于文本的游戏。游戏中有一个4x4的方格，放置了四个物体：一个智能体（玩家）、一个陷阱、一个目标和一堵墙。
- en: '![](img/109cb766-7ba7-4c1d-b8b6-4d53282de1e5.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/109cb766-7ba7-4c1d-b8b6-4d53282de1e5.png)'
- en: GridWorld project structure
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GridWorld项目结构
- en: 'The project has the following structure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 项目具有以下结构：
- en: '`DeepQNetwork.java`: Provides the reference architecture for the DQN'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DeepQNetwork.java`：提供DQN的参考架构'
- en: '`Replay.java`: Generates replay memory for the DQN to ensure that the gradients
    of the deep network are stable and do not diverge across episodes'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Replay.java`：生成DQN的重放记忆，确保深度网络的梯度稳定，不会在多个回合中发散'
- en: '`GridWorld.java`: The main class used for training the DQN and playing the
    game.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GridWorld.java`：用于训练DQN和玩游戏的主类。'
- en: By the way, we perform the training on GPU and cuDNN for faster convergence.
    However, feel free to use the CPU backend as well if your machine does not have
    a GPU.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，我们在GPU和cuDNN上执行训练，以加快收敛速度。如果您的机器没有GPU，您也可以使用CPU后端。
- en: Generating the grid
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成网格
- en: 'We will be developing a simple game by initializing a grid in exactly the same
    way each time. The game starts with the agent (*A*), goal (*+*), pit (-), and
    wall (*W*). All elements are randomly placed on the grid in each game. This is
    such that the Q-learning just needs to learn how to move the agent from a known
    starting position to a known goal without hitting the pit, which gives negative
    rewards. Take a look at this screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个简单的游戏，每次初始化一个完全相同的网格。游戏从智能体（*A*）、目标（*+*）、陷阱（-）和墙（*W*）开始。每场游戏中，所有元素都被随机放置在网格上。这样，Q学习只需要学习如何将智能体从已知的起始位置移动到已知的目标位置，而不碰到陷阱（这会带来负面奖励）。请看这张截图：
- en: '![](img/a59c6f73-11ff-4199-8fd7-25476b7e85b0.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a59c6f73-11ff-4199-8fd7-25476b7e85b0.png)'
- en: A grid for the GridWorld game showing the elements (that is, agent, goal, pit,
    and wall)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显示游戏元素（即智能体、目标、陷阱和墙）的GridWorld游戏网格
- en: In short, the target of the game is to reach the goal, where the agent will
    receive a numerical reward. For simplicity, we will avoid a pit; if the agent
    lands on the pit, it gets penalized with a negative reward.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，游戏的目标是到达目标点，在那里智能体会获得一个数值奖励。为了简化，我们将避免陷阱；如果智能体落入陷阱，它将被处罚，获得负奖励。
- en: 'The wall can block the agent''s path too, but it offers no reward or penalty,
    so we''re safe. Since this is a simple way of defining the state, the agent can
    make the following moves (that is, actions):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 墙壁也能阻挡代理人的路径，但它不会提供奖励或惩罚，所以我们可以放心。由于这是定义状态的一种简单方式，代理人可以执行以下动作（即，行为）：
- en: Up
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向上
- en: Down
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向下
- en: Left
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向左
- en: Right
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向右
- en: 'This way, an action *a* can be defined as follows: `a ∈ A {up, down, left,
    right}`. Now let''s see, based on the preceding assumption, how the grid would
    look:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，动作 *a* 可以定义为如下：`a ∈ A {up, down, left, right}`。现在让我们看看，基于前面的假设，网格会是什么样子的：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the grid is constructed, it can be printed as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网格构建完成，可以按如下方式打印出来：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Calculating agent and goal positions
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算代理人和目标位置
- en: 'Now the search space for the agent is ready. So let''s calculate the initial
    position of the agent and the goal. First, we compute the initial position of
    the agent in the grid, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代理人的搜索空间已经准备好。接下来，让我们计算代理人和目标的初始位置。首先，我们计算代理人在网格中的初始位置，如下所示：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we calculate the position of the goal, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算目标的位置，如下所示：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now the generated grid can be considered as four separate grid planes, where
    each plane represents the position of each element. In the following diagram,
    the agent''s current grid position is (3, 0), the wall is at (0, 0), the pit is
    at (0, 1), and the goal is at (1, 0), which also means that all other elements
    are 0s:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，生成的网格可以视为四个独立的网格平面，每个平面代表每个元素的位置。在下图中，代理人当前的网格位置是 (3, 0)，墙壁位于 (0, 0)，陷阱位于
    (0, 1)，目标位于 (1, 0)，这也意味着所有其他元素为0：
- en: '![](img/826ed961-0272-49c2-9a9c-d8bd9a35bd10.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/826ed961-0272-49c2-9a9c-d8bd9a35bd10.png)'
- en: A generated grid can be considered as four separate grid planes
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的网格可以视为四个独立的网格平面
- en: Thus, we developed the grid such that some of the objects contain a *1* at the
    same *x*, *y* position (but different *z* positions), which indicates they're
    at the same position on the grid.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们设计了网格，使得某些物体在相同的 *x*，*y* 位置（但不同的 *z* 位置）包含一个 *1*，这表示它们在网格上的位置相同。
- en: Calculating the action mask
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算动作掩码
- en: 'Here, we set all the outputs to 0, except the one for the action we actually
    saw, such that the network multiplies its outputs by a mask corresponding to the
    one-hot encoded action. We can then pass 0 as the target for all unknown actions,
    and our neural network should thus perform fine. When we want to predict all actions,
    we can simply pass a mask of all 1s:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将所有输出设置为0，除了我们实际看到的动作对应的输出，这样网络就可以根据与独热编码动作对应的掩码来乘以输出。然后我们可以将0作为所有未知动作的目标，这样我们的神经网络应该能够很好地执行。当我们想要预测所有动作时，可以简单地传递一个全为1的掩码：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Providing guidance action
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供指导动作
- en: 'Now the agent''s action plan is known. The next task is providing some guidance
    to the agent moving from the current position towards the goal. For example, not
    all the action is accurate, that is, a bad move:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在代理人的行动计划已经确定。接下来的任务是为代理人提供一些指导，使其从当前的位置朝着目标前进。例如，并非所有的动作都是准确的，也就是说，某些动作可能是无效的：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the previous code block, we encoded the action as follows: `0` is up, `1`
    is down, `2` is left, and `3` is right. Otherwise, we treat the action as a bad
    move, and so the agent gets penalized.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将动作编码为如下：`0`代表向上，`1`代表向下，`2`代表向左，`3`代表向右。否则，我们将该动作视为无效操作，代理人将受到惩罚。
- en: Calculating the reward
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算奖励
- en: 'Now that the agent is provided with some guidance—reinforcement—the next task
    is to calculate the reward for each action the agent makes. Take a look at this
    code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代理人已经获得了一些指导——强化信号——接下来的任务是计算代理人执行每个动作的奖励。看看这段代码：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Flattening input for the input layer
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为输入层展平输入
- en: 'Then we need to convert the output of the network into a 1D-feature vector,
    to be used by the DQN. This flattening gets the output of the network; it flattens
    all its structure to create a single long-feature vector to be used by the dense
    layer. Take a look at this code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要将网络的输出转换为一个1D特征向量，供DQN使用。这个展平过程获取了网络的输出；它将所有结构展平，形成一个单一的长特征向量，供全连接层使用。看看这段代码：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Up to this point, we just created the logical skeleton for the `GridWorld`.
    Thus, we create the `DQN` before we start playing the game.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅创建了 `GridWorld` 的逻辑框架。因此，我们在开始游戏之前创建了 `DQN`。
- en: Network construction and training
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络构建与训练
- en: 'As I stated, we will create a DQN network using `MultiLayerNetwork` and the
    `MultiLayerConfiguration` configuration of DL4J, which will serve as our Q-function.
    Therefore, the first step is to create a `MultiLayerNetwork` by defining `MultiLayerConfiguration`.
    Since the state has 64 elements—4 x 4 x 4—our network has to have an input layer
    of 64 units, two hidden layers of 164 and 150 units each, and an output layer
    of 4, for four possible actions (up, down, left, and right). This is outlined
    here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，我们将使用 `MultiLayerNetwork` 和 DL4J 的 `MultiLayerConfiguration` 配置创建一个 DQN
    网络，它将作为我们的 Q 函数。因此，第一步是通过定义 `MultiLayerConfiguration` 创建一个 `MultiLayerNetwork`。由于状态有
    64 个元素—4 x 4 x 4—我们的网络需要一个包含 64 个单元的输入层，两个隐藏层，分别有 164 和 150 个单元，以及一个包含 4 个单元的输出层，用于四种可能的动作（上、下、左、右）。具体如下：
- en: '![](img/e4134ff8-c90a-4cbb-947b-d7b4efa6c89d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4134ff8-c90a-4cbb-947b-d7b4efa6c89d.png)'
- en: The structure of the DQN network, showing an input layer, two hidden layers,
    and an output layer
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 网络的结构，显示输入层、两个隐藏层和输出层
- en: 'Nevertheless, we will be using experience replay memory for training our DQN,
    which will help us store the transitions observed by the agent. This will allow
    the DQN to reuse this data later. By sampling from it randomly, the transitions
    that build up a batch are de-correlated. It has been shown that this greatly stabilizes
    and improves the DQN training procedure. Following the preceding config, the following
    code can be used to create such a `MultiLayerConfiguration`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将使用经验回放内存来训练我们的 DQN，它将帮助我们存储智能体观察到的转换。这将允许 DQN 在后续使用这些数据。通过从中随机采样，构建一个批次的转换可以去相关。研究表明，这大大稳定并改善了
    DQN 的训练过程。按照前述配置，以下代码可用于创建这样的 `MultiLayerConfiguration`：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we use this configuration to create a DQN:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用这个配置创建一个 DQN：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will discuss the parameters shortly, but, before that, we''ll look at how
    to create such a deep architecture. First, we define some parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会讨论参数，但在此之前，我们先看看如何创建这样一个深度架构。首先，我们定义一些参数：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we define the constructor to initialize these parameters:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义构造函数来初始化这些参数：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following is the implementation for the main loop of the algorithm:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该算法主循环的实现：
- en: We set up a `for` loop to the number of episodes while the game is in progress.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了一个 `for` 循环，直到游戏进行完毕。
- en: We run the Q-network forward.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行 Q 网络的前向传播。
- en: We use an epsilon-greedy implementation, so at time *t* with probability *ϵ,*
    the *agent* chooses a random action. However, with probability 1−*ϵ,* the action
    associated with the highest Q-value from our neural network is performed.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用的是 epsilon-贪心策略，因此在时间 *t* 时，以 *ϵ* 的概率，*智能体* 选择一个随机行动。然而，以 1−*ϵ* 的概率，执行来自我们神经网络的最大
    Q 值对应的行动。
- en: Then the agent takes an action *a*, which is determined in the previous step;
    we observe a new state *s*′ and reward *r[t]*[+1].
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，智能体采取一个行动 *a*，该行动在前一步骤中已确定；我们观察到一个新的状态 *s*′ 和奖励 *r[t]*[+1]。
- en: Then the Q-network forward pass is executed using *s*′, and the highest Q-value
    (maxQ) is stored.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 *s*′ 执行 Q 网络的前向传播，并存储最高的 Q 值（maxQ）。
- en: The agent's target value is then computed as reward + (gamma * maxQ) to train
    the network, where gamma is a parameter (0<=*γ*<=1).
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，计算智能体的目标值作为奖励 + (gamma * maxQ)，用于训练网络，其中 gamma 是一个参数（0<=*γ*<=1）。
- en: We aim to update the output associated with the action we just took for four
    possible outputs. Here, the agent's target output vector is the same as the output
    vector from the first execution, except the one output associated with an action
    to *reward + (gamma * maxQ)*.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的目标是更新与我们刚刚采取的行动相关联的四种可能输出的输出。在这里，智能体的目标输出向量与第一次执行时的输出向量相同，唯一不同的是与行动相关联的输出
    *奖励 + (gamma * maxQ)*。
- en: 'The preceding steps are for one episode, and then the loop iterates for the
    defined episode by the user. In addition, the grid is first constructed, and then
    the next reward for each move is computed and saved. In short, the preceding steps
    can be represented as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤是针对一个回合的，然后循环会根据用户定义的回合数进行迭代。此外，首先构建网格，然后计算并保存每个动作的下一个奖励。简而言之，上述步骤可以表示如下：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the preceding code block, network computes the observed reward for each
    mini batch of flattened input data. Take a look at this:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，网络计算每个迷你批次的扁平化输入数据所观察到的奖励。请看一下：
- en: '[PRE13]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding reward is calculated to estimate the optimal future value:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 上述奖励被计算出来，用于估算最优的未来值：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code block, the future reward is computed by taking the maximum
    value of the neural network output. Take a look at this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，通过取神经网络输出的最大值来计算未来的奖励。来看一下这个：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As stated before, the observed reward is computed once the network training
    starts. The combined input is computed as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，观察到的奖励是在网络训练开始后计算的。组合输入的计算方式如下：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then the network needs to compute the combined input for the next pass. Take
    a look at this code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，网络需要计算下一次传递的组合输入。来看一下这段代码：
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the previous code blocks, the map at each time step is saved with the `addToBuffer()`
    method, which is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码块中，每个时间步的地图通过`addToBuffer()`方法保存，如下所示：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then the DQNet takes the flattening input into batches for each episode, and
    the training starts. Then the current and target outputs by maximizing the reward
    are computed based on the current and target inputs. Take a look at this code
    block:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，DQNet将输入展平后以批量的方式输入每一回合，开始训练。然后根据当前输入和目标输入，通过最大化奖励来计算当前和目标输出。来看一下这个代码块：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code block, future rewards are computed by maximizing the
    value of the neural network output, as shown here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，通过最大化神经网络输出的值来计算未来的奖励，如下所示：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As I stated earlier, this is very simple game, and if the agent takes action
    2 (that is, left), one step results in reaching the goal. Therefore, we just keep
    all other outputs the same as before and change the one for the action we took.
    So, implementing experience replay is a better idea, which gives us mini-batch
    updating in an online learning scheme.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前所说，这是一个非常简单的游戏，如果智能体采取动作2（即，向左），一步就能到达目标。因此，我们只需保持所有其他输出与之前相同，改变我们所采取动作的输出。所以，实现经验回放是一个更好的主意，它在在线学习方案中给我们提供了小批量更新。
- en: 'It works such that we run the agent to collect enough transitions to fill up
    the replay memory, without training. For example, our memory may be of size 10,000\.
    Then, at every step, the agent will obtain a transition; we''ll add this to the
    end of the memory and pop off the earliest one. Take a look at this code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式是我们运行智能体收集足够的过渡数据来填充回放记忆，而不进行训练。例如，我们的记忆大小可能为10,000。然后，在每一步，智能体将获得一个过渡；我们会将其添加到记忆的末尾，并弹出最早的一个。来看一下这段代码：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, sample a mini batch of experiences from the memory randomly, and update
    our Q-function on that, similar to mini-batch gradient descent. Take a look at
    this code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从记忆中随机抽取一个小批量的经验，并在其上更新我们的Q函数，类似于小批量梯度下降。来看一下这段代码：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Playing the GridWorld game
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩GridWorld游戏
- en: 'For this project, I haven''t used any visualization to demonstrate the states
    and actions. Rather it is a text-based game, as I alluded to earlier. Then you
    can run the `GridWorld.java` class (containing the main method) using following
    invocation:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我没有使用任何可视化来展示状态和动作，而是采用了一种基于文本的游戏，正如我之前提到的那样。然后你可以运行`GridWorld.java`类（包含主方法），使用以下方式调用：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In this invocation, here''s the parameter description outlined:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个调用中，以下是参数描述：
- en: '`conf`: This is the `MultiLayerConfiguration` used to create the DQN'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf`：这是用于创建DQN的`MultiLayerConfiguration`'
- en: '`100000`: This is the replay memory capacity'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`100000`：这是回放记忆的容量。'
- en: '`.99f`: The discount'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.99f`：折扣因子'
- en: '`1d`: This is the epsilon'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1d`：这是epsilon'
- en: '`1024`: The batch size'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1024`：批量大小'
- en: '`500`: This is the update frequency; second 1,024 is the replay start size'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`500`：这是更新频率；第二个1,024是回放开始的大小'
- en: '`InputLength`: This is the input length of size x size x 2 + 1= 33 (considering
    size=4)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InputLength`：这是输入的长度，大小为x x 2 + 1 = 33（考虑到size=4）'
- en: '`4`: This is the number of possible actions that can be performed by the agent.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4`：这是智能体可以执行的可能动作的数量。'
- en: We initialize epsilon (*ϵ*-greedy action selection) to 1, which will decrease
    by a small amount on every episode. This way, it will eventually reach 0.1 and
    saturate. Based on the preceding setting, the training should be started, which
    will start generating a grid representing the map at each timestamp and the outputs
    of the DQN for the up/down/left/right order, followed by the index of the highest
    value.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将epsilon（*ϵ*贪婪动作选择）初始化为1，并且在每一回合后会减少一个小量。这样，最终它会降到0.1并保持不变。基于之前的设置，应该开始训练，训练过程会开始生成一个表示每个时间戳的地图网格，并输出DQN对于上/下/左/右顺序的结果，接着是最高值的索引。
- en: 'We do not have any module for a graphical representation of the game. So in
    the previous result, 0, 1, -1, and so on,the grid represents the map at each timestamp
    for five episodes. The numbers in brackets are just the outputs of the DQN, followed
    by the index of the highest value. Take a look at this code block:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有用于图形表示游戏的模块。所以，在前面的结果中，0、1、-1 等数字代表了每五个回合中每个时间戳的地图。括号中的数字只是 DQN 的输出，后面跟着最大值的索引。看看这个代码块：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Thus, the agent has been able to make a total score of 10 (that is, positive).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代理已经能够获得总分 10（即正分）。
- en: Frequently asked questions (FAQs)
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQs）
- en: Now that we have solved the GridWorld problem, there are other practical aspects
    in reinforcement learning and overall deep learning phenomena that need to be
    considered too. In this section, we will see some frequently asked questions that
    may be already on your mind. Answers to these questions can be found in Appendix.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经解决了 GridWorld 问题，那么在强化学习和整体深度学习现象中还有其他实际方面需要考虑。在本节中，我们将看到一些你可能已经想过的常见问题。答案可以在附录中找到。
- en: What is Q in Q-learning?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 Q 学习中的 Q？
- en: I understand that we performed the training on GPU and cuDNN for faster convergence.
    However, there is no GPU on my machine. What can I do?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我理解我们在 GPU 和 cuDNN 上进行了训练以加速收敛。然而，我的机器上没有 GPU。我该怎么办？
- en: There is no visualization, so it is difficult to follow the moves made by the
    agent toward the target.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有可视化，因此很难跟踪代理朝向目标的移动。
- en: Give a few more examples of reinforcement learning.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给出更多强化学习的例子。
- en: How do I reconcile the results obtained for our mini-batch processing?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我如何调和我们获得的小批处理处理结果？
- en: How would I reconcile the DQN?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我如何调和 DQN？
- en: I would like to save the trained network. Can I do that?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想保存已训练的网络。我可以做到吗？
- en: I would like to restore the saved (that is, trained) network. Can I do that?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想恢复已保存（即已训练）的网络。我可以做到吗？
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to develop a demo GridWorld game using DL4J, RL4J,
    and neural Q-learning, which acts as the Q-function. We also provided some basic
    theoretical background necessary for developing a deep QLearning network for playing
    the GridWorld game. However, we did not develop any module for visualizing the
    moves of the agent for the entire episodes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何使用 DL4J、RL4J 和神经 Q 学习开发一个演示版 GridWorld 游戏，其中 Q 学习充当 Q 函数。我们还提供了开发深度
    Q 学习网络以玩 GridWorld 游戏所需的一些基本理论背景。然而，我们没有开发任何模块来可视化代理在整个回合中的移动。
- en: In the next chapter, we will develop a very common end-to-end movie recommendation
    system project, but with the neural **Factorization Machine** (**FM**) algorithm.
    The MovieLens 1 million dataset will be used for this project. We will be using
    RankSys and Java-based FM libraries for predicting both movie ratings and rankings
    from the users. Nevertheless, Spark ML will be used for exploratory analysis of
    the dataset.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开发一个非常常见的端到端电影推荐系统项目，但使用神经**因式分解机**（**FM**）算法。该项目将使用 MovieLens 100 万数据集。我们将使用
    RankSys 和基于 Java 的 FM 库来预测用户的电影评分和排名。尽管如此，Spark ML 将用于对数据集进行探索性分析。
- en: Answers to questions
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的答案
- en: '**Answer** **to question 1:** Do not confuse the Q in Q-learning with the Q-function
    we have discussed in the previous parts. The Q-function is always the name of
    the function that accepts states and actions and spits out the value of that state-action
    pair. RL methods involve a Q-function but are not necessarily Q-learning algorithms.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1 的回答：** 不要将 Q 学习中的 Q 与我们在前面部分讨论的 Q 函数混淆。Q 函数始终是接受状态和动作并返回该状态-动作对值的函数名称。强化学习方法涉及
    Q 函数，但不一定是 Q 学习算法。'
- en: '**Answer** **to question 2:** No worries as you can perform the training on
    a CPU backend too. In that case, just remove the entries for CUDA and cuDNN dependencies
    from the `pom.xml` file and replace them with the CPU ones. The properties would
    be:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2 的回答：** 不用担心，你也可以在 CPU 后端进行训练。在这种情况下，只需从 `pom.xml` 文件中删除与 CUDA 和 cuDNN
    相关的条目，并用 CPU 版本替换它们。相应的属性为：'
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Don''t use these two dependencies:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 不要使用这两个依赖项：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Use only one, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 只使用一个，如下所示：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Then you are ready to get going with the CPU backend.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 那么你已经准备好使用 CPU 后端了。
- en: '**Answer** **to question 3:** As stated earlier, the initial target was to
    develop a simple text-based game. However, with some effort, all the moves can
    be visualized too. I want to leave this up to the readers. Nevertheless, the visualization
    module will be added to the GitHub repository very soon.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3的答案：** 如前所述，最初的目标是开发一个简单的基于文本的游戏。然而，通过一些努力，所有的动作也可以进行可视化。我希望读者自行决定。然而，可视化模块将在不久后添加到GitHub仓库中。'
- en: '**Answer** **to question 4:** Well, there are some basic examples of RL4J on
    the DL4J GitHub repository at [https://github.com/deeplearning4j/dl4j-examples/](https://github.com/deeplearning4j/dl4j-examples/tree/master/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j).
    Feel free to try to extend them to meet your needs.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题4的答案：** 好吧，DL4J GitHub仓库中有一些RL4J的基本示例，地址是[https://github.com/deeplearning4j/dl4j-examples/](https://github.com/deeplearning4j/dl4j-examples/tree/master/rl4j-examples/src/main/java/org/deeplearning4j/examples/rl4j)。欢迎尝试扩展它们以满足你的需求。'
- en: '**Answer** **to question 5:** Processing each mini-batch gives us the best
    weights/biases result for the input used in that mini-batch. This question evolves
    several subquestions related to this: i) How do we reconcile the results obtained
    for all mini-batches? ii) Do we take the average to come up with the final weights/biases
    for the trained network?'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题5的答案：** 处理每个小批次可以为该小批次使用的输入提供最佳的权重/偏置结果。这个问题涉及到几个子问题：i) 我们如何调和所有小批次得到的结果？ii)
    我们是否取平均值来得出训练网络的最终权重/偏置？'
- en: Therefore, each mini-batch contains the average of the gradients of individual
    errors. If you had two mini-batches, you could take the average of the gradient
    updates of both mini-batches to tweak the weights, to reduce the error for those
    samples.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个小批次包含单独错误梯度的平均值。如果你有两个小批次，你可以取这两个小批次的梯度更新的平均值来调整权重，以减少这些样本的误差。
- en: '**Answer** **to question 6:** Refer to question 5 to get the theoretical understanding.
    However, in our example, use the `setParams()` method from DL4J, which helps you
    reconcile network:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题6的答案：** 参考问题5以获得理论理解。然而，在我们的示例中，使用来自DL4J的`setParams()`方法，它有助于调和网络：'
- en: '[PRE28]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now the question would be: Where do we use such reconciling? Well, the answer
    is while computing the reward (see the `observeReward()` method).'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题是：我们在哪些地方使用这种调和方法呢？答案是，在计算奖励时（参见`observeReward()`方法）。
- en: '**Answer** **to question 7:** Saving the DQN is similar to saving another DL4J-based
    network. For this, I wrote a method called `saveNetwork()` that saves network
    parameters as a single ND4J object in JSON format. Take a look at this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题7的答案：** 保存DQN与保存其他基于DL4J的网络类似。为此，我编写了一个名为`saveNetwork()`的方法，它将网络参数作为单个ND4J对象以JSON格式保存。看一下这个：'
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Answer** **to question 8:** Restoring the DQN is similar to saving another
    DL4J-based network. For this, I wrote a method called `restoreNetwork()` that
    reconciles the params and reloads the saved network as `MultiLayerNetwork`. Here
    it is:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题8的答案：** 恢复DQN与保存其他基于DL4J的网络类似。为此，我编写了一个名为`restoreNetwork()`的方法，它调和参数并将保存的网络重新加载为`MultiLayerNetwork`。如下所示：'
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
