- en: Deep Learning for NLU and NLG Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于 NLU 和 NLG 问题的深度学习
- en: We have seen the rule-based approach and various machine learning techniques
    to solve NLP tasks in the previous chapters. In this chapter, we will see the
    bleeding edge subset of machine learning technique called **deep learning** (**DL**).
    In the past four to five years, neural networks and deep learning techniques have
    been creating a lot of buzz in the artificial intelligence area because many tech
    giants use these cutting-edge techniques to solve real-life problems, and the
    results from these techniques are extremely impressive. Tech giants such as Google,
    Apple, Amazon, OpenAI, and so on spend a lot of time and effort to create innovative
    solutions for real-life problems. These efforts are mostly to develop artificial
    general intelligence and make the world a better place for human beings.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经看过了基于规则的方法和各种机器学习技术来解决 NLP 任务。在本章中，我们将看到一种被称为**深度学习**（**DL**）的机器学习技术的前沿子集。在过去的四到五年里，神经网络和深度学习技术在人工智能领域引起了极大的关注，因为许多科技巨头利用这些尖端技术解决现实生活中的问题，而这些技术的结果令人非常印象深刻。像谷歌、苹果、亚马逊、OpenAI
    等科技巨头，花费了大量时间和精力去创造创新的解决方案来解决现实中的问题。这些努力主要是为了发展人工通用智能，并使世界变得更加美好。
- en: 'We will first understand the overall AI, in general, to give you a fair idea
    of why deep learning is creating a lot of buzz nowadays. We will cover the following
    topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将了解总体的 AI，概述其背景，以便让你了解为什么深度学习现在会引起如此大的关注。本章将涉及以下主题：
- en: How NLU and NLG are different from each other
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLU 和 NLG 之间的区别
- en: The basics of neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: Building NLP and NLG applications using various deep learning techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种深度学习技术构建 NLP 和 NLG 应用
- en: After understanding the basics of DL, we will touch on some of the most recent
    innovations happening in the deep learning field. So let's begin!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了深度学习的基础知识后，我们将讨论一些最近在深度学习领域发生的创新。那么，让我们开始吧！
- en: An overview of artificial intelligence
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能概览
- en: In this section, we will see the various aspects of AI and how deep learning
    is related to AI. We will see the AI components, various stages of AI, and different
    types of AI; at the end of this section, we will discuss why deep learning is
    one of the most promising techniques in order to achieve AI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到 AI 的各个方面以及深度学习与 AI 的关系。我们将看到 AI 的组成部分、AI 的不同阶段以及 AI 的不同类型；在本节末尾，我们将讨论为什么深度学习是实现
    AI 的最有前景的技术之一。
- en: The basics of AI
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 基础
- en: When we talk about AI, we think about an intelligent machine, and this is the
    basic concept of AI. AI is an area of science that is constantly progressing in
    the direction of enabling human-level intelligence in machines. The basic idea
    behind AI is to enable intelligence in machines so that they can also perform
    some of the tasks performed only by humans. We are trying to enable human-level
    intelligence in machines using some cool algorithmic techniques; in this process,
    whatever kind of intelligence is acquired by the machines is artificially generated.
    Various algorithmic techniques that are used to generate AI for machines are mostly
    part of machine learning techniques. Before getting into the core machine learning
    and deep learning part, we will understand other facts related to AI.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈到 AI 时，我们会想到智能机器，这是 AI 的基本概念。AI 是一个科学领域，持续朝着让机器具备人类水平的智能方向发展。AI 的基本思想是赋予机器智能，使其能够执行本只能由人类完成的任务。我们正在尝试通过一些很酷的算法技术，在机器中实现人类水平的智能；在这个过程中，机器获得的任何类型的智能都是人工生成的。用于生成
    AI 的各种算法技术大多数属于机器学习技术。在深入学习机器学习和深度学习核心部分之前，我们将了解一些与 AI 相关的其他事实。
- en: 'AI is influenced by many branches; in *Figure 9.1*, we will see those branches
    that heavily influence artificial intelligence as a single branch:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: AI 受到许多领域的影响；在*图 9.1*中，我们将看到那些对人工智能影响深远的领域，将它们作为一个整体来看：
- en: '![](img/3eb3f19e-7bdd-44de-b847-6e7ee61f62f0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eb3f19e-7bdd-44de-b847-6e7ee61f62f0.png)'
- en: 'Figure 9.1: AI influence by other branches'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：AI 受其他领域影响
- en: Components of AI
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 的组成部分
- en: First of all, we will see the key components of AI. These components will be
    quite useful for us to understand that direction the world is going.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看到 AI 的关键组成部分。这些组件对我们理解世界发展的方向非常有帮助。
- en: 'According to me, there are two components, that you can see in *Figure 9.2*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，你可以在*图 9.2*中看到两个组件：
- en: '![](img/0f6deded-5916-4866-97a9-5155f078c649.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f6deded-5916-4866-97a9-5155f078c649.png)'
- en: 'Figure 9.2: Components of AI'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：人工智能的组成部分
- en: Let's see the AI components in detail. We will also see some examples.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看人工智能的组成部分。我们还将看到一些例子。
- en: Automation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化
- en: Automation is a well-known component of AI. People around the world work heavily
    on automation and we have achieved tremendous success in the area of automatic
    tasks performed by machines. We will look at some examples that are intuitive
    enough for you to understand the automation concept in AI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化是人工智能中一个著名的组成部分。全球各地的人们在自动化领域投入了大量精力，并且我们在机器执行自动任务方面取得了巨大的成功。我们将看一些足够直观的例子，帮助你理解人工智能中的自动化概念。
- en: In the automobile sector, we are using automatic robots to manufacture vehicles.
    These robots follow a set of instructions and perform the definite tasks. Here,
    these robots are not intelligent robots that can interact with humans and ask
    questions or respond to human questions, but these robots are just following a
    set of instructions to achieve great accuracy and efficiency in manufacturing
    with high speed. So these kinds of robots are examples of automation in the AI
    area.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在汽车行业，我们正在使用自动机器人来制造车辆。这些机器人遵循一套指令，执行特定任务。在这里，这些机器人不是能够与人类互动、提问或回答问题的智能机器人，而是按照一组指令执行任务，以达到高速度、高精度和高效的制造效果。因此，这些机器人是人工智能领域自动化的例子。
- en: The other example is in the area of DevOps. Nowadays, DevOps is using machine
    learning to automate many human-intensive processes such as, in order to maintain
    in-house servers, the DevOps team gets a bunch of recommendations after analyzing
    various logs of servers, and after getting the recommendations, another machine
    learning model prioritizes the alerts and recommendations. This kind of application
    really saves time for the DevOps team to deliver great work on time. These kinds
    of applications really help us understand that automation is a very important
    component of AI.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是在DevOps领域。如今，DevOps正在利用机器学习自动化许多需要大量人工的过程。例如，为了维护内部服务器，DevOps团队在分析了各种服务器日志后，会收到一堆建议，接着，另一个机器学习模型会根据优先级对警报和建议进行排序。这种应用真正节省了DevOps团队的时间，使他们能够按时交付高质量的工作。这类应用帮助我们深刻理解自动化是人工智能中非常重要的组成部分。
- en: Now let's see how intelligence is going to impact the world as part of AI.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看智能作为人工智能的一部分将如何影响世界。
- en: Intelligence
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能
- en: When we say intelligence, as humans, our expectations are really high. Our goal
    is that we really want machines to understand our behavior and emotions. We also
    want machines to react intelligently based on human actions and all the reactions
    generated by machines should be in such a way that it mimics human intelligence.
    We want to achieve this goal since the mid 1900s. Around the globe, many researchers,
    groups of scientists, and communities are doing a lot of cool research to make
    machines as intelligent as humans.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈到智能时，作为人类，我们的期望非常高。我们的目标是让机器能够理解我们的行为和情感。我们还希望机器能根据人类的行为做出智能反应，所有由机器生成的反应都应该模拟人类的智能。自20世纪中期以来，我们一直致力于实现这一目标。全球许多研究人员、科学家团队和社区正在进行大量前沿研究，旨在使机器像人类一样智能。
- en: We want that after acquiring intelligence, machines will perform majority of
    the tasks for humans with better accuracy and this is the single broad expectation.
    During the last four-five years, we have started to successfully achieve this
    broad goal and, as a result of so many years of efforts, Google recently announced
    that Google Assistant can hear natural language from humans and interpret the
    speech signal as accurately as a human. The other example is that the Facebook
    research group did a very powerful research in order to build a system that is
    good at applying reasoning for questions and answers. Tesla and Google self-driving
    cars are a complex AI system but very useful and intelligent. Self-driving cars
    and chatbots are part of narrow AI. You can also find many other examples on the
    web, that are coming out now and then.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望，在获得智能之后，机器能够以更高的准确性完成大多数人类任务，这是我们广泛的期望。在过去的四五年里，我们开始成功地实现这一广泛目标。经过多年努力，谷歌最近宣布，Google
    Assistant能够听懂人类的自然语言，并像人类一样准确地解释语音信号。另一个例子是，Facebook研究小组进行了一项非常强大的研究，旨在构建一个能够运用推理进行问答的系统。特斯拉和谷歌的自动驾驶汽车是复杂的人工智能系统，但它们非常有用且智能。自动驾驶汽车和聊天机器人属于窄人工智能的范畴。你还可以在网上找到许多新的例子，时不时地会有新的技术问世。
- en: 'There are certain subcomponents that can be included as part of intelligence.
    Refer to *Figure 9.3*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些子组件可以作为智能的一部分。请参见*图9.3*：
- en: '![](img/554c0bd2-6c22-48cd-83d8-298ffc080763.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/554c0bd2-6c22-48cd-83d8-298ffc080763.png)'
- en: 'Figure 9.3: Subcomponents of intelligence'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：智能的子组件
- en: Intelligence is a combination of all the components described in the preceding
    figure. All these components--reasoning, learning, learning from experience, problem
    solving, perception, and linguistics intelligence--come very naturally to humans
    but not to machines. So we need the techniques that enable intelligence for machines.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 智能是前面图中描述的所有组件的结合。所有这些组件——推理、学习、从经验中学习、解决问题、感知和语言智能——对人类来说是非常自然的，但对机器来说却不是。因此，我们需要能够赋予机器智能的技术。
- en: Before learning the name of the techniques that we will use later in this chapter,
    let's understand the various stages of AI.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习本章稍后我们将使用的技术名称之前，让我们先了解一下人工智能的各个阶段。
- en: Stages of AI
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的阶段
- en: 'There are three main stages for the AI system. We will see the following stages
    in detail:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能系统有三个主要阶段。我们将详细查看以下各个阶段：
- en: Machine learning
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Machine intelligence
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器智能
- en: Machine consciousness
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器意识
- en: 'Before getting into the details of each stage of AI, refer to *Figure 9.4*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解人工智能每个阶段的细节之前，请参阅*图9.4*：
- en: '![](img/c2e3f1cb-a353-48e5-9db6-a6d10b86c164.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2e3f1cb-a353-48e5-9db6-a6d10b86c164.png)'
- en: 'Figure 9.4: Stages of AI (Image credit: https://cdn-images-1.medium.com/max/1600/0*aefkt8m-V66Wf5-j.png)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：人工智能的阶段（图片来源：[https://cdn-images-1.medium.com/max/1600/0*aefkt8m-V66Wf5-j.png](https://cdn-images-1.medium.com/max/1600/0*aefkt8m-V66Wf5-j.png)）
- en: We will begin from the bottom to the top, so we will understand the machine
    learning stage first, then machine intelligence, and finally machine consciousness.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从下到上开始学习，因此我们首先了解机器学习阶段，然后是机器智能，最后是机器意识。
- en: Machine learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: You have learned a lot of things about machine learning in the previous chapters,
    but I want to give you an AI perspective of it in this chapter.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章你已经学习了很多关于机器学习的内容，但我想在本章中从人工智能的角度给你讲解它。
- en: ML techniques are a set of algorithms that explain how to generate or reach
    the defined output. This kind of algorithm is used by intelligent systems that
    try to learn from experience. The system that uses MLL algorithms is keen to learn
    from the historical data or real-time data. So, at this stage of AI, we focus
    on algorithms that learn patterns or specific structures from the data using features
    that we have provided to the ML system. To make it clear, let's take an example.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术是一组算法，解释了如何生成或达到定义的输出。这种算法被智能系统使用，这些系统试图从经验中学习。使用机器学习算法的系统热衷于从历史数据或实时数据中学习。因此，在人工智能的这个阶段，我们专注于通过我们提供给机器学习系统的特征，从数据中学习模式或特定结构的算法。为了更清楚地说明这一点，让我们举个例子。
- en: Suppose you want to build a sentiment analysis application. We can use historical
    tagged data, hand-crafted features, and the Naive Bayes ML algorithm. As a result,
    we can have an intelligent system that has learned from its learning example--how
    to provide a sentiment tag for an unseen new data instance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想构建一个情感分析应用程序。我们可以使用历史标记数据、手工设计的特征和朴素贝叶斯机器学习算法。结果，我们可以拥有一个智能系统，该系统通过学习示例学会了如何为一个未见的新数据实例提供情感标签。
- en: Machine intelligence
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器智能
- en: Machine intelligence is again a set of algorithms, but most of the algorithms
    are heavily influenced by how the human brain learns and thinks. Using neuroscience,
    biology, and mathematics, the AI researcher came up with a set of advance-level
    algorithms that help machines to learn from the data without providing hand-crafted
    features. In this stage, algorithms use unlabeled or labeled data. Here, you just
    define the end goal, and the advanced algorithms figure out their own way to achieve
    the expected result.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 机器智能也是一组算法，但大多数算法受到人脑学习和思考方式的深刻影响。利用神经科学、生物学和数学，人工智能研究者提出了一组高级算法，帮助机器从数据中学习，而无需提供手工设计的特征。在这个阶段，算法使用的是无标签或有标签的数据。在这里，你只需要定义最终目标，先进的算法会找到自己的方式来实现预期结果。
- en: If you compare the algorithms that we are using at this stage with the traditional
    ML algorithms, then the major difference is that, in this machine intelligence
    stage, we are not giving hand-crafted features as input to any algorithm. As these
    algorithms are inspired by human brains, the algorithm itself learns the features
    and patterns and generates the output. Currently, the world of AI is in this stage.
    People across the world use these advanced algorithms that seem very promising
    to achieve human-like intelligence for machines.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将我们在这个阶段使用的算法与传统的机器学习算法进行比较，你会发现主要的区别在于，在这个机器智能阶段，我们没有为任何算法提供手工设计的特征输入。由于这些算法受到人脑的启发，算法本身会学习特征和模式，并生成输出。目前，人工智能的世界正处于这个阶段。全球各地的人们正在使用这些看似非常有前景的先进算法，以期为机器实现类人智能。
- en: '**Artificial neural networks** (**ANNs**) and deep learning techniques are
    used to achieve machine intelligence.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）和深度学习技术被用来实现机器智能。'
- en: Machine consciousness
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器意识
- en: Machine consciousness is one of the most discussed topics in AI as our end goal
    is to reach here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 机器意识是人工智能中最受讨论的主题之一，因为我们的最终目标是达到这一阶段。
- en: We want machines to learn the way humans learn. As humans, we don't need a lot
    of data; we don't take so much time to understand the abstract concepts. We learn
    from small amounts of data or without data. Most of the time, we learn from our
    experiences. If we want to build a system that is as conscious as a human, then
    we should know how to generate consciousness for machines. However, are we fully
    aware how our brain works and reacts in order to transfer this knowledge to machines
    and make them as conscious as we are? Unfortunately, right now we aren't aware
    of this. We expect that in this stage, machines learn without data or with very
    small amounts of data and use their self-experience to achieve the defined output.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望机器能像人类一样学习。作为人类，我们不需要大量数据；我们不需要很长时间就能理解抽象概念。我们通过少量数据或者没有数据来学习。大多数时候，我们是通过我们的经验来学习的。如果我们想构建一个像人类一样具有意识的系统，那么我们应该知道如何为机器生成意识。然而，我们是否完全了解大脑是如何工作和反应的，从而能够将这种知识转移到机器中，让它们像我们一样具有意识呢？不幸的是，目前我们并不完全了解这一点。我们期望，在这个阶段，机器可以在没有数据或只有非常少量数据的情况下学习，并利用自身的经验来达成既定的输出。
- en: 'There are a lot of interesting researches going on, and I encourage you to
    check out this YouTube video of researcher John Searle, who talks about machine
    consciousness: [https://www.youtube.com/watch?v=rHKwIYsPXLg](https://www.youtube.com/watch?v=rHKwIYsPXLg).
    This video may give you a fresh perspective about consciousness in AI.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有很多有趣的研究在进行，我鼓励你查看一下研究员John Searle在YouTube上分享的视频，他讲解了机器意识：[https://www.youtube.com/watch?v=rHKwIYsPXLg](https://www.youtube.com/watch?v=rHKwIYsPXLg)。这个视频可能会为你提供关于人工智能中意识的全新视角。
- en: We have seen the various stages of AI. ANNs and deep learning are a part of
    the machine intelligence stage. At the end of the *A brief overview of deep learning*
    section, you have the necessary details that can help you understand why deep
    learning is the new buzzword in AI.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了人工智能的不同阶段。人工神经网络（ANNs）和深度学习是机器智能阶段的一部分。在*深度学习简要概述*部分的结尾，你会获得一些必要的细节，帮助你理解为何深度学习成为了人工智能中的新热词。
- en: Now we are going to see the various types of AI.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到人工智能的各种类型。
- en: Types of artificial intelligence
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的类型
- en: 'There are three types of AI, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能有三种类型，如下所示：
- en: Artificial narrow intelligence
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工窄智能
- en: Artificial general intelligence
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工通用智能
- en: Artificial superintelligence
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工超智能
- en: Artificial narrow intelligence
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工窄智能
- en: '**Artificial narrow intelligence** (**ANI**) is a type of AI that covers some
    of the basic tasks such as template-based chatbot, basic personal assistant application
    like the initial versions of Siri by Apple.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工窄智能**（**ANI**）是一种人工智能，涵盖了一些基本任务，如基于模板的聊天机器人、像苹果最初版本的Siri那样的基础个人助手应用。'
- en: This type of intelligence is majorly focused on basic prototyping of applications.
    This type of intelligence is the starting point for any application and then you
    can improve the basic prototype. You can add the next layer of intelligence by
    adding artificial general intelligence, but only if your end users really need
    that kind of functionality. We have also seen this kind of basic chatbot in [Chapter
    7](0dc5bd44-3b7d-47ac-8b0d-51134007b483.xhtml), *Rule-Based System for NLP*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的智能主要集中在应用的基本原型设计上。它是任何应用的起点，之后你可以改进这个基本原型。你可以通过添加人工通用智能来增强智能层次，但只有在你的最终用户真的需要这种功能时才行。我们也在[第7章](0dc5bd44-3b7d-47ac-8b0d-51134007b483.xhtml)中看到了这种基本聊天机器人的示例，*基于规则的NLP系统*。
- en: Artificial general intelligence
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工通用智能
- en: '**Artificial general intelligence** (**AGI**) is a type of AI that is used
    to build systems that are capable of performing human-level tasks. What do I mean
    by human-level tasks? Tasks such as building self-driving cars. Google self-driving
    cars and Tesla autopilot are the most famous examples. Humanoid robots also try
    to use this type of AI.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工通用智能**（**AGI**）是一种人工智能，用来构建能够执行人类级别任务的系统。什么是人类级别的任务呢？比如构建自动驾驶汽车。谷歌的自动驾驶汽车和特斯拉的自动驾驶仪是最著名的例子。类人机器人也尝试使用这种类型的人工智能。'
- en: NLP-level examples are sophisticated chatbots that ignore spelling mistakes
    and grammatical mistakes and understand your query or questions. The deep learning
    techniques seem very promising for the understanding of the natural language by
    humans.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: NLP级别的示例是复杂的聊天机器人，它们能够忽略拼写错误和语法错误，并理解你的查询或问题。深度学习技术似乎对人类理解自然语言非常有前景。
- en: We are now at a stage where people and communities around the world use basic
    concepts, and by referring to each other's research works, try to build systems
    that have AGI.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正处于这样一个阶段，世界各地的人们和社区使用基本概念，通过相互参考彼此的研究工作，尝试构建具有AGI的系统。
- en: Artificial superintelligence
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工超级智能
- en: The way to achieve **artificial superintelligence** (**ASI**) is a bit difficult
    for us because, in this type of AI, we expect that machines are smarter than humans
    in order to learn specific tasks and capable enough to perform multiple tasks
    as humans do in their life. This kind of superintelligence is right now a dream
    for us, but we are trying to achieve this in such a way that machines and systems
    always complement human skills and will not create a threat for humans.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实现**人工超级智能**（**ASI**）的方式对我们来说有些困难，因为在这种类型的人工智能中，我们期望机器在学习特定任务时比人类更聪明，并且能够像人类一样执行多任务。这种超级智能目前对我们来说是一个梦想，但我们正在尝试以一种方式来实现它，使得机器和系统始终补充人类技能，并且不会对人类构成威胁。
- en: Goals and applications of AI
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能的目标和应用
- en: This is the time and section where we need to understand the goals and applications
    for AI in general for various areas. These goals and applications are just to
    give you an idea about the current state of AI-enabled applications, but if you
    can think of some crazy but useful application in any area, then you should try
    to include it in this list. You should try to implement various types and stages
    of AI in that application.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们需要理解人工智能在各个领域的目标和应用的时机和环节。这些目标和应用只是为了让你了解当前人工智能应用的状态，但如果你能想到一些疯狂但有用的应用场景，无论在哪个领域，都应该尝试将其纳入这个列表中。你应该尝试在这些应用中实现不同类型和阶段的人工智能。
- en: 'Now let''s see the areas where we want to integrate various stages of AI and
    make those applications AI-enabled:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们希望在哪些领域集成不同阶段的人工智能，并使这些应用具备人工智能功能：
- en: Reasoning
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理
- en: Machine learning
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Natural language processing
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Robotics
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人技术
- en: Implementing general intelligence
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现通用智能
- en: Computer vision
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: Automated learning and scheduling
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化学习和调度
- en: Speech analysis
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音分析
- en: 'You can refer to *Figure 9.5**,* which shows many different areas and related
    applications:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考*图 9.5*，它展示了许多不同的领域和相关的应用：
- en: '![](img/bb5e865d-edf1-43d8-8e2d-b739a33dfbed.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb5e865d-edf1-43d8-8e2d-b739a33dfbed.png)'
- en: 'Figure 9.5 : Various areas of AI and applications (Image credit: http://vincejeffs.com/)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：人工智能的各种领域和应用（图片来源：http://vincejeffs.com/）
- en: Now let's see the applications from some of the areas in the preceding list.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看前面列表中一些领域的应用。
- en: AI-enabled applications
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能应用
- en: 'Here, I will give you a brief idea about AI-enabled applications. Some of the
    applications are related to the NLP domain as well:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将简要介绍一些人工智能应用。有些应用与自然语言处理（NLP）领域也有关：
- en: Enabling reasoning for any system will be very exciting stuff. Under this area,
    we can build a Q/A system that can use the reasoning to derive answers for the
    asked questions.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为任何系统启用推理将是非常令人兴奋的事情。在这个领域，我们可以构建一个问答系统，利用推理来推导出所提问题的答案。
- en: If we can enable the reasoning for an AI-based system, then those systems will
    be very good at decision making and will improvise the existing decision making
    system.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们能为AI驱动的系统启用推理，那么这些系统将非常擅长决策，并将改善现有的决策系统。
- en: In machine learning, we want the perfect architecture of an ML-based application
    that can be decided by machines themselves. This is, according to me, an AI-enabled
    application in ML.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中，我们希望能够由机器自己决定的完美架构，来实现基于机器学习的应用。依我看，这就是一种AI驱动的机器学习应用。
- en: When we are talking in terms of AI-enabled NLP applications, then we really
    need NLP systems that can understand the context of human natural language and
    react and behave more like humans.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们讨论AI驱动的自然语言处理（NLP）应用时，我们确实需要能够理解人类自然语言语境并像人类一样反应和行为的NLP系统。
- en: Humanoid robots are the best application to describe an AI-enabled system. Robots
    should acquire perception, which is a long term AI goal.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类人机器人是描述AI驱动系统的最佳应用。机器人应该具备感知能力，这是一个长期的AI目标。
- en: When we are talking about general intelligence, according to me, systems should
    react more like humans do. Particularly machine reactions should match with real
    human behavior. After analyzing certain situations, machines should react the
    same or better than humans.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们讨论通用智能时，依我看，系统应该更像人类那样反应。特别是机器的反应应该与真实的人类行为相匹配。分析某些情况后，机器应当作出与人类相同甚至更好的反应。
- en: Nowadays, computer vision has many applications that give us solid proof that
    AI will be achieved very soon in this area. The applications are object identification,
    image recognition, skin cancer detection using image recognition techniques, generating
    face images from machines, generating text for images and vice versa, and others.
    All these applications give us concrete proof about AI-driven computer vision.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如今，计算机视觉有许多应用，它们为我们提供了坚实的证据，表明AI将在这个领域很快取得突破。这些应用包括物体识别、图像识别、使用图像识别技术检测皮肤癌、从机器生成面部图像、为图像生成文本及其反向操作等。这些应用为AI驱动的计算机视觉提供了具体证据。
- en: Automated learning and scheduling is a kind of building system that works for
    your personal assistance and manages your schedule. Regarding the AI part, we
    really expect that every user of the system will get a personalized experience
    so automating the learning of a person's personal choice is very important for
    AI-driven scheduling. To achieve this goal, automated learning systems should
    also learn how to choose the best suited model for a particular user.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化学习和调度是一种为个人提供助理并管理日程的系统。关于AI部分，我们真的期望每个系统用户能够获得个性化体验，因此自动化学习个人偏好对于AI驱动的调度非常重要。为了实现这一目标，自动化学习系统还应该学会如何为特定用户选择最合适的模型。
- en: Speech analysis is a different form of NL, but unfortunately, we are not talking
    about this concept in this book. Here, we are talking about a speech recognition
    system in terms of a potential AI-enabled area. By enabling AI with this speech
    recognition area, we can understand the human environment and thinking process
    that is generated under the effect of a person's sociology, psychology, and philosophy.
    We can also predict their personality.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音分析是一种不同形式的自然语言处理，然而不幸的是，在本书中我们并未涉及这一概念。在这里，我们讨论的是语音识别系统在AI驱动领域中的潜力。通过在语音识别领域启用AI，我们可以理解人在社会学、心理学和哲学的影响下所生成的环境与思维过程。我们还可以预测他们的个性。
- en: 'After seeing all these fascinating applications, there are three really interesting
    questions that come to our mind: what are the reasons that lead us to produce
    AI-driven systems, why the time is so perfect for us to build an AI-driven system,
    and how can we build an AI-enabled system?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到这些令人着迷的应用之后，有三个非常有趣的问题浮现在我们的脑海：是什么原因促使我们产生AI驱动的系统？为什么现在是构建AI驱动系统的最佳时机？我们该如何构建AI驱动的系统？
- en: Since the mid 1900s, we are trying to bring intelligence in machines. During
    this phase, researchers and scientists have given a lot of cool concepts. For
    example, an artificial neuron, also known as **McCulloch-Pitts model** (**MCP**),
    is inspired by the human brain and the purpose of this concept is to understand
    the human brain biological working process and represent this process in terms
    of mathematics and physics. So it will be helpful to implement AI for machines.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自20世纪中期以来，我们一直在尝试将智能引入机器。在这个过程中，研究人员和科学家提出了许多酷炫的概念。例如，人工神经元，也被称为**麦卡洛克-皮茨模型**（**MCP**），它的灵感来源于人脑，这一概念的目的是理解人脑的生物学工作过程，并用数学和物理的语言表示这一过程。因此，这对于实现机器的人工智能（AI）是非常有帮助的。
- en: 'They successfully gave the mathematical representation of how a single neuron
    works, but there is one outcome of this model that wasn''t good for training purposes.
    So researcher Frank Rosenblatt, in his 1958 paper, came up with the *perceptron*,
    introducing *dynamic weight* and *threshold* concepts. After this, many researchers
    have developed concepts such as backpropagation and multilayer neural networks
    based on the earlier concepts. The research community wanted to implement the
    developed concepts in practical applications, and the first researcher, Geoffrey
    Hinton, demonstrated the use of the generalized backpropagation algorithm to train
    multilayer neural networks. From that point, researchers and communities started
    to use this generalized model, but in the late 1900s, the amount of data was less
    compared to now and computational devices were slow as well as costly. So we didn''t
    get the expected results. However, with the result that was achieved at the time,
    the researcher had faith that these are the concepts that will be used to enable
    an AI-driven world. Now we have a lot of data as well as computation devices that
    are fast, cheap, and capable enough to process large amounts of data. When we
    apply these old concepts of ANNs in the current era to develop applications such
    as a universal machine translation system, speech recognition system, image recognition
    system, and so on, we get very promising results. Let''s take an example. Google
    is using ANNs to develop a universal machine translation system and this system
    will translate multiple languages. This is because we have large datasets available
    and also fast computation capabilities that can help us process the datasets using
    ANNs. We have used neural networks that are not one or two, but many, layers deep.
    The result achieved is so impressive that every big tech giant is using deep learning
    models to develop an AI-enabled system. According to me, data, computational capabilities,
    and solid old concepts are the key components that are perfect to develop an AI-driven
    system. You can refer to *Figure 9.6* for a brief history of the neural network:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 他们成功地给出了单个神经元如何工作的数学表示，但这个模型的一个结果并不适合训练目的。因此，研究员弗兰克·罗森布拉特在1958年的论文中提出了*感知机*，引入了*动态权重*和*阈值*的概念。之后，许多研究人员基于早期的概念发展了反向传播和多层神经网络等概念。研究界希望将这些已开发的概念应用于实际中，第一个实现这一目标的研究员是杰弗里·辛顿，他演示了如何使用广义反向传播算法训练多层神经网络。从那时起，研究人员和学术界开始使用这一广义模型，但在20世纪末，数据量远不如现在丰富，计算设备也既慢又昂贵，因此我们没有得到预期的结果。然而，尽管如此，研究人员相信，凭借当时取得的成果，这些概念将是实现AI驱动世界的关键。现在，我们拥有大量的数据和快速、廉价且足够强大的计算设备来处理这些数据。当我们将这些旧的人工神经网络（ANN）概念应用于当前时代，开发如通用机器翻译系统、语音识别系统、图像识别系统等应用时，我们取得了非常有希望的成果。举个例子，Google正在使用ANN开发一个通用机器翻译系统，该系统能够翻译多种语言。这是因为我们拥有丰富的大型数据集，以及能够帮助我们通过ANN处理这些数据集的快速计算能力。我们使用的神经网络并非只有一层或两层，而是深度多层。所取得的成果令人印象深刻，以至于每一家大型科技公司都在使用深度学习模型来开发AI驱动的系统。在我看来，数据、计算能力和扎实的旧概念是开发AI驱动系统的关键组成部分。你可以参考*图9.6*了解神经网络的简要历史：
- en: '![](img/22310548-27cd-42aa-81c0-7e85d666f896.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22310548-27cd-42aa-81c0-7e85d666f896.jpg)'
- en: 'Figure 9.6: History of ANN (Image credit: https://image.slidesharecdn.com/deeplearning-170124234229/95/deep-learning-9-638.jpg?cb=1485303074)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：人工神经网络的历史（图片来源：https://image.slidesharecdn.com/deeplearning-170124234229/95/deep-learning-9-638.jpg?cb=1485303074）
- en: '*Figure 9.7* will give you an idea about the long term history of neural networks:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.7*将让你对神经网络的长期历史有所了解：'
- en: '![](img/b6e94d72-7ea6-4939-8352-9451f1524f9c.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6e94d72-7ea6-4939-8352-9451f1524f9c.jpg)'
- en: 'Figure 9.7: History of ANN (Image credit: http://qingkaikong.blogspot.in/2016/11/machine-learning-3-artificial-neural.html)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：人工神经网络（ANN）的历史（图片来源：[http://qingkaikong.blogspot.in/2016/11/machine-learning-3-artificial-neural.html](http://qingkaikong.blogspot.in/2016/11/machine-learning-3-artificial-neural.html)）
- en: 'Now let''s move toward the next question: how can we enable AI? The answer
    is deep learning. This is one of the most favorite techniques to enable AI for
    non-AI systems. There are a few cases where deep learning is not used to enable
    AI, but in the NLP domain, deep learning is majorly used to enable AI. To develop
    general intelligence, we can use deep learning. We get very promising results
    from this technique. Experiments such as machines generating human faces understands
    the speech of humans more accurately in a noisy environment, self-driving cars,
    reasoning for question answer systems are just a few of the experiments. The deep
    learning techniques are using lots and lots of data and high computational capability
    to train systems on the given data. When we apply the right deep learning model
    on a large amount of data, we will get a magical, impressive, and promising result.
    These are the reasons deep learning is creating lot of buzz nowadays. So I guess
    now you know why deep learning is the buzzword in the AI world.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们向下一个问题迈进：如何使 AI 成为可能？答案是深度学习。这是使非 AI 系统具备 AI 功能的最常用技术之一。虽然有些情况下深度学习没有被用于使
    AI 成为可能，但在 NLP 领域，深度学习主要被用来使 AI 成为可能。为了发展通用智能，我们可以使用深度学习。我们从这项技术中得到了非常有前景的结果。诸如机器生成真人面孔、在嘈杂环境中更准确地理解人类语言、自驾车、为问答系统进行推理等实验，都是其中的一部分。深度学习技术使用大量的数据和高计算能力对给定数据进行系统训练。当我们将合适的深度学习模型应用于大量数据时，我们将得到神奇、令人印象深刻且充满前景的结果。这些就是深度学习如今成为热门话题的原因。所以，我想现在你明白为什么深度学习在
    AI 世界中成为流行词了。
- en: Further in this chapter, we will see deep learning techniques in detail and
    develop NLP applications using deep learning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后续部分，我们将详细探讨深度学习技术，并使用深度学习开发 NLP 应用。
- en: Comparing NLU and NLG
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较 NLU 和 NLG
- en: We have already seen the NLU and NLG definitions, details, and differences in
    [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure
    of Sentences*. In this section, we are comparing these two subareas of NLP in
    terms of an AI-enabled application.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第 3 章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)《句子结构理解》中看到了 NLU 和 NLG
    的定义、细节及其区别。在这一节中，我们将从一个 AI 驱动的应用角度对这两个 NLP 子领域进行比较。
- en: Natural language understanding
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: Earlier, we have seen that NLU is more about dealing with an understanding of
    the structure of the language, whether it is words, phrases, or sentences. NLU
    is more about applying various ML techniques on already generated NL. In NLU,
    we focus on syntax as well as semantics. We also try to solve the various types
    of ambiguities related to syntax and semantics. We have seen the lexical ambiguity,
    syntactic ambiguity, semantic ambiguity, and pragmatics ambiguity.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们看到 NLU 更多的是处理语言结构的理解，无论是单词、短语还是句子。NLU 更侧重于将各种机器学习（ML）技术应用于已经生成的自然语言（NL）。在
    NLU 中，我们关注句法和语义。我们还尝试解决与句法和语义相关的各种歧义。我们已经见识过词汇歧义、句法歧义、语义歧义和语用歧义。
- en: Now let's see where we can use AI that helps machines understand the language
    structure and meaning more accurately and efficiently. AI and ML techniques are
    not much behind to address these aspects of NL. To give an example, deep learning
    gives us an impressive result in machine translation. Now when we talk about solving
    syntactic ambiguity and semantic ambiguity, we can use deep learning. Suppose
    you have a NER tool that will use deep learning and Word2vec, then we can solve
    the syntactic ambiguity. This is just one application, but you can also improve
    parser results and POS taggers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看我们可以在哪里使用 AI 来帮助机器更准确高效地理解语言结构和意义。AI 和 ML 技术在解决自然语言（NL）这些方面并不落后。举个例子，深度学习在机器翻译中给我们带来了令人印象深刻的成果。现在，当我们谈论解决句法歧义和语义歧义时，我们可以使用深度学习。假设你有一个命名实体识别（NER）工具，它将使用深度学习和
    Word2vec，那么我们就能解决句法歧义。这只是一个应用，但你还可以通过它来改进解析器的结果和词性标注器。
- en: Now let's talk about pragmatics ambiguity, where we really need AGI as well
    as ASI. This ambiguity occurs when you try to understand the long distance context
    of a sentence with other previously written or spoken sentences, and it also depends
    on the speaker's intent of speaking or writing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来谈谈语用歧义，在这种情况下，我们真的需要人工通用智能（AGI）以及人工超级智能（ASI）。这种歧义出现在你尝试理解一句话的长距离上下文时，需要考虑其他先前写的或说过的话，并且还取决于说话者的表达意图。
- en: Let's see an example of pragmatics ambiguity. You and your friend are having
    a conversation, and your friend told you long ago that she had joined an NGO and
    would do some social activity for poor students. Now you ask her how was the social
    activity. In this case, you and your friend know about what social activities
    you are talking about. This is because as humans, our brain stores the information
    as well as knows when to fetch that information, how to interpret it, and what
    is the relevance of the fetched information to your current conversation that
    you are having with your friend. Both you and your friend can understand the context
    and relevance of each other's questions and answers, but machines don't have this
    kind of capability of understanding the context and speaker's intent.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个语用歧义的例子。你和你的朋友正在交谈，你的朋友很久以前告诉你她加入了一个NGO，并且会为贫困学生做一些社会活动。现在你问她社会活动如何。在这种情况下，你和你的朋友都知道你们在谈论的是什么社会活动。这是因为作为人类，我们的大脑存储了信息，并且知道何时获取这些信息，如何解读这些信息，以及这些信息与当前与你朋友交谈的内容的相关性。你和你的朋友都能理解彼此问题和答案的背景和相关性，但机器并不具备理解背景和说话者意图的能力。
- en: This is what we expect from an intelligent machine. We want the machine to understand
    this kind of complex situation as well. Enabling this kind of capability of resolving
    pragmatics ambiguity is included in MSI. This will definitely be possible in future,
    but right now, we are at a stage where machines are trying to adopt AGI and using
    statistical techniques to understand semantics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对智能机器的期望。我们希望机器也能理解这种复杂的情况。使机器能够解决语用歧义的能力被纳入了MSI。这在未来肯定是可能的，但目前我们正处于机器试图采用AGI并使用统计技术来理解语义的阶段。
- en: Natural language generation
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言生成
- en: 'NLG is an area where we are trying to teach machines how to generate NL in
    a sensible manner. This, in itself, is a challenging AI task. Deep learning has
    really helped us perform this kind of challenging task. Let me give you an example.
    If you are using Google''s new inbox, then you may notice that when you reply
    to any mail, you will get three most relevant replies in the form of sentences
    for the given mail. Google used millions of e-mails and made an NLG model that
    was trained using deep learning to generate or predict the most relevant reply
    for any given mail. You can refer to *Figure 9.8*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: NLG是我们尝试教机器如何以合理的方式生成自然语言的一个领域。这本身就是一项具有挑战性的AI任务。深度学习在帮助我们完成这种具有挑战性的任务方面确实起到了重要作用。让我给你举个例子。如果你正在使用谷歌的新收件箱，你可能会注意到，当你回复任何邮件时，你会得到三条最相关的回复，以句子的形式出现。谷歌使用了数百万封电子邮件，创建了一个NLG模型，并通过深度学习训练它，从而生成或预测任何给定邮件的最相关回复。你可以参考*图
    9.8*：
- en: '![](img/5d3f8aff-2f1b-416f-bbb4-014607e475c5.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d3f8aff-2f1b-416f-bbb4-014607e475c5.png)'
- en: 'Figure 9.8: Google''s new inbox smart reply'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：谷歌新收件箱的智能回复
- en: 'Apart from this application, there is another application: after seeing the
    images, the machine will provide a caption of a particular image. This is also
    an NLG application that uses deep learning. The task of generating language is
    less complex than the generation of NL, that is, coherence, and this is where
    we need AGI.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个应用，还有另一个应用：在看到图片后，机器会提供特定图片的说明。这也是一个使用深度学习的NLG应用。生成语言的任务比生成自然语言（NL）要简单一些，即连贯性，而这正是我们需要AGI的地方。
- en: We have talked a lot about the term, deep learning, but how does it actually
    work and why is it so promising? This we will see in an upcoming section of this
    chapter. We will explain the coding part for NLU and NLG applications. We are
    also going to develop NLU and NLG applications from scratch. Before that, you
    must understand the concepts of ANN and deep learning. I will include mathematics
    in upcoming sections and try my best to keep it simple. Let's dive deep into the
    world of ANN and deep learning!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经谈论了很多“深度学习”这个术语，但它到底是如何工作的，为什么它如此有前景呢？这个问题我们将在本章的后续部分中探讨。我们将解释NLU和NLG应用的编码部分。我们还将从零开始开发NLU和NLG应用。在此之前，你必须了解人工神经网络（ANN）和深度学习的概念。我将在后续章节中加入数学内容，并尽量保持简洁。让我们深入探索人工神经网络和深度学习的世界！
- en: A brief overview of deep learning
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习概述
- en: 'Machine learning is a sub-branch of AI and deep learning is a sub-branch of
    ML. Refer to *Figure 9.9*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是AI的一个子领域，深度学习是机器学习的一个子领域。请参见*图 9.9*：
- en: '![](img/335cc3a3-947b-4a28-b9b8-5fab7d792e9b.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/335cc3a3-947b-4a28-b9b8-5fab7d792e9b.png)'
- en: 'Figure 9.9: Deep learning as a sub-branch of ML'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：深度学习作为机器学习的一个子领域
- en: Deep learning uses ANN that is not just one or two layers, but many layers deep,
    called **deep neural network** (**DNN**). When we use DNN to solve a given problem
    by predicting a possible result for the same problem, it is called **deep learning**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使用人工神经网络（ANN），其不仅仅是一个或两个层，而是多个层，称为**深度神经网络**（**DNN**）。当我们使用DNN来解决某个问题，并通过预测该问题的可能结果时，这就是**深度学习**。
- en: Deep learning can use labeled data or unlabeled data, so we can say that deep
    learning can be used in supervised techniques as well as unsupervised techniques.
    The main idea of using deep learning is that using DNN and a humongous amount
    of data, we want the machines to generalize the particular tasks and provide us
    with a result that we think only humans can generate. Deep learning includes a
    bunch of techniques and algorithms that can help us solve various problems in
    NLP such as machine translation, question answering system, summarization, and
    so on. Apart from NLP, you can find other areas of applications such as image
    recognition, speech recognition, object identification, handwritten digit recognition,
    face detection, and artificial face generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习可以使用带标签的数据或不带标签的数据，因此我们可以说，深度学习可以应用于监督学习技术和无监督学习技术。使用深度学习的主要思想是，利用深度神经网络（DNN）和大量数据，我们希望让机器概括特定任务，并为我们提供一个我们认为只有人类才能生成的结果。深度学习包括一系列技术和算法，可以帮助我们解决自然语言处理（NLP）中的各种问题，例如机器翻译、问答系统、摘要等。除了NLP外，您还可以找到其他应用领域，如图像识别、语音识别、物体识别、手写数字识别、面部检测和人造面孔生成等。
- en: 'Deep learning seems promising to us in order to build AGI and ASI. You can
    see some of the applications where deep learning has been used, in *Figure 9.10*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习似乎在构建AGI（通用人工智能）和ASI（超人工智能）方面给我们带来了希望。您可以在*图9.10*中看到一些深度学习应用的实例：
- en: '![](img/40f17204-35b6-4839-a335-e0562411b318.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40f17204-35b6-4839-a335-e0562411b318.png)'
- en: 'Figure 9.10: Applications using deep learning (Image credit: http://www.fullai.org/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：使用深度学习的应用（图片来源：http://www.fullai.org/）
- en: This section gives you a brief overview about deep learning. We will see many
    aspects of deep learning in this chapter, but before that, I want to explain concepts
    that are related to deep learning and ANN. These concepts will help you understand
    the technicality of deep learning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将简要概述深度学习。我们将在本章中讨论深度学习的多个方面，但在此之前，我想解释一些与深度学习和人工神经网络（ANN）相关的概念。这些概念将帮助您理解深度学习的技术性内容。
- en: Basics of neural networks
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络基础
- en: The concept of neural networks is one of the oldest techniques in ML. Neural
    network is derived from the human brain. In this section, we will see the human
    brain's components and then derive the ANN.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的概念是机器学习（ML）中最古老的技术之一。神经网络来源于人类大脑。在本节中，我们将首先了解人脑的组成部分，然后推导出人工神经网络（ANN）。
- en: In order to understand ANN, we first need to understand the basic workflow of
    the human brain. You can refer to F*igure 9.11:*
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解人工神经网络（ANN），我们首先需要理解人脑的基本工作流程。您可以参考*图9.11*：
- en: '![](img/a2cf65ec-ebf8-470e-af8a-3d7c730d07c1.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2cf65ec-ebf8-470e-af8a-3d7c730d07c1.png)'
- en: 'Figure 9.11: Neurons of human brain (Image credit: https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：人脑神经元（图片来源：https://en.wikipedia.org/wiki/File:Blausen_0657_MultipolarNeuron.png）
- en: 'The human brain consists of an estimated hundreds of billion nerve cells called
    **neurons**. Each neuron performs three jobs that are mentioned as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑由大约数百亿个神经细胞组成，这些细胞称为**神经元**。每个神经元执行以下三个任务：
- en: 'Receiving a signal: It receives a set of signals from its **dendrites**'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收信号：它从**树突**接收一组信号。
- en: 'Deciding to pass the signal to the cell body: It integrates those signals together
    to decide whether or not the information should be passed on to the cell body'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定是否将信号传递到细胞体：它将这些信号整合在一起，决定是否将信息传递到细胞体。
- en: 'Sending the signal: If some of the signals pass a certain threshold, it sends
    these signals, called **action potentials**, onward via its axon to the next set
    of neurons'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发送信号：如果一些信号超过某个阈值，它会通过轴突将这些信号（称为**动作电位**）传递到下一个神经元。
- en: 'You can refer to *Figure 9.12*, which demonstrate components that are used
    to perform these three jobs in the biological neural network:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考*图9.12*，它展示了用于执行生物神经网络中三项任务的组件：
- en: '![](img/0e260ec9-4f13-4237-afbc-fb74a9120076.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e260ec9-4f13-4237-afbc-fb74a9120076.png)'
- en: 'Figure 9.12: Demonstrates the components that performs the three jobs'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：展示执行这三项任务的组件
- en: 'This is a very brief overview on how our brain learns and processes some decision.
    Now the question is: can we build an ANN that uses a non-biological substrate
    like silicon or other metal? We can build it, and then by providing a lot of computer
    power and data, we can solve the problems much faster as compared to humans.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是我们的大脑如何学习和处理决策的一个非常简要的概述。现在的问题是：我们能否构建一个使用非生物基材（如硅或其他金属）的人工神经网络？我们可以构建它，然后通过提供大量的计算能力和数据，我们可以比人类更快地解决问题。
- en: ANN is a biologically inspired algorithm that learns to identify the pattern
    in the dataset.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是一种受生物启发的算法，能够学习识别数据集中的模式。
- en: We have seen a brief history of ANN earlier in this chapter, but now it's time
    to see ANN and its history in detail.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章之前已经简要了解了人工神经网络的历史，现在是时候详细了解人工神经网络及其历史了。
- en: The first computation model of the neuron
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一个神经元计算模型
- en: 'In mid-1943, researchers McCulloch-Pitts invented the first computation model
    of a neuron. Their model is fairly simple. The model has a neuron that receives
    binary inputs, sums them, and, if the sum exceeds a certain threshold value, then
    output is one, if not then output is zero. You can see the pictorial representation
    in *Figure 9.13*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1943 年中期，研究人员 McCulloch 和 Pitts 发明了第一个神经元计算模型。他们的模型相当简单。该模型有一个神经元，它接收二进制输入，求和，如果总和超过某个阈值，则输出为
    1，否则输出为 0。你可以在 *图 9.13* 中看到它的示意图：
- en: '![](img/fca196dd-1bd8-44ae-959a-c85e4ae2733c.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fca196dd-1bd8-44ae-959a-c85e4ae2733c.png)'
- en: 'Figure 9.13: McCulloch-Pitts computation model of neuron (Image credit for
    NN: http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：McCulloch-Pitts 神经元计算模型（神经网络图片来源：http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html）
- en: It looks very simple, but as it was invented in the early days of AI, an invention
    of this kind of model was a really big deal.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来非常简单，但由于它是在人工智能的早期发明的，这种模型的发明当时是一项非常了不起的成就。
- en: Perceptron
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机
- en: After a few years of the invention of the first computational model of neuron,
    a psychologist named Frank Rosenblatt found out that the McCulloch-Pitts model
    did not have the mechanism to learn from the input data. So he invented the neural
    network that was built on the idea of the first computational model of neuron.
    Frank Rosenblatt called this model the **perceptron**. It is also called a **single-layer
    feedforward neural network**. We call this model a feed forward neural network
    because, in this neural network, data flows in only one direction--the forward
    direction.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在发明第一个神经元计算模型的几年后，一位名叫 Frank Rosenblatt 的心理学家发现，McCulloch-Pitts 模型没有从输入数据中学习的机制。因此，他发明了建立在第一个神经元计算模型基础上的神经网络。Frank
    Rosenblatt 将这个模型称为 **感知机**。它也被称为 **单层前馈神经网络**。我们称这个模型为前馈神经网络，因为在这个神经网络中，数据只沿一个方向流动——前向方向。
- en: 'Now let''s understand the working of the perceptron that has incorporated the
    idea of having weights on the given inputs. If you provide some training set of
    input output examples, it should learn a function from it by increasing and decreasing
    the weights continuously for each of the training examples, depending on what
    was the output of the given input example. These weight values are mathematically
    applied to the input such that after each iteration, the output prediction gets
    more accurate. This whole process is called **training**. Refer to *Figure 9.14*
    to understand the schematic of Rosenblatt''s perceptron:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解感知机的工作原理，感知机引入了对给定输入进行加权的概念。如果你提供一些输入输出示例的训练集，它应该通过对每个训练示例的权重不断增加或减少来学习一个函数，具体取决于给定输入示例的输出是什么。这些权重值在数学上应用于输入，使得每次迭代后，输出预测变得更加准确。整个过程被称为
    **训练**。请参考 *图 9.14* 来理解 Rosenblatt 感知机的示意图：
- en: '![](img/db870c10-9eaf-4c45-8c20-98c94b354709.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db870c10-9eaf-4c45-8c20-98c94b354709.png)'
- en: 'Figure 9.14: Schematic of Rosenblatt''s perceptron (Image credit for NN: http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14：Rosenblatt 感知机的示意图（神经网络图片来源：http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html）
- en: We will see the ANN-related mathematical concepts such as gradient descent,
    activation function, and loss function in the next section. So get ready for some
    mathematics!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到与人工神经网络相关的数学概念，如梯度下降、激活函数和损失函数。所以，准备好迎接一些数学内容吧！
- en: Understanding mathematical concepts for ANN
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解人工神经网络（ANN）的数学概念
- en: 'This section is very important because ML, ANN, and DL use a bunch of mathematical
    concepts and we are going to see some of the most important ones. These concepts
    will really help you optimize your ML, ANN, and DL models. We will also see different
    types of activation functions and some tips about which activation function you
    should select. We are going to see the following mathematical concepts:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本节非常重要，因为机器学习（ML）、人工神经网络（ANN）和深度学习（DL）使用了大量的数学概念，我们将看到其中一些最重要的概念。这些概念将真正帮助你优化你的机器学习、人工神经网络和深度学习模型。我们还将看到不同类型的激活函数，并提供一些选择激活函数的小贴士。我们将要学习以下数学概念：
- en: Gradient descent
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降法
- en: Activation function
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Loss function
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数
- en: Gradient descent
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降法
- en: Gradient descent is a very important optimization technique that has been used
    by almost any neural network. In order to explain these techniques, I want to
    give an example. I have a dataset of students' scores and hours of study for each
    of the students. We want to predict the test scores of a student just by his amount
    of hours of study. You would say that this looks like an ML linear regression
    example. You are right; we are using linear regression to make a prediction. Why
    linear regression and what is the connection with gradient descent? Let me answer
    this and then we will see the code and some cool visualization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法是一种非常重要的优化技术，几乎所有的神经网络都在使用它。为了说明这些技术，我想给出一个例子。我有一个数据集，包含了每个学生的考试成绩和学习时长。我们希望仅根据学生的学习时长来预测考试成绩。你可能会说，这看起来像是一个机器学习线性回归的例子。你说得对；我们正在使用线性回归来进行预测。那么，为什么使用线性回归？梯度下降与它有什么关系呢？让我来回答这个问题，然后我们将看看代码和一些有趣的可视化。
- en: Linear regression is the ML technique that uses statistical methods and allows
    us to study relationships between two continuous quantitative variables. Here,
    those variables are students' scores and students' hours of study. Usually in
    linear regression, we try to get a line that is the best fit for our dataset,
    which means that whatever calculation we are doing is just to get a line of the
    best fit for the given dataset. Getting this line of the best fit is the goal
    of linear regression.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是机器学习中的一种技术，利用统计方法，允许我们研究两个连续的定量变量之间的关系。在这里，这两个变量是学生的成绩和学生的学习时长。通常在进行线性回归时，我们试图得到一条最适合我们数据集的直线，这意味着我们所做的所有计算，目的都是为了找到一条最适合给定数据集的直线。得到这条最适合的直线就是线性回归的目标。
- en: Let's talk about the connection of linear regression with gradient descent.
    Gradient descent is the most popular optimization technique that we are using
    to optimize the accuracy of the linear regression and minimize the loss or error
    function. Gradient descent is the technique that helps us minimize our error function
    and maximize our prediction accuracy. The mathematical definition for gradient
    descent is that it is a first-order iterative optimization algorithm. This algorithm
    is used to find a local minimum of a function using gradient descent. Each taken
    step is proportional to the negative of the gradient of the function at the current
    point. You can think of gradient descent using this real-life example. Suppose
    you are at the top of a mountain and now you want to reach the bottom that has
    a beautiful lake, so you need to start descending it. Now you don't know in what
    direction you should start walking. In this case, you observe the land near you
    and try to find the way where the land tends to descend. This will give you an
    idea in what direction you should go. If you take your first steps in the descending
    direction and each time you follow the same logic, then it is very likely that
    you would reach the lake. This is exactly what we are doing with the mathematical
    formula for gradient descent. In ML and DL, we think about everything in terms
    of optimization so gradient descent is the technique that is used to minimize
    the loss function over time.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论线性回归与梯度下降的关系。梯度下降是我们用来优化线性回归准确度并最小化损失或误差函数的最流行优化技术。梯度下降是一种帮助我们最小化误差函数并最大化预测准确度的技术。梯度下降的数学定义是它是一种一阶迭代优化算法。该算法用于通过梯度下降找到函数的局部最小值。每一步的大小与当前点的梯度的负值成正比。你可以通过这个现实生活中的例子来理解梯度下降：假设你站在一座山顶，现在你想要走到山脚下的美丽湖泊，你需要开始下山。此时，你不知道应该朝哪个方向走。你观察周围的地形，尝试找到地形倾斜的地方。这样你就能大致知道应该往哪个方向走。如果你开始按照下坡的方向走，并且每次都遵循这个思路，那么你很有可能会到达湖泊。这正是我们在使用梯度下降的数学公式时所做的。在机器学习和深度学习中，我们把一切都看作是优化问题，所以梯度下降是用于随着时间推移最小化损失函数的技术。
- en: 'The other example is you have a deep bowl in which you put a small ball from
    its one end. You can observe that after some time, the ball reduces its speed
    and tries to reach the base of the bowl. Refer to *Figure 9.15*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，你有一个深碗，里面放了一个小球，从碗的一端开始。你会发现，经过一段时间后，小球减慢了速度，并试图到达碗底。参见*图 9.15*：
- en: '![](img/0ccd7026-a2d1-4e25-8359-f6f415e00701.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ccd7026-a2d1-4e25-8359-f6f415e00701.png)'
- en: 'Figure 9.15: Intuition of gradient descent (Image credit: https://iamtrask.github.io/img/sgd_optimal.png)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：梯度下降的直观理解（图片来源：https://iamtrask.github.io/img/sgd_optimal.png）
- en: 'You must also check out the image given at this GitHub link:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须查看这个 GitHub 链接中的图片：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
- en: This figure shows the process or steps of getting the line of best fit using
    gradient descent. It's just the visualization that gives you an overall idea of
    what we are going to do in the code. By the way, loss function, error function,
    and cost function are synonyms of each other. Gradient descent is also known as
    **steepest descent**.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了使用梯度下降得到最佳拟合线的过程或步骤。它只是一个可视化图，帮助你大致了解我们将在代码中做的事情。顺便提一下，损失函数、误差函数和代价函数是同义词。梯度下降也被称为**最陡下降**。
- en: Now let's get to the code and I will explain things as we go. Here, we are not
    developing a predictive model; we will implement and understand gradient descent.
    The dataset and code is at [https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample](https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看代码，并在过程中进行解释。在这里，我们不是在开发预测模型，而是实现和理解梯度下降。数据集和代码可以在[https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample](https://github.com/jalajthanaki/NLPython/tree/master/ch9/gradientdescentexample)找到。
- en: 'First of all, let''s understand the dataset. It is the dataset of the students''
    test scores and the amount of hours they studied. We know that between these two
    attributes, there should be a relationship--the less amount you study, poorer
    is the score of the student, and the more you study, better the score will be.
    We will prove the relationship using linear regression. The **X** value means
    the first column of the dataset, that is, the amount of hours the student studies,
    and **Y** value means the second column, which is the test score. Refer to *Figure
    9.16*:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解一下数据集。它是学生的测试分数和他们学习的小时数的数据集。我们知道这两个属性之间应该存在某种关系——学习的时间越少，学生的分数越差；学习的时间越多，分数就越高。我们将使用线性回归来证明这一关系。**X**
    值表示数据集的第一列，也就是学生学习的小时数，**Y** 值表示第二列，即测试分数。请参见*图 9.16*：
- en: '![](img/e4f467a3-28fe-4951-a6cb-8afb47dd4033.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4f467a3-28fe-4951-a6cb-8afb47dd4033.png)'
- en: 'Figure 9.16: Sample data from the dataset'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：数据集的样本数据
- en: Let's define the main function that is going to read our dataset and some basic
    hyperparameters. We have also called a function that we will use to compute the
    errors and actual gradient descent. You can see the code snippet in *Figure 9.17:*
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个主函数，它将读取我们的数据集以及一些基本的超参数。我们还调用了一个函数，用来计算误差和实际的梯度下降。你可以在*图 9.17*中看到代码片段：
- en: '![](img/d364e86d-0991-4537-879a-a3f8c1030013.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d364e86d-0991-4537-879a-a3f8c1030013.png)'
- en: 'Figure 9.17: Code snippet of gradient descent'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17：梯度下降的代码片段
- en: 'You can see the output in *Figure 9.18*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 9.18*中看到输出结果：
- en: '![](img/d9edc5f4-b226-47c6-b0cb-c4405e178a48.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9edc5f4-b226-47c6-b0cb-c4405e178a48.png)'
- en: 'Figure 9.18: Output of gradient descent'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18：梯度下降的输出结果
- en: 'As you can see in *Figure 9.18*, we have called two functions: `compute_error_for_line_points()`,
    which will compute the error between the actual value and predicted value, and
    `gradient_descent_runner()`, which will calculate the gradient for us. We need
    to understand first how we are going to calculate errors and then gradient descent.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*图 9.18*中看到的，我们调用了两个函数：`compute_error_for_line_points()`，它将计算实际值和预测值之间的误差，和`gradient_descent_runner()`，它将为我们计算梯度。我们需要先了解如何计算误差，然后再进行梯度下降。
- en: Calculating error or loss
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算误差或损失
- en: 'There are many ways to calculate the error for ML algorithms, but in this chapter
    we will be using one of the most popular techniques: sum of squared distance error.
    Now we are going straight into details.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机器学习算法误差的方法有很多，但在本章中我们将使用其中最流行的技术之一：平方和误差。现在我们将直接进入细节。
- en: 'What does this error function do for us? Recall our goal: we want to get the
    line of best fit for our dataset. Refer to *Figure 9.19*, which is the equation
    of line slope. Here, *m* is the slope of line, *b* is the *y* intercept, *x* and
    *y* are the data points--in our case, *x* is the numbers of hours the student
    studies and *y* is the test score. Refer to *Figure 9.19:*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个误差函数为我们做了什么？回想一下我们的目标：我们希望得到数据集的最佳拟合直线。请参见*图 9.19*，它是直线斜率的方程式。这里，*m* 是直线的斜率，*b*
    是 *y* 截距，*x* 和 *y* 是数据点——在我们的例子中，*x* 是学生学习的小时数，*y* 是测试分数。请参见*图 9.19*：
- en: '![](img/e2cb6dfd-29ae-4090-9ad4-5e03a397f7b7.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2cb6dfd-29ae-4090-9ad4-5e03a397f7b7.png)'
- en: 'Figure 9.19: Line slope equation (Image credit: https://www.tes.com/lessons/Xn3MVjd8CqjH-Q/y-mx-b)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19：直线斜率方程（图片来源：https://www.tes.com/lessons/Xn3MVjd8CqjH-Q/y-mx-b）
- en: Using the preceding equation, we are drawing the line and starting with random
    values of slope *m* and *y* intercept *b* and using our first column data points
    as value of x so that we get a value of *y*. In the training data, we already
    have *y* values, which means that we know the test score of each student. So for
    each student, we need to calculate the error. Let's take a very intuitive example;
    note that we are working with dummy values for explanation. Suppose you get *y*
    value of 41.0 by putting a random value of *m* and *b*. Now you have the actual
    value of *y*, that is, 52.5, then the difference between the predicted value and
    real value is 11.5\. This is just for a single data point but we need to calculate
    for every data point. So to do this kind of error calculation, we are using sum
    of squared distance error.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的方程，我们绘制了直线并从随机的斜率 *m* 和 *y* 截距 *b* 开始，同时使用我们的第一列数据点作为 *x* 的值，从而计算出 *y*
    的值。在训练数据中，我们已经有了 *y* 的值，也就是说，我们知道每个学生的考试成绩。因此，对于每个学生，我们需要计算误差。让我们举一个非常直观的例子；请注意，我们使用的是虚拟值来进行解释。假设你通过随机选取的
    *m* 和 *b* 值得到 *y* 的值为 41.0。现在，你有了 *y* 的实际值，即 52.5，那么预测值与实际值之间的差异是 11.5。这只是针对单一数据点的情况，但我们需要对每个数据点进行计算。因此，为了进行这种误差计算，我们使用了平方距离误差之和。
- en: Now how are we calculating the sum of squared distance error and why are we
    using sum of squared distance error?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们是如何计算平方距离误差之和的，为什么我们要使用平方距离误差之和呢？
- en: 'So let''s begin with the first question, the equation to calculate sum of squared
    distance error is given in *Figure 9.20*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们从第一个问题开始，计算平方距离误差之和的方程见 *图 9.20*：
- en: '![](img/8931db5e-3324-424f-85ed-a696f34d7cb6.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8931db5e-3324-424f-85ed-a696f34d7cb6.png)'
- en: 'Figure 9.20: Equation for calculating sum of squared distance error (Image
    credit: https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.20：计算平方距离误差之和的方程（图片来源：[https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png](https://spin.atomicobject.com/wp-content/uploads/linear_regression_error1.png)）
- en: As you can see, the last part *mx[i]+b* is the line that we have drawn by choosing
    a random value of *m* and *b* and we can actually put *y* in place of *mx[i]+b*.
    So here, we are calculating the difference between the original *y* value with
    the generated *y* value. We are subtracting the original *y* value and generated
    *y* value and squaring this value for each data point. We are squaring the value
    because we don't want to deal with negative values as we are performing sum after
    calculating square and we want to measure the overall magnitude. We don't want
    the actual value as we are trying to minimize this overall magnitude. Now back
    to the equation; we have calculated the square of the difference of the original
    *y* value and the generated *y* value. Now we are performing summation for all
    these points; we will use the sigma notation to indicate the summation operation
    for all the data points in the dataset. By this time, we have the sum value that
    indicates the error magnitude and we will divide these values by the total number
    of data points. After this, we will get the actual error value that we want.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，最后部分的 *mx[i]+b* 是我们通过选择随机的 *m* 和 *b* 值绘制出的直线，实际上我们可以将 *y* 替换为 *mx[i]+b*。因此，这里我们计算了原始
    *y* 值与生成的 *y* 值之间的差异。我们将原始 *y* 值与生成的 *y* 值相减，并对每个数据点的差值进行平方。我们平方该值是因为我们不想处理负值，在进行求和操作之前，计算平方后，我们希望衡量总体的大小。我们不关心实际值，因为我们希望最小化这个总体大小。现在回到方程；我们已经计算出了原始
    *y* 值与生成的 *y* 值之间差异的平方。接下来，我们对所有这些点进行求和；我们将使用 sigma 符号表示对数据集中所有数据点的求和操作。到此为止，我们得到了表示误差大小的总和值，并将这些值除以数据点的总数。经过这个步骤，我们就得到了我们想要的实际误差值。
- en: 'You can see the animated image at this GitHub link:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个 GitHub 链接查看动画图片：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif](https://github.com/jalajthanaki/NLPython/blob/master/ch9/gradientdescentexample/gradient_descent_example.gif)'
- en: 'You can see that the line is moving for each iteration in order to generate
    the line with the best fit for our dataset. We are updating the value of *m* and
    *b* according to the value of our error. Now, for each timestamp, the line is
    static and we need to calculate the error. Refer to *Figure 9.21*:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，每次迭代中，直线都会移动，以生成与我们数据集最佳匹配的直线。我们根据误差的值更新 *m* 和 *b* 的值。现在，对于每个时间戳，直线是静止的，我们需要计算误差。请参见
    *图 9.21*：
- en: '![](img/55438cce-8392-482b-89c1-518e5710f473.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55438cce-8392-482b-89c1-518e5710f473.jpg)'
- en: 'Figure 9.21: Calculating distance between line and data points at given timestamp
    (Image credit: http://statutor.org/c3/glmregression/IMAG004.JPG)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.21：计算给定时间戳时直线与数据点之间的距离（图片来源：http://statutor.org/c3/glmregression/IMAG004.JPG）
- en: Now we need to express our intuitive example and equation in a technical manner
    as per the given equation. Here, we are calculating the distance from each data
    point to the line that we have drawn, squaring them, summing them all together,
    and then dividing by the total number of points. So, after every iteration or
    timestamp, we can calculate our error value and get to know how bad our line is
    or how good our line is. If our line is bad, then to get the line of best fit,
    we update the values of *m* and *b*. So the error value provides us with an indication
    of whether there is a possibility of improvement or not in order to generate the
    line of best fit. So eventually we want to minimize the value of error that we
    are getting here in order to generate the line of best fit. How will we minimize
    this error and generate the line of best fit? The next step is called **gradient
    descent**.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要根据给定的方程式以技术方式表达我们直观的例子和方程式。在这里，我们计算每个数据点到我们绘制的直线的距离，对它们进行平方，将它们全部加起来，然后除以总点数。因此，在每次迭代或时间戳后，我们可以计算我们的误差值，并了解我们的直线有多糟糕或多好。如果我们的直线不好，那么为了得到最佳拟合线，我们会更新*m*和*b*的值。因此，误差值为我们提供了一种指示，即是否有可能改进以生成最佳拟合线。最终，我们希望最小化我们在这里获得的误差值，以生成最佳拟合线。我们如何最小化这个误差并生成最佳拟合线呢？下一步被称为**梯度下降**。
- en: A couple of reasons for sum of squared error is that, for linear regression,
    this is the most popular technique to calculate the error. You can also use this
    if you have a large dataset.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于平方误差的一些原因是，对于线性回归来说，这是计算误差的最流行技术。如果您有大数据集，您也可以使用这个技术。
- en: 'Let''s see the coding part and then we will jump to our core part of calculating
    gradient descent. See the code snippet in *Figure 9.22*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看编码部分，然后我们将跳转到计算梯度下降的核心部分。参考*图9.22*中的代码片段：
- en: '![](img/6a21e70f-f0b9-470a-8ade-14ad3528328d.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a21e70f-f0b9-470a-8ade-14ad3528328d.png)'
- en: 'Figure 9.22: Code snippet for calculating sum of squared error'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.22：计算平方误差和的代码片段
- en: Now let's see the next step--calculating gradient descent.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看下一步--计算梯度下降。
- en: Calculating gradient descent
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算梯度下降
- en: 'Using the error function, we know whether we should update our line in order
    to generate the line of best fit or not, but how to update the line is what we
    are going to see in this section. How will we minimize this error and generate
    the line of best fit? So to answer this question, first of all, let''s get some
    basic understanding about gradient descent and the coding part, where we are just
    left with our one last function, `gradient_descent_runner()`. Refer to *Figure
    9.23*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用误差函数，我们知道是否应该更新我们的直线以生成最佳拟合线，但如何更新直线是我们将在本节中看到的内容。我们如何最小化这个误差并生成最佳拟合线呢？因此，为了回答这个问题，首先让我们对梯度下降和编码部分有一些基本的理解，其中我们只剩下最后一个函数`gradient_descent_runner()`。参考*图9.23*：
- en: '![](img/28309b03-4d15-4525-97e6-8b57c4eb05fb.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28309b03-4d15-4525-97e6-8b57c4eb05fb.png)'
- en: 'Figure 9.23: A 3D graph for understanding gradient descent (Image credit: https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_surface.png)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.23：理解梯度下降的3D图表（图片来源：https://spin.atomicobject.com/wp-content/uploads/gradient_descent_error_surface.png）
- en: As you can see in *Figure 9.23*, this is a three dimensional graph. These two
    graphs are the same; their viewing angle is different. So these graphs show all
    the possible values of slope *m*, *y*-intercept *b,* and *error*. These are pairs
    of three values including *m*, *b,* and *error*. Here, the *x* axis is a slope
    value, the *y* axis is a *y*-intercept, and the *z* axis is the error value. We
    try to get the point where the error is the least. If you see the graph carefully,
    then you can observe that at the bottom of the curve, the error value is the least.
    The point where the value is the least is called **local minima** in ML. In complex
    datasets, you may find multiple local minima; here our dataset is simple so we
    have a single local minima. If you have a complex and high-dimensional dataset
    where you have multiple local minima, then you need to do a second-order optimization
    to decide which local minima you should choose for better accuracy. We are not
    going to see second-order optimization in this book. Now let's look back to our
    graph where we can visually identify the point that gives us the smallest error
    value and the same point also gives us the ideal value of *y*-intercept that is
    *b* and slope value that is *m*. When we get the ideal value for *b* and *m*,
    we will put these values in our *y=mx+c* equation and then magic will happen and
    we will get the line of best fit. This is not the only way to get the line of
    best fit, but my motive is to give you an in-depth idea about gradient descent
    so that we can later use this concept in DL.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在*图9.23*中看到的，这是一张三维图。 这两个图是相同的；它们的视角不同。 所以这些图显示了斜率*m*、*y*-截距*b*和*误差*的所有可能值。这些是包含*m*、*b*和*误差*的三个值的对。这里，*x*轴是斜率值，*y*轴是*y*-截距，*z*轴是误差值。我们试图找到误差最小的点。如果你仔细观察图形，你会发现曲线的底部，误差值最小。值最小的点在机器学习中被称为**局部最小值**。在复杂的数据集中，你可能会发现多个局部最小值；但在这里我们的数据集很简单，所以只有一个局部最小值。如果你有一个复杂且高维的数据集，其中有多个局部最小值，那么你需要进行二阶优化，以决定选择哪个局部最小值以获得更好的准确性。在本书中我们不会讲解二阶优化。现在让我们回到图形，在这里我们可以直观地确定给我们最小误差值的点，且同一个点也给出了理想的*y*-截距*b*和斜率值*m*。当我们得到*b*和*m*的理想值时，我们将这些值代入我们的*y=mx+c*方程式，然后奇迹就会发生，我们会得到最佳拟合线。这不是唯一的获取最佳拟合线的方法，但我的目的是让你深入了解梯度下降的概念，以便我们以后可以在深度学习中使用这个概念。
- en: Now visually, you can see the smallest point where the error is the smallest,
    but how to reach this point? The answer is by calculating the gradient. Gradient
    is also called **slope** but this is not the slope value *m* so don't get confused.
    We are talking about slope in the direction of getting us to that smallest error
    point. So we have some *b* value and some *m* value and after every iteration,
    we update these *b* and *m* values so that we can reach that smallest error value
    point. So in perspective of the three dimensional image, if you are at the top
    of the curve, for every iteration we calculate the gradient and error and then
    update the values of *m* and *b* to reach the bottom of that curve. We need to
    reach to the bottom of the curve and by calculating the gradient value, we get
    an idea about the direction in which we should take our next step. So gradient
    is the tangent line that keeps telling us the direction we need to move in, whether
    it's upward or downward, to reach the smallest error point and obtain ideal *b*
    and *m* values to generate the line of best fit. Refer to *Figure 9.24**:*
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从视觉上你可以看到误差最小的那个点，但如何到达这个点呢？答案是通过计算梯度。梯度也叫做**斜率**，但这不是斜率值*m*，所以不要混淆。我们在这里讨论的是朝着最小误差点前进的方向的斜率。因此，我们有一些*b*值和一些*m*值，并且每次迭代后，我们都会更新这些*b*和*m*值，以便我们能够到达那个最小误差值的点。从三维图像的角度来看，如果你在曲线的顶部，那么每次迭代我们都会计算梯度和误差，然后更新*m*和*b*的值，以便我们能够到达曲线的底部。我们需要到达曲线的底部，通过计算梯度值，我们可以知道应该朝哪个方向迈出下一步。因此，梯度是切线，它不断告诉我们需要朝哪个方向移动，无论是向上还是向下，以到达最小误差点并获得理想的*b*和*m*值，从而生成最佳拟合线。请参考*图9.24*：
- en: '![](img/878ff758-2c70-4dd2-a661-c83f951ffabd.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/878ff758-2c70-4dd2-a661-c83f951ffabd.png)'
- en: 'Figure 9.24: Gradient values and direction (Image credit: https://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24：梯度值和方向（图片来源：https://sebastianraschka.com/images/faq/closed-form-vs-gd/ball.png）
- en: 'Now let''s see the last but not least equation of calculating gradient descent.
    In *Figure 9.25*, you can see the equations of gradient descent that are nothing
    but a partial derivative of our error function. We have taken the equation of
    sum of squared error and performed partial derivatives with respect to *m* and
    *b* to calculate gradient descent. The outcome is in *Figure 9.25*:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看最后一个计算梯度下降的方程式。在*图 9.25*中，你可以看到梯度下降的方程式，它们只是我们误差函数的偏导数。我们取了平方误差和的方程，并对*m*和*b*分别进行偏导数计算，从而得出梯度下降。结果如*图
    9.25*所示：
- en: '![](img/29fe39a3-9a7f-4cfd-9db2-1a62096c4983.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29fe39a3-9a7f-4cfd-9db2-1a62096c4983.png)'
- en: 'Figure 9.25: Equations for calculating gradient descent (Image credit: https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.25：计算梯度下降的方程式（图片来源：https://spin.atomicobject.com/wp-content/uploads/linear_regression_gradient1.png）
- en: The left-hand side *∂* symbol is the symbol of partial derivative. Here, we
    have two equations because we take our error function and generate the partial
    derivative with respect to variable *m*, and in the second equation, we generate
    the partial derivative with respect to variable *b*. With these two equations,
    we will get the updated values of *b* and *m*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的*∂*符号是偏导数符号。在这里，我们有两个方程式，因为我们使用误差函数并对变量*m*求偏导数，在第二个方程式中，我们对变量*b*求偏导数。通过这两个方程式，我们将获得更新后的*b*和*m*的值。
- en: To calculate the gradient, we need to derive the partial derivative of the error
    function. For some problems in ML and DL, we don't know the partial derivative
    of the error function, which implies that we can't find the gradient. So we don't
    know how to deal with this kind of function. Your error function should be differentiable,
    which means that your error function should have partial derivatives. Another
    thing here is that we are using the linear equation, but if you have high-dimensional
    data, then you can use the non-linear function if you know the error function.
    Gradient descent doesn't give us the minima when we start for the first time.
    Gradient just tells us how to update our *m* and *b* values, whether we should
    update with a positive value or negative value. So gradient gives us an idea how
    to update values of *m* and *b*, which means that by calculating the gradient,
    we are getting the direction and trying to reach the point where we get the smallest
    error value and best values for *m* and *b*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算梯度，我们需要对误差函数进行偏导数推导。对于机器学习和深度学习中的一些问题，我们可能不知道误差函数的偏导数，这意味着我们无法计算梯度。因此，我们不知道该如何处理这种类型的函数。你的误差函数应该是可微分的，这意味着它应该有偏导数。另一个需要注意的是，我们使用的是线性方程，但如果你的数据是高维的，那么你可以使用非线性函数，只要你知道误差函数。梯度下降并不会在第一次迭代时就给出最小值。梯度只是告诉我们如何更新*m*和*b*的值，更新时是加上正值还是负值。所以梯度给我们一个更新*m*和*b*值的方向，这意味着通过计算梯度，我们得到了方向，并且在尽力找到误差值最小且*m*和*b*最优的点。
- en: 'Now it is time to jump to the code again and finish gradient descent. Refer
    to *Figure 9.26*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候再次跳转到代码并完成梯度下降了。请参考*图 9.26*：
- en: '![](img/98e08ba5-5746-4e21-9bbf-d77310291c34.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98e08ba5-5746-4e21-9bbf-d77310291c34.png)'
- en: 'Figure 9.26: Code snippet of the actual gradient descent runner function'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.26：实际梯度下降运行函数的代码片段
- en: 'In the code, we have multiplied `m_gradient` and `b_gradient` with the learning
    rate, so learning rate is an important hyperparameter. Be careful while selecting
    its value. If you select a very high value, your model may not train at all. If
    you select a very low value, then it would take a lot of time to train and there
    is a chance of overfitting as well. Refer to *Figure 9.27*, which provides you
    with an intuition about a good learning rate:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们将`m_gradient`和`b_gradient`与学习率相乘，因此学习率是一个重要的超参数。在选择其值时要小心。如果你选择一个非常高的值，模型可能根本无法训练。如果选择一个非常低的值，则可能需要很长时间进行训练，并且有过拟合的风险。请参考*图
    9.27*，它能给你关于合适学习率的直觉：
- en: '![](img/3356443c-239a-4cae-b929-f1b2c07852b4.jpeg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3356443c-239a-4cae-b929-f1b2c07852b4.jpeg)'
- en: 'Figure 9.27: Learning rate intuition (Image credit: http://cs231n.github.io/assets/nn3/learningrates.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.27：学习率直觉（图片来源：http://cs231n.github.io/assets/nn3/learningrates.jpeg）
- en: 'This is it for the coding part of linear regression and gradient descent. Let''s
    run the code and *Figure 9.28* will give you an idea about the output:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是线性回归和梯度下降的编码部分。让我们运行代码，*图 9.28*将给你一个关于输出的直觉：
- en: '![](img/50062f5a-ebb4-432e-93eb-b6d0f1e96ebb.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50062f5a-ebb4-432e-93eb-b6d0f1e96ebb.png)'
- en: 'Figure 9.28: Code snippet of output'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.28：输出的代码片段
- en: There are types of gradient descent, so let's name a few of them, but we are
    not going into detail. You can explore gradient descent with momentum, Adagrad,
    Adam, and so on.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降有多种类型，我们列举几个，但我们不会深入探讨。你可以探索带动量的梯度下降、Adagrad、Adam 等等。
- en: 'I will provide you with a link that will be helpful to you if you really want
    to explore more:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想深入探讨，我会提供一个链接，这对你会有帮助：
- en: '[https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/)'
- en: Now it's time to understand the activation function, so let's begin!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候理解激活函数了，让我们开始吧！
- en: Activation functions
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let's see the activation function first. I want to give you an idea at what
    stage of ANN we will use this activation function. In our discussion of the perceptron,
    we said that neural networks will generate an output of one if it exceeds a certain
    threshold; otherwise, the output will be zero. This whole mechanism to calculate
    the threshold and generate the output based on this threshold is taken care by
    the activation function.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看激活函数。我想给你一个大致的概念，说明我们会在神经网络的哪个阶段使用激活函数。在我们讨论感知器时，我们提到神经网络如果超过某个阈值，将生成输出
    1；否则，输出为 0。计算阈值并根据这个阈值生成输出的整个机制由激活函数负责。
- en: Activation functions are able to provide us with values that lie between 0 and
    1\. After this, using our threshold value, we can generate output value 1 or output
    value 0\. Suppose our threshold value is 0.777 and our activation function output
    is 0.457, then our resultant output will be 0; if our activation function output
    is 0.852, then our resultant output will be 1\. So, here is how the activation
    function works in ANN.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数能够提供位于 0 和 1 之间的值。之后，我们可以使用阈值生成输出值 1 或输出值 0。例如，如果我们的阈值是 0.777，激活函数的输出是 0.457，那么我们的结果输出将是
    0；如果激活函数的输出是 0.852，那么结果输出将是 1。所以，这就是激活函数在神经网络中工作的方式。
- en: Usually, in neural networks, we have a certain weight and input value for each
    neuron. We are summing them and generating the weighted sum value. When we pass
    these values through a non-linear function, this non-linear function activates
    certain numbers of neurons to get the output for a complex task; this activation
    process of neurons using certain non-linear mathematical functions is known as
    an **activation function** or **transfer function**. Activation functions map
    input nodes to the output nodes in a certain fashion using certain mathematical
    operations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在神经网络中，我们为每个神经元指定一个权重和输入值。我们对它们求和并生成加权和。当我们将这些值通过一个非线性函数时，这个非线性函数会激活一定数量的神经元，从而得到复杂任务的输出；这种通过特定的非线性数学函数激活神经元的过程称为**激活函数**或**传递函数**。激活函数通过一定的数学运算将输入节点映射到输出节点。
- en: The purpose of having an activation function in ANN is to introduce non-linearity
    in the network. Let's understand this step by step.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工神经网络（ANN）中使用激活函数的目的是引入非线性。让我们一步步理解这个过程。
- en: 'Let''s concentrate on the structure of ANN. This ANN structure can be further
    divided into three sections:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于神经网络的结构。这个结构可以进一步分为三个部分：
- en: '**Architecture:** Architecture is all about deciding the arrangement of neurons
    and layers in the ANN'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构：** 架构是关于决定神经网络中神经元和层次的排列方式。'
- en: '**Activities:** In order to generate the output of complex tasks, we need to
    see the activities of the neurons--how one neuron responds to another to generate
    complex behavior'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动：** 为了生成复杂任务的输出，我们需要观察神经元的活动——一个神经元如何响应另一个神经元，从而生成复杂的行为。'
- en: '**Learning rule:** When ANN generates the output, we need to update our ANN
    weight at each timestamp to optimize the output using the error function'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习规则：** 当神经网络生成输出时，我们需要在每个时间点更新 ANN 的权重，以便使用误差函数优化输出。'
- en: Activation function is part of the activities section. As we mentioned, we will
    introduce non-linearity into the ANN. The reason behind it is that without non-linearity,
    the ANN can't produce complex behavior to solve complex tasks. Most of the time
    in DL, we use non-linear activation functions to get the complex behavior. Apart
    from that, we also want to map our input to the output in a non-linear manner.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是活动部分的一部分。如前所述，我们将引入ANN中的非线性。背后的原因是，如果没有非线性，ANN无法产生复杂行为来解决复杂任务。在深度学习（DL）中，大多数时候我们使用非线性激活函数来获得复杂的行为。除此之外，我们还希望以非线性的方式将输入映射到输出。
- en: If you are not using a non-linear activation function, then the ANN won't give
    you significant amount of useful output for complex tasks because you are passing
    out the matrices, and if you are using more than one layer in your ANN with a
    linear activation function, you get an output that is the summation of the input
    value, weights, and bias from all the layers. This output gives you another linear
    function and that means that this linear function converts the behavior of a multi-layer
    ANN to single-layer ANN. This kind of behavior is not at all useful to solve complex
    tasks.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你没有使用非线性激活函数，那么ANN在处理复杂任务时不会给出大量有用的输出，因为你传递的是矩阵。如果你在ANN中使用多个层且使用线性激活函数，得到的输出将是输入值、权重和偏置在所有层中的求和。这个输出给你另一个线性函数，这意味着这个线性函数将多层ANN的行为转化为单层ANN。这种行为在解决复杂任务时完全没有用。 '
- en: 'I want to highlight the idea of connectionism. The connectionism in ANN is
    to use neurons that are interconnected with each other and produce complex behavior,
    just like human brains, and we cannot achieve this kind of behavior without introducing
    non-linearity in the ANN. Refer to *Figure 9.29* to understand activation function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我想强调连接主义的思想。在人工神经网络（ANN）中，连接主义是指使用彼此相连的神经元来产生复杂的行为，就像人类的大脑一样，我们无法在不引入非线性性的情况下在ANN中实现这种行为。参考*图
    9.29*来理解激活函数：
- en: '![](img/b6a090d1-e8d9-4265-8cb7-e23ef2f2ae4d.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6a090d1-e8d9-4265-8cb7-e23ef2f2ae4d.png)'
- en: 'Figure 9.29: ANN with activation function (Image credit: https://cdn-images-1.medium.com/max/800/0*bWX2_ecf3l6lKyVA.png)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.29：带有激活函数的ANN（图片来源：https://cdn-images-1.medium.com/max/800/0*bWX2_ecf3l6lKyVA.png）
- en: 'Here, we are going to cover these functions mentioned in the preceding image:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍前面图像中提到的这些函数：
- en: '**Transfer potential:** This is the function that aggregates inputs and weights.
    More specifically, this function performs the summation of inputs and weights'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传递潜力：** 这是聚合输入和权重的函数。更具体地说，这个函数执行输入与权重的求和'
- en: '**Activation function:** This function takes the output of the transfer potential
    function as input and applies a non-linear mathematical transformation using an
    activation function'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数：** 该函数将传递潜力函数的输出作为输入，并通过激活函数应用非线性的数学变换'
- en: '**Threshold function:** Based on the activation function, the threshold function
    either activates the neuron or does not activate'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阈值函数：** 基于激活函数，阈值函数要么激活神经元，要么不激活神经元'
- en: 'Transfer potential is a simple summation function that will perform the summing
    of inner dot product of the input to the weights of the connection. You can see
    the equation in *Figure 9.30*:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 传递潜力是一个简单的求和函数，它会对输入与连接权重的内积进行求和。你可以在*图 9.30*中看到该方程：
- en: '![](img/b062cfeb-aadd-4c24-a36b-07c72963f899.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b062cfeb-aadd-4c24-a36b-07c72963f899.png)'
- en: 'Figure 9.30: Summation equation for transfer potential (Image credit: https://cdn-images-1.medium.com/max/800/0*005k9F1JxQ0oKEeM.png)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.30：传递潜力的求和方程（图片来源：https://cdn-images-1.medium.com/max/800/0*005k9F1JxQ0oKEeM.png）
- en: This transfer potential is generally a dot product, but it can use any mathematical
    equation such as a multi-quadratic function.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个传递潜力通常是一个点积，但也可以使用任何数学方程，如多项式函数。
- en: The activation function, on the other hand, should be any differentiable and
    non-linear function. It needs to be differentiable so that we can calculate the
    error gradient, and this function has to have a non-linear property to gain complex
    behavior from the neural network. Typically, we are using the sigmoid function
    as activation function, which takes the transfer potential output value as input
    to calculate the final output and then calculates the error between our actual
    output and the generated one. Then, we will use the concept of calculating the
    gradient of error as well as applying a backpropagation optimization strategy
    in order to update the weight of the connection of the ANN.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，激活函数应该是任何可微且非线性的函数。它需要是可微的，以便我们可以计算误差梯度，并且这个函数必须具备非线性特性，才能从神经网络中获得复杂行为。通常，我们使用sigmoid函数作为激活函数，它将转移势能的输出值作为输入，计算最终输出，然后计算实际输出与生成输出之间的误差。接着，我们将利用计算误差梯度的概念，并应用反向传播优化策略来更新人工神经网络（ANN）连接的权重。
- en: '*Figure 9.31* expresses the transfer potential function in terms of theta,
    also called the **logit**, which we will be using in the equation of the logistic
    sigmoid activation function:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9.31*表示了转移势能函数，形式为theta，也称为**logit**，我们将在logistic sigmoid激活函数的方程中使用它：'
- en: '![](img/005346b4-39e1-49c8-af03-0251dfe912ae.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/005346b4-39e1-49c8-af03-0251dfe912ae.png)'
- en: 'Figure 9.31: Transfer potential output in the form of logit value (Image credit:
    https://cdn-images-1.medium.com/max/800/0*mPYW0-FKPTOSACPP.png)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.31：以logit值形式表示的转移势能输出（图片来源：https://cdn-images-1.medium.com/max/800/0*mPYW0-FKPTOSACPP.png）
- en: 'You can see the equation of the logistic sigmoid function in *Figure 9.32*:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图9.32*中看到logistic sigmoid函数的方程：
- en: '![](img/e50a73ae-faee-4fe8-a072-5ea00aa17d77.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e50a73ae-faee-4fe8-a072-5ea00aa17d77.png)'
- en: 'Figure 9.32: Logistic sigmoid activation function (Image credit: https://cdn-images-1.medium.com/max/800/0*SwSxznoodb2762_9.png)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.32：Logistic sigmoid激活函数（图片来源：https://cdn-images-1.medium.com/max/800/0*SwSxznoodb2762_9.png）
- en: The whole idea behind an activation function is roughly modeled the way neurons
    communicate in the brain with each other. Each one is activated through its action
    potential if it reaches a certain threshold; then we know whether to activate
    a neuron or not. The activation function simulates the spike of the brain's action
    potential. **Deep neural nets** (**DNN**) are called **universal approximator**
    functions because they can compute any function at any instance. They can calculate
    any differentiable linear as well as non-linear function. Now you might ask me
    when to use this activation function. We will see this in the next paragraph.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的核心思想大致是模拟神经元在大脑中彼此之间的通讯方式。每个神经元如果达到一定的阈值，就会通过其动作电位被激活；然后我们就能知道是否激活该神经元。激活函数模拟了大脑动作电位的跃升。**深度神经网络**（**DNN**）被称为**通用逼近器**，因为它们可以在任何时刻计算任何函数。它们可以计算任何可微的线性以及非线性函数。现在你可能会问，我什么时候使用这个激活函数呢？我们将在下一个段落中讨论。
- en: There is a variety of activation functions available. Be careful while using
    them. We should not use any of them just because it sounds new and cool. Here,
    we will talk about how you know which one you should use. We will see three main
    activation functions because of their wide usage in DL, although there are other
    activation functions that you can use.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种激活函数可供选择。使用时要小心，我们不应该仅仅因为某个函数听起来新颖和酷就去使用它。在这里，我们将讨论如何判断应该使用哪一个激活函数。我们将讨论三种主要的激活函数，因为它们在深度学习中应用广泛，尽管还有其他激活函数也可以使用。
- en: 'These three activation functions are mentioned as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是提到的三种激活函数：
- en: Sigmoid
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Tanh
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: ReLU and its variants
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU及其变种
- en: Sigmoid
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'The sigmoid function is very easy to understand in terms of its mathematical
    concepts. Its mathematical formula is shown in *Figure 9.33*:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学概念上来看，sigmoid函数非常容易理解。它的数学公式如*图9.33*所示：
- en: '![](img/2bccfbf5-54c4-44c4-8594-b8e7d88684b5.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bccfbf5-54c4-44c4-8594-b8e7d88684b5.png)'
- en: 'Figure 9.33: Sigmoid function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*QHPXkxGmIyxn7mH4BtRJXQ.png)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.33：Sigmoid函数方程（图片来源：https://cdn-images-1.medium.com/max/800/1*QHPXkxGmIyxn7mH4BtRJXQ.png）
- en: As you can see in *Figure 9.33*, the sigmoid function will take the given equation,
    take a number, and squash this number in the range of zero and one. It produces
    an s-shaped curve.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图9.33*中所见，sigmoid函数会将给定的方程代入一个数字，并将这个数字压缩到零和一之间，产生一个S形曲线。
- en: This function is the first one to be used in ANN as an activation function because
    it could be interpreted as the firing rate of neuron--zero means no firing and
    one is fully saturated firing. When we use this activation function for DNN, we
    get to know some limitations of this activation function that makes it less popular
    nowadays.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是ANN中第一个被用作激活函数的，因为它可以解释为神经元的发射率——零表示没有发射，1表示完全饱和的发射。当我们将这个激活函数应用于深度神经网络（DNN）时，我们会发现它存在一些限制，使得它在如今变得不太流行。
- en: 'Some basic problems with these functions are as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的一些基本问题如下：
- en: It suffers from the gradient vanishing problem
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它受梯度消失问题的影响
- en: It has a slow convergence rate
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有较慢的收敛速度
- en: It is not a zero-centric function
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不是一个零中心函数
- en: 'Let''s understand each of the problems in detail:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解每个问题：
- en: '**Vanishing gradient problem**: You can find this problem when you are training
    certain ANNs with gradient-based methods and mostly in ANNs with backpropagation.
    This problem makes it really hard to learn and tune the parameters of the earlier
    layers in the ANN. This becomes more problematic when you add more layers to your
    ANN. If we choose the activation function wisely, then this problem can be solved.
    I want to give you details about the problem first and then we will discuss the
    cause behind it.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**消失梯度问题**：当你使用基于梯度的方法训练某些人工神经网络（ANN），尤其是在使用反向传播的ANN中，你会遇到这个问题。这个问题使得很难学习和调整ANN中早期层的参数。当你增加更多层时，这个问题会变得更加严重。如果我们明智地选择激活函数，这个问题是可以解决的。我想先给你介绍这个问题的细节，然后我们再讨论其背后的原因。'
- en: Gradient-based methods learn the values of parameters by understanding how a
    small change in the input parameters and weights will affect the output of the
    NN. If this gradient is too small, then changes in the parameters will be such
    that they cause very small changes in the output of ANN. In this case, after some
    iterations the ANN can't learn the parameters effectively and will not converge
    in the way we want. This is exactly what happens in a gradient vanishing problem.
    The gradient of the output of the network with respect to the parameters in the
    early layers becomes very small. You can say that even if there is a large change
    in the value of the parameters for input layers and weights, this does not provide
    a big effect in the output.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的方法通过理解输入参数和权重的微小变化如何影响神经网络的输出，来学习参数的值。如果这个梯度太小，参数的变化将导致ANN输出的变化非常微小。在这种情况下，经过一些迭代后，ANN将无法有效地学习这些参数，也无法按照我们期望的方式收敛。这正是梯度消失问题的表现。网络输出相对于早期层的参数的梯度变得非常小。你可以说，即使输入层和权重的参数值发生了较大的变化，这对输出的影响也非常小。
- en: I'm giving you all these details because you can face this same problem with
    the sigmoid function as well. The most basic thing is that this vanishing gradient
    problem depends on the choice of your activation function. Sigmoid squashes the
    input into a small range of output in a non-linear manner. If you give a real
    number to the sigmoid function, it will squash that number in the range of [0,1].
    So there are large regions of input space that are mapped to a very small range.
    Even a large change in input parameters will produce a very small change in output
    because the gradient of this region is small. For the sigmoid function, when a
    neuron saturates close to either zero or one, the gradient at this region is very
    close to zero. During backpropagation, this local gradient will be multiplied
    by the gradient of each layer's output gate. So if the first layer maps to a large
    input region, we get a very small gradient as well as a very small change in the
    output of the first layer. This small change passes to the next layer and makes
    even smaller changes in the second layer's output. If we have a DNN, there is
    no change in the output after some layers. This is the problem with the sigmoid
    activation function.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我给你提供这些细节是因为你在使用 sigmoid 函数时也可能会遇到同样的问题。最基本的一点是，消失梯度问题依赖于你选择的激活函数。Sigmoid 函数以非线性方式将输入压缩到一个小范围的输出。如果你给
    sigmoid 函数一个实数，它会将该数值压缩到 [0,1] 的范围内。因此，输入空间中的大部分区域会映射到一个非常小的范围。即使输入参数发生了很大的变化，输出也会产生非常小的变化，因为该区域的梯度很小。对于
    sigmoid 函数，当神经元接近 0 或 1 时，该区域的梯度非常接近于零。在反向传播时，这个局部梯度将与每一层输出门的梯度相乘。因此，如果第一层映射到一个很大的输入区域，我们就会得到一个非常小的梯度以及非常小的第一层输出变化。这个小的变化会传递到下一层，并在第二层输出中产生更小的变化。如果我们有一个深度神经网络（DNN），经过几层后，输出就没有任何变化了。这就是
    sigmoid 激活函数的问题。
- en: 'You can refer to the vanishing gradient problem in detail at this link:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接详细了解消失梯度问题：
- en: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)'
- en: '**Low convergence rate**: Due to this vanishing gradient problem, sometimes
    the ANN with the sigmoid activation function converges very slowly.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**低收敛速度**：由于消失梯度问题，使用 sigmoid 激活函数的人工神经网络（ANN）有时会收敛得非常慢。'
- en: 'If you really want to dig deep into the vanishing gradient problem, then you
    can check out this link:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想深入了解消失梯度问题，那么你可以查看这个链接：
- en: '[https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html](https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html](https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html)'
- en: '**Non-zero-centric function**: The sigmoid function is not a zero-centric activation
    function. What this means is that the sigmoid function''s output range is [0,1],
    which means the value of the function''s output will be always positive so that
    makes the gradient of the weights become either all positive or all negative.
    This makes the gradient update go too far in different directions and this makes
    optimization harder.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**非零中心函数**：Sigmoid 函数不是一个零中心的激活函数。这意味着 sigmoid 函数的输出范围是 [0,1]，即函数的输出值始终为正，这使得权重的梯度要么全是正的，要么全是负的。这导致梯度更新在不同方向上走得过远，从而使优化变得更加困难。'
- en: Due to these limitations, the sigmoid function is not used recently in DNNs.
    Although you can solve these problems using other functions, you can also use
    the sigmoid activation function only at the last layer of your ANN.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些限制，最近在 DNN 中不再使用 sigmoid 函数。虽然你可以使用其他函数来解决这些问题，但你也可以只在 ANN 的最后一层使用 sigmoid
    激活函数。
- en: TanH
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TanH
- en: 'To overcome the problems of the sigmoid function, we will introduce an activation
    function named **hyperbolic tangent function** (**TanH**). The equation of TanH
    is given in *Figure 9.34*:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 sigmoid 函数的问题，我们将引入一种叫做**双曲正切函数**（**TanH**）的激活函数。TanH 的方程如 *图 9.34* 所示：
- en: '![](img/854e270c-fe31-4bf5-9c0a-116d5813e8c4.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/854e270c-fe31-4bf5-9c0a-116d5813e8c4.png)'
- en: 'Figure 9.34: Tanh activation function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*HJhu8BO7KxkjqRRMSaz0Gw.png)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34：Tanh 激活函数方程（图片来源：https://cdn-images-1.medium.com/max/800/1*HJhu8BO7KxkjqRRMSaz0Gw.png）
- en: This function squashes the input region in the range of [-1 to 1] so its output
    is zero-centric, which makes optimization easier for us. This function also suffers
    from the vanishing gradient problem, so we need to see other activation functions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将输入区域压缩到[-1到1]的范围内，因此它的输出是零中心的，这使得优化过程更容易。这个函数也有梯度消失问题，所以我们需要查看其他激活函数。
- en: ReLu and its variants
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLu及其变种
- en: '**Rectified Linear Unit** (**ReLu**) is the most popular function in the industry.
    See its equation in *Figure 9.35*:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLu**）是业界最流行的函数。请参见*图9.35*中的方程：'
- en: '![](img/02c4f3a4-8c9b-405a-88bd-47b79e3981dc.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02c4f3a4-8c9b-405a-88bd-47b79e3981dc.png)'
- en: 'Figure 9.35: ReLu activation function equation (Image credit: https://cdn-images-1.medium.com/max/800/1*JtJaS_wPTCshSvAFlCu_Wg.png)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.35：ReLu激活函数方程（图片来源：https://cdn-images-1.medium.com/max/800/1*JtJaS_wPTCshSvAFlCu_Wg.png）
- en: If you will see the ReLu mathematical equation, then you will know that it is
    just *max(0,x)*, which means that the value is zero when *x* is less than zero
    and linear with the slope of *1* when *x* is greater than or equal to zero. A
    researcher named Krizhevsky published a paper on image classification and said
    that they get six times faster convergence using ReLu as an activation function.
    You can read this research paper by clicking on [http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf).
    This function is simple and doesn't have any complex computation and is less expensive
    compared to sigmoid and TanH. This is the reason that this function learns faster.
    Apart from this, it also doesn't have the vanishing gradient problem.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到ReLu的数学方程，你会知道它就是*max(0,x)*，这意味着当*x*小于零时，值为零，而当*x*大于或等于零时，值则是斜率为*1*的线性函数。一位名为Krizhevsky的研究者在图像分类方面发表了一篇论文，指出他们使用ReLu作为激活函数时，收敛速度是以前的六倍。你可以通过点击[http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)阅读这篇研究论文。这个函数很简单，没有复杂的计算，且与sigmoid和TanH相比，计算开销更小。这也是这个函数学习速度更快的原因。除此之外，它还没有梯度消失问题。
- en: We used to apply the activation function in each layer present in the DNN. Nowadays,
    ReLu is used for most of the DNN, but it is applied to the hidden layers of DNN.
    The output layer should use softmax if you are solving a classification problem
    because the softmax function gives us the probability for each class. We have
    used the softmax activation function in the word2vec algorithm. In case of a regression
    problem, the output layer should use a linear function because the signal goes
    through unchanged.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以前会在DNN的每一层中应用激活函数。如今，ReLu被用于大多数DNN，但它只应用于DNN的隐藏层。如果你正在解决分类问题，则输出层应使用softmax，因为softmax函数可以为每个类别提供概率。在word2vec算法中，我们使用了softmax激活函数。在回归问题中，输出层应使用线性函数，因为信号不发生变化。
- en: 'Apart from all these wonderful advantages of ReLu, it has one problem: some
    units of the neural network can be fragile and die during training, which means
    that a big gradient flowing through a ReLu neuron could cause a weight update
    that makes it never activate on any data point again. So the gradient flowing
    through it will always be zero from that point on. To overcome this limitation
    of ReLu, a variant of ReLu has been introduced--Leaky ReLu. Instead of the function
    being zero when *x* is less than zero (*x<0*), Leaky ReLu has a small negative
    slope. Refer to *Figure 9.36*:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 除了ReLu的所有这些优点之外，它还有一个问题：神经网络的一些单元可能会很脆弱，在训练过程中死亡，这意味着流经ReLu神经元的大梯度可能会导致权重更新，从而使它在任何数据点上都不会再激活。因此，从那时起流经它的梯度将始终为零。为了克服ReLu的这一限制，引入了ReLu的一个变种——Leaky
    ReLu。当*x*小于零时（*x<0*），Leaky ReLu具有一个小的负斜率。请参见*图9.36*：
- en: '![](img/f25df7dd-d259-467e-a794-793304489049.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f25df7dd-d259-467e-a794-793304489049.png)'
- en: 'Figure 9.36: Leaky ReLu (Image credit: http://wangxinliu.com/images/machine_learning/leakyrelu.png)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.36：Leaky ReLu（图片来源：http://wangxinliu.com/images/machine_learning/leakyrelu.png）
- en: There is another variant called **maxout** that is a generalized form of both
    ReLu and Leaky ReLu, but it doubles the parameters of each neuron, which is a
    disadvantage.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个变种叫做**maxout**，它是ReLu和Leaky ReLu的广义形式，但它将每个神经元的参数翻倍，这是一个缺点。
- en: Now you know enough about activation functions, so which one should you use?
    The answer is ReLu, but if too many neurons die, then use Leaky ReLu or maxout.
    This activation function is applied to the hidden layer. For the output layer,
    use the softmax function if you are solving a classification problem or linear
    activation function if you are solving a regression problem. The sigmoid and TanH
    shouldn't be used in DNNs. This is quite an interesting research area and there
    is much room to come up with great activation functions.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了足够的激活函数，那么应该使用哪一个呢？答案是ReLu，但如果有太多神经元死亡，那么使用Leaky ReLu或maxout。这些激活函数应用于隐藏层。对于输出层，如果你正在解决分类问题，使用softmax函数；如果你正在解决回归问题，使用线性激活函数。sigmoid和TanH不应该在DNN中使用。这是一个相当有趣的研究领域，仍有很大的空间来提出更好的激活函数。
- en: 'There are other activation functions out there that you can check out: identity
    function, binary step function, ArcTan, and so on. Here, we will check the third
    important concept--loss functions.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有其他一些激活函数，你可以了解一下：恒等函数、二元阶跃函数、ArcTan 等等。在这里，我们将讨论第三个重要概念——损失函数。
- en: Loss functions
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'Sometimes, loss functions are also referred to as **cost functions** or **error
    functions**. A loss function gives us an idea of how good the ANN performs with
    respect to the given training examples. So first, we define the error function
    and when we start to train our ANN, we will get the output. We compare the generated
    output with the expected output given as part of the training data and calculate
    the gradient value of this error function. We backpropagate the error gradient
    in the network so that we can update the existing weights and bias values to optimize
    our generated output. The error function is the main part of the training. There
    are various error functions available. If you ask me which error function to choose,
    then there is no specific answer because all ANN training and optimization is
    based on this loss function. So it depends on your data and problem statement.
    If you ask somebody which error function you have used in your ANN, then indirectly
    you are asking them the whole logic of the training algorithm. Whatever error
    function you will use, make sure that the function must be differentiable. I have
    listed down some of the most popular error functions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，损失函数也被称为**代价函数**或**误差函数**。损失函数给我们提供了一个衡量ANN在给定训练样本上的表现好坏的依据。因此，首先我们定义误差函数，并在开始训练ANN时得到输出。我们将生成的输出与作为训练数据一部分的期望输出进行比较，并计算该误差函数的梯度值。我们将误差梯度反向传播到网络中，以便更新现有的权重和偏置值，优化我们的输出。误差函数是训练的核心部分。有各种误差函数可供选择。如果你问我应该选择哪个误差函数，那么没有具体的答案，因为所有ANN的训练和优化都是基于这个损失函数的。所以它取决于你的数据和问题陈述。如果你问某人他们在ANN中使用了哪个误差函数，那么间接地你是在询问他们训练算法的整体逻辑。无论你使用哪个误差函数，确保该函数是可微的。我列出了一些最常见的误差函数：
- en: Quadratic cost function also known as **mean squared error** or **sum squared
    error**
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二次损失函数，也叫做**均方误差**或**平方和误差**
- en: Cross-entropy cost function also known as **Bernoulli negative log likelihood**
    or **binary cross-entropy**
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉熵损失函数，也叫做**伯努利负对数似然**或**二元交叉熵**
- en: Kullback-Leibler divergence also known as **information divergence**, **information
    gain**, **relative entropy**, or **KLIC**
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库尔巴克-莱布勒散度，也叫做**信息散度**、**信息增益**、**相对熵**或**KLIC**
- en: Apart from these three, there are many other loss functions such as exponential
    cost, Hellinger distance, Generalized Kullback-Leibler divergence, and Itakura-Saito
    distance
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了这三个，还有许多其他损失函数，如指数损失、赫林格距离、广义的库尔巴克-莱布勒散度和板仓-斋藤距离
- en: In general, we are using sum of square error for regression and cross-entropy
    for categorical data and classification tasks.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们在回归问题中使用平方和误差，而在分类任务和类别数据中使用交叉熵。
- en: We have seen the most important mathematical and theoretical concepts to develop
    ANN. In the next section, we will see the implementation of our first ANN. Let's
    jump to the implementation part.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了开发人工神经网络（ANN）最重要的数学和理论概念。在下一部分，我们将看到如何实现我们的第一个ANN。让我们进入实现部分吧。
- en: Implementation of ANN
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ANN的实现
- en: In this section, we will implement our first ANN in Python using `numpy` as
    our dependency. During this implementation, you can relate how gradient descent,
    activation function, and loss function have been integrated into our code. Apart
    from this, we will see the concept of backpropagation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`numpy`作为依赖项，在 Python 中实现我们的第一个人工神经网络（ANN）。在实现过程中，你可以理解梯度下降、激活函数和损失函数是如何被整合进我们的代码中的。除此之外，我们还将看到反向传播的概念。
- en: We will see the implementation of a single-layer NN with backpropagation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到带有反向传播的单层神经网络的实现。
- en: Single-layer NN with backpropagation
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有反向传播的单层神经网络
- en: Here, we will see the concept of backpropagation first, then we will start coding
    and I will explain things as we code.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先会看到反向传播的概念，然后我们将开始编写代码，我会在编程时进行讲解。
- en: Backpropagation
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: In a single-layer neural network, we have input that we feed to the first layer.
    These layer connections have some weights. We use the input, weight, and bias
    and sum them. This sum passes through the activation function and generates the
    output. This is an important step; whatever output has been generated should be
    compared with the actual expected output. As per the error function, calculate
    the error. Now use the gradient of the error function and calculate the error
    gradient. The process is the same as we have seen in the gradient descent section.
    This error gradient gives you an indication of how you can optimize the generated
    output. Error gradient flows back in the ANN and starts updating the weight so
    that we get a better output in the next iteration. The process of flowing back
    the error gradient in ANN to update weight in order to generate more accurate
    output is called **backpropagation**. In short, backpropagation is a popular training
    technique to train a neural network by updating the weight via gradient descent.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在单层神经网络中，我们将输入传递到第一层。这些层之间的连接有一些权重。我们使用输入、权重和偏差，并将它们相加。这个和通过激活函数后产生输出。这个步骤非常重要，生成的输出应与实际期望输出进行比较。根据误差函数，计算误差。接下来，使用误差函数的梯度来计算误差梯度。这个过程和我们在梯度下降部分看到的相同。误差梯度指示了如何优化生成的输出。误差梯度回传到
    ANN 中，开始更新权重，以便在下一次迭代中得到更好的输出。将误差梯度回传到 ANN 中，更新权重，以生成更准确输出的过程被称为**反向传播**。简而言之，反向传播是一种流行的训练技术，通过梯度下降来更新权重，从而训练神经网络。
- en: All other aspects of calculation and math will be shown in the coding part.
    So let's code our own single-layer feedforward neural network with backpropagation.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 其他计算和数学部分将在编程部分展示。现在让我们编写自己的单层前馈神经网络，并实现反向传播。
- en: First, we will define the main function and our abstract steps. Here, we will
    give the input and output values. As our data is labeled, it is a supervised learning
    example. The second step will be the training, and we will repeat the training
    to iterate for 10,000 times. We will first start with a random weight and adjust
    the weight as per the activation function and error function. Refer to *Figure
    9.37**:*
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义主函数和抽象步骤。在这里，我们将给定输入和输出值。由于我们的数据是有标签的，这属于监督学习的例子。第二步是训练，我们将重复训练 10,000
    次。在训练开始时，我们会设置一个随机权重，并根据激活函数和误差函数调整权重。请参考*图 9.37*：
- en: '![](img/03a11c0f-d2bc-4e98-a6c2-b9f0e873be54.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03a11c0f-d2bc-4e98-a6c2-b9f0e873be54.png)'
- en: 'Figure 9.37: Code snippet of the main function for a single-layer ANN'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.37：单层人工神经网络（ANN）主函数的代码片段
- en: 'Here, we are using sigmoid as the activation function. We will use the sigmoid
    derivative to calculate gradient of the sigmoid curve. Our error function is a
    simple subtraction of the actual output from the generated output. We multiply
    this error value with the gradient to get the error gradient that helps us adjust
    the weight of NN. The new updated weight and input again passes through the ANN,
    calculates gradient descent of the sigmoid curve and error gradient, and adjusts
    the weight until we get minimum error. Refer to *Figure 9.38* and *Figure 9.39*:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 sigmoid 作为激活函数。我们将使用 sigmoid 的导数来计算 sigmoid 曲线的梯度。我们的误差函数是将实际输出与生成输出做简单的减法。我们将误差值与梯度相乘，以获得误差梯度，这有助于我们调整神经网络的权重。更新后的权重和输入再次通过
    ANN，计算 sigmoid 曲线的梯度下降和误差梯度，并调整权重，直到我们得到最小的误差。请参考*图 9.38*和*图 9.39*：
- en: '![](img/2fb5c460-e1c5-47a2-ad6a-f3ee8882a085.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fb5c460-e1c5-47a2-ad6a-f3ee8882a085.png)'
- en: 'Figure 9.38: Code snippet of a single-layer ANN'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.38：单层人工神经网络（ANN）的代码片段
- en: 'Refer to the following code snippet:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下代码片段：
- en: '![](img/f446caec-fd22-43e4-8771-e158be9be204.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f446caec-fd22-43e4-8771-e158be9be204.png)'
- en: 'Figure 9.39: Code snippet of ANN'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.39：ANN的代码片段
- en: 'When you run the code, you will get the following result. Refer to *Figure
    9.40*:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行代码时，你将获得以下结果。参考*图9.40*：
- en: '![](img/69ce0eab-3e60-4411-8c06-d3e494f48e10.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69ce0eab-3e60-4411-8c06-d3e494f48e10.png)'
- en: 'Figure 9.40: Output snippet of single layer ANN'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.40：单层ANN的输出片段
- en: Exercise
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Build a three-layer deep ANN using `numpy` as a dependency. (Hint: In a single-layer
    ANN, we used single layer, but here, you will use three layers. Backpropagation
    usually uses recursively taken derivatives, but in our one layer demo, there was
    no recursion. So you need to apply recursive derivatives.)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numpy`作为依赖，构建一个三层深度ANN。（提示：在单层ANN中，我们使用了单层，而在这里，你将使用三层。反向传播通常使用递归求导，但是在我们的一层示例中没有递归。所以你需要应用递归求导。）
- en: Deep learning and deep neural networks
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习和深度神经网络
- en: Now, just shift from ANN to DNN. In the upcoming section, we will see deep learning,
    architecture of DNN, and compare the approaches of DL for NLP and ML for NLP.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需从ANN转到DNN。在接下来的部分，我们将讨论深度学习、DNN的架构，并比较DL和NLP的ML方法。
- en: Revisiting DL
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾DL
- en: We have seen some basic details about DL. Here, the purpose is just to recall
    things as a little refresher. ANN that is not two or three layers but many layers
    deep is called DNN. When we use many layers deep neural networks on lots of data
    using lots of computing power, we call this process deep learning.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了关于DL的一些基本细节。这里的目的是简单回顾一下内容作为小小的复习。ANN不仅仅是两层或三层，而是有许多层的深度神经网络，这种网络叫做DNN。当我们在大量数据上使用大量计算能力训练多层深度神经网络时，我们称这种过程为深度学习。
- en: Let's see the architecture of a deep neural network.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看深度神经网络的架构。
- en: The basic architecture of DNN
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNN的基本架构
- en: 'In this section, we will see the architecture of a DNN. The pictorial representation
    looks very simple and is defined with some cool mathematical formulas in the form
    of activation function, activation function for hidden layer, loss function, and
    so on. In *Figure 9.41*, you can see the basic architecture of a DNN:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将看到DNN的架构。图示看起来很简单，并通过一些很酷的数学公式定义，例如激活函数、隐藏层的激活函数、损失函数等等。在*图9.41*中，你可以看到DNN的基本架构：
- en: '![](img/70fd1f89-7609-4bdc-86e2-a8786869b8be.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70fd1f89-7609-4bdc-86e2-a8786869b8be.png)'
- en: 'Figure 9.41: Architecture of DNN (Image credit: https://cdn-images-1.medium.com/max/800/1*5egrX--WuyrLA7gBEXdg5A.png)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.41：DNN的架构（图片来源：https://cdn-images-1.medium.com/max/800/1*5egrX--WuyrLA7gBEXdg5A.png）
- en: Now why are we using a multi-layer deep neural network, are there any certain
    reasons for this, and what is the significance of having many layers?
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们使用多层深度神经网络？这是有某些原因的吗？拥有多个层次有什么重要意义？
- en: 'Let me explain why we are using multi-layer DNNs. Suppose, as a coder, you
    want to develop a system that identifies the images of fruits. Now you have some
    images of oranges and apples and you develop a logic such as I can identify images
    using the color of the fruits and you have also added shape as an identification
    parameter. You do some coding and are ready with the result. Now if someone tells
    you that we also have images that are black and white. Now you need to redo your
    coding work. Some varieties of images are too complex for you, as a human, to
    code, although your brain is very good at identifying the actual fruit name. So
    if you have such a complex problem and you don''t know how to code or you know
    less details about the features or parameters that will be helpful for the machine
    to solve the problem, then you use a deep neural network. There are several reasons
    and those are mentioned as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释一下为什么我们使用多层DNN。假设作为一名编码员，你想开发一个识别水果图像的系统。现在你有一些橙子和苹果的图像，你开发了一个逻辑，例如可以通过水果的颜色来识别图像，并且你还添加了形状作为识别参数。你写了一些代码，并准备好了结果。现在如果有人告诉你，我们还有黑白图像，你就需要重新编写你的代码。一些图像的种类对你来说，作为人类，编码起来太复杂了，尽管你的大脑在识别实际水果名称方面非常擅长。那么，如果你有这样一个复杂的问题，并且你不知道如何编码，或者你对机器解决问题所需要的特征或参数了解不多，那么你就需要使用深度神经网络。这里有几个原因，接下来将提到：
- en: DNN has been derived using the abstract concept of how a human brain works.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNN是通过抽象人脑工作原理的概念推导出来的。
- en: Using DNN, we flip the approach of our coding. Initially, we provided features
    like color, shape, and so on to the machine to identify the fruit name in the
    given images, but with DNN and DL, we provide many examples to the machine and
    the machine will learn about the features by itself. After this, when we provide
    a new image of a fruit to the machine, it will predict the name of the fruit.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DNN时，我们改变了编码的方式。最初，我们将颜色、形状等特征提供给机器，让它在给定的图像中识别水果名称，但使用DNN和深度学习（DL）后，我们将许多示例提供给机器，机器会自行学习特征。之后，当我们提供一张新的水果图像给机器时，它将预测水果的名称。
- en: 'Now you really want to know how DNN can learn features by itself, so let''s
    highlight some points as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能想了解DNN如何自我学习特征，下面我们列出一些要点：
- en: DNN uses a cascade of many layers of non-linear processing units that are used
    for feature extraction and transformation. Each successive layer of DNN uses the
    output from the previous layer as input, and this process is very similar to how
    the human brain transmits information from one neuron to the other. So we try
    to implement the same structure with the help of DNN.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNN使用多个非线性处理单元的级联层，用于特征提取和转换。每一层的输出都作为下一层的输入，这一过程与人脑如何传递信息从一个神经元到另一个神经元非常相似。所以我们试图借助DNN实现相同的结构。
- en: In DL, features have been learned using multiple levels of representation with
    the help of DNNs. Higher levels of features or representation are derived from
    the lower level of features. So we can say that the concept of deriving features
    or representation in DNN is hierarchical. We learn something new using this lower
    level of ideas and we try to learn something extra. Our brain also uses and derives
    concepts in a hierarchical manner. This different level of features or representation
    is related to different levels of abstraction.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在深度学习（DL）中，特征通过多层表示学习，借助DNN的帮助，较高层次的特征或表示是从较低层次的特征中衍生出来的。因此，我们可以说，在DNN中提取特征或表示的概念是层次化的。我们通过这些低层次的想法学习新知识，并尝试学习额外的东西。我们的脑袋也以层次化的方式使用和推导概念。这些不同层次的特征或表示与不同的抽象层次相关。
- en: Multi-layers of DNN helps the machine to derive the hierarchical representation
    and this is the significance of having many layers as part of the architecture.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNN的多层结构帮助机器推导层次化的表示，这也是架构中有多个层次的重要性所在。
- en: With the help of DNN and mathematical concepts, machines are capable to mimic
    some of the processes of the human brain.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 借助DNN和数学概念，机器能够模拟人脑的一些处理过程。
- en: DL can be applied to a supervised as well as unsupervised dataset to develop
    NLP applications such as machine translation, summarization, question answering
    system, essay generation, image caption tagging, and so on.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习（DL）可以应用于监督数据集和无监督数据集，来开发NLP应用，如机器翻译、摘要生成、问答系统、文章生成、图像标签等。
- en: Now we will move to the next section where we will discuss the need of deep
    learning in NLP.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将进入下一部分，讨论深度学习在NLP中的需求。
- en: Deep learning in NLP
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在NLP中的应用
- en: The early era of NLP is based on the rule-based system, and for many applications,
    an early prototype is based on the rule-based system because we did not have huge
    amounts of data. Now, we are applying ML techniques to process natural language,
    using statistical and probability-based approaches where we are representing words
    in form of one-hot encoded format or co-occurrence matrix.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: NLP的早期阶段基于规则系统，许多应用的早期原型也是基于规则系统，因为我们没有大量的数据。如今，我们应用机器学习（ML）技术来处理自然语言，采用统计和基于概率的方法，其中我们将单词表示为一热编码格式或共现矩阵。
- en: In this approach, we are getting mostly syntactic representations instead of
    semantic representations. When we are trying out lexical-based approaches such
    as bag of words, ngrams, and so on, we cannot differentiate certain context.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们主要获得的是句法表示，而非语义表示。当我们尝试基于词汇的方式，如词袋模型、n-gram等时，我们无法区分某些上下文。
- en: 'We hope that all these issues will be solved by DNN and DL because nowadays,
    we have huge amounts of data that we can use. We have developed good algorithms
    such as word2vec, GloVe, and so on in order to capture the semantic aspect of
    natural language. Apart from this, DNN and DL provide some cool capabilities that
    are listed as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望所有这些问题都能通过DNN和DL解决，因为如今我们拥有可以利用的大量数据。我们已经开发出像word2vec、GloVe等优秀的算法，用于捕捉自然语言的语义方面。除此之外，DNN和DL还提供了一些很酷的功能，列举如下：
- en: '**Expressibility:** This capability expresses how well the machine can do approximation
    for a universal function'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表达能力：** 这一能力表达了机器能多好地对一个通用函数进行逼近。'
- en: '**Trainability:** This capability is very important for NLP applications and
    indicates how well and fast a DL system can learn about the given problem and
    start generating significant output'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可训练性：** 这一能力对于自然语言处理应用非常重要，它表示深度学习系统能够多好、多快地学习给定问题，并开始生成有意义的输出。'
- en: '**Generalizability:** This indicates how well the machine can generalize the
    given task so that it can predict or generate an accurate result for unseen data'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力：** 这表示机器能多好地泛化给定任务，从而能够为未见过的数据预测或生成准确的结果。'
- en: Apart from the preceding three capabilities, there are other capabilities that
    DL provides us with, such as interpretability, modularity, transferability, latency,
    adversarial stability, and security.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述的三种能力，深度学习还提供了其他能力，例如可解释性、模块化、可迁移性、延迟、对抗稳定性和安全性。
- en: We know languages are complex things to deal with and sometimes we also don't
    know how to solve certain NLP problems. The reason behind this is that there are
    so many languages in the world that have their own syntactic structure and word
    usages and meanings that you can't express in other languages in the same manner.
    So we need some techniques that help us generalize the problem and give us good
    results. All these reasons and factors lead us in the direction of the usage of
    DNN and DL for NLP applications.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，语言是非常复杂的事物，有时我们也不知道如何解决某些自然语言处理问题。其背后的原因是，世界上有许多语言，它们各自有自己的句法结构、词汇用法和含义，这些用法和含义不能以相同的方式在其他语言中表达。所以我们需要一些能够帮助我们泛化问题并给出好结果的技术。所有这些原因和因素将我们引导到使用深度神经网络（DNN）和深度学习（DL）来解决自然语言处理应用的问题。
- en: Now let's see the difference between classical NLP techniques and DL NLP techniques
    because that will connect our dots in terms of how DL can be more useful for us
    to solve NLP domain-related problems.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看经典自然语言处理技术与深度学习自然语言处理技术之间的区别，因为这将帮助我们理解深度学习如何更有效地帮助我们解决自然语言处理领域相关问题。
- en: Difference between classical NLP and deep learning NLP techniques
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典自然语言处理与深度学习自然语言处理技术的区别
- en: 'In this section, we will compare the classical NLP techniques and DL techniques
    for NLP. So let''s begin! Refer to *Figure 9.42*:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将比较经典的自然语言处理技术与深度学习技术。因此，开始吧！请参考*图 9.42*：
- en: '![](img/32b50a1a-d3b9-4b84-be2e-f38cf4f102b9.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32b50a1a-d3b9-4b84-be2e-f38cf4f102b9.png)'
- en: 'Figure 9.42: Classical NLP approach (Image credit: https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.42：经典自然语言处理方法（图片来源：[https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png](https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)）
- en: 'Refer to *Figure 9.43* for DL techniques:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图 9.43*了解深度学习技术：
- en: '![](img/ae3bd2b6-0018-4e98-9396-9120c9f2d9b1.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae3bd2b6-0018-4e98-9396-9120c9f2d9b1.png)'
- en: 'Figure 9.43: Deep learning approach for NLP (Image credit: https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.43：深度学习方法用于自然语言处理（图片来源：[https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png](https://s3.amazonaws.com/aylien-main/misc/blog/images/nlp-language-dependence-small.png)）
- en: In classical NLP techniques, we preprocessed the data in the early stages before
    generating features out of the data. In the next phase, we use hand-crafted features
    that are generated using NER tools, POS taggers, and parsers. We feed these features
    as input to the ML algorithm and train the model. We will check the accuracy,
    and if the accuracy is not good, we will optimize some of the parameters of the
    algorithm and try to generate a more accurate result. Depending on the NLP application,
    you can include the module that detects the language and then generates features.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在经典的自然语言处理技术中，我们在早期阶段对数据进行了预处理，然后从数据中生成特征。在接下来的阶段，我们使用手工制作的特征，这些特征是通过命名实体识别（NER）工具、词性标注器和解析器生成的。我们将这些特征作为输入提供给机器学习算法并训练模型。我们会检查准确性，如果准确性不好，我们会优化一些算法参数并尝试生成更准确的结果。根据自然语言处理应用的不同，您可以包括检测语言的模块，然后生成特征。
- en: Now let's see the deep learning techniques for an NLP application. In this approach,
    we do some basic preprocessing on the data that we have. Then we convert our text
    input data to a form of dense vectors. To generate the dense vectors, we will
    use word-embedding techniques such as word2vec, GloVe, doc2vec, and so on, and
    feed these dense vector embedding to the DNN. Here, we are not using hand-crafted
    features but different types of DNN as per the NLP application, such as for machine
    translation, we are using a variant of DNN called **sequence-to-sequence model**.
    For summarization, we are using another variant, that is, **Long short-term memory
    units**.(**LSTMs**). The multiple layers of DNNs generalize the goal and learn
    the steps to achieve the defined goal. In this process, the machine learns the
    hierarchical representation and gives us the result that we validate and tune
    the model as per the necessity.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看NLP应用中的深度学习技术。在这个方法中，我们会对已有数据进行一些基础的预处理。然后我们将文本输入数据转换为稠密向量的形式。为了生成这些稠密向量，我们将使用诸如word2vec、GloVe、doc2vec等词嵌入技术，并将这些稠密向量嵌入输入到DNN中。在这里，我们没有使用手工特征，而是根据NLP应用使用不同类型的DNN，例如在机器翻译中，我们使用一种叫做**序列到序列模型（sequence-to-sequence
    model）**的DNN变种；而在摘要生成中，我们使用另一种变种，即**长短时记忆单元（LSTMs）**。DNN的多层结构概括了目标，并学习达成目标的步骤。在这个过程中，机器学习到的是层次化的表示，并给出结果，我们再根据需要验证并调整模型。
- en: 'If you really want to see the coding of different variants of DNNs, then use
    this GitHub link:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想看看不同变种的DNN编码，可以使用这个GitHub链接：
- en: '[https://github.com/wagamamaz/tensorflow-tutorial](https://github.com/wagamamaz/tensorflow-tutorial)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/wagamamaz/tensorflow-tutorial](https://github.com/wagamamaz/tensorflow-tutorial)'
- en: 'The next section is the most interesting part of this chapter. We are going
    to build two major applications: one is for NLU and one is for NLG. We are using
    TensorFlow and Keras as our main dependencies to code the example. We will understand
    a variant of DNN such as sequence-to-sequence and LSTM as we code them for better
    understanding.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节是本章中最有趣的部分。我们将构建两个主要应用：一个是自然语言理解（NLU），另一个是自然语言生成（NLG）。我们将使用TensorFlow和Keras作为主要依赖库来编写示例代码。在编码过程中，我们将理解DNN的变种，如序列到序列（sequence-to-sequence）和长短时记忆网络（LSTM），以便更好地理解。
- en: Guess what we are going to build? We are going to build a machine translator
    as part of an NLP application and we will generate a summary from recipes. So
    let's jump to the coding part! I will give you some interesting exercises!
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 你猜我们将要构建什么？我们将构建一个机器翻译器，作为NLP应用的一部分，还将从食谱中生成摘要。那么，让我们进入编码部分吧！我会给你一些有趣的练习！
- en: Deep learning techniques and NLU
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术与自然语言理解（NLU）
- en: This section is coding-based and I will explain concepts as we go. The application
    that we are building here is one of the main applications in NLU.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容以编码为基础，我会在过程中解释相关概念。我们在这里构建的应用是自然语言理解（NLU）中的一个主要应用。
- en: There are so many languages spoken, written, or read by humans. Have you ever
    tried to learn a new language? If yes, then you know how difficult it is to acquire
    the skill of speaking a new language or writing a new language. Have you ever
    thought how Google translator is used in order to translate languages? If you
    are curious, then let's begin developing a machine translation application using
    a deep learning technique. Don't worry about questions like what type of DNN we
    will use because I'm explaining things to you in detail. So let's do some translation!
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 人类有许多种语言在使用、书写或阅读。你曾经尝试过学习一门新语言吗？如果有，你就会知道掌握一门新语言的口语或书写技巧是多么困难。你有没有想过谷歌翻译是如何用来翻译语言的？如果你感兴趣，那就让我们开始使用深度学习技术开发一个机器翻译应用吧。别担心关于我们会使用什么类型的DNN这种问题，因为我会详细为你解释。所以，让我们开始翻译吧！
- en: 'Note that DL takes a lot of computing power so we are not going to actually
    train the model, although I will give you details about the training code, we
    will use the trained model to replicate the results at our end. Just to give you
    an idea: Google uses 100 GPU for one week continuously to train the language translation
    model. So we get through the code, understand the concept, use an already trained
    model, and see the result.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，深度学习（DL）需要大量的计算能力，因此我们不会实际训练模型，尽管我会提供关于训练代码的详细信息，我们将使用已训练好的模型在本地复制结果。给你一个概念：谷歌为了训练语言翻译模型，使用了100个GPU连续训练一周。所以我们会通过代码，理解概念，使用已经训练好的模型，看看结果。
- en: 'If you want to use any specific version of TensorFlow, you can follow this
    command. If you want to install TensorFlow 0.12 version, you can install it with
    the following commands:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用特定版本的TensorFlow，你可以使用以下命令。如果你想安装TensorFlow 0.12版本，可以使用以下命令进行安装：
- en: '[PRE0]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to use the version of TensorFlow then when you run the code please
    update you import statements. You can use the simple following command to install
    TensorFlow for CPU. I''m using GPU version only:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用TensorFlow版本，那么在运行代码时，请更新你的导入语句。你可以使用以下简单命令来安装CPU版本的TensorFlow。我只使用GPU版本：
- en: '[PRE1]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to run on GPU, you can use a cloud platform such as Google Cloud,
    AWS, or any other cloud platform or you need a GPU-enabled computer. To install
    TensorFlow for GPU, you can follow this link:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在GPU上运行，你可以使用云平台，比如Google Cloud、AWS或其他任何云平台，或者你需要一台支持GPU的计算机。要安装GPU版本的TensorFlow，你可以通过这个链接进行操作：
- en: '[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)'
- en: Machine translation
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译
- en: '**Machine translation** (**MT**) is a widely known application in the NLU domain.
    Researchers and tech giants are experimenting a lot in order to make a single
    MT system that can translate any language. This MT system is called a universal
    machine translation system. So the long-term goal is that we want to build a single
    MT system that can translate English to German and the same MT system should also
    translate English to French. We are trying to make one system that can help us
    translate any language. Let''s talk about the efforts and experiments done by
    researchers till date to build a universal machine translation system.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器翻译**（**MT**）是自然语言理解（NLU）领域中一个广泛应用的技术。研究人员和科技巨头们正在进行大量实验，力图打造一个能够翻译任何语言的单一MT系统。这个MT系统被称为通用机器翻译系统。所以，长远目标是我们希望建立一个能够将英语翻译成德语的MT系统，并且同一个MT系统还应该能够将英语翻译成法语。我们正努力开发一个能够帮助我们翻译任何语言的系统。接下来，让我们谈谈迄今为止，研究人员在构建通用机器翻译系统方面所做的努力和实验。'
- en: In 1954, the first machine translation demo had been given, which translated
    250 words between Russian and English. This was a dictionary-based approach, and
    this approach used the mapping of words for source and target languages. Here,
    translation was done word by word and it wasn't able to capture syntactic information,
    which means that the accuracy was not good.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 1954年，首次展示了机器翻译demo，它能够在俄语和英语之间翻译250个单词。这是一种基于词典的方法，使用源语言和目标语言之间的单词映射关系进行翻译。翻译是逐词进行的，这种方法无法捕捉句法信息，意味着翻译的准确性较差。
- en: The next version was interlingual; it took the source language and generated
    an intermediary language to encode and represent a certain rule about the source
    language syntax, grammar, and so on and then generated a target language from
    the intermediary language. This approach was good compared to the first one but
    soon this approach was replaced by **statistical machine translation** (**SMT**)
    techniques.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个版本是中介语言翻译；它首先将源语言转换成一个中介语言，用来编码并表示源语言的某些规则，如语法、句法等，然后再从中介语言生成目标语言。与第一个版本相比，这种方法有一定的优势，但很快这种方法就被**统计机器翻译**（**SMT**）技术取代了。
- en: IBM used this SMT approach; they broke the text into segments and then compared
    it to an aligned bilingual corpus. After this, using statistical techniques and
    probabilities, the most likely translation was chosen.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: IBM采用了这种SMT方法；他们将文本分成若干段落，然后将其与对齐的双语语料库进行比较。之后，利用统计技术和概率，选择最可能的翻译结果。
- en: The most used SMT in the world is Google translation, and recently, Google published
    a paper stating that their machine translation system uses deep learning to generate
    the great result. We are using the TensorFlow library, which is an open source
    library for deep learning provided by Google. We will code to know how to do machine
    translation using deep learning.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 世界上最常用的SMT是Google翻译，最近，Google发布了一篇论文，指出他们的机器翻译系统使用深度学习来生成出色的翻译结果。我们使用的是TensorFlow库，它是Google提供的一个开源深度学习库。我们将通过编码来了解如何使用深度学习进行机器翻译。
- en: We are using movies subtitles as our dataset. This dataset includes both German
    and English languages. We are building a model that will translate the German
    language into English and vice versa. You can download the data from [http://opus.lingfil.uu.se/OpenSubtitles.php](http://opus.lingfil.uu.se/OpenSubtitles.php).
    Here, I'm using the pickle format of data. Using `pickle`, which is a Python dependency,
    we can serialize our dataset.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用电影字幕作为数据集。这个数据集包括德语和英语两种语言。我们正在构建一个将德语翻译成英语，反之亦然的模型。你可以从 [http://opus.lingfil.uu.se/OpenSubtitles.php](http://opus.lingfil.uu.se/OpenSubtitles.php)
    下载数据。在这里，我使用的是数据的 pickle 格式。使用 `pickle`（这是一个 Python 依赖库），我们可以序列化我们的数据集。
- en: 'To begin with, we are using LSTMs network that is used to remember long term
    and short term dependencies. We are using TensorFlow''s built-in `data_utils`
    class to preprocess the data. Then we need to define the vocabulary size on which
    we need to train the model. Here, our dataset has a small size of vocabulary so
    we are considering all the words in the dataset, but we define vocab (vocabulary)
    size such as 30,000 words, that is, a small set of training dataset. We will use
    the `data_utils` class to read the data from the data directory. This class gives
    us tokenized and formatted words from both languages. Then we define TensorFlow''s
    placeholder that are encoders and decoders for inputs. These both will be integer
    tensors that represent the discrete values. They are embedded into dense representation.
    We will feed our vocabulary words to the encoder and the encoded representation
    that is learned to the decoder. You can see the code at this Github link: [https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)[.](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 LSTM 网络，它用于记住长期和短期的依赖关系。我们使用 TensorFlow 内置的 `data_utils` 类来预处理数据。接下来，我们需要定义词汇表的大小，以便训练模型。在这里，我们的数据集词汇表较小，因此我们考虑数据集中的所有单词，但我们定义词汇表大小为
    30,000 个单词，也就是一个较小的训练数据集。我们将使用 `data_utils` 类从数据目录读取数据。该类为我们提供了来自两种语言的分词和格式化单词。然后我们定义
    TensorFlow 的占位符，它们是输入的编码器和解码器。这两个占位符将是表示离散值的整数张量，它们被嵌入到密集表示中。我们将把词汇表中的单词输入到编码器，并将学习到的编码表示传递给解码器。你可以查看该代码，访问这个
    Github 链接：[https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)[.](https://github.com/jalajthanaki/NLPython/tree/master/ch9/MT/Machine_Translation_GR_EN)
- en: 'Now we can build our model. You can see the code snippets in *Figure 9.44*,
    *Figure 9.45*, and *Figure 9.46*:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建我们的模型了。你可以在*图 9.44*、*图 9.45* 和 *图 9.46* 中查看代码片段：
- en: '![](img/f4ef819d-cc0a-4548-8ff7-6133c93cc300.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4ef819d-cc0a-4548-8ff7-6133c93cc300.png)'
- en: 'Figure 9.44: Code snippet for MT'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.44：机器翻译代码片段
- en: '![](img/6ddc743f-d3f6-41be-a541-b9daef007fce.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ddc743f-d3f6-41be-a541-b9daef007fce.png)'
- en: 'Figure 9.45: Code snippet for MT'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.45：机器翻译代码片段
- en: '![](img/d9f73c8c-821a-4a91-a700-78a88fa3a8b9.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9f73c8c-821a-4a91-a700-78a88fa3a8b9.png)'
- en: 'Figure 9.46: Code snippet for MT'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.46：机器翻译代码片段
- en: 'Now let''s understand this encoder- and decoder-based system. Google recently
    published a paper where they discuss the system that they integrated into their
    translation system, that is, **neural machine translation** (**NMT**). It is an
    encoder decoder-based model with the new NMT architecture. Earlier, Google translated
    from language A to language English and then to language B. Now, Google translator
    can translate directly from one language to the other. Refer to *Figure 9.47*:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们理解这个基于编码器和解码器的系统。谷歌最近发布了一篇论文，讨论了他们在翻译系统中集成的系统，即 **神经机器翻译**（**NMT**）。它是一个基于编码器-解码器的模型，具有新的
    NMT 架构。早期，谷歌翻译是从语言 A 翻译到英语，再翻译到语言 B。现在，谷歌翻译可以直接从一种语言翻译到另一种语言。请参考*图 9.47*：
- en: '![](img/b5fc7c9c-052a-41d1-862c-597580b4dce2.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5fc7c9c-052a-41d1-862c-597580b4dce2.png)'
- en: 'Figure 9.47: LSTM-based encoder and decoder architecture (Image credit: https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67
    )'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.47：基于 LSTM 的编码器和解码器架构（图片来源：https://camo.githubusercontent.com/242210d7d0151cae91107ee63bff364a860db5dd/687474703a2f2f6936342e74696e797069632e636f6d2f333031333674652e706e67
    ）
- en: Now with the existence of NMT, there is no need to memorize phrase-to-phrase
    translation. With the help of NMT, a translation system can encode semantics of
    the sentences. This encoding is generalized so that it can translate from Chinese
    to English, French to English as well as translate language pairs like Korean
    to Japanese, which has not been seen before.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有了 NMT，就不需要记住逐句翻译了。在 NMT 的帮助下，翻译系统可以编码句子的语义。这种编码是泛化的，因此它可以从中文翻译成英文，从法文翻译成英文，也能翻译像韩文到日文这样的语言对，即使这些翻译对之前从未见过。
- en: 'Now can we use this simple LSTM-based encoder-decoder architecture? We will
    see some of the fundamental details of the architecture. Refer to *Figure 9.48*:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个简单的基于 LSTM 的编码器-解码器架构吗？我们将看到该架构的一些基本细节。请参考*图 9.48*：
- en: '![](img/1bfafce8-bdeb-437a-b0d3-988c3a855dcc.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bfafce8-bdeb-437a-b0d3-988c3a855dcc.png)'
- en: 'Figure 9.48: LSTM recurrent NN for translation (Image credit: https://smerity.com/articles/2016/google_nmt_arch.html)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.48：用于翻译的 LSTM 循环神经网络（图片来源：https://smerity.com/articles/2016/google_nmt_arch.html）
- en: We can use LSTM recurrent NN to encode a sentence of language A. The RNN splits
    a hidden state **S**, as shown in *Figure 9.48*. This **S** represents the vectorized
    content of the sentence. After that, we pass this vectorized form to the decoder
    that generates the translated sentence in language B, word by word. It's easy
    to understand this architecture, isn't it? However, this architecture has some
    drawbacks. This architecture has limited memory. The hidden state **S** of the
    LSTM is where we are trying to cram the whole sentence that we want to translate,
    but here **S** is usually a few hundred floating point numbers long. We need to
    fit our sentence into this fixed dimensionality and if we force our sentence to
    fit into this fixed dimensionality, then our network becomes more lossy, which
    means that we lose some information if we are forcefully fitting our sentence
    into a fixed size of dimensionality. We could increase the hidden size of LSTMs
    because their main purpose is to remember long-term dependencies, but if we increase
    the hidden size, then the training time increases exponentially. So, we should
    not use an architecture that takes a lot of time to converge.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 LSTM 循环神经网络来编码 A 语言的句子。RNN 会拆分一个隐藏状态**S**，如*图 9.48*所示。这个**S**表示句子的向量化内容。之后，我们将这个向量化形式传递给解码器，解码器逐词生成翻译后的
    B 语言句子。理解这个架构不难，对吧？然而，这个架构也有一些缺点。该架构的内存有限。LSTM 的隐藏状态**S**就是我们试图将整个待翻译句子塞入其中的地方，但**S**通常只有几百个浮点数那么长。我们需要把句子适配到这个固定的维度，如果我们强行将句子适配到这个固定的维度，那么我们的网络会变得更具损失性，这意味着如果我们强行把句子压缩到一个固定的维度大小，部分信息就会丢失。我们本可以增加
    LSTM 的隐藏层大小，因为它们的主要功能是记住长期依赖关系，但如果我们增加隐藏层大小，训练时间就会呈指数增长。因此，我们不应该使用一个需要大量时间才能收敛的架构。
- en: 'We will introduce another architecture--attention-based encoder-decoder model.
    As humans, when we see a long sentence and we need to translate it, then we probably
    glance back at the source sentence a couple of times to make sure that we are
    capturing all the details. The human mind iteratively pays attention to the relevant
    parts of the source sentence. We want the neural network do the same thing for
    us by letting it store and refer to the previous output of the LST. This increases
    the storage of our model without changing the functionality of LSTMs. Refer to
    *Figure 9.49*:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍另一种架构——基于注意力的编码器-解码器模型。作为人类，当我们看到一个长句子并且需要翻译时，我们可能会回头看几次源句子，以确保我们捕捉到了所有的细节。人类大脑会反复关注源句子中的相关部分。我们希望神经网络为我们做同样的事情，让它存储并参考
    LST 的先前输出。这增加了我们模型的存储量，同时不改变 LSTM 的功能。请参考*图 9.49*：
- en: '![](img/f9e91ecb-06ff-442c-83f8-49d575088208.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9e91ecb-06ff-442c-83f8-49d575088208.png)'
- en: 'Figure 9.49: Architecture of attention-based NMT (Image credit: https://heuritech.files.wordpress.com/2016/01/trad_attention1.png?w=470)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.49：基于注意力的 NMT 架构（图片来源：https://heuritech.files.wordpress.com/2016/01/trad_attention1.png?w=470）
- en: 'Once we have the LSTM output from the encoders stored, we can query each output
    asking how relevant it is to the current computation happening in the decoder.
    Each encoder output gets a relevancy score that we can convert to a probability
    score using the softmax activation function. Refer to *Figure 9.50*:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从编码器获得了 LSTM 的输出并存储下来，我们就可以查询每个输出，询问它与当前解码器中发生的计算的相关性。每个编码器输出都会得到一个相关性评分，我们可以使用
    softmax 激活函数将其转换为概率分数。请参考*图 9.50*：
- en: '![](img/c29a7b47-d0f5-4a0e-9bc4-95ece769d7b1.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c29a7b47-d0f5-4a0e-9bc4-95ece769d7b1.png)'
- en: 'Figure 9.50: SoftMax function to generate the relevance score (Image credit:
    https://smerity.com/)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.50：SoftMax 函数用于生成相关性分数（图片来源： https://smerity.com/）
- en: 'Then, we extract a context vector that is the weighted summation of the encoder''s
    output depending on how relevant they are. Now let''s get back to the code. To
    implement this attention-based functionality, we will use TensorFlow''s built-in
    embedding attention sequence-to-sequence function. This function will take encoder
    and decoder inputs as arguments as well as some additional hyperparameters. This
    function is the same architecture that we have discussed. TensorFlow has some
    really great built-in models that we can use easily. Refer to *Figure* *9.51*
    for the code snippet:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取一个上下文向量，它是根据编码器输出的加权和，权重取决于它们的相关性。现在让我们回到代码。为了实现这个基于注意力的功能，我们将使用 TensorFlow
    内置的嵌入式注意力序列到序列函数。该函数将编码器和解码器的输入作为参数，以及一些额外的超参数。这个函数与我们讨论的架构相同。TensorFlow 有一些非常棒的内置模型，我们可以轻松使用。请参考
    *图 9.51* 查看代码片段：
- en: '![](img/54e5ca7f-7f32-49fc-a610-f911509a9e7f.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54e5ca7f-7f32-49fc-a610-f911509a9e7f.png)'
- en: 'Figure 9.51: Code snippet for attention-based sequence-to-sequence model'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.51：基于注意力的序列到序列模型的代码片段
- en: 'Refer to *Figure 9.52* for the output of the preceding code:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 *图 9.52* 查看前面代码的输出：
- en: '![](img/2288a615-d3dc-4e07-9581-93bfbee1bbd5.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2288a615-d3dc-4e07-9581-93bfbee1bbd5.png)'
- en: 'Figure 9.52: Output for MT'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.52：机器翻译的输出
- en: You can also follow this link, [https://www.tensorflow.org/tutorials/seq2seq](https://www.tensorflow.org/tutorials/seq2seq),
    to run the MT example without making your customized code. This tutorial is an
    example of French to English and English to French translation system. This is
    a really easy way to run this example. I would recommend you to use this way because
    customized code is much complicated to begin with.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过这个链接 [https://www.tensorflow.org/tutorials/seq2seq](https://www.tensorflow.org/tutorials/seq2seq)
    来运行机器翻译示例，而无需编写自定义代码。这个教程是法语到英语和英语到法语的翻译系统示例。这是一种非常简单的运行方式。我建议你使用这种方法，因为自定义代码一开始会更加复杂。
- en: 'First, you need to download 2.4GB training `giga-fren.tar` dataset from this
    link: `http://www.statmt.org/wmt10/training-giga-fren.tar`.'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要从这个链接下载 2.4GB 的训练数据集 `giga-fren.tar`：`http://www.statmt.org/wmt10/training-giga-fren.tar`。
- en: Now you need to store this data in the `data_dir` directory and save your trained
    model time to time. For this, we need to create a checkpoint directory inside
    `train_dir`.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你需要将这些数据存储在 `data_dir` 目录中，并定期保存你训练的模型。为此，我们需要在 `train_dir` 内创建一个检查点目录。
- en: 'After that, you can execute the following command:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以执行以下命令：
- en: '[PRE2]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If the preceding command takes a lot of memory of the GPU, then execute this
    command:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果前面的命令占用了大量GPU内存，可以执行这个命令：
- en: '[PRE3]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once your epoch reaches 340 K with batch size 64, you can use the model for
    translation before that also you can use it but accuracy will as follows:'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你的周期（epoch）达到340 K，批量大小为64，你可以在此之前使用模型进行翻译，不过准确率会如下所示：
- en: '[PRE4]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our German to English (GR_EN) translation model gives us a fairly good result
    and we are doing only one round of training, but if we really want to get the
    same accuracy that Google has in their translation system, then we need to train
    this model for several weeks using high computational capability such as 100 GPUs
    for several weeks continuously running. Here, we are definitely not going to implement
    that model, but I will explain its working. So let's dive conceptually.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的德语到英语（GR_EN）翻译模型给出了相当不错的结果，我们仅进行了一个训练回合，但如果我们真的想获得与谷歌翻译系统相同的准确性，那么我们需要使用高计算能力，如连续几周使用
    100 个 GPU 来训练该模型。在这里，我们肯定不会实现这个模型，但我会解释它的工作原理。那么，让我们从概念上深入了解。
- en: 'If the output doesn''t have sufficient context for the encoded source sentence,
    then the model won''t be able to give us a good translation result. In this case,
    we need to give the information about the future words so that the encoder output
    is determined by the words on the left and right. As humans, we use this kind
    of full context to understand the meaning of the sentence. This will happen on
    the machine level by including bidirectional encoders so that it contains two
    **recurrent neural nets** (**RNN**). One goes forward over the sentence and the
    other goes backward. So for each word, it concatenates the vector outputs that
    produce the vector with context of both sides. Refer to *Figure 9.53*:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出没有足够的上下文信息来表示编码后的源句子，那么模型将无法给我们提供良好的翻译结果。在这种情况下，我们需要提供未来词汇的信息，这样编码器的输出就会由左侧和右侧的词汇决定。作为人类，我们使用这种完整的上下文来理解句子的意思。在机器层面，通过包含双向编码器来实现这一点，使其包含两个**循环神经网络**（**RNN**）。一个从前往后遍历句子，另一个从后往前遍历。因此，对于每个单词，它将连接产生双侧上下文向量的输出。参见*图9.53*：
- en: '![](img/ffcd4d3b-eac4-456c-a130-7548bcfc79d7.png)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffcd4d3b-eac4-456c-a130-7548bcfc79d7.png)'
- en: 'Figure 9.53: Bidirectional RNN architecture for MT (Image credit: http://img.yantuwan.com/weiyuehao.php?http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2wCIHNabCic1Hr144r4PAuSDMLNMHGgWz12GtibYdgF1jTvHtuniauHYSw/0?wx_fmt=png)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.53：用于机器翻译的双向RNN架构（图片来源：http://img.yantuwan.com/weiyuehao.php?http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW81aVM2Gdgrxfsa0vR4YVib2wCIHNabCic1Hr144r4PAuSDMLNMHGgWz12GtibYdgF1jTvHtuniauHYSw/0?wx_fmt=png)
- en: Google has included lots of layers to the models as well as encoders that have
    one bidirectional RNN layer and seven unidirection layers. The decoder has eight
    unidirectional RNN layers. If you add more layers, then the training time increases.
    Here, we are using only one bidirectional layer. If all layers are bidirectional,
    then the whole layer would have to finish their computation before other layer
    dependencies could start their computation. With the help of a unidirection layer,
    we can perform computation in parallel.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在模型中加入了大量的层次结构，以及包含一个双向RNN层和七个单向层的编码器。解码器有八个单向RNN层。如果你增加更多的层次，训练时间会相应增加。在这里，我们仅使用一个双向层。如果所有层都是双向的，那么整个层次必须在其他层的计算开始之前完成其计算。借助单向层，我们可以并行执行计算。
- en: This is all about machine translation. We have generated the machine translation
    output but still there is more room for improvement. Using DL, we are going to
    build a single universal machine translation system.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这完全是关于机器翻译的内容。我们已经生成了机器翻译的输出，但仍然有改进的空间。通过使用深度学习（DL），我们将构建一个通用的单一机器翻译系统。
- en: Now let's begin our next part of coding that is based on NLG.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始基于自然语言生成（NLG）的下一部分编程。
- en: Deep learning techniques and NLG
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术与自然语言生成（NLG）
- en: In this section, we are going build a very simple but intuitive application
    for NLG. We are going to generate a one-line summary from shot articles. We will
    see all the details about summarization in this section.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将构建一个非常简单但直观的自然语言生成（NLG）应用程序。我们将从简短的文章中生成一句话的摘要。在这一部分，我们将详细探讨摘要生成的所有细节。
- en: This application took a lot of training time so you can put your model to train
    on CPU and meanwhile, you can do some other task. If you don't have any other
    task, then let me give you one.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这个应用程序需要大量的训练时间，因此你可以将模型放到CPU上训练，同时可以做一些其他任务。如果你没有其他任务，那让我给你分配一个任务。
- en: Exercise
  id: totrans-440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Try to figure out how you can generate a Wikipedia article by just providing
    some starting character sequences. Don''t take me wrong! I''m serious! You seriously
    need to think on this. This is the dataset that you can use: [https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset).
    Jump to the download section and download this dataset named Download WikiText-103
    word level (181 MB).'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试想一想，如何仅通过提供一些起始字符序列就能生成一篇维基百科文章。别误会我！我很认真！你真的需要认真思考这个问题。这是你可以使用的数据集：[https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset](https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset)。跳转到下载部分并下载名为“Download
    WikiText-103 word level (181 MB)”的该数据集。
- en: '(Hint: See this link, [https://github.com/kumikokashii/lstm-text-generator](https://github.com/kumikokashii/lstm-text-generator).)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: （提示：参见此链接，[https://github.com/kumikokashii/lstm-text-generator](https://github.com/kumikokashii/lstm-text-generator)。）
- en: Don't worry ;after understanding the concepts of summarization, you can attempt
    this. So let's begin the summarization journey!
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心；理解了摘要的概念后，你就可以尝试这个了。让我们开始摘要之旅吧！
- en: Recipe summarizer and title generation
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 菜谱摘要器和标题生成
- en: Before jumping into the code, I want to give you some brief background about
    summarization. Architecture and other technical parts will be understood as we
    code.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入代码之前，我想先给你一些关于摘要的简要背景。架构和其他技术部分将在编码过程中逐步理解。
- en: Semantics is a really big deal in NLP. As data increases in the density of the
    text, information also increases. Nowadays, people around you really expect that
    you say the most important thing effectively in a short amount of time.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 语义在NLP中至关重要。随着数据密度的增加，信息量也在增加。如今，周围的人确实希望你能在短时间内高效地说出最重要的事情。
- en: Text summarization started in the 90s. The Canadian government built a system
    named **forecast generator** (**FoG**) that uses weather forecast data and generates
    a summary. That was the template-based approach where the machine just needed
    to fill in certain values. Let me give you an example, **Saturday will be sunny
    with 10% chances of rain**. The word *sunny* and *10%* are actually generated
    by FoG.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要起源于90年代。加拿大政府开发了一个名为**预测生成器**（**FoG**）的系统，利用天气预报数据生成摘要。那时采用的是基于模板的方法，机器只需要填充某些特定的值。让我给你举个例子，**周六将是晴天，降雨概率为10%**。其中的*晴天*和*10%*其实是FoG生成的。
- en: The other areas are finance, medical, and so on. In the recent world, doctors
    find the summarization of a patient's medical history very useful and they can
    diagnose people efficiently and effectively.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 其他领域包括金融、医疗等。在当今世界，医生发现总结患者的病史非常有用，他们可以高效、有效地进行诊断。
- en: 'There are two types of summary that are given as follows:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两种类型的摘要：
- en: Extractive
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽取式
- en: Abstractive
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象性
- en: Most summarization tools in the past were of the extractive type; they selected
    an existing set of words from the article to create a summary for the article.
    As humans, we do something more; that is, when we summarize, we build an internal
    semantic representation of what we have read. Using this internal semantic representation,
    we can summarize text. This kind of summarization is called **abstractive summarization**.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的大多数摘要工具都是抽取式的；它们从文章中选择现有的词语集合来创建摘要。而我们人类做的更多；也就是说，当我们总结时，我们会构建一个内部的语义表示，这个表示基于我们所阅读的内容。利用这个内部的语义表示，我们可以总结文本。这种摘要方式被称为**抽象摘要**。
- en: So let's build an abstractive summarization tool using Keras.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们使用Keras构建一个抽象摘要工具。
- en: Keras is a high-level wrapper for TensorFlow and Theano. This example needs
    multiple GPUs for more than 12 hours. If you want to reproduce the result at your
    end, then it shall take a lot of computation power.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个高层封装的TensorFlow和Theano工具。这个示例需要多个GPU支持，训练时间超过12小时。如果你想在本地重现结果，那将需要大量的计算资源。
- en: 'These are the steps for the coding part. Here, for the first time, we are using
    Python 3:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是编码部分的步骤。这里，我们首次使用Python 3：
- en: 'Clone the GitHub Repository:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆GitHub仓库：
- en: '[https://github.com/jalajthanaki/recipe-summarization](https://github.com/jalajthanaki/recipe-summarization)'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/recipe-summarization](https://github.com/jalajthanaki/recipe-summarization)'
- en: 'Initialized submodules:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 已初始化子模块：
- en: '[PRE5]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Go inside the folder:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入文件夹：
- en: '[PRE6]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Install dependencies:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装依赖：
- en: '[PRE7]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set up directories:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置目录：
- en: '[PRE8]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Scrape recipes from the web or use the existing one at this link:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网页抓取菜谱或使用这个链接中的现有菜谱：
- en: '[PRE9]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Tokenize the data:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行分词：
- en: '[PRE10]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Initialize word embeddings with `GloVe vectors`:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`GloVe向量`初始化词嵌入：
- en: 'Get the GloVe vectors trained model:'
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取GloVe向量训练模型：
- en: '[PRE11]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize embeddings:'
  id: totrans-473
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化嵌入：
- en: '[PRE12]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the model:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE13]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Make predictions:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行预测：
- en: '[PRE14]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here, for vectorization, we are using GloVe because we want a global-level
    representation of the words for summarization, and we are using the sequence-to-sequence
    model (Seq2Seq model) to train our data. Seq2Seq is the same model that we discussed
    in the *Machine translation* section. See the code snippets in *Figure 9.54*,
    *Figure 9.55*, and *Figure 9.56*, and after training, you can see the output in
    *Figure 9.57*:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们使用GloVe进行向量化，因为我们希望获得全球级别的词语表示用于摘要，并且我们使用的是序列到序列模型（Seq2Seq模型）来训练数据。Seq2Seq就是我们在*机器翻译*部分讨论的模型。查看*图9.54*、*图9.55*和*图9.56*中的代码片段，训练完成后，你可以在*图9.57*中看到输出：
- en: '![](img/774350b6-b0c1-4852-8511-ce23d7b33e67.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](img/774350b6-b0c1-4852-8511-ce23d7b33e67.png)'
- en: 'Figure 9.54: Tokenization code snippet'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.54：分词代码片段
- en: 'Refer to the following figure for vocab building using GloVe:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图示以使用 GloVe 进行词汇构建：
- en: '![](img/3e784e55-84ac-4929-8f20-b64e775d0ada.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e784e55-84ac-4929-8f20-b64e775d0ada.png)'
- en: 'Figure 9.55: Vocab building using GloVe'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.55：使用 GloVe 进行词汇构建
- en: 'Refer to the following figure to train the model:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图示以训练模型：
- en: '![](img/964a4407-12e0-4da8-9f5f-fe3571ea7c66.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/964a4407-12e0-4da8-9f5f-fe3571ea7c66.png)'
- en: 'Figure 9.56: Training of the model'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.56：模型训练
- en: 'Examples are given in the following figure:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示给出了示例：
- en: '![](img/84094480-18c5-45b1-8d25-32a62a772ce9.png)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84094480-18c5-45b1-8d25-32a62a772ce9.png)'
- en: 'Figure 9.57: Prediction result of the model'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.57：模型的预测结果
- en: I know that the summarization example will take a lot of computational power
    and maybe there will be a situation where your local machine does not have enough
    memory (RAM) to run this code. In that case, don't worry; there are various cloud
    options available that you can use. You can use Google Cloud, Amazon Web Services
    (AWS), or any other.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，摘要示例将需要大量的计算能力，可能会出现您的本地机器内存（RAM）不足以运行此代码的情况。在这种情况下，不用担心；有多种云服务选项可供使用。您可以使用
    Google Cloud、Amazon Web Services（AWS）或任何其他服务。
- en: 'Now you have enough idea about the NLU and NLG applications. I have also put
    one more application related to the NLG domain at this GitHub Link:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经对自然语言理解（NLU）和自然语言生成（NLG）应用有了足够的了解。我还在此 GitHub 链接中放置了一个与 NLG 领域相关的应用：
- en: '[https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/tensorflow/models/tree/master/im2txt](https://github.com/tensorflow/models/tree/master/im2txt)'
- en: This application generates captions for images; this is a kind of combined application
    of computer vision and NLG. Necessary details are on GitHub so check out this
    example as well.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序为图像生成标题；这是一种计算机视觉与 NLG 结合的应用。相关的详细信息在 GitHub 上，所以也可以查看这个示例。
- en: In the next section, we will see the gradient descent-based optimization strategy.
    TensorFlow provides us with some variants of the gradient descent algorithm. Once
    we have an idea of how all these variants work and what are the drawbacks and
    advantages of each of them, then it will be easy for us to choose the best option
    for the optimization of our DL algorithm. So let's understand the gradient descent-based
    optimization.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论基于梯度下降的优化策略。TensorFlow 为我们提供了一些梯度下降算法的变种。一旦我们了解这些变种的工作原理以及它们各自的优缺点，那么选择最适合优化我们深度学习算法的选项将变得很容易。因此，让我们来理解基于梯度下降的优化。
- en: Gradient descent-based optimization
  id: totrans-496
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于梯度下降的优化
- en: In this section, we will discuss gradient descent-based optimization options
    that are provided by TensorFlow. Initially, it will not be clear which optimization
    option you should use, but as and when you know the actual logic of the DL algorithm,
    it will became much clearer to you.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 TensorFlow 提供的基于梯度下降的优化选项。最初，可能不清楚应该使用哪个优化选项，但随着您了解深度学习算法的实际逻辑，您会变得更加清晰。
- en: We use a gradient descent-based approach to develop an intelligent system. Using
    this algorithm, the machine can learn how to identify patterns from the data.
    Here, our end goal is to obtain the local minimum and the objective function is
    the final prediction that the machine will make or result that is generated by
    the machine. In the gradient descent-based algorithm, we are not concentrating
    on how to achieve the best final goal for our objective function in the first
    step, but we will iteratively or repeatedly take small steps and select the intermediate
    best option that leads us to achieve the final best option, that is, our local
    minima. This kind of educated guess and check method works well to obtain local
    minima. When the DL algorithm obtains local minima, the algorithm can generate
    the best result. We have already seen the basic gradient descent algorithm. If
    you face overfitting and underfitting situations, you can optimize the algorithm
    using different types of gradient descent. There are various flavors of gradient
    descent that can help us in order to generate the ideal local minima, control
    the variance of the algorithm, update our parameters, and lead us to converge
    our ML or DL algorithm. Let's take an example. If you have function Y = X², then
    the partial derivative of the given function is 2X. When we randomly guess the
    stating value and we start with value X = 3, then Y = 2(3) =6 and to obtain local
    minima, we need to take a step in the negative direction--so Y = -6\. After the
    first iteration, if you guess the value X = 2.3, then Y = 2(2.3) = 4.6 and we
    need to move in the negative direction again--Y = -4.6--because we get a positive
    value. If we get a negative value, then we move in the positive direction. After
    certain iterations, the value of Y is very near zero and that is our local minima.
    Now let's start with basic gradient descent. Let's start exploring varieties of
    gradient descent.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于梯度下降法的方法来开发智能系统。通过这种算法，机器可以学习如何从数据中识别模式。在这里，我们的最终目标是获得局部最小值，而目标函数是机器将做出的最终预测或机器生成的结果。在梯度下降算法中，我们并不专注于在第一步就实现目标函数的最佳最终目标，而是通过迭代或反复采取小步骤，选择中间的最佳选项，以帮助我们实现最终的最佳选项，也就是局部最小值。这种有根据的猜测与检验方法有助于获得局部最小值。当深度学习算法获得局部最小值时，算法可以生成最佳结果。我们已经看过了基础的梯度下降算法。如果你遇到过拟合或欠拟合的情况，你可以通过不同类型的梯度下降来优化算法。梯度下降有多种变体，可以帮助我们生成理想的局部最小值，控制算法的方差，更新我们的参数，并最终使我们的机器学习或深度学习算法收敛。让我们举个例子。如果你有函数
    Y = X²，那么该函数的偏导数是 2X。当我们随机猜测起始值，并以 X = 3 开始时，Y = 2(3) = 6，为了获得局部最小值，我们需要朝负方向移动——所以
    Y = -6。在第一次迭代后，如果你猜测 X = 2.3，那么 Y = 2(2.3) = 4.6，我们需要再次朝负方向移动——Y = -4.6——因为我们得到的是一个正值。如果得到负值，则需要朝正方向移动。经过若干次迭代后，Y
    的值会接近零，这就是我们的局部最小值。现在让我们从基础的梯度下降法开始，探索梯度下降法的各种变体。
- en: '**Basic gradient descent**'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础梯度下降法**'
- en: In basic gradient descent, we calculate the gradient of loss function with regards
    to the parameters present in the entire training dataset, and we need to calculate
    gradient for the entire dataset to perform a single update. For a single update,
    we need to consider the whole training dataset as well as all parameters so it
    is very slow. You can see the equation in *Figure 9.58:*
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在基础梯度下降法中，我们计算整个训练数据集中的参数相对于损失函数的梯度，并且为了执行一次更新，我们需要计算整个数据集的梯度。为了进行一次更新，我们必须考虑整个训练数据集以及所有参数，所以这个过程非常慢。你可以在*图
    9.58*中看到这个方程：
- en: '![](img/7b8ef16a-da38-4822-862e-66d42a50b2a2.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b8ef16a-da38-4822-862e-66d42a50b2a2.png)'
- en: 'Figure 9.58: Equation for gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.58：梯度下降的方程（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: You can find the sample logic code for understanding purposes in *Figure 9.59:*
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 9.59*中找到理解此方法的示例逻辑代码：
- en: '![](img/4224bd92-3687-45ac-b1be-64c67d519676.png)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4224bd92-3687-45ac-b1be-64c67d519676.png)'
- en: 'Figure 9.59: Sample code for gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.59：梯度下降的示例代码（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: As this technique is slow, we will introduce a new technique called Stochastic
    Gradient Descent.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种方法较慢，我们将引入一种新的技术，叫做随机梯度下降法。
- en: '**Stochastic gradient descent**'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降法**'
- en: In this technique, we update the parameters for each training example and label
    so we just need to add a loop for our training dataset and this method updates
    the parameters faster compared to basic gradient descent. You can see the equation
    in *Figure 9.60:*
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个技术中，我们会为每个训练样本和标签更新参数，因此我们只需要为训练数据集添加一个循环，并且这种方法相比基础梯度下降法更新参数更快。你可以在*图9.60*中看到公式：
- en: '![](img/9ea66aa6-aeaf-46f8-aa38-cd42d767e815.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ea66aa6-aeaf-46f8-aa38-cd42d767e815.png)'
- en: 'Figure 9.60: Equation for stochastic gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.60：随机梯度下降公式（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: You can find the sample logic code for understanding purposes in *Figure 9.61:*
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图9.61*中找到用于理解的示例逻辑代码：
- en: '![](img/4983e3bb-2285-4c0b-880b-f9e100da46a0.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4983e3bb-2285-4c0b-880b-f9e100da46a0.png)'
- en: 'Figure 9.61: Sample code for stochastic gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.61：随机梯度下降示例代码（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: This method also has some issues. This method makes convergence complicated
    and sometimes updating the parameters is too fast. The algorithm can overshoot
    the local minima and keep running. To avoid this problem, another method is introduced
    called Mini-Batch Gradient Descent.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也有一些问题。该方法使得收敛变得复杂，有时更新参数的速度过快。算法可能会超过局部最小值并继续运行。为避免这个问题，引入了另一种方法，叫做小批量梯度下降。
- en: '**Mini-batch gradient descent**'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**'
- en: In this method, we will take the best part from both basic gradient descent
    and stochastic gradient descent. We will take a subset of the training dataset
    as a batch and update the parameters from them. This type of gradient descent
    is used for basic types of ANNs.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们将结合基础梯度下降法和随机梯度下降法的优点。我们将从训练数据集中取出一个子集作为小批量，并从中更新参数。这种类型的梯度下降法适用于基本的人工神经网络（ANN）。
- en: You can see the equation in *Figure 9.62:*
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图9.62*中看到公式：
- en: '![](img/ea637bf4-9d39-42be-b88c-db54c3eb1513.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea637bf4-9d39-42be-b88c-db54c3eb1513.png)'
- en: 'Figure 9.62: Equation for mini-batch gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.62：小批量梯度下降公式（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: You can find the sample logic code for understanding purposes in *Figure 9.63:*
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图9.63*中找到用于理解的示例逻辑代码：
- en: '![](img/23b21169-fb95-4586-88f0-4e59625d1649.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/23b21169-fb95-4586-88f0-4e59625d1649.png)'
- en: 'Figure 9.63: Sample code for mini-batch gradient descent (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.63：小批量梯度下降示例代码（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: If we have a high-dimensional dataset, then we can use some other gradient descent
    method; let's begin with momentum.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个高维数据集，那么我们可以使用一些其他的梯度下降方法；让我们从动量法开始。
- en: '**Momentum**'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '**动量**'
- en: If all the possible parameters' values surface curves much more steeply in one
    dimension than in another, then in this kind of case, this are very common around
    local optima. In these scenarios, SGD oscillates across the slopes. So to solve
    this oscillation issue, we will use the momentum method. You can see the equation
    in *Figure 9.64:*
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有可能参数的值表面曲线在一个维度上比在另一个维度上陡峭得多，那么这种情况在局部最优附近非常常见。在这些场景中，SGD会在坡度之间震荡。为了解决这个震荡问题，我们将使用动量法。你可以在*图9.64*中看到公式：
- en: '![](img/d0de2757-8da0-4dd8-9622-31f965891b99.png)'
  id: totrans-526
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0de2757-8da0-4dd8-9622-31f965891b99.png)'
- en: 'Figure 9.64: Equation for momentum (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.64：动量公式（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: If you see the equation, we are adding a fraction of the direction of the gradient
    from the previous time step to the current step, and we amplify the parameter
    update in the right direction that speeds up our convergence and reduces the oscillation.
    So here, the concept of momentum is similar to the concept of momentum in physics.
    This variant doesn't slow down when local minima is obtained because at that time,
    the momentum is high. In this situation, our algorithm can miss the local minima
    entirely and this problem can be solved by Nesterov accelerated gradient.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看方程，我们将上一时间步的梯度方向的一部分加到当前步骤中，并将参数更新放大到正确的方向，这加速了我们的收敛，并减少了振荡。所以在这里，动量的概念与物理学中的动量概念相似。当达到局部最小值时，这种变种并不会减慢，因为此时动量较大。在这种情况下，我们的算法可能完全错过局部最小值，而这个问题可以通过
    Nesterov 加速梯度来解决。
- en: '**Nesterov accelerated gradient**'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nesterov 加速梯度**'
- en: 'This method was invented by Yurii Nesterov. He was trying to solve the issue
    that occurred in the momentum technique. He has published a paper that you can
    see at this link:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法由 Yurii Nesterov 发明。他试图解决动量技术中出现的问题。他已发布了一篇论文，你可以通过此链接查看：
- en: You can see the equation in *Figure 9.65:*
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 9.65*中看到该方程：
- en: '![](img/301534eb-f693-43e5-bf8f-3b1607dd3f3d.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![](img/301534eb-f693-43e5-bf8f-3b1607dd3f3d.png)'
- en: 'Figure 9.65: Equation for Nesterov accelerated gradient (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.65：Nesterov 加速梯度的方程（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: As you can see, we are doing the same calculation that we have done for momentum
    but we have changed the order of calculation. In momentum, we compute the gradient
    make jump in that direction amplified by the momentum, whereas in the Nesterov
    accelerated gradient method, we first make a jump based on the previous momentum
    then calculate gradient and after that we add a correction and generate the final
    update for our parameter. This helps us provide parameter values more dynamically.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们做的是与动量法相同的计算，只是改变了计算顺序。在动量法中，我们计算梯度并按动量放大该方向的跳跃，而在 Nesterov 加速梯度法中，我们首先基于之前的动量进行跳跃，然后计算梯度，接着我们做一个修正，最终生成参数的更新值。这帮助我们更加动态地提供参数值。
- en: '**Adagrad**'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adagrad**'
- en: Adagrad is stands for adaptive gradient. This method allows the learning rate
    to adapt based on the parameters. This algorithm provides a big update for infrequent
    parameters and a small update for frequent parameters. You can see the equation
    in *Figure 9.66:*
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 代表自适应梯度。这种方法允许学习率基于参数进行自适应调整。该算法为不常出现的参数提供较大的更新，为频繁出现的参数提供较小的更新。你可以在*图
    9.66*中看到该方程：
- en: '![](img/666299f3-4f5a-46af-86f7-2deef8e1188f.png)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![](img/666299f3-4f5a-46af-86f7-2deef8e1188f.png)'
- en: 'Figure 9.66: Equation for Adagrad (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.66：Adagrad 方程（图片来源：http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges）
- en: This method provides a different learning rate for every parameter at the given
    timestamp based on the past gradient computed for that parameter. Here, we don't
    need to manually tune our learning rate although it has a limitation. As per the
    equation, the learning rate is always decreasing as the accumulation of the squared
    gradients placed in the denominator is always positive, and as the denominator
    grows, the whole term will decrease. Sometimes, the learning rate becomes so small
    that the ML-model stops learning. To solve this problem. the method called Adadelta
    has come into the picture.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法为每个参数提供一个基于该参数过去梯度计算的不同学习率。在这里，我们无需手动调整学习率，尽管它有一个局限性。根据方程，学习率始终下降，因为分母中平方梯度的累积值始终为正，随着分母的增大，整个项会减少。有时，学习率变得非常小，导致机器学习模型停止学习。为了解决这个问题，Adadelta
    方法应运而生。
- en: '**Adadelta**'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adadelta**'
- en: Adadelta is an extension of Adagrad. In Adagrad, we constantly add the square
    root to the sum causing the learning rate to decrease. Instead of summing all
    the past square roots, we restrict the window to the accumulated past gradient
    to a fixed size.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: Adadelta 是 Adagrad 的扩展。在 Adagrad 中，我们不断将平方根加入求和，从而导致学习率逐渐减小。与其将所有过去的平方根相加，我们将累积的过去梯度窗口限制为一个固定大小。
- en: You can see the equation in *Figure 9.67:*
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 9.67*中看到该方程：
- en: '![](img/72c1a17c-1bd5-4af0-93d7-c52406dbcd26.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72c1a17c-1bd5-4af0-93d7-c52406dbcd26.png)'
- en: 'Figure 9.67: Equation for Adadelta (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.67：Adadelta 的方程式（图片来源：[http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges](http://sebastianruder.com/optimizing-gradient_descent/index.html#challenges)）
- en: As you can see in the equation, we will use the sum of gradient as a decaying
    average of all past squared gradients. Here, the running average *E[g²][t]* at
    a given timestamp is dependent on the previous average and the current gradient.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在方程式中看到的，我们将使用梯度的总和作为所有过去的平方梯度的衰减平均值。这里，给定时间戳的运行平均值 *E[g²][t]* 依赖于前一个平均值和当前梯度。
- en: After seeing all the optimization techniques, you know how we can calculate
    the individual learning rate for each parameter, how we can calculate the momentum,
    and how we can prevent the decaying learning rate. Still, there is room for improvement
    by applying some adaptive momentum and that leads us to our final optimization
    method called **Adam**.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 经过对所有优化技术的了解，你现在知道我们如何计算每个参数的个别学习率，如何计算动量，以及如何防止学习率衰减。不过，通过应用一些自适应动量，依然有改进的空间，这就引出了我们最后的优化方法——**Adam**。
- en: '**Adam**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adam**'
- en: Adam stands for adaptive momentum estimation. As we are calculating the learning
    rate for each parameter, we can also store the momentum changes for each of them
    separately.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: Adam 代表自适应动量估计。在计算每个参数的学习率时，我们还可以单独存储每个参数的动量变化。
- en: You can see the equation in *Figure 9.68:*
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 9.68*中看到方程式：
- en: '![](img/ffc6ace5-3525-4373-95cf-9de59ab2ca74.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc6ace5-3525-4373-95cf-9de59ab2ca74.png)'
- en: 'Figure 9.68: Mean and variance for Adam (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.68：Adam 的均值和方差（图片来源：[http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges](http://sebastianruder.com/optimizing-gradient_descent/index.html#challenges)）
- en: First, we will be calculating the mean of the gradient, then we are going to
    calculate the uncentered variance of the gradient, and use these values to update
    the parameters. Just like an Adadelta. You can see the equation of Adam in *Figure
    9.69:*
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算梯度的均值，然后计算梯度的非中心方差，并使用这些值来更新参数。这就像 Adadelta 一样。你可以在*图 9.69*中看到 Adam
    的方程式：
- en: '![](img/2fe35988-9e9c-466d-9bd0-4bb0368661d8.png)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe35988-9e9c-466d-9bd0-4bb0368661d8.png)'
- en: 'Figure 9.69: Equation for Adam (Image credit: http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges)'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.69：Adam 的方程式（图片来源：[http://sebastianruder.com/optimizing-gradient-descent/index.html#challenges](http://sebastianruder.com/optimizing-gradient_descent/index.html#challenges)）
- en: So now you want to know which method we should use; according to me, Adam is
    the best overall choice because it outperforms the other methods. You can also
    use Adadelta and Adagrad. If your data is sparse, then you should not use SGD,
    momentum, or Nesterov.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 那么现在你可能想知道我们应该使用哪种方法；依我看，Adam 是最好的整体选择，因为它在性能上超越了其他方法。你也可以使用 Adadelta 和 Adagrad。如果你的数据是稀疏的，那么你应该避免使用
    SGD、动量方法或 Nesterov。
- en: Artificial intelligence versus human intelligence
  id: totrans-556
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能与人类智能
- en: From the last one year, you may have heard this kind of question. In the AI
    world, these kinds of questions have become common. People have created a hype
    that AI will make humanity vanish and machines will take away all the powers from
    us. Now let me tell you, this is not the truth. These kinds of threats sound like
    science-fiction stories. According to me, AI is in its high pace development phase
    but its purpose is to complement humanity and make human life easier. We are still
    figuring out some of the complex and unknown truths of this universe that can
    help us provide more insight on how we can build AI-enabled systems. So AI is
    purely going to help us. AI will amaze our lives for sure but it is not going
    to be saturated with its inventions soon. So enjoy this AI phase and contribute
    to the AI ecosystem in a positive manner.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年中，你可能听到过这样的问题。在人工智能的世界里，这类问题已经变得非常常见。人们制造了一种炒作，认为人工智能将使人类消失，机器将夺走我们的一切权力。现在让我告诉你，这并不是真相。这类威胁听起来像是科幻故事。依我看，人工智能正处于高速发展的阶段，但其目的是补充人类并使人类生活更轻松。我们仍在探索宇宙中一些复杂且未知的真理，这些真理能帮助我们提供更多关于如何构建人工智能系统的洞见。所以，人工智能纯粹是为了帮助我们。人工智能肯定会让我们的生活充满惊喜，但它不会很快饱和。享受这个人工智能阶段，并以积极的方式为人工智能生态系统做出贡献。
- en: People have concerns that AI will take away our jobs. It will not take away
    your job. It will make your job easier. If you are a doctor and want to give your
    final words on some cancer report, AI will help you. In the Information Technology
    (IT) industry, there's a concern that AI will replace the coders. If you believe
    that very soon researchers and tech companies will be able to build machines that
    are more powerful than humans and that the AI shift will happen soon and machines
    will take away our jobs, then it is better for you to acquire ML, DL, and AI related
    skill sets to have jobs, and perhaps you are the last person on this planet who
    has some job to do! We assume that AI would take away some jobs, but this AI ecosystem
    will also create so many new jobs. So don't worry! This discussion can be ongoing
    but I want to really give you guys some time window to think on this.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 有人担心人工智能会夺走我们的工作。它不会夺走你的工作，它会让你的工作变得更轻松。如果你是医生，想要对某份癌症报告给出最终意见，人工智能会帮助你。在信息技术（IT）行业，有人担心人工智能会取代程序员。如果你相信研究人员和科技公司很快就会建造出比人类更强大的机器，认为人工智能的转变很快会发生，机器会夺走我们的工作，那么你最好掌握机器学习（ML）、深度学习（DL）和人工智能相关的技能，以确保你还有工作做，也许你是这个星球上最后一个还有工作的人！我们假设人工智能会夺走一些工作，但这个人工智能生态系统也会创造出许多新的工作机会。所以不要担心！这场讨论可以继续下去，但我真的想给你们一点时间思考这个问题。
- en: Summary
  id: totrans-559
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations guys! We have made it to the last chapter! I really appreciate
    your efforts. In this chapter, you have learned a lot of things such as artificial
    intelligence aspects that help you understand why deep learning is the buzzword
    nowadays. We have seen the concept of ANNs. We have seen concepts such as gradient
    descent, various activation functions, and loss functions. We have seen the architecture
    of DNN and the DL life cycle. We have also touched on the basics of the sequence-to-sequence
    model and developed applications such as machine translation, title generation,
    and summarization. We have also seen the gradient descent-based optimization techniques.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜大家！我们已经到达最后一章了！我非常感谢你们的努力。在这一章中，你们学到了很多东西，比如人工智能的各个方面，帮助你们理解为什么深度学习如今成为热门词汇。我们已经了解了人工神经网络（ANNs）的概念，学习了梯度下降、各种激活函数和损失函数等概念。我们还看到了深度神经网络（DNN）的架构和深度学习生命周期。我们还涉及了序列到序列模型的基础，并开发了机器翻译、标题生成和摘要等应用。我们还学习了基于梯度下降的优化技术。
- en: The next sections are Appendices A to C, that will provide you with an overview
    about frameworks such as hadoop, spark, and so on. You can also see the installation
    guide for these frameworks as well as other tools and libraries. Apart from this,
    you can find cheatsheets for many Python libraries that are very handy if you
    are new to Python. There are some tips from my side if you really want to improve
    your data science as well as NLP skills. I have also provided Gitter links in
    the appendices that you can use to connect with me in case you have any questions.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分是附录A到C，这些内容将为你提供关于框架（如hadoop、spark等）的概述。你还可以看到这些框架以及其他工具和库的安装指南。除此之外，你还可以找到很多Python库的备忘单，如果你是Python新手，这些备忘单会非常有用。如果你真的想提升你的数据科学和自然语言处理（NLP）技能，我还提供了一些我的个人建议。我还在附录中提供了Gitter链接，你可以通过这些链接联系我，若有任何问题可以随时向我请教。
