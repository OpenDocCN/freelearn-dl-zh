- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Improving Inference Performance with MXNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MXNet提升推理性能
- en: In previous chapters, we leveraged MXNet’s capabilities to solve **computer
    vision** and **natural language processing tasks**. In those chapters, the focus
    was on obtaining the maximum performance out of **pre-trained models**, leveraging
    the **Model Zoo** API from GluonCV and GluonNLP. We trained these models using
    different approaches from scratch, including **transfer learning** and **fine-tuning**.
    In the previous chapter, we explored how some advanced techniques can be leveraged
    to optimize the training process. Finally, in this chapter, we will focus on improving
    the performance of the inference process itself, accelerating how we can obtain
    results from our models with several topics related to **edge** **AI computing**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们利用MXNet的功能解决了**计算机视觉**和**自然语言处理任务**。这些章节的重点是从**预训练模型**中获得最大性能，利用GluonCV和GluonNLP的**模型库**API。我们使用从头开始的不同方法训练这些模型，包括**迁移学习**和**微调**。在上一章中，我们探索了如何利用一些高级技术优化训练过程。最后，在本章中，我们将重点提高推理过程本身的性能，加速从我们的模型中获得结果，并讨论与**边缘****AI计算**相关的多个主题。
- en: To achieve the objective of optimizing the performance of our inference pipeline,
    MXNet contains different features. We have already briefly discussed some of those
    features, such as the concept of **Automatic Mixed Precision** (**AMP**), which
    was introduced in the previous chapter to increase the training performance and
    can also be used to increase the inference performance. We will revisit it in
    this chapter, along with other features, such as **hybridization**. Moreover,
    we will further optimize how to use data types efficiently, leveraging the speed-ups
    associated with using the **INT8** data type with **quantization**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现优化推理管道性能的目标，MXNet包含了不同的功能。我们已经简要讨论过其中的一些功能，例如在前一章中介绍的**自动混合精度**（**AMP**），它可以提高训练性能，同时也可以用来提升推理性能。在本章中，我们将重新讨论这一点，以及其他功能，如**混合化**。此外，我们还将进一步优化如何有效利用数据类型，借助**量化**中的**INT8**数据类型加速推理过程。
- en: Moreover, we will explore how our models work in terms of operations, understanding
    how they work internally with the help of the **MXNet profiler**. We will then
    take a step forward with the help of MXNet GluonCV Model Zoo and learn how to
    export our models to **ONNX**, which allows us to use our models in different
    frameworks, such as deploying our models on NVIDIA hardware platforms, such as
    the **NVIDIA Jetson** family of products.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探索我们的模型在操作方面的工作原理，了解它们如何在**MXNet分析器**的帮助下内部运行。然后，我们将借助MXNet GluonCV模型库，进一步学习如何将我们的模型导出为**ONNX**格式，使用该格式，我们可以将模型应用于不同的框架，例如将我们的模型部署到NVIDIA硬件平台上，如**NVIDIA
    Jetson**系列产品。
- en: Finally, we will apply all these techniques together, taking as examples problems
    already explored in the book. For our computer vision task, we will choose image
    segmentation, and for our natural language processing task, we will choose translating
    text from English to German.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结合应用所有这些技术，选择书中已经探讨过的问题作为例子。对于计算机视觉任务，我们将选择图像分割；对于自然语言处理任务，我们将选择将英文文本翻译成德文。
- en: 'Specifically, this chapter contains the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具体包含以下食谱：
- en: Introducing inference optimization features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍推理优化功能
- en: Optimizing inference for image segmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化图像分割的推理
- en: Optimizing inference when translating text from English to German
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化将英文文本翻译为德文时的推理
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    apply:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了《前言》中指定的技术要求外，以下内容适用：
- en: Ensure that you have completed *Recipe 1*, *Installing MXNet*, from [*Chapter
    1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running with MXNet*.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第1章*](B16591_01.xhtml#_idTextAnchor016)中的*食谱1*，*安装MXNet*。
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural Language Processing*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，以及[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*利用自然语言处理理解文本*。
- en: Ensure that you have completed [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    *Optimizing Models with Transfer Learning and Fine-Tuning*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第7章*](B16591_07.xhtml#_idTextAnchor148)，*通过迁移学习与微调优化模型*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub网址找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09)。
- en: 'Furthermore, you can access each recipe directly from Google Colab, for example,
    for the first recipe of this chapter: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从Google Colab访问每个配方，例如，本章第一个配方：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb)。
- en: Introducing inference optimization features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入推理优化功能
- en: In the previous chapters, we have seen how we can leverage MXNet, GluonCV, and
    GluonNLP to retrieve pre-trained models in certain datasets (such as ImageNet,
    MS COCO, or IWSLT2015) and use them for our specific tasks and datasets. Furthermore,
    we used transfer learning and fine-tuning techniques to improve the algorithmic
    performance of those tasks/datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到如何利用MXNet、GluonCV和GluonNLP从特定数据集（如ImageNet、MS COCO或IWSLT2015）中获取预训练模型，并将其应用于我们的特定任务和数据集。此外，我们还使用了迁移学习和微调技术来提高这些任务/数据集的算法性能。
- en: In this recipe, we will introduce (and revisit) several concepts and features
    that will optimize our inference loops to improve our runtime performance, and
    we will analyze the trade-offs involved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将介绍（并重温）几个概念和功能，这些内容将优化我们的推理循环，以提高运行时性能，同时分析其中的权衡。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但这完全不难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下步骤：
- en: Hybridizing our models
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合我们的模型
- en: Applying float16 and AMP for inference
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用float16和AMP进行推理
- en: Applying quantization by using INT8
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用INT8进行量化
- en: Profiling our models
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对我们的模型进行性能分析
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个步骤。
- en: Hybridizing our models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合我们的模型
- en: In the initial chapters where we were exploring the features of MXNet, we focused
    on **imperative programming**. If you have coded in the past with languages such
    as Java, C/C++, or Python, it is very likely you used imperative programming.
    It is the usual way of coding, as it is more flexible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的章节中，我们在探索MXNet的特性时，重点介绍了**命令式编程**。如果你以前用过Java、C/C++或Python等语言编程，那么你很可能使用过命令式编程。这是一种常见的编码方式，因为它更灵活。
- en: 'With imperative programming, a step-by-step sequential execution of the statements
    set in the code is expected. For example, typically in our evaluation paths, we
    run these statements step-by-step inside a loop:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令式编程中，通常期待代码中的语句按顺序逐步执行。例如，在我们的评估路径中，通常会逐步执行这些语句，通常是在一个循环内部：
- en: Load new samples from our data loader.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据加载器中加载新样本。
- en: Transform the input and expected output so that it can be consumed by our model
    and our metrics computations.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换输入和预期输出，以便它们可以被我们的模型和指标计算所使用。
- en: Pass the input through the model to compute the output.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入传递给模型以计算输出。
- en: Compare the model output with the expected output and update the corresponding
    metrics.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型输出与预期输出进行比较，并更新相应的指标。
- en: In this programming paradigm, each of the statements is executed in sequence,
    and the output can be checked or debugged for each step if we wait for its completion
    (as MXNet uses **lazy evaluation**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编程范式中，每个语句按顺序执行，输出可以在每一步完成后进行检查或调试（因为MXNet使用**惰性求值**）。
- en: With a different programming paradigm, called **symbolic programming**, symbols
    are used instead, which are basically abstractions for operations, and no actual
    computation happens until a defined point (typically known as the compile step).
    This is especially useful for **deep learning**, as all models can be defined
    as graphs, use this graph as a symbol, optimize the operation paths in the underlying
    graph, and only run the optimized computation when needed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的编程范式，称为**符号编程**，其中使用的是符号，符号本质上是操作的抽象，直到定义的某一点（通常称为编译步骤）才会进行实际计算。这对于**深度学习**尤其有用，因为所有模型都可以定义为图，使用这个图作为符号，优化底层图中的操作路径，并仅在需要时运行优化后的计算。
- en: However, as the computation hasn’t happened yet, the output for each step cannot
    be checked or debugged, making the finding and fixing of issues much more difficult.
    On the other hand, due to the capabilities of graph optimization, symbolic programming
    requires less memory and is faster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于计算尚未发生，因此每个步骤的输出无法检查或调试，这使得查找和修复问题变得更加困难。另一方面，由于图优化的能力，符号编程需要更少的内存且速度更快。
- en: Thankfully, with MXNet, we can leverage the best of both worlds. We can define
    our model with imperative programming, test it, debug it, and fix it with the
    usual mechanisms (*print* statements, tests, debugging, and so on). When we are
    ready for optimization, we just need to call the `hybridize` function, and it
    will take care of everything under the hood, working with our graph in symbolic
    programming. This approach is called hybrid programming and is one of the best
    advantages of MXNet. Moreover, there is no hardware limitation for this feature,
    and it can be used for both CPU and GPU computations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用MXNet，我们可以充分利用两者的优势。我们可以使用命令式编程定义模型，进行测试、调试和修复（通过*print*语句、测试、调试等机制）。当我们准备好进行优化时，我们只需要调用`hybridize`函数，它会处理底层的一切工作，与我们的图形在符号编程中一起工作。这种方法被称为混合编程，是MXNet的最佳优势之一。此外，这个特性没有硬件限制，可以用于CPU和GPU计算。
- en: 'As a toy example, we can run some experiments with the inference of a model
    and compare the different results for different configurations. Specifically,
    these are the configurations we will test:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个玩具示例，我们可以进行一些实验，通过推理模型并比较不同配置的不同结果。具体来说，这些是我们将测试的配置：
- en: 'CPU:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：
- en: With imperative execution
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令式执行
- en: With symbolic execution and default parameters
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和默认参数
- en: With symbolic execution with a specific backend
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和特定后端
- en: With symbolic execution, specific backend, and static allocation of memory
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、特定后端和静态内存分配
- en: With symbolic execution, specific backend, static allocation of memory, and
    invariant input shapes
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、特定后端、静态内存分配和不变输入形状
- en: 'GPU:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU：
- en: With imperative execution
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令式执行
- en: With symbolic execution and default parameters
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和默认参数
- en: With symbolic execution and static allocation of memory
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和静态内存分配
- en: With symbolic execution, static allocation of memory and invariant input shapes
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、静态内存分配和不变输入形状
- en: 'Please note that in order to verify the computation time properly, we are adding
    calls to the `mx.nd.waitall()` function. The method chosen is to use the **ADE20K**
    validation split (dataset available from MXNet GluonCV) and process it with a
    **DeepLabv3** model. We will be using a batch size of 4:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了正确验证计算时间，我们添加了对`mx.nd.waitall()`函数的调用。选择的方法是使用**ADE20K**验证集（数据集可通过MXNet
    GluonCV获得），并使用**DeepLabv3**模型进行处理。我们将使用批量大小为4：
- en: 'For the initial CPU computing configuration, with imperative execution, the
    processing of the dataset by the model took the following time:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于初始的CPU计算配置，使用命令式执行时，模型对数据集的处理时间如下：
- en: '[PRE0]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the second CPU computing configuration, we just need to leverage the MXNet
    hybrid programming model and transform our model with the following:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二种CPU计算配置，我们只需利用MXNet混合编程模型并使用以下方式转换我们的模型：
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, the optimizations performed reduced to almost half the computation
    time.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所看到的，所做的优化将计算时间减少了近一半。
- en: 'For the third CPU computing configuration, we just need to slightly modify
    our hybridization call to define a specific backend. We will leverage our Intel
    CPU architecture, use the `MKLDNN` backend, and transform our model with the following:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第三种CPU计算配置，我们只需稍微修改我们的混合化调用以定义特定的后端。我们将利用我们的Intel CPU架构，使用`MKLDNN`后端，并使用以下方式转换我们的模型：
- en: '[PRE2]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, the specific backend further reduced the computation time by
    ~20%.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，特定的后端进一步将计算时间减少了约20%。
- en: 'For the fourth CPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to use the static memory allocation.
    We can update our call with the following:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第四个CPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望使用静态内存分配。我们可以使用以下方式更新我们的调用：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, the static memory allocation allowed us to reduce the computation
    time by another ~4%.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，静态内存分配使我们能够将计算时间再减少约4%。
- en: 'For the fifth CPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to leverage our invariant input
    shapes (we have already preprocessed our data to have the same input shape, `480x480`).
    We can update our call with the following:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第五个CPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望利用不变的输入形状（我们已经预处理了数据，使其具有相同的输入形状，`480x480`）。我们可以使用以下方式更新我们的调用：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the invariant input shape constraint allowed us to reduce the
    computation time by another ~2%.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，不变输入形状约束使我们能够将计算时间再减少约2%。
- en: 'For the initial GPU computing configuration, with imperative execution, the
    processing of the dataset by the model took the following time:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于初始的GPU计算配置，采用命令式执行，模型处理数据集的时间如下：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the second GPU computing configuration, we just need to leverage the MXNet
    hybrid programming model and transform our model with the following:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二个GPU计算配置，我们只需要利用MXNet混合编程模型，并用以下方式转换我们的模型：
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, when the optimizations are performed in the GPU, they yield almost
    no improvement in the computation time as GPUs are already optimized internally
    for these types of computations.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，当在GPU上执行优化时，由于GPU已经针对这些类型的计算进行了内部优化，因此几乎没有提高计算时间。
- en: 'For GPU computing, there are no specific backends to be selected. Therefore,
    for the third GPU computing configuration, we just need to slightly modify our
    hybridization call to define that we want to use static memory allocation. We
    can update our call with the following:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于GPU计算，没有需要选择的特定后端。因此，对于第三个GPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望使用静态内存分配。我们可以使用以下方式更新我们的调用：
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, the static memory allocation produced another negligible improvement
    on the GPU.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，静态内存分配在GPU上带来了另一个微不足道的改进。
- en: 'For the fourth GPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to leverage our invariant input
    shapes (we already preprocessed our data to have the same input shape, 480x480).
    We can update our call with the following:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第四个GPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望利用不变的输入形状（我们已经预处理了数据，使其具有相同的输入形状，480x480）。我们可以使用以下方式更新我们的调用：
- en: '[PRE8]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, the invariant input shape constraint produced another negligible
    improvement on the GPU.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，不变输入形状约束在GPU上带来了另一个微不足道的改进。
- en: The results show that when using the CPU, we can reduce the inference time to
    half the original time, which is a significant improvement. With the GPU, the
    improvements are negligible due to the internal optimizations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，当使用CPU时，我们可以将推理时间减少到原始时间的一半，这是一个显著的改进。使用GPU时，由于内部优化，改进几乎可以忽略不计。
- en: Important Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note in the code how we used the `mx.nd.waitall()` function to verify
    that all computations have been strictly completed before computing the time these
    operations took.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在代码中我们是如何使用`mx.nd.waitall()`函数来验证所有计算是否已经严格完成，然后才计算这些操作所花费的时间。
- en: Applying float16 and AMP for inference
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用float16和AMP进行推理
- en: In the previous chapter, [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172), *Improving
    Training Performance with MXNet*, we introduced the `float16` data type and AMP
    optimization, an extremely simple way to leverage this half-precision data type
    only when it was most useful.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，[*第8章*](B16591_08.xhtml#_idTextAnchor172)，*使用MXNet提升训练性能*，我们介绍了`float16`数据类型和AMP优化，这是一种极为简单的方式，仅在最有用时才使用这一半精度数据类型。
- en: In *Recipe 1*, *Introducing training optimization features*, from the previous
    chapter, we compared single-precision (`float32`) and half-precision (`float16`)
    data types, understanding their characteristics and memory/speed trade-offs. You
    are encouraged to review the recipe if you haven’t already done so as it is very
    relevant to this topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*食谱1*，*介绍训练优化特性*，上一章中，我们比较了单精度（`float32`）和半精度（`float16`）数据类型，理解它们的特性和内存/速度折衷。如果你还没有复习这个食谱，建议你回顾一下，因为它与本主题非常相关。
- en: As most concepts were introduced previously, in this section, we will focus
    on how to apply AMP to the inference process. As usual, MXNet provides a very
    simple interface for this operation, just requiring a call to the `amp.convert_hybrid_block()`
    function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如大多数概念之前已介绍的那样，本节将重点讨论如何将AMP应用于推理过程。像往常一样，MXNet为此操作提供了一个非常简单的接口，只需调用`amp.convert_hybrid_block()`函数即可。
- en: This optimization can be applied to both CPU and GPU environments, so let’s
    run these experiments.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化可以应用于CPU和GPU环境，因此让我们来运行这些实验。
- en: 'To modify our CPU model to use AMP, we just need the following line of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改我们的CPU模型以使用AMP，我们只需要以下一行代码：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With this modified model, the processing of the dataset took the following
    time:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的模型，处理数据集的时间如下：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, AMP produced negligible improvements on the CPU. This is due
    to the largest gains being achieved on the backward pass required for training,
    but not necessary during inference. Furthermore, CPUs do not typically have specific
    circuitry to work directly with float16, limiting the improvements.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AMP在CPU上几乎没有产生改善。这是因为最大的收益出现在训练时所需的反向传递过程中，但在推理过程中并不需要。此外，CPU通常没有专门的电路来直接处理float16，这限制了改进效果。
- en: 'To modify the GPU model to use AMP, we just need the following line of code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改GPU模型以使用AMP，我们只需要以下一行代码：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this modified model, the processing of the dataset took the following
    time:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的模型，处理数据集的时间如下：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, AMP produced excellent results on the GPU, reducing the inference
    time to almost ~25%. This is due to GPUs having specific circuitry to work directly
    with float16, improving the results massively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AMP在GPU上产生了优秀的结果，将推理时间减少了大约~25%。这是因为GPU具有专门的电路来直接处理float16，极大地改善了结果。
- en: Important Note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The `amp.convert_hybrid_block()` function accepts different parameters. You
    are encouraged to try different options (such as `cast_optional_params`) to find
    the optimal configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`amp.convert_hybrid_block()`函数接受不同的参数。鼓励你尝试不同的选项（如`cast_optional_params`）以找到最佳配置。'
- en: Applying quantization by using Int8
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Int8进行量化
- en: In the previous sections, we saw how to optimize our inference loops by using
    different approaches optimizing how to use the CPU and GPU for maximum performance,
    given a model. We also explored how to leverage single-precision (float32) and
    half-precision (float16) data types. In this section, we will explore how our
    data inputs, our model parameters, and the different arithmetic calculations among
    them can be optimized with a new data type, Int8.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到如何通过使用不同的方法来优化推理循环，优化如何使用CPU和GPU以获得最大性能，给定一个模型。我们还探讨了如何利用单精度（float32）和半精度（float16）数据类型。在本节中，我们将探讨如何通过一种新的数据类型Int8来优化我们的数据输入、模型参数以及它们之间的不同算术运算。
- en: This data type modification has larger implications than a change in precision.
    We are also modifying the underlying representation from a floating-point number
    to an integer, which yields a reduction in both memory and computing requirements.
    Let’s analyze this data type.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据类型的修改比精度变化更有深远的影响。我们还将底层表示从浮点数修改为整数，这样可以减少内存和计算要求。让我们分析一下这种数据类型。
- en: 'Int8 indicates two things: that it is a data type that only supports integer
    numbers (no floating radix point), and that the amount of bits used to store a
    single number in this format is 8 bits. The most important features of this format
    are the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Int8表示两件事：它是一种仅支持整数数字（没有浮动小数点）的数据类型，并且在这种格式下存储单个数字所用的位数是8位。此格式的最重要特性如下：
- en: Capability of representing integer numbers from -128 to 127, or from 0 to 255
    (depending on whether it is of the signed or unsigned type)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能表示从-128到127，或从0到255的整数（取决于它是有符号还是无符号类型）
- en: Constant precision (each consecutive number differs by exactly 1)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常数精度（每个连续的数字相差恰好1）
- en: 'To explain the core idea behind `Int8` quantization, and also to show the loss
    of precision, we can display the approximated value of the number 1/3 (one third-th)
    in both formats (Float32 and Int8) with this code excerpt:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释`Int8`量化的核心思想，并展示精度损失，我们可以用以下代码片段显示数字1/3（即三分之一）在`Float32`和`Int8`两种格式下的近似值：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This yields the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see, none of the representations are exact, with `float32` yielding
    a very high precision as expected. With `Int8`, we did a small shortcut; we used
    two 8-bit integers, `85` and `255`, and used one as the scaling factor. This scaling
    factor is typically applied for several sets of numbers at the same time. It can
    be the same scaling factor for the whole model (unlikely), per layer, and so on.
    The scaling factor does not need to be represented in `Int8`; it can be a `float32`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有表示方式都不是完全精确的，`float32`表现出了非常高的精度，符合预期。使用`Int8`时，我们做了一个小的简化；我们使用了两个8位整数，`85`和`255`，并用其中一个作为缩放因子。这个缩放因子通常会同时应用于多个数字集。它可以是整个模型的相同缩放因子（不太可能），也可以是每层的缩放因子，等等。这个缩放因子不需要以`Int8`表示，它可以是`float32`。
- en: Important Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For this particular example, the chosen Int8 representation is more exact to
    the intended number, but this is a coincidence. In common scenarios, there is
    a loss of precision that translates to a loss of performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的例子，选择的`Int8`表示比目标数值更精确，但这只是巧合。在常见的场景中，存在精度损失，进而导致性能损失。
- en: To minimize this loss of performance, typically quantization tuning techniques
    ask for a **calibration dataset**. This dataset is then used to compute the parameters
    that minimize the mentioned performance loss.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化性能损失，通常量化调优技术会要求一个**校准数据集**。然后，使用该数据集来计算减少性能损失的参数。
- en: In addition to using a calibration dataset, there are several techniques to
    optimize the computation of the most accurate Int8 values, and MXNet provides
    a very simple API to facilitate the optimization of our networks. With a simple
    call to the `mx.contrib.quantization.quantize_net_v2()` function, we will update
    our network to Int8.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用校准数据集外，还有一些技术可以优化最准确的`Int8`值的计算，而MXNet提供了一个非常简单的API来促进我们网络的优化。通过简单调用`mx.contrib.quantization.quantize_net_v2()`函数，我们将更新我们的网络为`Int8`。
- en: 'Specifically, for our experiments, this is the call that we used:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，对于我们的实验，这就是我们使用的调用：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '`Int8` quantization is a nuanced process, and tailoring it for a specific application
    requires in-depth analysis and some trial-and-error experiments. For more information
    regarding the parameters involved, you are encouraged to read the function documentation:
    [https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825](https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Int8`量化是一个复杂的过程，针对特定应用的定制需要深入的分析和一些反复试验。关于涉及的参数，建议阅读以下函数文档：[https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825](https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825)。'
- en: 'With this modified CPU-based model, the processing of the dataset took the
    following time:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的基于CPU的模型，处理数据集所需的时间如下：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, `Int8` produced a strong improvement in the CPU, yielding almost
    another ~50% reduction in runtime.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`Int8`在CPU上产生了显著的提升，几乎减少了约50%的运行时间。
- en: Unfortunately, for GPUs, this feature cannot be introduced. Although recent
    GPUs have dedicated `Int8` circuitry, this is quite a new development and MXNet
    does not support these operators yet.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于GPU，这个特性无法引入。尽管最近的GPU有专门的`Int8`电路，但这还是一个比较新的发展，MXNet尚不支持这些操作符。
- en: Profiling our models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对我们的模型进行分析
- en: 'In this recipe, we have seen how to use different techniques to optimize our
    inference loops. However, sometimes, even after introducing these optimization
    techniques, our models might not reach the runtime performance we are targeting.
    This could be due to a number of reasons:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们已经看到如何使用不同的技术来优化推理循环。然而，有时即使引入了这些优化技术，我们的模型仍可能无法达到我们预期的运行时性能。这可能是由于以下几个原因：
- en: Architecture is not optimal for edge computing.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构并不适合边缘计算。
- en: Operators have not been optimized adequately.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作符尚未得到充分优化。
- en: Data transfers among components.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件之间的数据传输。
- en: Memory leaks.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存泄漏。
- en: In order to verify how our model is working internally, to check where to optimize
    further and/or investigate the possible reasons why our models might not be performing
    well, MXNet provides us with a tool for low-level analysis called the MXNet profiler.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的模型内部是如何工作的，检查需要进一步优化的地方和/或调查模型性能不佳的可能原因，MXNet 提供了一种低级分析工具，称为 MXNet 性能分析器。
- en: The MXNet profiler runs in the background and records all operations and data
    transfers happening on our models in real time. It is also very lightweight, consuming
    a minimal amount of resources. Best of all, it is extremely easy to configure
    and use.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 性能分析器在后台运行，实时记录模型中发生的所有操作和数据传输。它也非常轻量，占用的资源非常少。最重要的是，它极易配置和使用。
- en: 'In order to profile a set of statements, we need to take two steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析一组语句，我们需要采取两个步骤：
- en: Configure the profiler.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置性能分析器。
- en: Start and stop the profiler before and after the statements to be profiled.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在要进行性能分析的语句之前和之后启动与停止性能分析器。
- en: 'To configure the profiler, we just need one line of code, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置性能分析器，我们只需要一行代码，如下所示：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To start and stop the profiler, we need to add the following lines at the beginning
    and end of the statements to be analyzed:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动和停止性能分析器，我们需要在要分析的语句的开始和结束处添加以下几行代码：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Please note how we need three statements to stop recording: finalize all instructions,
    stop recording, and dump the information of the file configured in the first step.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要三条语句来停止录制：完成所有指令、停止录制，并转储在第一步中配置的文件的信息。
- en: Important Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Please note in the code how we used the `mx.nd.waitall()` function to verify
    that all computations have been strictly completed before computing the time these
    operations took.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意代码中我们如何使用 `mx.nd.waitall()` 函数来验证所有计算已严格完成，然后再计算这些操作所花费的时间。
- en: 'The instructions described previously generate a JSON file, which can then
    be analyzed with tracing applications. I recommend the Tracing app included with
    Google Chrome as it is very easy to use and access. Simply type the following
    in the address bar: `chrome://tracing`.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的指令会生成一个 JSON 文件，随后可以通过追踪应用程序进行分析。我推荐使用 Google Chrome 中包含的 Tracing 应用程序，因为它非常易于使用和访问。只需在地址栏中输入以下内容：`chrome://tracing`。
- en: In order to verify the functionality of the MXNet profiler, let’s take the example
    of **ResNet** architectures, which are used extensively, such as in our image
    segmentation task, being the backbone of the DeepLabv3 network we use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 MXNet 性能分析器的功能，我们以广泛使用的**ResNet**架构为例，这些架构在我们的图像分割任务中被广泛应用，作为我们所使用的 DeepLabv3
    网络的骨干。
- en: 'The typical architecture of a ResNet network is the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 网络的典型架构如下：
- en: '![Figure 9.1 – ResNet50 model architecture](img/B16591_09_1.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – ResNet50 模型架构](img/B16591_09_1.jpg)'
- en: Figure 9.1 – ResNet50 model architecture
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – ResNet50 模型架构
- en: Please note that in *Figure 9.1*, the initial steps (stage 1) are convolution,
    batch normalization, and activation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图 9.1*中，初始步骤（阶段 1）是卷积、批量归一化和激活。
- en: 'From our profiled model, the Google Chrome Tracing app provides the following
    screen:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们分析过的模型中，Google Chrome Tracing 应用程序提供了以下屏幕：
- en: '![Figure 9.2 – Profiling ResNet](img/B16591_09_2.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 性能分析 ResNet](img/B16591_09_2.jpg)'
- en: Figure 9.2 – Profiling ResNet
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 性能分析 ResNet
- en: 'In *Figure 9.2*, we can see the general execution of the model. Zooming on
    the earlier layers, we can see it as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.2*中，我们可以看到模型的一般执行情况。放大早期层，我们可以看到如下：
- en: '![Figure 9.3 – Profiling ResNet: Zoom](img/B16591_09_3.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 性能分析 ResNet: 放大](img/B16591_09_3.jpg)'
- en: 'Figure 9.3 – Profiling ResNet: Zoom'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.3 – 性能分析 ResNet: 放大'
- en: In *Figure 9.3*, we can see how the stage 1 steps of convolution, batch normalization,
    and activation are clearly displayed. We can also now very clearly see how the
    batch normalization operation takes about 4x longer than the convolution and activation
    steps, potentially indicating an avenue of improvement.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.3*中，我们可以看到阶段 1 的卷积、批量归一化和激活步骤被清晰地展示出来。我们还可以非常清楚地看到，批量归一化操作所需的时间大约是卷积和激活步骤的
    4 倍，这可能表明了一个优化的方向。
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理……
- en: 'In this recipe, we have taken a deeper look into how MXNet and Gluon can help
    us optimize our inference loops. We have leveraged our hardware (CPUs and GPUs)
    by addressing each of the steps in the inference loop:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了 MXNet 和 Gluon 如何帮助我们优化推理循环。我们通过解决推理循环中的每一个步骤，充分利用了我们的硬件（CPU 和 GPU）：
- en: Reused the work done for data loading in the previous chapter
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重用了上一章中数据加载的工作
- en: Optimized graph computation via hybridization
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过混合化优化图计算
- en: Analyzed different data types and combined the accuracy and precision of float32
    with the speed-ups of `float16` (leveraging the specific circuitry of GPUs) where
    possible, using AMP
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析了不同的数据类型，并在可能的情况下，使用AMP结合`float16`的加速与`float32`的精度，利用GPU的特定电路
- en: Taken another step forward by using Int8 quantization
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用Int8量化迈出了进一步的步伐
- en: Analyzed low-level performance using the MXNet profiler
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MXNet分析器分析了低级性能
- en: 'We compared each of these features by running several experiments and comparing
    the performance before and after a specific optimization, emphasizing potential
    trade-offs that have to be taken into account when using these optimizations.
    To summarize, these were the results:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行多个实验，比较了每个特性的效果，比较了在特定优化前后的性能，强调了使用这些优化时需要考虑的潜在权衡。总结来说，结果如下：
- en: '| **Feature** | **Result on** **CPU (ms)** | **Result on** **GPU (ms)** |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **CPU上的结果（毫秒）** | **GPU上的结果（毫秒）** |'
- en: '| Standard | 115 | 13.3 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 115 | 13.3 |'
- en: '| Hybridize / Default | 65 | 12.9 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / 默认 | 65 | 12.9 |'
- en: '| Hybridize / MKLDNN | 56 | N/A |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN | 56 | N/A |'
- en: '| Hybridize / MKLDNN + Static Alloc | 54 | 12.8 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN + 静态分配 | 54 | 12.8 |'
- en: '| Hybridize / MKLDNN + Static Alloc + Invariant Shape | 52 | 12.6 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN + 静态分配 + 不变形状 | 52 | 12.6 |'
- en: '| AMP | 54 | 3.5 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AMP | 54 | 3.5 |'
- en: '| Int8 Quantization | 36 | N/A |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Int8量化 | 36 | N/A |'
- en: Table 9.1 – Summary of features and results for the CPU and GPU
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 – CPU和GPU的特性及结果汇总
- en: 'In the next recipes, we will apply all these optimization techniques concurrently
    for the best cases for the CPU (MKL-DNN + static allocation + invariant shape
    + Int8 quantization) and GPU (static allocation + invariant shape + automatic
    mixed precision) to optimize two familiar tasks: image segmentation and text translation.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的食谱中，我们将同时应用所有这些优化技术，针对CPU（MKL-DNN + 静态分配 + 不变形状 + Int8量化）和GPU（静态分配 + 不变形状
    + 自动混合精度）优化两项常见任务：图像分割和文本翻译。
- en: There’s more…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'All the optimization features shown in this recipe have been thoroughly described
    in the literature. In this section, we share some introductory links to start
    understanding each of the features in depth:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中展示的所有优化特性都在文献中进行了详细描述。在本节中，我们分享了一些入门链接，以便深入理解每个特性：
- en: '**Hybridization**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合化**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**Automatic Mixed Precision (AMP)**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动混合精度（AMP）**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**Int8 quantization**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Int8量化**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**MXNet profiler**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MXNet分析器**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: Optimizing inference for image segmentation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化图像分割的推理
- en: In the previous recipe, we saw how we can leverage MXNet and Gluon to optimize
    the inference of our models, applying different techniques, such as improving
    the runtime performance using hybridization; how using half-precision (float16)
    in combination with AMP can strongly reduce our inference times; and how to take
    advantage of further optimizations with data types such as Int8 quantization.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们展示了如何利用 MXNet 和 Gluon 来优化模型推理，应用了不同的技术，例如使用混合化提高运行时性能；如何结合 AMP 使用半精度（float16）显著减少推理时间；以及如何利用
    Int8 量化等数据类型进一步优化。
- en: Now, we can revisit a problem we have been working with throughout the book,
    image segmentation. We have worked with this task in recipes from previous chapters.
    In *Recipe 4*, *Segmenting objects semantically with MXNet Model Zoo – PSPNet
    and DeepLabv3*, from [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*, we introduced the task and the datasets that we
    will be using in this recipe, *MS COCO and Penn-Fudan Pedestrian*, and learned
    how to use pre-trained models from GluonCV Model Zoo.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重新审视本书中一直在处理的一个问题：图像分割。我们在前几章的食谱中已经处理过这个任务。在*食谱 4*，《使用 MXNet Model Zoo
    进行语义图像分割—PSPNet 和 DeepLabv3》中，来自[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)《使用计算机视觉分析图像》，我们介绍了这个任务以及我们将在本食谱中使用的数据集，*MS
    COCO 和 Penn-Fudan Pedestrian*，并学习了如何使用来自 GluonCV Model Zoo 的预训练模型。
- en: Furthermore, in *Recipe 3*, *Improving performance for segmenting images*, from
    [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), *Optimizing Models with Transfer
    Learning and Fine-Tuning*, we compared the different approaches that we could
    take when dealing with a target dataset, training our models from scratch or leveraging
    past knowledge from pre-trained models and adjust them for our task, using the
    different modalities of transfer learning and fine-tuning. Lastly, in *Recipe
    2*, *Optimizing training for image segmentation*, from [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172),
    *Improving Training Performance with MXNet*, we applied different techniques to
    improve the runtime performance of our training loops.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在*食谱 3*，《提升图像分割性能》中，来自[*第 7 章*](B16591_07.xhtml#_idTextAnchor148)《通过迁移学习和微调优化模型》中，我们比较了处理目标数据集时可以采取的不同方法，是否从头开始训练我们的模型，或者利用预训练模型的先验知识并针对我们的任务进行调整，使用不同的迁移学习和微调方式。最后，在*食谱
    2*，《优化图像分割训练》中，来自[*第 8 章*](B16591_08.xhtml#_idTextAnchor172)《通过 MXNet 提升训练性能》中，我们应用了不同的技术来提升训练循环的运行时性能。
- en: Therefore, in this recipe, we will apply all the introduced optimization techniques
    for the specific task of optimizing the inference of an image segmentation model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本食谱中，我们将应用所有介绍的优化技术，专注于优化图像分割模型的推理任务。
- en: Getting ready
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的章节一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但这绝对不难。
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be using the following steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用以下步骤：
- en: Applying inference optimization techniques
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: Visualizing and profiling our models
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化和分析我们的模型
- en: Exporting our models to ONNX and TensorRT
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的模型导出到 ONNX 和 TensorRT
- en: Let’s dive into each of these steps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨每个步骤。
- en: Applying inference optimization techniques
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: In *Recipe 1*, *Introducing inference optimization features*, at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take in the inference of a machine learning
    model, including hybridization, AMP, and Int8 quantization.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在*食谱 1*，《介绍推理优化功能》中，我们展示了不同的优化技术如何提高推理过程中各个步骤的性能，包括混合化、AMP 和 Int8 量化。
- en: In this section, we will show how, with MXNet and Gluon, just with a few lines
    of code, we can easily apply each and every technique we've introduced and verify
    the results of each technique.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示如何仅通过几行代码，在 MXNet 和 Gluon 中轻松应用我们介绍的每一项技术，并验证每项技术的结果。
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the CPU:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不应用这些优化技术，作为基准，以下是使用 CPU 获取的定量结果：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can display an image for qualitative results:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示图像以获取定性结果：
- en: '![Figure 9.4 – Qualitative results: CPU baseline](img/B16591_09_4.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 定性结果：CPU 基准](img/B16591_09_4.jpg)'
- en: 'Figure 9.4 – Qualitative results: CPU baseline'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 定性结果：CPU基线
- en: As expected from the quantitative metrics, *Figure 9.4* shows excellent results
    as well.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量指标可以预见，*图9.4*也显示了出色的结果。
- en: 'As concluded in the previous recipe, for maximum performance on the CPU, the
    best approach is the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的食谱所总结的，对于CPU上的最大性能，最佳方法是：
- en: 'Use hybridization: Using the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合化：使用Intel MKL-DNN后端，结合静态内存分配和不变输入形状。
- en: Do not use AMP.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要使用AMP。
- en: Use Int8 quantization.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Int8量化。
- en: Let’s apply each of these techniques for our current specific task, image segmentation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些技术应用于我们当前的特定任务——图像分割。
- en: 'For hybridization, we just need one line of code (which includes the necessary
    parameters for the Intel `MKLDNN` backend, combined with static memory allocation
    and invariant input shapes):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合化，我们只需要一行代码（其中包括Intel `MKLDNN`后端所需的参数，结合静态内存分配和不变输入形状）：
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We do not need to add an AMP step as it was shown not to add benefits to CPU-based
    workloads.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要添加AMP步骤，因为它在CPU工作负载中未能提供任何好处。
- en: 'For `Int8` quantization, we need two separate steps. On one hand, we need to
    define the calibration dataset. This can be achieved with a small number of lines:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`Int8`量化，我们需要两个独立的步骤。一方面，我们需要定义校准数据集。这可以通过少量代码实现：
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, to apply `Int8` quantization, optimized using the calibration dataset,
    just another line of code is required:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了应用`Int8`量化，并使用校准数据集进行优化，只需再加一行代码：
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Applying these optimization techniques, these are the quantitative results
    obtained for an optimized CPU inference:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些优化技术，得到的优化CPU推理定量结果如下：
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see, the differences in performance (`0.959` versus `0.960` and `0.473`
    versus `0.474`) are negligible. However, with these inference optimization techniques,
    we have been able to reduce the inference runtime by 4x (8.4 seconds versus 27.6
    seconds), which is an impressive result.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，性能差异（`0.959`对比`0.960`和`0.473`对比`0.474`）可以忽略不计。然而，通过这些推理优化技术，我们已经将推理运行时减少了4倍（8.4秒对比27.6秒），这是一个令人印象深刻的结果。
- en: 'We can also display an image for qualitative results:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以显示一张图像来展示定性结果：
- en: '![Figure 9.5 – Qualitative results: CPU-optimized inference](img/B16591_09_5.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 定性结果：CPU优化推理](img/B16591_09_5.jpg)'
- en: 'Figure 9.5 – Qualitative results: CPU-optimized inference'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 定性结果：CPU优化推理
- en: As expected from the quantitative metrics, *Figure 9.5* shows excellent results
    as well, with negligible differences (if any).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量指标可以预见，*图9.5*也显示了出色的结果，差异几乎可以忽略不计（如果有的话）。
- en: 'What about GPU-based inference? Let’s follow the same steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，基于GPU的推理怎么样呢？让我们按照相同的步骤进行：
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the GPU:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不应用这些优化技术，作为基线，这些是使用GPU获得的定量结果：
- en: '[PRE24]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As expected, there is no change relative to algorithmic performance from the
    CPU baseline. The runtime inference is indeed twice as fast in the GPU (`13.1`
    seconds versus `27.6` seconds).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，与CPU基线相比，算法性能没有变化。GPU的推理运行时确实是CPU的两倍快（`13.1`秒对比`27.6`秒）。
- en: 'We can display an image for qualitative results:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以显示一张图像来展示定性结果：
- en: '![Figure 9.6 – Qualitative results: GPU baseline](img/B16591_09_6.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 定性结果：GPU基线](img/B16591_09_6.jpg)'
- en: 'Figure 9.6 – Qualitative results: GPU baseline'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 定性结果：GPU基线
- en: As expected from the quantitative metrics, *Figure 9.6* shows excellent results
    as well.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从定量指标可以预见，*图9.6*也显示了出色的结果。
- en: 'As concluded in the previous recipe, for maximum performance on the GPU, the
    best approach is the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的食谱所总结的，对于GPU上的最大性能，最佳方法是：
- en: '**Use hybridization**: Using static memory allocation and invariant input shapes.
    Do not use the Intel MKL-DNN backend.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用混合化**：使用静态内存分配和不变输入形状。不要使用Intel MKL-DNN后端。'
- en: Use AMP**.**
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AMP**。**
- en: Do not use Int8 quantization (not supported).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要使用Int8量化（不支持）。
- en: Let’s apply each of these techniques to our current specific task, image segmentation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些技术应用于我们当前的特定任务——图像分割。
- en: 'For hybridization, we just need one line of code (which includes the necessary
    parameters for static memory allocation and invariant input shapes):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合化，我们只需要一行代码（其中包括静态内存分配和不变输入形状所需的参数）：
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For AMP, we need to follow two simple steps, a forward pass and the conversion
    of the model, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AMP，我们需要遵循两个简单的步骤，即前向传播和模型的转换，如下所示：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: No further steps are required.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要进一步的步骤。
- en: 'By applying these optimization techniques, these are the quantitative results
    obtained for an optimized GPU inference:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这些优化技术，我们获得了优化GPU推断的量化结果：
- en: '[PRE27]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see, the differences in performance (`0.960` versus `0.960` and `0.474`
    versus `0.474`) are non-existent. Furthermore, with these inference optimization
    techniques, we have been able to reduce the inference runtime by 15x (0.85 seconds
    versus 13.1 seconds), which is an impressive result.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，性能上的差异（`0.960`与`0.960`，以及`0.474`与`0.474`）是不存在的。此外，通过这些推断优化技术，我们已经成功将推断运行时间缩短了15倍（0.85秒与13.1秒），这是一个令人印象深刻的成果。
- en: 'We can also display an image for qualitative results:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示一张图片来展示定性结果：
- en: '![Figure 9.7 – Qualitative results: GPU-optimized inference](img/B16591_09_7.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 定性结果：GPU优化推断](img/B16591_09_7.jpg)'
- en: 'Figure 9.7 – Qualitative results: GPU-optimized inference'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 定性结果：GPU优化推断
- en: As expected from the quantitative metrics, *Figure 9.7* shows excellent results
    as well, with negligible differences (if any) from the results in *Figure 9.6*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如定量指标所预期的那样，*图9.7*也显示了出色的结果，与*图9.6*中的结果几乎没有（如果有的话）可忽略的差异。
- en: Visualizing and profiling our models
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化和剖析我们的模型
- en: In the previous sections, we saw the different techniques that we could apply
    to optimize our inference loops, and the results these techniques achieved. However,
    how exactly do these techniques work? Why are they faster?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们看到了可以应用于优化推断循环的不同技术，以及这些技术所取得的结果。但是，这些技术究竟是如何工作的？为什么它们更快？
- en: 'We are going to use two tools that MXNet provides for exactly this purpose:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MXNet提供的两个工具来达到这个目的：
- en: '**Model visualization**'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可视化**'
- en: '**Model profiling**'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型剖析**'
- en: Model visualization provides us with an intuitive way to see how the different
    layers interact with each other. This is particularly interesting for networks
    that use ResNet backbones (such as `DeepLabv3`, which we use for image segmentation
    in this recipe) because of the **residuals** being transferred through layers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可视化为我们提供了一种直观的方式来看待不同层之间的交互。对于使用ResNet骨干（例如我们在本文档中用于图像分割的`DeepLabv3`），这尤为重要，因为**残差**通过层进行传递。
- en: 'Visualizing our model architecture with MXNet is very easy. When working with
    symbolic models, just one line of code is necessary. In our case, as we work with
    Gluon models, these are the lines of code necessary:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 用MXNet可视化我们的模型架构非常简单。当使用符号模型时，只需一行代码即可。在我们的情况下，由于我们使用Gluon模型，需要以下几行代码：
- en: '[PRE28]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As shown in the previous recipe, ResNet-based networks are composed of ResNet
    blocks, which include the convolution, batch normalization, and activation steps.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一篇文章所示，基于ResNet的网络由ResNet块组成，其中包括卷积、批量归一化和激活步骤。
- en: 'For our CPU-optimized model (hybridized and `Int8`-quantized), this is what
    some of the connections among those blocks look like:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的CPU优化模型（混合化和`Int8`量化），以下是这些块之间部分连接的样子：
- en: '![Figure 9.8 – GraphViz of ResNet blocks (CPU-optimized)](img/B16591_09_8.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – ResNet块的GraphViz（CPU优化）](img/B16591_09_8.jpg)'
- en: Figure 9.8 – GraphViz of ResNet blocks (CPU-optimized)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – ResNet块的GraphViz（CPU优化）
- en: As we can see in *Figure 9.8*, there are no individual blocks for each of the
    expected ResNet block operations; they are all part of single blocks that perform
    all computations. This combination of operations is aptly called operator fusion,
    where as many operations as possible are fused together, instead of computing
    an operation and then the next one (with the typical data transfers occurring).
    The largest benefit is that fused operations can happen in the same memory space.
    This is one of the optimizations performed by hybridization, as once the graph
    for the network is finished, it is quite straightforward to find the operations
    that are candidates to be fused.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图9.8*中所看到的，预期的ResNet块操作没有单独的块；它们都是执行所有计算的单个块的一部分。这些操作的组合称为运算符融合，其中尽可能多的操作被融合在一起，而不是计算一个操作，然后是下一个操作（通常发生数据传输）。最大的好处在于融合的操作可以在相同的内存空间中进行。这是混合化执行的优化之一，因为一旦网络的图形完成，很容易找到候选融合操作。
- en: OK, so the model visualization tells us those optimizations will happen, but
    how can we verify they are actually happening? This is what model profiling is
    good at, and can also help us understand issues happening during runtime.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，模型可视化告诉我们这些优化会发生，但我们如何验证它们实际上正在发生呢？这正是模型性能分析擅长的地方，也可以帮助我们了解运行时发生的问题。
- en: 'As mentioned in the recipe, *Introducing inference optimization features*,
    and the section Profiling our models, the output of model profiling is a JSON
    file that can be visualized with tools such as the Google Chrome Tracing app.
    For a non-optimized CPU workload, our `DeepLabv3` model shows the following timing
    profile:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如食谱中所述，*引入推理优化功能*，以及“对我们模型进行性能分析”一节中提到，模型性能分析的输出是一个 JSON 文件，可以使用 Google Chrome
    Tracing 应用等工具进行可视化。对于一个未优化的 CPU 工作负载，我们的 `DeepLabv3` 模型显示了以下的时间配置文件：
- en: '![Figure 9.9 – Profiling DeepLabv3: Non-optimized CPU workload](img/B16591_09_9.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – DeepLabv3 性能分析：未优化的 CPU 工作负载](img/B16591_09_9.jpg)'
- en: 'Figure 9.9 – Profiling DeepLabv3: Non-optimized CPU workload'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – DeepLabv3 性能分析：未优化的 CPU 工作负载
- en: 'In *Figure 9.9*, we can see the following characteristics:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 9.9* 中，我们可以看到以下特征：
- en: Almost all of the tasks are handled by a single process.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有任务都由单个进程处理。
- en: Around 80 ms into the operation, all tasks have been sent to be dispatched,
    and the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在操作开始约 80 毫秒后，所有任务已被派发，并且控制已返回，操作继续进行（延迟评估和 `mx.nd.waitall`）。
- en: All tasks have been sent to be dispatched around 80ms into the operation, and
    the control is returned for operations to continue (lazy evaluation and mx.nd.waitall).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有任务在操作开始约 80 毫秒时已被派发，并且控制已返回，操作继续进行（延迟评估和 mx.nd.waitall）。
- en: All operations are atomic and executed individually.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都是原子操作，并逐个执行。
- en: The full operation takes around 800 ms.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的操作大约需要 800 毫秒。
- en: 'For a CPU-optimized workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个 CPU 优化的工作负载，我们的 DeepLabv3 模型显示了以下的时间配置文件：
- en: '![Figure 9.10 – Profiling DeepLabv3: Optimized CPU workload](img/B16591_09_10.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 – DeepLabv3 性能分析：优化后的 CPU 工作负载](img/B16591_09_10.jpg)'
- en: 'Figure 9.10 – Profiling DeepLabv3: Optimized CPU workload'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – DeepLabv3 性能分析：优化后的 CPU 工作负载
- en: 'In *Figure 9.10*, we can see the following characteristics:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 9.10* 中，我们可以看到以下特征：
- en: Almost all of the tasks are handled by a single process, similar to the non-optimized
    counterpart.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有任务都由单个进程处理，类似于未优化的版本。
- en: Around 5 ms into the operation, all tasks have been sent to be dispatched, and
    the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`),
    much faster than the non-optimized counterpart.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在操作开始约 5 毫秒后，所有任务已被派发，并且控制已返回，操作继续进行（延迟评估和 `mx.nd.waitall`），比未优化版本要快得多。
- en: Memory is used in a synchronous/structured way, in stark contrast to the non-optimized
    counterpart.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存的使用是同步/结构化的，这与未优化的版本形成鲜明对比。
- en: All operations are fused together, again in stark contrast to the non-optimized
    counterpart.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都被融合在一起，这与未优化的版本形成鲜明对比。
- en: The full operation takes around 370 ms.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的操作大约需要 370 毫秒。
- en: 'In summary, for CPU-based optimizations, we can clearly see the effects:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于基于 CPU 的优化，我们可以清晰地看到效果：
- en: Hybridization has fused all operators together, basically executing almost the
    full workload in a single operation.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合化将所有操作符融合在一起，基本上在一个操作中执行几乎全部工作负载。
- en: The MKL-DNN backend and Int8 quantization have improved those operations with
    accelerated operators.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MKL-DNN 后端和 Int8 量化通过加速操作改善了这些操作。
- en: 'For our GPU-optimized model (hybridized and AMP-ed), this is what some of the
    connections among ResNet blocks look like:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 GPU 优化模型（混合化并使用 AMP），以下是一些 ResNet 模块之间连接的情况：
- en: '![Figure 9.11 – GraphViz of ResNet blocks (GPU-optimized)](img/B16591_09_11.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 – ResNet 模块的 GraphViz（GPU 优化）](img/B16591_09_11.jpg)'
- en: Figure 9.11 – GraphViz of ResNet blocks (GPU-optimized)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 – ResNet 模块的 GraphViz（GPU 优化）
- en: As we can see in *Figure 9.11*, this is a very different visualization than
    the CPU-optimized one as all the individual blocks of the expected ResNet block
    operations can be identified. As we saw in the first recipe of this chapter, hybridization
    had a very limited effect on GPUs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 9.11* 所示，这与 CPU 优化后的可视化完全不同，因为所有预期的 ResNet 模块操作的独立块都可以被识别出来。正如本章第一个食谱中所提到的，混合化对
    GPU 的影响非常有限。
- en: So, where do the accelerations come from? Let’s get some help from model profiling.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，性能加速是从哪里来的呢？让我们借助模型性能分析来解答。
- en: 'For a non-optimized GPU workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未优化的GPU工作负载，我们的DeepLabv3模型显示出以下时间轮廓：
- en: '![Figure 9.12 – Profiling DeepLabv3: Non-optimized GPU workload](img/B16591_09_12.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![图9.12 – 深度分析DeepLabv3：未优化的GPU工作负载](img/B16591_09_12.jpg)'
- en: 'Figure 9.12 – Profiling DeepLabv3: Non-optimized GPU workload'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 深度分析DeepLabv3：未优化的GPU工作负载
- en: 'In *Figure 9.12*, we can see the following characteristics:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.12*中，我们可以看到以下特点：
- en: Almost all of the tasks are handled by two GPU processes.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有任务都由两个GPU进程处理。
- en: Around 40 ms into the operation, all tasks have been sent to be dispatched,
    and the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在操作开始约40毫秒时，所有任务已被发送进行调度，控制返回以继续操作（懒评估和`mx.nd.waitall`）。
- en: Asynchronous/unstructured usage of memory.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存的异步/非结构化使用。
- en: All operations are atomic and executed individually.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都是原子性的并单独执行。
- en: The full operation takes around 150 ms.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整操作大约需要150毫秒。
- en: 'For a GPU-optimized workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 对于优化过的GPU工作负载，我们的DeepLabv3模型显示出以下时间轮廓：
- en: '![Figure 9.13 – Profiling DeepLabv3: Optimized GPU workload](img/B16591_09_13.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图9.13 – 深度分析DeepLabv3：优化过的GPU工作负载](img/B16591_09_13.jpg)'
- en: 'Figure 9.13 – Profiling DeepLabv3: Optimized GPU workload'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 深度分析DeepLabv3：优化过的GPU工作负载
- en: 'In *Figure 9.13*, we can see the following characteristics:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.13*中，我们可以看到以下特点：
- en: Almost all of the tasks are handled by two processes, similar to the non-optimized
    counterpart.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有任务都由两个进程处理，类似于未优化的版本。
- en: Around 4 ms into the operation, all tasks have been sent to be dispatched, and
    the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`),
    much faster than the non-optimized counter-part.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在操作开始约4毫秒时，所有任务已被发送进行调度，控制返回以继续操作（懒评估和`mx.nd.waitall`），比未优化的版本快得多。
- en: Synchronous/structured usage of memory, in stark contrast to the non-optimized
    counterpart.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存的同步/结构化使用，与未优化的版本形成鲜明对比。
- en: All operations are atomic and executed individually; similar to the non-optimized
    counterpart, they are just much faster. For example, large convolution operations
    take ~1 ms in the GPU non-optimized case, whereas they take one-third of that
    time (~0.34 ms) in the GPU-optimized case.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都是原子性的并单独执行；与未优化的版本类似，只是速度更快。例如，大型卷积操作在GPU未优化的情况下大约需要1毫秒，而在GPU优化的情况下只需要三分之一的时间（约0.34毫秒）。
- en: The full operation takes around 55 ms (one-third of the non-optimized time).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整操作大约需要55毫秒（为未优化时间的三分之一）。
- en: 'In summary, for GPU-based optimizations, we can clearly see the effects:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于基于GPU的优化，我们可以清晰地看到效果：
- en: Hybridization, as expected, has no effect and no operator fusion can be identified.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如预期的那样，混合化没有效果，也没有发现操作融合。
- en: AMP makes operations run much faster if the GPU has float16-dedicated circuitry.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果GPU具有专用的float16电路，AMP使得操作运行得更快。
- en: Exporting our models to ONNX and TensorRT
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将我们的模型导出到ONNX和TensorRT
- en: 'MXNet and GluonCV also provide tools to export our models externally. This
    makes the most sense for optimizing runtime computation times (inference) might
    need:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet和GluonCV也提供了将我们的模型导出到外部的工具。这对于优化运行时计算时间（推理）最为有用，可能需要：
- en: Specific algorithms that MXNet/GluonCV might not support
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXNet/GluonCV可能不支持的特定算法
- en: Deployment and optimizations on specific hardware platforms
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定硬件平台上的部署和优化
- en: In this section, we are going to study one example of each category.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将研究每个类别的一个示例。
- en: For specific algorithms, we are going to export our models in ONNX format. **ONNX**
    stands for **Open Neural Network eXchange** and is an open format that describes
    how deep learning models can be stored and shared. This is extremely useful to
    leverage specific tools for highly specialized tasks. For example, **ONNX Runtime**
    has really powerful inference tools, including quantization (for example, ONNX
    Runtime has support for GPU-based INT8 quantization). Therefore, we can export
    our model in ONNX format and start working directly with ONNX Runtime.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的算法，我们将把我们的模型导出为ONNX格式。**ONNX**代表**开放神经网络交换格式**，它是一个开放的格式，描述了深度学习模型如何存储和共享。这对于利用特定工具执行高度专业化任务极为有用。例如，**ONNX
    Runtime**拥有强大的推理工具，包括量化（例如，ONNX Runtime支持基于GPU的INT8量化）。因此，我们可以将模型导出为ONNX格式，并直接开始使用ONNX
    Runtime进行工作。
- en: 'As usual, MXNet will allow us to accomplish this with just a few lines of code.
    We will need to carry out two steps. Firstly, we need to transform our model from
    Gluon to symbolic format (hybridizing and then exporting):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，MXNet 只需几行代码就能帮助我们完成这个任务。我们需要执行两个步骤。首先，我们需要将模型从 Gluon 格式转换为符号格式（先混合，再导出）：
- en: '[PRE29]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we can transform the symbolic model into ONNX:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将符号模型转换为 ONNX：
- en: '[PRE30]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'ONNX also provides a checker to verify our model has been exported correctly.
    This can be done with the following lines of code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 还提供了一个检查器，用于验证我们的模型是否正确导出。可以使用以下代码行来完成：
- en: '[PRE31]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: And that’s it! Following these instructions, we will have our ONNX model stored
    in a file (in our example, `'deeplab_resnet101_coco_pt_gpu_hybrid.onnx'`), ready
    to be used with any tool that accepts ONNX models as input.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！按照这些指示，我们将把 ONNX 模型存储在文件中（在我们的示例中为 `'deeplab_resnet101_coco_pt_gpu_hybrid.onnx'`），并准备好在任何接受
    ONNX 模型作为输入的工具中使用。
- en: On the other hand, sometimes we would like to deploy and/or optimize our models
    on specific hardware platforms, such as the NVIDIA family of products (for example,
    Nvidia Jetson platforms). Specifically, Nvidia works with a specific machine learning
    framework designed to run inference on their own hardware. This framework is called
    **TensorRT**.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有时我们希望在特定硬件平台上部署和/或优化我们的模型，例如 NVIDIA 系列产品（例如，Nvidia Jetson 平台）。具体来说，Nvidia
    提供了一个特定的机器学习框架，用于在其硬件上运行推理。这个框架叫做**TensorRT**。
- en: Although MXNet features direct TensorRT integration, it’s not enabled by default,
    requiring building MXNet directly from the source, with specific parameters enabling
    TensorRT integration. Much more straightforwardly, we can leverage our recently
    described ONNX export to generate a TensorRT-capable model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MXNet 提供了直接的 TensorRT 集成，但默认情况下并未启用，需要从源代码直接构建 MXNet，并启用特定参数来支持 TensorRT
    集成。更为直接的是，我们可以利用刚才描述的 ONNX 导出，生成一个支持 TensorRT 的模型。
- en: 'To achieve this, it is enough to write a few lines of code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，只需写几行代码：
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With this, we will write a serialized TensorRT-capable model.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们将编写一个序列化的 TensorRT 可用模型。
- en: 'We can verify the model can be read by deserializing and reading it. We can
    do so with the following lines of code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过反序列化和读取模型来验证它是否可以被正确读取。我们可以使用以下代码行做到这一点：
- en: '[PRE33]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And we are done! In this section, we have been able to successfully write ONNX
    and TensorRT models. Congratulations!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！在这一节中，我们已经成功地编写了 ONNX 和 TensorRT 模型。恭喜！
- en: How it works...
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we have applied the different inference optimization techniques
    seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs)
    to optimize our model runtime performance by doing the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们应用了本章第一节中提到的不同推理优化技术，利用我们的硬件（CPU 和 GPU）通过以下方式优化模型的运行时性能：
- en: Hybridizing the model
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合模型
- en: Leveraging AMP
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 AMP
- en: Quantizing with the INT8 data type for accelerated inference
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 INT8 数据类型进行量化以加速推理
- en: Moreover, we have learned how to use model visualizations (powered by GraphViz)
    and the MXNet profiler and have used these tools to analyze the inference optimizations
    from a low-level perspective.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还学习了如何使用模型可视化工具（由 GraphViz 提供支持）和 MXNet 分析器，并利用这些工具从低级角度分析推理优化。
- en: Finally, we have learned how to export our models for specific scenarios and
    purposes, using the ONNX and TensorRT libraries.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学会了如何将模型导出到特定场景和目的，使用 ONNX 和 TensorRT 库。
- en: There’s more…
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: In this recipe, we have presented the inference optimization problem from a
    post-training perspective. We were given a (pre-)trained model and tried to squeeze
    as much performance as we could from it.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们从训练后角度提出了推理优化问题。我们拿到了一个（预）训练的模型，并尽可能挖掘它的性能。
- en: 'However, there is another avenue that can be explored, which starts thinking
    about maximizing inference performance from a machine learning model design perspective.
    This is known as **model compression** and is an active area of research, with
    lots of improvements published periodically. Recently active research topics include
    the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有另一条可以探索的途径，那就是从机器学习模型设计的角度开始思考如何最大化推理性能。这就是所谓的**模型压缩**，它是一个活跃的研究领域，定期会发布很多改进。近期的活跃研究课题包括：
- en: '**Knowledge distillation**: [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识蒸馏**：[https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)'
- en: '**Pruning**: [https://arxiv.org/pdf/1510.00149.pdf](https://arxiv.org/pdf/1510.00149.pdf)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：[https://arxiv.org/pdf/1510.00149.pdf](https://arxiv.org/pdf/1510.00149.pdf)'
- en: '**Quantization-aware training**: [https://arxiv.org/pdf/1712.05877.pdf](https://arxiv.org/pdf/1712.05877.pdf)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化感知训练**：[https://arxiv.org/pdf/1712.05877.pdf](https://arxiv.org/pdf/1712.05877.pdf)'
- en: Optimizing inference when translating text from English to German
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化从英语翻译到德语的推理
- en: 'In the initial recipe, we saw how we can leverage MXNet and Gluon to optimize
    the inference of our models, applying different techniques: improving the runtime
    performance using hybridization; how using half-precision (float16) in combination
    with AMP can strongly reduce our inference times; and how to take advantage of
    further optimizations with data types such as Int8 quantization.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的食谱中，我们展示了如何利用MXNet和Gluon优化模型的推理过程，应用了不同的技术：通过混合化提高运行时性能；如何结合AMP使用半精度（float16）显著减少推理时间；以及如何通过数据类型（如Int8量化）进一步优化。
- en: 'Now, we can revisit a problem we have been working with throughout the book:
    translating English to German. We have worked with translation tasks in recipes
    from previous chapters. In *Recipe 4*, *Translating text from Vietnamese to English*,
    from [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with
    Natural Language Processing*, we introduced the task of translating text, while
    also learning how to use pre-trained models from GluonCV Model Zoo.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重新审视本书中一直在讨论的一个问题：将英语翻译成德语。我们在前几章的食谱中处理过翻译任务。在[*第6章*](B16591_06.xhtml#_idTextAnchor121)《利用自然语言处理理解文本》的*食谱4*《将越南语文本翻译成英语》中，我们介绍了翻译文本的任务，同时学习了如何使用来自GluonCV
    Model Zoo的预训练模型。
- en: 'Furthermore, in *Recipe 4*, *Improving performance for translating English
    to German*, from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), *Optimizing
    Models with Transfer Learning and Fine-Tuning*, we introduced the datasets that
    we will be using in this recipe: **WMT 2014** and **WMT 2016**. We also compared
    the different approaches that we could take when dealing with a target dataset:
    training our models from scratch or leveraging past knowledge from pre-trained
    models and adjusting them for our task, using the different modalities of transfer
    learning and fine-tuning. Lastly, in *Recipe 3*, *Optimizing training for translating
    English to German*, from [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172), *Improving
    Training Performance with MXNet*, we applied different techniques to improve the
    runtime performance of our training loops.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在[*第7章*](B16591_07.xhtml#_idTextAnchor148)《通过迁移学习和微调优化模型》的*食谱4*《提高英语到德语翻译的性能》中，我们介绍了在本食谱中将使用的数据集：**WMT
    2014** 和 **WMT 2016**。我们还比较了处理目标数据集时可以采用的不同方法：从零开始训练我们的模型，或利用预训练模型的过去知识，并根据我们的任务进行调整，使用不同的迁移学习和微调方式。最后，在[*第8章*](B16591_08.xhtml#_idTextAnchor172)《通过MXNet提高训练性能》的*食谱3*《优化英语到德语翻译的训练》中，我们应用了不同的技术来提高训练循环的运行时性能。
- en: Therefore, in this recipe, we will apply all the introduced optimization techniques
    for the specific task of optimizing the inference for translating English to German.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本食谱中，我们将应用所有介绍过的优化技术，专门用于优化从英语到德语翻译的推理过程。
- en: Getting ready
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章一样，在本食谱中我们将使用一些矩阵运算和线性代数，但这并不难。
- en: How to do it...
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将进行以下步骤：
- en: Applying inference optimization techniques
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: Profiling our models
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对我们的模型进行分析
- en: Exporting our models
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导出我们的模型
- en: Let’s dive into each of these steps.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些步骤。
- en: Applying inference optimization techniques
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: In *Recipe 1*, *Introducing inference optimization features*, at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take in the inference of a machine learning
    model, including hybridization, AMP, and Int8 quantization.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始的*食谱1*《引入推理优化功能》中，我们展示了不同的优化技术如何改善机器学习模型推理过程中各个步骤的性能，包括混合化、AMP和Int8量化。
- en: In this section, we will show how, with MXNet and Gluon, just with a few lines
    of code, we can easily apply each and every technique introduced and verify the
    results of each technique.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何通过 MXNet 和 Gluon，仅用几行代码，我们就能轻松应用每个介绍过的技术，并验证每项技术的结果。
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the CPU:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不应用这些优化技术，作为基准，以下是使用 CPU 获得的定量结果：
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以检查模型在一个句子示例上的表现。在我们的案例中，我们选择了 *I learn new things every day*，并且获得的输出如下：
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中获得的德语句子（*Ich lerne neue Dinge, die in jedem Fall auftreten*）的意思是 *我学习在每种情况下都会出现的新事物*，因此，正如结果所示，文本几乎被完美地从英语翻译成了德语。
- en: 'As concluded in the previous recipe, for maximum performance on the CPU, the
    best approach is the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一个方案中总结的，为了在 CPU 上实现最大性能，最佳的方法如下：
- en: 'Use hybridization: Using the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合化：使用 Intel MKL-DNN 后端，结合静态内存分配和不变输入形状。
- en: Do not use AMP.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要使用 AMP。
- en: Use Int8 quantization.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Int8 量化。
- en: Unfortunately, we won’t be able to use Int8 quantization, as this is not supported
    for GluonNLP models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，我们无法使用 Int8 量化，因为它不支持 GluonNLP 模型。
- en: Let’s apply each of these techniques for our current specific task, translating
    from English to German.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为当前特定任务应用每一项技术，即从英语翻译成德语。
- en: 'For hybridization, we just need a couple of lines of code (which include the
    necessary parameters for the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes, and the hybridization of the loss function
    as well):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合化，我们只需要几行代码（包括所需的参数，这些参数是针对 Intel MKL-DNN 后端的，结合静态内存分配和不变输入形状，同时对损失函数进行混合化）：
- en: '[PRE36]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We do not need to add any steps connected to AMP as it was shown not to add
    benefits to CPU-based workloads. Similarly, GluonNLP does not support Int8 quantization,
    and therefore, we don’t need to make any further changes to our code.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要添加与 AMP 相关的任何步骤，因为已经证明它对基于 CPU 的工作负载没有益处。同样，GluonNLP 不支持 Int8 量化，因此我们不需要对代码做任何进一步的修改。
- en: 'Applying these optimization techniques, these are the quantitative results
    obtained for an optimized CPU inference:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些优化技术后，以下是优化后的 CPU 推理所获得的定量结果：
- en: '[PRE37]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we can see, the differences in performance (1.53 versus 1.53 for the loss
    and 26.40 versus 26.40 for the BLEU score) are negligible. However, with these
    inference optimization techniques, we have been able to reduce the inference runtime
    by 20% (313 seconds versus 374 seconds), which is a very good result.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，性能差异（损失函数为 1.53 与 1.53，BLEU 分数为 26.40 与 26.40）几乎可以忽略不计。然而，使用这些推理优化技术，我们成功将推理运行时间缩短了
    20%（313 秒比 374 秒），这是一个非常好的结果。
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以检查模型在一个句子示例上的表现。在我们的案例中，我们选择了 *I learn new things every day*，并且获得的输出如下：
- en: '[PRE38]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German. Moreover, the results are equivalent to the non-optimized
    case (as expected).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中获得的德语句子（*Ich lerne neue Dinge, die in jedem Fall auftreten*）的意思是 *我学习在每种情况下都会出现的新事物*，因此，正如结果所示，文本几乎被完美地从英语翻译成了德语。此外，结果与未优化的情况相同（如预期）。
- en: 'What about GPU-based inference? Let’s follow the same steps:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，基于 GPU 的推理如何呢？我们来按照相同的步骤操作：
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the GPU:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不应用这些优化技术，作为基准，以下是使用 GPU 获得的定量结果：
- en: '[PRE39]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As expected, there is no change relative to algorithmic performance from the
    CPU baseline. Runtime inference is indeed six times as fast in the GPU (61.7 seconds
    versus 374 seconds).
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如预期的那样，相对于 CPU 基线，算法性能没有变化。推理运行时间在 GPU 上确实是 CPU 的六倍快（61.7 秒对比 374 秒）。
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以通过一个句子示例检查我们的模型表现如何。在我们的例子中，我们选择了 *I learn new things every day*，输出结果如下：
- en: '[PRE40]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German (and is equivalent to both CPU cases).
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出中获得的德语句子（*Ich lerne neue Dinge, die in jedem Fall auftreten*）的意思是 *我学习每天都出现的新事物*，因此从结果来看，文本几乎被完美地从英语翻译成了德语（并且与两个
    CPU 情况相等）。
- en: 'As concluded in the previous recipe, for maximum performance on the GPU, the
    best approach is the following:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的步骤中总结的那样，为了在 GPU 上获得最大性能，最佳方法如下：
- en: 'Use hybridization: Using static memory allocation and invariant input shapes.
    Do not use the Intel MKL-DNN backend.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合化：使用静态内存分配和不变输入形状。不要使用 Intel MKL-DNN 后端。
- en: Use AMP.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AMP。
- en: Do not use Int8 quantization (not supported).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不使用 Int8 量化（不支持）。
- en: Unfortunately, we won’t be able to use AMP, as this is not supported for GluonNLP
    models.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于 GluonNLP 模型不支持 AMP，我们将无法使用 AMP。
- en: Let’s apply each of these techniques for our current specific task, translating
    from English to German.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些技术应用于我们当前的具体任务，从英语翻译成德语。
- en: 'For hybridization, we just need a couple of lines of code (which include the
    necessary parameters for static memory allocation and invariant input shapes,
    and the hybridization of the loss function as well):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合化，我们只需要几行代码（包括静态内存分配和不变输入形状所需的参数，以及损失函数的混合化）：
- en: '[PRE41]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We do not need to add any steps connected to AMP or Int8 quantization, as GluonNLP
    does not support these features. Therefore, no further steps are required.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要添加与 AMP 或 Int8 量化相关的任何步骤，因为 GluonNLP 不支持这些功能。因此，不需要进一步的步骤。
- en: 'By applying these optimization techniques, these are the quantitative results
    obtained for an optimized GPU inference:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这些优化技术，以下是优化 GPU 推理后的定量结果：
- en: '[PRE42]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As we can see, the differences in performance (1.53 versus 1.53 for the loss
    and 26.40 versus 26.40 for the BLEU score) are negligible. However, with these
    inference optimization techniques, we have been able to reduce the inference runtime
    by 10% (56.3 seconds versus 61.7 seconds), which is a very good result.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，性能差异（损失为 1.53 与 1.53，BLEU 得分为 26.40 与 26.40）是微不足道的。然而，通过这些推理优化技术，我们已经能够将推理运行时间减少了
    10%（56.3 秒对比 61.7 秒），这是一个非常好的结果。
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 从定性角度来看，我们还可以通过一个句子示例检查我们的模型表现如何。在我们的例子中，我们选择了 *I learn new things every day*，输出结果如下：
- en: '[PRE43]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German. Moreover, the results are equivalent to the non-optimized
    case (as expected).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中获得的德语句子（*Ich lerne neue Dinge, die in jedem Fall auftreten*）的意思是 *我学习每天都出现的新事物*，因此从结果来看，文本几乎被完美地从英语翻译成了德语。而且，结果与非优化情况（如预期）相当。
- en: Profiling our models
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对我们的模型进行分析
- en: In the previous sections, we saw the different techniques that we could apply
    to optimize our inference loops, and the results these techniques achieved. However,
    how exactly do these techniques work? Why are they faster?
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了可以应用于优化推理循环的不同技术，以及这些技术所取得的结果。然而，这些技术究竟是如何工作的？为什么它们更快？
- en: In this section, we are going to use the MXNet profiler, which can help us understand
    issues happening during runtime.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 MXNet 分析器，这有助于我们理解在运行时发生的问题。
- en: 'As mentioned in the initial section, the output of model profiling is a JSON
    file that can be visualized with tools such as the Google Chrome Tracing app.
    For a non-optimized CPU workload, our transformer model shows the following timing
    profile:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如初始部分所述，模型性能分析的输出是一个JSON文件，可以使用诸如Google Chrome Tracing应用等工具进行可视化。对于未优化的CPU工作负载，我们的Transformer模型显示以下时间分析：
- en: '![Figure 9.14 – Profiling Transformer: Non-optimized CPU workload](img/B16591_09_14.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14 – Transformer性能分析：未优化的CPU工作负载](img/B16591_09_14.jpg)'
- en: 'Figure 9.14 – Profiling Transformer: Non-optimized CPU workload'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – Transformer性能分析：未优化的CPU工作负载
- en: 'In *Figure 9.14*, we can see the following characteristics:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.14*中，我们可以看到以下特点：
- en: Almost all of the tasks are handled by two processes.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有的任务都由两个进程处理。
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有等待时间（惰性计算和`mx.nd.waitall`）。
- en: Synchronous/structured usage of memory.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存的同步/结构化使用。
- en: All operations are atomic and executed individually.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都是原子操作，并且逐个执行。
- en: The full operation takes around 1,200 ms.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整操作大约需要1,200毫秒。
- en: 'For a CPU-optimized workload, our Transformer model shows the following timing
    profile:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 对于经过CPU优化的工作负载，我们的Transformer模型显示以下时间分析：
- en: '![Figure 9.15 – Profiling Transformer: Optimized CPU workload](img/B16591_09_15.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.15 – Transformer性能分析：优化后的CPU工作负载](img/B16591_09_15.jpg)'
- en: 'Figure 9.15 – Profiling Transformer: Optimized CPU workload'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – Transformer性能分析：优化后的CPU工作负载
- en: 'In *Figure 9.15*, we can see the following characteristics:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.15*中，我们可以看到以下特点：
- en: Almost all of the tasks are handled by two processes, similar to the non-optimized
    counterpart.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有的任务都由两个进程处理，类似于未优化的情况。
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`), similar
    to the non-optimized counterpart.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有等待时间（惰性计算和`mx.nd.waitall`），与未优化的情况类似。
- en: Memory is used in a more asynchronous/structured way, in comparison to the non-optimized
    counterpart.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与未优化的情况相比，内存以更异步/结构化的方式使用。
- en: Some operations are fused together. Although the visualizations are not very
    clear, operator fusion (hybridization) seems to be working, with most of the time
    spent on fused operations.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些操作被融合在一起。尽管可视化不太清晰，但操作符融合（混合化）似乎在起作用，且大部分时间都花费在融合操作上。
- en: The full operation takes around 720 ms.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整操作大约需要720毫秒。
- en: 'Let’s take a zoomed look into one of the operator fusion steps:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细观察其中一个操作符融合步骤：
- en: '![Figure 9.16 – Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)](img/B16591_09_16.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.16 – Transformer性能分析：优化后的CPU工作负载（聚焦于OperatorFusion）](img/B16591_09_16.jpg)'
- en: 'Figure 9.16 – Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – Transformer性能分析：优化后的CPU工作负载（聚焦于OperatorFusion）
- en: In *Figure 9.16* we can see how operator fusion has fused together several different
    operations, including embeddings, layer normalization, fully connected layers,
    and MKL-DNN-accelerated layers.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.16*中，我们可以看到操作符融合如何将多个不同的操作融合在一起，包括嵌入、层归一化、全连接层和MKL-DNN加速的层。
- en: 'In summary, for CPU-based optimizations, we can clearly see the effects:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于基于CPU的优化，我们可以清楚地看到以下效果：
- en: Hybridization has fused most of the operators together, although the visualization
    is difficult to see, and this happens many times.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合化已经将大多数操作符融合在一起，尽管可视化较为困难，而且这种情况发生了多次。
- en: The MKL-DNN backend has improved those operations with accelerated operators.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MKL-DNN后端通过加速的操作符改进了这些操作。
- en: Let’s discuss the GPU case now.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论GPU情况。
- en: 'For a non-optimized GPU workload, our Transformer model shows the following
    timing profile:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未优化的GPU工作负载，我们的Transformer模型显示以下时间分析：
- en: '![Figure 9.17 – Profiling Transformer: Non-optimized GPU workload](img/B16591_09_17.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.17 – Transformer性能分析：未优化的GPU工作负载](img/B16591_09_17.jpg)'
- en: 'Figure 9.17 – Profiling Transformer: Non-optimized GPU workload'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – Transformer性能分析：未优化的GPU工作负载
- en: 'In *Figure 9.17*, we can see the following characteristics:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.17*中，我们可以看到以下特点：
- en: Tasks are mostly handled by several (three) GPU processes.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务主要由几个（三个）GPU进程处理。
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有等待时间（惰性计算和`mx.nd.waitall`）。
- en: Memory is gradually increasing.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存逐渐增加。
- en: All operations are atomic and executed individually.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有操作都是原子操作，并且逐个执行。
- en: Several copies from/to CPU, which do not seem to degrade performance.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多个从/到CPU的拷贝，这似乎没有降低性能。
- en: The full operation takes around 580 ms.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整操作大约需要580毫秒。
- en: 'For a GPU-optimized workload, our Transformer model shows the following timing
    profile:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPU 优化的工作负载，我们的 Transformer 模型展示了以下时间性能：
- en: '![Figure 9.18 – Profiling Transformer: Optimized GPU workload](img/B16591_09_18.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.18 – 性能分析 Transformer：优化后的 GPU 工作负载](img/B16591_09_18.jpg)'
- en: 'Figure 9.18 – Profiling Transformer: Optimized GPU workload'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 性能分析 Transformer：优化后的 GPU 工作负载
- en: 'In *Figure 9.18*, we can see the following characteristics:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.18*中，我们可以看到以下特点：
- en: Almost all of the tasks are handled by three processes, similar to the non-optimized
    counterpart.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有任务都由三个进程处理，类似于未优化的版本。
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`), similar
    to the non-optimized counterpart.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎没有等待时间（懒加载和 `mx.nd.waitall`），与未优化版本类似。
- en: More asynchronous/unstructured usage of memory, in comparison to the non-optimized
    counterpart.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与未优化版本相比，内存的异步/非结构化使用更多。
- en: Some operations are fused together. Although the visualizations are not very
    clear, operator fusion (hybridization) seems to be working, spending most of the
    time in fused operations.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些操作被融合在一起。尽管可视化不够清晰，但操作融合（混合化）似乎有效，绝大部分时间都花费在了融合的操作上。
- en: Data copy operations from/to CPU do not seem to degrade performance, although
    there are several.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从/到 CPU 的数据复制操作似乎不会影响性能，尽管有几个操作。
- en: The full operation takes around 260 ms.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个操作大约需要 260 毫秒。
- en: 'Let’s take a zoomed look into one of the operator fusion steps:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看一下其中一步操作融合的过程：
- en: '![Figure 9.19 – Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)](img/B16591_09_19.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.19 – 性能分析 Transformer：优化后的 GPU 工作负载（聚焦操作融合）](img/B16591_09_19.jpg)'
- en: 'Figure 9.19 – Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – 性能分析 Transformer：优化后的 GPU 工作负载（聚焦操作融合）
- en: In *Figure 9.19*, we can see how operator fusion has fused together several
    different operations, including embeddings, layer normalization, and fully connected
    layers.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.19*中，我们可以看到操作融合如何将几个不同的操作融合在一起，包括嵌入、层归一化和全连接层。
- en: In summary, for GPU-based optimizations, we can clearly see the effect of hybridization,
    where all operations have been fused together, although the visualization is difficult
    to interpret, and this happens many times.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，对于基于 GPU 的优化，我们可以清楚地看到混合化的效果，所有操作都已被融合在一起，尽管可视化较难解读，而且这种情况发生了很多次。
- en: Exporting our models
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出我们的模型
- en: MXNet and GluonNLP also provide tools to export our models. However, these tools
    are mostly for internal usage of MXNet/Gluon. The reason for this is that GluonNLP
    mostly deals with `save()` function.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 和 GluonNLP 也提供了导出模型的工具。然而，这些工具主要是为 MXNet/Gluon 的内部使用而设计。原因是 GluonNLP 主要处理
    `save()` 函数。
- en: 'This function can be easily called:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以很容易地调用：
- en: '[PRE44]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can verify the files associated with the model, the parameters (the `.params`
    extension), and the architecture (the `.json` extension) have been saved with
    these commands:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下命令验证与模型相关的文件，参数（`.params` 扩展名）和架构（`.json` 扩展名）是否已被保存：
- en: '[PRE45]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And we are done! In this section, we have been able to successfully export our
    Transformer model. Congratulations!
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！在这一节中，我们成功地导出了我们的 Transformer 模型。恭喜！
- en: How it works...
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we have applied the different inference optimization techniques
    seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs)
    to optimize our model runtime performance by hybridizing the model.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们应用了本章第一个食谱中看到的不同推理优化技术，利用我们的硬件（CPU 和 GPU）通过混合化模型来优化模型的运行性能。
- en: Moreover, we have learned how to use the MXNet profiler to analyze the inference
    optimizations from a low-level perspective.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经学习了如何使用 MXNet 性能分析器从低级别的角度分析推理优化。
- en: Finally, we have learned how to export our models using internal MXNet libraries.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经学会了如何使用 MXNet 内部库导出我们的模型。
- en: There’s more…
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In this recipe, we presented the inference optimization problem from a post-training
    perspective. We were given a (pre-)trained model and we tried to squeeze as much
    performance as we could from it.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们从训练后的角度展示了推理优化问题。我们得到一个（预）训练好的模型，并尝试从中挤出尽可能多的性能。
- en: 'However, there is another avenue that can be explored, which starts thinking
    about maximizing inference performance from a machine learning model design perspective.
    Several improvements to how LLMs can be used without large compute workloads have
    been published, such as the following:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有另一条可以探索的途径，这条途径从机器学习模型设计的角度思考如何最大化推理性能。已经发布了几种改进方法，展示了如何在没有大量计算工作负载的情况下使用大型语言模型（LLM），例如以下几种：
- en: '**Low Ranking Adaptation (LORA)**: [https://arxiv.org/pdf/2012.13255.pdf](https://arxiv.org/pdf/2012.13255.pdf)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低排名适应（LORA）**: [https://arxiv.org/pdf/2012.13255.pdf](https://arxiv.org/pdf/2012.13255.pdf)'
- en: '**LORA meets pruning**: [https://arxiv.org/pdf/2305.18403.pdf](https://arxiv.org/pdf/2305.18403.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LORA与剪枝**: [https://arxiv.org/pdf/2305.18403.pdf](https://arxiv.org/pdf/2305.18403.pdf)'
- en: '**GPT4All (quantization)**: [https://gpt4all.io](https://gpt4all.io)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT4All（量化）**: [https://gpt4all.io](https://gpt4all.io)'
- en: '**Int4 quantization**: [https://arxiv.org/pdf/2301.12017.pdf](https://arxiv.org/pdf/2301.12017.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Int4 量化**: [https://arxiv.org/pdf/2301.12017.pdf](https://arxiv.org/pdf/2301.12017.pdf)'
