- en: '*Chapter 5*: Solving the Reinforcement Learning Problem'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：解决强化学习问题'
- en: In the previous chapter, we provided the mathematical foundations for modeling
    a reinforcement learning (RL) problem. In this chapter, we'll lay the foundations
    for solving it. Many of the following chapters will focus on some specific solution
    approaches that will build on this foundation. To this end, we'll first cover
    the **dynamic programming** (**DP**) approach, with which we'll introduce some
    key ideas and concepts. DP methods provide optimal solutions to **Markov decision
    processes** (**MDPs**) yet require the complete knowledge and a compact representation
    of the state transition and reward dynamics of the environment. This could be
    severely limiting and impractical in a realistic scenario, where the agent is
    either directly trained in the environment itself or in a simulation of it. The
    **Monte Carlo** and **temporal difference** (**TD**) approaches, which we'll cover
    later, unlike DP, use sampled transitions from the environment and relax the aforementioned
    limitations. Finally, we'll also talk in detail about what makes a simulation
    model suitable for RL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们为强化学习（RL）问题的建模提供了数学基础。在本章中，我们将奠定解决该问题的基础。接下来的许多章节将专注于一些特定的解决方法，这些方法将基于这个基础进行展开。为此，我们将首先讲解**动态规划**（**DP**）方法，通过它我们将介绍一些关键的理念和概念。DP方法为**马尔可夫决策过程**（**MDPs**）提供了最优解，但需要对环境的状态转移和奖励动态有完整的知识和紧凑的表示。在现实场景中，这可能会变得极为限制且不切实际，因为在这种情况下，代理要么直接在环境中进行训练，要么在该环境的仿真中进行训练。与DP不同，后面我们将讲解的**蒙特卡洛**（**Monte
    Carlo**）和**时序差分**（**TD**）方法使用来自环境的采样转移，并放宽了上述限制。最后，我们还将详细讨论什么样的仿真模型适合用于强化学习。
- en: 'In particular, here are the sections in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章特别介绍了以下几个部分：
- en: Exploring dynamic programming
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索动态规划
- en: Training your agent with Monte Carlo methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法训练你的代理
- en: Temporal difference learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: Understanding the importance of simulation in reinforcement learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解仿真在强化学习中的重要性
- en: Exploring dynamic programming
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索动态规划
- en: DP is a branch of mathematical optimization that proposes optimal solution methods
    to MDPs. Although most real-world problems are too complex to optimally solve
    via DP methods, the ideas behind these algorithms are central to many RL approaches.
    So, it is important to have a solid understanding of them. Throughout this chapter,
    we'll go from these optimal methods to more practical approaches by systematically
    introducing approximations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DP是数学优化的一个分支，提出了针对MDPs的最优解法。尽管大多数现实问题过于复杂，无法通过DP方法得到最优解，但这些算法背后的思想对许多RL方法至关重要。因此，深入理解这些方法是非常重要的。在本章中，我们将从这些最优方法出发，通过系统地介绍近似方法，逐步过渡到更实际的解决方案。
- en: We'll start this section by describing an example that will serve as a use case
    for the algorithms that we will introduce later in the chapter. Then, we will
    cover how to do prediction and control using DP.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过描述一个示例来开始本节，该示例将作为我们后续介绍的算法的用例。接下来，我们将讲解如何使用动态规划（DP）进行预测和控制。
- en: Let's get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Example use case – Inventory replenishment of a food truck
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例用例——食品车的库存补充
- en: Our use case involves a food truck business that needs to decide how many burger
    patties to buy every weekday to replenish its inventory. Inventory planning is
    an important class of problems in retail and manufacturing that a lot of companies
    need to deal with all the time. Of course, for pedagogical reasons, our example
    is much simpler than what you would see in practice. However, it should still
    give you an idea about this problem class.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用例涉及一个食品车业务，需要决定每个工作日购买多少个汉堡肉饼以补充库存。库存规划是零售和制造业中一个重要的问题类别，许多公司都需要不断处理这类问题。当然，为了教学的需要，我们的示例比实际情况要简单得多。然而，它仍然能帮助你理解这个问题类别。
- en: 'Now, let''s dive into the example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入到这个例子中：
- en: Our food truck operates downtown during the weekdays.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的食品车在工作日运营于市中心。
- en: 'Every weekday morning, the owner needs to decide on how many burger patties
    to buy with the following options: ![](img/Formula_05_001.png). The cost of a
    single patty is ![](img/Formula_05_002.png)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作日上午，店主需要决定购买多少个汉堡肉饼，选择如下：![](img/Formula_05_001.png)。单个肉饼的成本是![](img/Formula_05_002.png)。
- en: The food truck can store the patties up to a capacity of ![](img/Formula_05_003.png)
    during the weekdays. However, since the truck does not operate over the weekend,
    and any inventory unsold by Friday evening spoils, if during a weekday, the number
    of patties purchased and the existing inventory exceeds the capacity, the excess
    inventory also spoils.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 餐车在工作日最多可以存储 ![](img/Formula_05_003.png) 个肉饼。然而，由于餐车在周末不运营，且任何到周五晚上仍未售出的库存都会变质，如果在工作日内，购买的肉饼数量和现有库存超过容量，超出的库存也会变质。
- en: 'Burger demand for any weekday is a random variable ![](img/Formula_05_004.png)
    with the following probability mass function:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作日的汉堡需求是一个随机变量 ![](img/Formula_05_004.png)，其概率质量函数如下：
- en: '![](img/Formula_05_005.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_005.jpg)'
- en: The net revenue per burger (after the cost of the ingredients other than the
    patty) is ![](img/Formula_05_006.png).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个汉堡的净收入（扣除除肉饼之外的其他原料成本）是 ![](img/Formula_05_006.png)。
- en: Sales in a day is the minimum of the demand and the available inventory, since
    the truck cannot sell more burgers than the number of patties available.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一天的销售量是需求量与可用库存中的最小值，因为餐车不能卖出超过可用肉饼数量的汉堡。
- en: So, what we have is a multi-step inventory planning problem and our goal is
    to maximize the total expected profit (![](img/Formula_05_007.png)) in a week.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们面临的是一个多步骤的库存规划问题，我们的目标是在一周内最大化总预期利润 (![](img/Formula_05_007.png))。
- en: Info
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The one-step inventory planning problem is often called "the newsvendor problem."
    It is about balancing the cost of overage and underage given a demand distribution.
    For many common demand distributions, this problem can be solved analytically.
    Of course, many real-world inventory problems are multi-step, similar to what
    we will solve in this chapter. You can read more about the newsvendor problem
    at [https://en.wikipedia.org/wiki/Newsvendor_model](https://en.wikipedia.org/wiki/Newsvendor_model).
    We will solve a more sophisticated version of this problem in [*Chapter 15*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329),
    *Supply Chain Management*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单步库存规划问题通常被称为“新闻vendor问题”。它是关于在给定需求分布的情况下平衡过剩和不足的成本。对于许多常见的需求分布，这个问题可以通过解析方法解决。当然，许多现实世界中的库存问题是多步骤的，类似于我们在本章中将要解决的问题。你可以在[https://en.wikipedia.org/wiki/Newsvendor_model](https://en.wikipedia.org/wiki/Newsvendor_model)
    阅读更多关于新闻vendor问题的内容。我们将在[*第15章*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329)《供应链管理》中解决这个问题的更复杂版本。
- en: So far so good. Next, let's create this environment in Python.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利。接下来，让我们在 Python 中创建这个环境。
- en: Implementing the food truck environment in Python
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Python 中实现餐车环境
- en: What we are about to create is a simulation environment for the food truck example
    as per the dynamics we described above. In doing so, we will use the popular framework
    designed for exactly the same purpose, which is OpenAI's Gym library. Chances
    are you have probably come across it before. But if not, that is perfectly fine
    since it does not play a critical role in this example. We will cover what you
    need to know as we go through it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将创建的是一个餐车示例的仿真环境，依据我们上述描述的动态。在这个过程中，我们将使用一个专门为此目的设计的流行框架，即 OpenAI 的 Gym 库。你可能以前听说过它。如果没有，也完全没问题，因为它在本示例中并不起决定性作用。我们将逐步讲解你需要知道的内容。
- en: Info
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'OpenAI''s Gym is the standard library for defining RL environments and developing
    and comparing solution methods. It is also compatible with various RL solution
    libraries, such as RLlib. If you are not already familiar with the Gym environment,
    take a look at its concise documentation here: [https://gym.openai.com/docs/](https://gym.openai.com/docs/).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 Gym 是定义强化学习（RL）环境、开发和比较解决方法的标准库。它还兼容各种 RL 解决库，如 RLlib。如果你还不熟悉 Gym 环境，可以查看它的简明文档：[https://gym.openai.com/docs/](https://gym.openai.com/docs/)。
- en: 'Now, let''s go into the implementation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入实现部分：
- en: 'We start by importing the libraries we will need:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入我们需要的库：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we create a Python class, which is initialized with the environment parameters
    we described in the previous section:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 Python 类，该类初始化时使用我们在上一节中描述的环境参数：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The state is a tuple of the day of the week (or the weekend) and the starting
    inventory level for the day. Again, the action is the number of patties to purchase
    before the sales start. This purchased inventory becomes available immediately.
    Note that this is a fully observable environment, so the state space and the observation
    space are the same. Possible inventory levels are 0, 100, 200, and 300 at the
    beginning of a given day (because of how we defined the action set, possible demand
    scenarios, and the capacity); except we start Monday with no inventory.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态是一个元组，包括星期几（或周末）和当天的起始库存水平。再次强调，动作是销售开始前要购买的肉饼数量。这些购买的库存会立即可用。请注意，这是一个完全可观察的环境，因此状态空间和观察空间是相同的。给定某一天的可能库存水平是
    0、100、200 和 300（因为我们定义了动作集、可能的需求情景和容量）；除非我们从星期一开始没有库存。
- en: 'Next, let''s define a method that calculates the next state and the reward
    along with the relevant quantities, given the current state, the action, and the
    demand. Note that this method does not change anything in the object:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个方法，给定当前状态、动作和需求，计算下一个状态和奖励，以及相关的量。请注意，这个方法不会改变对象的任何内容：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we define a method that returns all possible transitions and rewards for
    a given state-action pair using the `get_next_state_reward` method, together with
    the corresponding probabilities. Notice that different demand scenarios will lead
    to the same next state and reward if the demand exceeds the inventory:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个方法，使用 `get_next_state_reward` 方法返回给定状态-动作对的所有可能转换和奖励，以及相应的概率。请注意，如果需求超过库存，不同的需求情景将导致相同的下一个状态和奖励：
- en: '[PRE3]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's all we need for now. Later, we will add other methods to this class to
    be able to simulate the environment. Now, we'll dive into DP with the policy evaluation
    methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们所需的内容就这些。稍后，我们将向这个类中添加其他方法，以便能够模拟环境。现在，我们将深入讨论使用策略评估方法的动态规划（DP）。
- en: Policy evaluation
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略评估
- en: In MDPs (and RL), our goal is to obtain (near) optimal policies. But how do
    we even evaluate a given policy? After all, if we cannot evaluate it, we cannot
    compare it against another policy and decide which one is better. So, we start
    discussing the DP approaches with **policy evaluation** (also called the **prediction
    problem**). There are multiple ways to evaluate a given policy. In fact, in [*Chapter
    4*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080), *Making of the Markov Decision
    Process*, when we defined the state-value function, we discussed how to calculate
    it analytically and iteratively. Well, that is policy evaluation! In this section,
    we will go with the iterative version, which we'll turn to next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程（MDPs）和强化学习（RL）中，我们的目标是获得（近似）最优策略。那么我们如何评估给定的策略呢？毕竟，如果我们不能评估它，就无法与其他策略进行比较，进而决定哪个更好。因此，我们开始讨论使用**策略评估**（也称为**预测问题**）的动态规划方法。有多种方式可以评估给定的策略。事实上，在
    [*第 4 章*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080)，《马尔可夫决策过程的构建》中，当我们定义状态值函数时，我们讨论了如何通过解析和迭代的方式计算它。嗯，这就是策略评估！在本节中，我们将采用迭代版本，接下来我们会详细介绍。
- en: Iterative policy evaluation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 迭代策略评估
- en: Let's first discuss the iterative policy evaluation algorithm and refresh your
    mind on what we covered in the previous chapter. Then, we will evaluate the policy
    that the owner of the food truck already has (the base policy).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论迭代策略评估算法，并回顾一下我们在上一章中覆盖的内容。然后，我们将评估食品车老板已经拥有的策略（即基础策略）。
- en: Iterative policy iteration algorithm
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 迭代策略迭代算法
- en: 'Recall that the value of a state is defined as follows for a given policy:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，状态的值是根据给定的策略如下定义的：
- en: '![](img/Formula_05_008.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_008.jpg)'
- en: '![](img/Formula_05_009.png) is the expected discounted cumulative reward starting
    in state ![](img/Formula_05_010.png) and following policy ![](img/Formula_05_011.png).
    In our food truck example, the value of the state ![](img/Formula_05_012.png)
    is the expected reward (profit) of a week that starts with zero inventory on Monday.
    The policy that maximizes ![](img/Formula_05_013.png) would be the optimal policy!'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_05_009.png) 是从状态 ![](img/Formula_05_010.png) 开始并遵循策略 ![](img/Formula_05_011.png)
    的预期折扣累积奖励。在我们的食品车示例中，状态 ![](img/Formula_05_012.png) 的值是从零库存开始的星期一的预期奖励（利润）。最大化
    ![](img/Formula_05_013.png) 的策略就是最优策略！'
- en: 'Now, the Bellman equation tells us the state values must be consistent with
    each other. It means that the expected one-step reward together with the discounted
    value of the next state should be equal to the value of the current state. More
    formally, this is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，贝尔曼方程告诉我们状态值必须彼此一致。这意味着预期的单步奖励加上下一个状态的折扣值应该等于当前状态的值。更正式地说，表达式如下：
- en: '![](img/Formula_05_014.jpg)![](img/Formula_05_015.jpg)![](img/Formula_05_016.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_014.jpg)![](img/Formula_05_015.jpg)![](img/Formula_05_016.jpg)'
- en: 'Since we know all the transition probabilities for this simple problem, we
    can analytically calculate this expectation:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道这个简单问题的所有转移概率，我们可以解析地计算出这个期望：
- en: '![](img/Formula_05_017.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_017.jpg)'
- en: The ![](img/Formula_05_018.png) term at the beginning is because the policy
    may suggest taking actions probabilistically given the state. Since the transition
    probabilities depend on the action, we need to account for each possible action
    the policy may lead us to.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 开头的 ![](img/Formula_05_018.png) 项是因为策略可能会根据状态概率性地选择动作。由于转移概率依赖于动作，我们需要考虑策略可能引导我们执行的每一个可能动作。
- en: 'Now, all we need to do to obtain an iterative algorithm is to convert the Bellman
    equation into an update rule as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们所需要做的就是将贝尔曼方程转换为如下的更新规则，从而获得一个迭代算法：
- en: '![](img/Formula_05_019.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_019.jpg)'
- en: '*A single round of updates,* ![](img/Formula_05_020.png)*, involves updating
    all the state values*. The algorithm stops until the changes in state values are
    sufficiently small in successive iterations. We won''t go into the proof, but
    this update rule can be shown to converge to ![](img/Formula_05_021.png) as ![](img/Formula_05_022.png)
    This algorithm is called **iterative policy evaluation** with **expected update**
    since we take into account all possible one-step transitions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*一次更新过程，* ![](img/Formula_05_020.png)*，涉及更新所有状态值*。该算法会在状态值的变化在连续迭代中变得足够小后停止。我们不讨论证明，但这个更新规则可以证明会收敛到
    ![](img/Formula_05_021.png)，当 ![](img/Formula_05_022.png) 时。这个算法被称为**迭代政策评估**，具有**期望更新**，因为我们考虑了所有可能的单步转移。'
- en: One last note before we implement this method is that rather than carrying two
    copies of the state values for ![](img/Formula_05_023.png) and ![](img/Formula_05_024.png),
    and replacing ![](img/Formula_05_025.png) with ![](img/Formula_05_026.png) after
    a full round of updates, we'll just make in-place updates. This tends to converge
    faster since we make the latest estimate for the value of a state immediately
    available to be used for the other state updates.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现这个方法之前，最后要提到的是，与其为 ![](img/Formula_05_023.png) 和 ![](img/Formula_05_024.png)
    维护两个状态值副本，并在一次完整的更新后将 ![](img/Formula_05_025.png) 替换为 ![](img/Formula_05_026.png)，我们将直接进行原地更新。这种方法往往收敛得更快，因为我们会立即将最新的状态值估算结果用于其他状态的更新。
- en: Next, let's evaluate a base policy for the inventory replenishment problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来评估库存补充问题的一个基础策略。
- en: Iterative evaluation of a base inventory replenishment policy
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于库存补充策略的迭代评估
- en: 'Imagine that the owner of the food truck has the following policy: At the beginning
    of a weekday, the owner replenishes the inventory up to 200 or 300 patties, with
    equal probability. For example, if the inventory at the beginning of the day is
    100, they are equally likely to purchase 100 or 200 patties. Let''s evaluate this
    policy and see how much profit we should expect in the course of a week:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设食品卡车的老板有以下策略：在一个工作日的开始，老板将库存补充到200或300个肉饼，且概率相等。例如，如果当天开始时库存为100个，他们有相同的概率购买100个或200个肉饼。我们来评估这个策略，看看在一周内我们应该预期多少利润：
- en: 'We first define a function that returns a `policy` dictionary, in which the
    keys correspond to the states. The value that corresponds to a state is another
    dictionary that has actions as the keys and the probability of selecting that
    action in that state as the values:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一个返回`policy`字典的函数，其中字典的键对应状态。每个状态对应的值是另一个字典，字典的键是动作，值是该状态下选择该动作的概率：
- en: '[PRE4]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now the policy evaluation. We define a function that will calculate the expected
    update for a given state and the corresponding policy for that state:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是政策评估。我们定义一个函数，用于计算给定状态的期望更新和该状态对应的策略：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In other words, this function calculates ![](img/Formula_05_027.png) for a given
    ![](img/Formula_05_028.png).
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 换句话说，这个函数为给定的 ![](img/Formula_05_028.png) 计算 ![](img/Formula_05_027.png)。
- en: 'The policy evaluation function executes the expected updates for all states
    until the state values converge (or it reaches a maximum number of iterations):'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略评估函数会对所有状态执行预期的更新，直到状态值收敛（或达到最大迭代次数）：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s elaborate on how this function works:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们详细说明一下这个函数的工作原理：
- en: a) The `policy_evaluation` function receives an environment object, which will
    be an instance of the `FoodTruck` class in our example.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `policy_evaluation`函数接收一个环境对象，在我们的示例中它将是`FoodTruck`类的一个实例。
- en: b) The function evaluates the specified policy, which is in the form of a dictionary
    that maps states to action probabilities.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 该函数评估指定的策略，策略以字典的形式存在，字典将状态映射到动作概率。
- en: c) All the state values are initialized to 0 unless an initialization is passed
    into the function. The state values for the terminal states (states corresponding
    to the weekend in this example) are not updated since we don't expect any reward
    from that point on.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 所有的状态值初始为0，除非在函数中传入了初始化值。终止状态（在本例中对应周末的状态）的状态值不会被更新，因为从那时起我们不再期望任何奖励。
- en: d) We define an epsilon value to use as a threshold for convergence. If the
    maximum change between updates among all the state values is less than this threshold
    in a given round, the evaluation is terminated.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 我们定义了一个epsilon值作为收敛的阈值。如果在某一轮更新中，所有状态值的最大变化小于该阈值，则评估终止。
- en: e) Since this is an episodic task with a finite number of steps, we set the
    discount factor `gamma` to 1 by default.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 由于这是一个具有有限步数的阶段性任务，我们默认将折扣因子`gamma`设置为1。
- en: f) The function returns the state values, which we will need later.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f) 该函数返回状态值，稍后我们将需要这些状态值。
- en: 'Now, we evaluate the base policy the owner has using this function. First,
    create a `foodtruck` object from the class we defined above:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用这个函数来评估所有者的基础策略。首先，从我们之前定义的类中创建一个`foodtruck`对象：
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Get the base policy for the environment:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取环境的基础策略：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Evaluate the base policy and get the corresponding state values – specifically
    for the initial state:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估基础策略并获取对应的状态值——特别是对于初始状态：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The results will look like the following:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The state-value of `("Mon", 0)`, which is the initial state, is 2515 under this
    policy. Not a bad profit for a week!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个策略下，`("Mon", 0)`这个初始状态的状态值为2515。对于一周的时间来说，这可不算坏的利润！
- en: Great job so far! Now you are able to evaluate a given policy and calculate
    the state values corresponding to that policy. Before going into improving the
    policy, though, let's do one more thing. Let's verify that simulating the environment
    under this policy leads to a similar reward.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止做得很好！现在你已经能够评估给定的策略并计算出对应的状态值。在进入策略改进之前，我们再做一件事。让我们验证一下，在这个策略下模拟环境是否会导致类似的奖励。
- en: Comparing the policy evaluation against a simulation
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较策略评估与模拟结果
- en: 'In order to be able to simulate the environment, we need to add a few more
    methods to the `FoodTruck` class:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够模拟环境，我们需要向`FoodTruck`类添加一些额外的方法：
- en: 'Create a `reset` method, which simply initializes/resets the object to Monday
    morning with zero inventory. We will call this method before we start an episode,
    every time:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`reset`方法，简单地将对象初始化/重置为周一早晨，并将库存设为零。每次开始一个阶段时，我们都会调用这个方法：
- en: '[PRE11]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, define a method to check if a given state is terminal or not. Remember
    that episodes terminate at the end of the week in this example:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个方法来检查给定状态是否是终止状态。请记住，在本例中，阶段会在周末结束时终止：
- en: '[PRE12]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, define the `step` method that simulates the environment for a one-time
    step given the current state and the action:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，定义`step`方法来模拟给定当前状态和动作的一次环境步骤：
- en: '[PRE13]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The method returns the new state, one-step reward, whether the episode is complete,
    and any additional information we would like to return. This is the standard Gym
    convention. It also updates the state stored within the class.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法返回新的状态、一轮奖励、该阶段是否完成，以及我们想要返回的任何附加信息。这是标准的Gym约定。它还会更新类中的状态。
- en: 'Now that our `FoodTruck` class is ready for the simulation. Next, let''s create
    a function that chooses an action from a – possibly probabilistic – policy given
    a state:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们的`FoodTruck`类已经准备好进行模拟。接下来，让我们创建一个函数，在给定状态的情况下，从一个（可能是概率性的）策略中选择一个动作：
- en: '[PRE14]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s create a function (outside of the class) to simulate a given policy:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数（在类外部）来模拟给定的策略：
- en: '[PRE15]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `simulate_policy` function simply performs the following actions:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`simulate_policy`函数仅执行以下操作：'
- en: a) Receives a policy dictionary that returns the actions and the corresponding
    probabilities the policy suggests for a given state.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 接收一个策略字典，返回给定状态下该策略建议的动作及其相应的概率。
- en: b) It simulates the policy for a specified number of episodes.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 它模拟策略执行指定次数的回合。
- en: c) Within an episode, it starts at the initial state and probabilistically selects
    the actions suggested by the policy at each step.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在一次回合内，它从初始状态开始，并在每一步按概率选择策略建议的动作。
- en: d) The selected action is passed to the environment, which transitions into
    the next state as per the dynamics of the environment.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 选择的动作传递给环境，环境根据其动态转移到下一个状态。
- en: Now, let's simulate the environment with the base policy!
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们用基础策略来模拟环境！
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result should look like the following:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果应该如下所示：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Great! This closely matches what we calculated analytically! Now it is time
    to use this iterative policy evaluation method for something more useful: finding
    optimal policies!'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！这和我们通过解析计算得到的结果非常接近！现在是时候利用这种迭代策略评估方法做些更有用的事情了：寻找最优策略！
- en: Policy iteration
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略迭代
- en: Now that we have a way of evaluating a given policy, we can use it to compare
    two policies and iteratively improve them. In this section, we first discuss how
    policies are compared. Then, we introduce the policy improvement theorem and finally
    put everything together in the policy improvement algorithm.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种评估给定策略的方法，可以用它来比较两个策略并迭代改进它们。在这一节中，我们首先讨论如何比较策略。接着，我们介绍策略改进定理，最后将所有内容整合到策略改进算法中。
- en: Policy comparison and improvement
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略比较与改进
- en: 'Suppose that we have two policies, ![](img/Formula_05_029.png) and ![](img/Formula_05_030.png),
    that we would like to compare. We say ![](img/Formula_05_030.png) is as good as
    ![](img/Formula_05_032.png) if:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个策略，![](img/Formula_05_029.png) 和 ![](img/Formula_05_030.png)，我们想要比较它们。我们说如果满足以下条件，![](img/Formula_05_030.png)
    就和 ![](img/Formula_05_032.png) 一样好：
- en: '![](img/Formula_05_033.png).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_05_033.png)。'
- en: In other words, if the state values under a policy ![](img/Formula_05_034.png)
    are greater than or equal to the state values under another policy ![](img/Formula_05_035.png)
    for all possible states, then it means ![](img/Formula_05_036.png) is as good
    as ![](img/Formula_05_037.png). If this relation is a strict inequality for any
    state ![](img/Formula_05_028.png), then ![](img/Formula_05_039.png) is a better
    policy than ![](img/Formula_05_040.png). This should be intuitive since a state-value
    represents the expected cumulative reward from that point on.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果在一个策略 ![](img/Formula_05_034.png) 下的状态值大于或等于另一个策略 ![](img/Formula_05_035.png)
    下的状态值，且对于所有可能的状态都成立，那么意味着 ![](img/Formula_05_036.png) 和 ![](img/Formula_05_037.png)
    一样好。如果对于任何状态 ![](img/Formula_05_028.png)，这种关系是严格不等式，那么 ![](img/Formula_05_039.png)
    就是比 ![](img/Formula_05_040.png) 更好的策略。这应该是直观的，因为状态值表示从该点开始的期望累积奖励。
- en: 'Now, the question is how we go from ![](img/Formula_05_037.png) to a better
    policy ![](img/Formula_05_030.png). For that, we need to recall the action-value
    function we defined in [*Chapter 4*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080),
    *Making of the Markov Decision Process*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是我们如何从 ![](img/Formula_05_037.png) 过渡到更好的策略 ![](img/Formula_05_030.png)。为此，我们需要回顾在
    [*第4章*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080) 中定义的动作价值函数，*马尔科夫决策过程的构建*：
- en: '![](img/Formula_05_043.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_043.jpg)'
- en: 'Remember that the definition of the action-value function is a bit nuanced.
    It is the expected cumulative future reward when:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，动作价值函数的定义有些细微之处。它是当以下情况发生时的期望累积未来奖励：
- en: Action ![](img/Formula_05_044.png) is taken at the current state ![](img/Formula_05_045.png).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前状态 ![](img/Formula_05_045.png) 执行动作 ![](img/Formula_05_044.png)。
- en: Then the policy ![](img/Formula_05_046.png) is followed.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后遵循该策略 ![](img/Formula_05_046.png)。
- en: The nuance is that policy ![](img/Formula_05_047.png) may normally suggest another
    action when in state ![](img/Formula_05_048.png). The q-value represents a one-time
    deviation from policy ![](img/Formula_05_035.png) that happens in the current
    step.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中的细微之处是，策略 ![](img/Formula_05_047.png) 在状态 ![](img/Formula_05_048.png) 时，通常可能会建议另一个动作。q值表示当前步骤中从策略
    ![](img/Formula_05_035.png) 偏离的即时偏差。
- en: How does this help with improving the policy though? The **policy improvement
    theorem** suggests that *if it is better to select* ![](img/Formula_05_050.png)
    *initially when in state* ![](img/Formula_05_051.png) *and then follow* ![](img/Formula_05_052.png)
    *rather than following* ![](img/Formula_05_053.png) *all along, selecting* ![](img/Formula_05_050.png)
    *every time when in state* ![](img/Formula_05_055.png) *is a better policy than*
    ![](img/Formula_05_053.png). In other words, if ![](img/Formula_05_057.png), then
    we can improve ![](img/Formula_05_037.png) by taking action ![](img/Formula_05_059.png)
    when in state ![](img/Formula_05_060.png) and following ![](img/Formula_05_061.png)
    for the rest of the states. We don't include it here but the proof of this theorem
    is actually quite intuitive, and it is available *Sutton & Barto, 2018*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这对改进策略有什么帮助呢？**策略改进定理**表明，*如果在状态* ![](img/Formula_05_051.png) *时选择* ![](img/Formula_05_050.png)
    *初始动作比一直选择* ![](img/Formula_05_053.png) *更好，则在状态* ![](img/Formula_05_055.png)
    *时，每次选择* ![](img/Formula_05_050.png) *而不是一直跟随* ![](img/Formula_05_053.png) *将是一个比*
    ![](img/Formula_05_053.png) *更好的策略。换句话说，如果* ![](img/Formula_05_057.png)，*那么我们可以通过在状态*
    ![](img/Formula_05_060.png) *采取动作* ![](img/Formula_05_059.png) *并在其余状态下跟随* ![](img/Formula_05_061.png)
    *来改进* ![](img/Formula_05_037.png)。我们这里不包括该定理的证明，但其实它是非常直观的，并且可以参考 *Sutton & Barto,
    2018*。
- en: 'Let''s generalize this argument. Say some policy ![](img/Formula_05_030.png)
    is at least as good as another policy ![](img/Formula_05_061.png) if, for all
    ![](img/Formula_05_064.png), the following holds:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对这个论点进行一般化。假设某个策略 ![](img/Formula_05_030.png) 至少和另一个策略 ![](img/Formula_05_061.png)
    一样好，如果对于所有 ![](img/Formula_05_064.png)，以下条件成立：
- en: '![](img/Formula_05_065.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_065.jpg)'
- en: Then, all we need to do to improve a policy is to choose actions that maximize
    the respective q-values for each state. Namely,
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要做的就是选择能够最大化每个状态的q值的动作来改进策略。即，
- en: '![](img/Formula_05_066.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_066.jpg)'
- en: 'One last note before we close this discussion: Although we described the policy
    improvement method for deterministic policies, only a single action is suggested
    by the policies for a given state ![](img/Formula_05_067.png), the method holds
    for stochastic policies as well.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这个讨论之前，有一个最后的说明：尽管我们描述了确定性策略的策略改进方法，对于给定状态，策略只建议一个单一动作 ![](img/Formula_05_067.png)，但这个方法同样适用于随机策略。
- en: So far, so good! Now, let's turn this policy improvement into an algorithm that
    will allow us to find optimal policies!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利！现在，让我们将这个策略改进转化为一个算法，帮助我们找到最优策略！
- en: The policy iteration algorithm
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略迭代算法
- en: 'The policy iteration algorithm simply includes starting with an arbitrary policy,
    followed by a policy evaluation step, and then with a policy improvement step.
    This procedure, when repeated, eventually leads to an optimal policy. This process
    is depicted in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代算法的基本过程是：从任意策略开始，进行策略评估步骤，然后进行策略改进步骤。当这个过程重复进行时，最终会得到一个最优策略。这个过程在下面的图中得到了展示：
- en: '![](img/B14160_05_1.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_1.jpg)'
- en: Figure 5.1 – Generalized policy iteration
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 广义策略迭代
- en: Actually, iterating between some forms of policy evaluation and policy improvement
    steps is a general recipe for solving RL problems. That is why this idea is named
    **generalized policy iteration (GPI)** *Sutton & Barto, 2018*. It is just that
    the policy iteration method we describe in this section involves specific forms
    of these steps.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在某些形式的策略评估和策略改进步骤之间迭代，是解决强化学习问题的一种通用方法。这就是为什么这个想法被称为**广义策略迭代（GPI）** *Sutton
    & Barto, 2018*。只不过我们在这一节描述的策略迭代方法涉及这些步骤的特定形式。
- en: Let's implement a policy iteration for the food truck environment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为餐车环境实现一个策略迭代。
- en: Implementing a policy iteration for the inventory replenishment problem
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为库存补货问题实现策略迭代
- en: 'We have already coded the policy evaluation and expected update steps. What
    we need additionally for the policy iteration algorithm is the policy improvement
    step, and then we can obtain an optimal policy! This is exciting, so, let''s dive
    right in:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经编写了策略评估和期望更新步骤。我们为策略迭代算法所需要的附加步骤是策略改进步骤，完成后我们就能得到最优策略！这真令人兴奋，让我们开始吧：
- en: 'Let''s start by implementing the policy improvement as we described above:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从实现上述描述的策略改进开始：
- en: '[PRE18]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This function searches for the action that gives the maximum q-value for a given
    state using the value functions obtained under the current policy. For the terminal
    states, the q-value is always equal to 0.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数根据当前策略下得到的值函数，搜索在给定状态下产生最大q值的动作。对于终止状态，q值始终为0。
- en: 'Now, we put everything together in a policy iteration algorithm:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将所有内容整合到一个策略迭代算法中：
- en: '[PRE19]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This algorithm starts with a random policy, and in each iteration, implements
    the policy evaluation and improvement steps. It stops when the policy becomes
    stable.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该算法从一个随机策略开始，在每次迭代中实施策略评估和改进步骤。当策略稳定时，它停止。
- en: And the big moment! Let's find out the optimal policy for our food truck and
    see what the expected weekly profit is!
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终于迎来了关键时刻！让我们找出食品车的最优策略，并看看预期的周利润是多少！
- en: '[PRE20]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result should look like the following:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果应如下所示：
- en: '[PRE21]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We have just figured out a policy that gives an expected weekly profit of $2,880\.
    This is a significant improvement over the base policy! Well, thanks for supporting
    your local business!
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚找到了一个策略，它的预期周利润为$2,880。这比基础策略有了显著的提升！感谢您支持本地企业！
- en: You can see from the output that there were two policy improvement steps over
    the random policy. The third policy improvement step did not lead to any changes
    in the policy and the algorithm terminated.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果可以看出，相较于随机策略，进行了两次策略改进。第三次策略改进未导致策略的任何变化，算法因此终止。
- en: 'Let''s see what the optimal policy looks like:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最优策略是什么样的：
- en: '![](img/B14160_05_2.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_2.jpg)'
- en: Figure 5.2 – Optimal policy for the food truck example
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 食品车示例的最优策略
- en: 'What the policy iteration algorithm comes up with is quite intuitive. Let''s
    analyze this policy for a moment:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代算法得出的结果非常直观。让我们分析一下这个策略：
- en: On Monday and Tuesday, it is guaranteed that 400 burgers will be sold in the
    remainder of the week. Since the patties can be safely stored during the weekdays,
    it makes sense to fill the inventory up to capacity.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在周一和周二，保证在剩余的时间内将售出400个汉堡。由于肉饼可以在工作日安全存储，因此将库存填充至最大容量是有意义的。
- en: At the beginning of Wednesday, there is a chance that the total number of sales
    will be 300 till the end of the week and 100 patties will be spoiled. However,
    this is a rather small likelihood and the expected profit is still positive.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在周三开始时，可能会发生直到本周末销售总量为300，并且100个肉饼会变质的情况。然而，这种情况的可能性较小，预期利润仍然为正。
- en: For Thursday and Friday, it makes more sense to be more conservative and de-risk
    costly spoilage in case the demand is less than the inventory.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于周四和周五，更保守一些是更明智的，以防需求少于库存，从而避免昂贵的变质损失。
- en: Congratulations! You have successfully solved an MDP using policy iteration!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功地使用策略迭代解决了一个MDP问题！
- en: Tip
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The optimal policy, we have found, heavily depends on the cost of patties, the
    net revenue per unit, as well as the demand distribution. You can gain more intuition
    on the structure of the optimal policy and how it changes by modifying the problem
    parameters and solving it again.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现的最优策略，严重依赖于肉饼的成本、每单位的净收入以及需求分布。通过修改问题参数并再次求解，你可以更直观地理解最优策略的结构以及它如何变化。
- en: You have come a long way! We built an exact solution method starting with the
    fundamentals of MDP and DP. Next, we will look into another algorithm that is
    often more efficient than policy iteration.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经走了很长一段路！我们从MDP和DP的基础开始构建了一个精确的求解方法。接下来，我们将研究另一种通常比策略迭代更高效的算法。
- en: Value iteration
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'Policy iteration requires us to fully evaluate the policies until the state
    values converge before we perform an improvement step. In more sophisticated problems,
    it could be quite costly to wait for a complete evaluation. Even in our example,
    a single policy evaluation step took 5-6 sweeps over all states until it converged.
    It turns out that we can get away with terminating the policy evaluation before
    it converges without losing the convergence guarantee of the policy iteration.
    In fact, we can even combine policy iteration and policy improvement into a single
    step by turning the Bellman optimality equation we introduced in the previous
    chapter into an update rule:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代要求我们完全评估所有策略，直到状态值收敛，然后才进行改进步骤。在更复杂的问题中，等待完成评估可能会非常耗费计算资源。即使在我们的例子中，单次策略评估步骤也需要对所有状态进行5-6次遍历才能收敛。事实证明，我们可以在策略评估未收敛时提前终止，而不会失去策略迭代的收敛保证。实际上，我们甚至可以通过将上一章中介绍的贝尔曼最优方程转化为更新规则，将策略迭代和策略改进合并为一个步骤：
- en: '![](img/Formula_05_068.jpg)![](img/Formula_05_069.jpg)![](img/Formula_05_070.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_068.jpg)![](img/Formula_05_069.jpg)![](img/Formula_05_070.jpg)'
- en: We simply perform the update for all states again and again until the state
    values converge. This algorithm is called **value iteration**.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断对所有状态执行更新，直到状态值收敛为止。这个算法称为**价值迭代**。
- en: Tip
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Notice the difference between a policy evaluation update and a value iteration
    update. The former selects the actions from a given policy, hence the ![](img/Formula_05_071.png)
    term in front of the expected update. The latter, on the other hand, does not
    follow a policy but actively searches for the best actions through the ![](img/Formula_05_072.png)
    operator.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意策略评估更新和价值迭代更新之间的区别。前者从给定的策略中选择动作，因此在期望更新前有![](img/Formula_05_071.png)项。而后者则不遵循策略，而是通过![](img/Formula_05_072.png)运算符主动搜索最佳动作。
- en: This is all we need to implement the value iteration algorithm. So, let's dive
    right into the implementation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是实现价值迭代算法所需要的全部内容。接下来，让我们直接进入实现部分。
- en: Implementing the value iteration for the inventory replenishment problem
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现库存补充问题的价值迭代
- en: To implement value iteration, we'll use the `policy_improvement` function we
    defined earlier. However, after improving the policy for each state, we'll also
    update the state-value estimate of the state.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现价值迭代，我们将使用之前定义的`policy_improvement`函数。然而，在改进每个状态的策略后，我们还将更新该状态的状态值估计。
- en: 'Now, we can go ahead and implement the value iteration using the following
    steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按照以下步骤实现价值迭代：
- en: 'We first define the value iteration function as defined above with in-place
    replacement of the state values:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义如上所述的价值迭代函数，通过原地替换状态值来实现：
- en: '[PRE22]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we execute the value iteration and observe the value of the initial state:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们执行价值迭代并观察初始状态的值：
- en: '[PRE23]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The result should look like the following:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果应如下所示：
- en: '[PRE24]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Value iteration gives the optimal policy, but with less computational effort
    compared to the policy iteration algorithm! It took only a total of 6 sweeps over
    the state space with the value iteration, while the policy iteration arrived at
    the same optimal policy after 20 sweeps (17 for policy evaluation and 3 for policy
    improvement).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 价值迭代提供了最优策略，但相比于策略迭代算法，它的计算量更小！价值迭代只对状态空间进行6次遍历，而策略迭代则需要20次遍历（17次用于策略评估，3次用于策略改进）。
- en: Now, remember our discussion around generalized policy improvement. You can,
    in fact, combine the policy improvement step with a truncated policy evaluation
    step, which, in some sophisticated examples where the state values change significantly
    after the policy improvement, converge faster than both policy iteration and value
    iteration algorithms.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，记住我们关于广义策略改进的讨论。实际上，你可以将策略改进步骤与截断的策略评估步骤结合起来，在一些复杂的示例中，当策略改进后状态值发生显著变化时，收敛速度比策略迭代和价值迭代算法都要快。
- en: Great work! We have covered how to solve the prediction problem for MDPs using
    DP and then two algorithms to find optimal policies. And they worked great for
    our simple example. On the other hand, DP methods suffer from two important drawbacks
    in practice. Let's discuss what those are next, and why we need the other approaches
    that we will introduce later in the chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们已经涵盖了如何使用动态规划解决MDP的预测问题，以及找到最优策略的两种算法，并且它们在我们的简单示例中表现出色。另一方面，动态规划方法在实践中有两个重要的缺点。接下来我们将讨论这些缺点，以及为什么我们需要在本章后续介绍的其他方法。
- en: Drawbacks of dynamic programming
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态规划的缺点
- en: DP methods are great to learn to get a solid grasp of how MDPs can be solved.
    They are also much more efficient compared to direct search algorithms or linear
    programming methods. On the other hand, in practice, these algorithms are still
    either intractable or impossible to use. Let's elaborate on why.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划方法非常适合学习如何扎实掌握MDP的解决方法。与直接搜索算法或线性规划方法相比，它们效率更高。另一方面，在实际应用中，这些算法仍然要么不可行，要么无法使用。接下来我们将详细说明为什么。
- en: The curse of dimensionality
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度灾难
- en: Both the policy iteration and the value iteration algorithms iterate over the
    entire state space, multiple times, until they arrive at an optimal policy. We
    also store the policy, the state values, and the action values for each state
    in a tabular form. Any realistic problem, on the other hand, would have a gigantic
    number of possible states, explained by a phenomenon called the **curse of dimensionality**.
    This refers to the fact that the possible number of values of a variable (states)
    grows exponentially as we add more dimensions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代和价值迭代算法都会多次遍历整个状态空间，直到找到最优策略。同时，我们还以表格形式存储每个状态的策略、状态值和动作值。然而，任何实际问题都会有一个巨大的可能状态数，这一现象被称为**维度灾难**。这指的是，随着我们增加维度，变量（状态）的可能值数呈指数增长。
- en: Consider our food truck example. In addition to keeping track of patties, let's
    assume we also keep track of burger buns, tomatoes, and onions. Also assume that
    the capacity for each of these items is 400, and we have a precise count of the
    inventory. The possible number of states in this case would be ![](img/Formula_05_073.png),
    that is, greater than ![](img/Formula_05_074.png) This is a ridiculous number
    of states to keep track of for such a simple problem.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的餐车示例为例。除了跟踪肉饼，我们还假设同时跟踪汉堡包胚、番茄和洋葱。还假设这些物品的每个容量是400，并且我们精确地统计了库存数量。在这种情况下，可能的状态数量将是![](img/Formula_05_073.png)，即大于![](img/Formula_05_074.png)。这是一个荒谬的状态数量，要为这么一个简单的问题跟踪这么多状态。
- en: 'One mitigation for the curse of dimensionality is **asynchronous dynamic programming**:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 解决维度灾难的一个方法是**异步动态规划**：
- en: This approach suggests not sweeping over the entire state space in each iteration
    of policy improvement but focusing on the states that are more likely to be encountered.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种方法建议在每次策略改进迭代中，避免遍历整个状态空间，而是集中在更有可能被遇到的状态上。
- en: For many problems, not all parts of the state space are of equal importance.
    Therefore, it is wasteful to wait for a complete sweep of the state space before
    there is an update to the policy.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于许多问题，状态空间的各个部分并不是同等重要的。因此，在对整个状态空间进行完整遍历之前等待更新策略是浪费的。
- en: With an asynchronous algorithm, we can simulate the environment in parallel
    to the policy improvement, observe which states are visited, and update the policy
    and the value functions for those states.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异步算法，我们可以在进行策略改进的同时并行模拟环境，观察哪些状态被访问，并更新这些状态的策略和值函数。
- en: At the same time, we can pass the updated policy to the agent so the simulation
    would continue with the new policy.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，我们可以将更新后的策略传递给代理，这样模拟将继续以新的策略进行。
- en: Given that the agent sufficiently explores the state space, the algorithm would
    converge to an optimal solution eventually.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假设代理足够探索了状态空间，算法最终会收敛到最优解。
- en: A more important tool that we use to address this problem, on the other hand,
    is **function approximators**, such as deep neural networks. Think about it! What
    is the benefit in storing a separate policy/state-value/action-value for the inventory
    levels 135, 136, 137? Not much, really. Function approximators represent what
    we would like to learn in a much more compact manner (although approximately)
    compared to a tabular representation. In fact, in many cases, deep neural networks
    are only a meaningful choice for function approximation due to their representation
    power. That is why, starting from the next chapter, we will exclusively focus
    on deep RL algorithms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们用来解决这个问题的一个更重要的工具是**函数逼近器**，比如深度神经网络。想一想！如果我们为库存水平135、136、137分别存储一个单独的策略/状态值/动作值，那有什么好处呢？其实并没有什么太大意义。与表格表示法相比，函数逼近器以一种更紧凑（尽管是近似的）方式表示我们想要学习的内容。事实上，在许多情况下，深度神经网络之所以成为函数逼近的一个有意义的选择，是因为它们具有强大的表示能力。这也是为什么从下一章开始，我们将专注于深度强化学习算法的原因。
- en: The need for a complete model of the environment
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对环境完整模型的需求
- en: In the methods we have used so far, we have relied on the transition probabilities
    of the environment in our policy evaluation, policy iteration, and value iteration
    algorithms to obtain optimal policies. This is a luxury that we usually don't
    have in practice. It is either these probabilities are very difficult to calculate
    for each possible transition (which is often impossible to even enumerate), or
    we simply don't know them. You know what is much easier to obtain? A sample trajectory
    of transitions, either from the environment itself or from its **simulation**.
    In fact, simulation is a particularly important component in RL, as we will discuss
    separately towards the end of this chapter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们目前使用的方法中，我们依赖于环境的状态转移概率来进行策略评估、策略迭代和价值迭代算法，从而获得最优策略。然而，这种方法在实际中通常并不可行。通常要么这些概率对于每一个可能的转移计算起来非常困难（甚至往往无法列举出来），要么我们根本不知道这些概率。你知道什么更容易获得吗？一个从环境本身或其**模拟**中获得的样本轨迹。事实上，模拟是强化学习中一个特别重要的组成部分，我们将在本章末尾单独讨论。
- en: Then the question becomes how we use sample trajectories to learn near-optimal
    policies. Well, this is exactly what we'll cover next in the rest of this chapter
    with Monte Carlo and TD methods. The concepts you will learn are at the center
    of many of the advanced RL algorithms.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 那么问题变成了如何使用样本轨迹来学习近似最优策略。实际上，这正是我们将在本章剩余部分通过蒙特卡洛（Monte Carlo）和时序差分（TD）方法讲解的内容。你将学到的这些概念是许多高级强化学习（RL）算法的核心。
- en: Training your agent with Monte Carlo methods
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛方法训练你的代理
- en: 'Let''s say you would like to learn the chance of flipping heads with a particular,
    possibly biased, coin:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要学习一个特定的、可能存在偏差的硬币翻转正面朝上的概率：
- en: One way of calculating this is through a careful analysis of the physical properties
    of the coin. Although this could give you the precise probability distribution
    of the outcomes, it is far from being a practical approach.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算这个概率的一种方法是通过仔细分析硬币的物理属性。尽管这样做能够给出结果的精确概率分布，但这远不是一种实用的方法。
- en: Alternatively, you can just flip the coin many times and look at the distribution
    in your sample. Your estimate could be a bit off if you don't have a large sample,
    but it will do the job for most practical purposes. The math you need to deal
    with using the latter method will be incomparably simpler.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，你可以多次投掷硬币，查看样本中的分布。如果你的样本量不大，估计值可能会有些偏差，但对于大多数实际应用来说，这已经足够了。使用后一种方法时需要处理的数学问题将简单得多。
- en: 'Just like in the coin example, we can estimate the state values and action
    values in an MDP from random samples. **Monte Carlo (MC)** estimation is a general
    concept that refers to making estimations through repeated random sampling. In
    the context of RL, it refers to *a collection of methods that estimates state
    values and action values using sample trajectories of complete episodes*. Using
    random samples is incredibly convenient, and in fact essential, for any realistic
    RL problem, because the environment dynamics (state transition and reward probability
    distributions) are often either of the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 就像硬币示例中一样，我们可以通过随机样本估计MDP中的状态值和动作值。**蒙特卡洛（MC）**估计是一种通用概念，指的是通过重复的随机采样进行估计。在强化学习的背景下，它指的是*通过完整回合的样本轨迹估计状态值和动作值的方法集合*。使用随机样本非常方便，事实上，对于任何实际的RL问题来说都是至关重要的，因为环境动态（状态转移和奖励概率分布）通常具有以下特点：
- en: Too complex to deal with
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过于复杂，无法处理
- en: Not known in the first place
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始时未知
- en: Monte Carlo methods, therefore, are powerful methods that allow an RL agent
    to learn optimal policies only from the experience it collects through interacting
    with its environment, without knowing how the environment works.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，蒙特卡罗方法是强大的方法，可以让强化学习代理仅通过与环境交互收集的经验来学习最优策略，而无需了解环境是如何工作的。
- en: In this section, we'll first look into estimating the state values and the action
    values for a given policy with MC methods. Then, we'll cover how to make improvements
    to obtain optimal policies.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将研究如何使用蒙特卡罗方法估计给定策略的状态值和动作值。然后，我们将介绍如何进行改进以获得最优策略。
- en: Monte Carlo prediction
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡罗预测
- en: As in the DP methods, we need to be able to evaluate a given policy ![](img/Formula_05_075.png)
    to be able to improve it. In this section, we'll cover how to evaluate a policy
    by estimating the corresponding state and action values. In doing so, we'll briefly
    revisit the grid world example from the previous chapter and then go into the
    food truck inventory replenishment problem.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与DP方法一样，我们需要能够评估给定策略！[](img/Formula_05_075.png)，才能改进它。在本节中，我们将介绍如何通过估计相应的状态和动作值来评估策略。在此过程中，我们将简要回顾上一章的网格世界示例，并进一步探讨食品卡车库存补充问题。
- en: Estimating the state-value function
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估计状态值函数
- en: 'Remember that the value of a state ![](img/Formula_05_010.png) under policy
    ![](img/Formula_05_061.png), ![](img/Formula_05_078.png) is defined as the expected
    cumulative reward when started in state ![](img/Formula_05_079.png):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在策略！[](img/Formula_05_061.png)下，状态！[](img/Formula_05_010.png)的值，！[](img/Formula_05_078.png)定义为从状态！[](img/Formula_05_079.png)开始时的期望累计奖励：
- en: '![](img/Formula_05_080.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_080.jpg)'
- en: MC prediction suggests simply observing (many) sample **trajectories**, sequences
    of state-action-reward tuples, starting in ![](img/Formula_05_081.png), to estimate
    this expectation. This is similar to flipping a coin to estimate its distribution
    from a sample.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: MC预测建议通过观察（多个）样本**轨迹**，即一系列的状态-动作-奖励元组，从状态！[](img/Formula_05_081.png)开始，以估计这种期望值。这类似于抛硬币来估计其分布。
- en: It is best to explain Monte Carlo methods with an example. In particular, it
    will be quite intuitive to see how it works in the grid world example, so let's
    revisit that next.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最好通过一个例子来解释蒙特卡罗方法。特别是在网格世界的例子中，它的工作原理非常直观，因此我们接下来将再次回顾它。
- en: Using sample trajectories for state value estimation
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用样本轨迹进行状态值估计
- en: Recall that the robot in the grid world receives +1 reward for every move as
    long as it does not crash. When it does crash into a wall, the episode ends. Assume
    that this robot can be controlled only to a certain extent. When instructed to
    go in a particular direction, there is a 70% chance of it following the command.
    There is a 10% chance of the robot going in each of the other three directions.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，网格世界中的机器人每次移动都能获得+1的奖励，只要它不撞墙。当它撞到墙壁时，该回合结束。假设这个机器人只能在一定程度上被控制。当指示它朝某个特定方向移动时，它有70%的概率会按命令执行。其余三个方向的概率各为10%。
- en: 'Consider a deterministic policy ![](img/Formula_05_061.png) illustrated in
    * Figure 5.3 (a)*. If the robot starts in state (1,2), two example trajectories
    it can follow, ![](img/Formula_05_083.png) and ![](img/Formula_05_084.png), and
    the corresponding probabilities of making each of the transitions are shown in
    * Figure 5.3 (b)*:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个确定性策略！[](img/Formula_05_061.png)，如*图5.3 (a)*所示。如果机器人从状态(1,2)开始，它可以遵循的两个示例轨迹，！[](img/Formula_05_083.png)和！[](img/Formula_05_084.png)，以及每个转移的相应概率如*图5.3
    (b)*所示：
- en: '![](img/B14160_05_3.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_3.jpg)'
- en: Figure 5.3 – a) A deterministic policy π, b) Two sample trajectories under π
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – a) 一个确定性策略π，b) 在π下的两个样本轨迹
- en: Tip
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Notice that the robot follows a random trajectory but the policy itself is deterministic,
    which means the action taken (the command sent to the robot) in a given state
    is always the same. The randomness comes from the environment due to probabilistic
    state transitions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，机器人遵循的是随机轨迹，但策略本身是确定性的，这意味着在给定状态下采取的动作（发给机器人的命令）始终是相同的。随机性来自环境，由于概率性的状态转移。
- en: 'For the trajectory ![](img/Formula_05_085.png), the probability of observing
    it and the corresponding discounted return are as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 对于轨迹！[](img/Formula_05_085.png)，观察到该轨迹的概率和相应的折扣回报如下：
- en: '![](img/Formula_05_086.jpg)![](img/Formula_05_087.jpg)![](img/Formula_05_088.jpg)![](img/Formula_05_089.jpg)![](img/Formula_05_090.jpg)![](img/Formula_05_091.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_086.jpg)![](img/Formula_05_087.jpg)![](img/Formula_05_088.jpg)![](img/Formula_05_089.jpg)![](img/Formula_05_090.jpg)![](img/Formula_05_091.jpg)'
- en: 'And for ![](img/Formula_05_092.png):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 那对于 ![](img/Formula_05_092.png) 来说：
- en: '![](img/Formula_05_093.jpg)![](img/Formula_05_094.jpg)![](img/Formula_05_095.jpg)![](img/Formula_05_096.jpg)![](img/Formula_05_097.jpg)![](img/Formula_05_098.jpg)![](img/Formula_05_099.jpg)![](img/Formula_05_100.jpg)![](img/Formula_05_101.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_093.jpg)![](img/Formula_05_094.jpg)![](img/Formula_05_095.jpg)![](img/Formula_05_096.jpg)![](img/Formula_05_097.jpg)![](img/Formula_05_098.jpg)![](img/Formula_05_099.jpg)![](img/Formula_05_100.jpg)![](img/Formula_05_101.jpg)'
- en: 'For these two example trajectories, we were able to calculate the corresponding
    probabilities and the returns. To calculate the state-value of ![](img/Formula_05_102.png),
    though, we need to evaluate the following expression:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个示例轨迹，我们能够计算出相应的概率和回报。然而，要计算 ![](img/Formula_05_102.png) 的状态值，我们需要评估以下表达式：
- en: '![](img/Formula_05_103.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_103.jpg)'
- en: 'That means we need to identify the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要识别以下内容：
- en: Every single possible trajectory ![](img/Formula_05_104.png) that can originate
    from ![](img/Formula_05_105.png) under policy ![](img/Formula_05_106.png)
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一条可能的轨迹 ![](img/Formula_05_104.png)，它可以从 ![](img/Formula_05_105.png) 出发，依据策略
    ![](img/Formula_05_106.png)
- en: The probability of observing ![](img/Formula_05_107.png) under ![](img/Formula_05_108.png),
    ![](img/Formula_05_109.png)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 ![](img/Formula_05_107.png) 下观察到 ![](img/Formula_05_108.png)，![](img/Formula_05_109.png)
    的概率
- en: The corresponding discounted return, ![](img/Formula_05_110.png)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相应的折扣回报，![](img/Formula_05_110.png)
- en: Well, that is an impossible task. Even in this simple problem, there is an infinite
    number of possible trajectories.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，那是一个不可能完成的任务。即便是在这个简单的问题中，可能的轨迹数也是无限的。
- en: 'This is exactly where Monte Carlo prediction comes in. It simply tells us to
    estimate the value of state ![](img/Formula_05_111.png) by averaging the sample
    returns as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是蒙特卡洛预测的作用所在。它简单地告诉我们通过如下方式，利用样本回报的平均值来估计状态 ![](img/Formula_05_111.png) 的值：
- en: '![](img/Formula_05_112.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_112.jpg)'
- en: That's it! Sample trajectories and returns are all you need to estimate the
    value of the state.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！样本轨迹和回报就是你估计状态值所需的全部。
- en: Tip
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that ![](img/Formula_05_021.png) denotes the true value of the state, whereas
    ![](img/Formula_05_114.png) denotes an estimate.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![](img/Formula_05_021.png) 表示状态的真实值，而 ![](img/Formula_05_114.png) 表示估计值。
- en: 'At this point, you may be asking the following questions:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会问以下问题：
- en: '*How come it''s enough to have two sample returns to estimate a quantity that
    is the outcome of an infinite number of trajectories?* It is not. The more sample
    trajectories you have, the more accurate your estimate is.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*怎么会仅凭两个样本回报就能估计出一个结果，这个结果是无限多个轨迹的结果？* 其实并不行。你有的样本轨迹越多，你的估计就越准确。'
- en: '*How do we know we have enough sample trajectories?* That is hard to quantify.
    But more complex environments, especially when there is a significant degree of
    randomness, would require more samples for an accurate estimation. It is a good
    idea, though, to check if the estimate is converging as you add more trajectory
    samples.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们怎么知道我们有足够的样本轨迹？* 这个问题很难量化。但在更复杂的环境中，尤其是当存在显著的随机性时，可能需要更多的样本才能做出准确的估计。不过一个好主意是，在增加更多轨迹样本时，检查估计值是否趋于收敛。'
- en: '![](img/Formula_05_115.png) *and* ![](img/Formula_05_116.png) *have very different
    likelihoods of occurring. Is it appropriate to assign them equal weights in the
    estimation?* This is indeed problematic when we have only two trajectories in
    the sample. However, as we sample more trajectories, we can expect to observe
    the trajectories in the sample occurring proportionally to their true probabilities
    of occurrence.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_05_115.png) *和* ![](img/Formula_05_116.png) *发生的概率是非常不同的。将它们在估计中赋予相等的权重合适吗？*
    当我们只有两个轨迹样本时，这确实是一个问题。然而，随着样本轨迹的增多，我们可以期待样本中轨迹的出现与它们真实发生的概率成比例。'
- en: '*Can we use the same trajectory in estimating the values of the other states
    it visits?* Yes! Indeed, that is what we will do in Monte Carlo prediction.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*我们能否使用相同的轨迹来估计它访问的其他状态的值？* 是的！实际上，这就是我们在蒙特卡洛预测中所做的事情。'
- en: Let's elaborate on how we can use the same trajectory in estimating the values
    of different states next.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来详细讨论如何使用相同的轨迹来估计不同状态的值。
- en: First-visit versus every-visit Monte Carlo prediction
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 首次访问与每次访问的蒙特卡洛预测
- en: 'If you recall what the Markov property is, it simply tells us the future depends
    on the current state, not the past. Therefore, we can think of, for example, ![](img/Formula_05_117.png)
    as three separate trajectories that originate from states ![](img/Formula_05_118.png),
    and ![](img/Formula_05_119.png). Let''s call the latter two trajectories ![](img/Formula_05_120.png)
    and ![](img/Formula_05_121.png). So, we can obtain a value estimate for all the
    states visited by the trajectories in our sample set. For instance, the value
    estimate for state ![](img/Formula_05_122.png) would be as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下什么是马尔可夫性质，它简单地告诉我们，未来依赖于当前状态，而不是过去。因此，我们可以把例如 ![](img/Formula_05_117.png)
    看作是从状态 ![](img/Formula_05_118.png) 和 ![](img/Formula_05_119.png) 出发的三个独立轨迹。我们称后两个轨迹为
    ![](img/Formula_05_120.png) 和 ![](img/Formula_05_121.png)。因此，我们可以为样本集中的所有访问过的状态获得一个值估计。例如，状态
    ![](img/Formula_05_122.png) 的值估计如下：
- en: '![](img/Formula_05_123.jpg)![](img/Formula_05_124.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_123.jpg)![](img/Formula_05_124.jpg)'
- en: Since there is no other trajectory visiting state ![](img/Formula_05_125.png),
    we used a single return to estimate the state-value. Notice the discounts are
    applied to the rewards according to their time distance to the initial time-step.
    That is why the exponents of the ![](img/Formula_05_126.png) discount reduced
    by one.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有其他轨迹访问状态 ![](img/Formula_05_125.png)，我们使用单一回报来估计状态值。注意，折现因子会根据回报与初始时间步的时间距离应用。这就是为什么
    ![](img/Formula_05_126.png) 折现因子的指数减少了一。
- en: 'Consider the following set of trajectories in *Figure 5.4* and assume we again
    want to estimate ![](img/Formula_05_127.png). None of the sample trajectories
    actually originate from state ![](img/Formula_05_128.png) but that is totally
    fine. We can use trajectories ![](img/Formula_05_129.png), ![](img/Formula_05_130.png),
    and ![](img/Formula_05_131.png) for the estimation. But then there is an interesting
    case here: ![](img/Formula_05_132.png) visits state ![](img/Formula_05_133.png)
    twice. Should we use the return only from its first visit or from each of its
    visits?'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑图*5.4*中的一组轨迹，并假设我们再次想估计 ![](img/Formula_05_127.png)。这些样本轨迹中没有任何轨迹真正从状态 ![](img/Formula_05_128.png)
    出发，但这完全没问题。我们可以使用轨迹 ![](img/Formula_05_129.png)、![](img/Formula_05_130.png) 和
    ![](img/Formula_05_131.png) 来进行估计。但这里有一个有趣的情况：![](img/Formula_05_132.png) 访问了状态
    ![](img/Formula_05_133.png) 两次。我们应该仅使用它第一次访问的回报，还是每次访问的回报？
- en: '![](img/B14160_05_4.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_4.jpg)'
- en: Figure 5.4 – Monte Carlo estimation of ![](img/Formula_05_134.png)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 蒙特卡洛估计 ![](img/Formula_05_134.png)
- en: 'Both these approaches are valid. The former method is called **first-visit
    MC method** and the latter is called **every-visit MC method**. They compare to
    each other as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都是有效的。前者被称为**首次访问MC方法**，后者被称为**每次访问MC方法**。它们的比较如下：
- en: Both converge to true ![](img/Formula_05_135.png) as the number of visits approaches
    infinity.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着访问次数趋近于无限，它们都收敛到真实的 ![](img/Formula_05_135.png)。
- en: The first-visit MC method gives an unbiased estimate of the state-value whereas
    the every-visit MC method is biased.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首次访问MC方法给出了状态值的无偏估计，而每次访问MC方法则是有偏的。
- en: The **mean squared error** (**MSE**) of the first-visit MC method is higher
    with fewer samples but lower than every-visit MSE with more samples.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差** (**MSE**) 在首次访问MC方法中样本较少时较高，但在样本较多时低于每次访问的MSE。'
- en: The every-visit MC method is more natural to use with function approximations.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次访问的MC方法在使用函数逼近时更为自然。
- en: 'If it sounds complicated, it is actually not! Just remember these things:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果听起来很复杂，其实并不复杂！只需记住以下几点：
- en: We are trying to estimate a parameter, such as the state-value of ![](img/Formula_05_136.png),
    ![](img/Formula_05_137.png).
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们尝试估计一个参数，例如 ![](img/Formula_05_136.png)、![](img/Formula_05_137.png) 的状态值。
- en: By observing the random variable ![](img/Formula_05_138.png) , the discounted
    value returns starting from ![](img/Formula_05_139.png), many times.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过观察随机变量 ![](img/Formula_05_138.png) ，从 ![](img/Formula_05_139.png) 开始，折现值返回多次。
- en: It is both valid to consider trajectories from their first visit of ![](img/Formula_05_140.png)
    on, or from, their every visit.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑从它们首次访问 ![](img/Formula_05_140.png) 开始的轨迹，或者从每次访问开始的轨迹，都是有效的。
- en: Now it is time to implement Monte Carlo prediction next!
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现蒙特卡洛预测了！
- en: Implementing first-visit Monte Carlo estimation of the state values
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现首次访问的蒙特卡洛状态值估计
- en: We have used the grid world example to get a visual intuition, and now let's
    go back to our food truck example for the implementation. Here, we will implement
    the first-visit MC method, but the implementation can be easily modified to every-visit
    MC. This can be done by removing the condition that calculates the return only
    if the state does not appear in the trajectory up to that point. For the food
    truck example, since a trajectory will never visit the same state – because the
    day of the week, which is part of the state, changes after every transition –
    both methods are identical.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了网格世界示例来获得直观的理解，现在让我们回到我们的餐车示例来进行实现。在这里，我们将实现首次访问蒙特卡洛（MC）方法，但这个实现可以很容易地修改为每次访问MC。只需要去掉在计算回报时的条件，即只有当状态在此之前的轨迹中没有出现时才计算回报。对于餐车示例，由于轨迹中每天的状态都不同——因为每次转换后，状态中的星期几部分都会变化——所以这两种方法是相同的。
- en: 'Let''s follow these steps for the implementation:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照这些步骤进行实现：
- en: 'We start by defining a function that takes a trajectory and calculates the
    returns from the first visit for each state that appears in the trajectory:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一个函数，接收一个轨迹并计算从首次访问开始的每个状态的回报：
- en: '[PRE25]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This function does the following:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数执行以下操作：
- en: a) Takes a dictionary `returns` as input, whose keys are states and values are
    lists of returns calculated in some other trajectories.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 输入一个字典`returns`，其键是状态，值是一些其他轨迹中计算出的回报列表。
- en: b) Takes a list `trajectory` as input, which is a list of state-action-reward
    tuples.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 输入一个`trajectory`列表，它是一个状态-动作-奖励元组的列表。
- en: c) Appends the calculated return for each state to `returns`. If that state
    has never been visited by another trajectory before, it initializes the list.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 将每个状态的计算回报添加到`returns`中。如果该状态以前从未被其他轨迹访问过，则初始化该状态的列表。
- en: d) Traverses the trajectory backward to conveniently calculate the discounted
    returns. It applies the discount factor in each step.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 从轨迹的末尾开始反向遍历，方便计算折扣回报。每一步都会应用折扣因子。
- en: e) Checks if a state is visited earlier in the trajectory after each calculation.
    It saves the calculated return to the `returns` dictionary only for the first
    visit of a state.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 在每次计算后检查某个状态是否在轨迹中已被访问过。如果该状态是首次访问，则将计算的回报保存到`returns`字典中。
- en: 'Next, we implement a function that simulates the environment for a single episode
    with a given policy and returns the trajectory:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现一个函数，用给定的策略模拟一个单一的回合，并返回轨迹：
- en: '[PRE26]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, implement the first-visit return Monte Carlo function, which simulates
    the environment for a specified number of episodes/trajectories with a given policy.
    We keep track of the trajectories and average the returns for each state calculated
    by the `first_visit_return` function:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实现在指定回合数/轨迹数的环境中模拟，使用给定策略，并跟踪轨迹，计算每个状态的回报平均值，这些回报由`first_visit_return`函数计算：
- en: '[PRE27]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We make sure that we create an environment instance (or we can simply use the
    one from the previous sections). Also obtain the base policy, which fills the
    patty inventory up to 200 or 300 with equal chance:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们确保创建一个环境实例（或者直接使用前面章节中的环境实例）。同时获取基本策略，这个策略会以相等的概率将汉堡饼库存填充到200或300：
- en: '[PRE28]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s use the first-visit MC method to estimate the state values from
    1,000 trajectories:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用首次访问MC方法，通过1,000个轨迹来估计状态值：
- en: '[PRE29]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result, `v_est`, will look like the following:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果，`v_est`，将如下所示：
- en: '[PRE30]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, remember that we can use DP''s policy evaluation method to calculate the
    true state values for comparison:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，记住我们可以使用动态规划（DP）中的策略评估方法来计算真实的状态值进行比较：
- en: '[PRE31]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The true state values will look very similar:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实的状态值看起来会非常相似：
- en: '[PRE32]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can obtain the estimations with a different number of trajectories, such
    as 10, 100, and 1000\. Let''s do that and compare how the state value estimations
    get closer to the true values, as shown in *Figure 5.5*:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用不同数量的轨迹来获得估计结果，例如10、100和1000个轨迹。我们来做一下这个比较，看看状态值的估计如何逐渐接近真实值，如*图5.5*所示：
- en: '![](img/B14160_05_5.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_5.jpg)'
- en: Figure 5.5 – First-visit Monte Carlo estimates versus true state values
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 首次访问蒙特卡洛估计与真实状态值
- en: 'Let''s analyze the results a bit more closely:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地分析一下结果：
- en: a) As we collected more trajectories, the estimates got closer to the true state
    values. You can increase the number of trajectories to even higher numbers to
    obtain even better estimates.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: a) 随着我们收集更多的轨迹，估计值越来越接近真实的状态值。你可以增加轨迹的数量，甚至使用更高的数量来获得更精确的估计。
- en: b) After we collected 10 trajectories, no values were estimated for ("Tue",
    200). This is because this state was never visited within those 10 trajectories.
    This highlights the importance of collecting enough trajectories.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在我们收集了10条轨迹后，状态("Tue", 200)没有被估算值。这是因为在这10条轨迹中该状态从未被访问过。这凸显了收集足够轨迹的重要性。
- en: c) No values are estimated for states that started the day with 300 units of
    inventory. This is because, under the base policy, these states are impossible
    to visit. But then, we have no idea about the value of those states. On the other
    hand, they might be valuable states that we want our policy to lead us to. This
    is an **exploration problem** we need to address.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: c) 对于那些在当天开始时拥有300单位库存的状态，没有估算值。这是因为在基础策略下，这些状态是无法访问的。但是，我们对这些状态的值一无所知。另一方面，这些状态可能是我们希望策略引导我们去的有价值的状态。这是一个我们需要解决的**探索问题**。
- en: 'Now we have a way of estimating state values without knowing the environment
    dynamics, and by only using the agent''s experience in the environment. Great
    job so far! However, an important problem remains. With the state value estimates
    alone, we cannot really improve the policy on hand. To see why this is the case,
    recall how we improved the policy with the DP methods, such as value iteration:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种估算状态值的方法，无需了解环境的动态，只需使用智能体在环境中的经验。到目前为止，做得很棒！然而，仍然存在一个重要问题。仅凭状态值估算，我们无法真正改进当前的策略。为了理解为什么会这样，回想一下我们是如何通过动态规划方法（如值迭代）来改进策略的：
- en: '![](img/Formula_05_141.jpg)![](img/Formula_05_142.jpg)![](img/Formula_05_143.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_141.jpg)![](img/Formula_05_142.jpg)![](img/Formula_05_143.jpg)'
- en: We used the state value estimates *together with the transition probabilities*
    to obtain action (q) values. We then chose, for each state, the action that maximized
    the q-value of the state. Right now, since we don't assume knowledge of the environment,
    we can't go from the state values to action values.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们结合*状态值估算与转移概率*来获得行动（q）值。然后，我们为每个状态选择一个最大化该状态q值的行动。现在，由于我们假设不了解环境，我们无法通过状态值来推导行动值。
- en: 'This leaves us with one option: We need to estimate the action values directly.
    Fortunately, it will be similar to how we estimated the state values. Let''s look
    into using Monte Carlo methods to estimate the action values next.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们留下了一个选择：我们需要直接估算行动值。幸运的是，这将类似于我们估算状态值的方式。接下来，我们来探讨使用蒙特卡洛方法估算行动值。
- en: Estimating the action-value function
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估算行动值函数
- en: 'Action values, ![](img/Formula_05_144.png), represent the expected discounted
    cumulative return when starting in state ![](img/Formula_05_045.png), taking action
    ![](img/Formula_05_146.png), and following the policy ![](img/Formula_05_075.png).
    Consider the following trajectory:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 行动值，![](img/Formula_05_144.png)，表示从状态![](img/Formula_05_045.png)开始，执行行动![](img/Formula_05_146.png)，并遵循策略![](img/Formula_05_075.png)时的预期折现累计回报。考虑以下轨迹：
- en: '![](img/Formula_05_148.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_148.jpg)'
- en: 'The discounted return observed can be used in estimating ![](img/Formula_05_149.png),
    ![](img/Formula_05_150.png), and so on. We can then use them to determine the
    best action for a given state ![](img/Formula_05_010.png), as in the following:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用观察到的折现回报来估算![](img/Formula_05_149.png)，![](img/Formula_05_150.png)等等。然后，我们可以用它们来确定给定状态![](img/Formula_05_010.png)的最佳行动，如下所示：
- en: '![](img/Formula_05_152.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_152.jpg)'
- en: 'Here is the challenge: What if we don''t have the action-value estimates for
    all possible ![](img/Formula_05_059.png) that can be taken in state ![](img/Formula_05_154.png)?
    Consider the grid world example. If the policy is always to go right when in state
    ![](img/Formula_05_155.png), we will never have a trajectory that starts with
    state-action pairs ![](img/Formula_05_156.png), or ![](img/Formula_05_157.png).
    So, even if one of those actions gives a higher action value than ![](img/Formula_05_158.png),
    we will never discover it. The situation was similar with the base policy in the
    food truck example. In general, this is the case when we use a deterministic policy,
    or a stochastic policy that does not take all of the actions with some positive
    probability.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个挑战：如果我们没有所有可能的![](img/Formula_05_059.png)的行动值估算，怎么办？考虑网格世界的例子。如果策略总是选择在状态![](img/Formula_05_155.png)下往右走，我们将永远不会有一个以状态-行动对![](img/Formula_05_156.png)或![](img/Formula_05_157.png)开始的轨迹。因此，即使这些行动中的某一个提供的行动值高于![](img/Formula_05_158.png)，我们也永远无法发现它。这个问题在食物车示例中的基础策略下也是类似的。一般来说，当我们使用确定性策略，或者使用一个在某些状态下不会以正概率选择所有行动的随机策略时，都会遇到这种情况。
- en: So, what we have here is essentially an *exploration* problem, which is a fundamental
    challenge in RL.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们在这里面临的本质上是一个*探索*问题，这是强化学习中的一个基本挑战。
- en: 'There are two possible solutions to this:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题有两种可能的方案：
- en: Starting the trajectories with a random action selected in a random initial
    state, and then following a policy ![](img/Formula_05_061.png) as usual. This
    is called **exploring starts**. This ensures choosing all state-action pairs at
    least once, so we can estimate the action-values. The drawback of this approach
    is that we need to keep starting episodes with random initializations. If we want
    to learn the action-values from ongoing interaction with the environment, without
    frequent restarts, this method will not be too helpful.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个随机选择的动作开始轨迹，在一个随机初始状态下，然后像往常一样遵循策略 ![](img/Formula_05_061.png)。这被称为 **探索性起始**。这样可以确保每个状态-动作对至少被选择一次，从而可以估计动作值。这个方法的缺点是，我们需要始终从随机初始化开始每个episode。如果我们希望通过与环境的持续互动来学习动作值，而不需要频繁重新开始，那么这种方法就不是特别有用。
- en: The other and more common solution to the exploration problem is to maintain
    a policy that selects all actions in any state with positive probability. More
    formally, we need a policy that satisfies ![](img/Formula_05_160.png), for all
    ![](img/Formula_05_161.png) and for all ![](img/Formula_05_162.png); where ![](img/Formula_05_163.png)
    is the set of all possible states and ![](img/Formula_05_164.png) is the set of
    all possible actions available in state ![](img/Formula_05_165.png). Such a policy
    ![](img/Formula_05_166.png) is called a **soft policy**.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索问题的另一种更常见的解决方案是保持一个策略，在任何状态下都以正概率选择所有动作。更正式地说，我们需要一个满足 ![](img/Formula_05_160.png)
    的策略，适用于所有 ![](img/Formula_05_161.png) 和所有 ![](img/Formula_05_162.png)；其中 ![](img/Formula_05_163.png)
    是所有可能状态的集合，![](img/Formula_05_164.png) 是状态 ![](img/Formula_05_165.png) 中所有可能动作的集合。这样的策略
    ![](img/Formula_05_166.png) 称为 **软策略**。
- en: In the next section, we will use soft policy action-value estimation, which
    will in turn be used for policy improvement.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用软策略动作值估计，进而用于策略改进。
- en: Monte Carlo control
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蒙特卡罗控制
- en: Monte Carlo control refers to a collection of methods that find optimal/near-optimal
    policies using the *samples* of discounted return. In other words, this is learning
    optimal policies from experience. And, since we depend on experience to discover
    optimal policies, we have to explore, as we explained above. Next, we'll implement
    ![](img/Formula_05_167.png)**-greedy** policies that enable us to explore during
    training, which is a particular form of soft policy. After that, we'll cover two
    different flavors of Monte Carlo control, namely on-policy and off-policy methods.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡罗控制指的是一类方法，通过使用*折扣回报*的样本来找到最优/近似最优的策略。换句话说，这就是通过经验学习最优策略。而且，由于我们依赖经验来发现最优策略，我们必须进行探索，正如我们上面所解释的那样。接下来，我们将实现
    ![](img/Formula_05_167.png) **ε-greedy** 策略，使我们能够在训练过程中进行探索，这是一种特殊形式的软策略。之后，我们将讨论蒙特卡罗控制的两种不同变体，即在策略方法和离策略方法。
- en: Implementing ![](img/Formula_05_168.png)-greedy policies
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 ![](img/Formula_05_168.png)-greedy 策略
- en: Very similar to what we did in bandit problems, an ![](img/Formula_05_1681.png)-greedy
    policy picks an action at random with ε probability; and with ![](img/Formula_05_169.png)
    probability, it selects the action that maximizes the action-value function. This
    way, we continue to explore all state-action pairs while selecting the best actions
    we identify with high likelihood.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在赌徒问题中所做的非常相似，![](img/Formula_05_1681.png)-greedy 策略以 ε 的概率随机选择一个动作；以 ![](img/Formula_05_169.png)
    的概率选择最大化动作值函数的动作。这样，我们可以继续探索所有状态-动作对，同时选择我们识别出的最佳动作，并且高概率地做出选择。
- en: 'Let''s now implement a function that converts a deterministic action to an
    ![](img/Formula_05_170.png)-greedy one, which we will need later. The function
    assigns ![](img/Formula_05_171.png) probability to the best action, and ![](img/Formula_05_172.png)
    probability to all other actions:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现一个函数，将一个确定性动作转换为 ![](img/Formula_05_170.png)-greedy 动作，稍后我们将需要它。该函数将为最佳动作分配
    ![](img/Formula_05_171.png) 的概率，并为所有其他动作分配 ![](img/Formula_05_172.png) 的概率：
- en: '[PRE33]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Exploration is necessary to find optimal policies during training. On the other
    hand, we would not want to take exploratory actions after the training/during
    inference but take the best ones. Therefore, the two policies differ. To make
    the distinction, the former is called the **behavior policy** and the latter is
    called the **target policy**. We can make the state and the action values aligned
    with the former or the latter, leading to two different classes of methods: **on-policy**
    and **off-policy**. Let''s compare the two approaches in detail next.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，探索是找到最优策略所必需的。另一方面，在训练结束后（即推理阶段），我们并不希望进行探索性行动，而是选择最佳行动。因此，这两种策略是不同的。为了区分这两者，前者被称为**行为策略**，后者被称为**目标策略**。我们可以使状态和值函数与前者或后者对齐，从而产生两种不同的方法：**在策略方法**和**离策略方法**。接下来我们将详细比较这两种方法。
- en: On-policy versus off-policy methods
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在策略与离策略方法
- en: Remember that the state and the action values are associated with a particular
    policy, hence the notation ![](img/Formula_05_173.png) and ![](img/Formula_05_174.png).
    On-policy methods estimate the state and the action values for the behavior policy
    used during training, such as the one that generates the training data/the experience.
    Off-policy methods estimate the state and the action values for a policy that
    is other than the behavior policy, such as the target policy. We ideally want
    to decouple exploration from value estimation. Let's look into why this is the
    case in detail.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，状态和值函数与特定的策略相关联，因此使用![](img/Formula_05_173.png)和![](img/Formula_05_174.png)符号。在策略方法估计的是用于训练的行为策略下的状态和值函数，例如生成训练数据/经验的策略。离策略方法则估计的是与行为策略不同的策略下的状态和值函数，例如目标策略。我们理想的情况是将探索与值估计分开。接下来我们将详细探讨为什么要这样做。
- en: The impact of on-policy methods on value function estimates
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在策略方法对值函数估计的影响
- en: Exploratory policies are usually not optimal as they take random actions once
    in a while for the sake of exploration. Since on-policy methods estimate the state
    and action values for the behavior policy, that sub-optimality is reflected in
    the value estimates.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性策略通常不是最优的，因为它们会不定期地采取随机动作以进行探索。由于在策略方法是针对行为策略来估计状态和值函数的，这种次优性会反映在值估计中。
- en: 'Consider the following modified grid world example to see how involving the
    effects of the exploratory actions in value estimation could be potentially harmful:
    The robot needs to choose between going left or right in state ![](img/Formula_05_175.png)
    of a ![](img/Formula_05_176.png) grid world, and between up and down in states
    1 and 3\. The robot follows the actions perfectly, so there is no randomness in
    the environment. This is illustrated in *Figure 5.6*:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下修改后的网格世界示例，以观察将探索性行动的影响纳入值估计可能会带来的负面影响：机器人需要在![](img/Formula_05_175.png)状态下选择向左或向右，并在状态1和3之间选择向上或向下。机器人完美地遵循这些动作，因此环境中没有随机性。此情况在*图5.6*中有所展示：
- en: '![](img/B14160_05_6.jpg)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_6.jpg)'
- en: Figure 5.6 – Modified grid world
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 修改后的网格世界
- en: 'The robot has an ![](img/Formula_05_177.png)-greedy policy, which suggests
    taking the best action with 0.99 chance, and an exploratory action with 0.01 chance.
    The best policy in state 3 is to go up with a high likelihood. In state 1, the
    choice does not really matter. The state value estimates obtained in an on-policy
    manner will then be the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 该机器人采用了一个![](img/Formula_05_177.png)-贪婪策略，意味着以0.99的概率选择最佳行动，以0.01的概率选择探索性行动。在状态3下，最佳策略是以较高的可能性向上移动。在状态1中，选择其实并不重要。通过在策略上进行估计得到的状态值如下：
- en: '![](img/Formula_05_178.jpg)![](img/Formula_05_179.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_178.jpg)![](img/Formula_05_179.jpg)'
- en: A policy obtained in an on-policy fashion for state 2 will suggest going left,
    towards state 1\. On the other hand, in this deterministic environment, the robot
    could perfectly avoid the big penalty when there is no exploration involved. An
    on-policy method would fail to identify this since the exploration influences
    the value estimates and yields a sub-optimal policy as a result. On the other
    hand, in some cases, we may want the agent to take the impact of exploration into
    account if, for example, the samples are collected using a physical robot and
    some states are very costly to visit.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方式下，为状态2获得的策略会建议向左走，朝向状态1。另一方面，在这个确定性环境中，当没有探索时，机器人可以完美地避免大的惩罚。基于策略的方法无法识别这一点，因为探索会影响价值估计，从而导致次优策略。另一方面，在某些情况下，如果例如样本是通过物理机器人收集的，且某些状态访问成本非常高，我们可能希望智能体考虑探索的影响。
- en: Sample efficiency comparison between on-policy and off-policy methods
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于策略和离策略方法的样本效率比较
- en: As mentioned above, off-policy methods estimate the state and the action values
    for a policy that is different than the behavior policy. On-policy methods, on
    the other hand, estimate these values only for the behavior policy. When we cover
    deep RL on-policy methods in later chapters, we will see that this will require
    the on-policy methods to discard the past experience once the behavior policy
    is updated. Off-policy deep RL methods, however, can reuse the past experience
    again and again. This is a significant advantage in sample efficiency, especially
    if it is costly to obtain the experience.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，离策略方法估计与行为策略不同的策略的状态和动作值。而基于策略的方法，只估算行为策略的状态和动作值。当我们在后面的章节中讨论深度强化学习的基于策略方法时，我们将看到，这将要求基于策略的方法在行为策略更新后丢弃过去的经验。然而，离策略的深度强化学习方法可以一次又一次地重复利用过去的经验。这在样本效率上是一个显著的优势，尤其是在获得经验的成本很高时。
- en: Another area where off-policy methods' ability to use the experience generated
    by a policy other than the behavior policy comes in handy is when we want to warm-start
    the training based on the data generated by a non-RL controller, such as classical
    PID controllers or a human operator. This is especially useful when the environment
    is hard/expensive to simulate or collect experience from.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 离策略方法能够利用由非行为策略生成的经验的另一个场景是，当我们希望基于非强化学习控制器（例如经典的PID控制器或人工操作员）生成的数据来热启动训练时。这在环境难以模拟或采集经验时尤为有用。
- en: Advantages of on-policy methods
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于策略方法的优势
- en: 'The natural question is then why do we even talk about on-policy methods instead
    of just ignoring them? There are several advantages of on-policy methods:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，天然的问题是，为什么我们还要讨论基于策略的方法，而不是直接忽略它们呢？基于策略方法有几个优势：
- en: As mentioned above, if the cost of sampling from the environment is high (the
    actual environment rather than a simulation), we may want to have a policy that
    reflects the impact of exploration to avoid catastrophic outcomes.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如上所述，如果从环境中采样的成本很高（指实际环境而非模拟环境），我们可能希望有一个能够反映探索影响的策略，以避免灾难性后果。
- en: Off-policy methods, when combined with function approximators, could have issues
    with converging to a good policy. We will discuss this in the next chapter.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离策略方法与函数逼近器结合时，可能会出现收敛到一个良好策略的问题。我们将在下一章讨论这一点。
- en: On-policy methods are easier to work with when the action space is continuous,
    again, as we will discuss later.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当动作空间是连续时，基于策略的方法更容易操作，正如我们稍后会讨论的那样。
- en: With that, it is finally time to implement first on-policy and then off-policy
    Monte Carlo methods!
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们终于可以实现基于策略的蒙特卡洛方法，然后是离策略的蒙特卡洛方法！
- en: On-policy Monte Carlo control
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于策略的蒙特卡洛控制
- en: The GPI framework we described earlier, which suggests going back and forth
    between some forms of policy evaluation and policy improvement, is also what we
    use with the Monte Carlo methods to obtain optimal policies. In each cycle, we
    collect a trajectory from a complete episode, then estimate the action-values
    and update the policy, and so on.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前描述的GPI框架，它建议在某些形式的策略评估和策略改进之间来回交替，这也是我们与蒙特卡洛方法一起用来获得最优策略的方式。在每个周期中，我们从一个完整的情节中收集一个轨迹，然后估算动作值并更新策略，依此类推。
- en: 'Let''s implement an on-policy MC control algorithm and use it to optimize the
    food truck inventory replenishment:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个基于策略的蒙特卡洛控制算法，并用它来优化餐车库存补充：
- en: 'We first create a function that generates a random policy where all actions
    are equally likely to be taken, to be used to initialize the policy:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建一个生成随机策略的函数，在这个策略中所有动作的概率是一样的，用于初始化策略：
- en: '[PRE34]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And now, we build the on-policy first-visit MC control algorithm:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们构建在策略的首次访问蒙特卡洛控制算法：
- en: '[PRE35]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This is very similar to the first-visit MC prediction method, with the following
    key differences:'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与首次访问蒙特卡洛预测方法非常相似，关键的不同之处如下：
- en: a) Instead of estimating the state values, ![](img/Formula_05_180.png), we estimate
    the action values, ![](img/Formula_05_181.png).
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 我们不再估计状态值 ![](img/Formula_05_180.png)，而是估计动作值 ![](img/Formula_05_181.png)。
- en: b) To explore all state-action pairs, we use ε-greedy policies.
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 为了探索所有的状态-动作对，我们使用 ε-贪婪策略。
- en: c) This is a first-visit approach, but instead of checking if the state appeared
    earlier in the trajectory, we check if the state-action pair appeared before in
    the trajectory before we update the ![](img/Formula_05_182.png) estimate.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 这是一个首次访问方法，但我们不检查状态是否早些时候出现在轨迹中，而是在更新 ![](img/Formula_05_182.png) 估计值之前，检查状态-动作对是否在轨迹中出现过。
- en: d) Whenever we update the ![](img/Formula_05_183.png) estimate of a state-action
    pair, we also update the policy to assign the highest probability to the action
    that maximizes the ![](img/Formula_05_184.png), which is ![](img/Formula_05_185.png).
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 每当我们更新状态-动作对的 ![](img/Formula_05_183.png) 估计值时，我们还会更新策略，给与能最大化 ![](img/Formula_05_184.png)
    的动作最高概率，即 ![](img/Formula_05_185.png)。
- en: 'Use the algorithm to optimize the food truck policy:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该算法优化餐车策略：
- en: '[PRE36]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Display the `policy` dictionary. You will see that it is the optimal one that
    we found earlier with DP methods:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 `policy` 字典。你会看到它是我们之前使用 DP 方法找到的最优策略：
- en: '[PRE37]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's it! This algorithm does not use any knowledge of the environment, unlike
    the DP methods. It learns from its interaction with the (simulation of the) environment!
    And, when we run the algorithm long enough, it converges to the optimal policy.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！这个算法不像 DP 方法那样使用任何环境知识。它从与（环境模拟）的交互中学习！而且，当我们运行该算法足够长时间时，它会收敛到最优策略。
- en: Great job! Next, let's proceed to off-policy MC control.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！接下来，让我们继续进行离策略蒙特卡洛控制。
- en: Off-policy Monte Carlo control
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离策略蒙特卡洛控制
- en: In an off-policy approach, we have a set of samples (trajectories) collected
    under some behavior policy ![](img/Formula_05_186.png), yet we would like to use
    that experience to estimate the state and action values under a target policy
    ![](img/Formula_05_046.png). This requires us to use a trick called **importance
    sampling**. Let's look into what it is next, and then describe off-policy Monte
    Carlo control.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在离策略方法中，我们有一组样本（轨迹），这些样本是在某种行为策略 ![](img/Formula_05_186.png) 下收集的，但我们希望利用这些经验来估计在目标策略
    ![](img/Formula_05_046.png) 下的状态和动作值。这要求我们使用一种称为 **重要性采样** 的技巧。接下来我们将深入了解它，并描述离策略蒙特卡洛控制。
- en: Importance sampling
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重要性采样
- en: 'Let''s start describing importance sampling in a simple game setting, in which
    you roll a six-sided die. Depending on what comes up on the die, you receive a
    random reward. This could be something like this: If you get a 1, you get a reward
    from a normal distribution with mean 10 and variance 25\. There are similar hidden
    reward distributions for all outcomes, which are unknown to you. Let''s denote
    the reward you receive when the face ![](img/Formula_05_188.png) comes up with
    the random variable ![](img/Formula_05_189.png).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个简单的游戏设置开始描述重要性采样，在这个游戏中，你投掷一个六面骰子。根据骰子上出现的点数，你会获得一个随机奖励。可能是这样的情况：如果你得到
    1，那么你将获得一个均值为 10，方差为 25 的正态分布的奖励。对于所有的结果，都有类似的隐藏奖励分布，这些分布对你来说是未知的。我们用随机变量 ![](img/Formula_05_189.png)
    来表示当面朝 ![](img/Formula_05_188.png) 时，你所获得的奖励。
- en: You want to estimate the expected reward after a single roll, which is
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你想估计一次投掷后的期望奖励，即
- en: '![](img/Formula_05_190.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_190.jpg)'
- en: where ![](img/Formula_05_191.png) is the probability of observing side ![](img/Formula_05_192.png).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_05_191.png) 是观察到面朝 ![](img/Formula_05_192.png) 的概率。
- en: 'Now, assume that you have two dice to choose from, A and B, with different
    probability distributions. You first pick A, and roll the die ![](img/Formula_05_193.png)
    times. Your observations are as in the following table:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你有两个骰子可以选择，A 和 B，它们具有不同的概率分布。你首先选择 A，并投掷骰子 ![](img/Formula_05_193.png)
    次。你的观察结果如下表所示：
- en: '![](img/B14160_05_7.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_7.jpg)'
- en: Table 5.1 – Observed rewards after n rolls with die A
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 使用骰子 A 投掷 n 次后的观察奖励
- en: 'Here, ![](img/Formula_05_194.png) denotes the reward observed after the ![](img/Formula_05_195.png)
    roll when the observed side is ![](img/Formula_05_196.png). The estimated expected
    reward with die A,  ![](img/Formula_05_197.png), is simply given by the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_05_194.png) 表示在第 ![](img/Formula_05_195.png) 次投掷后观察到的奖励，当观察到的面是
    ![](img/Formula_05_196.png) 时。使用骰子A的估计期望奖励，![](img/Formula_05_197.png)，简单地给出如下公式：
- en: '![](img/Formula_05_198.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_198.jpg)'
- en: Now, the question is whether we can use this data to estimate the expected reward
    with die B, such as ![](img/Formula_05_199.png), without any new observations?
    The answer is yes if we know ![](img/Formula_05_200.png) and ![](img/Formula_05_201.png).
    Here is how.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们是否可以使用这些数据来估计使用骰子B时的期望奖励，比如 ![](img/Formula_05_199.png)，而无需任何新的观测？答案是，如果我们知道
    ![](img/Formula_05_200.png) 和 ![](img/Formula_05_201.png)，那么答案是肯定的。下面是方法。
- en: 'In the current estimation, each observation has a weight of 1\. Importance
    sampling suggests scaling these weights according to ![](img/Formula_05_202.png).
    Now, consider the observation ![](img/Formula_05_203.png). If with die B, we are
    three times as likely to observe ![](img/Formula_05_204.png) compared to die A,
    then we increase the weight of this observation in the sum to 3\. More formally,
    we do the following:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的估计中，每个观测的权重为1。重要性采样建议根据 ![](img/Formula_05_202.png) 来调整这些权重。现在，考虑观测 ![](img/Formula_05_203.png)。如果使用骰子B时，我们观察到
    ![](img/Formula_05_204.png) 的可能性是使用骰子A时的三倍，那么我们将在总和中增加该观测的权重，增加至3。更正式地，我们执行如下操作：
- en: '![](img/Formula_05_205.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_205.jpg)'
- en: 'Here, the ratio ![](img/Formula_05_206.png) is called the importance sampling
    ratio. By the way, the above preceding estimation, ![](img/Formula_05_207.png),
    is called **ordinary importance sampling**. We can also normalize the new estimate
    with respect to the new weights and obtain **weighted importance sampling** as
    follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，比例 ![](img/Formula_05_206.png) 被称为重要性采样比率。顺便说一下，上述的前一个估计，![](img/Formula_05_207.png)，被称为**普通重要性采样**。我们还可以根据新的权重对新估计进行归一化，从而得到**加权重要性采样**，如下所示：
- en: '![](img/Formula_05_208.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_208.jpg)'
- en: Now, after this detour, let's go back to using this to get off-policy predictions.
    In the context of Monte Carlo prediction, observing a particular side of the die
    corresponds to observing a particular trajectory, and the die reward corresponds
    to the total return.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个绕道之后，让我们回到使用这些来获得脱离策略的预测。在蒙特卡罗预测的上下文中，观察骰子的某一面对应于观察一个特定的轨迹，而骰子的奖励则对应于总回报。
- en: 'If you are thinking *"well, we don''t really know the probability of observing
    a particular trajectory, that''s why we are using Monte Carlo methods, isn''t
    it?"* You are right, but we don''t need to. Here is why. Starting in some ![](img/Formula_05_209.png),
    the probability of observing a trajectory ![](img/Formula_05_210.png) under a
    behavior policy ![](img/Formula_05_211.png) is as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想 *“嗯，我们并不真正知道观察特定轨迹的概率，这就是我们使用蒙特卡罗方法的原因，不是吗？”* 你是对的，但我们不需要知道。原因如下。从某个 ![](img/Formula_05_209.png)
    开始，在行为策略 ![](img/Formula_05_211.png) 下观察到轨迹 ![](img/Formula_05_210.png) 的概率如下：
- en: '![](img/Formula_05_212.jpg)![](img/Formula_05_213.jpg)![](img/Formula_05_214.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_212.jpg)![](img/Formula_05_213.jpg)![](img/Formula_05_214.jpg)'
- en: 'The expression is the same for the target policy, except ![](img/Formula_05_035.png)
    replaces ![](img/Formula_05_216.png). Now, when we calculate the importance sampling
    ratio, the transition probabilities cancel out and we end up with the following
    (*Sutton & Barto, 2018*):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对于目标策略，表达式是一样的，唯一的不同是 ![](img/Formula_05_035.png) 替换了 ![](img/Formula_05_216.png)。现在，当我们计算重要性采样比率时，转移概率会相互抵消，我们最终得到如下结果（*Sutton
    & Barto, 2018*）：
- en: '![](img/Formula_05_217.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_217.jpg)'
- en: 'With that, we go from estimating the expectation under the behavior policy:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就可以从估计行为策略下的期望值开始：
- en: '![](img/Formula_05_218.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_218.jpg)'
- en: 'to estimating the expectation under the target policy:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 估计目标策略下的期望值：
- en: '![](img/Formula_05_219.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_219.jpg)'
- en: 'Let''s close this section with a few notes on importance sampling before we
    implement it:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现之前，让我们通过一些关于重要性采样的笔记来结束这一部分：
- en: In order to be able to use the samples obtained under behavior policy ![](img/Formula_05_211.png)
    to estimate the state and action values under ![](img/Formula_05_221.png), it
    is required to have ![](img/Formula_05_222.png) if ![](img/Formula_05_223.png).
    Since we would not want to impose what ![](img/Formula_05_224.png) could be taken
    under the target policy, ![](img/Formula_05_225.png) is usually chosen as a soft
    policy.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了能够使用在行为策略下获得的样本！[](img/Formula_05_211.png) 来估计在！[](img/Formula_05_221.png)
    下的状态和值动作，需要满足！[](img/Formula_05_222.png)，如果！[](img/Formula_05_223.png)成立。由于我们不希望强加目标策略下可能采取的！[](img/Formula_05_224.png)，通常选择！[](img/Formula_05_225.png)作为一种软策略。
- en: In the weighted importance sampling, if the denominator is zero, then the estimate
    is considered zero.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加权重要性采样中，如果分母为零，则该估计值被视为零。
- en: The formulas above ignored the discount in the return, which is a bit more complicated
    to deal with.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述公式忽略了回报中的折扣因素，这处理起来稍微复杂一些。
- en: The observed trajectories can be chopped with respect to the first-visit or
    every-visit rule.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察到的轨迹可以根据首次访问或每次访问规则进行切分。
- en: The ordinary importance sampling is unbiased but can have very high variance.
    The weighted importance sampling, on the other hand, is biased but usually has
    a much lower variance, hence it's preferred in practice.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普通的重要性采样是无偏的，但可能具有非常高的方差。加权重要性采样则是有偏的，但通常具有较低的方差，因此在实践中更受青睐。
- en: This was a quite detour, and we hope you are still with us! If you are, let's
    go back to coding!
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点绕远了，希望你还跟得上！如果你跟得上，我们回到编码吧！
- en: Application to the inventory replenishment problem
  id: totrans-377
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用于库存补充问题
- en: 'In our application of the off-policy Monte Carlo method, we use the weighted
    importance sampling. The behavior policy is chosen as an ![](img/Formula_05_226.png)-greedy
    policy, yet the target is a greedy policy maximizing the action value for each
    state. Also, we use an incremental method to update the state and action value
    estimates as follows (Sutton & Barto, 2018):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应用离策略蒙特卡罗方法时，我们使用加权重要性采样。行为策略选择为！[](img/Formula_05_226.png)-贪心策略，而目标是最大化每个状态下动作值的贪心策略。此外，我们使用增量方法更新状态和值动作估计，如下所示（Sutton
    & Barto，2018）：
- en: '![](img/Formula_05_227.jpg)![](img/Formula_05_228.jpg)![](img/Formula_05_229.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_227.jpg)![](img/Formula_05_228.jpg)![](img/Formula_05_229.jpg)'
- en: 'And this:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以及这个：
- en: '![](img/Formula_05_230.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_230.jpg)'
- en: 'Now, the implementation can be done as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以按如下方式实现：
- en: 'Let''s first define the function for the incremental implementation of the
    off-policy Monte Carlo:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义用于增量实现离策略蒙特卡罗的函数：
- en: '[PRE38]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We use the off-policy MC to optimize the food truck inventory replenishment
    policy:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用离策略蒙特卡罗（MC）方法来优化食品车库存补充策略：
- en: '[PRE39]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Finally, display the obtained policy. You will see that it is the optimal one!
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，展示获得的策略。你会看到它是最优的！
- en: '[PRE40]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Yay! We are done with the Monte Carlo methods for now. Congratulations, this
    was not the easiest part! You deserve a break. Next, we will dive into yet another
    very important topic: temporal-difference learning.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 耶！我们暂时完成了蒙特卡罗方法。恭喜，这部分并不简单！你应该休息一下。接下来，我们将深入探讨另一个非常重要的话题：时序差分学习。
- en: Temporal-difference learning
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时序差分学习
- en: 'The first class of methods to solve MDP we covered in this chapter was DP:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的解决MDP的第一类方法是动态规划（DP）：
- en: It needs to completely know the environment dynamics to be able to find the
    optimal solution.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要完全了解环境的动态，才能找到最优解。
- en: It allows us to progress toward the solution with one-step updates of the value
    functions.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许我们通过对价值函数进行一步步更新来推进解决方案。
- en: 'We then covered the MC methods:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了蒙特卡罗（MC）方法：
- en: They only require the ability to sample from the environment, therefore they
    learn from experience, as opposed to knowing the environment dynamics – a huge
    advantage over DP.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们只要求能够从环境中采样，因此它们从经验中学习，而不是知道环境的动态——这是相对于动态规划（DP）的巨大优势。
- en: But they need to wait for a complete episode trajectory to update a policy.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是它们需要等待一个完整的剧集轨迹来更新策略。
- en: '**Temporal-difference (TD)** methods are, in some sense, the best of both worlds:
    They learn from experience, and they can update the policy after each step by
    **bootstrapping**. This comparison of TD to DP and MC is illustrated in *Table
    5.2*:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '**时序差分（TD）**方法在某种意义上是两全其美：它们从经验中学习，并且可以通过**自举**在每一步后更新策略。TD与DP和MC的比较在*表5.2*中进行了说明：'
- en: '![](img/B14160_05_8.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_05_8.jpg)'
- en: Table 5.2 – Comparison of DP, MC, and TD learning methods
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 – 动态规划（DP）、蒙特卡罗（MC）和时序差分（TD）学习方法的比较
- en: As a result, TD methods are central in RL and you will encounter them again
    and again in various forms. In this section, you will learn how to implement TD
    methods in tabular form. Modern RL algorithms, which we will cover in the following
    chapters, implement TD methods with function approximations such as neural networks.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TD方法在强化学习中占据核心地位，你将反复遇到它们的不同形式。在本节中，你将学习如何在表格形式中实现TD方法。我们将在接下来的章节中介绍现代强化学习算法，这些算法使用函数逼近方法（如神经网络）来实现TD方法。
- en: One-step TD learning – TD(0)
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一步TD学习 – TD(0)
- en: TD methods can update a policy after a single state transition or multiple ones.
    The former version is called one-step TD learning or TD(0) and it is easier to
    implement compared to ![](img/Formula_05_231.png)-step TD learning. We'll start
    covering one-step TD learning with the prediction problem. We'll then introduce
    an on-policy method, SARSA, and then an off-policy algorithm, the famous Q-learning.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: TD方法可以在单次状态转移或多次状态转移后更新策略。前者被称为一步TD学习或TD(0)，相比之下，![](img/Formula_05_231.png)-步TD学习更复杂，实施起来更困难。我们将从一步TD学习开始，首先探讨预测问题。然后，我们将介绍一种基于策略的方法SARSA，再介绍一种离策略的算法——著名的Q-learning。
- en: TD prediction
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TD预测
- en: 'Remember how we can write the state-value function of a policy ![](img/Formula_05_046.png)
    in terms of one-step reward and the value of the next state:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们如何将一个策略的状态-价值函数 ![](img/Formula_05_046.png) 通过一步奖励和下一个状态的价值来表示：
- en: '![](img/Formula_05_233.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_233.jpg)'
- en: 'When the agent takes an action under policy ![](img/Formula_05_061.png) while
    in state ![](img/Formula_05_010.png), it observes three random variables realized:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体在状态 ![](img/Formula_05_010.png) 下执行策略 ![](img/Formula_05_061.png) 所采取的动作时，它会观察到三个随机变量的实现：
- en: '![](img/Formula_05_236.png)'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_05_236.png)'
- en: '![](img/Formula_05_237.png)'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_05_237.png)'
- en: '![](img/Formula_05_238.png)'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/Formula_05_238.png)'
- en: 'We already know the probability of observing ![](img/Formula_05_059.png) as
    part of the policy, but the latter two come from the environment. The observed
    quantity ![](img/Formula_05_240.png) gives us a new estimate for ![](img/Formula_05_241.png)
    based on a single sample. Of course, we don''t want to completely discard the
    existing estimate and replace it with the new one, because the transitions are
    usually random, and we could have also observed completely different ![](img/Formula_05_242.png)
    and ![](img/Formula_05_243.png) values even with the same action ![](img/Formula_05_244.png).
    *The idea in TD learning is that we use this observation to update the existing
    estimate *![](img/Formula_05_245.png) *by moving it in the direction of this new
    estimate.* And the step-size ![](img/Formula_05_246.png) controls how aggressively
    we move towards the new estimate. More formally, we use the following update rule:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道在策略中观察到的 ![](img/Formula_05_059.png) 的概率，但后两个来自环境。观察到的量 ![](img/Formula_05_240.png)
    基于单一样本为我们提供了 ![](img/Formula_05_241.png) 的新估计。当然，我们不希望完全丢弃现有的估计并用新估计替换它，因为过渡通常是随机的，即使采取相同的行动
    ![](img/Formula_05_244.png)，我们也可能观察到完全不同的 ![](img/Formula_05_242.png) 和 ![](img/Formula_05_243.png)
    值。*TD学习中的思想是，我们利用这个观察结果来更新现有的估计* ![](img/Formula_05_245.png) *，使其朝着这个新估计的方向移动*。步长
    ![](img/Formula_05_246.png) 控制我们向新估计迈进的速度。更正式地，我们使用以下更新规则：
- en: '![](img/Formula_05_247.jpg)![](img/Formula_05_248.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_247.jpg)![](img/Formula_05_248.jpg)'
- en: The term in the square brackets is called the **TD error**. As is obvious from
    the name, it estimates how much the current state-value estimate of ![](img/Formula_05_249.png),
    ![](img/Formula_05_250.png) is off from the truth based on the latest observation.
    ![](img/Formula_05_251.png) would completely ignore the new signal while ![](img/Formula_05_252.png)
    would completely ignore the existing estimate. Again, since a single observation
    is often noisy, and the new estimate itself uses another erroneous estimate (![](img/Formula_05_253.png));
    the new estimate cannot be solely relied on. Therefore, the ![](img/Formula_05_254.png)
    value is chosen between ![](img/Formula_05_255.png) and 1, and often closer to
    ![](img/Formula_05_256.png).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号中的项被称为**TD误差**。顾名思义，它估计当前的状态-价值估计 ![](img/Formula_05_249.png)，![](img/Formula_05_250.png)
    与基于最新观察的真实值之间的偏差。![](img/Formula_05_251.png) 将完全忽略新的信号，而 ![](img/Formula_05_252.png)
    将完全忽略现有的估计。再次强调，由于单次观察通常是噪声的，并且新估计本身使用了另一个错误的估计（![](img/Formula_05_253.png)）；因此，不能完全依赖新估计。为此，![](img/Formula_05_254.png)
    的值在 ![](img/Formula_05_255.png) 和 1 之间选择，且通常更接近 ![](img/Formula_05_256.png)。
- en: 'With that, it is trivial to implement a TD prediction method to evaluate a
    given policy ![](img/Formula_05_046.png) and estimate the state values. Follow
    along to use TD prediction to evaluate the base policy in the food truck example:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以轻松实现一个 TD 预测方法来评估给定的策略 ![](img/Formula_05_046.png) 并估计状态值。请继续跟随，使用 TD
    预测在食品卡车示例中评估基本策略：
- en: 'First, we implement the TD prediction as described above as in the following
    function:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们实现如上所述的 TD 预测，如下函数所示：
- en: '[PRE41]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This function simply simulates a given environment using a specified policy
    over a specified number of iterations. After each observation, it does a one-step
    TD update using the given ![](img/Formula_05_258.png) step size and the discount
    factor ![](img/Formula_05_259.png).
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数简单地通过指定的策略，在指定的迭代次数内模拟给定的环境。在每次观察后，它使用给定的 ![](img/Formula_05_258.png) 步长和折扣因子
    ![](img/Formula_05_259.png) 进行一步的 TD 更新。
- en: 'Next, we get the base policy as defined by the `base_policy` function we introduced
    before:'
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们获得之前介绍的 `base_policy` 函数定义的基本策略：
- en: '[PRE42]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, let''s estimate the state values using TD prediction for ![](img/Formula_05_260.png)
    and ![](img/Formula_05_261.png) over 100,000 steps:'
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们通过 TD 预测估算 ![](img/Formula_05_260.png) 和 ![](img/Formula_05_261.png) 在
    100,000 步中得到的状态值：
- en: '[PRE43]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'After rounding, the state-value estimates will look like the following:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过四舍五入，状态值估计将如下所示：
- en: '[PRE44]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: If you go back to the DP methods section and check what the true state values
    are under the base policy, which we obtained using the policy evaluation algorithm,
    you will see that the TD estimates are very much in line with them.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回到 DP 方法部分，查看在基本策略下真实的状态值（我们使用策略评估算法获得），你会发现 TD 估计与这些值非常一致。
- en: So, great! We have successfully evaluated a given policy using TD prediction
    and things are working as expected. On the other hand, just like with the MC methods,
    we know that we have to estimate the action values to be able to improve the policy
    and find an optimal one in the absence of the environment dynamics. Next, we'll
    look into two different methods, SARSA and Q-learning, which do exactly that.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经成功使用 TD 预测评估了给定的策略，一切按预期工作。另一方面，就像 MC 方法一样，我们知道必须估算动作值，才能在缺乏环境动态的情况下改进策略并找到最优策略。接下来，我们将研究两种不同的方法，SARSA
    和 Q-learning，它们正是为此而设计。
- en: On-policy control with SARSA
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 SARSA 进行策略控制
- en: 'With slight additions and modifications to the TD(0), we can turn it into an
    optimal control algorithm. Namely, we will do the following:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对 TD(0) 做轻微的添加和修改，我们可以将其转化为一个最优控制算法。具体来说，我们将执行以下操作：
- en: Ensure that we always have a soft policy, such as ![](img/Formula_05_262.png)-greedy,
    to try all actions for a given state over time.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们始终拥有一个软策略，例如 ![](img/Formula_05_262.png)-贪婪，以便随着时间的推移尝试在给定状态下的所有动作。
- en: Estimate the action values.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计动作值。
- en: Improve the policy based on the action-value estimates.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于动作-价值估计来改进策略。
- en: 'And we will do all of these at each step and using the observations ![](img/Formula_05_263.png),
    hence the name **SARSA**. In particular, the action values are updated as follows:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个步骤中使用观察值 ![](img/Formula_05_263.png) 来执行所有这些操作，因此得名 **SARSA**。特别地，动作值的更新如下：
- en: '![](img/Formula_05_264.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_264.jpg)'
- en: Now, let's dive into the implementation!
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解实现过程！
- en: 'We define the function `sarsa`, which will take as the arguments the environment,
    the discount factor ![](img/Formula_05_265.png), the exploration parameter ![](img/Formula_05_266.png),
    and the learning step size ![](img/Formula_05_267.png). Also, implement the usual
    initializations:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了函数 `sarsa`，它将接受环境、折扣因子 ![](img/Formula_05_265.png)、探索参数 ![](img/Formula_05_266.png)
    和学习步长 ![](img/Formula_05_267.png) 作为参数。此外，实施常规初始化：
- en: '[PRE45]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we implement the algorithm loop, in which we simulate the environment
    for a single step, observe ![](img/Formula_05_268.png) and ![](img/Formula_05_269.png),
    choose the next action ![](img/Formula_05_270.png) based on ![](img/Formula_05_271.png)
    and the ![](img/Formula_05_272.png)-greedy policy, and update the action-value
    estimates:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们实现算法循环，在该循环中，我们模拟环境的单步操作，观察 ![](img/Formula_05_268.png) 和 ![](img/Formula_05_269.png)，基于
    ![](img/Formula_05_271.png) 和 ![](img/Formula_05_272.png)-贪婪策略选择下一个动作 ![](img/Formula_05_270.png)，并更新动作-价值估计：
- en: '[PRE46]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, let''s execute the algorithm with a selection of hyperparameters, such
    as ![](img/Formula_05_273.png), and over 1 million iterations:'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们使用一些超参数（例如 ![](img/Formula_05_273.png)）并执行算法，进行 100 万次迭代：
- en: '[PRE47]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The `policy` we obtain is the following:'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得的 `policy` 如下：
- en: '[PRE48]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note that the exploratory actions would be ignored when implementing this policy
    after training, and we would simply always pick the action with the highest probability
    for each state.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，实施该策略后，探索性动作将被忽略，我们将简单地为每个状态始终选择概率最高的动作。
- en: 'The action values, for example, for the state Monday – 0 beginning inventory
    (accessed via `Q[(''Mon'', 0)]`), are as follows:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，状态为星期一 – 0 初始库存（通过 `Q[('Mon', 0)]` 访问）时的动作值如下：
- en: '[PRE49]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And that''s it! We have successfully implemented the TD(0) algorithm for our
    example. Notice that, however, the policy we obtain is a near-optimal one, not
    the optimal one we obtained with the DP methods. There are also inconsistencies
    in the policy, such as having a policy of buying 300 patties both when in states
    (Tuesday, 0) and (Tuesday, 100). There are several culprits for not getting the
    optimal policy:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经成功实现了针对我们的示例的 TD(0) 算法。然而，注意到，我们获得的策略是一个近似最优策略，而不是我们通过动态规划方法获得的最优策略。策略中也存在一些不一致性，例如在状态（星期二，0）和（星期二，100）时都有买
    300 个肉饼的策略。未能获得最优策略的原因有很多：
- en: SARSA converges to an optimal solution in the limit, such as when ![](img/Formula_05_274.png).
    In practice, we run the algorithm for a limited number of steps. Try increasing
    ![](img/Formula_05_275.png) and you will see that the policy (usually) will get
    better.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA 在极限情况下会收敛到最优解，例如当 ![](img/Formula_05_274.png) 时。在实际操作中，我们会运行该算法有限步数。尝试增加
    ![](img/Formula_05_275.png)，你会发现策略（通常）会变得更好。
- en: The learning rate ![](img/Formula_05_254.png) is a hyperparameter that needs
    to be tuned. The speed of the convergence depends on this selection.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 ![](img/Formula_05_254.png) 是需要调整的超参数。收敛的速度依赖于这个选择。
- en: This is an on-policy algorithm. Therefore, the action values reflect the exploration
    due to the ![](img/Formula_05_277.png)-greedy policy, which is not what we really
    want in this example. Because, after training, there will be no exploration while
    following the policy in practice (since we need the exploration just to discover
    the best actions for each state). The policy we would use in practice is not the
    same as the policy we estimated the action values for.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个基于策略的算法。因此，动作值反映了由于 ![](img/Formula_05_277.png)-贪婪策略而产生的探索性，这不是我们在这个示例中真正想要的。因为在训练后，执行策略时不会进行探索（因为我们只需要探索来发现每个状态的最佳动作）。我们实际使用的策略与我们为其估算动作值的策略不同。
- en: Next, we turn to Q-learning, which is an off-policy TD method.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们转向 Q-learning，它是一种脱政策的 TD 方法。
- en: Off-policy control with Q-learning
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于 Q-learning 的脱政策控制
- en: As mentioned above, we would like to isolate the action-value estimates from
    the exploration effect, which means having an off-policy method. Q-learning is
    such an approach, which makes it very powerful, and as a result, a very popular
    one.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们希望将动作值估计与探索效应隔离开来，这意味着需要一种脱政策的方法。Q-learning 就是这样的一种方法，这使得它非常强大，因此也非常流行。
- en: 'Here are how the action values are updated in Q-learning:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Q-learning 中动作值的更新方式：
- en: '![](img/Formula_05_278.jpg)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_278.jpg)'
- en: Notice that instead of ![](img/Formula_05_279.png), we have the term ![](img/Formula_05_280.png).
    The difference may look small, but it is key. *It means that the action the agent
    uses to update the action-value,* ![](img/Formula_05_281.png)*, is not necessarily
    the one it will take in the next step when in* ![](img/Formula_05_282.png)*,*
    ![](img/Formula_05_283.png)*. Instead,* ![](img/Formula_05_284.png) *is an action
    that maximizes* ![](img/Formula_05_285.png)*, just like what we would use if not
    in training.* As a result, no exploratory actions are involved in action-value
    estimates and they are aligned with the policy that would be followed after training.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，代替 ![](img/Formula_05_279.png)，我们有了项 ![](img/Formula_05_280.png)。这看起来可能很小，但它是关键。*这意味着代理用于更新动作值的动作*，![](img/Formula_05_281.png)*，不一定是在*
    ![](img/Formula_05_282.png)* 中下一个步骤时执行的动作*，![](img/Formula_05_283.png)*。相反，* ![](img/Formula_05_284.png)
    *是一个最大化* ![](img/Formula_05_285.png)* 的动作，就像我们在非训练时会使用的那样。*因此，动作值估计中不涉及探索性动作，它们与训练后实际会遵循的策略一致。
- en: It means the action the agent takes in the next step, such as ![](img/Formula_05_286.png),
    is not used in the update. Instead, we use the maximum action value for the state
    ![](img/Formula_05_287.png), such as ![](img/Formula_05_288.png), in the update.
    Such an action ![](img/Formula_05_281.png) is what we would use after training
    with those action values, hence no exploratory actions are involved in action-value
    estimations.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着智能体在下一步所采取的行动，例如 ![](img/Formula_05_286.png)，不会被用于更新。相反，我们在更新中使用状态 ![](img/Formula_05_287.png)
    的最大动作值，例如 ![](img/Formula_05_288.png)。这种动作 ![](img/Formula_05_281.png) 是我们在使用这些动作值进行训练后会采用的，因此在动作值估计中不涉及探索性动作。
- en: 'The implementation of Q-learning is only slightly different than that of SARSA.
    Let''s go ahead and see Q-learning in action:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的实现与SARSA的实现仅有微小差异。我们接下来看看Q学习的实际应用：
- en: 'We start by defining the `q_learning` function with the usual initializations:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过定义常见初始化的 `q_learning` 函数来开始：
- en: '[PRE50]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we implement the main loop, where the action the agent takes in ![](img/Formula_05_290.png)
    comes from the ![](img/Formula_05_291.png)-greedy policy. During the update, the
    maximum of ![](img/Formula_05_292.png) is used:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们实现主循环，其中智能体在 ![](img/Formula_05_290.png) 中采取的行动来自 ![](img/Formula_05_291.png)-贪婪策略。在更新过程中，使用
    ![](img/Formula_05_292.png) 的最大值：
- en: '[PRE51]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We return the policy stripped of the exploratory actions after the main loop
    is finished:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主循环结束后，我们返回去除探索性动作的策略：
- en: '[PRE52]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we execute the algorithm with a selection of the hyperparameters,
    such as the following:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过选择超参数来执行算法，例如以下内容：
- en: '[PRE53]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Observe the returned `policy`:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察返回的 `policy`：
- en: '[PRE54]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: You will see that this hyperparameter set gives you the optimal policy (or something
    close to it depending on how randomization plays out in your case).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到这个超参数集为你提供了最佳策略（或者根据随机化的不同，可能接近最佳策略）。
- en: That concludes our discussion on Q-learning. Next, let's discuss how these approaches
    can be extended to ![](img/Formula_05_193.png)-step learning.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对Q学习的讨论。接下来，让我们讨论这些方法如何扩展到 ![](img/Formula_05_193.png)-步学习。
- en: n-step TD learning
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n步TD学习
- en: 'In the Monte Carlo methods, we collected complete episodes before making a
    policy update. With TD(0), on the other extreme, we updated the value estimates
    and the policy after a single transition in the environment. One could possibly
    find a sweet spot by following a path in between by updating the policy after
    ![](img/Formula_05_294.png)-steps of transitions. For ![](img/Formula_05_295.png),
    the two-step return looks like the following:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在蒙特卡洛方法中，我们在进行策略更新之前会收集完整的回合。另一方面，在TD(0)方法中，我们在环境中的一次过渡后就会更新价值估计和策略。通过在中间路径上更新策略，例如在
    ![](img/Formula_05_294.png)-步过渡后，可能会找到一个合适的平衡点。对于 ![](img/Formula_05_295.png)，两步回报看起来如下：
- en: '![](img/Formula_05_296.jpg)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_296.jpg)'
- en: 'And the general form is the following:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 其一般形式如下：
- en: '![](img/Formula_05_297.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_05_297.jpg)'
- en: This form can be used in the TD update to reduce the weight of the estimates
    used in bootstrapping, which could be especially inaccurate at the beginning of
    the training. We don't include the implementation here as it gets a bit messy
    but still wanted to bring this alternative to your attention for you to have it
    in your toolkit.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式可以在TD更新中使用，以减少在自举中使用的估计值的权重，特别是在训练初期，这些估计值可能会非常不准确。我们在这里不包括实现，因为它会变得有些复杂，但仍然想提醒你，这个替代方法可以作为你工具箱中的一种选择。
- en: With that, we have completed the TD methods! Before finishing the chapter, let's
    take a closer look at the importance of simulations in RL.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们完成了TD方法！在结束本章之前，让我们更深入地了解模拟在强化学习中的重要性。
- en: Understanding the importance of simulation in reinforcement learning
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模拟在强化学习中的重要性
- en: 'As we''ve mentioned multiple times so far, and especially in the first chapter
    when we talked about RL success stories, RL''s hunger for data is orders of magnitude
    greater than regular deep learning. That is why it takes many months to train
    some complex RL agents, over millions and billions of iterations. Since it is
    often impractical to collect such data in a physical environment, we heavily rely
    on simulation models in training RL agents. This brings some challenges along
    with it:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们多次提到的，特别是在第一章讨论强化学习成功案例时，强化学习对数据的需求远远大于常规深度学习。这就是为什么训练一些复杂的强化学习智能体通常需要数月时间，经历数百万或数十亿次迭代。由于在物理环境中收集如此数据通常不可行，我们在训练强化学习智能体时高度依赖模拟模型。这也带来了一些挑战：
- en: Many businesses don't have a simulation model for their business processes.
    This makes it challenging to put RL technology to use in the business.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多企业没有自己的业务过程仿真模型，这使得在业务中应用强化学习技术变得具有挑战性。
- en: When a simulation model exists, it is often too simplistic to capture the real-world
    dynamics. As a result, RL models could easily overfit the simulation environment
    and may fail in deployment. It takes significant time and resources to calibrate
    and validate a simulation model to make it reflect reality sufficiently.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存在仿真模型时，它通常过于简单，无法捕捉真实世界的动态。因此，强化学习模型可能容易过拟合仿真环境，并在部署时失败。要校准和验证一个仿真模型，使其足够反映现实，通常需要大量的时间和资源。
- en: In general, deploying an RL agent that is trained in simulation in the real
    world is not easy, because, well, they are two different worlds. This is against
    the core principle in machine learning that says the training and the test should
    follow the same distribution. This is known as the **simulation-to-real (sim2real)**
    gap.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，将一个在仿真中训练的强化学习代理部署到现实世界并不容易，因为，它们是两个不同的世界。这与机器学习中的核心原则相悖，该原则认为训练和测试应遵循相同的分布。这被称为**仿真到现实（sim2real）**的差距。
- en: Increased fidelity in simulation comes with slowness and compute resource consumption,
    which is a real disadvantage for fast experimentation and RL model development.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仿真精度的提高通常伴随着速度变慢和计算资源的消耗，这对于快速实验和强化学习（RL）模型开发来说是一个真实的劣势。
- en: Many simulation models are not generic enough to cover scenarios that have not
    been encountered in the past but are likely to be encountered in the future.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多仿真模型不够通用，无法涵盖过去未遇到但未来可能遇到的场景。
- en: A lot of commercial simulation software could be hard to integrate (due to the
    lack of a proper API) with the languages RL packages are naturally available in,
    such as Python.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多商业仿真软件可能很难与强化学习包天然支持的语言（如Python）进行集成（由于缺乏适当的API）。
- en: Even when integration is possible, the simulation software may not be flexible
    enough to work with the algorithms. For example, it may not reveal the state of
    the environment, reset it when needed, define terminal states, and so on.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使可以进行集成，仿真软件可能也不够灵活，无法与算法兼容。例如，它可能无法揭示环境的状态，在需要时重置环境，定义终止状态，等等。
- en: Many simulation vendors allow a limited number of sessions per license, whereas
    RL model development is the fastest – you can run thousands of simulation environments
    in parallel.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多仿真供应商在每个许可证下允许的会话数量有限，而强化学习模型开发是最快的——你可以并行运行成千上万的仿真环境。
- en: In this book, we will cover some techniques to overcome some of these challenges,
    such as domain randomization for the sim2real gap and offline RL for environments
    without simulation. However, the key message of this section is that you usually
    should invest in your simulation model to get the best out of RL. In particular,
    your simulation model should be fast, accurate, and scalable to many sessions.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将介绍一些克服这些挑战的技术，例如针对sim2real差距的领域随机化和针对没有仿真的环境的离线强化学习。然而，本节的关键内容是，通常你应该投资于你的仿真模型，以便从强化学习中获得最佳效果。特别是，你的仿真模型应该快速、准确，并且能够扩展到多个会话。
- en: With this, we conclude this chapter. Great work! This marks a milestone in our
    journey with this book. We have come a long way and built a solid foundation of
    RL solution approaches! Next, let's summarize what we have learned and see what
    is coming up in the next chapter.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们结束了这一章。干得好！这标志着我们在本书旅程中的一个里程碑。我们已经走了很长一段路，并建立了强化学习解决方案方法的坚实基础！接下来，让我们总结一下我们所学的内容，并看看下一章将带来什么。
- en: Summary
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered three important approaches to solving MDPs: DP,
    Monte Carlo methods, and temporal-difference learning. We have seen that while
    DP provides exact solutions to MDPs, it relies on the knowledge of the environment.
    Monte Carlo and TD learning methods, on the other hand, explore the environment
    and learn from experience. TD learning, in particular, can learn from even a single
    step transition in the environment. Along the way, we also discussed on-policy
    methods, which estimate the value functions for a behavior policy, and off-policy
    methods for a target policy. Finally, we also discussed the importance of the
    simulator in RL experiments and what to pay attention to when working with one.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了解决MDP的三种重要方法：动态规划（DP）、蒙特卡洛方法和时序差分学习（TD）。我们已经看到，虽然动态规划提供了MDP的精确解，但它依赖于对环境的了解。另一方面，蒙特卡洛和TD学习方法则是通过探索环境并从经验中学习。特别地，TD学习方法可以仅凭环境中的单步转移进行学习。在此过程中，我们还讨论了策略方法，它估计行为策略的价值函数，以及目标策略的离策略方法。最后，我们还讨论了模拟器在强化学习实验中的重要性，以及在使用模拟器时需要注意的事项。
- en: Next, we'll take our journey to the next stage and dive into deep RL, which
    will enable us to solve some real-world problems using RL. Particularly, in the
    next chapter, we'll cover deep Q-learning in detail.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把旅程推向下一个阶段，深入探讨深度强化学习，这将使我们能够使用强化学习解决一些实际问题。特别地，在下一章中，我们将详细介绍深度Q学习。
- en: See you there!
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 到时候见！
- en: Exercises
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 习题
- en: Change these values to see how the optimal policy changes for the modified problem.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 改变这些值，观察修改后的问题中最优策略的变化。
- en: Add a policy evaluation step after the policy improvement step in the value
    iteration algorithm. You can set the number of iterations you want to perform
    the evaluation for before you go back to policy improvement. Use the `policy_evaluation`
    function with a `max_iter` value of your choice. Also, be careful about how you
    track changes in the state values.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在价值迭代算法中的策略改进步骤后，添加一个策略评估步骤。你可以设置一个迭代次数，用于执行评估，然后再返回策略改进。使用`policy_evaluation`函数，并选择一个`max_iter`值。同时，注意如何跟踪状态值的变化。
- en: References
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction.*
    A Bradford Book. URL: [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton, R. S., & Barto, A. G. (2018). *强化学习：一种介绍*。A Bradford Book。网址：[http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)
- en: 'Barto, A. (2006). *Reinforcement learning*. University of Massachusetts – Amherst
    CMPSCI 687\. URL: [https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto, A. (2006). *强化学习*。马萨诸塞大学 – 阿姆赫斯特 CMPSCI 687。网址：[https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)
- en: 'Goldstick, J. (2009). *Importance sampling. Statistics 406: Introduction to
    Statistical Computing at the University of Michigan*: [http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf](http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf)'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goldstick, J. (2009). *重要性抽样。密歇根大学统计学 406：统计计算导论*：[http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf](http://dept.stat.lsa.umich.edu/~jasoneg/Stat406/lab7.pdf)
