- en: Exploring the Learning Environment Landscape - Roboschool, Gym-Retro, StarCraft-II,
    DeepMindLab
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索学习环境的全景 - Roboschool、Gym-Retro、StarCraft-II、DeepMindLab
- en: You have come a long way in your quest to get hands-on experience in building
    intelligent agents to solve a variety of challenging problems. In the previous
    chapters, we looked into several environments that are available in OpenAI Gym.
    In this chapter, we will look beyond the Gym and look at some of the other well
    developed environments that you can use to train your intelligent agents or run
    experiments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在你不断积累经验的过程中，已经走过了很长一段路，目的是通过动手实践构建智能体，解决各种具有挑战性的问题。在前几章中，我们探讨了 OpenAI Gym 中提供的几个环境。在本章中，我们将超越
    Gym，看看一些其他开发完善的环境，供你训练智能体或进行实验。
- en: 'Before we look at other open source libraries that provide good learning environments
    for developing intelligent agents, let''s have a look at a recent class of environments
    added to the OpenAI Gym library. If, like me, you are interested in robotics,
    you will like this one a lot. Yes! It is the robotics class of environments, which
    provides very useful environments for robotic manipulation tasks such as fetching,
    sliding, pushing, and so on with a robotic arm. These robotics environments are
    based on the MuJoCo engine and you may recall from [Chapter 3](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12)[,](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12) *Getting
    Started with OpenAI Gym and Deep Reinforcement Learning*, that the MuJoCo engine
    requires a paid license, unless you are a student and using MuJoCo for personal
    or class use. A summary of these robotics environments is shown in the following
    screenshot, with the environment names and a brief description for each, so that
    you can check them out if you are interested in exploring such problems:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看其他提供良好学习环境的开源库，以帮助开发智能体之前，先来看一下最近添加到 OpenAI Gym 库中的一类环境。如果你像我一样对机器人技术感兴趣，你一定会非常喜欢这个。没错！它就是机器人环境类，提供了许多非常有用的环境，用于机器人臂执行抓取、滑动、推动等操作。这些机器人环境基于
    MuJoCo 引擎，你可能还记得在[第3章](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12)[,](part0056.html#1LCVG0-22c7fc7f93b64d07be225c00ead6ce12)
    *《OpenAI Gym 和深度强化学习入门》*中提到过，MuJoCo 引擎需要付费许可，除非你是学生，并且仅用于个人或课堂用途。以下截图总结了这些机器人环境，包括每个环境的名称和简要描述，供你参考，如果你有兴趣探索这些问题：
- en: '![](img/00290.jpeg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00290.jpeg)'
- en: Gym interface-compatible environments
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Gym 接口兼容的环境
- en: In this section, we will have a deeper look into environments that are compatible
    with the Gym interface out of the box. You should be able to use any of the agents
    we developed in the previous chapters in these environments. Let's get started
    and look at a few very useful and promising learning environments.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨与 Gym 接口完全兼容的环境。你应该能够在这些环境中使用我们在前几章中开发的任何智能体。让我们开始，看看一些非常有用且充满前景的学习环境。
- en: Roboschool
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Roboschool
- en: Roboschool[ (https://github.com/openai/roboschool)](https://github.com/openai/roboschool)
    provides several environments for controlling robots in simulation. It was released
    by OpenAI and the environments have the same interface as the OpenAI Gym environments
    that we have been using in this book. The Gym's MuJoCo-based environments offer
    a rich variety of robotic tasks, but MuJoCo requires a license for use after the
    free trial. Roboschool provides eight environments that quite closely match the
    MuJoCo ones, which is a good news as it offers a free alternative. Apart from
    these eight environments, Roboschool also offers several new and challenging environments.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Roboschool[ (https://github.com/openai/roboschool)](https://github.com/openai/roboschool)
    提供了几个用于仿真控制机器人的环境。它由 OpenAI 发布，且这些环境与我们在本书中使用的 OpenAI Gym 环境接口相同。Gym 的基于 MuJoCo
    的环境提供了丰富多样的机器人任务，但 MuJoCo 在免费试用期过后需要许可证。Roboschool 提供了八个环境，这些环境与 MuJoCo 环境非常相似，这是一个好消息，因为它提供了一个免费的替代方案。除了这八个环境，Roboschool
    还提供了几个新的、具有挑战性的环境。
- en: 'The following table shows a quick comparison between the MuJoCo Gym environments
    and the Roboschool environments:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了 MuJoCo Gym 环境与 Roboschool 环境之间的快速对比：
- en: '| **Brief description** | **MuJoCo environment** | **Roboschool environment**
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| **简要描述** | **MuJoCo 环境** | **Roboschool 环境** |'
- en: '| Make a one-legged 2D robot hop forward as fast as possible | Hopper-v2![](img/00291.jpeg)
    | RoboschoolHopper-v1![](img/00292.jpeg) |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 让一只单腿 2D 机器人尽可能快地向前跳跃 | Hopper-v2![](img/00291.jpeg) | RoboschoolHopper-v1![](img/00292.jpeg)
    |'
- en: '| Make a 2D robot walk | Walker2d-v2![](img/00293.jpeg) | RoboschoolWalker2d-v1![](img/00294.jpeg)
    |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 让一个2D机器人行走 | Walker2d-v2![](img/00293.jpeg) | RoboschoolWalker2d-v1![](img/00294.jpeg)
    |'
- en: '| Make a four-legged 3D robot walk  | Ant-v2![](img/00295.jpeg) | RoboschoolAnt-v1![](img/00296.jpeg)
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 让一个四足3D机器人行走 | Ant-v2![](img/00295.jpeg) | RoboschoolAnt-v1![](img/00296.jpeg)
    |'
- en: '| Make a bipedal 3D robot walk forward as fast as possible without falling
    | Humanoid-v2![](img/00297.jpeg) | RoboschoolHumanoid-v1![](img/00298.jpeg) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 让一个双足3D机器人尽可能快地行走而不摔倒 | Humanoid-v2![](img/00297.jpeg) | RoboschoolHumanoid-v1![](img/00298.jpeg)
    |'
- en: 'A full list of environments that are available as part of the Roboschool library,
    with their state and action spaces, is provided in the following table for your
    quick reference:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了Roboschool库中可用环境的完整列表，包括它们的状态空间和动作空间，供您快速参考：
- en: '| Env ID | Roboschool env | obs space | action space |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 环境ID | Roboschool环境 | 观察空间 | 动作空间 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RoboschoolInvertedPendulum-v1 | ![](img/00299.jpeg) | Box(5,) | Box(1,) |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolInvertedPendulum-v1 | ![](img/00299.jpeg) | Box(5,) | Box(1,) |'
- en: '| RoboschoolInvertedPendulumSwingup-v1 | ![](img/00300.jpeg) | Box(5,) | Box(1,)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolInvertedPendulumSwingup-v1 | ![](img/00300.jpeg) | Box(5,) | Box(1,)
    |'
- en: '| RoboschoolInvertedDoublePendulum-v1 | ![](img/00301.jpeg) | Box(9,) | Box(1,)
    |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolInvertedDoublePendulum-v1 | ![](img/00301.jpeg) | Box(9,) | Box(1,)
    |'
- en: '| RoboschoolReacher-v1 | ![](img/00302.jpeg) | Box(9,) | Box(2,) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolReacher-v1 | ![](img/00302.jpeg) | Box(9,) | Box(2,) |'
- en: '| RoboschoolHopper-v1 | ![](img/00303.jpeg) | Box(15,) | Box(3,) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolHopper-v1 | ![](img/00303.jpeg) | Box(15,) | Box(3,) |'
- en: '| RoboschoolWalker2d-v1 | ![](img/00304.jpeg) | Box(22,) | Box(6,) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolWalker2d-v1 | ![](img/00304.jpeg) | Box(22,) | Box(6,) |'
- en: '| RoboschoolHalfCheetah-v1 | ![](img/00305.jpeg) | Box(26,) | Box(6,) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolHalfCheetah-v1 | ![](img/00305.jpeg) | Box(26,) | Box(6,) |'
- en: '| RoboschoolAnt-v1 | ![](img/00306.jpeg) | Box(28,) | Box(8,) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolAnt-v1 | ![](img/00306.jpeg) | Box(28,) | Box(8,) |'
- en: '| RoboschoolHumanoid-v1 | ![](img/00307.jpeg) | Box(44,) | Box(17,) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolHumanoid-v1 | ![](img/00307.jpeg) | Box(44,) | Box(17,) |'
- en: '| RoboschoolHumanoidFlagrun-v1 | ![](img/00308.jpeg) | Box(44,) | Box(17,)
    |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolHumanoidFlagrun-v1 | ![](img/00308.jpeg) | Box(44,) | Box(17,)
    |'
- en: '| RoboschoolHumanoidFlagrunHarder-v1 | ![](img/00309.jpeg) | Box(44,) | Box(17,)
    |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolHumanoidFlagrunHarder-v1 | ![](img/00309.jpeg) | Box(44,) | Box(17,)
    |'
- en: '| RoboschoolPong-v1 | ![](img/00310.jpeg) | Box(13,) | Box(2,) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| RoboschoolPong-v1 | ![](img/00310.jpeg) | Box(13,) | Box(2,) |'
- en: Quickstart guide to setting up and running Roboschool environments
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速入门指南：设置和运行Roboschool环境
- en: 'Roboschool environments make use of the open source Bulletphysics engine instead
    of the proprietary MuJoCo engine. Let''s quickly have a look at a Roboschool environment
    so that you know how to use any environment from the Roboschool library if you
    happen to find it useful for your work. To get started, we will have to first
    install the Roboschool Python library in our `rl_gym_book` conda environment.
    Because the library depends on several components, including the Bulletphysics
    engine, there are several installation steps involved, which are listed in the
    official Roboschool GitHub repository here: [https://github.com/openai/roboschool](https://github.com/openai/roboschool).
    To make things simpler, you can use the script in the book''s code repository
    at `ch9/setup_roboschool.sh `to automatically compile and install the `Roboschool`
    library for you. Follow these steps to run the script:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Roboschool环境使用开源的Bulletphysics引擎，而不是专有的MuJoCo引擎。让我们快速浏览一个Roboschool环境，以便您了解如何使用Roboschool库中的任何环境，如果您觉得它对您的工作有帮助的话。首先，我们需要在`rl_gym_book`
    conda环境中安装Roboschool Python库。由于该库依赖于多个组件，包括Bulletphysics引擎，因此安装过程涉及几个步骤，具体步骤可以在官方Roboschool
    GitHub仓库中找到：[https://github.com/openai/roboschool](https://github.com/openai/roboschool)。为了简化操作，您可以使用本书代码仓库中的脚本`ch9/setup_roboschool.sh`，该脚本会自动编译和安装`Roboschool`库。请按照以下步骤运行该脚本：
- en: Activate the `rl_gym_book` conda environment using `source activate rl_gym_book`.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活`rl_gym_book` conda环境，使用`source activate rl_gym_book`。
- en: Navigate to the `ch9` folder with `cd ch9`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cd ch9`导航到`ch9`文件夹。
- en: Make sure that the script's execution bit is set as `chmod a+x setup_roboschool.sh`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保脚本的执行权限已设置为`chmod a+x setup_roboschool.sh`。
- en: Run the script with `sudo`:`./setup_roboschool.sh`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sudo`运行脚本：`./setup_roboschool.sh`。
- en: 'This should install the required system dependencies, fetch and compile a compatible
    source code of the bullet3 physics engine; pull the Roboschool source code to
    the `software` folder under your home directory; and finally compile, build, and
    install the Roboschool library in the `rl_gym_book` conda environment. If the
    setup completes successfully, you will see the following message printed on the
    console:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会安装所需的系统依赖项，获取并编译兼容的bullet3物理引擎源代码；将Roboschool源代码拉取到您主目录下的`software`文件夹；最后在`rl_gym_book`
    conda环境中编译、构建并安装Roboschool库。如果设置成功完成，您将在控制台看到以下信息：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can run a quickstart demo script using the following command:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令运行一个快速入门演示脚本：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will launch a funny-to-watch robo-race in which you will see a hopper,
    half-cheetah, and humanoid running a race! The interesting aspect is that each
    of the robots is being controlled by a reinforcement learning-based trained policy.
    The race will look similar to this snapshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动一场有趣的机器人竞赛，您将看到一只跳跃者、半猎豹和类人机器人进行比赛！有趣的是，每个机器人都由基于强化学习训练的策略控制。比赛将看起来像这个快照：
- en: '![](img/00311.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00311.jpeg)'
- en: Once it has been installed, you can create a Roboschool environment and use
    one of the agents we developed in an earlier chapter to train and run on these
    environments.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以创建一个Roboschool环境，并使用我们在前面章节中开发的代理来训练并在这些环境中运行。
- en: 'You can use the `run_roboschool_env.py` script in this chapter''s code repository
    at  [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9) to
    check out any of the Roboschool environments. For example, to check out the `RoboschoolInvertedDoublePendulum-v1`
    environment, you can run the following script:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用本章代码仓库中的`run_roboschool_env.py`脚本 [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9)
    来查看任何Roboschool环境。例如，要查看`RoboschoolInvertedDoublePendulum-v1`环境，您可以运行以下脚本：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can use any other Roboschool environment name from the previous table, as
    well as new Roboschool environments when they are made available.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用前面表格中列出的任何其他Roboschool环境名称，以及在发布时提供的新的Roboschool环境。
- en: Gym retro
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gym retro
- en: 'Gym Retro ([https://github.com/openai/retro](https://github.com/openai/retro))
    is a relatively new (released on May 25, 2018) Python library released by OpenAI ([https://blog.openai.com/gym-retro/](https://blog.openai.com/gym-retro/))
    as a research platform for developing reinforcement learning algorithms for game
    playing. Although the Atari suite of 60+ games was available in OpenAI Gym, the
    total number of games available was limited. Gym Retro supports the use of games
    developed for several console/retro gaming platforms, such as Nintendo''s NES,
    SNES, Game Boy consoles, Sega Genesis, and Sega Master System to name a few. This
    is made possible with the use of emulators using the Libretro API:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Gym Retro ([https://github.com/openai/retro](https://github.com/openai/retro))
    是OpenAI于2018年5月25日发布的一个相对较新的Python库 ([https://blog.openai.com/gym-retro/](https://blog.openai.com/gym-retro/))，作为一个用于开发游戏玩法强化学习算法的研究平台。尽管在OpenAI
    Gym中有一个包含60多个游戏的Atari游戏合集，但可用的游戏总数有限。Gym Retro支持使用为多个控制台/复古游戏平台开发的游戏，例如任天堂的NES、SNES、Game
    Boy控制台、世嘉Genesis和世嘉Master System等。通过使用Libretro API的模拟器，这一切成为可能：
- en: '![](img/00312.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00312.jpeg)'
- en: Gym Retro provides convenient wrappers to turn more than 1,000 such video games
    into Gym interface-compatible learning environments! Isn't that great! Several
    new learning environments but with the same interface, so that we can easily train
    and test the agents we have developed so far without any necessary changes to
    the code...
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Gym Retro提供了便捷的包装器，将超过1000款视频游戏转化为兼容Gym接口的学习环境！是不是很棒！多个新的学习环境，但接口保持一致，这样我们就可以轻松地训练和测试我们到目前为止开发的代理，无需对代码做任何必要的修改……
- en: 'To get a feel for how easy it is to use the environments in Gym Retro, let''s
    put the installation steps aside for a moment and quickly look at the code to
    create a new Gym Retro environment once it is installed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了感受在Gym Retro中使用环境的简易性，我们暂时把安装步骤放在一边，快速看看安装后如何创建一个新的Gym Retro环境的代码：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code snipped will create an `env` object that has the same interfaces and
    methods, such as `step(...)`, `reset()` and `render()`, as all the Gym environments
    we have seen before.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段将创建一个`env`对象，该对象具有与我们之前看到的所有Gym环境相同的接口和方法，例如`step(...)`、`reset()`和`render()`。
- en: Quickstart guide to setup and run Gym Retro
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gym Retro的快速入门指南
- en: 'Let''s try out the Gym Retro library quickly by installing the pre-built binaries
    using pip with the following command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用以下命令，快速安装预构建的二进制文件并试用Gym Retro库：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the installation is successful, we can have a sneak peek into one of the
    available Gym Retro environments using the following script:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，我们可以通过以下脚本快速查看其中一个可用的Gym Retro环境：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Running this script will bring up a window with the Airstriker game and show
    the spaceship taking random actions. The game window will look something like
    this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此脚本后，将会弹出一个窗口，显示《Airstriker》游戏并展示飞船执行随机动作。游戏窗口将呈现如下图所示：
- en: '![](img/00313.jpeg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00313.jpeg)'
- en: 'Before we move on, one thing to note is that the **ROM** (**Read-Only Memory**)
    file that contains the whole game data is not made freely available for all games.
    The ROMs for some non-commercial console games such as Airstriker (used in the
    previous script), Fire, Dekadrive, Automaton, Fire, Lost Marbles, and so on are
    included with the Gym Retro library and are free to use. Other games, such as
    the Sonic series (Sonic The Hedgehog, Sonic The Hedgehog 2, Sonic 3 & Knuckles)
    require ROMs to be purchased for legal use from places such as [Steam](https://store.steampowered.com/app/71113/Sonic_The_Hedgehog/).
    This is a barrier for hobbyists, students, and other enthusiasts who wish to develop
    algorithms using such environments. But at least this barrier is relatively small,
    as it costs about USD 1.69 on Steam for the Sonic The Hedgehog ROM. Once you have
    the ROM files for the games, the Gym Retro library provides a script to import
    them into the library like so:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，值得注意的是，包含整个游戏数据的**ROM**（**只读内存**）文件并不是所有游戏都可以免费获得。一些非商业性的游戏机游戏的ROM文件，如《Airstriker》（在前面的脚本中使用的游戏）、《Fire》、Dekadrive、Automaton、《Fire》、《Lost
    Marbles》等，已包含在Gym Retro库中，可以免费使用。其他游戏，如《刺猬索尼克》系列（《刺猬索尼克》、《刺猬索尼克 2》、《刺猬索尼克 3 &
    Knuckles》），需要购买ROM文件以合法使用，可以通过像[Steam](https://store.steampowered.com/app/71113/Sonic_The_Hedgehog/)这样的平台进行购买。这对于希望在此类环境中开发算法的爱好者、学生以及其他热衷者来说，是一个障碍。但至少这个障碍相对较小，因为在Steam上购买《刺猬索尼克》的ROM大约需要1.69美元。一旦你拥有了游戏的ROM文件，Gym
    Retro库提供了一个脚本，允许你将这些文件导入到库中，方法如下：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that when creating a new Gym Retro environment, we need the name of the
    game as well as the state of the game `retro.make(game='NAME_OF_GAME', state='NAME_OF_STATE')`
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在创建新的Gym Retro环境时，我们需要提供游戏名称以及游戏状态`retro.make(game='游戏名称', state='状态名称')`
- en: 'To get a list of available Gym Retro environments, you can run the following
    command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取可用Gym Retro环境的列表，可以运行以下命令：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And to get a list of available game states, you can run the following Python
    script:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取可用游戏状态的列表，可以运行以下Python脚本：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So far, we have gotten ourselves familiar with the Gym Retro library. Let's
    analyze what advantages or new features this library provides us in addition to
    what we have already seen and used. First, the Gym Retro library utilizes newer
    games consoles (such as the SEGA Genesis) than the Atari console. For comparison,
    the SEGA Genesis games console has 500 times as much RAM as the Atari console,
    enabling better visuals and a greater range of controls. This provides us with
    learning environments that are relatively sophisticated, and some more complex
    tasks and challenges for our intelligent agents to learn and solve. Second, several
    of these console games are progressive in nature, where the complexity of the
    game typically increases with every level and the levels have several similarities
    in some aspects (such as the goal, object appearances, physics, and so on) while
    also providing diversity in other aspects (such as the layout, new objects, and
    so on). Such a training environment with levels of progressively increasing difficulty
    help in developing intelligent agents that can learn to solve tasks in general
    without being very task/environment-specific (such as overfitting in supervised
    learning). Agents can learn to transfer their skills and learning from one level
    to another, and then to another game. This area is under active research and is
    usually known as curriculum learning, staged learning, or incremental evolution.
    After all, ultimately we are interested in developing intelligent agents that
    can learn to solve tasks in general and not just the specific tasks the agent
    was trained on. The Gym Retro library provides some useful, though game-only,
    environments to facilitate such experiments and research.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经熟悉了 Gym Retro 库。接下来，分析一下这个库在我们已经看到和使用的内容之外，提供了哪些优势或新特性。首先，Gym Retro
    库使用了比 Atari 主机更新的游戏主机（如 SEGA Genesis）。做个对比，SEGA Genesis 游戏主机的 RAM 是 Atari 主机的
    500 倍，这使得它能够提供更好的视觉效果和更广泛的控制选项。这为我们提供了相对复杂的学习环境，并且为我们的智能体提供了一些更复杂的任务和挑战，供它们学习和解决。其次，这些主机游戏中的几个是渐进性的，游戏的复杂度通常随着每一关的提升而增加，而且各关卡在某些方面（例如目标、物体外观、物理效果等）具有许多相似之处，同时在其他方面（如布局、新物体等）也提供了多样性。这样的训练环境，随着难度逐步增加的关卡，帮助开发能够学习解决一般任务的智能体，而不仅仅是特定的任务或环境（如监督学习中的过拟合）。智能体能够学会将自己在一个关卡中的技能和知识转移到下一个关卡，进而转移到另一个游戏中。这个领域正在积极研究，通常被称为课程学习、分阶段学习或渐进式进化。毕竟，我们最终的目标是开发能够学习解决一般任务的智能体，而不仅仅是训练中给定的具体任务。Gym
    Retro 库提供了一些有用的、仅限于游戏的环境来促进此类实验和研究。
- en: Other open source Python-based learning environments
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他开源基于 Python 的学习环境
- en: In this section, we will discuss recent Python-based learning environments that
    provide a good platform for intelligent agent development but don't necessarily
    have a Gym-compatible environment interface. Although they do not provide Gym-compatible
    interfaces, the environments we will be discussing in this section were carefully
    selected to make sure that either a Gym wrapper (to make it compatible with the
    Gym interface) is available, or they are easy to implement in order to use and
    experiment with the agents we have developed through this book. As you can guess,
    this list of good Python-based learning environments for developing intelligent
    agents will grow in the future, as this area is being very actively researched
    at the moment. The book's code repository will have information and quickstart
    guides for new environments as they become available in the future. Sign up for
    update notifications at the book's GitHub repository to get those updates. In
    the following sub-sections, we will discuss some of the most promising learning
    environments that are readily available for use.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些最近的基于 Python 的学习环境，这些环境为智能体开发提供了良好的平台，但不一定具有与 Gym 兼容的环境接口。虽然它们没有提供与
    Gym 兼容的接口，但本节中讨论的这些环境经过精心挑选，确保要么已经有 Gym 的包装器（使其与 Gym 接口兼容），要么它们很容易实现，可以用来测试和实验我们在本书中开发的智能体。正如你所猜测的那样，未来将有更多的优秀基于
    Python 的智能体开发学习环境出现，因为这一领域目前正在积极研究。本书的代码库将提供新环境的信息和快速入门指南，一旦这些新环境发布，你可以在本书的 GitHub
    仓库中注册获取更新通知。在接下来的子章节中，我们将讨论一些现成的、有前景的学习环境，供大家使用。
- en: StarCraft II - PySC2
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 星际争霸 II - PySC2
- en: StarCraft II is very popular, in fact one of the most successful real-time strategy
    games ever, and is played by millions of people worldwide. It even has a world
    championship league ([https://wcs.starcraft2.com/en-us/](https://wcs.starcraft2.com/en-us/))!
    The environment is quite complex and the main goal is to build an army base, manage
    an economy, defend the base, and destroy enemies. The player controls the base
    camp and the army from a third-person view of the scene. If you are not familiar
    with StarCarft, you should watch a few games online to get a feel for how complex
    and fast-paced the game is.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 《星际争霸 II》非常受欢迎，实际上是有史以来最成功的实时战略游戏之一，全球有数百万人在玩这款游戏。它甚至有一个世界锦标联赛（[https://wcs.starcraft2.com/en-us/](https://wcs.starcraft2.com/en-us/)）！游戏环境相当复杂，主要目标是建立一个军队基地，管理经济，防守基地，并摧毁敌人。玩家从第三人称视角控制基地和军队。如果你不熟悉《星际争霸》，你应该先观看几场在线比赛，以便了解游戏的复杂性和快速节奏。
- en: For humans to play this real-time strategy game well, it takes a lot of practice (several
    months even; in fact, professional players train for several years), planning,
    and quick responses. Although software agents can press several software buttons
    per frame to make pretty fast moves, speed of action is not the only factor that
    contributes to victory. The agent has to multi-task and micro-manage the army
    units, and maximize their score, which is several orders of magnitude more complex
    than the Atari games.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人类要想在这款实时战略游戏中表现出色，需要大量的练习（甚至需要几个月，实际上职业玩家需要数年训练），计划和快速反应。尽管软件代理可以在每帧中按下多个软件按钮，做出相当快速的动作，但行动速度并不是唯一决定胜利的因素。代理还需要进行多任务处理和微管理军队单位，并最大化得分，这比Atari游戏复杂几个数量级。
- en: Blizzard, the company that made StarCraft II, released the StarCraft II API,
    which provides the necessary hooks to interface with the StarCraft II game and
    control it without limitations. That enables several new possibilities, such as
    the development of the intelligent agents that we are after. They even have a
    separate **End User License Agreement** (**EULA**) for open use of the environment
    under an AI and machine learning license! This was a very welcome move by a company
    such as Blizzard, which is in the business of making and selling games. The open
    source the **StarCraft2** (**SC2**) client protocol implementation and provides
    the Linux installation package, along with several accessories such as the map
    packs, free to download from their GitHub page at [https://github.com/Blizzard/s2client-proto](https://github.com/Blizzard/s2client-proto).
    On top of that, Google DeepMind has open sourced their PySC2 library, which exposes
    the SC2 client interfaces through Python and provides a wrapper that makes it
    an RL environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 《星际争霸 II》的制作公司暴雪发布了《星际争霸 II》API，提供了与《星际争霸 II》游戏接口的必要钩子，使得玩家可以不受限制地控制游戏。这为开发我们所追求的智能代理提供了新的可能性。暴雪甚至为在AI和机器学习许可下公开使用该环境提供了单独的**最终用户许可协议**（**EULA**）！对于像暴雪这样从事游戏制作和销售的公司来说，这一举措非常受欢迎。暴雪将**StarCraft2**（**SC2**）客户端协议实现开源，并提供了Linux安装包，以及多个附加组件，如地图包，用户可以从他们的GitHub页面免费下载，链接为[https://github.com/Blizzard/s2client-proto](https://github.com/Blizzard/s2client-proto)。除此之外，Google
    DeepMind还开源了他们的PySC2库，通过Python暴露了SC2客户端接口，并提供了一个封装，使其成为一个强化学习（RL）环境。
- en: 'The following screenshot shows the PySC2 UI, with the feature layers available
    as observations to the agents shown on the right and a simplified overview of
    the game scene on the left:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了PySC2的用户界面，右侧是可供代理作为观察的特征层，左侧是游戏场景的简化概览：
- en: '![](img/00314.jpeg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00314.jpeg)'
- en: If you are interested in these types of environment, and especially if you are
    a game developer, you might be interested in the Dota 2 environment as well. Dota
    2 is a real-time strategy game, like StarCraft II, and is played between two teams
    of five players with each player controlling a hero character. You can learn more
    about how OpenAI developed a team of five neural network-based agents that have
    learned to work in a team, played 180 years' worth of games in a day, learned
    to overcome several challenges (including high-dimensional and continuous state
    and action spaces, and long-term horizons), all while playing against themselves
    using self-play! You can read more about the five-agent team at https://blog.openai.com/openai-five/.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些类型的环境感兴趣，尤其是如果你是游戏开发者，你可能也对 Dota 2 环境感兴趣。Dota 2 是一款实时战略游戏，和 StarCraft
    II 类似，由两队五名玩家进行对战，每个玩家控制一个英雄角色。你可以了解更多关于 OpenAI 如何开发一个由五个神经网络代理组成的团队，他们学会了团队协作，单日内进行
    180 年游戏量的训练，学习如何克服多个挑战（包括高维连续状态和动作空间以及长期的决策时间），而这一切都发生在自我对战中！你可以在 https://blog.openai.com/openai-five/
    阅读更多关于五代理团队的内容。
- en: Quick start guide to setup and run StarCraft II PySC2 environment
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: StarCraft II PySC2 环境设置和运行的快速入门指南
- en: We will look at how you can quickly set up and get started with the StarCraft
    II environment. As always, use the README files in the code repository for the
    latest up-to-date instructions, as things as the links and the versions may change.
    If you have not done so already, star and watch the book's code repository to
    get notified about changes and updates.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示如何快速设置并开始使用 StarCraft II 环境。和往常一样，请使用代码库中的 README 文件获取最新的操作说明，因为链接和版本可能会发生变化。如果你还没有这么做，请为该书的代码库添加星标并关注，以便收到关于更改和更新的通知。
- en: Downloading the StarCraft II Linux packages
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载 StarCraft II Linux 包
- en: 'Download the latest Linux packages for the StarCraft game from https://github.com/Blizzard/s2client-proto#downloads
    and extract it onto your hard disk at `~/StarCraftII`. For example, to download
    version 4.1.2 to your `~/StarCraftII/` folder, you can use the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从 https://github.com/Blizzard/s2client-proto#downloads 下载 StarCraft 游戏的最新 Linux
    包，并将其解压到硬盘上的 `~/StarCraftII` 目录。例如，要将版本 4.1.2 下载到你的`~/StarCraftII/`文件夹中，可以使用以下命令：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s unzip and extract the files to the `~/StarCraftII/` directory:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将文件解压到`~/StarCraftII/`目录：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that, as mentioned on the download page, the files are password protected
    with the password `'iagreetotheeula`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如下载页面所述，这些文件是受密码保护的，密码是`'iagreetotheeula`。
- en: By typing that, Blizzard ensures we agree to be bound by the terms of their
    AI and machine learning license, found on the download page.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过输入该命令，暴雪确保我们同意接受其 AI 和机器学习许可协议，详情请见下载页面。
- en: Downloading the SC2 maps
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载 SC2 地图
- en: We will need the StarCraft II map packs and the mini games pack to get started.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要 StarCraft II 地图包和迷你游戏包才能开始。
- en: Download the map packs from [https://github.com/Blizzard/s2client-proto#map-packs](https://github.com/Blizzard/s2client-proto#map-packs)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从[https://github.com/Blizzard/s2client-proto#map-packs](https://github.com/Blizzard/s2client-proto#map-packs)下载地图包
- en: Extract them to your `~/StarCraftII/Maps` directory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解压到你的`~/StarCraftII/Maps`目录。
- en: 'As an example, let''s download the Ladder maps released for 2018 season 2 using
    the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将使用以下命令下载 2018 年第二赛季发布的梯子地图：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s unzip the maps to the `~/StarCraftII/Maps` directory:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将地图解压到`~/StarCraftII/Maps`目录：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we will download and unzip the mini game map files:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将下载并解压迷你游戏地图文件：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Installing PySC2
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PySC2
- en: 'Let''s install the PySC2 library for the RL environment interface, along with
    the required dependencies. This step is going to be straightforward, as there
    is a PyPi Python package for the PySC2 library:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装 PySC2 库以供 RL 环境接口使用，并安装所需的依赖项。这个步骤会很简单，因为 PySC2 库在 PyPi 上有提供 Python 包：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Playing StarCraftII yourself or running sample agents
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩 StarCraft II 或运行示例代理
- en: 'To test whether the installation went fine and to see what the StarCarftII
    learning environment looks like, you can quickly start up a randomly-acting agent
    on the Simple64 map or the CollectMineralShards map using the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试安装是否成功并查看 StarCraft II 学习环境的样子，你可以使用以下命令在 Simple64 地图或 CollectMineralShards
    地图上快速启动一个随机行为的代理：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can also load another available map for the environment. For example, the
    following command loads the CollectMineralShards map:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以加载环境中的其他可用地图。例如，以下命令加载 CollectMineralShards 地图：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This should bring up a UI showing you the actions taken by the random agent,
    which gives you an idea of what the valid actions are and helps you to visualize
    what is going on in the environment as the agent is acting.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会弹出一个 UI，显示随机智能体所采取的动作，帮助你了解有效的动作是什么，并帮助你可视化智能体在环境中行动时的情况。
- en: 'To play the game yourself, PySC2 offers a human agent interface, which is quite
    useful for debugging (and, if you are interested, playing!) purposes. The following
    is the command to run and play the game yourself:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 若要自己玩这个游戏，PySC2 提供了一个人类智能体接口，这对于调试（如果你感兴趣，也可以用来玩游戏！）非常有用。以下是运行并亲自玩游戏的命令：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can also run a sample agent that is scripted to collect mineral shards,
    which is one of the tasks in the game, using the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以运行一个示例智能体，该智能体的脚本任务是收集矿物碎片，这是游戏中的一项任务，使用以下命令：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Watch the book's code repository for new agent source code and instructions
    to train and test new agents with advanced skills. You can also customize the
    agents we developed in the previous chapter to learn to play StarCraftII. If you
    do, send a pull request to the book's code repository, shoot the author an email,
    or shout it out so that everyone knows what cools things you have done!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注本书的代码库，获取新的智能体源代码以及训练和测试具有高级技能的新智能体的说明。你还可以自定义我们在上一章中开发的智能体，学习如何玩《星际争霸II》。如果你这样做了，请向本书的代码库提交一个拉取请求，给作者发封邮件，或者大声说出来，让大家知道你做了哪些很酷的事情！
- en: DeepMind lab
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepMind lab
- en: DeepMind Lab ([https://github.com/deepmind/lab](https://github.com/deepmind/lab))
    is a 3D learning environment that provides a suite of environments with challenging
    tasks, such as 3D navigation through mazes and puzzle solving. It is built based
    on a handful of pieces of open source software, including the famous Quake III
    Arena.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind Lab ([https://github.com/deepmind/lab](https://github.com/deepmind/lab))
    是一个 3D 学习环境，提供了一系列具有挑战性任务的环境，例如通过迷宫进行 3D 导航和解谜。它是基于一系列开源软件构建的，包括著名的《Quake III
    Arena》。
- en: 'The environment interface is very similar to the Gym interface that we have
    used extensively in this book so far. To get a feel for what the environment interface
    actually looks like, have a look at the following code snippet:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 环境接口与我们在本书中广泛使用的 Gym 接口非常相似。为了让你了解环境接口的实际样子，可以查看以下代码片段：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This code, though not one-to-one compatible with the OpenAI Gym interface, provides
    a very similar interface.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码虽然与 OpenAI Gym 接口不是一一兼容，但提供了一个非常相似的接口。
- en: DeepMind Lab learning environment interface
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepMind Lab 学习环境接口
- en: We will briefly discuss the environment interface for **DeepMind Lab (DM Lab)**,
    so that you are familiar with it, can see the similarities with the OpenAI Gym
    interface, and can start experimenting with agents in DM Lab environments!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要讨论 **DeepMind Lab (DM Lab)** 的环境接口，以便你能够熟悉它，看到它与 OpenAI Gym 接口的相似之处，并开始在
    DM Lab 环境中进行智能体实验！
- en: reset(episode=-1, seed=None)
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: reset(episode=-1, seed=None)
- en: This is similar to the `reset()` method we saw in the Gym interface, but unlike
    Gym environments, DM Lab's `reset` method call does not return the observation.
    We will see later how to get observations, so for now, we will discuss DM Lab's `reset(episode=-1,
    seed=None)` method. It resets the environment to an initial state and needs to
    be called at the end of every episode, in order for a new episode to be created.
    The optional `episode` argument takes an integer value to specify the level in
    a specific episode. If the `episode` value is not set, or is set to `-1`, the
    levels are loaded in numerical order. The `seed` argument is also optional and
    is used to seed the environment's random number generator for reproducibility
    purposes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于我们在 Gym 接口中看到的 `reset()` 方法，但与 Gym 环境不同，DM Lab 的 `reset` 方法调用不会返回观察结果。我们稍后会看到如何获取观察结果，所以现在我们讨论的是
    DM Lab 的 `reset(episode=-1, seed=None)` 方法。它将环境重置为初始状态，并且需要在每一集的结尾调用，以便创建新的集。可选的
    `episode` 参数接受一个整数值，用于指定特定集中的关卡。如果未设置 `episode` 值，或者将其设置为 `-1`，则关卡将按数字顺序加载。`seed`
    参数也是可选的，用于为环境的随机数生成器设置种子，以便实现可重复性。
- en: step(action, num_steps=1)
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: step(action, num_steps=1)
- en: This is similar to the Gym interface's `step(action)` method, but like with
    the `reset(...)` method, the call to this method does not return the next observation
    (or reward, done, and info). Calling this method advances the environment by `num_steps`
    number of frames, executing the action defined by `action` in every frame. This
    action-repeat behavior is useful in cases where we would like the same action
    to be applied for four or so consecutive frames, which was actually found by several
    researchers to help with learning. There are Gym environment wrappers that accomplish
    this action-repeat behavior.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 Gym 接口的`step(action)`方法类似，但与`reset(...)`方法一样，调用该方法不会返回下一个观测（或奖励、结束状态、信息）。调用此方法会将环境推进`num_steps`帧，且在每一帧中执行由`action`定义的动作。这种动作重复的行为在我们希望相同的动作在连续四帧左右应用时非常有用，事实上，几位研究人员发现这种方法有助于学习。有一些
    Gym 环境包装器可以实现这种动作重复行为。
- en: observations()
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: observations()
- en: 'This is the method we would use after the call to `reset(...)` or `step(action)`
    to receive the observations from DM Lab environments. This method returns a Python
    dictionary object with every type of observation that we specified from the list
    of available types for the environment. For example, if we wanted **RGBD** (**Red-Green-Blue-Depth**)
    information about the environment as the observation type, we can specify that
    when we initialize the environment using the `''RGBD''` key, we can then retrieve
    this information from the returned observation dictionary using the same `''RGBD''`
    key. A simple example to illustrate this is shown here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在调用`reset(...)`或`step(action)`之后用来接收来自 DM Lab 环境的观测的方法。该方法返回一个 Python 字典对象，其中包含我们从环境的可用类型列表中指定的每种类型的观测。例如，如果我们希望将环境的**RGBD**（**红绿蓝深度**）信息作为观测类型，我们可以在初始化环境时通过`'RGBD'`键指定，随后可以通过相同的`'RGBD'`键从返回的观测字典中检索该信息。下面是一个简单的示例来说明这一点：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There are also other observation types that are supported by DM Lab environments.
    We can use `observation_spec()` to get a list of supported observation types,
    which we will discuss very shortly.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DM Lab 环境还支持其他类型的观测。我们可以使用`observation_spec()`来获取支持的观测类型列表，我们将在稍后详细讨论。
- en: is_running()
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: is_running()
- en: This method is analogous (in the opposite sense) to the `done` Boolean returned
    by the Gym interface's `step(action)` method.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法类似于（但意义相反）Gym 接口的`step(action)`方法返回的`done`布尔值。
- en: This method will return `False` when the environment is done with an episode
    or stops running. It will return `True` as long as the environment is running.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境完成一个回合或停止运行时，该方法将返回`False`。只要环境仍在运行，它将返回`True`。
- en: observation_spec()
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: observation_spec()
- en: This method is similar to `env.observation_space()`, which we used with the
    Gym environments. This method returns a list specifying all the available observations
    supported by the DM Lab environment. It also includes specifications about level-dependent
    custom observations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法类似于我们在 Gym 环境中使用的`env.observation_space()`。该方法返回一个列表，指定了 DM Lab 环境支持的所有可用观测。它还包括有关与关卡相关的自定义观测的规格说明。
- en: 'The specifications contain the name, type, and shape of the tensor or string
    that will be returned if that specification name is specified in the observation
    list (such as the `''RGBD''` example previously). For example, the following code
    snippet lists two items in the list that will be returned to give you an idea
    about what the specs contain:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规格包含了如果在观测列表中指定了该规格名称时将返回的张量或字符串的名称、类型和形状（例如之前提到的`'RGBD'`示例）。例如，以下代码片段列出了列表中将返回的两个项，帮助你了解规格内容：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To quickly understand how this method can be used, let''s look at the following
    lines of code and the output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速理解如何使用这个方法，让我们看看以下代码行和输出：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: action_spec()
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: action_spec()
- en: Similar to the `observation_spec()` the `action_spec()` method returns a list
    containing the min, max, and a name for each of the elements in the space.  The
    `min` and `max` values respectively represent the minimum and maximum value that
    the corresponding element in the action space can be set to. The length of this
    list will equal the dimension/shape of the action space. This is analogous to
    `env.action_space`, which we have been using with the Gym environments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`observation_spec()`，`action_spec()`方法返回一个列表，其中包含空间中每个元素的最小值、最大值和名称。`min`和`max`值分别代表动作空间中相应元素可以设置的最小值和最大值。这个列表的长度等于动作空间的维度/形状。这类似于我们在
    Gym 环境中使用的`env.action_space`。
- en: 'The following code snippet gives us a quick look into what the return values
    from a call to this method will look like:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段让我们快速了解调用此方法时返回值的样子：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: num_steps()
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: num_steps()
- en: This utility method is like a counter that counts the number of frames executed
    by the environment since the last `reset()` call.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具方法就像一个计数器，它计算自上次调用`reset()`以来环境执行的帧数。
- en: fps()
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fps()
- en: This utility method returns the number of frames (or environment steps) executed
    per actual (wall-clock) second. This is useful to keep track of the environment
    execution speeds and how fast the agent can sample from the environment.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具方法返回每秒钟实际（墙钟）执行的帧数（或环境步数）。这个方法对于跟踪环境的执行速度以及代理从环境中采样的速度非常有用。
- en: events()
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: events()
- en: This utility method can be useful for debugging as it returns a list of events
    that have occurred since the last call to `reset()` or `step(...)`. The returned
    tuple contains a name and a list of observations.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具方法可以在调试时非常有用，因为它返回自上次调用`reset()`或`step(...)`以来发生的事件列表。返回的元组包含一个名称和一组观察数据。
- en: close()
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: close()
- en: Like the `close()` method available with Gym environments, this method also
    closes the environment instance and releases the underlying resources, such as
    the Quake III Arena instance.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 与Gym环境中的`close()`方法类似，这个方法也会关闭环境实例并释放底层资源，例如Quake III Arena实例。
- en: Quick start guide to setup and run DeepMind Lab
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置和运行DeepMind Lab的快速入门指南
- en: With the familiarity we got after the brief discussion about the DeepMind Lab
    environment interface in the previous section, we are ready to get some hands
    on experience with this learning environment. In the following sub-sections, we
    will go through the steps to setup DeepMind Lab and  run a sample agent.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面简短讨论了DeepMind Lab环境接口之后，我们已经准备好亲自体验这个学习环境。在接下来的子章节中，我们将逐步介绍设置DeepMind Lab并运行一个示例代理的过程。
- en: Setting up and installing DeepMind Lab and its dependencies
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置和安装DeepMind Lab及其依赖项
- en: 'The DeepMind Lab library uses Bazel as the build tool, which in turn requires
    Java. The book''s code repository has a script that you can run to easily set
    up DeepMind Lab. You can find the script under the chapter9 folder at [https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9).
    You can run the script using the following command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind Lab库使用Bazel作为构建工具，而Bazel又依赖于Java。书中的代码库有一个脚本，可以帮助你轻松设置DeepMind Lab。你可以在章节9文件夹下找到这个脚本，路径是[https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/tree/master/ch9)。你可以使用以下命令运行该脚本：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This script will take some time to finish, but will automatically install all
    the necessary packages and libraries, including Bazel and its dependencies, and
    set everything up for you.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本可能需要一些时间来完成，但它会自动安装所有必要的包和库，包括Bazel及其依赖项，并为你设置好一切。
- en: Playing the game, testing a randomly acting agent, or training your own!
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩游戏、测试随机行为代理或训练自己的代理！
- en: 'Once the installation is complete, you can test the game using your keyboard
    inputs by running the following commands:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以通过运行以下命令，使用键盘输入测试游戏：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can also test it with the help of a randomly acting agent using the following
    command:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过以下命令，在随机行为代理的帮助下进行测试：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To get started with your own agent development, you can use the example agent
    script that was already configured to interact with the DeepMind Lab environment.
    You can find the script at `~/HOIAWOG/ch9/deepmindlab/python/random_agent.py`.
    To start training that agent, you can use the following command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始自己的代理开发，你可以使用已经配置好的示例代理脚本，它可以与DeepMind Lab环境进行交互。你可以在`~/HOIAWOG/ch9/deepmindlab/python/random_agent.py`找到该脚本。要开始训练这个代理，你可以使用以下命令：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we looked at several interesting and valuable learning environments,
    saw how their interfaces are set up, and even got hands-on with those environments
    using the quickstart guides for each environment and the setup scripts available
    in the book's code repository. We first looked at environments that have interfaces
    compatible with the OpenAI Gym interface that we are now very familiar with. Specifically
    in this category, we explored the Roboschool and Gym Retro environments.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了几个有趣且有价值的学习环境，了解了它们的界面设置，甚至通过每个环境的快速入门指南和书中代码库中的设置脚本亲自操作了这些环境。我们首先查看了与我们现在非常熟悉的OpenAI
    Gym接口兼容的环境。具体来说，在这一类别中，我们探索了Roboschool和Gym Retro环境。
- en: We also looked at other useful learning environments that did not necessarily
    have a Gym-compatible environment interface, but had a very similar API and so
    it was easy to adapt our agent code or implement a wrapper around the learning
    environment to make it compatible with the Gym API. Specifically, we explored
    the famous real-time strategy game-based StarCraft II environment and the DeepMind
    Lab environment. We also very briefly touched upon the DOTA2 environment, which
    has been used to train both single agents and a team of agents, trained by OpenAI,
    which successfully defeated amateur human players, and even some professional
    gaming teams, in the DOTA 2 contest.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还查看了其他有用的学习环境，这些环境不一定具备与Gym兼容的环境接口，但它们拥有非常相似的API，因此我们可以轻松地调整我们的智能体代码，或在学习环境周围实现一个包装器，使其兼容Gym
    API。具体来说，我们探索了著名的实时战略游戏《星际争霸II》环境和DeepMind Lab环境。我们还简要地提到了DOTA2环境，该环境曾被用于训练单一智能体和一个智能体团队，由OpenAI训练，这个团队成功击败了业余人类玩家，甚至一些职业电竞团队，在DOTA
    2比赛中取得胜利。
- en: We saw the different sets of tasks and environments available in each of these
    learning environment libraries and tried out examples to get a feel for the environment,
    and also to get an idea about how we can use the agents we developed in the previous
    chapters to train and solve the challenging tasks in these relatively new learning
    environments.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察了每个学习环境库中提供的不同任务和环境，并尝试了一些示例，以熟悉这些环境，同时了解如何使用我们在前几章中开发的智能体来训练并解决这些相对较新的学习环境中的挑战性任务。
