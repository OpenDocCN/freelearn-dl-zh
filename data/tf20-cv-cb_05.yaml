- en: '*Chapter 5*: Reducing Noise with Autoencoders'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：使用自编码器减少噪声'
- en: 'Among the most interesting families of deep neural networks is the autoencoder
    family. As their name suggests, their sole purpose is to digest their input, and
    then reconstruct it back into its original shape. In other words, an autoencoder
    learns to copy its input to its output. Why? Because the side effect of this process
    is what we are after: not to produce a tag or classification, but to learn an
    efficient, high-quality representation of the images that have been passed to
    the autoencoder. The name of such a representation is **encoding**.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络家族中，最有趣的家族之一就是自编码器家族。正如其名称所示，它们的唯一目的就是处理输入数据，然后将其重建为原始形状。换句话说，自编码器学习将输入复制到输出。为什么？因为这一过程的副作用就是我们所追求的目标：不是生成标签或分类，而是学习输入到自编码器的图像的高效、高质量表示。这种表示的名称是**编码**。
- en: 'How do they achieve this? By training two networks in tandem: an **encoder**,
    which takes images and produces the encoding, and a **decoder**, which takes the
    encoding and tries to reconstruct the input from its information.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 它们是如何实现这一点的呢？通过同时训练两个网络：一个**编码器**，它接受图像并生成编码，另一个是**解码器**，它接受编码并尝试从中重建输入数据。
- en: In this chapter, we will cover the basics, starting with a simple fully connected
    implementation of an autoencoder. Later, we'll create a more common and versatile
    convolutional autoencoder. We will also learn how to apply autoencoders in more
    practical contexts, such as denoising images, detecting outliers in a dataset,
    and creating an inverse image search index. Sound interesting?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从基础开始，首先实现一个简单的全连接自编码器。之后，我们将创建一个更常见且多功能的卷积自编码器。我们还将学习如何在更实际的应用场景中使用自编码器，比如去噪图像、检测数据集中的异常值和创建逆向图像搜索索引。听起来有趣吗？
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Creating a simple fully connected autoencoder
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个简单的全连接自编码器
- en: Creating a convolutional autoencoder
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个卷积自编码器
- en: Denoising images with autoencoders
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器去噪图像
- en: Spotting outliers using autoencoders
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器检测异常值
- en: Creating an inverse image search index with deep learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习创建逆向图像搜索索引
- en: Implementing a variational autoencoder
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个变分自编码器
- en: Let's get started!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Although using a GPU is always a good idea, some of these recipes (especially
    *Creating a simple fully connected autoencoder*) work well with a mid-tier CPU,
    such as an Intel i5 or i7\. If any particular recipe depends on external resources
    or requires preparatory steps, you''ll find specific preparation instructions
    in the *Getting ready* section. You can promptly access all the code for this
    chapter here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用GPU始终是一个好主意，但其中一些示例（特别是*创建一个简单的全连接自编码器*）在中端CPU（如Intel i5或i7）上运行良好。如果某个特定的示例依赖外部资源或需要预备步骤，您将在*准备工作*部分找到详细的准备说明。您可以随时访问本章的所有代码：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下链接，观看《代码实践》视频：
- en: '[https://bit.ly/3qrHYaF](https://bit.ly/3qrHYaF).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3qrHYaF](https://bit.ly/3qrHYaF)。'
- en: Creating a simple fully connected autoencoder
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个简单的全连接自编码器
- en: '**Autoencoders** are unusual in their design, as well as in terms of their
    functionality. That''s why it''s a great idea to master the basics of implementing,
    perhaps, the simplest version of an autoencoder: a fully connected one.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**在设计和功能上都很独特。这也是为什么掌握自编码器基本原理，尤其是实现可能是最简单版本的自编码器——全连接自编码器，是一个好主意。'
- en: In this recipe, we'll implement a fully connected autoencoder to reconstruct
    the images in `Fashion-MNIST`, a standard dataset that requires minimal preprocessing,
    allowing us to focus on the autoencoder itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将实现一个全连接自编码器，来重建`Fashion-MNIST`中的图像，这是一个标准数据集，几乎不需要预处理，允许我们专注于自编码器本身。
- en: Are you ready? Let's get started!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？让我们开始吧！
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Fortunately, `Fashion-MNIST` comes bundled with TensorFlow, so we don't need
    to download it on our own.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`Fashion-MNIST`已经与TensorFlow一起打包，因此我们无需自己下载。
- en: 'We''ll use `OpenCV`, a famous computer vision library, to create a mosaic so
    that we can compare the original images with the ones reconstructed by the autoencoder.
    You can install `OpenCV` effortlessly with `pip`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`OpenCV`，一个著名的计算机视觉库，来创建一个马赛克，以便我们能够将原始图像与自编码器重建的图像进行对比。你可以通过`pip`轻松安装`OpenCV`：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that all the preparations have been handled, let's take a look at the recipe!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有准备工作都已完成，来看看具体步骤吧！
- en: How to do it…
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps, to implement a simple yet capable autoencoder:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤，来实现一个简单而有效的自编码器：
- en: 'Import the necessary packages to implement the fully connected autoencoder:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包来实现全连接自编码器：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a function that will build the autoencoder''s architecture. By default,
    the encoding or latent vector dimension is *128*, but *16*, *32*, and *64* are
    good values too:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来构建自编码器的架构。默认情况下，编码或潜在向量的维度是*128*，但*16*、*32*和*64*也是不错的选择：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define a function that will plot a sample of general images against their original
    counterparts, in order to visually assess the autoencoder''s performance:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于将一组常规图像与其原始对应图像进行对比绘制，以便直观评估自编码器的性能：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previous block selects 15 random indices, which we''ll use to pick the
    same sample images from the `original` and `generated` batches. Next, let''s define
    an inner function so that we can stack a sample of 15 images in a 3x5 grid:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前一个代码块选择了15个随机索引，我们将用它们从`original`和`generated`批次中挑选相同的样本图像。接下来，定义一个内部函数，这样我们就可以将15张图像按3x5网格排列：
- en: '[PRE4]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, define another inner function so that we can add text on top of an image.
    This will be useful for distinguishing the generated images from the originals,
    as we''ll see shortly:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义另一个内部函数，以便我们可以在图像上添加文字。这将有助于区分生成的图像和原始图像，稍后我们将看到如何操作：
- en: '[PRE5]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Wrap up this function by selecting the same images from the original and generated
    groups. Then, stack both groups together to form a mosaic, resize it so that it''s
    860x860 in size, label the original and generated tiles in the mosaic using `add_text()`,
    and display the result:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成该函数，通过从原始和生成的图像组中选择相同的图像。然后，将这两组图像堆叠在一起形成马赛克，调整其大小为860x860，使用`add_text()`在马赛克中标注原始图像和生成图像，并显示结果：
- en: '[PRE6]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Download (or load, if cached) `Fashion-MNIST`. Because this is not a classification
    problem, we are only keeping the images, not the labels:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载（或加载缓存的）`Fashion-MNIST`。由于这不是一个分类问题，我们只保留图像，而不保留标签：
- en: '[PRE7]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Normalize the images:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化：
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Reshape the images into vectors:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像重塑为向量：
- en: '[PRE9]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and mean squared Error (`''mse''`) as the loss function. Why? We''re not interested
    in getting the classification right but reconstructing the input as closely as
    possible, which translates into minimizing the overall error:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建自编码器并进行编译。我们将使用`'adam'`作为优化器，均方误差（`'mse'`）作为损失函数。为什么？我们不关心分类是否正确，而是尽可能准确地重建输入，这意味着要最小化总体误差：
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Fit the autoencoder over 300 epochs, a figure high enough to allow the network
    to learn a good representation of the input. To speed up the training process
    a bit, we''ll pass batches of `1024` vectors at a time (feel free to change the
    batch size based on your hardware capabilities). Notice how the input features
    are also the labels or targets:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在300个epoch上拟合自编码器，这是一个足够大的数字，能够让网络学习到输入的良好表示。为了加快训练过程，我们每次传入`1024`个向量的批次（可以根据硬件能力自由调整批次大小）。请注意，输入特征也是标签或目标：
- en: '[PRE11]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Make predictions on the test set (basically, generate copies of the test vectors):'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集进行预测（基本上就是生成测试向量的副本）：
- en: '[PRE12]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reshape the predictions and test vectors back to grayscale images of dimensions
    28x28x1:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测值和测试向量重新调整为28x28x1尺寸的灰度图像：
- en: '[PRE13]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Generate a comparative plot of the original images against the ones produced
    by the autoencoder:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成原始图像与自编码器生成图像的对比图：
- en: '[PRE14]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here''s the result:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 5.1 – Mosaic of the original images (top three rows) compared with
    the'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 原始图像（前三行）与'
- en: generated ones (bottom three rows)](img/B14768_05_001.jpg)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像（底部三行）](img/B14768_05_001.jpg)
- en: Figure 5.1 – Mosaic of the original images (top three rows) compared with the
    generated ones (bottom three rows)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 原始图像（前三行）与生成的图像（底部三行）
- en: Judging by the results, our autoencoder did a pretty decent job. In all cases,
    the shape of the clothing items is well-preserved. However, it isn't as accurate
    at reconstructing the inner details, as shown by the T-shirt in the sixth row,
    fourth column, where the horizontal stripe in the original is missing in the produced
    copy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果来看，我们的自编码器做得相当不错。在所有情况下，服装的形状都得到了很好的保留。然而，它在重建内部细节时并不如预期准确，如第六行第四列中的 T 恤所示，原图中的横条纹在生成的副本中缺失了。
- en: How it works…
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we learned that autoencoders work by combining two networks
    into one: the encoder and the decoder. In the `build_autoencoder()` function,
    we implemented a fully connected autoencoding architecture, where the encoder
    portion takes a 784-element vector and outputs an encoding of 128 numbers. Then,
    the decoder picks up this encoding and expands it through several stacked dense
    (fully connected) layers, where the last one creates a 784-element vector (the
    same dimensions that the input contains).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们了解到自编码器是通过将两个网络结合成一个来工作的：编码器和解码器。在 `build_autoencoder()` 函数中，我们实现了一个完全连接的自编码架构，其中编码器部分接受一个
    784 元素的向量并输出一个包含 128 个数字的编码。然后，解码器接收这个编码并通过几个堆叠的全连接层进行扩展，最后一个层生成一个 784 元素的向量（与输入的维度相同）。
- en: The training process thus consists of minimizing the distance or error between
    the input the encoder receives and the output the decoder produces. The only way
    to achieve this is to learn encodings that minimize the information loss when
    compressing the inputs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程因此包括最小化编码器接收的输入与解码器产生的输出之间的距离或误差。实现这一目标的唯一方法是学习能在压缩输入时最小化信息损失的编码。
- en: 'Although the loss function (in this case, `MSE`) is a good measure to see if
    the autoencoder is progressing in its learning, with these particular networks,
    visual verification is just as relevant, if not more. That''s why we implemented
    the `plot_original_vs_generated()` function: to check that the copies look like
    their original counterparts.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管损失函数（在此情况下为 `MSE`）是衡量自编码器学习进展的好方法，但对于这些特定的网络，视觉验证同样重要，甚至可能更为关键。这就是我们实现 `plot_original_vs_generated()`
    函数的原因：检查副本是否看起来像它们的原始对应物。
- en: Why don't you try changing the encoding size? How does it affect the quality
    of the copies?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么不试试改变编码大小呢？它是如何影响副本质量的？
- en: See also
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you''re wondering why `Fashion-MNIST` exists at all, take a look at the
    official repository here: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道为什么 `Fashion-MNIST` 会存在，可以查看这里的官方仓库：[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)。
- en: Creating a convolutional autoencoder
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建卷积自编码器
- en: As with regular neural networks, when it comes to images, using convolutions
    is usually the way to go. In the case of autoencoders, this is no different. In
    this recipe, we'll implement a convolutional autoencoder to reproduce images from
    `Fashion-MNIST`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与常规神经网络一样，处理图像时，使用卷积通常是最好的选择。在自编码器的情况下，这也是一样的。在这个食谱中，我们将实现一个卷积自编码器，用于重建 `Fashion-MNIST`
    中的图像。
- en: The distinguishing factor is that in the decoder, we'll use reverse or transposed
    convolutions, which upscale volumes instead of downscaling them. This is what
    happens in traditional convolutional layers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于，在解码器中，我们将使用反向或转置卷积，它可以放大体积，而不是缩小它们。这是传统卷积层中发生的情况。
- en: This is an interesting recipe. Are you ready to begin?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的食谱。你准备好开始了吗？
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Because there are convenience functions in TensorFlow for downloading `Fashion-MNIST`,
    we don''t need to do any manual preparations on the data side. However, we must
    install `OpenCV` so that we can visualize the outputs of the autoencoder. This
    can be done with the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 TensorFlow 提供了方便的函数来下载 `Fashion-MNIST`，所以我们不需要在数据端做任何手动准备。然而，我们必须安装 `OpenCV`，以便我们可以可视化自编码器的输出。可以使用以下命令来完成：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Without further ado, let's get started.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 事不宜迟，让我们开始吧。
- en: How to do it…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Follow these steps to implement a fully functional convolutional autoencoder:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现一个完全功能的卷积自编码器：
- en: 'Let''s import the necessary dependencies:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入必要的依赖：
- en: '[PRE16]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the `build_autoencoder()` function, which internally builds the autoencoder
    architecture and returns the encoder, the decoder, and the autoencoder itself.
    Start defining the input and the first set of 32 convolutional filters:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_autoencoder()`函数，该函数内部构建自编码器架构，并返回编码器、解码器以及自编码器本身。首先定义输入层和第一组32个卷积过滤器：
- en: '[PRE17]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the second set of convolutions (64 this time):'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义第二组卷积层（这次是64个卷积核）：
- en: '[PRE18]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define the output layers of the encoder:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义编码器的输出层：
- en: '[PRE19]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In *Step 2*, we defined the encoder model, which is a regular convolutional
    neural network. The next block defines the decoder model, starting with the input
    and 64 transposed convolution filters:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*步骤2*中，我们定义了编码器模型，这是一个常规的卷积神经网络。下一块代码定义了解码器模型，从输入和64个反卷积过滤器开始：
- en: '[PRE20]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the second set of transposed convolutions (32 this time):'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义第二组反卷积层（这次是32个卷积核）：
- en: '[PRE21]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the output layer of the decoder:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义解码器的输出层：
- en: '[PRE22]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The decoder uses `Conv2DTranspose` layers, which expand their inputs to generate
    larger output volumes. Notice that the further we go into the decoder, the fewer
    filters the `Conv2DTranspose` layers use. Finally, define the autoencoder:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器使用`Conv2DTranspose`层，该层将输入扩展以生成更大的输出体积。注意，我们进入解码器的层数越多，`Conv2DTranspose`层使用的过滤器就越少。最后，定义自编码器：
- en: '[PRE23]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The autoencoder is the end-to-end architecture. This starts with the input layer,
    which goes into the encoder, and ends with an output layer, which is the result
    of passing the encoder's output through the decoder.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自编码器是端到端的架构。它从输入层开始，进入编码器，最后通过解码器输出层，得出结果。
- en: 'Define a function that will plot a sample of general images against their original
    counterparts. This will help us visually assess the autoencoder''s performance.
    (This is the same function we defined in the previous recipe. For a more complete
    explanation, refer to the *Creating a simple fully connected autoencoder* recipe
    of this chapter.) Take a look at the following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将一般图像样本与其原始图像进行对比绘制。这将帮助我们直观评估自编码器的性能。（这是我们在前一个示例中定义的相同函数。有关更完整的解释，请参考本章的*创建简单的全连接自编码器*一节。）请看以下代码：
- en: '[PRE24]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define an inner helper function in order to stack a sample of images in a 3x5
    grid:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个内部辅助函数，用于将图像样本堆叠成一个3x5的网格：
- en: '[PRE25]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, define a function that will put text on an image in a given position:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数，将文本放置到图像的指定位置：
- en: '[PRE26]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, create a mosaic containing both the original and generated images:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建一个包含原始图像和生成图像的马赛克：
- en: '[PRE27]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Download (or load, if cached) `Fashion-MNIST`. We are only interested in the
    images; therefore, we can drop the labels:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载（或加载，如果已缓存）`Fashion-MNIST`。我们只关心图像，因此可以丢弃标签：
- en: '[PRE28]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Normalize the images and add a channel dimension to them:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化并添加通道维度：
- en: '[PRE29]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, we are only interested in the autoencoder, so we''ll ignore the other
    two return values of the `build_autoencoder()` function. However, in different
    circumstances, we could want to keep them. We''ll train the model using `''adam''`
    and use `''mse''` as the loss function since we want to reduce the error, not
    optimize for classification accuracy:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里，我们只关心自编码器，因此会忽略`build_autoencoder()`函数的其他两个返回值。然而，在不同的情况下，我们可能需要保留它们。我们将使用`'adam'`优化器训练模型，并使用`'mse'`作为损失函数，因为我们希望减少误差，而不是优化分类准确性：
- en: '[PRE30]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Train the autoencoder over 300 epochs, in batches of 512 images at a time.
    Notice how the input images are also the labels:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在300个训练周期中训练自编码器，每次批处理512张图像。注意，输入图像也是标签：
- en: '[PRE31]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Make copies of the test set:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制测试集：
- en: '[PRE32]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Reshape both the predictions and the test images back to 28x28 (no channel
    dimension):'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果和测试图像的形状调整回28x28（无通道维度）：
- en: '[PRE33]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Generate a comparative mosaic of the original images and the copies outputted
    by the autoencoder:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成原始图像与自编码器输出的复制图像的对比马赛克：
- en: '[PRE34]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s take a look at the result:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '![Figure 5.2 – Mosaic of the original images (top three rows), compared with'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – 原始图像的马赛克（前三行），与'
- en: those produced by the convolutional autoencoder (bottom three rows)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自编码器生成的图像（后三行）
- en: '](img/B14768_05_002.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_05_002.jpg)'
- en: Figure 5.2 – Mosaic of the original images (top three rows), compared with those
    produced by the convolutional autoencoder (bottom three rows)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 原始图像的马赛克（前三行），与卷积自编码器生成的图像（后三行）进行对比
- en: As we can see, the autoencoder has learned a good encoding, which allowed it
    to reconstruct the input images with minimal detail loss. Let's head over to the
    next section to understand how it works!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，自编码器已经学到了一个很好的编码，它使得它能够以最小的细节损失重建输入图像。让我们进入下一个部分，了解它是如何工作的！
- en: How it works…
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In this recipe, we learned that a convolutional autoencoder is one of the most
    common yet powerful members of this family of neural networks. The encoder portion
    of the architecture is a regular convolutional neural network that relies on convolutions
    and dense layers to downsize the output and produce a vector representation. The
    decoder is the interesting part because it has to deal with the converse problem:
    to reconstruct the input based on the synthesized feature vector, also known as
    an encoding.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们了解到卷积自编码器是这一系列神经网络中最常见且最强大的成员之一。该架构的编码器部分是一个常规的卷积神经网络，依赖卷积和密集层来缩小输出并生成向量表示。解码器则是最有趣的部分，因为它必须处理相反的问题：根据合成的特征向量（即编码）重建输入。
- en: How does it do this? By using a transposed convolution (`Conv2DTranspose`).
    Unlike traditional `Conv2D` layers, these produce shallower volumes (fewer filters),
    but they are wider and taller. The result is an output layer with only one filter,
    and 28x28 dimensions, which is the same shape as the input. Fascinating, isn't
    it?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何做到的呢？通过使用转置卷积（`Conv2DTranspose`）。与传统的`Conv2D`层不同，转置卷积产生的是较浅的体积（较少的过滤器），但是它们更宽更高。结果是输出层只有一个过滤器，并且是28x28的维度，这与输入的形状相同。很有趣，不是吗？
- en: The training process consists of minimizing the error between the output (the
    generated copies) and the input (the original images). Therefore, MSE is a fitting
    loss function because it provides us with this very information.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括最小化输出（生成的副本）和输入（原始图像）之间的误差。因此，均方误差（MSE）是一个合适的损失函数，因为它为我们提供了这个信息。
- en: Finally, we assessed the performance of the autoencoder by visually inspecting
    a sample of test images, along with their synthetic counterparts.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过目视检查一组测试图像及其合成的对照图像来评估自编码器的性能。
- en: Tip
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In an autoencoder, the size of the encoding is crucial to guarantee the decoder
    has enough information to reconstruct the input.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，编码的大小至关重要，以确保解码器有足够的信息来重建输入。
- en: See also
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Here''s a great explanation of transposed convolutions: [https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于转置卷积的很好的解释：[https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba)。
- en: Denoising images with autoencoders
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器去噪图像
- en: Using images to reconstruct their input is great, but are there more useful
    ways to apply autoencoders? Of course there are! One of them is image denoising.
    As the name suggests, this is the act of restoring damaged images by replacing
    the corrupted pixels and regions with sensible values.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用图像重建输入是很棒的，但有没有更有用的方式来应用自编码器呢？当然有！其中之一就是图像去噪。如其名所示，这就是通过用合理的值替换损坏的像素和区域来恢复损坏的图像。
- en: In this recipe, we'll purposely damage the images in `Fashion-MNIST`, and then
    train an autoencoder to denoise them.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将故意损坏`Fashion-MNIST`中的图像，然后训练一个自编码器去噪它们。
- en: Getting ready
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: '`Fashion-MNIST` can easily be accessed using the convenience functions TensorFlow
    provides, so we don''t need to manually download the dataset. On the other hand,
    because we''ll be creating some visualizations using `OpenCV`, we must install
    it, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`Fashion-MNIST`可以通过TensorFlow提供的便利函数轻松访问，因此我们不需要手动下载数据集。另一方面，因为我们将使用`OpenCV`来创建一些可视化效果，所以我们必须安装它，方法如下：'
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Let's get started!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Follow these steps to implement a convolutional autoencoder capable of restoring
    damaged images:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现一个能够恢复损坏图像的卷积自编码器：
- en: 'Import the required packages:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE36]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define the `build_autoencoder()` function, which creates the corresponding
    neural architecture. Notice that this is the same architecture we implemented
    in the previous recipe; therefore, we won''t go into too much detail here. For
    an in-depth explanation, please refer to the *Creating a convolutional autoencoder*
    recipe:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_autoencoder()`函数，它创建相应的神经网络架构。请注意，这是我们在前一个教程中实现的相同架构；因此，我们在这里不再详细讲解。有关详细解释，请参见*创建卷积自编码器*教程：
- en: '[PRE37]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that we''ve created the encoder model, let''s create the decoder:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经创建了编码器模型，接下来创建解码器：
- en: '[PRE38]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Finally, define the autoencoder itself and return the three models:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，定义自编码器本身并返回三个模型：
- en: '[PRE39]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the `plot_original_vs_generated()` function, which creates a comparative
    mosaic of the original and generated images. We''ll use this function later to
    show the noisy images and their restored counterparts. Similar to `build_autoencoder()`,
    this function works in the same way we defined it in the *Creating a simple fully
    connected autoencoder* recipe, so if you want a detailed explanation, please review
    that recipe:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `plot_original_vs_generated()` 函数，该函数创建原始图像与生成图像的比较拼图。我们稍后将使用此函数来显示噪声图像及其恢复后的图像。与
    `build_autoencoder()` 类似，该函数的工作方式与我们在*创建一个简单的全连接自编码器*食谱中定义的相同，因此如果您需要详细解释，请查阅该食谱：
- en: '[PRE40]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define an inner helper function that will stack a sample of images in a 3x5
    grid:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个内部辅助函数，将一组图像按 3x5 网格堆叠：
- en: '[PRE41]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define a function that will put custom text on top of an image, in a certain
    location:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将自定义文本放置在图像上的特定位置：
- en: '[PRE42]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create the mosaic with both the original and the generated images, label each
    sub-grid, and display the result:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建包含原始图像和生成图像的拼图，标记每个子网格并显示结果：
- en: '[PRE43]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Load `Fashion-MNIST` using TensorFlow''s handy function. We will only keep
    the images since the labels are unnecessary:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的便捷函数加载 `Fashion-MNIST`。我们将只保留图像，因为标签不需要：
- en: '[PRE44]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Normalize the images and add a single color channel to them using `np.expand_dims()`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化，并使用 `np.expand_dims()` 为其添加单一颜色通道：
- en: '[PRE45]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Generate two tensors with the same dimensions as `X_train` and `X_test`, respectively.
    These will correspond to random `0.5`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成两个与 `X_train` 和 `X_test` 相同维度的张量。它们将对应于随机的 `0.5`：
- en: '[PRE46]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Purposely damage both `X_train` and `X_test` by adding `train_noise` and `test_noise`,
    respectively. Make sure that the values remain between `0` and `1` using `np.clip()`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过分别向 `X_train` 和 `X_test` 添加 `train_noise` 和 `test_noise` 来故意损坏这两个数据集。确保使用
    `np.clip()` 将值保持在 `0` 和 `1` 之间：
- en: '[PRE47]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create the autoencoder and compile it. We''ll use `''adam''` as our optimizer
    and `''mse''` as our loss function, given that we''re interested in reducing the
    error instead of improving accuracy:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建自编码器并编译它。我们将使用`'adam'`作为优化器，`'mse'`作为损失函数，因为我们更关心减少误差，而不是提高准确率：
- en: '[PRE48]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Fit the model for `300` epochs, on batches of `1024` noisy images at a time.
    Notice that the features are the noisy images, while the labels or targets are
    the original ones, prior to being damaged:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型训练 `300` 个周期，每次批量处理 `1024` 张噪声图像。注意，特征是噪声图像，而标签或目标是原始图像，即未经损坏的图像：
- en: '[PRE49]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Make predictions with the trained model. Reshape both the noisy and generated
    images back to 28x28, and scale them up to the [0, 255] range:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型进行预测。将噪声图像和生成图像都重新调整为28x28，并将它们缩放到[0, 255]范围内：
- en: '[PRE50]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, display the mosaic of noisy versus restored images:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，显示噪声图像与恢复图像的拼图：
- en: '[PRE51]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Here''s the result:'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the
    network (bottom)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3 – 噪声图像（顶部）与网络恢复的图像（底部）拼图'
- en: '](img/B14768_05_003.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_05_003.jpg)'
- en: Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the network
    (bottom)
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 噪声图像（顶部）与网络恢复的图像（底部）拼图
- en: Look how damaged the images at the top are! The good news is that, in most instances,
    the autoencoder did a good job of restoring them. However, it couldn't denoise
    the images closer to the edges of the mosaic properly, which is a sign that more
    experimentation can be done to improve their performance (to be fair, these bad
    examples are hard to discern, even for humans).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 看看顶部的图像有多么受损！好消息是，在大多数情况下，自编码器成功地恢复了它们。然而，它无法正确去除拼图边缘部分的噪声，这表明可以进行更多实验以提高性能（公平地说，这些坏例子即使对人类来说也很难辨别）。
- en: How it works…
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The novelty in this recipe is the practical use of the convolutional autoencoder.
    Both the network and other building blocks have been covered in depth in the last
    two recipes, so let's focus on the denoising problem itself.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱的新颖之处在于实际应用卷积自编码器。网络和其他构建模块在前两个食谱中已被详细讨论，因此我们将重点关注去噪问题本身。
- en: To recreate a real-life scenario of damaged images, we added a heavy amount
    of Gaussian noise to both the training and test sets in the `Fashion-MNIST` dataset.
    This kind of noise is known as salt and pepper because the damaged image looks
    as though it had these seasonings spilled all over it.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重现实际中损坏图像的场景，我们在`Fashion-MNIST`数据集的训练集和测试集中加入了大量的高斯噪声。这种噪声被称为“盐和胡椒”，因为损坏后的图像看起来像是洒满了这些调料。
- en: To teach our autoencoder how the images once looked, we used the noisy ones
    as the features and the originals as the target or labels. This way, after 300
    epochs, the network learned an encoding capable of, on many occasions, mapping
    salt and peppered instances to satisfyingly restored versions of them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了教会我们的自编码器图像原本的样子，我们将带噪声的图像作为特征，将原始图像作为目标或标签。这样，经过300个epoch后，网络学会了一个编码，可以在许多情况下将带盐和胡椒噪声的实例映射到令人满意的恢复版本。
- en: Nonetheless, the model is not perfect, as we saw in the mosaic, where the network
    was unable to restore the images at the edges of the grid. This is a demonstration
    of how difficult repairing a damaged image can be.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型并不完美，正如我们在拼图中看到的那样，网络无法恢复网格边缘的图像。这证明了修复损坏图像的困难性。
- en: Spotting outliers using autoencoders
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器检测异常值
- en: Another great application of autoencoders is outlier detection. The idea behind
    this use case is that the autoencoder will learn an encoding with a very small
    error for the most common classes in a dataset, while its ability to reproduce
    scarcely represented categories (outliers) will be much more error-prone.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的另一个重要应用是异常检测。这个应用场景的理念是，自编码器会对数据集中最常见的类别学习出一个误差非常小的编码，而对于稀有类别（异常值）的重建能力则会误差较大。
- en: With this premise in mind, in this recipe, we'll rely on a convolutional autoencoder
    to detect outliers in a subsample of `Fashion-MNIST`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个前提，在本教程中，我们将依赖卷积自编码器来检测`Fashion-MNIST`子集中的异常值。
- en: Let's begin!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: 'To install `OpenCV`, use the following `pip` command:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`OpenCV`，请使用以下`pip`命令：
- en: '[PRE52]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We'll rely on TensorFlow's built-in convenience functions to load the `Fashion-MNIST`
    dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依赖TensorFlow内建的便捷函数来加载`Fashion-MNIST`数据集。
- en: How to do it…
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成这个教程：
- en: 'Import the required packages:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包：
- en: '[PRE53]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Set a random seed to guarantee reproducibility:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置随机种子以保证可重复性：
- en: '[PRE54]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define a function that will build the autoencoder architecture. This function
    follows the same structure we studied in the *Creating a convolutional autoencoder*
    recipe, so if you want a deeper explanation, please go back to that recipe. Let''s
    start by creating the encoder model:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来构建自编码器架构。这个函数遵循我们在*创建卷积自编码器*教程中学习的结构，如果你想了解更深入的解释，请回到那个教程。让我们从创建编码器模型开始：
- en: '[PRE55]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, build the decoder:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，构建解码器：
- en: '[PRE56]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Lastly, build the autoencoder and return the three models:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，构建自编码器并返回三个模型：
- en: '[PRE57]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, define a function that will contrive a dataset of two classes, where
    one of them represents an anomaly or outlier. Start by selecting the instances
    corresponding to the two classes of interest, and then shuffle them to break any
    possible ordering bias:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，定义一个函数来构建一个包含两个类别的数据集，其中一个类别表示异常或离群点。首先选择与这两个类别相关的实例，然后将它们打乱，以打破可能存在的顺序偏差：
- en: '[PRE58]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, from the anomalous category, select a number of instances proportional
    to `corruption_proportion`. Finally, create the final dataset by merging the regular
    instances with the outliers:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从异常类别中选择与`corruption_proportion`成比例的实例。最后，通过将常规实例与离群点合并来创建最终的数据集：
- en: '[PRE59]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Load `Fashion-MNIST`. Merge both the train and test sets into a single dataset:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`Fashion-MNIST`。将训练集和测试集合并为一个数据集：
- en: '[PRE60]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Define the regular and anomalous labels, and then create the anomalous dataset:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义常规标签和异常标签，然后创建异常数据集：
- en: '[PRE61]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Add a channel dimension to the dataset, normalize it, and divide it into 80%
    for training and 20% for testing:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向数据集中添加一个通道维度，进行归一化，并将数据集分为80%作为训练集，20%作为测试集：
- en: '[PRE62]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and `''mse''` as the loss function since this gives us a good measure of the model''s
    error:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建自编码器并编译它。我们将使用`'adam'`作为优化器，`'mse'`作为损失函数，因为这可以很好地衡量模型的误差：
- en: '[PRE63]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Train the autoencoder for 300 epochs, on batches of `1024` images at a time:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自编码器训练300个epoch，每次处理`1024`张图像：
- en: '[PRE64]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Make predictions on the data to find the outliers. We''ll compute the mean
    squared error between the original image and the one produced by the autoencoder:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行预测以找出异常值。我们将计算原始图像与自动编码器生成图像之间的均方误差：
- en: '[PRE65]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Select the indices of the images with errors greater than the 99.9% quantile.
    These will be our outliers:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择误差大于99.9%分位数的图像索引。这些将是我们的异常值：
- en: '[PRE66]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Save a comparative image of the original and generated images for each outlier:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个异常值保存原始图像与生成图像的比较图像：
- en: '[PRE67]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here''s an example of an outlier:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个异常值的示例：
- en: '![Figure 5.4 – Left: Original outlier. Right: Reconstructed image.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4 – 左：原始异常值。右：重建图像。'
- en: '](img/B14768_05_004.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_05_004.jpg)'
- en: 'Figure 5.4 – Left: Original outlier. Right: Reconstructed image.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 左：原始异常值。右：重建图像。
- en: As we can see, we can harness the knowledge stored in the encoding learned by
    the autoencoder to easily detect anomalous or uncommon images in a dataset. We'll
    look at this in more detail in the next section.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们可以利用自动编码器学习的编码知识轻松检测数据集中的异常或不常见图像。我们将在下一节中更详细地讨论这一点。
- en: How it works…
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The idea behind this recipe is very simple: outliers, by definition, are rare
    occurrences of an event or class within a dataset. Therefore, when we train an
    autoencoder on a dataset that contains outliers, it won''t have sufficient time
    nor examples to learn a proper representation of them.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方背后的思想非常简单：根据定义，异常值是数据集中事件或类别的稀有发生。因此，当我们在包含异常值的数据集上训练自动编码器时，它将没有足够的时间或示例来学习它们的适当表示。
- en: By leveraging the low confidence (in other words, the high error) the network
    will display when reconstructing anomalous images (in this example, T-shirts),
    we can select the worst copies in order to spot outliers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用网络在重建异常图像（在此示例中为T恤）时表现出的低置信度（换句话说，高误差），我们可以选择最差的副本来发现异常值。
- en: However, for this technique to work, the autoencoder must be great at reconstructing
    the regular classes (for instance, sandals); otherwise, the false positive rate
    will be too high.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使此技术有效，自动编码器必须擅长重建常规类别（例如，凉鞋）；否则，误报率将太高。
- en: Creating an inverse image search index with deep learning
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习创建逆图像搜索索引
- en: Because the whole point of an autoencoder is to learn an encoding or a low-dimensional
    representation of a set of images, they make for great feature extractors. Furthermore,
    we can use them as the perfect building blocks of image search indices, as we'll
    discover in this recipe.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因为自动编码器的核心目的是学习图像集合的编码或低维表示，它们是非常优秀的特征提取器。此外，正如我们将在本配方中发现的那样，我们可以将它们作为图像搜索索引的完美构建模块。
- en: Getting ready
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Let''s install `OpenCV` with `pip`. We''ll use it to visualize the outputs
    of our autoencoder, in order to visually assess the effectiveness of the image
    search index:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`pip`安装`OpenCV`。我们将用它来可视化自动编码器的输出，从而直观地评估图像搜索索引的有效性：
- en: '[PRE68]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: We'll start implementing the recipe in the next section.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节开始实现这个配方。
- en: How to do it…
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to create your own image search index:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建您自己的图像搜索索引：
- en: 'Import the necessary libraries:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE69]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Define `build_autoencoder()`, which instantiates the autoencoder. First, let''s
    assemble the encoder part:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_autoencoder()`，该函数实例化自动编码器。首先，让我们组装编码器部分：
- en: '[PRE70]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The next step is to define the decoder portion:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义解码器部分：
- en: '[PRE71]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Finally, build the autoencoder and return it:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，构建自动编码器并返回它：
- en: '[PRE72]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define a function that will compute the Euclidean distance between two vectors:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个计算两个向量之间欧几里得距离的函数：
- en: '[PRE73]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define the `search()` function, which uses the search index (a dictionary of
    feature vectors paired with their corresponding images) to retrieve the most similar
    results to a query vector:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`search()`函数，该函数使用搜索索引（一个将特征向量与相应图像配对的字典）来检索与查询向量最相似的结果：
- en: '[PRE74]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Load the `Fashion-MNIST` dataset. Keep only the images:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`Fashion-MNIST`数据集。仅保留以下图像：
- en: '[PRE75]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Normalize the images and add a color channel dimension:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行归一化并添加颜色通道维度：
- en: '[PRE76]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Build the autoencoder and compile it. We''ll use `''adam''` as the optimizer
    and `''mse''` as the loss function since this gives us a good measure of the model''s
    error:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建自动编码器并进行编译。我们将使用`'adam'`作为优化器，`'mse'`作为损失函数，因为这样可以很好地衡量模型的误差：
- en: '[PRE77]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Train the autoencoder for 10 epochs, on batches of `512` images at a time:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练自动编码器10个周期，每次批处理`512`张图像：
- en: '[PRE78]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Create a new model, which we''ll use as a feature extractor. It''ll receive
    the same inputs as the autoencoder and will output the encoding learned by the
    autoencoder. In essence, we are using the encoder part of the autoencoder to turn
    images into vectors:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新模型，我们将用它作为特征提取器。它将接收与自编码器相同的输入，并输出自编码器学到的编码。实质上，我们是使用自编码器的编码器部分将图像转换为向量：
- en: '[PRE79]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create the search index, comprised of the feature vectors of `X_train`, along
    with the original images (which must be reshaped back to 28x28 and rescaled to
    the range [0, 255]):'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建搜索索引，由`X_train`的特征向量和原始图像组成（原始图像必须重新调整为28x28并重新缩放到[0, 255]的范围）：
- en: '[PRE80]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Compute the feature vectors of `X_test`, which we will use as our sample of
    query images. Also, reshape `X_test` to 28x28 and rescale its values to the range
    [0, 255]:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`X_test`的特征向量，我们将把它用作查询图像的样本。并将`X_test`调整为28x28的形状，并将其值重新缩放到[0, 255]的范围：
- en: '[PRE81]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Select 16 random test images (with their corresponding feature vectors) to
    use as queries:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择16个随机测试图像（以及其对应的特征向量）作为查询：
- en: '[PRE82]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Perform a search for each of the images in the test sample and save a side-to-side
    visual comparison of the test query, along with the results fetched from the index
    (which, remember, is comprised of the train data):'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试样本中的每个图像进行搜索，并保存查询图像与从索引中提取的结果之间的并排视觉对比（记住，索引是由训练数据组成的）：
- en: '[PRE83]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Here''s an example of a search result:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是一个搜索结果的示例：
- en: '![Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results,
    all of which contain shoes too'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – 左：鞋子的查询图像。右：最佳的16个搜索结果，所有结果也都包含鞋子'
- en: '](img/B14768_05_005.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_05_005.jpg)'
- en: 'Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results,
    all of which contain shoes too'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 左：鞋子的查询图像。右：最佳的16个搜索结果，所有结果也都包含鞋子
- en: As the preceding image demonstrates, our image search index is a success! We'll
    see how it works in the next section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的图像所示，我们的图像搜索索引是成功的！我们将在下一部分看到它是如何工作的。
- en: How it works…
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we learned how to leverage the distinguishing trait of an autoencoder,
    which is to learn an encoding that greatly compresses the information in the input
    images, resulting in minimal loss of information. Then, we used the encoder part
    of a convolutional autoencoder to extract the features of fashion item photos
    and construct an image search index.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们学习了如何利用自编码器的独特特征——学习一个大大压缩输入图像信息的编码，从而实现最小的信息损失。然后，我们使用卷积自编码器的编码器部分提取时尚物品照片的特征，并构建了一个图像搜索索引。
- en: By doing this, using this index as a search engine is as easy as computing the
    Euclidean distance between a query vector (corresponding to a query image) and
    all the images in the index, selecting only those that are closest to the query.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，使用这个索引作为搜索引擎就像计算查询向量（对应于查询图像）与索引中所有图像之间的欧几里得距离一样简单，只选择那些最接近查询的图像。
- en: The most important aspect in our solution is to train an autoencoder that is
    good enough to produce high-quality vectors, since they make or break the search
    engine.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解决方案中最重要的方面是训练一个足够优秀的自编码器，以生成高质量的向量，因为它们决定了搜索引擎的成败。
- en: See also
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The implementation is based on the great work of Dong *et al.*, whose paper
    can be read here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现基于Dong *等人*的出色工作，论文可在此阅读：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5)。
- en: Implementing a variational autoencoder
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现变分自编码器
- en: Some of the most modern and complex use cases of autoencoders are **Variational
    Autoencoders** (**VAEs**). They differ from the rest of the autoencoders in that,
    instead of learning an arbitrary function, they learn a probability distribution
    of the input images. We can then sample this distribution to produce new, unseen
    data points.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的一些最现代且复杂的应用场景是**变分自编码器**（**VAE**）。它们与其他自编码器的不同之处在于，变分自编码器并不是学习一个任意的函数，而是学习输入图像的概率分布。我们可以从这个分布中采样，以生成新的、未见过的数据点。
- en: A **VAE** is, in fact, a generative model, and in this recipe, we'll implement
    one.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**VAE**实际上是一个生成模型，在这个食谱中，我们将实现一个。'
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: We don't need any special preparation for this recipe, so let's get started
    right away!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要为这个食谱做任何特别的准备，所以让我们立即开始吧！
- en: How to do it…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps to learn how to implement and train a **VAE**:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤学习如何实现和训练**VAE**：
- en: 'Import the necessary packages:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包：
- en: '[PRE84]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Because we''ll be using the `tf.function` annotation soon, we must tell TensorFlow
    to run functions eagerly:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为我们很快会使用`tf.function`注解，所以我们必须告诉TensorFlow以急切执行（eager execution）的方式运行函数：
- en: '[PRE85]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Define a class that will encapsulate our implementation of the `self.z_log_var`
    and `self.z_mean` are the parameters of the latent Gaussian distribution that
    we''ll learn:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个类，封装我们实现`self.z_log_var`和`self.z_mean`的功能，它们是我们将学习的潜在高斯分布的参数：
- en: '[PRE86]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Define some members that will store the inputs and outputs of the `encoder`,
    `decoder`, and `vae`:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一些成员变量，用于存储`encoder`、`decoder`和`vae`的输入和输出：
- en: '[PRE87]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Define the `build_vae()` method, which builds the variational autoencoder architecture
    (notice that we are using dense layers instead of convolutions):'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_vae()`方法，该方法构建变分自编码器架构（请注意，我们使用的是全连接层而不是卷积层）：
- en: '[PRE88]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Notice that the encoder is just a fully connected network that produces three
    outputs: `self.z_mean`, which is the mean of the Gaussian distribution we are
    training to model, `self.z_log_var`, which is the logarithmic variance of this
    distribution, and `z`, a sample point in that probability space. In order to generate
    the `z` simple, we must wrap a custom function, `sampling()` (implemented in *Step
    5*), in a `Lambda` layer.'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，编码器只是一个完全连接的网络，它产生三个输出：`self.z_mean`，这是我们训练建模的高斯分布的均值；`self.z_log_var`，这是该分布的对数方差；以及`z`，这是该概率空间中的一个样本点。为了简单地生成`z`，我们必须在`Lambda`层中包装一个自定义函数`sampling()`（在*第5步*中实现）。
- en: 'Next, define the decoder:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义解码器：
- en: '[PRE89]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The decoder is just another fully connected network. The decoder will take
    samples from the latent dimension in order to reconstruct the inputs. Finally,
    connect the encoder and decoder to create the **VAE** model:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器只是另一个完全连接的网络。解码器将从潜在维度中取样，以重构输入。最后，将编码器和解码器连接起来，创建**VAE**模型：
- en: '[PRE90]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Define the `train()` method, which trains the variational autoencoder. Therefore,
    it receives the train and test data, as well as the number of epochs and the batch
    size:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train()`方法，该方法训练变分自编码器。因此，它接收训练和测试数据，以及迭代次数和批次大小：
- en: '[PRE91]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define the reconstruction loss as the MSE between the inputs and outputs:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将重建损失定义为输入和输出之间的均方误差（MSE）：
- en: '[PRE92]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '`kl_loss` is the `reconstruction_loss`:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`kl_loss`是`reconstruction_loss`：'
- en: '[PRE93]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Configure the `self.vae` model so that it uses `vae_loss` and `Adam()` as the
    optimizer (with a learning rate of 0.003). Then, fit the network over the specified
    number of epochs. Finally, return the three models:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`self.vae`模型，使其使用`vae_loss`和`Adam()`作为优化器（学习率为0.003）。然后，在指定的迭代次数内拟合网络。最后，返回三个模型：
- en: '[PRE94]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Define a function that will generate a random sample or point from the latent
    space, given the two relevant parameters (passed in the `arguments` array); that
    is, `z_mean` and `z_log_var`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将在给定两个相关参数（通过`arguments`数组传递）时生成潜在空间中的随机样本或点；即，`z_mean`和`z_log_var`：
- en: '[PRE95]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Notice that `epsilon` is a random Gaussian vector.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`epsilon`是一个随机的高斯向量。
- en: 'Define a function that will generate and plot images generated from the latent
    space. This will give us an idea of the **shapes** that are closer to the distribution,
    and the ones that are nearer to the tails of the curve:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数将生成并绘制从潜在空间生成的图像。这将帮助我们了解靠近分布的**形状**，以及接近曲线尾部的形状：
- en: '[PRE96]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Create a range of values that span from -4 to 4 in both the X and Y axes. We''ll
    use these to generate and visualize samples at each location:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个值的范围，X轴和Y轴的值从-4到4。我们将使用这些值在每个位置生成和可视化样本：
- en: '[PRE97]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Use the decoder to generate a new sample for each combination of `z_mean` and
    `z_log_var`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用解码器为每个`z_mean`和`z_log_var`的组合生成新的样本：
- en: '[PRE98]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Reshape the sample and place it in the corresponding cell in the grid:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑样本，并将其放置在网格中的相应单元格中：
- en: '[PRE99]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Add the ticks and axes labels, and then display the plot:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加刻度和坐标轴标签，然后显示图形：
- en: '[PRE100]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Load the `Fashion-MNIST` dataset. Normalize the images and add a color channel
    to them:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`Fashion-MNIST`数据集。对图像进行归一化，并添加颜色通道：
- en: '[PRE101]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Instantiate and build the **variational autoencoder**:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化并构建**变分自编码器**：
- en: '[PRE102]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Train the models for 100 epochs:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型100个周期：
- en: '[PRE103]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Use the decoder to generate new images and plot the result:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用解码器生成新图像并绘制结果：
- en: '[PRE104]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Here''s the result:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 5.6 – Visualization of the latent space learned by the VAE'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6 – VAE学习的潜在空间的可视化'
- en: '](img/B14768_05_006.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_05_006.jpg)'
- en: Figure 5.6 – Visualization of the latent space learned by the VAE
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – VAE学习的潜在空间可视化
- en: Here, we can see the collection of points that comprise the latent space and
    the corresponding clothing item for each of these points. This is a representation
    of the probability distribution the network learned, in which the item at the
    center of such a distribution resembles a T-shirt, while the ones at the edges
    look more like pants, sweaters, and shoes.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到构成潜在空间的点集，以及这些点对应的服装项目。 这是网络学习的概率分布的一个表示，其中分布中心的项目类似于T恤，而边缘的项目则更像裤子、毛衣和鞋子。
- en: Let's move on to the next section.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入下一节。
- en: How it works…
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned that a **variational autoencoder** is an advanced,
    more complex type of autoencoder that, instead of learning an arbitrary, vanilla
    function to map inputs to outputs, learns a probability distribution of the inputs.
    This gives it the ability to generate new, unseen images that make it a precursor
    of more modern generative models, such as **Generative Adversarial Networks**
    (**GANs**).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们了解到，**变分自编码器**是一种更先进、更复杂的自编码器，它不像学习一个任意的、简单的函数来将输入映射到输出，而是学习输入的概率分布。这样，它就能够生成新的、未见过的图像，成为更现代生成模型的前驱，例如**生成对抗网络**（**GANs**）。
- en: The architecture is not that different from the others autoencoder we studied
    in this chapter. The key to understanding the power of a `z`, which we generate
    using the `sampling()` function, within a Lambda layer.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构与我们在本章中学习的其他自编码器并没有太大不同。理解`z`的关键在于，我们通过`sampling()`函数在Lambda层中生成`z`。
- en: This means that in each iteration, the whole network is optimizing the `z_mean`
    and `z_log_var` parameters so that it closely resembles the probability distribution
    of the inputs. It does this because it's the only way the random samples (`z`)
    are going to be of such high quality that the decoder will be able to generate
    better, more realistic outputs.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，在每次迭代中，整个网络都在优化`z_mean`和`z_log_var`参数，使其与输入的概率分布尽可能接近。这样做是因为，只有这样，随机样本（`z`）的质量才足够高，解码器才能生成更好、更逼真的输出。
- en: See also
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'A key component we can use to tune the **VAE** is the **Kullback-Leibler**
    divergence, which you can read more about here: [https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用来调节**VAE**的一个关键组件是**Kullback-Leibler**散度，您可以在这里阅读更多内容：[https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)。
- en: Note that **VAE**s are the perfect runway to generative models, which we'll
    cover in depth in the next chapter!
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**VAE**是生成模型的完美开端，我们将在下一章深入讨论它！
