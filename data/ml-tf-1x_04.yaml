- en: Cats and Dogs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 猫和狗
- en: Back in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml), *Your First
    Classifier*, we constructed a simple neural network for our character recognition
    effort. We ended the chapter with commendable mid-80% accuracy. Good start, but
    we can do much better!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)《你的第一个分类器》中，我们为字符识别任务构建了一个简单的神经网络。我们在这一章的结尾取得了令人称赞的80%中期准确率。不错的开始，但我们可以做得更好！
- en: In this chapter, we will retrofit our earlier classifier with far more powerful
    network architecture. Then, we'll delve into a much more difficult problem—handling
    color images from the CIFAR-10 dataset. The images will be much more difficult
    (cats, dogs, airplanes, and so on), so we'll bring more powerful tools to the
    table—specifically, a convolutional neural network. Let's begin.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将为之前的分类器添加更强大的网络架构。接下来，我们将深入探讨一个更具挑战性的问题——处理CIFAR-10数据集中的彩色图像。图像会更加复杂（猫、狗、飞机等），因此我们将使用更强大的工具——具体来说，是卷积神经网络。让我们开始吧。
- en: Revisiting notMNIST
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视notMNIST
- en: 'Let''s start our effort incrementally by trying the technical changes on the
    `notMNIST` dataset we used in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. You can write the code as you go through the chapter,
    or work on the book''s repository at:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从渐进的方式开始，在[第2章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)《你的第一个分类器》中使用的`notMNIST`数据集上尝试技术变更。你可以在阅读本章时编写代码，或者在书籍的代码库中进行操作：
- en: '[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py](https://github.com/mlwithtf/mlwithtf/blob/master/chapter_02/training.py)'
- en: 'We will begin with the following imports:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从以下导入开始：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are not many substantial changes here. The real horsepower is already
    imported with the `tensorflow` package. You'll notice that we reuse our `data_utils`
    work from before. However, we'll need some changes there.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有太多实质性的变化。真正的计算能力已经通过`tensorflow`包导入。你会注意到，我们再次使用了之前的`data_utils`工作。然而，我们那里需要做一些更改。
- en: The only difference from before is the `math` package, which we will use for
    ancillary `math` functions, such as `ceiling`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前唯一的不同是`math`包，我们将使用它来处理辅助的`math`函数，例如`ceiling`。
- en: Program configurations
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序配置
- en: 'Now, let''s look at our old program configurations, which are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下我们以前的程序配置，如下所示：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will need more configurations this time. Here is what we will use now:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们需要更多的配置。以下是我们现在将使用的配置：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The first four configurations are familiar:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个配置是熟悉的：
- en: We will still train for a certain number of steps (`num_steps`), just as we
    did earlier. But, you'll notice the number of steps has gone up. They will get
    even higher because our datasets will be more complex and require more training.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仍然会训练一定数量的步骤（`num_steps`），就像以前一样。但你会注意到，步骤数量已经增加。它们将变得更加庞大，因为我们的数据集会更复杂，需要更多的训练。
- en: We will revisit subtleties around the learning rate (`learning_rate`) later,
    but to start with, you are already familiar with it.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们稍后会重新审视学习率（`learning_rate`）的细节，但首先你已经熟悉它了。
- en: We will review results intermediately every five hundred steps, which is trivially
    controlled by the `data_showing_step` variable.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在每五百步时中期回顾结果，这可以通过`data_showing_step`变量轻松控制。
- en: Finally, `log_location` controls where our TensorBoard logs are dumped. We are
    quite familiar with this from [Chapter 3](a6bb2a79-d492-4620-a28b-72ec62523593.xhtml),
    *The TensorFlow Toolbox*. We will use it again in this chapter but without explanations
    this time.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`log_location`控制我们TensorBoard日志的存储位置。我们在[第3章](a6bb2a79-d492-4620-a28b-72ec62523593.xhtml)《TensorFlow工具箱》中已经对它非常熟悉。这一章我们将再次使用它，但这次不再做解释。
- en: 'The next configuration—the **random seed** (`SEED`) variable - can be helpful.
    This can be left unset and TensorFlow will randomize numbers on each run. However,
    having a `seed` variable set, and constant across runs, will allow consistency
    from run to run as we debug our system. If you do use it, which you should do
    to start off, you can set it to any number you wish: your birthday, anniversary
    date, first phone number, or lucky number. I use the ZIP code for my beloved neighborhood.
    Enjoy the small things.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个配置——**随机种子**（`SEED`）变量——可能会很有帮助。这个变量可以不设置，TensorFlow会在每次运行时随机化数字。然而，设置一个`seed`变量并且在每次运行中保持一致，将有助于我们在调试系统时保持一致性。如果你使用它（推荐从一开始就使用），你可以将它设置为任何你喜欢的数字：你的生日、纪念日、第一次电话号码或幸运数字。我用的是我心爱的社区的邮政编码。享受那些小事吧。
- en: Finally, we will encounter seven new variables—`batch_size`, `patch_size`, `depth_inc`,
    `num_hidden_inc`, `conv_layers`, `stddev`, and `dropout_prob`. These are at the
    heart of how our newer, more advanced **Convolutional neural networks** (**CNNs**)
    works and will be introduced in context as we explore the network we're using.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们会遇到七个新变量——`batch_size`、`patch_size`、`depth_inc`、`num_hidden_inc`、`conv_layers`、`stddev`
    和 `dropout_prob`。这些是我们更新版、更先进的**卷积神经网络**（**CNN**）工作的核心，在我们探讨所使用的网络时，会在具体上下文中引入。
- en: Understanding convolutional networks
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积神经网络
- en: 'CNNs are more advanced neural networks specialized for machine learning with
    images. Unlike the hidden layers we used before, CNNs have some layers that are
    not fully connected. These convolutional layers have depth in addition to just
    width and height. The general principle is that an image is analyzed patch by
    patch. We can visualize the 7x7 patch in the image as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是专为机器学习中的图像处理而设计的更先进的神经网络。与我们之前使用的隐藏层不同，CNN有一些未完全连接的层。这些卷积层除了宽度和高度外，还有深度。一般原则是，图像是按补丁逐个分析的。我们可以如下可视化图像中的7x7补丁：
- en: '![](img/83aa17bb-6c42-4643-b9b5-81c484187653.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83aa17bb-6c42-4643-b9b5-81c484187653.png)'
- en: 'This reflects a 32x32 greyscale image, with a 7x7 patch. Example of sliding
    the patch from left to right is given as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这反映了一个32x32的灰度图像，使用了7x7的补丁。以下是从左到右滑动补丁的示例：
- en: '![](img/bdaa4b88-36da-4346-8a89-59584f189dcd.png)![](img/461f4a5e-855a-41cc-8dc8-fae26ab4656c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdaa4b88-36da-4346-8a89-59584f189dcd.png)![](img/461f4a5e-855a-41cc-8dc8-fae26ab4656c.png)'
- en: If this were a color image, we'd be sliding our patch simultaneously over three
    identical layers.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一张彩色图像，我们会同时在三个相同的层上滑动补丁。
- en: You probably noticed that we slid the patch over by one pixel. That is a configuration
    as well; we could have slid more, perhaps by two or even three pixels each time.
    This is the stride configuration. As you can guess, the larger the stride, the
    fewer the patches we will end up covering and thus, the smaller the output layer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到我们每次滑动补丁时，只移动了一个像素。这也是一种配置；我们也可以滑动得更多，也许每次移动两个甚至三个像素。这就是步幅配置。正如你猜测的那样，步幅越大，我们最终覆盖的补丁越少，因此输出层会更小。
- en: Matrix math, which we will not get into here, is performed to reduce the patch
    (with the full depth driven by the number of channels) into an output depth column.
    The output is just a single in height and width but many pixels deep. As we will
    slide the patch across, over, and across iteratively, the sequence of depth columns
    form a block with a new length, width, and height.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵运算，我们在此不作详细讨论，用于将补丁（其完整深度由通道数决定）压缩成一个输出深度列。输出只有一个高度和宽度，但深度很大。随着我们迭代地滑动补丁，深度列的序列形成了一个具有新长度、宽度和高度的块。
- en: There is another configuration at play here—the padding along the sides of the
    image. As you can imagine, the more padding you have the more room the patch has
    to slide and veer off the edge of the image. This allows more strides and thus,
    a larger length and width for the output volume. You'll see this in the code later
    as `padding='SAME'` or `padding='VALID'`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种配置在此起作用——图像边缘的填充。如你所想，填充越多，补丁滑动的空间就越大，可以越过图像的边缘。这使得步幅增大，从而输出体积的长度和宽度也更大。你稍后会在代码中看到这个配置，`padding='SAME'`
    或 `padding='VALID'`。
- en: 'Let''s see how these add up. We will first select a patch:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看这些是如何累加的。首先我们选择一个补丁：
- en: '![](img/8cfad793-1383-4f9d-9b06-f7f4b4c88927.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cfad793-1383-4f9d-9b06-f7f4b4c88927.png)'
- en: 'However, the patch is not just the square, but the full depth (for color images):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，补丁不仅仅是方形的，而是整个深度（针对彩色图像）：
- en: '![](img/73820b82-074d-41dc-ae5c-53cd8b6981b5.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73820b82-074d-41dc-ae5c-53cd8b6981b5.png)'
- en: 'We will then convolve that into a 1x1 volume, but with depth, as shown in the
    following diagram. The depth of the resulting volume is configurable and we will
    use `inct_depth` for this configuration in our program:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将其卷积成一个1x1的体积，但有深度，如下图所示。结果体积的深度是可配置的，我们将在程序中使用`inct_depth`来设置这个配置：
- en: '![](img/05389cd9-d779-4c51-b334-0d7609676e19.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05389cd9-d779-4c51-b334-0d7609676e19.png)'
- en: 'Finally, as we slide the patch across, over, and across again, through the
    original image, we will produce many such 1x1xN volumes, which itself creates
    a volume:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们继续滑动补丁时，补丁会多次穿越原始图像，生成多个1x1xN的体积，这些将组合成一个体积：
- en: '![](img/8c88a030-d437-434d-907f-c037089ade35.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c88a030-d437-434d-907f-c037089ade35.png)'
- en: We will then convolve that into a 1x1 volume.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将其卷积成一个1x1的体积。
- en: 'Finally, we will squeeze each layer of the resulting volume using a `POOL`
    operation. There are many types, but simple **max pooling** is typical:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`POOL`操作压缩结果体积的每一层。这里有许多类型，但简单的**最大池化**是最典型的：
- en: '![](img/01223d35-9aa6-4dbb-ba9c-3df61cadfa9f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01223d35-9aa6-4dbb-ba9c-3df61cadfa9f.png)'
- en: Much like with the sliding patches we used earlier, there will be a patch (except
    this time, we will take the maximum number of the patch) and a stride (this time,
    we'll want a larger stride to squeeze the image). We are essentially reducing
    the size. Here, we will use a 3x3 patch with a stride of 2.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前使用的滑动补丁一样，这里会有一个补丁（不过这次我们会取补丁的最大值）和一个步长（这次我们需要一个更大的步长来压缩图像）。我们本质上是在减少图像大小。在这里，我们将使用一个3x3的补丁，步长为2。
- en: Revisiting configurations
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视配置
- en: 'Now that we have been introduced to convolutional neural networks, let''s revisit
    the configurations that we encountered earlier: `batch_size`, `patch_size`, `depth_inc`,
    `num_hidden_inc`, `conv_layers`, `stddev`, and `dropout_prob`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了卷积神经网络，让我们重新审视之前遇到的配置：`batch_size`，`patch_size`，`depth_inc`，`num_hidden_inc`，`conv_layers`，`stddev`
    和 `dropout_prob`：
- en: Batch size (`batch_size`)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小（`batch_size`）
- en: Patch size (`patch_size`)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 补丁大小（`patch_size`）
- en: Depth increment (`depth_inc`)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度增量（`depth_inc`）
- en: Number hidden increment (`num_hidden_inc`)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层增量（`num_hidden_inc`）
- en: Convolutional layers (`conv_layers`)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层（`conv_layers`）
- en: Standard deviation (`stddev`)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准差（`stddev`）
- en: Dropout probability (`dropout_prob`)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃概率（`dropout_prob`）
- en: Constructing the convolutional network
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建卷积网络
- en: 'We will skip explanations for the two utility functions, reformat and accuracy,
    as we''ve already encountered these in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. Instead, we will jump directly to the neural network
    configuration. For comparison, the following figure shows our model from [Chapter
    2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml), *Your First Classifier*, and the
    next figure shows our new model. We''ll run the new model on the same `notMNIST`
    dataset to see the accuracy boost that we will get (hint: good news!):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过对两个工具函数`reformat`和`accuracy`的解释，因为我们已经在[第2章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)中遇到过它们，*你的第一个分类器*。相反，我们将直接跳到神经网络配置部分。为了比较，以下图展示了我们在[第2章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)中看到的模型，*你的第一个分类器*，接下来的图展示了我们的新模型。我们将在相同的`notMNIST`数据集上运行新模型，看看我们能获得的准确率提升（提示：好消息！）。
- en: '![](img/1ab3311b-ae1f-46ce-96c4-b3f095eae98b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ab3311b-ae1f-46ce-96c4-b3f095eae98b.png)'
- en: 'The following figure is our new model:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是我们的新模型：
- en: '![](img/365f9caf-dc2c-4ac9-bfd2-1df7927d6cbf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/365f9caf-dc2c-4ac9-bfd2-1df7927d6cbf.png)'
- en: 'First, we will encounter a `helper` function, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将遇到一个`helper`函数，具体如下：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we will call it later, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将在稍后调用它，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `fc_first_layer_dimen` function calculates the dimensions of the first fully
    connected layer. Recall how CNN's typically use a series of layers with a smaller
    window layer after layer. Here, we've decided to reduce the dimensions by half
    for each convolutional layer we used. This also shows why having input images
    highly divisible by powers of two makes things nice and clean.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`fc_first_layer_dimen`函数计算第一个全连接层的维度。回想一下，CNN通常使用一系列逐层变小的窗口层。在这里，我们决定将每个卷积层的维度缩小一半。这也说明了为什么输入图像在被2的幂次方整除时，事情变得干净而简洁。'
- en: Let's now parse the actual network. This is generated using the `nn_model` method
    and called later when training the model, and again when testing against the validation
    and test sets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们解析实际的网络。它是通过`nn_model`方法生成的，并在训练模型时稍后调用，在验证集和测试集上进行测试时再次调用。
- en: 'Recall how CNN''s are usually composed of the following layers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 记得CNN通常由以下几层组成：
- en: Convolutional layers
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Rectified linear unit layers
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性整流单元（ReLU）层
- en: Pooling layers
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layers
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: The convolutional layers are usually paired with **RELU** layers and repeated.
    That is what we've done—we've got three nearly identical **CONV-RELU** layers
    stacked on top of each other.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通常与**RELU**层配对并重复使用。这正是我们所做的——我们将三个几乎相同的**CONV-RELU**层堆叠在一起。
- en: 'Each of the paired layers appears as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每一对配对的层如下所示：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The major difference across the three nearly identical layers (`Layer_1`, `Layer_2`,
    and `Layer_3`) is how the output of one is fed to the next in a series. So, the
    first layer begins by taking in data (the image data) but the second layer begins
    by taking in the pooling layer output from the first layer, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 三个几乎相同的层（`Layer_1`、`Layer_2`和`Layer_3`）之间的主要区别在于如何将一个层的输出传递给下一个层。所以，第一层开始时接受数据（图像数据），但第二层开始时接受来自第一层的池化层输出，具体如下：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Similarly, the third layer begins by taking in the pooling layer output from
    the second layer, as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，第三层开始时接受来自第二层的池化层输出，具体如下：
- en: '[PRE7]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is another major difference across the three `CONV`-`RELU` layers, that
    is, the layers get squeezed. It might help to peek at the `conv` variable after
    each layer is declared using a couple of `print` statements like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 三个`CONV`-`RELU`层之间有另一个主要的区别，即这些层被压缩了。你可以通过在每个层声明后使用几个`print`语句来看一下`conv`变量，这可能会有所帮助：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will reveal the following structures:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将揭示以下结构：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We ran this with the `notMNIST` dataset, so we will see an original input size
    of 28x28 to no surprise. More interesting are the sizes of successive layers—14x14
    and 7x7\. Notice how the filters for successive convolutional layers are squeezed.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用`notMNIST`数据集运行了这个，因此我们将看到原始输入大小为28x28，这不奇怪。更有趣的是连续层的大小——14x14和7x7。注意，连续卷积层的滤波器是如何被压缩的。
- en: 'Let''s make things more interesting and examine the entire stack. Add the following
    `print` statements to peek at the `CONV`, `RELU`, and `POOL` layers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们让事情变得更有趣，检查整个堆栈。添加以下`print`语句来查看`CONV`、`RELU`和`POOL`层：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Add similar statements after the other two `CONV`-`RELU`-`POOL` stacks and
    you''ll find the following output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他两个`CONV`-`RELU`-`POOL`堆栈后添加类似的语句，你将得到以下输出：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will ignore the outputs from the validation and test instances (those are
    the same, except with a height of 10000 instead of `32` as we're processing the
    validation and test sets rather than a minibatch).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将忽略来自验证集和测试集实例的输出（它们是相同的，只是由于我们在处理验证集和测试集而不是小批量数据，因此高度是10000而不是`32`）。
- en: We will see from the outputs how the dimension is squeezed at the `POOL` layer
    (`28` to `14`) and how that squeeze then carries to the next `CONV` layer. At
    the third and final `POOL` layer, we will end up with a 4x4 size.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从输出中看到，`POOL`层如何压缩维度（从`28`到`14`），以及这种压缩如何传递到下一个`CONV`层。在第三个也是最后一个`POOL`层，我们将得到一个4x4的大小。
- en: 'There is another feature on the final `CONV` stack—a `dropout` layer that we
    will use when training, which is as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最终`CONV`堆栈中还有一个特性——我们在训练时会使用的`dropout`层，具体如下：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This layer utilizes the `dropout_prob = 0.8` configuration we set earlier. It
    randomly drops neurons on the layer to prevent overfitting by disallowing nodes
    from coadapting to neighboring nodes with dropouts; they can never rely on a particular
    node being present.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层利用了我们之前设置的`dropout_prob = 0.8`配置。它随机丢弃该层上的神经元，以通过禁止节点与相邻节点共同适应而防止过拟合；节点永远不能依赖某个特定节点的存在。
- en: 'Let''s proceed through our network. We''ll find a fully connected layer followed
    by a `RELU`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续前进，看看我们的网络。我们会找到一个全连接层，后面跟着一个`RELU`层：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we will end with a fully connected layer, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将以一个全连接层结束，具体如下：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This is typical for the convolutional network. Typically, we will end up with
    a fully connected, `RELU` layer and finally a fully connected layer that holds
    scores for each class.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是卷积网络中的典型做法。通常，我们会以一个全连接的`RELU`层结束，最后是一个全连接层，它保存每个类别的得分。
- en: 'We skipped some details along the way. Most of our layers were initialized
    with three other values—`weights`, `biases`, and `strides`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在过程中跳过了一些细节。我们的大多数层都通过三个其他值进行了初始化——`weights`、`biases`和`strides`：
- en: '![](img/1a019ee4-889d-4cec-a30c-b3ca3bce9577.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a019ee4-889d-4cec-a30c-b3ca3bce9577.png)'
- en: The `weights` and `biases` are themselves initialized with other variables.
    I didn't say this will be easy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`weights`和`biases`本身是通过其他变量初始化的。我没有说这会很容易。'
- en: The most important variable here is `patch_size`, which denotes the size of
    the filter we slide across the image. Recall that we set this to 5 early on, so
    we will use 5x5 patches. We will also get reintroduced to the `stddev` and `depth_inc`
    configurations that we set up earlier.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里最重要的变量是`patch_size`，它表示我们滑过图像的滤波器大小。回想一下，我们早些时候将其设置为5，所以我们将使用5x5的补丁。我们还将重新介绍我们之前设置的`stddev`和`depth_inc`配置。
- en: Fulfilment
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完成
- en: Likely, by now, you must have many questions running through your mind—why three
    convolutional layers rather than two or four? Why a stride of one? Why a patch
    size of five? Why end up with fully connected layers rather than start with them?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，到现在为止，你脑海中一定有许多问题——为什么是三个卷积层而不是两个或四个？为什么步幅是1？为什么补丁大小是5？为什么最终是全连接层，而不是从全连接层开始？
- en: There is some method to the madness here. At the core, CNN's are built around
    image processing and patches are built around the features being sought. Why some
    configurations work well while others do not is not fully understood, though general
    rules do follow intuition. The exact network architectures are discovered, honed,
    and increasingly inch toward perfection through thousands of trials and many errors.
    It continues to be a research-grade task.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一定的方法可循。从核心上讲，卷积神经网络（CNN）是围绕图像处理构建的，而补丁则围绕着待提取的特征构建。为什么某些配置效果很好，而其他配置效果不佳，目前尚不完全理解，尽管一些普遍的规则符合直觉。准确的网络架构是通过数千次的试验和许多错误发现、磨练的，并且不断朝着完美的方向迈进。这仍然是一个研究级的任务。
- en: The practitioner's general approach is often to find a well working, existing
    architecture (for example, AlexNet, GoogLeNet, ResNet) and tweak them for use
    with a specific dataset. That is what we did; we started with AlexNet and tweaked
    it. Perhaps, that is not fulfilling, but it works and remains the state of practice
    in 2016.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从业者的一般方法是找到一个已经有效的现有架构（例如，AlexNet、GoogLeNet、ResNet），并针对特定数据集进行调整。这就是我们所做的；我们从AlexNet开始并进行了调整。也许，这并不令人满足，但它有效，并且在2016年仍然是实践的常态。
- en: Training day
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练日
- en: It will be more fulfilling, however, to see our training in action and how we
    will improve upon what we did earlier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，看到我们的训练过程并了解如何改进我们之前所做的工作，会更加令人满足。
- en: 'We will prepare the training dataset and labels as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照如下方式准备训练数据集和标签：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we will run the trainer, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将运行训练器，如下所示：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is very similar to what we did in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*. We instantiated the network, passed in an initial set
    of weights and biases, and defined a `loss` function using the training labels.
    Our optimizer is then defined to minimize that `loss`, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在[第二章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)中做的非常相似，*你的第一个分类器*。我们实例化了网络，传入了一组初始权重和偏差，并定义了一个使用训练标签的`loss`函数。然后，我们定义了优化器，目标是最小化该`loss`，如下所示：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will then use the `weights` and `biases` to predict labels for the validation
    and, eventually, the training set:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用`weights`和`biases`来预测验证集标签，最终是训练集标签：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The complete code for training session is as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练代码如下：
- en: '![](img/511d8afb-d417-4be7-b0da-a6d71bf75963.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/511d8afb-d417-4be7-b0da-a6d71bf75963.png)'
- en: 'Finally, we will run the session. We will use the `num_steps` variable that
    we set earlier and run through the training data in chunks (`batch_size`.) We
    will load small chunks of the training data and associated labels, and run the
    session as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将运行会话。我们将使用之前设置的`num_steps`变量，并按块（`batch_size`）遍历训练数据。我们将加载小块的训练数据和相应的标签，并按如下方式运行会话：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We will get back predictions on the minibatch, which we will compare against
    the actual labels to get accuracy on the minibatch.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对小批量数据进行预测，并将其与实际标签进行比较，以计算小批量的准确率。
- en: 'We will use the following `valid_prediction` that we declared earlier:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前声明的`valid_prediction`：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we will evaluate the validation set predictions against the actual labels
    we know, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将评估验证集的预测结果与实际标签进行比较，如下所示：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After we''ve run through all the steps, we will do the same in our test set:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们完成所有步骤之后，我们将在测试集上做同样的事情：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see the actual execution of the training, validation, and test was
    not that different from before. What is different from before is the accuracy.
    Notice that we''ve broken out of the 80s into the 90s on test set accuracy:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，实际的训练、验证和测试执行与之前没有太大不同。不同之处在于准确率。注意，我们已经突破了80%的准确率，进入了测试集准确率的90%：
- en: '![](img/1c25f2c5-ac0d-46c3-a118-6171a8660e77.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c25f2c5-ac0d-46c3-a118-6171a8660e77.png)'
- en: Actual cats and dogs
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际的猫和狗
- en: We've demonstrated our new tools on the `notMNIST` dataset, which was helpful
    as it served to provide a comparison to our earlier simpler network setup. Now,
    let's progress to a more difficult problem—actual cats and dogs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在`notMNIST`数据集上展示了我们新的工具，这很有帮助，因为它为我们提供了与之前更简单网络设置的比较。现在，让我们进阶到一个更困难的问题——实际的猫和狗。
- en: 'We''ll utilize the CIFAR-10 dataset. There will be more than just cats and
    dogs, there are 10 classes—airplanes, automobiles, birds, cats, deer, dogs, frogs,
    horses, ships, and trucks. Unlike the `notMNIST` set, there are two major complexities,
    which are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 CIFAR-10 数据集。这个数据集中不只有猫和狗，还有10个类别——飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。与 `notMNIST`
    数据集不同，这里有两个主要的复杂性，具体如下：
- en: There is far more heterogeneity in the photos, including background scenes
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 照片中有更多的异质性，包括背景场景
- en: The photos are color
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些照片是彩色的
- en: We have not worked with color datasets before. Luckily, it is not that different
    from the usual black and white dataset—we will just add another dimension. Recall
    that our previous 28x28 images were flat matrices. Now, we'll have 32x32x3 matrices
    - the extra dimension represents a layer for each red, green, and blue channels.
    This does make visualizing the dataset more difficult, as stacking up images will
    go into a fourth dimension. So, our training/validation/test sets will now be
    32x32x3xSET_SIZE in dimension. We'll just need to get used to having matrices
    that we cannot visualize in our familiar 3D space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有处理过彩色数据集。幸运的是，它与通常的黑白数据集并没有太大区别——我们只需增加一个维度。回想一下，我们之前的28x28图像是平面矩阵。现在，我们将有32x32x3的矩阵——额外的维度代表每个红色、绿色和蓝色通道的层。这样做确实让数据集的可视化变得更加困难，因为堆叠图像将进入第四维度。所以，我们的训练/验证/测试集现在将是32x32x3xSET_SIZE的维度。我们只需要习惯处理那些我们无法在熟悉的3D空间中可视化的矩阵。
- en: The mechanics of the color dimension are the same though. Just as we had floating
    point numbers representing shades of grey earlier, we will now have floating point
    numbers representing shades of red, green, and blue.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色维度的机制是一样的。就像之前我们有浮动点数字表示灰度的不同深浅，现在我们将有浮动点数字表示红色、绿色和蓝色的不同深浅。
- en: 'Recall how we loaded the `notMNIST` dataset:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们如何加载 `notMNIST` 数据集吗？
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `num_channels` variable dictated the color channels. It was just one until
    now.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_channels` 变量决定了颜色通道。直到现在，它只有一个通道。'
- en: 'We''ll load the CIFAR-10 set similarly, except this time, we''ll have three
    channels returned, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将类似地加载 CIFAR-10 数据集，不过这次我们会返回三个通道，如下所示：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Not reinventing the wheel.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 不要重新发明轮子。
- en: Recall how we automated the dataset grab, extraction, and preparation for our
    `notMNIST` dataset in [Chapter 2](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml),
    *Your First Classifier*? We put those pipeline functions into the `data_utils.py`
    file to separate our pipeline code from our actual machine learning code. Having
    that clean separation and maintaining clean, generic functions allows us to reuse
    those for our current project.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们如何自动化抓取、提取和准备 `notMNIST` 数据集的过程吗？这在[第2章](0197f632-3ce2-4032-9abd-83b3720c7127.xhtml)《你的第一个分类器》中讲解过。我们将这些流程函数放进了
    `data_utils.py` 文件中，以便将流程代码与实际的机器学习代码分开。拥有这样的清晰分离，并保持干净、通用的函数，可以让我们在当前项目中重用它们。
- en: 'In particular, we will reuse nine of those functions, which are as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们将重用其中的九个函数，具体如下：
- en: '`download_hook_function`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`download_hook_function`'
- en: '`download_file`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`download_file`'
- en: '`extract_file`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`extract_file`'
- en: '`load_class`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_class`'
- en: '`make_pickles`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_pickles`'
- en: '`randomize`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randomize`'
- en: '`make_arrays`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_arrays`'
- en: '`merge_datasets`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`merge_datasets`'
- en: '`pickle_whole`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pickle_whole`'
- en: Recall how we used those functions inside an overarching function, `prepare_not_mnist_dataset`,
    which ran the entire pipeline for us. We just reused that function earlier, saving
    ourselves quite a bit of time.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们如何在一个总的函数 `prepare_not_mnist_dataset` 中使用那些函数，这个函数为我们运行了整个流程。我们之前只是重用了这个函数，为自己节省了不少时间。
- en: Let's create an analogous function for the CIFAR-10 set. In general, you should
    save your own pipeline functions, try to generalize them, isolate them into a
    single module, and reuse them across projects. As you do your own projects, this
    will help you focus on the key machine learning efforts rather than spending time
    on rebuilding pipelines.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为 CIFAR-10 数据集创建一个类似的函数。一般来说，你应该保存自己的流程函数，尝试将它们通用化，独立成一个模块，并在不同项目中重用它们。当你做自己的项目时，这会帮助你专注于关键的机器学习工作，而不是花时间去重建流程。
- en: 'Notice the revised version of `data_utils.py`; we have an overarching function
    called `prepare_cifar_10_dataset` that isolates the dataset details and pipelines
    for this new dataset, which is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更新版的 `data_utils.py`；我们有一个总的函数叫做 `prepare_cifar_10_dataset`，它将数据集细节和这个新数据集的流程隔离开来，如下所示：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is a quick overview of the preceding code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是之前代码的快速概览：
- en: We will grab the dataset from Alex Krizhevsky's site at the University of Toronto
    using `cifar_dataset_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'`
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 `cifar_dataset_url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'`
    从亚历克斯·克里日夫斯基（Alex Krizhevsky）在多伦多大学的站点获取数据集
- en: We will use `dataset_size = 170498071` to validate whether we've received the
    file successfully, rather than some truncated half download
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用 `dataset_size = 170498071` 来验证我们是否已成功接收到文件，而不是某个被截断的半下载文件
- en: We will also declare some details based on our knowledge of the dataset
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将基于对数据集的了解声明一些细节
- en: We will segment our set of 60,000 images into training, validation, and test
    sets of `45000`, `5000`, and `10000` images respectively
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把 60,000 张图片划分为训练集、验证集和测试集，分别包含 `45000`、`5000` 和 `10000` 张图片
- en: There are ten classes of images, so we have `num_of_classes = 10`
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集共有十个类别，因此我们有 `num_of_classes = 10`
- en: These are color images with red, green, and blue channels, so we have `num_of_channels
    = 3`
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些是带有红、绿、蓝三个通道的彩色图像，因此我们有 `num_of_channels = 3`
- en: We will know the images are 32x32 pixels, so we have `image_size = 32` that
    we'll use for both width and height
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们知道图片的尺寸是32x32像素，因此我们设置 `image_size = 32`，并将其应用于宽度和高度
- en: Finally, we will know the images are 8-bit on each channel, so we have `image_depth
    = 255`
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们知道每个通道的图像深度为8位，因此我们设置 `image_depth = 255`
- en: The data will end up at `/datasets/CIFAR-10/`
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据将被保存到 `/datasets/CIFAR-10/` 目录
- en: Much like we did with the `notMNIST` dataset, we will download the dataset only
    if we don't already have it. We will unarchive the dataset, do the requisite transformations,
    and save preprocessed matrices as pickles using `pickle_cifar_10`. If we find
    the `pickle` files, we can reload intermediate data using the `load_cifar_10_from_pickles`
    method.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前处理 `notMNIST` 数据集一样，我们只有在尚未拥有数据集的情况下才会下载它。我们将解压数据集，进行必要的转换，并使用 `pickle_cifar_10`
    将预处理后的矩阵保存为 pickle 文件。如果我们找到 `pickle` 文件，就可以使用 `load_cifar_10_from_pickles` 方法重新加载中间数据。
- en: 'The following are the three helper methods that we will use to keep the complexity
    of the main method manageable:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将用来保持主方法简洁的三个辅助方法：
- en: '`pickle_cifar_10`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pickle_cifar_10`'
- en: '`load_cifar_10_from_pickles`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_cifar_10_from_pickles`'
- en: '`load_cifar_10_pickle`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_cifar_10_pickle`'
- en: 'The functions are defined as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 函数定义如下：
- en: '![](img/4169bb8d-f6cc-4ac9-968a-1895971a4c1c.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4169bb8d-f6cc-4ac9-968a-1895971a4c1c.png)'
- en: 'The `load_cifar_10_pickle` method allocates numpy arrays to train and test
    data and labels as well as load existing pickle files into these arrays. As we
    will need to do everything twice, we will isolate the `load_cifar_10_pickle` method,
    which actually loads the data and zero-centers it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_cifar_10_pickle` 方法会分配 NumPy 数组来存储训练和测试数据及其标签，并将现有的 pickle 文件加载到这些数组中。由于我们需要执行两次这个操作，我们将提取出
    `load_cifar_10_pickle` 方法，它实际上加载数据并将其中心化：'
- en: '![](img/9f4b7d1d-507d-481a-b64d-87b48fbcea69.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f4b7d1d-507d-481a-b64d-87b48fbcea69.png)'
- en: Much like earlier, we will check to see if the `pickle` files exist already
    and if so, load them. Only if they don't exist (the `else` clause), we actually
    save `pickle` files with the data we've prepared.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前一样，我们将检查 `pickle` 文件是否已存在，如果存在就加载。如果不存在（即 `else` 部分），我们才会将已准备好的数据保存为 `pickle`
    文件。
- en: Saving the model for ongoing use
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存模型以供后续使用
- en: 'To save variables from the tensor flow session for future use, you can use
    the `Saver()` function. Let''s start by creating a `saver` variable right after
    the `writer` variable:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 TensorFlow 会话中的变量保存以供将来使用，你可以使用 `Saver()` 函数。我们从创建一个 `saver` 变量开始，紧接着是 `writer`
    变量：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, in the training loop, we will add the following code to save the model
    after every `model_saving_step`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在训练循环中，我们将添加以下代码，以便在每次 `model_saving_step` 后保存模型：
- en: '[PRE27]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After that, whenever we want to restore the model using the `saved` model,
    we can easily create a new `Saver()` instance and use the `restore` function as
    follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，每当我们想要使用 `saved` 模型恢复模型时，我们可以轻松创建一个新的 `Saver()` 实例并使用 `restore` 函数，如下所示：
- en: '[PRE28]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding code, we use the `tf.train.latest_checkpoint` so that TensorFlow
    will automatically choose the latest model checkpoint. Then, we create a new `Saver`
    instance named restore. Finally, we can use the `restore` function to load the
    `saved` model to the session graph:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们使用 `tf.train.latest_checkpoint`，这样 TensorFlow 会自动选择最新的模型检查点。然后，我们创建一个新的
    `Saver` 实例，命名为 `restore`。最后，我们可以使用 `restore` 函数将 `saved` 模型加载到会话图中：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You should note that we must restore after we run the `tf.global_variables_initializer`.
    Otherwise, the loaded variables will be overridden by the initializer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，在运行 `tf.global_variables_initializer` 后，我们必须进行恢复。否则，加载的变量将被初始化器覆盖。
- en: Using the classifier
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分类器
- en: 'Now that we''ve enhanced the classifier to load random images, we''ll start
    with choosing these random images with the exact size and shape of our training/testing
    images. We''ll need to add placeholders for these user-provided images, so we''ll
    add the following lines in the appropriate locations:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经增强了分类器，以加载随机图像，接下来我们将选择与训练/测试图像大小和形状完全相同的随机图像。我们需要为这些用户提供的图像添加占位符，因此我们将在适当的位置添加以下行：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will grab the image provided by the user via the following command-line
    parameter and run our session on the image:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过以下命令行参数获取用户提供的图像，并在该图像上运行我们的会话：
- en: '![](img/c5f0d571-ec3a-4f81-846e-5d210979b15d.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5f0d571-ec3a-4f81-846e-5d210979b15d.png)'
- en: 'We will follow almost the exact sequence as we did earlier. Running a `test`
    file through the script using the `-e` switch will yield an extra output, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照与之前几乎相同的顺序进行操作。通过 `-e` 开关运行 `test` 文件将产生额外的输出，如下所示：
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Voila! We just classified an arbitrary image.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！我们刚刚对一个任意图像进行了分类。
- en: Skills learned
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学到的技能
- en: 'You should have learned these skills in the chapter:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在本章中学会了这些技能：
- en: Preparing more advanced color training and test data
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备更高级的颜色训练和测试数据
- en: Setting up a convolutional neural network graph
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置卷积神经网络图
- en: Parameters and configurations associated with CNN's
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 CNN 相关的参数和配置
- en: Creating a full system including hooks for TensorBoard
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个完整的系统，包括 TensorBoard 的钩子
- en: Piping in real-world data
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入真实世界数据
- en: Summary
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Excellent! We just built a much more advanced classifier, swapped in and out
    models, and even started applying our classifier to arbitrary models. True to
    our chapter's name, we've also trained our system to differentiate cats and dogs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们刚刚建立了一个更高级的分类器，切换了不同的模型，甚至开始将我们的分类器应用于任意模型。正如本章名称所示，我们还训练了系统区分猫和狗。
- en: In the next chapter, we will start working with sequence-to-sequence models
    and write an English to French translator with TensorFlow.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始使用序列到序列的模型，并使用 TensorFlow 编写一个英法翻译器。
