- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: The TextWorld Environment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TextWorld 环境
- en: In this chapter, we will now use RL to solve text-based interactive fiction
    games, using the environment published by Microsoft Research called TextWorld.
    This will provide a good illustration of how RL can be applied to complicated
    environments with a rich observation space. In addition, we’ll touch on deep NLP
    methods a bit and play with LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用强化学习（RL）来解决基于文本的互动小说游戏，使用微软研究院发布的环境——TextWorld。这将很好地展示强化学习如何应用于具有丰富观察空间的复杂环境。此外，我们还将稍微涉及一下深度自然语言处理（NLP）方法，并与大语言模型（LLMs）进行一些互动。
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: Cover a brief historical overview of interactive fiction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简要回顾互动小说的历史
- en: Study the TextWorld environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究TextWorld环境
- en: Implement the simple baseline deep Q-network (DQN) method, and then try to improve
    it by adding several tweaks to the observation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现简单的基准深度Q网络（DQN）方法，然后尝试通过对观察进行若干调整来改善它
- en: Use pretrained tronsformers from the Hugging Face Hub to implement sentence
    embedding for our agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hugging Face Hub中的预训练变压器（transformers）为我们的智能体实现句子嵌入
- en: Use OpenAI ChatGPT to check the power of modern Large Language Models (LLMs)
    on interactive fiction games
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用OpenAI ChatGPT检查现代大语言模型（LLMs）在互动小说游戏中的能力
- en: Interactive fiction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 互动小说
- en: As you have already seen, computer games are not only entertaining for humans
    but also provide challenging problems for RL researchers due to the complicated
    observations and action spaces, long sequences of decisions to be made during
    the gameplay, and natural reward systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，电脑游戏不仅对人类有娱乐性，还由于其复杂的观察和行动空间、游戏过程中的长决策序列以及自然的奖励系统，为强化学习研究人员提供了具有挑战性的问题。
- en: Arcade games like those on the Atari 2600 are just one of many genres that the
    gaming industry has. Let’s take a step back and take a quick look at the historical
    perspective. The Atari 2600 platform peaked in popularity during the late 70s
    and early 80s. Then followed the era of Z80 and clones, which evolved into the
    period of the PC-compatible platforms and consoles we have now. Over time, computer
    games continually become more complex, colorful, and detailed in terms of graphics,
    which inevitably increased hardware requirements. This trend makes it harder for
    RL researchers and practitioners to apply RL methods to the more recent games;
    for example, almost everybody can train an RL agent to solve an Atari game, but
    for StarCraft II, DeepMind had to burn electricity for weeks, leveraging clusters
    of graphics processing unit (GPU) machines. Of course, this activity is needed
    for future research, as it allows us to check ideas and optimize methods, but
    the complexity of StarCraft II and Dota, for example, makes them prohibitively
    expensive for most people.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 像Atari 2600上的街机游戏只是游戏行业众多类型中的一种。让我们退一步，从历史的角度快速回顾一下。Atari 2600平台在70年代末和80年代初的受欢迎程度达到了巅峰。随后进入了Z80及其克隆机的时代，演变成现在的PC兼容平台和游戏主机时代。随着时间的推移，电脑游戏在复杂性、色彩和图形细节方面不断发展，这不可避免地提高了硬件需求。这一趋势使得强化学习研究人员和从业者在应用强化学习方法到更现代的游戏时遇到困难。例如，几乎每个人都能训练一个强化学习智能体来解决Atari游戏，但对于《星际争霸II》来说，DeepMind不得不花费数周的电力，利用图形处理单元（GPU）集群。当然，这项工作对未来的研究至关重要，因为它使我们能够检验想法并优化方法，但《星际争霸II》和《Dota》这类游戏的复杂性使得它们对大多数人来说费用过高。
- en: 'There are several ways of solving this problem:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以解决这个问题：
- en: The first one is to take games that are “in the middle” of the complexities
    of Atari and StarCraft. Luckily, there are literally thousands of games from the
    Z80, NES, Sega, and C64 platforms.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个方法是选择那些“介于”Atari和《星际争霸》复杂性之间的游戏。幸运的是，来自Z80、NES、Sega和C64平台的游戏多得不可计数。
- en: Another way is to take a challenging game but make a simplification to the environment.
    There are several Doom environments (available in Gym), for example, that use
    the game engine as a platform, but the goal is much simpler than in the original
    game, like navigating the corridor, gathering weapons, or shooting enemies. Those
    microgames are also available on StarCraft II.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是选择一个具有挑战性的游戏，但简化其环境。有几个Doom环境（可以在Gym中获得），例如，它们使用游戏引擎作为平台，但目标比原始游戏简单得多，比如导航走廊、收集武器或射击敌人。这些微型游戏在《星际争霸II》中也可以找到。
- en: The third, and completely different, approach is to take some game that may
    not be very complex in terms of observation but requires long-term planning, complex
    exploration of the state space, and has challenging interactions between objects.
    An example of this family is the famous Montezuma’s Revenge from the Atari suite,
    which is still challenging even for modern RL methods.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三种完全不同的方法是，选择一些可能在观察上并不复杂，但需要长期规划、复杂的状态空间探索，并且具有物体间具有挑战性互动的游戏。这一类的例子是著名的雅达利游戏《Montezuma’s
    Revenge》，即使对于现代的强化学习方法来说，这款游戏依然具有挑战性。
- en: The last approach is quite appealing, due to the accessibility of resources,
    combined with still having a complexity that reaches the edge of RL methods’ limits.
    Another example of this is text-based games, which are also known as interactive
    fiction. This genre is almost dead now, being made obsolete by modern games and
    hardware progress, but at the time of Atari and Z80, interactive fiction was provided
    concurrently with traditional games. Instead of using rich graphics to show the
    game state (which was tricky for hardware from the 70s), these games relied on
    the players’ minds and imagination.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种方法颇具吸引力，因为它的资源可获得性加上仍然具有达到强化学习方法极限的复杂性。另一个例子是基于文本的游戏，这些游戏也被称为互动小说。这个类型的游戏现在几乎消失了，被现代游戏和硬件的发展所淘汰，但在雅达利和Z80时代，互动小说和传统游戏是同时提供的。这些游戏并不依赖丰富的图形来展示游戏状态（70年代的硬件难以实现这一点），而是依赖玩家的思维和想象力。
- en: The gaming process was communicated via text when the description of the current
    game state was given to the player. An example is, You are standing at the end
    of a road before a small brick building. Around you is a forest. A small stream
    flows out of the building and down a gully.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏过程通过文本传达，当游戏的当前状态描述呈现给玩家时，例如：你正站在一条小路的尽头，前方是一座小砖砌建筑。你周围是一片森林。一条小溪从建筑物中流出，沿着山谷流下。
- en: As you can see in Figure [13.1](#x1-220004r1), this is the very beginning of
    the Adventure game from 1976, which was the first game of this kind. Actions in
    the game were given in the form of free-text commands, which normally had a simple
    structure and a limited set of words, for example, “verb + noun.”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[13.1](#x1-220004r1)所示，这是1976年冒险游戏的开端，这也是这一类型游戏的第一款。游戏中的动作通过自由文本命令的形式呈现，通常结构简单，词汇量有限，例如，“动词
    + 名词”。
- en: '![PIC](img/file150.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file150.png)'
- en: 'Figure 13.1: An example of the interactive fiction game process'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：互动小说游戏过程的示例
- en: Despite the simplistic descriptions, in the 80s and early 90s, hundreds of large
    and small games were developed by individual developers and commercial studios.
    Those games sometimes required many hours of gameplay, contained thousands of
    locations, and had a lot of objects to interact with. For example, Figure [13.2](#x1-220006r2)
    shows part of the Zork I game map published by Infocom in 1980.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管描述简洁，在80年代和90年代初，数百款大小游戏由个人开发者和商业工作室开发。这些游戏有时需要数小时的游戏时间，包含成千上万的地点，并有许多物体可供互动。例如，图[13.2](#x1-220006r2)展示了1980年Infocom发布的Zork
    I游戏地图的一部分。
- en: '![PIC](img/file151.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file151.jpg)'
- en: 'Figure 13.2: The underground portion of the Zork I map (for better visualization,
    refer to https://packt.link/gbp/9781835882702)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：Zork I地图的地下部分（为了更好的可视化，请参考 https://packt.link/gbp/9781835882702）
- en: 'As you can imagine, the challenges of such games could be increased almost
    infinitely, as complex interactions between objects, exploration of game states,
    communication with other characters, and other real-life scenarios could be included.
    There are many such games available on the Interactive Fiction Archive website:
    [http://ifarchive.org](http://ifarchive.org).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，这类游戏的挑战几乎可以无限增加，因为它们可以包括物体之间的复杂交互、游戏状态的探索、与其他角色的沟通以及其他现实生活场景。在互动小说档案馆网站上，有许多此类游戏可以体验：[http://ifarchive.org](http://ifarchive.org)。
- en: 'In June 2018, Microsoft Research released an open source project that aimed
    to provide researchers and RL enthusiasts with a simple way to experiment with
    text-based games using familiar tools. Their project, called TextWorld, is available
    on GitHub ([https://github.com/microsoft/TextWorld](https://github.com/microsoft/TextWorld))
    and provides the following functionality:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年6月，微软研究院发布了一个开源项目，旨在为研究人员和强化学习爱好者提供一种简单的方式，使用熟悉的工具实验文本游戏。这个名为TextWorld的项目可以在GitHub上找到（[https://github.com/microsoft/TextWorld](https://github.com/microsoft/TextWorld)），并提供了以下功能：
- en: 'A Gym environment for text-based games. It supports games in two formats: Z-machine
    bytecode (versions 1–8 are supported) and Glulx games.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于基于文本的游戏的Gym环境。它支持两种格式的游戏：Z-machine字节码（支持版本1-8）和Glulx游戏。
- en: A game generator that allows you to produce randomly generated quests with pre-defined
    complexity like the number of objects, the description, and the quest length.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个游戏生成器，允许你生成具有预定义复杂度的随机生成任务，如物品数量、描述和任务长度。
- en: The capability to tune (for generated games) the complexity of the environment
    by peeking at the game state. For example, intermediate rewards could be enabled,
    which will give a positive reward to the agent every time it makes a step in the
    right direction. Several such factors will be described in the next section.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整（对于生成的游戏）环境复杂度的能力，可以通过查看游戏状态来实现。例如，可以启用中间奖励，每当代理在正确的方向上迈出一步时，就会给予正向奖励。接下来的部分将描述几个这样的因素。
- en: 'As we progress with this chapter, we will experiment with several games to
    explore the environment’s capabilities and implement several versions of training
    code to solve the generated games. You need to generate them by using the provided
    script: Chapter13/game/make_games.sh. It generates 21 games of length 5, using
    different seed values to ensure variability between the games. The complexity
    of the games will not be very high, but you can use them as a basis for your own
    experiments and idea validation.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章内容的推进，我们将尝试几个游戏，以探索环境的能力，并实现多个版本的训练代码来解决生成的游戏。你需要通过提供的脚本生成它们：`Chapter13/game/make_games.sh`。它将生成21个长度为5的游戏，使用不同的种子值确保游戏之间的变化性。这些游戏的复杂度不会很高，但你可以将它们作为自己实验和验证想法的基础。
- en: The environment
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: 'At the time of writing, the TextWorld environment supports only Linux and macOS
    platforms (for Windows, you can use a Docker container) and internally relies
    on the Inform 7 system ([https://inform7.com](https://inform7.com)). There are
    two web pages for the project: one is the Microsoft Research web page ( [https://www.microsoft.com/en-us/research/project/textworld/](https://www.microsoft.com/en-us/research/project/textworld/)),
    which contains general information about the environment, and the another is on
    GitHub ([https://github.com/microsoft/TextWorld](https://github.com/microsoft/TextWorld))
    and describes installation and usage. Let’s start with installation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，TextWorld环境仅支持Linux和macOS平台（对于Windows，你可以使用Docker容器），并且内部依赖于Inform 7系统（[https://inform7.com](https://inform7.com)）。该项目有两个网页：一个是微软研究网页（[https://www.microsoft.com/en-us/research/project/textworld/](https://www.microsoft.com/en-us/research/project/textworld/)），包含有关环境的一般信息；另一个在GitHub上（[https://github.com/microsoft/TextWorld](https://github.com/microsoft/TextWorld)），描述了安装和使用方式。让我们从安装开始。
- en: Installation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: Installation can be done with simple pip install textworld==1.6.1\. All the
    examples in this chapter were tested with the latest 1.6.1 release of the package.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 安装可以通过简单的pip命令：`pip install textworld==1.6.1`完成。本章中的所有示例都使用最新的1.6.1版本包进行了测试。
- en: 'Once installed, the package can be imported in Python code, and it also provides
    two command-line utilities for game generation and gameplay: tw-make and tw-play.
    They are not needed if you have ambitious plans to solve full-featured interactive
    fiction games from [http://ifarchive.org](http://ifarchive.org), but in our case,
    we will start with artificially generated quests for simplicity.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 安装后，可以在Python代码中导入该包，并提供两个命令行工具用于游戏生成和游戏玩法：tw-make和tw-play。如果你有雄心勃勃的计划，要解决来自[http://ifarchive.org](http://ifarchive.org)的全功能交互式小说游戏，则不需要它们，但在我们的例子中，我们将从人工生成的任务开始，以简化流程。
- en: Game generation
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏生成
- en: 'The tw-make utility allows you to generate games with the following characteristics:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: tw-make工具允许你生成具有以下特点的游戏：
- en: 'Game scenario: For example, you can choose a classic quest with the aim of
    using objects and following some sequence of actions, or a “coin collection” scenario,
    when the player needs to navigate the scenes and find coins'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏场景：例如，你可以选择一个经典任务，目标是使用物品并遵循一系列动作，或是一个“收集金币”场景，玩家需要在各个场景中找到金币。
- en: 'Game theme: You can set up the interior of the game, but at the moment, only
    the “house” and “basic” themes exist'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏主题：你可以设置游戏的内部环境，但目前只存在“房子”和“基础”两种主题。
- en: 'Object properties: You can include adjectives with objects; for instance, it
    might be the “green key” that opens the box, not just the “key”'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象属性：你可以为对象添加形容词；例如，它可能是“绿色的钥匙”打开箱子，而不仅仅是“钥匙”。
- en: 'The number of parallel quests that the game can have: By default, there is
    only one sequence of actions to be found, but you can change this and allow the
    game to have subgoals and alternative paths'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏可以拥有的并行任务数量：默认情况下，游戏中只有一个动作序列可以找到，但你可以更改此设置，允许游戏拥有子目标和替代路径。
- en: 'The length of the quest: You can define how many steps the player needs to
    take before reaching the end or solution of the game'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务的长度：你可以定义玩家在达到游戏的结局或解决方案之前需要采取多少步。
- en: 'Random seeds: You can use these to generate reproducible games'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机种子：你可以使用这些种子来生成可复现的游戏。
- en: The resulting game generated could be in Glulx or Z-machine format, which are
    standard portable virtual machine instructions that are widely used for normal
    games and supported by several interactive fiction interpreters, so you can play
    the generated games in the same way as normal interactive fiction games.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的游戏可以是 Glulx 或 Z-machine 格式，这些是标准的便携式虚拟机指令，广泛用于普通游戏，并且被多个互动小说解释器支持，因此你可以像玩普通互动小说游戏一样玩生成的游戏。
- en: 'Let’s generate some games and check what they bring us:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一些游戏，看看它们带来什么：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The command generates three files: t1.ulx, t1.ni, and t1.json. The first one
    contains bytecode to be loaded into the interpreter, and the others are extended
    data that could be used by the environment to provide extra information during
    the gameplay.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令会生成三个文件：t1.ulx、t1.ni 和 t1.json。第一个包含要加载到解释器中的字节码，其他两个是扩展数据，环境可以利用这些数据在游戏过程中提供额外的信息。
- en: 'To play the game in interactive mode, you can use any interactive fiction interpreter
    supporting the Glulx format, or use the provided utility tw-play, which might
    not be the most convenient way to play interactive fiction games but will enable
    you to check the result:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要在互动模式下玩游戏，你可以使用任何支持 Glulx 格式的互动小说解释器，或者使用提供的工具 tw-play，这可能不是玩互动小说游戏的最便捷方式，但它可以让你检查结果：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Observation and action spaces
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察和动作空间。
- en: 'Generating and playing a game might be fun, but the core value of TextWorld
    is in its ability to provide an RL interface for generated or existing games.
    Let’s check what we can do with the game we just generated in the previous section:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成并玩游戏可能很有趣，但 TextWorld 的核心价值在于它能为生成的或现有的游戏提供强化学习接口。让我们来看看我们可以用刚刚在上一节中生成的游戏做些什么：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we registered the generated game and created the environment. You might
    notice that we are not using the Gymnasium make() function, but instead, we use
    a function from the textworld module, which has the same name. This is not a mistake.
    In fact, the latest TextWorld release (at the time of writing) removed dependency
    on Gym API packages and provides their own environment class that looks very similar
    to the Env class (but not exactly the same).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们注册了生成的游戏并创建了环境。你可能注意到我们没有使用 Gymnasium 的 make() 函数，而是使用了来自 textworld 模块的同名函数。这不是错误。事实上，在撰写时，最新的
    TextWorld 版本移除了对 Gym API 包的依赖，并提供了自己的环境类，它与 Env 类非常相似（但不完全相同）。
- en: 'I believe this removal is temporary and part of the transition from OpenAI
    Gym to Farama Gymnasium. But at the moment, there are several aspects we have
    to take into account when using TextWorld:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这个移除是暂时的，且是从 OpenAI Gym 向 Farama Gymnasium 过渡的一部分。但目前，在使用 TextWorld 时，我们必须考虑几个方面：
- en: You have to create games using the textworld.gym.make() function, not gym.make().
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须使用 textworld.gym.make() 函数来创建游戏，而不是 gym.make()。
- en: Created environments don’t have observation and action space specifications.
    By default, both observation and actions are strings.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建的环境没有观察和动作空间的规格。默认情况下，观察和动作都是字符串类型。
- en: The function step() in the environment doesn’t return the is_truncated flag,
    just observation, reward, flag is_done, and a dictionary with extra information.
    Because of that, you cannot apply Gymnasium wrappers to this environment — small
    “adapter” wrapper has to be created.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境中的 step() 函数不会返回 is_truncated 标志，只会返回观察、奖励、标志 is_done 和一个包含额外信息的字典。因此，你不能将
    Gymnasium 的包装器应用于这个环境——必须创建一个小的“适配器”包装器。
- en: In previous versions of TextWorld, they provided tokenization functions, but
    they were removed, so we’ll need to deal with text preprocessing ourselves.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TextWorld 的早期版本中，他们提供了分词功能，但这些功能已被移除，因此我们需要自己处理文本预处理。
- en: Let’s now take a look at additional information the game engine provides us.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下游戏引擎提供的额外信息。
- en: Extra game information
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外的游戏信息。
- en: 'Before we start planning our first training code, we need to discuss one additional
    functionality of TextWorld that we will use. As you might guess, even a simple
    problem might be too challenging for us:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始规划第一个训练代码之前，我们需要讨论一个我们将使用的TextWorld额外功能。正如你可能猜到的，甚至是一个简单的问题也可能对我们来说太具挑战性：
- en: Observations are text sequences of up to 200 tokens from the vocabulary of size
    1,250\. Actions could be up to eight tokens long. Generated games have five actions
    to be executed in the correct order. So, our chance of randomly finding the proper
    sequence of 8 × 5 = 40 tokens is something around ![1215040](img/eq48.png). This
    is not very promising, even with the fastest GPUs. Of course, we have start- and
    end-sequence tokens, which we can take into account to increase our chances; still,
    the probability of finding the correct sequence of actions with random exploration
    is tiny.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察是由最多200个标记组成的文本序列，这些标记来自大小为1250的词汇表。动作可以长达8个标记。生成的游戏有五个动作需要按正确的顺序执行。因此，我们随机找到正确的8
    × 5 = 40个标记的正确序列的机会大约是![1215040](img/eq48.png)。即便是最快的GPU，这个概率也不太乐观。当然，我们有开始和结束序列标记，我们可以考虑这些来提高我们的成功机会；不过，即使如此，随机探索找到正确的动作序列的概率依然很小。
- en: Another challenge is the partially observable Markov decision process (POMDP)
    nature of the environment, which comes from the fact that our inventory in the
    game is usually not shown. It is a normal practice in interactive fiction games
    to display the objects your character possesses only after some explicit command,
    like inventory. But our agent has no idea about the previous state. So, from its
    point of view, the situation after the command take apple is exactly the same
    as before (with the difference that the apple is no longer mentioned in the scene
    description). We can deal with that by stacking states, as we did in Atari games,
    but we need to do it explicitly, and the amount of information the agent needs
    to process will increase significantly.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个挑战是环境的部分可观察马尔可夫决策过程（POMDP）特性，这源于游戏中我们的物品栏通常不显示的事实。在互动小说游戏中，通常只有在某些明确的命令下（如查看物品栏）才会显示角色拥有的物品。但我们的智能体并不了解先前的状态。因此，从它的角度来看，在执行命令take
    apple之后，情况与之前完全相同（唯一的区别是苹果不再出现在场景描述中）。我们可以通过堆叠状态来处理这个问题，就像我们在Atari游戏中所做的那样，但我们需要明确地进行，并且智能体需要处理的信息量将显著增加。
- en: 'With all this being said, we should make some simplifications in the environment.
    Luckily, TextWorld provides us with convenient means for such workarounds. During
    the game registration, we can pass extra flags to enrich the observation space
    with extra pieces of more structured information. Here is the list of internals
    that we can peek into:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，我们应该对环境做一些简化。幸运的是，TextWorld为这种变通提供了方便的手段。在游戏注册过程中，我们可以传递额外的标志，将更多结构化的信息添加到观察空间中。以下是我们可以窥探的内部信息列表：
- en: A separate description of the current room, as it will be given by the look
    command
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前房间的独立描述，就像通过look命令获得的那样
- en: The current inventory
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前物品栏
- en: The name of the current location
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前地点的名称
- en: The facts of the current world state
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前世界状态的事实
- en: The last action and the last command performed
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上一条动作和执行的上一条命令
- en: The list of admissible commands in the current state
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态下可接受的命令列表
- en: The sequence of actions to execute to win the game
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赢得游戏所需执行的动作序列
- en: In addition, besides extra structured observations provided on every step, we
    can ask TextWorld to give us intermediate rewards every time we move in the right
    direction in the quest. As you might guess, this is extremely helpful for speeding
    up the convergence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了每一步提供的额外结构化观察之外，我们还可以要求TextWorld在我们朝正确方向移动时每次给我们提供中间奖励。正如你可能猜到的，这对于加速收敛非常有帮助。
- en: 'The most useful features in the additional information we can add are admissible
    commands, which enormously decrease our action space from 1250^(40) to just a
    dozen, and intermediate rewards, which guide the training in the right direction.
    To enable this extra information, we need to pass an optional argument to the
    register_game() method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加的附加信息中最有用的功能是可接受的命令，这极大地减少了我们的动作空间，从1250^(40)降到只有十几个，并且中间奖励可以引导训练朝正确的方向发展。为了启用这些额外信息，我们需要向register_game()方法传递一个可选的参数：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As you can see, the environment now provides us with extra information in the
    dictionary that was empty before. In this state, only three commands make sense
    (go east, inventory, and look). Let’s try the first one:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，现在环境中提供了之前为空的字典中的额外信息。在这种状态下，只有三个命令是有意义的（向东走、查看物品和观察）。让我们试试第一个命令：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The command was accepted, and we were given an intermediate reward of 1\. Okay,
    that’s great. Now we have everything needed to implement our first baseline DQN
    agent to solve TextWorld problems! But before that, we need to dive a bit into
    the natural language processing (NLP) world.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 命令已被接受，我们获得了 1 的中间奖励。好的，太棒了。现在我们已经具备了解决 TextWorld 问题所需的一切来实现我们的第一个基准 DQN 代理！但在此之前，我们需要稍微深入一下自然语言处理（NLP）领域。
- en: The deep NLP basics
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 NLP 基础
- en: In this short section, I’m going to walk you through deep NLP building blocks
    and standard approaches. This domain is evolving at enormous speed, especially
    now, as ChatGPT and LLMs have set a new standards in chatbots and text processing.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我将带你了解深度 NLP 的构建模块和标准方法。这个领域正在以惊人的速度发展，尤其是在现在，ChatGPT 和大语言模型（LLMs）已经在聊天机器人和文本处理方面设定了新标准。
- en: The material in this section just scratches the surface and covers the most
    common and standard building blocks. Some of them, like RNNs and LSTMs, might
    even look outdated — I still believe this is fine, as being aware of historical
    perspective is important. For simple tasks, you might consider using simple tools
    depending on what is most suitable for the task at hand, even if they are not
    hyped anymore.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容仅涉及表面，涵盖了最常见和标准的构建模块。其中一些，如 RNN 和 LSTM，可能看起来已经过时——但我依然认为这没关系，因为了解历史背景是很重要的。对于简单任务，你可以根据任务的需求选择最合适的工具，即使它们现在不再流行。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）
- en: NLP has its own specifics that make it different from computer vision or other
    domains. One such feature is processing variable-length objects. At various levels,
    NLP deals with objects that could have different lengths; for example, a word
    in a language could contain several characters. Sentences are formed from variable-length
    word sequences. Paragraphs or documents consist of varying numbers of sentences.
    Such variability is not NLP-specific and can arise in different domains, like
    in signal processing or video processing. Even standard computer vision problems
    could be seen as a sequence of some objects, like an image captioning problem
    when a neural network (NN) can focus on various amounts of regions of the same
    image to better describe the image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 有其独特性，使其与计算机视觉或其他领域不同。其中一个特点是处理可变长度的对象。在不同层次上，NLP 处理的对象可能具有不同的长度；例如，一种语言中的单词可能包含多个字符。句子是由可变长度的单词序列组成的。段落或文档由不同数量的句子构成。这种可变性并非
    NLP 特有，它也出现在不同的领域中，比如信号处理或视频处理。即便是标准的计算机视觉问题，也可以视为某种对象的序列，比如图像字幕问题，在这种问题中，神经网络（NN）可以集中关注同一图像的不同区域，以便更好地描述图像。
- en: RNNs provide one of the standard building blocks to deal with this variability.
    An RNN is a network with fixed input and output that is applied to a sequence
    of objects and can pass information along this sequence. This information is called
    the hidden state, and it is normally just a vector of numbers of some size.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 提供了应对这种可变性的标准构建模块之一。RNN 是一个具有固定输入和输出的网络，它被应用于一系列对象，并且能够在这个序列中传递信息。这个信息被称为隐藏状态，通常只是一个包含若干数字的向量。
- en: 'In the following diagram, we have an RNN with one input, which is a fixed-sized
    vector of numbers; the output is another vector. What makes it different from
    a standard feed-forward or convolutional NN is two extra gates: one input and
    one output. The extra input feeds the hidden state from the previous item into
    the RNN unit, and the extra output provides a transformed hidden state to the
    next sequence:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们有一个 RNN，其输入是一个固定大小的数字向量；输出是另一个向量。与标准的前馈神经网络（NN）或卷积神经网络（CNN）不同的是，这里有两个额外的门：一个输入门和一个输出门。额外的输入门将上一个项的隐藏状态传递到
    RNN 单元，而额外的输出门则将转换后的隐藏状态提供给下一个序列：
- en: '![PIC](img/file152.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file152.png)'
- en: 'Figure 13.3: The structure of an RNN building block'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：RNN 构建模块的结构
- en: 'As an RNN has two inputs, it can be applied to input sequences of any length,
    just by passing the hidden state produced by the previous entry to the next one.
    In Figure [13.4](#x1-227007r4), an RNN is applied to the sentence this is a cat,
    producing the output for every word in the sequence. During the application, we
    have the same RNN applied to every input item, but by having the hidden state,
    it can now pass information along the sequence:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RNN有两个输入，它可以应用于任何长度的输入序列，只需将前一个输入产生的隐藏状态传递给下一个输入即可。在图[13.4](#x1-227007r4)中，RNN被应用于句子"this
    is a cat"，并为序列中的每个单词生成输出。在应用过程中，我们对每个输入项都使用相同的RNN，但通过传递隐藏状态，它现在可以沿着序列传递信息：
- en: '![PIC](img/file153.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file153.png)'
- en: 'Figure 13.4: How an RNN is applied to a sentence'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：RNN如何应用于句子
- en: This is similar to convolutional NNs, when we have the same set of filters applied
    to various locations of the image, but the difference is that a convolutional
    NN can’t pass the hidden state.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于卷积神经网络（CNN），当我们对图像的不同位置应用相同的一组过滤器时，区别在于卷积神经网络无法传递隐藏状态。
- en: Despite the simplicity of this model, it adds an extra degree of freedom to
    the standard feed-forward NN model. The feed-forward NNs are determined by their
    input and always produce the same output for some fixed input (during the inference,
    of course, and not during the training). An RNN’s output depends not only on the
    input but also on the hidden state, which could be changed by the NN itself. So,
    the NN could pass some information from the beginning of the sequence to the end
    and produce a different output for the same input in different contexts. This
    context dependency is very important in NLP, as in natural language, a single
    word could have a completely different meaning in different contexts, and the
    meaning of a whole sentence could be changed by a single word.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个模型很简单，但它为标准的前馈神经网络模型增加了额外的自由度。前馈神经网络的输出由其输入决定，对于某个固定输入总是产生相同的输出（当然是在推理过程中，而非训练过程中）。RNN的输出不仅依赖于输入，还依赖于隐藏状态，而隐藏状态可能会被神经网络自身改变。因此，神经网络可以将序列开始时的信息传递到序列末尾，并根据不同的上下文为相同的输入生成不同的输出。这种上下文依赖性在NLP中非常重要，因为在自然语言中，单个词语在不同的上下文中可能有完全不同的含义，而整个句子的意义可能仅凭一个词的变化而发生变化。
- en: Of course, such flexibility comes with its own cost. RNNs usually require more
    time to train and can produce some weird behavior, like loss oscillations or sudden
    amnesia during the training. However, the research community has already done
    a lot of work and is still working hard to make RNNs more practical and stable,
    so RNNs and their modern alternatives like transformers can be seen as a standard
    building block of the systems that need to process variable-length input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种灵活性也有其成本。循环神经网络（RNN）通常需要更多的训练时间，并且可能会产生一些奇怪的行为，如训练过程中的损失波动或突然的遗忘。然而，研究界已经做了大量工作，并且仍在努力使RNN变得更加实用和稳定，因此RNN及其现代替代品，如变换器（transformers），可以被视为需要处理变长输入的系统的标准构建模块。
- en: In our example, we’ll use the evolution of RNNs, called the Long Short-Term
    Memory (LSTM) model, which was first proposed in 1995 by Sepp Hochreiter and Jürgen
    Schmidhuber in the paper LSTM can solve hard long time lag problems, and then
    published in 1996 at a Neural Information Processing Systems (NIPS) conference
    [[HS96](#)]. This model is very similar to the RNN we just discussed, but has
    more complicated internal structure to address some RNN problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们将使用RNN的演化版本——长短期记忆（LSTM）模型，该模型最早由Sepp Hochreiter和Jürgen Schmidhuber于1995年在论文《LSTM可以解决长期延迟问题》中提出，并于1996年在神经信息处理系统（NIPS）会议上发布[[HS96](#)]。这个模型与我们刚刚讨论的RNN非常相似，但它具有更复杂的内部结构，以解决一些RNN的问题。
- en: Word embedding
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Another standard building block of modern DL-driven NLP is word embeddings,
    which is also called word2vec by one of the most popular training methods for
    simple tasks. The idea comes from the problem of representing our language sequences
    in NNs. Normally, NNs work with fixed-sized vectors of numbers, but in NLP, we
    normally have words or characters as input to the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度学习驱动的自然语言处理（NLP）另一个标准构建模块是词嵌入，也称为word2vec，这是一种最流行的训练方法，用于处理简单任务。这个想法源于在神经网络中表示语言序列的问题。通常，神经网络处理的是固定大小的数字向量，但在NLP中，我们通常使用单词或字符作为模型的输入。
- en: While older methods like word2vec are commonly used for more simple tasks and
    remain very relevant in the field, other methods such as BERT and transformers
    are widely used for more complex tasks. We’ll briefly discuss transformers later
    in this chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像 word2vec 这样的旧方法通常用于较简单的任务，并且在该领域仍然非常相关，但其他方法，如 BERT 和变换器，广泛应用于更复杂的任务。我们将在本章后面简要讨论变换器。
- en: 'One possible solution might be one-hot encoding our dictionary, which is when
    every word has its own position in the input vector and we set this number to
    1 when we encounter this word in the input sequence. This is a standard approach
    for NNs when you have to deal with some relatively small discrete set of items
    and want to represent them in an NN-friendly way. Unfortunately, one-hot encoding
    doesn’t work very well for several reasons:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可能的解决方案是对我们的词典进行 one-hot 编码，也就是每个单词在输入向量中都有自己独立的位置，当我们在输入序列中遇到这个单词时，将该位置设置为
    1。这是神经网络处理一些相对较小的离散项集并希望以神经网络友好的方式表示它们时的标准方法。不幸的是，one-hot 编码由于几个原因并不十分有效：
- en: Our input set is usually not small. If we want to encode only the most commonly
    used English dictionary, it will contain at least several thousand words. The
    Oxford English Dictionary has 170,000 commonly used words and 50,000 obsolete
    and rare words. This is only established vocabulary and doesn’t count slang, new
    words, scientific terms, abbreviations, typos, jokes, Twitter/X memes, and so
    on. And this is only for the English language!
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的输入集通常不小。如果我们只想对最常用的英语词典进行编码，它至少包含几千个单词。牛津英语词典有 170,000 个常用词和 50,000 个过时或稀有词汇。这仅仅是已建立的词汇表，还不包括俚语、新词、科学术语、缩写、拼写错误、笑话、Twitter/X
    的梗等等。而且这只是针对英语语言！
- en: The second problem related to the one-hot representation of words is the uneven
    frequency of vocabulary. There are relatively small sets of very frequent words,
    like a and cat, but a very large set of much more rarely used words, like covfefe
    or bibliopole, and those rare words can occur only once or twice in a very large
    text corpus. So, our one-hot representation is very inefficient in terms of space.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与单词的 one-hot 表示相关的第二个问题是词汇表频率的不均衡。有一些非常频繁的单词集，像 "a" 和 "cat"，但也有大量不常用的单词，像 "covfefe"
    或 "bibliopole"，这些罕见的单词可能在一个非常大的文本语料库中只出现一两次。因此，我们的 one-hot 表示在空间方面非常低效。
- en: Another issue with simple one-hot representation is not capturing a word’s relations.
    For example, some words are synonyms and have the same meaning, but they will
    be represented by different vectors. Some words are used very frequently together,
    like United Nations or fair trade, and this fact is also not captured in one-hot
    representation.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的 one-hot 表示的另一个问题是无法捕捉到单词之间的关系。例如，一些单词是同义词，具有相同的含义，但它们会被不同的向量表示。一些单词常常一起使用，如
    "United Nations" 或 "fair trade"，而这一事实也未能在 one-hot 表示中体现出来。
- en: To overcome all this, we can use word embeddings, which map every word in some
    vocabulary into a dense, fixed-length vector of numbers. These numbers are not
    random but trained on a large corpus of text to capture the context of words.
    A detailed description of word embeddings is beyond the scope of this book, but
    this is a really powerful and widely used NLP technique to represent words, characters,
    and other objects in some sequence. For now, you can think about them as just
    mapping words into number vectors, and this mapping is convenient for the NN to
    be able to distinguish words from each other. To obtain this mapping, two methods
    exist. First, you can download pretrained vectors for the language that you need.
    There are several sources of embeddings available; just search on Google for “GloVe
    pretrained vectors” or “word2vec pretrained” (GloVe and word2vec are different
    methods used to train such vectors, which produce similar results). An alternate
    way to obtain embeddings is to train them on your own dataset. To do this, you
    can either use special tools, such as fastText ([https://fasttext.cc/](https://fasttext.cc/),
    an open source utility from Facebook), or just initialize embeddings randomly
    and allow your model to adjust them during normal training.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一切，我们可以使用词嵌入，它将某个词汇表中的每个单词映射为一个密集的、固定长度的数字向量。这些数字不是随机的，而是通过大规模文本语料库训练得到的，以捕捉单词的上下文。词嵌入的详细描述超出了本书的范围，但这是一种非常强大且广泛使用的自然语言处理（NLP）技术，用于表示单词、字符和其他序列中的对象。现在，你可以将它们理解为只是将单词映射为数字向量，而这种映射对于神经网络（NN）能够区分单词之间的差异非常方便。为了获得这种映射，有两种方法。首先，你可以下载所需语言的预训练向量。有多个可用的嵌入源，只需在Google上搜索“GloVe预训练向量”或“word2vec预训练”（GloVe和word2vec是训练这些向量的不同方法，产生相似的结果）。获取嵌入的另一种方法是自行在你的数据集上进行训练。为此，你可以使用特殊工具，如fastText（[https://fasttext.cc/](https://fasttext.cc/)，Facebook的开源工具），或直接随机初始化嵌入并允许模型在正常训练过程中调整它们。
- en: In addition, LLMs (and, in general, any sequence-to-sequence architectures)
    can produce very high-quality embeddings of texts. The OpenAI ChatGPT API has
    a special request that converts any piece of text into an embedding vector.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs（以及一般的任何序列到序列架构）可以生成非常高质量的文本嵌入。OpenAI的ChatGPT API有一个特殊请求，可以将任何文本转换为嵌入向量。
- en: The Encoder-Decoder architecture
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器架构
- en: 'Another model that is widely used in NLP is called Encoder-Decoder, or seq2seq.
    It originally comes from machine translation, when your system needs to accept
    a sequence of words in the source language and produce another sequence in the
    target language. The idea behind seq2seq is to use an RNN to process an input
    sequence and encode this sequence into some fixed-length representation. This
    RNN is called an encoder. Then you feed the encoded vector into another RNN, called
    a decoder, which has to produce the resulting sequence in the target language.
    An example of this idea is shown next, where we are translating an English sentence
    into Russian:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在NLP中广泛使用的模型是编码器-解码器模型，或称seq2seq。它最初来自机器翻译，当你的系统需要接受源语言中的一系列单词，并在目标语言中生成另一系列单词时使用。seq2seq的基本思想是使用一个RNN来处理输入序列，并将该序列编码成某个固定长度的表示。这种RNN被称为编码器。然后，你将编码后的向量输入到另一个RNN中，称为解码器，解码器必须生成目标语言中的结果序列。下面是这个思想的一个示例，我们正在将英文句子翻译成俄语：
- en: '![PIC](img/file154.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file154.png)'
- en: 'Figure 13.5: The Encoder-Decoder architecture in machine translation'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：机器翻译中的编码器-解码器架构
- en: This model (with a lot of modern tweaks and extensions) is still a major workhorse
    of machine translation, but is general enough to be applicable to a much wider
    set of domains, for example, audio processing, image annotation, and video captioning.
    In our TextWorld example, we’ll use it to generate embeddings of variable-sized
    observations from the environment.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型（通过大量现代化的调整和扩展）仍然是机器翻译的主要工作马，但它足够通用，可以应用于更广泛的领域，例如音频处理、图像标注和视频字幕生成。在我们的TextWorld示例中，我们将使用它来生成来自环境的变大小观察的嵌入。
- en: RNNs continue to be very effective in certain contexts, but in recent years,
    NLP has seen significant advancements with the introduction of the more complex
    Transformer models. Let’s take a look at Transformer architecture next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在某些上下文中依然非常有效，但近年来，随着更复杂的Transformer模型的引入，NLP领域发生了重大进展。接下来，我们将看看Transformer架构。
- en: Transformers
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer模型
- en: 'Transformers is an architecture proposed in 2017 in the paper Attention is
    all you need by Vaswani et al. from Google [[Vas17](#)]. At a high-level, it uses
    the same encoder-decoder architecture we just discussed, but adds several improvements
    to the underlying building blocks, which turned out to be very important for addressing
    existing RNN problems:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是在 2017 年由谷歌的 Vaswani 等人提出的架构，发表于论文《Attention is all you need》[[Vas17](#)]。从高层次看，它使用了我们刚刚讨论的相同的编码器-解码器架构，但对底层构建模块进行了若干改进，这些改进对于解决现有
    RNN 问题至关重要：
- en: 'Positional encoding: This injects information about the input and output sequences’
    positions into embeddings'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置信息编码：它将关于输入和输出序列位置的信息注入到嵌入中。
- en: 'Attention mechanism: This concept was proposed in 2015 and could be seen as
    a trainable way for systems to focus on specific parts of input sequences. In
    transformers, attention was heavily used (which you can guess from the paper’s
    title)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制：这个概念是在 2015 年提出的，可以看作是一种可训练的方式，使得系统能够集中注意力于输入序列的特定部分。在 transformer 中，注意力机制得到了广泛应用（从论文标题中可以猜到这一点）。
- en: 'Nowadays, transformers are at the core of almost every NLP and DL system, including
    LLMs. I’m not going to go deep into this architecture, as there are lots of resources
    available about this topic, but if you’re curious, you can check the following
    article: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，transformer 是几乎所有自然语言处理（NLP）和深度学习（DL）系统的核心，包括大型语言模型（LLMs）。我不会深入探讨这一架构，因为关于这个话题有很多资源，但如果你感兴趣，可以查看以下文章：[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)。
- en: Now we have everything needed to implement our first baseline DQN agent to solve
    TextWorld problems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了实现第一个基线 DQN 智能体来解决 TextWorld 问题所需的一切。
- en: Baseline DQN
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基线 DQN
- en: 'Getting back to our TextWorld environment, the following are the major challenges:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的 TextWorld 环境，以下是主要的挑战：
- en: Text sequences might be problematic on their own, as we discussed earlier in
    this chapter. The variability of sequence lengths might cause vanishing and exploding
    gradients in RNNs, slow training, and convergence issues. In addition to that,
    our TextWorld environment provides us with several such sequences that we need
    to handle separately. Our scene description string, for example, might have a
    completely different meaning to the agent than the inventory string, which describes
    our possessions.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本序列本身可能会成为问题，正如我们在本章前面讨论的那样。序列长度的可变性可能导致 RNN 中的梯度消失和爆炸、训练缓慢以及收敛问题。除此之外，我们的
    TextWorld 环境提供了几个需要单独处理的此类序列。例如，我们的场景描述字符串对智能体可能有完全不同的含义，而物品栏字符串则描述我们的物品。
- en: Another obstacle is the action space. As you have seen in the previous section,
    TextWorld might provide us with a list of commands that we can execute in every
    state. It significantly reduces the action space we need to choose from, but there
    are other complications. One of them is that the list of admissible commands changes
    from state to state (as different locations might allow different commands to
    be executed). Another issue is that every entry in the admissible commands list
    is a sequence of words.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个障碍是动作空间。正如你在上一节中看到的，TextWorld 可能会为我们提供在每个状态下可以执行的命令列表。它显著减少了我们需要从中选择的动作空间，但也存在其他复杂性。其中之一是可接受命令列表会根据状态的不同而变化（不同位置可能允许执行不同的命令）。另一个问题是，可接受命令列表中的每一项都是一个由单词组成的序列。
- en: Potentially, we might get rid of both of those variabilities by building a dictionary
    of all possible commands and using it as a discrete, fixed-size action space.
    In simple games, this might work, as the number of locations and objects is not
    that large. You can try this as an exercise, but we will follow a different path.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有可能通过构建一个包含所有可能命令的字典，并将其作为一个离散的、固定大小的动作空间，从而消除这两种变动性。在简单的游戏中，这可能有效，因为位置和物体的数量并不大。你可以尝试这个方法作为练习，但我们将沿着不同的路径前进。
- en: 'Thus far, you have seen only discrete action spaces having a small number of
    predefined actions, and this influenced the architecture of the DQN: the output
    from the network predicted Q-values for all actions in one pass, which was convenient
    both during the training and model application (as we need all Q-values for all
    actions to find argmax anyway). But this choice of DQN architecture is not something
    dictated by the method, so if needed, we can tweak it. And our issue with a variable
    number of actions might be solved this way. To get a better understanding of how,
    let’s check the architecture of our TextWorld baseline DQN, as shown in the following
    figure:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只见过离散的动作空间，其中包含少量预定义的动作，这影响了DQN的架构：网络的输出通过一次传递预测所有动作的Q值，这在训练和模型应用过程中都非常方便（因为我们无论如何都需要所有动作的Q值来找到最大Q值）。但这种DQN架构的选择并不是方法决定的，因此如果需要，我们可以进行调整。我们关于动作数量可变的问题可能通过这种方式得到解决。为了更好地理解这种方式，让我们来看一下TextWorld基线DQN的架构，如下图所示：
- en: '![PIC](img/file155.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file155.png)'
- en: 'Figure 13.6: The architecture of the TextWorld baseline DQN'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：TextWorld基线DQN的架构
- en: The major part of the diagram is occupied by preprocessing blocks. On input
    to the network (blocks on the left), we get variable sequences of individual parts
    of observations (“Raw text”, “Description”, and “Inventory”) and the sequence
    of one action command to be evaluated. This command will be taken from the admissible
    commands list, and the goal of our network will be to predict a single Q-value
    for the current game state and this particular command. This approach is different
    from the DQNs we have used before, but as we don’t know in advance which commands
    will be evaluated in every state, we will evaluate every command individually.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的主要部分由预处理模块占据。在网络的输入端（图左边的模块），我们得到的是由各个观察部分（“原始文本”、“描述”和“库存”）组成的可变序列，以及一个需要评估的动作命令序列。这个命令将从可接受命令列表中选取，网络的目标是为当前游戏状态和该特定命令预测一个Q值。这个方法与我们之前使用的DQN有所不同，但由于我们无法预先知道每个状态下将评估哪些命令，因此我们将单独评估每个命令。
- en: Those four input sequences (which are lists of token IDs in our vocabulary)
    will be passed through an embeddings layer and then fed into separate LSTM RNNs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个输入序列（即我们词汇表中词元ID的列表）将通过嵌入层，然后输入到不同的LSTM RNN中。
- en: The goal of LSTM networks (which are called “Encoders” in the figure, since
    LSTMs are concrete implementations of encoders) is to convert variable-length
    sequences into fixed-size vectors. Every input piece is processed by its own LSTM
    with separated weights, which will allow the network to capture different data
    from different input sequences. Later in this chapter, we’ll replace LSTMs with
    pretrained transformers from the Hugging Face Hub to check the effect of using
    a much smarter and larger model on the same problem.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络（在图中称为“编码器”，因为LSTM是编码器的具体实现）的目标是将可变长度的序列转换为固定大小的向量。每个输入部分都会由其自己的LSTM进行处理，使用独立的权重，这使得网络能够捕捉来自不同输入序列的不同数据。在本章后面，我们将用来自Hugging
    Face Hub的预训练变换器替换LSTM，来验证在相同问题上使用一个更智能、更大模型的效果。
- en: 'The output from the encoders is concatenated into one single vector and passed
    to the main DQN network. As our variable-length sequences have been transformed
    into fixed-size vectors, the DQN network is simple: just several feed-forward
    layers producing one single Q-value. This is less efficient computationally, but
    for the baseline, it is fine.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出会被拼接成一个单一的向量，并传递给主DQN网络。由于我们的可变长度序列已被转换为固定大小的向量，DQN网络很简单：只有几层前馈层，输出一个Q值。这在计算上效率较低，但作为基准来说是可以接受的。
- en: 'The complete source code is in the Chapter13 directory and it includes the
    following modules:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的源代码位于Chapter13目录中，包含以下模块：
- en: 'train_basic.py: A baseline training program'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: train_basic.py：一个基线训练程序
- en: 'lib/common.py: Common utilities to set up the Ignite engine and hyperparameters'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/common.py：设置Ignite引擎和超参数的公共工具
- en: 'lib/preproc.py: The preprocessing pipeline, including embeddings and encoder
    classes'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/preproc.py：包括嵌入和编码器类的预处理管道
- en: 'lib/model.py: The DQN model and DQN agent with helper functions'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: lib/model.py：包含帮助函数的DQN模型和DQN代理
- en: We won’t be presenting the full source code in the chapter. Instead, we will
    be explaining only the most important or tricky parts in the subsequent sections.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会展示完整的源代码。相反，我们将在后续的部分仅解释最重要或最棘手的部分。
- en: Observation preprocessing
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察预处理
- en: 'Let’s start with the leftmost part of our pipeline (Figure [13.6](#x1-231002r6)).
    On the input, we’re going to get several lists of tokens, both for the individual
    state observation and for our command that we’re going to evaluate. But as you
    have already seen, the TextWorld environment produces the string and a dict with
    the extended information, so we need to tokenize the strings and get rid of non-relevant
    information. That’s the responsibility of the TextWorldPreproc class, which is
    defined in the lib/preproc.py module:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从管道的最左侧部分开始（图 [13.6](#x1-231002r6)）。在输入端，我们将获得多个令牌列表，分别用于单独的状态观察和我们即将评估的命令。正如你已经看到的，TextWorld
    环境生成的是字符串和包含扩展信息的字典，所以我们需要对字符串进行分词并去除不相关的信息。这是 `TextWorldPreproc` 类的职责，该类定义在 `lib/preproc.py`
    模块中：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The class implements the gym.Wrapper interface, so it will transform the TextWorld
    environment observations and actions in the way we need. The constructor accepts
    several flags, which simplifies future experiments. For example, you can disable
    the usage of admissible commands or intermediate rewards, set the limit of tokens,
    or change the set of observation fields to be processed.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该类实现了 `gym.Wrapper` 接口，因此它将按照我们需要的方式转换 TextWorld 环境中的观察和动作。构造函数接受多个标志，这简化了未来的实验。例如，您可以禁用使用可接受命令或中间奖励、设置令牌的限制或更改要处理的观察字段集。
- en: 'Next, the num_fields property returns the count of observation sequences, which
    is used to get the idea of the encoded observation’s shape:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`num_fields` 属性返回观察序列的数量，用于了解编码后的观察形状：
- en: '[PRE6]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The _maybe_tokenize() method performs tokenization of input string. If no vocabulary
    is given, the string is returned unchanged. We will use this functionality in
    the transformer version, as Hugging Face libraries are performing their own tokenization.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`_maybe_tokenize()` 方法执行输入字符串的分词处理。如果没有提供词汇表，则字符串会原样返回。我们将在 transformer 版本中使用此功能，因为
    Hugging Face 库会执行它们自己的分词处理。'
- en: 'The _encode() method is the heart of the observation transformation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`_encode()` 方法是观察数据转换的核心：'
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding method takes the observation string and the extended information
    dictionary and returns a single dictionary with the following keys:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法接受观察字符串和扩展信息字典，并返回一个包含以下键的单一字典：
- en: 'obs: The list of lists with the token IDs of input sequences.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`obs`：包含输入序列的令牌 ID 列表的列表。'
- en: 'admissible_commands: A list with commands available from the current state.
    Every command is tokenized and converted into the list of token IDs.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`admissible_commands`：当前状态下可用命令的列表。每个命令都会被分词并转换为令牌 ID 列表。'
- en: In addition, the method remembers the extra information dictionary and raw admissible
    commands list. This is not needed for training, but will be useful during the
    model application, to be able to get back the command text from the index of the
    command.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该方法会记住额外的信息字典和原始的可接受命令列表。这对于训练来说不是必须的，但在模型应用期间非常有用，能够根据命令的索引获取回命令文本。
- en: 'With the _encode() method defined, implementation of the reset() and step()
    methods is simple — we’re encoding observations and handling intermediate rewards
    (if they are enabled):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了 `_encode()` 方法后，`reset()` 和 `step()` 方法的实现就很简单了——我们在编码观察数据并处理中间奖励（如果启用）：
- en: '[PRE8]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s worth noting that the step() method is expecting 4 items to be returned
    from the wrapped environment, but returns 5 elements. This hides the TextWorld
    environment incompatibility with the modern Gym interface we’ve already discussed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`step()` 方法期望从封装的环境中返回 4 项，但实际上返回了 5 项。这隐藏了我们之前讨论的 TextWorld 环境与现代 Gym
    接口的不兼容问题。
- en: 'Finally, there are two properties that give access to the remembered state:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，有两个属性可以访问记住的状态：
- en: '[PRE9]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To illustrate how the preceding class is supposed to be applied and what it
    does with the observation, let’s check the following small interactive session.
    Here, we register the game, asking for inventory, intermediate reward, admissible
    commands, and scene description:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明前述类的应用方式及其如何处理观察内容，让我们来看一下下面的小型交互示例。在这里，我们注册了游戏，并请求库存、中间奖励、可接受的命令和场景描述：
- en: '[PRE10]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, that’s our raw observation obtained from the TextWorld environment. Now
    let’s extract the game vocabulary and apply our preprocessor:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是我们从 TextWorld 环境中获得的原始观察数据。现在让我们提取游戏词汇并应用我们的预处理器：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s try to execute an action. The 0th action corresponds to the first entry
    in the admissible commands list, which is “drop sponge” in our case:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试执行一个动作。第0个动作对应于可接受命令列表中的第一个条目，在我们的例子中是“丢掉海绵”：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, we no longer have the sponge, but it wasn’t the right action
    to take, thus an intermediate reward was not given.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们不再有海绵，但这并不是正确的动作，因此没有给予中间奖励。
- en: Okay, this representation still can’t be fed directly into NNs, but it is much
    closer to what we want.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个表示法仍然不能直接输入到神经网络中，但它已经比之前更接近我们想要的形式了。
- en: Embeddings and encoders
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入和编码器
- en: 'The next step in the preprocessing pipeline is implemented in two classes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理管道中的下一步由两个类实现：
- en: 'Encoder: A wrapper around the LSTM unit that transforms one single sequence
    (after embeddings have been applied) into a fixed-size vector'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Encoder：一个LSTM单元的包装器，将一个单一序列（在应用了嵌入后）转换成一个固定大小的向量
- en: 'Preprocessor: This class is responsible for the application of embeddings and
    the transformation of individual sequences with corresponding encoder classes'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理器：该类负责应用嵌入和使用相应的编码器类转换单个序列
- en: 'The Encoder class is simpler, so let’s start with it:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Encoder类比较简单，所以我们先从它开始：
- en: '[PRE13]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The logic is that: we apply the LSTM layer and return its hidden state after
    processing the sequence.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑是：我们应用LSTM层，并在处理完序列后返回其隐藏状态。
- en: 'The Preprocessor class is a bit more complicated, as it combines several Encoder
    instances and is also responsible for embeddings:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Preprocessor类要复杂一些，因为它结合了多个Encoder实例，并且也负责嵌入：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the constructor, we create an embeddings layer, which will map every token
    in our dictionary into a fixed-size dense vector. Then we create num_sequences
    instances of Encoder for every input sequence and one additional instance to encode
    command tokens.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们创建了一个嵌入层，该层将我们词典中的每个标记映射到一个固定大小的稠密向量。然后，我们为每个输入序列创建了`num_sequences`个Encoder实例，并创建了一个额外的实例来编码命令标记。
- en: 'The internal method _apply_encoder() takes the batch of sequences (every sequence
    is a list of token IDs) and transforms it with an encoder:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 内部方法`_apply_encoder()`接受一批序列（每个序列是一个标记ID的列表）并使用编码器进行转换：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In earlier versions of PyTorch, we needed to sort a batch of variable-length
    sequences before RNN application. Since PyTorch 1.0, this is no longer needed,
    as this sorting and transformation is handled by the PackedSequence class internally.
    To enable this functionality, we need to pass the enforce_sorted=False parameter.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期版本的PyTorch中，我们需要在应用RNN之前对可变长度的序列进行排序。从PyTorch 1.0版本开始，这已经不再需要，因为排序和转换由PackedSequence类内部处理。为了启用此功能，我们需要传递`enforce_sorted=False`参数。
- en: 'The encode_observations() method takes a batch of observations (from TextWorldPreproc)
    and encodes them into a tensor:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_observations()`方法接受一批观察结果（来自TextWorldPreproc）并将其编码成一个张量：'
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Besides variable sequences, we can pass extra “flags” fields directly into the
    encoded tensor. This functionality will be used in later experiments and extensions
    to the basic method.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可变序列外，我们还可以将额外的“标志”字段直接传递到编码后的张量中。这个功能将在后续的实验和对基本方法的扩展中使用。
- en: 'Finally, two methods, encode_sequences() and encode_commands(), are used to
    apply different encoders to the batch of variable-length sequences:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`encode_sequences()`和`encode_commands()`两个方法被用来将不同的编码器应用于可变长度序列的批处理：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The DQN model and the agent
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN模型和代理
- en: 'With all those preparations made, let’s look at the brains of our agent: the
    DQN model. It should accept vectors of num_sequences ×encoder_size and produce
    a single scalar value. But there is one difference from the other DQN models covered,
    which is in the way we apply the model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在做了所有这些准备工作之后，让我们看看我们代理的“大脑”：DQN模型。它应该接受`num_sequences × encoder_size`的向量并产生一个单一的标量值。但是与其他DQN模型的不同之处在于，我们应用模型的方式：
- en: '[PRE18]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, the forward() method accepts two batches — observations
    and commands — producing the batch of Q-values for every pair. Another method,
    q_values(), takes one observation produced by the Preprocessor class and the tensor
    of encoded commands, then applies the model and returns a list of Q-values for
    every command.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，`forward()`方法接受两个批次——观察结果和命令——为每一对生成Q值批次。另一个方法`q_values()`接受由预处理器类生成的一个观察结果和编码后的命令张量，随后应用模型并返回每个命令的Q值列表。
- en: In the model.py module, we have the DQNAgent class, which takes the preprocessor
    and implements the PTAN Agent interface to hide the details of observation preprocessing
    on decision-making.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 model.py 模块中，我们有 DQNAgent 类，它接收预处理器并实现 PTAN Agent 接口，以隐藏决策过程中的观察预处理细节。
- en: Training code
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练代码
- en: With all the preparations and preprocessing in place, the rest of the code is
    almost the same as we already implemented in previous chapters, so I won’t repeat
    the training code; I will just describe the training logic.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有准备和预处理工作完成后，其余的代码与我们在之前章节中实现的几乎相同，因此我不会重复训练代码；我只会描述训练逻辑。
- en: 'To train the model, the Chapter13/train_basic.py utility has to be used. It
    allows several command-line arguments to change the training behavior:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，必须使用 Chapter13/train_basic.py 工具。它允许通过几个命令行参数来改变训练行为：
- en: '-g or --game: This is the prefix of the game files in the games directory.
    The provided script generates several games named simpleNN.ulx, where NN is the
    game seed.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -g 或 --game：这是游戏文件的前缀，位于 games 目录中。提供的脚本会生成几个名为 simpleNN.ulx 的游戏，其中 NN 是游戏的种子值。
- en: '-s or --suffices: This is the count of games to be used during the training.
    If you specify 1 (which is the default), the training will be performed only on
    the file simple1.ulx. If option -s 10 is given, 10 games with indices 1…10 will
    be registered and used for training. This option is used to increase the variability
    in the training games, as our goal is not just to learn how to play concrete games
    but also (hopefully) to learn how to behave in other similar games.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -s 或 --suffices：这是训练期间使用的游戏数量。如果指定 1（默认值），则训练只会在文件 simple1.ulx 上进行。如果指定 -s 10，则会注册并使用编号从
    1 到 10 的 10 个游戏进行训练。此选项用于增加训练游戏的多样性，因为我们的目标不仅是学习如何玩具体的游戏，还希望（如果可能的话）学习如何在其他类似的游戏中表现。
- en: '-v or --validation: This is the suffix of the game to be used for validation.
    It equals to -val by default and defines the game file that will be used to check
    the generalization of our trained agent.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -v 或 --validation：这是用于验证的游戏的后缀。默认值为 -val，并定义将用于检查我们训练智能体泛化能力的游戏文件。
- en: '--params: This means the hyperparameters to be used. Two sets are defined in
    lib/common.py: small and medium. The first one has a small number of embeddings
    and encoder vectors, which is great for solving a couple of games quickly; however,
    this set struggles with converging when many games are used for training.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --params：这表示将使用的超参数。lib/common.py 中定义了两组：small 和 medium。第一组具有较少的嵌入和编码向量，非常适合快速解决少数几个游戏；然而，当使用许多游戏进行训练时，这一组会遇到收敛困难的问题。
- en: '--dev: This option specifies the device name for computations.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --dev：该选项指定计算使用的设备名称。
- en: '-r or --run: This is the name of the run and is used in the name of the save
    directory and TensorBoard.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -r 或 --run：这是运行的名称，通常用作保存目录和 TensorBoard 的名称。
- en: During the training, validation is performed every 100 training iterations and
    the validation game is run on the current network. The reward and the number of
    steps are recorded in TensorBoard and help us to understand the generalization
    capabilities of our agent. Generalization in RL is known to be a large issue,
    as with a limited set of trajectories, the training process has a tendency to
    overfit to some states, which doesn’t guarantee good behavior on unseen games.
    In comparison to Atari games, where the gameplay normally doesn’t change much,
    the variability of interactive fiction games might be high, due to different quests,
    objects, and the way they communicate. So, it’s an interesting experiment to check
    how our agent is able to generalize between games.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每进行100次训练迭代会执行一次验证，并在当前网络上运行验证游戏。奖励和步骤数会记录在 TensorBoard 中，帮助我们理解智能体的泛化能力。强化学习中的泛化一直是一个大问题，因为在有限的轨迹集上，训练过程有过拟合到某些状态的倾向，这并不能保证在未见过的游戏中表现良好。与
    Atari 游戏相比，后者的游戏玩法通常变化不大，互动小说游戏的可变性可能更高，因为它们有不同的任务、物品以及沟通方式。因此，检查我们的智能体能在不同游戏之间如何泛化是一个有趣的实验。
- en: Training results
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练结果
- en: 'By default, the script games/make_games.sh generates 20 games with names from
    simple1.ulx to simple20.ulx, plus a game for validation: simple-val.ulx.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，脚本 games/make_games.sh 会生成20个游戏，名称从 simple1.ulx 到 simple20.ulx，还有一个用于验证的游戏：simple-val.ulx。
- en: 'To begin, let’s train the agent on one game, using the small hyperparameters
    set:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们在一个游戏上训练智能体，使用较小的超参数设置：
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Option -s specifies the number of game indices that will be used for training.
    In this case, only one will be used. The training stops when the average number
    of steps in the game drops below 15, which means the agent has found the proper
    sequence of steps and can reach the end of the game in an efficient way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 选项 -s 指定将用于训练的游戏索引数量。在这种情况下，只会使用一个。当游戏中的平均步数降到 15 以下时，训练就会停止，这意味着智能体已经找到了正确的步骤序列，并能够高效地完成游戏。
- en: 'In the case of one game, it takes just 3 minutes and about 120 episodes to
    solve the game. The following figure shows the reward and number of steps dynamics
    during the training:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单场游戏，解决游戏只需要 3 分钟和大约 120 回合。下图展示了训练过程中的奖励和步数变化：
- en: '![PIC](img/B22150_13_07.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_13_07.png)'
- en: 'Figure 13.7: Training reward (left) and count of steps in episodes (right)
    for training on one game'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：在一场游戏中训练的奖励（左）和回合步数（右）
- en: But if we check the validation reward (which is a reward obtained on the game
    simple-val.ulx), we see zero improvement over time. In my case, validation reward
    was zero and count of steps on validation episodes were 50 (which is a default
    time limit). It just means that the learned agent wasn’t able to generalize.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们检查验证奖励（即在游戏 simple-val.ulx 上获得的奖励），我们会看到它随着时间的推移没有任何改善。在我的情况下，验证奖励是零，验证回合的步数是
    50（这是默认的时间限制）。这仅仅意味着学到的智能体没有办法进行泛化。
- en: 'If we try to increase the number of games used for the training, the convergence
    will require more time, as the network needs to discover more sequences of actions
    in different states. The following are the same charts for reward and steps for
    20 games (with option -s 20 passed):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试增加用于训练的游戏数量，收敛将需要更多的时间，因为网络需要在不同的状态中发现更多的动作序列。以下是 20 场游戏（传递选项 -s 20）的奖励和步数图表：
- en: '![PIC](img/B22150_13_08.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_13_08.png)'
- en: 'Figure 13.8: Training reward (left) and count of steps in episodes (right)
    for training on 20 games'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8：在 20 场游戏中训练的奖励（左）和回合步数（右）
- en: As you can see, it takes almost two hours to converge, but still, our small
    hyperparameter set is able to improve the performance on 20 games played during
    the training.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，收敛大约需要两个小时，但我们的这个小型超参数集依然能够提升在训练过程中进行的 20 场游戏中的表现。
- en: Validation metrics, as shown in the following figure, are now slightly more
    interesting — at the end of the training, the agent was able to obtain the score
    of 2 (with maximum 6) and somewhere in the middle of the training, it got 4\.
    But count of steps on validation game are still 50, which means that the agent
    just walks around semi-randomly executing some actions. Not very impressive.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，验证指标现在稍微有些有趣 —— 在训练结束时，智能体能够获得 2 分（最大为 6 分），并且在训练的中间阶段，它获得了 4 分。但验证游戏中的步数仍然是
    50，这意味着智能体只是半随机地四处走动并执行一些动作。表现并不令人印象深刻。
- en: '![PIC](img/B22150_13_09.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_13_09.png)'
- en: 'Figure 13.9: Validation reward during the training on 20 games'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9：在 20 场游戏中训练时的验证奖励
- en: I haven’t tried different hyperparameters on this agent (you can do this with
    -s medium).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有在这个智能体上尝试不同的超参数（你可以通过 -s medium 来做到这一点）。
- en: Tweaking observations
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整观察值
- en: Our first series of attempts will be in feeding more information to the agent.
    Here, I will just briefly introduce the changes made and effect they had on a
    training result. You can find the full example in Chapter13/train_preproc.py.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一次尝试将是在给智能体提供更多的信息。在这里，我将简要介绍所做的更改以及它们对训练结果的影响。你可以在 Chapter13/train_preproc.py
    中找到完整的示例。
- en: Tracking visited rooms
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪访问过的房间
- en: First, you will notice that our agent has no idea whether the current room was
    already visited or not. In situations when the agent already knows the optimal
    way to the goal, it might be not needed (as generated games always have different
    rooms). But if the policy is not perfect, it might be useful to have a clear indication
    that we’re visiting the same room over and over again.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你会注意到我们的智能体无法判断当前房间是否已经访问过。当智能体已经知道通往目标的最佳路径时，可能不需要这个信息（因为生成的游戏总是有不同的房间）。但如果策略不完美，可能会有用，能明确指示我们是否在一遍遍地访问同一个房间。
- en: To feed this knowledge into the observation, I implemented a simple room tracking
    in the preproc.LocationWrapper class, which tracks visited rooms over the episode.
    Then this flag is concatenated to the agent’s observation as a single 1 if the
    room was visited before or 0 if it is a new location.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些知识输入到观察中，我在preproc.LocationWrapper类中实现了一个简单的房间跟踪，它跟踪整个回合中访问过的房间。然后，这个标志会作为一个单一的1（如果之前访问过该房间）或0（如果是新位置）被拼接到智能体的观察中。
- en: To train our agent with this extension, you can run train_preproc.py with the
    additional command-line option --seen-rooms.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个扩展来训练我们的智能体，可以通过额外的命令行选项--seen-rooms运行train_preproc.py。
- en: The following are charts comparing our baseline version with this extra observation
    on 20 games. As you can see, reward on training games are almost the same, but
    validation reward was improved — we were able to get non-zero validation reward
    almost during the whole training. But count of steps on validation game are still
    50.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将我们基准版本与这个额外观察在20局游戏中的比较图表。如你所见，训练游戏的奖励几乎相同，但验证奖励有所提高——我们几乎在整个训练过程中都能获得非零的验证奖励。不过，验证游戏中的步骤数仍然是50。
- en: '![PIC](img/B22150_13_10.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_10.png)'
- en: 'Figure 13.10: Training reward (left) and validation reward (right) on 20 games'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：20局游戏中的训练奖励（左）和验证奖励（右）
- en: 'But after trying this extension on 200 games (you need to change the script
    to generate them), I’ve got an interesting result: after 14 hours of training
    and 8,000 episodes, the agent was not just getting the maximum score on validation
    game but was able to do this efficiently (with count of steps less than 10). This
    is shown in Figure [13.11](#x1-238004r11) and Figure [13.12](#x1-238005r12).'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 但在对200局游戏进行尝试后（你需要更改脚本来生成这些游戏），我得到了一个有趣的结果：经过14小时的训练和8000个回合后，智能体不仅在验证游戏中获得了最高分，而且能够高效地做到这一点（步骤数小于10）。这在图[13.11](#x1-238004r11)和图[13.12](#x1-238005r12)中展示。
- en: '![PIC](img/B22150_13_11.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_11.png)'
- en: 'Figure 13.11: Training reward (left) and episode steps (right) on 200 games'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：200局游戏中的训练奖励（左）和回合步骤（右）
- en: '![PIC](img/B22150_13_12.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_12.png)'
- en: 'Figure 13.12: Validation reward (left) and episode steps (right)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12：验证奖励（左）和回合步骤（右）
- en: Relative actions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相对动作
- en: The second attempt to improve the agent’s learning was about the action space.
    In principle, our agent’s task is to navigate the rooms and perform specific actions
    on objects around (like opening the locker and taking something out of it). So,
    navigation is a very important aspect in learning process.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 改进智能体学习的第二次尝试是关于动作空间的。从原则上讲，我们的智能体任务是导航房间并在周围的物体上执行特定的操作（例如打开储物柜并取出东西）。因此，导航在学习过程中是一个非常重要的方面。
- en: At the moment, we move around by executing “absolute coordinate” commands, like
    “go north” or “go east”, which are room-specific, as different rooms might have
    different exits available. In addition, after executing some action, the inverse
    action (to get back to the original room) depends on the first action. For example,
    if we are in the room with an exit to the north, after using this exit, we need
    to execute “go south” to get back. But our agent has no memory of the history
    of actions, so after going north, we have no idea how to get back.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们通过执行“绝对坐标”命令来移动，例如“向北走”或“向东走”，这些命令是特定于房间的，因为不同的房间可能有不同的出口可用。此外，在执行某些动作后，逆动作（返回原房间）依赖于第一个动作。例如，如果我们在一个有北方出口的房间，使用了这个出口后，我们需要执行“向南走”来返回。但我们的智能体没有动作历史的记忆，所以在向北走后，我们不知道如何返回。
- en: In the previous section, we added information about whether the room was visited
    or not. Now we’ll transform absolute actions into relative actions. To get that,
    our wrapper preproc.RelativeDirectionsWrapper tracks our “heading direction” and
    replaces the “go north” or “go east” commands with “go left”, “go right”, “go
    forward”, or “go back” depending on the heading direction. In this example, when
    we’re in the room with an exit to the north and we’re heading north, we need to
    execute the command “go forward” to use the exit. After that, we can run the command
    “go back” to step back in the originating room. Hopefully, this transformation
    will allow our model to navigate the TextWorld games with more ease.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们添加了有关房间是否被访问的信息。现在，我们将绝对动作转化为相对动作。为了实现这一点，我们的包装器`preproc.RelativeDirectionsWrapper`会跟踪我们的“朝向方向”，并根据朝向方向将“向北走”或“向东走”命令替换为“向左走”、“向右走”、“向前走”或“向后走”。例如，当我们处在一个出口朝北的房间里且我们的朝向是北时，我们需要执行“向前走”命令以使用出口。然后，我们可以执行“向后走”命令返回原来的房间。希望这种转化能让我们的模型更轻松地在TextWorld游戏中导航。
- en: 'To enable this extension, you need to run train_preproc.py with the --relative-actions
    command-line option. This extension also requires “seen rooms” to be enabled,
    so here, we’re testing the effect of both modifications combined. On 20 games,
    training dynamics and validation results are very similar to the baseline version
    (Figure [13.13](#x1-239002r13)):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用此扩展，你需要使用`--relative-actions`命令行选项运行`train_preproc.py`。此扩展还需要启用“已见房间”，因此在这里，我们测试了这两项修改结合后的效果。在20场游戏中，训练动态和验证结果与基准版本非常相似（如图[13.13](#x1-239002r13)所示）：
- en: '![PIC](img/B22150_13_13.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_13.png)'
- en: 'Figure 13.13: Training reward (left) and validation reward (right) on 20 games'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.13：20场游戏中的训练奖励（左）和验证奖励（右）
- en: 'But on 200 games, the agent was able to get the maximum score on validation
    game after just 2.5 hours (instead of 13 in the “Seen rooms” extension). The number
    of steps on validation was also decreased below 10\. But, unfortunately, after
    further training, validation metrics reverted to lower validation scores, so the
    agent overfitted to the games and unlearned the skills it had:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 但在200场游戏上，智能体在仅2.5小时内就能在验证游戏中获得最高分（而不是“已见房间”扩展中的13小时）。验证中的步数也减少到了不到10步。但不幸的是，经过进一步训练后，验证指标恢复到较低的验证分数，因此智能体对游戏进行了过拟合，并遗忘了它已掌握的技能：
- en: '![PIC](img/B22150_13_14.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_14.png)'
- en: 'Figure 13.14: Validation reward (left) and episode steps (right) on 200 games'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.14：200场游戏中的验证奖励（左）和回合步数（右）
- en: Objective in observation
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观察中的目标
- en: Another idea is to feed the game objective into the agent observations. The
    objective is presented as text at the beginning of the game, for example, First
    thing I need you to do is to try to venture east. Then, venture south. After that,
    try to go to the south. Once you succeed at that, try to go west. If you can finish
    that, pick up the coin from the floor of the chamber. Once that’s all handled,
    you can stop!.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个想法是将游戏目标传递给智能体的观察值。目标在游戏开始时以文本形式呈现，例如：首先，我需要你做的事情是尝试向东前进。然后，向南前进。之后，尝试向南走。完成后，尝试向西走。如果你能完成这些，捡起房间地板上的硬币。完成所有这些后，你可以停止！
- en: This information might be useful for the agent to plan its actions, so let’s
    add it to the encoded vectors. We don’t need to implement another wrapper, as
    our existing ones are flexible enough already. Just a couple of extra arguments
    need to be passed to them. To enable the objective, you need to run train_preproc.py
    with the --objective command-line argument.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息可能对智能体规划行动有所帮助，因此让我们将其添加到编码向量中。我们不需要实现另一个包装器，因为现有的包装器已经足够灵活。只需要向它们传递几个额外的参数。要启用目标，你需要使用`--objective`命令行参数运行`train_preproc.py`。
- en: 'Results on 20 games are almost identical to the baseline and shown in Figure [13.15](#x1-240002r15):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在20场游戏中的结果几乎与基准线完全相同，结果如图[13.15](#x1-240002r15)所示：
- en: '![PIC](img/B22150_13_15.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_15.png)'
- en: 'Figure 13.15: Training reward (left) and validation reward (right) on 20 games'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.15：20场游戏中的训练奖励（左）和验证奖励（右）
- en: 'Training on 200 games was less successful than for previous modifications:
    during the validation, score was around 2-4 but never reached 6\. Charts for reward
    and validation reward are shown in Figure [13.16](#x1-240003r16):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在200场游戏上的训练比之前的修改效果差：在验证过程中，得分在2到4之间，但从未达到6分。奖励和验证奖励的图表如图[13.16](#x1-240003r16)所示：
- en: '![PIC](img/B22150_13_16.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22150_13_16.png)'
- en: 'Figure 13.16: Training reward (left) and validation reward (right) on 200 games'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.16：在 200 局游戏中的训练奖励（左）和验证奖励（右）
- en: Transformers
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer
- en: The next approach we’ll try is pretrained language models, which is a de facto
    standard in modern NLP. Thanks to public model repositories, like the [Hugging
    Face Hub](https://huggingface.co/docs/hub/en/index), we don’t need to train them
    from scratch, which might be very costly. We can just plug the pretrained model
    into our architecture and fine-tune a small portion of our network to our dataset.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将尝试的是预训练语言模型，这是现代自然语言处理中的事实标准。得益于像 [Hugging Face Hub](https://huggingface.co/docs/hub/en/index)
    这样的公共模型库，我们不需要从头开始训练这些模型，这可能会非常耗费资源。我们只需要将预训练模型接入我们的架构，并对网络的一小部分进行微调，适应我们的数据集。
- en: There is a wide variety of models — different sizes, datasets they were pretrained
    on, training techniques, etc. But all of them use a simple API, so plugging them
    into our code is simple and straightforward.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种各样的模型——不同的大小、它们所预训练的语料库、训练技术等等。但所有这些模型都使用一个简单的 API，因此将它们接入我们的代码非常简单直接。
- en: 'First, we need to install the libraries. For our task, we’ll use the package
    sentence-transformers==2.6.1, which you need to install manually. Once this is
    done, you can use it to compute embeddings of any sentences given as strings:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装库。对于我们的任务，我们将使用包 `sentence-transformers==2.6.1`，你需要手动安装。一旦安装完成，你就可以用它计算任何给定字符串的嵌入：
- en: '[PRE20]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here we used the all-MiniLM-L6-v2 model, which is relatively small — 22M parameters
    trained on 1.2B tokens. You can find more information on the Hugging Face website:
    [https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们使用的是 all-MiniLM-L6-v2 模型，它相对较小——22M 参数，训练于 12 亿个标记。你可以在 Hugging Face 网站上找到更多信息：[https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)。
- en: In our case, we’ll use the high-level interface, where we feed strings with
    sentences, and the library and model are doing all the conversion for us. But
    there is lots of flexibility if needed.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将使用高级接口，我们将包含句子的字符串输入，库和模型会为我们完成所有转换工作。不过，如果需要，仍然有很多灵活性。
- en: The preproc.TransformerPreprocessor class implements the same interface as our
    old Preprocessor class (which used LSTM for embeddings) and I’m not going to show
    the code as it is very straightforward.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`preproc.TransformerPreprocessor` 类实现了与我们旧的 `Preprocessor` 类（使用 LSTM 进行嵌入）相同的接口，我不会展示代码，因为它非常直观。'
- en: 'To train our agent with transformers, you need to run the Chapter13/train_tr.py
    module. During the training, transformers turned out to be slower (2 FPS vs 6
    FPS on my machine), which is not surprising, as the model is much more complicated
    than LSTM models. But training dynamics is better on 20 and 200 games. In Figure [13.17](#x1-241017r17),
    you can see training reward and count of episode steps for transformers and baseline.
    The baseline version required 1,000 episodes to reach 15 steps, where transformers
    required just 400\. Validation on 20 games had worse reward than baseline version
    (max score was 2):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Transformer 训练我们的智能体，需要运行 Chapter13/train_tr.py 模块。在训练过程中，Transformer 显得比较慢（在我的机器上是
    2 FPS 对比 6 FPS），这并不令人惊讶，因为模型比 LSTM 模型复杂得多。不过，在 20 局和 200 局的训练中，动态表现更好。在图 [13.17](#x1-241017r17)
    中，你可以看到 Transformer 和基线版本的训练奖励及回合步数。基线版本需要 1,000 个回合才能达到 15 步，而 Transformer 只需要
    400 个回合。对 20 局游戏的验证结果比基线版本差（最大得分为 2）：
- en: '![PIC](img/B22150_13_17.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_13_17.png)'
- en: 'Figure 13.17: Training reward (left) and training episodes length (right) on
    20 games'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.17：在 20 局游戏中的训练奖励（左）和训练回合长度（右）
- en: The same situation was on 200 games — the agent learns more efficiently (in
    terms of games), but validation is not great. This could be explained by the much
    larger capacity of transformers — the embeddings they produce are almost 20 times
    larger than our baseline model (384 vs 20), so it is easier for our agent to just
    memorize the correct sequence of steps instead of trying to find high-level generic
    observations to actions mapping.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在 200 局游戏中也是相同的情况——智能体学习效率更高（以游戏数量计算），但验证结果并不理想。这可以通过 Transformer 的容量大得多来解释——它们产生的嵌入几乎是我们的基线模型的
    20 倍（384 对比 20），所以智能体更容易记住正确的步骤序列，而不是试图找到高层次的通用观察到行动的映射关系。
- en: ChatGPT
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT
- en: To finalize the discussion of TextWorld, let’s try a different approach — using
    LLMs. Right after public release at the end of 2022, OpenAI ChatGPT became very
    popular and literally transformed the chatbot and text-based assistant landscape.
    Just in a year since its release, hundreds of new use cases appeared and thousands
    of applications using LLMs under the hood were developed. Let’s try to apply this
    technology to our problem of solving TextWorld games.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束对TextWorld的讨论，让我们尝试另一种方法——使用LLM。在2022年底公开发布后，OpenAI的ChatGPT迅速流行开来，几乎改变了聊天机器人和基于文本的助手的格局。仅仅一年时间，数百个新的使用场景出现，并且数千个使用LLM技术的应用程序被开发出来。让我们尝试将这项技术应用到解决TextWorld游戏的问题上。
- en: Setup
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: First, you will need an account on [https://openai.com](https://openai.com).
    We’ll start our experiment with an interactive web-based chat, which could be
    tried for free and without registration (at the moment of writing), but our next
    example will use the ChatGPT API, for which you will need to generate an API key
    at [https://platform.openai.com](https://platform.openai.com). Once the key is
    created, you need to set it to the environment variable OPENAI_API_KEY in the
    shell you’re using.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要在[https://openai.com](https://openai.com)上注册一个账号。我们将从一个基于网页的交互式聊天开始实验，目前可以免费试用且无需注册（在撰写时），但接下来的示例将使用ChatGPT
    API，您需要在[https://platform.openai.com](https://platform.openai.com)生成一个API密钥。一旦密钥创建完成，您需要将其设置为您正在使用的shell中的环境变量OPENAI_API_KEY。
- en: 'We’ll also use the langchain library to communicate with ChatGPT from Python,
    so please install it with the following commands:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用langchain库从Python与ChatGPT进行通信，因此请使用以下命令安装它：
- en: '[PRE21]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that these packages are quite dynamic and new versions might break compatibility.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些包非常动态，新版本可能会破坏兼容性。
- en: Interactive mode
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互模式
- en: 'In our first example, we’ll use the web-based ChatGPT interface, asking it
    to generate game commands from room descriptions and game objectives. The code
    is in Chapter13/chatgpt_interactive.py and it does the following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个示例中，我们将使用基于网页的ChatGPT接口，要求其根据房间描述和游戏目标生成游戏指令。代码位于Chapter13/chatgpt_interactive.py，功能如下：
- en: Starts the TextWorld environment for the game ID given in the command line
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动命令行中给定的游戏ID的TextWorld环境
- en: Creates the prompt for ChatGPT with instructions, game objective, and room description
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为ChatGPT创建包含指令、游戏目标和房间描述的提示
- en: Writes this prompt to the console
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将该提示写入控制台
- en: Reads the command to be executed from the console
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从控制台读取要执行的指令
- en: Executes the command in the environment
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中执行该指令
- en: Repeats from step 2 until the limit of steps has been reached or until we’ve
    solved the game
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第2步开始重复，直到达到步数限制或游戏解决
- en: So, your task is to copy the generated prompt and paste it into the [https://chat.openai.com](https://chat.openai.com)
    web interface. ChatGPT will generate the command that has to be entered into the
    console.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，您的任务是复制生成的提示并将其粘贴到[https://chat.openai.com](https://chat.openai.com)的网页界面中。ChatGPT将生成必须输入到控制台中的指令。
- en: 'The full code is very simple and short. It has just a single play_game function,
    which executes the game loop using the created environment:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码非常简单且简短。它只有一个play_game函数，使用创建的环境执行游戏循环：
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'During environment creation, we ask just for two extra information pieces:
    room description and game objective. In principle, both are present in free-text
    observations, so we could parse them from this text. But for convenience, we ask
    TextWorld to provide this explicitly.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境创建过程中，我们只需要额外的两项信息：房间描述和游戏目标。原则上，这两者都可以从自由文本观察中提取，因此我们可以从这些文本中解析它们。但为了方便起见，我们要求TextWorld明确提供这些信息。
- en: 'In the beginning of the play_game function, we reset the environment and generate
    the initial prompt:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在play_game函数的开头，我们重置环境并生成初始提示：
- en: '[PRE23]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: I haven’t spent much time designing it, as basically, everything worked from
    the first attempt and I’m sure it could be improved. The last sentence, “Reply
    with just a command in lowercase and nothing else,” prevents the chatbot from
    being too verbose and saves us from parsing the output.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有花太多时间设计它，因为基本上，第一次尝试就成功了，我确信它可以得到改进。最后一句话“仅以小写形式回复一个指令，且不包含其他内容”可以防止聊天机器人过于冗长，并帮助我们省去解析输出的麻烦。
- en: 'Then we execute the loop until the game is solved or the limit of steps has
    been reached:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行循环，直到游戏解决或达到步数限制：
- en: '[PRE24]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The continuation prompt is much simpler — we just give the obtained observation
    (which is an outcome of the command) and new room description. We don’t need to
    pass the objective again, as the web interface keeps the context of conversation,
    so the chatbot is aware of our prior instructions.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 后续提示则简单得多——我们只提供获得的观察结果（命令的结果）和新的房间描述。我们无需再次传递目标，因为网页界面会保留对话的上下文，所以聊天机器人知道我们的先前指令。
- en: 'Let’s take a look at one game test (with seed 1). I stripped the room descriptions
    to decrease the verbosity; otherwise, it would take several pages of the book.
    But you should copy the generated text fully:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个游戏测试（种子1）。为了减少冗长，我删除了房间描述；否则，内容会占用几页书。但你应该完整复制生成的文本：
- en: '[PRE25]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see, the LLM was able to solve the task perfectly. What is even more
    spectacular is that overall task is harder — we ask it to generate commands and
    not to make a decision from a list of “admissible commands” as earlier in the
    chapter.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，LLM能够完美地解决任务。更令人惊叹的是，整体任务更为复杂——我们要求它生成命令，而不是像本章前面那样从“可接受命令”列表中做出决策。
- en: ChatGPT API
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT API
- en: Since copy-pasting is tedious and boring, let’s automate our agent using the
    ChatGPT API. We’ll use the langchain library ([https://python.langchain.com/](https://python.langchain.com/)),
    which provides enough flexibility and control to leverage the LLM functionality.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由于复制粘贴既乏味又无聊，让我们通过ChatGPT API来自动化我们的代理。我们将使用langchain库（[https://python.langchain.com/](https://python.langchain.com/)），它提供了足够的灵活性和控制力，可以充分利用LLM功能。
- en: 'The full code example is in Chapter13/chatgpt_auto.py. Here, I will cover the
    core function, play_game():'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '完整的代码示例在Chapter13/chatgpt_auto.py中。这里，我将介绍核心函数play_game():'
- en: '[PRE26]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our initial prompt is the same as before — we’re instructing the chatbot about
    the kind of the game we’re playing and asking it to reply only with commands to
    be fed into the game.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始提示与之前相同——我们指示聊天机器人游戏的类型，并要求它仅回复将要输入游戏的命令。
- en: 'Then we reset the environment and generate the first message, passing the information
    from TextWorld:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们重置环境并生成第一条消息，传递TextWorld的信息：
- en: '[PRE27]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The variable context is very important and it contains the list of all messages
    (both from human and the chatbot) in our conversation so far. We’ll pass those
    messages to the chatbot to preserve the process of the game. This is needed because
    the game objective is being shown only once and not repeated again. Without the
    history, the agent doesn’t have enough information to perform the required sequence
    of steps. On the other hand, having lots of text passed to the chatbot might lead
    to high costs (as the ChatGPT API is billed for tokens being processed). Our game
    is not long (5-7 steps is enough to finish the task), so it is not a major concern,
    but for more complex games, history might be optimized.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 变量上下文非常重要，它包含了我们到目前为止所有消息的列表（包括来自人类和聊天机器人的消息）。我们将把这些消息传递给聊天机器人，以保留游戏的过程。这是必要的，因为游戏目标只展示一次，不会重复。如果没有历史记录，代理没有足够的信息来执行所需的步骤序列。另一方面，传递大量文本给聊天机器人可能导致高成本（因为ChatGPT
    API按处理的token计费）。我们的游戏不长（5到7步就能完成任务），所以这不是一个主要问题，但对于更复杂的游戏，历史记录可能需要优化。
- en: 'Then the game loop follows, which is very similar to what we had in interactive
    version, but without console communication:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是游戏循环，和我们在交互式版本中看到的非常相似，但没有控制台通信：
- en: '[PRE28]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the continuation prompt, we pass the history of conversation, result of last
    command, description of the current room, and ask for the next command.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续提示中，我们传递了对话历史、上一个命令的结果、当前房间的描述，并请求下一条命令。
- en: 'We also limit the amount of steps to prevent the agent from getting stuck in
    loops (it happens sometimes). If the game isn’t solved after 20 steps, we exit
    the loop:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还限制了步骤数量，以防止代理陷入循环（有时会发生）。如果在20步之后游戏未完成，我们会退出循环：
- en: '[PRE29]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: I did experiment with the preceding code on 20 TextWorld games (with seeds 1…20)
    and it was able to solve 9 games out of 20\. Most of the failed situations were
    because the agent went into the loop — issuing the wrong command not properly
    interpreted by TextWorld (like “take the key” instead of “take the key from the
    box”), or getting stuck in navigation.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾在20个TextWorld游戏（种子1到20）上实验过前面的代码，它成功解决了20个游戏中的9个。大多数失败的情况是由于代理进入了循环——发出了错误的命令，TextWorld没有正确解析（例如“拿起钥匙”而不是“从箱子里拿起钥匙”），或者在导航上卡住了。
- en: In two games, ChatGPT failed because of generating the command “exit”, which
    makes TextWorld stop immediately. Most likely, detecting this command or prohibiting
    its generation in the prompt might increase the number of solved games. But still,
    even 9 games solved by the agent without any prior training is quite an impressive
    result. In terms of ChatGPT costs, running the experiment took 450K tokens to
    be processed, which cost me $0.20\. Not a big price for having fun!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在两款游戏中，ChatGPT失败是因为生成了“exit”命令，这会使TextWorld立即停止。很可能，检测到这个命令或禁止在提示中生成它，可能会增加解决游戏的数量。但即使如此，代理在没有任何预训练的情况下解决了9款游戏，依然是相当令人印象深刻的结果。从ChatGPT的成本来看，运行实验的处理费用为450K个token，花费了我$0.20。对于这样有趣的体验，价格并不高！
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have seen how DQN can be applied to interactive fiction
    games, which is an interesting and challenging domain at the intersection of RL
    and NLP. You learned how to handle complex textual data with NLP tools and experimented
    with fun and challenging interactive fiction environments, with lots of opportunities
    for future practical experimentation. In addition, we used the transformer model
    from the Hugging Face library and experimented with ChatGPT.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经看到DQN如何应用于互动小说游戏，这是一个有趣且具有挑战性的领域，位于强化学习（RL）和自然语言处理（NLP）的交叉点。你学习了如何利用NLP工具处理复杂的文本数据，并在有趣且充满挑战的互动小说环境中进行实验，未来有许多实际实验的机会。此外，我们还使用了Hugging
    Face库中的transformer模型，并与ChatGPT进行了实验。
- en: In the next chapter, we will continue our exploration of “RL in the wild” and
    check the applicability of RL methods in web automation.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探索“野外的RL”，并检查RL方法在网页自动化中的适用性。
