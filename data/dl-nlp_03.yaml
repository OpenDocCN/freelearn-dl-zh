- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第三章*'
- en: Introduction to Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Describe Deep Learning and its applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述深度学习及其应用
- en: Differentiate between Deep Learning and machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分深度学习和机器学习
- en: Explore neural networks and their applications
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络及其应用
- en: Understand the training and functioning of a neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解神经网络的训练与工作原理
- en: Use Keras to create neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Keras创建神经网络
- en: This chapter aims to introduce you to neural networks, their applications in
    Deep Learning, and their general drawbacks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在向你介绍神经网络、它们在深度学习中的应用及其普遍的缺点。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous two chapters, you learned about the basics of natural language
    processing, its importance, the steps required to prepare text for processing,
    and two algorithms that aid a machine in understanding and executing tasks based
    on natural language. However, to cater to higher, more complicated natural language
    processing problems, such as creating a personal voice assistant like *Siri* and
    *Alexa*, additional techniques are required. Deep learning systems, such as neural
    networks, are often used in natural language processing, and so we're going to
    cover them in this chapter. In the following chapters, you learn how to use neural
    networks for the purpose of natural language processing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，你了解了自然语言处理的基础知识、它的重要性、准备文本进行处理的步骤以及帮助机器理解并执行基于自然语言任务的两种算法。然而，为了应对更高层次、更复杂的自然语言处理问题，如创建类似*Siri*和*Alexa*的个人语音助手，还需要额外的技术。深度学习系统，如神经网络，常用于自然语言处理，因此我们将在本章中讨论它们。在接下来的章节中，你将学习如何使用神经网络进行自然语言处理。
- en: This chapter begins with an explanation on deep learning and how it is different
    from machine learning. Then, it discusses neural networks, which make up a large
    part of deep learning techniques, and their basic functioning along with real-world
    applications. Additionally, it introduces **Keras**, a Python deep learning library.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先解释深度学习及其与机器学习的不同之处。然后，讨论神经网络，它是深度学习技术的核心部分，以及它们的基本功能和实际应用。此外，本章还介绍了**Keras**，一个Python深度学习库。
- en: Introduction to Deep Learning
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习简介
- en: Artificial Intelligence is the idea of agents possessing the natural intelligence
    of humans. This natural intelligence includes the ability to plan, understand
    human language, learn, make decisions, solve problems, and recognize words, images
    and objects. When building these agents, this intelligence is known as artificial
    intelligence, since it is human-made. These agents do not refer to physical objects.
    They are, in fact, a reference to software that demonstrates artificial intelligence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是指拥有类似人类自然智能的智能体。这种自然智能包括计划、理解人类语言、学习、做决策、解决问题以及识别单词、图像和物体的能力。在构建这些智能体时，这种智能被称为人工智能，因为它是人为制造的。这些智能体并不指代物理对象。实际上，它们是指能展示人工智能的软件。
- en: There are two types of artificial intelligence—narrow and generalized. Narrow
    artificial intelligence is the kind of artificial intelligence that we are currently
    surrounded by; it is any single agent possessing one of the several capabilities
    of natural intelligence. The application areas of natural language processing
    that you learned about in the first chapter of this book are examples of narrow
    Artificial Intelligence, because they are agents capable of carrying out a single
    task, such as, a machine being able to automatically summarize an article. There
    do exist Technologies do exist that are capable of more than one task, such as
    self-driving cars, but these are still considered a combination of several narrow
    AIs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能有两种类型——窄域人工智能和广域人工智能。窄域人工智能是我们目前所接触到的人工智能类型；它是任何拥有自然智能若干能力之一的单一智能体。本书第一章中你了解的自然语言处理应用领域就是窄域人工智能的例子，因为它们是能够执行单一任务的智能体，例如，机器能够自动总结文章。确实存在能够执行多项任务的技术，如自动驾驶汽车，但这些技术仍被认为是多个窄域人工智能的组合。
- en: Generalized artificial intelligence is the possession of all human capabilities
    and more, in a single agent, rather than one or two capabilities in a single agent.
    AI experts claim that once AI has surpassed this goal of generalized AI and it
    is smarter and more adept than humans themselves in all fields, it will become
    super artificial intelligence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 广义人工智能是指在一个智能体中拥有所有人类能力及更多能力，而不是一个智能体中仅有一到两个能力。AI专家声称，一旦人工智能超越了广义人工智能的目标，在所有领域中比人类更聪明、更熟练，它将成为超级人工智能。
- en: As mentioned in the previous chapters, natural language processing is an approach
    to achieving artificial intelligence, by enabling machines to understand and communicate
    with humans in the natural language of humans. Natural language processing prepares
    textual data and transforms it into a form that machines are able to process—a
    numerical form. This is where deep learning comes in.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，自然语言处理是一种实现人工智能的方法，通过使机器能够理解并与人类以自然语言进行沟通。自然语言处理准备文本数据并将其转换为机器能够处理的形式——即数值形式。这就是深度学习的应用领域。
- en: Like natural language processing and machine learning, deep learning is also
    a category of techniques and algorithms. It is a subfield of machine learning
    because both these approaches share the same primary principle—both machine learning
    and deep learning algorithms take input and use it to predict output.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 像自然语言处理和机器学习一样，深度学习也是一种技术和算法类别。它是机器学习的一个子领域，因为这两种方法共享相同的主要原理——无论是机器学习还是深度学习算法，都从输入中获取信息并使用它来预测输出。
- en: '![Fig 3.1: Deep Learning as a subfield of Machine Learning'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：深度学习作为机器学习的子领域'
- en: '](img/C13783_03_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_01.jpg)'
- en: 'Fig 3.1: Deep learning as a subfield of machine learning'
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：深度学习作为机器学习的一个子领域
- en: When trained on a training dataset, both types of algorithms (machine learning
    and deep learning) aim to minimize the difference between the actual outcomes
    and their predicted outcomes. This aids them in forming an association between
    the input and the output, thus resulting in higher accuracy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当在训练数据集上训练时，两种类型的算法（机器学习和深度学习）都旨在最小化实际结果和预测结果之间的差异。这帮助它们在输入和输出之间建立关联，从而提高准确性。
- en: Comparing Machine Learning and Deep Learning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较机器学习和深度学习
- en: While both these approaches are based on the same principle—predicting output
    from input—they achieve this in different ways, which is why deep learning has
    been categorized as a separate approach. Additionally, one of the main reasons
    for deep learning coming about was the increased accuracy these models provide
    in their prediction process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种方法都基于相同的原理——从输入预测输出——但它们通过不同的方式实现这一点，这也是深度学习被归类为一种独立方法的原因。此外，深度学习出现的一个主要原因是这些模型在预测过程中的准确性得到了提高。
- en: While machine learning models are quite self-sufficient, they still need human
    intervention to determine that a prediction is incorrect, and thus they need to
    get better at performing that particular task. Deep learning models, on the other
    hand, are capable of determining whether a prediction is incorrect or not by themselves.
    Thus, deep learning models are self-sufficient; they can make decisions and improve
    their efficiency without human interventions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然机器学习模型相当自足，但它们仍然需要人工干预来判断预测是否错误，因此需要在执行特定任务时变得更好。而深度学习模型则能够自己判断预测是否错误。因此，深度学习模型是自足的；它们能够在没有人工干预的情况下做出决策并提高效率。
- en: To better understand this, let's take the example of an air conditioner whose
    temperature settings can be controlled by voice commands. Let's say that when
    the air conditioner hears the word "hot," it decreases the temperature, and when
    it hears the word "cold," it increases the temperature. If this were a machine
    learning model, then the air conditioner would learn to recognize these two words
    in different sentences over time. However, if this were a deep learning model,
    it could learn to alter the temperature based on words and sentences similar to
    the words "hot" and "cold," such as "It's a little warm" or "I'm freezing!" and
    so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们以一个可以通过语音命令控制温度设置的空调为例。假设当空调听到“热”这个词时，它会降低温度，而当它听到“冷”这个词时，它会升高温度。如果这是一个机器学习模型，那么空调会随着时间的推移学会在不同的句子中识别这两个词。然而，如果这是一个深度学习模型，它可以根据与“热”和“冷”类似的词语和句子（如“有点热”或“我快冻死了！”等）来学习调整温度。
- en: This is an example that directly relates to natural language processing since
    the model understands the natural language of humans and acts on what it has understood.
    In this book we will be sticking to using deep learning models for the purpose
    of natural language processing, though in reality they can be used in almost every
    field. They are currently involved in automating the task of driving, by enabling
    a vehicle to recognize stop signs, read traffic signals, and halt for pedestrians.
    The medical field is also utilizing deep learning methods to detect diseases at
    early stages – cancer cells. But since our focus in this book is on enabling machines
    to understand the natural language of humans, let's get back to that.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个直接与自然语言处理相关的例子，因为该模型能够理解人类的自然语言，并根据其理解做出反应。在本书中，我们将专注于使用深度学习模型进行自然语言处理，尽管实际上它们几乎可以应用于每个领域。目前，它们在自动化驾驶任务中也有所应用，使得车辆能够识别停车标志、读取交通信号，并在行人面前停车。医疗领域也在利用深度学习方法检测早期的疾病——如癌细胞。然而，由于本书的重点是让机器理解人类的自然语言，我们还是回到这个主题。
- en: Deep learning techniques are most often used in the supervised learning way,
    that is, they are provided with labelled data to learn from. However, the key
    difference between machine learning methods and deep learning methods is that
    the latter require insanely large amounts of data which didn't exist before. Thus,
    deep learning has only recently become advantageous. It also requires quite a
    bit of computing power since it needs to be trained on such large amounts of data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术通常用于监督学习方式，即它们会接受标记数据进行学习。然而，机器学习方法与深度学习方法之间的关键区别在于，后者需要极为庞大的数据量，这是之前不存在的。因此，深度学习直到最近才变得具有优势。它还需要相当大的计算能力，因为它需要在如此庞大的数据集上进行训练。
- en: The main difference, however, is in a algorithms themselves. If you've studied
    machine learning before, then you're aware of the variety of algorithms that exist
    to solve classification and regression problems, as well as unsupervised learning
    ones. Deep learning systems differ from these algorithms because they use Artificial
    Neural Networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，主要的区别在于算法本身。如果你以前学习过机器学习，那么你应该知道解决分类和回归问题的各种算法，以及无监督学习的问题。深度学习系统与这些算法的不同之处在于，它们使用的是人工神经网络。
- en: Neural Networks
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: Often neural networks and deep learning are terms that are used interchangeably.
    They do not mean the same thing, however, so let's learn the difference.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习通常是互换使用的术语。它们并不意味着相同的东西，因此让我们来了解它们之间的区别。
- en: As mentioned before, deep learning is an approach that follows the same principle
    as machine learning, but does so with more accuracy and efficiency. Deep learning
    systems make use of artificial neural networks, which are computing models on
    their own. So, basically, neural networks are a part of the deep learning approach
    but are not the deep learning approach on their own. They are frameworks that
    are incorporated by deep learning methods.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习是一种遵循与机器学习相同原则的方法，但它具备更高的准确性和效率。深度学习系统利用人工神经网络，这些神经网络本身就是计算模型。因此，基本上，神经网络是深度学习方法的一部分，但并不是深度学习方法的全部。它们是被深度学习方法所整合的框架。
- en: '![Fig 3.2: Neural Networks as a part of the Deep Learning Approach'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2: 神经网络作为深度学习方法的一部分'
- en: '](img/C13783_03_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_02.jpg)'
- en: 'Fig 3.2: Neural Networks as a part of the deep learning Approach'
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.2: 神经网络作为深度学习方法的一部分'
- en: Artificial neural networks are based on a framework inspired by the biological
    neural networks found in the human brain. These neural networks are made of nodes
    that enable the networks to learn from images, text, real-life objects, and other
    things, to be able to execute tasks and predict things accuracy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络基于一个受人脑中生物神经网络启发的框架。这些神经网络由节点组成，使得网络能够从图像、文本、现实物体等中学习，从而能够执行任务并进行准确预测。
- en: Neural networks consist of layers, which we will take a look at in the following
    section. The number of layers that a network has can be anywhere from three to
    hundreds. Neural networks that are made of only three or four layers are called
    shallow neural networks, whereas networks that have many more layers than that
    are referred to as deep neural networks. Thus, the neural networks used by the
    deep learning approach are deep neural networks and they possess several layers.
    Due to this, deep learning models are very well suited to complex tasks such as
    facial recognition translating text, and so on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由多个层组成，我们将在接下来的部分中深入了解。这些层的数量可以从三层到数百层不等。由三层或四层构成的神经网络称为浅层神经网络，而层数更多的网络则被称为深度神经网络。因此，深度学习方法使用的神经网络是深度神经网络，它们包含多个层。由于这一点，深度学习模型非常适合处理复杂任务，如人脸识别、文本翻译等。
- en: These layers break down the input into several levels of abstraction. As a result,
    the deep learning model is better able to learn from and understand the input,
    be it images or text or another form of input, which aids it in making decisions
    and predicting things the way our human mind does.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层将输入分解为多个抽象级别。因此，深度学习模型能够更好地从输入中学习并理解，无论是图像、文本还是其他形式的输入，这有助于它做出决策并像人类大脑一样进行预测。
- en: Let's go through an example to understand these layers. Imagine that you're
    in your bedroom doing some work and you notice you're sweating. That's your input
    data—the fact that you're feeling hot and so in your head a little voice goes
    "I'm feeling hot!" Next, you might wonder why you're feeling so hot—"Why am I
    feeling so hot?" This is a thought. You'll then try to come up with a solution
    to this problem, maybe by taking a shower—"Let me take a quick shower." This is
    a decision that you've made. But then you remember that you've got to leave for
    work soon—"But, I need to leave the house soon." This is a memory. You might try
    to convince yourself by thinking "Isn't there enough time to squeeze in a quick
    shower, though?" This is the process of a reasoning. Lastly, you'll probably act
    on your thoughts by either thinking "I'm going to take a shower," or, "there's
    no time for a shower, never mind." This is decision making and in the event you
    do take a shower, it is an action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这些层。假设你正在卧室里做些工作，突然注意到自己在出汗。这就是你的输入数据——你感到很热，于是脑海中浮现出一个声音：“我觉得很热！”接着，你可能会想为什么自己会感到这么热：“为什么我会这么热？”这是一个思考。然后你会尝试解决这个问题，或许通过洗个澡来缓解：“让我快速洗个澡。”这是你做出的决策。但随后你记得自己很快就要出门上班：“但是，我得很快离开家。”这是一个记忆。你可能会尝试说服自己：“其实，还是有足够时间快速洗个澡吧？”这是推理的过程。最后，你可能会根据自己的想法做出行动，或者心里想着：“我要去洗澡了”，或者“没时间洗澡了，算了。”这就是决策过程，如果你真的去洗了澡，那就是一种行动。
- en: The multiple layers in a deep neural network allow the model to go through these
    different levels of processing just like the mind does, thus building upon the
    principles of biological neural networks. These layers are how and why deep learning
    models are able to perform tasks and predict outputs with such high accuracy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络中的多层结构使得模型能够像大脑一样经历不同的处理层级，从而建立起生物神经网络的原理。这些层正是深度学习模型能够高精度完成任务和预测输出的原因。
- en: Neural Network Architecture
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: 'Neural network architecture refers to the elements that are the building blocks
    of a neural network. While there are several different types of neural networks,
    the basic architecture and foundation remains constant. The architecture includes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构指的是构成神经网络的基本元素。尽管有多种不同类型的神经网络，但基本架构和基础结构保持不变。该架构包括：
- en: '**Layers**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层**'
- en: '**Nodes**'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**'
- en: '**Edges**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘**'
- en: '**Biases**'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差**'
- en: '**Activation functions**'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**'
- en: The Layers
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层
- en: As mentioned before, neural networks are made up of layers. While the number
    of these layers varies from model to model and is dependent on the task at hand,
    there are only three types of layers. Each layer is made up of individual nodes
    and the number of these nodes depends on the requirement of the layer and the
    neural network as a whole. A node can be thought of as a neuron.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络由多个层组成。虽然这些层的数量因模型而异，并且依赖于当前的任务，但只有三种类型的层。每一层由多个节点组成，节点的数量取决于该层以及整个神经网络的需求。一个节点可以看作是一个神经元。
- en: 'The layers present in a neural network are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的层如下所示：
- en: '*The input layer*'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入层*'
- en: As the name suggests, this is the layer that consists of the input data entering
    the neural network. It is a mandatory layer as every neural network requires input
    data to learn from and perform operations on to be able to generate an output.
    This layer can only occur once in a neural network. Each input node is connected
    to each node present in the proceeding layer.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顾名思义，这一层由进入神经网络的输入数据组成。它是一个必需的层，因为每个神经网络都需要输入数据进行学习和执行操作，从而生成输出。此层在神经网络中只能出现一次。每个输入节点与后续层中的每个节点相连。
- en: The variables or characteristics of input data are known as features. The target
    output is dependent on these features. For example, take the iris dataset. (The
    Iris dataset is one of the most popular datasets for machine learning beginners.
    It consists of data of three different types of flowers. Each instance has four
    features and one target class.) The classification label of a flower is dependent
    on the four features—petal length and width, and sepal length and width. The features,
    and thus the input layer, is denoted by **X**, and each individual featured is
    denoted by **X1**, **X2**, ... , **Xn**.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入数据的变量或特征被称为特征。目标输出依赖于这些特征。例如，以鸢尾花数据集为例。（鸢尾花数据集是机器学习初学者中最流行的数据集之一。它包含三种不同类型花卉的数据。每个实例有四个特征和一个目标类别。）花卉的分类标签取决于四个特征——花瓣的长度和宽度，以及萼片的长度和宽度。特征，因此输入层，被表示为**X**，每个单独的特征被表示为**X1**、**X2**、...、**Xn**。
- en: '*The hidden layer*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐藏层*'
- en: This is the layer where the actual computation is done. It comes after the input
    layer, since it acts on the input provided by the input layer, and before the
    output layer, since it generates the output that is provided by the output layer.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是进行实际计算的层。它位于输入层之后，因为它作用于由输入层提供的输入，并且位于输出层之前，因为它生成由输出层提供的输出。
- en: A hidden layer is made up of nodes known as "activation nodes." Each node possesses
    an activation function, which is a mathematical function that is performed on
    the inputs received by an activation node to generate an output. Activation functions
    will be discussed later on in this chapter.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐藏层由称为“激活节点”的节点组成。每个节点拥有一个激活函数，这是一个对激活节点接收到的输入执行的数学函数，用于生成输出。本章后续会讨论激活函数。
- en: This is the only type of layer that can occur multiple times, and thus in deep
    neural networks, there can be up to hundreds of hidden layers present. The number
    of hidden layers depends on the task at hand.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是唯一可以多次出现的层，因此在深度神经网络中，可能存在最多上百个隐藏层。隐藏层的数量取决于具体任务。
- en: The output generated by the nodes of one hidden layer are fed into the proceeding
    hidden layer as input. The output generated by each activation node of a hidden
    layer is sent to each activation node of the next layer.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个隐藏层的节点生成的输出将作为输入传递到下一个隐藏层。每个隐藏层的激活节点生成的输出被发送到下一层的每个激活节点。
- en: '*The output layer*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出层*'
- en: This is the last layer of the neural network and it consists of nodes that provide
    the final outcome of all the processing and computing. This is also a mandatory
    layer since a neural network must produce an output based on input data.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是神经网络的最后一层，包含提供所有处理和计算结果的节点。这也是一个必需的层，因为神经网络必须根据输入数据生成输出。
- en: In the case of the iris dataset, the output for a particular instance of a flower
    would be the category of that flower—Iris setosa, Iris virginica, or Iris versicolor.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以鸢尾花数据集为例，某一花卉实例的输出将是该花卉的类别——鸢尾花 Setosa、鸢尾花 Virginica 或鸢尾花 Versicolor。
- en: The output, often known as the target, is denoted as **y**.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出，通常称为目标，表示为**y**。
- en: '![Fig 3.3: A Neural Network with 2 Hidden Layers'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3：具有 2 个隐藏层的神经网络'
- en: '](img/C13783_03_03.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_03.jpg)'
- en: 'Fig 3.3: A Neural Network with 2 Hidden Layers'
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.3：具有 2 个隐藏层的神经网络
- en: Nodes
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点
- en: 'Each activation node or neuron possess the following components:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个激活节点或神经元具有以下组件：
- en: An activation
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活
- en: This is the current state of the node—whether it is active or not.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是节点的当前状态——它是否处于激活状态。
- en: A threshold value (optional)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值（可选）
- en: If present, this determines whether a neuron is activated or not, depending
    on whether the weighted sum is above or below this threshold value.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果存在，该值决定了一个神经元是否被激活，具体取决于加权和是否高于或低于此阈值。
- en: An activation function
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: This is what computes a new activation for the activation node based on the
    inputs and the weighted sum.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是根据输入和加权和计算激活节点的新激活值的函数。
- en: An output function
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出函数
- en: This generates the output for the particular activation node based on the activation
    function.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会根据激活函数生成特定激活节点的输出。
- en: Input neurons have no such components as they don't perform computation, nor
    do they have any preceding neurons. Similarly, output neurons don't have these
    components, since they don't perform computation, nor do they have proceeding
    neurons.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入神经元没有像这样的组件，因为它们不进行计算，也没有前置神经元。类似地，输出神经元也没有这些组件，因为它们不进行计算，也没有后续神经元。
- en: The Edges
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 边缘
- en: '![ Fig 3.4: The Weighted Connections of a Neural Network'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![ 图 3.4：神经网络的加权连接'
- en: '](img/C13783_03_04.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_04.jpg)'
- en: 'Fig 3.4: The Weighted Connections of a Neural Network'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.4：神经网络的加权连接
- en: Each of the arrows in the preceding diagram represents a connection between
    two nodes from two different layers. A connection is known as an edge. Each edge
    that leads to an activation node has its own weight, which can be considered as
    a sort of impact that one node has on the other node. Weights can be either positive
    or negative.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，每一条箭头代表两个不同层的节点之间的连接。这样的连接被称为边缘。每个指向激活节点的边缘都有自己的权重，可以视为一个节点对另一个节点的影响程度。权重可以是正的，也可以是负的。
- en: Take a look at the earlier diagram. Before the values reach the activation function,
    their values are multiplied by the weights assigned to their respective connections.
    These multiplied values are then added together to obtain a weighted sum. This
    weighted sum is basically a measure of how much impact that node has on the output.
    Thus if the value is low, that means that it doesn't really affect the output
    that much and so it's not that important. If the value is high, then it shares
    a strong correlation with the target output and thus plays a role in determining
    what the output is.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 看看之前的图示。在值到达激活函数之前，它们的值会先与分配给各自连接的权重相乘。然后这些乘积的值会相加，得到一个加权和。这个加权和本质上是衡量该节点对输出的影响程度。如果值较低，意味着它对输出的影响不大，因此也不那么重要。如果值较高，则意味着它与目标输出有强烈的相关性，因此在确定输出时起着重要作用。
- en: Biases
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏置
- en: A bias is a node, and each layer of a neural network has its own bias node,
    except for the output layer. Thus, each layer has its own bias node. The bias
    node holds a value, known as the bias. This value is incorporated in the process
    of calculating the weighted sum and so also plays a role in determining the output
    generated by a node.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置是一个节点，神经网络的每一层都有自己的偏置节点，输出层除外。因此，每一层都有自己的偏置节点。偏置节点保存一个值，称为偏置。这个值会在计算加权和的过程中被加入，因此它在确定节点生成的输出中也起到了作用。
- en: Bias is an important aspect of neural networks because it allows the activation
    function to shift either to the right or to the left. This helps the model to
    better fit the data and thus produce accurate outputs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 偏置是神经网络中的一个重要方面，因为它允许激活函数向右或向左平移。这有助于模型更好地拟合数据，从而生成准确的输出。
- en: Activation Functions
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions are functions that are part of the activation nodes found
    in the hidden layers of neural networks. They serve the purpose of introducing
    non-linearity into neural networks, which is really important, as without them
    neural networks would just have linear functions, leaving no difference between
    them and linear regression models. This defeats the purpose of neural networks,
    because then they wouldn't be able to learn complex functional relationships that
    exist within data. Activation functions also need to be differentiable for backpropagation
    to occur. This will be discussed in future sections of this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络隐含层中激活节点的一部分。它们的作用是为神经网络引入非线性，这是非常重要的，因为没有它们，神经网络将只有线性函数，这样就和线性回归模型没有区别了。这就违背了神经网络的初衷，因为没有非线性，神经网络就无法学习数据中的复杂函数关系。激活函数还需要是可微的，以便进行反向传播。这个内容将在本章的后续部分讨论。
- en: Basically, an activation node calculates the weighted sum of the inputs it receives,
    adds the bias, and then applies an activation function to this value. This generates
    an output for that particular activation node which is then used as input by the
    proceeding layer. This output is known as an activation value. Therefore, the
    proceeding activation node in the next layer will receive multiple activation
    values from preceding activation nodes and calculate a new weighted sum. It will
    apply its activation function to this value to generate its own activation value.
    This is how data flows through a neural network. Thus, an activation function
    helps convert an input signal into an output signal.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，一个激活节点计算它接收到的输入的加权和，添加偏置值，然后对这个值应用激活函数。这会为该特定激活节点生成一个输出，该输出随后作为输入传递给下一层。这个输出被称为激活值。因此，下一层的激活节点将接收到来自前一层激活节点的多个激活值，并计算一个新的加权和。它会对这个值应用激活函数，生成它自己的激活值。这就是数据在神经网络中流动的方式。因此，激活函数帮助将输入信号转换为输出信号。
- en: This process of calculating the weighted sum, applying an activation function,
    and producing an activation value is known as feedforward.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 计算加权和、应用激活函数并产生激活值的过程称为前向传播。
- en: There are several activation functions (Logistic, TanH, ReLU, and so on). The
    Sigmoid function is one of the most popular and simple activation functions out
    there. When represented mathematically, this function looks like
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种激活函数（如Logistic、TanH、ReLU等）。Sigmoid函数是其中最流行且最简单的激活函数之一。当用数学形式表示时，这个函数看起来像
- en: '![Figure 3.5: Expression for sigmoid function'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5：Sigmoid函数的表达式'
- en: '](img/C13783_03_05.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_05.jpg)'
- en: 'Figure 3.5: Expression for sigmoid function'
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.5：Sigmoid函数的表达式
- en: As you can see, this function is non-linear.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个函数是非线性的。
- en: Training a Neural Network
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: So far, we know that once an input is provided to a neural network, it enters
    the input layer which is an interface that exists to pass on the input to the
    next layer. If a hidden layer is present, then the inputs are sent to the activation
    nodes of the hidden layer via weighted connections. The weighted sum of all the
    inputs received by the activations nodes is calculated by multiplying the inputs
    with their respective weights and adding these values up along with the bias.
    The activation function generates an activation value from the weighted sum and
    this is passed on to the nodes in the next layer. If the next layer is another
    hidden layer, then it uses the activation values from the previous hidden layer
    as inputs and repeats the activation process. However, if the proceeding layer
    is the output layer, then the output is provided by the neural network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道，一旦输入提供给神经网络，它会进入输入层，这是一个用于将输入传递到下一层的接口。如果存在隐藏层，则输入会通过加权连接发送到隐藏层的激活节点。激活节点接收到的所有输入的加权和是通过将输入与各自的权重相乘，然后将这些值加上偏置值来计算的。激活函数从加权和中生成激活值，并将其传递到下一层的节点。如果下一层是另一个隐藏层，则它将使用来自前一隐藏层的激活值作为输入，并重复激活过程。然而，如果下一层是输出层，则神经网络会提供输出。
- en: From all of this information, we can conclusively say that there are three parts
    of the deep learning model that have an impact on the output generated by the
    model—the inputs, the connection weights and biases, and the activation functions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些信息中，我们可以得出结论，深度学习模型中有三个部分会影响模型生成的输出——输入、连接权重和偏置、以及激活函数。
- en: '![Figure 3.6: Aspects of a deep learning model that impact the output'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6：影响输出的深度学习模型方面'
- en: '](img/C13783_03_06.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_06.jpg)'
- en: 'Figure 3.6: Aspects of a deep learning model that impact the output'
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.6：影响输出的深度学习模型方面
- en: While the inputs are taken from the dataset, the former two are not. Thus, the
    following two questions arise—who or what decides what the weight is for a connection?
    How do we know which activation functions to use? Let's tackle these questions
    one by one.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输入来自数据集，但前两个部分不是。那么，接下来就会有两个问题：谁或什么决定连接的权重是多少？我们怎么知道该使用哪些激活函数？让我们逐一解决这些问题。
- en: Calculating Weights
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算权重
- en: Weights play a very important role in multilayer neural networks, since altering
    the weight of a single connection can completely alter the weights assigned to
    further connections and thus the outputs generated by the proceeding layers. Thus,
    having the optimal weights is necessary to create an accurate deep learning model.
    This sounds like a lot of pressure, but lucky for us, deep learning models are
    capable of finding the optimal weights all on their own. To understand this better,
    let's take the example of linear regression.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 权重在多层神经网络中起着非常重要的作用，因为改变单一连接的权重会完全改变分配给进一步连接的权重，从而影响后续层生成的输出。因此，拥有最优的权重对于创建一个准确的深度学习模型是必要的。听起来好像压力很大，但幸运的是，深度学习模型能够自主找到最优的权重。为了更好地理解这一点，让我们以线性回归为例。
- en: 'Linear regression is a supervised machine learning algorithm that, as suggested
    by the name itself, is suitable to solve regression problems (datasets whose output
    is in the form of continuous numerical values, such as the selling prices of houses).
    This algorithm assumes there exists a linear relationship between the input (the
    features) and the output (the target). Basically, it believes that there exists
    a line of best fit that accurately describes the relationship between the input
    and output variables. It uses this to predict future numerical values. In a scenario
    where there is only one input feature, the equation for this line is:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种监督式机器学习算法，顾名思义，它适用于解决回归问题（输出为连续数值的数据集，例如房屋的售价）。该算法假设输入（特征）与输出（目标）之间存在线性关系。基本上，它认为存在一条最佳拟合线，可以准确描述输入和输出变量之间的关系。它使用这个关系来预测未来的数值。在只有一个输入特征的情况下，这条线的方程式为：
- en: '![Figure 3.7: Expression for linear regression'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：线性回归的表达式'
- en: '](img/C13783_03_07.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_07.jpg)'
- en: 'Figure 3.7: Expression for linear regression'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.7：线性回归的表达式
- en: Where,
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '**y** is the target output'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**y** 是目标输出'
- en: '**c** is the y-intercept'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**c** 是 y 轴截距'
- en: '**m** is the model coefficient'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**m** 是模型系数'
- en: '**x** is the input feature'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** 是输入特征'
- en: Similar to the connections in neural networks, the input features have values
    attached to them too—they're called model coefficients. In a way, these model
    coefficients determine the importance a feature has in determining the output,
    which is similar to what the weights in neural networks do. It is important to
    ensure these model coefficients are of the correct value so as to get correct
    predictions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于神经网络中的连接，输入特征也附带了数值——它们被称为模型系数。在某种程度上，这些模型系数决定了特征在确定输出中的重要性，这类似于神经网络中的权重作用。确保这些模型系数的值正确是非常重要的，以便获得准确的预测。
- en: Let's say that we want to predict the selling price of a house based on how
    many bedrooms it has. So, the price of the house is our target output and the
    number of bedrooms it has is our input feature. Since this is a supervised learning
    method, our model will be fed a dataset that contains instances of our input feature
    matched with the correct target output.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想预测房屋的售价，依据是它有多少个卧室。所以，房屋的售价是我们的目标输出，卧室的数量是我们的输入特征。由于这是一个监督学习方法，我们的模型将被提供一个数据集，其中包含输入特征与正确的目标输出的匹配实例。
- en: '![Fig 3.8: Sample Dataset for Linear Regression'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8：线性回归的样本数据集'
- en: '](img/C13783_03_08.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_08.jpg)'
- en: 'Fig 3.8: Sample Dataset for Linear Regression'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.8：线性回归的样本数据集
- en: Now, our linear regression model needs to find a model coefficient that describes
    the impact of the number of bedrooms on the selling price of the house. It does
    this by making use of two algorithms—the loss function and the gradient descent
    algorithm.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的线性回归模型需要找到一个模型系数，用来描述卧室数量对房屋售价的影响。它通过使用两种算法——损失函数和梯度下降算法——来实现这一目标。
- en: The Loss Function
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失函数
- en: The loss function is also sometimes known as the cost function.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数有时也被称为成本函数。
- en: For classification problems, the loss function calculates the difference between
    the predicted probability of a particular category and the category itself. For
    example, let's say you have a binary classification problem that needs to predict
    whether a house will be sold or not. There are only two outputs—"yes" and "no."
    A classification model when fitted on this data will predict the probability of
    an instance of data falling in either the "yes" category or the "no" category.
    Let's say the "yes" category has a value of 1, and "no" has a value of 0\. Thus,
    if the output probability is closer to 1 it will fall in the "yes" category. The
    loss function for this model will measure this difference.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，损失函数计算特定类别的预测概率与该类别本身之间的差异。例如，假设你有一个二分类问题，需要预测一座房子是否会售出。只有两个输出——“是”和“否”。在拟合这个数据的分类模型时，模型会预测数据实例属于“是”类别或“否”类别的概率。假设“是”类别的值为1，“否”类别的值为0。因此，如果输出概率更接近1，则它会落入“是”类别。该模型的损失函数将衡量这种差异。
- en: For regression problems, the loss function calculates the error between actual
    values and predicted values. The house price example from the previous section
    is a regression problem and so the loss function is calculating the error between
    the actual price of a house, and the price that our model predicted. Thus, in
    a way, the loss function helps the model self-evaluate its performance. Obviously,
    the model's aim is to predict the price that is exactly, if not closest to, the
    actual price. To do this, it needs to minimize the loss function as much as possible.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，损失函数计算实际值与预测值之间的误差。上一节中的房价例子是一个回归问题，因此损失函数计算的是房子的实际价格与模型预测的价格之间的误差。因此，从某种意义上说，损失函数帮助模型自我评估其性能。显然，模型的目标是预测一个与实际价格完全相同，或者至少最接近的价格。为了做到这一点，它需要尽可能地最小化损失函数。
- en: The only factor that is directly affecting the price predicted by the model
    is the model coefficient. To arrive at the model coefficient that is best suited
    for the problem at hand, the model needs to keep improving the values for the
    model coefficient. Let's call each different value an update of the model coefficient.
    So, with each update of the model coefficient, the model must calculate the error
    between the actual price and the price that the model has predicted using that
    update of the model coefficient.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一直接影响模型预测价格的因素是模型系数。为了得到最适合当前问题的模型系数，模型需要不断改进模型系数的值。我们将每个不同的值称为模型系数的更新。因此，随着每次模型系数的更新，模型必须计算实际价格与使用该模型系数更新后的预测价格之间的误差。
- en: Once the function has reached its minimum value, the model coefficient at this
    minimum point is chosen as the final model coefficient. This value is stored and
    used in the linear equation described above by the linear regression algorithm.
    From that point onwards, whenever the model is fed input data in the form of how
    many bedrooms a house has without target outputs, it uses the linear equation
    with the apt model coefficient to calculate and predict the price that that house
    will be sold at.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦该函数达到了最小值，模型系数在此最小点的值被选为最终的模型系数。该值被存储，并在上述线性回归算法的线性方程中使用。从此之后，每当模型接收到房子的卧室数量等输入数据而没有目标输出时，它会使用带有适当模型系数的线性方程来计算并预测这座房子将以多少价格售出。
- en: There are many different kinds of loss functions—such as MSE (for regression
    problems) and Log Loss (for classification problems). Let's take a look at how
    they work.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同种类的损失函数——例如 MSE（用于回归问题）和 Log Loss（用于分类问题）。让我们来看看它们是如何工作的。
- en: 'The Mean Squared Error function calculates the difference between the actual
    values and the predicted values, squares this difference, and then averages it
    out across the entire dataset. The function, when expressed mathematically, looks
    like this:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差函数计算实际值与预测值之间的差异，将其平方后，再对整个数据集取平均。该函数用数学表达式表示如下：
- en: '![Figure 3.9: Expression for mean squared error function'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9：均方误差函数的表达式'
- en: '](img/C13783_03_09.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_09.jpg)'
- en: 'Figure 3.9: Expression for mean squared error function'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.9：均方误差函数的表达式
- en: Where,
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '**n** is the total number of data points'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**n** 是数据点的总数'
- en: '**yi** is the ith actual value'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**yi** 是第i个实际值'
- en: '**xi** is the input'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**xi** 是输入'
- en: '**f()** is the function being carried out on the input to generate the output,
    therefore'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**f()** 是对输入执行的函数，用来生成输出，因此'
- en: '**f(xi)** is the predicted value'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**f(xi)** 是预测值'
- en: 'Log loss is used for classification models whose output is a probability value
    in the range of 0 and 1\. The higher the difference between the predicted probability
    and the actual category, the higher the log loss. The mathematical representation
    of the log loss functions is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失用于输出为0到1之间概率值的分类模型。预测概率与实际类别之间的差异越大，对数损失越高。对数损失函数的数学表示为：
- en: '![Figure 3.10: Expression for log loss function'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10：对数损失函数的表达式'
- en: '](img/C13783_03_10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_10.jpg)'
- en: 'Figure 3.10: Expression for log loss function'
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.10：对数损失函数的表达式
- en: Where,
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '**N** is the total number of data points'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**N** 是数据点的总数'
- en: '**yi** is the ith actual label'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**yi** 是第i个实际标签'
- en: '**p** is the predicted probability'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**p** 是预测概率'
- en: The Gradient Descent Algorithm
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度下降算法
- en: The process of evaluating the model's performance via the loss function is one
    that the model carries out independently, as is the process for updating and ultimately
    choosing the model coefficients.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过损失函数评估模型性能的过程是由模型独立执行的，更新并最终选择模型系数的过程也是如此。
- en: Imagine that you're on a mountain and you want to climb back down and reach
    the absolute bottom. It's cloudy and there are quite a few peaks so you can't
    exactly see where the bottom is, or which direction it is in, you just know that
    you need to get there. You start your journey at 5000 meters above sea level,
    and you decide to take large steps. You take a step and then you check your phone
    to see how many meters above sea level you are. Your phone says you are 5003 meters
    above sea level, which means you've gone in the wrong direction. Now, you take
    a large step in another direction and your phone says you are 4998 meters above
    sea level. This means you're getting closer to the bottom, but how do you know
    that this step was the one with the steepest descent? What if you took a step
    in another direction that brought you down to 4996 meters above sea level? Thus,
    you check your position after taking a step in each possible direction, and whichever
    takes you closest the bottom, is the one you choose.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在一座山上，想要下山并到达真正的底部。天空多云，山峰众多，你无法确切知道底部在哪儿，也不知道应该朝哪个方向走，你只知道你需要到达那里。你从海拔5000米的地方开始，决定迈大步。你迈出一步，然后检查手机，看看自己距离海平面有多少米。手机显示你距离海平面5003米，说明你走错了方向。现在，你朝另一个方向迈大步，手机显示你距离海平面4998米。这意味着你离底部更近了，但你怎么知道这一步是下降最快的那一步呢？如果你朝另一个方向走，发现自己降到了4996米呢？因此，你会检查每个可能方向上的位置，选择那个最接近底部的方向。
- en: You keep repeating this process, and then you reach a point where your phone
    says you are 100 meters above sea level. When you take another step, your phone's
    reading remains the same—100 meters above sea level. Finally, you have reached
    what seems to be the bottom since a step in any direction from this point results
    in you still being 100 meters above sea level.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你不断重复这个过程，直到你的手机显示你位于海拔100米的地方。当你再迈出一步时，手机的读数仍然保持不变——海拔100米。最终，你到达了一个看起来像是底部的地方，因为从这个点出发的任何方向，都会导致你依然处于海拔100米的位置。
- en: '![Fig 3.11: Updating Parameters'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11：更新参数'
- en: '](img/C13783_03_11.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_11.jpg)'
- en: 'Fig 3.11: Updating Parameters'
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.11：更新参数
- en: This is how the gradient descent algorithm works. The algorithm descends a plot
    of the loss function against possible values for the model coefficient and the
    y-intercept, like you descended the mountain. It starts off with an assigned value
    for the model coefficient—this is you standing at a point 5000 meters above sea
    level. It calculates the gradient of the plot at this point. This gradient tells
    the model which direction it should move in to update the coefficient in order
    to get closer to the global minimum, which is the end goal. So, it takes a step
    and arrives at a new point with a new model coefficient. It repeats the process
    of calculating the gradient, obtaining a direction to move in, updating the coefficient,
    and taking another step. It checks to see that this step is the one that provides
    it with the steepest descent. With each step that it takes, it arrives at a new
    model coefficient and calculates the gradient at that point. This process is repeated
    until the value of the gradient doesn't change for a number of trials. This means
    that the algorithm has reached the global minimum and has converged. The model
    coefficient at this point is used as the final model coefficient in the linear
    equations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是梯度下降算法的工作原理。该算法沿着损失函数与模型系数和截距的可能值的图形下降，就像你在下山一样。它从给定的模型系数值开始——这就像你站在海平面上方5000米的某个点。它会计算该点处图形的梯度。这个梯度告诉模型应该朝哪个方向移动，以更新系数，进而接近全局最小值，这也是最终目标。因此，它采取一步，来到了一个新点，拥有了新的模型系数。它重复计算梯度、获取移动方向、更新系数并采取一步的过程。它检查是否这一步提供了最陡的下降。每次它迈出一步，都到达一个新的模型系数，并计算该点的梯度。这个过程会重复，直到梯度的值在多次试验中不再变化。这意味着算法已经达到了全局最小值并且收敛。此时的模型系数会作为线性方程中的最终模型系数。
- en: In neural networks, the gradient descent algorithm and loss function work together
    to find values to be assigned to connections as weights and to biases. These values
    are updated by minimizing the loss function using the gradient descent algorithm,
    as is the case in linear regression models. Additionally, with the case of linear
    regression, there is always only one minimum, due to the fact that the loss function
    is bowl shaped. This makes it easy for the gradient descent algorithm to find
    it and be sure that this is the lowest point. In the case of neural networks,
    however, it is not that simple. The activation functions used by neural networks
    serve the purpose of introducing non-linearity to the situation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，梯度下降算法和损失函数共同作用，找到分配给连接的权重和偏置的值。这些值通过最小化损失函数来更新，使用的是梯度下降算法，就像线性回归模型中一样。此外，在线性回归的情况下，由于损失函数是碗形的，因此总是只有一个最小值。这使得梯度下降算法很容易找到它，并且可以确定这是最低点。然而，在神经网络的情况下，情况并不如此简单。神经网络使用的激活函数目的在于引入非线性因素。
- en: As a result, the plot of the loss function of a neural network is not a bowl-shaped
    curve, and this does not have just one minimum point. Instead, it has several
    minimums, only one of which is the global minima. The rest are known as local
    minima. This sounds like a major issue, but it is, in fact, alright for the gradient
    descent algorithm to reach a local minima and choose the weight values at that
    point, due to the fact that most local minima are usually quite close to the global
    minimum. There are modified versions of the gradient descent algorithm that are
    also used when designing neural networks. Stochastic and batch-sized gradient
    descent are two of them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络的损失函数图像并不是碗形曲线，并且它不只有一个最小点。相反，它有多个最小值，其中只有一个是全局最小值，其余的被称为局部最小值。这听起来像是一个重大问题，但实际上，梯度下降算法能够达到一个局部最小值并选择该点的权重值是没问题的，因为大多数局部最小值通常离全局最小值非常近。为了设计神经网络，也有一些修改版的梯度下降算法被使用。随机梯度下降和批量梯度下降就是其中的两种。
- en: Let's say our loss function is MSE, and we need the gradient descent algorithm
    to update one weight (w) and one bias (b).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的损失函数是均方误差（MSE），并且我们需要梯度下降算法更新一个权重（w）和一个偏置（b）。
- en: '![Figure 3.12: Expression for gradient of loss function'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12：损失函数梯度的表达式'
- en: '](img/C13783_03_12.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_12.jpg)'
- en: 'Figure 3.12: Expression for gradient of loss function'
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.12：损失函数梯度的表达式
- en: 'The gradient is the partial derivative of the loss function, with respect to
    the weight and the bias. The mathematical representation of this is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是损失函数对权重和偏置的偏导数。其数学表示如下：
- en: '![Figure 3.13: Expression of gradient with partial derivaive of loss function'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13：损失函数偏导数的梯度表示'
- en: '](img/C13783_03_13.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_13.jpg)'
- en: 'Figure 3.13: Expression of gradient with partial derivaive of loss function'
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.13：损失函数偏导数的梯度表示
- en: The result of this is the gradient of the loss function at the current position.
    This also tells us which direction we should move in to continue updating the
    weight and the bias.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这样得到的结果是当前点的损失函数的梯度。这也告诉我们应该朝哪个方向移动，以继续更新权重和偏置。
- en: The size of the step taken is adjusted by a parameter called the learning rate
    and is a very sensitive parameter in the gradient descent algorithm. It is called
    alpha and is denoted by α. If the learning rate is too small, then the algorithm
    will take too many tiny steps and thus take too long to reach the minimum. However,
    if the learning rate is too large then the algorithm might miss the minimum altogether.
    Thus, it is important to tweak and test out the algorithm using different learning
    rates to ensure the right one is chosen.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步的步长大小是由一个称为学习率的参数来调整的，它是梯度下降算法中一个非常敏感的参数。它被称为 alpha，并用符号 α 表示。如果学习率太小，算法会采取过多的微小步骤，从而需要很长时间才能达到最小值。然而，如果学习率过大，算法可能会完全错过最小值。因此，调整并测试不同的学习率以确保选择正确的学习率非常重要。
- en: 'The learning rate is multiplied with the gradient calculated at each step in
    order to modify the size of the step, thus the step size of each step is not always
    the same. Mathematically, this looks like:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率与每一步计算出的梯度相乘，以修改步长的大小，因此每一步的步长不一定相同。数学上，这可以表示为：
- en: '![Figure 3.14: Expression for learning rate multipled with gradient'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14：学习率与梯度相乘的表达式'
- en: '](img/C13783_03_14.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_14.jpg)'
- en: 'Figure 3.14: Expression for learning rate multiplied with gradient'
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.14：学习率与梯度相乘的表达式
- en: And,
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以及，
- en: '![Figure 3.15: Expression for learning rate multipled with gradient at each
    step'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15：每一步学习率与梯度相乘的表达式'
- en: '](img/C13783_03_15.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_15.jpg)'
- en: 'Figure 3.15: Expression for learning rate multiplied with gradient at each
    step'
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.15：每一步学习率与梯度相乘的表达式
- en: The values are subtracted from the previous values of the weight and bias because
    the partial derivatives point in the direction of the steepest ascent, but our
    aim is to descend.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值是从之前的权重和偏置值中减去的，因为偏导数指向的是最陡上升的方向，而我们的目标是下降。
- en: '![Fig 3.16: Learning Rate'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16：学习率'
- en: '](img/C13783_03_16.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_16.jpg)'
- en: 'Fig 3.16: Learning Rate'
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.16：学习率
- en: Backpropagation
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播
- en: Linear regression is basically a neural network, but without a hidden layer
    and with an identity activation function (which is a linear function, therefore
    linearity). Hence, the learning process remains the same as the one described
    in the previous sections—the loss function aims to minimize the error by having
    the gradient descent algorithm constantly update the weights till the global minimum
    is reached.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归本质上是一个神经网络，只不过没有隐藏层，并且激活函数是恒等函数（即线性函数，因此是线性的）。因此，学习过程与前面几节描述的相同——损失函数的目标是通过让梯度下降算法不断更新权重，直到达到全局最小值，从而最小化误差。
- en: However, when dealing with larger, more complicated neural networks that are
    not linear in nature, the loss calculated is sent back through the network to
    each layer, which then begins the process of weight updating again. The loss is
    propagated backwards, therefore this is known as backpropagation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理更大、更复杂的非线性神经网络时，计算出的损失会通过网络反向传播到每一层，然后开始更新权重的过程。损失被反向传播，因此这一过程被称为反向传播（Backpropagation）。
- en: Backpropagation is performed using the partial derivatives of the loss function.
    It involves calculating the loss of every node in every layer by propagating backwards
    in the neural network. Knowing the loss of every node allows the network to understand
    which weights are having a drastic negative impact on the output and the loss.
    Thus, the gradient descent algorithm is able to reduce the weights of these connections
    that have high error rates, consequently reducing the impact that that node has
    on the network's output.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是通过使用损失函数的偏导数来执行的。它涉及到通过在神经网络中反向传播来计算每个层中每个节点的损失。了解每个节点的损失可以让网络理解哪些权重对输出和损失产生了剧烈的负面影响。因此，梯度下降算法可以减少这些连接的权重，这些连接的错误率较高，从而减少该节点对网络输出的影响。
- en: 'When dealing with many layers in a neural network, there are many activation
    functions working on the inputs. This can be represented as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理神经网络中的多层时，许多激活函数作用于输入。这个过程可以表示为如下：
- en: '![Figure 3.17: Expression for backpropagation function'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.17：反向传播函数的表达式'
- en: '](img/C13783_03_17.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_17.jpg)'
- en: 'Figure 3.17: Expression for backpropagation function'
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.17：反向传播函数的表达式
- en: Here **X**, **Y**, and **Z** are activation functions. As we can see, **f(x)**
    is a composite function, thus, backpropagation can be seen as an application of
    the chain rule. The chain rule is the formula used to calculate the partial derivatives
    of a composite function, which is what we're doing through backpropagation. Therefore,
    by applying the chain rule to the preceding function (known as the forward propagation
    function since values are moving in the forward direction to generate an output)
    and calculating the partial derivatives with respect to each weight, we will be
    able to determine exactly how much of an impact each node has on the final output.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里**X**、**Y**和**Z**是激活函数。正如我们所看到的，**f(x)**是一个复合函数，因此，反向传播可以视为链式法则的应用。链式法则是用于计算复合函数偏导数的公式，这正是我们在反向传播过程中所做的。通过将链式法则应用于前述的函数（通常称为前向传播函数，因为数值朝着前向方向流动以生成输出），并计算相对于每个权重的偏导数，我们将能够精确地确定每个节点对最终输出的影响程度。
- en: The loss of the final node present in the output layer is the total loss of
    the entire neural network, because it is in the output layer and so the loss of
    all the previous nodes gets accumulated. The input nodes present in the input
    layer do not have a loss because they don't have an impact on the neural network.
    The input layer is merely an interface that sends the input to the activation
    nodes present in the hidden layers.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层中最后一个节点的损失是整个神经网络的总损失，因为它位于输出层，因此所有前面节点的损失都会累积到这一层。输入层中的输入节点没有损失，因为它们对神经网络没有影响。输入层仅仅是一个接口，将输入发送到隐藏层中的激活节点。
- en: Therefore, the process of backpropagation is the process of updating the weights
    using the gradient descent algorithm and the loss function.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，反向传播过程就是通过梯度下降算法和损失函数来更新权重的过程。
- en: Note
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on the math of backpropagation, click here: https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于反向传播的数学原理，请点击此链接：https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html
- en: Designing a Neural Network and Its Applications
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计神经网络及其应用
- en: 'Common machine learning techniques are used when training and designing a neural
    network. Neural networks can be classified as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和设计神经网络时，通常使用一些常见的机器学习技术。神经网络可以被分类为：
- en: Supervised neural networks
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督神经网络
- en: Unsupervised neural networks
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督神经网络
- en: Supervised neural networks
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有监督神经网络
- en: These are like the example used in the previous section (predicting the price
    of the house based on how many rooms it has). Supervised neural networks are trained
    on datasets consisting of sample inputs with their corresponding outputs. These
    are suitable for noise classification and making predictions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些就像前一节中使用的例子（根据房间数量预测房屋价格）。有监督神经网络是在由样本输入和其对应输出组成的数据集上进行训练的。这些方法适用于噪声分类和预测任务。
- en: 'There are two types of supervised learning methods:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的监督学习方法：
- en: Classification
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: This is for problems that have discrete categories or classes as target outputs,
    for example the Iris dataset. The neural network learns from sample inputs and
    outputs how to correctly classify new data.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是针对那些目标输出为离散类别或类的问题，例如鸢尾花数据集。神经网络从样本输入和输出中学习如何正确分类新数据。
- en: Regression
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: This is for problems that have a range of continuous numerical values as target
    outputs, like the price of a house example. The neural network describes the causal
    relationship between the inputs and their outputs.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是针对那些目标输出为一系列连续数值的问题，比如房价的例子。神经网络描述了输入与输出之间的因果关系。
- en: Unsupervised neural networks
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督神经网络
- en: These neural networks are trained on data without any target output, and thus
    are able to recognize and draw out patterns and inferences from the data. This
    makes them well-suited for tasks such as identifying category relationships and
    discovering natural distributions in data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经网络是在没有任何目标输出的数据上进行训练的，因此能够识别并提取数据中的模式和推断。这使得它们非常适合执行如识别类别关系和发现数据中自然分布等任务。
- en: Clustering
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: A cluster analysis is the grouping together of similar inputs. These neural
    networks can be used for gene sequence analysis and object recognition, amongst
    other things.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是将相似的输入分组在一起。这些神经网络可以用于基因序列分析和物体识别等任务。
- en: Neural networks that are capable of pattern recognition can be trained both
    by supervised or unsupervised methods. They play a key role in text classification
    and speech recognition.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 能够进行模式识别的神经网络可以通过监督学习或无监督学习方法进行训练。它们在文本分类和语音识别中发挥着关键作用。
- en: 'Exercise 17: Creating a neural network'
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习17：创建神经网络
- en: In this exercise, we're going to implement a simple, classic neural network,
    by following the workflow outlined earlier, to predict whether a review is positive
    or negative.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将实现一个简单的经典神经网络，通过遵循之前概述的工作流程，来预测评论是正面还是负面。
- en: This is a natural language processing problem, since the neural network is going
    to be fed rows of sentences that are actually reviews. Each review has a label
    in the training set—either 0 for negative or 1 for positive. This label is dependent
    on the words present in the review and so, our neural network needs to understand
    the meaning of the review and accordingly label it. Ultimately, our neural network
    needs to be able to predict whether a review is positive or negative.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个自然语言处理问题，因为神经网络将接收一行行的句子，这些句子实际上是评论。每个评论在训练集中都有一个标签——0表示负面，1表示正面。这个标签依赖于评论中出现的单词，因此我们的神经网络需要理解评论的含义并据此进行标注。最终，我们的神经网络需要能够预测评论是正面还是负面。
- en: Note
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Download the dataset from the link:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 从链接下载数据集：
- en: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2003
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2003
- en: The following steps will help you with the solution.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你解决这个问题。
- en: 'Open up a new Jupyter notebook by typing the following command in the directory
    you''d like to code in:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你想要编写代码的目录中，输入以下命令来打开一个新的Jupyter笔记本：
- en: '[PRE0]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, import `pandas` so that you can store the data in a dataframe:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入`pandas`，以便你可以将数据存储在数据框中：
- en: '[PRE1]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Import the regular expressions package
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入正则表达式包
- en: '[PRE2]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a function to preprocess the reviews by removing the `HTML` tags, escaped
    quotes and normal quotes:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来预处理评论，去除`HTML`标签、转义引号和普通引号：
- en: '[PRE3]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Apply this function to the reviews currently stored in your dataframe:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个函数应用于当前存储在数据框中的评论：
- en: '[PRE4]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Import `train_test_split` from `scikit-learn` to divide this data into a training
    set and a validation set:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`scikit-learn`中导入`train_test_split`，以便将这些数据分为训练集和验证集：
- en: '[PRE5]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Import `nltk` and `stopwords` from `nltk` library:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`nltk`库中导入`nltk`和`stopwords`：
- en: '[PRE6]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now machine learning and deep learning models require numerical data as input,
    and currently our data is in the form of text. Thus, we're going to use an algorithm
    called CountVectorizer to convert the words present in the reviews into word count
    vectors
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，机器学习和深度学习模型要求输入数据为数值型数据，而我们当前的数据是文本形式。因此，我们将使用一种名为CountVectorizer的算法，将评论中的单词转换为词频向量。
- en: '[PRE7]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our data is clean and prepped now!
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的数据现在已经清理并准备好了！
- en: We're going to create a two-layer neural network. When defining a neural network,
    the number of layers does not include the input layer since it's a given that
    an input layer exists and because the input layer isn't a part of the computation
    process. So, a two-layer neural network includes an input layer, one hidden layer
    and an output layer.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个两层的神经网络。在定义神经网络时，层数不包括输入层，因为输入层是默认存在的，而且输入层不参与计算过程。因此，一个两层的神经网络包括一个输入层，一个隐藏层和一个输出层。
- en: 'Import the model and the layers from Keras:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras 导入模型和层：
- en: '[PRE8]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Initiate the neural network:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化神经网络：
- en: '[PRE9]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add the hidden layer. Specify the number of nodes the layer will have the activation
    function the nodes possess and what the input for the layer is:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加隐藏层。指定该层的节点数量、节点的激活函数以及该层的输入：
- en: '[PRE10]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Add the output layer. Once again, specify the number of nodes and the activation
    function. We're going to use the `sigmoid` function here because this is a binary
    classification problem (predicting whether a review is positive or negative).
    We're going to have only one output node since the output is just one value—either
    1 or 0\.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加输出层。同样，指定节点数量和激活函数。由于这是一个二分类问题（预测评论是正面还是负面），我们将在这里使用`sigmoid`函数。我们只会有一个输出节点，因为输出只是一个值——要么是
    1，要么是 0\。
- en: '[PRE11]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''re going to compile the neural network now, and decide which loss function,
    optimization algorithm and performance metric we want to use. Since the problem
    is a binary classification one, we''re going to use `binary_crossentropy` as our
    `loss` function. The optimization algorithm is basically the gradient descent
    algorithm. Different versions and modifications of gradient descent exist. In
    this case, we''re going to use the `Adam` algorithm, which is an extension of
    stochastic gradient descent:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们要编译神经网络，并决定使用哪个损失函数、优化算法和性能指标。由于这是一个二分类问题，我们将使用`binary_crossentropy`作为我们的`loss`函数。优化算法基本上是梯度下降算法。梯度下降有不同的版本和变体。在这种情况下，我们将使用`Adam`算法，这是随机梯度下降的扩展：
- en: '[PRE12]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s summarize our model and see what''s going on:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下我们的模型，看看发生了什么：
- en: '[PRE13]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output you''ll get will look something like this:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到的输出将类似于这样：
- en: '![Figure 3.18: Model summary ](img/C13783_03_18.jpg)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 3.18: 模型摘要](img/C13783_03_18.jpg)'
- en: 'Figure 3.18: Model summary'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.18: 模型摘要'
- en: 'Now, it''s time to train the model. Fit the neural network on the `X_train`
    and `y_train` data we had divided earlier:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候训练模型了。使用我们之前划分的`X_train`和`y_train`数据来拟合神经网络：
- en: '[PRE14]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That's it! Our neural network is now ready for testing.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就这样！我们的神经网络现在准备好进行测试了。
- en: 'Transform the input validation data into word count vectors and evaluate the
    neural network. Print the accuracy score to see how your network is doing:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入验证数据转换为词频向量并评估神经网络。打印准确率分数，看看网络的表现如何：
- en: '[PRE15]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Your score might be a little different, but it should be close to 0.875\.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的分数可能会稍有不同，但应该接近 0.875\。
- en: Which is a pretty good score. So, there you have it. You just created your first
    ever neural network, trained it, and validated it.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个相当不错的分数。所以，就是这样。你刚刚创建了你的第一个神经网络，训练了它，并验证了它。
- en: '**Expected output:**'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 3.19: Expected accuracy score'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.19: 预期准确率分数](img/C13783_03_19.jpg)'
- en: '](img/C13783_03_19.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_03_19.jpg)'
- en: 'Figure 3.19: Expected accuracy score'
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.19: 预期准确率分数'
- en: 'Save your model:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存你的模型：
- en: '[PRE16]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Fundamentals of Deploying a Model as a Service
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署模型作为服务的基础
- en: The purpose of deploying a model as a service is for other people to view and
    access it with ease, and in other ways besides just looking at your code on GitHub.
    There are different types of model deployments, depending on why you've created
    the model in the first place. You could say there are three types—a streaming
    model (one that constantly learns as it is constantly fed data and then makes
    predictions), an analytics as a service model (AaaS—one that is open for anyone
    to interact with) and an on-line model (one which is only accessible by people
    working within the same company).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 部署模型作为服务的目的是让其他人能够轻松查看和访问它，而不仅仅是通过查看你在 GitHub 上的代码。根据你创建模型的初衷，模型的部署方式有不同类型。可以说有三种类型——流式模型（一个不断学习的模型，随着不断输入数据并做出预测），分析即服务模型（AaaS——一个供任何人互动的模型）和在线模型（一个只允许公司内部人员访问的模型）。
- en: The most common way of showcasing your work is through a web application. There
    are multiple deployment platforms that aid and allow you to deploy your models
    through them, such as Deep Cognition, MLflow, and others.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 展示你工作的最常见方式是通过 Web 应用程序。有多个部署平台可以帮助你通过它们部署你的模型，如 Deep Cognition、MLflow 等。
- en: Flask is the easiest micro web framework to use to deploy your own model without
    using an existing platform. It is written in Python. Using this framework, you
    can build a Python API for your model that will easily generate predictions and
    display them for you.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Flask 是最容易使用的微型 Web 框架，可用于在不使用现有平台的情况下部署你自己的模型。它是用 Python 编写的。使用这个框架，你可以为你的模型构建一个
    Python API，该 API 将轻松生成预测并为你显示结果。
- en: 'The flow is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: Create a directory for the API
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 API 创建一个目录
- en: Copy your pre-trained neural network model to this directory
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的预训练神经网络模型复制到这个目录中。
- en: Write a program that loads this model, preprocess the input so that it matches
    the training input of your model, use the model to make predictions and prepare,
    send, display this prediction.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个程序，加载这个模型，预处理输入数据，使其与模型的训练输入匹配，使用该模型进行预测并准备、发送、显示这个预测结果。
- en: To test and run the API, you simply need to type the applications name along
    with **.run()**.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 测试和运行 API 时，你只需键入应用程序名称，并加上**.run()**。
- en: In the case of the neural network we created in, we would save that model and
    load it into a new Jupyter notebook. We would convert input data (the cleaned
    reviews) into word count vectors so that the input data for our API would be the
    same as the training data. Then, we would use our models to generate predictions
    and display them.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建的神经网络的情况下，我们会保存该模型，并将其加载到一个新的 Jupyter 笔记本中。我们会将输入数据（清洗后的评论）转换为词频向量，以确保
    API 的输入数据与训练数据相同。然后，我们会使用模型生成预测并显示它们。
- en: 'Activity 4: Sentiment Analysis of Reviews'
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 4：评论情感分析
- en: In the activity, we are going to review comments from a dataset and categorize
    them as positive or negat**ive.** The following steps will help you with the solution.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将审查一个数据集中的评论，并将其分类为正面或负面。以下步骤将帮助你完成解决方案。
- en: Note
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will find the dataset at the following link:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接找到数据集：
- en: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2004
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/TrainingByPackt/Deep-Learning-for-Natural-Language-Processing/tree/master/Lesson%2004
- en: Open a new `Jupyter` notebook. Import the dataset.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的`Jupyter`笔记本。导入数据集。
- en: Import the necessary Python packages and necessary classes. Load the dataset
    in a dataframe.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的 Python 包和类。将数据集加载到数据框中。
- en: Import the necessary libraries to clean and prepare the data. Create an array
    for your cleaned text to be stored in. Using a `for` loop, iterate through every
    instance (every review).
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库来清洗和准备数据。创建一个数组来存储清洗后的文本。使用`for`循环，遍历每个实例（每条评论）。
- en: Import CountVectorizer and convert the words into word count vectors. Create
    an array to store each unique word as its own column, hence making them independent
    variables.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 CountVectorizer 并将单词转换为词频向量。创建一个数组来存储每个独特单词作为单独的列，从而使它们成为独立变量。
- en: Import necessary label encoding entities.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的标签编码实体。
- en: Divide the dataset into training and testing sets.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和测试集。
- en: Create the neural network model.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络模型。
- en: Train the model and validate it.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并验证它。
- en: Evaluate the neural network and print the accuracy scores to see how it's doing.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估神经网络并打印准确率评分，查看它的表现如何。
- en: '**Expected output:**'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 3.20: Accuracy score'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20：准确率评分'
- en: '](img/C13783_03_20.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_03_20.jpg)'
- en: 'Figure 3.20: Accuracy score'
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.20：准确率评分
- en: Note
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 302.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 活动的解决方案可以在第 302 页找到。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to a subset of machine learning—deep learning.
    You learned about the differences and similarities between the two categories
    of techniques and understood the requirement for deep learning and its applications.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了机器学习的一个子集——深度学习。你了解了这两种技术类别之间的异同，并理解了深度学习的需求及其应用。
- en: Neural networks are artificial representations of the biological neural networks
    that are present in the human brain. Artificial neural networks are frameworks
    that are incorporated by deep learning models and have proven to be increasingly
    efficient and accurate. They are used in several fields, from training self-driving
    cars to detecting cancer cells in very early stages.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是对人脑中生物神经网络的人工表示。人工神经网络是深度学习模型中所采用的框架，已经证明它们在效率和准确性上不断提高。它们被应用于多个领域，从训练自动驾驶汽车到在非常早期阶段检测癌细胞。
- en: We studied the different components of a neural network and learned a network
    trains and corrects itself, with the help of the loss function, the gradient descent
    algorithm and backpropagation. You also learned how to perform sentiment analysis
    on text inputs! Furthermore, you learned the basics of deploying a model as a
    service.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了神经网络的不同组件，并了解了在损失函数、梯度下降算法和反向传播的帮助下，网络如何进行自我训练和修正。你还学会了如何对文本输入进行情感分析！此外，你还学习了将模型部署为服务的基本知识。
- en: In the coming chapters, you will learn more about neural networks and their
    different types, along with which neural network to use in what situations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将了解更多关于神经网络及其不同类型的内容，并学习在不同情况下使用哪种神经网络。
