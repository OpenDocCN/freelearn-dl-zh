- en: 4\. Neural Network Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 神经网络训练
- en: This chapter describes neural network training. When we talk about "training"
    in this context, we mean obtaining the optimal weight parameters automatically
    from training data. In this chapter, we will introduce a criterion called a loss
    function; this enables a neural network to learn. The purpose of training is to
    discover the weight parameters that lead to the smallest value of the loss function.
    In this chapter, we will be introduced to the method of using the gradient of
    a function, called a gradient method, to discover the smallest loss function value.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍神经网络的训练。当我们在这个语境中谈论“训练”时，我们指的是从训练数据中自动获取最佳权重参数。在本章中，我们将介绍一种称为损失函数的标准，它使得神经网络能够学习。训练的目的是发现能够使损失函数值最小的权重参数。本章还将介绍一种通过函数梯度发现最小损失函数值的方法，这种方法被称为梯度法。
- en: Learning from Data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: The essential characteristic of a neural network is its ability to learn from
    data. Training from data means that weight parameter values can be automatically
    determined. If you have to determine all the parameters manually, it is quite
    hard work. For example, for a sample perceptron, as shown in *Chapter 2*, *Perceptrons*,
    we determined the parameter values manually while looking at the truth table.
    There are as few as three parameters. However, in an actual neural network, the
    number of parameters can range between thousands and tens of thousands. For deep
    learning with more layers, the number of parameters may reach hundreds of millions.
    It is almost impossible to determine them manually. This chapter describes neural
    network training, or how to determine parameter values from data, and implements
    a model that learns handwritten digits from the MNIST dataset with Python.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心特性是其从数据中学习的能力。从数据中学习意味着权重参数的值可以自动确定。如果你需要手动确定所有参数，这将是一项非常艰巨的任务。例如，对于一个样本感知机，如*第二章*所示的*感知机*，我们在查看真值表时手动确定了参数值。这里只有三个参数。然而，在实际的神经网络中，参数的数量可以从几千到几万不等。对于具有更多层次的深度学习，参数数量可能达到数亿。手动确定这些参数几乎是不可能的。本章描述了神经网络的训练，或者说如何从数据中确定参数值，并实现了一个使用Python从MNIST数据集学习手写数字的模型。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For a linearly separable problem, a perceptron can learn automatically from
    data. That training, when completed a finite number of times, can solve a linearly
    separable problem, which is known as "the perceptron convergence theorem." On
    the other hand, a nonlinear separation problem cannot be solved (automatically).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性可分问题，感知机可以通过数据自动学习。当训练完成一定次数时，它能够解决线性可分问题，这就是所谓的“感知机收敛定理”。另一方面，非线性分离问题是无法解决的（自动化）。
- en: Data-Driven
  id: totrans-6
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据驱动
- en: Data is critical in machine learning. Machine learning looks for an answer in
    the data, finds a pattern in the data, and tells a story based on it. It can do
    nothing without data. Therefore, "data" exists at the center of machine learning.
    We can say that this data-driven approach is a departure from a "man"-centered
    approach.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在机器学习中至关重要。机器学习在数据中寻找答案，发现数据中的模式，并基于此讲述一个故事。没有数据，它什么也做不了。因此，“数据”处于机器学习的核心。我们可以说，这种数据驱动的方法是对以“人”为中心方法的偏离。
- en: Usually, when we solve a problem—especially when we need to find a pattern—we
    must consider various things to find an answer. "This problem seems to have this
    pattern." "No, there may be a cause somewhere else." Based on our experience and
    intuition, we advance this task through trial and error. Machine learning avoids
    human intervention as much as possible. It tries to find an answer (pattern) from
    the collected data. Moreover, a neural network and deep learning have an important
    characteristic in common in that they can avoid human intervention more than traditional
    machine learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们解决一个问题时——尤其是当我们需要找出一个模式时——我们必须考虑各种因素来找到答案。“这个问题似乎有这样的模式。”“不，可能在别的地方有原因。”基于我们的经验和直觉，我们通过反复试验推进这一任务。机器学习尽量避免人为干预。它试图从收集到的数据中找到答案（模式）。此外，神经网络和深度学习有一个共同的重要特性，那就是它们能够比传统的机器学习更好地避免人为干预。
- en: Let's look at a specific problem here. Suppose that we want to implement a program
    that recognizes the number "5", for example. Let's suppose that our goal is implementing
    the program that determines whether handwritten images, as shown in *Figure 4.1*,
    are "5" or not "5". This problem seems relatively simple. What algorithm can we
    use?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个具体的问题。假设我们想实现一个识别数字"5"的程序。假设我们的目标是实现一个程序，判断手写图像（如*图4.1*所示）是"5"还是不是"5"。这个问题看起来相对简单。我们可以使用什么算法呢？
- en: '![Figure 4.1: Sample handwritten digits – how "5" is written varies from person
    to person'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1：手写数字示例——"5"的书写方式因人而异'
- en: '](img/fig04_1.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_1.jpg)'
- en: 'Figure 4.1: Sample handwritten digits – how "5" is written varies from person
    to person'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.1：手写数字示例——"5"的书写方式因人而异
- en: When you try to design a program that can classify "5" correctly, you will find
    that it is a more difficult problem than expected. We can easily recognize "5",
    but it is difficult to clarify the rule for recognizing an image as "5". As shown
    in *Figure 4.1*, how it is written differs from person to person. This tells us
    that finding the rule for recognizing "5" will be hard work and that it may take
    a lot of time.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试设计一个能正确分类"5"的程序时，你会发现这比预期的要难得多。我们可以轻松识别"5"，但很难明确识别图像为"5"的规则。如*图4.1*所示，书写方式因人而异。这告诉我们，找到识别"5"的规则将是艰苦的工作，并且可能需要大量的时间。
- en: Now, instead of "working out" the algorithm that recognizes "5" from scratch,
    we want to use data effectively to solve the problem. One of the methods we can
    use is to extract features from an image and use machine learning technology to
    learn the pattern of the features. A feature indicates a converter that is designed
    to extract essential data (important data) from input data (input image) accurately.
    The feature of an image is usually described as a vector. Famous features in the
    field of computer vision include SIFT, SURF, and HOG. You can use these features
    to convert image data into vectors and use a classifier in machine learning, such
    as SVM and KNN, to learn the converted vectors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再是从零开始"推导"出识别"5"的算法，而是希望有效利用数据来解决问题。我们可以使用的方法之一是从图像中提取特征，并使用机器学习技术来学习这些特征的模式。特征指的是一个转换器，它被设计用来准确地从输入数据（输入图像）中提取重要数据（关键数据）。图像的特征通常被描述为一个向量。在计算机视觉领域，著名的特征包括SIFT、SURF和HOG。你可以使用这些特征将图像数据转换为向量，并使用机器学习中的分类器，如SVM和KNN，来学习转换后的向量。
- en: In this machine learning approach, a "machine" discovers a pattern from the
    collected data. This can solve a problem more efficiently and reduce the burden
    on a "person" compared to when we invent an algorithm from scratch. However, we
    must note that the features that are used when images are converted into vectors
    are designed by a "man." This is because good results cannot be obtained without
    using features that are suitable for the problem (or without designing the features).
    For example, to recognize the face of a dog, a person may need to select the features
    that are different from those for recognizing "5". After all, even the approach
    of using features and machine learning may need suitable features to be selected
    by a "man," depending on the problem.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种机器学习方法中，"机器"从收集到的数据中发现一个模式。与我们从头开始发明算法相比，这可以更高效地解决问题，并减少对"人"的负担。然而，我们必须注意，当图像被转换成向量时，所使用的特征是由"人"设计的。因为没有使用适合问题的特征（或者没有设计特征），是无法获得良好结果的。例如，要识别狗的面部，可能需要选择与识别"5"不同的特征。毕竟，即使是使用特征和机器学习的方法，也可能需要根据问题选择合适的特征，这些特征仍然由"人"来选择。
- en: So far, we have discussed two approaches to machine learning problems. These
    two approaches are shown in the upper rows in *Figure 4.2*. Meanwhile, the approach
    to using a neural network (deep learning) is shown in the lower row of *Figure
    4.2*. It is represented by a block without human intervention.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了两种机器学习方法。这两种方法如*图4.2*中的上排所示。同时，使用神经网络（深度学习）的方法则显示在*图4.2*的下排。它通过一个没有人工干预的模块来表示。
- en: 'As shown in *Figure 4.2*, a neural network learns images "as they are." In
    the second approach, an example that uses features and machine learning, called
    human-designed features, are used, while in a neural network, a "machine" learns
    important features from images:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.2*所示，神经网络学习的是图像"原样"。在第二种方法中，使用特征和机器学习的示例，称为人工设计的特征，而在神经网络中，"机器"从图像中学习重要的特征：
- en: '![Figure 4.2: A paradigm shift from man-made rules to a "machine" learning
    from data –'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.2：从人工规则到从数据中学习的“机器”的范式转变 – '
- en: a block without human intervention is shown in gray
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人工干预的模块以灰色显示
- en: '](img/fig04_2.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_2.jpg)'
- en: 'Figure 4.2: A paradigm shift from man-made rules to a "machine" learning from
    data – a block without human intervention is shown in gray'
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.2：从人工规则到从数据中学习的“机器”的范式转变——没有人工干预的模块以灰色显示
- en: Note
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Deep learning is sometimes called "end-to-end machine learning." "**End-to-end**"
    means "from one end to the other end," that is, the acquisition of the desired
    result (output) from raw data (input).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习有时被称为“端到端机器学习。”“**端到端**”意味着“从一端到另一端”，也就是说，从原始数据（输入）中获取期望的结果（输出）。
- en: The advantage of a neural network is that it can solve all the problems in the
    same flow; for example, whether trying to recognize "5", a dog, or a human face,
    a neural network learns the provided data patiently, trying to discover a pattern
    in the given problem. A neural network can learn data as it is "end-to-end," regardless
    of the problem to solve.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的优势在于它可以在相同的流程中解决所有问题；例如，无论是试图识别“5”、一只狗还是一个人脸，神经网络都会耐心地学习提供的数据，努力发现在给定问题中的模式。神经网络可以“端到端”地学习数据，无论是解决什么问题。
- en: Training Data and Test Data
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据和测试数据
- en: In this chapter, we will cover neural network training, beginning with some
    best practices when handling data in machine learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍神经网络训练，从处理机器学习数据的一些最佳实践开始。
- en: In machine learning problems, we usually use **training data** and **test data**
    according to the purpose. First, we use only training data to find optimal parameters.
    Then, we use test data to evaluate the ability of the trained model. Why should
    we divide training data and test data? Because we want the generalization capability
    of the model. We must separate the training data and test data because we want
    to evaluate this **generalization** correctly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习问题中，我们通常根据目的使用**训练数据**和**测试数据**。首先，我们仅使用训练数据来寻找最优参数。然后，使用测试数据来评估训练好的模型的能力。为什么要将训练数据和测试数据分开？因为我们希望模型具备泛化能力。我们必须将训练数据和测试数据分开，因为我们要正确评估这种**泛化**能力。
- en: Generalization means the ability of unknown data (data that is not contained
    in the training data), and the ultimate goal of machine learning is to obtain
    this generalization. For example, handwritten digit recognition may be used in
    a system for reading postal codes on postcards automatically. In that case, handwritten
    digit recognition must be able to recognize the characters written by "someone."
    That "someone" is not "a specific character written by a specific person," but
    "an arbitrary character is written by an arbitrary person." Even if the model
    can distinguish only your training data well, it may have learned only specific
    characters of the person's handwriting contained in the data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化是指未知数据（不包含在训练数据中的数据）的能力，机器学习的最终目标是获得这种泛化能力。例如，手写数字识别可以用于自动读取明信片上的邮政编码。在这种情况下，手写数字识别必须能够识别“某个人”写的字符。这个“某个人”并不是“某个特定的人写的特定字符”，而是“一个任意的人写的任意字符”。即便模型能很好地区分你的训练数据，它可能只学会了数据中某个人的特定书写风格。
- en: Therefore, if you use only one dataset to learn parameters and evaluate them,
    the correct evaluation will not be provided. This results in a model that can
    handle a certain dataset well but cannot handle another one. When a model has
    become too adapted to only one dataset, **overfitting** occurs. Avoiding overfitting
    is an important challenge in machine learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你只使用一个数据集来学习参数并评估它们，那么无法提供正确的评估。这会导致一个可以很好地处理某个数据集，但无法处理另一个数据集的模型。当模型过于适应仅一个数据集时，**过拟合**就会发生。避免过拟合是机器学习中的一个重要挑战。
- en: Loss Function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'How do you answer when you are asked, "How happy are you now?". We may usually
    answer vaguely: "I am moderately happy" or "I am not very happy." You may be surprised
    if someone answers, "My current happiness score is 10.23" because the person can
    only quantify their happiness with one score. If such a person exists, the person
    may lead their life only based on their "happiness score."'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你被问到“你现在有多幸福？”时，你会怎么回答？我们通常会模糊地回答：“我挺开心的”或者“我不太开心。”如果有人回答：“我当前的幸福分数是10.23”，你可能会感到惊讶，因为这个人只能用一个分数量化自己的幸福。如果真有这样的人，他可能只根据自己的“幸福分数”来引导生活。
- en: This "happiness score" is an allegory used to illustrate some similar things
    which occur in neural network training. In neural network training, one "score"
    is used to indicate the current status. Based on the score, optimal weight parameters
    are searched for. As this person looks for an "optimal life" based on the "happiness
    score," a neural network searches for optimal parameters using "one score" as
    a guide. The score that's used in neural network training is called a **loss function**.
    Although any function can be used as the loss function, the sum of squared errors
    or a cross-entropy error is usually used.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个“幸福评分”是一个寓言，用来说明在神经网络训练中发生的一些类似现象。在神经网络训练中，使用一个“评分”来表示当前的状态。基于这个评分，搜索最佳的权重参数。就像这个人根据“幸福评分”寻找“最佳生活”，神经网络则通过“一个评分”来寻找最优的参数。在神经网络训练中使用的评分叫做**损失函数**。虽然任何函数都可以作为损失函数，但通常使用的是平方误差之和或交叉熵误差。
- en: Note
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A loss function is an index that indicates the "poorness" of a neural network's
    ability. It indicates how unfit the current neural network is for labeled data
    and how it deviates from labeled data. You may feel that it's unnatural for the
    "poorness of ability" to be the score, but you can interpret the loss function
    multiplied by a negative value as the score of the opposite of "how poor the ability
    is" (that is, the score of "how good the ability is"). "To minimize poorness of
    ability" is the same as "to maximize the goodness of ability." Therefore, the
    index of the "poorness" of ability is essentially the same as that of the "goodness"
    of ability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是一个指标，用来表示神经网络能力的“差劲”程度。它表示当前神经网络对标记数据的不适应程度，以及它与标记数据的偏差程度。你可能觉得“能力差劲”作为评分有些不自然，但你可以将损失函数乘以一个负值，解释为“能力好的评分”（即“能力好”的评分）。“最小化能力差劲”与“最大化能力好”是等价的。因此，能力的“差劲”指标与能力的“好”指标本质上是相同的。
- en: Sum of Squared Errors
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平方误差之和
- en: 'There are a few functions that are used as loss functions. Probably the most
    famous one is the **sum of squared errors**. It is expressed by the following
    equation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种函数可以用作损失函数。可能最著名的是**平方误差之和**。它通过以下方程表示：
- en: '| ![18](img/Figure_4.2a.png) | (4.1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| ![18](img/Figure_4.2a.png) | (4.1) |'
- en: 'Here, *y*k is the output of the neural network, *t*k is labeled data, and *k*
    is the number of dimensions of the data. For example, in the section, *Handwritten
    Digit Recognition*, of *Chapter 3*, *Neural networks*, *y*k, and *t*k are data
    items that consist of 10 elements:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y*k是神经网络的输出，*t*k是标记数据，*k*是数据的维度数量。例如，在*第3章：神经网络*的*手写数字识别*部分中，*y*k和*t*k是由10个元素组成的数据项：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The elements of these arrays correspond to numbers "0," "1," "2," ... in order
    from the first index. Here, the output of the neural network, y, is the output
    of a softmax function. The output of the softmax function can be interpreted as
    a probability. In this example, the probability of "0" is 0.1, that of "1" is
    0.05, that of "2" is 0.6, and so on. Meanwhile, t is labeled data. In the labeled
    data, the correct label is 1 and the other labels are 0\. Here, label "2" is 1,
    which indicates that the correct answer is "2." Setting 1 for the correct label
    and 0 for other labels is called **one-hot representation**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组的元素对应于从第一个索引开始按顺序排列的数字“0”，“1”，“2”，...。在这里，神经网络的输出y是经过softmax函数处理的输出。softmax函数的输出可以解释为一个概率。在这个例子中，“0”的概率是0.1，“1”的概率是0.05，“2”的概率是0.6，依此类推。同时，t是标记数据。在标记数据中，正确的标签是1，其他标签是0。这里，标签“2”是1，表示正确答案是“2”。将正确标签设为1，其他标签设为0，称为**独热编码表示**。
- en: 'As shown in equation (4.1), the sum of squared errors is the sum of the squares
    of the differences between the outputs of the neural network and the corresponding
    elements of the correct teacher data. Now, let''s implement the sum of squared
    errors in Python. You can implement it as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程(4.1)所示，平方误差之和是神经网络输出与正确教师数据相应元素之间差值的平方和。现在，让我们在Python中实现平方误差之和。你可以按照以下方式实现：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, the `y` and `t` arguments are NumPy arrays. Because this simply implements
    equation (4.1), we won''t explain this here. Now, we will use this function to
    perform a calculation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`y`和`t`参数是NumPy数组。由于这只是实现方程式(4.1)，我们在此不再详细解释。现在，我们将使用这个函数进行计算：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are two examples here. In the first one, the correct answer is "2", and
    the output of the neural network is the largest at "2." Meanwhile, in the second
    one, the correct answer is "2," but the output of the neural network is the largest
    at "7." As the result of this experiment shows, the loss function of the first
    example is smaller, which indicates that the difference in the labeled data is
    smaller. In other words, the sum of squared errors indicates that the output in
    the first example fits the labeled data better.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个例子。在第一个例子中，正确答案是“2”，神经网络的输出在“2”时最大。与此同时，在第二个例子中，正确答案是“2”，但神经网络的输出在“7”时最大。实验结果显示，第一个例子的损失函数较小，这表明标记数据之间的差异较小。换句话说，平方误差的和表明第一个例子的输出与标记数据更为匹配。
- en: Cross-Entropy Error
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交叉熵误差
- en: 'Other than the sum of squared errors, a **cross-entropy error** is also often
    used as a loss function. It is expressed by the following equation:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了平方误差和，**交叉熵误差**也常被用作损失函数。其表达式如下：
- en: '| ![19](img/Figure_4.2b.png) | (4.2) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| ![19](img/Figure_4.2b.png) | (4.2) |'
- en: 'Here, log indicates the natural logarithm, that is, the logarithm to the base
    of *e (log*e*)*. yk is the output of the neural network and tk is the correct
    label. In tk, only the index for the correct label is 1; the other indices are
    0 (one-hot representation). Therefore, equation (4.2) only calculates the logarithm
    of the output that corresponds to the correct label, 1\. For example, if "2" is
    the index of the correct label, and the corresponding output from the neural network
    is 0.6, a cross-entropy error is `-log 0.6 = 0.51`. If the output for "2" is 0.1,
    the error is `-log 0.1 = 2.30`. A cross-entropy error depends on the output result
    from the correct label. *Figure 4.3* shows the graph of this natural logarithm:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，log表示自然对数，即以*e (log*e*)*为底的对数。yk是神经网络的输出，tk是正确标签。在tk中，只有正确标签的索引为1；其他索引为0（独热表示法）。因此，方程式（4.2）仅计算对应正确标签的输出的对数，1。例如，如果“2”是正确标签的索引，并且神经网络对应的输出是0.6，则交叉熵误差为`-log
    0.6 = 0.51`。如果“2”的输出是0.1，则误差为`-log 0.1 = 2.30`。交叉熵误差依赖于正确标签的输出结果。*图4.3*显示了这个自然对数的图形：
- en: '![Figure 4.3: Graph of the natural logarithm y = log x'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3：自然对数 y = log x 的图形'
- en: '](img/fig04_3.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_3.jpg)'
- en: 'Figure 4.3: Graph of the natural logarithm y = log x'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.3：自然对数 y = log x 的图形
- en: As shown in *Figure 4.3*, *y* is 0 when *x* is 1, and the value of *y* is getting
    smaller as *x* approaches 0\. Therefore, since the output corresponding to the
    correct label is larger, equation (4.2) approaches 0\. When the output is 1, the
    cross-entropy error becomes 0\. When the output corresponding to the correct label
    is smaller, the value of equation (4.2) is larger.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.3*所示，*y*在* x *为1时为0，且随着*x*接近0，*y*的值变得更小。因此，由于正确标签对应的输出更大，方程式（4.2）趋向于0。当输出为1时，交叉熵误差为0。当正确标签的输出较小时，方程式（4.2）的值较大。
- en: 'Now, let''s implement a cross-entropy error:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个交叉熵误差：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, the y and t arguments are NumPy arrays. When `np.log` is calculated,
    a very small value, delta, is added. If `np.log(0)` is calculated, `-inf`, which
    indicates minus infinity, is returned. At this point, the calculation cannot be
    advanced further. To avoid this, a very small value is added so that minus infinity
    does not occur. Now, let''s use `cross_entropy_error(y, t)` for ease of calculation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，y和t参数是NumPy数组。当计算`np.log`时，会加上一个非常小的值delta。如果计算`np.log(0)`，则返回`-inf`，表示负无穷。在这种情况下，计算无法继续进行。为避免这种情况，会加上一个非常小的值，以避免负无穷的出现。现在，为了便于计算，我们使用`cross_entropy_error(y,
    t)`：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the first example, the output of the correct label is 0.6 and the cross-entropy
    error is 0.51\. In the next example, the output of the correct label is as small
    as 0.1 and the cross-entropy error is 2.3\. These results are consistent with
    what we've discussed so far.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，正确标签的输出为0.6，交叉熵误差为0.51。在下一个例子中，正确标签的输出小到只有0.1，交叉熵误差为2.3。这些结果与我们到目前为止讨论的一致。
- en: Mini-Batch Learning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小批量学习
- en: For a machine learning problem, training data is used for training. To be precise,
    it means finding the loss function for the training data and finding the parameters
    that make that value as small as possible. Therefore, all the training data must
    be used to obtain the loss function. If there are 100 pieces of training data,
    the sum of their 100 loss functions must be used as the index.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习问题，训练数据用于训练。准确地说，它意味着找到训练数据的损失函数，并找到使该值尽可能小的参数。因此，必须使用所有训练数据来获得损失函数。如果有100条训练数据，则必须将它们100个损失函数的和作为指标。
- en: 'In the example of the loss function we described earlier, the loss function
    for one piece of data was used. For a cross-entropy error, equation (4.3) can
    calculate the sum of the loss functions for all training data:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前描述的损失函数示例中，使用的是单条数据的损失函数。对于交叉熵误差，方程（4.3）可以计算所有训练数据的损失函数之和：
- en: '| ![20](img/Figure_4.3a.png) | (4.3) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![20](img/Figure_4.3a.png) | (4.3) |'
- en: Suppose that the number of data elements is N. tnk means the k-th value of the
    n-th data (ynk is the output of the neural network, and tnk is labeled data).
    Although this equation seems a little complicated, it is only an extension of
    equation (4.2), which expresses the loss function for one piece of data for N
    items of data. In the end, it is divided by `N` for normalization. Division by
    N calculates the "average loss function" per data. The average can be used as
    a consistent index, regardless of the amount of training data. For example, even
    when the number of training data elements is 1,000 or 10,000, you can calculate
    the average loss function per data element.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据元素的数量为N。tnk表示第n条数据的第k个值（ynk是神经网络的输出，tnk是标记数据）。虽然这个方程看起来有点复杂，但它只是方程（4.2）的扩展，表示对于N条数据的单条数据损失函数。最终，它除以`N`进行归一化。除以N计算每条数据的“平均损失函数”。这个平均值可以作为一致的指标，而不受训练数据量的影响。例如，即使训练数据的数量为1,000或10,000，也可以计算每个数据元素的平均损失函数。
- en: The MNIST dataset contains 60,000 items of training data. Calculating the sum
    of the loss functions for all this data takes a while. Big data sometimes contains
    millions or tens of millions of pieces of data. In that case, calculating the
    loss functions for all the data is not practical. Therefore, some data is extracted
    to approximate all the data. Also, in neural network training, some training data
    is selected, and training is conducted for each group of data, which is called
    a mini-batch (small collection). For example, 100 pieces of data are selected
    at random from 60,000 items of training data to be used for training. This training
    method is called **mini-batch training**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含60,000条训练数据。计算所有这些数据的损失函数之和需要一些时间。大数据有时包含数百万或数千万条数据。在这种情况下，计算所有数据的损失函数并不实际。因此，提取部分数据来近似所有数据。此外，在神经网络训练中，选取一些训练数据，并对每一组数据进行训练，这种方式称为小批量（mini-batch）训练。例如，从60,000条训练数据中随机选择100条数据进行训练。这种训练方式叫做**小批量训练**。
- en: 'Now, let''s write some code that selects the specified amount of data from
    the training data at random for mini-batch training. Before that, the following
    is the code for loading the MNIST dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一些代码，从训练数据中随机选择指定数量的数据进行小批量训练。在此之前，以下是加载MNIST数据集的代码：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As described in *Chapter 3*, *Neural Networks*, the `load_mnist` function loads
    the MNIST dataset. It is located in the `dataset/mnist.py` file provided with
    this book. This function loads the training and test data. By specifying the `one_hot_label=True`
    argument, you can use one-hot representation, where the correct label is 1 and
    the other labels are 0.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第三章*《神经网络》中所述，`load_mnist`函数加载了MNIST数据集。它位于本书提供的`dataset/mnist.py`文件中。该函数加载训练和测试数据。通过指定`one_hot_label=True`参数，你可以使用独热编码表示法，其中正确标签为1，其他标签为0。
- en: When you load the preceding MNIST data, you will find that the number of training
    data is 60,000 and that the input data contains 784 rows of image data (originally
    28x28). Labeled data is data with 10 rows. Therefore, the shapes of `x_train`
    and `t_train` are (60000, 784) and (60000, 10), respectively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你加载之前的MNIST数据时，你会发现训练数据的数量是60,000，输入数据包含784行图像数据（最初为28x28）。标记数据是具有10行的数据。因此，`x_train`和`t_train`的形状分别为（60000，784）和（60000，10）。
- en: 'Now, how can we extract 10 pieces of data at random from the training data?
    We can write the following code by using NumPy''s `np.random.choice()` function:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如何从训练数据中随机提取10条数据？我们可以通过使用NumPy的`np.random.choice()`函数编写以下代码：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By using `np.random.choice()`, you can select the desired number of numerals
    at random from the specified numerals. For example, `np.random.choice(60000, 10)`
    selects 10 numerals at random from the numerals between 0 and less than 60,000\.
    In the actual code, as shown here, you can obtain the indices as an array for
    selecting mini-batches:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`np.random.choice()`，你可以从指定的数字中随机选择所需数量的数字。例如，`np.random.choice(60000, 10)`会从0到小于60,000的数字中随机选择10个数字。在实际代码中，如这里所示，你可以获取索引作为一个数组，用于选择小批量：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, you can specify the randomly selected indices to extract mini-batches.
    We will use these mini-batches to calculate loss functions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以指定随机选择的索引来提取小批量数据。我们将使用这些小批量来计算损失函数。
- en: Note
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To measure television viewership, not all households, but selected ones, are
    targeted. For example, by measuring viewership among 1,000 households randomly
    selected from Tokyo, you can approximate the viewership throughout Tokyo. The
    viewership among these 1,000 households is not exactly the same as the whole viewership,
    but it can be used as an approximate value. Like the viewership described here,
    the loss function of a mini-batch is measured by using sample data to approximate
    the whole data. In short, a small group of randomly selected data (mini-batch)
    is used as the approximation of the whole training data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量电视观众人数，并不是所有家庭都参与，而是选定的家庭。例如，通过测量从东京随机选取的1,000户家庭的收视情况，你可以大致估算整个东京的收视人数。这1,000户家庭的收视情况与整个收视情况并不完全相同，但可以作为一个近似值。就像这里描述的收视情况一样，小批量的损失函数是通过使用样本数据来近似整个数据来衡量的。简而言之，随机选择的小部分数据（小批量）被用作整个训练数据的近似值。
- en: Implementing Cross-Entropy Error (Using Batches)
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现交叉熵误差（使用小批量）
- en: 'How can we use batch data such as mini-batches to implement a cross-entropy
    error? By improving the cross-entropy error we implemented earlier, which targets
    only one piece of data, we can implement it easily. Here, we will support both
    the input of a single piece of data and the input of data as batches:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用小批量数据来实现交叉熵误差？通过改进我们之前实现的交叉熵误差（仅针对一条数据），我们可以轻松实现它。这里，我们将支持单条数据输入和批量数据输入：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, `y` is the output of the neural network, and `t` is labeled data. If `y`
    is one-dimensional (that is, to calculate the cross-entropy error for one piece
    of data), the shape of the data is changed. The average cross-entropy error per
    data is calculated by normalization based on the amount of data in a batch.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`y`是神经网络的输出，`t`是标签数据。如果`y`是一维的（也就是说，要计算一条数据的交叉熵误差），则数据的形状会发生变化。每条数据的平均交叉熵误差是通过根据批量数据量进行归一化来计算的。
- en: 'If labeled data is provided as labels (not in one-hot representation format
    but as labels such as "2" and "7"), we can implement a cross-entropy error as
    follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签数据是作为标签提供的（不是以独热表示格式，而是以“2”和“7”等标签形式），我们可以如下实现交叉熵误差：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Please note that if `t` of an element is 0 in one-hot representation, its cross-entropy
    error is also `0`, and you can ignore this calculation. In other words, if you
    can obtain the output of the neural network for a correct label, you can calculate
    the cross-entropy error. Therefore, for `t` as the one-hot representation, `t
    * np.log(y)` is used, while for `t` as labels, `np.log( y[np.arange(batch_size),
    t] )` is used for the same processing (here, the description of "a very small
    value, `1e-7`" has been omitted for visibility).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果某个元素在独热表示中`t`为0，则其交叉熵误差也为`0`，你可以忽略此计算。换句话说，如果你能够获取神经网络对正确标签的输出，就可以计算交叉熵误差。因此，对于作为独热表示的`t`，使用`t
    * np.log(y)`，而对于作为标签的`t`，使用`np.log(y[np.arange(batch_size), t])`进行相同的处理（这里为了可视化，已省略“一个非常小的值`1e-7`”的描述）。
- en: For reference, we can cover `np.log( y[np.arange(batch_size), t] )` briefly.
    `np.arange(batch_size)` generates an array from 0 to `batch_size-1`. When `batch_size`
    is 5, `np.arange(batch_size)` generates a NumPy array, [0, 1, 2, 3, 4]. `t` contains
    labels, as in [2, 7, 0, 9, 4] and `y[np.arange(batch_size), t]` extracts the output
    of the neural network corresponding to the correct label for each piece of data
    (in this example, `y[np.arange(batch_size), t]` generates a NumPy array, `[y[0,2],
    y[1,7], y[2,0], y[3,9], y[4,4]]`).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，我们可以简要介绍一下`np.log( y[np.arange(batch_size), t] )`。`np.arange(batch_size)`会生成一个从0到`batch_size-1`的数组。当`batch_size`为5时，`np.arange(batch_size)`生成一个NumPy数组，[0,
    1, 2, 3, 4]。`t`包含标签，如[2, 7, 0, 9, 4]，而`y[np.arange(batch_size), t]`则提取每个数据的正确标签对应的神经网络输出（在这个例子中，`y[np.arange(batch_size),
    t]`生成的NumPy数组为`[y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]`）。
- en: Why Do We Configure a Loss Function?
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么我们要配置损失函数？
- en: Some people may wonder why we introduce a loss function. For example, in the
    case of number recognition, we want parameters to improve recognition accuracy.
    Isn't it extra work to introduce a loss function? Our goal is to achieve a neural
    network that maximizes recognition accuracy. So, surely, we should use "recognition
    accuracy" as a score?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会想，为什么我们要引入损失函数？例如，在数字识别的情况下，我们希望参数提高识别准确率。引入损失函数难道不是多余的工作吗？我们的目标是实现一个最大化识别准确率的神经网络。那么，难道我们不应该使用“识别准确率”作为评分标准吗？
- en: You can find the answer to this question by paying attention to the role of
    the "derivative" in neural network training. This will be explained in detail
    in the next section. Neural network training looks for optimal parameters (weights
    and biases) so that the value of the loss function is the smallest. To look for
    the position of the smallest loss function, the derivative (gradient, to be precise)
    of a parameter is calculated, and the parameter value is updated gradually, based
    on the value of the derivative.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过关注“导数”在神经网络训练中的作用，来找到这个问题的答案。下一节将详细解释这一点。神经网络训练的目标是寻找最优的参数（权重和偏置），使损失函数的值最小。为了寻找损失函数最小的值，需要计算某个参数的导数（准确来说是梯度），并根据导数的值逐步更新参数值。
- en: For example, suppose that a virtual neural network exists here. We will pay
    attention to one weight parameter in the neural network. Here, the derivative
    of the loss function of the weight parameter indicates how the loss function changes
    when the value of the weight parameter is changed a little. If the derivative
    becomes a negative value, you can reduce the loss function by changing the weight
    parameter in a positive direction. On the other hand, if the derivative is a positive
    value, you can reduce the loss function by changing the weight parameter in the
    negative direction. However, when the value of the derivative becomes 0, the value
    of the loss function does not change, no matter how the weight parameter is moved.
    Updating the weight parameter is stopped there.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设这里存在一个虚拟的神经网络。我们将关注神经网络中的一个权重参数。在这里，权重参数的损失函数的导数表示当权重参数的值稍微改变时，损失函数的变化情况。如果导数变为负值，则可以通过将权重参数沿正方向调整来减少损失函数。另一方面，如果导数是正值，则可以通过将权重参数沿负方向调整来减少损失函数。然而，当导数值为0时，不管如何移动权重参数，损失函数的值都不会改变。此时，权重参数的更新将停止。
- en: We cannot use recognition accuracy as the score because the derivative becomes
    0 at almost all positions, preventing parameters from being updated. Now, let's
    neatly summarize this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能使用识别准确率作为评分标准，因为在几乎所有位置，导数都会变为0，导致参数无法更新。现在，让我们简洁地总结一下这一点。
- en: Note
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When training a neural network, we should not use recognition accuracy as the
    score. The reason is that if you use recognition accuracy as the score, the derivative
    of the parameters will be zero in most places.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，我们不应使用识别准确率作为评分标准。原因是，如果使用识别准确率作为评分标准，大多数地方的参数导数将为零。
- en: So why does recognition accuracy as the score lead the derivative of the parameter
    to 0 at almost all positions? Well, to explain this, let's consider another example.
    Say that a neural network can recognize 32 out of 100 items of training data.
    This means that the recognition accuracy is 32%. If we use the recognition accuracy
    as the score, slightly changing the weight parameter will leave it at 32% and
    cause no change. Slightly adjusting the parameters does not improve recognition
    accuracy. Even if the recognition accuracy is improved, the change will not be
    continuous, such as 32.0123…%, but discontinuous, such as 33% and 34%. On the
    other hand, if the loss function is used as the score, the current value of the
    loss function is represented as a value, such as 0.92543… Slightly changing the
    parameter value also changes the loss function continuously, such as 0.93432…
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么把识别精度作为评分会导致参数的导数在几乎所有位置都为 0 呢？为了说明这一点，我们考虑另一个例子。假设一个神经网络能识别训练数据中的 100
    个项目中的 32 个。这意味着识别精度是 32%。如果我们将识别精度作为评分，稍微改变权重参数，它仍会保持在 32%，不会发生变化。稍微调整参数不会提高识别精度。即使识别精度提高，变化也不会是连续的，比如
    32.0123…%，而是突发性的，比如 33% 和 34%。另一方面，如果使用损失函数作为评分，损失函数的当前值表示为一个值，比如 0.92543…稍微改变参数值也会使损失函数连续变化，比如
    0.93432…
- en: Slightly adjusting the parameter only changes the recognition accuracy a bit,
    and any change is discontinuous and sudden. This is also true of the "step function"
    of an activation function. If you use a step function for an activation function,
    a neural network cannot learn appropriately for the same reason. The reason for
    this is that the derivative of a step function is 0 almost anywhere (positions
    other than 0), as shown in *Figure 4.4*. When you use a step function, a slight
    change to the parameter is erased by the step function, and the value of the loss
    function shows no changes, even if you use it as the score.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微调整参数只会稍微改变识别精度，且任何变化都是不连续和突发的。这对于激活函数的“阶跃函数”也是一样的。如果使用阶跃函数作为激活函数，神经网络也无法正确学习，原因相同。阶跃函数的导数在几乎所有位置（除
    0 外）都为 0，如*图 4.4*所示。当使用阶跃函数时，参数的微小变化会被阶跃函数抹去，损失函数的值不会发生变化，即使你将其用作评分。
- en: 'A step function changes only at some moments, like a shishi-odoshi or scarecrow.
    On the other hand, for the derivative (tangent) of a sigmoid function, the output
    (value of the vertical axis) changes continuously and the gradient of the curve
    also changes continuously, as shown in *Figure 4.4*. In short, the derivative
    of a sigmoid function is not 0 at any position. This is important for "training"
    in a neural network. Because the gradient is never 0, a neural network can learn
    correctly:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 阶跃函数只在某些时刻发生变化，就像石上水流或稻草人。另一方面，Sigmoid 函数的导数（切线）会连续变化，纵轴的输出（值）也会不断变化，曲线的梯度也在不断变化，如*图
    4.4*所示。简而言之，Sigmoid 函数的导数在任何位置都不为 0。这对于神经网络的“训练”至关重要。因为梯度从不为 0，神经网络可以正确学习：
- en: '![Figure 4.4: Step function and sigmoid function – the gradient of a step function
    is 0 at almost all positions, while the gradient of a sigmoid function (tangent)
    is never 0'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4：阶跃函数与 Sigmoid 函数——阶跃函数的梯度在几乎所有位置为 0，而 Sigmoid 函数（切线）的梯度从不为 0'
- en: '](img/fig04_4.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_4.jpg)'
- en: 'Figure 4.4: Step function and sigmoid function – the gradient of a step function
    is 0 at almost all positions, while the gradient of a sigmoid function (tangent)
    is never 0'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.4：阶跃函数与 Sigmoid 函数——阶跃函数的梯度在几乎所有位置为 0，而 Sigmoid 函数（切线）的梯度从不为 0
- en: Numerical Differentiation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数值微分
- en: The gradient method uses information from the gradient to determine which direction
    to follow. This section describes what a gradient is and its characteristics,
    beginning with a "derivative."
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度法使用梯度信息来确定前进的方向。本节将介绍梯度是什么以及它的特性，从“导数”开始。
- en: Derivative
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导数
- en: For example, let's assume that you ran 2 km in 10 minutes from the start of
    a full marathon. You can calculate the speed as *2 / 10 = 0.2* [km/minute]. You
    ran at a speed of 0.2 km per minute.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你在 10 分钟内跑了 2 公里，刚开始全程马拉松。你可以计算出速度为 *2 / 10 = 0.2* [公里/分钟]。你以每分钟 0.2 公里的速度跑步。
- en: In this example, we calculated how much the "running distance" changed over
    "time." Strictly speaking, this calculation indicates the "average speed" for
    10 minutes because you ran 2 km in 10 minutes. A derivative indicates the amount
    of change at "a certain moment." Therefore, by minimizing the time of 10 minutes
    (the distance in the last 1 minute, the distance in the last 1 second, the distance
    in the last 0.1 seconds, and so on), you can obtain the amount of change at a
    certain moment (instantaneous speed).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们计算了“行驶距离”随“时间”的变化量。严格来说，这个计算表示的是10分钟的“平均速度”，因为你在10分钟内跑了2公里。导数表示的是在“某一时刻”的变化量。因此，通过缩小10分钟的时间（比如过去1分钟的距离、过去1秒的距离、过去0.1秒的距离，等等），你可以得到某一时刻的变化量（瞬时速度）。
- en: 'Thus, a derivative indicates the amount of change at a certain moment. This
    is defined by the following equation:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，导数表示在某一时刻的变化量。这个定义由以下方程表示：
- en: '| ![21](img/Figure_4.4a.png) | (4.4) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ![21](img/Figure_4.4a.png) | (4.4) |'
- en: Equation (4.4) indicates the derivative of a function. The left-hand side ![22](img/Figure_4.4b.png)
    indicates the derivative of *f(x)* with respect to *x* – the degree of changes
    of f(x) with respect to *x*. The derivative expressed by equation (4.4) indicates
    how the value of the function, *f(x)*, changes because of a "slight change" in
    *x*. Here, the slight change, *h*, is brought close to 0 infinitely, which is
    indicated as ![23](img/Figure_4.4c.png).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（4.4）表示一个函数的导数。左侧的![22](img/Figure_4.4b.png)表示* f(x)* 关于*x*的导数——即f(x)对x的变化程度。方程（4.4）表示的导数表明了由于*x*的“微小变化”，函数*f(x)*的值如何发生变化。在这里，微小变化*h*被无限接近0，这表示为![23](img/Figure_4.4c.png)。
- en: 'Let''s write a program to obtain the derivative of a function based on equation
    (4.4). To implement equation (4.4) directly, you can assign a small value to h
    for calculation purposes:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个程序，根据方程（4.4）来求取一个函数的导数。为了直接实现方程（4.4），你可以为计算目的给h赋一个小值：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The function is named `numerical_diff(f, x)`, after **numerical differentiation**.
    It takes two arguments: the function, f, and the argument, x, of the function,
    f. This implementation seems correct, but two improvements can be made.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数名为`numerical_diff(f, x)`，即**数值微分**。它接受两个参数：函数f和函数f的自变量x。这个实现看起来是正确的，但可以做出两个改进。
- en: 'The preceding implementation uses a small value of `10e-50` ("0.00...1" containing
    50 0s) as h because we want to use the smallest possible value as h (we want to
    bring h infinitely close to 0 if possible). But the problem of a **rounding error**
    occurs here. A rounding error occurs in the final calculation result by omitting
    a numeric value in the small range of a decimal (for example, by omitting eight
    or more places of decimals). The following example shows a rounding error in Python:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的实现使用了一个小值`10e-50`（即“0.00...1”包含50个零）作为h，因为我们希望使用尽可能小的值作为h（如果可能的话，我们希望将h无限接近0）。但是这里出现了**舍入误差**的问题。舍入误差是指在最终的计算结果中，因忽略小范围内的数字（例如，忽略八位或更多小数位）而产生的误差。以下例子展示了Python中的舍入误差：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When you represent `1e-50` in the float32 type (a 32-bit floating-point number),
    the value becomes 0.0\. You cannot express it correctly. Using too small value
    causes a problem in computer calculation. Now, here is the first improvement.
    You can use 10−4 as the small value, h. It is known that a value of around 10−4
    brings about good results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在float32类型（32位浮点数）中表示`1e-50`时，值变成了0.0。你无法正确表示它。使用过小的值会导致计算机计算时出现问题。现在，这是第一个改进。你可以将10−4作为小值h来使用。已知值约为10−4时会产生较好的结果。
- en: 'The second improvement is in terms of the difference in the function, f. The
    preceding implementation calculates the difference in the function f between x
    + h and x. You should observe that this calculation causes an error in the first
    place. As shown in *Figure 4.5*, the "true derivative " corresponds to the gradient
    of the function at the position of *x* (called a tangent), while the derivative
    in this implementation corresponds to the gradient between (*x* + *h*) and x.
    Therefore, the true derivative (true tangent) is not strictly identical to the
    value of this implementation. This difference occurs because you cannot bring
    *h* close to 0 infinitely:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个改进是在函数 f 的差异方面。前面的实现计算了 x + h 和 x 之间函数 f 的差异。你应该注意到，这种计算首先会引入误差。如*图 4.5*所示，"真实导数"对应于函数在
    *x* 位置的梯度（称为切线），而这个实现中的导数对应于 (*x* + *h*) 和 x 之间的梯度。因此，真实导数（真实切线）与这个实现的值不完全相同。这个差异的原因是你无法将
    *h* 无限接近于 0：
- en: '![Figure 4.5: True derivative (true tangent) and numerical differentiation
    (tangent'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5: 真实导数（真实切线）与数值微分（切线'
- en: by approximation) are different in value
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过近似计算）在数值上是不同的
- en: '](img/fig04_5.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_5.jpg)'
- en: 'Figure 4.5: True derivative (true tangent) and numerical differentiation (tangent
    by approximation) are different in value'
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 4.5: 真实导数（真实切线）和数值微分（通过近似获得的切线）在数值上是不同的'
- en: 'As shown in *Figure 4.5*, a numerical differential contains an error. To reduce
    this error, you can calculate the difference of the function, (*f*), between (*x
    + h*) and (*x - h*). This difference is called a **central difference** because
    it is calculated around *x* (on the other hand, the difference between (*x + h*)
    and *x* is called a **forward difference**). Now, let''s implement a numerical
    differentiation (numerical gradient) based on these two improvements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 4.5*所示，数值微分包含误差。为了减小这个误差，你可以计算函数（*f*）在 (*x + h*) 和 (*x - h*) 之间的差异。这种差异称为**中心差分**，因为它是围绕
    *x* 计算的（另一方面，(*x + h*) 和 *x* 之间的差异称为**前向差分**）。现在，让我们基于这两个改进实现数值微分（数值梯度）：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: As the preceding code shows, calculating a derivative by using a very small
    value difference is called **numerical differentiation**. On the other hand, obtaining
    a derivative with the expansion is called an "analytical solution" or "analytically
    obtaining a derivative," for example, by using the word "analytic." You can obtain
    the derivative of *y* = *x*2 analytically as ![24](img/Figure_4.5a.png). Therefore,
    you can calculate the derivative of *y* as *x* = 2, and this is 4\. An analytic
    derivative is the "true derivative" without errors.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，通过使用非常小的值差异来计算导数被称为**数值微分**。另一方面，通过扩展获得导数称为“解析解”或“解析求导”，例如，使用“analytic”一词。你可以通过解析方式得到
    *y* = *x*² 的导数，如![24](img/Figure_4.5a.png)。因此，你可以计算出 *y* 对 *x* 的导数为 2，结果是 4。解析导数是没有误差的“真实导数”。
- en: Examples of Numerical Differentiation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值微分的示例
- en: 'Let''s differentiate an easy function by using numerical differentiation. The
    first example is the quadratic function expressed by the following equation:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过数值微分来求导一个简单的函数。第一个例子是由以下方程表示的二次函数：
- en: '| ![25](img/Figure_4.5b.png) | (4.5) |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ![25](img/Figure_4.5b.png) | (4.5) |'
- en: 'Implement equation (4.5) in Python as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中实现方程（4.5）如下：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Draw the graph of this function. The following shows the code for drawing a
    graph and the resulting graph (*Figure 4.6*):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这个函数的图像。以下是绘制图像的代码和结果图像（*图 4.6*）：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now calculate the differentials of the function when x=5 and x=10:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在计算当 x=5 和 x=10 时的函数微分：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The differential calculated here is the amount of change of *f(x)* for *x*,
    which corresponds to the gradient of the function. By the way, the analytical
    solution of *f (x) = 0.01x*2 *+ 0.1x* is ![26](img/Figure_4.6c.png) *= 0.02x +
    0.1*. The true derivative when *x=5* and 10 are 0.2 and 0.3, respectively. They
    are not strictly identical to the results from numerical differentiation, but
    the error is very small. Actually, the error is so small that they can be regarded
    as almost identical values:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里计算的微分是 *f(x)* 对 *x* 的变化量，它对应于函数的梯度。顺便提一下，*f(x) = 0.01x*² + 0.1x* 的解析解是![26](img/Figure_4.6c.png)
    *= 0.02x + 0.1*。当 *x=5* 和 *x=10* 时，真实导数分别为 0.2 和 0.3。它们与数值微分的结果不完全相同，但误差非常小。实际上，误差小到可以认为它们几乎是相同的值：
- en: '![Figure 4.6: Graph of f (x) = 0.01x2 + 0.1x'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6: f (x) = 0.01x² + 0.1x 的图像'
- en: '](img/fig04_6.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_6.jpg)'
- en: 'Figure 4.6: Graph of *f* (*x*) = 0.01*x*2 + 0.1*x*'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：图形 *f* (*x*) = 0.01*x*2 + 0.1*x*
- en: 'We will use the preceding results of our numerical differentiation to plot
    graphs of lines whose gradients are the values of the numerical differentiation.
    The results are shown in *Figure 4.7*. Here, you can see that the derivatives
    correspond to the tangents of the function (the source code is located at `ch04/gradient_1d.py`):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前面的数值微分结果来绘制梯度为数值微分值的直线图。结果如*图 4.7*所示。在这里，您可以看到导数对应于函数的切线（源代码位于`ch04/gradient_1d.py`）：
- en: '![Figure 4.7: Tangents when x = 5 and x = 10 – using the values from numerical
    differentiation as the ](img/fig04_7.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7：当 *x* = 5 和 *x* = 10 时的切线 – 使用数值微分的值作为切线的梯度](img/fig04_7.jpg)'
- en: 'Figure 4.7: Tangents when *x* = 5 and *x* = 10 – using the values from numerical
    differentiation as the gradients of lines'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.7：当 *x* = 5 和 *x* = 10 时的切线 – 使用数值微分的值作为直线的梯度
- en: Partial Derivative
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏导数
- en: 'Next, let''s look at the function expressed by equation (4.6). This simple
    equation calculates the square sum of the arguments. Note that it has two variables,
    unlike the previous example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看方程（4.6）表示的函数。这个简单的方程计算了参数的平方和。请注意，它有两个变量，不像前面的例子那样只有一个变量：
- en: '| ![27](img/Figure_4.7a.png) | (4.6) |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| ![27](img/Figure_4.7a.png) | (4.6) |'
- en: 'You can implement it in Python as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Python 中这样实现：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, it is assumed that NumPy arrays are passed as arguments. The function
    simply squares each element of the NumPy arrays and sums it up (`np.sum(x**2)`
    can implement the same processing). Now, let''s draw the graph of this function.
    This three-dimensional graph appears as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里假设传递的是 NumPy 数组作为参数。该函数简单地对每个 NumPy 数组的元素进行平方并求和（`np.sum(x**2)`也能实现相同的处理）。现在，让我们绘制这个函数的图形。该三维图形如下所示：
- en: '![Figure 4.8: Graph of'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：图形'
- en: '](img/fig04_8.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_8.jpg)'
- en: 'Figure 4.8: Graph of ![28](img/Figure_4.8a.png)'
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：图形 ![28](img/Figure_4.8a.png)
- en: Now, we want to calculate the derivative of equation (4.6). Here, please note
    that equation (4.6) has two variables. Therefore, you must specify for which of
    the two variables, *x*0 and *x*1, the differentials are calculated. The derivative
    of a function that consists of multiple variables is called a **partial derivative**.
    They are expressed as ![29](img/Figure_4.8b.png).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们要计算方程（4.6）的导数。这里，请注意方程（4.6）有两个变量。因此，您必须指定对于 *x*0 和 *x*1 中的哪一个变量计算微分。由多个变量组成的函数的导数称为**偏导数**。它们表示为
    ![29](img/Figure_4.8b.png)。
- en: 'To illustrate this, consider the following two partial derivative problems
    and their solutions:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，考虑以下两个偏导数问题及其解决方案：
- en: '`x0` when `x0 = 3` and `x1 = 4`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`x0` 当 `x0 = 3` 且 `x1 = 4` 时：'
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`x1` when `x0 = 3` and `x1 = 4`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`x1` 当 `x0 = 3` 且 `x1 = 4` 时：'
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To solve these problems, a function with one variable is defined, and the derivative
    for the function is calculated. For example, in `x1=4` is defined, and the function,
    which has only one variable, `x0`, is passed to the function to calculate a numerical
    differentiation. Based on the results, the answer to `6.00000000000378`, and the
    answer to `7.999999999999119`. They are mostly the same as the solutions from
    analytical differentiation.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，定义了一个只有一个变量的函数，并计算该函数的导数。例如，当`x1=4`时，定义了该函数，其中只有一个变量`x0`，并将其传递给该函数进行数值微分。根据结果，答案是`6.00000000000378`，答案是`7.999999999999119`。它们与解析微分的解大致相同。
- en: In this way, the partial derivative calculates the gradient at a certain position,
    such as the differentiation for one variable. However, for the partial derivative,
    one of the variables is targeted, and the other variables are fixed at a certain
    value. In the preceding implementation, a new function was defined to hold the
    other variables at a specific value. The newly defined function was passed to
    the previous numerical differential function to calculate the partial derivative.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，偏导数计算了某一位置的梯度，就像对一个变量的微分一样。然而，对于偏导数来说，某一个变量是目标变量，其他变量则固定在某个特定值。在前面的实现中，定义了一个新函数来保持其他变量在特定值上。这个新定义的函数被传递给先前的数值微分函数来计算偏导数。
- en: Gradient
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度
- en: 'In the previous example, the partial derivatives of *x*0 and *x*1 were calculated
    for each variable. Now, we want to calculate the partial derivatives of *x*0 and
    *x*1 collectively. For example, let''s calculate the partial derivatives of (*x*0,
    *x*1) when `x0 = 3` and `x1 = 4` as (![31](img/Figure_4.8b1.png))The vector that
    collectively indicates the partial differentials of all the variables, such as
    (![32](img/Figure_4.8b2.png)) is called a **gradient**. You can implement a gradient
    as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，分别计算了*x*0和*x*1的偏导数。现在，我们希望计算*x*0和*x*1的偏导数总和。例如，假设我们要计算当`x0 = 3`和`x1
    = 4`时(*x*0, *x*1)的偏导数，如![31](img/Figure_4.8b1.png)。表示所有变量偏导数总和的向量，如![32](img/Figure_4.8b2.png)，被称为**梯度**。你可以通过以下方式实现梯度：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Implementing the `numerical_gradient(f, x)` function seems a little complicated,
    but the processes are almost the same as those in numerical differentiation for
    one variable. Note that `np.zeros_like(x)` generates an array that has the same
    shape as `x` and whose elements are all zero.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 实现`numerical_gradient(f, x)`函数看起来有点复杂，但过程几乎和单变量数值微分中的步骤一样。请注意，`np.zeros_like(x)`生成一个与`x`形状相同且元素全为零的数组。
- en: 'The `numerical_gradient(f, x)` function takes the `f (function)` and `x (NumPy
    array)` arguments and obtains numerical differentiations for each element of the
    NumPy array, `x`. Now, let''s use this function to calculate a gradient. Here,
    we will obtain the gradients at points (3, 4), (0, 2), and (3, 0):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`numerical_gradient(f, x)`函数接受`f（函数）`和`x（NumPy数组）`作为参数，并为NumPy数组`x`中的每个元素获取数值微分。现在，让我们使用这个函数来计算梯度。在这里，我们将获得点（3,
    4），（0, 2）和（3, 0）处的梯度：'
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: The actual result is [6.0000000000037801, 7.9999999999991189], but [6., 8.]
    is returned. This is because a returned NumPy array is formatted to enhance the
    visibility of the values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 实际结果是[6.0000000000037801, 7.9999999999991189]，但返回的是[6., 8.]。这是因为返回的NumPy数组进行了格式化，以便更清晰地显示数值。
- en: Thus, we can calculate the gradient at each point of (*x*0, *x*1). The preceding
    example shows that the gradient for point (3, 4) is (6, 8), that for point (0,
    2) is (0, 4), and that for point (3, 0) is (6, 0). What do these gradients mean?
    To understand this, let's look at the gradients of ![33](img/Figure_4.8g.png).
    Here, we will make the gradients negative and draw the vectors (the source code
    is located at `ch04/gradient_2d.py`).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以计算(*x*0, *x*1)每个点的梯度。上面的例子显示，点（3, 4）处的梯度是（6, 8），点（0, 2）处的梯度是（0, 4），点（3,
    0）处的梯度是（6, 0）。这些梯度意味着什么呢？为了理解这一点，让我们看看![33](img/Figure_4.8g.png)的梯度。在这里，我们将使梯度变为负数，并绘制向量（源代码位于`ch04/gradient_2d.py`）。
- en: 'The gradients of ![34](img/Figure_4.8g1.png) are shown as the vectors (arrows)
    that have the direction toward the lowest point, as shown in *Figure 4\. 9*. In
    *Figure 4.9*, the gradients seem to point at "the lowest position (smallest value)"
    of the function, *f*(*x*0, *x*1). Just like a compass, the arrows point to one
    point. The more distant they are from "the lowest position," the larger the size
    of the arrow:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![34](img/Figure_4.8g1.png)的梯度显示为指向最低点的向量（箭头），如*图4.9*所示。在*图4.9*中，梯度似乎指向函数*f*(*x*0,
    *x*1)的“最低位置（最小值）”。就像指南针一样，箭头指向一个点。它们距离“最低位置”越远，箭头的大小就越大：'
- en: '![Figure 4.9: Gradients of'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9：梯度'
- en: '](img/fig04_9.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_9.jpg)'
- en: 'Figure 4.9: Gradients of ![35](img/Figure_4.9a.png)'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.9：![35](img/Figure_4.9a.png)的梯度
- en: In the example shown in *Figure 4.9*, the gradients point at the lowest position,
    but this is not always the case. In fact, gradient points in the lower direction
    at each position. To be more precise, the direction of a gradient is **the direction
    that reduces the value of the function most at each position**. This is an important
    point, so please keep this in mind.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.9*所示的示例中，梯度指向最低位置，但情况并非总是如此。事实上，梯度在每个位置都指向较低的方向。更准确地说，梯度的方向是**在每个位置上使函数值减少最快的方向**。这是一个重要的概念，请务必记住这一点。
- en: Gradient Method
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度法
- en: Many machine learning problems look for optimal parameters during training.
    A neural network also needs to find optimal parameters (weights and biases) during
    training. The optimal parameter here is the parameter value when the loss function
    takes the minimum value. However, a loss function can be complicated. The parameter
    space is vast, and we cannot guess where it takes the minimum value. A gradient
    method makes good use of gradients to find the minimum value (or the smallest
    possible value) of the function.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习问题在训练过程中需要寻找最优参数。神经网络在训练过程中也需要找到最优参数（权重和偏差）。这里的最优参数是指损失函数取最小值时的参数值。然而，损失函数可能很复杂，参数空间也很庞大，我们无法直接猜测最小值的所在位置。梯度法利用梯度来寻找函数的最小值（或最小可能值）。
- en: A gradient shows the direction that reduces the value of the function most at
    each position. Therefore, whether the position that a gradient points in is really
    the minimum value of the function, in other words, whether the direction is really
    the one to take, cannot be guaranteed. Actually, in a complicated function, the
    direction that a gradient points to is not the minimum value in most cases.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度表示在每个位置上最能减少函数值的方向。因此，梯度指向的位置是否真的是函数的最小值，换句话说，梯度指向的方向是否真的是正确的方向，是无法保证的。实际上，在复杂的函数中，梯度指向的方向在大多数情况下并不是最小值所在的方向。
- en: Note
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The gradient is 0 at the local minimum, the minimum, and at a point called the
    saddle point of a function. A local minimum is locally the smallest value, which
    is the minimum value in a limited range. A saddle point is a position of the local
    maximum in one direction and of the local minimum in another direction. A gradient
    method looks for the position where a gradient is 0, but where the position is
    not always the global minimum (it can be the local minimum or a saddle point).
    When a function has a complicated and distorted shape, learning enters an (almost)
    flat land and a stagnant period called a "plateau" might occur, leading to stagnation
    in training.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在局部最小值、最小值以及一个被称为函数鞍点的点处，梯度为 0。局部最小值是局部范围内的最小值，即在一个有限范围内的最小值。鞍点是在一个方向上为局部最大值，在另一个方向上为局部最小值的位置。梯度法寻找的是梯度为
    0 的位置，但这个位置并不总是全局最小值（它可能是局部最小值或鞍点）。当一个函数具有复杂且扭曲的形状时，学习可能会进入一个（几乎）平坦的区域，并且可能会出现一个被称为“平台期”的停滞期，导致训练停滞不前。
- en: Even if the direction of a gradient does not always point at the global minimum
    value, moving in that direction can reduce the value of the function the most.
    Therefore, to look for the position of the minimum value or to look for the position
    where the function has the smallest possible value, you should determine the direction
    of movement based on the information about gradients.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 即使梯度的方向并不总是指向全局最小值，沿着这个方向移动也可以最大程度地减少函数值。因此，若要寻找最小值的位置或寻找函数具有最小可能值的位置，应该根据梯度信息来确定移动的方向。
- en: Now, let's look at the gradient method. In the gradient method, you move a fixed
    distance from the current position in the gradient direction. By doing this, you
    obtain a gradient at the new position and move in the gradient direction again.
    Thus, you move in the gradient direction repeatedly. Reducing the value of a function
    gradually by going in the gradient direction repeatedly is known as the **gradient
    method**. This method is often used in optimization problems for machine learning.
    It is typically used when training neural networks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下梯度法。在梯度法中，你会沿着梯度方向从当前位置移动一个固定的距离。通过这样做，你会在新位置上获得一个梯度，然后再次沿着梯度方向移动。这样，你会反复沿梯度方向移动。通过反复沿梯度方向移动逐渐减少函数值的过程被称为**梯度法**。这种方法常用于机器学习中的优化问题，尤其是在神经网络的训练中。
- en: Note
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A gradient method is called by another name if it looks for the minimum or the
    maximum value. To be precise, the method for the minimum value is called the **gradient
    descent method**, while the method for the maximum value is called the **gradient
    ascent method**. However, reversing the sign of a loss function can change this
    from a problem for the minimum value into a problem for the maximum value. So,
    the difference between "descent" and "ascent" is not especially important. Generally,
    a "gradient descent" method is often used in neural networks (deep learning).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果梯度法用于寻找最小值或最大值，它有另一个名称。准确来说，寻找最小值的方法称为 **梯度下降法**，而寻找最大值的方法称为 **梯度上升法**。然而，反转损失函数的符号可以将最小值问题转换为最大值问题。因此，“下降”与“上升”之间的区别并不是特别重要。通常，**梯度下降法**在神经网络（深度学习）中被广泛使用。
- en: 'Now, let''s express a gradient method with an equation. Equation (4.7) shows
    a gradient method:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用一个公式表示梯度法。公式 (4.7) 展示了一个梯度法：
- en: '| ![36](img/Figure_4.9b.png) | (4.7) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ![36](img/Figure_4.9b.png) | (4.7) |'
- en: In equation (4.7), η adjusts the amount to be updated. This is called a **learning
    rate** in neural network. A learning rate determines how much needs to be learned
    and how much to update the parameters.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 (4.7) 中，η 调整更新的幅度。这在神经网络中称为 **学习率**。学习率决定了需要学习多少以及如何更新参数。
- en: Equation (4.7) shows an update equation for one training instance, and the step
    is repeated. Each step updates the variable values, as shown in equation (4.7),
    and the step is repeated several times to reduce the value of the function gradually.
    This example has two variables, but even when the number of variables is increased,
    a similar equation—a partial differential value for each variable—is used for
    updating.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 (4.7) 展示了一个训练实例的更新方程，步骤会重复进行。每一步都按照公式 (4.7) 更新变量值，并且这个步骤会重复多次，以逐步减小函数的值。这个例子有两个变量，但即使增加了变量数量，也使用类似的方程——每个变量的偏导数——来进行更新。
- en: You must specify the value of the learning rate, such as 0.01 and 0.001, in
    advance. Generally, if this value is too large or too small, you cannot reach
    a "good place." In neural network training, we usually check whether training
    is successful by changing the value of the learning rate.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须提前指定学习率的值，如 0.01 或 0.001。通常，如果这个值过大或过小，就无法达到“好的位置”。在神经网络训练中，我们通常通过调整学习率来检查训练是否成功。
- en: 'Now, let''s implement a gradient descent method in Python. This can be done
    as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在 Python 中实现一个梯度下降法。可以按如下方式进行：
- en: '[PRE21]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `f` argument is a function to optimize, the `init_x` argument is an initial
    value, the `lr` argument is a learning rate, and the `step_num` argument is the
    number of repetitions in a gradient method. The gradient of the function is obtained
    by `numerical_gradient(f, x)` and the gradient updated by multiplying it by the
    learning rate, which is repeated the number of times specified by `step_num`.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`f` 参数是需要优化的函数，`init_x` 参数是初始值，`lr` 参数是学习率，`step_num` 参数是梯度法中重复的次数。通过 `numerical_gradient(f,
    x)` 获取函数的梯度，并将其乘以学习率进行更新，重复指定的 `step_num` 次数。'
- en: You can use this function to obtain the local minimum of the function and even
    the minimum value if you are lucky. Now, let's try solving a problem.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用此函数获取函数的局部最小值，甚至在运气好的时候得到全局最小值。现在，让我们尝试解决一个问题。
- en: '**Question**: Obtain the minimum value of ![37](img/Figure_4.9d.png) with a
    gradient method:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：使用梯度法获取 ![37](img/Figure_4.9d.png) 的最小值：'
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, specify (-3.0, 4.0) as the initial value and start looking for the minimum
    value by using a gradient method. The final result is (-6.1e-10, 8.1e-10), which
    is almost near (0, 0). Actually, the true minimum value is (0, 0). You successfully
    obtained almost correct results by using a gradient method. *Figure 4.10* shows
    the process of updating with a gradient method. The origin is the lowest position,
    and you can see that the result is approaching it gradually. The source code to
    draw this graph is located at `ch04/gradient_method.py` (`ch04/gradient_method.py`
    does not display dashed lines, which show the contour lines in the graph):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，指定初始值为 (-3.0, 4.0)，并通过梯度法开始寻找最小值。最终结果为 (-6.1e-10, 8.1e-10)，几乎接近 (0, 0)。实际上，真实的最小值是
    (0, 0)。通过使用梯度法，你成功地获得了几乎正确的结果。*图 4.10* 显示了使用梯度法更新的过程。原点是最低点，你可以看到结果逐渐接近它。绘制该图的源代码位于
    `ch04/gradient_method.py`（`ch04/gradient_method.py` 并未显示虚线，这些虚线表示图中的等高线）：
- en: '![Figure 4.10: Updating  with a gradient method – the dashed lines show'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.10：使用梯度方法更新——虚线表示'
- en: the contour lines of the function
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的等高线
- en: '](img/fig04_10.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_10.jpg)'
- en: 'Figure 4.10: Updating ![38](img/Figure_4.10a.png) with a gradient method –
    the dashed lines show the contour lines of the function'
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.10：使用梯度方法更新![38](img/Figure_4.10a.png)——虚线表示函数的等高线
- en: 'As mentioned earlier, an overly large or small learning rate does not achieve
    good results. Let''s do some experiments regarding both cases here:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，过大或过小的学习率都无法取得良好的结果。让我们在这里做一些关于这两种情况的实验：
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As this experiment shows, the result diverges to a large value if the learning
    rate is too large. On the other hand, almost no updates occur if the learning
    rate is too small. Setting an appropriate learning rate is important.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个实验所示，如果学习率过大，结果会发散到一个很大的值。另一方面，如果学习率过小，则几乎不会发生更新。设置合适的学习率非常重要。
- en: Note
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: A parameter such as a learning rate is called a **hyperparameter**. It is different
    from the parameters (weights and biases) of a neural network in terms of its characteristics.
    Weight parameters in a neural network can be obtained automatically with training
    data and a training algorithm, while a hyperparameter must be specified manually.
    Generally, you must change this hyperparameter to various values to find a value
    that enables good training.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 类似学习率这样的参数被称为**超参数**。它与神经网络的参数（权重和偏置）在特性上有所不同。神经网络中的权重参数可以通过训练数据和训练算法自动获得，而超参数必须手动指定。通常，你需要将超参数设置为不同的值，找到一个能够良好训练的值。
- en: Gradients for a Neural Network
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络的梯度
- en: 'You must also calculate gradients in neural network training. The gradients
    here are those of a loss function for weight parameters. For example, let''s assume
    that a neural network has the weight W (2x3 array) only, and the loss function
    is L. In this case, we can express the gradient as ![39](img/Figure_4.10b.png).
    The following equation shows this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练中，你还必须计算梯度。这里的梯度是损失函数对于权重参数的梯度。例如，假设一个神经网络只有权重W（2x3数组），损失函数是L。在这种情况下，我们可以将梯度表示为![39](img/Figure_4.10b.png)。以下方程显示了这一点：
- en: '| ![40](img/Figure_4.10c.png) | (4.8) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| ![40](img/Figure_4.10c.png) | (4.8) |'
- en: Each element of ![41](img/Figure_4.10e.png) is the partial derivative for each
    element. For example, the element at the first row and column, ![42](img/Figure_4.10f.png),
    indicates how a slight change in w11 changes the loss function, L. What is important
    here is that the shape of ![43](img/Figure_4.10g.png) is the same as that of W.
    Actually, in equation (4.8), both W and ![44](img/Figure_4.10h.png) are the same
    (2x3) in shape.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每个元素的![41](img/Figure_4.10e.png)是每个元素的偏导数。例如，第一行第一列的元素![42](img/Figure_4.10f.png)表示w11的微小变化如何影响损失函数L。这里重要的是，![43](img/Figure_4.10g.png)的形状与W的形状相同。实际上，在公式(4.8)中，W和![44](img/Figure_4.10h.png)的形状是相同的（2x3）。
- en: 'Now, let''s implement a program that calculates a gradient by taking an easy
    neural network as an example. To do that, we will implement a class named `simpleNet`
    (the source code is located at `ch04/gradient_simplenet.py`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个程序，通过使用一个简单的神经网络来计算梯度。为此，我们将实现一个名为`simpleNet`的类（源代码位于`ch04/gradient_simplenet.py`）：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, the `softmax` and `cross_entropy_error` methods in `common/functions.py`
    are being used. The `numerical_gradient` method in `common/gradient.py` is also
    being used. The `simpleNet` class has only one instance variable, which is the
    weight parameters with a shape of 2x3\. It has two methods: one is `predict(x)`
    for prediction, and the other is `loss(x, t)` for obtaining the value of the loss
    function. Here, the `x` argument is the input data and the `t` argument is a correct
    label. Now, let''s try using `simpleNet`:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了`common/functions.py`中的`softmax`和`cross_entropy_error`方法。同时也使用了`common/gradient.py`中的`numerical_gradient`方法。`simpleNet`类只有一个实例变量，即形状为2x3的权重参数。它有两个方法：一个是用于预测的`predict(x)`，另一个是用于计算损失函数值的`loss(x,
    t)`。这里，`x`参数是输入数据，`t`参数是正确标签。现在，让我们尝试使用`simpleNet`：
- en: '[PRE25]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, let''s obtain the gradients,using `numerical_gradient(f, x)`. The `f(W)`
    function defined here takes a dummy argument, `W`. Because the `f(x)` function
    is executed inside `numerical_gradient(f, x)`, `f(W)` is defined for consistency:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`numerical_gradient(f, x)`来计算梯度。这里定义的`f(W)`函数接受一个虚拟参数`W`。因为`f(x)`函数在`numerical_gradient(f,
    x)`中执行，所以为了保持一致，`f(W)`被定义：
- en: '[PRE26]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `f` argument of `numerical_gradient(f, x)` is a function and the `x` argument
    is the argument to the function, `f`. Therefore, a new function, `f`, is defined
    here. It takes `net.W` as an argument and calculates the loss function. The newly
    defined function is passed to `numerical_gradient(f, x)`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`numerical_gradient(f, x)` 的 `f` 参数是一个函数，`x` 参数是函数 `f` 的输入参数。因此，这里定义了一个新函数
    `f`，它以 `net.W` 作为参数并计算损失函数。新定义的函数被传递给 `numerical_gradient(f, x)`。'
- en: '`numerical_gradient(f, net.W)` returns `dW`, which is a two-dimensional 2x3
    array. `dW` shows that ![45](img/Figure_4.10i.png) for ![46](img/Figure_4.10j.png)
    is around `0.2`, for example. This indicates that when w11 is increased by h,
    the value of the loss function increases by 0.2h. ![47](img/Figure_4.10k.png)
    is about `-0.5`, which indicates that when w23 is increased by h, the value of
    the loss function decreases by 0.5h. Therefore, to reduce the loss function, you
    should update w23 in a positive direction and w11 in a negative direction. You
    can also see that updating w23 contributes to the reduction more than updating
    w11.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`numerical_gradient(f, net.W)` 返回 `dW`，它是一个二维的 2x3 数组。`dW` 显示例如：![45](img/Figure_4.10i.png)
    对应的 ![46](img/Figure_4.10j.png) 大约为 `0.2`。这表明，当 w11 增加 h 时，损失函数的值增加了 0.2h。![47](img/Figure_4.10k.png)
    大约为 `-0.5`，这表明当 w23 增加 h 时，损失函数的值减少了 0.5h。因此，为了减少损失函数，应该将 w23 更新为正方向，而 w11 更新为负方向。你还可以看到，更新
    w23 对减少损失函数的贡献大于更新 w11。'
- en: 'In the preceding implementation, the new function is written as `def f(x):…`
    In Python, you can use a `lambda` notation to write and implement a simple function,
    as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的实现中，新函数写成了 `def f(x):…`。在 Python 中，您可以使用 `lambda` 表达式来编写并实现一个简单的函数，如下所示：
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After obtaining the gradients for a neural network, all you have to do is use
    a gradient method to update the weight parameters. In the next section, we will
    implement all these training processes for a two-layer neural network.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得神经网络的梯度后，您只需使用梯度方法更新权重参数。在下一节中，我们将为一个两层神经网络实现所有这些训练过程。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The `numerical_gradient()` function we used here is slightly different from
    the previous implementation for handling multi-dimensional arrays such as the
    weight parameter, `W`. However, these changes are simple and are only for handling
    multidimensional arrays. For further details, please refer to the source code
    (`common/gradient.py`).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的 `numerical_gradient()` 函数与之前处理多维数组（如权重参数 `W`）的实现稍有不同。然而，这些更改非常简单，仅用于处理多维数组。有关详细信息，请参考源代码（`common/gradient.py`）。
- en: Implementing a Training Algorithm
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现训练算法
- en: So far, we have learned about the basics of neural network training. Important
    keywords such as "loss function", "mini-batch", "gradient", and "gradient descent
    method" have appeared in succession. Here, we will look at the procedure of neural
    network training for review purposes. Let's go over the neural network training
    procedure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了神经网络训练的基础知识。诸如“损失函数”、“小批量”、“梯度”和“梯度下降法”等重要关键词已经相继出现。接下来，我们将回顾一下神经网络训练的流程。让我们一起回顾神经网络训练的步骤。
- en: Presupposition
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设条件
- en: A neural network has adaptable weights and biases. Adjusting them so that they
    fit the training data is called "training." Neural network training consists of
    four steps.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络具有可调的权重和偏差。调整它们以适应训练数据的过程称为“训练”。神经网络训练包括四个步骤。
- en: Step 1 (mini-batch)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第 1 步（小批量）
- en: Select some data at random from the training data. The selected data is called
    a mini-batch. The purpose here is to reduce the value of the loss function for
    the mini-batch.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据中随机选择一些数据。选中的数据称为小批量。这里的目的是减少小批量的损失函数值。
- en: Step 2 (calculating gradients)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2 步（计算梯度）
- en: To reduce the loss function for the mini-batch, calculate the gradient for each
    weight parameter. The gradient shows the direction that reduces the value of the
    loss function the most.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少小批量的损失函数，计算每个权重参数的梯度。梯度表示最能减少损失函数值的方向。
- en: Step 3 (updating parameters)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3 步（更新参数）
- en: Update the weight parameters slightly in the gradient direction.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度方向上稍微更新权重参数。
- en: Step 4 (repeating)
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步（重复）
- en: Repeat *steps* *1*, *2*, and *3*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 重复 *步骤* *1*、*2* 和 *3*。
- en: The preceding four steps are used for neural network training. This method uses
    a gradient descent method to update parameters. Because the data used here is
    selected at random as a mini-batch, it is referred to as **stochastic gradient
    descent**. "Stochastic" means "selecting data at random stochastically." Therefore,
    stochastic gradient descent means "the gradient descent method for randomly selected
    data." In many deep learning frameworks, stochastic gradient descent is usually
    implemented as the **SGD** function, which is named after its initials.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上述四个步骤用于神经网络训练。此方法使用梯度下降法来更新参数。由于这里使用的数据是随机选取的小批量数据，因此称之为**随机梯度下降**。 “随机”意味着“随机选择数据。”因此，随机梯度下降表示“对随机选取的数据应用梯度下降法”。在许多深度学习框架中，随机梯度下降通常作为
    **SGD** 函数实现，其名称源自其首字母。
- en: Now, let's implement the neural network that actually learns handwritten digits.
    Here, a two-layer neural network (with one hidden layer) will use the MNIST dataset
    for training.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个实际学习手写数字的神经网络。在这里，一个两层神经网络（包含一个隐藏层）将使用 MNIST 数据集进行训练。
- en: A Two-Layer Neural Network as a Class
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为一个类的两层神经网络
- en: 'First, let''s implement a two-layer neural network as a class. This class is
    named `TwoLayerNet` and is implemented as follows (implementing `TwoLayerNet`
    is based on the Python source code provided by the CS231n (*Convolutional Neural
    Networks for Visual Recognition* ([http://cs231n.github.io/](http://cs231n.github.io/))
    course at Stanford University). The source code is located at `ch04/two_layer_net.py`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实现一个两层神经网络作为类。这个类被命名为 `TwoLayerNet`，其实现如下（`TwoLayerNet` 的实现基于斯坦福大学 CS231n
    课程《*卷积神经网络与视觉识别*》（[http://cs231n.github.io/](http://cs231n.github.io/)）提供的 Python
    源代码。源代码位于 `ch04/two_layer_net.py`）：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The implementation of this class is a little long, but nothing that new appears.
    It has many things in common with the implementation of forward processing a neural
    network covered in the previous chapter. First, let''s look at the variables and
    methods that were used in this class. *Table 4.1* shows the important variables,
    while *Table 4.2* shows all the methods:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的实现稍微有些长，但并没有出现什么新的内容。它与上一章中讲解的神经网络前向处理实现有很多相似之处。首先，让我们来看一下在这个类中使用的变量和方法。*表
    4.1* 显示了重要的变量，而 *表 4.2* 显示了所有的方法：
- en: '![Table 4\. 1: Variables used in the TwoLayerNet class'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 4.1: 在 TwoLayerNet 类中使用的变量'
- en: '](img/Figure_4_Table01.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4_Table01.jpg)'
- en: 'Table 4\. 1: Variables used in the TwoLayerNet class'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '表 4.1: 在 TwoLayerNet 类中使用的变量'
- en: '![Table 4.2: Methods used in the TwoLayerNet class'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![表 4.2: 在 TwoLayerNet 类中使用的方法'
- en: '](img/Figure_4_Table02.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4_Table02.jpg)'
- en: 'Table 4.2: Methods used in the TwoLayerNet class'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '表 4.2: 在 TwoLayerNet 类中使用的方法'
- en: 'The `TwoLayerNet` class has two dictionary variables, `params` and `grads`,
    as instance variables. The `params` variable contains the weight parameters. For
    example, the weight parameters for layer 1 are stored in `params[''W1'']` as a
    NumPy array. You can access the bias for layer 1 using `params[''b1'']`. Here
    is an example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`TwoLayerNet` 类有两个字典变量，`params` 和 `grads`，作为实例变量。`params` 变量包含权重参数。例如，第 1 层的权重参数存储在
    `params[''W1'']` 中，类型为 NumPy 数组。你可以使用 `params[''b1'']` 访问第 1 层的偏置。以下是一个示例：'
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As shown here, the `params` variable contains all the parameters required for
    this network. The weight parameters contained in the `params` variable are used
    for predicting (forward processing). You can make a prediction as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，`params` 变量包含了该网络所需的所有参数。`params` 变量中的权重参数用于预测（前向处理）。你可以按如下方式进行预测：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `grads` variable contains the gradient for each parameter so that it corresponds
    to the `params` variable. When you calculate gradients by using the `numerical_gradient()`
    method, gradient information is stored in the `grads` variable, as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`grads` 变量包含每个参数的梯度，以便与 `params` 变量一一对应。当你使用 `numerical_gradient()` 方法计算梯度时，梯度信息会存储在
    `grads` 变量中，如下所示：'
- en: '[PRE31]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, let's look at the implementation of the methods in `TwoLayerNet`. The `__init__`
    (`self`, `input_size`, `hidden_size`, `output_size`) method is the initialization
    method of the class ( called when `TwoLayerNet` is generated). The arguments are
    the numbers of neurons in the input layer, in the hidden layer, and the output
    layer in order from left to right. For handwritten digit recognition, a total
    of 784 input images that are 28x28 in size are provided and 10 classes are returned.
    Therefore, we specify the `input_size=784` and `output_size=10` arguments and
    set an appropriate value for `hidden_size` as the number of hidden layers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下 `TwoLayerNet` 中方法的实现。`__init__` (`self`, `input_size`, `hidden_size`,
    `output_size`) 方法是类的初始化方法（在生成 `TwoLayerNet` 时调用）。参数依次是输入层、隐藏层和输出层的神经元数量。从左到右排列。对于手写数字识别，提供了总共
    784 张 28x28 大小的输入图像，并返回 10 个类别。因此，我们指定了 `input_size=784` 和 `output_size=10` 参数，并设置适当的
    `hidden_size` 作为隐藏层的数量。
- en: This initialization method also initializes the weight parameters. Determining
    what values to set as the initial weight parameters is important for successful
    neural network training. We will discuss the initialization of weight parameters
    in detail later. Here, the weights are initialized by using the random numbers
    based on Gaussian distribution, and the biases are initialized by 0\. `predict(self,
    x)`, and `accuracy(self, x, t)` are almost the same as in the implementation of
    predicting in relation to the neural network, which we looked at in the previous
    chapter. If you have any questions, please refer to the previous chapter. The
    `loss(self, x, t)` method calculates the value of the loss function. It obtains
    a cross-entropy error based on the result of `predict()` and the correct label.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该初始化方法还初始化了权重参数。确定初始权重参数的值对于神经网络训练的成功至关重要。我们将在后面详细讨论权重参数的初始化。这里，权重通过使用基于高斯分布的随机数来初始化，偏置被初始化为
    0。`predict(self, x)` 和 `accuracy(self, x, t)` 与上一章中我们看到的神经网络预测实现几乎相同。如果你有任何疑问，请参考上一章。`loss(self,
    x, t)` 方法计算损失函数的值。它基于 `predict()` 的结果和正确标签来获得交叉熵误差。
- en: The remaining `numerical_gradient(self, x, t)` method calculates the gradient
    of each parameter. It uses numerical differentiation to calculate the gradient
    for the loss function of each parameter. The `gradient(self, x, t)` method will
    be implemented in the next chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的 `numerical_gradient(self, x, t)` 方法计算每个参数的梯度。它使用数值微分来计算每个参数的损失函数的梯度。`gradient(self,
    x, t)` 方法将在下一章中实现。
- en: Note
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '`numerical_gradient(self, x, t)` uses numerical differentiation to calculate
    the gradients of the parameters. In the next chapter, we will look at how to calculate
    gradients quickly using backpropagation, which returns almost the same result
    as using numerical differentiation, but with faster processing. The method for
    obtaining a gradient through backpropagation will be implemented as `gradient(self,
    x, t)` in the next chapter. If you want to save time, you can use `gradient(self,
    x, t)` instead of `numerical_gradient(self, x, t)` because neural network training
    takes time.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`numerical_gradient(self, x, t)` 使用数值微分计算参数的梯度。在下一章中，我们将讨论如何使用反向传播快速计算梯度，反向传播返回的结果几乎与使用数值微分得到的结果相同，但处理速度更快。通过反向传播获取梯度的方法将在下一章实现为
    `gradient(self, x, t)`。如果你想节省时间，可以使用 `gradient(self, x, t)` 替代 `numerical_gradient(self,
    x, t)`，因为神经网络训练本身就需要一定的时间。'
- en: Implementing Mini-Batch Training
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现小批量训练
- en: 'Here, we will use mini-batch training to implement neural network training.
    In mini-batch training, we extract some data randomly from training data (called
    a mini-batch) and use it to update the parameters using a gradient method. Let''s
    conduct training for the `TwoLayerNet` class by using the MNIST dataset (the source
    code is located at `ch04/train_neuralnet.py`):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用小批量训练来实现神经网络训练。在小批量训练中，我们从训练数据中随机提取一些数据（称为小批量），并使用它来通过梯度方法更新参数。我们将使用
    MNIST 数据集（源代码位于 `ch04/train_neuralnet.py`）来进行 `TwoLayerNet` 类的训练：
- en: '[PRE32]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, the size of a mini-batch is 100\. Each time, 100 pieces of data (image
    data and correct label data) are extracted randomly from 60,000 pieces of training
    data. Then, gradients are obtained for the mini-batch, and the parameters are
    updated using **stochastic gradient descent** (**SGD**). Here, the number of updates
    made by a gradient method;that is, the number of iterations is 10,000\. At each
    update, the loss function for the training data is calculated, and the value is
    added to the array. *Figure 4.11* shows the graph of how the value of this loss
    function changes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，mini-batch的大小为100。每次，从60,000条训练数据中随机抽取100条数据（图像数据和正确标签数据）。然后，针对该mini-batch计算梯度，并使用**随机梯度下降**（**SGD**）更新参数。这里，梯度方法的更新次数，即迭代次数为10,000。每次更新时，都会计算训练数据的损失函数，并将其值添加到数组中。*图4.11*显示了损失函数值变化的图形。
- en: '*Figure 4.11* shows that, as the number of training increases, the value of
    the loss function decreases. It indicates that training is successful. The weight
    parameters of the neural network are adapting to the data gradually. The neural
    network is indeed learning. By being exposed to data repeatedly, it is approaching
    the optimal weight parameters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11*显示，随着训练次数的增加，损失函数的值逐渐减小。这表明训练成功。神经网络的权重参数正在逐渐适应数据。神经网络确实在学习。通过反复接触数据，神经网络正在接近最优的权重参数：'
- en: '![Figure 4.11: Transition of the loss function – the image on the left shows
    the transition up to 10,000 iterations, while the image on the right shows the
    transition up to 1,000 iterations'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11：损失函数的变化——左侧图像显示了10,000次迭代的变化，右侧图像显示了1,000次迭代的变化'
- en: '](img/fig04_11.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_11.jpg)'
- en: 'Figure 4.11: Transition of the loss function – the image on the left shows
    the transition up to 10,000 iterations, while the image on the right shows the
    transition up to 1,000 iterations'
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.11：损失函数的变化——左侧图像显示了10,000次迭代的变化，右侧图像显示了1,000次迭代的变化
- en: Using Test Data for Evaluation
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用测试数据进行评估
- en: The result of *Figure 4.11* shows that repeatedly training the data reduces
    the value of the loss function gradually. However, the value of the loss function
    is the value of "the loss function for the mini-batch of training data." The reduction
    in the value of the loss function for the training data indicates that the neural
    network is learning well. However, this result does not prove that it can handle
    a different dataset as well as this one.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11*的结果表明，反复训练数据会逐渐减小损失函数的值。然而，损失函数的值是“mini-batch训练数据的损失函数值”。训练数据的损失函数值的减小表明神经网络正在良好地学习。然而，这个结果并不证明它可以像处理当前数据集一样处理不同的数据集。'
- en: In neural network training, we must check whether data other than training data
    can be recognized correctly. We must check whether "overfitting" does not occur.
    Overfitting means that only the number of images contained in the training data
    can be recognized correctly, and those that are not contained there cannot be
    recognized, for example.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练中，我们必须检查是否能正确识别训练数据以外的数据。我们必须检查是否没有发生“过拟合”。过拟合意味着只能正确识别训练数据中包含的图像，而无法识别那些未包含的图像。
- en: The goal of neural network training is to obtain generalization capability.
    To do that, we must use data that is not contained in the training data to evaluate
    the generalization capability of the neural network. In the next implementation,
    we will record the recognition accuracy for the test data and the training data
    periodically during training. We will record the recognition accuracy for the
    test data and the training data for each epoch.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的目标是获得泛化能力。为此，我们必须使用不包含在训练数据中的数据来评估神经网络的泛化能力。在接下来的实现中，我们将在训练过程中定期记录测试数据和训练数据的识别准确率。我们将为每个epoch记录测试数据和训练数据的识别准确率。
- en: Note
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: An epoch is a unit. One epoch indicates the number of iterations when all the
    training data has been used for training. For example, let's assume that 100 mini-batches
    are used to learn 10,000 pieces of training data. After a stochastic gradient
    descent method is repeated 100 times, all the training data has been seen. In
    this case, `100 iterations = 1 epoch`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是一个单位。一个epoch表示所有训练数据用于训练时的迭代次数。例如，假设使用100个mini-batch来学习10,000条训练数据。经过100次随机梯度下降方法的重复，所有训练数据都会被看到。在这种情况下，`100次迭代
    = 1个epoch`。
- en: 'Now, we will change the previous implementation slightly to gain a correct
    evaluation. Here, the differences from the previous implementation are shown in
    bold:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将稍微调整前面的实现，以获得正确的评估。这里，和之前实现的不同之处已用**粗体**标出：
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding example, the recognition accuracy is calculated for all the
    training and test data and the results are recorded for each epoch. The recognition
    accuracy is calculated for each epoch because it takes time if it is calculated
    repeatedly in a `for` statement. Also, we do not need to record recognition accuracy
    frequently (all we need is the approximate transition of recognition accuracy).
    Therefore, the transition of recognition accuracy is recorded for each epoch of
    training data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们计算了所有训练数据和测试数据的识别精度，并在每个训练轮次中记录了结果。每个训练轮次都会计算一次识别精度，因为在`for`语句中重复计算会消耗时间。而且，我们不需要频繁记录识别精度（我们只需要识别精度的近似变化）。因此，记录了每个训练轮次的训练数据识别精度变化。
- en: 'Now, let''s show the results of the preceding code in a graph:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过图表展示前面代码的结果：
- en: '![Figure 4.12: Transition of recognition accuracy for training data and test
    data. The horizontal'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.12：训练数据和测试数据的识别精度变化。横轴'
- en: axis shows the epochs
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 横轴表示训练轮次
- en: '](img/fig04_12.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig04_12.jpg)'
- en: 'Figure 4.12: Transition of recognition accuracy for training data and test
    data. The horizontal axis shows the epochs'
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.12：训练数据和测试数据的识别精度变化。横轴表示训练轮次
- en: In *Figure 4.12*, the solid line shows the recognition accuracy of the training
    data, while the dashed line shows that of the test data. As you can see, as the
    number of epochs increases (training advances), the recognition accuracies for
    both the training data and the test data improve. Here, we can see that the two
    recognition accuracies are almost the same as the two lines mostly overlap. This
    indicates that overfitting did not occur here.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图4.12*中，实线表示训练数据的识别精度，而虚线表示测试数据的识别精度。如您所见，随着训练次数的增加（训练的进展），训练数据和测试数据的识别精度都得到了提高。在这里，我们可以看到两条识别精度曲线几乎重合，这表明没有发生过拟合。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter described neural network training. First, we introduced a `score`
    called a loss function so that a neural network can learn. The goal of neural
    network training is to discover the weight parameters that lead to the smallest
    value of the loss function. Then, we learned how to use the gradient of a function,
    called the gradient method, to discover the smallest loss function value. This
    chapter covered the following points:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了神经网络训练。首先，我们介绍了一个叫做损失函数的`得分`，使神经网络能够学习。神经网络训练的目标是发现能够使损失函数值最小的权重参数。接着，我们学习了如何使用函数的梯度（即梯度法）来发现损失函数的最小值。本章涵盖了以下要点：
- en: In machine learning, we use training data and test data.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中，我们使用训练数据和测试数据。
- en: Training data is used for training, while test data is used to evaluate the
    generalization capability of the trained model.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据用于训练，而测试数据用于评估训练模型的泛化能力。
- en: A loss function is used as a score in neural network training. Weight parameters
    are updated so that the value of the loss function will decrease.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数作为神经网络训练中的得分函数使用。权重参数通过更新，使得损失函数的值逐渐减小。
- en: To update the weight parameters, their gradients are used to update their values
    in the gradient direction repeatedly.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更新权重参数，我们使用它们的梯度，按照梯度方向反复更新它们的值。
- en: Calculating a derivative based on the difference when very small values are
    provided is called numerical differentiation.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当给定非常小的值时，基于差异来计算导数的过程称为数值微分。
- en: You can use numerical differentiation to obtain the gradients for the weight parameters.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用数值微分来获得权重参数的梯度。
- en: Numerical differentiation takes time to calculate, but its implementation is
    easy. On the other hand, backpropagation, which will be described in the next
    chapter, is slightly complicated, but it can calculate gradients quickly.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数值微分需要时间来计算，但其实现较为简单。另一方面，反向传播将在下一章中描述，虽然它稍微复杂一些，但能够快速计算梯度。
