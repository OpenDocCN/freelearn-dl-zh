- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Transfer Learning with BERT
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT进行迁移学习
- en: 'Deep learning models really shine with large amounts of training data. Having
    enough labeled data is a constant challenge in the field, especially in NLP. A
    successful approach that has yielded great results in the last couple of years
    is that of transfer learning. A model is trained in an unsupervised or semi-supervised
    way on a large corpus and then fine-tuned for a specific application. Such models
    have shown excellent results. In this chapter, we will build on the IMDb movie
    review sentiment analysis and use transfer learning to build models using **GloVe**
    (**Global Vectors for Word Representation**) pre-trained embeddings and **BERT**
    (**Bi-Directional Encoder Representations from Transformers**) contextual models.
    In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型在大量训练数据下表现最好。在该领域，特别是在自然语言处理（NLP）中，拥有足够的标注数据一直是一个巨大的挑战。近年来，迁移学习作为一种成功的方法，取得了显著的成果。模型首先在一个大型语料库上以无监督或半监督的方式进行训练，然后针对特定应用进行微调。这些模型已经展示了出色的效果。在本章中，我们将基于IMDb电影评论情感分析的任务，使用迁移学习构建基于**GloVe**（**全局词向量表示**）预训练词向量和**BERT**（**双向编码器表示变换器**）上下文模型的模型。本章将涵盖以下内容：
- en: Overview of transfer learning and use in NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习概述及其在NLP中的应用
- en: Loading pre-trained GloVe embeddings in a model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模型中加载预训练的GloVe词向量
- en: Building a sentiment analysis model using pre-trained GloVe embeddings and fine-tuning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的GloVe词向量和微调构建情感分析模型
- en: Overview of contextual embeddings using Attention – BERT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Attention的上下文词嵌入概述 – BERT
- en: Loading pre-trained BERT models using the Hugging Face library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Hugging Face库加载预训练的BERT模型
- en: Using pre-trained and custom BERT-based fine-tuned models for sentiment analysis
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练和自定义的基于BERT的微调模型进行情感分析
- en: Transfer learning is a core concept that has made rapid advances in NLP possible.
    We will discuss transfer learning first.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是使NLP取得快速进展的核心概念，我们首先讨论迁移学习。
- en: Transfer learning overview
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习概述
- en: Traditionally, a machine learning model is trained for performance on a specific
    task. It is only expected to work for that task and is not likely to have high
    performance beyond that task. Let's take the example of the problem of classifying
    the sentiment of IMDb movie reviews *Chapter 2*, *Understanding Sentiment in Natural
    Language with BiLSTMs*. The model that was trained for this particular task was
    optimized for performance on this task alone. A separate set of labeled data specific
    to a different task is required if we wish to train another model. Building another
    model might not be effective if there isn't enough labeled data for that task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，机器学习模型是针对特定任务的性能进行训练的。它只期望在该任务上有效，并且不太可能在该任务之外表现得很好。以IMDb电影评论情感分类问题为例，参考*第2章*，*使用BiLSTM理解自然语言中的情感*。为这个特定任务训练的模型只针对这个任务进行了优化。如果我们想训练另一个模型，就需要一组与之不同任务相关的标注数据。如果没有足够的标注数据来支持该任务，构建另一个模型可能是无效的。
- en: 'Transfer learning is the concept of learning a fundamental representation of
    the data that can be adapted to different tasks. In the case of transfer learning,
    a more abundantly available dataset may be used to distill knowledge and in building
    a new ML model for a specific task. Through the use of this knowledge, this new
    ML model can have decent performance even when there is not enough labeled data
    available for a traditional ML approach to return good results. For this scheme
    to be effective, there are a few important considerations:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是学习数据的基本表示的概念，这种表示可以适应不同的任务。在迁移学习中，可以使用更丰富的可用数据集来蒸馏知识，并为特定任务构建新的机器学习模型。通过利用这些知识，新的机器学习模型即使在没有足够的标注数据的情况下，也能取得不错的性能，这通常是传统机器学习方法无法获得良好结果的情况。为了使这一方案有效，有几个重要的考虑因素：
- en: The knowledge distillation step, called **pre-training**, should have an abundant
    amount of data available relatively cheaply
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识蒸馏步骤，称为**预训练**，应该有大量的可用数据，而且这些数据相对便宜
- en: Adaptation, often called fine-tuning, should be done with data that shares similarities
    with the data used for pre-training
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应性调整，通常称为微调，应该在与预训练数据相似的数据上进行
- en: 'The figure below illustrates this concept:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示说明了这个概念：
- en: '![](img/B16252_04_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_04_01.png)'
- en: 'Figure 4.1: Comparing traditional machine learning with transfer learning'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：比较传统机器学习与迁移学习
- en: This technique has been very effective in computer vision. ImageNet is often
    used as the dataset for pre-training. Specific models are then fine-tuned for
    a variety of tasks such as image classification, object detection, image segmentation,
    and pose detection, among others.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术在计算机视觉中非常有效。ImageNet通常被用作预训练的数据集。然后，特定的模型会针对各种任务进行微调，如图像分类、物体检测、图像分割和姿势检测等。
- en: Types of transfer learning
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的类型
- en: The concepts of **domains** and **tasks** underpin the concept of transfer learning.
    A domain represents a specific area of knowledge or data. News articles, social
    media posts, medical records, Wikipedia entries, and court judgments could be
    considered examples of different domains. A task is a specific objective or action
    within a domain. Sentiment analysis and stance detection of tweets are specific
    tasks in the social media posts domain. Detection of cancer and fractures could
    be different tasks in the domain of medical records. Different types of transfer
    learning have different combinations of source and target domains and tasks. Three
    main types of transfer learning, namely domain adaptation, multi-task learning,
    and sequential learning, are described below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**领域**和**任务**的概念支撑了迁移学习的概念。领域代表了一个特定的知识或数据领域。新闻文章、社交媒体帖子、医疗记录、维基百科条目和法院判决等可以视为不同领域的例子。任务是在领域中的特定目标或动作。推文的情感分析和立场检测是社交媒体帖子领域中的具体任务。癌症和骨折的检测可能是医疗记录领域中的不同任务。不同类型的迁移学习有不同的源领域和目标领域及任务的组合。下面描述了三种主要的迁移学习类型，分别是领域适应、多任务学习和顺序学习。'
- en: Domain adaptation
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 领域适应
- en: In this setting, the domains of source and target tasks are usually the same.
    However, the differences are related to the distribution of training and testing
    data. This case of transfer learning is related to a fundamental assumption in
    any machine learning task – the assumption that training and testing data are
    *i.i.d*. The first *i* stands for *independent*, which implies that each sample
    is independent of the others. In practice, this assumption can be violated when
    there are feedback loops, like in recommendation systems. The second section is
    *i.d.*, which stands for *identically distributed* and implies that the distribution
    of labels and other characteristics between training and test samples is the same.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，源任务和目标任务的领域通常是相同的。然而，它们之间的差异与训练数据和测试数据的分布有关。这种迁移学习的情况与任何机器学习任务中的一个基本假设有关——假设训练数据和测试数据是*i.i.d.*。第一个*i*代表*独立*，意味着每个样本与其他样本是独立的。在实践中，当存在反馈循环时（如推荐系统中的情况），这一假设可能会被违反。第二部分是*i.d.*，代表*同分布*，意味着训练样本和测试样本之间标签及其他特征的分布是相同的。
- en: Suppose the domain was animal photos, and the task was identifying cats in the
    photos. This task can be modeled as a binary classification problem. The identically
    distributed assumption implies that the distribution of cats in the photos between
    training and test samples is similar. This also implies that characteristics of
    photos, such as resolutions, lighting conditions, and orientations, are very similar.
    In practice, this assumption is also frequently violated.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设领域是动物照片，任务是识别照片中的猫。这个任务可以建模为一个二分类问题。同分布假设意味着训练样本和测试样本中猫的分布是相似的。这也意味着照片的特征，如分辨率、光照条件和方向等，是非常相似的。实际上，这一假设也常常被违反。
- en: There is a case about a very early perceptron model built to identify tanks
    in the woods. The model was performing quite well on the training set. When the
    test set was expanded, it was discovered that all the pictures of tanks in woods
    were taken on sunny days, whereas the pictures of woods without tanks were taken
    on a cloudy day.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个关于早期感知器模型的案例，该模型用于识别树林中的坦克。模型在训练集上表现得相当好。当测试集被扩展时，发现所有的树林中的坦克照片都是在晴天拍摄的，而没有坦克的树林照片则是在阴天拍摄的。
- en: In this case, the network learned to differentiate sunny and cloudy conditions
    more than the presence or absence of tanks. During testing, the pictures supplied
    were from a different distribution, but the same domain, which led to the model
    failing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，网络学会了区分晴天和阴天的条件，而非坦克的有无。在测试过程中，提供的图片来自不同的分布，但属于相同的领域，这导致了模型的失败。
- en: Dealing with similar situations is called domain adaptation. There are many
    techniques for domain adaptation, one of which is data augmentation. In computer
    vision, images in the training set can be cropped, warped, or rotated, and varying
    amounts of exposure or contrast or saturation can be applied to them. These transformations
    would increase the training data and could mitigate the gap between training and
    potential testing data. Similar techniques are used in speech and audio by adding
    random noises, including street sounds or background chatter, to an audio sample.
    Domain adaptation techniques are well known in traditional machine learning with
    several resources already available on it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 处理相似情境的过程被称为领域适应。领域适应有很多技术，其中之一就是数据增强。在计算机视觉中，训练集中的图像可以被裁剪、扭曲或旋转，并可以对其应用不同程度的曝光、对比度或饱和度。这些变换可以增加训练数据量，并且能够减少训练数据和潜在测试数据之间的差距。在语音和音频处理中，也会使用类似的技术，通过向音频样本中添加随机噪声，包括街道声或背景闲聊声来进行增强。领域适应技术在传统机器学习中非常成熟，已有许多相关资源。
- en: However, what makes transfer learning exciting is using data from a different
    source domain or task for pre-training results in improvements in model performance
    on a different task or domain. There are two types of transfer learning in this
    area. The first one is multi-task learning, and the second one is sequential learning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，迁移学习令人兴奋之处在于，使用来自不同源领域或任务的数据进行预训练，可以在另一个任务或领域上提升模型性能。这个领域有两种迁移学习方式。第一种是多任务学习，第二种是序列学习。
- en: Multi-task learning
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多任务学习
- en: 'In multi-task learning, data from different but related tasks are passed through
    a set of common layers. Then, there may be task-specific layers on the top that
    learn about a particular task objective. *Figure 4.2* shows the multi-task learning
    setting:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，不同但相关的任务数据会通过一组共享层进行处理。然后，可能会在顶部添加一些任务特定的层，这些层用于学习特定任务目标的相关信息。*图 4.2*展示了多任务学习的设置：
- en: '![A picture containing building, drawing  Description automatically generated](img/B16252_04_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含建筑物的图片，自动生成的描述](img/B16252_04_02.png)'
- en: 'Figure 4.2: Multi-task transfer learning'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：多任务迁移学习
- en: The output of these task-specific layers would be evaluated on different loss
    functions. All the training examples for all the tasks are passed through all
    the layers of the model. The task-specific layers are not expected to do well
    for all the tasks. The expectation is that the common layers learn some of the
    underlying structure that is shared by the different tasks. This information about
    structure provides useful signals and improves the performance of all the models.
    The data for each task has many features. However, these features may be used
    to construct representations that can be useful in other related tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务特定层的输出将通过不同的损失函数进行评估。所有任务的所有训练样本都会通过模型的所有层进行处理。任务特定层并不期望能够在所有任务上表现良好。期望是共享层能够学习一些不同任务间共享的潜在结构。关于结构的信息提供了有用的信号，并且提升了所有模型的性能。每个任务的数据都有许多特征，但这些特征可以用于构建对其他相关任务有用的表示。
- en: Intuitively, people learn some elementary skills before mastering more complex
    skills. Learning to write requires first becoming skilled in holding a pen or
    pencil. Writing, drawing, and painting can be considered different tasks that
    share a standard "layer" of holding a pen or pencil. The same concept applies
    while learning a new language where the structure and grammar of one language
    may help with learning a related language. Learning Latin-based languages like
    French, Italian, and Spanish becomes more comfortable if one of the other Latin
    languages is known, as these languages share word roots.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，人们在掌握更复杂技能之前，通常会先学习一些基础技能。学习写字首先需要掌握握笔或铅笔的技巧。写字、绘画和画画可以视为不同的任务，它们共享一个标准的“层”，即握笔或铅笔的技能。同样的概念适用于学习新语言的过程，其中一种语言的结构和语法可能有助于学习相关语言。学习拉丁语系语言（如法语、意大利语和西班牙语）会变得更加轻松，如果你已经掌握了其中一种拉丁语言，因为这些语言共享词根。
- en: Multi-task learning increases the amount of data available for training by pooling
    data from different tasks together. Further, it forces the network to generalize
    better by trying to learn representations that are common across tasks in shared
    layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习通过将来自不同任务的数据汇聚在一起，从而增加了可用于训练的数据量。此外，它通过在共享层中尝试学习任务间通用的表示，强制网络更好地进行泛化。
- en: Multi-task learning is a crucial reason behind the recent success of models
    such as GPT-2 and BERT. It is the most common technique used for pre-training
    models that are then used for specific tasks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习是近年来像GPT-2和BERT这样的模型取得成功的关键原因。它是预训练模型时最常用的技术，之后这些模型可以用于特定任务。
- en: Sequential learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列学习
- en: 'Sequential learning is the most common form of transfer learning. It is named
    so because it involves two simple steps executed in sequence. The first step is
    pre-training and the second step is fine-tuning. These steps are shown in *Figure
    4.3*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 序列学习是最常见的迁移学习形式。之所以这样命名，是因为它包含了两个按顺序执行的简单步骤。第一步是预训练，第二步是微调。这些步骤如*图 4.3*所示：
- en: '![](img/B16252_04_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_04_03.png)'
- en: 'Figure 4.3: Sequential learning'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：序列学习
- en: The first step is to pre-train a model. The most successful pre-trained models
    use some form of multi-task learning objectives, as depicted on the left side
    of the figure. A portion of the model used for pre-training is then used for different
    tasks shown on the right in the figure. This reusable part of the pre-trained
    model depends on the specific architecture and may have a different set of layers.
    The reusable partition shown in *Figure 4.3* is just illustrative. In the second
    step, the pre-trained model is loaded and added as the starting layer of a task-specific
    model. The weights learned by the pre-trained model can be frozen during the training
    of the task-specific model, or those weights can be updated or fine-tuned. When
    the weights are frozen, then this pattern of using the pre-trained model is called
    *feature extraction*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是预训练一个模型。最成功的预训练模型使用某种形式的多任务学习目标，如图中左侧所示。用于预训练的模型的一部分随后会用于图中右侧所示的不同任务。这个可重用的预训练模型部分依赖于具体的架构，可能具有不同的层。图
    4.3 中显示的可重用部分仅为示意图。在第二步中，预训练模型被加载并作为任务特定模型的起始层。预训练模型学到的权重可以在任务特定模型训练时被冻结，或者这些权重可以更新或微调。当权重被冻结时，这种使用预训练模型的模式称为*特征提取*。
- en: Generally, fine-tuning gives better performance than a feature extraction approach.
    However, there are some pros and cons to both approaches. In fine-tuning, not
    all weights get updated as the task-specific training data may be much smaller
    in size. If the pre-trained model is an embedding for words, then other embeddings
    can become stale. If the task is such that it has a small vocabulary or has many
    out-of-vocabulary words, then this can hurt the performance of the model. Generally,
    if the source and target tasks are similar, then fine-tuning would produce better
    results.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，微调相比于特征提取方法会提供更好的性能。然而，这两种方法各有优缺点。在微调中，并非所有权重都会被更新，因为任务特定的训练数据可能较小。如果预训练模型是单词的嵌入，那么其他嵌入可能会变得过时。如果任务的词汇量较小或包含许多词汇外的单词，这可能会影响模型的性能。通常来说，如果源任务和目标任务相似，微调会产生更好的结果。
- en: An example of such a pre-trained model is Word2vec, which we saw in *Chapter
    1*, *Essentials of NLP*. There is another model of generating word-level embeddings
    called **GloVe** or **Global Vectors for Word Representation**, introduced in
    2014 by researchers from Stanford. Let's take a practical tour of transfer learning
    by re-building the IMDb movie sentiment analysis using GloVe embeddings in the
    next section. After that, we shall take a tour of BERT and apply BERT in the same
    sequential learning setting.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一个预训练模型的例子是Word2vec，我们在*第1章*，*自然语言处理基础*中提到过。还有一个生成词级嵌入的模型叫做**GloVe**，即**全局词表示向量**，由斯坦福大学的研究人员于2014年提出。接下来，我们将通过在下一节中使用GloVe嵌入重新构建IMDb电影情感分析，来进行一次实际的迁移学习之旅。之后，我们将探讨BERT并在同样的序列学习环境下应用BERT。
- en: IMDb sentiment analysis with GloVe embeddings
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IMDb情感分析与GloVe嵌入
- en: In *Chapter 2*, *Understanding Sentiment in Natural Language with BiLSTMs*,
    a BiLSTM model was built to predict the sentiment of IMDb movie reviews. That
    model learned embeddings of the words from scratch. This model had an accuracy
    of 83.55% on the test set, while the SOTA result was closer to 97.4%. If pre-trained
    embeddings are used, we expect an increase in model accuracy. Let's try this out
    and see the impact of transfer learning on this model. But first, let's understand
    the GloVe embedding model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*，*使用 BiLSTM 理解自然语言情感*中，构建了一个 BiLSTM 模型来预测 IMDb 电影评论的情感。该模型从头开始学习词的嵌入。该模型在测试集上的准确率为
    83.55%，而当前最先进的结果接近 97.4%。如果使用预训练的嵌入，我们预期模型的准确性会有所提升。让我们试试看，并了解迁移学习对这个模型的影响。但首先，我们来了解一下
    GloVe 嵌入模型。
- en: GloVe embeddings
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GloVe 嵌入
- en: In *Chapter 1*, *Essentials of NLP*, we discussed the Word2Vec algorithm, which
    is based on skip-grams with negative sampling. The GloVe model came out in 2014,
    a year after the Word2Vec paper came out. The GloVe and Word2Vec models are similar
    as the embeddings generated for a word are determined by the words that occur
    around it. However, these context words occur with different frequencies. Some
    of these context words appear more frequently in the text compared to other words.
    Due to this difference in frequencies of occurrence, training data for some words
    may be more common than other words.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第1章*，*NLP 基础*中，我们讨论了基于负采样跳字模型的 Word2Vec 算法。GloVe 模型于 2014 年发布，比 Word2Vec 论文晚了一年。GloVe
    和 Word2Vec 模型相似，都是通过周围的单词来确定一个词的嵌入。然而，这些上下文单词的出现频率不同。有些上下文单词在文本中出现的频率高于其他单词。由于这种出现频率的差异，某些词的训练数据可能比其他词更常见。
- en: Beyond this part, Word2Vec does not use these statistics of co-occurrence in
    any way. GloVe takes these frequencies into account and posits that the co-occurrences
    provide vital information. The *Global* part of the name refers to the fact that
    the model considers these co-occurrences over the entire corpus. Rather than focus
    on the probabilities of co-occurrence, GloVe focuses on the ratios of co-occurrence
    considering probe words.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 超过这一部分，Word2Vec 并未以任何方式使用这些共现统计数据。GloVe 考虑了这些频率，并假设共现提供了重要的信息。名称中的*Global*部分指的是模型在整个语料库中考虑这些共现的事实。GloVe
    并不专注于共现的概率，而是专注于考虑探测词的共现比率。
- en: In the paper, the authors take the example of the words *ice* and *steam* to
    illustrate the concept. Let's say that *solid* is another word that is going to
    be used to probe the relationship between ice and steam. A probability of occurrence
    of solid given steam is *p*[solid|steam]. Intuitively, we expect this probability
    to be small. Conversely, the probability of occurrence of solid with ice is represented
    by *p*[solid|ice] and is expected to be large. If ![](img/B16252_04_001.png) is
    computed, we expect this value to be significant. If the same ratio is computed
    with the probe word being gas, the opposite behavior would be expected. In cases
    where both are equally probable, either due to the probe word being unrelated,
    or equally probable to occur with the two words, then the ratio should be closer
    to 1\. An example of a probe word close to both ice and steam is *water*. An example
    of a word unrelated to ice or steam is *fashion*. GloVe ensures that this relationship
    is factored into the embeddings generated for the words. It also has optimizations
    for rare co-occurrences, numerical stability issues computation, and others.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者以*冰*和*蒸汽*为例来说明这一概念。假设*固体*是另一个将用于探测冰和蒸汽关系的词。给定蒸汽时，固体的出现概率为*p*[solid|steam]。直观上，我们预期这个概率应该很小。相反，固体与冰一起出现的概率表示为*p*[solid|ice]，预计会很大。如果计算
    ![](img/B16252_04_001.png)，我们预期这个值会很显著。如果用气体作为探测词来计算同样的比率，我们会预期相反的行为。如果两者出现的概率相等，不论是由于探测词与之无关，还是与两个词出现的概率相同，那么比率应接近
    1。一个同时与冰和蒸汽都相关的探测词是*水*。一个与冰或蒸汽无关的词是*时尚*。GloVe 确保这种关系被纳入到生成的词嵌入中。它还优化了稀有共现、数值稳定性问题的计算等方面。
- en: Now let us see how to use these pre-trained embeddings for predicting sentiment.
    The first step is to load the data. The code here is identical to the code used
    in *Chapter 2*, *Understanding Sentiment in Natural Language with BiLSTMs*; it's
    provided here for the sake of completeness.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用这些预训练的嵌入来预测情感。第一步是加载数据。这里的代码与*第2章*，*使用 BiLSTM 理解自然语言情感*中的代码相同；此处提供是为了完整性。
- en: All the code for this exercise is in the file `imdb-transfer-learning.ipynb`
    located in the `chapter4-Xfer-learning-BERT` directory in GitHub.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的所有代码都在GitHub的`chapter4-Xfer-learning-BERT`目录中的文件`imdb-transfer-learning.ipynb`里。
- en: Loading IMDb training data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载IMDb训练数据
- en: 'TensorFlow Datasets or the `tfds` package will be used to load the data:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Datasets或`tfds`包将被用于加载数据：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the additional 50,000 reviews that are unlabeled are ignored for
    the purpose of this exercise. After the training and test sets are loaded as shown
    above, the content of the reviews needs to be tokenized and encoded:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，额外的50,000条未标注的评论在本练习中被忽略。训练集和测试集加载完毕后，评论内容需要进行分词和编码：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The code shown above tokenizes the review text and constructs a vocabulary.
    This vocabulary is used to construct a tokenizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的代码对评论文本进行了分词，并构建了一个词汇表。这个词汇表用于构建分词器：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that text was converted to lowercase before encoding. Converting to lowercase
    helps reduce the vocabulary size and may benefit the lookup of corresponding GloVe
    vectors later on. Note that capitalization may contain important information,
    which may help in tasks such as NER, which we covered in previous chapters. Also
    note that all languages do not distinguish between capital and small letters.
    Hence, this particular transformation should be applied after due consideration.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在编码之前，文本已经被转换为小写。转换为小写有助于减少词汇量，并且可能有利于后续查找对应的GloVe向量。注意，大写字母可能包含重要信息，这对命名实体识别（NER）等任务有帮助，我们在前面的章节中已涉及过。此外，并非所有语言都区分大写和小写字母。因此，这一转换应在充分考虑后执行。
- en: 'Now that the tokenizer is ready, the data needs to be tokenized, and sequences
    padded to a maximum length. Since we are interested in comparing performance with
    the model trained in *Chapter 2*, *Understanding Sentiment in Natural Language
    with BiLSTMs*, we can use the same setting of sampling a maximum of 150 words
    of the review. The following convenience methods help in performing this task:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在分词器已准备好，数据需要进行分词，并将序列填充到最大长度。由于我们希望与*第二章*《使用BiLSTM理解自然语言中的情感》中训练的模型进行性能比较，因此可以使用相同的设置，即最多采样150个评论词汇。以下便捷方法有助于执行此任务：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the data is encoded using the convenience functions above like so:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据使用上面提到的便捷函数进行编码，如下所示：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: At this point, all the training and test data is ready for training.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，所有的训练和测试数据已准备好进行训练。
- en: Note that in limiting the size of the reviews, only the first 150 tokens will
    be counted for a long review. Typically, the first few sentences of the review
    have the context or description, and the latter part of the review has the conclusion.
    By limiting to the first part of the review, valuable information could be lost.
    The reader is encouraged to try a different padding scheme where tokens from the
    first part of the review are dropped instead of the second part and observe the
    difference in the accuracy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在限制评论大小时，长评论只会计算前150个词。通常，评论的前几句包含上下文或描述，后半部分则是结论。通过限制为评论的前部分，可能会丢失有价值的信息。建议读者尝试一种不同的填充方案，即丢弃评论前半部分的词汇，而不是后半部分，并观察准确度的变化。
- en: The next step is the foremost step in transfer learning – loading the pre-trained
    GloVe embeddings and using these as the weights of the embedding layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是迁移学习中的关键步骤——加载预训练的GloVe嵌入，并将其用作嵌入层的权重。
- en: Loading pre-trained GloVe embeddings
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载预训练的GloVe嵌入
- en: 'First, the pre-trained embeddings need to be downloaded and unzipped:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要下载并解压预训练的嵌入：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that this is a huge download of over 800 MB, so this step may take some
    time to execute. Upon unzipping, there will be four different files, as shown
    in the output above. Each file has a vocabulary of 400,000 words. The main difference
    is the dimensions of embeddings generated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个下载文件很大，超过800MB，因此执行这一操作可能需要一些时间。解压后，会有四个不同的文件，如上面的输出所示。每个文件包含40万个词汇。主要的区别是生成的嵌入维度不同。
- en: 'In the previous chapter, an embedding dimension of 64 was used for the model.
    The nearest GloVe dimension is 50, so let''s use that. The file format is quite
    simple. Each line of the text has multiple values separated by spaces. The first
    item of each row is the word, and the rest of the items are the values of the
    vector for each dimension. So, in the 50-dimensional file, each row will have
    51 columns. These vectors need to be loaded up in memory:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，模型使用了 64 的嵌入维度。最近的 GloVe 维度是 50，所以我们将使用该维度。文件格式非常简单。每一行文本有多个由空格分隔的值。每行的第一个项目是单词，其余项目是每个维度的向量值。因此，在
    50 维的文件中，每一行将有 51 列。这些向量需要加载到内存中：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the code processed the file correctly, you shouldn't see any errors and you
    should see a dictionary size of 400,000 words. Once these vectors are loaded,
    an embedding matrix needs to be created.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代码正确处理了文件，你应该不会看到任何错误，并且字典大小应为 400,000 个单词。一旦这些向量加载完成，就需要创建一个嵌入矩阵。
- en: Creating a pre-trained embedding matrix using GloVe
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GloVe 创建预训练的嵌入矩阵
- en: 'So far, we have a dataset, its vocabulary, and a dictionary of GloVe words
    and their corresponding vectors. However, there is no correlation between these
    two vocabularies. The way to connect them is through the creation of an embedding
    matrix. First, let''s initialize an embedding matrix of zeros:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了数据集、它的词汇表以及 GloVe 单词及其对应向量的字典。然而，这两个词汇表之间没有关联。将它们连接起来的方式是通过创建一个嵌入矩阵。首先，我们来初始化一个全零的嵌入矩阵：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that this is a crucial step. When a pre-trained word list is used, finding
    a vector for each word in the training/test is not guaranteed. Recall the discussion
    on transfer learning earlier, where the source and target domains are different.
    One way this difference manifests itself is through having a mismatch in tokens
    between the training data and the pre-trained model. As we go through the next
    steps, this will become more apparent.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个关键步骤。当使用预训练的词汇表时，无法保证在训练/测试过程中为每个单词找到向量。回想一下之前关于迁移学习的讨论，其中源域和目标域是不同的。差异的一种表现形式是训练数据与预训练模型之间的标记不匹配。随着我们继续进行接下来的步骤，这一点会变得更加明显。
- en: After this embedding matrix of zeros is initialized, it needs to be populated.
    For each word in the vocabulary of reviews, the corresponding vector is retrieved
    from the GloVe dictionary.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化了这个全零的嵌入矩阵之后，需要对其进行填充。对于评论词汇表中的每个单词，从 GloVe 字典中获取相应的向量。
- en: 'The ID of the word is retrieved using the encoder, and then the embedding matrix
    entry corresponding to that entry is set to the retrieved vector:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的 ID 是通过编码器获取的，然后将与该条目对应的嵌入矩阵条目设置为获取到的向量：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: During the data loading step, we saw that the total number of tokens was 93,931\.
    Out of these, 14,553 words could not be found, which is approximately 15% of the
    tokens. For these words, the embedding matrix will have zeros. This is the first
    step in transfer learning. Now that the setup is completed, we will need to use
    TensorFlow to use these pre-trained embeddings. There will be two different models that
    will be tried – the first will be based on feature extraction and the second one
    on fine-tuning.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据加载步骤中，我们看到总共有 93,931 个标记。其中 14,553 个单词无法找到，大约占标记的 15%。对于这些单词，嵌入矩阵将是零。这是迁移学习的第一步。现在设置已经完成，我们将需要使用
    TensorFlow 来使用这些预训练的嵌入向量。将尝试两种不同的模型——第一种基于特征提取，第二种基于微调。
- en: Feature extraction model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征提取模型
- en: As discussed earlier, the feature extraction model freezes the pre-trained weights
    and does not update them. An important issue with this approach in the current
    setup is that there are a large number of tokens, over 14,000, that have zero
    embedding vectors. These words could not be matched to an entry in the GloVe word
    list.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，特征提取模型冻结了预训练的权重，并且不更新它们。当前设置中这种方法的一个重要问题是，有大量的标记（超过 14,000 个）没有嵌入向量。这些单词无法与
    GloVe 单词列表中的条目匹配。
- en: To minimize the chances of not finding matches between the pre-trained vocabulary
    and task-specific vocabulary, ensure that similar tokenization schemes are used.
    GloVe uses a word-based tokenization scheme like the one provided by the Stanford
    tokenizer. As seen in *Chapter 1*, *Essentials of NLP*, this works better than
    a whitespace tokenizer, which is used for the training data above. We see 15%
    unmatched tokens due to different tokenizers. As an exercise, the reader can implement
    the Stanford tokenizer and see the reduction in unknown tokens.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽量减少预训练词汇表和任务特定词汇表之间未匹配的情况，确保使用相似的分词方案。GloVe使用的是基于单词的分词方案，类似于斯坦福分词器提供的方案。如*第1章*所述，*自然语言处理基础*，这种方法比上述用于训练数据的空格分词器效果更好。我们发现由于不同的分词器，存在15%的未匹配词汇。作为练习，读者可以实现斯坦福分词器并查看未识别词汇的减少情况。
- en: Newer methods like BERT use parts of subword tokenizers. Subword tokenization
    schemes can break up words into parts, which minimizes this chance of mismatch
    in tokens. Some examples of subword tokenization schemes are **Byte Pair Encoding**
    (**BPE**) or WordPiece tokenization. The BERT section of this chapter explains
    subword tokenization schemes in more detail.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更新的方法，如BERT，使用部分子词分词器。子词分词方案可以将单词分解为更小的部分，从而最小化词汇不匹配的机会。一些子词分词方案的例子包括**字节对编码**（**BPE**）或WordPiece分词法。本章中的BERT部分更详细地解释了子词分词方案。
- en: If pre-trained vectors were not used, then the vectors for all the words would
    start with nearly zero and get trained through gradient descent. In this case,
    the vectors are already trained, so we expect the training to go along much faster.
    For a baseline, one epoch of training of the BiLSTM model while training embeddings
    takes between 65 seconds and 100 seconds, with most values around 63 seconds on
    an Ubuntu machine with an i5 processor and an Nvidia RTX-2070 GPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有使用预训练的词向量，那么所有单词的向量将几乎从零开始，通过梯度下降进行训练。在这种情况下，词向量已经经过训练，因此我们期望训练会更快。作为基准，BiLSTM模型在训练词嵌入时，一个周期的训练大约需要65秒到100秒之间，大多数值大约在63秒左右，这是在一台配有i5处理器和Nvidia
    RTX-2070 GPU的Ubuntu机器上进行的。
- en: 'Now, let''s build the model and plug in the embedding matrix generated above
    into the model. Some basic parameters need to be set up:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建模型，并将上面生成的嵌入矩阵插入到模型中。需要设置一些基本参数：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A convenience function being set up will enable fast switching. This method
    enables building models with the same architecture but different hyperparameters:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个便捷的函数可以实现快速切换。这个方法使得可以使用相同的架构但不同的超参数来构建模型：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model is identical to what was used in the previous chapter with the exception
    of the highlighted code pieces above. First, a flag can now be passed to this
    method that specifies whether the embeddings should be trained further or frozen.
    This parameter is set to false as it''s the default value. The second change is
    in the definition of the `Embedding` layer. A new parameter, `weights`, loads
    the embedding matrix as the weights for the layer. Just after this parameter,
    a Boolean parameter called `trainable` is passed that determines whether the weights
    of this layer should be updated during training time. A feature extraction-based
    model can now be created like so:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型与上一章中使用的模型完全相同，唯一不同的是上述的高亮代码片段。首先，现在可以传递一个标志来指定是否进一步训练词嵌入或冻结它们。此参数默认设置为false。第二个变化出现在`Embedding`层的定义中。一个新参数`weights`用于将嵌入矩阵作为层的权重加载。在这个参数之后，传递了一个布尔参数`trainable`，该参数决定在训练过程中该层的权重是否应更新。现在可以像这样创建基于特征提取的模型：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This model has about 4.8 million trainable parameters. It should be noted that
    this model is considerably smaller than the previous BiLSTM model, which had over
    12 million parameters. A simpler or smaller model will train faster and possibly
    be less likely to overfit as the model capacity is lower.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型大约有480万个可训练参数。需要注意的是，这个模型比之前的BiLSTM模型小得多，后者有超过1200万个参数。一个更简单或更小的模型将训练得更快，并且由于模型容量较小，可能更不容易发生过拟合。
- en: This model needs to be compiled with the loss function, optimizer, and metrics
    for observation progress of the model. Binary cross-entropy is the right loss
    function for this problem of binary classification. The Adam optimizer is a decent
    choice in most cases.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型需要使用损失函数、优化器和度量指标进行编译，以便观察模型的进展。二元交叉熵是处理二元分类问题的合适损失函数。Adam优化器在大多数情况下是一个不错的选择。
- en: '**Adaptive Moment Estimation or Adam Optimizer**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**自适应矩估计或Adam优化器**'
- en: The simplest optimization algorithm used in backpropagation for the training
    of deep neural networks is mini-batch **Stochastic Gradient Descent** (**SGD**).
    Any error in the prediction is propagated back and weights, called parameters,
    of the various units are adjusted according to the error. Adam is a method that
    eliminates some of the issues of SGD such as getting trapped in sub-optimal local
    optima, and having the same learning rate for each parameter. Adam computes adaptive
    learning rates for each parameter and adjusts them based on not only the error
    but also previous adjustments. Consequently, Adam converges much faster than other
    optimization methods and is recommended as the default choice.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络训练中的反向传播中，最简单的优化算法是小批量**随机梯度下降**（**SGD**）。任何预测误差都会反向传播，调整各个单元的权重（即参数）。Adam是一个方法，解决了SGD的一些问题，例如陷入次优局部极小值，以及对每个参数使用相同的学习率。Adam为每个参数计算自适应学习率，并根据误差以及先前的调整来调整它们。因此，Adam比其他优化方法收敛得更快，并且被推荐作为默认选择。
- en: 'The metrics that will be observed are the same as before, accuracy, precision,
    and recall:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将观察到的指标与之前相同，包括准确率、精确率和召回率：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After setting up batches for preloading, the model is ready for training. Similar
    to previously, the model will be trained for 10 epochs:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好预加载批次后，模型准备进行训练。与之前相似，模型将训练10个周期：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: A few things can be seen immediately. The model trained significantly faster.
    Each epoch took approximately 17 seconds with a maximum of 28 seconds for the
    first epoch. Secondly, the model has not overfit. The final accuracy is just over
    81% on the training set. In the previous setup, the accuracy on the training set
    was 99.56%.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点可以立刻看出。该模型的训练速度显著提高。每个周期大约需要17秒，第一个周期最多需要28秒。其次，模型没有过拟合。训练集上的最终准确率略高于81%。在之前的设置中，训练集上的准确率是99.56%。
- en: It should also be noted that the accuracy was still increasing at the end of
    the tenth epoch, with lots of room to go. This indicates that training this model
    for longer would probably increase accuracy further. Quickly changing the number
    of epochs to 20 and training the model yields an accuracy of just over 85% on
    the testing set, with precision at 80% and recall at 92.8%.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，在第十个周期结束时，准确率仍在上升，仍有很大的提升空间。这表明，继续训练该模型可能会进一步提高准确率。将周期数快速更改为20并训练模型，测试集上的准确率达到了85%以上，精确率为80%，召回率为92.8%。
- en: 'For now, let''s understand the utility of this model. To make an assessment
    of the quality of this model, performance on the test set should be evaluated:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解这个模型的实用性。为了评估该模型的质量，应该在测试集上评估其表现：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Compared to the previous model's accuracy of 83.6% on the test set, this model
    produces an accuracy of 82.82%. This performance is quite impressive because this
    model is just 40% of the size of the previous model and represents a 70% reduction
    in training time for a less than 1% drop in accuracy. This model has a slightly
    better recall for slightly worse accuracy. This result should not be entirely
    unexpected. There are over 14,000 word vectors that are zeros in this model! To
    fix this issue, and also to try the fine-tuning sequential transfer learning approach,
    let's build a fine-tuning-based model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前模型在测试集上83.6%的准确率相比，这个模型的准确率为82.82%。这个表现相当令人印象深刻，因为该模型的体积仅为前一个模型的40%，并且训练时间减少了70%，而准确率仅下降不到1%。该模型的召回率略高，准确率稍差。这个结果并不完全出乎意料。该模型中有超过14,000个零向量！为了解决这个问题，并且尝试基于微调的顺序迁移学习方法，我们来构建一个基于微调的模型。
- en: Fine-tuning model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调模型
- en: 'Creating the fine-tuning model is trivial when using the convenience function.
    All that is needed is to pass the `train_emb` parameter as true:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用便捷函数创建微调模型非常简单。只需要将`train_emb`参数设置为true即可：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This model is identical to the feature extraction model in size. However, since
    the embeddings will be fine-tuned, training is expected to take a little longer.
    There are several thousand zero embeddings, which can now be updated. The resulting
    accuracy is expected to be much better than the previous model. The model is compiled
    with the same loss function, optimizer, and metrics, and trained for 10 epochs:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的大小与特征提取模型相同。然而，由于嵌入向量将会被微调，训练预计会稍微耗时一些。现在可以更新几千个零嵌入。最终的准确率预计会比之前的模型好得多。该模型使用相同的损失函数、优化器和指标进行编译，并训练了10个周期：
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This accuracy is very impressive but needs to be checked against the test set:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个准确率非常令人印象深刻，但需要在测试集上验证：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: That is the best result we have obtained so far at an accuracy of 87.1%. Data
    about state-of-the-art results on datasets are maintained by the [paperswithcode.com](http://paperswithcode.com)
    website. Research papers that have reproducible code are featured on the leaderboards
    for datasets. This result would be about seventeenth on the SOTA result on the
    [paperswithcode.com](http://paperswithcode.com) website at the time of writing!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们目前为止取得的最佳结果，准确率为87.1%。关于数据集的最新成果数据可以通过[paperswithcode.com](http://paperswithcode.com)网站获取。具有可重复代码的研究论文会在数据集的排行榜上展示。这个结果在撰写时大约排在[paperswithcode.com](http://paperswithcode.com)网站的SOTA（最新技术）结果中第十七位！
- en: It can also be seen that the network is overfitting a little bit. A `Dropout`
    layer can be added between the `Embedding` layer and the first `LSTM` layer to
    help reduce this overfitting. It should also be noted that this network is still
    much faster than training embeddings from scratch. Most epochs took 24 seconds
    for training. Overall, this model is smaller in size, takes much less time to
    train, and has much higher accuracy! This is why transfer learning is so important
    in machine learning in general and NLP more specifically.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以看出，网络有一点过拟合。可以在`Embedding`层和第一个`LSTM`层之间添加一个`Dropout`层，以帮助减少这种过拟合。还需要注意的是，这个网络的训练速度仍然比从头开始训练嵌入要快得多。大多数训练周期仅花费了24秒。总体而言，这个模型的体积较小，训练所需时间少，且准确率更高！这就是为什么迁移学习在机器学习和自然语言处理（NLP）中如此重要的原因。
- en: So far, we have seen the use of context-free word embeddings. The major challenge
    with this approach is that a word could have multiple meanings depending on the
    context. The word *bank* could refer to a place for storing money and valuables
    and also the side of a river. A more recent innovation in this area is BERT, published
    in May 2019\. The next step in improving the accuracy of movie review sentiment
    analysis is to use a pre-trained BERT model. The next section explains the BERT
    model, its vital innovations, and the impact of using this model for the task
    at hand. Please note that the BERT model is enormous! If you do not have adequate
    local computing resources, using Google Colab with a GPU accelerator would be
    an excellent choice for the next section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到上下文无关的词嵌入的使用。使用这种方法的主要挑战在于，单词根据上下文可能有多重含义。比如单词*bank*，它既可以指存放钱财和贵重物品的地方，也可以指河岸。在这个领域中，最近的创新是BERT，它在2019年5月发布。提高电影评论情感分析准确性的下一步是使用一个预训练的BERT模型。下一部分将解释BERT模型、其重要的创新以及使用该模型进行当前任务的影响。请注意，BERT模型非常庞大！如果您的本地计算资源不足，使用带有GPU加速器的Google
    Colab是下一部分的一个绝佳选择。
- en: BERT-based transfer learning
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于BERT的迁移学习
- en: Embeddings like GloVe are context-free embeddings. Lack of context can be limiting
    in NLP contexts. As discussed before, the word bank can mean different things
    depending on the context. **Bi-directional Encoder Representations** **from Transformers**,
    or **BERT**, came out of Google Research in May 2019 and demonstrated significant
    improvements on baselines. The BERT model builds on several innovations that came
    before it. The BERT paper also introduces several innovations of ERT works.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 像GloVe这样的嵌入是上下文无关的嵌入。在自然语言处理的语境下，缺乏上下文可能会有一定的局限性。正如之前讨论的，单词bank在不同的上下文中可以有不同的意思。**双向编码器表示**
    **来自变换器**，或称**BERT**，是Google研究团队于2019年5月发布的，展示了在基准测试中的显著改进。BERT模型建立在之前的多个创新之上。BERT的论文还介绍了ERT工作中的几项创新。
- en: Two foundational advancements that enabled BERT are the **encoder-decoder network**
    architecture and the **Attention mechanism**. The Attention mechanism further
    evolved to produce the **Transformer architecture**. The Transformer architecture
    is the fundamental building block of BERT. These concepts are covered next and
    detailed further in later chapters. After these two sections, we will discuss
    specific innovations and structures of the BERT model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使BERT成为可能的两个基础性进展是**编码器-解码器网络**架构和**注意力机制**。注意力机制进一步发展，产生了**变换器架构**。变换器架构是BERT的基础构建块。这些概念将在接下来的章节中详细介绍。在这两个部分之后，我们将讨论BERT模型的具体创新和结构。
- en: Encoder-decoder networks
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器网络
- en: We have seen the use of LSTMs and BiLSTMs on sentences modeled as sequences
    of words. These sequences can be of varying lengths as sentences are composed
    of a different number of words. Recall that in *Chapter 2*, *Understanding Sentiment
    in Natural Language with BiLSTMs*, we discussed the core concept of an LSTM being
    a unit unrolled in time. For each input token, the LSTM unit generated an output.
    Consequently, the number of outputs produced by the LSTM depends on the number
    of input tokens. All of these input tokens are combined through a `TimeDistributed()`
    layer for use by later `Dense()` layers in the network. The main issue is that
    the input and output sequence lengths are linked. This model cannot handle variable-length
    sequences effectively. Translation-type tasks where the input and the output may
    have different lengths, consequently, won't do well with this architecture.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了 LSTM 和 BiLSTM 在将句子建模为单词序列中的应用。这些序列可以具有不同的长度，因为句子的单词数量不同。回想一下在 *第 2 章*，*利用
    BiLSTM 理解自然语言中的情感* 中，我们讨论了 LSTM 的核心概念是一个按时间展开的单元。对于每个输入符号，LSTM 单元都会生成一个输出。因此，LSTM
    产生的输出数量取决于输入符号的数量。所有这些输入符号通过 `TimeDistributed()` 层结合起来，以供后续网络中的 `Dense()` 层使用。主要问题在于输入和输出序列的长度是相互关联的。该模型无法有效处理变长序列。因此，输入和输出可能具有不同长度的翻译类任务将无法很好地适应这种架构。
- en: The solution to these challenges was posed in a paper titled *Sequence to Sequence
    Learning with Neural Networks* written by Ilya Sutskever et al. in 2014\. This
    model is also referred to as the **seq2seq** model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战的方法是在 2014 年由 Ilya Sutskever 等人撰写的论文《*Sequence to Sequence Learning with
    Neural Networks*》中提出的。该模型也被称为 **seq2seq** 模型。
- en: 'The basic idea is shown in the figure below:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想如下面的图所示：
- en: '![](img/B16252_04_04.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_04_04.png)'
- en: 'Figure 4.4: Encoder-decoder network'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：编码器-解码器网络
- en: The model is divided into two parts – an encoder and a decoder. A special token
    that denotes the end of the input sequence is appended to the input sequence.
    Note that now the input sequence can have any length as this end of sentence token,
    (**EOS**) in the figure above, denotes the end. In the figure above, the input
    sequence is denoted by tokens (**I**[1], **I**[2], **I**[3],…). Each input token,
    after vectorization, is passed to an LSTM model. The output is only collected
    from the last (**EOS**) token. The vector generated by the encoder LSTM network
    for the (**EOS**) token is a representation of the entire input sequence. It can
    be thought of as a summary of the entire input. A variable-length sequence has
    not been transformed into a fixed-length or dimensional vector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型分为两个部分——编码器和解码器。一个特殊的符号用于表示输入序列的结束，并附加到输入序列中。请注意，现在输入序列的长度可以是任意的，因为上述图中的这个句子结束符号（**EOS**）表示输入的结束。在上述图中，输入序列由符号（**I**[1]，**I**[2]，**I**[3]，…）表示。每个输入符号在向量化后传递给
    LSTM 模型。输出只从最后一个（**EOS**）符号收集。编码器 LSTM 网络为（**EOS**）符号生成的向量是整个输入序列的表示。它可以被看作是整个输入的总结。一个变长序列并没有被转换成固定长度或维度的向量。
- en: This vector becomes the input to the decoder layer. The model is auto-regressive
    in the sense that the output generated by the previous step of the decoder is
    fed into the next step as input. Output generation continues until the special
    (**EOS**) token is generated. This scheme allows the model to determine the length
    of the output sequence. It breaks apart the dependency between the length of the
    input and output sequences. Conceptually, this is a straightforward model to understand.
    However, this is a potent model. Many tasks can be cast as a sequence-to-sequence
    problem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量成为解码器层的输入。该模型是自回归的，意味着解码器前一步生成的输出被输入到下一步作为输入。输出生成会持续进行，直到生成特殊的（**EOS**）符号为止。这种方案允许模型确定输出序列的长度。它打破了输入序列和输出序列长度之间的依赖关系。从概念上讲，这是一个易于理解的模型。然而，这是一个强大的模型。许多任务可以被转化为序列到序列的问题。
- en: Some examples include translating a sentence from one language to another, summarizing
    an article where the input sequence is the text of the article and the output
    sequence is the summary, or question-answering where the question is the input
    sequence and the output is the answer. Speech recognition is a sequence-to-sequence
    problem with input sequences of 10 ms samples of voice, and the output is text.
    At the time of its release, it garnered much attention because it had a massive
    impact on the quality of Google Translate. In nine months of work using this model,
    the team behind the seq2seq model was able to provide much higher performance
    than that after over 10 years of improvements in Google Translate.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一些例子包括将一句话从一种语言翻译成另一种语言、总结一篇文章，其中输入序列是文章的文本，输出序列是摘要，或者是问答问题，其中问题是输入序列，答案是输出序列。语音识别是一个序列到序列的问题，输入序列是10毫秒的语音样本，输出是文本。在发布时，这个模型引起了广泛关注，因为它对Google
    Translate的质量产生了巨大的影响。在使用该模型的九个月里，seq2seq模型背后的团队取得了比Google Translate经过10多年改进后更高的性能。
- en: '**The Great A.I. Awakening**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**伟大的人工智能觉醒**'
- en: The New York Times published a fantastic article with the above title in 2016
    that documents the journey of deep learning and especially the authors of the
    seq2seq paper and its dramatic effect on the quality of Google Translate. This
    article is highly recommended to see how transformational this architecture was
    for NLP. This article is available at [https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 《纽约时报》在2016年发布了一篇精彩的文章，标题为上述内容，记录了深度学习的发展历程，特别是关于seq2seq论文的作者以及该论文对Google Translate质量的巨大影响。这篇文章强烈推荐阅读，能够展示这一架构对自然语言处理（NLP）的变革性影响。文章链接：[https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html](https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html)。
- en: With these techniques at hand, the next innovation was the use of the Attention
    mechanism, which allows the modeling of dependencies between tokens irrespective
    of their distance. The Attention model became the cornerstone of the **Transformer
    model**, described in the next section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握这些技术后，下一步的创新便是引入了**注意力机制**，它允许建模不同tokens之间的依赖关系，无论它们之间的距离如何。注意力模型成为了**Transformer模型**的基石，后者将在下一节中介绍。
- en: Attention model
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注意力模型
- en: In the encoder-decoder model, the encoder part of the network creates a fixed
    dimensional representation of the input sequence. As the input sequence length
    grows, more and more of the input is compressed into this vector. The encodings
    or hidden states generated by processing the input tokens are not available to
    the decoder layer. The encoder states are hidden from the decoder. The Attention
    mechanism allows the decoder part of the network to see the encoder hidden states.
    These hidden states are depicted in *Figure 4.4* as the output of each of the
    input tokens, (I[1], I[2], I[3],…), but shown only as feeding in to the next input
    token.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器模型中，网络的编码器部分创建了输入序列的固定维度表示。随着输入序列长度的增加，越来越多的输入被压缩到这个向量中。通过处理输入tokens生成的编码或隐藏状态无法被解码器层访问。编码器状态对解码器是隐藏的。注意力机制使得网络的解码器部分能够看到编码器的隐藏状态。这些隐藏状态在*图4.4*中被表示为每个输入token的输出（I[1]，I[2]，I[3]，…），但只显示为输入到下一个token。
- en: In the Attention mechanism, these input token encodings will also be made available
    to the decoder layer. This is called **General Attention**, and it refers to the
    ability of output tokens to directly have a dependence on the encodings or hidden
    states of input tokens. The main innovation here is the decoder operates on a
    sequence of vectors generating by encoding the input rather than one fixed vector
    generated at the end of the input. The Attention mechanism allows the decoder
    to focus its attention on a subset of the encoded input vectors while decoding,
    hence the name.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中，这些输入token的编码也会提供给解码器层。这被称为**一般注意力**，它指的是输出tokens直接依赖于输入tokens的编码或隐藏状态的能力。这里的主要创新是解码器基于一系列由编码输入生成的向量进行操作，而不是在输入结束时生成的固定向量。注意力机制使得解码器在解码时能够将注意力集中在编码输入向量的子集上，因此得名。
- en: There is another form of attention, called **self-attention**. Self-attention
    enables connections between different encodings of input tokens in different positions.
    As depicted in the model in *Figure 4.4*, an input token only sees the encoding
    of the previous token. Self-attention will allow it to look at the encodings of
    previous tokens. Both forms are an improvement to the encoder-decoder architecture.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种注意力形式被称为**自注意力**。自注意力使得不同位置的输入标记之间可以建立连接。如*图4.4*中的模型所示，输入标记只看前一个标记的编码。自注意力则使其能够查看前面标记的编码。这两种形式都是对编码器-解码器架构的改进。
- en: 'While there are many Attention architectures, a prevalent form is called **Bahdanau
    Attention**. It is named after the first author of the paper, published in 2016,
    where this Attention mechanism was proposed. Building on the encoder-decoder network,
    this form enables each output state to look at the encoded inputs and learn some
    weights for each of these inputs. Consequently, each output could focus on different
    input tokens. An illustration of this model is shown in *Figure 4.5*, which is
    a modified version of *Figure 4.4*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有很多种注意力架构，但一种流行的形式被称为**巴赫达努注意力**。它以该论文的第一作者命名，这篇论文于2016年发表，其中提出了这种注意力机制。基于编码器-解码器网络，这种形式使得每个输出状态可以查看编码后的输入并为这些输入学习一些权重。因此，每个输出可以关注不同的输入标记。该模型的示意图如*图4.5*所示，这是*图4.4*的修改版本：
- en: '![](img/B16252_04_05.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_04_05.png)'
- en: 'Figure 4.5: Bahdanau Attention architecture'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：巴赫达努注意力架构
- en: Two specific changes have been made in the Attention mechanism when compared
    to the encoder-decoder architecture. The first change is in the encoder. The encoder
    layer here uses BiLSTMs. The use of BiLSTMs allows each word to learn from the
    words preceding and succeeding them both. In the standard encoder-decoder architecture,
    LSTMs were used, which meant each input word could only learn from the words before
    it.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于编码器-解码器架构，注意力机制有两个具体的变化。第一个变化发生在编码器中。此处的编码器层使用的是BiLSTM（双向长短时记忆网络）。使用BiLSTM使得每个单词都能够从它前后的单词中学习。在标准的编码器-解码器架构中使用的是LSTM，这意味着每个输入单词只能从它之前的单词中学习。
- en: The second change is related to how the decoder uses the output of the encoders.
    In the previous architecture, only the output of the last token, the end-of-sentence
    token, used the summary of the entire input sequence. In the Bahdanau Attention
    architecture, the hidden state output of each input token is multiplied by an
    *alignment weight* that represents the degree of match between the input token
    at a specific position with the output token in question. A context vector is
    computed by multiplying each input hidden state output with the corresponding
    alignment weight and concatenating all the results. This context vector is fed
    to the output token along with the previous output token.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个变化与解码器如何使用编码器的输出有关。在之前的架构中，只有最后一个标记的输出，即句子结束标记，才使用整个输入序列的摘要。在巴赫达努注意力（Bahdanau
    Attention）架构中，每个输入标记的隐藏状态输出会乘以一个*对齐权重*，该权重表示特定位置的输入标记与目标输出标记之间的匹配程度。通过将每个输入的隐藏状态输出与对应的对齐权重相乘并将所有结果连接起来，可以计算出一个上下文向量。该上下文向量与先前的输出标记一起被输入到输出标记中。
- en: '*Figure 4.5* shows this computation, for only the second output token. This
    alignment model with the weights for each output token can help point to the most
    helpful input tokens in generating that output token. Note that some of the details
    have been simplified for brevity and can be found in the paper. We will implement
    Attention from scratch in later chapters.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.5*展示了这种计算过程，仅针对第二个输出标记。这个对齐模型以及每个输出标记的权重，可以帮助指向在生成该输出标记时最有用的输入标记。请注意，某些细节已被简化以便简洁，详细内容可以在论文中找到。我们将在后续章节中从零开始实现注意力机制。'
- en: '**Attention is not an explanation**'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力机制不是一种解释**'
- en: It can be tempting to interpret the alignment scores or attention weights as
    an explanation of the model predicting a particular output token. A paper with
    the title of this information box was published that tests this hypothesis that
    Attention is an explanation. The conclusion from the research is that Attention
    should not be interpreted as an explanation. Different attention weights on the
    same set of inputs may result in the same outputs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将对齐分数或注意力权重解读为模型预测特定输出标记的解释是非常诱人的。曾有一篇名为“注意力机制是一种解释”的论文，测试了这一假设。研究的结论是，注意力机制不应被解释为一种解释。即使在相同的输入集上使用不同的注意力权重，也可能会产生相同的输出。
- en: The next advancement to the Attention model came in the form of the Transformer
    architecture in 2017\. The Transformer model is the key to the BERT architecture,
    so let's understand that next.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Attention 模型的下一次进展体现在 2017 年的 Transformer 架构中。Transformer 模型是 BERT 架构的核心，所以接下来我们来理解它。
- en: Transformer model
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 模型
- en: Vaswani et al. published a ground-breaking paper in 2017 titled *Attention Is
    All You Need*. This paper laid the foundation of the Transformer model, which
    has been behind most of the recent advanced models such as ELMo, GPT, GPT-2, and
    BERT. The transformer model is built on the Attention model by taking the critical
    innovation from it – enabling the decoder to see all of the input hidden states
    while getting rid of the recurrence in it, which makes the model slow to train
    due to the sequential nature of processing the input sequences.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Vaswani 等人于 2017 年发表了一篇开创性论文，标题为 *Attention Is All You Need*。这篇论文奠定了 Transformer
    模型的基础，Transformer 模型也成为了许多最近先进模型的核心，比如 ELMo、GPT、GPT-2 和 BERT。Transformer 模型是在
    Attention 模型的基础上，通过其关键创新来构建的——使解码器能够看到所有输入的隐藏状态，同时去除其中的递归结构，这样可以避免因处理输入序列的顺序性而导致训练过程缓慢。
- en: The Transformer model has an encoder and a decoder part. This encoder-decoder
    structure enables it to perform best on machine translation-type tasks. However,
    not all tasks need full encoder and decoder layers. BERT only uses the encoder
    part, while generative models like GPT-2 use the decoder part. In this section,
    only the encoder part of the architecture is covered. The next chapter deals with
    the generation of text and the best models that use the Transformer decoder. Hence,
    the decoder will be covered in that chapter.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型包括编码器和解码器部分。这种编码器-解码器结构使得它在机器翻译类任务中表现最佳。然而，并非所有任务都需要完整的编码器和解码器层。BERT
    只使用编码器部分，而像 GPT-2 这样的生成模型则使用解码器部分。本节只讨论架构的编码器部分。下一章将讨论文本生成以及使用 Transformer 解码器的最佳模型。因此，解码器将在那一章中讲解。
- en: '**What is a Language Model?**'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是语言模型？**'
- en: A **Language Model** (**LM**) task is traditionally defined as predicting the
    next word in a sequence of words. LMs are particularly useful for text generation,
    but less for classification. GPT-2 is an example of a model that fits this definition
    of an LM. Such a model only has context from the words or tokens that have occurred
    on its left (reverse for a right-to-left language). This is a trade-off that is
    appropriate in the generation of text. However, in other tasks such as question-answering
    or translation, the full sentence should be available. In such a case, using a
    bi-directional model that can use the context from both sides is useful. BERT
    is such a model. It loses the auto-regression property in favor of gaining context
    from both sides of a word of the token.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型**（**LM**）任务通常被定义为预测一个单词序列中的下一个词。语言模型特别适用于文本生成，但在分类任务中效果较差。GPT-2 就是一个符合语言模型定义的例子。这样的模型只会从其左侧出现的词或符号中获取上下文（对于从右至左的语言，则相反）。这种做法在文本生成任务中是合理的。然而，在其他任务中，比如问答或翻译，完整的句子应该是可用的。在这种情况下，使用能够从两侧获取上下文的双向模型是有用的。BERT
    就是这样一个模型。它放弃了自回归特性，以便从词或符号的两侧获取上下文信息。'
- en: 'An encoder block of the Transformer has sub-layers parts – the multi-head self-attention
    sub-layer and a feed-forward sub-layer. The self-attention sub-layer looks at
    all the words of the input sequence and generates an encoding for these words
    in the context of each other. The feed-forward sublayer is composed of two layers
    using linear transformations and a ReLU activation in between. Each encoder block
    is composed of these two sub-layers, while the entire encoder is composed of six
    such blocks, as shown in *Figure 4.6*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的编码器模块由多个子层组成——多头自注意力子层和前馈子层。自注意力子层查看输入序列中的所有词，并生成这些词在彼此上下文中的编码。前馈子层由两层线性变换和中间的
    ReLU 激活组成。每个编码器模块由这两个子层组成，而整个编码器则由六个这样的模块构成，如 *图 4.6* 所示：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_04_06.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![一部手机的截图  自动生成的描述](img/B16252_04_06.png)'
- en: 'Figure 4.6: Transformer encoder architecture'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：Transformer 编码器架构
- en: A residual connection around the multi-head attention block and the feed-forward
    block is made in each encoder block. While adding the output of the sublayer with
    the input it received, layer normalization is performed. The main innovation here
    is the **Multi-Head Attention** block. There are eight identical attention blocks
    whose outputs are concatenated to produce the multi-head attention output. Each
    attention block takes in the encoding and defines three new vectors called the
    query, key, and value vectors. Each of these vectors is defined as 64-dimensional,
    though this size is a hyperparameter that can be tuned. The query, key, and value
    vectors are learned through training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个编码器块中，都会对多头注意力块和前馈块进行残差连接。当将子层的输出与其接收到的输入相加时，会进行层归一化。这里的主要创新是**多头注意力**块。共有八个相同的注意力块，其输出被连接起来，生成多头注意力输出。每个注意力块接受编码并定义三个新向量，分别称为查询、键和值向量。这些向量被定义为64维，虽然这个维度是一个可以调节的超参数。查询、键和值向量通过训练来学习。
- en: To understand how this works, let's assume that the input has three tokens.
    Each token has a corresponding embedding. Each of these tokens is initialized
    with its query, key, and value vectors. A weight vector is also initialized, which,
    when multiplied with the embedding of the input token, produces the key for that
    token. After the query vector is computed for a token, it is multiplied by the
    key vectors of all the input tokens. Note that the encoder has access to all the
    inputs, on both sides of each token. As a result, a score has now been computed
    by taking the query vector of the word in question and the value vector of all
    the tokens in the input sequence. All of these scores are passed through a softmax.
    The result can be interpreted as providing a sense of which tokens of the input
    are important to this particular input token.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个过程，假设输入包含三个标记。每个标记都有一个对应的嵌入。每个标记都会初始化其查询、键和值向量。同时还初始化一个权重向量，当它与输入标记的嵌入相乘时，生成该标记的键。计算出标记的查询向量后，它将与所有输入标记的键向量相乘。需要注意的是，编码器能够访问所有输入，位于每个标记的两侧。因此，已经通过获取相关词的查询向量和输入序列中所有标记的值向量，计算出了一个得分。所有这些得分都会通过软最大化（softmax）处理。结果可以解释为，为了让特定输入标记了解输入中的哪些标记是重要的，提供了一种衡量方式。
- en: In a way, the input token in question is attentive to the other tokens with
    a high softmax score. This score is expected to be high when the input token attends
    to itself but can be high for other tokens as well. Next, this softmax score is
    multiplied by the value vector of each token. All these value vectors of the different
    input tokens are then summed up. Value vectors of tokens with higher softmax scores
    will have a higher contribution to the output value vector of the input token
    in question. This completes the calculation of the output for a given token in
    the Attention layer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种程度上讲，相关输入标记会对其他具有高软最大得分的标记保持关注。当输入标记对自身保持关注时，这个得分预期会很高，但它也可以对其他标记保持较高的关注。接下来，这个软最大得分将与每个标记的值向量相乘。然后，将所有这些不同输入标记的值向量加总起来。具有更高软最大得分的标记的值向量将对相关输入标记的输出值向量贡献更大。这完成了在注意力层中计算给定标记的输出。
- en: Multi-head self-attention creates multiple copies of the query, key, and value
    vectors along with the weights matrix used to compute the query from the embedding
    of the input token. The paper proposed eight heads, though this could be experimented
    with. An additional weight matrix is used to combine the multiple outputs of each
    of the heads and concatenate them together into one output value vector.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 多头自注意力机制生成查询、键和值向量的多个副本，并使用权重矩阵来计算从输入标记的嵌入中获取查询。论文中提出了八个头，虽然可以对此进行实验。另一个权重矩阵用于将每个头的多个输出结合起来，并将它们连接成一个输出值向量。
- en: This output value vector is fed to the feed-forward layer, and the output of
    the feed-forward layer goes to the next encoder block or becomes the output of
    the model at the final encoder block.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出值向量被输入到前馈层，前馈层的输出将传递到下一个编码器块，或者在最后一个编码器块中成为模型的输出。
- en: While the core BERT model is essentially the core Transformer encoder model,
    there are a few specific enhancements it introduced that are covered next. Note
    that using the BERT model is much easier as all of these details are abstracted.
    Knowing these details may, however, help in understanding BERT inputs and outputs.
    The code to use BERT for the IMDb sentiment analysis follows the next section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: BERT核心模型实际上是Transformer编码器模型的核心，但引入了一些特定的增强功能，接下来会详细介绍。注意，使用BERT模型要容易得多，因为所有这些细节都被抽象化了。然而，了解这些细节可能有助于理解BERT的输入和输出。在下一节中将介绍如何使用BERT进行IMDb情感分析的代码。
- en: The bidirectional encoder representations from transformers (BERT) model
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**BERT**（Bidirectional Encoder Representations from Transformers）模型'
- en: The emergence of the Transformer architecture was a seminal moment in the NLP
    world. This architecture has driven a lot of innovation through several derivative
    architectures. BERT is one such model. It was released in 2018\. The BERT model
    only uses the encoder part of the Transformer architecture. The layout of the
    encoder is identical to the one described earlier with twelve encoder blocks and
    twelve attention heads. The size of the hidden layers is 768\. These sets of parameters
    are referred to as *BERT Base*. These hyperparameters result in a total model
    size of 110 million parameters. A larger model was also published with 24 encoder
    blocks, 16 attention heads, and a hidden unit size of 1,024\. Since the paper
    came out, a number of different variants of BERT like ALBERT, DistilBERT, RoBERTa,
    CamemBERT, and so on have also emerged. Each of these models has tried to improve
    the BERT performance in terms of accuracy or in terms of training/inference time.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的出现是自然语言处理领域的一个重要时刻。这种架构通过几种衍生架构推动了许多创新。BERT就是这样一种模型。它于2018年发布。BERT模型仅使用Transformer架构的编码器部分。编码器的布局与前面描述的相同，包括12个编码器块和12个注意力头。隐藏层的大小为768。这组超参数被称为*BERT
    Base*。这些超参数导致总模型大小为1.1亿参数。还发布了一个更大的模型，其中包括24个编码器块、16个注意力头和隐藏单元大小为1,024。自论文发布以来，还出现了许多BERT的不同变体，如ALBERT、DistilBERT、RoBERTa、CamemBERT等等。每个模型都试图通过提高准确性或训练/推理时间来改进BERT的性能。
- en: The way BERT is pre-trained is unique. It uses the multi-task transfer learning
    principle explained above to pre-train on two different objectives. The first
    objective is the **Masked Language Model** (**MLM**) task. In this task, some
    of the input tokens are masked randomly. The model has to predict the right token
    given the tokens on both sides of the masked token. Specifically, a token in the
    input sequence is replaced with a special `[MASK]` token 80% of the time. In 10%
    of the cases, the selected token is replaced with another random token from the
    vocabulary. In the last 10% of the cases, the token is kept unchanged. Further,
    this happens for 15% of the overall tokens in a batch. The consequence of this
    scheme is that the model cannot rely on certain tokens being present and is forced
    to learn a contextual representation based on the distribution of the tokens before
    and after any given token. Without this masking, the bidirectional nature of the
    model means each word would be able to indirectly *see* itself from either direction.
    This would make the task of predicting the target token really easy.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练方式是独特的。它采用了上面解释的多任务迁移学习原理，用于两个不同的目标进行预训练。第一个目标是**Masked Language Model**（MLM）任务。在这个任务中，一些输入标记会被随机屏蔽。模型必须根据屏蔽标记的两侧的标记来预测正确的标记。具体来说，输入序列中的一个标记在80%的情况下会被特殊的`[MASK]`标记替换。在10%的情况下，选择的标记会被词汇表中的另一个随机标记替换。在最后的10%的情况下，标记保持不变。此方案的结果是模型无法依赖于特定的标记存在，并被迫根据给定标记之前和之后的标记的分布学习上下文表示。如果没有这种屏蔽，模型的双向性质意味着每个单词能间接地从任何方向*看到*自己，这将使得预测目标标记的任务变得非常容易。
- en: The second objective the model is pre-trained on is **Next Sentence Prediction**
    (**NSP**). The intuition here is that there are many NLP tasks that deal with
    pairs of sentences. For example, a question-answering problem can model the question
    as the first sentence, and the passage to be used to answer the question becomes
    the second sentence. The output from the model may be a span identifier that identifies
    the start and end token indices in the passage provided as the answer to the question.
    In the case of sentence similarity or paraphrasing, both sentence pairs can be
    passed in to get a similarity score. The NSP model is trained by passing in sentence
    pairs with a binary label that indicates whether the second sentence follows the
    first sentence. 50% of the training examples are passed as actual next sentences
    from the corpus with the label **IsNext**, while in the other 50% a random sentence
    is passed with the output label **NotNext**.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预训练的第二个目标是 **下一句预测**（**NSP**）。这里的直觉是，许多自然语言处理任务都涉及一对句子。例如，一个问答问题可以将问题建模为第一句，而用于回答问题的段落则成为第二句。模型的输出可能是一个跨度标识符，用于识别段落中作为问题答案的开始和结束标记索引。在句子相似性或释义任务中，可以将两个句子对传入模型，得到一个相似性分数。NSP
    模型通过传入带有二元标签的句子对进行训练，标签指示第二句是否跟随第一句。在 50% 的训练样本中，传入的是真正的后续句子，并带有标签 **IsNext**，而在另外
    50% 的样本中，传入的是随机句子，并带有标签 **NotNext**。
- en: BERT also addresses a problem we saw in the GloVe example above – out-of-vocabulary
    tokens. About 15% of the tokens were not in the vocabulary. To address this problem,
    BERT uses the **WordPiece** tokenization scheme with a vocabulary size of 30,000
    tokens. Note that this is much smaller than the GloVe vocabulary size. WordPiece
    belongs to a class of tokenization schemes called **subword** tokenization. Other
    members of this class are **Byte Pair Encoding** (**BPE**), SentencePiece, and
    the Unigram language model. Inspiration for the WordPiece model came from the
    Google Translate team working with Japanese and Korean texts. If you recall the
    discussion on tokenization in the first chapter, we showed that the Japanese language
    does not use spaces for delimiting words. Hence, it is hard to tokenize it into
    words. Methods developed for creating vocabularies for such languages are quite
    useful for applying to languages like English and keeping the dictionary size
    down to a reasonable size.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 还解决了我们在上面 GloVe 示例中看到的一个问题——词汇表外的标记。大约 15% 的标记不在词汇表中。为了解决这个问题，BERT 使用了
    **WordPiece** 分词方案，词汇表大小为 30,000 个标记。请注意，这个词汇表比 GloVe 的词汇表要小得多。WordPiece 属于一种叫做
    **子词** 分词的类别。这个类别的其他成员包括 **字节对编码**（**BPE**）、SentencePiece 和 unigram 语言模型。WordPiece
    模型的灵感来自于 Google 翻译团队在处理日语和韩语文本时的工作。如果你记得第一章中关于分词的讨论，我们曾提到日语不使用空格来分隔单词。因此，很难将其分词成单词。为这类语言创建词汇表的方法对于像英语这样的语言也非常有用，可以保持词典大小在合理范围内。
- en: Consider the German translation of the phrase *Life Insurance Company*. This
    would translate to *Lebensversicherungsgesellschaft*. Similarly, *Gross Domestic
    Product* would translate to *Bruttoinlandsprodukt*. If words are taken as such,
    the size of the vocabulary would be very large. A subword approach could represent
    these words more efficiently.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以德语翻译 *Life Insurance Company* 这个词组为例，它将被翻译为 *Lebensversicherungsgesellschaft*。同样，*Gross
    Domestic Product* 将翻译为 *Bruttoinlandsprodukt*。如果将单词直接作为词汇，那么词汇表的大小将非常庞大。采用子词方法可以更高效地表示这些单词。
- en: A smaller dictionary reduces training time and memory requirements. If a smaller
    dictionary does not come at the cost of out-of-vocabulary tokens, then it is quite
    useful. To help understand the concept of subword tokenization, consider an extreme
    example where the tokenization breaks apart the work into individual characters
    and numbers. The size of this vocabulary would be 37 – with 26 alphabets, 10 numbers,
    and space. An example of a subword tokenization scheme is to introduce two new
    tokens, *-ing* and *-tion*. Every word that ends with these two tokens can be
    broken into two subwords – the part before the suffix and one of the two suffixes.
    This can be done through knowledge of the language grammar and constructs, using
    techniques such as stemming and lemmatization. The WordPiece tokenization approach
    used in BERT is based on BPE. In BPE, the first step is defining a target vocabulary
    size.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 更小的字典可以减少训练时间和内存需求。如果较小的字典不以牺牲词汇外标记为代价，那么它非常有用。为了帮助理解子词标记化的概念，可以考虑一个极端的例子，其中标记化将单词拆分为单个字符和数字。这个词汇表的大小将是37——包括26个字母、10个数字和空格。一个子词标记化方案的例子是引入两个新标记，*
    -ing*和* -tion*。每个以这两个标记结尾的单词都可以被拆分为两个子词——后缀之前的部分和这两个后缀之一。通过对语言语法和结构的理解，可以使用词干提取和词形还原等技术来实现这一点。BERT使用的WordPiece标记化方法基于BPE。在BPE中，第一步是定义目标词汇表大小。
- en: Next, the entire text is converted to a vocabulary of just the individual character
    tokens and mapped to the frequency of occurrence. Now multiple passes are made
    on this to combine pairs of tokens so as to maximize the frequency of the bigram
    created. For each subword created, a special token is added to denote the end
    of the word so that detokenization can be performed. Further, if the subword is
    not the start of the word, a `##` tag is added to help in reconstructing the original
    words. This process is continued until the desired vocabulary is hit, or the base
    condition of a minimum frequency of 1 is hit for tokens. BPE maximizes the frequency,
    and WordPiece builds on top of this to include another objective.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，整个文本被转换成仅由单个字符标记组成的词汇表，并映射到出现频率。然后，对此进行多次遍历，将标记对组合在一起，以最大化生成的二元组的频率。对于每个生成的子词，添加一个特殊标记来表示单词的结束，以便进行去标记化处理。此外，如果子词不是单词的开头，则添加`##`标签，以帮助重建原始单词。这个过程会一直进行，直到达到所需的词汇量，或者达到标记的最小频率条件（频率为1）。BPE最大化频率，而WordPiece则在此基础上增加了另一个目标。
- en: The objective for WordPiece includes increasing mutual information by considering
    the frequencies of the tokens being merged along with the frequency of the merged
    bigram. This introduces a minor adjustment to the model. RoBERTa from Facebook
    experimented with using a BPE model and did not see a material difference in performance.
    The GPT-2 generative model is based on the BPE model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: WordPiece的目标包括通过考虑被合并标记的频率以及合并后的二元组的频率来增加互信息。这对模型做出了细微调整。Facebook的RoBERTa尝试使用BPE模型，但并未看到性能上有显著差异。GPT-2生成模型基于BPE模型。
- en: 'To take an example from the IMDb dataset, here is an example sentence:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以IMDB数据集为例，以下是一个示例句子：
- en: '[PRE26]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After tokenization with BERT, it would look like this:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BERT进行标记化后，它看起来像这样：
- en: '[PRE27]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Where `[CLS]` and `[SEP]` are special tokens, which will be introduced shortly.
    Note how the word *lured* was broken up as a consequence. Now that we understand
    the underlying construct of the BERT model, let's try to use it for transfer learning
    on the IMDb sentiment classification problem. The first step is preparing the
    data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`[CLS]`和`[SEP]`是特殊的标记，它们将在稍后介绍。注意，由此产生的单词*lured*被拆分的方式。现在我们理解了BERT模型的基本结构，接下来让我们尝试用它进行IMDB情感分类问题的迁移学习。第一步是准备数据。'
- en: All the code for the BERT implementation can be found in the `imdb-transfer-learning.ipynb`
    notebook in this chapter's GitHub folder, in the section *BERT-based transfer
    learning*. Please run the code in the section titled *Loading IMDb training data*
    to ensure the data is loaded prior to proceeding.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 所有BERT实现的代码可以在本章的GitHub文件夹中的`imdb-transfer-learning.ipynb`笔记本中找到，位于*基于BERT的迁移学习*部分。请运行标题为*加载IMDB训练数据*的代码段，以确保在继续之前数据已被加载。
- en: Tokenization and normalization with BERT
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用BERT进行标记化和规范化
- en: 'After reading the description of the BERT model, you may be bracing yourself
    for a difficult implementation in code. Have no fear. Our friends at Hugging Face
    have provided pre-trained models as well as abstractions that make working with
    advanced models like BERT a breeze. The general flow for getting BERT to work
    will be:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了BERT模型的描述后，你可能会为实现代码而感到紧张。但不要害怕，Hugging Face的朋友们已经提供了预训练模型和抽象化接口，使得使用像BERT这样先进的模型变得轻松。让BERT正常工作的通用流程是：
- en: Load a pre-trained model
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练模型
- en: Instantiate a tokenizer and tokenize the data
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化分词器并对数据进行分词
- en: Set up a model and compile it
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置模型并进行编译
- en: Fit the model on the data
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于数据
- en: 'These steps won''t take more than a few lines of code each. So let''s get started.
    The first step is to install the Hugging Face libraries:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤每个都不会超过几行代码。所以让我们开始吧。第一步是安装Hugging Face库：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The tokenizer is the first step – it needs to be imported before it can be
    used:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器是第一步——在使用之前需要导入它：
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'That is all there is to load a pre-trained tokenizer! A few things to note
    in the code above. First, there are a number of models published by Hugging Face
    that are available for download. A full list of the models and their names can
    be found at [https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html).
    Some key BERT models that are available are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是加载预训练分词器的全部内容！在上面的代码中有几点需要注意。首先，Hugging Face发布了多个可供下载的模型。完整的模型和名称列表可以在[https://huggingface.co/transformers/pretrained_models.html](https://huggingface.co/transformers/pretrained_models.html)找到。以下是一些可用的关键BERT模型：
- en: '| Model Name | Description |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 描述 |'
- en: '| `bert-base-uncased` / `bert-base-cased` | Variants of the base BERT model
    with 12 encoder layers, hidden size of 768 units, and 12 attention heads for a
    total of ~110 million parameters. The only difference is whether the inputs were
    cased or all lowercase. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| `bert-base-uncased` / `bert-base-cased` | 基础BERT模型的变体，具有12层编码器，768个隐藏单元和12个注意力头，总参数量约为1.1亿。唯一的区别是输入是大小写敏感的还是全小写的。
    |'
- en: '| `bert-large-uncased` / `bert-large-cased` | This model has 24 encoder layers,
    1,024 hidden units, and 16 attention heads for a total of ~340 million parameters.
    Similar split by cased and lowercase models. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `bert-large-uncased` / `bert-large-cased` | 该模型具有24层编码器，1,024个隐藏单元和16个注意力头，总参数量约为3.4亿。大小写模型的分割方式相似。
    |'
- en: '| `bert-base-multilingual-cased` | Parameters here are the same as `bert-base-cased`
    above, trained on 104 languages with the largest Wikipedia entries. However, it
    is not recommended to use the uncased version for international languages, while
    that model is available. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `bert-base-multilingual-cased` | 该模型的参数与上面的`bert-base-cased`相同，训练于104种语言，并使用了最大的维基百科条目。然而，不建议在国际语言中使用无大小写版本，尽管该模型是可用的。
    |'
- en: '| `bert-base-cased-finetuned-mrpc` | This model has been fine-tuned on the
    Microsoft Research Paraphrase Corpus task for paraphrase identification in the
    news domain. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `bert-base-cased-finetuned-mrpc` | 该模型已在微软研究的释义识别任务上进行微调，专注于新闻领域的同义句识别。 |'
- en: '| `bert-base-japanese` | Same size as the base model but trained on Japanese
    text. Note that both the MeCab and WordPiece tokenizers are used. |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `bert-base-japanese` | 与基础模型相同大小，但训练于日语文本。注意，这里使用了MeCab和WordPiece分词器。 |'
- en: '| `bert-base-chinese` | Same size as the base model but trained on cased-simplified
    Chinese and traditional Chinese. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `bert-base-chinese` | 与基础模型相同大小，但训练于简体中文和繁体中文的大小写文本。 |'
- en: Any of the values on the left can be used in the `bert_name` variable above
    to load the appropriate tokenizer. The second line in the code above downloads
    the configuration and the vocabulary file from the cloud and instantiates a tokenizer.
    This loader takes a number of parameters. Since a cased English model is being
    used, we don't want the tokenizer to convert words to lowercase as specified by
    the `do_lower_case` parameter. Note that the default value of this parameter is
    `True`. The input sentences will be tokenized to a maximum of 150 tokens, as we
    saw in the GloVe model as well. `pad_to_max_length` further indicates that the
    tokenizer should also pad the sequences it generates.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的任何值都可以在上面的`bert_name`变量中使用，以加载适当的分词器。上面代码的第二行从云端下载配置和词汇文件，并实例化分词器。该加载器需要多个参数。由于我们使用的是一个大小写敏感的英文模型，因此我们不希望分词器将单词转换为小写，这是由`do_lower_case`参数指定的。请注意，默认值为`True`。输入句子将被分词，最多为150个标记，就像我们在GloVe模型中看到的那样。`pad_to_max_length`进一步表明，分词器也应填充它生成的序列。
- en: 'The first argument, `add_special_tokens`, deserves some explanation. In the
    example so far, we have taken a sequence and a maximum length. If the sequence
    is shorter than this maximum length, then the sequence is padded with a special
    padding token. However, BERT has a special way to encode its sequence due to the
    next sentence prediction task pre-training. It needs a way to provide two sequences
    as the input. In the case of classification, like the IMDb sentiment prediction,
    the second sequence is just left empty. There are three sequences that need to
    be provided to the BERT model:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数`add_special_tokens`需要一些解释。在到目前为止的示例中，我们处理了一个序列和一个最大长度。如果序列短于该最大长度，则会用一个特殊的填充令牌进行填充。然而，由于下一句预测任务的预训练，BERT有一种特殊的方式来编码它的序列。它需要一种方式来提供两个序列作为输入。在分类任务中，比如IMDb情感预测，第二个序列只是留空。BERT模型需要提供三种序列：
- en: '`input_ids`: This corresponds to the tokens in the inputs converted into IDs.
    This is what we have been doing thus far in other examples. In the IMDb example,
    we only have one sequence. However, if the problem required passing in two sequences,
    then a special token, `[SEP]`, would be added in between the sequences. `[SEP]`
    is an example of a special token that has been added by the tokenizer. Another
    special token, `[CLS]`, is appended to the start of the inputs. `[CLS]` stands
    for classifier token. The embedding for this token can be viewed as the summary
    of the inputs in the case of a classification problem, and additional layers on
    top of the BERT model would use this token. It is also possible to use the sum
    of the embeddings of all the inputs as an alternative.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids`：这对应于输入中的令牌，这些令牌被转换为ID。这就是我们在其他示例中一直在做的事情。在IMDb示例中，我们只有一个序列。然而，如果问题需要传入两个序列，那么一个特殊的令牌`[SEP]`将被添加到两个序列之间。`[SEP]`是由分词器添加的一个特殊令牌的示例。另一个特殊令牌`[CLS]`则会被附加到输入的开始位置。`[CLS]`代表分类器令牌。在分类问题的情况下，这个令牌的嵌入可以被视为输入的摘要，BERT模型上方的额外层会使用这个令牌。也可以使用所有输入的嵌入之和作为一种替代方案。'
- en: '`token_type_ids`: If the input contains two sequences, for a question-answering
    problem, for example, then these IDs tell the model indicates which `input_ids`
    correspond to which sequence. In some texts, this is referred to as the segment
    identifiers. The first sequence would be the first segment, and the second sequence
    would be the second segment.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids`：如果输入包含两个序列，例如在问答问题中，那么这些ID告诉模型哪个`input_ids`对应于哪个序列。在一些文本中，这被称为段标识符。第一个序列是第一个段，第二个序列是第二个段。'
- en: '`attention_mask`: Given that the sequences are padded, this mask tells the
    model where the actual tokens end so that the attention calculation does not use
    the padding tokens.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask`：由于序列被填充，这个掩码告诉模型实际的令牌在哪里结束，从而确保注意力计算时不使用填充令牌。'
- en: Given that BERT can take two sequences as input, understanding the padding is
    essential as it can be confusing how padding works in the context of the maximum
    sequence length when a pair of sequences is provided. The maximum sequence length
    refers to the combined length of the pair. There are three different ways to do
    truncation if the combined length exceeds the maximum length. The first two could
    be to reduce the lengths from either the first or the second sequence. The third
    way is to truncate from the lengthiest sequence, a token at a time so that the
    lengths of the pair are only off by one at maximum. In the constructor, this behavior
    can be configured by passing the `truncation_strategy` parameter with the values
    `only_first`, `only_second`, or `longest_first`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BERT可以接受两个序列作为输入，理解填充是至关重要的，因为当提供一对序列时，如何在最大序列长度的上下文中处理填充可能会引起混淆。最大序列长度指的是一对序列的组合长度。如果组合长度超过最大长度，有三种不同的截断方式。前两种方式是从第一个或第二个序列减少长度。第三种方式是从最长的序列开始截断，一次去除一个令牌，直到两者的长度最多只差一个。在构造函数中，可以通过传递`truncation_strategy`参数，并设置其值为`only_first`、`only_second`或`longest_first`来配置此行为。
- en: '*Figure 4.7* shows how an input sequence is converted into the three input
    sequences listed above:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.7*展示了如何将一个输入序列转换为上面列出的三个输入序列：'
- en: '![A close up of a keyboard  Description automatically generated](img/B16252_04_07.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of a keyboard  Description automatically generated](img/B16252_04_07.png)'
- en: 'Figure 4.7: Mapping inputs to BERT sequences'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：将输入映射到BERT序列
- en: 'If the input sequence was *Don''t be lured*, then the figure above shows how
    it is tokenized with the WordPiece tokenizer as well as the addition of special
    tokens. The example above sets a maximum sequence length of nine tokens. Only
    one sequence is provided, hence the token type IDs or segment IDs all have the
    same value. The attention mask is set to 1, where the corresponding entry in the
    tokens is an actual token. The following code is used to generate these encodings:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入序列是 *Don't be lured*，那么上图显示了如何使用 WordPiece 分词器对其进行分词，并添加了特殊标记。上述示例设置了最大序列长度为九个标记。只提供了一个序列，因此令牌类型
    ID 或段 ID 都具有相同的值。注意力掩码设置为 1，其中对应的令牌条目是真实令牌。以下代码用于生成这些编码：
- en: '[PRE30]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Even though we won''t be using a pair of sequences in this chapter, it is useful
    to be aware of how the encodings look when a pair is passed. If two strings are
    passed to the tokenizer, then they are treated as a pair. This is shown in the
    code below:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在本章中我们不会使用一对序列，了解当传入一对序列时，编码的样子也是很有用的。如果传入两个字符串，它们将被视为一对。这在下面的代码中有展示：
- en: '[PRE32]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The input IDs have two separators to distinguish between the two sequences.
    The token type IDs help distinguish which tokens correspond to which sequence.
    Note that the token type ID for the padding token is set to 0\. In the network,
    it is never used as all the values are multiplied by the attention mask.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 ID 有两个分隔符，以区分两个序列。令牌类型 ID 有助于区分哪些令牌对应于哪个序列。请注意，填充令牌的令牌类型 ID 被设置为 0。在网络中，它从不被使用，因为所有值都会与注意力掩码相乘。
- en: 'To perform encoding of the inputs for all the IMDb reviews, a helper function
    is defined, as shown below:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对所有 IMDb 评论的输入进行编码，定义了一个辅助函数，如下所示：
- en: '[PRE34]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The method is pretty straightforward. It takes the input tensor and uses UTF-8
    decoding. Using the tokenizer, this input is converted into the three sequences.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法相当简单。它接受输入张量并使用 UTF-8 解码。通过分词器，将输入转换为三个序列。
- en: This would be a great opportunity to implement a different padding algorithm.
    For example, implement an algorithm that takes the last 150 tokens instead of
    the first 150 and compare the performance of the two methods.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的机会，可以实现一种不同的填充算法。例如，实现一个算法，取最后 150 个令牌，而不是前 150 个，并比较两种方法的性能。
- en: 'Now, this needs to be applied to every review in the training data:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这需要应用于训练数据中的每一条评论：
- en: '[PRE35]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Labels of the reviews are also converted into categorical values. Using the
    `sklearn` package, the training data is split into training and validation sets:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 评论的标签也被转换为分类值。使用 `sklearn` 包，训练数据被拆分为训练集和验证集：
- en: '[PRE36]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A little more data processing is required to wrangle the inputs into three
    input dictionaries in `tf.DataSet` for easy use in training:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 需要做一些额外的数据处理，将输入转换成三个输入字典，并在 `tf.DataSet` 中使用，以便在训练中轻松使用：
- en: '[PRE38]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These training and validation sequences are converted into a dataset like so:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些训练和验证序列被转换成数据集，如下所示：
- en: '[PRE39]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: A batch size of 16 has been used here. The memory of the GPU is the limiting
    factor here. Google Colab can support a batch length of 32\. An 8 GB RAM GPU can
    support a batch size of 16\. Now, we are ready to train a model using BERT for
    classification. We will see two approaches. The first approach will use a pre-built
    classification model on top of BERT. This is shown in the next section. The second
    approach will use the base BERT model and adds custom layers on top to accomplish
    the same task. This technique will be demonstrated in the section after.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了批处理大小为 16。GPU 的内存是这里的限制因素。Google Colab 支持的批处理长度为 32。一个 8GB 内存的 GPU 支持的批处理大小为
    16。现在，我们准备好使用 BERT 进行分类训练模型。我们将看到两种方法。第一种方法将使用一个预构建的分类模型在 BERT 之上。这将在下一个部分中展示。第二种方法将使用基础的
    BERT 模型，并在其上添加自定义层来完成相同的任务。这个技术将在后面的部分中演示。
- en: Pre-built BERT classification model
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预构建的 BERT 分类模型
- en: 'Hugging Face libraries make it really easy to use a pre-built BERT model for
    classification by providing a class to do so:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 库通过提供一个类，使得使用预构建的 BERT 模型进行分类变得非常简单：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'That was quite easy, wasn''t it? Note that the instantiation of the model will
    require a download of the model from the cloud. However, these models are cached
    on the local machine if the code is being run from a local or dedicated machine.
    In the Google Colab environment, this download will be run every time a Colab
    instance is initialized. To use this model, we only need to provide an optimizer
    and a loss function and compile the model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当简单，不是吗？请注意，模型的实例化将需要从云端下载模型。然而，如果代码是在本地或专用机器上运行的，这些模型会被缓存到本地机器上。在Google Colab环境中，每次初始化Colab实例时都会执行此下载。要使用此模型，我们只需提供优化器和损失函数并编译模型：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This model is actually quite simple in layout as its summary shows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型实际上在结构上相当简单，正如其摘要所示：
- en: '[PRE42]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: So, the model has the entire BERT model, a dropout layer, and a classifier layer
    on top. This is as simple as it gets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，模型包含整个BERT模型，一个丢弃层，以及顶部的分类器层。这是最简化的版本。
- en: 'The BERT paper suggests some settings for fine-tuning. They suggest a batch
    size of 16 or 32, run for 2 to 4 epochs. Further, they suggest using one of the
    following learning rates for Adam: 5e-5, 3e-5, or 2e-5\. Once this model is up
    and running in your environment, please feel free to train with different settings
    to see the impact on accuracy.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: BERT论文建议了一些微调设置。它们建议批次大小为16或32，训练2到4个周期。此外，它们还建议使用以下Adam优化器的学习率之一：5e-5、3e-5或2e-5。模型在你的环境中启动并运行后，欢迎使用不同的设置进行训练，看看对准确度的影响。
- en: 'In the previous section, we batched the data into sets of 16\. Here, the Adam
    optimizer is configured to use a learning rate of 2e-5\. Let''s train this model
    for 3 epochs. Note that training is going to be quite slow:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们将数据分批为16个一组。这里，Adam优化器配置为使用2e-5的学习率。我们将训练此模型3个周期。请注意，训练将会非常慢：
- en: '[PRE44]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The validation accuracy is quite impressive for the little work we have done
    here if it holds on the test set. That needs to be checked next. Using the convenience
    methods from the previous section, the test data will be tokenized and encoded
    in the right format:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在测试集上能够保持这个验证准确度，那么我们所做的工作就相当值得称赞。接下来需要进行这一验证。使用上一节中的便捷方法，测试数据将被标记化并编码为正确的格式：
- en: '[PRE46]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Evaluating the performance of this model on the test dataset, we get the following:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据集上评估该模型的表现，得到如下结果：
- en: '[PRE47]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The model accuracy is almost 88%! This is higher than the best GloVe model shown
    previously, and it took much less code to implement.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的准确度几乎达到了88%！这比之前展示的最佳GloVe模型还要高，而且实现的代码要少得多。
- en: In the next section, let's try to build custom layers on top of the BERT model
    to take transfer learning to the next level.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将尝试在BERT模型之上构建自定义层，进一步提升迁移学习的水平。
- en: Custom model with BERT
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义BERT模型
- en: The BERT model outputs contextual embeddings for all of the input tokens. The
    embedding corresponding to the `[CLS]` token is generally used for classification
    tasks, and it represents the entire document. The pre-built model from Hugging
    Face returns the embeddings for the entire sequence as well as this *pooled output*,
    which represents the entire document as the output of the model. This pooled output
    vector can be used in future layers to help with the classification task. This
    is the approach we will take in building a customer model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型为所有输入的标记输出上下文嵌入。通常，`[CLS]`标记对应的嵌入用于分类任务，代表整个文档。Hugging Face提供的预构建模型返回整个序列的嵌入，以及这个*池化输出*，它表示整个文档作为模型的输出。这个池化输出向量可以在未来的层中用于帮助分类任务。这就是我们构建客户模型时将采取的方法。
- en: The code for this section is under the heading *Customer Model With BERT* in
    the same notebook as above.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码位于同一笔记本的*客户模型与BERT*标题下。
- en: 'The starting point for this exploration is the base `TFBertModel`. It can be
    imported and instantiated like so:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本次探索的起点是基础的`TFBertModel`。可以这样导入并实例化：
- en: '[PRE49]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Since we are using the same pre-trained model, the cased BERT-Base model, we
    can reuse the tokenized and prepared data from the section above. If you haven't
    already, take a moment to ensure the code in the *Tokenization and normalization
    with BERT* section has been run to prepare the data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是相同的预训练模型，即大小写敏感的BERT-Base模型，因此可以复用上一节中的标记化和准备好的数据。如果你还没有操作，花点时间确保已经运行了*使用BERT的标记化和标准化*这一节中的代码来准备数据。
- en: 'Now, the custom model needs to be defined. The first layer of this model is
    the BERT layer. This layer will take three inputs, namely the input tokens, attention
    masks, and token type IDs:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，需要定义自定义模型。该模型的第一层是 BERT 层。此层将接受三个输入，分别是输入标记、注意力掩码和标记类型 ID：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'These names need to match the dictionary defined in the training and testing
    dataset. This can be checked by printing the specification of the dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这些名称需要与训练和测试数据集中定义的字典匹配。可以通过打印数据集的规范来检查这一点：
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The BERT model expects these inputs in a dictionary. It can also accept the
    inputs as named arguments, but this approach is clearer and makes it easy to trace
    the inputs. Once the inputs are mapped, the output of the BERT model can be computed:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型期望这些输入以字典形式传递。它也可以接受作为命名参数的输入，但这种方式更加清晰，并且有助于追踪输入。一旦输入映射完成，就可以计算 BERT
    模型的输出：
- en: '[PRE54]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The first output has embeddings for each of the input tokens including the
    special tokens `[CLS]` and `[SEP]`. The second output corresponds to the output
    of the `[CLS]` token. This output will be used further in the model:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输出包含每个输入标记的嵌入，包括特殊标记 `[CLS]` 和 `[SEP]`。第二个输出对应于 `[CLS]` 标记的输出。该输出将在模型中进一步使用：
- en: '[PRE56]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The model above is only illustrative, to demonstrate the technique. We add
    a dense layer and a couple of dropout layers before an output layer. Now, the
    customer model is ready for training. The model needs to be compiled with an optimizer,
    loss function, and metrics to watch for:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型仅用于演示这项技术。我们在输出层之前添加了一个密集层和几个丢弃层。现在，客户模型已经准备好进行训练。该模型需要用优化器、损失函数和指标来编译：
- en: '[PRE57]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here is what this model looks like:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个模型的样子：
- en: '[PRE58]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_04_08.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_04_08.png)'
- en: 'This custom model has 154,202 additional trainable parameters in addition to
    the BERT parameters. The model is ready to be trained. We will use the same settings
    from the previous BERT section and train the model for 3 epochs:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自定义模型在 BERT 参数基础上增加了 154,202 个可训练参数。该模型已经准备好进行训练。我们将使用与之前 BERT 部分相同的设置，并将模型训练
    3 个周期：
- en: '[PRE59]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Evaluating on the test set gives an accuracy of 86.29%. Note that the test
    data encoding steps used in the pretrained BERT model section are used here as
    well:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上评估得到 86.29% 的准确率。请注意，这里使用的预训练 BERT 模型部分中的测试数据编码步骤也在此处使用：
- en: '[PRE61]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Fine-tuning of BERT is run for a small number of epochs with a small value
    for Adam''s learning rate. If a lot of fine-tuning is done, then there is a risk
    of BERT forgetting its pretrained parameters. This can be a limitation while building
    custom models on top as a few epochs may not be sufficient to train the layers
    that have been added. In this case, the BERT model layer can be frozen, and training
    can be continued further. Freezing the BERT layer is fairly easy, though it needs
    the re-compilation of the model:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的微调通常在较少的周期内进行，并使用较小的 Adam 学习率。如果进行大量微调，则存在 BERT 忘记其预训练参数的风险。在构建自定义模型时，这可能会成为一个限制，因为几个周期可能不足以训练添加的层。在这种情况下，可以冻结
    BERT 模型层，并继续进一步训练。冻结 BERT 层相对简单，但需要重新编译模型：
- en: '[PRE63]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can check the model summary to verify that the number of trainable parameters
    has changed to reflect the BERT layer being frozen:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查模型摘要，以验证可训练参数的数量已经改变，以反映 BERT 层被冻结：
- en: '[PRE64]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_04_09.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated](img/B16252_04_09.png)'
- en: 'Figure 4.8: Model summary'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：模型摘要
- en: We can see that all the BERT parameters are now set to non-trainable. Since
    the model was being recompiled, we also took the opportunity to change the learning
    rate.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，现在所有的 BERT 参数都已设置为不可训练。由于模型正在重新编译，我们也趁机修改了学习率。
- en: Changing the sequence length and learning rate during training are advanced
    techniques in TensorFlow. The BERT model also used 128 as the sequence length
    for initial epochs, which was changed to 512 later in training. It is also common
    to see a learning rate increase for the first few epochs and then decrease as
    training proceeds.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中改变序列长度和学习率是 TensorFlow 中的高级技术。BERT 模型最初使用了 128 作为序列长度，并在后期训练中将其改为 512。通常，在训练的前几个周期中，学习率会逐步增加，然后随着训练的进行而下降。
- en: 'Now, training can be continued for a number of epochs like so:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以像这样继续训练多个周期：
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The training output has not been shown for brevity. Checking the model on the
    test set yields 86.96% accuracy:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，未显示训练输出。对测试集进行检查时，模型准确率为86.96%：
- en: '[PRE66]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: If you are contemplating whether the accuracy of this custom model is lower
    than the pre-trained model, then it is a fair question to ponder over. A bigger
    network is not always better, and overtraining can lead to a reduction in model
    performance due to overfitting. Something to try in the custom model is to use
    the output encodings of all the input tokens and pass them through an LSTM layer
    or concatenate them together to pass through dense layers and then make the prediction.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在思考这个自定义模型的准确率是否低于预训练模型，这是一个值得深思的问题。更大的网络并不总是更好，过度训练可能会导致模型性能下降，因为过拟合是其中的一个原因。你可以尝试在自定义模型中使用所有输入标记的输出编码，将它们传递到LSTM层，或者将它们连接起来通过全连接层然后进行预测。
- en: Having done the tour of the encoder side of the Transformer architecture, we
    are ready to look into the decoder side of the architecture, which is used for
    text generation. That will be the focus of the next chapter. Before we go there,
    let's review everything we covered in this chapter.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了Transformer架构的编码器部分的学习后，我们准备深入探讨该架构的解码器部分，它用于文本生成。这将是下一章的重点。在此之前，让我们回顾一下本章所涵盖的所有内容。
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Transfer learning has made a lot of progress possible in the world of NLP, where
    data is readily available, but labeled data is a challenge. We covered different
    types of transfer learning first. Then, we took pre-trained GloVe embeddings and
    applied them to the IMDb sentiment analysis problem, seeing comparable accuracy
    with a much smaller model that takes much less time to train.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习在NLP领域取得了很大的进展，在这个领域中，数据易于获取，但标签数据是一个挑战。我们首先介绍了不同类型的迁移学习。然后，我们将预训练的GloVe嵌入应用于IMDb情感分析问题，看到一个训练时间更短、规模更小的模型也能得到相当的准确率。
- en: Next, we learned about seminal moments in the evolution of NLP models, starting
    from encoder-decoder architectures, attention, and Transformer models, before
    understanding the BERT model. Using the Hugging Face library, we used a pre-trained
    BERT model and a custom model built on top of BERT for the purpose of sentiment
    classification of IMDb reviews.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了NLP模型演变中的关键时刻，从编码器-解码器架构、注意力机制和Transformer模型开始，然后了解了BERT模型。使用Hugging
    Face库，我们使用了一个预训练的BERT模型，并基于BERT构建了一个自定义模型，用于IMDb评论的情感分类。
- en: BERT only uses the encoder part of the Transformer model. The decoder side of
    the stack is used in text generation. The next two chapters will focus on completing
    the understanding of the Transformer model. The next chapter will use the decoder
    side of the stack to perform text generation and sentence completion. The chapter
    after that will use the full encoder-decoder network architecture for text summarization.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: BERT仅使用Transformer模型的编码器部分。堆栈的解码器部分用于文本生成。接下来的两章将重点完成对Transformer模型的理解。下一章将使用堆栈的解码器部分进行文本生成和句子补全。接下来的章节将使用完整的编码器-解码器网络架构进行文本摘要。
- en: Thus far, we have trained embeddings for tokens in the models. A considerable
    amount of lift can be achieved by using pre-trained embeddings. The next chapter
    will focus on the concept of transfer learning and the use of pre-trained embeddings
    like BERT.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经为模型中的标记训练了嵌入。通过使用预训练的嵌入，可以获得相当大的提升。下一章将重点介绍迁移学习的概念以及使用像BERT这样的预训练嵌入。
