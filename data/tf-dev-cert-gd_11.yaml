- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: NLP with TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行自然语言处理
- en: Text data is inherently sequential, defined by the order in which words occur.
    Words follow one another, building upon previous ideas and shaping those to come.
    Understanding the sequence of words and the context in which they are applied
    is straightforward for humans. However, this poses a significant challenge to
    feed-forward networks such as **convolutional neural networks** (**CNNs**) and
    traditional **deep neural networks** (**DNNs**). These models treat text data
    as independent inputs; hence, they miss the interconnected nature and flow of
    language. For example, let’s take the sentence “*The cat, which is a mammal, likes
    to chase mice*." Humans immediately recognize the relationship between the cat
    and mice, as we process the entire sentence as a whole and not individual units.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据本质上是顺序性的，由单词出现的顺序定义。单词相互衔接，建立在前一个思想的基础上，并塑造接下来的思想。人类理解单词的顺序以及它们所应用的上下文非常直接。然而，这对前馈网络（如**卷积神经网络**（**CNNs**））和传统的**深度神经网络**（**DNNs**）构成了巨大挑战。这些模型将文本数据视为独立的输入，因此它们忽略了语言的内在联系和流动性。例如，考虑这句话：“*The
    cat, which is a mammal, likes to chase mice*。”人类会立即识别出猫与老鼠之间的关系，因为我们将整句话作为一个整体来处理，而不是单个单位。
- en: A **recurrent neural network** (**RNN**) is a type of neural network designed
    to handle sequential data such as text and time-series data. When working with
    text data, RNNs’ memory enables them to recall earlier parts of a sequence, aiding
    them to understand the context in which words are used in a text. For example,
    with a sentence such as “*As an archaeologist, John loves discovering ancient
    artifacts,*” an RNN, in this context, can infer that an archaeologist will be
    excited about ancient artifacts, and in this sentence, the archaeologist is John.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）是一种旨在处理顺序数据（如文本和时间序列数据）的神经网络类型。处理文本数据时，RNN 的记忆能力使它能够回忆起序列中的早期部分，帮助它理解单词在文本中使用的上下文。例如，考虑这样一句话：“*作为一名考古学家，约翰喜欢发现古代文物*”，在这种情况下，RNN
    能推断出考古学家会对古代文物感兴趣，而且这句话中的考古学家是约翰。'
- en: In this chapter, we will begin to explore the world of RNNs and take a look
    under the hood to understand how the inner mechanisms of RNNs work together to
    maintain a form of memory. We will explore the pros and cons of RNNs when working
    with text data, after which we will switch our attention to investigating its
    variants, such as **long short-term memory** (**LSTM**) and **gated recurrent
    units** (**GRUs**). Next, we will use the knowledge we have gained to build a
    multiclass text classifier. We will then explore the power of transfer learning
    in the field of **natural language processing** (**NLP**). Here, we will see how
    to apply pretrained word embeddings to our workflow. To close this chapter, we
    will use RNNs to build a child story generator; here, we will see RNNs in action
    as they generate text data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将开始探索 RNN 的世界，并深入研究其内部机制，了解 RNN 如何协同工作以保持一种记忆形式。我们将探讨在处理文本数据时，RNN 的优缺点，然后我们将转向研究它的变体，如**长短期记忆**（**LSTM**）和**门控递归单元**（**GRU**）。接下来，我们将运用所学知识构建一个多类文本分类器。然后，我们将探索迁移学习在**自然语言处理**（**NLP**）领域的强大能力。在这一部分，我们将看到如何将预训练的词嵌入应用到我们的工作流中。为了结束本章，我们将使用
    RNN 构建一个儿童故事生成器；在这里，我们将看到 RNN 如何在生成文本数据时发挥作用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The anatomy of RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 的结构
- en: Text classification with RNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RNN 进行文本分类
- en: NLP with transfer learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习进行自然语言处理
- en: Text generation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: Understanding sequential data processing – from traditional neural networks
    to RNNs and LSTMs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解顺序数据处理——从传统神经网络到 RNN 和 LSTM
- en: In traditional neural networks, as we discussed earlier in this book, we see
    an arrangement of densely interconnected neurons, devoid of any form of memory.
    When we feed a sequence of data to these networks, it’s an all-or-nothing transaction
    – the entire sequence is processed at once and converted into a singular vector
    representation. This approach is quite different from how humans process and comprehend
    text data. When we read, we naturally analyze text word by word, understanding
    that important words – those that have the power to shift the entire message of
    a sentence – can be positioned anywhere within it. For example, let's consider
    the sentence “*I loved the movie, despite some critics.*” Here, the word “*despite*”
    is pivotal, altering the direction of the sentiment expressed in the sentence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统神经网络中，正如我们在本书前面所讨论的那样，网络中排列着密集连接的神经元，并且没有任何形式的记忆。当我们将一串数据输入这些网络时，这是一种“全有或全无”的处理方式——整个序列一次性被处理并转换为一个单一的向量表示。这种方法与人类处理和理解文本数据的方式大不相同。当我们阅读时，我们自然而然地按单词逐一分析文本，并理解重要的单词——那些能够改变整个句子含义的单词——可以出现在句子的任何位置。例如，考虑句子“*我喜欢这部电影，尽管一些评论家不喜欢*”。在这里，单词“*尽管*”至关重要，改变了句子中情感表达的方向。
- en: RNNs don’t just consider the value of individual words through embeddings; they
    also take into account the sequence or relative order of these words. This ordering
    of words gives them meaning and allows humans to effectively communicate with
    one another. RNNs are unique in their ability to retain context from one timestamp
    (or one word in the case of a sentence) to the next, thereby preserving the sequential
    coherence of the input. For example, in the sentence “*I visited Rome last year,
    and I found the Colosseum fascinating,*” an RNN would understand that “*the Colosseum*”
    relates to “*Rome*” because of the sequence of words. However, there’s a catch
    – this context retention can fail in longer sentences where the distance between
    related words increases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: RNN不仅通过嵌入考虑单个单词的值，它们还会考虑这些单词的顺序或相对位置。单词的顺序赋予它们意义，并使得人类能够有效地进行交流。RNN的独特之处在于它们能够从一个时间点（或在句子的情况下，从一个单词）到下一个时间点保持上下文，从而保持输入的顺序一致性。例如，在句子“*我去年访问了罗马，我觉得斗兽场非常迷人*”中，RNN会理解“*斗兽场*”与“*罗马*”的关系，这是因为单词的顺序。然而，有一个问题——在更长的句子中，当相关单词之间的距离增大时，这种上下文保持可能会失败。
- en: This is precisely where gated variants of RNNs such as LSTM networks come in.
    LSTMs are designed with a special “cell state” architecture that enables them
    to manage and retain information over longer sequences. So, even in a lengthy
    sentence such as “*I visited Rome last year, experienced the rich culture, enjoyed
    the delicious food, met wonderful people, and I found the Colosseum fascinating,*”
    an LSTM could still link “*the Colosseum*” with “*Rome*,” understanding the broader
    context despite the length and complexity of the sentence. We have only scratched
    the surface. Let’s now examine the anatomy of these powerful networks. We will
    begin with RNNs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是门控变种RNN，如LSTM网络的作用所在。LSTM通过一种特殊的“单元状态”架构设计，使其能够在更长的序列中管理和保留信息。因此，即使是像“*我去年访问了罗马，体验了丰富的文化，享受了美味的食物，结识了很棒的人，我觉得斗兽场非常迷人*”这样的长句，LSTM仍然能够将“*斗兽场*”与“*罗马*”联系起来，理解句子尽管长度和复杂性较大，但它们之间的广泛联系。我们仅仅是触及了表面。接下来，我们将检查这些强大网络的结构。我们将从RNN开始。
- en: The anatomy of RNNs
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN的结构
- en: In the previous section, we talked about RNNs’ ability to handle sequential
    data; let’s drill down into how an RNN does this. The key differentiator between
    RNNs and feed-forward networks is their internal memory, as shown in *Figure 11**.1*,
    which enables RNNs to process input sequences while retaining information from
    previous steps. This attribute empowers RNNs to suitably exploit the temporal
    dependencies in sequences such as text data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了RNN处理序列数据的能力；现在让我们深入了解RNN是如何做到这一点的。RNN和前馈神经网络的关键区别在于它们的内部记忆，如*图11.1*所示，这使得RNN能够处理输入序列的同时保留来自前一步的信息。这一特性使得RNN能够充分利用序列中如文本数据的时间依赖性。
- en: '![Figure 11.1 – The anatomy of an RNN](img/B18118_11_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图11.1 – RNN的结构](img/B18118_11_001.jpg)'
- en: Figure 11.1 – The anatomy of an RNN
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – RNN的结构
- en: '*Figure 11**.2* shows a clearer picture of an RNN and its inner workings. Here,
    we can see a series of interconnected units through which data flows in a sequential
    fashion, one element at a time. As each unit processes the input data, it sends
    the output to the next unit in a similar fashion to how feed-forward networks
    work. The key difference lies in the feedback loop, which equips RNNs with the
    memory of previous inputs, empowering them with the ability to comprehend entire
    sequences.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.2* 显示了一个更清晰的 RNN 图和它的内部工作原理。在这里，我们可以看到一系列相互连接的单元，数据以顺序的方式逐个元素流动。当每个单元处理输入数据时，它将输出发送到下一个单元，类似于前馈网络的工作方式。关键的不同之处在于反馈回路，它使
    RNN 拥有了之前输入的记忆，从而使它们能够理解整个序列。'
- en: '![Figure 11.2 – An expanded view of an RNN showing its operation across multiple
    timesteps](img/B18118_11_002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 展示 RNN 在多个时间步中的操作的扩展视图](img/B18118_11_002.jpg)'
- en: Figure 11.2 – An expanded view of an RNN showing its operation across multiple
    timesteps
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 展示 RNN 在多个时间步中的操作的扩展视图
- en: Let’s imagine we are dealing with sentences, and we want our RNN to learn something
    about their grammar. Each word in the sentence represents a time step, and at
    each one, the RNN considers the current word and also the “context” from the previous
    words (or steps). Let’s go over a sample sentence. Let’s say we have a sentence
    with five words – “*Barcelona is a nice city.*” This sentence has five time steps,
    one for each word. At time step 1, we feed the word “*Barcelona*” into our RNN.
    The network learns something about this word (in reality, it would learn from
    the word’s vector representation), produces an output, and also a hidden state
    capturing what it has learned, as illustrated in *Figure 11**.2*. Now, we unroll
    the RNN to timestep 2\. We input the next word, “*is*,” into our network, but
    we also input the hidden state from timestep 1\. This hidden state represents
    the “memory” of the network, allowing the network to take into account what it
    has seen so far. The network produces a new output and a new hidden state.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在处理句子，并且希望我们的 RNN 学习句子的语法。句子中的每个单词代表一个时间步长，在每个时间步，RNN 会考虑当前单词以及来自前一个单词（或步骤）的“上下文”。让我们看一个示例句子。假设我们有一个包含五个单词的句子——“*巴塞罗那是一个美丽的城市*。”这个句子有五个时间步，每个单词对应一个时间步。在时间步
    1，我们将单词“*巴塞罗那*”输入到 RNN 中。网络学习到有关这个单词的一些信息（实际上，它会从该单词的向量表示中学习），然后生成一个输出，同时也会生成一个隐藏状态，捕捉它所学到的内容，如
    *图 11.2* 所示。现在，我们将 RNN 展开到时间步 2。我们将下一个单词“*是*”输入到网络中，同时也将时间步 1 的隐藏状态输入。这一隐藏状态代表了网络的“记忆”，使得网络能够考虑它迄今为止所看到的内容。网络生成一个新的输出和一个新的隐藏状态。
- en: This process continues, with the RNN unrolling further for each word in the
    sentence. At each time step, the network takes the current word and the hidden
    state from the previous time step as input, producing an output and a new hidden
    state. When you “unroll” an RNN in this way, it may look like a deep feed-forward
    network with shared weights across each layer (since each timestep uses the same
    underlying RNN cell for its operations), but it’s more accurately a single network
    that’s being applied to each timestep, passing along the hidden state as it goes.
    RNNs have the ability to learn from and remember sequences of arbitrary lengths;
    however, they do have their own limitations. One key issue with RNNs is their
    struggle with capturing long-term dependencies due to the vanishing gradient problem.
    This happens because the influence of time steps over future time steps can diminish
    over long sequences, as a result of repeated multiplications during backpropagation,
    which makes the gradients exceedingly small and thus harder to learn from. To
    address this, we will apply more advanced versions of RNNs such as LSTM and GRU
    networks. These architectures apply gating mechanisms to control the flow of information
    in the network and make it easier for the model to learn long-term dependencies.
    Let’s examine these variants of RNNs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程继续进行，RNN 会根据句子中的每个单词进一步展开。在每个时间步，网络会将当前的单词和来自上一时间步的隐藏状态作为输入，生成输出和新的隐藏状态。当你以这种方式“展开”一个
    RNN 时，它看起来可能像是一个具有共享权重的深度前馈网络（因为每个时间步都使用相同的 RNN 单元进行操作），但更准确地说，它是一个应用于每个时间步的单一网络，随着传递隐藏状态而进行计算。RNN
    具有学习和记住任意长度序列的能力；然而，它们也有自己的局限性。RNN 的一个关键问题是，由于梯度消失问题，它难以捕捉长期依赖关系。出现这种问题是因为在反向传播过程中，时间步对未来时间步的影响可能会随着长序列的增长而减弱，导致梯度变得非常小，从而更难进行学习。为了解决这个问题，我们将应用
    RNN 的更高级版本，如 LSTM 和 GRU 网络。这些架构通过应用门控机制来控制网络中信息的流动，使得模型更容易学习长期依赖关系。接下来我们来看看这些
    RNN 的变种。
- en: Variants of RNNs – LSTM and GRU
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 的变种 — LSTM 和 GRU
- en: 'Let’s imagine we are working on a movie review classification project, and
    while inspecting our dataset, we find a sentence such as this one: “*The movie
    started off boring and slow, but it really picked up toward the end, and the climax
    was amazing.*” By examining the sentence, we see that the initial set of words
    used by the reviewer portrays a negative sentiment by using words such as “slow”
    and “boring,” but the sentiment takes a shift to a more positive one with the
    use of phrases such as “picked up” and “climax was amazing.” If we use a simple
    RNN for this task, due to its inherent limitation to retain information over longer
    sequences, it may misclassify the sentence by attaching undue importance to the
    earlier negative tone of the review.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在进行一项电影评论分类项目，在检查我们的数据集时，我们发现一条类似这样的句子：“*这部电影开始时很无聊且节奏缓慢，但到最后真的是越来越好，高潮部分令人惊叹。*”通过分析这个句子，我们看到评论者最初使用的词汇呈现出负面情感，如“缓慢”和“无聊”，但情感在后面发生了转变，变得更加积极，词组如“越来越好”和“高潮部分令人惊叹”都在传达更为正面的情感。如果我们使用简单的
    RNN 来处理这个任务，由于它本身无法很好地保留长序列的信息，它可能会因为过分强调评论中最初的负面情绪而误分类这个句子。
- en: Conversely, LSTMs and GRUs are designed to handle long-term dependencies, which
    makes them effective in not just capturing the change in sentiment but also for
    other NLP tasks, such as machine translation, text summarization, and question
    answering, where they outshine their simpler counterparts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，LSTM 和 GRU 被设计用来处理长期依赖关系，这使得它们不仅能有效捕捉情感变化，还能在其他自然语言处理任务中表现出色，例如机器翻译、文本摘要和问答系统，在这些任务中，它们优于其更简单的对手。
- en: LSTMs
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆网络）
- en: 'LSTMs are a specialized type of RNN designed to address the vanishing gradient
    problem, enabling LSTMs to effectively handle long-term dependencies in sequential
    data. To resolve this issue LSTM introduced a new structure called a memory cell,
    which essentially acts as an information carrier with the ability to preserve
    information over an extended period. Unlike standard RNNs, which feed information
    from one step to the next and tend to lose it over time, an LSTM with the aid
    of its memory cell can store and retrieve information from any point in the input
    sequence, irrespective of its length. Let’s examine how an LSTM decides what information
    it will store in its memory cell. An LSTM is made up of four main components,
    as shown in *Figure 10**.3*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种专门设计用来解决梯度消失问题的 RNN 类型，使得 LSTM 能够有效处理序列数据中的长期依赖。为了解决这个问题，LSTM 引入了一种称为记忆单元的新结构，它本质上作为信息载体，并具有在延长期间内保留信息的能力。与标准的
    RNN 不同，后者将信息从一个步骤传递到下一个步骤，并随时间逐渐丢失，LSTM 借助其记忆单元可以从输入序列的任何点存储和检索信息。让我们来看看 LSTM
    是如何决定存储在其记忆单元中的信息的。如图 *Figure 10**.3* 所示，一个 LSTM 由四个主要组件组成：
- en: '![10.3 – LSTM architecture](img/B18118_11_003.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![10.3 – LSTM 架构](img/B18118_11_003.jpg)'
- en: 10.3 – LSTM architecture
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 10.3 – LSTM 架构
- en: 'These components allow LSTMs to store and access information over long sequences.
    Let’s look at each of the components:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件使得 LSTM 能够在长序列中存储和访问信息。让我们来看看每个组件：
- en: '**Input gate**: The input gate decides what new information will be stored
    in the memory cell. It is made up of a sigmoid and a tanh layer. The sigmoid layer
    produces output values between zero and one, representing the importance level
    of each value in the input, where zero means “not important at all” and one represents
    “very important.” The tanh layer generates a set of candidate values that can
    be added to the state, essentially suggesting what new information should be stored
    in the memory cell. The outputs of both layers are merged by performing element-wise
    multiplication. This element-wise operation produces an input modulation gate
    that effectively filters the new candidate values, by deciding which information
    is important enough to be stored in the memory cell.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：输入门决定将存储在记忆单元中的新信息。它由 Sigmoid 和 tanh 层组成。Sigmoid 层产生介于零和一之间的输出值，表示输入中每个值的重要性水平，其中零意味着“一点也不重要”，而一表示“非常重要”。tanh
    层生成一组候选值，这些值可以添加到状态中，基本上建议应该将哪些新信息存储在记忆单元中。这两个层的输出通过逐元素乘法进行合并。这种逐元素操作生成一个输入调制门，有效地过滤新的候选值，通过决定哪些信息足够重要以存储在记忆单元中。'
- en: '**Forget gate**: This gate decides what information will be retained and what
    information will be discarded. It uses a sigmoid layer to return output values
    between zero and one. If a unit in the forget gate returns an output value close
    to zero, the LSTM will remove the information in the corresponding unit of the
    cell state.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：这个门决定哪些信息将被保留，哪些信息将被丢弃。它使用 Sigmoid 层返回介于零和一之间的输出值。如果遗忘门中的单元返回接近零的输出值，LSTM
    将从细胞状态的相应单元中删除信息。'
- en: '**Output gate**: The output gate determines what the next hidden state should
    be. Like the other gates, it also uses a sigmoid function to decide which parts
    of the cell state make it to the output.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：输出门决定下一个隐藏状态应该是什么。像其他门一样，它也使用 S 型函数来决定哪些细胞状态的部分将成为输出。'
- en: '**Cell state**: The cell state is the “memory” of the LSTM cell. It is updated
    based on the output from the forget and input gates. It can remember information
    for use later in the sequence.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细胞状态**：细胞状态是 LSTM 细胞的“记忆”。它基于遗忘门和输入门的输出进行更新。它可以记住信息以供后续序列使用。'
- en: The gate mechanisms are paramount because they allow the LSTM to automatically
    learn appropriate context-dependent ways to read, write, and reset cells in the
    memory. These capabilities enable LSTMs to handle longer sequences, making them
    particularly useful for many complex sequential tasks where standard RNNs fall
    short, due to their inability to handle long-term dependencies, such as machine
    translation, text generation, time-series prediction, and video analysis.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 门机制至关重要，因为它们允许 LSTM 自动学习适合上下文的读取、写入和重置记忆单元的方式。这些能力使得 LSTM 能够处理更长的序列，特别适用于许多复杂的顺序任务，标准
    RNN 由于无法处理长期依赖而不足，如机器翻译、文本生成、时间序列预测和视频分析。
- en: Bidirectional Long Short-Term Memory (BiLSTM)
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双向长短期记忆（BiLSTM）
- en: '**Bidirectional Long Short-Term Memory** (**BiLSTM**) is an extension of traditional
    LSTM networks. However, unlike LSTMs, which process information in a sequential
    fashion from start to finish, BiLSTMs run two LSTMs simultaneously – one processes
    sequential data from the start to the end and the other from the end to the start,
    as illustrated in *Figure 11**.4*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向长短时记忆网络**（**BiLSTM**）是传统 LSTM 网络的扩展。然而，与按顺序从开始到结束处理信息的 LSTM 不同，BiLSTM 同时运行两个
    LSTM ——一个从开始处理序列数据到结束，另一个从结束到开始处理，如*图 11.4*所示。'
- en: '![Figure 11.4 – The information flow in a BiLSTM](img/B18118_11_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – BiLSTM 中的信息流](img/B18118_11_004.jpg)'
- en: Figure 11.4 – The information flow in a BiLSTM
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – BiLSTM 中的信息流
- en: By doing this, BiLSTMs can capture both the past and future context of each
    data point in the sequence. Because of BiLSTMs’ ability to comprehend the context
    from both directions in a sequence of data, they are well suited for tasks such
    as text generation, text classification, sentiment analysis, and machine translation.
    Now, let's examine GRUs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，BiLSTM 能够捕捉序列中每个数据点的过去和未来上下文。由于 BiLSTM 能够从数据序列的两个方向理解上下文，因此它们非常适合执行文本生成、文本分类、情感分析和机器翻译等任务。现在，让我们来看看
    GRU。
- en: GRUs
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GRU
- en: In 2014, *Cho et al.* introduced the **GRU** architecture as a viable alternative
    to LSTMs. GRUs were designed to achieve two primary goals – one was to overcome
    the vanishing gradient issues that plagued traditional RNNs, and the other was
    to streamline LSTM architecture for increased computational efficiency while maintaining
    the ability to model long-term dependencies. Structurally, the GRU has two primary
    gates, as illustrated in *Figure 11**.5*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2014 年，*Cho 等人*提出了 **GRU** 架构，作为 LSTM 的一种可行替代方案。GRU 旨在实现两个主要目标——一个是克服困扰传统 RNN
    的梯度消失问题，另一个是简化 LSTM 架构，以提高计算效率，同时保持建模长期依赖的能力。从结构上看，GRU 有两个主要门，如*图 11.5*所示。
- en: '![Figure 11.5 – A GRU’s architecture](img/B18118_11_005.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – GRU 的架构](img/B18118_11_005.jpg)'
- en: Figure 11.5 – A GRU’s architecture
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – GRU 的架构
- en: One key difference between the GRU and LSTM is the absence of a separate cell
    state in GRUs; instead, they use a hidden state to transfer and manipulate information
    as well as streamline its computational needs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 和 LSTM 之间的一个关键区别是 GRU 中没有独立的细胞状态；相反，它们使用隐状态来传递和操作信息，并简化其计算需求。
- en: 'GRUs have two main gates, the update gate and the reset gate. Let’s examine
    them:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 有两个主要的门，更新门和重置门。让我们来看看它们：
- en: '**Update gate**: The update gate condenses the input and forget gates in LSTMs.
    It determines how much of the past information needs to be carried forward to
    the current state and which information needs to be discarded.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新门**：更新门将 LSTM 中的输入门和遗忘门进行了简化。它决定了多少过去的信息需要传递到当前状态，以及哪些信息需要被丢弃。'
- en: '**Reset gate**: This gate defines how much of the past information should be
    forgotten. It helps the model evaluate the relative importance of new input against
    past memory.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重置门**：此门定义了应忘记多少过去的信息。它帮助模型评估新输入与过去记忆之间的相对重要性。'
- en: Along with these two primary gates, a GRU introduces a “candidate hidden state.”
    This candidate hidden state combines the new input and the previous hidden state,
    and by doing so, it develops a preliminary version of the hidden state for the
    current time step. This candidate then plays an important role in determining
    the final hidden state, ensuring that the GRU retains relevant context from the
    past while accommodating new information. When deciding between LSTMs and GRUs,
    the choice is largely dependent on the specific application and the computational
    resources available. For some applications, the increased computational efficiency
    of GRU is more appealing. For example, in real-time processing, such as text-to-speech,
    or when working on tasks with short sequences, such as sentiment analysis of tweets,
    GRUs could prove to be an excellent choice in comparison to LSTMs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这两个主要门，GRU 还引入了“候选隐状态”。此候选隐状态结合了新输入和先前的隐状态，通过这种方式，它为当前时间步开发了隐状态的初步版本。然后，这个候选隐状态在决定最终隐状态时发挥重要作用，确保
    GRU 保留来自过去的相关上下文，同时接纳新信息。在决定选择 LSTM 还是 GRU 时，选择往往取决于特定的应用场景和可用的计算资源。对于某些应用，GRU
    提供的计算效率更具吸引力。例如，在实时处理（如文本转语音）中，或者在处理短序列任务（如推文的情感分析）时，GRU 相较于 LSTM 可能是一个更好的选择。
- en: We have provided a high-level discussion on RNNs and their variants. Let’s now
    proceed to apply these new architectures to a real-world use case. Will they outperform
    the standard DNNs or CNNs? Let’s find out in a text classification case study.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提供了关于RNN及其变种的高层次讨论。现在，让我们开始将这些新架构应用于实际的使用案例。它们会比标准的DNN或CNN表现更好吗？让我们在文本分类案例研究中一探究竟。
- en: "Text classification using the AG News dataset – \La comparative study"
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AG News数据集进行文本分类——一个比较研究
- en: The AG News dataset is a collection of more than 1 million news articles, collected
    from over 2,000 news sources by a news search engine called ComeToMyHead. The
    dataset is distributed across four categories – namely, world, sports, business,
    and science and technology – and it is available on **TensorFlow Datasets** (**TFDS**).
    The dataset is made up of 120,000 training samples (30,000 from each category),
    and the test set contains 7,600 examples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AG News数据集是一个包含超过100万篇新闻文章的集合，数据来自一个名为ComeToMyHead的新闻搜索引擎，覆盖了2000多个新闻来源。该数据集分为四个类别——即世界、体育、商业、科技——并且可通过**TensorFlow
    Datasets**（**TFDS**）获取。数据集包括120,000个训练样本（每个类别30,000个），测试集包含7,600个样本。
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This experiment may take about an hour to run, due to the size of the dataset
    and the number of models; hence, it is important to ensure your notebook is GPU-enabled.
    Again, you could take a smaller subset to ensure your experiments run much faster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集的大小和模型数量，此实验可能需要大约一个小时才能完成，因此确保你的笔记本支持GPU是很重要的。你也可以选择一个较小的子集，以确保实验能够更快地运行。
- en: 'Let’s start building our model:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建我们的模型：
- en: 'We will begin by loading the necessary libraries for this experiment:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载本次实验所需的库：
- en: '[PRE0]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These imports form the building blocks, enabling us to solve this text classification
    problem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入形成了构建模块，使我们能够解决这个文本分类问题。
- en: 'Then, we load the AG News dataset from TFDS:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从TFDS加载AG News数据集：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We use this code to load our dataset from TFDS – the `tfds.load` function fetches
    and loads the AG News dataset. We set the `with_info` argument to `True`; this
    ensures the metadata of our dataset, such as the total number of samples and the
    version, is also collected. This metadata information is stored in the `info`
    variable. We also set `as_supervised` to `True`; we do this to ensure that data
    is loaded in input and label pairs, where the input is the news article and the
    label is the corresponding category. Then, we split the data into a training set
    and a test set.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用此代码从TFDS加载我们的数据集——`tfds.load`函数会获取并加载AG News数据集。我们将`with_info`参数设置为`True`；这确保了我们数据集的元数据，如样本总数和版本，也会被收集。此元数据信息存储在`info`变量中。我们还将`as_supervised`设置为`True`；我们这样做是为了确保数据以输入和标签对的形式加载，其中输入是新闻文章，标签是对应的类别。然后，我们将数据划分为训练集和测试集。
- en: 'Now, we need to prepare our data for modeling:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要为建模准备数据：
- en: '[PRE15]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we perform data preparatory steps such as tokenization, sequencing, and
    padding, using TensorFlow’s Keras API. We initialize the tokenizer, which we use
    to convert our data from text to a sequence of integers. We set the `num_words`
    parameter to `20000`. This means we will only consider the top 20,000 occurring
    words in our dataset for tokenization; less frequently occurring words below this
    limit will be ignored. We set the `oov_token="<OOV>"` parameter to ensure we cater
    to unseen words that we may encounter during model inferencing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们执行数据预处理步骤，如分词、序列化和填充，使用TensorFlow的Keras API。我们初始化了分词器，用于将我们的数据从文本转换为整数序列。我们将`num_words`参数设置为`20000`。这意味着我们只会考虑数据集中出现频率最高的20,000个单词进行分词；低于此频率的词汇将被忽略。我们将`oov_token="<OOV>"`参数设置为确保我们能处理在模型推理过程中可能遇到的未见过的单词。
- en: Then, we extract the training data and store it in the `train_texts` variable.
    We tokenize and transform our data into a sequence of integers by mapping numerical
    values to tokens, using the `fit_on_texts` and `texts_to_sequences()` methods
    respectively. We apply padding to each sequence to ensure that the data we will
    input into our models is of a consistent shape. We set `padding` to `post`; this
    will ensure padding is applied at the end of a sequence. We now have our data
    in a well-structured format, which we will feed into our deep-learning models
    for text classification shortly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取训练数据并将其存储在 `train_texts` 变量中。我们通过使用 `fit_on_texts` 和 `texts_to_sequences()`
    方法分别将数据标记化并转化为整数序列。我们对每个序列应用填充，以确保输入模型的数据具有一致的形状。我们将 `padding` 设置为 `post`；这将确保填充在序列的末尾应用。现在，我们的数据已经以良好的结构化格式准备好，稍后我们将其输入深度学习模型进行文本分类。
- en: 'Before we start modeling, we want to split our data into training and validation
    sets. We do this by splitting our training set into 80 percent for training and
    20 percent for validation:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始建模之前，我们需要将数据分为训练集和验证集。我们通过将训练集拆分为 80% 的训练数据和 20% 的验证数据来实现：
- en: '[PRE23]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We convert our labels to one-hot encoding vectors, after which we split our
    training data using the `train_test_split` function from scikit-learn. We set
    our `test_size` to `0.2`; this means we will have 80 percent of our data for training
    and the remaining 20 percent for validation purposes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将标签转换为 one-hot 编码向量，然后使用 scikit-learn 的 `train_test_split` 函数将训练数据拆分。我们将 `test_size`
    设置为 `0.2`；这意味着我们将 80% 的数据用于训练，其余 20% 用于验证目的。
- en: 'Let’s set the `vocab_size`, `embedding_dim`, and `max_length` parameters:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置 `vocab_size`、`embedding_dim` 和 `max_length` 参数：
- en: '[PRE31]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We set `vocab_size` and `embedding_dim` to `20000` and `64`, respectively. When
    selecting your `vocab_size`, it is important to strike a good balance between
    computation efficiency, model complexity, and the ability to capture language
    nuances, while we use our embedding dimension to represent each word in our vocabulary
    by a 64-dimensional vector. The `max_length` parameter is set to match the longest
    tokenized and padded sequence in our data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `vocab_size` 和 `embedding_dim` 分别设置为 `20000` 和 `64`。在选择 `vocab_size` 时，重要的是在计算效率、模型复杂度和捕捉语言细微差别的能力之间找到良好的平衡，而我们使用嵌入维度通过
    64 维向量表示词汇表中的每个单词。`max_length` 参数设置为与数据中最长的标记化和填充序列匹配。
- en: 'We begin building our models, starting with a DNN:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们开始构建模型，从 DNN 开始：
- en: '[PRE34]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Using the `Sequential` API from TensorFlow, we build a DNN made up of an embedding
    layer, a flatten layer, two hidden layers, and an output layer for multiclass
    classification.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Sequential` API，我们构建了一个 DNN，包含一个嵌入层、一个展平层、两个隐藏层和一个用于多类别分类的输出层。
- en: 'Then, we build a CNN architecture:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建一个 CNN 架构：
- en: '[PRE43]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We use a `Conv1D` layer made up of 128 filters (feature detectors) and a kernel
    size of `5`; this means it will consider five words at a time. Our architecture
    uses `GlobalMaxPooling1D` to downsample the output of the convolutional layer
    to the most significant features. We feed the output of the pooling layer into
    a fully connected layer for classification.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个由 128 个滤波器（特征检测器）和 `5` 的卷积核大小组成的 `Conv1D` 层；这意味着它会同时考虑五个单词。我们的架构使用 `GlobalMaxPooling1D`
    对卷积层的输出进行下采样，提取最重要的特征。我们将池化层的输出输入到一个全连接层进行分类。
- en: 'Then, we build an LSTM model:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建一个 LSTM 模型：
- en: '[PRE52]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Our LSTM architecture is made up of two LSTM layers made up of 32 units each.
    In the first LSTM layer, we set `return_sequences` to `True`; this allows the
    first LSTM layer to pass the complete sequence it received as output to the next
    LSTM layer. The idea here is to allow the second LSTM layer access to the context
    of the entire sequence; this equips it with the ability to better understand and
    capture dependencies across the entire sequence. We then feed the output of the
    second LSTM layer into the fully connected layers to classify our data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 LSTM 架构由两个 LSTM 层组成，每个层有 32 个单元。在第一个 LSTM 层中，我们将 `return_sequences` 设置为
    `True`；这允许第一个 LSTM 层将它收到的完整序列作为输出传递给下一个 LSTM 层。这里的目的是让第二个 LSTM 层能够访问整个序列的上下文；这使它能够更好地理解并捕捉整个序列中的依赖关系。然后，我们将第二个
    LSTM 层的输出输入到全连接层进行分类。
- en: 'For our final model, let’s use a bidirectional LSTM:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于我们的最终模型，我们使用一个双向 LSTM：
- en: '[PRE61]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Here, instead of the LSTM layers, two bidirectional LSTM layers are added. Note
    that the first layer also has `return_sequences=True` to return the full outputs
    to the next layer. Using a bidirectional wrapper allows each LSTM layer access
    to both past and future context when processing each element of the input sequence,
    providing additional contextual information when compared with a unidirectional
    LSTM.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们没有使用LSTM层，而是增加了两层双向LSTM。请注意，第一层也设置了`return_sequences=True`，以将完整的输出传递给下一层。使用双向包装器可以让每个LSTM层在处理输入序列中的每个元素时同时访问过去和未来的上下文，与单向LSTM相比，提供了更多的上下文信息。
- en: Stacking BiLSTM layers can help us build higher-level representations of the
    full sequence. The first BiLSTM extracts features by looking at the text from
    both directions while preserving the entire sequence. The second BiLSTM can then
    build on those features by further processing them. The final classification is
    carried out by the output layer in the fully connected layer. Our experimental
    models are now all set up, so let’s proceed with compiling and fitting them next.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠BiLSTM层可以帮助我们构建更高层次的完整序列表示。第一层BiLSTM通过从两个方向查看文本来提取特征，同时保持整个序列的完整性。第二层BiLSTM则可以在这些特征的基础上进一步处理它们。最终的分类是由全连接层中的输出层完成的。我们的实验模型现在都已设置好，接下来让我们继续编译并拟合它们。
- en: 'Let’s compile and fit all the models we have built so far:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编译并拟合我们目前为止构建的所有模型：
- en: '[PRE69]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We use a `for` loop to compile and fit all four models. We set `verbose` to
    `False`; this way, we don’t print the training information. We train for 10 epochs.
    Do expect this step to take a while, as we have a massive dataset and are experimenting
    with four models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`for`循环编译并拟合所有四个模型。我们将`verbose`设置为`False`；这样，我们就不会打印训练信息。我们训练10个周期。请预期这个步骤需要一些时间，因为我们有一个庞大的数据集，并且正在尝试四个模型。
- en: 'Let’s evaluate our model on unseen data:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在未见过的数据上评估我们的模型：
- en: '[PRE77]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'To evaluate our model, we need to prepare our test data in the right fashion.
    We first extract our text data from our `test_dataset`, after which we tokenize
    the text using the tokenizer from our training process. The tokenized text is
    then converted into a sequence of integers, and padding is applied to ensure all
    sequences are of the same length as the longest sequence in our training data.
    Just like we did during training, we also one-hot-encode our test labels, and
    then we apply a `for` loop to iterate over each individual model, generating the
    test loss and accuracy for all our models. The output is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们需要以正确的方式准备我们的测试数据。我们首先从`test_dataset`中提取文本数据，然后使用我们训练过程中得到的分词器对文本进行分词。分词后的文本会被转换为整数序列，并应用填充，以确保所有序列的长度与训练数据中最长的序列相同。就像我们在训练过程中所做的那样，我们还对测试标签进行独热编码，然后应用`for`循环迭代每个单独的模型，生成所有模型的测试损失和准确率。输出如下：
- en: '[PRE96]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: From our returned results, we can see that our LSTM model achieved the highest
    accuracy (90.08%); other models performed quite well too. We can take this performance
    as a good starting point; we can also apply some of the ideas we used in [*Chapter
    8*](B18118_08.xhtml#_idTextAnchor186)*, Handling Overfitting,* and [*Chapter 10*](B18118_10.xhtml#_idTextAnchor226)*,
    Introduction to Natural Language Processing,* to improve our results here.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的返回结果来看，我们可以看到LSTM模型达到了最高的准确率（90.08%）；其他模型的表现也相当不错。我们可以把这个表现作为一个良好的起点；我们也可以将我们在[*第8章*](B18118_08.xhtml#_idTextAnchor186)《处理过拟合》和[*第10章*](B18118_10.xhtml#_idTextAnchor226)《自然语言处理导论》中使用的一些方法应用到这里，以进一步改善我们的结果。
- en: In [*Chapter 10*](B18118_10.xhtml#_idTextAnchor226), *Introduction to Natural
    Language Processing,* we talked about pretrained embeddings. These embeddings
    are trained on a large corpus of text data. Let’s see how we can leverage them;
    perhaps they can help us achieve a better result in this case.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B18118_10.xhtml#_idTextAnchor226)《自然语言处理导论》中，我们讨论了预训练的嵌入。这些嵌入是在大量文本数据上训练的。让我们看看如何利用它们；也许它们能帮助我们在这种情况下取得更好的结果。
- en: Using pretrained embeddings
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的嵌入
- en: 'In [*Chapter 9*](B18118_09.xhtml#_idTextAnchor210), *Transfer Learning,* we
    explored the concept of transfer learning. Here, we will revisit this concept
    as it relates to word embeddings. In all the models we have built up so far, we
    trained our word embeddings from scratch. Now, we will examine how to leverage
    pretrained embeddings that have been trained on massive amounts of text data,
    such as Word2Vec, GloVe, and FastText. Using these embeddings can be advantageous
    for two reasons:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 9 章*](B18118_09.xhtml#_idTextAnchor210)《迁移学习》中，我们探讨了迁移学习的概念。在这里，我们将重新审视这一概念，并与词嵌入相关联。在迄今为止构建的所有模型中，我们都是从零开始训练我们的词嵌入。现在，我们将探讨如何利用已经在大量文本数据上训练好的预训练嵌入，例如
    Word2Vec、GloVe 和 FastText。使用这些嵌入有两个主要优点：
- en: Firstly, they are already trained on a massive and diverse set of data, so they
    have a rich understanding of language.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它们已经在大量且多样化的数据集上进行了训练，因此它们对语言有着深刻的理解。
- en: Secondly, the training process is much faster, since we will skip training our
    own word embeddings from scratch. Instead, we can build our models on the information
    packed in these embeddings, focusing on the task at hand.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，训练过程更快，因为我们跳过了从头开始训练自己词嵌入的步骤。相反，我们可以在这些嵌入中所包含的信息基础上构建模型，专注于当前任务。
- en: It is important to note that using pretrained embeddings isn’t always the right
    choice. For example, if you work on niche-based text data such as medical or legal
    data, sectors that apply a lot of domain-specific terminology may be underrepresented.
    When we use a pretrained embedding blindly for these use cases, they may lead
    to suboptimal performance. In these types of scenarios, you can either train your
    own embedding, which comes with an increased computational cost, or use a more
    balanced approach and fine-tune pretrained embeddings on your data. Let’s see
    how we can apply pretrained embeddings in our workflow.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，使用预训练嵌入并不总是正确的选择。例如，如果你处理的是专业领域的文本数据，如医学或法律数据，包含大量特定领域术语的行业可能没有得到充分的表示。当我们盲目使用预训练的嵌入来处理这些用例时，可能会导致表现不佳。在这些情况下，你可以选择训练自己的嵌入，尽管这会增加计算成本，或者采用更平衡的方式，在你的数据上微调预训练的嵌入。让我们来看一下如何在工作流中应用预训练嵌入。
- en: Text classification using pretrained embedding
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练嵌入进行文本分类
- en: 'To follow this experiment, you will need to use the second notebook in this
    chapter’s GitHub repository called `modelling with pretrained embeddings`. We
    will continue with the same dataset. This time, we will focus on using our best
    model with the GloVe pretrained embedding. We will use our best model (LSTM) from
    our initial round of experiments. Let’s start:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个实验，你需要使用本章 GitHub 仓库中的第二个笔记本，名为 `modelling with pretrained embeddings`。我们将继续使用相同的数据集。这一次，我们将重点使用我们最好的模型与
    GloVe 预训练嵌入。我们将使用在初步实验中得到的最佳模型（LSTM）。让我们开始吧：
- en: 'We will begin by importing the necessary libraries for this experiment:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入本实验所需的库开始：
- en: '[PRE97]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Once we import our libraries, we will download our pretrained embeddings.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们导入了库，就可以下载预训练的嵌入。
- en: 'Run the following commands to download the pretrained embeddings:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令来下载预训练的嵌入：
- en: '[PRE104]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We will download the GloVe 6B embedding files from the Stanford NLP website
    into our Colab notebooks, using the `wget` command. Then, we unzip the compressed
    files. We can see that this file contains different pretrained embeddings, as
    shown in *Figure 11**.6*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过 `wget` 命令从斯坦福 NLP 网站下载 GloVe 6B 嵌入文件到我们的 Colab 笔记本中，然后解压这些压缩文件。我们可以看到，这些文件包含了不同的预训练嵌入，如*图
    11.6*所示：
- en: '![Figure 11.6 – The directory displaying the GloVe 6B embeddings files](img/B18118_11_006.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 显示 GloVe 6B 嵌入文件的目录](img/B18118_11_006.jpg)'
- en: Figure 11.6 – The directory displaying the GloVe 6B embeddings files
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 显示 GloVe 6B 嵌入文件的目录
- en: The GloVe 6B embedding is made up of a 6 billion-token word embedding, trained
    by researchers at Stanford University and made publicly available to us all. For
    computational reasons, we will use the 50-dimensional vectors. You may wish to
    try out higher dimensions for richer representations, especially when working
    on tasks that require you to capture more complex semantic relationships, but
    also be mindful of the compute required.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 6B 嵌入由 60 亿词元的词向量组成，由斯坦福大学的研究人员训练，并公开提供给我们使用。出于计算考虑，我们将使用 50 维的向量。你也许希望尝试更高维度的嵌入，以获得更丰富的表示，尤其是在处理需要捕捉更复杂语义关系的任务时，但也需要注意所需的计算资源。
- en: 'Then, we load the AG News dataset:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载 AG News 数据集：
- en: '[PRE106]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: We load our dataset and split it into training and testing sets.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载数据集并将其拆分为训练集和测试集。
- en: 'Then, we tokenize and sequence our training set:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对训练集进行分词和序列化：
- en: '[PRE110]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: We prepare our data for modeling, just like our last experiment. We set the
    vocabulary size to 2,000 words and use `OOV` for out-of-vocabulary words. Then,
    we tokenize and pad the data to ensure consistency in the length of it.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像上次实验一样准备数据进行建模。我们将词汇表大小设置为2,000个单词，并使用`OOV`来表示词汇表外的单词。然后，我们对数据进行分词和填充，以确保数据长度的一致性。
- en: 'Then, we process our test data:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们处理测试数据：
- en: '[PRE120]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: We process our testing data in a similar fashion to our training data. However,
    it is important to note that we do not apply `fit_on_texts` to our test set, ensuring
    that the tokenizer remains the same as the training set.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以类似训练数据的方式处理测试数据。然而，值得注意的是，我们不会对测试集应用`fit_on_texts`，以确保分词器与训练集相同。
- en: 'Then, we set the embedding parameters:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置嵌入参数：
- en: '[PRE126]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: We define the size of our vocabulary and we set the dimensionality of our embeddings
    to `50`. We use this because we are working with 50d pretrained embeddings.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了词汇表的大小，并将词向量的维度设置为`50`。我们使用这个设置是因为我们正在使用50维的预训练词向量。
- en: 'Apply the pretrained word embeddings:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用预训练的词向量：
- en: '[PRE128]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: We access the `glove.6B.50d.txt` file and read it line by line. Each line contains
    a word and its corresponding embeddings. We cross-match words in the GloVe file
    with those in our own vocabulary, constructed with the Keras tokenizer. If there
    is a match, we take the corresponding word index from our own vocabulary and update
    our initially zero-initialized embedding matrix at that index with the GloVe embeddings.
    Conversely, words that do not match will remain as zero vectors in the matrix.
    We will use this embedding matrix to initialize the weight of our embedding layer
    shortly. We use the file path to the 50d embeddings, as shown in *Figure 11**.6*.
    You can do this by right-clicking on the specified file and copying the file path.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们访问`glove.6B.50d.txt`文件，并逐行读取。每一行包含一个单词及其对应的词向量。我们将GloVe文件中的单词与我们自己用Keras分词器构建的词汇表中的单词进行交叉匹配。如果匹配成功，我们从自己的词汇表中获取对应的单词索引，并将该索引位置的初始为零的嵌入矩阵更新为GloVe词向量。相反，未匹配的单词将在矩阵中保持为零向量。我们将使用这个嵌入矩阵来初始化我们的嵌入层权重。我们使用50维词向量文件的路径，如*图11.6*所示。你可以通过右键点击指定文件并复制文件路径来实现。
- en: 'Let’s build, compile, and train our LSTM model:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建、编译并训练我们的LSTM模型：
- en: '[PRE137]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: While building our model, the key difference from our previous experiment is
    the embedding layer initialization. Here, we leverage our pretrained embedding
    matrix, and to ensure the weights remain unchanged, we set the trainable parameter
    to `false`. Everything else in our model’s architecture remains the same. Then,
    we compile and fit our model for 10 epochs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，与我们之前的实验的主要区别是嵌入层的初始化。在这里，我们利用预训练的嵌入矩阵，并为了确保权重保持不变，我们将可训练参数设置为`false`。模型架构中的其他部分保持不变。然后，我们编译并训练模型10个epoch。
- en: 'Finally, we evaluate our model:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们评估我们的模型：
- en: '[PRE156]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: We reached an accuracy of 89% on our test set, although we did not outperform
    our best model when we didn’t apply pretrained embeddings. Perhaps you may want
    to try out larger embedding dimensions from `glove6B` or other word embeddings
    to improve our result here. That would be a good exercise and is well encouraged.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在测试集上的准确率达到了89%，尽管当我们没有应用预训练词向量时，并没有超越我们最好的模型。也许你可以尝试使用`glove6B`中的更大维度的词向量，或者其他的词嵌入来提升我们的结果。那将是一个很好的练习，并且非常值得鼓励。
- en: Now, it is time to move on to another exciting topic – text generation with
    LSTM.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候进入另一个激动人心的话题——使用LSTM生成文本。
- en: Using LSTMs to generate text
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM生成文本
- en: We have explored LSTMs in text classification. Now, we will look at how to generate
    text that you would see in a novel, blog post, or children’s storybook, ensuring
    that is coherent and consistent with what we expect from these types of texts.
    LSTMs prove useful here, due to their ability to capture and remember intricate
    patterns for long sequences. When we train an LSTM on a large volume of text data,
    we allow it to learn the linguistic structure, style, and nuances. It can apply
    this to generate new sentences in line with the style and approach of the training
    set.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了LSTM在文本分类中的应用。现在，我们将看看如何生成小说、博客文章或儿童故事书中可能出现的文本，确保这些文本连贯且符合我们对这些类型文本的预期。LSTM在这里非常有用，因为它能够捕捉和记住长序列中复杂的模式。当我们用大量文本数据训练LSTM时，我们让它学习语言结构、风格和细微差别。它可以利用这些知识生成与训练集风格和方法一致的新句子。
- en: Let’s imagine we are playing a word prediction game with our friends. The goal
    is to coin a story in which each friend comes up with a word to continue the story.
    To begin, we have a set of words, which we will call the seed, to set the tone
    for our story. From the seed sentence, each friend contributes a subsequent word
    until we have a complete story. We can also apply this idea to LSTMs – we feed
    a seed sentence into our model, and then we ask it to predict the next word, just
    like our word prediction game. This time, however, the game is played only by
    the LSTM, and we just need to specify the number of words it will play for. The
    result of each round of the game will serve as input to the LSTM for the next
    round, until we achieve our specified number of words.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在和朋友玩一个词汇预测游戏。目标是创造一个故事，每个朋友都提供一个单词来继续这个故事。为了开始，我们有一组单词，我们称之为种子词，以设定故事的基调。从种子句子开始，每个朋友都会贡献下一个单词，直到故事完成。我们也可以将这个想法应用到LSTM中——我们将一个种子句子输入到模型中，然后让它预测下一个单词，就像我们玩词汇预测游戏一样。然而，这次游戏只有LSTM在进行，而我们只需要指定它会生成的单词数。每轮游戏的结果将作为下一轮输入，直到我们达到指定的单词数。
- en: 'The question that comes to mind is, how does an LSTM know what word to predict
    next? This is where the concept of **windowing** comes into play. Let’s say we
    have a sample sentence such as “*I once had a dog called Jack.*” When we apply
    windowing with a window size of four to this sentence, we will have the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 脑海中出现的问题是，LSTM是如何知道接下来预测哪个单词的呢？这就是**滑动窗口**概念发挥作用的地方。假设我们有一个样本句子，比如“*我曾经有一只狗叫做杰克*”。当我们对这个句子应用大小为四的滑动窗口时，我们将得到以下结果：
- en: “*I once* *had a*”
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*我曾经* *有一只*”
- en: “*once had* *a dog*”
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*曾经有* *一只狗*”
- en: “*had a* *dog called*”
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*曾经有* *一只狗叫做*”
- en: “*a dog* *called Jack*”
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “*一只狗* *叫做杰克*”
- en: 'We can now split each of these sentences into input-output pairs. For example,
    in the first sentence, we will have “*I once had*” as the input, and the output
    will be “*a*.” We will apply the same approach to all the other sentences, thus
    giving us the following input-output pairs:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这些句子拆分为输入-输出对。例如，在第一个句子中，输入是“*曾经有*”，输出是“*一只*”。我们将对所有其他句子应用相同的方法，得到以下输入-输出对：
- en: ([“*I*”, “*once*”, “*had*”], “*a*”)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ([“*我*”, “*曾经*”, “*有*”], “*一只*”)
- en: ([“*once*”, “*had*”, “*a*”], “*dog*”)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ([“*曾经*”, “*有*”, “*一只*”], “*狗*”)
- en: ([“*had*”, “*a*”, “*dog*”], “*called*”)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ([“*有*”, “*一只*”, “*狗*”], “*叫做*”)
- en: ([“*a*”, “*dog*”, “*called*”], “*Jack*”)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ([“*一只*”, “*狗*”, “*叫做*”], “*杰克*”)
- en: By using windowing, our LSTM focuses on the most recent set of words, which
    often holds the most relevant information to predict the next word in our text.
    Also, when we work with smaller, fixed-length sequences, it streamlines our training
    process and also optimizes our memory usage. Let’s see how we can apply this idea
    in our next case study.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用滑动窗口，我们的LSTM专注于最最近的词汇集，这些词汇通常包含最相关的信息，用于预测下一个单词。而且，当我们处理较小的固定长度序列时，它能简化我们的训练过程，并优化内存使用。接下来，让我们看看如何将这一思想应用到下一个案例研究中。
- en: Story generation using LSTMs
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LSTM进行故事生成
- en: 'In this case study, imagine you are a new NLP engineer working for a London-based
    start-up called Readrly. Your job is to build an AI storyteller for the company.
    You have been provided with a training dataset called `stories.txt`, which contains
    30 sample stories. Your job is to train an LSTM to generate exciting children’s
    stories. Let’s return to our notebook and see how to make this happen:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，假设你是一个新加入伦敦初创公司Readrly的NLP工程师。你的工作是为公司构建一个AI讲故事系统。你收到了一个名为`stories.txt`的训练数据集，其中包含30个示例故事。你的任务是训练一个LSTM来生成有趣的儿童故事。让我们回到我们的笔记本，看看如何实现这一目标：
- en: 'As we did previously, we will begin by importing all the libraries required
    for this task:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同我们之前做的那样，我们将从导入所有任务所需的库开始：
- en: '[PRE160]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'Then, we load the `stories.txt` dataset:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载`stories.txt`数据集：
- en: '[PRE166]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: We read our story file and convert all the contents to lowercase to ensure our
    data is in a consistent form, avoiding duplicate tokens generated from capitalized
    and non-capitalized versions of the same word.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取故事文件，并将所有内容转换为小写，以确保数据的一致性，避免由于大写和小写版本的同一单词而生成重复的标记。
- en: Note
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This step helps us to reduce the vocabulary size by removing semantic nuances
    created by capitalization. However, this step should be carried out judiciously,
    as it may have a negative effect – for example, when we use the word “*march*.”
    If we lowercase the word, it will denote some form of walking, while with a capital
    M, it refers to the month of the year.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步有助于通过删除由大小写带来的语义差异来减少词汇量。然而，这一步应该谨慎操作，因为它可能产生负面影响——例如，当我们使用“*march*”这个词时。如果将其转为小写，它将表示某种形式的行走，而大写的M则指代月份。
- en: 'Tokenize the text:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本进行分词：
- en: '[PRE167]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: We calculate the total number of unique words in our vocabulary. We add one
    to account for out-of-vocabulary words.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算词汇表中唯一单词的总数。我们加1以考虑词汇表外的单词。
- en: 'Then, we convert the text to sequences:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将文本转换为序列：
- en: '[PRE170]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: In this step, we develop a dataset made up of *n*-gram sequences, where each
    entry in the “input sequence” is a sequence of words (that is, word numbers) that
    appear in the text. For every sequence of *n* words, *n*-1 words will be our input
    features, and the *n*-th word is the label that our model tries to predict.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们开发一个由*n*元组序列组成的数据集，其中“输入序列”中的每个条目都是在文本中出现的单词序列（即，单词编号）。对于每个*n*个单词的序列，*n*-1个单词将作为输入特征，*n*-th个单词是我们模型尝试预测的标签。
- en: Note
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: An n-gram is a sequence of n words from a given text.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一个n-gram是从给定文本中提取的n个单词的序列。
- en: 'For example, let’s say we have a sample sentence such as “*The dog played with
    the cat.*” Using a 2-gram (bigram) model on a sentence would split it into the
    following bigrams:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一句示例句子：“*The dog played with the cat.*” 使用2元组（bigram）模型对这句话进行分割，将其分成以下的大写二元组：
- en: (“*The*”, “*dog*”), (“*dog*”, “*played*”), (“*played*”, “*with*”), (“*with*”,
    “*the*”), (“*the*”, “*cat*”)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: (“*The*”, “*dog*”), (“*dog*”, “*played*”), (“*played*”, “*with*”), (“*with*”,
    “*the*”), (“*the*”, “*cat*”)
- en: 'Alternatively, if we use a 3-gram (trigram) model, it would split the sentence
    into trigrams:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们使用3元组（trigram）模型，它会将句子分割成三元组：
- en: (“*The*”, “*dog*”, “*played*”), (“*dog*”, “*played*”, “*with*”), (“*played*”,
    “*with*”, “*the*”), (“*with*”, “*the*”, “*cat*”)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (“*The*”, “*dog*”, “*played*”), (“*dog*”, “*played*”, “*with*”), (“*played*”,
    “*with*”, “*the*”), (“*with*”, “*the*”, “*cat*”)
- en: '*N*-gram models are common in NLP for text prediction, spelling correction,
    language modeling, and feature extraction.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*N*-gram模型在自然语言处理（NLP）中广泛应用于文本预测、拼写纠正、语言建模和特征提取。'
- en: 'Then, we pad the sequences:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们对序列进行填充：
- en: '[PRE177]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: We use padding to ensure our input data is of a consistent format. We set `padding`
    to `pre` to ensure that our LSTM captures the most recent words at the end of
    each sequence; these words are relevant to predict the next word in the sequence.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用填充来确保输入数据格式的一致性。我们将`padding`设置为`pre`，以确保我们的LSTM在每个序列的末尾捕捉到最新的单词；这些单词对于预测序列中的下一个单词是相关的。
- en: 'We now split the sequences into features and labels:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将序列拆分为特征和标签：
- en: '[PRE181]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: Here, we one-hot-encode our labels to represent them as vectors. Then, we build
    our model.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过将标签进行一热编码（one-hot encoding）来表示它们为向量。然后，我们构建我们的模型。
- en: 'Create the model:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型：
- en: '[PRE185]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: We build a text generation model using a bidirectional LSTM, as it has the ability
    to capture both past and future data points. Our embedding layer transforms each
    word’s numerical representation into a dense vector, with a dimension of 200 each.
    The next layer is the bidirectional LSTM layer with 200 units, after which we
    feed the output data into the dense layers. Then, we compile and fit the model
    for 300 epochs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用双向LSTM构建一个文本生成模型，因为它能够捕捉过去和未来的数据点。我们的嵌入层将每个单词的数字表示转换为一个密集的向量，每个向量的维度为200。接下来是具有200个单元的双向LSTM层，之后我们将输出数据传递给全连接层。然后，我们编译并训练模型300个周期。
- en: 'Create a function to make the predictions:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来进行预测：
- en: '[PRE195]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: We construct the `story_generator` function. We start with the seed text, which
    we use to prompt the model to generate more text for our children’s stories. The
    seed text is transformed into its tokenized form, after which it is padded at
    the beginning to match the expected length of the input sequence, `max_sequence_len-1`.
    To predict the next word’s token, we use the `predict` method and apply `np.argmax`
    to select the most likely next word. The predicted token is then mapped back to
    its corresponding word, which is appended to the existing seed text. The process
    is repeated until we achieve the desired number of words (`next_words`), and the
    function returns the fully generated text (`seed_text +` `next_words`)
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了 `story_generator` 函数。我们从种子文本开始，用它来提示模型为我们的儿童故事生成更多的文本。种子文本被转换为标记化形式，然后在开头进行填充，以匹配输入序列的预期长度
    `max_sequence_len-1`。为了预测下一个词的标记，我们使用 `predict` 方法并应用 `np.argmax` 来选择最可能的下一个词。预测的标记随后被映射回对应的词，并附加到现有的种子文本上。这个过程会重复，直到生成所需数量的词（`next_words`），函数返回完整生成的文本（`seed_text
    + next_words`）。
- en: 'Let’s generate the text:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们生成文本：
- en: '[PRE212]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: 'We define the seed text, which is the `input_text` variable here. Our choice
    of next words we want the model to generate is `50`, and we pass in our trained
    model and also `max_sequence_len`. When we run the code, it returns the following
    output:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了种子文本，这里是 `input_text` 变量。我们希望模型生成的下一个词的数量是 `50`，并传入已训练的模型以及 `max_sequence_len`。当我们运行代码时，它返回以下输出：
- en: '[PRE215]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE215]'
- en: The sample of generated text indeed resembles something you might find in a
    children’s storybook. It continues the initial prompt coherently and creatively.
    While this example showcases the power of LSTMs as text generators, in this era,
    we leverage **large language models** (**LLMs**) for such applications. These
    models are trained on massive datasets, and they have a more sophisticated understanding
    of language; hence, they can generate more compelling stories when we prompt or
    fine-tune them properly.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本示例确实像你可能在儿童故事书中看到的内容。它连贯且富有创意地继续了初始提示。虽然这个例子展示了 LSTM 作为文本生成器的强大功能，但在这个时代，我们利用
    **大语言模型**（**LLM**）来实现这样的应用。这些模型在庞大的数据集上训练，并且具有更为复杂的语言理解能力，因此，当我们适当提示或微调它们时，它们可以生成更具吸引力的故事。
- en: We are now at the end of this chapter on NLP. You should now have the requisite
    foundational ideas to build your own NLP projects with TensorFlow. All you have
    learned here will also help you effectively navigate the NLP section of the TensorFlow
    Developer certificate exam. You have come a long way, and you should give yourself
    credit. We have one more chapter to go. Before we proceed to the chapter on time
    series, let's summarize what we learned in this chapter.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经进入了本章关于自然语言处理（NLP）的最后部分。你现在应该掌握了构建自己的 NLP 项目所需的基础知识，使用 TensorFlow 来实现。你在这里学到的所有内容，也将帮助你有效地应对
    TensorFlow 开发者证书考试中的 NLP 部分。你已经走了很长一段路，应该为自己鼓掌。我们还有一章内容。让我们在继续时间序列章节之前，回顾一下本章学到的内容。
- en: Summary
  id: totrans-371
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we embarked on a voyage through the world of RNNs. We began
    by looking at the anatomy of RNNs and their variants, and we explored the task
    of classifying news articles using different model architectures. We took a step
    further by applying pretrained word embeddings to our best-performing model in
    our quest to improve it. Here, we learned how to apply pretrained word embeddings
    in our workflow. For our final challenge, we took on the task of building a text
    generator to generate children’s stories.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们踏上了探索 RNN 世界的旅程。我们首先了解了 RNN 及其变体的结构，然后通过不同的模型架构探索了新闻文章分类任务。我们进一步应用了预训练的词嵌入，提升了我们最优模型的性能。在此过程中，我们学会了如何在工作流中应用预训练词嵌入。作为最后的挑战，我们进行了构建文本生成器的任务，用来生成儿童故事。
- en: In the next chapter, we will examine time series, explore its unique characteristics,
    and uncover various methods of building forecasting models. We will tackle a time-series
    problem, where we will master how to prepare, train, and evaluate time-series
    data.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨时间序列，了解它的独特特点，并揭示构建预测模型的各种方法。我们将解决一个时间序列问题，在其中掌握如何准备、训练和评估时间序列数据。
- en: Questions
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let’s test what we learned in this chapter:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试本章所学：
- en: Load the IMDB movies review dataset from TensorFlow and preprocess it.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 TensorFlow 加载 IMDB 电影评论数据集并进行预处理。
- en: Build a CNN movie classifier.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个 CNN 电影分类器。
- en: Build an LSTM movie classifier.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个 LSTM 电影分类器。
- en: Use the GloVe 6B embedding to apply a pretrained word embedding to LSTM architecture.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GloVe 6B嵌入将预训练的词嵌入应用到LSTM架构中。
- en: Evaluate all the models, and save your best-performing one.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估所有模型，并保存表现最好的那个。
- en: Further reading
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, you can check out the following resources:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多信息，您可以查看以下资源：
- en: Cho, K., et al. (2014). *Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation*. arXiv preprint arXiv:1406.1078.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho, K., 等. (2014). *使用RNN编码器-解码器学习短语表示用于统计机器翻译*。arXiv预印本 arXiv:1406.1078。
- en: Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory*. Neural computation,
    9(8), 1735–1780.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., & Schmidhuber, J. (1997). *长短期记忆*。神经计算，9(8)，1735–1780。
- en: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation
    of Word Representations in Vector Space*. arXiv preprint arXiv:1301.3781.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *高效估计词向量表示方法*。arXiv预印本
    arXiv:1301.3781。
- en: '*The Unreasonable Effectiveness of Recurrent Neural* *Networks*: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*递归神经网络的非理性有效性*：[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。'
- en: Part 4 – Time Series with TensorFlow
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分 – 使用TensorFlow处理时间序列
- en: In this part, you will learn to build time series forecasting applications with
    TensorFlow. You will understand how to perform preprocess and build models for
    time series data. In this part, you will also learn to generate forecast for time
    series.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将学习如何使用TensorFlow构建时间序列预测应用程序。你将了解如何对时间序列数据进行预处理并构建模型。在这一部分，你还将学习如何生成时间序列的预测。
- en: 'This section comprises the following chapters:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction to Time Series,
    Sequences, and Predictions*'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B18118_12.xhtml#_idTextAnchor291)，*时间序列、序列和预测简介*'
- en: '[*Chapter 13*](B18118_13.xhtml#_idTextAnchor318), *Time Series, Sequence and
    Prediction with TensorFlow*'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B18118_13.xhtml#_idTextAnchor318)，*使用TensorFlow进行时间序列、序列和预测*'
