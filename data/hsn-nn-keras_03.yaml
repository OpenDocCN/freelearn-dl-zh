- en: A Deeper Dive into Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨神经网络
- en: In this chapter, we will encounter more in-depth details of neural networks.
    We will start from building a perceptron. Moving on, we will learn about activation
    functions. And we will also be training our first perceptron.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地了解神经网络。我们将从构建一个感知机开始。接下来，我们将学习激活函数。我们还将训练我们的第一个感知机。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: From the biological to the artificial neuron – the perceptron
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元——感知机
- en: Building a perceptron
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建感知机
- en: Learning through errors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过错误学习
- en: Training a perceptron
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练感知机
- en: Backpropagation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Scaling the perceptron
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展感知机
- en: A single layered network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单层网络
- en: From the biological to the artificial neuron – the perceptron
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元——感知机
- en: 'Now that we have briefly familiarized ourselves with some insights on the nature
    of data processing, it''s about time we see how the artificial cousins of our
    own biological neurons work themselves. We start with a creation of Frank Rosenblatt,
    dating back to the 1950s. He called this invention of his the **Perceptron** ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)).
    Essentially, you can think of the perceptron as a single neuron in an **artificial
    neural network** (**ANN**). Understanding how a single perceptron propagates information
    forward will serve as an excellent stepping stone to understanding the more state-of-the-art
    networks that we will face in later chapters:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经简要了解了一些关于数据处理本质的见解，是时候看看我们自己生物神经元的人工“亲戚”是如何工作的了。我们从弗兰克·罗森布拉特（Frank Rosenblatt）在1950年代的创作开始。他将这一发明称为**感知机**
    ([http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf))。从本质上讲，你可以将感知机看作是**人工神经网络**（**ANN**）中的一个单一神经元。理解一个单一感知机如何向前传播信息，将成为理解我们在后续章节中将遇到的更先进网络的一个绝佳垫脚石：
- en: '![](img/70ac79de-d77e-4e4d-8cc4-516a38d34757.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70ac79de-d77e-4e4d-8cc4-516a38d34757.png)'
- en: Building a perceptron
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建感知机
- en: For now, we will define a perceptron using six specific mathematical representations
    that demonstrate its learning mechanism. These representations are the inputs,
    weights, bias term, summation, and the activation function. The output will be
    functionally elaborate upon here under.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将使用六个特定的数学表示来定义一个感知机，这些表示展示了它的学习机制。它们分别是输入、权重、偏置项、求和以及激活函数。输出将在下面进一步展开说明。
- en: Input
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入
- en: Remember how a biological neuron takes in electrical impulses from its dendrites?
    Well, the perceptron behaves in a similar fashion, yet it prefers to ingest numbers
    in lieu of electricity. Essentially, it takes in feature inputs, as shown in the
    preceding diagram. This particular perceptron only has three input channels, these
    being *x[1]*, *x[2]*, and *x[3]*. These feature inputs (*x[1]*, *x[2]*, and *x[3]*)
    can be any independent variable that you choose to represent your observation
    by. Simply speaking, if we want to predict whether it will be sunny or rainy on
    any given day, we can record independent variables such as temperature and air
    pressure per day, along with the appropriate output class of that day (whether
    the day itself was sunny or rainy). We will then feed these independent variables
    that we have, one day at a time, as input features into our perceptron model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得生物神经元是如何从它的树突接收电信号的吗？那么，感知机的行为方式类似，但它更倾向于接收数字而不是电流。实际上，它接收特征输入，如前图所示。这个特定的感知机只有三个输入通道，分别是*x[1]*、*x[2]*和*x[3]*。这些特征输入（*x[1]*、*x[2]*和*x[3]*）可以是你选择的任何独立变量，用来表示你的观察结果。简单来说，如果我们想预测某天是否会晴天或下雨，我们可以记录每天的独立变量，如温度和气压，以及当天的输出类别（当天是晴天还是下雨）。然后，我们将这些独立变量，一天一天地输入到我们的感知机模型中。
- en: Weights
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重
- en: So, we know how data flows into our simple neuron, but how do we transform this
    data into actionable knowledge? How do we build a model that takes these input
    features, and represents them in a manner that helps us predict the weather on
    any given day?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们知道数据是如何流入我们简单的神经元的，但我们如何将这些数据转化为可操作的知识呢？我们如何构建一个模型，将这些输入特征表示出来，并帮助我们预测某天的天气呢？
- en: Giving us two features that we can use as input in our model, for the binary
    classification task of determining *rainy* or *sunny* days.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 给我们提供了两个特征，可以作为输入用于我们的模型，在二分类任务中判断*雨天*或*晴天*。
- en: Well, the first step will be to pair up each input feature with a respective
    weight. You can think of this weight as the relative importance this specific
    input feature has with respect to the output class that we are trying to predict.
    In other words, the weight for our input feature temperature should ideally reflect
    exactly how much this input feature is related to the output classes. These weights
    are randomly initialized at first, and are learned as our models see more and
    more data. Our hope here in doing this is that after enough iterations through
    our data, these weights will be nudged in the right direction, and learn the ideal
    configuration of temperature and pressure values that correspond to rainy and
    sunny days. We actually know, from domain knowledge, that temperature is highly
    correlated to weather, and hence will expect our model to, ideally, learn heavier
    weights for this feature, as data propagates through it. This can be somewhat
    comparable to the myelin sheath that covers axons in a biological neuron. If a
    specific neuron fires frequently, its myelin sheath is said to thicken, which
    insulates the axon, and allows the neuron to communicate faster next time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，第一步是将每个输入特征与其对应的权重配对。你可以把这个权重看作是该特定输入特征相对于我们试图预测的输出类别的相对重要性。换句话说，我们的输入特征“温度”的权重应该反映出这个输入特征与输出类别的相关程度。这些权重一开始是随机初始化的，随着我们的模型看到越来越多的数据，它们会被学习到。我们这么做的希望是，在足够多的迭代后，这些权重会被引导到正确的方向，并学习到与温度和气压值对应的理想配置，这些配置对应于雨天和晴天。事实上，从领域知识来看，我们知道温度与天气高度相关，因此我们期望模型理想情况下会为这个特征学习到更大的权重，随着数据的传播，这个特征的权重会逐渐增大。这在某种程度上可以与生物神经元中的髓鞘相比较。如果一个特定的神经元频繁地激活，它的髓鞘会变厚，从而使神经元的轴突得到绝缘，下次可以更快地传递信号。
- en: Summation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总和
- en: 'So, now we have our input features flowing into our perceptron, with each input
    feature paired up with a randomly initialized weight. The next step is fairly
    easy. First, we represent all three of our features and their weights as two different
    3 x 1 matrices. We want to use these two matrices to represent the combined effect
    of our input features and their weights. As you will recall from high school mathematics,
    you cannot actually multiply two 3 x 1 matrices together. So, we have to perform
    a little mathematical trick to reductively represent our two matrices as one value.
    We simply transpose our feature matrix, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们的输入特征已经流入了感知机，每个输入特征都与一个随机初始化的权重配对。下一步相对简单。首先，我们将所有三个特征及其权重表示为两个不同的 3
    x 1 矩阵。我们希望用这两个矩阵来表示输入特征及其权重的综合效应。正如你从高中数学中回忆到的那样，你实际上不能将两个 3 x 1 矩阵直接相乘。所以，我们需要执行一个小的数学技巧，将这两个矩阵简化为一个值。我们只需将特征矩阵转置，如下所示：
- en: '![](img/6fc54096-4dd9-4917-84e3-b125cae8fde0.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fc54096-4dd9-4917-84e3-b125cae8fde0.png)'
- en: We can use this new transposed feature matrix (of dimension 3 x 1) and multiply
    it with the weight matrix (of dimension 1 x 3). When we perform a matrix-by-matrix
    multiplication, the result we obtain is referred to as the **dot product** of
    these two matrices. In our case, we compute the dot product of our transposed
    feature matrix and our weights matrix. Doing so, we are able to reduce our two
    matrices to one single scalar value, which represents the collective influence
    of all of our input features and their respective weights. Now, we will see how
    we can use this collective representation and gauge it against a certain threshold
    to assess the quality of this representation. In other words, we will use a function
    to assess whether this scalar representation encodes a useful pattern to remember.
    A useful pattern will ideally be one that helps our model distinguish between
    the different classes in our data, and thereby output correct predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个新的转置特征矩阵（维度为 3 x 1），并将其与权重矩阵（维度为 1 x 3）相乘。当我们执行矩阵乘法时，得到的结果称为这两个矩阵的**点积**。在我们的例子中，我们计算的是转置特征矩阵与权重矩阵的点积。通过这样做，我们可以将这两个矩阵简化为一个单一的标量值，这个值代表了所有输入特征及其相应权重的综合影响。接下来，我们将看看如何使用这个综合表示，并与某个阈值进行对比，以评估该表示的质量。换句话说，我们将使用一个函数来评估这个标量表示是否编码了一个有用的模式。理想的有用模式应该是帮助我们的模型区分数据中的不同类别，从而输出正确预测的模式。
- en: Introducing non-linearity
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入非线性
- en: So now, we know how data enters a perceptron unit, and how associated weights
    are paired up with each input feature. We also know how to represent our input
    features, and their respective weights, as *n* x 1 matrices, where *n* is the
    number of input features. Lastly, we saw how we can transpose our feature matrix
    to be able to compute its dot product with the matrix containing its weights.
    This operation left us with one single scalar value. So, what's next? This is
    not a bad time to take a step back and ponder over what we are trying to achieve,
    as this will help us to understand the idea behind why we want to employ something
    like an activation function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在，我们知道了数据是如何进入感知机单元的，如何将相关的权重与每个输入特征配对。我们还知道如何将输入特征及其相应的权重表示为*n* x 1矩阵，其中*n*是输入特征的数量。最后，我们看到如何转置我们的特征矩阵，以便计算它与包含权重的矩阵的点积。这个操作最终得到一个单一的标量值。那么，接下来呢？现在是时候稍微停下来，思考一下我们到底在追求什么，这有助于我们理解为什么我们想要使用类似激活函数这样的概念。
- en: Well, you see, real-word data is often non-linear. What we mean by this is that
    whenever we attempt to model an observation as a function of different inputs,
    this function itself cannot be represented linearly, or on a straight line.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，正如你所看到的，现实世界中的数据通常是非线性的。我们所说的意思是，当我们尝试将某个观察值作为不同输入的函数来建模时，这个函数本身无法线性表示，或者说，不能用直线来表示。
- en: 'If all patterns in data only constituted straight lines, we would probably
    not be discussing neural networks at all. Techniques such as **Support Vector
    Machines** (**SVMs**) or even linear regression already excel at this task:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中的所有模式仅由直线构成，那么我们可能根本不会讨论神经网络。像**支持向量机**（**SVMs**）或甚至线性回归等技术已经非常擅长这个任务：
- en: '![](img/f2d549a7-a668-4df6-bc93-beabd654adb4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2d549a7-a668-4df6-bc93-beabd654adb4.png)'
- en: Modeling sunny and rainy days with temperature, for example, will produce a
    non-linear curve. In effect, this just means that we cannot possibly separate
    our decision boundary using a straight line. In other words, on some days, it
    may rain despite high temperatures, and on other days, it may remain sunny despite
    low temperatures.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用温度来建模晴天和雨天会产生一条非线性曲线。实际上，这意味着我们无法通过一条直线来划分我们的决策边界。换句话说，在某些日子里，尽管气温较高，可能会下雨；而在其他日子里，尽管气温较低，可能会保持晴朗。
- en: 'This is because temperature is not linearly related to the weather. The weather
    outcome on any given day is very likely to be a complex function, involving interactive
    variables such as wind speed, air pressure, and more. So, on any given day, a
    temperature of 13 degrees could mean a sunny day in Berlin, Germany, but a rainy
    day in London, UK:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为温度与天气之间的关系并不是线性的。任何给定日子的天气结果很可能是一个复杂的函数，涉及风速、气压等交互变量。因此，在任何给定的一天，13度的温度可能意味着德国柏林是晴天，但在英国伦敦却是雨天：
- en: '![](img/59bf7c16-f222-4ddf-b179-f6e2ef347415.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59bf7c16-f222-4ddf-b179-f6e2ef347415.png)'
- en: 'There are some cases, of course, where a phenomenon may be linearly represented.
    In physics, for example, the relationship between the mass of an object and its
    volume can be linearly defined, as shown in the following screenshots:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，某些情况下，现象可能可以用线性方式表示。例如，在物理学中，物体的质量与体积之间的关系可以线性定义，如以下截图所示：
- en: '![](img/56356469-bfba-4e93-a959-43701ceade75.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56356469-bfba-4e93-a959-43701ceade75.png)'
- en: 'This is an example of a non-linear function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非线性函数的例子：
- en: '![](img/13255c57-c769-455a-bd7a-69e9929d72b0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13255c57-c769-455a-bd7a-69e9929d72b0.png)'
- en: '| **A linear function** | **A non-linear function** |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **线性函数** | **非线性函数** |'
- en: '| *Y = mx + b* | *Y = mx² + b* |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| *Y = mx + b* | *Y = mx² + b* |'
- en: Here, *m *is the slope of the line, *x* is any point (an input or an *x*-value)
    on the line, and *b* is where the line crosses the *y* axis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*m*是直线的斜率，*x*是直线上的任何点（输入或*x*值），而*b*是直线与*y*轴的交点。
- en: Unfortunately, linearity is often not guaranteed with real-world data, as we
    model observations using multiple features, each of which could have a varied
    and disproportional contribution towards determining our output classes. In fact,
    our world is extremely non-linear, and hence, to capture this non-linearity in
    our perceptron model, we need it to incorporate non-linear functions that are
    capable of representing such phenomena. By doing so, we increase the capacity
    of our neuron to model more complex patterns that actually exist in the real world,
    and draw decision boundaries that would not be possible, were we to only use linear
    functions. These types of functions, used to model non-linear relationships in
    our data, are known as **activation functions**.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，现实世界中的数据通常不保证线性，因为我们用多个特征来建模观察数据，每个特征可能在确定输出类别时有不同且不成比例的贡献。事实上，我们的世界是高度非线性的，因此，为了捕捉感知机模型中的这种非线性，我们需要引入能够表示这种现象的非线性函数。通过这样做，我们增加了神经元建模实际存在的更复杂模式的能力，并且能够绘制出如果只使用线性函数则无法实现的决策边界。这些用于建模数据中非线性关系的函数，被称为**激活函数**。
- en: Activation functions
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Well, basically what we have done so far is represent our different input features
    and their weights in a lower dimension, as a scalar representation. We can use
    this reduced representation and pass it through a simple non-linear function that
    tells us whether our representation is above or below a certain threshold value.
    Similar to the weights we initialized before, this threshold value can be thought
    of as a learnable parameter of our perceptron model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，到目前为止，我们所做的就是将不同的输入特征及其权重表示为低维标量表示。我们可以使用这种简化的表示，并通过一个简单的非线性函数来判断我们的表示是否超过某个阈值。类似于我们之前初始化的权重，这个阈值可以被视为感知机模型的一个可学习参数。
- en: 'In other words, we want our perceptron to figure out the ideal combinations
    of weights and a threshold, allowing it to reliably match our inputs to the correct
    output class. Hence, we compare our reduced feature representation with a threshold
    value, and then activate our perceptron unit if we are above this threshold value,
    or do nothing otherwise. This very function that compares our reduced feature
    value against a threshold, is known as an **activation function**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们希望我们的感知机找出理想的权重组合和阈值，使其能够可靠地将输入匹配到正确的输出类别。因此，我们将简化后的特征表示与阈值进行比较，如果超过该阈值，我们就激活感知机单元，否则什么也不做。这个比较简化特征值和阈值的函数，就被称为**激活函数**：
- en: '![](img/e39b47b9-f526-41a4-9bb0-c9d9193c74e9.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e39b47b9-f526-41a4-9bb0-c9d9193c74e9.png)'
- en: These non-linear functions come in different forms, and will be explored in
    further detail in subsequent chapters. For now, we present two different activation
    functions; the **heavy set step** and the **logistic sigmoid** activation functions.
    The perceptron unit that we previously showed you was originally implemented with
    such a heavy step function, leading to binary outputs of 1 (active) or 0 (inactive).
    Using the step function in our perceptron unit, we observe that a value above
    the curve will lead to activation (1), whereas a value below or on the curve will
    not lead to the activation unit firing (0). This process may be summarized in
    an algebraic manner as well.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些非线性函数有不同的形式，将在后续章节中进行更详细的探讨。现在，我们展示两种不同的激活函数；**重阶跃**和**逻辑 sigmoid** 激活函数。我们之前展示的感知机单元最初是通过这样的重阶跃函数实现的，从而产生二进制输出
    1（激活）或 0（非激活）。使用感知机单元中的阶跃函数时，我们观察到，当值位于曲线之上时，会导致激活（1），而当值位于曲线下方或在曲线上时，则不会触发激活单元（0）。这个过程也可以用代数方式来总结。
- en: 'The following screenshot shows the heavy step function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了重阶跃函数：
- en: '![](img/960b3966-48d8-4dd9-8ce8-4c83dc1a62a3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/960b3966-48d8-4dd9-8ce8-4c83dc1a62a3.png)'
- en: 'The output threshold formula is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输出阈值公式如下：
- en: '![](img/33960858-cb2e-4c20-9ada-1370f805a9bb.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33960858-cb2e-4c20-9ada-1370f805a9bb.jpg)'
- en: 'In essence, a step function is not really a non-linear function, as it can
    be rewritten as two finite linear combinations. Hence, this piece-wise constant
    function is not very flexible in modeling real-world data, which is often more
    probabilistic than binary. The logistic sigmoid, on the other hand, is indeed
    a non-linear function, and may model data with more flexibility. This function
    is known for **squishing** its input to an output value between 0 and 1, which
    makes it a popular function for representing probabilities, and is a commonly
    employed activation function for neurons in modern neural networks:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，阶跃函数并不算真正的非线性函数，因为它可以被重写为两个有限的线性组合。因此，这种分段常数函数在建模现实世界数据时并不够灵活，因为现实数据通常比二元数据更具概率性。另一方面，逻辑
    sigmoid 函数确实是一个非线性函数，并且可以更灵活地建模数据。这个函数以**压缩**其输入到一个 0 到 1 之间的输出值而闻名，因此它成为表示概率的流行函数，也是现代神经网络中常用的激活函数：
- en: '![](img/d6b9bb4c-33a7-4d97-a7ce-4b4c89f9e714.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6b9bb4c-33a7-4d97-a7ce-4b4c89f9e714.png)'
- en: Each type of activation function comes with its own set of advantages and disadvantages
    that we will also delve into in later chapters. For now, you can intuitively think
    about the choice of different activation functions as a consideration based on
    your specific type of data. In other words, we ideally try to experiment and pick
    a function that best captures the underlining trends that may be present in your
    data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每种激活函数都有其一组优缺点，我们将在后续章节中进一步探讨。现在，你可以直观地将不同激活函数的选择看作是基于你数据的特定类型的考虑。换句话说，我们理想的做法是进行实验并选择一个最能捕捉数据中潜在趋势的函数。
- en: Hence, we will employ such activation functions to threshold the incoming inputs
    of a neuron. Inputs are consequentially transformed and gauged against this activation
    threshold, in turn causing a neuron to fire, or abstain therefrom. In the following
    illustration, we can visualize the decision boundary produced by an activation
    function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将采用这种激活函数来阈值化神经元的输入。输入会相应地进行转换，并与该激活阈值进行比较，从而导致神经元激活，或者保持不激活。在以下插图中，我们可以可视化由激活函数产生的决策边界。
- en: '![](img/b89a9e7b-9b18-4410-af79-11b3f253cb6e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b89a9e7b-9b18-4410-af79-11b3f253cb6e.png)'
- en: Understanding the role of the bias term
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解偏置项的作用
- en: So, now we have a good idea of how data enters our perceptron; it is paired
    up with weights and reduced through a dot product, only to be compared to an activation
    threshold. Many of you may ask at this point, *what if we wanted our threshold
    to adapt to different patterns in data?* In other words, what if the boundaries
    of the activation function were not ideal to separately identify the specific
    patterns we want our model to learn? We need to be able to play with the form
    of our activation curve, so as to guarantee some flexibility in the sort of patterns
    each neuron may locally capture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们大致了解了数据如何进入感知器；它与权重配对并通过点积缩减，然后与激活阈值进行比较。此时，你们很多人可能会问，*如果我们希望我们的阈值适应数据中的不同模式怎么办？*
    换句话说，如果激活函数的边界并不理想，无法单独识别我们希望模型学习的特定模式怎么办？我们需要能够调整激活曲线的形式，以确保每个神经元能够局部捕捉到一定的模式灵活性。
- en: 'And how exactly will we shape our activation function? Well, one way to do
    this is by introducing a **bias term** into our model. This is depicted by the
    arrow leaving the first input node (marked with the number ''1'') in the following
    diagram:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们究竟如何塑造我们的激活函数呢？一种方法是通过在模型中引入**偏置项**来实现。下图中，箭头从第一个输入节点（标记为数字“1”）离开，便是这一过程的示意：
- en: '![](img/3a0db2c1-49f6-4a50-ae58-30be9a9a2dbb.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a0db2c1-49f6-4a50-ae58-30be9a9a2dbb.png)'
- en: Representatively, we can think of this bias as the weight of a **fictional**
    input*.* This fictional input is said to be always present, allowing our activation
    unit to fire at will, without requiring any input features to be explicitly present
    (as shown in the green circle previously). The motivation behind this term is
    to be able to manipulate the shape of our activation function, which in turn impacts
    the learning of our model. We want our shape to flexibly fit different patterns
    in our data. The weight of the bias term is updated in the same manner as all
    the other weights are. What makes it different is that it is not disturbed by
    its input neuron, which simply always holds a constant value (as shown previously).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 具有代表性的是，我们可以将这个偏置项视为一个**虚拟**输入*。这个虚拟输入被认为始终存在，使得我们的激活单元可以随意触发，而无需任何输入特征明确存在（如前面绿色圆圈所示）。这个术语背后的动机是能够操控激活函数的形状，从而影响我们模型的学习。我们希望我们的形状能够灵活地适应数据中的不同模式。偏置项的权重与其他所有权重一样，以相同的方式进行更新。不同之处在于，它不受输入神经元的干扰，输入神经元始终保持一个常量值（如前所示）。
- en: 'So, how do we actually influence our activation threshold using this bias term?
    Well, lets consider a simplified example. Suppose we have some outputs generated
    by a stepped activation function, which produces either a ''0''or a ''1''for every
    output, like so:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何通过这个偏置项实际影响我们的激活阈值呢？让我们考虑一个简化的例子。假设我们有一些由阶跃激活函数生成的输出，它为每个输出产生‘0’或‘1’，如下所示：
- en: '![](img/d7c86ad9-1663-40a0-bdcd-866715c87e7d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7c86ad9-1663-40a0-bdcd-866715c87e7d.png)'
- en: 'We can then rewrite this formula to include the bias term, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将这个公式重写，包含偏置项，如下所示：
- en: '![](img/330ee91e-aa4e-411f-a4e7-add0eb9cb6f6.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/330ee91e-aa4e-411f-a4e7-add0eb9cb6f6.png)'
- en: In other words, we are using yet another mathematical trick and redefining the
    threshold value as the negative of our bias term (*Threshold = -(bias)*). This
    bias term is randomly initialized at the beginning of our training session, and
    is iteratively updated as the model sees more examples, and learns from these
    examples. Hence, it is important to understand that although we randomly initialize
    model parameters, such as the weights and biases, our hope is to actually show
    the model enough input examples and their corresponding output classes. In doing
    so, we want our model to learn from its errors, searching for the ideal parametric
    combinations of weights and bias corresponding to the correct output classes.
    Do note that when we initialize different weights, what we are actually doing
    is modifying the steepness of our activation function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们使用了另一个数学技巧，将阈值重新定义为偏置项的负值（*Threshold = -(bias)*）。这个偏置项在我们训练开始时是随机初始化的，并随着模型看到更多示例并从中学习而逐步更新。因此，重要的是要理解，尽管我们随机初始化模型参数，例如权重和偏置项，但我们的目标实际上是给模型足够的输入示例及其对应的输出类别。在此过程中，我们希望模型从错误中学习，寻找与正确输出类别对应的理想权重和偏置的参数组合。请注意，当我们初始化不同的权重时，我们实际上在做的是修改激活函数的陡峭度。
- en: 'The following graph shows how different weights impact the steepness of a sigmoid
    activation function:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了不同权重如何影响sigmoid激活函数的陡峭度：
- en: '![](img/e9682b24-892c-4c74-8889-5664bfba37d4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9682b24-892c-4c74-8889-5664bfba37d4.png)'
- en: We essentially hope that by tinkering with the steepness of our activation function,
    we are able to ideally capture a certain underlying pattern in our data. Similarly,
    when we initialize different bias terms, what we are actually trying to do is
    shift the activation function in an optimal manner (to the left or to the right),
    so as to trigger activation corresponding to specific configurations of input
    and output features.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们希望通过调整激活函数的陡峭度，能够理想地捕捉到数据中的某些潜在模式。类似地，当我们初始化不同的偏置项时，我们实际在做的是以最佳方式（向左或向右）平移激活函数，从而触发与特定输入输出特征配置对应的激活。
- en: 'The following graph shows how different bias terms impact the position of a
    sigmoid activation function:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了不同偏置项如何影响sigmoid激活函数的位置：
- en: '![](img/9ebaca6c-ca47-458c-8054-cc4482fc514a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ebaca6c-ca47-458c-8054-cc4482fc514a.png)'
- en: Output
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出
- en: 'In our simple perceptron model, we denote the actual output class as *y*, and
    the predicted output classes as ![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png).
    The output classes simply refer to the different classes in our data that we are
    trying to predict. To elaborate, we use the input features (*x[n]*), such as temperature
    (*x[1]*) and air pressure (*x[2]*) on a given day, to predict whether that specific
    day is a sunny or rainy one (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)).
    We can then compare our model''s predictions with the actual output class of that
    day, denoting whether that day was indeed rainy or sunny. We can denote this simple
    comparison as (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png) - *y*), which
    allows us to observe by how much our perceptron missed the mark, on average. But
    more on that later. For now, we can represent our entire prediction model using
    all that we have learned so far, in a mathematical manner:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的感知器模型中，我们将实际的输出类别表示为 *y*，将预测的输出类别表示为 ![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)。输出类别只是指我们在数据中尝试预测的不同类别。具体来说，我们使用输入特征（*x[n]*），例如某一天的温度（*x[1]*）和气压（*x[2]*），来预测那天是晴天还是雨天（![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)）。然后我们可以将模型的预测与当天的实际输出类别进行比较，判断那天是否确实是雨天或晴天。我们可以将这种简单的比较表示为（![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)
    - *y*），这样我们就能观察到感知器平均上偏差了多少。但稍后我们会更详细讨论这个问题。现在，我们可以以数学方式表示我们迄今为止学到的整个预测模型：
- en: '![](img/e4f8f3d6-b751-40c5-b4e1-4bd2a691bdab.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4f8f3d6-b751-40c5-b4e1-4bd2a691bdab.png)'
- en: 'The following diagram displays an example of the preceding formula:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了前述公式的一个例子：
- en: '![](img/dae36234-c075-4bd3-b38a-5522521562b2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dae36234-c075-4bd3-b38a-5522521562b2.png)'
- en: 'If we graphically plot our prediction line (![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png))
    shown precedingly, we will get to visualize the decision boundary separating our
    entire feature space into two subspaces. In essence, plotting a prediction line
    simply gives us an idea of what the model has learned, or how the model chooses
    to separate the hyperplane containing all our data points into the various output
    classes that interest us. Actually, by plotting out this line, we are able to
    visualize how well our model does by simply placing observations of sunny and
    rainy days on this feature space, and then checking whether our decision boundary
    ideally separates the output classes, as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将之前显示的预测线（![](img/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)）绘制出来，我们将能够可视化出决策边界，它将我们的整个特征空间分成两个子空间。实质上，绘制预测线仅仅是让我们了解模型学到了什么，或者模型如何选择将包含所有数据点的超平面划分为我们关心的不同输出类别。实际上，通过绘制这条线，我们能够可视化地看到模型的表现，只需将晴天和雨天的观察数据放置到这个特征空间中，然后检查我们的决策边界是否理想地将输出类别分开，具体如下：
- en: '![](img/79bf9bfc-0a83-49bc-9c33-867ffd4ff6bc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79bf9bfc-0a83-49bc-9c33-867ffd4ff6bc.png)'
- en: Learning through errors
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过错误学习
- en: All we essentially do to our input data is compute a dot product, add a bias
    term, pass it through a non-linear equation, and then compare our prediction to
    the real output value, taking a step in the direction of the actual output. This
    is the general architecture of an artificial neuron. You will soon see how this
    structure, configured repetitively, gives rise to some of the more complex neural
    networks around.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对输入数据所做的基本操作就是计算点积，添加偏置项，通过非线性方程进行处理，然后将预测与实际输出值进行比较，朝着实际输出的方向迈进一步。这就是人工神经元的基本结构。你很快就会看到，如何通过重复配置这种结构，产生一些更为复杂的神经网络。
- en: Exactly how we converge to ideal parametric values by taking a step in the right
    direction is through a method known as the **backward propagation of errors**,
    or **backpropagation** for short. But to propagate errors backwards, we need a
    metric to assess how well we are doing with respect to our goal. We define this
    metric as a loss, and calculate it using a loss function. This function attempts
    to incorporate the residual difference between what our model thinks it sees,
    and the actual ground reality. Mathematically speaking, this is shown as (*y*
    - ![](img/7a282e6b-02ad-450b-bfc8-37467f25c131.png)). It is important to understand
    here that the loss values can actually be defined as a function of our model parameters.
    Thus, tweaking these parameters permits us to reduce our loss and get our predictions
    closer to actual output values. You will see exactly what we mean by this when
    we review the full training process of our perceptron.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一种被称为**误差反向传播**（简称**反向传播**）的方法，准确地调整参数值，使其收敛到理想的值。为了实现误差反向传播，我们需要一种度量标准来评估我们在达成目标方面的进展。我们将这种度量标准定义为损失，并通过损失函数来计算。该函数试图将模型所认为的输出和实际结果之间的残差差异纳入考虑。从数学角度来看，这表现为(*y*
    - ![](img/7a282e6b-02ad-450b-bfc8-37467f25c131.png))。在这里，理解损失值实际上可以作为我们模型参数的函数非常重要。因此，通过调整这些参数，我们可以减少损失并使预测值更接近实际输出值。我们将在回顾感知机的完整训练过程时，详细理解这一点。
- en: The mean squared error loss function
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均方误差损失函数
- en: 'A prominently used loss function is the **mean squared error** (**MSE**) function,
    represented algebraically in the following formula. As you will notice, this function,
    at its core, simply compares the actual model output (*y*) with the predicted
    model output (![](img/1a12876d-d701-4c8b-9986-b147145150ab.png)). This function
    is particularly helpful for us to asses our predictive power, as this function
    models the loss quadratically. That is to say, if our model performs poorly, and
    our predicted and actual output values become more and more divergent, the loss
    increases by an exponent of two, allowing us to penalize higher errors more severely:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的损失函数是**均方误差**（**MSE**）函数，代数表示如下公式。正如你所注意到的，这个函数本质上只是将实际模型输出(*y*)与预测的模型输出（![](img/1a12876d-d701-4c8b-9986-b147145150ab.png)）进行比较。这个函数特别有助于我们评估预测能力，因为它以二次方式建模损失。也就是说，如果我们的模型表现不佳，且预测值与实际输出之间的差距越来越大，损失值将以平方的方式增加，从而更严厉地惩罚较大的误差。
- en: '![](img/97716134-e976-4bdc-a971-85dc25d9b587.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97716134-e976-4bdc-a971-85dc25d9b587.png)'
- en: The average MSE between output values *y[i]*, and predicted values ![](img/a8625ea3-68b5-432c-94d1-e0da34ae2337.png).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出值*y[i]*与预测值之间的平均均方误差（MSE）！[](img/a8625ea3-68b5-432c-94d1-e0da34ae2337.png)。
- en: We will revisit this notion to understand how we reduce the difference between
    what our model predicts versus the actual output using various types of loss functions.
    For now, it suffices to know that our model's loss may be minimized through a
    process known as **gradient descent**. As we will soon see, gradient descent is
    simply grounded in calculus and implemented through backpropagation-based algorithms.
    The process of mathematically reducing the difference between predicted and actual
    output, by tuning the parameters of a network, is actually what makes the network
    learn. This tuning occurs as we train our model, by showing it new examples of
    inputs and associated outputs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新审视这一概念，以了解如何通过不同类型的损失函数减少模型预测与实际输出之间的差异。现在，知道我们模型的损失可以通过**梯度下降**过程最小化就足够了。正如我们将很快看到的那样，梯度下降本质上是基于微积分的，并通过基于反向传播的算法实现。通过调整网络的参数，数学上减少预测值与实际输出之间的差异，实际上就是使网络能够学习的过程。在训练模型的过程中，我们会通过向模型展示新的输入和相关输出的例子来进行这一调整。
- en: Training a perceptron
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练感知机
- en: So far, we have a clear grasp of how data actually propagates through our perceptron.
    We also briefly saw how the errors of our model can be propagated backwards. We
    use a loss function to compute a loss value at each training iteration. This loss
    value tells us how far our model's predictions lie from the actual ground truth.
    But what then?
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经清楚地掌握了数据是如何在感知机中传播的。我们还简要地看到了模型的误差如何向后传播。我们使用损失函数在每次训练迭代中计算损失值。这个损失值告诉我们，模型的预测与实际真实值之间的差距有多大。那么接下来呢？
- en: Quantifying loss
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失量化
- en: Since the loss value gives us an indication of the difference between our predicted
    and actual outputs, it stands to reason that if our loss value is high, then there
    is a big difference between our model's predictions and the actual output. Conversely,
    a low loss value indicates that our model is closing the distance between the
    predicted and actual output. Ideally, we want our loss to converge to zero, which
    means that there is in effect not much difference between what our model thinks
    it sees, and what it is actually shown. We make our loss converge to zero by simply
    using another mathematical trick, grounded in calculus. How, you ask?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失值能够反映我们预测输出与实际输出之间的差异，因此可以推测，如果损失值较高，那么我们的模型预测与实际输出之间的差异也很大。相反，较低的损失值意味着我们的模型正在缩小预测值与实际输出之间的差距。理想情况下，我们希望损失值收敛到零，这意味着模型预测与实际输出之间几乎没有差异。我们通过另一个数学技巧使损失收敛到零，这个技巧基于微积分。那么，怎么做到呢？
- en: Loss as a function of model weights
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型权重对损失的影响
- en: Well, remember when we said that we can also think of our loss value as a *function*
    of the model parameters? Consider this. Our loss value tells us how far our model
    is from the actual prediction. This same loss can also be redefined as a function
    of our model's weight (θ). Recall that these weights are what actually lead to
    our model's prediction at each training iteration. Thinking about this intuitively,
    we want to be able to change our model weights with respect to the loss, so as
    to reduce our prediction errors as much as possible.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，记得我们说过可以将损失值看作是模型参数的*函数*吗？考虑这个问题。我们的损失值告诉我们模型与实际预测之间的距离。这同样的损失值也可以重新定义为模型权重（θ）的函数。回想一下，这些权重实际上是在每次训练迭代中导致模型预测的因素。从直观上讲，我们希望能够根据损失来调整模型权重，从而尽可能减少预测误差。
- en: 'More mathematically put, we want to minimize our loss function so as to iteratively
    update the weights for our model, and ideally converge to the best possible weights.
    These will be the best weights in the sense that they will best be able to represent
    features that are predictive of our output classes. This process is known as **loss
    optimization**, and can be mathematically illustrated as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更数学化地说，我们希望最小化损失函数，从而迭代地更新模型的权重，理想情况下收敛到最佳的权重。这些权重是最优的，因为它们能够最好地表示那些能预测输出类别的特征。这个过程被称为**损失优化**，可以通过以下数学方式表示：
- en: '![](img/7bd1ef52-965a-4e97-9f0d-89a1f67b0f66.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bd1ef52-965a-4e97-9f0d-89a1f67b0f66.png)'
- en: Gradient descent
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降
- en: 'Note that we represent our ideal model weights (θ^(***)) as the minima of our
    loss function over the entire training set. In other words, for all the feature
    inputs and labeled outputs we show our model, we want it to find a place in our
    feature space where the overall difference between the actual (*y*) and predicted
    (![](img/0577a5df-72ab-422b-9fc6-41d0b83225a7.png)) values are the smallest. The
    feature space we refer to is all the different possible combinations of weights
    that the model may initialize. For the sake of having a simplified representation
    of our loss function, we denote it as *J*(θ). We can now iteratively solve for
    the minimum of our loss function, *J*(θ), and descend the hyperplane to converge
    to a global minimum. This process is what we call **gradient descent**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将理想模型的权重（θ^(***)) 表示为在整个训练集上的损失函数的最小值。换句话说，对于我们向模型展示的所有特征输入和标签输出，我们希望它能在特征空间中找到一个地方，使得实际值
    (*y*) 和预测值 (![](img/0577a5df-72ab-422b-9fc6-41d0b83225a7.png)) 之间的总体差异最小。我们所指的特征空间是模型可能初始化的所有不同权重组合。为了简化表示，我们将损失函数表示为
    *J*(θ)。现在，我们可以通过迭代的方法求解损失函数 *J*(θ) 的最小值，并沿着超平面下降，收敛到全局最小值。这个过程就是我们所说的**梯度下降**：
- en: '![](img/34fc1e60-e821-4528-9982-7f694c6bfdae.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34fc1e60-e821-4528-9982-7f694c6bfdae.png)'
- en: Backpropagation
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'For the more mathematically oriented, you must be wondering how exactly we
    descend our gradient iteratively. Well, as you know, we start by initializing
    random weights to our model, feed in some data, compute dot products, and pass
    it through our activation function along with our bias to get a predicted output.
    We use this predicted output and the actual output to estimate the errors in our
    model''s representations, using the loss function. Now here comes the calculus.
    What we can do now is differentiate our loss function, *J*(θ), with respect to
    the weights of our model (θ). This process essentially lets us compare how changes
    in our model''s weights affect the changes in our model''s loss. The result of
    this differentiation gives us the gradient of our J(θ) function at the current
    model weight (θ) along with the direction of the highest ascent. By highest ascent,
    we meant the direction in which the difference between prediction and output values
    seem higher. Hence, we simply take a step in the opposite direction, and descend
    the gradient of our loss function, *J*(θ), with respect to our model weights (θ).
    We present an algorithmic representation of this concept, in form of pseudo-code,
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些更加注重数学的读者，你一定在想我们是如何迭代地下降梯度的。好吧，正如你所知道的，我们从初始化模型的随机权重开始，输入一些数据，计算点积，然后将其通过激活函数和偏置传递，得到预测输出。我们使用这个预测输出和实际输出来估计模型表示中的误差，使用损失函数。现在进入微积分部分。我们现在可以做的是对我们的损失函数
    *J*(θ) 对模型的权重（θ）进行求导。这个过程基本上让我们比较模型权重的变化如何影响模型损失的变化。这个微分的结果给出了当前模型权重（θ）下 *J*(θ)
    函数的梯度以及最大上升的方向。所谓的最大上升方向，指的是预测值和输出值之间的差异似乎更大的方向。因此，我们只需朝相反的方向迈出一步，下降我们损失函数 *J*(θ)
    相对于模型权重（θ）的梯度。我们用伪代码以算法的形式呈现这一概念，如下所示：
- en: '![](img/fcde7a14-32f2-406f-b062-ac487bd7a92b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcde7a14-32f2-406f-b062-ac487bd7a92b.png)'
- en: 'The following graph is a visualization of the gradient descent algorithm:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是梯度下降算法的可视化：
- en: '![](img/a23c2d4c-88b7-44d0-85a8-7513668a9f6f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a23c2d4c-88b7-44d0-85a8-7513668a9f6f.png)'
- en: 'As we see, the gradient descent algorithm allows us to take steps down the
    loss hyperplane, until our model converges to some optimal parameters. At this
    point, the difference between our model''s predictions and reality will be quite
    negligible, and we can consider our model trained!:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，梯度下降算法允许我们沿着损失超平面向下走，直到我们的模型收敛到某些最优参数。在这一点上，模型预测值和实际值之间的差异将非常微小，我们可以认为模型已经训练完成！
- en: '![](img/8975d35f-d47b-4475-99f6-4efeaf0f526d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8975d35f-d47b-4475-99f6-4efeaf0f526d.png)'
- en: Thus, we compute the changes in our network weights with respect to the changes
    of the values generated by the loss function (i.e. the gradients of the network
    weights). Then, we proportionally update the network weights, in the opposite
    direction of the computed gradients, so as to adjust for the errors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们计算网络权重的变化，以对应损失函数生成的值的变化（即网络权重的梯度）。然后，我们根据计算出的梯度的相反方向，按比例更新网络权重，从而调整误差。
- en: Computing the gradient
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算梯度
- en: Now that we are familiar with the backpropagation algorithm as well as the notion
    of gradient decscent, we can address more technical questions. Questions like, *how
    do we actually compute this gradient?* As you know, our model does not have the
    liberty of visualizing the loss landscape, and picking out a nice path of descent.
    In fact, our model cannot tell what is up, or what is down. All it knows, and
    will ever know, is numbers. However, as it turns out, numbers can tell a lot!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了反向传播算法和梯度下降的概念，我们可以解决一些更技术性的问题。比如，*我们到底是如何计算这个梯度的？* 正如你所知道的，我们的模型并没有直观地了解损失的地形，也无法挑选出一条合适的下降路径。事实上，我们的模型并不知道什么是上，什么是下。它所知道的，且永远只会知道的，就是数字。然而，事实证明，数字能告诉我们很多东西！
- en: 'Let''s reconsider our simple perceptron model to see how we can backpropagate
    its errors by computing the gradient of our loss function, *J*(θ), iteratively:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新考虑一下我们简单的感知器模型，看看如何通过迭代地计算损失函数 *J*(θ) 的梯度来反向传播其误差：
- en: '![](img/d983710e-6717-49bc-83c6-45c5464dc97d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d983710e-6717-49bc-83c6-45c5464dc97d.png)'
- en: 'What if we wanted to see how changes in the weights of the second layer impact
    the changes in our loss? Obeying the rules of calculus, we can simply differentiate
    our loss function, *J*(θ), with respect to the weights of the second layer (θ[*2*]).
    Mathematically, we can actually represent this in a different manner as well.
    Using the chain rule, we can show how changes in our loss with respect to the
    second layer weights are actually a product of two different gradients themselves.
    One represents the changes in our losses with respect to the model''s prediction,
    and the other shows the changes in our model''s prediction with respect to the
    weights in the second layer. This may be represented as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要看到第二层权重的变化如何影响损失的变化怎么办？遵循微积分规则，我们可以简单地对损失函数 *J*(θ) 进行求导，得到损失函数相对于第二层权重（θ[*2*]）的变化。在数学上，我们也可以用不同的方式表示这一点。通过链式法则，我们可以表示损失相对于第二层权重的变化其实是两个不同梯度的乘积。一个梯度表示损失相对于模型预测的变化，另一个表示模型预测相对于第二层权重的变化。这个可以表示为：
- en: '![](img/93d30d82-fee8-4614-b9ea-962b1b3ed8aa.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93d30d82-fee8-4614-b9ea-962b1b3ed8aa.png)'
- en: 'As if this wasn''t complicated enough, we can even take this recursion further.
    Let''s say that instead of modeling the impact of the changing weights of the
    second layer (θ*[2]*), we wanted to propagate all the way back and see how our
    loss changes with respect to the weights of our first layer. We then simply redefine
    this equation using the chain rule, as we did earlier. Again, we are interested
    in the change in our model''s loss with respect to the model weights for our first
    layer, (θ[*1*]). We define this using the product of three different gradients;
    the changes in our loss with respect to output, changes in our output with respect
    to our hidden layer value, and finally, the changes in our hidden layer value
    with respect to our first layer weights. We can summarize this as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 仿佛这还不够复杂，我们甚至可以进一步推进这种递归。假设我们想要研究第二层权重（θ*[2]*）变化的影响，而不是仅仅建模其影响，我们想要回溯到第一层权重，看看损失函数如何随第一层权重的变化而变化。我们只需像之前那样使用链式法则重新定义这个方程。再次强调，我们关心的是模型损失相对于第一层权重（θ[*1*]）的变化。我们将这个变化定义为三个不同梯度的乘积：损失相对于输出的变化、输出相对于隐藏层值的变化，最后是隐藏层值相对于第一层权重的变化。我们可以总结为如下：
- en: '![](img/58df4217-ce8f-438f-97bb-50e3dcb16802.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58df4217-ce8f-438f-97bb-50e3dcb16802.png)'
- en: And so, this is how we use the loss function, and backpropagate the errors by
    computing the gradient of our loss function with respect to every single weight
    in our model. Doing so, we are able to adjust the course of our model in the right
    direction, being the direction of the highest descent, as we saw before. We do
    this for our entire dataset, denoted as an epoch. And what about the size of our
    step? Well, that is determined by the learning rate we set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我们如何使用损失函数，通过计算损失函数相对于模型中每个权重的梯度来进行误差反向传播。通过这样做，我们能够将模型的调整方向引导到正确的地方，也就是之前提到的最大下降方向。我们对整个数据集进行这一操作，这一过程被称为一个迭代（epoch）。那我们的步长呢？这个步长是由我们设置的学习率决定的。
- en: The learning rate
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率
- en: 'While somewhat intuitive, the learning rate of a model simply determines how
    fast it can learn. Mathematically put, the learning rate determines the exact
    size of the step we take at each iteration, as we descend the loss landscape to
    converge to ideal weights. Setting the right learning rate for your problem can
    be challanging, specially when the loss landscape is complex and full of surprises,
    as can be seen in the illustration here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率虽然看起来直观，但它决定了模型学习的速度。用数学的语言来说，学习率决定了我们在每次迭代时采取的步长大小，随着我们沿着损失函数的地形下降，逐步逼近理想的权重。为你的问题设置正确的学习率可能是一个挑战，特别是当损失函数的地形复杂且充满了意外时，正如这里的插图所示：
- en: '![](img/334bbf3f-b157-4b18-b044-7d05e9a7d981.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/334bbf3f-b157-4b18-b044-7d05e9a7d981.png)'
- en: This is quite an important notion. If we set a learning rate too small, then
    naturally, our model learns less than it potentially could per any given training
    iteration. Even worse with low learning rates is when our model gets stuck in
    a local minimum, thinking that it has reached a global minimum. Conversely, a
    high learning rate could, on the other hand, deter our model from capturing patterns
    of predictive value.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当重要的概念。如果我们设置的学习率太小，那么自然，在每次训练迭代中，我们的模型学习的内容会比它实际能够学习的少。更糟糕的是，低学习率可能会导致我们的模型陷入局部最小值，误以为它已经达到了全局最小值。相反，如果学习率过高，可能会使模型无法捕捉到有预测价值的模式。
- en: If our steps are too large, we may simply keep overshooting over any global
    minima present in our feature space of weights, and hence, never converge on our
    ideal model weights.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的步伐太大，我们可能会不断越过我们特征空间中的任何全局最小值，因此，永远无法收敛到理想的模型权重。
- en: 'One solution to this problem is to set an adaptive learning rate, responsive
    to the specific loss landscape it may encounter during training. We will explore
    various implementations of adaptive learning rates (such as Momentum, Adadelta,
    Adagrad, RMSProp, and more) in subsequent chapters:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是设置一个自适应学习率，能够根据训练过程中遇到的特定损失景观进行响应。在后续章节中，我们将探索各种自适应学习率的实现（如动量法、Adadelta、Adagrad、RMSProp等）：
- en: '![](img/b3074f96-b948-4089-903d-e4d549c8b997.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3074f96-b948-4089-903d-e4d549c8b997.png)'
- en: Scaling the perceptron
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展感知机
- en: 'So, we have seen so far how a single neuron may learn to represent a pattern,
    as it is trained. Now, let''s say we want to leverage the learning mechanism of
    an additional neuron, in parallel. With two perceptron units in our model, each
    unit may learn to represent a different pattern in our data. Hence, if we wanted
    to scale the previous perceptron just a little bit by adding another neuron, we
    may get a structure with two fully connected layers of neurons, as shown in the
    following diagram:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到一个神经元如何通过训练学习表示一个模式。现在，假设我们想要并行地利用另一个神经元的学习机制。在我们的模型中，两个感知机单元每个可能会学习表示数据中的不同模式。因此，如果我们想通过添加另一个神经元来稍微扩展前面的感知机，我们可能会得到一个具有两层全连接神经元的结构，如下图所示：
- en: '![](img/4a4de24d-5e18-41a8-80b6-f87143eb4f6a.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a4de24d-5e18-41a8-80b6-f87143eb4f6a.png)'
- en: Note here that the feature weights, as well as the additional fictional input
    we will have per neuron to represent our bias, have both disappeared. To simplify
    our representation, we have instead denoted both the scalar dot product, and our
    bias term, as a single symbol.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里，特征权重以及每个神经元用来表示偏置的额外虚拟输入都已经消失。为了简化表示，我们将标量点积和偏置项合并为一个符号。
- en: We choose to represent this mathematical function as the letter *z*. The value
    of *z* is then fed to the activation function, just as we previously did, thus
    *y* = *g*(*z*). As you can see in the preceding diagram, our input features connect
    to two different neurons, each of which may adjust its weights and biases to learn
    a specific and distinct representation from the data it is fed. These representations
    are then used to predict our output classes, and updated as we train our model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择用字母*z*来表示这个数学函数。然后，*z*的值被输入到激活函数中，正如我们之前所做的那样，*y* = *g*(*z*)。如前面的图所示，我们的输入特征连接到两个不同的神经元，每个神经元可以调整其权重和偏置，从而学习从数据中提取特定且独特的表示。这些表示随后用于预测我们的输出类别，并在我们训练模型时进行更新。
- en: A single layered network
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单层网络
- en: 'Right, now we have seen how to leverage two versions of our perception unit,
    in parallel, enabling each individual unit to learn a different underlying pattern
    that is possibly present in the data we feed it. We naturally want to connect
    these neurons to output neurons, which fire to indicate the presence of a specific
    output class. In our sunny-rainy day classification example, we have two output
    classes (sunny or rainy), hence a predictive network tasked to solve this problem
    will have two output neurons. These neurons will be supported by the learning
    of neurons from the previous layer, and ideally will represent features that are
    informative for predicting either a rainy or a sunny day. Mathematically speaking,
    all that is simply happening here is the forward propagation of our transformed
    input features, followed by the backward propagation of the errors in our prediction.
    One way of thinking about this is to visualize each node in the following diagram
    as the holder of a specific number. Similarly, each arrow can be seen as picking
    up a number from a node, performing a weighted computation on it, and carrying
    it forward to the next layer of nodes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经看到了如何并行利用我们感知单元的两种版本，使每个单元能够学习我们喂入数据中可能存在的不同潜在模式。我们自然希望将这些神经元连接到输出神经元，这些输出神经元会触发，表示特定输出类别的存在。在我们的晴雨天分类示例中，我们有两个输出类别（晴天或雨天），因此，负责解决此问题的预测网络将有两个输出神经元。这些神经元将得到来自前一层神经元的学习支持，理想情况下，它们将代表对于预测晴天或雨天的有用特征。从数学角度来说，实际上发生的只是我们转化后的输入特征的前向传播，随后是我们预测中的误差的反向传播。我们可以将每个节点视为持有一个特定数字，类似地，每个箭头可以看作是从一个节点中提取一个数字，执行加权计算，然后将其传递到下一个节点层。
- en: '![](img/147f084b-24fe-4d7a-a103-fffb516e8af7.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/147f084b-24fe-4d7a-a103-fffb516e8af7.png)'
- en: Now, we have a neural network with one hidden layer. We call this a hidden layer
    as the state of this layer is not directly enforced, as opposed to the input and
    output layers. Their representations are not hardcoded by the designer of the
    network. Rather, they are inferred by the network, as data propagates through
    it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个带有一个隐藏层的神经网络。我们称其为隐藏层，因为该层的状态并不是直接强加的，区别于输入层和输出层。它们的表示不是由网络设计者硬编码的，而是通过数据在网络中传播时推断出来的。
- en: As we saw, the input layer holds our input values. The set of arrows connecting
    the input layer to the hidden layer simply compute the bias adjusted dot product
    (*z*), of our input features (*x*) and their respective weights (θ[*1*]). The
    (*z*) values then reside in the hidden layer neurons, until we apply our non-linear
    function, *g*(*x*), to these values. After this, the arrows leading away from
    the hidden layer compute the dot product of *g*(*z*) and the weights corresponding
    to *the* hidden layer, (θ*[2]*), before carrying the result forward to the two
    output neurons, ![](img/25c7dba2-7da4-41b4-af22-9d3bdbd503a2.png) and ![](img/1af86b7f-9ad1-45e2-8dbc-8ab4b4dcb6a7.png).
    Notice that with each layer comes a respective weights matrix, which is iteratively
    updated by differentiating our loss function with respect to the weight matrices
    from the previous training iteration. Hence, we train a neural network by descending
    the gradient of our loss function relative to our model weights, converging to
    a global minimum.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，输入层保存了我们的输入值。连接输入层和隐藏层的一组箭头只是计算了输入特征（*x*）和它们各自权重（θ[*1*]）的偏置调整点积（*z*）。然后，(*z*)
    值会存储在隐藏层神经元中，直到我们对这些值应用我们的非线性函数， *g*(*x*)。之后，离开隐藏层的箭头会计算 *g*(*z*) 和与隐藏层对应的权重（θ[*2*]）的点积，然后将结果传递到两个输出神经元，![](img/25c7dba2-7da4-41b4-af22-9d3bdbd503a2.png)
    和 ![](img/1af86b7f-9ad1-45e2-8dbc-8ab4b4dcb6a7.png)。请注意，每一层都有相应的权重矩阵，这些矩阵通过对损失函数相对于上一个训练迭代中的权重矩阵求导，进行迭代更新。因此，我们通过对损失函数相对于模型权重的梯度下降训练神经网络，最终收敛到全局最小值。
- en: Experimenting with TensorFlow playground
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow playground 中进行实验
- en: 'Let''s see how different neurons can actually capture different patterns in
    our data using a dummy example. Suppose that we have two output classes in our
    data, as plotted in the following diagram. The task of our neural network is to
    learn the decision boundaries separating our two output classes. Plotting this
    two-dimensional dataset, we get something similar to the following diagram, where
    we see several decision boundaries that classify the different possible outputs:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个虚拟的例子来看一下不同的神经元如何捕捉到我们数据中的不同模式。假设我们在数据中有两个输出类别，如下图所示。我们神经网络的任务是学习将这两个输出类别分开的决策边界。绘制这个二维数据集，我们得到一个类似以下图表的图像，在其中我们看到几个决策边界，将不同的可能输出分类：
- en: '![](img/d0c7325b-b6d8-4f85-9fa0-0f4d098ab377.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c7325b-b6d8-4f85-9fa0-0f4d098ab377.png)'
- en: 'We will employ a phenomenal open-source tool to visualize our model''s learnings,
    known as TensorFlow playground. This tool simply allows to simulate a neural network
    with some synthetic data, and lets us actually *see* what patterns our neurons
    are picking up on*.* It lets you tinker with all the concepts we have overviewed
    so far, including different types and forms of input features, activation functions,
    learning rate, and many more. We highly encourage you to experiment with the different
    synthetic datasets they provide, play with the input features and progressively
    add neurons, as well as hidden layers to see how this affects learning. Do also
    experiment with different activation functions to see how your model can capture
    various complex patterns from data. Seeing, is indeed believing!  (Or more scientifically
    put, nullius in verba).  As we can see in our diagram below, both neurons in the
    hidden layer are actually capturing different curvatures in our feature space,
    learning a specific pattern in the data. You can visualize the weights of our
    model by observing the thickness of the lines connecting the layers. You can also
    visualize the output of each neuron (the shaded blue and white areas shown within
    the neurons) to see what underlining pattern that specific neuron is capturing
    in our data. This representation, as you will see by experimenting on the playground,
    is iteratively updated and converges to an ideal value, given the form and type
    of the data, activation functions, and learning rates used:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个出色的开源工具来可视化我们模型的学习过程，这个工具叫做 TensorFlow Playground。这个工具简单地模拟了一个神经网络，并使用一些合成数据，让我们实际*看到*我们的神经元正在捕捉哪些模式。它允许你调整我们迄今为止概述的所有概念，包括不同类型和形式的输入特征、激活函数、学习率等。我们强烈建议你尝试不同的合成数据集，玩弄输入特征，并逐步添加神经元以及隐藏层，以观察这些变化如何影响学习。也可以尝试不同的激活函数，看看你的模型如何从数据中捕捉各种复杂的模式。的确，眼见为实！(或者更科学地说，nullius
    in verba)。如我们在下面的图表中所见，隐藏层中的两个神经元实际上在捕捉特征空间中的不同曲率，从而学习到数据中的特定模式。你可以通过观察连接各层的线条粗细来可视化我们模型的权重。你还可以通过观察每个神经元的输出（显示在神经元内部的阴影蓝白区域）来查看该神经元在数据中捕捉到的底层模式。正如你在实验
    Playground 时看到的那样，这一表示方式是逐步更新并收敛到理想值的，具体取决于数据的形式和类型、使用的激活函数和学习率：
- en: '![](img/5da2a3e3-fe20-4d99-9b70-366fc12aeb3c.png) ![](img/6256fd9d-ef64-4cc7-9e5c-f83691beb5af.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5da2a3e3-fe20-4d99-9b70-366fc12aeb3c.png) ![](img/6256fd9d-ef64-4cc7-9e5c-f83691beb5af.png)'
- en: A model with one hidden layer, two neurons, and the sigmoid activation function,
    trained for 1,000 epochs
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有一个隐藏层、两个神经元和 sigmoid 激活函数的模型，经过 1,000 轮训练
- en: Capturing patterns heirarchically
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化地捕捉模式
- en: We previously saw how a specific model configuration with two neurons, each
    equipped with a sigmoid activation function, manages to capture two different
    curvatures in our feature space, which is then combined to plot our decision boundary,
    represented by the aforementioned output. However, this is just one possible configuration,
    leading to one possible decision boundary.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到过，具有两个神经元的特定模型配置，每个神经元都配备了 sigmoid 激活函数，能够捕捉到我们特征空间中的两种不同曲率，然后将其结合起来绘制出我们的决策边界，以上图所示的输出为代表。然而，这只是其中一种可能的配置，导致了一个可能的决策边界。
- en: 'The following diagram shows a model with two hidden layers with the sigmoid
    activation function, trained for 1,000 epochs:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个具有两个隐藏层并使用 sigmoid 激活函数的模型，经过 1,000 轮训练：
- en: '![](img/b6b339b1-a220-4cf6-8ff9-3be17075ea68.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6b339b1-a220-4cf6-8ff9-3be17075ea68.png)'
- en: 'The following diagram shows a model with one hidden layer, composed of two
    neurons, with a rectified linear unit activation function, trained for 1,000 epochs,
    on the same dataset:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个包含一个隐藏层的模型，该隐藏层由两个神经元组成，使用了整流线性单元激活函数，在相同的数据集上训练了1,000个周期：
- en: '![](img/0c033539-6573-48a8-b4c7-3dd577cfb9cd.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c033539-6573-48a8-b4c7-3dd577cfb9cd.png)'
- en: 'The following diagram shows a model with one hidden layer, composed of three
    neurons, with a rectified linear unit activation function, again on the same dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个包含一个隐藏层的模型，该隐藏层由三个神经元组成，使用了整流线性单元激活函数，仍然是在相同的数据集上：
- en: '![](img/651adca8-95d9-4f8c-bbd6-f7cb212cbb23.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/651adca8-95d9-4f8c-bbd6-f7cb212cbb23.png)'
- en: Note that by using different activation functions, and manipulating the number
    of hidden layers and their neurons, we can achieve very different decision boundaries.
    It is up to us to asses which of them is ideally predictive, and is suitable for
    our use case. Mostly, this is done through experimentation, although domain knowledge
    about the data you are modelling may go a long way.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过使用不同的激活函数，并操作隐藏层的数量和其神经元的数量，我们可以实现非常不同的决策边界。我们需要评估哪种配置最能预测，并且适合我们的使用案例。通常，这通过实验来完成，尽管对所建模数据的领域知识也可能起到很大的作用。
- en: Steps ahead
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前进的步骤
- en: Congratulations! In just a few pages, we have already come a long way. Now you
    know how neural networks learn, and have an idea of the higher-level mathematical
    constructs that permit it to learn from data. We saw how a single neuron, namely
    the perceptron, is configured. We saw how this neural unit transforms its input
    features as data propagates forward through it. We also understood the notion
    of representing non-linearity through activation functions, and how multiple neurons
    may be organized in a layer, allowing each individual neuron in the layer to represent
    different patterns from our data. These learned patterns are updated at each training
    iteration, for each neuron. We know now that this is done by computing the loss
    between our predictions and actual output values, and adjusting the weights of
    each neuron in the model, until we find an ideal configuration.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！在短短几页的内容中，我们已经走了很长一段路。现在你知道神经网络是如何学习的，并且对允许它从数据中学习的高级数学结构有了了解。我们看到单个神经元（即感知器）是如何配置的。我们看到这个神经单元是如何在数据向前传播过程中转化其输入特征的。我们还理解了通过激活函数表示非线性概念，以及如何在一个层中组织多个神经元，从而使该层中的每个神经元能够表示我们数据中的不同模式。这些学习到的模式在每次训练迭代时都会被更新，每个神经元的权重都会调整，直到我们找到理想的配置。
- en: In fact, modern neural networks employ various types of neurons, configured
    in diverse ways, for different predictive tasks. While the underlining learning
    architecture of neural networks always remains the same, the specific configuration
    of neurons, in terms of their number, inter-connectivity, activation functions
    used, etc. are elements which define the different types of neural network architectures
    you may come across. For the time being, we leave you with a comprehensive illustration
    generously provided by the Asimov institute.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，现代神经网络采用了各种类型的神经元，以不同的方式配置，用于不同的预测任务。虽然神经网络的基本学习架构始终保持不变，但神经元的具体配置，例如它们的数量、互联性、所使用的激活函数等，都是定义不同类型神经网络架构的因素。为了便于理解，我们为您提供了由阿西莫夫研究所慷慨提供的全面插图。
- en: 'In the following diagram, you can see some prominent types of neurons, or *cells*,
    along with their configurations that form some of the most commonly used state
    of the art neural networks, which you will also throughout the course of this
    book:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，您可以看到一些突出的神经元类型，或*细胞*，以及它们的配置，这些配置构成了您将在本书中看到的一些最常用的最先进的神经网络：
- en: '![](img/64f53973-778e-40c8-b812-05381d4e291a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64f53973-778e-40c8-b812-05381d4e291a.png)'
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Now that we have achieved a comprehensive understanding of neural learning systems,
    we can start getting our hands dirty. We will soon implement our first neural
    network, test it out for a classic classification task, and practically face many
    of the concepts we have covered here. In doing so, we will cover a detailed overview
    of the exact nature of loss optimization, and the evaluation metrics of neural
    networks.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对神经学习系统有了全面的理解，我们可以开始动手实践。我们将很快实现我们的第一个神经网络，测试它在经典分类任务中的表现，并在实践中面对我们在这里讨论的许多概念。在此过程中，我们将详细介绍损失优化的确切性质以及神经网络的评估指标。
