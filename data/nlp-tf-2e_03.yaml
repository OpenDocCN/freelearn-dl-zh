- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Word2vec – Learning Word Embeddings
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec – 学习词嵌入
- en: In this chapter, we will discuss a topic of paramount importance in NLP—Word2vec,
    a data-driven technique for learning powerful numerical representations (that
    is, vectors) of words or tokens in a language. Languages are complex. This warrants
    sound language understanding capabilities in the models we build to solve NLP
    problems. When transforming words to a numerical representation, a lot of methods
    aren’t able to sufficiently capture the semantics and contextual information that
    word carries. For example, the feature representation of the word *forest* should
    be very different from *oven* as these words are rarely used in similar contexts,
    whereas the representations of *forest* and *jungle* should be very similar. Not
    being able to capture this information leads to underperforming models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论NLP中一个至关重要的话题——Word2vec，一种数据驱动的技术，用于学习语言中词或符号的强大数值表示（即向量）。语言是复杂的，这要求我们在构建解决NLP问题的模型时具备良好的语言理解能力。将词转换为数值表示时，许多方法无法充分捕捉词所携带的语义和上下文信息。例如，词*forest*的特征表示应与*oven*的表示有很大不同，因为这两个词很少在类似的语境中使用，而*forest*和*jungle*的表示应该非常相似。无法捕捉到这些信息会导致模型性能不佳。
- en: Word2vec tries to overcome this problem by learning word representations by
    consuming large amounts of text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec试图通过大量文本学习词表示来克服这个问题。
- en: Word2vec is called a *distributed representation*, as the semantics of the word
    are captured by the activation pattern of the full representation vector, in contrast
    to a single element of the representation vector (for example, setting a single
    element in the vector to 1 and rest to 0 for a single word).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec被称为*分布式表示*，因为词的语义通过完整表示向量的激活模式来捕获，这与表示向量中的单一元素（例如，将向量中的单一元素设置为1，其余为0以表示单个词）不同。
- en: In this chapter, we will learn the mechanics of several Word2vec algorithms.
    But first, we will discuss the classical approaches to solving this problem and
    their limitations. This then motivates us to look at learning neural-network-based
    Word2vec algorithms that deliver state-of-the-art performance when finding good
    word representations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习几个Word2vec算法的工作原理。但首先，我们将讨论解决此问题的经典方法及其局限性。然后，这促使我们研究基于神经网络的Word2vec算法，这些算法在找到良好的词表示时能够提供最先进的性能。
- en: 'We will train a model on a dataset and analyze the representations learned
    by the model. We visualize (using t-SNE, a visualization technique for high-dimensional
    data) these learned word embeddings for a set of words on a 2D canvas in *Figure
    3.1*. If you take a closer look, you will see that similar things are placed close
    to each other (for example, numbers in the cluster in the middle):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个数据集上训练一个模型，并分析模型学习到的表示。我们使用t-SNE（一种用于高维数据可视化的技术）将这些学习到的词嵌入可视化，在*图3.1*的二维画布上展示。如果仔细观察，你会发现相似的事物被放置得很近（例如，中间聚集的数字）：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png](img/B14070_03_01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png](img/B14070_03_01.png)'
- en: 'Figure 3\. 1: An example visualization of learned word embeddings using t-SNE'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：使用t-SNE可视化学习到的词嵌入示例
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-分布式随机邻居嵌入**（**t-SNE**）'
- en: This is a dimensionality reduction technique that projects high-dimensional
    data to a two-dimensional space. This allows us to imagine how high-dimensional
    data is distributed in space, because humans are generally not so good at intuitively
    understanding data in more than three dimensions. You will learn about t-SNE in
    more detail in the next chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种降维技术，将高维数据投影到二维空间。这使我们能够想象高维数据在空间中的分布，因为人类通常不太擅长直观理解超过三维的数据。你将在下一章详细了解t-SNE。
- en: 'This chapter covers this information through the following main topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过以下几个主要主题涵盖此信息：
- en: What is a word representation or meaning?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是词表示或词义？
- en: Classical approaches to learning word representations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习词表示的经典方法
- en: Word2vec – a neural network-based approach to learning word representation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec – 基于神经网络的词表示学习方法
- en: The skip-gram algorithm
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳字模型算法
- en: The Continuous Bag-of-Words algorithm
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续词袋模型算法
- en: By the end of this chapter, you will have gained a thorough understanding of
    how the history of word representations has led to Word2vec, how to utilize two
    different Word2vec algorithms, and the vital importance of Word2vec for NLP.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将全面了解单词表示的历史如何发展到Word2vec，如何使用两种不同的Word2vec算法，以及Word2vec在NLP中的至关重要性。
- en: What is a word representation or meaning?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是单词表示或意义？
- en: What is meant by the word *meaning*? This is more of a philosophical question
    than a technical one. So, we will not try to discern the best answer for this
    question, but accept a more modest answer, that is, *meaning* is the idea conveyed
    by or some representation associated with a word. For example, when you hear the
    word “cat” you conjure up a mental picture of something that meows, has four legs,
    has a tail, and so on; then, if you hear the word “dog,” you again formulate a
    mental image of something that barks, has a bigger body than a cat, has four legs,
    has a tail, and so on. In this new space (that is, the mental pictures), it is
    easier for you to understand that cats and dogs are similar than by just looking
    at the words. Since the primary objective of NLP is to achieve human-like performance
    in linguistic tasks, it is sensible to explore principled ways of representing
    words for machines. To achieve this, we will use algorithms that can analyze a
    given text corpus and come up with good numerical representations of words (that
    is, word embeddings) such that words that fall within similar contexts (for example,
    *one* and *two*, *I* and *we*) will have similar numerical representations compared
    to words that are unrelated (for example, *cat* and *volcano*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “*meaning*”这个词是什么意思？这是一个哲学性的问题，更多的是哲学性的问题，而非技术性的问题。所以，我们不会尝试去找出这个问题的最佳答案，而是接受一个更为谦逊的答案，即*meaning*是通过一个词所传达的想法或与之相关的某种表示。例如，当你听到“cat”这个词时，你会在脑海中浮现出一只会喵喵叫、有四条腿、有尾巴的动物的画面；接着，如果你听到“dog”这个词，你又会想象出一只会汪汪叫、比猫体型大、有四条腿、有尾巴的动物。在这个新空间中（即脑海中的画面），你比仅仅通过文字理解，更容易看出猫和狗之间的相似性。由于自然语言处理（NLP）的主要目标是在人类语言任务中实现类似人类的表现，因此探索为机器表示单词的有原则的方式是明智的。为了实现这一目标，我们将使用可以分析给定文本语料库并生成单词的良好数值表示的算法（即，词嵌入），这样那些处于相似语境中的单词（例如，*one*和*two*，*I*和*we*）将具有相似的数值表示，而与之无关的单词（例如，*cat*和*volcano*）则会有不同的表示。
- en: First, we will discuss some classical approaches to achieve this and then move
    on to understanding recent, more sophisticated methods that use neural networks
    to learn feature representations and deliver state-of-the-art performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论一些经典的方法来实现这一点，然后转向理解近期更为复杂的利用神经网络学习特征表示并取得最先进表现的方法。
- en: Classical approaches to learning word representation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典的单词表示学习方法
- en: In this section, we will discuss some of the classical approaches used for numerically
    representing words. It is important to have an understanding of the alternatives
    to word vectors, as these methods are still used in the real world, especially
    when limited data is available.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些用于数值表示单词的经典方法。了解单词向量的替代方法非常重要，因为这些方法在实际应用中仍然被使用，尤其是在数据有限的情况下。
- en: More specifically, we will discuss common representations, such as **one-hot
    encoding** and **Term Frequency-Inverse Document Frequency** (**TF-IDF**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们将讨论一些常见的表示方法，如**独热编码**和**词频-逆文档频率**（**TF-IDF**）。
- en: One-hot encoded representation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码表示
- en: 'One of the simpler ways of representing words is to use the one-hot encoded
    representation. This means that if we have a vocabulary of size *V*, for each
    *i*^(th) word *w*[i], we will represent the word *w*[i] with a *V*-length vector
    [0, 0, 0, …, 0, 1, 0, …, 0, 0, 0] where the *i*^(th) element is 1 and other elements
    are 0\. As an example, consider this sentence:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表示单词的一个简单方法是使用独热编码表示。这意味着，如果我们有一个大小为*V*的词汇表，对于每个第*i*个单词*w*[i]，我们将用一个长度为*V*的向量[0,
    0, 0, …, 0, 1, 0, …, 0, 0, 0]来表示这个单词，其中第*i*个元素是1，其他元素为0。例如，考虑以下句子：
- en: '*Bob and Mary are good friends.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob和Mary是好朋友。*'
- en: 'The one-hot encoded representation of each word might look like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的独热编码表示可能如下所示：
- en: '*Bob*: [1,0,0,0,0,0]'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob*: [1,0,0,0,0,0]'
- en: '*and*: [0,1,0,0,0,0]'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*and*: [0,1,0,0,0,0]'
- en: '*Mary*: [0,0,1,0,0,0]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mary*: [0,0,1,0,0,0]'
- en: '*are*: [0,0,0,1,0,0]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*are*: [0,0,0,1,0,0]'
- en: '*good*: [0,0,0,0,1,0]'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*good*: [0,0,0,0,1,0]'
- en: '*friends*: [0,0,0,0,0,1]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*friends*: [0,0,0,0,0,1]'
- en: However, as you might have already figured out, this representation has many
    drawbacks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你可能已经发现的那样，这种表示有许多缺点。
- en: This representation does not encode the similarity between words in any way
    and completely ignores the context in which the words are used. Let’s consider
    the dot product between the word vectors as the similarity measure. The more similar
    two vectors are, the higher the dot product is for those two vectors. For example,
    the representation of the words *car* and *automobile* will have a similarity
    distance of 0, while *car* and *pencil* will also have the same value.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方式并没有编码单词之间的相似性，完全忽略了单词使用的上下文。让我们考虑单词向量之间的点积作为相似性度量。两个向量越相似，它们的点积就越高。例如，*car*
    和 *automobile* 的表示将具有 0 的相似性距离，而 *car* 和 *pencil* 的表示也将具有相同的值。
- en: This method becomes extremely ineffective for large vocabularies. Also, for
    a typical NLP task, the vocabulary easily can exceed 50,000 words. Therefore,
    the word representation matrix for 50,000 words will result in a very sparse 50,000
    × 50,000 matrix.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大词汇量的情况，这种方法变得非常低效。此外，对于典型的自然语言处理任务，词汇量很容易超过 50,000 个单词。因此，对于 50,000 个单词，词表示矩阵将生成一个非常稀疏的
    50,000 × 50,000 矩阵。
- en: However, one-hot encoding plays an important role even in state-of-the-art word
    embedding learning algorithms. We use one-hot encoding to represent words numerically
    and feed them into neural networks so that the neural networks can learn better
    and smaller numerical feature representations of the words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一热编码在最先进的词嵌入学习算法中仍然发挥着重要作用。我们使用一热编码将单词表示为数字，并将其输入神经网络，以便神经网络能够更好地学习单词的更小的数字特征表示。
- en: One-hot encoding is also known as a localist representation (the opposite to
    the distributed representation), as the feature representation is decided by the
    activation of a single element in the vector.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码（one-hot encoding）也被称为局部表示（与分布式表示相对），因为特征表示是通过向量中单个元素的激活来决定的。
- en: We will now discuss another technique for representing words, known as the TF-IDF
    method.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论另一种表示单词的技术，称为 TF-IDF 方法。
- en: The TF-IDF method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF 方法
- en: '**TF-IDF** is a frequency-based method that takes into account the frequency
    with which a word appears in a corpus. This is a word representation in the sense
    that it represents the importance of a specific word in a given document. Intuitively,
    the higher the frequency of the word, the more important that word is in the document.
    For example, in a document about cats, the word *cats* will appear more often
    than in a document that isn’t about cats. However, just calculating the frequency
    would not work because words such as *this* and *is* are very frequent in documents
    but do not contribute much information. TF-IDF takes this into consideration and
    gives values of near-zero for such common words.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 是一种基于频率的方法，考虑到单词在语料库中出现的频率。这是一种词表示，表示了一个特定单词在给定文档中的重要性。直观地说，单词出现的频率越高，说明这个单词在文档中的重要性越大。例如，在关于猫的文档中，单词
    *cats* 会比在不涉及猫的文档中出现得更频繁。然而，仅仅计算频率是不够的，因为像 *this* 和 *is* 这样的单词在文档中非常频繁，但并没有提供太多信息。TF-IDF
    考虑了这一点，并给这些常见单词分配接近零的值。'
- en: 'Again, *TF* stands for term frequency and *IDF* stands for inverse document
    frequency:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，*TF* 代表词频（term frequency），*IDF* 代表逆文档频率（inverse document frequency）：
- en: '*TF(w*[i]*)* = number of times w[i] appear / total number of words'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(w*[i]*)* = w[i] 出现的次数 / 单词总数'
- en: '*IDF(w*[i]*)* = log(total number of documents / number of documents with w[i]
    in it)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF(w*[i]*)* = log(文档总数 / 包含 w[i] 的文档数量)'
- en: '*TF-IDF(w*[i]*)* = TF(w[i]) x IDF(w[i])'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF(w*[i]*)* = TF(w[i]) x IDF(w[i])'
- en: 'Let’s do a quick exercise. Consider two documents:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个快速练习。考虑两个文档：
- en: 'Document 1: *This is about cats. Cats are great companions*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 1: *This is about cats. Cats are great companions*.'
- en: 'Document 2: *This is about dogs. Dogs are very loyal*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 2: *This is about dogs. Dogs are very loyal*.'
- en: 'Now let’s crunch some numbers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来做一些计算：
- en: '*TF-IDF (cats, doc1)* = (2/8) * log(2/1) = 0.075'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF (cats, doc1)* = (2/8) * log(2/1) = 0.075'
- en: '*TF-IDF (this, doc2)* = (1/8) * log(2/2) = 0.0'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF (this, doc2)* = (1/8) * log(2/2) = 0.0'
- en: Therefore, the word *cats* is informative, while *this* is not. This is the
    desired behavior we needed in terms of measuring the importance of words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词 *cats* 是有信息量的，而 *this* 不是。这就是我们在衡量单词重要性时所需要的期望行为。
- en: Co-occurrence matrix
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共现矩阵
- en: 'Co-occurrence matrices, unlike one-hot-encoded representations, encode the
    context information of words, but require maintaining a V × V matrix. To understand
    the co-occurrence matrix, let’s take two example sentences:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵不同于 one-hot 编码表示，它编码了词汇的上下文信息，但需要维持一个 V × V 的矩阵。为了理解共现矩阵，我们来看两个例子句子：
- en: '*Jerry and Mary are friends*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*杰瑞和玛丽是朋友*。'
- en: '*Jerry buys flowers for Mary*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*杰瑞为玛丽买花*。'
- en: 'The co-occurrence matrix will look like the following matrix. We only show
    one half of the matrix, as it is symmetrical:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵将如下所示。我们仅显示矩阵的一半，因为它是对称的：
- en: '|  | Jerry | and | Mary | are | friends | buys | flowers | for |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | Jerry | and | Mary | are | friends | buys | flowers | for |'
- en: '| Jerry | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Jerry | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
- en: '| and |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| and |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Mary |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Mary |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: '| are |  |  |  | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| are |  |  |  | 0 | 1 | 0 | 0 | 0 |'
- en: '| friends |  |  |  |  | 0 | 0 | 0 | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| friends |  |  |  |  | 0 | 0 | 0 | 0 |'
- en: '| buys |  |  |  |  |  | 0 | 1 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| buys |  |  |  |  |  | 0 | 1 | 0 |'
- en: '| flowers |  |  |  |  |  |  | 0 | 1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| flowers |  |  |  |  |  |  | 0 | 1 |'
- en: '| for |  |  |  |  |  |  |  | 0 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| for |  |  |  |  |  |  |  | 0 |'
- en: However, it is not hard to see that maintaining such a co-occurrence matrix
    comes at a cost as the size of the matrix grows polynomially with the size of
    the vocabulary. Furthermore, it is not straightforward to incorporate a context
    window size larger than 1\. One option is to have a weighted count, where the
    weight for a word in the context deteriorates with the distance from the word
    of interest.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很容易看出，维持这样的共现矩阵是有成本的，因为随着词汇表大小的增加，矩阵的大小会呈多项式增长。此外，增加一个大于 1 的上下文窗口大小也并不简单。一种选择是使用加权计数，其中词汇在上下文中的权重随着与目标词的距离而减小。
- en: As you can see, these methods are very limited in their representational power.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些方法在表示能力上非常有限。
- en: For example, in the one-hot encoded method, all words will have the same vector
    distance to each other. The TF-IDF method represents a word with a single number
    and is unable to capture the semantics of words. Finally calculating the co-occurrence
    matrix is very expensive and provides limited information about a word’s context.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 one-hot 编码方法中，所有单词之间的向量距离是相同的。TF-IDF 方法用一个单一的数字表示一个词，无法捕捉词汇的语义。最后，计算共现矩阵非常昂贵，并且提供的关于词汇上下文的信息有限。
- en: We end our discussion about simple representations of words here. In the following
    section, we will first develop an intuitive understanding of word embeddings by
    working through an example. Then we will define a loss function so that we can
    use machine learning to learn word embeddings. Also, we will discuss two Word2vec
    algorithms, namely, the **skip-gram** and **Continuous Bag-of-Words** (**CBOW**)
    algorithms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里结束关于词汇简单表示的讨论。在接下来的部分中，我们将通过实例首先培养对词嵌入的直观理解。然后我们将定义一个损失函数，以便使用机器学习来学习词嵌入。此外，我们还将讨论两种
    Word2vec 算法，分别是 **skip-gram** 和 **连续词袋（CBOW）** 算法。
- en: An intuitive understanding of Word2vec – an approach to learning word representation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 Word2vec 的直观理解 —— 一种学习词汇表示的方法
- en: “You shall know a word by the company it keeps.”
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你将通过一个词汇的伴侣知道它的含义。”
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – J.R. Firth
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – J.R. Firth
- en: This statement, uttered by J. R. Firth in 1957, lies at the very foundation
    of Word2vec, as Word2vec techniques use the context of a given word to learn its
    semantics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话由 J. R. Firth 在 1957 年说出，它是 Word2vec 的基础，因为 Word2vec 技术利用给定词汇的上下文来学习其语义。
- en: Word2vec is a groundbreaking approach that allows computers to learn the meaning
    of words without any human intervention. Also, Word2vec learns numerical representations
    of words by looking at the words surrounding a given word.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 是一种开创性的方法，它允许计算机在没有任何人工干预的情况下学习词汇的含义。此外，Word2vec 通过观察给定词汇周围的词汇来学习词汇的数值表示。
- en: 'We can test the correctness of the preceding quote by imagining a real-world
    scenario. Imagine you are sitting an exam and you find this sentence in your first
    question: “Mary is a very stubborn child. Her pervicacious nature always gets
    her in trouble.” Now, unless you are very clever, you might not know what *pervicacious*
    means. In such a situation, you automatically will be compelled to look at the
    phrases surrounding the word of interest. In our example, *pervicacious* is surrounded
    by *stubborn*, *nature*, and *trouble*. Looking at these three words is enough
    to determine that pervicacious in fact means the state of being stubborn. I think
    this is adequate evidence to observe the importance of context for a word’s meaning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过想象一个真实场景来测试前面引述的正确性。想象你正在参加考试，在第一题中遇到这句话：“玛丽是一个非常固执的孩子。她固执的天性总是让她惹上麻烦。”现在，除非你非常聪明，否则你可能不知道
    *pervicacious* 的意思。在这种情况下，你会自动被迫查看周围的词组。在我们的例子中，*pervicacious* 被 *固执*，*天性* 和 *麻烦*
    包围。看这三个词就足够判断 *pervicacious* 其实意味着固执的状态。我认为这足以证明上下文对单词含义的重要性。
- en: Now let’s discuss the basics of Word2vec. As already mentioned, Word2vec learns
    the meaning of a given word by looking at its context and representing it numerically.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下 Word2vec 的基础知识。正如前面提到的，Word2vec 通过观察给定单词的上下文来学习该单词的含义，并以数字的形式表示它。
- en: By *context*, we refer to a fixed number of words in front of and behind the
    word of interest. Let’s take a hypothetical corpus with *N* words. Mathematically,
    this can be represented by a sequence of words denoted by *w*[0], *w*[1], …, *w*[i],
    and *w*[N], where *w*[i] is the *i*^(th) word in the corpus.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *上下文*，我们指的是单词前后固定数量的词。假设我们有一个包含 *N* 个单词的假设语料库。用数学表示，这可以表示为一个单词序列，记为 *w*[0]，*w*[1]，…，*w*[i]
    和 *w*[N]，其中 *w*[i] 是语料库中的第 *i* 个单词。
- en: Next, if we want to find a good algorithm that is capable of learning word meanings,
    given a word, our algorithm should be able to predict the context words correctly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们想找到一个能够学习单词含义的好算法，给定一个单词，我们的算法应该能够正确预测上下文单词。
- en: 'This means that the following probability should be high for any given word
    *w*[i]:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，对于任何给定的单词 *w*[i]，以下概率应该很高：
- en: '![](img/B14070_03_001.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_001.png)'
- en: To arrive at the right-hand side of the equation, we need to assume that given
    the target word (*w*[i]), the context words are independent of each other (for
    example, *w*[i-2] and *w*[i-1] are independent). Though not entirely true, this
    approximation makes the learning problem practical and works well in practice.
    Let’s go through an example to understand the computations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到等式的右边，我们需要假设，在给定目标单词（*w*[i]）的情况下，上下文单词彼此之间是独立的（例如，*w*[i-2] 和 *w*[i-1] 是独立的）。尽管这并不完全正确，但这种近似使得学习问题变得可行，并且在实践中效果良好。让我们通过一个例子来理解这些计算。
- en: 'Exercise: does queen = king – he + she?'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习：queen = king - he + she 吗？
- en: 'Before proceeding further, let’s do a small exercise to understand how maximizing
    the previously mentioned probability leads to finding good meaning (or representations)
    of words. Consider the following very small corpus:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们做一个小练习，了解如何通过最大化之前提到的概率来找到单词的良好含义（或表示）。考虑以下一个非常小的语料库：
- en: '*There was a very rich king. He had a beautiful queen. She was very kind*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*曾经有一位非常富有的国王。他有一位美丽的王后。她非常善良*。'
- en: 'To keep the exercise simple, let’s do some manual preprocessing and remove
    the punctuation and the uninformative words:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化练习，让我们手动预处理，去除标点符号和无信息的词：
- en: '*was rich king he had beautiful queen she was kind*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*曾经有富有的国王他有美丽的王后她很善良*'
- en: 'Now let’s form a set of tuples for each word with their context words in the
    format (*target word --> context word 1*, *context word 2*). We will assume a
    context window size of 1 on either side:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为每个单词及其上下文单词形成一组元组，格式为（*目标单词 --> 上下文单词1*，*上下文单词2*）。我们假设上下文窗口大小为两边各 1：
- en: '*was --> rich*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*was --> 富有*'
- en: '*rich --> was, king*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*富有 --> 曾经有, 国王*'
- en: '*king --> rich, he*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*国王 --> 富有, 他*'
- en: '*he --> king, had*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*he --> 国王, had*'
- en: '*had --> he, beautiful*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*有 --> 他, 美丽的*'
- en: '*beautiful --> had, queen*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*美丽的 --> 有, 王后*'
- en: '*queen --> beautiful, she*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*王后 --> 美丽的, 她*'
- en: '*she --> queen, was*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*她 --> 王后, 是*'
- en: '*was --> she, kind*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*是 --> 她, 善良*'
- en: '*kind --> was*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*善良 --> 曾经有*'
- en: 'Remember, our goal is to be able to predict the words on the right given the
    word on the left. To do this, for a given word, the words on the right-side context
    should share a high numerical or geometrical similarity with the words on the
    left-side context. In other words, the word of interest should be conveyed by
    the surrounding words. Now let’s consider actual numerical vectors to understand
    how this works. For simplicity, let’s only consider the tuples highlighted in
    bold. Let’s begin by assuming the following for the word *rich*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的目标是能够根据左边的单词预测右边的单词。为此，对于一个给定的单词，右边语境中的单词应该与左边语境中的单词在数值或几何上有高度的相似性。换句话说，感兴趣的单词应该通过周围的单词来传达。现在，让我们考虑实际的数值向量，以了解这一过程是如何运作的。为了简单起见，让我们只考虑加粗的元组。让我们从假设*rich*这个单词的情况开始：
- en: '*rich --> [0,0]*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*rich --> [0,0]*'
- en: To be able to correctly predict *was* and *king* from *rich*, was and *king*
    should have high similarity with the word *rich*. The Euclidean distance will
    be used to measure the distance between words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够正确预测*was*和*king*，这两个词应与*rich*有较高的相似度。将使用欧几里得距离来衡量单词之间的距离。
- en: 'Let’s try the following values for the words *king* and *rich*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试以下*king*和*rich*的值：
- en: '*king --> [0,1]*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*king --> [0,1]*'
- en: '*was --> [-1,0]*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*was --> [-1,0]*'
- en: 'This works out fine as the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点是可行的，如下所示：
- en: '*Dist(rich,king)* = 1.0'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dist(rich,king)* = 1.0'
- en: '*Dist(rich,was)* = 1.0'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dist(rich,was)* = 1.0'
- en: 'Here, *Dist* is the Euclidean distance between two words. This is illustrated
    in *Figure 3.3*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*Dist*表示两个单词之间的欧几里得距离。如*图3.3*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_04.jpg](img/B14070_03_02.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_04.jpg](img/B14070_03_02.png)'
- en: 'Figure 3.2: The positioning of word vectors for the words “rich”, “was” and
    “king”'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：单词“rich”，“was”和“king”的词向量位置
- en: 'Now let’s consider the following tuple:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑以下元组：
- en: '*king --> rich, he*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*king --> rich, he*'
- en: 'We have established the relationship between *king* and *rich* already. However,
    it is not done yet; the more we see a relationship, the closer these two words
    should be. So, let’s first adjust the vector of king so that it is a bit closer
    to *rich*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经建立了*king*和*rich*之间的关系。然而，这还没有完成；我们看到的关系越多，这两个单词之间的距离应该越近。因此，让我们首先调整*king*的向量，使其更接近*rich*：
- en: '*king --> [0,0.8]*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*king --> [0,0.8]*'
- en: 'Next, we will need to add the word *he* to the picture. The word *he* should
    be closer to *king*. This is all the information that we have right now about
    the word *he: he --> [0.5,0.8]*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将单词*he*添加到图中。单词*he*应该更接近*king*。这是我们目前关于单词*he*的所有信息：he --> [0.5,0.8]。
- en: 'At this moment, the graph with the words looks like *Figure 3.4*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，带有单词的图表看起来像*图3.4*：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_05.jpg](img/B14070_03_03.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_05.jpg](img/B14070_03_03.png)'
- en: 'Figure 3.3: The positioning of word vectors for the words “rich”, “was”, “king,”
    and “he”'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：单词“rich”，“was”，“king”和“he”的词向量位置
- en: 'Now let’s proceed with the next two tuples: *queen --> beautiful, she* and
    *she --> queen, was*. Note that I have swapped the order of the tuples as this
    makes it easier for us to understand the example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续下两个元组：*queen --> beautiful, she* 和 *she --> queen, was*。请注意，我交换了元组的顺序，这样我们更容易理解示例：
- en: '*she --> queen, was*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*she --> queen, was*'
- en: Now, we will have to use our prior knowledge of English to proceed further.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将需要使用我们先前的英语知识来继续。
- en: 'It is a reasonable decision to place the word *she* the same distance from
    *was* that *he* is from *was*, because their usage in the context of the word
    *was* is equivalent. Therefore, let’s use this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将单词*she*与*was*保持与*he*与*was*相同的距离是一个合理的决定，因为它们在*was*这个单词的语境中的使用是等价的。因此，让我们使用这个：
- en: '*she --> [0.5,0.6]*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*she --> [0.5,0.6]*'
- en: 'Next, we will use the word *queen* close to the word *she: queen --> [0.0,0.6]*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用与单词*she*接近的*queen*：queen --> [0.0,0.6]。
- en: 'This is illustrated in *Figure 3.5*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.5*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_06.jpg](img/B14070_03_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_06.jpg](img/B14070_03_04.png)'
- en: 'Figure 3.4: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” and “queen”'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：单词“rich”，“was”，“king”，“he”，“she”和“queen”的词向量位置
- en: 'Next, we only have the following tuple:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只有以下元组：
- en: '*queen --> beautiful, she*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*queen --> beautiful, she*'
- en: 'Here, the word *beautiful* is found. It should be approximately the same distance
    from the words *queen* and *she*. Let’s use the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里找到了单词*beautiful*。它应该与单词*queen*和*she*保持大致相同的距离。让我们使用以下表示：
- en: '*beautiful --> [0.25,0]*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*beautiful --> [0.25,0]*'
- en: 'Now we have the following graph depicting the relationships between words.
    When we observe *Figure 3.6*, it seems to be a very intuitive representation of
    the meanings of words:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了以下图表，描绘了词之间的关系。当我们观察*图3.6*时，它似乎是对单词含义的非常直观的表示：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg](img/B14070_03_05.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg](img/B14070_03_05.png)'
- en: 'Figure 3.5: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” “queen,” and “beautiful”'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：词向量在“rich”，“was”，“king”，“he”，“she”，“queen”，“beautiful”这些词上的位置
- en: 'Now, let’s look at the question that has been lurking in our minds since the
    beginning of this exercise. Are the quantities in this equation equivalent: *queen*
    = *king* - *he* + *she*? Well, we’ve got all the resources that we’ll need to
    solve this mystery now. Let’s try the right-hand side of the equation first:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下从一开始就萦绕在我们心中的问题。这些方程式中的数量是等价的吗：*皇后* = *国王* - *他* + *她*？好了，我们现在拥有了解开这个谜题所需的所有资源。让我们先尝试方程式的右侧：
- en: = *king* – *he* + *she*
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: = *国王* – *他* + *她*
- en: = *[0,0.8]* – *[0.5,0.8]* + *[0.5,0.6]*
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: = *[0,0.8]* – *[0.5,0.8]* + *[0.5,0.6]*
- en: = *[0,0.6]*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: = *[0,0.6]*
- en: It all works out in the end. If you look at the word vector we obtained for
    the word *queen*, you see that this is exactly the same as the answer we deduced
    earlier.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最终一切都得到了验证。如果你看我们为单词*queen*得到的词向量，你会发现这与我们之前推导出的答案完全相同。
- en: Note that this is a crude way to show how word embeddings are learned, and this
    might differ from the exact positions of word embeddings learned using an algorithm.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一种粗略的方式来展示如何学习词嵌入，并且这可能与使用算法学习到的词嵌入的确切位置有所不同。
- en: Also keep in mind that this is an unrealistically scaled-down exercise with
    regard to what a real-world corpus might look like. So, you will not be able to
    work out these values by hand just by crunching a dozen numbers. Sophisticated
    function approximators such as neural networks do this job for us. But, to use
    neural networks, we need to formulate our problem in a mathematically assertive
    way. However, this is a good exercise to show the power of word vectors.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住，这个练习在规模上不现实，与真实世界语料库的样子相比有很大的简化。因此，您无法仅通过手动计算几个数字来推导出这些值。复杂的函数逼近器，如神经网络，替我们完成了这项工作。但是，为了使用神经网络，我们需要以数学上严谨的方式来表述问题。然而，这个练习是展示词向量能力的一个很好的方法。
- en: Now that we have a good understanding of how Word2vec enables us to learn word
    representations, let’s look at the actual algorithms Word2vec utilizes in the
    next two sections.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对Word2vec如何帮助我们学习词表示有了更好的理解，接下来让我们看一下Word2vec在接下来的两节中使用的实际算法。
- en: The skip-gram algorithm
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型算法
- en: 'The first algorithm we will talk about is known as the **skip-gram algorithm**:
    a type of Word2vec algorithm. As we have discussed in numerous places, the meaning
    of a word can be elicited from the contextual words surrounding it. However, it
    is not entirely straightforward to develop a model that exploits this way of learning
    word meanings. The skip-gram algorithm, introduced by Mikolov et al. in 2013,
    is an algorithm that does exploit the context of the words in a written text to
    learn good word embeddings.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要讲解的第一个算法被称为**跳字模型算法**：一种Word2vec算法。如我们在多个地方讨论过的，单词的含义可以从其上下文单词中推断出来。然而，开发一个利用这种方式来学习单词含义的模型并不是完全直接的。由Mikolov等人在2013年提出的跳字模型算法，正是利用文本中单词的上下文来学习良好的词嵌入。
- en: Let’s go through the skip-gram algorithm step by step. First, we will discuss
    the data preparation process. Understanding the format of the data puts us in
    a great position to understand the algorithm. We will then discuss the algorithm
    itself. Finally, we will implement the algorithm using TensorFlow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步讲解跳字模型算法。首先，我们将讨论数据准备过程。了解数据的格式使我们能够更好地理解算法。然后，我们将讨论算法本身。最后，我们将使用TensorFlow实现该算法。
- en: From raw text to semi-structured text
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从原始文本到半结构化文本
- en: 'First, we need to design a mechanism to extract a dataset that can be fed to
    our learning model. Such a dataset should be a set of tuples of the format (target,
    context). Moreover, this needs to be created in an unsupervised manner. That is,
    a human should not have to manually engineer the labels for the data. In summary,
    the data preparation process should do the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设计一个机制来提取可以输入到学习模型的数据集。这样的数据集应该是（目标词，上下文词）格式的元组集合。此外，这一过程需要以无监督的方式进行。也就是说，不需要人工为数据手动标注标签。总之，数据准备过程应该完成以下工作：
- en: Capture the surrounding words of a given word (that is, the context)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉给定单词的周围单词（即上下文）
- en: Run in an unsupervised manner
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以无监督的方式运行
- en: 'The skip-gram model uses the following approach to design a dataset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型采用以下方法设计数据集：
- en: 'For a given word *w*[i], a context window size of *m* is assumed. By *context
    window size*, we mean the number of words considered as context on a single side.
    Therefore, for *w*[i], the context window (including the target word *w*[i]) will
    be of size *2m+1* and will look like this: *[w*[i-m]*, …, w*[i-1]*, w*[i]*, w*[i+1]*,
    …, w*[i+m]*]*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的单词 *w*[i]，假设其上下文窗口大小为 *m*。所谓的*上下文窗口大小*，是指在单侧考虑的上下文单词数量。因此，对于 *w*[i]，上下文窗口（包括目标单词
    *w*[i]）的大小将为 *2m+1*，并且将呈现如下形式： *[w*[i-m]*, …, w*[i-1]*, w*[i]*, w*[i+1]*, …, w*[i+m]*]*。
- en: 'Next, (target, context) tuples are formed as *[…, (w*[i]*, w*[i-m]*), …, (w*[i]*,w*[i-1]*),
    (w*[i]*,w*[i+1]*), …, (w*[i]*,w*[i+m]*), …]*; here, ![](img/B14070_03_002.png)
    and *N* is the number of words in the text. Let’s use the following sentence and
    a context window size (*m*) of 1:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，(目标词, 上下文词) 的元组将被构建为 *[…, (w*[i]*, w*[i-m]*), …, (w*[i]*,w*[i-1]*), (w*[i]*,w*[i+1]*),
    …, (w*[i]*,w*[i+m]*), …]*；其中，![](img/B14070_03_002.png) 和 *N* 是文本中的单词数。让我们使用以下句子，设定上下文窗口大小（*m*）为
    1：
- en: '*The dog barked at the mailman*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*The dog barked at the mailman*。'
- en: 'For this example, the dataset would be as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，数据集将如下所示：
- en: '*[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the,
    mailman)]*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the,
    mailman)]*'
- en: Once the data is in the *(target, context)* format, we can use a neural network
    to learn the word embeddings.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据转化为 *(目标词, 上下文词)* 的格式，我们就可以使用神经网络来学习词向量。
- en: Understanding the skip-gram algorithm
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 skip-gram 算法
- en: First, let’s identify the variables and notation we need to learn the word embeddings.
    To store the word embeddings, we need two V × D matrices, where *V* is the vocabulary
    size and *D* is the dimensionality of the word embeddings (that is, the number
    of elements in the vector that represent a single word). *D* is a user-defined
    hyperparameter. The higher *D* is, the more expressive the word embeddings learned
    will be. We need two matrices, one to represent the context words and one to represent
    the target words. These matrices will be referred to as the *context* *embedding
    space (or context embedding layer)* and the *target* *embedding space (or target
    embedding layer)*, or in general as the embedding space (or the embedding layer).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定学习词向量所需的变量和符号。为了存储词向量，我们需要两个 V × D 的矩阵，其中 *V* 是词汇表的大小，*D* 是词向量的维度（即表示单个单词的向量中的元素个数）。*D*
    是一个用户定义的超参数。*D* 越大，学习到的词向量就越具有表现力。我们需要两个矩阵，一个用于表示上下文单词，另一个用于表示目标单词。这些矩阵将被称为 *上下文*
    *嵌入空间（或上下文嵌入层）* 和 *目标* *嵌入空间（或目标嵌入层）*，或者通常称为嵌入空间（或嵌入层）。
- en: Each word will be represented with a unique ID in the range [1, *V+1*]. These
    IDs are passed to the embedding layer to look up corresponding vectors. To generate
    these IDs, we will use a special object called a Tokenizer that’s available in
    TensorFlow. Let’s refer to an example target-context tuple (w[i], w[j]), where
    the target word ID is *w*[i], and one of the context words is w[j]. The corresponding
    target embedding of *w*[i] is *t*[i], and the corresponding context embedding
    of *w*[j] is *c*[j]. Each target-context tuple is accompanied by a label (0 or
    1), denoted by *y*[i], where true target-context pairs will get a label of 1,
    and negative (or false) target-context candidates will get a label of 0\. It is
    easy to generate negative target-context candidates by sampling a word that does
    not appear in the context of a given target as the context word. We will talk
    about this in more detail later.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词将在范围[1，*V+1*]内用唯一的ID表示。这些ID将传递给嵌入层以查找对应的向量。为了生成这些ID，我们将使用TensorFlow中可用的一个名为Tokenizer的特殊对象。让我们参考一个示例目标-上下文元组(w[i],
    w[j])，其中目标词ID是*w*[i]，而上下文词之一是*w*[j]。*w*[i]的相应目标嵌入是*t*[i]，而*w*[j]的相应上下文嵌入是*c*[j]。每个目标-上下文元组都伴随一个标签（0或1），由*y*[i]表示，真实的目标-上下文对将获得标签1，而负（或假）目标-上下文候选将获得标签0。通过对不在给定目标词上下文中出现的单词进行抽样，很容易生成负目标-上下文候选。稍后我们将详细讨论这一点。
- en: 'At this point, we have defined the necessary variables. Next, for each input
    *w*[i], we will look up the embedding vectors from the context embedding layer
    corresponding to the input. This operation provides us with *c*[i], which is a
    *D*-sized vector (that is, a *D*-long embedding vector). We do the same for the
    input *w*[j], using the context embedding space to retrieve *c*[j]. Afterward,
    we calculate the prediction output for (*w*[i] *,w*[i]*)* using the following
    transformation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们已经定义了必要的变量。接下来，对于每个输入*w*[i]，我们将从上下文嵌入层中查找对应于输入的嵌入向量。这个操作为我们提供了*c*[i]，这是一个*D*大小的向量（即，一个*D*长的嵌入向量）。我们对输入*w*[j]执行相同操作，使用上下文嵌入空间检索*c*[j]。随后，我们使用以下转换计算(*w*[i]
    *,w*[i]*)*的预测输出：
- en: '*logit(w*[i]*, w*[i]*)* = *c*[i] *.t*[j]'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*logit(w*[i]*, w*[i]*)* = *c*[i] *.t*[j]'
- en: '*ŷ*[ij] = *sigmoid(logit(w*[i]*, w*[i]*))*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ*[ij] = *sigmoid(logit(w*[i]*, w*[i]*))*'
- en: Here, *logit(w*[i]*, w*[i]*)* represents the unnormalized scores (that is, logits),
    *ŷ*[i] is a single-valued predicted output (representing the probability of context
    word belonging in the context of the target word).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*logit(w*[i]*, w*[i]*)*表示未归一化的分数（即logits），*ŷ*[i]是单值预测输出（表示上下文词属于目标词上下文的概率）。
- en: 'We will visualize both the conceptual (*Figure 3.7*) and implementation (*Figure
    3.8*) views of the skip-gram model. Here is a summary of the notation:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同时展示跳字模型的概念（*图3.7*）和实现（*图3.8*）。以下是符号的总结：
- en: '*V*: This is the size of the vocabulary'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V*: 词汇表的大小'
- en: '*D*: This is the dimensionality of the embedding layer'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*: 这是嵌入层的维度'
- en: '*w*[i]: Target word'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[i]: 目标词'
- en: '*w*[j]: Context word'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[j]: 上下文词'
- en: '*t*[i]: Target embedding of the word *w*[i]'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t*[i]: 单词*w*[i]的目标嵌入'
- en: '*c*[j]: Context embedding of the word *w*[j]'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c*[j]: 单词*w*[j]的上下文嵌入'
- en: '*y*[i]: This is the one-hot-encoded output word corresponding to *x*[i]'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[i]: 这是与*x*[i]对应的单热编码输出词'
- en: '*ŷ*[i]: This is the predicted output for *x*[i]'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ*[i]: 这是*x*[i]的预测输出'
- en: '*logit(w*[i]*, w*[j]*)*: This is the unnormalized score for the input *x*[i]'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*logit(w*[i]*, w*[j]*)*: 这是输入*x*[i]的未归一化分数'
- en: '![](img/B14070_03_06.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_06.png)'
- en: 'Figure 3.6: The conceptual skip-gram model'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.6: 跳字模型的概念'
- en: '![](img/B14070_03_07.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_07.png)'
- en: 'Figure 3.7: The implementation of the skip-gram model'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '图3.7: 跳字模型的实现'
- en: Using both the existing and derived entities, we can now use the cross-entropy
    loss function to calculate the loss for a given data point *[(w*[i]*, w*[j]*),
    y*[i]*]*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用现有和派生的实体，我们现在可以使用交叉熵损失函数来计算给定数据点*[(w*[i]*, w*[j]*), y*[i]*]*的损失。
- en: 'For binary labels, the cross-entropy loss for a single sample ![](img/B14070_03_003.png)
    is computed as:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元标签，单个样本的交叉熵损失为![](img/B14070_03_003.png)：
- en: '![](img/B14070_03_004.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_004.png)'
- en: 'Where ![](img/B14070_03_005.png) is the predicted label for ![](img/B14070_03_006.png).
    For multi-class classification problems, we generalize the loss by computing the
    term ![](img/B14070_03_007.png) for each class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B14070_03_005.png)是![](img/B14070_03_006.png)的预测标签。对于多类分类问题，我们通过计算每个类别的项![](img/B14070_03_007.png)来推广损失：
- en: '![](img/B14070_03_008.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_008.png)'
- en: Where ![](img/B14070_03_009.png) represents the value of the ![](img/B14070_03_010.png)
    index of the ![](img/B14070_03_011.png), where ![](img/B14070_03_011.png) is a
    one hot encoded vector representing the label of the data point.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B14070_03_009.png) 表示 ![](img/B14070_03_010.png) 索引的值，![](img/B14070_03_011.png)
    是表示数据点标签的一维独热编码向量。
- en: Typically, when training neural networks, this loss is computed for each sample
    in a given batch, then averaged to compute the loss of the batch. Finally, the
    batch losses are averaged over all the batches in the dataset to compute the final
    loss.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练神经网络时，这个损失值是针对给定批次中的每个样本计算的，然后取平均以计算批次的损失值。最后，批次的损失值在数据集中的所有批次上取平均，以计算最终的损失值。
- en: '**Why does the original word embeddings paper use two embedding layers?**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么原始的词嵌入论文使用了两个嵌入层？**'
- en: The original paper (by Mikolov et al., 2013) uses two distinct V × D embedding
    spaces to denote words in the target space (words when used as the target) and
    words in the contextual space (words used as context words). One motivation to
    do this is that a word does not occur in its own context often. So, we want to
    minimize the probability of such things happening.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文（由Mikolov等人，2013年）使用了两个不同的V × D嵌入空间来表示目标空间中的单词（作为目标使用的单词）和上下文空间中的单词（作为上下文单词使用的单词）。这样做的一个动机是单词在自己的上下文中出现的频率较低。因此，我们希望尽量减少这种情况发生的概率。
- en: For example, for the target word *dog*, it is highly unlikely that the word
    *dog* is also found in its context (*P(dog*|*dog) ~ 0*). Intuitively, if we feed
    the (*w*[i]=*dog* and *w*[j]=*dog*) data point to the neural network, we are asking
    the neural network to give a higher loss if the neural network predicts *dog*
    as a context word of *dog*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于目标单词 *dog*，很少可能在它的上下文中也出现单词 *dog*（*P(dog*|*dog) ~ 0*）。直观地说，如果我们将数据点（*w*[i]=*dog*
    和 *w*[j]=*dog*）输入神经网络，我们要求神经网络如果预测 *dog* 为 *dog* 的上下文单词时，给出较高的损失值。
- en: In other words, we are asking the word embedding of the word *dog* to have a
    very high distance to the word embedding of the word *dog*. This creates a strong
    contradiction as the distance between the embeddings of the same word will be
    0\. Therefore, we cannot achieve this if we only have a single embedding space.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们要求单词 *dog* 的词嵌入与单词 *dog* 的词嵌入之间有非常大的距离。这会产生一个强烈的矛盾，因为同一单词的嵌入之间的距离应该是0。因此，如果我们只有一个嵌入空间，无法实现这一点。
- en: However, having two separate embedding spaces for target words and contextual
    words allows us to have this property because this way we have two separate embedding
    vectors for the same word. In practice, as long as you avoid feeding input-output
    tuples, having the same word as input and output allows us to work with a single
    embedding space and eliminates the need for two distinct embedding layers.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有两个独立的目标单词和上下文单词的嵌入空间使我们能够具备这一特性，因为这样我们为同一个单词拥有了两个独立的嵌入向量。在实践中，只要避免将输入输出元组相同，输入和输出都是同一个单词时，我们可以使用单一的嵌入空间，并且不需要两个独立的嵌入层。
- en: Let’s now implement the data generation process with TensorFlow.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用TensorFlow来实现数据生成过程。
- en: Implementing and running the skip-gram algorithm with TensorFlow
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现和运行skip-gram算法
- en: We are now going to get our hands dirty with TensorFlow and implement the algorithm
    from end to end. First, we will discuss the data we’re going to use and how TensorFlow
    can help us to get that data in the format the model accepts. We will implement
    the skip-gram algorithm with TensorFlow and finally train the model and evaluate
    it on data that was prepared.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探索TensorFlow，并从头到尾实现该算法。首先，我们将讨论我们要使用的数据以及TensorFlow如何帮助我们将数据转换为模型所接受的格式。我们将使用TensorFlow实现skip-gram算法，最后训练模型并在准备好的数据上进行评估。
- en: Implementing the data generators with TensorFlow
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现数据生成器
- en: First, we will investigate how data can be generated in the correct format for
    the model. For this exercise, we are going to use the BBC news articles dataset
    available at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    It contains 2,225 news articles belonging to 5 topics, business, entertainment,
    politics, sport, and tech, which were published on the BBC website between 2004-2005.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将研究如何以模型接受的正确格式生成数据。在这个练习中，我们将使用[http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html)中提供的BBC新闻文章数据集。该数据集包含2,225篇新闻文章，涵盖5个主题：商业、娱乐、政治、体育和科技，这些文章是在2004年至2005年间发布在BBC网站上的。
- en: 'We write the function `download_data()` below to download the data to a given
    folder and extract it from its compressed format:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面编写了`download_data()`函数，用于将数据下载到指定的文件夹并从压缩格式中提取数据：
- en: '[PRE0]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The function first creates the `data_dir` if it doesn’t exist. Next, if the
    `bbc-fulltext.zip` file does not exist, it will be downloaded from the provided
    URL. If `bbc-fulltext.zip` has not been extracted yet, it will be extracted to
    `data_dir`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先创建`data_dir`，如果它不存在的话。接下来，如果`bbc-fulltext.zip`文件不存在，它将从提供的URL下载。如果`bbc-fulltext.zip`尚未解压，它将被解压到`data_dir`。
- en: 'We can call this function as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下调用这个函数：
- en: '[PRE1]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With that, we are going to focus on reading the data contained in the news
    articles (in `.txt` format) into the memory. To do that, we will define the `read_data()`
    function, which takes a data directory path (`data_dir`), and reads the `.txt`
    files (except for the README file) found in the data directory:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将专注于将新闻文章中的数据（以`.txt`格式）读取到内存中。为此，我们将定义`read_data()`函数，该函数接受一个数据目录路径（`data_dir`），并读取数据目录中的`.txt`文件（不包括README文件）：
- en: '[PRE2]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the `read_data()` function defined, let’s use it to read in the data and
    print some samples as well as some statistics:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好`read_data()`函数后，我们来使用它读取数据并打印一些样本以及一些统计信息：
- en: '[PRE3]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will print the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE4]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we said at the beginning of this section, there are 2,225 stories with close
    to a million words. In the next step, we need to tokenize each story (in the form
    of a long string) to a list of tokens (or words). Along with that, we will perform
    some preprocessing on the text:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节开始时所说的，系统中包含2,225个故事，总字数接近一百万。接下来的步骤，我们需要将每个故事（以长字符串的形式）进行分词，转换成一个令牌（或单词）列表。同时，我们还将对文本进行一些预处理：
- en: Lowercase all the characters
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有字符转换为小写
- en: Remove punctuation
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号
- en: 'All of these can be achieved with the `tensorflow.keras.preprocessing.text.Tokenizer`
    object. We can define a Tokenizer as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以通过`tensorflow.keras.preprocessing.text.Tokenizer`对象来实现。我们可以如下定义一个Tokenizer：
- en: '[PRE5]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, you can see some of the most popular keyword arguments and their default
    values used when defining a Tokenizer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到定义Tokenizer时常用的一些关键字参数及其默认值：
- en: '`num_words` – Defines the size of the vocabulary. Defaults to `None`, meaning
    it will consider all the words appearing in the text corpus. If set to the integer
    n, it will only consider the n most common words appearing in the corpus.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_words` – 定义词汇表的大小。默认为`None`，表示它会考虑文本语料库中出现的所有单词。如果设置为整数n，它将只考虑语料库中出现的n个最常见单词。'
- en: '`filters` – Defines any characters that need to be omitted during preprocessing.
    By default, it defines a string containing most of the common punctuation marks
    and symbols.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filters` – 定义在预处理过程中需要排除的字符。默认情况下，它定义了一个包含大多数常见标点符号和符号的字符串。'
- en: '`lower` – Defines whether the text needs to be converted to lowercase.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lower` – 定义是否需要将文本转换为小写。'
- en: '`split` – Defines the character that the words will be tokenized on.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`split` – 定义用于分词的字符。'
- en: 'Once the Tokenizer is defined, you can call its `fit_on_texts()` method with
    a list of strings (where each string is a news article) so that the Tokenizer
    will learn the vocabulary and map the words to unique IDs:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了Tokenizer，您可以调用其`fit_on_texts()`方法并传入一个字符串列表（每个字符串都是一篇新闻文章），这样Tokenizer就会学习词汇表并将单词映射到唯一的ID：
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s take a moment to analyze what the Tokenizer has produced after it has
    been fitted on the text. Once it has been fitted, the Tokenizer will have two
    important attributes populated: `word_index` and `index_word`. Here `word_index`
    is a dictionary that maps each word to a unique ID. The `index_word` attribute
    is the opposite of `word_index`, that is, a dictionary that maps each unique word
    ID to the corresponding word:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间分析一下Tokenizer在文本拟合后产生的结果。一旦Tokenizer被拟合，它将填充两个重要的属性：`word_index`和`index_word`。其中，`word_index`是一个字典，将每个单词映射到一个唯一的ID。`index_word`属性是`word_index`的反向映射，即一个字典，将每个唯一的单词ID映射到相应的单词：
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note how we are using the length of the `word_index` dictionary to derive the
    vocabulary size. We need an additional 1 as the ID 0 is a reserved ID and will
    not be used for any word. This will output the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们是如何通过`word_index`字典的长度来推导词汇表大小的。我们需要额外加1，因为ID 0是保留的ID，不会用于任何单词。这样将输出以下内容：
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The more frequent a word is in the corpus, the lower the ID will be. Words
    such as “the”, “to” and “of” which tend to be common (and are called stop words)
    are in fact the most common words. As the next step, we are going to refine our
    Tokenizer object to have a limited-sized vocabulary. Because we are working with
    a relatively small corpus, we have to make sure the vocabulary is not too large,
    as it can lead to poorly learned word vectors due to the lack of data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个词在语料库中出现得越频繁，它的 ID 就越低。像“the”、“to”和“of”这样的常见词（被称为停用词）实际上是最常见的单词。接下来的步骤，我们将精细调整我们的分词器对象，以便它具有一个有限大小的词汇表。因为我们处理的是一个相对较小的语料库，所以我们必须确保词汇表不要太大，因为过大的词汇表可能由于数据不足而导致单词向量学习不佳：
- en: '[PRE9]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we have a total vocabulary of more than 30,000 words, we’ll restrict the
    size of the vocabulary to 15,000\. This means the Tokenizer will only keep the
    most common 15,000 words as the vocabulary. When we restrict a vocabulary this
    way, a new problem arises. As the Tokenizer’s vocabulary does not encompass all
    possible words in the true vocabulary, out-of-vocabulary words (or OOV words)
    can rear their heads. Some solutions are to replace OOV words with a special token
    (such as <`UNK`>) or remove them from the corpus. This is possible by passing
    the string you want to replace OOV tokens with to the `oov_token` argument in
    the Tokenizer. In this case, we will remove OOV words. If we are careful when
    setting the size of the vocabulary, omitting some of the rare words would not
    harm learning the context of words accurately.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的词汇表包含超过 30,000 个单词，我们将词汇表大小限制为 15,000。这样，分词器将只保留最常见的 15,000 个单词作为词汇表。当我们以这种方式限制词汇表时，出现了一个新问题。由于分词器的词汇表并不包含真实词汇表中的所有可能单词，可能会出现词汇表外的单词（即
    OOV 单词）。一种解决方案是用一个特殊的标记（如 <`UNK`>）替换 OOV 单词，或者将它们从语料库中移除。通过将要替换 OOV 标记的字符串传递给分词器的
    `oov_token` 参数，可以实现这一点。在这种情况下，我们将删除 OOV 单词。如果我们在设置词汇表大小时小心谨慎，忽略一些稀有词不会影响准确学习单词的上下文。
- en: 'We can have a look at the transformation done on the text by the Tokenizer
    as follows. Let’s convert a string of the first 100 characters of the first story
    in our corpus (stored in the `news_stories` variable):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看分词器对文本进行的转换。接下来，让我们转换我们语料库中第一篇故事的前 100 个字符（存储在 `news_stories` 变量中）：
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then we can call the `tokenizer`'s `texts_to_sequences()` method to convert
    a list of documents (where each document is a string) to a list of list of word
    IDs (that is, each document is converted to a list of word IDs).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用 `tokenizer` 的 `texts_to_sequences()` 方法，将一组文档（每个文档是一个字符串）转换为一个包含单词
    ID 列表的列表（即每个文档都转换为一个单词 ID 列表）。
- en: '[PRE11]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will print out:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印输出：
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now have our Tokenizer sorted. There’s nothing left to do but to convert
    all of our news articles to sequences of word IDs with a single line of code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的分词器已经配置好了。接下来，我们只需要用一行代码将所有新闻文章转换为单词 ID 的序列：
- en: '[PRE13]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s move on to generating skip-grams using the `tf.keras.preprocessing.sequence.skipgrams()`
    function, provided by TensorFlow. We call the function on a sample phrase representing
    the first 5 words extracted from the first article in the dataset:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 TensorFlow 提供的 `tf.keras.preprocessing.sequence.skipgrams()` 函数生成跳字模型。我们在一个示例短语上调用该函数，示例短语代表从数据集中提取的前
    5 个单词：
- en: '[PRE14]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s consider a window size of 1\. This means, for a given target word, we
    define the context as one word from each side of the target word.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个窗口大小为 1。意味着对于给定的目标单词，我们定义上下文为目标单词两侧各一个单词。
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We have all the ingredients to define extract skip-grams from the sample phrase
    we chose as follows. When run, this function will output data in the exact format
    we need the data in, that is, (target-context) tuples as inputs and corresponding
    labels (0 or 1) as outputs:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经具备了从我们选择的示例短语中提取跳字模型的所有要素。运行时，此函数将输出我们需要的数据格式，即（目标-上下文）元组作为输入，相应的标签（0 或
    1）作为输出：
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s take a moment to reflect on some of the important arguments that have
    been used:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间来回顾一些重要的参数：
- en: '`sequence` `(list[str]` or `list[int])` – A list of words or word IDs.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence` `(list[str]` 或 `list[int])` – 一个包含单词或单词 ID 的列表。'
- en: '`vocabulary_size` `(int)` – Size of the vocabulary.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary_size` `(int)` – 词汇表的大小。'
- en: '`window_size` `(int)` – Size of the window to be considered for the context.
    `window_size` defines the length on each side.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size` `(int)` – 要考虑的上下文窗口大小。`window_size` 定义了窗口的两侧长度。'
- en: '`negative_samples` `(int)` – Fraction of negative candidates to generate. For
    example, a value of 1 means there will be an equal number of positive and negative
    skipgram candidates. A value of 0 means there will not be any negative candidates.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_samples` `(int)` – 生成负向候选词的比例。例如，值为 1 表示正向和负向 skipgram 候选词的数量相等。值为
    0 则表示不会生成负向候选词。'
- en: '`shuffle` `(bool)` – Whether to shuffle the generated inputs or not.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle` `(bool)` – 是否对生成的输入进行洗牌。'
- en: '`categorical (bool)` – Whether to produce labels as categorical (that is, one-hot
    encoded) or integers.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical (bool)` – 是否将标签生成分类形式（即，独热编码）或整数。'
- en: '`sampling_table` `(np.ndarray)` – An array of the same size as the vocabulary.
    An element in a given position in the array represents the probability of sampling
    the word indexed by that position in the Tokenizer’s word ID to word mapping.
    As we will see soon, this is a handy way to avoid common uninformative words being
    over-sampled much.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampling_table` `(np.ndarray)` – 与词汇表大小相同的数组。数组中给定位置的元素表示根据该位置在分词器的词 ID 到词映射中的索引采样该单词的概率。正如我们很快会看到的，这是一种便捷的方法，可以避免常见的无信息词被过度采样。'
- en: '`seed` `(int)` – If shuffling is enabled, this is the random seed to be used
    for shuffling.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` `(int)` – 如果启用了洗牌，这是用于洗牌的随机种子。'
- en: 'With the inputs and labels generated, let’s print some data:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成输入和标签后，我们来打印一些数据：
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will produce:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE19]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For example, since the word “sales” appears in the context of the word “ad”,
    it is considered a positive candidate. On the other hand, since the word “racing”
    (randomly sampled from the vocabulary) does not appear in the context of the word
    “ad”, it is added as a negative candidate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于单词“sales”出现在“ad”这个词的上下文中，因此它被视为一个正向候选词。另一方面，由于单词“racing”（从词汇中随机抽取）没有出现在“ad”这个词的上下文中，因此它被视为一个负向候选词。
- en: When selecting negative candidates, the `skipgrams()` function selects them
    randomly, giving uniform weights to all the words in the vocabulary. However,
    the original paper explains that this can lead to poor performance. A better strategy
    is to use the unigram distribution as a prior for selecting negative context words.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择负向候选词时，`skipgrams()` 函数会随机选择它们，并对词汇表中的所有单词赋予相同的权重。然而，原文中解释说，这可能导致性能不佳。一种更好的策略是使用
    unigram 分布作为选择负向上下文词的先验。
- en: 'You might be wondering what a unigram distribution is. It represents the frequency
    counts of unigrams (or tokens) found in the text. Then the frequency counts are
    easily converted to probabilities (or normalized frequencies) by dividing them
    by the sum of all frequencies. The most amazing thing is that you don’t have to
    compute this by hand for every corpus of text! It turns out that if you take any
    sufficiently large corpus of text, compute the normalized frequencies of unigrams,
    and order them from high to low, you’ll see that the corpus approximately follows
    a certain constant distribution. For the word with rank *math* in a corpus of
    *math* unigrams, the normalized frequency *f*[k] is given by:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道什么是 unigram 分布。它表示文本中单字（或标记）的频率计数。然后，通过将这些频率除以所有频率的总和，频率计数可以轻松地转换为概率（或归一化频率）。最神奇的是，你不需要手动为每个文本语料库计算这个！事实证明，如果你取一个足够大的文本语料库，计算
    unigram 的归一化频率，并将它们从高到低排序，你会发现语料库大致遵循某种恒定的分布。对于一个包含 *math* 个 unigram 的语料库中排名为
    *math* 的单词，其归一化频率 *f*[k] 给出如下公式：
- en: '![](img/B14070_03_013.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_013.png)'
- en: Here, *math* is a hyperparameter that can be tuned to match the true distribution
    more closely. This is known as *Zipf’s law*. In other words, if you have a vocabulary
    where words are ranked (ID-ed) from most common to least common, you can approximate
    the normalized frequency of each word using Zipf’s law. We will be sampling words
    according to the probabilities output through Zipf’s law instead of giving equal
    probabilities to the words. This means words are sampled according to their presence
    (that is, the more frequent, the higher the chance of being sampled) in the corpus.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*math* 是一个超参数，可以调节以更接近真实分布。这就是所谓的 *Zipf’s law*。换句话说，如果你有一个词汇表，其中单词按从最常见到最不常见的顺序排列（ID
    排序），你可以使用 Zipf 定律来近似每个单词的归一化频率。我们将根据 Zipf 定律输出的概率来采样单词，而不是对所有单词赋予相等的概率。这意味着单词的采样将根据它们在语料库中的出现频率进行（也就是说，越常见的单词，越有可能被采样）。
- en: 'To do that, we can use the `tf.random.log_uniform_candidate_sampler()` function.
    This function takes a batch of positive context candidates of shape `[b, num_true]`,
    where `b` is the batch size and `num_true` is the number of true candidates per
    example (1 for the skip-gram model), and it outputs a [`num_sampled`] sized array,
    where `num_sampled` is the number of negative samples we need. We will discuss
    the nitty-gritty of this function soon, while going through an exercise. But let’s
    first generate some positive candidates using the `tf.keras.preprocessing.sequence.skipgrams()`
    function:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用 `tf.random.log_uniform_candidate_sampler()` 函数。该函数接受一个大小为 `[b, num_true]`
    的正上下文候选词批次，其中 `b` 是批次大小，`num_true` 是每个示例的真实候选词数量（对于 skip-gram 模型来说为1），并输出一个大小为
    [`num_sampled`] 的数组，其中 `num_sampled` 是我们需要的负样本数量。我们稍后将详细讨论这个函数的工作原理，并通过实际操作进行说明。但在此之前，让我们先使用
    `tf.keras.preprocessing.sequence.skipgrams()` 函数生成一些正向候选词：
- en: '[PRE20]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that we’re specifying `negative_samples=0`, as we will be generating negative
    samples with the candidate sampler. Let’s now discuss how we can use the `tf.random.log_uniform_candidate_sampler()`
    function to generate negative candidates. Here we will first use this function
    to generate negative candidates for a single word:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们指定了 `negative_samples=0`，因为我们将使用候选样本生成器来生成负样本。接下来我们讨论如何使用 `tf.random.log_uniform_candidate_sampler()`
    函数来生成负候选词。这里我们将首先使用该函数为单个词生成负候选词：
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This function takes the following arguments:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受以下参数：
- en: '`true_classes` `(np.ndarray` or `tf.Tensor)` – A tensor containing true target
    words. This needs to be a [`b, num_true`] sized array, where `num_true` denotes
    the number of true context candidates per example. Since we have one context word
    per example, this is 1.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`true_classes` `(np.ndarray` 或 `tf.Tensor)` – 一个包含真实目标词的张量。它需要是一个大小为 [`b, num_true`]
    的数组，其中 `num_true` 表示每个示例的真实上下文候选词的数量。由于每个示例只有一个上下文词，这个值为1。'
- en: '`num_true` `(int)` – The number of true context terms per example.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_true` `(int)` – 每个示例的真实上下文词的数量。'
- en: '`num_sampled` `(int)` – The number of negative samples to generate.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_sampled` `(int)` – 要生成的负样本数量。'
- en: '`unique` `(bool)` – Whether to generate unique samples or with replacement.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unique` `(bool)` – 是否生成唯一样本或允许重复采样。'
- en: '`range_max` `(int)` – The size of the vocabulary.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`range_max` `(int)` – 词汇表的大小。'
- en: 'It returns:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回：
- en: '`sampled_candidates` `(tf.Tensor)` – A tensor of size [`num_sampled`] containing
    negative candidates'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampled_candidates` `(tf.Tensor)` – 一个大小为 [`num_sampled`] 的张量，包含负候选词。'
- en: '`true_expected_count` `(tf.Tensor)` – A tensor of size [`b, num_true`]; the
    probability of each true candidate being sampled (according to Zipf’s law)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`true_expected_count` `(tf.Tensor)` – 一个大小为 [`b, num_true`] 的张量；表示每个真实候选词被抽样的概率（根据齐普夫定律）。'
- en: '`sampled_expected_count` `(tf.Tensor)` – A tensor of size [`num_sampled`];
    the probabilities of each negative sample occurring along with true candidates,
    if sampled from the corpus'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sampled_expected_count` `(tf.Tensor)` – 一个大小为 [`num_sampled`] 的张量；如果从语料库中抽取，表示每个负样本与真实候选词一同出现的概率。'
- en: 'We will not worry too much about the latter two entities. The most important
    to us is `sampled_candidates`. When calling the function, we have to make sure
    `true_classes` has the shape `[b, num_true]`. In our case, we will run this in
    a single input word ID, which will be in the shape [1, 1]. It returns the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必过于担心后面两个实体。对我们来说，最重要的是 `sampled_candidates`。调用该函数时，我们必须确保 `true_classes`
    的形状是 `[b, num_true]`。在我们的情况下，我们将在单个输入词 ID 上运行该函数，形状为 [1, 1]。它将返回以下内容：
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, putting everything together, let’s write a data generator function that
    generates batches of data for the model. This function, named `skip_gram_data_generator()`,
    takes the following arguments:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将所有内容结合起来，我们来编写一个数据生成器函数，为模型生成数据批次。这个函数名为 `skip_gram_data_generator()`，接受以下参数：
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by the Tokenizer’s `texts_to_sequences()` function.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences` `(List[List[int]])` – 一个包含词 ID 的列表列表。这是由分词器的 `texts_to_sequences()`
    函数生成的输出。'
- en: '`window_size` `(int)` – The window size for the context.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size` `(int)` – 上下文窗口大小。'
- en: '`batch_size` `(int)` – The batch size.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` `(int)` – 批次大小。'
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_samples` `(int)` – 每个示例要生成的负样本数量。'
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary_size` `(int)` – 词汇表大小。'
- en: '`seed` – The random seed.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` – 随机种子。'
- en: 'It will return a batch of data containing:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 它将返回一个包含以下内容的数据批次：
- en: A batch of target word IDs
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一批目标词 ID
- en: A batch of corresponding context word IDs (both positive and negative)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一批对应的上下文词ID（包括正例和负例）
- en: A batch of labels (0 and 1)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一批标签（0和1）
- en: 'The function signature looks as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 函数签名如下：
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, we are going to shuffle the news articles so that every time we generate
    data, they are fetched in a different order. This helps the model to generalize
    better:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将打乱新闻文章的顺序，这样每次生成数据时，它们都会以不同的顺序被获取。这有助于模型更好地进行泛化：
- en: '[PRE24]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, for each text sequence in the corpus we generate positive skip grams.
    `positive_skip_grams` contains tuples of (target, context) word pairs in that
    order:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于语料库中的每个文本序列，我们生成正向skip-gram。`positive_skip_grams`包含按顺序排列的(target, context)词对元组：
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Note that we are passing a `sampling_table` argument. This is another strategy
    to enhance the performance of Word2vec models. `sampling_table` is simply an array
    that is the same size as your vocabulary and specifies a probability at each index
    of the array with which the word indexed by that index will be sampled during
    skip gram generation. This technique is known as subsampling. Each word *w*[i]
    is sampled with the probability given by the following equation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们传递了一个`sampling_table`参数。这是提高Word2vec模型性能的另一种策略。`sampling_table`只是一个与词汇表大小相同的数组，并在数组的每个索引中指定一个概率，该索引处的词将会在skip-gram生成过程中被采样。这个技术被称为子采样。每个词*w*[i]的采样概率由以下公式给出：
- en: '![](img/B14070_03_014.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_014.png)'
- en: Here, *t* is a tunable parameter. It defaults to 0.00001 for a large enough
    corpus. In TensorFlow, you can generate this table easily as follows.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t*是一个可调参数。对于足够大的语料库，它的默认值为0.00001。在TensorFlow中，您可以通过如下方式轻松生成此表：
- en: 'You don’t need the exact frequencies to compute the sampling table, as we can
    leverage Zipf’s law to approximate those frequencies:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 计算采样表时不需要确切的频率，因为我们可以利用齐普夫定律来近似这些频率：
- en: '[PRE26]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For each tuple contained in `positive_skip_grams`, we generate `negative_samples`
    number of negative candidates. We then populate targets, contexts, and label lists
    with both positive and negative candidates:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`positive_skip_grams`中包含的每个元组，我们生成`negative_samples`数量的负样本。然后，我们用正负样本填充目标、上下文和标签列表：
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will then convert these to arrays as follows and randomly shuffle the data.
    When shuffling, you have to make sure all the arrays are consistently shuffled.
    Otherwise, you will corrupt the labels associated with the inputs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将按如下方式将这些转换为数组，并随机打乱数据。在打乱时，您必须确保所有数组都一致地被打乱。否则，您将会破坏与输入相关联的标签：
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, batches of data are generated as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据批次生成如下：
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, we will look at the specifics of the model we’re going to use.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看我们将要使用的模型的具体细节。
- en: Implementing the skip-gram architecture with TensorFlow
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现skip-gram架构
- en: We will now walk through an implementation of the skip-gram algorithm that uses
    the TensorFlow library. The full exercise is available in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` exercise directory.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将走过一个使用TensorFlow库实现的skip-gram算法。完整的练习可以在`Ch3_word2vec.ipynb`中找到，该文件位于`Ch03-Word-Vectors`练习目录中。
- en: 'First, let’s define the hyperparameters of the model. You are free to change
    these hyperparameters to see how they affect final performance (for example, `batch_size
    = 1024` or `batch_size = 2048`). However, since this is a simpler problem than
    the more complex real-world problems, you might not see any significant differences
    (unless you change them to extremes, for example, `batch_size = 1` or `num_sampled
    = 1`):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义模型的超参数。您可以自由更改这些超参数，查看它们如何影响最终的性能（例如，`batch_size = 1024` 或 `batch_size
    = 2048`）。然而，由于这是一个比复杂的现实世界问题更简单的问题，您可能不会看到任何显著差异（除非您将它们更改为极端值，例如，`batch_size =
    1` 或 `num_sampled = 1`）：
- en: '[PRE30]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we define the model. To do this, we will be relying on the Functional
    API of Keras. We need to go beyond the simplest API, that is, the Sequential API,
    as this model requires two input streams (one for the context and one for the
    target).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型。为此，我们将依赖Keras的功能性API。我们需要超越最简单的API，即顺序API，因为这个模型需要两个输入流（一个用于上下文，另一个用于目标）。
- en: 'We will start off with an import. Then we will clear any current running sessions,
    to make sure there aren’t any other models occupying the hardware:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先进行导入。然后清除任何当前正在运行的会话，以确保没有其他模型占用硬件：
- en: '[PRE31]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will define two input layers:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义两个输入层：
- en: '[PRE32]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note how the shape is defined as `()`. When defining the `shape` argument, the
    actual output shape will have a new undefined dimension (i.e. `None` sized) added.
    In other words, the final output shape will be `[None]`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`shape`是如何定义为`()`的。当定义`shape`参数时，实际的输出形状将会添加一个新的未定义维度（即大小为`None`）。换句话说，最终的输出形状将是`[None]`。
- en: 'Next, we define two embedding layers: a target embedding layer and a context
    embedding layer. These layers will be used to look up the embeddings for target
    and context word IDs that will be generated by the input generation function.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义两个嵌入层：目标嵌入层和上下文嵌入层。这些层将用于查找目标和上下文词ID的嵌入，这些词ID将由输入生成函数生成。
- en: '[PRE33]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the embedding layers defined, let’s look up the embeddings for the word
    IDs that will be fed to the input layers:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好嵌入层后，接下来我们来看一下将传入输入层的词ID的嵌入：
- en: '[PRE34]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We now need to compute the dot product of `target_out` and `context_out`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要计算`target_out`和`context_out`的点积。
- en: 'To do that, we are going to use the `tf.keras.layers.Dot` layer:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用`tf.keras.layers.Dot`层：
- en: '[PRE35]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we define our model as a `tf.keras.models.Model` object, where we
    specify `inputs` and `outputs` arguments. `inputs` need to be one or more input
    layers, and `outputs` can be one or more outputs produced by a series of `tf.keras.layers`
    objects:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型定义为一个`tf.keras.models.Model`对象，其中我们指定了`inputs`和`outputs`参数。`inputs`需要是一个或多个输入层，而`outputs`可以是一个或多个由一系列`tf.keras.layers`对象生成的输出：
- en: '[PRE36]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We compile the model using a loss function and an optimizer:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用损失函数和优化器来编译模型：
- en: '[PRE37]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s see a summary of our model by calling the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过调用以下内容来查看模型的摘要：
- en: '[PRE38]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出：
- en: '[PRE39]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Training and evaluating the model will be the next item on our agenda.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估模型将是我们接下来的议程。
- en: Training and evaluating the model
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: Our training process is going to be very simple as we have defined a function
    to generate batches of data in the exact format the model needs them in. But before
    we go ahead with training the model, we need to think about how we evaluate word
    vector models. The idea of word vectors is that words sharing semantic similarity
    will have a smaller distance between them, whereas words with no similarity will
    be far apart. To compute the similarities between words, we can use the cosine
    distance. We picked a set of random word IDs and stored them in `valid_term_ids`
    during our hyperparameter discussion. We will implement a way to compute the closest
    `k` words to each of those terms at the end of every epoch.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练过程将非常简单，因为我们已经定义了一个函数来生成模型所需格式的数据批次。但在我们继续进行模型训练之前，我们需要考虑如何评估词向量模型。词向量的概念是共享语义相似性的词之间的距离较小，而没有相似性的词之间的距离较大。为了计算词与词之间的相似度，我们可以使用余弦距离。在我们的超参数讨论中，我们随机选取了一组词ID并将它们存储在`valid_term_ids`中。我们将在每个周期结束时实现一种方法，计算这些术语的最接近的`k`个词。
- en: 'For this, we utilize Keras callbacks. Keras callbacks give you a way to execute
    some important operation(s) at the end of every training iteration, epoch, prediction
    step, and so on. You can see a full list of the available callbacks at [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).
    Since we need a bespoke evaluation mechanism designed for word vectors, we will
    need to implement our own callback. Our callback will take a list of word IDs
    intended as the validation words, a model containing the embedding matrix, and
    a Tokenizer to decode word IDs:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用Keras回调函数。Keras回调函数为你提供了一种在每次训练迭代、每个周期、每个预测步骤等结束时执行重要操作的方式。你可以在[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)查看所有可用回调函数的完整列表。由于我们需要一个专门为词向量设计的评估机制，我们将需要实现自己的回调函数。我们的回调函数将接受一个包含验证词的词ID列表、一个包含嵌入矩阵的模型以及一个用于解码词ID的Tokenizer：
- en: '[PRE40]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The evaluation will be done at the end of a training epoch, therefore we will
    override the `on_epoch_end()` function. The function extracts the embeddings from
    the context embedding layer.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 评估将在每个训练周期结束时进行，因此我们将重写`on_epoch_end()`函数。该函数从上下文嵌入层中提取嵌入。
- en: Then the embeddings are normalized to have a unit length. Afterward, embeddings
    corresponding to validation words are extracted to a separate matrix called `valid_embeddings`.
    Then the cosine distance is computed between the validation embeddings and all
    word embeddings, which results in a `[valid_size, vocabulary size]` sized matrix.
    From this, we extract the top `k` similar words and display them through `print`
    statements.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，嵌入向量被归一化为单位长度。之后，提取与验证词对应的嵌入向量到一个单独的矩阵中，称为`valid_embeddings`。接着计算验证嵌入与所有词嵌入之间的余弦距离，得到一个`[valid_size,
    vocabulary size]`大小的矩阵。我们从中提取出最相似的`k`个词，并通过`print`语句显示它们。
- en: 'Finally, the model can be trained as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型可以按如下方式进行训练：
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We are simply defining an instance of the callback first. Next, we train the
    model for several epochs. In each, we generate skip gram data (while shuffling
    the order of the articles) and call `skip_gram_model.fit()` on the data. Here’s
    the result after five epochs of training:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简单地定义了一个回调实例。接下来，我们训练模型若干个周期。在每个周期中，我们生成跳字模型数据（同时打乱文章的顺序），并对数据调用`skip_gram_model.fit()`。以下是五个周期训练后的结果：
- en: '[PRE42]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here, we denote some of the most sensible word vectors learned. For example,
    we can see that two of the most similar words to the word “months” are “days”
    and “weeks”. The title “mr” is accompanied by male names such as “scott” and “tony”.
    The word “premier” appears as a similar word to “champion”. You can further experiment
    with:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了一些最具代表性的学习到的词向量。例如，我们可以看到，与“months”最相似的两个词是“days”和“weeks”。“mr”这一称呼常与男性名字如“scott”和“tony”一起出现。词语“premier”与“champion”具有相似性。你还可以进一步实验：
- en: Different negative candidate sampling methods available at [https://www.tensorflow.org/api_docs/python/tf/random](https://www.tensorflow.org/api_docs/python/tf/random)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可在[https://www.tensorflow.org/api_docs/python/tf/random](https://www.tensorflow.org/api_docs/python/tf/random)找到不同的负样本候选采样方法
- en: Different hyperparameter choices (such as the embedding size and the number
    of negative samples)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的超参数选择（例如嵌入向量大小和负样本数量）
- en: In this section, we discussed the skip-gram algorithm from end to end. We saw
    how we can use functions in TensorFlow to transform data. Then we implemented
    the skip-gram architecture using layers in Keras and the Functional API. Finally,
    we trained the model and visually inspected its performance on some test data.
    We will now discuss another popular Word2vec algorithm known as the **Continuous
    Bag-of-Words** (**CBOW**) model.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从头到尾讨论了跳字算法。我们展示了如何使用TensorFlow中的函数来转换数据。然后我们使用Keras中的层和功能性API实现了跳字架构。最后，我们训练了模型，并在一些测试数据上直观地检查了其性能。接下来，我们将讨论另一个流行的Word2vec算法——**连续词袋（CBOW）**模型。
- en: The Continuous Bag-of-Words algorithm
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续词袋模型（Continuous Bag-of-Words）
- en: 'The CBOW model works in a similar way to the skip-gram algorithm, with one
    significant change in the problem formulation. In the skip-gram model, we predict
    the context words from the target word. However, in the CBOW model, we predict
    the target word from contextual words. Let’s compare what data looks like for
    the skip-gram algorithm and the CBOW model by taking the previous example sentence:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW模型与跳字模型算法的工作原理类似，但在问题的表述上有一个显著的变化。在跳字模型中，我们从目标词预测上下文词。然而，在CBOW模型中，我们从上下文词预测目标词。我们通过取前面例子中的句子来比较跳字算法和CBOW模型的数据表现：
- en: '*The dog barked at the mailman.*'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '*The dog barked at the mailman.*'
- en: 'For the skip-gram algorithm, the data tuples—*(input word, output word)*—might
    look like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跳字算法，数据元组—*(输入词, 输出词)*—可能看起来是这样的：
- en: '*(dog, the)*, *(dog, barked)*, *(barked, dog)*, and so on'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '*(dog, the)*，*(dog, barked)*，*(barked, dog)*，等等'
- en: 'For CBOW, the data tuples would look like the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CBOW，数据元组则会是如下形式：
- en: '*([the, barked], dog)*, *([dog, at], barked)*, and so on'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '*([the, barked], dog)*, *([dog, at], barked)*，等等'
- en: 'Consequently, the input of the CBOW has a dimensionality of 2 × m × D, where
    *m* is the context window size and *D* is the dimensionality of the embeddings.
    The conceptual model of CBOW is shown in *Figure 3.13*:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CBOW的输入维度为2 × m × D，其中*m*是上下文窗口的大小，*D*是嵌入向量的维度。CBOW的概念模型如*图3.13*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_29.png](img/B14070_03_08.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_29.png](img/B14070_03_08.png)'
- en: 'Figure 3.8: The CBOW model'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：CBOW模型
- en: We will not go into great detail about the intricacies of CBOW as it is quite
    similar to skip-gram. For example, once the embeddings are aggregated (that is,
    concatenated or summed), they flow through a softmax layer to finally compute
    the same loss as we did with the skip-gram algorithm. However, we will discuss
    the algorithm’s implementation (though not in depth) to get a clear understanding
    of how to properly implement CBOW. The full implementation of CBOW is available
    at `ch3_word2vec.ipynb` in the `Ch03-Word-Vectors` exercise folder.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会过多讨论CBOW的细节，因为它与skip-gram非常相似。例如，一旦嵌入被聚合（即拼接或求和），它们将通过softmax层，最终计算出与skip-gram算法相同的损失。然而，我们将讨论该算法的实现（尽管不深入），以便清楚地理解如何正确实现CBOW。CBOW的完整实现可在`Ch03-Word-Vectors`练习文件夹中的`ch3_word2vec.ipynb`中找到。
- en: Generating data for the CBOW algorithm
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为CBOW算法生成数据
- en: Unfortunately, unlike for the skip-gram algorithm, we do not have a handy function
    to generate data for the CBOW algorithm at our disposal. Therefore, we will need
    to implement this function ourselves.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与skip-gram算法不同，我们没有现成的函数来为CBOW算法生成数据。因此，我们需要自己实现这个函数。
- en: You can find the implementation of this function (named `cbow_grams()`) in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` folder. The procedure will be quite similar to the
    one we used for skip-grams. However, the format of the data will be slightly different.
    Therefore, we will discuss the format of the data returned by this function.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Ch03-Word-Vectors`文件夹中的`ch3_word2vec.ipynb`文件中找到该函数（名为`cbow_grams()`）的实现。这个过程与我们在skip-grams中使用的非常相似。然而，数据格式会略有不同。因此，我们将讨论该函数返回的数据格式。
- en: 'The function takes the same arguments as the `skip_gram_data_generator()` function
    we discussed earlier:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受与我们之前讨论的`skip_gram_data_generator()`函数相同的参数：
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by Tokenizer’s `texts_to_sequences()` function.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequences`（`List[List[int]]`）– 词ID的列表列表。这是Tokenizer的`texts_to_sequences()`函数生成的输出。'
- en: '`window_size` `(int)` – The window size for the context.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window_size`（`int`）– 上下文的窗口大小。'
- en: '`batch_size` `(int)` – The batch size.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`（`int`）– 批次大小。'
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`negative_samples`（`int`）– 每个样本生成的负样本数量。'
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocabulary_size`（`int`）– 词汇表大小。'
- en: '`seed` – The random seed.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` – 随机种子。'
- en: 'The data returned also has a slightly different format. It will return a batch
    of data containing:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的数据格式也略有不同。它将返回一个包含以下内容的数据批次：
- en: A batch of target word IDs, these target words are both positive and negative.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批次的目标词ID，这些目标词包括正样本和负样本。
- en: A batch of corresponding context word IDs. Unlike skip-grams, for CBOW, we need
    all the words in the context, not just one. For example, if we define a batch
    size of `b` and window size of `w`, this will be a `[b, 2w]` sized tensor.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批次对应的上下文词ID。与skip-gram不同，对于CBOW，我们需要上下文中的所有词，而不仅仅是一个。例如，如果我们定义批次大小为`b`，窗口大小为`w`，则这是一个`[b,
    2w]`大小的张量。
- en: A batch or labels (0 and 1).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个批次或标签（0和1）。
- en: We will now learn about the specifics of the algorithm.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来学习该算法的具体细节。
- en: Implementing CBOW in TensorFlow
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现CBOW
- en: 'We will use the same hyperparameters as before:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与之前相同的超参数：
- en: '[PRE43]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Just as before, let’s first clear out any remaining sessions, if there are
    any:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，让我们先清除掉任何剩余的会话（如果有的话）：
- en: '[PRE44]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We define two input layers. Note how the second input layer is defined to have
    `2 x window_size` dimensions. This means the final shape of that layer will be
    `[None, 2 x window_size]`:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个输入层。注意第二个输入层被定义为具有`2 x window_size`的维度。这意味着该层的最终形状将是`[None, 2 x window_size]`：
- en: '[PRE45]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s now define two embedding layers: one for the context words and one for
    the target words. We will feed the inputs from the input layers and produce `context_out`
    and `target_out`:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来定义两个嵌入层：一个用于上下文词，另一个用于目标词。我们将从输入层输入数据，并生成`context_out`和`target_out`：
- en: '[PRE46]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If you look at the shape of `context_out`, you will see that it has the shape
    `[None, 2, 128]`, where `2` is `2 x window_size`, due to taking the whole context
    around a word. This needs to be reduced to `[None, 128]` by taking the average
    of all the context words. This is done by using a Lambda layer:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看`context_out`的形状，你会看到它的形状是`[None, 2, 128]`，其中`2`是`2 x window_size`，这是因为它考虑了一个词周围的整个上下文。这需要通过对所有上下文词的平均值进行降维，变为`[None,
    128]`。这一操作是通过使用Lambda层完成的：
- en: '[PRE47]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We pass a `Lambda` function to the `tf.keras.layers.Lambda` layer to reduce
    the `context_out` tensor on the second dimension to produce a `[None, 128]` sized
    tensor. With both the `target_out` and `mean_context_out` tensors having the shape
    `[None, 128]`, we can compute the dot product of the two to produce an output
    tensor `[None, 1]`:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个`Lambda`函数传递给`tf.keras.layers.Lambda`层，以在第二维度上减少`context_out`张量，从而生成一个大小为`[None,
    128]`的张量。由于`target_out`和`mean_context_out`张量的形状都是`[None, 128]`，我们可以计算这两者的点积，生成一个输出张量`[None,
    1]`：
- en: '[PRE48]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With that, we can define the final model as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们可以将最终模型定义如下：
- en: '[PRE49]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Similar to `skip_gram_model`, we will compile `cbow_model` as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`skip_gram_model`，我们将按如下方式编译`cbow_model`：
- en: '[PRE50]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Again, if you would like to see the summary of the model, you can run `cbow_model.summary()`.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看模型的摘要，可以运行`cbow_model.summary()`。
- en: Training and evaluating the model
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和评估模型
- en: 'The model training is identical to how we trained the skip-gram model. First,
    let’s define a callback to find the top k words similar to the words defined in
    the `valid_term_ids` set:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练与我们训练skip-gram模型的方式相同。首先，让我们定义一个回调函数，用于找到与`valid_term_ids`集合中定义的词最相似的前k个词：
- en: '[PRE51]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we train `cbow_model` for several epochs:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们训练`cbow_model`若干轮：
- en: '[PRE52]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output should look like the following. We have cherry-picked some of the
    most sensible word vectors learned:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示。我们挑选了一些最合理的词向量进行展示：
- en: '[PRE53]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: From visual inspection, it seems CBOW has learned some effective word vectors.
    Similar to the skip-gram model, it has picked words like “years” and “days” as
    similar to “months”. Numerical values such as “5bn” have “5m” and “7bn” around
    them. But it’s important to remember that visual inspection is just a quick and
    dirty way to evaluate word vectors.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉检查来看，CBOW似乎已经学到了有效的词向量。类似于skip-gram模型，它已经将“years”和“days”这样的词与“months”进行了类比。像“5bn”这样的数字值周围有“5m”和“7bn”。但重要的是要记住，视觉检查只是评估词向量的一种快速而粗略的方式。
- en: 'Typically, word vectors are evaluated on some downstream tasks. One of the
    popular tasks is the word analogical reasoning task. It focuses on answering questions
    like:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，词向量会在一些下游任务中进行评估。一个流行的任务是词类比推理任务。它主要聚焦于回答类似以下的问题：
- en: '*Athens is to Greece as Baghdad to ____*'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '*雅典之于希腊，如巴格达之于____*'
- en: 'The answer is `Iraq`. How is the answer computed? If the word vectors are sensible,
    then:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是`Iraq`。答案是如何计算的？如果词向量合理，那么：
- en: '`Word2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)`'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)`'
- en: or
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '`Word2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)`'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)`'
- en: The answer is computed as the vector given by `Word2vec(Baghdad) - Word2vec(Athens)
    + Word2vec(Greece)`. The next step for this analogy task would be to see if the
    most similar vector to the resulting vector is given by the word Iraq. This way,
    accuracy can be computed for an analogy reasoning task. However, we will not utilize
    this task in this chapter, as our dataset is not big enough to perform well in
    this task.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 答案通过计算`Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)`得到。这个类比任务的下一步是查看与结果向量最相似的词是否是Iraq。通过这种方式，可以计算类比推理任务的准确度。然而，由于我们的数据集不够大，不能很好地执行此任务，所以我们在本章中不会使用这个任务。
- en: Here, we conclude our discussion on the CBOW algorithm. Though CBOW shares similarities
    with the skip-gram algorithm, it had some architectural differences as well as
    differences in data.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们结束了对CBOW算法的讨论。尽管CBOW与skip-gram算法有相似之处，但它在架构和数据上也存在差异。
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Word embeddings have become an integral part of many NLP tasks and are widely
    used for tasks such as machine translation, chatbots, image caption generation,
    and language modeling. Not only do word embeddings act as a dimensionality reduction
    technique (compared to one-hot encoding), they also give a richer feature representation
    than other techniques. In this chapter, we discussed two popular neural-network-based
    methods for learning word representations, namely the skip-gram model and the
    CBOW model.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入已成为许多NLP任务的核心部分，广泛应用于机器翻译、聊天机器人、图像描述生成和语言建模等任务中。词嵌入不仅作为一种降维技术（与独热编码相比），还提供了比其他技术更丰富的特征表示。在本章中，我们讨论了两种基于神经网络的学习词表示的流行方法，即skip-gram模型和CBOW模型。
- en: First, we discussed the classical approaches to this problem to develop an understanding
    of how word representations were learned in the past. We discussed various methods,
    such as using WordNet, building a co-occurrence matrix of the words, and calculating
    TF-IDF.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们讨论了该问题的经典方法，以便了解过去是如何学习词表示的。我们讨论了多种方法，例如使用WordNet、构建词的共现矩阵，以及计算TF-IDF。
- en: Next, we explored neural-network-based word representation learning methods.
    First, we worked out an example by hand to understand how word embeddings or word
    vectors can be calculated to help us understand the computations involved.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨了基于神经网络的词表示学习方法。首先，我们手工计算了一个例子，以理解词嵌入或词向量是如何计算的，帮助我们理解涉及的计算过程。
- en: Next, we discussed the first word-embedding learning algorithm—the skip-gram
    model. We then learned how to prepare the data to be used for learning. Later,
    we examined how to design a loss function that allows us to use word embeddings
    using the context words of a given word. Finally, we discussed how to implement
    the skip-gram algorithm using TensorFlow.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了第一个词嵌入学习算法——skip-gram模型。然后我们学习了如何准备数据以供学习使用。随后，我们研究了如何设计一个损失函数，使我们能够利用给定词的上下文词来使用词嵌入。最后，我们讨论了如何使用TensorFlow实现skip-gram算法。
- en: Then we reviewed the next choice for learning word embeddings—the CBOW model.
    We also discussed how CBOW differs from the skip-gram model. Finally, we discussed
    a TensorFlow implementation of CBOW as well.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们回顾了下一种学习词嵌入的方法——CBOW模型。我们还讨论了CBOW与skip-gram模型的区别。最后，我们还讨论了CBOW的TensorFlow实现。
- en: In the next chapter, we will learn several other word embedding learning techniques
    known as Global Vectors, or GloVe, and Embeddings from Language Models, or ELMo.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习几种其他的词嵌入学习技术，分别是全球向量（Global Vectors，简称GloVe）和语言模型的嵌入（Embeddings from
    Language Models，简称ELMo）。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的GitHub页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，与超过1000名成员一起学习： [https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
