- en: Anomaly Detection and Recommendation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测和推荐系统
- en: This chapter will look at auto-encoder models and recommendation systems. Although
    these two use cases may seem very different, they both rely on finding different
    representations of data. These representations are similar to the embeddings we
    saw in [Chapter 7](03f666ab-60ce-485a-8090-c158b29ef306.xhtml), *Natural Language
    Processing Using Deep Learning*. The first part of this chapter introduces unsupervised
    learning where there is no specific outcome to be predicted. The next section
    provides a conceptual overview of auto-encoder models in a machine learning and
    deep neural network context in particular. We will show you how to build and apply
    an auto-encoder model to identify anomalous data. Such atypical data may be bad
    data or outliers, but could also be instances that require further investigation,
    for example, fraud detection. An example of applying anomaly detection is detecting
    when an individual's credit card spending pattern differs from their usual behavior.
    Finally, this chapter closes with a use case on how to apply recommendation systems
    for cross-sell and up-sell opportunities using the retail dataset that was introduced
    in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training Deep Prediction
    Models*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨自编码器模型和推荐系统。尽管这两个应用场景看起来非常不同，但它们都依赖于找到数据的不同表示。这些表示类似于我们在[第7章](03f666ab-60ce-485a-8090-c158b29ef306.xhtml)《*使用深度学习的自然语言处理*》中看到的嵌入。本文的第一部分介绍了无监督学习，在这种学习中没有特定的预测结果。接下来的部分提供了关于自编码器模型的概念性概述，特别是在机器学习和深度神经网络的背景下。我们将向您展示如何构建并应用自编码器模型来识别异常数据。这些非典型数据可能是坏数据或离群值，但也可能是需要进一步调查的实例，例如欺诈检测。应用异常检测的一个例子是，当个人的信用卡消费模式与他们的常规行为不同的时候。最后，本章以一个用例结束，讲解如何使用在[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)《*训练深度预测模型*》中介绍的零售数据集来应用推荐系统进行交叉销售和追加销售。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is unsupervised learning?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: How do auto-encoders work?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器是如何工作的？
- en: Training an auto-encoder in R
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在R中训练自编码器
- en: Using auto-encoders for anomaly detection
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器进行异常检测
- en: Use case – collaborative filtering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例——协同过滤
- en: What is unsupervised learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是无监督学习？
- en: So far, we have focused on models and techniques that broadly fall under the
    category of supervised learning. Supervised learning is supervised because the
    task is for the machine to learn the relationship between a set of variables or
    features and one or more outcomes. For example, in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),
    *Training Deep Prediction Models*, we wanted to predict whether someone would
    visit a store in the next 14 days. In this chapter, we will delve into methods
    of unsupervised learning. In contrast with supervised learning, where there is
    an outcome variable(s) or labeled data is being used, unsupervised learning does
    not use any outcomes or labeled data. Unsupervised learning uses only input features
    for learning. A common example of unsupervised learning is cluster analysis, such
    as k-means clustering, where the machine learns hidden or latent clusters in the
    data to minimize a criterion (for example, the smallest variance within a cluster).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注的是广义上属于监督学习类别的模型和技术。监督学习之所以叫做监督，是因为任务是让机器学习一组变量或特征与一个或多个结果之间的关系。例如，在[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)《*训练深度预测模型*》中，我们希望预测某人是否会在接下来的14天内访问商店。在本章中，我们将深入探讨无监督学习的方法。与监督学习不同，监督学习需要有结果变量或标签数据，而无监督学习则不使用任何结果或标签数据。无监督学习仅使用输入特征进行学习。无监督学习的一个常见示例是聚类分析，例如k均值聚类，其中机器学习数据中的隐藏或潜在聚类，以最小化一个标准（例如，聚类内的最小方差）。
- en: Another unsupervised learning method is to find another representation of the
    data, or to reduce the input data into a smaller dataset without losing too much
    information in the process, this is known as dimensionality reduction. The goal
    of dimensionality reduction is for a set of *p* features to find a set of latent
    variables, *k*, so that *k < p*. However, with *k* latent variables, *p* raw variables
    can be reasonably reproduced. We used **p****rincipal component analysis** (**PCA**) in
    the neural networks example from [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml),
    *Training a Prediction Model*. In that example, we saw that there is a trade-off
    between the number of dimensions and the information loss, as shown in *Figure
    2.1*. Principal component analysis uses an orthogonal transformation to go from
    the raw data to the principal components. In addition to being uncorrelated, the
    principal components are ordered from the component that explains the most variance
    to that which explains the least. Although all principal components can be used
    (in which case the dimensionality of the data is not reduced), only components
    that explain a sufficiently large amount of variance (for example, based on high
    eigenvalues) are included and components that account for relatively little variance
    are dropped as noise or unnecessary. In the neural networks example in [Chapter
    2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training a Prediction Model*, we
    had 624 inputs after eliminating the features with zero variance. When we applied
    PCA, we found that 50% of our variance (information) by our data could be represented
    in just 23 principal components.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无监督学习方法是寻找数据的另一种表示方式，或者将输入数据缩减为一个更小的数据集，同时在过程中尽量不丢失过多信息，这就是所谓的降维。降维的目标是通过一组*p*特征找到一组潜在变量*k*，使得*k
    < p*。然而，使用*k*个潜在变量时，可以合理地重建*p*个原始变量。我们在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)的神经网络示例中使用了**主成分分析**（**PCA**），*训练预测模型*。在这个例子中，我们看到维度数量与信息损失之间存在一个权衡，如*图2.1*所示。主成分分析使用正交变换将原始数据转换为主成分。除了不相关外，主成分按从解释方差最多的成分到解释方差最少的成分的顺序排列。尽管可以使用所有主成分（这样数据的维度就不会减少），但只有解释了足够多方差的成分（例如，基于高特征值）才会被保留，而解释相对较少方差的成分则会被视为噪声或不必要的部分。在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)的神经网络示例中，*训练预测模型*，我们在去除方差为零的特征后，得到了624个输入。当我们应用PCA时，我们发现数据的50%的方差（信息）仅能通过23个主成分来表示。
- en: How do auto-encoders work?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动编码器是如何工作的？
- en: 'Auto-encoders are a form of dimensionality reduction technique. When they are
    used in this manner, they mathematically and conceptually have similarities to
    other dimensionality reduction techniques such as PCA. Auto-encoders consist of
    two parts: an encoder which creates a representation of the data, and a decoder
    which tries to reproduce or predict the inputs. Thus, the hidden layers and neurons
    are not maps between an input and some other outcome, but are self (auto)-encoding.
    Given sufficient complexity, auto-encoders can simply learn the identity function,
    and the hidden neurons will exactly mirror the raw data, resulting in no meaningful
    benefit. Similarly, in PCA, using all the principal components also provides no
    benefit. Therefore, the best auto-encoder is not necessarily the most accurate
    one, but one that reveals some meaningful structure or architecture in the data
    or one that reduces noise, identifies outliers, or anomalous data, or some other
    useful side-effect that is not necessarily directly related to accurate predictions
    of the model inputs.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种降维技术。当它们以这种方式使用时，数学上和概念上与其他降维技术（如PCA）有相似之处。自动编码器由两部分组成：编码器，它创建数据的表示；解码器，它试图重建或预测输入。因此，隐藏层和神经元不是输入和其他结果之间的映射，而是自编码（auto）过程。考虑到足够的复杂性，自动编码器可以简单地学习身份函数，隐藏神经元将完全复制原始数据，结果是没有任何有意义的收益。类似地，在PCA中，使用所有主成分也不会带来任何好处。因此，最好的自动编码器不一定是最准确的，而是能够揭示数据中某种有意义结构或架构的，或者能够减少噪声、识别异常值或异常数据，或者带来其他有用的副作用，这些副作用不一定直接与模型输入的准确预测相关。
- en: Auto-encoders with a lower dimensionality than the raw data are called **undercomplete**;
    by using an undercomplete auto-encoder, one can force the auto-encoder to learn
    the most important features of the data. One common application of auto-encoders
    is to pre-train deep neural networks or other supervised learning models. In addition,
    it is possible to use the hidden features themselves. We will see this later on
    for anomaly detection. Using an undercomplete model is effectively a way to regularize
    the model. However, it is also possible to train overcomplete auto-encoders where
    the hidden dimensionality is greater than the raw data, so long as some other
    form of regularization is used.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 低维度自编码器称为 **欠完备**；通过使用欠完备自编码器，可以迫使自编码器学习数据中最重要的特征。自编码器的一个常见应用是对深度神经网络或其他监督学习模型进行预训练。此外，也可以使用隐藏特征本身。我们稍后会看到它在异常检测中的应用。使用欠完备模型实际上是一种正则化模型的方法。然而，也可以训练过完备自编码器，其中隐藏层的维度大于原始数据，只要使用了其他形式的正则化。
- en: 'There are broadly two parts to auto-encoders:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器大致有两个部分：
- en: First, an encoding function, *f()**,* encodes the raw data, *x*, to the hidden
    neurons, *H*
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，编码函数 *f()* 将原始数据 *x* 编码到隐藏神经元 *H* 中
- en: Second, a decoding function, *g()*, decodes *H* back to *x*
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，解码函数 *g()* 将 *H* 解码回 *x*
- en: 'The following diagram shows an undercomplete encoder, where we have fewer nodes
    in the hidden layer. The output layer on the right is the decoded version of the
    input layer on the left. The task of the hidden layer is to store as much information
    as possible about the input layer (encode the input layer) so that the input layer
    can be re-constructed (or decoded):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个欠完备编码器，其中隐藏层的节点较少。右侧的输出层是左侧输入层的解码版本。隐藏层的任务是尽可能多地存储输入层的信息（编码输入层），以便输入层可以被重建（或解码）：
- en: '![](img/a2f06db4-841d-4384-8a9d-f9d534282ecf.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2f06db4-841d-4384-8a9d-f9d534282ecf.png)'
- en: 'Figure 9.1: An example of an auto-encoder'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：自编码器的示例
- en: Regularized auto-encoders
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化自编码器
- en: An undercomplete auto-encoder is a form of a regularized auto-encoder, where
    the regularization occurs through using a shallower (or in some other way lower)
    dimensional representation than the data. However, regularization can be accomplished
    through other means as well. These are penalized auto-encoders.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欠完备自编码器是一种正则化自编码器，其正则化通过使用比数据更浅（或以其他方式更低）维度的表示来实现。然而，正则化也可以通过其他方式实现。这些就是惩罚自编码器。
- en: Penalized auto-encoders
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惩罚自编码器
- en: As we have seen in previous chapters, one approach to preventing overfitting is
    to use penalties, that is, regularization. In general, our goal is to minimize
    the reconstruction error. If we have an objective function, *F*, we may optimize
    *F(y, f(x))*, where *f()* encodes the raw data inputs to generate predicted or
    expected *y* values. For auto-encoders, we have *F(x, g(f(x)))*, so that the machine
    learns the weights and functional form of *f()* and *g()* to minimize the discrepancy
    between *x* and the reconstruction of *x*, namely *g(f(x))*. If we want to use
    an overcomplete auto-encoder, we need to introduce some form of regularization
    to force the machine to learn a representation that does not simply mirror the
    input. For example, we might add a function that penalizes based on complexity,
    so that instead of optimizing *F(x, g(f(x)))*, we optimize *F(x, g(f(x))) + P(f(x))*,
    where the penalty function, *P*, depends on the encoding or the raw inputs, *f()*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前几章中看到的，防止过拟合的一种方法是使用惩罚，即正则化。通常，我们的目标是最小化重建误差。如果我们有一个目标函数 *F*，我们可以优化 *F(y,
    f(x))*，其中 *f()* 对原始数据输入进行编码，以生成预测或期望的 *y* 值。对于自编码器，我们有 *F(x, g(f(x)))*，因此机器学习
    *f()* 和 *g()* 的权重及功能形式，以最小化 *x* 和 *x* 的重建值 *g(f(x))* 之间的差异。如果我们想使用过完备自编码器，我们需要引入某种形式的正则化，迫使机器学习到一种表示，而不仅仅是复制输入。例如，我们可以添加一个基于复杂度的惩罚函数，这样我们就可以优化
    *F(x, g(f(x))) + P(f(x))*，其中惩罚函数 *P* 取决于编码或原始输入 *f()*。
- en: Such penalties differ somewhat from those we have seen before, in that the penalty
    is designed to induce sparseness, not of the parameters but rather of the latent
    variables, *H*, which are the encoded representations of the raw data. The goal
    is to learn a latent representation that captures the essential features of the
    data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种惩罚方法与我们之前看到的有所不同，因为它的设计目的是促使潜变量 *H* 的稀疏性，而不是参数的稀疏性，*H* 是原始数据的编码表示。目标是学习一个捕捉数据本质特征的潜在表示。
- en: Another type of penalty that can be used to provide regularization is one based
    on the derivative. Whereas sparse auto-encoders have a penalty that induces sparseness
    of the latent variables, penalizing the derivatives results in the model learning
    a form of *f()* that is relatively insensitive to minor perturbations of the raw
    input data, *x*. What we mean by this is that it forces a penalty on functions
    where the encoding varies greatly for changes in *x*, preferring regions where
    the gradient is relatively flat.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可以用于提供正则化的惩罚是基于导数的惩罚。稀疏自编码器有一种惩罚方法，可以促使潜变量的稀疏性，而对导数的惩罚则使得模型学习到一种对原始输入数据 *x*
    的微小扰动相对不敏感的 *f()* 形式。我们所指的意思是，它对那些在 *x* 变化时编码变化很大的函数施加惩罚，偏好那些梯度相对平坦的区域。
- en: Denoising auto-encoders
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'Denoising auto-encoders remove noise or denoise data, and are a useful technique
    for learning a latent representation of raw data (*Vincent, P., Larochelle, H.,
    Bengio, Y., and Manzagol, P. A. (2008, July); Bengio, Y.,Courville, A., and Vincent,
    P. (2013)*). We said that the general task of an auto-encoder was to optimize:
    *F(x, g(f(x)))*. However, for a denoising auto-encoder, the task is to recover
    *x* from a noisy or corrupted version of *x*. One application of denoising auto-encoders
    is to restore old images that may be blurred or corrupted.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器能够去除噪声或还原数据，是学习原始数据潜在表示的有用技术 (*Vincent, P., Larochelle, H., Bengio, Y.,
    和 Manzagol, P. A. (2008年7月); Bengio, Y., Courville, A., 和 Vincent, P. (2013年)*)。我们说过，自编码器的常见任务是优化：*F(x,
    g(f(x)))*。然而，对于去噪自编码器来说，任务是从噪声或受损的 *x* 版本中恢复 *x*。去噪自编码器的一个应用是恢复可能已经模糊或损坏的旧图像。
- en: 'Although denoising auto-encoders are used to try and recover the true representation
    from corrupted data or data with noise, this technique can also be used as a regularization
    tool. As a method of regularization, rather than having noisy or corrupted data
    and attempting to recover the truth, the raw data is purposefully corrupted. This
    forces the auto-encoder to do more than merely learn the identity function, as
    the raw inputs are no longer identical to the output. This process is shown in
    the following diagram:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管去噪自编码器用于从受损数据或带噪声的数据中尝试恢复真实的表示，但这一技术也可以作为正则化工具使用。作为一种正则化方法，不是处理带噪声或受损的数据并试图恢复真实数据，而是故意让原始数据受到损坏。这迫使自编码器不仅仅学习身份函数，因为原始输入不再与输出完全相同。这个过程如下图所示：
- en: '![](img/cecb2097-2cdd-4670-b035-d78abdab2adb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cecb2097-2cdd-4670-b035-d78abdab2adb.png)'
- en: 'Figure 9.2: Denoising auto-encoders'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：去噪自编码器
- en: The remaining choice is what the function, *N()*, which adds the noise or corrupts
    *x*, should be. Two choices are to add noise through a stochastic process or for
    any given training iteration to only include a subset of the raw *x* inputs. In
    the next section, we will explore how to actually train auto-encoder models in
    R.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的选择是 *N()* 函数，它负责添加噪声或损坏 *x*，应该是什么样的。两种选择是通过随机过程添加噪声，或在每次训练迭代中仅包括原始 *x* 输入的一个子集。在下一节中，我们将探讨如何在
    R 中实际训练自编码器模型。
- en: Training an auto-encoder in R
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 R 中训练自编码器
- en: In this section, we are going to train an auto-encoder in R and show you that
    it can be used as a dimensionality reduction technique. We will compare it with
    the approach we took in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml),
    *Training a Prediction Model*, where we used PCA to find the principal components in
    the image data. In that example, we used PCA and found that 23 factors was sufficient
    to explain 50% of the variance in the data. We built a neural network model using
    just these 23 factors to classify a dataset with either *5* or *6*. We got 97.86%
    accuracy in that example.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何在 R 中训练自编码器，并向你展示它如何作为降维技术使用。我们将它与在[第2章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)《训练预测模型》中采用的方法进行比较，在那一章中，我们使用
    PCA 找到图像数据中的主成分。在那个例子中，我们使用 PCA 发现 23 个因子足以解释数据中 50% 的方差。我们使用这 23 个因子构建了一个神经网络模型来分类包含
    *5* 或 *6* 的数据集，并且在该例子中，我们的准确率为 97.86%。
- en: 'We are going to follow a similar process in this example, and we will use the
    `MINST` dataset again. The following code from `Chapter8/encoder.R` loads the
    data. We will use half the data for training an auto-encoder and the other half
    will be used to build a classification model to evaluate how good the auto-encoder
    is at dimensionality reduction. The first part of the code is similar to what
    we have seen in previous examples; it loads and normalizes the data so that the
    values are between 0.0 and 1.0:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将遵循类似的过程，并再次使用`MINST`数据集。以下来自`Chapter8/encoder.R`的代码加载数据。我们将使用一半的数据来训练自编码器，另一半将用于构建分类模型，以评估自编码器在降维方面的效果。代码的第一部分与之前的示例相似；它加载并归一化数据，使得数值介于0.0和1.0之间：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we will move on to our first auto-encoder. We will use `16` hidden neurons
    in our auto-encoder and use tanh as the activation function. We use 20% of our
    data as validation to provide an unbiased estimate of how the auto-encoder performs.
    Here is the code. To keep it concise, we are only showing part of the output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入我们的第一个自编码器。我们将在自编码器中使用`16`个隐藏神经元，并使用tanh作为激活函数。我们使用20%的数据作为验证集，以便提供自编码器表现的无偏估计。以下是代码。为了简洁起见，我们只显示部分输出：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The validation loss is `0.0275`, which shows that the model is performing quite
    well. Another nice feature is that if you run the code in RStudio, it will show
    the training metrics in graphs, which will automatically update as the model is
    trained. This is shown in the following screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 验证损失为`0.0275`，这表明模型表现相当好。另一个不错的特点是，如果你在RStudio中运行代码，它会在图形中显示训练指标，并在模型训练过程中自动更新。这在以下截图中有所展示：
- en: '![](img/b5df9a51-7449-4d84-9866-a936851eaa41.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5df9a51-7449-4d84-9866-a936851eaa41.png)'
- en: 'Figure 9.3: Model metrics showing in the Viewer pane in RStudio'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：RStudio中查看器面板显示的模型指标
- en: 'Once the model has completed training, you can also plot the model architecture
    and model metrics using the following code (the output is also included). By calling
    the plot function, you can see the plots for the accuracy and the loss on the
    training and validation datasets:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型完成训练，你还可以使用以下代码绘制模型架构和模型指标（输出也包含在内）。通过调用plot函数，你可以查看训练集和验证集上的准确率和损失图表：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code produces the following plot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了以下图表：
- en: '![](img/b87e1342-5aa0-4420-9dd2-5f4ee19de801.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b87e1342-5aa0-4420-9dd2-5f4ee19de801.png)'
- en: 'Figure 9.4: Auto-encoder model metrics'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：自编码器模型指标
- en: 'The preceding plots shows that the validation accuracy is relatively stable,
    but it probably peaked after epoch 20\. We will now train a second model with
    `32` hidden nodes instead in the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表显示，验证准确率相对稳定，但它可能在第20个周期后已经达到峰值。现在，我们将用`32`个隐藏节点训练第二个模型，代码如下：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Our validation loss has improved to `0.0175`, so let''s try `64` hidden nodes:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证损失已经改善至`0.0175`，接下来我们试试`64`个隐藏节点：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our validation loss here is `0.0098`, which again is an improvement. We have
    probably got to the stage where adding more hidden nodes will cause the model
    to overfit because we are only using `16800` rows to train the autoencoder. We
    could look at applying regularization, but since our first models have an accuracy
    of `0.01`, we are doing well enough.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证损失为`0.0098`，这再次表明有所改进。我们可能已经到达了一个阶段，在这个阶段增加更多隐藏节点会导致模型过拟合，因为我们只使用了`16800`行数据来训练自编码器。我们可以考虑应用正则化，但由于我们的第一个模型准确率为`0.01`，所以表现已经足够好。
- en: Accessing the features of the auto-encoder model
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问自编码器模型的特征
- en: 'We can extract the deep features from the model, that is, the values for the
    hidden neurons in the model. For this, we will use the model with 16 hidden nodes.
    We will examine the distribution of correlations using the `ggplot2` package,
    as shown in the following code. The results are shown in *Figure 9.5*. The deep
    features have small correlations, that is, usually with an absolute value of *<.20*.
    This is what we expect in order for the auto-encoder to work. This means that
    the features should not duplicate information between them:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从模型中提取深度特征，也就是模型中隐藏神经元的值。为此，我们将使用具有16个隐藏节点的模型。我们将使用`ggplot2`包检查相关性的分布，如以下代码所示。结果显示在*图9.5*中。深度特征之间的相关性较小，也就是说，通常其绝对值小于*<.20*。这是我们期望的情况，以确保自编码器正常工作。这意味着特征之间不应重复信息：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code produces the following plot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下图表：
- en: '![](img/03a95bd7-b69b-41c5-8f33-375fb78d513e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03a95bd7-b69b-41c5-8f33-375fb78d513e.png)'
- en: 'Figure 9.5: Correlation between weights in the hidden layer of the auto-encoder'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：自编码器隐藏层权重之间的相关性
- en: 'In [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training a Prediction
    Model*, we used PCA for dimensionality reduction and found that for a binary classification
    task of telling the difference between 5 and 6, we could still get 97.86% accuracy,
    even if we only used 23 features as input. These 23 features were the 23 **principal
    components** and accounted for 50% of the variance in our dataset. We will use
    the weights in the auto-encoder to perform the same experiment. Note that we trained
    the auto-encoder on 50% of the data, and that we are using the other 50% of the
    data for the binary classification task, that is, we do not want to try and build
    a classification task on data that was used to build the auto-encoder:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)《训练预测模型》中，我们使用PCA进行降维，发现即使仅使用23个特征作为输入，二分类任务中区分5和6的准确率仍然可以达到97.86%。这23个特征是**主成分**，并且它们占据了数据集中50%的方差。我们将使用自编码器中的权重来执行相同的实验。请注意，我们在50%的数据上训练了自编码器，并且使用剩下的50%数据进行二分类任务，即我们不想尝试在用于构建自编码器的数据上进行分类任务：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our model gets `97.96%` accuracy, which is a slight improvement on the `97.86%`
    accuracy we achieved in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml), *Training
    a Prediction Model*. It is not really a surprise that the two models are very
    similar as the mathematical foundations for PCA involves matrix decomposition,
    while the auto-encoder uses back-propagation to set the matrix weights for the
    hidden layer. In fact, if we dropped the non-linear activation function, our encodings
    would be very similar to PCA. This demonstrates that auto-encoder models can be
    used effectively as a dimensionality reduction technique.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型达到了`97.96%`的准确率，略高于在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)《训练预测模型》中获得的`97.86%`的准确率。事实上，这两个模型非常相似并不令人惊讶，因为PCA的数学基础涉及矩阵分解，而自编码器则使用反向传播来设置隐藏层的矩阵权重。实际上，如果我们去掉非线性激活函数，我们的编码结果将非常类似于PCA。这表明，自编码器模型可以有效地用作降维技术。
- en: Using auto-encoders for anomaly detection
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器进行异常检测
- en: 'Now that we have built an auto-encoder and accessed the features of the inner
    layers, we will move on to an example of how auto-encoders can be used for anomaly
    detection. The premise here is quite simple: we take the reconstructed outputs
    from the decoder and see which instances have the most error, that is, which instances
    are the most difficult for the decoder to reconstruct. The code that is used here
    is in `Chapter9/anomaly.R`, and we will be using the `UCI HAR` dataset that we
    have already been introduced to in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml),
    *Training a Prediction Model*. If you have not already downloaded the data, go
    back to that chapter for instructions on how to do so.. The first part of the
    code loads the data, and we subset the features to only use the ones with mean,
    sd, and skewness in the feature names:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经构建了自编码器并访问了内部层的特征，接下来我们将介绍自编码器如何用于异常检测的示例。这里的前提非常简单：我们从解码器中获取重构输出，并查看哪些实例的误差最大，也就是说，哪些实例是解码器最难重构的。这里使用的代码位于`Chapter9/anomaly.R`，并且我们将使用已经在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)《训练预测模型》中介绍过的`UCI
    HAR`数据集。如果你还没有下载数据，可以回到该章节查看如何下载数据。代码的第一部分加载了数据，我们对子集特征进行了筛选，仅使用了名称中包含均值、标准差和偏度的特征：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can build our auto-encoder model. This is going to be a stacked auto-encoder
    with two `40` neuron hidden encoder layers and two 40-neuron hidden decoder layers.
    For conciseness, we have removed some of the output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建我们的自编码器模型。这个模型将是一个堆叠自编码器，包含两个`40`个神经元的隐藏编码器层和两个40个神经元的隐藏解码器层。为了简洁起见，我们省略了部分输出：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see the layers and number of parameters for the model by calling the
    summary function, like so:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用summary函数来查看模型的层次结构和参数数量，如下所示：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Our validation loss is `0.0088`, which means that our model is good at encoding
    the data. Now, we will use the test set on the auto-encoder and get the reconstructed
    data. This will create a dataset with the same size as the test set. We will then
    select any instance where the sum of the squared error (se) between the predicted
    values and the test set is greater than 4.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的验证损失是`0.0088`，这意味着我们的模型在编码数据方面表现良好。现在，我们将使用测试集对自编码器进行测试，并获得重构的数据。这将创建一个与测试集大小相同的数据集。然后，我们将选择任何预测值与测试集之间的平方误差（se）和大于4的实例。
- en: These are the instances that the auto-encoder had the most trouble in reconstructing,
    and therefore they are potential anomalies. The limit value of 4 is a hyperparameter;
    if it is set higher, fewer potential anomalies are detected and if it is set lower,
    more potential anomalies are detected. This value would be different according
    to the dataset used.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是自编码器在重构时遇到最多困难的实例，因此它们是潜在异常。4的限制值是一个超参数；如果设置得更高，则检测到的潜在异常较少；如果设置得更低，则检测到的潜在异常较多。这个值会根据所使用的数据集而有所不同。
- en: There are 6 classes in this dataset. We want to analyze if the anomalies are
    spread over all of our classes or if they are specific to some classes. We will
    print out a table of the frequencies of our classes in our test set, and we will
    see that the distribution of our classes is fairly even. When printing out a table
    of the frequencies of our classes of our potential anomalies, we can see that
    most of them are in the `WALKING_DOWNSTAIRS` class. The potential anomalies are
    shown in *Figure 9.6:*
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含6个类别。我们想分析异常是否分布在所有类别中，还是仅限于某些类别。我们将打印出测试集中各类别的频率表，并且可以看到各类别的分布相对均匀。当打印出潜在异常类别的频率表时，我们可以看到大多数异常集中在`WALKING_DOWNSTAIRS`类别中。潜在异常如*图9.6所示：*
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can plot this with the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码进行绘制：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/85752c3f-a60c-42ca-9ac1-6f3f8d3e92f8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85752c3f-a60c-42ca-9ac1-6f3f8d3e92f8.png)'
- en: 'Figure 9.6: Distribution of the anomalies'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：异常的分布
- en: In this example, we used a deep auto-encoder model to learn the features of
    actimetry data from smartphones. Such work can be useful for excluding unknown
    or unusual activities, rather than incorrectly classifying them. For example,
    as part of an app that classifies what activity you engaged in for how many minutes,
    it may be better to simply leave out a few minutes where the model is uncertain
    or the hidden features do not adequately reconstruct the inputs, rather than to
    aberrantly call an activity walking or sitting when it was actually walking downstairs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用了深度自编码器模型来学习来自智能手机的动作数据特征。这样的工作对于排除未知或不寻常的活动非常有用，而不是错误地将它们分类。例如，在一个应用程序中，分类你进行了什么活动以及持续了多少分钟，可能更好的是直接忽略模型不确定的几分钟，或者隐藏特征没有充分重构输入的情况，而不是错误地将活动分类为走路或坐着，而实际上是下楼走路。
- en: Such work can also help to identify where the model tends to have more issues.
    Perhaps further sensors and additional data are needed to represent walking downstairs
    or more could be done to understand why walking downstairs tends to produce relatively
    high error rates.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的工作也有助于识别模型可能存在问题的地方。或许需要额外的传感器和数据来表示下楼走路，或者需要更多工作来理解为什么下楼走路会产生相对较高的错误率。
- en: These deep auto-encoders are also useful in other contexts where identifying
    anomalies is important, such as with financial data or credit card usage patterns.
    Anomalous spending patterns may indicate fraud or that a credit card has been
    stolen. Rather than attempt to manually search through millions of credit card
    transactions, one could train an auto-encoder model and use it to identify anomalies
    for further investigation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些深度自编码器在其他需要识别异常的场景中也非常有用，例如金融数据或信用卡使用模式。异常的消费模式可能表明存在欺诈或信用卡被盗。与其尝试手动搜索数百万次信用卡交易，不如训练一个自编码器模型，并用它来识别异常以进行进一步调查。
- en: Use case – collaborative filtering
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 – 协同过滤
- en: This use-case is about collaborative filtering. We are going to build a recommendation
    system based on embeddings created from a deep learning model. To do this, we
    are going to use the same dataset we used in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, which is the retail transactional database. If you have
    not already downloaded the database, then go to the following link, [https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles),
    and select *Let’s Get Sort-of-Real*. Select the option for the smallest dataset,
    titled *All transactions for a randomly selected sample of 5,000 customers*. Once
    you have read the terms and conditions and downloaded the dataset to your computer,
    unzip it into a directory called `dunnhumby/in` under the code folder. Ensure
    that the files are unzipped directly under this folder, and not a subdirectory,
    as you may have to copy them after unzipping the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本用例是关于协同过滤的。我们将基于从深度学习模型创建的嵌入（embeddings）来构建一个推荐系统。为此，我们将使用在[第 4 章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)中使用的相同数据集，*训练深度预测模型*，即零售交易数据库。如果你还没有下载数据库，可以访问以下链接，[https://www.dunnhumby.com/sourcefiles](https://www.dunnhumby.com/sourcefiles)，并选择*Let’s
    Get Sort-of-Real*。选择名为*为 5,000 名顾客随机抽样的所有交易*的最小数据集选项。在阅读了条款并下载了数据集后，将其解压到代码文件夹下名为
    `dunnhumby/in` 的目录中。确保文件直接解压到该文件夹下，而不是子目录，因为解压后你可能需要复制它们。
- en: 'The data contains details of retail transactions linked by basket IDs. Each
    transaction has a date and a store code, and some are also linked to customers.
    Here are the fields that we will use in this analysis:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含通过购物篮 ID 关联的零售交易详情。每笔交易都有一个日期和商店代码，部分交易还与顾客关联。以下是我们将在本次分析中使用的字段：
- en: '| **Field-name** | **Description** | **Format** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **字段名** | **描述** | **格式** |'
- en: '| `CUST_CODE` | Customer Code. This links the transactions/visits to a customer.
    | Char |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `CUST_CODE` | 顾客代码。此字段将交易/访问与顾客关联。 | 字符 |'
- en: '| `SPEND` | Spend associated to the items bought. | Numeric |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `SPEND` | 与所购商品相关的花费。 | 数值 |'
- en: '| `PROD_CODE` | Product Code. | Char |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE` | 产品代码。 | 字符 |'
- en: '| `PROD_CODE_10` | Product Hierarchy Level 10 Code. | Char |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_10` | 产品层级 10 代码。 | 字符 |'
- en: '| `PROD_CODE_20` | Product Hierarchy Level 20 Code. | Char |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_20` | 产品层级 20 代码。 | 字符 |'
- en: '| `PROD_CODE_30` | Product Hierarchy Level 30 Code. | Char |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_30` | 产品层级 30 代码。 | 字符 |'
- en: '| `PROD_CODE_40` | Product Hierarchy Level 40 Code. | Char |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `PROD_CODE_40` | 产品层级 40 代码。 | 字符 |'
- en: If you want more details on the structure of the files, you can go back and
    re-read the use case in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*. We are going to use this dataset to create a recommendation
    engine. There are a family of machine learning algorithms called **Market Basket
    Analysis** that can be used with transactional data, but this use case is based
    on collaborative filtering. Collaborative filtering are recommendations based
    on the ratings people give to products. They are commonly used for music and film
    recommendations, where people rate the items, usually on a scale of 1-5\. Perhaps
    the best known recommendation system is Netflix because of the Netflix prize ([https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于文件结构的细节，可以回头重新阅读[第 4 章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)中的用例，*训练深度预测模型*。我们将使用这个数据集来创建推荐引擎。这里有一类叫做**市场购物篮分析**的机器学习算法，可以与交易数据一起使用，但本用例基于协同过滤。协同过滤是一种基于人们对产品评分的推荐方法。它们通常用于音乐和电影推荐，用户会对物品进行评分，通常是
    1 到 5 分。也许最著名的推荐系统是 Netflix，因为 Netflix 奖（[https://en.wikipedia.org/wiki/Netflix_Prize](https://en.wikipedia.org/wiki/Netflix_Prize)）。
- en: 'We are going to use our dataset to create implicit rankings of how much a customer
    *rates* an item. If you are not familiar with implicit rankings, then they are
    rankings that are derived from data rather than explicitly assigned by the user.
    We will use one of the product codes, `PROD_CODE_40`, and calculate the quantiles
    of the spend for that product code. The quantiles will divide the fields into
    5 roughly equally sized groups. We will use these to assign a rating to each customer
    for that product based on how much they spent on that product code. The top 20%
    of customers will get a rating of 5, the next 20% will get a rating of 4, and
    so on. Each customer/product code combination that exists will have a rating from
    1-5:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们的数据集来创建隐式排名，评估客户如何*评价*某个商品。如果你不熟悉隐式排名，它们是通过数据推导出的排名，而不是用户显式指定的排名。我们将使用一个产品代码，`PROD_CODE_40`，并计算该产品代码的消费分位数。分位数将把字段划分为大致相等的5个组。我们将用这些分位数为每个客户基于他们对该产品代码的消费额分配一个评分。排名前20%的客户将获得评分5，接下来的20%将获得评分4，以此类推。每个存在的客户/产品代码组合将会有一个从1到5的评分：
- en: There is a rich history of using quantiles in retail loyalty systems. One of
    the earliest segmentation approaches for retail loyalty data was called **RFM
    analysis**. RFM is an acronym for Recency, Frequency, and Monetary spend. It gives
    each customer a ranking 1 (lowest) – 5 (highest) on each of these categories,
    with an equal number of customers in each ranking. For *Recency*, the 20% of the
    customers that visited most recently would be given a 5, the next 20% would be
    given a 4, and so on. For *Frequency*, the top 20% of customers with the most
    transactions would be given a 5, the next 20% would be given a 4, and so on. Similarly
    for *Monetary* spend, the top 20% of the customers by revenue would be given a
    5, the next 20% would be given a 4, and so on. The numbers would then be concatenated,
    so a customer with an RFM of 453 would be 4 for Recency, 5 for Frequency, and
    3 for Monetary spend. Once the score has been calculated, it can be used for many
    purposes, for example, cross-sell, churn analysis, and so on. RFM analysis was
    very popular in the late 1990's / early 2000's with many marketing managers because
    it is easily implemented and well-understood. However, it is not flexible and
    is being replaced with machine learning techniques.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在零售忠诚度系统中使用分位数有着丰富的历史。零售忠诚度数据的早期细分方法之一被称为**RFM分析**。RFM是Recency、Frequency和Monetary支出的首字母缩写。它为每个客户在这些类别中打分，1为最低，5为最高，每个评分类别中包含相同数量的客户。对于*Recency*，最近访问的20%客户会得到评分5，接下来的20%得到评分4，以此类推。对于*Frequency*，交易最多的20%客户会得到评分5，接下来的20%得到评分4，以此类推。类似地，对于*Monetary*支出，根据收入排名前20%的客户会得到评分5，接下来的20%得到评分4，以此类推。最终将这些分数连接在一起，因此一个RFM为453的客户在Recency上是4，Frequency上是5，Monetary支出上是3。一旦计算出评分，它可以用于很多目的，例如交叉销售、客户流失分析等。RFM分析在90年代末和2000年代初非常流行，许多营销经理都喜欢使用它，因为它易于实施且理解容易。然而，它不够灵活，正在被机器学习技术所取代。
- en: Preparing the data
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'The code to create our ratings is in `Chapter9/create_recommend.R`. The first
    part of the code runs through the raw transactional data. The data is in separate
    CSV files, so it processes each file, selects the records that have a customer
    linked (that is, `CUST_CODE!=""`) to them, and then groups the sales by `CUST_CODE`
    and `PROD_CODE_40`. It then appends the results to a temporary file and moves
    on to the next input file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建评分的代码在`Chapter9/create_recommend.R`中。代码的第一部分处理原始交易数据。数据保存在不同的CSV文件中，所以它会处理每个文件，选择与客户关联的记录（即，`CUST_CODE!=""`），然后按`CUST_CODE`和`PROD_CODE_40`分组销售数据。接着将结果附加到临时文件中，然后继续处理下一个输入文件：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This section groups by customer and product code for the `117` input files.
    As we process each file, we rename the customer code to `cust_id` and the product
    department code to `prod_id`. Once we are done, the combined file will obviously have
    duplicate customer-product code combinations; that is, we need to group again
    over the combined data. We do that by opening up the temporary file and grouping
    over the fields again:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本节按客户和产品代码对`117`个输入文件进行分组。在处理每个文件时，我们将客户代码重命名为`cust_id`，产品部门代码重命名为`prod_id`。处理完成后，合并的文件显然会有重复的客户-产品代码组合；也就是说，我们需要再次对合并的数据进行分组。我们通过打开临时文件，再次对字段进行分组来实现这一点：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We could have tried to load all of the transactional data and run a group on
    that data, but that would have been memory and computationally expensive. By running
    it in two steps, we reduce the amount of data we need to process at each stage,
    which means it is more likely to run on machines with limited memory.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以尝试加载所有交易数据并对其进行分组，但那样会非常占用内存和计算资源。通过分两步进行处理，我们减少了每个阶段需要处理的数据量，这意味着在内存有限的机器上运行的可能性更大。
- en: 'Once we have the total spend for each customer and product department code
    combination, we can create the ratings. Thanks to the excellent `tidyr` packages,
    it only takes a few lines to assign a rating to each row. First, we group by the
    `prod_id` field, and use the quantile function to return quantiles for the sales
    for each product code. These quantiles will return the sales ranges that correspond
    to splitting the customers into `5` equal sized groups. We then use these quantiles
    to assign rankings:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了每个客户和产品部门代码组合的总支出，我们就可以创建评分。得益于优秀的`tidyr`包，只需要几行代码就能为每一行分配评分。首先，我们按照`prod_id`字段进行分组，并使用分位数函数返回每个产品代码的销售分位数。这些分位数将返回将客户分成`5`个等大小组的销售范围。然后，我们使用这些分位数来分配排名：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The only thing remaining is to save the results. Before we do, we do a couple
    of sanity checks to ensure that our ratings are evenly distributed from 1-5 overall.
    We then select a random product code and check that our ratings are evenly distributed
    from 1-5 for those products:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一剩下的就是保存结果。在我们执行之前，我们做了几个基本检查，确保我们的评分从1到5分布均匀。然后，我们随机选择一个产品代码，并检查该产品的评分是否从1到5均匀分布：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Everything looks good here: the count for `rating=1` is higher at `68246` against
    `62162` to `63682` for ratings `2` to `5`, but that is not really a concern as
    collaborative filtering models do not expect an even distribution of ratings.
    For the individual item (`D00008`), the distribution is even at `596` or `597`
    for each rating.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一切看起来都很好：`rating=1`的数量为`68246`，而`2`到`5`的评分范围为`62162`到`63682`，但这并不是什么问题，因为协同过滤模型并不要求评分分布均匀。对于单个商品（`D00008`），每个评分的分布均匀，分别为`596`或`597`。
- en: Building a collaborative filtering model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建协同过滤模型
- en: 'Before we jump into applying a deep learning model, we should follow the same
    practice as we have done in previous chapters and create a benchmark accuracy
    score using a standard machine learning algorithm. It is quick, easy, and will
    give us confidence that our deep learning model is working better than just using
    *normal* machine learning. Here are the 20 lines of code to do collaborative filtering
    in R. This code can be found in `Chapter8/ml_recommend.R`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始应用深度学习模型之前，我们应该按照前几章的做法，使用标准的机器学习算法创建一个基准准确度评分。这很快，容易实现，并且能让我们确信深度学习模型的效果比单纯使用*普通*机器学习要好。以下是用R语言实现协同过滤的20行代码。这个代码可以在`Chapter8/ml_recommend.R`中找到：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This code creates a collaborative filtering model, and the MSE for the model
    is `0.9748`. As before, we do this because most of the work for this sample is
    in data preparation and not model building, so it is relatively easy to use a
    base machine learning algorithm to compare the performance against a deep learning
    model. The code here uses standard R libraries to create a recommendation system,
    and as you can see, it is relatively simple because the data is already in the
    expected format. If you want more information on this collaborative filtering
    algorithm, then search for `user based collaborative filtering in r`, or go through
    the doc pages.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了一个协同过滤模型，且该模型的均方误差（MSE）为`0.9748`。和之前一样，我们这样做是因为这个示例的大部分工作都集中在数据准备上，而非模型构建，因此，使用基础机器学习算法进行比较并不困难。这里的代码使用了标准的R库来创建推荐系统，正如你所见，这相对简单，因为数据已经处于预期格式。如果你想了解更多关于这个协同过滤算法的信息，可以搜索`user
    based collaborative filtering in r`，或者查看文档页面。
- en: Now lets focus on creating a deep learning model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于创建深度学习模型。
- en: Building a deep learning collaborative filtering model
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度学习协同过滤模型
- en: 'Here, we will see if we can build a deep learning model to beat the previous
    approach! The following code is in `Chapter9/keras_recommend.R`. The first part
    loads the dataset and creates new IDs for the customer and product codes. This
    is because Keras expects the indexes to be sequential, starting at zero, and unique:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看看是否能构建一个深度学习模型来超越之前的方法！以下代码位于`Chapter9/keras_recommend.R`。第一部分加载数据集并为客户和产品代码创建新的
    ID。这是因为 Keras 期望索引是顺序的，从零开始，并且唯一：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We have 5,000 unique customers and 9 unique product codes. This is not typical
    of most collaborative filtering examples; usually, the number of products is much
    higher than the number of customers. The next part creates the model. We will
    create embedding layers for the customer and the products and then calculate the
    dot product of those embedding layers. An embedding layer is a lower-order representation
    of the data and is exactly the same as the encoders in the auto-encoder examples
    we saw earlier. We will also have a bias term for each customer and product –
    this performs a sort of normalization on the data. If a particular product is
    very popular, or a customer has a lot of high ratings, this accounts for this.
    We will use 10 factors in our embedding layer for both customers and products.
    We will use some L2 regularization in our embeddings to prevent overfitting. The
    following code defines the model architecture:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 5000 个独特的客户和 9 个独特的产品代码。这与大多数协同过滤的例子不同；通常情况下，产品的数量要远高于客户的数量。接下来的部分是创建模型。我们将为客户和产品创建嵌入层，然后计算这些嵌入层的点积。嵌入层是数据的低阶表示，正如我们之前看到的自动编码器中的编码器一样。我们还将为每个客户和产品设置一个偏置项——这对数据进行了一种归一化处理。如果某个产品非常流行，或者某个客户有很多高评分，这将对此进行补偿。我们将在嵌入层中使用
    10 个因素来表示客户和产品。我们将在嵌入层中使用一些 L2 正则化，以防止过拟合。以下代码定义了模型架构：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we are ready to build the model. We are going to hold out 10% of our data
    for validation:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好构建模型了。我们将从数据中抽取 10% 用于验证：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Our model achieved an MSE of `0.9508`, which is an improvement on the MSE of `0.9748`
    that we got on our machine learning model. Our deep learning model is overfitting,
    but one reason for this is because we have a relatively small database. I tried
    increasing the regularization, but this did not improve the model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型达到了 `0.9508` 的 MSE，这比我们在机器学习模型中得到的 `0.9748` 的 MSE 有了改进。我们的深度学习模型存在过拟合问题，但其中一个原因是因为我们的数据库相对较小。我尝试增加正则化，但这并没有改善模型。
- en: Applying the deep learning model to a business problem
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习模型应用于商业问题
- en: Now that we have a model, how can we use it? The most typical example of using
    a collaborative filtering model is to recommend items to people they have not
    rated yet. That concept works well in domains such as music and movie recommendations
    where collaborative filtering models are often applied. However, we are going
    to use it for a different purpose. One concern of marketing managers is the **Share
    of wallet** they get from a customer. The definition of this (from [https://en.wikipedia.org/wiki/Share_of_wallet](https://en.wikipedia.org/wiki/Share_of_wallet))
    is the *percentage ('share') of a customer's expenses ('of wallet') for a product
    that goes to the firm selling the product*. It basically measures the value of
    a customer on the percentage of the potential spend they could have with us. As
    an example, we may have customers who visit our shop regularly and spend a considerable
    amount. But are they buying all of their goods from us? Maybe they buy their fresh
    food elsewhere, that is, they purchase their meat, fruit, vegetables, and so on, at
    other stores. We can use collaborative filtering to find customers where the collaborative
    filtering model predicts that they purchase certain products in our store, but
    in fact they do not. Remember that collaborative filtering works on the basis
    of making recommendations based on what other similar customers do. So, if customer
    A does not purchase meat, fruit, vegetables, and so on, at our store when other
    similar customers do, then we could try and entice them to spend more at our stores
    by sending them offers for these products.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个模型，我们如何使用它呢？协同过滤模型的最典型应用是向用户推荐他们还未评分的商品。这个概念在音乐和电影推荐等领域效果很好，协同过滤模型通常在这些领域中得到应用。然而，我们将把它用于不同的目的。市场营销经理关心的一个问题是他们从客户那里获得的**钱包份额**。这个定义（来自[https://en.wikipedia.org/wiki/Share_of_wallet](https://en.wikipedia.org/wiki/Share_of_wallet)）是指*客户对某产品的支出（‘钱包’的份额）中，分配给销售该产品的公司的百分比*。它基本上衡量了一个客户在我们这里可能支出的百分比，从而评估该客户的价值。举个例子，我们可能有一些客户定期访问我们的商店并消费相当一部分金额。但他们是否从我们这里购买了所有商品？也许他们把新鲜食品从其他地方购买，也就是说，他们在其他商店购买肉类、水果、蔬菜等。我们可以使用协同过滤来找到那些模型预测他们在我们商店购买某些商品，但实际上他们并没有这样做的客户。记住，协同过滤是基于其他相似客户的行为来推荐的。因此，如果客户A在我们商店没有像其他相似客户那样购买肉类、水果、蔬菜等，我们可以通过向他们发送这些产品的优惠来尝试吸引他们在我们的商店消费更多。
- en: We will look for customer-product department codes where the prediction is greater
    than 4, but the actual value is less than 2\. These customers should be purchasing
    these goods from us (according to the model), so by sending them vouchers for
    items in these departments, we can capture a greater amount of their spending.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将寻找预测值大于4但实际值小于2的客户-产品部门代码。这些客户应该按照模型在我们这里购买这些商品，因此通过向他们发送这些部门商品的优惠券，我们可以捕获更多的消费额。
- en: 'A collaborative filtering model should work well for this type of analysis.
    The basis of this algorithm is to find the recommend products based on the activity
    of similar customers, so it already adjusts for the scale of spend. For example,
    if the prediction for a customer is that their spend on fresh fruit and vegetables
    should be 5, that is based on the comparison with other similar customers. Here
    is the evaluation code, which is also in `Chapter8/kerarecommend.R`. The first
    part of the code generates the predictions and links it back. We output a few
    metrics, which look impressive, but note that they are run on all the data, including
    the data that the model was trained on, so these metrics are overly optimistic.
    We make one adjustment to the predictions – some of these values are greater than
    5 or less than 1, so we change them back to valid values. This produces a very
    small improvement on our metrics:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤模型应该非常适合这种类型的分析。该算法的基础是根据相似客户的活动来推荐产品，因此它已经调整了消费规模。例如，如果对某个客户的预测是他们在新鲜水果和蔬菜上的消费应该为5，那是基于与其他相似客户的比较。以下是评估代码，它也在`Chapter8/kerarecommend.R`中。代码的第一部分生成预测并将其链接回去。我们输出了一些指标，这些指标看起来很有说服力，但请注意，它们是在所有数据上运行的，包括模型训练时使用的数据，因此这些指标过于乐观。我们对预测进行了一些调整——有些值大于5或小于1，因此我们将它们调整回有效值。这对我们的指标产生了非常小的改善：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we can look at the customer-product department codes that have the biggest
    difference between predicted ratings and actual ratings:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查看那些预测评分与实际评分差距最大的客户-产品部门代码：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This gives us a list of customers and the products we should send them offers
    for. For example, for the second row, the actual rating is `1` and the predicted
    rating is `4.306837`. This customer is not purchasing the items for this product
    code and our model *predicts* he should be purchasing these items.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一份客户清单，以及我们应该向他们提供优惠的产品。例如，在第二行中，实际评分是 `1`，而预测评分是 `4.306837`。该客户并没有购买此产品代码的商品，而我们的模型
    *预测* 他应该购买这些商品。
- en: 'We can also look at cases where the actual rating is much higher than the predicted
    value. These are customers who are over-spending in that department compared to
    other similar customers:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看实际评分远高于预测值的案例。这些是与其他相似客户相比，在该部门过度消费的客户：
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: What can we do with these recommendations? Our model assigns a score of 1-5
    based on a customers' spend in each product department, so if a customer has a
    high actual rating compared to the predicted value in general, they are over-spending
    in these departments compared to similar customers. These people are probably
    not spending in other departments, so they should be targeted as part of a cross-sell
    campaign; that is, they should be sent offers for products in other departments
    to tempt them to purchase there.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如何利用这些推荐？我们的模型根据客户在每个产品部门的消费情况为其打分（1-5），因此如果客户的实际评分与预测值相比普遍较高，说明他们在这些部门的消费超出了与之类似的客户。那些人可能在其他部门没有消费，所以他们应该作为交叉销售活动的目标；也就是说，应该向他们提供其他部门的产品优惠，以吸引他们在那里购买。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: I hope that this chapter has shown you that deep learning is not just about
    computer vision and NLP problems! In this chapter, we covered using Keras to build
    auto-encoders and recommendation systems. We saw that auto-encoders can be used
    as a form of dimensionality reduction and, in their simplest forms with only one
    layer, they are similar to PCA. We used an auto-encoder model to create an anomaly
    detection system. If the reconstruction error in the auto-encoder model was over
    a threshold, then we marked that instance as a potential anomaly. Our second major
    example in this chapter built a recommendation system using Keras. We constructed
    a dataset of implicit ratings from transactional data and built a recommendation
    system. We demonstrated the practical application of this model by showing you how
    it could be used for cross-sell purposes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本章能够让你明白，深度学习不仅仅是关于计算机视觉和自然语言处理的问题！在这一章中，我们讲解了如何使用 Keras 构建自编码器和推荐系统。我们看到自编码器可以用作一种降维方法，而且在其最简单的形式中，只有一层时，它们与主成分分析（PCA）相似。我们使用自编码器模型创建了一个异常检测系统。如果自编码器模型中的重构误差超过某个阈值，我们将该实例标记为潜在的异常。我们在本章中的第二个主要示例构建了一个使用
    Keras 的推荐系统。我们通过交易数据构建了一个隐式评分的数据集，并建立了推荐系统。我们通过展示该模型如何用于交叉销售目的，演示了其实际应用。
- en: In the next chapter, we will look at various options for training your deep
    learning model in the cloud. If you do not have a GPU on your local machine, cloud
    providers such as AWS, Azure, Google Cloud, and Paperspace allow you to access
    GPU instances cheaply. We will cover all of these options in the next chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论在云端训练深度学习模型的各种选项。如果你的本地机器没有 GPU，像 AWS、Azure、Google Cloud 和 Paperspace
    等云服务提供商允许你以低廉的价格访问 GPU 实例。我们将在下一章中介绍这些选项。
