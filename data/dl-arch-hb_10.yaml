- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Exploring Model Evaluation Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索模型评估方法
- en: 'A trained deep learning model without any form of validation cannot be deployed
    to production. Production, in the context of the machine learning software domain,
    refers to the deployment and operation of a machine learning model in a live environment
    for actual consumption of its predictions. More broadly, model evaluation serves
    as a critical component in any deep learning project. Typically, a deep learning
    project will result in many models being built, and a final model will be chosen
    to serve in a production environment. A good model evaluation process for any
    project leads to the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一个没有经过任何验证的训练深度学习模型不能部署到生产环境中。在机器学习软件领域，生产指的是将机器学习模型部署到真实环境中并实际使用其预测结果。更广泛地说，模型评估在任何深度学习项目中都起着至关重要的作用。通常，一个深度学习项目会构建多个模型，最终选择一个模型在生产环境中使用。任何项目的良好模型评估过程都能带来以下结果：
- en: A better-performing final model through model comparisons and metrics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过模型比较和评估指标得到更好的最终模型性能
- en: Fewer production prediction mishaps by understanding common model pitfalls
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过了解常见的模型陷阱，减少生产预测中的错误
- en: More closely aligned practitioner and final model behaviors through model insights
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过模型洞察使得实践者与最终模型的行为更加一致
- en: A higher probability of project success through success metric evaluation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过成功指标评估提高项目成功的概率
- en: A final model that is less biased and fairer and produces more trusted predictions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个偏差更小、更公平并且能够产生更可信预测的最终模型
- en: 'Generally, the model evaluation process leads to more informed decisions across
    the entire machine learning life cycle. In this first chapter of *Part 2* of the
    book, we will discuss all the categories of model evaluation that will help to
    achieve these benefits. Additionally, we will dive deep into some of these categories
    in the next four chapters, which all belong to *Part 2* of the book. Specifically,
    we will cover the following topics in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型评估过程能在整个机器学习生命周期中帮助做出更明智的决策。在本书*第二部分*的第一章中，我们将讨论所有有助于实现这些收益的模型评估类别。此外，在接下来的四章中，我们将深入探讨这些类别，这些章节都属于本书的*第二部分*。具体来说，我们将在本章讨论以下主题：
- en: Exploring the different model evaluation methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同的模型评估方法
- en: Engineering the base model evaluation metric
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基础模型评估指标
- en: Exploring custom metrics and their applications
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索自定义指标及其应用
- en: Exploring statistical tests for comparing model metrics
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索用于比较模型指标的统计测试
- en: Relating the evaluation metric to success
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将评估指标与成功关联起来
- en: Directly optimizing the metric
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接优化指标
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will have a practical implementation using the Python programming
    language. To complete it, you will only need to install the `matplotlib` library
    in Python.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过使用Python编程语言进行实际操作。要完成这一部分，你只需在Python中安装`matplotlib`库。
- en: The code files are available on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在GitHub上找到，链接为[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10)。
- en: Exploring the different model evaluation methods
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索不同的模型评估方法
- en: Most practitioners are familiar with accuracy-related metrics. This is the most
    basic evaluation method. Typically, for supervised problems, a practitioner will
    treat an accuracy-related metric as the golden source of truth. In the context
    of model evaluation, the term “accuracy metrics” is often used to collectively
    refer to various performance metrics such as accuracy, F1 score, recall, precision,
    and mean squared error. When coupled with a suitable cross-validation partitioning
    strategy, using metrics as a standalone evaluation strategy can go a long way
    in most projects. In deep learning, accuracy-related metrics are typically used
    to monitor the progress of the model at each epoch. The monitoring process can
    subsequently be extended to perform early stopping to stop training the model
    when it doesn’t improve anymore and to determine when to reduce the learning rate.
    Additionally, the best model weights can be loaded at the end of the training
    process defined by the weights that achieved the best metric score on the validation
    dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数实践者都熟悉与准确性相关的指标。这是最基本的评估方法。通常，对于监督学习问题，实践者会将准确性相关的指标视为黄金标准。在模型评估的背景下，“准确性指标”这一术语通常用于统称各种性能指标，如准确率、F1
    分数、召回率、精度和均方误差。结合适当的交叉验证划分策略，使用指标作为独立评估策略，在大多数项目中可以取得很好的效果。在深度学习中，准确性相关的指标通常用于监控每个周期中模型的进展。随后，监控过程可以扩展到执行早停，当模型不再改善时停止训练，并确定何时减少学习率。此外，在训练过程结束时，可以加载最佳的模型权重，这些权重是在验证数据集上取得最佳指标分数时的权重。
- en: 'Accuracy-related metrics alone do not provide a complete picture of a machine
    learning model’s capabilities and behavior. This is particularly true in unsupervised
    projects where accuracy metrics are superficial and only relevant to specific
    distributions. Gaining a more complete understanding of a model allows you to
    make more informed decisions across the machine’s life cycle. Some examples of
    the ways you can gain more understanding of the model includethe following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅依靠与准确性相关的指标并不能完整地反映一个机器学习模型的能力和行为。特别是在无监督项目中，准确性指标是表面性的，只对特定分布相关。对模型进行更全面的理解，可以让你在机器的生命周期中做出更明智的决策。以下是一些获得更深入理解模型的方式：
- en: Model insights can be a proxy to assess the accuracy of the data being used.
    If the data is deemed to be inaccurate or has some slight flaws, you can transition
    back into the data preparation stage of the machine learning life cycle.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型洞察可以作为评估所用数据准确性的代理。如果数据被认为是不准确的或存在一些轻微缺陷，可以重新进入机器学习生命周期中的数据准备阶段。
- en: In cases where bias is detected, the model may need to be retrained to remove
    the bias.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果检测到偏差，可能需要重新训练模型以去除偏差。
- en: It is also important to evaluate whether the model can recognize patterns in
    the way that domain experts do. If it cannot, a different model or data preparation
    method may be required.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型是否能够像领域专家一样识别模式同样很重要。如果不能，可能需要采用不同的模型或数据准备方法。
- en: It is necessary to consider whether the model can exhibit common sense in its
    predictions. If not, it may be necessary to apply special post-processing techniques
    to enforce common sense.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要考虑模型是否能够在预测中体现常识。如果不能，可能需要应用特殊的后处理技术来强制执行常识。
- en: Exposing other performance metrics such as inference speed and model size can
    be critical when choosing a model. You don’t want a model that is too slow or
    too big to fit into your targeted production environment.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露其他性能指标，如推理速度和模型大小，在选择模型时可能至关重要。你不希望选择一个过慢或过大的模型，导致无法适应目标生产环境。
- en: In addition to helping a project progress toward success, gathering insights
    from a model can also help identify potential issues early on. In the machine
    learning life cycle, which was introduced in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015),
    *Deep Learning Life Cycle*, it is important to remember that projects may fail
    during planning or when delivering model insights. Failing is a natural part of
    the machine learning process and, in fact, many projects are not meant to succeed.
    This could be due to a variety of factors, such as data engineering not being
    suited for machine learning or the task being too complex. However, it is important
    to fail fast and dump the project in order to be able to redirect resources to
    other use cases that have a higher chance of success. In cases where the project
    is critical and cannot be dumped, identifying the root cause of the failure quickly
    can improve the execution efficiency of the project by diverting resources to
    fix it and cyclically transitioning between the stages of the machine learning
    life cycle. In order for a project to fail fast, you have to have a responsible
    and confident way to be able to determine whether the model is not working. In
    summary, this ability to fail quickly can be very beneficial, as it saves time
    and resources that might have otherwise been wasted.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了帮助项目朝着成功方向发展外，从模型中收集洞见还可以帮助早期识别潜在问题。在[**第 1 章**](B18187_01.xhtml#_idTextAnchor015)中介绍的机器学习生命周期，*深度学习生命周期*，中，重要的是要记住，项目可能在规划阶段或在提供模型洞见时失败。失败是机器学习过程中的自然组成部分，事实上，许多项目注定不会成功。这可能是由于各种因素，如数据工程不适合机器学习或任务过于复杂。然而，重要的是要尽快失败并放弃该项目，从而将资源重新分配到更有成功机会的其他用例中。在项目至关重要且无法放弃的情况下，快速识别失败的根本原因可以通过将资源引导到修复问题上，并循环地在机器学习生命周期的各个阶段之间过渡，从而提高项目的执行效率。为了让项目能够快速失败，必须有一种负责任且有信心的方式来判断模型是否无法正常工作。总之，这种快速失败的能力是非常有益的，因为它节省了本可能浪费的时间和资源。
- en: 'The following list shows a sufficient range of methods that can be utilized
    to evaluate deep learning models, with some of them being general methods that
    can work for non-deep learning models too:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表展示了一系列足够广泛的方法，可用于评估深度学习模型，其中一些方法也是通用的，适用于非深度学习模型：
- en: '**Evaluation metric engineering**: While evaluation metrics are commonly used
    in many projects, the practice of evaluation metric engineering is often overlooked.
    In this chapter, we will take a closer look at metric engineering and explore
    the process of selecting appropriate evaluation metrics. We’ll start by discussing
    foundational baseline evaluation metrics that are suitable for various types of
    problems. Then, we’ll move on to explore how to upgrade the baseline evaluation
    metric to one that is specific to the domain and use case of the project. So,
    in short, this chapter will help you understand the importance of metric engineering
    and guide you through the process of selecting the right metrics for your project.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标工程**：虽然评估指标在许多项目中常常被使用，但评估指标工程的实践通常被忽视。在本章中，我们将仔细探讨指标工程，并探索选择合适评估指标的过程。我们将首先讨论适用于各种问题类型的基础基线评估指标。然后，我们将进一步探讨如何将基线评估指标升级为适合项目领域和用例的特定指标。简而言之，本章将帮助你理解指标工程的重要性，并指导你选择适合项目的评估指标。'
- en: '**Learning curves**: Learning curves determine the level of fit for a deep
    learning model.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习曲线**：学习曲线用于确定深度学习模型的拟合程度。'
- en: '**Lift charts**: A lift chart offers a visual representation of the performance
    of a predictive model. It shows how much better the model is at predicting positive
    outcomes compared to random chance.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提升图**：提升图提供了预测模型性能的可视化表示。它显示了模型在预测正向结果方面比随机机会要好多少。'
- en: '**Receiver operating characteristic** (**ROC**) **curves**: A graphical representation
    of the performance of a binary classification model. They plot the **true positive
    rate** (**TPR**) against the **false positive rate** (**FPR**) at various classification
    thresholds.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）**曲线**：二分类模型性能的图形表示。它们在不同的分类阈值下绘制**真正例率**（**TPR**）与**假正例率**（**FPR**）。'
- en: '**Confusion matrix**: A confusion matrix is a performance evaluation tool that
    measures the classification accuracy of a machine learning model. It compares
    the predicted and actual outcomes of a model’s predictions and presents them in
    a matrix format.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：混淆矩阵是一种性能评估工具，用于测量机器学习模型的分类准确度。它比较模型预测和实际结果，并以矩阵格式呈现。'
- en: '**Feature importances**: This is the process of determining which features
    in a dataset have the most influence on the output of a machine learning model.
    Additionally, it is useful for identifying the most important factors in a given
    problem and can help improve the model’s overall performance.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征重要性**：这是确定数据集中哪些特征对机器学习模型输出影响最大的过程。此外，它有助于识别给定问题中最重要的因素，并可以帮助提高模型的整体性能。'
- en: '**A/B testing**: A/B testing for machine learning involves comparing the performance
    of two different models or any algorithms on a specific task, in order to determine
    which model performs better in practice. This can help practitioners make more
    informed decisions about which models to use or how to improve existing models.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B测试**：机器学习的A/B测试涉及比较两种不同模型或算法在特定任务上的表现，以确定哪个模型在实践中表现更好。这可以帮助从业者更明智地决定使用哪些模型或如何改进现有模型。'
- en: '**Cohort analysis**: Cohort analysis is a technique for evaluating the performance
    of a model on different subgroups or cohorts of users. It can help identify whether
    the model is performing differently for different groups and can be useful for
    understanding how to improve the model for specific segments.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**队列分析**：队列分析是一种评估模型在不同用户子群或队列上表现的技术。它可以帮助确定模型是否对不同群体表现出不同的性能，并且可以用于了解如何针对特定分段改进模型。'
- en: '**Residual analysis**: Residual analysis is a technique used to check the goodness
    of fit of a regression model by examining the difference between the observed
    values and the predicted values (**residuals**). It helps identify patterns or
    outliers in the residuals that may indicate areas for improvement in the model.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差分析**：残差分析是一种通过检查观测值与预测值（**残差**）之间的差异来检查回归模型拟合优度的技术。它有助于识别残差中可能指示模型改进方向的模式或异常值。'
- en: '**Confidence intervals**: Confidence intervals are a measure of the uncertainty
    in an estimate. They can be used to determine the range of values in which the
    true performance of a model is likely to fall with a certain level of confidence.
    Confidence intervals can be useful for comparing the performance of different
    models or for determining whether the performance of a model is statistically
    significant.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**置信区间**：置信区间是估计中不确定性的一种度量。它可以用于确定在一定置信水平下，模型的真实性能可能落在的值范围内。置信区间对于比较不同模型的性能或确定模型性能是否具有统计显著性很有用。'
- en: '**Gathering insights from predictions**: This will be covered in [*Chapter
    11*](B18187_11.xhtml#_idTextAnchor172), *Explaining Neural* *Network Predictions*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从预测中获取见解**：这将在[*第11章*](B18187_11.xhtml#_idTextAnchor172)，*解释神经网络预测*中进行讨论。'
- en: '**Interpreting neural networks**: This will be covered in [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184),
    *Interpreting* *Neural Networks*.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释神经网络**：这将在[*第12章*](B18187_12.xhtml#_idTextAnchor184)，*解释神经网络*中进行讨论。'
- en: '**Bias and fairness analysis**: This will be covered in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见和公平性分析**：这将在[*第13章*](B18187_13.xhtml#_idTextAnchor196)，*探索偏见和公平性*中进行讨论。'
- en: '**Adversarial analysis**: This will be covered in [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing* *Adversarial Performance*.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗分析**：这将在[*第14章*](B18187_14.xhtml#_idTextAnchor206)，*分析对抗性能*中进行讨论。'
- en: In this book, we will only be covering methods that relate to neural networks
    in some way. In the next section, we will start with the engineering baseline
    evaluation method, which is the model evaluation metric.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们只会涵盖某种方式与神经网络相关的方法。在下一节中，我们将从工程基线评估方法开始，这是模型评估的度量标准。
- en: Engineering the base model evaluation metric
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工程基础模型评估指标
- en: Engineering a metric for your use case is a skill that is often overlooked.
    This is most likely because most projects work on a publicly available dataset,
    which almost always already has a metric proposed. This includes projects on Kaggle
    and many public datasets people use to benchmark against. However, this does not
    happen in real life and a metric doesn’t just get served to you. Let’s explore
    this topic further here and gain this skillset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的使用场景设计一个度量标准是一项常常被忽视的技能。这很可能是因为大多数项目使用的是公开可用的数据集，这些数据集几乎总是已经有了一个度量标准。包括 Kaggle
    上的项目和许多人用来进行基准测试的公共数据集。然而，现实生活中并不是这样，度量标准并不会直接提供给你。让我们在这里进一步探讨这个话题，掌握这一技能。
- en: The model evaluation metric is the first evaluation method that is essential
    in supervised projects, excluding unsupervised-based projects. There are a few
    baseline metrics that exist to be the *de facto* metrics depending on the problem
    and target type. Additionally, there are also more customized versions of these
    baseline metrics that are catered to special objectives. For example, generative-based
    tasks can be evaluated through a special human-based opinion score called the
    mean opinion score. The recommended go-to strategy here is to always start with
    a baseline metric and work your way up to metrics that reflect how errors should
    be distributed properly in different conditions in your use case, similar to how
    it is recommended to build models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估指标是监督学习项目中至关重要的首要评估方法，排除无监督学习项目。根据问题和目标类型，有一些基准指标被广泛使用，成为*事实标准*。此外，也有一些定制化版本的基准指标，专门针对特殊目标。例如，生成式任务可以通过一种名为平均意见分数（mean
    opinion score）的特殊人工评分来进行评估。这里推荐的策略是：总是从基准指标开始，然后逐步调整到能够反映错误在不同条件下如何合理分布的指标，这类似于推荐如何构建模型。
- en: 'Here are baseline metrics for different conditions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同条件下的基准指标：
- en: '**Binary** **classification problems**:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类问题**：'
- en: '**Accuracy**: This is the percentage of correctly classified examples (true
    positives and true negatives) out of all examples. It is the most widely known
    evaluation metric across any domain. However, in reality, this is a skewed metric
    that can cloud the actual positive prediction performance of the model due to
    the natural oversupply of negatives in most datasets. If there are 99 negative
    examples and 1 positive example, predicting negative all the time without a model
    can get you 99% accuracy! Accuracy still remains the main method for model evaluation,
    but it is not practically used. When somebody says the model is *accurate*, they
    probably aren’t using the accuracy metric.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：这是所有样本中被正确分类的示例（真阳性和真阴性）的百分比。它是各个领域中最广泛知晓的评估指标。然而，实际上，这是一个偏颇的指标，可能会因为大多数数据集中自然存在的负样本过多，掩盖模型实际的正向预测表现。如果数据集中有
    99 个负样本和 1 个正样本，仅仅通过始终预测为负样本，就能获得 99% 的准确率！准确率仍然是模型评估的主要方法，但它并不总是能实际使用。当有人说模型是*准确的*，他们可能并不是在使用准确率这一指标。'
- en: '**Precision**: This is the proportion of true positives among all predicted
    positive examples. It is a robust alternative that focuses on false positives.
    A prediction threshold is needed here for binary classification projects.'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**：这是所有预测为正样本的示例中，真阳性占的比例。它是一个健壮的替代指标，重点关注假阳性。这里对于二分类项目需要设置一个预测阈值。'
- en: '**Recall**: This is the proportion of true positives among all actual positive
    examples. It is a robust alternative that focuses on false negatives. A prediction
    threshold is needed here for binary classification projects.'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：这是所有实际为正样本的示例中，真阳性占的比例。它是一个健壮的替代指标，重点关注假阴性。这里对于二分类项目需要设置一个预测阈值。'
- en: '**F1 score**: The F1 score is the harmonic mean of precision and recall. It
    provides a balanced measure of a model’s performance. The formula of harmonic
    mean is'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 分数**：F1 分数是精确率和召回率的调和均值。它提供了一个平衡的模型性能衡量标准。调和均值的公式是'
- en: n _  1 _ x1+  1 _ x2…  1 _ xn
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: n _  1 _ x1+  1 _ x2…  1 _ xn
- en: where is the total number of samples and is the individual sample value. There
    is also the F2 score that weighs recall more than precision. Use the F1 score
    if you care about both false positives and false negatives equally, and use the
    F2 score if you care about false negatives more than false positives. For example,
    in an intrusion detection use case, if you want to capture any intruders, you
    can’t afford to have false negatives, but you can afford to have false positives,
    so using the F2 score would be better. Note that the harmonic mean is utilized
    instead of the arithmetic mean to ensure that extreme values are penalized. For
    example, a recall of 1.0 and a precision of 0.01 will result in 0.012 instead
    of something close to 0.5\. A prediction threshold is needed here.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，表示样本总数，表示单个样本的值。还有F2分数，它对召回率的权重高于精度。如果你对假阳性和假阴性同样重视，可以使用F1分数；如果更关注假阴性，可以使用F2分数。例如，在入侵检测的应用场景中，如果你想捕捉到所有的入侵者，不能容忍假阴性，但假阳性是可以接受的，因此使用F2分数会更合适。注意，使用的是调和平均而非算术平均，以确保对极端值进行惩罚。例如，召回率为1.0而精度为0.01时，结果将是0.012，而不是接近0.5。这里需要一个预测阈值。
- en: '**Area under the receiver operating characteristic curve** (**AUC ROC**): This
    is the area under the ROC curve, which provides a measure of a model’s ability
    to distinguish between positive and negative examples. No threshold is needed
    here. It is recommended for use when the positive and negative classes are balanced
    so you don’t need to tune the prediction threshold like for F1 and F2.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**接收者操作特征曲线下面积**（**AUC ROC**）：这是ROC曲线下的面积，提供了衡量模型区分正例与负例能力的标准。这里不需要阈值。推荐在正负类别平衡时使用，因为不需要像F1和F2那样调整预测阈值。'
- en: '**Mean average precision** (**mAP**): This is an extension on top of the precision
    metric where instead of a single threshold, multiple thresholds are used to compute
    precision and averaged up to obtain the more robust precision value at different
    thresholds. For multiple classes, average precision is computed independently
    and averaged up to obtain mAP.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均精度**（**mAP**）：这是精度度量的扩展，在这里，除了使用单一阈值，还使用多个阈值来计算精度，并对结果取平均，以获得在不同阈值下更稳健的精度值。对于多类别问题，平均精度会独立计算每个类别，并取平均得到mAP。'
- en: '**Multiclass** **classification problems**:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多分类问题**：'
- en: '**Macro**: Calculate any binary classification metric for each class individually
    and then average them'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宏平均**：分别计算每个类别的二分类指标，然后对它们取平均。'
- en: '**Micro**: Determine the overall true positive and false positive rates by
    considering the highest predicted class, and then use these rates to calculate
    the overall precision'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微平均**：通过考虑预测类别中最高的类别，确定整体的真阳性率和假阳性率，然后使用这些比率来计算整体的精度。'
- en: '**Regression problems**:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归问题**：'
- en: '**Mean squared error** (**MSE**): This is the average of the squared differences
    between predicted and actual values.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）：这是预测值与实际值之间的平方差的平均值。'
- en: '**Root mean squared error** (**RMSE**): This is the square root of the MSE.
    It provides values at the same scale as the target data and is recommended over
    MSE.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）：这是均方误差（MSE）的平方根。它提供与目标数据相同尺度的值，通常推荐使用RMSE而非MSE。'
- en: '**Mean absolute error** (**MAE**): This is the average of the absolute differences
    between predicted and actual values. Use this over RMSE when you care about differences
    between predicted and actual labels without caring about its sign.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）：这是预测值与实际值之间绝对差的平均值。当你关心预测值与实际标签之间的差异，而不关心差异的正负时，使用MAE优于RMSE。'
- en: '**R-squared**: A measure of how well the model fits the data, which ranges
    from 0 (poor fit) to 1 (perfect fit).'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R平方**：衡量模型拟合数据的程度，其值范围从0（拟合差）到1（拟合完美）。'
- en: '**Multilabel**:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签问题**：'
- en: '**Label ranking average precision** (**LRAP**): This is the average precision
    of each ground truth label assigned to a particular sample. It takes into consideration
    the ranking of labels predicted against the ground truth labels and assigns scores
    appropriately according to how far or close a ground truth label is in the ranks.
    LRAP is beneficial for use cases such as movie recommendation systems, where predicting
    multiple labels and their rankings is important. It evaluates the model’s ability
    to predict the correct labels and their order of relevance, making it an ideal
    metric for tasks that require accurate and meaningful rankings, such as genre
    recommendations.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签排序平均精度** (**LRAP**)：这是对每个真实标签分配给特定样本的平均精度。它考虑了预测标签与真实标签的排名，并根据真实标签在排名中的远近适当地分配分数。LRAP适用于电影推荐系统等用例，在这些用例中，预测多个标签及其排名非常重要。它评估模型预测正确标签及其相关性顺序的能力，是需要精确且有意义排名的任务（如类别推荐）的理想度量。'
- en: '**Image and video labels**: Raw image and video frames, when used directly
    as labels, require their own set of custom metrics that can provide more meaningful
    evaluations compared to standard regression metrics. These are as follows:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像和视频标签**：当直接将原始图像和视频帧用作标签时，需使用一套自定义度量标准，能够提供比标准回归度量更有意义的评估。这些标准如下：'
- en: '**Peak signal-to-noise ratio** (**PSNR**): This is a measure of the quality
    of a reconstructed image or video based on the difference between the original
    and the reconstructed image.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**峰值信噪比** (**PSNR**)：这是衡量重建图像或视频质量的度量标准，基于原始图像与重建图像之间的差异。'
- en: '**Structural similarity index** (**SSIM**): This is a measure of the structural
    similarity between two images or video frames.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构相似性指数** (**SSIM**)：这是衡量两张图像或视频帧之间结构相似性的度量标准。'
- en: '**Text labels**: Although standard classification loss and metrics can be used
    for text prediction tasks, some metrics can measure much higher-level concepts
    that are more in tune with human intuition. These are as follows:'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本标签**：尽管标准的分类损失和度量可用于文本预测任务，但一些度量可以衡量更高级的概念，这些概念更符合人类直觉。具体包括：'
- en: '**Bleu score**: This is a measure of the similarity between machine-generated
    text and human-generated text based on n-gram overlap.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**蓝色分数**：这是衡量机器生成文本和人类生成文本之间相似性的度量，基于n-gram重叠。'
- en: '**Word error rate** (**WER**): This is a measure of the error rate in automatic
    speech recognition systems based on the number of errors in word sequences.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词错误率** (**WER**)：这是自动语音识别系统中的错误率衡量标准，基于词序列中的错误数量。'
- en: '**Human quality-based metrics**: These are non-programmatically computable
    metrics that can only be evaluated by humans manually:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于人类质量的度量**：这些是无法通过编程计算的度量，必须由人工手动评估：'
- en: '**Mean opinion score** (**MOS**): This is a subjective quality rating given
    by human observers, which can be used to validate and calibrate objective quality
    metrics'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均意见分数** (**MOS**)：这是由人类观察者给予的主观质量评分，可以用于验证和校准客观质量度量。'
- en: '**User engagement**: Metrics such as time spent on a website or app, click-through
    rate, or bounce rate can be used to measure user engagement and satisfaction'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户参与度**：可以使用网站或应用上的停留时间、点击率或跳出率等度量来评估用户参与度和满意度。'
- en: '**Task completion rate**: This is the proportion of users who successfully
    complete a given task or goal, which can be used to evaluate the usability and
    effectiveness of a product or service'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务完成率**：这是成功完成特定任务或目标的用户比例，可用于评估产品或服务的可用性和有效性。'
- en: 'The baseline metrics are a set of common metrics that should be your first
    choices depending on your problem types and conditions. With that said, the choice
    of which base metric to use still depends on the specific problem and the trade-offs
    between the different aspects of the model’s performance that are important for
    the task at hand. Here are step-by-step recommendations on how to actually choose
    and utilize an appropriate model evaluation metric:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基准度量是根据你的问题类型和条件，应该首先选择的一组常见度量。话虽如此，选择使用哪个基准度量仍然取决于具体问题，以及模型性能不同方面之间的重要权衡。以下是逐步建议，帮助你如何选择和使用合适的模型评估度量：
- en: Understand the problem. Consider the nature of the problem, the data, and the
    desired outcomes. This is a key step that will help you identify the key criteria
    that are most important for your task.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解问题。考虑问题的性质、数据和期望的结果。这是一个关键步骤，它将帮助你识别出对任务最重要的关键标准。
- en: Be mindful of the quality of the data. The pillars of data quality (representativeness,
    consistency, comprehensiveness, uniqueness, fairness, and validity) introduced
    in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*,
    will all affect what the metric actually represents. If you are evaluating a model’s
    performance on a bad dataset, then the chosen metric may not reflect the model’s
    true performance on real-world data.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 留意数据的质量。在[*第一章*](B18187_01.xhtml#_idTextAnchor015)中介绍的数据质量支柱（代表性、一致性、全面性、唯一性、公平性和有效性）会影响度量标准所代表的实际意义。如果你在一个不良数据集上评估模型的性能，那么所选择的度量标准可能无法反映模型在真实世界数据上的真实表现。
- en: Consider the perspective of the users who will be interacting with the model
    or the output of the model. What are their expectations and requirements? What
    are the relevant quality factors that need to be taken into account?
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑与模型或模型输出进行交互的用户的视角。他们的期望和需求是什么？需要考虑哪些相关的质量因素？
- en: Define clear objectives for what the predictions need to accomplish and the
    opposite of what they need to do.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 明确定义预测需要实现的目标以及它们需要避免的目标。
- en: Choose a metric that aligns with your defined objective. The metric you choose
    should align with your overall objective. For example, in a binary classification
    medical use case for detecting cancer, making a false positive diagnosis of cancer
    can ruin many years of a patient’s life, so choose a metric such as precision
    to allow you to quantitatively reduce the number of false positives.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与定义目标一致的度量标准。你选择的度量标准应该与你的整体目标一致。例如，在二分类医学用例中用于检测癌症，错误地做出癌症假阳性诊断可能会毁掉患者多年的生活，因此应选择诸如精准度等度量标准，以量化减少假阳性。
- en: Consider base metrics that are commonly used in similar problems and how they
    might need to be adapted or modified to suit the current problem. Are there any
    unique aspects of the problem that require a different type of metric or a modification
    of an existing metric?
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑在类似问题中常用的基础度量标准，以及它们如何需要适应或修改以适应当前的问题。这个问题是否有任何独特的方面需要不同类型的度量标准或对现有度量标准的修改？
- en: 'A single metric may not always capture the full performance of a model. Consider
    using multiple metrics that evaluate different aspects of the model’s performance.
    A clear example of this is the creation of an F1 score that combines two metrics:
    precision and recall.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单一度量标准可能无法全面捕捉模型的表现。考虑使用多个度量标准来评估模型表现的不同方面。一个明显的例子是创建一个将精准度和召回率两个度量标准结合的F1分数。
- en: Consider the trade-offs between metrics. For example, in a binary classification
    project, increasing recall performance by modifying the model’s prediction threshold
    can adversely affect precision. Evaluate the trade-offs (if any) between the metrics
    and choose the one that best aligns with your objectives.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑度量标准之间的权衡。例如，在二分类项目中，通过修改模型的预测阈值提高召回率的表现可能会不利于精准度。评估度量标准之间的权衡（如果有的话），并选择最能与目标对齐的度量标准。
- en: Cross-validate the metric. Making sure the metric is computed on a validation
    and holdout set instead of just the training set is essential to estimating the
    model’s real-world performance. Computing the metric on both the training data
    and validation data in every epoch will also allow you to visualize the learning
    curve using the chosen metric directly instead of using the utilized loss.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证度量标准。确保度量标准是在验证集和持出集上计算的，而不仅仅是在训练集上，这对于估计模型在真实世界中的表现至关重要。在每个epoch中计算训练数据和验证数据上的度量标准，也能让你通过所选度量标准直接可视化学习曲线，而不是使用已采用的损失函数。
- en: Consider optimizing your model to the metric directly. Some metrics can be approximately
    reproduced in the deep learning libraries directly and utilized as loss. Using
    direct optimization can sometimes help you get a better model specifically for
    the metric you’ve chosen. However, there might also be some pitfalls that can
    happen. We will discuss this more extensively later in the *Directly optimizing
    the* *metric* section.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑直接针对度量优化你的模型。有些度量可以在深度学习库中直接近似重现，并作为损失函数使用。直接优化有时可以帮助你获得一个更适合你选择的度量的模型。然而，也可能会出现一些陷阱，我们将在后面的*直接优化度量*部分详细讨论这一点。
- en: Build, evaluate, and compare many models against the evaluation metric by iteratively
    improving it or just using a diverse variety of techniques. The process of building
    a machine learning model might just be the shortest process in the entire ML lifecycle.
    But remember that the amount of effort you put into the building and experimentation
    process can determine whether a project fails or succeeds. Building a variety
    of models and iteratively improving them can be a hard and time-consuming task.
    Having boilerplate code that can adapt to most use cases can make this process
    way faster and more seamless so that you can put your effort into more pressing
    issues in a project. Alternatively, you can also consider using AutoML tools to
    consistently get a variety of models trained for each use case. Look forward to
    the last chapter in this book where we will get a feel for how AutoML tools can
    streamline the model-building process!
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建、评估并比较许多模型，针对评估度量通过迭代改进或使用多种技术进行调整。构建机器学习模型的过程可能只是整个ML生命周期中最短的过程。但请记住，你在构建和实验过程中投入的努力可以决定项目的成败。构建多种模型并迭代改进可能是一个艰难且耗时的任务。拥有可以适应大多数应用场景的模板代码，可以让这个过程变得更加高效和无缝，这样你就能将精力集中在项目中更为紧迫的问题上。或者，你也可以考虑使用AutoML工具来持续为每个应用场景训练不同的模型。期待本书最后一章，我们将感受AutoML工具如何简化模型构建过程！
- en: Make sure you define a success criterion that relates to the chosen metric.
    To translate a chosen evaluation metric into an actual success metric, it is important
    to establish a clear understanding of what constitutes success for the given problem.
    This will typically involve defining a threshold or target level of performance
    that the model needs to achieve in order to be considered successful. Sometimes,
    the success criteria can have a more fine-grained definition based on specific
    types of error or specific groups of data. We will explore this in depth later
    in this chapter.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你定义与所选度量相关的成功标准。将所选评估度量转化为实际的成功度量时，重要的是要清楚地了解对于给定问题，什么才算成功。这通常涉及定义一个模型需要达到的性能阈值或目标水平，才能被认为是成功的。有时，成功标准可以根据特定类型的错误或特定数据组有更细致的定义。我们将在本章后面深入探讨这一点。
- en: By carefully considering these recommendations, it is possible to select a unique
    metric that accurately measures the relevant aspects of the problem. Ultimately,
    this can help to develop more accurate and effective machine learning solutions
    that will lead to better performance and more successful outcomes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仔细考虑这些建议，可以选择一个独特的度量，准确衡量问题的相关方面。最终，这有助于开发更准确、更有效的机器学习解决方案，从而带来更好的性能和更成功的结果。
- en: Base metrics are a group of metrics that are commonly used by many practitioners
    to evaluate the performance of a model. However, in some cases, a particular problem
    may have additional criteria and unique behaviors that need to be considered when
    assessing model performance. In such situations, it may be necessary to adopt
    alternative or customized metrics that better suit the specific needs of the problem
    at hand. Base metrics can be further adapted to the additional ideals you want
    to use to judge your models. The next topic will explore custom metrics and their
    applications, including when it is appropriate to use them as the most suitable
    metric for a specific use case.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基础度量是一组常用的度量，许多从业人员使用它们来评估模型的性能。然而，在某些情况下，特定问题可能有额外的标准和独特的行为，需要在评估模型性能时加以考虑。在这种情况下，可能需要采用替代的或定制的度量，这些度量更适合该问题的具体需求。基础度量可以根据你希望用来评判模型的额外标准进一步调整。下一部分将探讨自定义度量及其应用，包括在何种情况下将其作为最合适的度量来用于特定的应用场景。
- en: Exploring custom metrics and their applications
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索自定义度量及其应用
- en: Base metrics are generally sufficient to meet the requirements of most use cases.
    However, custom metrics build upon base metrics and incorporate additional goals
    that are specific to a given scenario. It’s helpful to think of base metrics as
    a bachelor’s degree and custom metrics as a master’s or PhD degree. It’s perfectly
    fine to use only base metrics if they meet your needs and you don’t have any additional
    requirements.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基本指标通常足以满足大多数使用场景的需求。然而，自定义指标是在基本指标的基础上构建的，融入了特定场景下的附加目标。可以将基本指标视为本科学位，而将自定义指标视为硕士或博士学位。如果基本指标满足你的需求，并且没有其他额外要求，那么只使用基本指标也是完全可以的。
- en: Custom ideals often arise naturally early on in a project and are highly dependent
    on the specific use case. Most real use cases don’t expose their chosen metrics
    to the public, even when the prediction of the model is meant to be utilized publicly,
    such as **Open AI**’s **ChatGPT**. However, in machine learning competitions,
    companies with real use cases accompanied by data publish their chosen metric
    publicly to find the best model that can be built. In such a setting for a project,
    the company that hosts the competition is incentivized to perform good-quality
    metric engineering work that reflects its ideals for its use case. A metric can
    affect the resulting best model and will ultimately cost the company money when
    it doesn’t engineer a good metric that matches their ideals. Some competitions
    provide prizes of up to 100,000 USD!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标通常在项目初期自然出现，并且高度依赖于具体的应用场景。大多数实际应用场景不会公开其选择的指标，即使模型的预测最终是公开使用的，比如**Open
    AI**的**ChatGPT**。然而，在机器学习竞赛中，拥有实际应用场景和数据的公司会公开其选择的指标，以便找到可以建立的最佳模型。在这样的竞赛项目中，举办比赛的公司有动力进行高质量的指标工程工作，以反映其应用场景的理想。一个指标可以影响最终的最佳模型，如果公司没有工程出与其理想匹配的好指标，最终会花费大量资金。一些比赛提供高达100,000美元的奖金！
- en: 'In this section, we will present some common and publicly shared custom ideals
    along with associated metrics and use cases from machine learning competitions
    that could be useful for you to consider, regardless of your particular use case:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些常见且公开分享的自定义指标，以及来自机器学习竞赛的相关度量标准和使用案例，这些对于你考虑特定应用场景可能会有所帮助：
- en: '| **Ideals** | **Use case** | **Custom metric** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **理想** | **使用案例** | **自定义指标** |'
- en: '| For time-series regression point-based forecasting, the targets are seasonal
    and can fluctuate widely based on the season the data is in. We want a metric
    that can make sure errors aren’t weighted heavily to any one season. | **M5 forecasting—accuracy
    (Kaggle)**: This involves predicting the number of Walmart retail goods units
    sold. The competition provided sales time-series data from Walmart that followed
    a hierarchical structure, beginning at the item level and progressing to department,
    product category, and store levels. The data was generously provided and covered
    three regions in the United States: California, Texas, and Wisconsin. | **Weighted
    root mean squared scaled error** (**WRMSSE**): The main part here is the RMSSE,
    which is a modification of RMSE. Before applying the root of MSE, RMSSE divides
    the standard MSE by the MSE that uses most recent observation as ground truth.
    This makes sure all RMSE from any season will be scaled into the same range of
    values. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 对于基于时间序列回归的点预测，目标是季节性的，并且可能会根据数据所在的季节大幅波动。我们需要一个度量标准，确保错误不会对任何一个季节的权重过重。
    | **M5 预测—准确性（Kaggle）**：这涉及到预测沃尔玛零售商品的销量。该比赛提供了沃尔玛的销售时间序列数据，数据遵循层级结构，从商品层级开始，逐步向部门、产品类别和门店层级扩展。数据慷慨地提供，覆盖了美国的三个地区：加利福尼亚、德克萨斯和威斯康星州。
    | **加权均方根标准化误差**（**WRMSSE**）：这里的主要部分是RMSSE，它是RMSE的一个修改版。在应用均方误差的平方根之前，RMSSE将标准的MSE除以使用最近观测作为真实值的MSE。这确保了任何季节的RMSE都会被缩放到相同的值域。
    |'
- en: '| Some labels/classes don’t matter as much in reality, either because they
    don’t occur as much or they just don’t impact post-prediction decisions as much.
    Don’t judge the model too much on unimportant labels; put further emphasis on
    the errors of more important labels/classes. | `any` was made to account for all
    other types of hemorrhage not accounted for with a specific label. The `any` label
    had 2-3 times more data than any other label alone. | `any` label was weighted
    more than any other label even though it had more data. This shows that the significance
    of a label for a metric is not exclusive to the scarcity of the data associated
    with the label in a dataset; it really depends on the specific problem context.
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 有些标签/类别在实际中并没有那么重要，可能是因为它们的出现频率较低，或者它们对预测后的决策影响不大。不要过度评价不重要标签的模型表现；应将更多关注放在更重要标签/类别的错误上。
    | `any`标签用于考虑所有其他未使用特定标签表示的出血类型。`any`标签的数据量是其他任何单一标签的2-3倍。 | `any`标签的权重比其他任何标签都要高，尽管它的数据量更多。这表明，标签对指标的重要性并不单纯依赖于与该标签相关的数据稀缺性；它实际上取决于特定问题的上下文。
    |'
- en: '| **M5 Forecasting—accuracy (Kaggle)**: This involves predicting the number
    of Walmart retail goods units sold. The competition provided time-series sales
    data from Walmart that followed a hierarchical structure, beginning at the item
    level and progressing to department, product category, and store levels. The data
    was generously provided and covered three regions in the United States: California,
    Texas, and Wisconsin. | **WRMSSE**: The competition author valued more unit sales
    forecast from products that provided more significant sales in dollars. The weight
    for each product is obtained by using the sales volumes for the product in the
    last 28 observations of the training sample (sum of units sold multiplied by their
    respective prices). |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **M5预测——准确度（Kaggle）**：这涉及到预测沃尔玛零售商品的销售数量。竞赛提供了沃尔玛的时间序列销售数据，这些数据遵循分层结构，从商品级别开始，逐步扩展到部门、产品类别和门店级别。数据慷慨提供，涵盖了美国的三个地区：加利福尼亚、德克萨斯和威斯康星州。
    | **WRMSSE**：竞赛的组织者更重视那些带来更多销售额的产品的单位销售预测。每个产品的权重是通过使用训练样本最后28个观察值中的产品销售量（销售单位的总和乘以其各自的价格）来获得的。
    |'
- en: '|  | **Walmart Recruiting—Store Sales Forecasting (Kaggle)**: This involves
    forecasting the sales of Walmart goods. | **Weighted mean absolute error** (**WMAE**):
    Walmart weighed holiday weeks forecast error more than non-holiday weeks by five-fold,
    as they have much higher sales during holiday weeks. |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | **沃尔玛招聘——门店销售预测（Kaggle）**：这涉及到沃尔玛商品的销售预测。 | **加权绝对误差**（**WMAE**）：沃尔玛将假期周的预测误差加权为非假期周的五倍，因为假期周的销售额远高于非假期周。
    |'
- en: '| We don’t really care too much about small errors, as they can be tolerated,
    but we care about big errors because they can result in the triggering of unwanted
    actions by consuming the predictions. | **Google Analytics customer revenue prediction
    (Kaggle)**: This is a regression problem about predicting the total revenue generated
    by a customer for an online store. The revenue values were highly skewed and contained
    many zero values, which made it challenging to evaluate the performance of the
    participating models using traditional metrics such as MSE or MAE. | **Root mean
    squared log error (RMSLE)**: RMSLE applies a logarithmic transformation to the
    predicted and actual values before computing RMSE, which helps to penalize large
    errors more heavily than small errors. A natural way of doing this is to simply
    train the model to predict the log values of the target and apply RMSE to achieve
    RMSLE. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 我们其实并不太在意小的错误，因为它们可以被容忍，但我们关心的是大的错误，因为它们可能会导致触发不想要的操作，消耗了预测结果。 | **谷歌分析客户收入预测（Kaggle）**：这是一个回归问题，旨在预测在线商店中一个客户的总收入。收入值高度偏斜且包含许多零值，这使得使用传统的度量标准（如MSE或MAE）来评估参与模型的表现变得具有挑战性。
    | **均方根对数误差（RMSLE）**：RMSLE在计算RMSE之前，对预测值和实际值应用对数变换，这有助于比小错误更严重地惩罚大错误。实现这一点的自然方式是简单地训练模型去预测目标的对数值，并应用RMSE来实现RMSLE。
    |'
- en: '| **Diabetic retinopathy detection (Kaggle)**: This is a multiclass problem
    about predicting whether high-resolution retina images have diabetic retinopathy
    or not. The problem has five classes: one for no disease and the other four for
    different severities of the disease. | **Quadratic Weighted Kappa (QWK) score**:
    The Kappa score is a statistical measure that provides a single value to quantify
    the degree of agreement between predicted and actual labels in multi-class classification
    tasks. The quadratic weighing mechanism enables the Kappa score to become more
    robust to minor errors and more sensitive to larger ones. The quadratic weighting
    scheme can address issues where the Kappa score may be inflated by a high proportion
    of agreement in easy-to-classify categories while still providing an accurate
    representation of the level of agreement for the more difficult cases. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **糖尿病视网膜病变检测（Kaggle）**：这是一个多类问题，旨在预测高分辨率视网膜图像是否存在糖尿病视网膜病变。该问题有五个类别：一个表示无病，其他四个表示不同程度的病变。
    | **二次加权卡帕系数（QWK）**：卡帕系数是一种统计度量，用于量化多类分类任务中预测标签与实际标签之间的一致程度。二次加权机制使卡帕系数对小错误更为稳健，对大错误更为敏感。二次加权方案可以解决当高分类一致性导致卡帕系数膨胀的问题，同时仍能准确表示更难分类的情况的一致性水平。
    |'
- en: '|  | **Two Sigma Connect** **Rental listing inquiries (Kaggle)**: This involves
    predicting how popular an apartment rental listing is based on the listing content,
    such as text descriptions, photos, number of bedrooms, and price. | **Log loss**:
    Log loss is an evaluation metric that emphasizes wrong predictions by penalizing
    models that are confident about incorrect predictions. |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | **Two Sigma Connect** **租赁列表查询（Kaggle）**：这是一个基于房源内容（如文字描述、照片、卧室数量和价格）预测公寓租赁列表受欢迎程度的问题。
    | **对数损失（Log loss）**：对数损失是一种评估指标，它通过惩罚那些对错误预测过于自信的模型，来强调错误预测的重要性。 |'
- en: '| In a multiclass problem, we have a higher tolerance for our multiclass model
    in our use case where we can consume several of the top predicted classes instead
    of using the single most probable class to maximize the true positive hit rate
    performance of the model. | **Airbnb new user bookings (Kaggle)**: This is a multiclass
    problem for predicting the country in which a new user will make their first booking
    with Airbnb (including a no-booking class). The predicted class will allow Airbnb
    to share more personalized content with their community, decrease the average
    time to first booking, and better forecast demand.Airbnb had a relaxed requirement
    for the multiclass predictions a model produced, where they could perform personalized
    content based on the top five countries instead of a single country to improve
    the true positive hit rate. | **Normalized discounted cumulative gain (NDCG) of
    top-k classes**: NDCG is a measure of the ranking effectiveness of top-k classes,
    usually used for recommendation use cases. This metric pairs nicely with the tolerance
    Airbnb has for multiclass predictions. It would work well for any multiclass use
    case that can tolerate using several top predicted classes instead of the single
    predicted top class. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 在一个多类问题中，我们对多类模型的容忍度较高，因为在我们的使用案例中，我们可以使用多个预测类别，而不是仅使用最可能的单一类别，以最大化模型的真正正例命中率表现。
    | **Airbnb新用户预订（Kaggle）**：这是一个多类问题，旨在预测新用户将在哪个国家/地区首次进行Airbnb预订（包括无预订类别）。预测的类别将帮助Airbnb为其社区提供更个性化的内容，缩短首次预订的平均时间，并更好地预测需求。Airbnb对模型产生的多类预测有较为宽松的要求，可以根据前五个国家而非单一国家执行个性化内容，以提高真正正例命中率。
    | **标准化折扣累计增益（NDCG）前k类**：NDCG是衡量前k类排序效果的指标，通常用于推荐系统。该指标与Airbnb对多类预测的容忍度相匹配。它适用于任何能够容忍使用多个预测类别，而不是仅使用最预测类别的多类使用案例。'
- en: '| For multi-object video-based tracking use cases, we want to use a metric
    that penalizes undesired tracking behaviors, such as false object identification
    and failure to track objects consistently over time. Additionally, we need a metric
    that can be computed by the multiple trajectories created by a model through time
    against a fixed set of ground truth tracking trajectories. | `Track 1`, we have
    created a large-scale synthetic dataset of animated people. All camera feeds in
    our dataset are high-resolution (1080p) feeds with frame rates of 30 frames per
    second. | **Identity F1 score (IDF1)**: IDF1 handles this by evaluating the overall
    performance of the tracking algorithm based on how well it matches predicted tracks
    to ground truth tracks, taking into account the identity of each object over time.
    The algorithm penalizes false positives, false negatives, and identity switches,
    which occur when the algorithm incorrectly identifies two detections as the same
    object or incorrectly assigns two different identities to the same object over
    time. Most importantly, the model can dynamically determine matching trajectories
    based on a version of the **intersection over union** (**IOU**) algorithm between
    predicted and ground truth trajectories. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: 对于基于视频的多目标跟踪应用场景，我们希望使用一种衡量标准，能够惩罚不希望出现的跟踪行为，例如错误的物体识别和未能持续稳定地跟踪物体。此外，我们需要一种可以通过模型随着时间生成的多个轨迹与固定的真实轨迹集进行比较来计算的度量标准。
    | `Track 1`，我们创建了一个大规模的合成数据集，包含动画人物。我们数据集中的所有摄像机视频源都具有高清分辨率（1080p），帧率为每秒30帧。 |
    **身份F1得分（IDF1）**：IDF1通过评估跟踪算法的整体表现来处理此问题，具体是根据它将预测轨迹与真实轨迹匹配的效果，考虑到每个物体的身份随时间变化。该算法会惩罚假阳性、假阴性以及身份切换（即算法错误地将两个检测结果当作同一物体，或错误地将两个不同的物体身份归为同一个物体）。最重要的是，模型可以基于**交并比**（**IOU**）算法的一个版本，动态地确定匹配轨迹，该算法比较预测轨迹与真实轨迹之间的交集和并集。
- en: '| We care about bias in our model’s performance a lot more than the overall
    model’s performance. | **Jigsaw unintended bias in toxicity classification (Kaggle)**
    [https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evalu):
    This involves predicting the different intensities of toxicity in text data. The
    data contains identities that the competition author wishes to optimize against
    in the name of bias. The identities involved aremale, female, transgender, other
    gender, heterosexual, homosexual (gay or lesbian), bisexual, other sexual orientation,
    Christian, Jewish, Muslim, Hindu, Buddhist, atheist, other religion, black, white,
    Asian, Latino, other race or ethnicity, physical disability, intellectual or learning
    disability, psychiatric or mental illness, and other disability. | **Bias-focused
    AUC**: The competition focuses on a weighted combination of multiple AUC metric
    that are computed on a different subset of the dataset based on specific identities
    that can be mentioned in the text data row. Overall, AUC is combined equally with
    the next three bias-focused metrics.**Identity-based subgroup AUC**: This involves
    analyzing a dataset that only includes comments mentioning a specific identity
    subgroup. A low score in this metric indicates that the model struggles to distinguish
    between toxic and non-toxic comments related to that identity. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: 我们更关心模型性能中的偏差，而非整体模型的表现。 | **Jigsaw意外偏差在毒性分类中的应用（Kaggle）** [https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evalu)：这项任务涉及预测文本数据中毒性的不同强度。数据集包含竞争作者希望在偏差优化方面进行处理的身份。涉及的身份包括男性、女性、跨性别、其他性别、异性恋、同性恋（男同性恋或女同性恋）、双性恋、其他性取向、基督教徒、犹太教徒、穆斯林、印度教徒、佛教徒、无神论者、其他宗教、黑人、白人、亚洲人、拉丁裔、其他种族或族裔、身体残疾、智力或学习障碍、精神或心理疾病以及其他残疾。
    | **偏差焦点AUC**：该竞赛关注多个AUC度量的加权组合，这些AUC度量是基于数据集中不同身份的特定子集计算的，这些身份可以在文本数据行中提到。总体而言，AUC与接下来的三个偏差焦点度量等权结合。**基于身份的子群AUC**：这涉及分析仅包含提到特定身份子群的评论的数据集。在该度量中得分较低表示模型难以区分与该身份相关的有毒评论和无毒评论。
- en: '|  |  | **Background positive, subgroup negative (BPSN) AUC**: This involves
    evaluating the model’s performance on non-toxic examples mentioning the identity
    and toxic examples that do not. A low score in this metric suggests that the model
    may incorrectly label non-toxic examples related to the identity as toxic. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  |  | **背景正例，子群负例（BPSN）AUC**：这涉及评估模型在提到身份的非毒性示例和不提到身份的毒性示例上的表现。此指标得分较低表示模型可能错误地将与身份相关的非毒性示例标记为毒性示例。
    |'
- en: '| **Background negative, subgroup positive (BNSP) AUC**: This involves assessing
    the model’s performance on toxic examples mentioning the identity and non-toxic
    examples that do not. A low score in this metric indicates that the model may
    incorrectly label toxic examples related to the identity as non-toxic. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **背景负例，子群正例（BNSP）AUC**：这涉及评估模型在提到身份的毒性示例和不提到身份的非毒性示例上的表现。此指标得分较低表示模型可能错误地将与身份相关的毒性示例标记为非毒性示例。
    |'
- en: Table 10.1 – A table of custom ideals with example use cases and metrics used
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 – 自定义理想的表格，包含示例用例和使用的指标
- en: In the second ideal, the general metric idea is to apply weights to any metric
    you’d like to choose based on what you deem more important. Additionally, weights
    can be applied more flexibly to any auxiliary data that does not act as an input
    to a model, nor as a target to the model. Finally, for the last ideal, look forward
    to [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    to discover methods to optimize against bias.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个理想中，通用的指标思想是根据您认为更重要的内容对任何选择的指标应用权重。此外，权重还可以更加灵活地应用于任何不作为模型输入或目标的数据。最后，关于最后一个理想，期待[*第13章*](B18187_13.xhtml#_idTextAnchor196)，*探索偏见和公平性*，以发现优化偏见的方法。
- en: While the examples of custom ideals and metrics we’ve discussed are useful guidelines,
    it’s important to remember that there are many different metrics and ideals that
    may be relevant to your specific use case. Don’t be afraid to dig deeper into
    your problem domain and identify metrics that are unique to your particular situation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们讨论的自定义理想和指标示例是有用的指南，但重要的是要记住，可能有许多不同的指标和理想与您的特定用例相关。不要害怕深入挖掘您的问题领域，找出对您的特定情况独特的指标。
- en: The examples we’ve given can serve as a helpful cheat sheet for developing custom
    metrics that are tailored to your needs. By understanding the reasoning behind
    the use of these special metrics in specific domains, you can gain insight into
    the factors that should be considered when evaluating model performance. Ultimately,
    the key is to choose metrics that align with your goals and capture the most important
    aspects of your problem domain.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的示例可以作为开发量身定制的指标的有用备忘单。通过理解在特定领域使用这些特殊指标背后的原因，您可以深入了解在评估模型性能时应考虑的因素。最终，关键是选择与您的目标一致、并能够捕捉到问题领域中最重要方面的指标。
- en: Next, we will discuss a robust strategy to compare the metric performance of
    different models across multiple metric values computed from different cross-validation
    folds or dataset partitions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论一种强健的策略，用于比较不同模型在多个指标值下的性能，这些指标值是从不同的交叉验证折叠或数据集划分中计算得出的。
- en: Exploring statistical tests for comparing model metrics
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索用于比较模型指标的统计检验
- en: In machine learning, metric-based model evaluation often involves using averages
    of aggregated metrics from different folds or partitions, such as holdout and
    validation sets, to compare the performance of various models. However, relying
    solely on these average performance metrics may not provide a comprehensive assessment
    of a model’s performance and generalizability. A more robust approach to model
    evaluation is the incorporation of statistical hypothesis tests, which assess
    whether observed differences in performance are statistically significant or due
    to random chance.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，基于指标的模型评估通常涉及使用来自不同折叠或划分（如保留集和验证集）的聚合指标的平均值，以比较各种模型的性能。然而，仅依赖这些平均性能指标可能无法提供对模型性能和泛化能力的全面评估。一个更强健的模型评估方法是引入统计假设检验，评估观察到的性能差异是否具有统计显著性，或是否只是由于随机机会造成的。
- en: Statistical hypothesis tests are procedures used to determine whether observed
    data provides sufficient evidence to reject a null hypothesis in favor of an alternative
    hypothesis, helping to quantify the likelihood that the observed differences are
    due to random chance or a genuine effect. In statistical tests, the null hypothesis
    (H0) is a default assumption that states there is no effect or relationship between
    variables, serving as a basis for comparison against the alternative hypothesis
    with the goal of determining whether the observed data provides enough evidence
    to reject this default assumption. For the purpose of comparing model metric performance
    across multiple partitions and datasets, the null hypothesis is typically that
    there is no difference between the performances of the models, while the alternative
    hypothesis is that there are differences.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 统计假设检验是用来确定观察到的数据是否提供了足够的证据以拒绝零假设，支持替代假设的程序，它帮助量化观察到的差异是否由随机机会或真实效应造成。在统计检验中，零假设（H0）是默认假设，表示变量之间没有效应或关系，作为与替代假设进行比较的基础，目的是确定观察到的数据是否提供了足够的证据以拒绝这一默认假设。在比较多个分区和数据集中的模型指标性能时，零假设通常是模型的性能之间没有差异，而替代假设则是存在差异。
- en: 'Overall, statistical tests offer a formal framework to objectively determine
    whether differences in performance are significant or due to chance. Additionally,
    statistical tests offer a comprehensive understanding of model performance by
    accounting for variability and uncertainty in metrics. The following table shows
    common statistical tests that you can consider using, along with the Python code
    needed to execute it, how to interpret the result, and when to use it:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，统计检验提供了一个正式的框架，以客观地判断性能差异是否显著或仅为偶然。此外，统计检验通过考虑指标中的变异性和不确定性，提供了对模型性能的全面理解。下表展示了常见的统计检验方法，包括执行所需的
    Python 代码、如何解释结果以及何时使用它们：
- en: '| **Statistical test** | **Python code** | **Result interpretation** | **Recommended
    use** |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **统计检验** | **Python 代码** | **结果解释** | **推荐使用** |'
- en: '| Paired t-test | `from scipy.stats` `import ttest_rel``t_stat, p_value =`
    `ttest_rel(model1_scores, model2_scores)` | If `p_value` < 0.05, there’s a significant
    difference between the models. | Use this when comparing two dependent samples
    with normally distributed differences. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 配对 t 检验 | `from scipy.stats` `import ttest_rel``t_stat, p_value =` `ttest_rel(model1_scores,
    model2_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。 | 用于比较两个依赖样本，且差异呈正态分布时。 |'
- en: '| Mann-Whitney U test | `from scipy.stats` `import mannwhitneyu``u_stat, p_value
    =` `mannwhitneyu(model1_scores, model2_scores)` | If `p_value` < 0.05, there’s
    a significant difference between the models. | Use this when comparing two independent
    samples with non-normally distributed data or ordinal data. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Mann-Whitney U 检验 | `from scipy.stats` `import mannwhitneyu``u_stat, p_value
    =` `mannwhitneyu(model1_scores, model2_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。
    | 用于比较两个独立样本，且数据为非正态分布或序数数据时。 |'
- en: '| **Analysis of** **variance** (**ANOVA**) | `from scipy.stats import f_oneway
    f_stat, p_value = f_oneway(model1_scores,` `model2_scores, model3_scores)` | If
    `p_value` < 0.05, there’s a significant difference among the models. | Use this
    when comparing three or more independent samples with normally distributed data
    and equal variances. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| **方差分析** (**ANOVA**) | `from scipy.stats import f_oneway f_stat, p_value
    = f_oneway(model1_scores,` `model2_scores, model3_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。
    | 用于比较三个或更多独立样本，且数据呈正态分布且方差相等时。 |'
- en: '| Kruskal-Wallis H test | `from scipy.stats` `import kruskal``h_stat, p_value
    = kruskal(model1_scores,` `model2_scores, model3_scores)` | If `p_value` < 0.05,
    there are significant differences between the models. | Use this when comparing
    three or more independent samples with non-normally distributed data or ordinal
    data. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Kruskal-Wallis H 检验 | `from scipy.stats` `import kruskal``h_stat, p_value
    = kruskal(model1_scores,` `model2_scores, model3_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。
    | 用于比较三个或更多独立样本，且数据为非正态分布或序数数据时。 |'
- en: Table 10.2 – Common statistical tests with details on their Python implementation,
    result interpretations, and recommendations on when to use them
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10.2 – 常见统计检验方法及其 Python 实现、结果解释和推荐使用场景
- en: These recommendations can help you choose the appropriate statistical test based
    on the conditions and assumptions of your data, such as the number of samples,
    the type of data (dependent or independent), and the distribution of the data
    (normal or non-normal).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些建议可以帮助你根据数据的条件和假设选择适当的统计检验方法，例如样本数量、数据类型（依赖性或独立性）以及数据的分布（正态分布或非正态分布）。
- en: Next, we will discuss how the outcome of metric engineering can be converted
    to a success criterion.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何将度量工程的结果转化为成功标准。
- en: Relating the evaluation metric to success
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将评估指标与成功联系起来
- en: Defining success in a machine learning project is crucial and should be done
    at the early stages of the project as introduced in the *Defining success* section
    in [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*.
    Success can be defined as achieving higher-level objectives, such as improving
    the efficiency of processes or increasing the accuracy of processes in comparison
    to manual labor. In some rare cases, machine learning can enable processes that
    were previously impossible due to human limitations. The ultimate success of achieving
    these objectives is to save costs or earn more revenue for an organization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，定义成功是至关重要的，应当在项目初期就进行定义，正如在[*第一章*](B18187_01.xhtml#_idTextAnchor015)的*定义成功*部分中介绍的那样，*深度学习生命周期*。成功可以定义为实现更高层次的目标，例如提高流程的效率或相比人工劳动提高流程的准确性。在一些罕见的情况下，机器学习可以使得以前由于人类限制无法实现的流程成为可能。实现这些目标的最终成功是为组织节省成本或创造更多的收入。
- en: A model with a metric performance score of 0.80 F1 score or 0.00123 RMSE doesn’t
    really mean anything at face value and has to be translated to something tangible
    in the use case. For instance, one should answer questions such as what estimated
    model score can allow the project to achieve the targeted cost savings or revenue
    improvements. Quantifying the success that can be obtained from model performance
    is essential, particularly as machine learning projects can be expensive to execute.
    Sometimes, the return on investment can be low if the model fails to perform at
    a certain level. After selecting the evaluation metric, it’s important to establish
    a metric threshold for success that is realistic, achievable, and based on the
    business objective.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型的度量性能得分为0.80的F1分数或0.00123的RMSE，实际上并没有直接意义，需要在使用案例中转化为可衡量的内容。例如，应该回答诸如什么样的估计模型得分可以使项目实现预定的成本节约或收入增长的问题。从模型性能中量化可以获得的成功是至关重要的，特别是在机器学习项目可能很昂贵的情况下。如果模型无法达到一定的性能水平，投资回报率可能会很低。选择评估指标后，重要的是要为成功设定一个现实、可达的指标阈值，并且该阈值应当基于业务目标。
- en: As a finale to the topic, let’s go through an example workflow to relate the
    evaluation metric to success based on a hypothetical use case.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本主题的总结，我们通过一个示例工作流来将评估指标与基于假设使用案例的成功联系起来。
- en: Let’s consider a use case to identify defective products in a manufacturing
    process using image data. Let’s assume that the cost of producing a product is
    $50 and it has a retail price of $200\. If that product is defective and it makes
    it through to a customer, this will result in an additional chargeback of $1,000
    and the return of the $200 paid by the customer to compensate for the defective
    product, which may have caused harm. On the other hand, if a good product is identified
    as defective and scrapped, it results in a cost of $250; $50 is used to produce
    it and $200 is lost opportunity cost as it is scrapped.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们要考虑一个使用图像数据来识别制造过程中的缺陷产品的使用案例。假设生产一个产品的成本为50美元，而其零售价格为200美元。如果该产品是有缺陷的并且被送到客户手中，那么将导致1000美元的额外退款费用，并且需要退还客户支付的200美元，以补偿缺陷产品可能造成的损害。另一方面，如果一个合格的产品被误判为缺陷并被报废，那么就会产生250美元的成本；其中50美元用于生产该产品，200美元则是作为报废损失的机会成本。
- en: Let’s say that we have a dataset of 10,000 product images, which is the amount
    of products produced a month. If we don’t use a model and produce and ship all
    10,000 products, we would have 95 defective products, which would cost $500,000
    in production costs (10,000 x $50) and $95,000 in chargeback costs, resulting
    in a total cost of $595,000\. After selling all the non-defective products, the
    company will gain $1,981,000 in sales (9,905 x $200). The total money earned will
    be $1,386,000.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含 10,000 张产品图像的数据集，这是每月生产的产品数量。如果我们不使用模型，生产并运输所有 10,000 个产品，我们将有 95
    个有缺陷的产品，这将导致 $500,000 的生产成本（10,000 x $50）和 $95,000 的退货费用，总成本为 $595,000。销售所有非缺陷产品后，公司将获得
    $1,981,000 的销售收入（9,905 x $200）。总收入将是 $1,386,000。
- en: Consider a case in which we use a trained deep learning model to classify these
    images as either good (negative) or defective (positive). To make sure it is worth
    using a deep learning model to optimize this process, let’s say the company needs
    to gain at least $120,000 a year in cash to make this worthwhile. Let’s also consider
    that maintaining a machine learning model costs $20,000 per month. The threshold
    in any metric needs to relate to the result of making at least $30,000 gains per
    month.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用一个训练好的深度学习模型来将这些图像分类为合格（负类）或有缺陷（正类）。为了确保使用深度学习模型来优化此过程是值得的，假设公司需要每年至少赚取
    $120,000 才能使此过程值得投资。我们还要考虑到，维护一个机器学习模型的成本是每月 $20,000。任何度量的阈值都需要与每月至少赚取 $30,000
    的收益相关。
- en: 'Say we want to use an F score-based metric. Since false negatives affect the
    total money more than false positives, we might want to use the F2 score instead
    of the F1 score. But will models with the same metric score exhibit different
    monetary returns for either of the two metrics? This is essential to understand
    so that a proper success threshold can be set. Let’s attempt to use Python code
    to analyze score behavior:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想使用基于 F 分数的度量。由于假阴性对总金额的影响比假阳性更大，我们可能想使用 F2 分数而不是 F1 分数。但是，使用相同度量分数的模型是否会在这两个度量中表现出不同的货币回报？这是理解的关键，以便可以设置适当的成功阈值。让我们尝试使用
    Python 代码来分析分数行为：
- en: 'First, we define the number of actual positives and actual negatives and the
    total data:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义实际正例和实际负例的数量以及总数据量：
- en: '[PRE0]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let’s define methods to compute precision, recall, F1 score, and F2 score:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义方法来计算精准率、召回率、F1 分数和 F2 分数：
- en: '[PRE1]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s define the method to compute the final cash we will have:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义计算最终现金的方法：
- en: '[PRE2]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s use the `compute_total_cash` method to compute the baseline cash for
    the current setup where no model is used. This is so that we can find out the
    cash success threshold required for a model to be considered valuable enough to
    be used:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `compute_total_cash` 方法来计算当前设置下的基准现金，这时没有使用任何模型。这样我们可以找出一个模型被认为有足够价值用于使用的现金成功阈值：
- en: '[PRE3]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will simulate every possible combination of true positives (`tp`),
    false positives (`fp`), true negatives (`tn`), and false negatives (`fn`) and
    compute the F1 and F2 scores:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将模拟所有可能的真阳性（`tp`）、假阳性（`fp`）、真阴性（`tn`）和假阴性（`fn`）组合，并计算 F1 和 F2 分数：
- en: '[PRE4]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s plot both of the scores independently against the total cash return
    while drawing horizontal lines using `threshold_cash_line` and `baseline_cash`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们分别绘制这两个分数与总现金回报的关系，同时使用 `threshold_cash_line` 和 `baseline_cash` 绘制水平线：
- en: '[PRE5]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will result in the following figure:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下图形：
- en: "![](img/B18187_10.01_(A).jpg)![Figure 10.1 – Cash \uFEFFversus F1 score and\
    \ cash \uFEFFversus F2 score](img/B18187_10.01_(B).jpg)"
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18187_10.01_(A).jpg)![图 10.1 – 现金与 F1 分数的关系，以及现金与 F2 分数的关系](img/B18187_10.01_(B).jpg)'
- en: Figure 10.1 – Cash versus F1 score and cash versus F2 score
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 现金与 F1 分数的关系，以及现金与 F2 分数的关系
- en: The figure suggests that the F2 score has huge fluctuations in the lower score
    range even though it weights recall higher. The goal here is to make sure a metric
    can be properly linked to success, so having wider cash fluctuations with the
    same score isn’t a desirable trait. F1 score would probably be the wiser choice
    here. Using the topmost horizontal line (which references the minimum 30,000 cash
    threshold we need to hit) as a reference, we want to find a point where it is
    not even possible to get a lower score than the threshold in the F1 score graph.
    Roughly, a 0.65 F1 score should guarantee that the model can produce a score.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图表表明，尽管F2分数对召回率进行了较高的加权，但在较低分数范围内波动很大。这里的目标是确保度量标准能与成功正确关联，因此，在相同分数下出现更大的现金波动并不是一个理想特征。F1分数在这种情况下可能是更明智的选择。以最上方的水平线（它指的是我们需要达到的最低30,000现金阈值）为参考，我们想找到一个点，在F1分数图中，无法获得比阈值更低的分数。大致上，0.65的F1分数应能保证模型产生一个有效分数。
- en: This example demonstrates the level of analysis required to properly choose
    a metric and find a threshold that can be directly linked to success while taking
    into consideration both monetary profit and loss. However, it is important to
    note that not all machine learning projects can be measured in terms of dollar
    cost. Some projects may not be directly related to cash, and that is perfectly
    acceptable. However, to be successful, it is important to quantify the value of
    the model in a way that stakeholders can understand. If nobody understands the
    value that the model provides, the project is unlikely to be successful.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本例展示了正确选择度量标准并找到可以直接与成功关联的阈值所需的分析层次，同时考虑到货币的盈利与亏损。然而，重要的是要注意，并非所有机器学习项目都能以美元成本来衡量。一些项目可能与现金无关，这完全是可以接受的。然而，要想取得成功，关键是以利益相关者能理解的方式量化模型的价值。如果没有人理解模型所带来的价值，那么项目不太可能成功。
- en: Next, let’s dive into the idea of directly optimizing the metric in a deep learning
    model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入探讨在深度学习模型中直接优化度量标准的概念。
- en: Directly optimizing the metric
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接优化度量标准
- en: The loss and the metric used to train a deep learning model are two separate
    components. One of the tricks you can use to improve a model’s accuracy performance
    against the chosen metric is to directly optimize against it instead of just monitoring
    performance for the purpose of choosing the best performing model weights and
    using early stopping. In other words, using the metric as a loss directly!
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练深度学习模型的损失函数和度量标准是两个独立的组成部分。你可以用来提高模型准确性的技巧之一是直接针对度量标准进行优化，而不仅仅是监控性能，以选择表现最佳的模型权重并使用提前停止。换句话说，就是直接将度量标准作为损失函数使用！
- en: By directly optimizing for the metric of interest, the model has a chance to
    improve in a way that is relevant to the end goal rather than optimizing for a
    proxy loss function that may not be directly related to the ultimate performance
    of the model. This simply means that the model can result in a much better performance
    when using the metric as a loss directly.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通过直接优化感兴趣的度量标准，模型有机会以与最终目标相关的方式改进，而不是仅仅为了优化一个可能与模型的最终表现无直接关系的代理损失函数。这意味着，当将度量标准作为损失函数直接使用时，模型可能会取得更好的表现。
- en: 'However, not all metrics can be used as a loss, as not all metrics can be differentiable.
    Remember that backpropagation requires all functions used to be differentiable
    so that gradients can be computed to update the neural network weights. Note that
    discontinuous methods are not all differentiable. Here are some common discontinuous
    functions that are not differentiable, along with NumPy methods to look out for
    easy identification:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有度量标准都可以用作损失函数，因为并非所有度量标准都是可微的。记住，反向传播要求所使用的所有函数都是可微的，以便计算梯度来更新神经网络的权重。请注意，离散的函数并不都可微。以下是一些常见的不可微离散函数，以及用于快速识别的NumPy方法：
- en: '`np.min`, `np.max`, `np.argmin`, and `np.argmax`'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.min`，`np.max`，`np.argmin`，和`np.argmax`'
- en: '`np.clip`'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.clip`'
- en: Other functions in NumPy to look out for are `np.sign`, `np.piecewise`, `np.digitize`,
    `np.searchsorted`, `np.histogram`, `np.fft`, `np.count_nonzero`, `np.round`, `np.cumsum`,
    and `np.percentile`
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy中需要注意的其他函数包括`np.sign`，`np.piecewise`，`np.digitize`，`np.searchsorted`，`np.histogram`，`np.fft`，`np.count_nonzero`，`np.round`，`np.cumsum`，和`np.percentile`
- en: Additionally, using a metric as a loss function can sometimes lead to suboptimal
    performance because the metric does not always capture all aspects of the problem
    that the model needs to learn in order to perform well. Some important aspects
    of a problem may be difficult to measure directly or to include in a metric. These
    aspects may form the foundation needed for a model to learn before it can proceed
    to slowly get better at the chosen metric. For example, in image recognition,
    the model needs to learn to recognize more abstract features, such as texture,
    lighting, or viewpoint before it can attempt to get better at accuracy. If these
    features are not captured in the metric, the model may not learn to recognize
    them, resulting in suboptimal performance. A good solution here is to experiment
    with more conventional loss functions initially and then fine-tune the model using
    either only the metric as a loss or a combination of the original loss and the
    metric as a loss.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用度量作为损失函数有时可能导致次优性能，因为度量并不总是能够捕捉到模型需要学习的所有方面，这些方面对模型的良好表现至关重要。问题的一些重要方面可能很难直接度量或包含在度量中。这些方面可能构成模型学习的基础，模型必须先学习这些内容，才能逐渐在选定的度量上取得更好的表现。例如，在图像识别中，模型需要先学习识别更抽象的特征，如纹理、光照或视角，才能尝试在准确性上取得进展。如果这些特征没有在度量中得到体现，模型可能不会学会识别它们，导致次优的性能。一个好的解决方案是，初期可以先尝试更常见的损失函数，然后使用度量作为损失函数，或者结合原始损失函数和度量来微调模型。
- en: While using a metric as a loss function can be beneficial in some cases, it’s
    not a surefire method of improving performance. The efficacy of this approach
    largely depends on the specific use case and the complexity of the problem being
    addressed. The performance boost achieved through this method might be minimal
    and too nuanced for some projects to even consider. However, when used successfully,
    it can lead to meaningful improvements in performance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在某些情况下使用度量作为损失函数可能是有益的，但这并不是提升性能的万无一失的方法。这种方法的有效性在很大程度上依赖于具体的应用场景以及所处理问题的复杂性。通过这种方法获得的性能提升可能微乎其微，对于某些项目来说，甚至难以察觉。然而，当成功应用时，它可以带来性能的实质性提升。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we briefly explored an overview of different model evaluation
    methods and how they can be used to measure the performance of a deep learning
    model. We started with the topic of metric engineering among all the introduced
    methods. We introduced common base model evaluation metrics. On top of this, we
    discussed the limitations of using base model evaluation metrics and introduced
    the concept of engineering a model evaluation metric tailored to the specific
    problem at hand. We also explored the idea of optimizing directly against the
    evaluation metric by using it as a loss function. While this approach can be beneficial,
    it is important to consider the potential pitfalls and limitations, as well as
    the specific use case for which this approach may be appropriate.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要概述了不同的模型评估方法，以及如何利用这些方法来衡量深度学习模型的性能。我们从所有介绍的方法中，首先探讨了度量工程的话题。我们介绍了常见的基本模型评估度量标准。基于此，我们讨论了使用基本模型评估度量标准的局限性，并引入了为特定问题量身定制模型评估度量标准的概念。我们还探讨了通过将度量作为损失函数直接进行优化的想法。虽然这种方法可以带来好处，但也需要考虑潜在的陷阱和局限性，以及这种方法适用于的特定使用场景。
- en: The evaluation of deep learning models requires careful consideration of appropriate
    evaluation methods, metrics, and statistical tests. Hopefully, after reading through
    this chapter, I have helped ease your journey into metric engineering, encouraged
    you to take the first step toward deeper metric engineering by following the guidelines
    provided, and highlighted the importance of metric engineering as a valuable component
    to improve model evaluation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型的评估需要仔细考虑适当的评估方法、度量标准和统计检验。希望通过阅读本章内容，我能帮助你顺利进入度量工程的领域，鼓励你根据提供的指南迈出深入度量工程的第一步，并强调度量工程作为提升模型评估的重要组成部分的意义。
- en: However, some information can be hidden behind a single metric value no matter
    how good and appropriate the final chosen metric is. In the next chapter, we will
    cover a key method that can help uncover either wanted or unwanted hidden behaviors
    of your neural model based on how your neural network model makes predictions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论最终选择的度量标准多么优秀和恰当，某些信息仍然可能隐藏在单一的度量值背后。在下一章，我们将介绍一种关键方法，帮助揭示神经网络模型在做出预测时隐藏的、无论是期望的还是不期望的行为。
