- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Explaining Neural Network Predictions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释神经网络预测
- en: Have you ever wondered why a facial recognition system flagged a photo of a
    person with a darker skin tone as a false positive while identifying people with
    lighter skin tones correctly? Or why a self-driving car decided to swerve and
    cause an accident, instead of braking and avoiding the collision? These questions
    illustrate the importance of understanding why a model predicts a certain value
    for critical use cases. By providing explanations for a model’s predictions, we
    can gain insights into how the model works and why it made a specific decision,
    which is crucial for transparency, accountability, trust, regulatory compliance,
    and improved performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过，为什么一个面部识别系统将一张肤色较深的人的照片标记为假阳性，而正确识别肤色较浅的人？或者，为什么一辆自动驾驶汽车选择转向并造成了事故，而不是刹车并避开碰撞？这些问题展示了在关键应用场景下理解模型为何做出某个预测值的重要性。通过为模型的预测提供解释，我们可以深入了解模型如何工作以及为何做出特定决策，这对于透明度、问责性、信任度、符合法规的要求以及提高表现至关重要。
- en: In this chapter, we will explore neural network-specific methods for explaining
    model predictions. Additionally, we will discuss how to quantify the quality of
    an explanation method. We will also discuss the challenges and limitations of
    model explanations and how to evaluate their effectiveness.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨用于解释模型预测的神经网络特定方法。此外，我们还将讨论如何量化解释方法的质量。我们还将讨论模型解释的挑战和局限性，以及如何评估其有效性。
- en: 'Specifically, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，将涵盖以下主题：
- en: Exploring the value of prediction explanations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索预测解释的价值
- en: Demystifying prediction explanation techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解密预测解释技术
- en: Exploring gradient-based prediction explanations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索基于梯度的预测解释
- en: Trusting and understanding integrated gradients
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任和理解集成梯度
- en: Using integrated gradients to aid in understanding the predictions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集成梯度来帮助理解预测
- en: Explaining prediction explanations automatically
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动解释预测
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索预测解释中的常见陷阱及如何避免它们
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些 Python 编程语言中的实际实现。要完成它，你需要在计算机上安装以下库：
- en: '`pandas`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`captum`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`captum`'
- en: '`transformers-interpret`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-interpret`'
- en: '`transformers`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers`'
- en: '`pytorch`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`numpy`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在 GitHub 上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11)。
- en: Exploring the value of prediction explanations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索预测解释的价值
- en: First off, the concept of explaining a model through its predictions is referred
    to by many other names, including explainable AI, trustable AI, transparent AI,
    interpretable machine learning, responsible AI, and ethical AI. Here, we will
    refer to the paradigm as **prediction explanations**, which is a clear and short
    way to refer to it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过模型的预测来解释模型的概念被称为许多其他名称，包括可解释 AI、可信 AI、透明 AI、可解释机器学习、负责任的 AI 和伦理 AI。在这里，我们将这种范式称为**预测解释**，这是一个简洁且明确的说法。
- en: '**Prediction explanations** is not a technique that is adopted by most machine
    learning practitioners. The value of prediction explanations highly depends on
    the exact use case. Even though it is stated that explanations can increase transparency,
    accountability, trust, regulatory compliance, and improved model performance,
    not everybody cares about these points. Instead of understanding the benefits,
    let’s look at it from a different perspective and explore some of the common factors
    that drove practitioners to adopt prediction explanations that can be attributed
    to the following conditions:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测解释**并不是大多数机器学习从业者采用的技术。预测解释的价值在很大程度上取决于具体的应用场景。尽管有说法认为，解释能够提高透明度、问责性、信任度、符合法规的要求，以及提升模型表现，但并不是每个人都关心这些问题。与其了解其好处，我们不如从另一个角度来审视，探索一些常见因素，这些因素促使从业者采用预测解释，通常归因于以下条件：'
- en: 'The prediction explanations technique provides the following benefits regarding
    the utilization of your built model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预测解释技术为利用您构建的模型提供以下好处：
- en: '**Transparency**: Prediction explanations allow model prediction consumers
    to have access to reasons, which will, in turn, enforce the confidence of the
    consumer in the predictions made. A transparent model allows consumers to objectively
    gauge their intelligence, which increases trust and increases adoption.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明性**：预测解释使得模型预测的消费者能够了解理由，这反过来会增强消费者对预测结果的信任。一个透明的模型使消费者能够客观评估其智能，从而增加信任度并促进采用。'
- en: '**Accountability**: Prediction explanations allow consumers to perform root
    cause analysis, which is especially important in critical use cases or in use
    cases where there is a human in the loop to make the final decision with a model’s
    prediction as a reference.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：预测解释使得消费者能够进行根本原因分析，这在关键使用案例或在决策过程中有人工干预、以模型预测作为参考的使用案例中尤其重要。'
- en: '**Trust**: Without trust, nobody will use the prediction of the model. Prediction
    explanations provide a small boost toward achieving higher trust.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任**：没有信任，没人会使用模型的预测。预测解释有助于在建立更高信任的过程中提供一小步推动。'
- en: '**Regulatory compliance**: Some governments enforce laws that require decisions
    made by computer systems to be explainable in certain industries such as banks
    and insurance.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性要求**：一些政府执行法律，要求在某些行业（如银行和保险）中，由计算机系统做出的决策必须可解释。'
- en: '**Metric performance**: The capability to perform root cause analysis can lead
    to a better understanding of either the model’s behavior or the training dataset.
    This will, in turn, allow machine learning practitioners to improve or fix the
    issues found using prediction explanations, and eventually lead to improved metric
    performance.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量性能**：进行根本原因分析的能力可以帮助更好地理解模型的行为或训练数据集。这反过来又能使机器学习从业者通过预测解释来改进或修复发现的问题，最终提高度量性能。'
- en: 'This list of benefits makes it worth it to utilize prediction explanations
    on any use case and model. However, the value of using prediction explanations
    increases exponentially with certain conditions. When the utilization is based
    on a specific goal, the method becomes twice as useful. Let’s take a step further
    and explore some of the common conditions that drove practitioners to utilize
    the prediction explanations technique:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这份好处清单使得在任何使用案例和模型中使用预测解释都变得值得。然而，在某些条件下，使用预测解释的价值呈指数增长。当使用是基于特定目标时，该方法的效果将加倍。让我们进一步探讨一些促使从业者采用预测解释技术的常见条件：
- en: '**Critical and high-impact use case**: In these use cases, the model’s decisions
    can typically result in significant consequences for human welfare, safety, or
    well-being. Understanding the model’s behavior in every way can help mitigate
    the worst case from happening. This can range from billions of money lost to actual
    human life lost.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键和高影响力的使用案例**：在这些使用案例中，模型的决策通常会对人类福利、安全或福祉产生重大后果。全面理解模型的行为有助于降低最坏情况的发生风险。这些后果可能从数十亿的损失到实际的人命丧失。'
- en: '**Failing to achieve the required threshold for success**: A machine learning
    project can’t move forward to the model deployment stage in the machine learning
    life cycle if it can’t even achieve success thresholds during the model development
    stage. Understanding the behavior of the model toward different inputs can help
    signal whether the data is of bad quality, help indicate whether the data has
    biased patterns that promote overfitting, and generally help debug how to improve
    the model’s performance.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未达到成功所需的阈值**：如果机器学习项目在模型开发阶段无法达到成功的阈值，那么它就无法进入机器学习生命周期中的模型部署阶段。理解模型对不同输入的行为可以帮助判断数据质量是否差，帮助发现数据是否存在促进过拟合的偏差模式，并且通常有助于调试如何改进模型的性能。'
- en: '**The model makes wrong predictions with the simplest examples**: Making wrong
    predictions with the most complex examples is expected, especially when humans
    also could have made mistakes in making the same decisions. When it comes to the
    simplest examples, making an error would indicate that the model is not learning
    the right things. Understanding what a model is focusing on could be key to figuring
    out why a model failed.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型在最简单的例子中做出错误预测**：在最复杂的例子中做出错误预测是可以预见的，尤其是当人类在做出相同决策时也可能犯错。然而，在最简单的例子中出现错误，说明模型没有学习到正确的内容。了解模型聚焦的内容可能是理解模型失败原因的关键。'
- en: '**Regulatory laws mandate accountability for using machine learning models
    in decision-making**: This means prediction explanations will be required not
    typically as a method for the machine learning practitioner to understand how
    the model behaves or with different inputs, but instead, to be used after a model
    is deployed, which will allow the people that consume the predictions understand
    why a certain decision has been made for accountability.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监管法律要求在决策过程中对使用机器学习模型的责任进行追溯**：这意味着预测解释将不再仅仅是机器学习从业人员理解模型如何在不同输入下行为的工具，而是在模型部署后使用，帮助使用这些预测结果的人们理解为何做出某个决策，从而确保责任可追溯。'
- en: '**The model fails to make proper predictions after it is deployed**: Have you
    ever thought about whether a good model with high accuracy metric performance
    in your cross-validation setup means that the model pays attention to the right
    things? Even with cross-validation partitioning strategies, data can still be
    very biased toward a certain set of conditions. This means that when the model
    gets deployed into the real world, the data it encounters during the production
    operation can be from a distribution and set of conditions that are different
    than what was available in the original dataset used for model development. When
    this happens, prediction explanations can help uncover biases and unwanted behaviors
    of the model either in the new data or data used for model development. In other
    words, you’ll need to quantify wrong predictions from right predictions.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型在部署后未能做出正确预测**：你是否曾考虑过，在交叉验证设置中，具有高准确性度量性能的好模型是否意味着模型关注的是正确的内容？即使使用交叉验证分区策略，数据仍然可能偏向某些特定的条件。这意味着，当模型在实际环境中部署时，它遇到的数据可能来自于与原始模型开发数据集不同的分布和条件。在这种情况下，预测解释可以帮助揭示模型在新数据或用于模型开发的数据中的偏差和不良行为。换句话说，你需要量化正确预测和错误预测之间的差异。'
- en: It is fine to not utilize prediction explanation techniques in your use case
    and development workflow. However, when these edge cases happen, know that prediction
    explanation techniques are your key tool to help overcome your obstacles. But
    what exactly do prediction explanations explain? In the next section, we will
    discuss a short overview of prediction explanation methods and dive into the method
    category we will be introducing in this chapter, which is specifically for neural
    networks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的使用案例和开发工作流中，不使用预测解释技术也是可以的。然而，当遇到这些极端情况时，预测解释技术将是你克服障碍的关键工具。但预测解释究竟解释了什么呢？在接下来的部分，我们将简要介绍预测解释方法，并深入探讨本章将介绍的、专门针对神经网络的预测解释方法。
- en: Demystifying prediction explanation techniques
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解密预测解释技术
- en: Prediction explanations is a technique that attempts to explain the logic behind
    a model’s decision, given input data. Some machine learning models are built to
    be more transparent and explainable out of the box. One example is a decision
    tree model, which is built from the ground up using explicit conditioning rules
    to split data into multiple partitions that result in specific decisions, allowing
    the predictions to be explained through the explicit rules that were used to predict
    the data sample. However, models such as neural networks are treated like a black
    box without any straightforward way to retrieve the reasons the decision was made
    directly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 预测解释是一种尝试解释模型决策背后逻辑的技术，给定输入数据。一些机器学习模型被设计为开箱即用时就更具透明性和可解释性。例如，决策树模型就是一个从头开始构建的模型，使用显式的条件规则将数据划分成多个分区，最终得出特定的决策，从而可以通过预测数据样本时使用的显式规则来解释预测结果。然而，像神经网络这样的模型通常被视为黑箱，没有直接的方式来获取决策背后的原因。
- en: 'The logic of a model’s decision on a data sample can be explained and presented
    in a variety of ways, so long it contains information on how the final decision
    was made. Additionally, predictions made by a machine learning model can be explained
    in either a model-agnostic or model-specific way. There are a few types of explanations
    that can be made using the model’s predictions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个模型对数据样本的决策逻辑可以通过多种方式进行解释和展示，只要它包含关于最终决策如何做出的信息。此外，机器学习模型做出的预测可以以模型无关或特定模型的方式进行解释。使用模型的预测可以进行几种类型的解释：
- en: '**Saliency-based explanation**: This is also known as importance attribution,
    feature importance, or feature impact'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于显著性（Saliency）的解释**：这也被称为重要性归因、特征重要性或特征影响'
- en: '**Transparent model logic-based explanation**: Provide a rationale on why a
    decision is made'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明模型逻辑基础的解释**：提供为什么做出某个决策的理由'
- en: '**Exemplar-based explanation**: Using similar data to reason why a label is
    predicted'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于示例的解释**：使用相似的数据推理为什么预测某个标签'
- en: 'Neural networks, being a black box out of the box, can only be explained through
    saliency-based explanation or exemplar-based explanation. However, workarounds
    have been invented to achieve indirect transparent model logic-based explanations
    for neural networks using knowledge distillation methods from a neural network
    to more transparent and interpretable models such as linear models. Additionally,
    attention mechanisms implemented in transformers provide a shallow way to check
    for feature importance through its attention maps as a saliency method but fall
    short in the following components:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络作为一个“黑箱”，只能通过基于显著性（saliency）的方法或基于示例（exemplar）的方法进行解释。然而，已经发明了绕道方法，通过使用从神经网络到更透明和可解释模型（如线性模型）的知识蒸馏方法，来实现间接的透明模型逻辑解释。此外，Transformer中实现的注意力机制提供了一种通过其注意力图检查特征重要性的浅层方法，但在以下几个方面有所不足：
- en: Attention weights are not feature-specific as they are obtained by calculating
    the interactions between all input tokens and the output token of interest. This
    means the weight does not represent a true reliance on an isolated feature.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力权重并非特定于某个特征，因为它们是通过计算所有输入标记与感兴趣输出标记之间的相互作用得到的。这意味着该权重并不代表对单一特征的真正依赖。
- en: The inputs attended to by the attention mechanism can still not be used in later
    parts of the network.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制关注的输入在网络的后续部分仍然无法使用。
- en: Attention maps can be biased and cause some features to be neglected.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力图可能存在偏差，并导致某些特征被忽视。
- en: Additionally, neural-networks-based explanation techniques should be opted over
    model-agnostic explanation techniques as they are capable of providing more detailed
    and nuanced explanations while being more efficient in terms of both computational
    requirements and time. In this chapter, we will focus on saliency-based explanations
    for neural networks using more reliable methods.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，应该优先选择基于神经网络的解释技术而非模型无关的解释技术，因为它们能够提供更详细和细致的解释，同时在计算需求和时间方面更为高效。在本章中，我们将专注于使用更可靠的方法进行神经网络的基于显著性的解释。
- en: In the next section, we will dive into the core workhorse behind neural network
    model-specific explanations, which are gradient-based saliency explanations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入探讨神经网络模型特定解释背后的核心工作——基于梯度的显著性解释。
- en: Exploring gradient-based prediction explanations
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索基于梯度的预测解释
- en: Most up-to-date neural network-based explanation techniques today are variations
    of using the gradients that can be obtained through backpropagation. Gradient-based
    explanations for neural network models work because they rely on the fundamental
    principle of how the weights in a neural network are updated during the training
    process using backpropagation. During backpropagation, the partial derivatives
    of the loss function concerning the weights in the network are calculated, which
    gives us the gradient of the loss function concerning the weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最先进的神经网络基于解释技术是使用通过反向传播获得的梯度的变种。基于梯度的神经网络模型解释之所以有效，是因为它们依赖于神经网络中权重在训练过程中如何通过反向传播更新的基本原理。在反向传播过程中，计算网络中权重的损失函数的偏导数，从而得到损失函数相对于权重的梯度。
- en: This gradient provides us with a measure of how much the input data contributes
    to the overall loss. Remember that gradients measure the sensitivity of the input
    value concerning the loss function. This means it provides the degree of fluctuation
    of the predictions when you modify the specific input value, which represents
    the importance of the input data. Input data can be chosen to be the weights of
    the neural network or the actual input to the entire neural network. In most cases,
    the actual input-based explanations are enough to provide clarity on the important
    feature groups. However, sometimes, a more fine-grained explanation is needed
    to decode the underlying characteristics of the highly attributed and important
    actual input data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个梯度为我们提供了输入数据对整体损失的贡献程度。记住，梯度衡量的是输入值相对于损失函数的敏感度。这意味着它提供了修改特定输入值时预测结果波动的程度，代表了输入数据的重要性。输入数据可以选择为神经网络的权重，或者是整个神经网络的实际输入。在大多数情况下，基于实际输入的解释已足以提供对重要特征组的清晰解释。然而，有时需要更精细的解释来解码高度归因且重要的实际输入数据的潜在特征。
- en: 'For example, consider the use case of identifying animal breeds with image
    data using a CNN. For an image with a dog standing on grass, if an attribution
    method signals that both the dog and grass are important, why is the grass important?
    This can be for one of the following reasons:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑使用卷积神经网络（CNN）根据图像数据识别动物品种的情况。如果一张狗站在草地上的图像，某个归因方法显示狗和草地都很重要，那么草地为什么重要呢？这可能是以下几个原因之一：
- en: The fur of the dog and the grass are both identified by a single convolutional
    filter.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗的毛发和草地是通过一个卷积滤波器识别的。
- en: Fur and grass are explicitly separately identified by different filters but
    shown as important similarly because the model might have overfitted to think
    that dogs need to be accompanied by grass. This might mean that the training dataset
    only contains images of dogs with grass and signals the need to add more dog images
    without grass.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毛发和草地通过不同的滤波器被明确区分，但由于模型可能过拟合，认为狗必须伴随草地出现，因此它们也被视为同样重要。这可能意味着训练数据集只包含带有草地的狗的图像，暗示需要添加更多没有草地的狗的图像。
- en: This will require diving deeper into important filters that are attributed highly
    among the filters that are activated and have something to do with the grass and
    the dog. Even after pinpointing the filters that are both important, highly activated
    for the image, and have something to do with the grass and the dog, you can’t
    know for sure what patterns the filter is identifying. Is it a high-level feature
    such as the dog’s shape? Or is it fur? In cases like this, it is highly beneficial
    to visualize the patterns directly. This topic will be discussed in more detail
    in [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184), *Interpreting* *Neural Networks*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要更深入地挖掘在激活的滤波器中与草地和狗相关的那些重要滤波器。即便确定了那些既重要又高度激活的滤波器，并且与草地和狗有关，仍然无法确切知道该滤波器识别的模式是什么。它是一个高级特征，比如狗的形状？还是毛发？在这种情况下，直接可视化这些模式是非常有益的。这个主题将在[*第12章*](B18187_12.xhtml#_idTextAnchor184)中详细讨论，*解释*
    *神经网络*。
- en: Gradients by themselves, however, are not reliable enough to be used as-is as
    an explanation method. More generally, the quality of pure gradients to be used
    as a feature importance explanation method is low. However, the quality of explanation
    methods is not a quantitative metric. The perceived quality of the explanations
    generated is subjective to the requirements of the consumer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，梯度本身并不可靠，不能直接作为解释方法。更一般来说，纯梯度作为特征重要性解释方法的质量较低。然而，解释方法的质量并不是一个定量度量。生成的解释的感知质量是主观的，取决于消费者的需求。
- en: Consider a scenario where a financial institution wants to develop a credit
    scoring model to determine loan approvals. The institution is interested in using
    an explanation technique to understand which features are most important in the
    model’s predictions. A bank might prioritize an explanation technique that focuses
    on the features that have the greatest impact on creditworthiness, such as credit
    history, income, and outstanding debts. This would help the bank make informed
    lending decisions and manage its risks effectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某金融机构希望开发一个信用评分模型，以确定贷款批准。该机构希望使用一种解释技术来了解哪些特征对模型预测最为重要。银行可能更倾向于选择一种解释技术，重点解释那些对信用状况影响最大的特征，如信用历史、收入和未偿还的债务。这将帮助银行做出明智的贷款决策，并有效地管理风险。
- en: On the other hand, an individual loan applicant may prefer a more comprehensive
    explanation that provides insight into how the model evaluated their specific
    financial situation, including factors beyond the top three most important features,
    such as their employment history and recent financial hardships. This would help
    the applicant make informed decisions about how to improve their creditworthiness
    in the future. Selecting an appropriate explanation technique is important as
    it should cater to the audience and their specific needs to make the explanation
    clear and useful for its intended purpose.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，个别贷款申请人可能更倾向于获得一种更为全面的解释，解释应提供关于模型如何评估他们的具体财务状况的洞见，包括除了前三个最重要特征之外的因素，如他们的就业历史和近期的财务困境。这将帮助申请人做出明智的决策，以便在未来改善他们的信用状况。选择合适的解释技术非常重要，因为它应当针对受众及其特定需求，确保解释清晰且有助于实现其预定目的。
- en: Even so, there are axiomatic-based evaluations, which involve evaluating based
    on a set of principles or axioms that are considered desirable. Of all techniques,
    a technique called **integrated gradients** stands out for its focus on neural
    network models while being developed to satisfy widely accepted axioms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，仍然存在基于公理的评估方法，这些评估方法是基于一组被认为是理想的原则或公理来进行的。在所有技术中，一种名为**集成梯度**的技术因其专注于神经网络模型，并在开发过程中力求满足广泛接受的公理而脱颖而出。
- en: Next, we’ll understand how integrated gradients explain a neural network prediction
    and understand how the method satisfies widely accepted axioms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解集成梯度如何解释神经网络预测，并理解该方法如何满足广泛接受的公理。
- en: Trusting and understanding integrated gradients
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任并理解集成梯度
- en: First off, the integrated gradient technique is available off-the-shelf in a
    few open source libraries such as **shap** and **captum**. We can utilize the
    method from the library without reimplementing the method. However, obtaining
    explanations without understanding the technicalities behind the technique can
    reduce trust in the explanations. If you don’t trust the technique, the explanation
    results themselves hardly mean anything. Subsequently, if you don’t explain your
    predictions, the prediction results themselves hardly mean anything! In this section,
    we will dive into what integrated gradients do so that you can trust what integrated
    gradients can explain to you.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，集成梯度技术在一些开源库中已经可以直接使用，例如**shap**和**captum**。我们可以直接利用这些库中的方法，而无需重新实现该方法。然而，如果在不了解技术细节的情况下使用该方法，可能会降低对解释结果的信任。如果你不信任该技术，那么解释结果本身几乎没有任何意义。因此，如果你不解释你的预测，预测结果本身也几乎没有任何意义！在这一部分，我们将深入探讨集成梯度的作用，以便你可以信任集成梯度所能解释的内容。
- en: 'Integrated gradients add a few extra components to the basic gradient-based
    feature importance-based explanations. These components are geared toward satisfying
    a few critical axioms that make or break the reliability of an explanation method.
    These axioms are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 集成梯度在基于基本梯度的特征重要性解释方法中添加了一些额外的组成部分。这些组成部分旨在满足几个关键公理，这些公理决定了解释方法的可靠性。这些公理如下：
- en: '**Sensitivity**: A model that produces two different predictions on two data
    samples that differ in that only a single feature should be given a non-zero importance
    score. Activation functions such as ReLU break this axiom with a zero input value
    as the gradients would also be reduced to zero, even when the predictions are
    different between the two predictions.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感性**：对于两个数据样本，模型在这两个样本之间给出了两个不同的预测，而这两个样本唯一的不同之处在于一个特征应被赋予非零重要性分数。像ReLU这样的激活函数会打破这一公理，因为当输入值为零时，梯度也会被降至零，尽管两个预测之间的结果是不同的。'
- en: '**Implementation invariance**: Two models that have the same performance across
    all possible data must produce the same importance scores. Assume that any external
    data and data received during the entire lifetime of the model is included.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现不变性**：两个在所有可能数据上表现相同的模型，必须产生相同的重要性得分。假设任何外部数据以及在整个模型生命周期中接收到的数据都包括在内。'
- en: '**Completeness**: One of the best ways to make a good-quality explanation for
    both human and machine learning explanations is to provide counterfactual reports.
    To evaluate model accuracy performance, having a baseline model works the same
    way. For explanation methods, this involves engineering a baseline data sample
    that can produce a neutral prediction. Completeness means that the importance
    score for every feature column must add up to the difference in prediction score
    between the baseline sample and the targeted sample. This axiom can be more useful
    in some use cases, such as in regression, where the prediction is directly used
    as an output rather than in a multiclass setting where the predictions are only
    used for choosing the top class.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：对于人类和机器学习的解释，提供反事实报告是制作高质量解释的最佳方法之一。为了评估模型的准确性表现，使用基线模型的效果相同。对于解释方法，这意味着构建一个可以产生中立预测的基线数据样本。完整性意味着每个特征列的重要性得分必须加起来等于基线样本与目标样本之间预测得分的差异。这个公理在一些使用场景中可能更有用，比如回归问题中，预测值直接作为输出，而不是在多类设置中，预测值仅用于选择最优类别。'
- en: '**Linearity**: If you linearly combine two neural network models, which is
    a simple weighted addition, the explanation of the combined neural network on
    a data sample must be the same weighted addition of the individual explanation
    of the two neural network models. Linear behaviors that are implemented in the
    model should be respected. A straightforward axiom is specifically designed as
    some methods generate unaccountable importance values.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：如果你线性地组合两个神经网络模型，即简单的加权相加，那么在数据样本上的组合神经网络的解释必须等于两个单独神经网络模型解释的加权相加。模型中实现的线性行为应当得到尊重。一个直接的公理专门设计用于一些方法产生无法解释的重要性值。'
- en: 'Now that we understand the core axioms that can be used to compare methods,
    know that integrated gradients satisfy all of these axioms. Simply put, integrated
    gradients integrate the gradients from samples that are sampled from a straight-line
    path (linear interpolation) from a chosen baseline data sample and the target
    data sample and multiply it by the difference between the predictions of the target
    sample and baseline sample. The path integral of the gradients is the area under
    the curve of the gradient values along the straight path. This value represents
    the change in the model output as the input feature’s value changes along the
    path. The rate of change directly translates to feature importance. By multiplying
    this value by the difference between the actual input value and the baseline input
    value, we get the contribution of the feature to the model output for integrated
    gradients. *Figure 11**.1* showcases this process with the results obtained by
    computing integrated gradients on the predictions of a pre-trained ResNet50 model
    on an image of an orange. The model is capable of predicting the orange class
    and predicts the image to be an orange with a 46% probability:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了可以用来比较方法的核心公理，值得注意的是，集成梯度满足所有这些公理。简而言之，集成梯度通过从一条直线路径（线性插值）中采样的样本来集成梯度，路径从选定的基线数据样本到目标数据样本，然后将其乘以目标样本和基线样本预测值之间的差异。梯度的路径积分就是梯度值沿直线路径下的曲线下面积。这个值表示输入特征值沿路径变化时，模型输出的变化。变化速率直接转化为特征重要性。通过将这个值与实际输入值和基线输入值之间的差异相乘，我们得到集成梯度中该特征对模型输出的贡献。*图
    11.1*展示了这个过程，结果是通过在一个橙子图像上计算预训练的 ResNet50 模型预测的集成梯度。该模型能够预测橙子类别，并以46%的概率预测该图像为橙子：
- en: '![Figure 11.1 – An illustration of how integrated gradients are computed for
    an image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class](img/B18187_11_01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 使用预训练的 ResNet50 模型计算橙子图像的集成梯度的示意图，该模型能够预测橙子类别](img/B18187_11_01.jpg)'
- en: Figure 11.1 – An illustration of how integrated gradients are computed for an
    image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 使用经过预训练的ResNet50模型计算集成梯度的示意图，该模型能够预测橙子类别的图像
- en: 'More intuitively, this means that integrated gradients measure the rate of
    change of model predictions as the target input feature’s value changes toward
    a neutral baseline sample and provides accountable importance that is properly
    scaled according to the predictions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更直观地说，这意味着集成梯度衡量的是当目标输入特征的值朝向中立基准样本变化时，模型预测的变化率，并提供了根据预测正确缩放的可解释的重要性：
- en: Feature importances = prediction difference x Area under gradients along path
    curve
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性 = 预测差异 × 梯度路径曲线下的面积
- en: This allows it to capture the sensitivity of the model’s output to the feature
    across its entire range of values. Integrated gradients ensure that the contribution
    of each feature is proportional to its effect on the model’s output by aggregating
    the gradients from samples in the linear interpolated path between the baseline
    and target. Using plain gradients as the core allows integrated gradients to satisfy
    implementation invariance as the chain rule used to obtain partial derivatives
    allows gradients to be computed partially.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它能够捕捉模型输出对特征在整个值范围内变化的敏感度。集成梯度确保通过从基准样本到目标样本之间线性插值路径中的样本聚合梯度，确保每个特征的贡献与其对模型输出的影响成比例。使用普通梯度作为核心，集成梯度能够满足实现不变性，因为用于计算偏导数的链式法则使得梯度可以部分计算。
- en: For integrated gradients, if a feature has twice the effect on the model’s output
    compared to another feature, its attribution score will be twice as high because
    the gradients along the path will be twice as high for that feature, thus satisfying
    linearity. Additionally, since the importance is scaled by the difference in predictions
    between the baseline input and target input, integrated gradients will satisfy
    the completeness axiom. These are local explanations that provide reasons for
    predictions through individual data samples. As a bonus, by aggregating the local
    feature importance of all data samples through mean or median, you can obtain
    the global feature importance of the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于集成梯度，如果某一特征对模型输出的影响是另一特征的两倍，那么它的归因得分将是另一特征的两倍，因为该特征沿路径的梯度将是另一特征的两倍，从而满足线性关系。此外，由于重要性是通过基准输入和目标输入之间的预测差异来缩放的，集成梯度将满足完整性公理。这些是局部解释，通过个体数据样本提供预测结果的理由。作为附加优势，通过通过均值或中位数聚合所有数据样本的局部特征重要性，你可以获得模型的全局特征重要性。
- en: Next, we will attempt to use integrated gradients to explain predictions from
    a model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将尝试使用集成梯度来解释模型的预测结果。
- en: Using integrated gradients to aid in understanding predictions
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成梯度来辅助理解预测结果
- en: 'At the time of writing, two packages provide easy-to-use classes and methods
    to compute integrated gradients, which are the `captum` and `shap` libraries.
    In this tutorial, we will be using the `captum` library. The `captum` library
    supports models from TensorFlow and PyTorch. We will be using PyTorch here. In
    this tutorial, we will be working on explaining a SoTA transformer model called
    **DeBERTA** on the task of text sentiment multiclass classification. Let’s go
    through the use case step by step:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 截至写作时，两个软件包提供了易于使用的类和方法来计算集成梯度，分别是`captum`和`shap`库。在本教程中，我们将使用`captum`库。`captum`库支持来自TensorFlow和PyTorch的模型。我们将在这里使用PyTorch。在本教程中，我们将通过解释一个名为**DeBERTA**的SoTA（最先进）变压器模型，来处理文本情感多分类任务。让我们一步一步地来了解这个用例：
- en: 'First, let’s import the necessary libraries and methods:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库和方法：
- en: '[PRE0]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will import a custom text sentiment dataset made for this tutorial:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导入为本教程自定义的文本情感数据集：
- en: '[PRE1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This data contains 100 rows with a balanced distribution between three sentiment
    classes called “neutral”, “positive,” and “negative.” We will dive into a few
    samples of this dataset later.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该数据包含100行，三种情感类别（“中立”，“积极”，“消极”）的分布均衡。稍后我们将深入分析该数据集中的几个样本。
- en: 'Now, we will define the label mapping that will be used to map the sentiment
    labels to a numeric ID and vice versa:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义标签映射，用于将情感标签映射到数字ID以及反向映射：
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will create a method that applies text pre-processing that tokenizes
    the text data using a pre-trained, byte-pair-encoding-based tokenizer:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个方法，应用文本预处理，通过使用预训练的基于字节对编码的分词器对文本数据进行分词：
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To train a model, we need to have a cross-validation strategy, so we will use
    a simple train and validation split here:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了训练一个模型，我们需要制定交叉验证策略，因此我们将在这里使用一个简单的训练和验证集划分：
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since we will be using the PyTorch model from the Hugging Face Transformers
    library, we need to use the PyTorch dataset format and split the data that will
    be used to train the model. Here, we will define the PyTorch dataset and initialize
    both the training and validation dataset instances:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将使用来自Hugging Face Transformers库的PyTorch模型，我们需要使用PyTorch数据集格式并拆分数据集以训练模型。在这里，我们将定义PyTorch数据集并初始化训练和验证数据集实例：
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that the dataset has been pre-processed and is ready to be used for training,
    let’s load our randomly initialized DeBERTA model from Hugging Face:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在数据集已经预处理完成，准备好用于训练，让我们从Hugging Face加载我们随机初始化的DeBERTA模型：
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since this is a multiclass setup, we will use the accuracy metric that will
    be computed at each epoch, along with the cross-entropy loss, which will be used
    to train the model:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个多类别设置，我们将使用准确率度量，它将在每个周期计算，并与交叉熵损失一起使用，后者将用于训练模型：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will define the training arguments:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义训练参数：
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will save the checkpoints in the `results` folder, train the model with
    100 iterations of warmup, load the best model at the end of training, and set
    the model learning rate to a very small number of 0.000025, which is required
    to properly converge to a solution.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把检查点保存到`results`文件夹中，用100次热身迭代训练模型，在训练结束时加载最佳模型，并将模型学习率设置为非常小的0.000025，这是正确收敛到解所必需的。
- en: 'Next, we will initialize the Hugging Face `trainer` instance, which will take
    in the training arguments and use them to execute the actual training process:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化Hugging Face的`trainer`实例，它将接收训练参数并利用这些参数执行实际的训练过程：
- en: '[PRE9]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that an early stopping of 20 iterations is used. This stops the model
    from being trained when the model doesn’t improve on the validation partition
    for the specified iterations, along with 1,000 epochs of training.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，使用了20次迭代的提前停止。当模型在指定的迭代次数内没有在验证集上有所改进时，提前停止将停止模型的训练，同时进行1,000次训练周期。
- en: Finally, let’s train and print the final best model evaluation score!
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们进行训练并打印出最终的最佳模型评估分数！
- en: '[PRE10]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the following output:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生如下输出：
- en: '[PRE11]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '95.23% accuracy is a pretty good score for a multiclass model. You might think
    that the model is good enough to be deployed, but is it more than what meets the
    eye? Let’s investigate the model’s behavior through prediction explanations using
    integrated gradients and see whether the model is performing. First, let’s define
    the explainer instance from the `transformers_interpret` library:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 95.23%的准确率对于一个多类别模型来说是一个相当不错的分数。你可能认为该模型已经足够好，能够部署，但它真的是表面看到的那么简单吗？让我们通过使用集成梯度的预测解释来调查模型的行为，看看它的表现如何。首先，让我们从`transformers_interpret`库定义解释器实例：
- en: '[PRE12]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `transformers_interpret` library uses the `captum` library under the hood
    and implements methods and classes to make it straightforward to explain and visualize
    text importance based on models made using the Hugging Face Transformers library.
    Among all these things, we’re mapping the importance score to a color code and
    mapping token IDs back to the actual token string.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`transformers_interpret`库在底层使用`captum`库，并实现了方法和类，使得基于Hugging Face Transformers库构建的模型可以轻松地解释和可视化文本重要性。在所有这些操作中，我们将重要性得分映射到颜色代码，并将令牌ID映射回实际的令牌字符串。'
- en: 'Next, we’ll explain two samples for the negative and positive labels from the
    validation partition, which is conveniently located in the first three indices:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将解释来自验证集的两个样本，分别是负标签和正标签，这些样本恰好位于前面三个索引位置：
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will result in the output displayed in *Figure 11**.2*:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致*图 11.2*中显示的输出：
- en: "![Figure 11.2 – \uFEFFtransformers_interpret-based integrated gradients results\
    \ on the validation dataset](img/B18187_11_02.jpg)"
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 基于transformers_interpret的集成梯度在验证数据集上的结果](img/B18187_11_02.jpg)'
- en: Figure 11.2 – transformers_interpret-based integrated gradients results on the
    validation dataset
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 基于transformers_interpret的集成梯度在验证数据集上的结果
- en: The words highlighted in green represent a positive attribution, while those
    highlighted in red represent a negative attribution toward the predicted label
    class. The darker the green, the stronger the positive attribution toward the
    predicted label class. The darker the red, the stronger the negative attribution
    toward the predicted label class in general.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色高亮的词表示积极的归因，而红色高亮的词表示对预测标签类别的负面归因。绿色越深，表示对预测标签类别的积极归因越强。红色越深，表示对预测标签类别的负面归因越强。
- en: The first and second examples show correctly predicted positive sentiment sentences.
    For the first example, based on common human sense, *amazing* is the word that
    should contribute the most to positive attribution. For the second example, *happy*
    should be the word that is emphasized as the word contribution toward positive
    sentiment. However, for both examples, the word *bored* is used instead as a strong
    indicator of the positive sentiment prediction, which is not the behavior that
    we want. This indicates that the dataset has a bias where the word *bored* could
    be present in all samples labeled with positive sentiment.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个和第二个示例显示了正确预测的积极情感句子。对于第一个示例，根据常识，*amazing* 是最应该对积极情感归因的词汇。对于第二个示例，*happy*
    应该是对积极情感做出贡献的词汇。然而，在这两个示例中，词汇 *bored* 被错误地用作强烈的积极情感预测指标，这不是我们想要的行为。这表明数据集存在偏差，词汇
    *bored* 可能在所有标记为积极情感的样本中都有出现。
- en: The third and fourth examples show a correctly predicted negative sentiment.
    In the third example, the sentences *never going to end* and *keeps dragging on*
    should be the focus of the negative sentiment prediction. In the fourth example,
    the word *wrong* and the phrases *has gone wrong* and *so frustrated* should be
    the focus of the negative sentiment prediction. However, both samples consistently
    show dependence on the word *day* to get their negative sentiment prediction.
    This indicates that the dataset has a bias regarding the word *day* occurring
    frequently in the samples that are labeled with negative sentiment.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个和第四个示例展示了正确预测的负面情感。在第三个示例中，句子 *never going to end* 和 *keeps dragging on*
    应该是负面情感预测的重点。在第四个示例中，词汇 *wrong* 以及短语 *has gone wrong* 和 *so frustrated* 应该是负面情感预测的重点。然而，这两个示例都始终依赖于词汇
    *day* 来进行负面情感预测。这表明数据集存在偏差，词汇 *day* 经常出现在标记为负面情感的样本中。
- en: All of this means that either the data has to be prepared all over again or
    more data that diversifies the distribution of word usage should be added to the
    dataset so that a proper model can be built.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些意味着，要么需要重新准备数据，要么需要添加更多的数据，以多样化词汇使用的分布，这样才能构建一个合适的模型。
- en: This tutorial demonstrates a single type of benefit that can be derived from
    utilizing prediction explanations. In particular, this shows the case where the
    model is not capturing the behavior required to be reliably deployed, even when
    the accuracy-based metric has a good score.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了利用预测解释可以获得的一种单一类型的好处。特别是，它展示了模型未能捕捉到可靠部署所需的行为，即便基于准确率的指标得分良好。
- en: The integrated gradients technique is a flexible technique that can be applied
    to any neural network model for any type of input variable type. To benefit from
    the explanation results, you will need to derive meaningful insights in the context
    of the business goal. To derive meaningful insights and conclusions from the results
    of integrated gradients, it’s essential to apply common sense and logical reasoning,
    as was presented in the tutorial manually. However, there is a method that you
    can use to try to obtain assistance in obtaining meaningful insights, especially
    when there is too much data and too many variations to decode manually. We will
    dive into this in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 集成梯度技术是一种灵活的技术，可以应用于任何神经网络模型以及任何类型的输入变量类型。为了从解释结果中受益，你需要在业务目标的背景下得出有意义的见解。要从集成梯度的结果中得出有意义的见解和结论，运用常识和逻辑推理至关重要，正如在教程中手动演示的那样。然而，有一种方法可以帮助你获得有意义的见解，尤其是在数据量过大且变数太多无法手动解码时。我们将在下一节中深入讨论这个方法。
- en: Explaining prediction explanations automatically
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动解释预测解释
- en: One useful method that helps in deriving insights from prediction explanations
    is none other than using **large language models** (**LLMs**) such as ChatGPT.
    ChatGPT is a transformer model that is trained to provide results that match logical
    reasoning related to the instructions provided. The theory here is that if you
    can format your prediction explanations data in a way that it can be fed into
    a transformer model, and instruct the LLM to derive insights from it, you will
    be able to obtain insights from multiple different perspectives.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一种从预测解释中得出洞见的有用方法就是使用**大型语言模型**（**LLMs**），例如ChatGPT。ChatGPT是一个训练用于提供与给定指令相关的逻辑推理结果的变换模型。理论上，如果你能将预测解释数据格式化，以便可以输入到变换模型中，并指示LLM从中获取洞见，你就能够从多个不同角度获得洞见。
- en: 'In the previous tutorial, we attempted to explain the explanations of four
    different samples consisting of two correctly predicted positive sentiment examples
    and two correctly predicted negative sentiment examples. Now, let’s use an LLM
    model to gain insights. Here, we will separately generate insights for the two
    correctly identified positive sentiment and two correctly identified negative
    sentiment examples as it somehow tends to provide more reasonable results compared
    to just plugging in all four examples. We used the following prompt:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的教程中，我们尝试解释了四个不同样本的解释，这些样本包括两个正确预测的正面情感示例和两个正确预测的负面情感示例。现在，让我们使用LLM模型来获得一些洞见。这里，我们将分别为两个正确识别的正面情感和两个正确识别的负面情感示例生成洞见，因为与直接插入所有四个示例相比，这种方法往往能提供更合理的结果。我们使用了以下提示：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the list of word tokens and attribution tuples are the contents of
    the `word_attributions` variable in the previous tutorial. For this prompt, the
    ChatGPT model returned the following response without modification:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，单词令牌和归属元组的列表是前面教程中`word_attributions`变量的内容。对于这个提示，ChatGPT模型返回了以下未修改的响应：
- en: '**Based on the given examples, it seems that the model is performing well in
    predicting positive sentiment, as the predicted label is correct in both cases.
    However, it is important to note that the token attributions should be interpreted
    with caution, as they are not always a reliable indicator of the importance of
    each word in** **the sentence.**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**根据给定的示例，模型在预测正面情感方面表现良好，因为在两个案例中预测的标签都是正确的。然而，值得注意的是，词汇归属应谨慎解读，因为它们并不总是可靠的指标，无法准确反映句子中每个单词的重要性。**'
- en: '**Looking at the token attributions for the first example, we can see that
    the highest attributed word is “bored” with a score of 0.494\. This might seem
    odd for a positive sentiment sentence, but it is possible that the model is attributing
    high importance to this word because it is negating a negative sentiment that
    was initially present. The model might have also learned that overcoming boredom
    is something that people consider to be positive. The second highest attributed
    words are “I” and “was”, both with scores above 0.3, which is reasonable since
    they indicate the writer’s personal experience and opinion. The other highly attributed
    words such as “to”, “from”, and “new” also seem to make sense in the context of**
    **the sentence.**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**观察第一个示例的词汇归属，我们可以看到最高归属的词是“bored”，得分为0.494。对于正面情感的句子来说，这可能看起来很奇怪，但模型可能高估了这个词的重要性，因为它是在否定一个最初存在的负面情感。模型也许已经学会了克服无聊是人们认为积极的事情。第二高归属的词是“I”和“was”，它们的得分都在0.3以上，这很合理，因为它们表明了写作者的个人经验和观点。其他高度归属的词，如“to”、“from”和“new”，也似乎在句子的语境中是合适的。**'
- en: '**Looking at the token attributions for the second example, we can see that
    the highest attributed word is “bored” again, with a score of 0.709\. This could
    be because the model is placing a lot of emphasis on the fact that the job keeps
    the writer from getting bored, which is seen as a positive thing. The second highest
    attributed word is “from” with a score of 0.415, which makes sense in the context
    of the sentence since it is indicating what the job is keeping the writer from
    doing. Other highly attributed words such as “to” and “that” also seem to make
    sense in the context of** **the sentence.**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**查看第二个例子的标记归属，我们可以看到归属分数最高的词是“bored”，得分为0.709。这可能是因为模型对“这份工作让作者不至于感到无聊”这一事实给予了很高的权重，这被视为一件积极的事情。第二高的归属词是“from”，得分为0.415，在句子的上下文中也是合理的，因为它表示这份工作让作者避免了做某事。其他高度归属的词语，如“to”和“that”，也似乎在**
    **句子的上下文中有意义。**'
- en: '**Overall, while the model is performing well in predicting positive sentiment,
    it is important to note that token attributions can be noisy and should be interpreted
    with caution. The highly attributed words seem to make sense in the context of
    the sentences, but it is important to look at the entire sentence and not just
    individual words to fully understand** **the sentiment.**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**总体来说，虽然模型在预测积极情绪方面表现良好，但需要注意的是，标记归属可能是嘈杂的，应该谨慎解读。高度归属的词语在句子的上下文中似乎是有道理的，但重要的是要看整个句子，而不仅仅是单个词语，以便全面理解**
    **情绪。**'
- en: 'This response provides a different take on our original insight and mentions
    the possibility that nothing is wrong with some skepticism. Overall, this is a
    nice response that gives us a different perspective on what could happen and signals
    that an LLM can be used to explain the results of prediction explanation as an
    assistance tool. Now, let’s replace the attribution results with the two examples
    that are correctly identified as negative sentiments with the following list:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个回应提供了对我们原始见解的不同看法，并提到一些怀疑的可能性并没有错。总体来说，这是一个不错的回应，它给我们提供了关于可能发生情况的不同视角，并表明大语言模型（LLM）可以用来解释预测结果的解释，作为一种辅助工具。现在，让我们用以下列表中的两个例子来替换归属结果，这些例子正确地识别为负面情绪：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results of the ChatGPT model provided the following response:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT模型提供了以下回应：
- en: '**From the provided examples, it seems like the model is correctly predicting
    the negative sentiment in both cases. However, there are a few things that could
    be analyzed to determine whether the model is performing well** **or not.**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**从提供的例子来看，模型似乎在这两种情况下都正确地预测了负面情绪。然而，有几个方面可以分析，以确定模型是否表现良好** **或不良。**'
- en: '**Firstly, the token attributions provided by the integrated gradients method
    indicate the contribution of each token towards the final prediction of the model.
    In both examples, the tokens with the highest positive attribution scores are
    “day” and “has gone wrong”, which seems to make sense as they are indicative of**
    **negative sentiment.**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**首先，集成梯度方法提供的标记归属指示了每个标记对模型最终预测的贡献。在这两个例子中，归属分数最高的标记是“day”和“has gone wrong”，这似乎是有道理的，因为它们表明了**
    **负面情绪。**'
- en: '**However, one issue with the provided token attributions is that some of the
    tokens have negative attribution scores, which means that they are contributing
    negatively towards the final prediction. For example, in the first example, the
    token “feels” has a negative attribution score, but it could be argued that it
    is actually indicative of negative sentiment. Similarly, in the second example,
    the token “’m” has a negative attribution score, but it is a contraction of “I
    am”, which is indicative of** **negative sentiment.**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**然而，提供的标记归属存在一个问题，即某些标记的归属分数为负，这意味着它们对最终预测产生了负面影响。例如，在第一个例子中，标记“feels”具有负归属分数，但可以认为它实际上表明了负面情绪。类似地，在第二个例子中，标记“''m”具有负归属分数，但它是“I
    am”的缩写，表示** **负面情绪。**'
- en: '**Another thing to consider is the relative score of the highest attributed
    tokens. In the first example, the token “day” has a significantly higher attribution
    score than any other token, which indicates that it is the most important feature
    for predicting negative sentiment in this case. However, in the second example,
    there are multiple tokens with similarly high attribution scores, such as “has
    gone wrong” and “frustrated”, which suggests that they are both equally important
    features for predicting** **negative sentiment.**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一个需要考虑的因素是最高归因令牌的相对得分。在第一个例子中，令牌“day”的归因得分显著高于其他任何令牌，这表明它在此案例中是预测负面情绪的最重要特征。然而，在第二个例子中，存在多个归因得分相似较高的令牌，例如“has
    gone wrong”和“frustrated”，这表明它们在预测负面情绪时是同样重要的特征。**'
- en: '**Overall, based on the provided examples and token attributions, it seems
    like the model is performing reasonably well in predicting negative sentiment.
    However, the analysis of the token attributions suggests that there may be some
    issues with the way the integrated gradients method is attributing scores to certain
    tokens. It is also important to consider the relative score and interpretability
    of the highest attributed tokens to determine whether they make sense in the context
    of the** **predicted label.**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**总体而言，基于提供的例子和令牌归因，模型在预测负面情绪方面表现得相当不错。然而，令牌归因的分析表明，集成梯度方法可能在将得分归因于某些令牌时存在一些问题。还需要考虑最高归因令牌的相对得分和可解释性，以确定它们在预测标签的上下文中是否合理。**'
- en: 'Both of ChatGPT’s responses seem to signal that it could be a valid behavior.
    Since most neural networks are configured to be non-linear models with many non-linear
    activations, remember that the model is capable of learning complex interactions
    between tokens. This means that tokens are mostly perceived by the model in a
    complex way that might make them depend on one another. A token that has a high
    attribution in the original context when applied to another context might not
    produce the same attribution as multiple tokens can depend on each other as a
    prerequisite to produce a strong attribution. Additionally, tokens are not restricted
    to interacting with tokens that are directly beside them and can be in any position
    of a text row. One way to verify whether the token is overfitted is by predicting
    on text data that only has the `day` token to see whether it will predict it as
    a negative sentiment and the `bored` token to see whether it will predict it as
    a positive sentiment:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT的两个回答似乎都表示这种行为是有效的。由于大多数神经网络被配置为非线性模型，并具有许多非线性激活，记住该模型能够学习令牌之间的复杂交互。这意味着令牌通常是通过一种复杂的方式被模型感知的，这可能使它们彼此依赖。一个在原始上下文中具有高归因的令牌，在应用到另一个上下文时，可能不会产生相同的归因结果，因为多个令牌可能会相互依赖，作为产生强归因的前提。此外，令牌并不局限于与直接相邻的令牌进行交互，它们可以出现在文本行的任何位置。验证令牌是否过拟合的一种方法是通过预测仅包含`day`令牌的文本数据，看看它是否会预测为负面情绪，并通过预测包含`bored`令牌的文本数据，看看它是否会预测为正面情绪：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will return the following response:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下响应：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is interesting to see that a single token for both `day` and `bored` will
    result in the prediction being a neutral sentiment. The more repetition tokens
    you add, the more the prediction will skew toward negative and positive sentiment,
    respectively. This proves that the model is indeed biased toward these specific
    words and is not using the words in the right way, as ChatGPT said. Note that
    it could very much be what ChatGPT predicted for your case.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，单独使用`day`和`bored`令牌时，预测结果是中性情绪。你添加更多重复令牌时，预测结果将分别偏向负面和正面情绪。这证明模型确实对这些特定的词存在偏见，并且没有以正确的方式使用这些词，正如ChatGPT所说。请注意，这很可能是ChatGPT对你的案例做出的预测。
- en: This tutorial showcases the explanations of text-based neural networks. However,
    it can be applied similarly to other data types, such as numerical data, categorical
    data, or any data that can be reliably represented in text format. Specifically
    for image-based prediction explanations, you can use models such as GPT-4 from
    OpenAI, which accepts image data along with text data. You can also consider using
    the visualization from the `transformers_interpret` library as an image and feed
    it into GPT-4!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程展示了基于文本的神经网络的解释。然而，它也可以类似地应用于其他数据类型，如数值数据、分类数据或任何可以可靠地表示为文本格式的数据。特别是对于基于图像的预测解释，您可以使用像OpenAI的GPT-4这样的模型，它同时接受图像数据和文本数据。您还可以考虑使用`transformers_interpret`库中的可视化图像，并将其输入到GPT-4中！
- en: Next, we will dive into common pitfalls when trying to explain your predictions
    and recommendations to avoid these pitfalls.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨在尝试解释您的预测和建议时常见的陷阱，并提供避免这些陷阱的建议。
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索预测解释中的常见陷阱以及如何避免它们
- en: 'Although prediction explanations have proven to be valuable tools in understanding
    AI models, several common pitfalls can hinder their effectiveness. In this section,
    we will discuss these pitfalls and provide strategies to avoid them, ensuring
    that prediction explanations remain a valuable resource for understanding and
    improving AI models. Some of the common pitfalls, along with their solutions,
    are as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预测解释已被证明是理解AI模型的宝贵工具，但有几个常见的陷阱可能会妨碍其效果。在本节中，我们将讨论这些陷阱，并提供避免它们的策略，确保预测解释继续成为理解和改进AI模型的有价值资源。以下是一些常见的陷阱及其解决方案：
- en: '**Over-reliance on explanations**: While prediction explanations can provide
    valuable insights into a model’s decision-making process, over-relying on these
    explanations can lead to incorrect conclusions. It’s important to remember that
    prediction explanations are just one piece of the puzzle and should be used in
    conjunction with other evaluation methods to gain a comprehensive understanding
    of a model’s performance. The solution here is to use a combination of evaluation
    methods, including performance metrics, cross-validation, and expert domain knowledge,
    to analyze and validate a model’s performance.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度依赖解释**：尽管预测解释可以为模型的决策过程提供宝贵的见解，但过度依赖这些解释可能导致错误的结论。重要的是要记住，预测解释只是拼图中的一块，应该与其他评估方法结合使用，以全面了解模型的表现。这里的解决方案是使用多种评估方法的组合，包括性能指标、交叉验证和专家领域知识，来分析和验证模型的表现。'
- en: '**Misinterpreting explanation results**: Interpreting prediction explanation
    results can be challenging, particularly when dealing with complex models and
    large datasets. Misinterpretation of these results can lead to incorrect conclusions
    about a model’s behavior and performance. The solution here is to collaborate
    with domain experts to help interpret explanation results and ensure that conclusions
    drawn from these explanations align with real-world knowledge and expectations.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**误解解释结果**：解释预测结果可能是具有挑战性的，尤其是当处理复杂模型和大型数据集时。误解这些结果可能会导致关于模型行为和性能的错误结论。这里的解决方案是与领域专家合作，帮助解释结果，并确保从这些解释中得出的结论与现实世界的知识和预期一致。'
- en: '**Ignoring model limitations**: Prediction explanations can provide valuable
    insights into a model’s decision-making process, but they cannot address the inherent
    limitations of the model itself. It’s essential to acknowledge and address these
    limitations to ensure that the model performs optimally. The solution here is
    to conduct thorough model evaluations to identify and address any limitations,
    such as overfitting, underfitting, or biased training data. Continuously reevaluate
    and update the model as needed to maintain optimal performance.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**忽视模型的局限性**：预测解释能够提供关于模型决策过程的宝贵见解，但它们无法解决模型本身的固有局限性。承认并解决这些局限性是至关重要的，以确保模型的最佳性能。这里的解决方案是进行彻底的模型评估，识别并解决任何局限性，如过拟合、欠拟合或偏倚的训练数据。不断地重新评估并根据需要更新模型，以维持其最佳性能。'
- en: '**Explanations for the wrong audience**: Different stakeholders may require
    different types of explanations based on their expertise and needs. Providing
    explanations that are too technical or too simplistic for the intended audience
    can hinder their understanding and use of the predictions. The solution is to
    tailor explanations to the needs and expertise of the target audience. Sometimes,
    a more global explanation of the model is needed instead of the per-prediction
    explanation, in which case you can consider using neural network interpretation
    techniques and aggregated evaluation metrics. For non-technical users, explaining
    predictions by providing raw feature importance isn’t enough and requires a natural
    language explanation of the insights obtained through feature importance, as introduced
    in the *Explaining prediction explanations automatically* section. For a technical-aware
    user, prediction explanations can be suitable.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**针对错误受众的解释**：不同的利益相关者可能需要根据其专业知识和需求提供不同类型的解释。为目标受众提供过于技术化或过于简化的解释，会妨碍他们对预测的理解和使用。解决方案是根据目标受众的需求和专业知识量身定制解释。有时，需要对模型进行更为整体的解释，而不是逐个预测的解释，这种情况下可以考虑使用神经网络解释技术和聚合评估指标。对于非技术用户，仅通过提供原始特征重要性来解释预测是不够的，还需要通过自然语言解释从特征重要性中获得的见解，如*自动解释预测解释*部分所介绍的那样。对于有技术意识的用户，预测解释是合适的。'
- en: By being aware of these common pitfalls and implementing strategies to avoid
    them, practitioners can ensure that prediction explanations remain a valuable
    tool for understanding and improving AI models. By combining prediction explanations
    with other evaluation methods and collaborating with domain experts, it’s possible
    to gain a comprehensive understanding of a model’s performance, behavior, and
    limitations, ultimately leading to more accurate and reliable AI systems that
    better serve their intended purpose.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 通过意识到这些常见的陷阱并实施避免策略，从业人员可以确保预测解释仍然是理解和改进人工智能模型的重要工具。通过将预测解释与其他评估方法结合，并与领域专家合作，可以全面了解模型的性能、行为和局限性，从而最终获得更准确、可靠的人工智能系统，更好地服务于其预期目标。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we gained a broad view of the prediction explanations landscape
    and dived into the integrated gradients technique, applied it practically to a
    use case, and even attempted to explain the integrated gradients results manually
    and automatically through LLMs. We also discussed common pitfalls in prediction
    explanations and provided strategies to avoid them, ensuring the effectiveness
    of these explanations in understanding and improving AI models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对预测解释的全貌进行了广泛的了解，深入探讨了集成梯度技术，实际应用于一个案例，并尝试通过LLMs手动和自动地解释集成梯度结果。我们还讨论了预测解释中的常见陷阱，并提供了避免这些陷阱的策略，确保这些解释在理解和改进人工智能模型中的有效性。
- en: Integrated gradients is a useful technique and tool to provide a form of saliency-based
    explanation of the predictions that your neural network makes. The process of
    understanding a model through prediction explanations provides many benefits that
    can help fulfill the criteria required to have a successful machine learning project
    and initiative. Even when everything is going well and the machine learning use
    case is not critical, uncovering the model’s behavior that you will potentially
    deploy through any prediction explanations technique can help you improve how
    your model is utilized.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 集成梯度是一种有用的技术和工具，可以为你的神经网络做出预测提供一种基于显著性的解释。通过预测解释来理解模型的过程提供了许多好处，这些好处可以帮助实现成功的机器学习项目和计划所需的标准。即使一切顺利，且机器学习应用场景并不关键，通过任何预测解释技术揭示你可能部署的模型行为，也可以帮助你改进模型的使用方式。
- en: However, saliency-based explanations only allow you to understand which input
    data or input neurons are important. But what patterns a neuron is capable of
    detecting from the input data remains unknown. In the next chapter, we will expand
    on this direction and uncover techniques to understand exactly what a neuron detects.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于显著性的解释只能帮助你了解哪些输入数据或输入神经元是重要的。但是，一个神经元从输入数据中能够检测到什么模式仍然是未知的。在下一章，我们将扩展这一方向，揭示理解神经元检测到的具体内容的技术。
- en: By being aware of common pitfalls and implementing strategies to avoid them,
    practitioners can ensure that prediction explanations remain a valuable tool for
    understanding and improving AI models. Combining prediction explanations with
    other evaluation methods and collaborating with domain experts can lead to more
    accurate and reliable AI systems that better serve their intended purpose.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过意识到常见的陷阱并实施避免这些陷阱的策略，从业人员可以确保预测解释继续作为理解和改进 AI 模型的有价值工具。将预测解释与其他评估方法结合，并与领域专家合作，可以创造出更加准确可靠的
    AI 系统，更好地服务于其预定目的。
- en: Before we wrap up, as a final word, be sure to allocate some time to understand
    your neural network model through prediction explanation techniques such as integrated
    gradients and consider the potential pitfalls to maximize their effectiveness!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，最后要说的是，确保抽出时间通过预测解释技术（如积分梯度）来理解你的神经网络模型，并考虑潜在的陷阱，以最大化其效果！
- en: Further reading
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Mukund Sundararajan, Ankur Taly, Qiqi Yan. *Axiomatic Attribution for Deep
    Networks*, In: International Conference on Machine Learning. 2017\. URL: [https://arxiv.org/abs/1703.01365v2](https://arxiv.org/abs/1703.01365v2).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukund Sundararajan, Ankur Taly, Qiqi Yan. *深度网络的公理化归因*，载于：国际机器学习会议，2017\. 网址：[https://arxiv.org/abs/1703.01365v2](https://arxiv.org/abs/1703.01365v2)。
