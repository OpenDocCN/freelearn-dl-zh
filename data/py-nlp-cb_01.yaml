- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Learning NLP Basics
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习NLP基础知识
- en: While working on this book, we were focusing on including recipes that should
    be useful for a wide variety of NLP projects. They range from simple to advanced,
    from dealing with grammar to dealing with visualizations, and in many of them,
    options for languages other than English are included. In this new edition, we
    have included new topics that cover using GPT and other large language models,
    explainable AI, a new chapter on transformers, and natural language understanding.
    We hope you find the book useful.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写这本书的过程中，我们专注于包括对各种NLP项目有用的食谱。它们从简单到复杂，从处理语法到处理可视化，在许多食谱中还包括了除英语之外的语言选项。在新版中，我们包括了使用GPT和其他大型语言模型、可解释人工智能、关于转换器的新章节以及自然语言理解的新主题。我们希望这本书对你有所帮助。
- en: The format of the book is that of a *programming cookbook*, where each recipe
    is a short mini-project with a concrete goal and a sequence of steps that need
    to be performed. There are few theoretical explanations and a focus on the practical
    goals and what needs to be done to achieve them.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的格式类似于**编程食谱**，其中每个食谱都是一个具有具体目标和需要执行的一系列步骤的短期迷你项目。理论解释很少，重点是实际目标和实现它们所需的工作。
- en: Before we can get on with the real work of NLP, we need to prepare our text
    for processing. This chapter will show you how to do it. By the end of the chapter,
    you will be able to have a list of words in a text with their parts of speech
    and lemmas or stems, and with very frequent words removed.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始真正的NLP工作之前，我们需要为文本处理做好准备。本章将向你展示如何进行。到本章结束时，你将能够得到一个包含文本中单词及其词性、词干或词根的列表，并且移除了非常频繁的单词。
- en: '**Natural Language Toolkit** (**NLTK**) and **spaCy** are two important packages
    that we will be working with in this chapter and throughout the book. Some other
    packages we will be using in the book are PyTorch and Hugging Face Transformers.
    We will also utilize the OpenAI API with the GPT models.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）和**spaCy**是我们在本章以及整本书中将要使用的重要库。书中还会使用到其他一些库，例如PyTorch和Hugging
    Face Transformers。我们还将利用OpenAI API和GPT模型。'
- en: 'The recipes included in this chapter are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含的食谱如下：
- en: Dividing text into sentences
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本划分为句子
- en: Dividing sentences into words – tokenization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子划分为单词——分词
- en: Part of speech tagging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Combining similar words – lemmatization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合相似词语——词形还原
- en: Removing stopwords
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Throughout this book, we will use **Poetry** to manage the Python package installations.
    You can use the latest version of Poetry since it conserves the previous versions’
    functionality. Once you install Poetry, managing which packages to install will
    be very easy. We will be using **Python 3.9** throughout the book. You will also
    need to have **Jupyter** installed in order to run the notebooks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将使用**Poetry**来管理Python包的安装。你可以使用最新版本的Poetry，因为它保留了之前版本的功能。一旦安装了Poetry，管理要安装的包将会非常容易。整本书我们将使用**Python
    3.9**。你还需要安装**Jupyter**以便运行笔记本。
- en: Note
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may try to use Google Colab in order to run the notebooks but you will need
    to tweak the code to make it work with Colab.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试使用Google Colab来运行笔记本，但你需要调整代码以便使其在Colab上工作。
- en: 'Follow these installation steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下安装步骤进行：
- en: 'Install **Git**: [https://github.com/git-guides/install-git](https://github.com/git-guides/install-git).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装**Git**：[https://github.com/git-guides/install-git](https://github.com/git-guides/install-git)。
- en: 'Install **Poetry**: [https://python-poetry.org/docs/#installation](https://python-poetry.org/docs/#installation).'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装**Poetry**：[https://python-poetry.org/docs/#installation](https://python-poetry.org/docs/#installation)。
- en: 'Install **Jupyter**: [https://jupyter.org/install](https://jupyter.org/install).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装**Jupyter**：[https://jupyter.org/install](https://jupyter.org/install)。
- en: 'Clone the GitHub repository that contains all the code from this book ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition))
    by issuing the following command in the terminal:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在终端中输入以下命令以克隆包含本书所有代码的GitHub仓库（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition)）：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the directory that contains the **pyproject.toml** file, run the commands
    using the terminal:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在包含**pyproject.toml**文件的目录中，使用终端运行以下命令：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Start the notebook engine:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动笔记本引擎：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, you should be able to run all the notebooks in your cloned repository.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该能够运行你克隆的仓库中的所有笔记本。
- en: 'If you prefer not to use Poetry, you can set up a virtual environment using
    the `requirements.txt` file provided in the book repository. You can do this in
    one of two ways. You can use `pip`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用Poetry，你可以使用书中提供的`requirements.txt`文件设置虚拟环境。你可以有两种方法来做这件事。你可以使用`pip`：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also use `conda`:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用`conda`：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Dividing text into sentences
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将文本分割成句子
- en: 'When we work with text, we can work with text units on different scales: the
    document itself, such as a newspaper article, the paragraph, the sentence, or
    the word. Sentences are the main unit of processing in many NLP tasks. For example,
    when we send data over to **Large Language Models** (**LLMs**), we frequently
    want to add some context to the prompt. In some cases, we would like that context
    to include sentences from a text so that the model can extract some important
    information from that text. In this section, we will show you how to divide a
    text into sentences.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本时，我们可以处理不同尺度的文本单元：文档本身，例如一篇报纸文章，段落，句子或单词。句子是许多NLP任务中的主要处理单元。例如，当我们将数据发送到**大型语言模型**（**LLMs**）时，我们经常想在提示中添加一些上下文。在某些情况下，我们希望这个上下文中包含文本中的句子，以便模型可以从该文本中提取一些重要信息。在本节中，我们将向您展示如何将文本分割成句子。
- en: Getting ready
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this part, we will be using the text of the book *The Adventures of Sherlock
    Holmes*. You can find the whole text in the book’s GitHub file ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt)).
    For this recipe we will need just the beginning of the book, which can be found
    in the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这部分，我们将使用《福尔摩斯探案集》的文本。你可以在这本书的GitHub文件中找到整个文本（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt)）。对于这个食谱，我们只需要书的开始部分，可以在文件[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt)中找到。
- en: In order to do this task, you will need the NLTK package and its sentence tokenizers,
    which are part of the Poetry file. Directions to install Poetry are described
    in the *Technical* *requirements* section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，你需要NLTK包及其句子分词器，它们是Poetry文件的一部分。安装Poetry的说明在*技术要求*部分中描述。
- en: How to do it…
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will now divide the text of a small piece of *The Adventures of Sherlock
    Holmes*, outputting a list of sentences. (Reference notebook: [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb).)
    Here, we assume that you are running the notebook, so the paths are all relative
    to the notebook location:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将分割《福尔摩斯探案集》一小部分的文本，输出句子列表。（参考笔记本：[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb)）。在这里，我们假设你正在运行笔记本，所以路径都是相对于笔记本位置的：
- en: 'Import the file utility functions from the **util** folder ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)):'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**util**文件夹中导入文件实用函数（[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb)）：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Read in the book part text:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取书籍的部分文本：
- en: '[PRE6]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `read_text_file` function is located in the `util` notebook we imported
    previously. Here is its source code:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`read_text_file`函数位于我们之前导入的`util`笔记本中。以下是它的源代码：'
- en: '[PRE7]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Print out the resulting text to make sure everything worked correctly and the
    file loaded:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出结果以确保一切正常并且文件已加载：
- en: '[PRE8]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The beginning of the printout will look like this:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 打印输出的开始部分将看起来像这样：
- en: '[PRE9]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Import the **nltk** package:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**nltk**包：
- en: '[PRE10]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If this is the first time you are running the code, you will need to download
    tokenizer data. You will not need to run this command after that:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你第一次运行代码，你需要下载分词器数据。之后你不需要再运行此命令：
- en: '[PRE11]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize the tokenizer:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化分词器：
- en: '[PRE12]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Divide the text into sentences using the tokenizer. The result will be a list
    of sentences:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词器将文本分割成句子。结果将是一个句子列表：
- en: '[PRE13]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Print the result:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果：
- en: '[PRE14]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It should look like this. There are newlines inside the sentences that come
    from the book formatting. They are not necessarily sentence endings:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它应该看起来像这样。句子中包含来自书籍格式的换行符，它们不一定是句子的结尾：
- en: '[PRE15]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Print the number of sentences in the result; there should be 11 sentences in
    total:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果中的句子数量；总共有11个句子：
- en: '[PRE16]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives the result:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给出以下结果：
- en: '[PRE17]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Although it might seem straightforward to divide a text into sentences by just
    using a regular expression to split it at the periods, in reality, it is more
    complicated. We use periods in places other than ends of sentences; for example,
    after abbreviations – for example, “Dr. Smith will see you now.” Similarly, while
    all sentences in English start with a capital letter, we also use capital letters
    for proper names. The approach used in `nltk` takes all these points into consideration;
    it is an implementation of an unsupervised algorithm presented in [https://aclanthology.org/J06-4003.pdf](https://aclanthology.org/J06-4003.pdf).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用正则表达式在句号处分割文本以将其分为句子可能看起来很简单，但实际上要复杂得多。我们在句子的其他地方也使用句号；例如，在缩写词之后——例如，“Dr.
    Smith will see you now.” 类似地，虽然英语中的所有句子都以大写字母开头，但我们也会使用大写字母来表示专有名词。`nltk`中使用的这种方法考虑了所有这些因素；它是一个无监督算法的实现，该算法在[https://aclanthology.org/J06-4003.pdf](https://aclanthology.org/J06-4003.pdf)中提出。
- en: There’s more…
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can also use a different strategy to parse the text into sentences, employing
    the other very popular NLP package, **spaCy**. Here is how it works:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用不同的策略来将文本解析为句子，采用另一个非常流行的NLP包，**spaCy**。以下是它是如何工作的：
- en: 'Import the spaCy package:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入spaCy包：
- en: '[PRE18]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first time you run the notebook, you will need to download a spaCy model.
    The model is trained on a large amount of English text and there are several tools
    that can be used with it, including the sentence tokenizer. Here, I’m downloading
    the smallest model, but you might try other ones (see [https://spacy.io/usage/models/](https://spacy.io/usage/models/)):'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次运行笔记本时，你需要下载spaCy模型。该模型是在大量英文文本上训练的，并且可以使用包括句子分词器在内的几个工具。在这里，我正在下载最小的模型，但你也可以尝试其他模型（见[https://spacy.io/usage/models/](https://spacy.io/usage/models/))。
- en: '[PRE19]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize the spaCy engine:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化spaCy引擎：
- en: '[PRE20]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Process the text using the spaCy engine. This line assumes that you have the
    **sherlock_holmes_part_of_text** variable initialized. If not, you need to run
    one of the earlier cells where the text is read into this variable:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用spaCy引擎处理文本。这一行假设你已经初始化了**sherlock_holmes_part_of_text**变量。如果没有，你需要运行之前的一个单元格，其中文本被读入这个变量：
- en: '[PRE21]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Get the sentences from the processed **doc** object, and print the resulting
    array and its length:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从处理后的**doc**对象中获取句子，并打印出结果数组和它的长度：
- en: '[PRE22]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The result will look like this:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将看起来像这样：
- en: '[PRE23]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'An important difference between spaCy and NLTK is the time it takes to complete
    the sentence-splitting process. The reason for this is that spaCy loads a language
    model and uses several tools in addition to the tokenizer, while the NLTK tokenizer
    has only one function: to separate the text into sentences. We can time the execution
    by using the `time` package and putting the code to split the sentences into the
    `main` function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy与NLTK之间的重要区别在于完成句子分割过程所需的时间。原因在于spaCy加载了一个语言模型，并使用除了分词器之外的其他工具，而NLTK的分词器只有一个功能：将文本分割成句子。我们可以通过使用`time`包并将分割句子的代码放入`main`函数中来计时：
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The spaCy algorithm takes 0.019 seconds, while the NLTK algorithm takes 0.0002\.
    The time is calculated by subtracting the current time (`time.time()`) from the
    start time that is set at the beginning of the code block. It is possible that
    you will get slightly different values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy算法耗时0.019秒，而NLTK算法耗时0.0002秒。时间是通过从代码块开始设置的时间减去当前时间(`time.time()`)来计算的。你可能会得到略微不同的值。
- en: The reason why you might use spaCy is if you are doing other processing with
    the package along with splitting it into sentences. The spaCy processor does many
    other things, and that is why it takes longer. If you are using other features
    of spaCy, there is no reason to use NLTK just for sentence splitting, and it’s
    better to employ spaCy for the whole pipeline.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会使用spaCy的原因是如果您在使用该包进行其他处理的同时，还需要将其分割成句子。spaCy处理器执行许多其他任务，这就是为什么它需要更长的时间。如果您正在使用spaCy的其他功能，就没有必要仅为了句子分割而使用NLTK，最好在整个流程中使用spaCy。
- en: 'It is also possible to use only the tokenizer without other tools from spaCy.
    Please see their documentation for more information: [https://spacy.io/usage/processing-pipelines](https://spacy.io/usage/processing-pipelines).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用spaCy的tokenizer而不使用其他工具。请参阅他们的文档以获取更多信息：[https://spacy.io/usage/processing-pipelines](https://spacy.io/usage/processing-pipelines)。
- en: Important note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: spaCy might be slower, but it is doing many more things in the background, and
    if you are using its other features, use it for sentence splitting as well.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy可能较慢，但它后台执行了许多更多的事情，如果您正在使用它的其他功能，那么在句子分割时也使用spaCy。
- en: See also
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can use NLTK and spaCy to divide texts in languages other than English.
    NLTK includes tokenizer models for Czech, Danish, Dutch, Estonian, Finnish, French,
    German, Greek, Italian, Norwegian, Polish, Portuguese, Slovene, Spanish, Swedish,
    and Turkish. In order to load those models, use the name of the language followed
    by the `.``pickle` extension:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用NLTK和spaCy来分割非英语语言的文本。NLTK包括捷克语、丹麦语、荷兰语、爱沙尼亚语、芬兰语、法语、德语、希腊语、意大利语、挪威语、波兰语、葡萄牙语、斯洛文尼亚语、西班牙语、瑞典语和土耳其语的tokenizer模型。为了加载这些模型，请使用语言名称后跟`.pickle`扩展名：
- en: '[PRE25]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'See the NLTK documentation to find out more: [https://www.nltk.org/index.html](https://www.nltk.org/index.html).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 查看NLTK文档以获取更多信息：[https://www.nltk.org/index.html](https://www.nltk.org/index.html)。
- en: 'Likewise, spaCy has models for other languages: Chinese, Dutch, English, French,
    German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. These
    models are trained on text in those languages. In order to use those models, you
    would have to download them separately. For example, for Spanish, use this command
    to download the model:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，spaCy也提供了其他语言的模型：中文、荷兰语、英语、法语、德语、希腊语、意大利语、日语、葡萄牙语、罗马尼亚语、西班牙语以及其他语言。这些模型都是在这些语言的文本上训练的。为了使用这些模型，您需要分别下载它们。例如，对于西班牙语，可以使用以下命令下载模型：
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, put this line in the code to use it:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将此行代码放入以使用它：
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'See the spaCy documentation to find out more: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查看spaCy文档以获取更多信息：[https://spacy.io/usage/models](https://spacy.io/usage/models)。
- en: Dividing sentences into words – tokenization
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将句子分割成单词 – 分词
- en: In many instances, we rely on individual words when we do NLP tasks. This happens,
    for example, when we build semantic models of texts by relying on the semantics
    – of individual words, or when we are looking for words with a specific part of
    speech. To divide text into words, we can use NLTK and spaCy to do this task for
    us.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们在进行NLP任务时依赖于单个单词。例如，当我们通过依赖单个单词的语义来构建文本的语义模型时，或者当我们寻找具有特定词性的单词时，这种情况就会发生。为了将文本分割成单词，我们可以使用NLTK和spaCy来为我们完成这个任务。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this part, we will be using the same text of the book *The Adventures of
    Sherlock Holmes*. You can find the whole text in the book’s GitHub repository.
    For this recipe, we will need just the beginning of the book, which can be found
    in the `sherlock_holmes_1.txt` file.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这部分，我们将使用书籍《福尔摩斯探案集》的相同文本。您可以在书籍的GitHub仓库中找到整个文本。对于这个食谱，我们只需要书的开始部分，这部分可以在`sherlock_holmes_1.txt`文件中找到。
- en: In order to do this task, you will need the NLTK and spaCy packages, which are
    part of the Poetry file. Directions to install Poetry are described in the *Technical*
    *requirements* section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，您将需要NLTK和spaCy包，它们是Poetry文件的一部分。在*技术* *要求*部分描述了安装Poetry的说明。
- en: '(Notebook reference: [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb).)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （笔记本参考：[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb)）。
- en: How to do it
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现
- en: 'The process is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: 'Import the **file_utils** notebook. Effectively, we run the **file_utils**
    notebook inside this one so we have access to its defined functions and variables:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**file_utils**笔记本。实际上，我们在一个笔记本中运行**file_utils**笔记本，这样我们就可以访问其定义的函数和变量：
- en: '[PRE28]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Read in the book snippet text:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取书籍片段文本：
- en: '[PRE29]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result should look like this:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应该看起来像这样：
- en: '[PRE30]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Import the **nltk** package:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**nltk**包：
- en: '[PRE31]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Divide the input into words. Here, we use the NLTK word tokenizer to split
    the text into individual words. The output of the function is a Python list of
    the words:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入分成单词。在这里，我们使用NLTK单词分词器将文本分割成单个单词。该函数的输出是一个包含单词的Python列表：
- en: '[PRE32]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output will be the list of words in the text and the length of the **words**
    list:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将是文本中的单词列表和**words**列表的长度：
- en: '[PRE33]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The output is a list, where each token is either a word or a punctuation mark.
    The NLTK tokenizer uses a set of rules to split the text into words. It splits
    but does not expand contractions, such as *don’t → do n’t* and *men’s → men ’s*,
    as in the preceding example. It treats punctuation and quotes as separate tokens,
    so the result includes words with no other marks.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个列表，其中每个标记要么是单词，要么是标点符号。NLTK分词器使用一组规则将文本分割成单词。它分割但不扩展缩写，如*don’t → do n’t*和*men’s
    → men ’s*，正如前面的例子所示。它将标点和引号视为单独的标记，因此结果包括没有其他标记的单词。
- en: There’s more…
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'Sometimes, it is useful not to split some words and use them as one unit. One
    example of this is in [*Chapter 3*](B18411_03.xhtml#_idTextAnchor067), in the
    *Representing phrases – phrase2vec* recipe, where we store phrases and not just
    individual words. The NLTK package allows us to do that using its custom tokenizer,
    `MWETokenizer`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，不将某些单词分开，而将它们作为一个整体使用是有用的。这种做法的一个例子可以在[*第3章*](B18411_03.xhtml#_idTextAnchor067)中找到，在*表示短语
    – phrase2vec*配方中，我们存储的是短语而不是单个单词。NLTK包允许我们使用其自定义分词器`MWETokenizer`来实现这一点：
- en: 'Import the **MWETokenizer** class:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**MWETokenizer**类：
- en: '[PRE34]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Initialize the tokenizer and indicate that the words **dim sum dinner** should
    not be split:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化分词器并指示单词**dim sum dinner**不应被分割：
- en: '[PRE35]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Add more words that should be kept together:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加更多应该保留在一起的单词：
- en: '[PRE36]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Use the tokenizer to split a sentence:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分词器分割一个句子：
- en: '[PRE37]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The result will contain the tokens split the same way as previously:'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将包含与之前相同方式的分割标记：
- en: '[PRE38]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Split a different sentence:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割不同的句子：
- en: '[PRE39]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this case, the tokenizer will put the phrases together into one unit and
    insert underscores instead of spaces:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，分词器会将短语组合成一个单元，并用下划线代替空格：
- en: '[PRE40]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can also use spaCy to do the tokenization. Word tokenization is one task
    in a larger array of tasks that spaCy accomplishes while processing text.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用spaCy进行分词。单词分词是spaCy在处理文本时完成的一系列任务中的一个任务。
- en: There's still more
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多
- en: 'If you are doing further processing on the text, it makes sense to use spaCy.
    Here is how it works:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在对文本进行进一步处理，使用spaCy是有意义的。以下是它是如何工作的：
- en: 'Import the **spacy** package:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**spacy**包：
- en: '[PRE41]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Execute this command only if you have not before:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅在您之前没有执行此命令的情况下执行此命令：
- en: '[PRE42]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Initialize the spaCy engine using the English model:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用英语模型初始化spaCy引擎：
- en: '[PRE43]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Divide the text into sentences:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分割成句子：
- en: '[PRE44]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Print the result:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果：
- en: '[PRE45]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output will be as follows:'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE46]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You will notice that the length of the word list is longer when using spaCy
    than NLTK. One of the reasons is that spaCy keeps the newlines, and each newline
    is a separate token. The other difference is that spaCy splits words with a dash,
    such as *high-power*. You can find the exact difference between the two lists
    by running the following line:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，当使用spaCy时，单词列表的长度比NLTK长。其中一个原因是spaCy保留了换行符，每个换行符都是一个单独的标记。另一个区别是spaCy会分割带有连字符的单词，如*high-power*。您可以通过运行以下行来找到两个列表之间的确切差异：
- en: '[PRE47]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This should result in the following output:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生以下输出：
- en: '[PRE48]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Important note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you are doing other processing with spaCy, it makes sense to use it. Otherwise,
    NLTK word tokenization is sufficient.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用spaCy进行其他处理，使用它是有意义的。否则，NLTK单词分词就足够了。
- en: See also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The NLTK package only has word tokenization for English.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK包只为英语提供单词分词。
- en: 'spaCy has models for other languages: Chinese, Dutch, English, French, German,
    Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. In order
    to use those models, you would have to download them separately. For example,
    for Spanish, use this command to download the model:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy有其他语言的模型：中文、荷兰语、英语、法语、德语、希腊语、意大利语、日语、葡萄牙语、罗马尼亚语、西班牙语和其他语言。为了使用这些模型，你需要单独下载它们。例如，对于西班牙语，使用以下命令下载模型：
- en: '[PRE49]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, put this line in the code to use it:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在代码中添加这一行来使用它：
- en: '[PRE50]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'See the spaCy documentation to find out more: [https://spacy.io/usage/models](https://spacy.io/usage/models).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 查看spaCy文档以获取更多信息：[https://spacy.io/usage/models](https://spacy.io/usage/models)。
- en: Part of speech tagging
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: In many cases, NLP processing depends on determining the parts of speech of
    the words in the text. For example, when we want to find out the named entities
    that appear in a text, we need to know the parts of speech of the words. In this
    recipe, we will again consider NLTK and spaCy algorithms.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，NLP处理取决于确定文本中单词的词性。例如，当我们想要找出文本中出现的命名实体时，我们需要知道单词的词性。在这个食谱中，我们再次考虑NLTK和spaCy算法。
- en: Getting ready
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this part, we will be using the same text of the book *The Adventures of
    Sherlock Holmes*. You can find the whole text in the book’s Github repository.
    For this recipe, we will need just the beginning of the book, which can be found
    in the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这部分，我们将使用书籍《福尔摩斯探案集》的相同文本。你可以在这本书的GitHub仓库中找到整个文本。对于这个食谱，我们只需要书的开始部分，这部分可以在文件[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt)中找到。
- en: In order to do this task, you will need the NLTK and spaCy packages, described
    in the *Technical* *requirements* section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，你需要NLTK和spaCy包，这些包在*技术要求*部分有所描述。
- en: We will also complete this task using the OpenAI API’s GPT model to demonstrate
    that it can complete it as well as spaCy and NLTK. For this part to run, you will
    need the `openai` package, which is included in the Poetry environment. You will
    also need your own OpenAI API key.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用OpenAI API的GPT模型来完成这个任务，以证明它也能像spaCy和NLTK一样完成。为了运行这部分，你需要`openai`包，该包包含在Poetry环境中。你还需要自己的OpenAI
    API密钥。
- en: How to do it…
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: In this recipe, we will use the spaCy package to label words with their parts
    of speech.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用spaCy包来标注单词的词性。
- en: 'The process is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 流程如下：
- en: 'Import the **util** file and the language **util** file. The language **util**
    file contains an import of spaCy and NLTK, as well as an initialization of the
    small spaCy model into the **small_model** object. These files also include functions
    to read in text from a file and tokenization functions using spaCy and NLTK:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入**util**文件和语言**util**文件。语言**util**文件包含spaCy和NLTK的导入，以及将小的spaCy模型初始化到**small_model**对象中。这些文件还包括从文件中读取文本和使用spaCy和NLTK的标记化函数：
- en: '[PRE51]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We will define the function that will output parts of speech for every word.
    In this function, we first process the input text using the spaCy model that results
    in a **Document** object. The resulting **Document** object contains an iterator
    with **Token** objects, and each **Token** object has information about parts
    of speech.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个函数，该函数将为每个单词输出词性。在这个函数中，我们首先使用spaCy模型处理输入文本，这会产生一个**Document**对象。产生的**Document**对象包含一个带有**Token**对象的迭代器，每个**Token**对象都包含有关词性的信息。
- en: We use this information to create the two lists, one with words and the other
    one with their respective parts of speech.
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用这些信息来创建两个列表，一个包含单词，另一个包含它们各自的词性。
- en: 'Finally, we zip the two lists to pair the words with the parts of speech and
    return the resulting list of tuples. We do this in order to easily print the whole
    list with their corresponding parts of speech. When you use part of speech tagging
    in your code, you can just iterate through the list of tokens:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们将两个列表进行压缩，将单词与词性配对，并返回结果列表的元组。我们这样做是为了能够轻松地打印出带有相应词性的整个列表。当你想在代码中使用词性标注时，你只需遍历标记列表：
- en: '[PRE52]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Read in the text:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文本：
- en: '[PRE53]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Run the preceding function using the text and the model as input:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文本和模型作为输入运行前面的函数：
- en: '[PRE54]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Print the output:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印输出：
- en: '[PRE55]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Part of the result is shown in the following; for the complete output, please
    see the Jupyter notebook ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb)):'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下部分结果显示如下；要查看完整的输出，请参阅 Jupyter 笔记本 ([https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb))：
- en: '[PRE56]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The resulting list contains tuples of words and parts of speech. The list of
    part of speech tags is available here: [https://universaldependencies.org/u/pos/](https://universaldependencies.org/u/pos/).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 结果列表包含单词和词性的元组。词性标签列表可在以下位置找到：[https://universaldependencies.org/u/pos/](https://universaldependencies.org/u/pos/)。
- en: There’s more
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多
- en: 'We can compare spaCy’s performance to NLTK in this task. Here are the steps
    for getting the parts of speech with NLTK:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 spaCy 的性能与 NLTK 在此任务中的性能进行比较。以下是使用 NLTK 获取词性的步骤：
- en: 'The imports have been taken care of in the language **util** file that we imported,
    so the first thing we do is create a function that outputs parts of speech for
    the words that are input. In it, we utilize the **word_tokenize_nltk** function
    that is also imported from the language **util** notebook:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入的语言 **util** 文件中已经处理了导入，所以我们首先创建一个函数，该函数输出输入单词的词性。在其中，我们利用也导入自语言 **util**
    笔记本的 **word_tokenize_nltk** 函数：
- en: '[PRE57]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, we apply the function to the text that we read in previously:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将该函数应用于之前读取的文本：
- en: '[PRE58]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Print out the result:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出结果：
- en: '[PRE59]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Part of the output is shown in the following. For the complete output, please
    see the Jupyter notebook:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下部分输出如下。要查看完整的输出，请参阅 Jupyter 笔记本：
- en: '[PRE60]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The list of part of speech tags that NLTK uses is different from what SpaCy
    uses, and can be accessed by running the following commands:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 使用的词性标签列表与 SpaCy 使用的不同，可以通过运行以下命令访问：
- en: '[PRE61]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Comparing the performance, we see that spaCy takes 0.02 seconds, while NLTK
    takes 0.01 seconds (your numbers might be different), so their performance is
    similar, with NLTK being a little better. However, the part of speech information
    is already available in the spaCy objects after the initial processing has been
    done, so if you are doing any further processing, spaCy is a better choice.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 比较性能，我们发现 spaCy 需要 0.02 秒，而 NLTK 需要 0.01 秒（你的数字可能不同），因此它们的性能相似，NLTK 略好。然而，词性信息在初始处理完成后已经存在于
    spaCy 对象中，所以如果你要进行任何进一步的处理，spaCy 是更好的选择。
- en: Important note
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: spaCy does all of its processing at once, and the results are stored in the
    **Doc** object. The part of speech information is available by iterating through
    **Token** objects.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 会一次性完成所有处理，并将结果存储在 **Doc** 对象中。通过迭代 **Token** 对象可以获得词性信息。
- en: There’s more
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多
- en: 'We can use the OpenAI API with the GPT-3.5 and GPT-4 models to perform various
    tasks, including many NLP ones. Here, we show how to use the OpenAI API to get
    NLTK-style parts of speech for input text. You can also specify in the prompt
    the output format and the style of the part of speech tags. For this code to run
    correctly, you will need your own OpenAI API key:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 GPT-3.5 和 GPT-4 模型通过 OpenAI API 执行各种任务，包括许多 NLP 任务。在这里，我们展示了如何使用 OpenAI
    API 获取输入文本的 NLTK 风格的词性。你还可以在提示中指定输出格式和词性标签的风格。为了使此代码正确运行，你需要自己的 OpenAI API 密钥：
- en: 'Import **openai** and create the OpenAI client using your API key. The **OPEN_AI_KEY**
    constant variable is set in the **../****util/file_utils.ipynb** file:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 **openai** 并使用您的 API 密钥创建 OpenAI 客户端。**OPEN_AI_KEY** 常量变量在 **../****util/file_utils.ipynb**
    文件中设置：
- en: '[PRE62]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Set the prompt:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置提示：
- en: '[PRE63]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Send the request to the OpenAI API. Some of the important parameters that we
    send to the API are the model we want to use, the temperature, which affects how
    much the response from the model will vary, and the maximum amount of tokens the
    model should return as a completion:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 OpenAI API 发送请求。我们发送到 API 的一些重要参数是我们想要使用的模型、温度，这会影响模型响应的变化程度，以及模型应返回的最大令牌数作为补全：
- en: '[PRE64]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Print the response:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印响应：
- en: '[PRE65]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output will look like this:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE66]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'To see just the GPT output, do the following:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要仅查看 GPT 输出，请执行以下操作：
- en: '[PRE67]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will be as follows:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE68]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We can use the **literal_eval** function to transform the response into a tuple.
    We request that the GPT model return only the answer without additional explanations
    so that there is no free text inside the answer and we can process it automatically.
    We do this in order to be able to compare the output of the OpenAI API to the
    NLTK output:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用**literal_eval**函数将响应转换为元组。我们要求GPT模型只返回答案，而不附加任何解释，这样答案中就没有自由文本，我们可以自动处理它。我们这样做是为了能够比较OpenAI
    API的输出与NLTK的输出：
- en: '[PRE69]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, let’s time the GPT function so we can compare its performance to the other
    methods we used previously:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们计时GPT函数，以便我们可以将其性能与其他先前使用的方法进行比较：
- en: '[PRE70]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The result will look like this:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果看起来像这样：
- en: '[PRE71]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The output of GPT is very similar to NLTK, but slightly different:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPT的输出与NLTK非常相似，但略有不同：
- en: '[PRE72]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This outputs the following:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会输出以下内容：
- en: '[PRE73]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The difference between GPT and NLTK is that GPT tags the word whole as an adjective
    and NLTK tags it as a noun. In this context, NLTK is correct.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: GPT与NLTK的区别在于，GPT将整个单词标记为形容词，而NLTK将其标记为名词。在这种情况下，NLTK是正确的。
- en: We see that the LLM outputs very similar results but is about 400 times slower
    than NLTK.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到LLM的输出非常相似，但比NLTK慢约400倍。
- en: See also
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'If you would like to tag a text in another language, you can do so by using
    spaCy’s models for other languages. For example, we can load the Spanish spaCy
    model to run it on Spanish text:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想用另一种语言标记文本，你可以使用spaCy的其他语言模型。例如，我们可以加载西班牙语spaCy模型来处理西班牙语文本：
- en: '[PRE74]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In the case that spaCy doesn’t have a model for the language you are working
    with, you can train your own model with spaCy. See [https://spacy.io/usage/training#tagger-parser](https://spacy.io/usage/training#tagger-parser).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果spaCy没有你正在使用的语言的模型，你可以使用spaCy训练自己的模型。请参阅[https://spacy.io/usage/training#tagger-parser](https://spacy.io/usage/training#tagger-parser)。
- en: Combining similar words – lemmatization
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合相似单词 - 词元化
- en: We can find the canonical form of the word using **lemmatization**. For example,
    the lemma of the word *cats* is *cat*, and the lemma for the word *ran* is *run*.
    This is useful when we are trying to match some word and don’t want to list out
    all the possible forms. Instead, we can just use its lemma.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**词元化**来找到单词的规范形式。例如，单词*cats*的词元是*cat*，而单词*ran*的词元是*run*。当我们试图匹配某些单词而不想列出所有可能形式时，这很有用。相反，我们只需使用其词元。
- en: Getting ready
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will be using the spaCy package for this recipe.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用spaCy包来完成这个任务。
- en: How to do it…
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: When the spaCy model processes a piece of text, the resulting `Document` object
    contains an iterator over the `Token` objects within it, as we saw in the *Part
    of speech tagging* recipe. These `Token` objects contain the lemma information
    for each word in the text.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当spaCy处理一段文本时，生成的`Document`对象包含一个迭代器，遍历其中的`Token`对象，正如我们在*词性标注*食谱中看到的。这些`Token`对象包含文本中每个单词的词元信息。
- en: 'Here are the steps for getting the lemmas:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 获取词元的过程如下：
- en: 'Import the file and language **utils** files. This will import spaCy and initialize
    the **small_model** object:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入文件和语言**工具**文件。这将导入spaCy并初始化**小型模型**对象：
- en: '[PRE75]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Create a list of words we want to lemmatize:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个我们想要词元化的单词列表：
- en: '[PRE76]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Create a **Document** object for each of the words:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个单词创建一个**文档**对象：
- en: '[PRE77]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Print the words and their lemmas for each of the words in the list:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印列表中每个单词及其词元：
- en: '[PRE78]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The result will be as follows:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE79]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The result shows correct lemmatization for all words. However, some words are
    ambiguous. For example, the word *leaves* could either be a verb, in which case
    the lemma is correct, or a noun, in which case this is the wrong lemma. If we
    give the spaCy continuous text instead of individual words, it is likely to correctly
    disambiguate the words.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示所有单词的正确词元化。然而，有些单词是模糊的。例如，单词*leaves*可以是动词，在这种情况下词元是正确的，或者它是名词，在这种情况下这个词元是错误的。如果我们给spaCy连续文本而不是单个单词，它很可能会正确地消除歧义。
- en: 'Now, apply lemmatization to the longer text. Here, we read in a small portion
    of the *Sherlock Holmes* text and lemmatize its every word:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将词元化应用于更长的文本。在这里，我们读取一小部分*福尔摩斯探案集*文本，并对其每个单词进行词元化：
- en: '[PRE80]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The partial result will be as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分结果将如下所示：
- en: '[PRE81]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: There’s more…
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'We can use the spaCy lemmatizer object to find out whether a word is in its
    base form or not. We might do this while manipulating the grammar of the sentence,
    for example, in the task of turning a passive sentence into an active one. We
    can get to the lemmatizer object by manipulating the spaCy pipeline, which includes
    various tools that are applied to the text. See [https://spacy.io/usage/processing-pipelines/](https://spacy.io/usage/processing-pipelines/)
    for more information. Here are the steps:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用spaCy词形还原对象来找出一个单词是否在其基本形式中。我们可能在操纵句子语法时这样做，例如，在将被动句转换为主动句的任务中。我们可以通过操纵spaCy管道来获取词形还原对象，该管道包括应用于文本的各种工具。有关更多信息，请参阅[https://spacy.io/usage/processing-pipelines/](https://spacy.io/usage/processing-pipelines/)。以下是步骤：
- en: 'The pipeline components are located in a list of tuples, **(component name,
    component)**. To get the lemmatizer component, we need to loop through this list:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道组件位于一个元组列表中，**（组件名称，组件）**。为了获取词形还原组件，我们需要遍历这个列表：
- en: '[PRE82]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, we can apply the **is_base_form** function call to every word in the Sherlock
    Holmes text:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将**is_base_form**函数调用应用于《福尔摩斯探案集》中的每个单词：
- en: '[PRE83]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'The partial result will be as follows:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 部分结果如下：
- en: '[PRE84]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Removing stopwords
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除停用词
- en: When we work with words, especially if we are considering the words’ semantics,
    we sometimes need to exclude some very frequent words that do not bring any substantial
    meaning into the sentence (words such as *but*, *can*, *we*, etc.). For example,
    if we want to get a rough sense of the topic of a text, we could count its most
    frequent words. However, in any text, the most frequent words will be stopwords,
    so we want to remove them before processing. This recipe shows how to do that.
    The stopwords list we are using in this recipe comes from the NLTK package and
    might not include all the words you need. You will need to modify the list accordingly.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理单词时，尤其是如果我们正在考虑单词的语义时，我们有时需要排除一些在句子中不带来任何实质性意义的非常频繁的单词（例如*但是*、*可以*、*我们*等）。例如，如果我们想对文本的主题有一个大致的了解，我们可以计算其最频繁的单词。然而，在任何文本中，最频繁的单词将是停用词，因此我们希望在处理之前移除它们。这个菜谱展示了如何做到这一点。我们在这个菜谱中使用的停用词列表来自NLTK包，可能不包括你需要的所有单词。你需要相应地修改列表。
- en: Getting ready
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will remove stopwords using spaCy and NLTK; these packages are part of the
    Poetry environment that we installed earlier.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用spaCy和NLTK来移除停用词；这些包是我们之前安装的Poetry环境的一部分。
- en: We will be using the *Sherlock Holmes* text referred to earlier. For this recipe,
    we will need just the beginning of the book, which can be found in the file at
    [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前提到的*福尔摩斯探案集*文本。对于这个菜谱，我们只需要书的开始部分，可以在文件[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt)中找到。
- en: In *step 1*, we run the utilities notebooks. In *step 2*, we import the `nltk`
    package and its stopwords list. In *step 3*, we download the stopwords data, if
    necessary. In *step 4*, we print out the stopwords list. In *step 5*, we read
    in a small portion of the *Sherlock Holmes* book. In *step 6*, we tokenize the
    text and print its length, which is 230\. In *step 7*, we remove the stopwords
    from the original words list by using a list comprehension. Then, we print the
    length of the result and see that the list length has been reduced to 105\. You
    will notice that in the list comprehension, we check whether the *lowercase* version
    of the word is in the stopwords list since all the stopwords are lowercase.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在**步骤 1**中，我们运行实用工具笔记本。在**步骤 2**中，我们导入`nltk`包及其停用词列表。在**步骤 3**中，如果需要，我们下载停用词数据。在**步骤
    4**中，我们打印出停用词列表。在**步骤 5**中，我们读取《福尔摩斯探案集》的一小部分。在**步骤 6**中，我们对文本进行分词并打印其长度，为230。在**步骤
    7**中，我们通过列表推导式从原始单词列表中移除停用词。然后，我们打印结果的长度，看到列表长度已减少到105。你会注意到在列表推导式中，我们检查单词的小写形式是否在停用词列表中，因为所有停用词都是小写的。
- en: How to do it…
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'In the recipe, we will read in the text file, tokenize the text, and remove
    the stopwords from the list:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将读取文本文件，对文本进行分词，并从列表中移除停用词：
- en: 'Run the file and language utilities notebooks:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行文件和语言实用工具笔记本：
- en: '[PRE85]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Import the NLTK stopwords list:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入NLTK停用词列表：
- en: '[PRE86]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The first time you run the notebook, download the **stopwords** data. You don’t
    need to download the stopwords again the next time you run the code:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一次运行笔记本时，下载**停用词**数据。下次运行代码时，无需再次下载停用词：
- en: '[PRE87]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Note
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here is a list of languages that NLTK supports for stopwords: Arabic, Azerbaijani,
    Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Kazakh,
    Nepali, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, and Turkish.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个NLTK支持的停用词语言列表：阿拉伯语、阿塞拜疆语、丹麦语、荷兰语、英语、芬兰语、法语、德语、希腊语、匈牙利语、意大利语、哈萨克语、尼泊尔语、挪威语、葡萄牙语、罗马尼亚语、俄语、西班牙语、瑞典语和土耳其语。
- en: 'You can see the stopwords that come with NLTK by printing the list:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过打印列表来查看NLTK附带的所有停用词：
- en: '[PRE88]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The result will be as follows:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE89]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Read in the text file:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取文本文件：
- en: '[PRE90]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Tokenize the text and print the length of the resulting list:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分词并打印结果列表的长度：
- en: '[PRE91]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The result will be as follows:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE92]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Remove the stopwords from the list using a list comprehension and print the
    length of the result. You will notice that in the list comprehension, we check
    whether the *lowercase* version of the word is in the stopwords list since all
    the stopwords are lowercase.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列表推导从列表中移除停用词并打印结果长度。你将注意到在列表推导中，我们检查单词的小写版本是否在停用词列表中，因为所有停用词都是小写的。
- en: '[PRE93]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The result will be as follows:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE94]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The code then filters the stopwords from the text and leaves the words from
    the text only if they do not also appear in the stopwords list. As we see from
    the lengths of the two lists, one unfiltered and the other without the stopwords,
    we remove more than half the words.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 代码随后会从文本中过滤掉停用词，并且只有当这些词不在停用词列表中时，才会保留文本中的单词。从两个列表的长度来看，一个未过滤，另一个没有停用词，我们移除了超过一半的单词。
- en: Important note
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You might find that some of the words in the stopwords list provided are not
    necessary or are missing. You will need to modify the list accordingly. The NLTK
    stopwords list is a Python list and you can add and remove elements using the
    standard Python list functions.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现提供的停用词列表中的某些单词是不必要的或缺失的。你需要相应地修改列表。NLTK的停用词列表是一个Python列表，你可以使用标准的Python列表函数添加和删除元素。
- en: There’s more…
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'We can also remove stopwords using spaCy. Here is how to do it:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用spaCy移除停用词。以下是这样做的方法：
- en: 'Assign the stopwords to a variable for convenience:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便，将停用词分配给一个变量：
- en: '[PRE95]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Tokenize the text and print its length:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本分词并打印其长度：
- en: '[PRE96]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'It will give us the following result:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它将给出以下结果：
- en: '[PRE97]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Remove the stopwords from the list using a list comprehension and print the
    resulting length:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列表推导从列表中移除停用词并打印结果长度：
- en: '[PRE98]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The result will be very similar to NLTK :'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将非常类似于NLTK：
- en: '[PRE99]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The stopwords from spaCy are stored in a set and we can add more words to it:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: spaCy中的停用词存储在一个集合中，我们可以向其中添加更多单词：
- en: '[PRE100]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'The result will be as follows:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE101]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Similarly, we can remove words if necessary:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，如果需要，我们可以移除单词：
- en: '[PRE102]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'The result will be as follows:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE103]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: We can also compile a stopwords list using the text we are working with and
    calculate the frequencies of the words in it. This provides you with an automatic
    way of removing stopwords, without the need for manual review.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用我们正在处理的文本来编译一个停用词列表，并计算其中单词的频率。这为你提供了一个自动移除停用词的方法，无需手动审查。
- en: There's still more
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多
- en: 'In this section, we will show you two ways of doing so. You will need to use
    the file at [https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt).
    The `FreqDist` object in the NLTK package counts the number of occurrences of
    each word that we later use to find the most frequent words and remove them as
    stopwords:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示两种实现方式。你需要使用文件[https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt)。NLTK包中的`FreqDist`对象计算每个单词的出现次数，这是我们后来用来找到最频繁的单词并将其移除作为停用词的依据：
- en: 'Import the NTLK **FreqDist** class:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入NTLK的**FreqDist**类：
- en: '[PRE104]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Define the function that will compile a list of stopwords:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个将编译停用词列表的函数：
- en: '[PRE105]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Define the stopwords list using the default settings, and print out the result
    and its length:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认设置定义停用词列表，并打印结果及其长度：
- en: '[PRE106]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The result will be as follows:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE107]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Now, use the function with the frequency cut-off of 5% (use the top 5% of the
    most frequent words as stopwords):'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用频率截止值为5%的函数（使用最频繁的5%的单词作为停用词）：
- en: '[PRE108]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The result will be as follows:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE109]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Now, use the absolute frequency cut-off of **100** (take the words that have
    a frequency greater than 100):'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用绝对频率截止值为**100**（选取频率大于100的单词）：
- en: '[PRE110]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'And the result is the following:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE111]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: The function that creates the stopwords list takes in the text and the `cut_off`
    parameter. It could be a float representing the percentage of frequency-ranked
    words that will be in the stopwords list. Alternatively, it could be an integer
    that represents the absolute threshold frequency, with words above it considered
    stopwords. In the function, we first tokenize the words from the book, then create
    a `FreqDist` object, and then create a list of tuples (word, word’s frequency)
    using the frequency distribution. We sort the list using the word frequency. We
    then check the `cut_off` parameter’s type and raise an error if it is not a float
    or an integer. If it is an integer, we return all words whose frequency is higher
    than the parameter as stopwords. If it is a float, we calculate the number of
    words to be returned using the parameter as the percentage.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 创建停用词列表的函数接受文本和`cut_off`参数。它可以是表示停用词列表中频率排名单词百分比的浮点数。或者，它也可以是一个表示绝对阈值频率的整数，高于该频率的单词被视为停用词。在函数中，我们首先从书中提取单词，然后创建一个`FreqDist`对象，接着使用频率分布创建一个包含元组（单词，单词频率）的列表。我们使用单词频率对列表进行排序。然后，我们检查`cut_off`参数的类型，如果它不是浮点数或整数，则引发错误。如果是整数，我们返回频率高于参数的所有单词作为停用词。如果是浮点数，我们使用参数作为百分比来计算要返回的单词数量。
