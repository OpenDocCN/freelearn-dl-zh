- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Learning DDPG, TD3, and SAC
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 DDPG、TD3 和 SAC
- en: In the previous chapter, we learned about interesting actor-critic methods,
    such as **Advantage Actor-Critic** (**A2C**) and **Asynchronous Advantage Actor-Critic**
    (**A3C**). In this chapter, we will learn several state-of-the-art actor-critic
    methods. We will start off the chapter by understanding one of the popular actor-critic
    methods called **Deep Deterministic Policy Gradient** (**DDPG**). DDPG is used
    only in continuous environments, that is, environments with a continuous action
    space. We will understand what DDPG is and how it works in detail. We will also
    learn the DDPG algorithm step by step.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们了解了有趣的演员-评论家方法，如 **优势演员-评论家** (**A2C**) 和 **异步优势演员-评论家** (**A3C**)。在本章中，我们将学习几种最先进的演员-评论家方法。我们将从理解一种流行的演员-评论家方法
    **深度确定性策略梯度** (**DDPG**) 开始。DDPG 仅在连续环境中使用，即具有连续动作空间的环境。我们将详细了解 DDPG 是什么以及它如何工作。我们还将逐步学习
    DDPG 算法。
- en: Going forward, we will learn about the **Twin Delayed Deep Deterministic Policy
    Gradient** (**TD3**). TD3 is an improvement over the DDPG algorithm and includes
    several interesting features that solve the problems faced in DDPG. We will understand
    the key features of TD3 in detail and also look into the algorithm of TD3 step
    by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解 **双延迟深度确定性策略梯度** (**TD3**)。TD3 是对 DDPG 算法的改进，包括解决 DDPG 中面临的几个问题的一些有趣特性。我们将详细了解
    TD3 的关键特性，还将逐步学习 TD3 的算法。
- en: Finally, we will learn about another interesting actor-critic algorithm, called
    **Soft Actor-Critic (SAC)**. We will learn what SAC is and how it works using
    the entropy term in the objective function. We will look into the actor and critic
    components of SAC in detail and then learn the algorithm of SAC step by step.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将学习另一种有趣的演员-评论家算法，称为 **软演员-评论家 (SAC)**。我们将学习 SAC 是什么以及它如何通过目标函数中的熵项工作。我们将详细了解
    SAC 的演员和评论家组件，然后逐步学习 SAC 算法。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Deep deterministic policy gradient (DDPG)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度 (DDPG)
- en: The components of DDPG
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG 的组成部分
- en: The DDPG algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG 算法
- en: Twin delayed deep deterministic policy gradient (TD3)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双延迟深度确定性策略梯度 (TD3)
- en: The key features of TD3
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3 的关键特性
- en: The TD3 algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3 算法
- en: Soft actor-critic (SAC)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软演员-评论家（SAC）
- en: The components of SAC
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAC 的组成部分
- en: The SAC algorithm
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAC 算法
- en: Deep deterministic policy gradient
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: DDPG is an off-policy, model-free algorithm, designed for environments where
    the action space is continuous. In the previous chapter, we learned how the actor-critic
    method works. DDPG is an actor-critic method where the actor estimates the policy
    using the policy gradient, and the critic evaluates the policy produced by the
    actor using the Q function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是一种离策略、无模型的算法，专为动作空间连续的环境设计。在前一章中，我们学习了演员-评论家方法的工作原理。DDPG 是一种演员-评论家方法，其中演员使用策略梯度估计策略，评论家使用
    Q 函数评估演员产生的策略。
- en: DDPG uses the policy network as an actor and deep Q network as a critic. One
    important difference between the DPPG and actor-critic algorithms we learned in
    the previous chapter is that DDPG tries to learn a deterministic policy instead
    of a stochastic policy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 使用策略网络作为演员和深度 Q 网络作为评论家。我们在前一章中学到的 DPPG 和演员-评论家算法之间的一个重要区别是，DDPG 尝试学习确定性策略而不是随机策略。
- en: First, we will get an intuitive understanding of how DDPG works and then we
    will look into the algorithm in detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将直观地了解 DDPG 的工作原理，然后详细学习算法。
- en: An overview of DDPG
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG 概述
- en: DDPG is an actor-critic method that takes advantage of both the policy-based
    method and the value-based method. It uses a deterministic policy ![](img/B15558_12_001.png)
    instead of a stochastic policy ![](img/B15558_03_139.png).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 是一种演员-评论家方法，充分利用了基于策略的方法和基于值的方法。它使用确定性策略 ![](img/B15558_12_001.png) 而不是随机策略
    ![](img/B15558_03_139.png)。
- en: 'We learned that a deterministic policy tells the agent to perform one particular
    action in a given state, meaning a deterministic policy maps the state to one
    particular action:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，确定性策略告诉代理在给定状态下执行一种特定的动作，这意味着确定性策略将状态映射到一个特定的动作：
- en: '![](img/B15558_12_003.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_003.png)'
- en: 'Whereas a stochastic policy maps the state to the probability distribution
    over the action space:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 而随机策略将状态映射到动作空间上的概率分布：
- en: '![](img/B15558_12_004.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_004.png)'
- en: In a deterministic policy, whenever the agent visits the state, it always performs
    the same particular action. But with a stochastic policy, instead of performing
    the same action every time the agent visits the state, the agent performs a different
    action each time based on a probability distribution over the action space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定性策略中，每当智能体访问某个状态时，它总是执行相同的特定动作。但是在随机策略中，智能体不是每次访问状态时都执行相同的动作，而是根据动作空间中的概率分布每次执行不同的动作。
- en: Now, we will look into an overview of the actor and critic networks in the DDPG
    algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将概览DDPG算法中的演员和评论员网络。
- en: Actor
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演员
- en: The actor in DDPG is basically the policy network. The goal of the actor is
    to learn the mapping between the state and action. That is, the role of the actor
    is to learn the optimal policy that gives the maximum return. So, the actor uses
    the policy gradient method to learn the optimal policy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG中的演员本质上就是策略网络。演员的目标是学习状态与动作之间的映射关系。也就是说，演员的作用是学习能带来最大回报的最优策略。因此，演员使用策略梯度方法来学习最优策略。
- en: Critic
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论员
- en: The critic is basically the value network. The goal of the critic is to evaluate
    the action produced by the actor network. How does the critic network evaluate
    the action produced by the actor network? Let's suppose we have a Q function;
    can we evaluate an action using the Q function? Yes! First, let's take a little
    detour and recap the use of the Q function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员本质上就是价值网络。评论员的目标是评估演员网络所产生的动作。评论员网络如何评估演员网络产生的动作呢？假设我们有一个Q函数，我们能否使用Q函数来评估一个动作呢？当然可以！首先，让我们稍微绕一下路，回顾一下Q函数的使用。
- en: 'We know that the Q function gives the expected return that an agent would obtain
    starting from state *s* and performing an action *a* following a particular policy.
    The expected return produced by the Q function is often called the Q value. Thus,
    given a state and action, we obtain a Q value:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，Q函数给出了一个智能体从状态*s*开始并执行动作*a*时，按照特定策略所获得的预期回报。Q函数产生的预期回报通常称为Q值。因此，给定一个状态和动作，我们可以得到一个Q值：
- en: If the Q value is high, then we can say that the action performed in that state
    is a good action. That is, if the Q value is high, meaning the expected return
    is high when we perform an action *a* in state *s*, we can say that the action
    *a* is a good action.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Q值很高，我们可以说在该状态下执行的动作是一个好的动作。也就是说，如果Q值很高，意味着当我们在状态*s*中执行动作*a*时，预期回报很高，我们可以说动作*a*是一个好的动作。
- en: If the Q value is low, then we can say that the action performed in that state
    is not a good action. That is, if the Q value is low, meaning the expected return
    is low when we perform an action *a* in state *s*, we can say that the action
    *a* is not a good action.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Q值很低，我们可以说在该状态下执行的动作不是一个好的动作。也就是说，如果Q值很低，意味着当我们在状态*s*中执行动作*a*时，预期回报很低，我们可以说动作*a*不是一个好的动作。
- en: Okay, now how can the critic network evaluate an action produced by the actor
    network based on the Q function (Q value)? Let's suppose the actor network performs
    a *down* action in state **A**. So, now, the critic computes the Q value of moving
    *down* in state **A**. If the Q value is high, then the critic network gives feedback
    to the actor network that the action *down* is a good action in state **A**. If
    the Q value is low, then the critic network gives feedback to the actor network
    that the *down* action is not a good action in state **A,** and so the actor network
    tries to perform a different action in state **A**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么评论网络如何基于Q函数（Q值）来评估演员网络所产生的动作呢？假设演员网络在状态**A**下执行了一个*down*动作。那么，现在，评论网络计算在状态**A**下执行*down*动作的Q值。如果Q值很高，那么评论网络会给演员网络反馈，表示在状态**A**下，*down*动作是一个好的动作。如果Q值很低，那么评论网络会给演员网络反馈，表示在状态**A**下，*down*动作不是一个好的动作，因此演员网络会尝试在状态**A**下执行一个不同的动作。
- en: Thus, with the Q function, the critic network can evaluate the action performed
    by the actor network. But wait, how can the critic network learn the Q function?
    Because only if it knows the Q function can it evaluate the action performed by
    the actor. So, how does the critic network learn the Q function? Here is where
    we use the **deep Q network** (**DQN**). We learned that with the DQN, we can
    use the neural network to approximate the Q function. So, now, we use the DQN
    as the critic network to compute the Q function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，借助 Q 函数，评论网络可以评估演员网络执行的动作。但是，等等，评论网络怎么学习 Q 函数呢？因为只有当它知道 Q 函数时，才能评估演员执行的动作。那么，评论网络如何学习
    Q 函数呢？这就是我们使用**深度 Q 网络**（**DQN**）的地方。我们了解到，利用 DQN，可以使用神经网络来近似 Q 函数。因此，现在我们使用 DQN
    作为评论网络来计算 Q 函数。
- en: Thus, in a nutshell, DDPG is an actor-critic method and so it takes advantage
    of policy-based and value-based methods. DDPG consists of an actor that is a policy
    network and uses the policy gradient method to learn the optimal policy and the
    critic, which is a deep Q network, and it evaluates the action produced by the
    actor.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，简而言之，DDPG 是一种演员-评论方法，它结合了基于策略和基于价值的方法。DDPG 由演员（一个策略网络）和评论（一个深度 Q 网络）组成，演员通过策略梯度方法来学习最优策略，评论则评估演员产生的动作。
- en: DDPG components
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG 组件
- en: Now that we have a basic understanding of how the DDPG algorithm works, let's
    go into further detail. We will understand how exactly the actor and critic networks
    work by looking at them separately.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 DDPG 算法的基本工作原理有了了解，接下来我们将深入探讨。通过分别查看演员和评论网络，我们将更好地理解它们的具体工作原理。
- en: Critic network
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论网络
- en: We learned that the critic network is basically the DQN and it uses the DQN
    to estimate the Q value. Now, let's learn how the critic network uses the DQN
    to estimate the Q value in more detail, along with a recap of the DQN.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，评论网络基本上就是 DQN，它利用 DQN 来估计 Q 值。现在，让我们更详细地了解评论网络如何使用 DQN 来估计 Q 值，并回顾一下 DQN。
- en: 'The critic evaluates the action produced by the actor. Thus, the input to the
    critic will be the state and also the action produced by the actor in that state,
    and the critic returns the Q value of the given state-action pair, as shown *Figure
    12.1*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 评论网络评估演员产生的动作。因此，评论的输入将是状态以及在该状态下由演员产生的动作，评论返回给定状态-动作对的 Q 值，如*图 12.1*所示：
- en: '![](img/B15558_12_01.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_01.png)'
- en: 'Figure 12.1: The critic network'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：评论网络
- en: To approximate the Q value in the critic, we can use the deep neural network,
    and if we use the deep neural network to approximate the Q value, then the network
    is called the DQN. Since we are using the neural network to approximate the Q
    value in the critic, we can represent the Q function with ![](img/B15558_12_005.png),
    where ![](img/B15558_12_006.png) is the parameter of the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在评论中近似 Q 值，我们可以使用深度神经网络，如果我们用深度神经网络来近似 Q 值，那么这个网络就叫做 DQN。由于我们使用神经网络来近似评论中的
    Q 值，所以我们可以用 ![](img/B15558_12_005.png) 来表示 Q 函数，其中 ![](img/B15558_12_006.png)
    是网络的参数。
- en: 'Thus, in the critic network, we approximate the Q value using the DQN and the
    parameter of the critic network is represented by ![](img/B15558_12_006.png),
    as shown in *Figure 12.2*:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在评论网络中，我们使用 DQN 来近似 Q 值，评论网络的参数由 ![](img/B15558_12_006.png) 表示，如*图 12.2*所示：
- en: '![](img/B15558_12_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_02.png)'
- en: 'Figure 12.2: The critic network'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：评论网络
- en: As we can observe from *Figure 12.2*, given state *s* and the action *a* produced
    by the actor, the critic network returns the Q value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如从*图 12.2*中可以观察到，给定状态 *s* 和由演员生成的动作 *a*，评论网络返回 Q 值。
- en: Now, let's look at how to obtain the action *a* produced by the actor. We learned
    that the actor is basically the policy network and it uses a policy gradient to
    learn the optimal policy. In DDPG, we learn a deterministic policy instead of
    a stochastic policy, so we can denote the policy with ![](img/B15558_12_008.png)
    instead of ![](img/B15558_12_009.png). The parameter of the actor network is represented
    by ![](img/B15558_11_043.png). So, we can represent our parameterized policy as
    ![](img/B15558_12_011.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何获得演员产生的动作 *a*。我们了解到，演员基本上是策略网络，它通过策略梯度方法来学习最优策略。在 DDPG 中，我们学习的是确定性策略，而不是随机策略，因此我们可以用
    ![](img/B15558_12_008.png) 来表示策略，而不是 ![](img/B15558_12_009.png)。演员网络的参数由 ![](img/B15558_11_043.png)
    表示。因此，我们可以将参数化策略表示为 ![](img/B15558_12_011.png)。
- en: 'Given a state *s* as the input, the actor network returns the action *a* to
    be performed in that state:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个状态 *s* 作为输入，演员网络返回在该状态下执行的动作 *a*：
- en: '![](img/B15558_12_012.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_012.png)'
- en: 'Thus, the critic network takes state *s* and action ![](img/B15558_12_012.png)
    produced by the actor network in that state as input and returns the Q value,
    as shown in *Figure 12.3*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评论家网络将状态 *s* 和演员网络在该状态下产生的动作 ![](img/B15558_12_012.png) 作为输入，返回 Q 值，如 *图
    12.3* 所示：
- en: '![](img/B15558_12_03.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_03.png)'
- en: 'Figure 12.3: The critic network'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3：评论家网络
- en: Okay, how can we train the critic network (DQN)? We generally train the network
    by minimizing the loss as the difference between the target value and predicted
    value. So, we can train the critic network by minimizing the loss as the difference
    between the target Q value and the Q value predicted by the network. But how can
    we obtain the target Q value? The target Q value is the optimal Q value and we
    can obtain the optimal Q value using the Bellman equation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何训练评论家网络（DQN）呢？我们通常通过最小化损失来训练网络，损失是目标值和预测值之间的差异。因此，我们可以通过最小化损失来训练评论家网络，损失是目标
    Q 值与网络预测的 Q 值之间的差异。但我们如何获得目标 Q 值呢？目标 Q 值是最优 Q 值，我们可以使用贝尔曼方程来获得最优 Q 值。
- en: 'We learned that the optimal Q function (Q value) can be obtained by using the
    Bellman optimality equation. Thus, the optimal Q function can be obtained using
    the Bellman optimality equation as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，最优 Q 函数（Q 值）可以通过使用贝尔曼最优性方程来获得。因此，最优 Q 函数可以通过以下贝尔曼最优性方程来获得：
- en: '![](img/B15558_12_014.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_014.png)'
- en: 'We know that ![](img/B15558_12_015.png) represents the immediate reward *r*
    we obtain while performing an action *a* in state *s* and moving to the next state
    ![](img/B15558_12_016.png), so we can just denote ![](img/B15558_12_015.png) with
    *r*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，![](img/B15558_12_015.png) 表示我们在状态 *s* 中执行动作 *a* 并转移到下一个状态 ![](img/B15558_12_016.png)
    时获得的即时奖励 *r*，因此我们可以将 ![](img/B15558_12_015.png) 简单表示为 *r*：
- en: '![](img/B15558_12_018.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_018.png)'
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer and
    taking the average value. We will learn more about this in a while. So, we can
    express the target Q value as the sum of the immediate reward and discounted maximum
    Q value of the next state-action pair, as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方程中，我们可以去除期望。我们将通过从重放缓冲区中采样 *K* 数量的过渡状态并取平均值来逼近期望。稍后我们将更详细地了解这个过程。所以，我们可以将目标
    Q 值表示为即时奖励和下一个状态-动作对的折扣最大 Q 值的和，如下所示：
- en: '![](img/B15558_12_019.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_019.png)'
- en: 'Thus, we can represent the loss function of the critic network as the difference
    between the target value (optimal Bellman Q value) and the predicted value (the
    Q value predicted by the critic network):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将评论家网络的损失函数表示为目标值（最优贝尔曼 Q 值）和预测值（评论家网络预测的 Q 值）之间的差异：
- en: '![](img/B15558_12_020.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_020.png)'
- en: Here, the action *a* is the action produced by the actor network, that is,![](img/B15558_12_021.png).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，动作 *a* 是由演员网络产生的动作，即 ![](img/B15558_12_021.png)。
- en: 'Instead of using the loss as simply the difference between the target value
    and the predicted value, we can use the mean squared error as our loss function.
    We know that in the DQN, we use the replay buffer and store the transitions as
    ![](img/B15558_12_022.png). So, we randomly sample a minibatch of *K* number of
    transitions from the replay buffer and train the network by minimizing the mean
    squared loss between the target value (optimal Bellman Q value) and the predicted
    value (Q value predicted by the critic network). Thus, our loss function is given
    as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不将损失作为目标值和预测值之间的差异，而是将均方误差作为我们的损失函数。我们知道，在 DQN 中，我们使用重放缓冲区并存储过渡状态为 ![](img/B15558_12_022.png)。因此，我们从重放缓冲区中随机抽取一个
    *K* 数量的过渡状态小批量，并通过最小化目标值（最优贝尔曼 Q 值）和预测值（评论家网络预测的 Q 值）之间的均方损失来训练网络。因此，我们的损失函数表示为：
- en: '![](img/B15558_12_023.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_023.png)'
- en: From the preceding equation, we can observe that both the target and predicted
    Q functions are parameterized by the same parameter ![](img/B15558_12_006.png).
    This will cause instability in the mean squared error and the network will learn
    poorly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中，我们可以观察到目标 Q 函数和预测的 Q 函数都由相同的参数 ![](img/B15558_12_006.png) 参数化。这将导致均方误差的不稳定，网络的学习效果会很差。
- en: So, we introduce another neural network to learn the target value, and it is
    usually referred to as the target critic network. The parameter of the target
    critic network is represented by ![](img/B15558_12_025.png). Our main critic network,
    which is used to predict Q values, learns the correct parameter ![](img/B15558_12_006.png)
    using gradient descent. The target critic network parameter ![](img/B15558_12_025.png)
    is updated by just copying the parameter of the main critic network ![](img/B15558_12_006.png).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入了另一个神经网络来学习目标值，通常称为目标评论家网络。目标评论家网络的参数用![](img/B15558_12_025.png)表示。我们的主要评论家网络用于预测Q值，并通过梯度下降学习正确的参数![](img/B15558_12_006.png)。目标评论家网络参数![](img/B15558_12_025.png)通过直接复制主要评论家网络的参数![](img/B15558_12_006.png)来更新。
- en: 'Thus, the loss function of the critic network can be written as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评论家网络的损失函数可以写作：
- en: '![](img/B15558_12_029.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_029.png)'
- en: Remember that the action *a*[i] in the preceding equation is the action produced
    by the actor network, that is, ![](img/B15558_12_030.png).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，前面方程中的动作*a*[i]是由演员网络生成的动作，即！[](img/B15558_12_030.png)。
- en: 'There is a small problem in the target value computation in our loss function
    due to the presence of the max term, as shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最大项的存在，在目标值计算中有一个小问题，如下所示：
- en: '![](img/B15558_12_06.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_06.png)'
- en: The max term means that we compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png) and select the action ![](img/B15558_12_031.png)
    as the one that has the maximum Q value. But when the action space is continuous,
    we cannot compute the Q value of all possible actions ![](img/B15558_12_031.png)
    in state ![](img/B15558_12_016.png). So, we need to get rid of the max term in
    our loss function. How can we do that?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最大项意味着我们计算所有可能动作的Q值！[](img/B15558_12_031.png)，在状态！[](img/B15558_12_016.png)下，并选择具有最大Q值的动作！[](img/B15558_12_031.png)。但是，当动作空间是连续时，我们无法计算所有可能动作！[](img/B15558_12_031.png)在状态！[](img/B15558_12_016.png)下的Q值。因此，我们需要去掉损失函数中的最大项。我们该如何做呢？
- en: Just as we use the target network in the critic, we can use a target actor network,
    and the parameter of the target actor network is denoted by ![](img/B15558_12_042.png).
    Now, instead of selecting the action ![](img/B15558_12_031.png) as the one that
    has the maximum Q value, we can generate an action ![](img/B15558_12_031.png)
    using the target actor network, that is, ![](img/B15558_12_039.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在评论家中使用目标网络一样，我们也可以使用目标演员网络，目标演员网络的参数用![](img/B15558_12_042.png)表示。现在，我们不再选择具有最大Q值的动作！[](img/B15558_12_031.png)，而是可以使用目标演员网络生成一个动作！[](img/B15558_12_031.png)，即！[](img/B15558_12_039.png)。
- en: 'Thus, as shown in *Figure 12.4*, to compute the Q value of the next state-action
    pair in the target, we feed state ![](img/B15558_12_016.png) and the action ![](img/B15558_12_031.png)
    produced by the target actor network parameterized by ![](img/B15558_12_042.png)
    to the target critic network, and it returns the Q value of the next state-action
    pair:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如*图12.4*所示，为了计算目标中的下一个状态-动作对的Q值，我们将状态！[](img/B15558_12_016.png)和由目标演员网络（参数化为！[](img/B15558_12_042.png)）生成的动作！[](img/B15558_12_031.png)输入到目标评论家网络中，它返回下一个状态-动作对的Q值：
- en: '![](img/B15558_12_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_04.png)'
- en: 'Figure 12.4: The target critic network'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4：目标评论家网络
- en: 'Thus, in our loss function, equation (1), we can remove the max term and instead
    of ![](img/B15558_12_031.png), we can write ![](img/B15558_12_044.png), as shown
    here:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的损失函数中，方程（1），我们可以去掉最大项，而是将![](img/B15558_12_031.png)写作![](img/B15558_12_044.png)，如下所示：
- en: '![](img/B15558_12_045.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_045.png)'
- en: 'To maintain a uniform notation, let''s represent the loss function with *J*:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持符号的一致性，我们用*J*表示损失函数：
- en: '![](img/B15558_12_046.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_046.png)'
- en: 'To reduce the clutter, we can denote the target value with *y* and write:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少杂乱，我们可以用*y*表示目标值，并写作：
- en: '![](img/B15558_12_047.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_048.png),
    and the action *a*[i] is the action produced by the main actor network, that is,
    ![](img/B15558_12_030.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*y*[i]是评论家的目标值，即！[](img/B15558_12_048.png)，而动作*a*[i]是由主要演员网络生成的动作，即！[](img/B15558_12_030.png)。
- en: 'To minimize the loss, we compute the gradients of the objective function ![](img/B15558_12_050.png)
    and update the main critic network parameter ![](img/B15558_12_006.png) by performing
    gradient descent:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化损失，我们计算目标函数![](img/B15558_12_050.png)的梯度，并通过梯度下降更新主要的评论家网络参数![](img/B15558_12_006.png)：
- en: '![](img/B15558_12_052.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_052.png)'
- en: 'Okay, what about the target critic network parameter ![](img/B15558_12_025.png)?
    How can we update it? We can update the parameter of the target critic network
    by just copying the parameter of the main critic network parameter ![](img/B15558_09_098.png)
    as shown here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那目标评论网络的参数![](img/B15558_12_025.png)呢？我们如何更新它？我们可以通过直接复制主评论网络参数![](img/B15558_09_098.png)的参数来更新目标评论网络的参数，如下所示：
- en: '![](img/B15558_12_055.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_055.png)'
- en: This is usually called the soft replacement and the value of ![](img/B15558_12_056.png)
    is often set to 0.001.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常被称为软替代，并且![](img/B15558_12_056.png)的值通常设置为0.001。
- en: Thus, we learned how the critic network uses the DQN to compute the Q value
    to evaluate the action produced by the actor network. In the next section, we
    will learn how the actor network learns the optimal policy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们学到评论网络如何使用DQN来计算Q值，以评估演员网络产生的动作。在下一节中，我们将学习演员网络如何学习最优策略。
- en: Actor network
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演员网络
- en: We have already learned that the actor network is the policy network and it
    uses the policy gradient to compute the optimal policy. We also learned that we
    represent the parameter of the actor network with ![](img/B15558_11_043.png),
    and so the parameterized policy is represented with ![](img/B15558_12_011.png).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到演员网络是策略网络，它使用策略梯度来计算最优策略。我们还学到我们通过![](img/B15558_11_043.png)表示演员网络的参数，因此参数化的策略表示为![](img/B15558_12_011.png)。
- en: 'The actor network takes state *s* as an input and returns the action *a*:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络以状态 *s* 作为输入，并返回动作 *a*：
- en: '![](img/B15558_12_059.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_059.png)'
- en: One important point that we may want to note down here is that we are using
    a deterministic policy. Since we are using a deterministic policy, we need to
    take care of the exploration-exploitation dilemma, because we know that a deterministic
    policy always selects the same action and doesn't explore new actions, unlike
    a stochastic policy, which selects different actions based on the probability
    distribution over the action space.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们可能需要注意的一个重要点是，我们使用的是确定性策略。由于我们使用的是确定性策略，我们需要处理探索-开发困境，因为我们知道确定性策略总是选择相同的动作，而不会探索新的动作，这与基于动作空间概率分布选择不同动作的随机策略不同。
- en: Okay, how can we explore new actions while using a deterministic policy? Note
    that DDPG is designed for an environment where the action space is continuous.
    Thus, we are using a deterministic policy in the continuous action space.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在使用确定性策略时，我们如何探索新的动作呢？请注意，DDPG是为动作空间连续的环境设计的。因此，我们在连续动作空间中使用确定性策略。
- en: 'Unlike the discrete action space, in the continuous action space, we have continuous
    values. So, to explore new actions, we can just add some noise ![](img/B15558_12_060.png)
    to the action produced by the actor network since the action is a continuous value.
    We generate this noise using a process called the Ornstein-Uhlenbeck random process.
    So, our modified action can be represented as:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与离散动作空间不同，在连续动作空间中，我们有连续的值。因此，为了探索新的动作，我们可以直接在演员网络产生的动作上添加一些噪声![](img/B15558_12_060.png)，因为动作是一个连续值。我们通过一种叫做奥恩斯坦-乌伦贝克随机过程的方式生成这个噪声。因此，我们修改后的动作可以表示为：
- en: '![](img/B15558_12_061.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_061.png)'
- en: For example, say the action ![](img/B15558_12_062.png) produced by the actor
    network is 13\. Suppose the noise ![](img/B15558_12_060.png) is 0.1, then our
    action becomes *a* = 13+0.1 = 13.1.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设演员网络产生的动作![](img/B15558_12_062.png)是13。假设噪声![](img/B15558_12_060.png)是0.1，那么我们的动作变成了
    *a* = 13+0.1 = 13.1。
- en: We learned that the critic network is represented by ![](img/B15558_12_064.png)
    and it evaluates the action produced by the actor using the Q value. If the Q
    value is high, then the critic tells the actor that it has produced a good action
    but when the Q value is low, then the critic tells the actor that it has produced
    a bad action.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到评论网络通过![](img/B15558_12_064.png)表示，它使用Q值来评估演员产生的动作。如果Q值很高，那么评论网络告诉演员它产生了一个好的动作；但当Q值很低时，评论网络告诉演员它产生了一个不好的动作。
- en: But wait! We learned that it is difficult to compute the Q value when the action
    space is continuous. That is, when the action space is continuous, it is difficult
    to compute the Q value of all possible actions in the state and take the maximum
    Q value. That is why we resorted to the policy gradient method. But now, we are
    computing the Q value with a continuous action space. How will this work?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等！我们了解到当动作空间是连续时，计算 Q 值是困难的。也就是说，当动作空间是连续的时，计算状态中所有可能动作的 Q 值并取最大 Q 值是困难的。这就是为什么我们转向策略梯度方法。但现在，我们正在计算具有连续动作空间的
    Q 值。这会怎么样？
- en: Note that, here in DDPG, we are not computing the Q value of all possible state-action
    pairs. We simply compute the Q value of state *s* and action *a* produced by the
    actor network.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 DDPG 中，我们并不计算所有可能的状态-动作对的 Q 值。我们只是简单地计算演员网络生成的状态 *s* 和动作 *a* 的 Q 值。
- en: The goal of the actor is to make the critic tell that the action it has produced
    is a good action. That is, the actor wants to get good feedback from the critic
    network. When does the critic give good feedback to the actor? The critic gives
    good feedback when the action produced by the actor has a maximum Q value. That
    is, if the action produced by the actor has a maximum Q value, then the critic
    tells the actor that it has produced a good action. So, the actor tries to generate
    an action in such a way that it can maximize the Q value produced by the critic.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 演员的目标是让评论者说出它产生的动作是一个好动作。也就是说，演员希望从评论者网络获得良好的反馈。评论者何时会给演员良好的反馈？当演员产生的动作具有最大的
    Q 值时，评论者会给出良好的反馈。因此，演员试图以一种方式生成动作，使得它可以最大化评论者生成的 Q 值。
- en: 'Thus, the objective function of the actor is to generate an action that maximizes
    the Q value produced by the critic network. So, we can write the objective function
    of the actor as:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，演员的目标函数是生成一个动作，最大化评论者网络生成的 Q 值。因此，我们可以将演员的目标函数写为：
- en: '![](img/B15558_12_065.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_065.png)'
- en: 'Where the action ![](img/B15558_12_066.png). Maximizing the above objective
    function ![](img/B15558_12_067.png) implies that we are maximizing the Q value
    produced by the critic network. Okay, how can we maximize the preceding objective
    function? We can maximize the objective function by performing gradient ascent
    and update the actor network parameter as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_066.png)。最大化上述目标函数 ![](img/B15558_12_067.png) 意味着我们在最大化评论者网络生成的
    Q 值。好的，我们如何最大化前述目标函数呢？我们可以通过执行梯度上升来最大化目标函数，并更新演员网络参数如下：
- en: '![](img/B15558_12_068.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_068.png)'
- en: 'Wait. Instead of updating the actor network parameter ![](img/B15558_11_043.png)
    just for a single state ![](img/B15558_12_070.png), we sample ![](img/B15558_12_071.png)
    number of states from the replay buffer ![](img/B15558_12_072.png) and update
    the parameter. So, now our objective function becomes:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。我们不仅仅为单个状态 ![](img/B15558_12_070.png) 更新演员网络参数 ![](img/B15558_11_043.png)，而是从重播缓冲区
    ![](img/B15558_12_072.png) 中采样 ![](img/B15558_12_071.png) 个状态并更新参数。因此，我们的目标函数现在变成：
- en: '![](img/B15558_12_073.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_073.png)'
- en: 'Where the action ![](img/B15558_12_074.png). Maximizing the preceding objective
    function implies that the actor tries to generate actions in such a way that it
    maximizes the Q value over all the sampled states. We can maximize the objective
    function by performing gradient ascent and update the actor network parameter
    as:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_074.png)。最大化前述目标函数意味着演员试图以一种方式生成动作，使得在所有采样状态下最大化 Q 值。我们可以通过执行梯度上升来最大化目标函数，并更新演员网络参数如下：
- en: '![](img/B15558_12_068.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_068.png)'
- en: To summarize, the objective of the actor is to generate action in such a way
    that it maximizes the Q value produced by the critic. So, we perform gradient
    ascent and update the actor network parameter.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，演员的目标是以一种方式生成动作，使得它最大化评论者生成的 Q 值。因此，我们执行梯度上升，并更新演员网络参数。
- en: 'Okay, what about the parameter of the target actor network? How can we update
    it? We can update the parameter of the target actor network by just copying the
    parameter of the main actor network parameter ![](img/B15558_11_043.png) by soft
    replacement, as shown here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么目标演员网络的参数如何更新？我们可以通过软替换仅复制主要演员网络参数 ![](img/B15558_11_043.png) 来更新目标演员网络参数，如下所示：
- en: '![](img/B15558_12_077.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_077.png)'
- en: Now that we have understood how actor and critic networks work, let's get a
    good understanding of what we have learned so far and how DDPG works exactly by
    putting all the concepts together.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了演员和评论家网络是如何工作的，让我们整理一下到目前为止所学的内容，并通过将所有概念整合起来，来深入理解 DDPG 是如何工作的。
- en: Putting it all together
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'To avoid getting lost in notations, first, let''s recollect the notations to
    understand DDPG better. We use four networks, two actor networks and two critic
    networks:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在符号中迷失，首先，让我们回顾一下符号，以便更好地理解 DDPG。我们使用四个网络，两个演员网络和两个评论家网络：
- en: The main critic network parameter is represented by ![](img/B15558_12_006.png)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主评论家网络参数表示为 ![](img/B15558_12_006.png)
- en: The target critic network parameter is represented by ![](img/B15558_12_025.png)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标评论家网络参数表示为 ![](img/B15558_12_025.png)
- en: The main actor network parameter is represented by ![](img/B15558_11_043.png)
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主演员网络参数表示为 ![](img/B15558_11_043.png)
- en: The target actor network parameter is represented by ![](img/B15558_12_042.png)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标演员网络参数表示为 ![](img/B15558_12_042.png)
- en: Note that DDPG is an actor-critic method, and so its parameters will be updated
    at every step of the episode, unlike the policy gradient method, where we generate
    complete episodes and then update the parameter. Okay, let's get started and understand
    how DDPG works.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，DDPG 是一种演员-评论家方法，因此它的参数将在每个回合的每一步都更新，这与策略梯度方法不同，后者是先生成完整的回合，然后再更新参数。好了，让我们开始，理解
    DDPG 是如何工作的。
- en: First, we initialize the main critic network parameter ![](img/B15558_12_006.png)
    and the main actor network parameter ![](img/B15558_11_043.png) with random values.
    We learned that the target network parameter is just a copy of the main network
    parameter. So, we initialize the target critic network parameter ![](img/B15558_12_025.png)
    by just copying the main critic network parameter ![](img/B15558_12_006.png).
    Similarly, we initialize the target actor network parameter ![](img/B15558_12_042.png)
    by just copying the main actor network parameter ![](img/B15558_11_043.png). We
    also initialize the replay buffer ![](img/B15558_12_072.png).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用随机值初始化主评论家网络参数 ![](img/B15558_12_006.png) 和主演员网络参数 ![](img/B15558_11_043.png)。我们了解到，目标网络参数只是主网络参数的副本。因此，我们通过简单地复制主评论家网络参数
    ![](img/B15558_12_006.png) 来初始化目标评论家网络参数 ![](img/B15558_12_025.png)。类似地，我们通过复制主演员网络参数
    ![](img/B15558_11_043.png) 来初始化目标演员网络参数 ![](img/B15558_12_042.png)。我们还初始化了回放缓冲区
    ![](img/B15558_12_072.png)。
- en: 'Now, for each step in the episode, first, we select an action, *a*, using the
    actor network:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每个回合的每一步，首先，我们使用演员网络选择一个动作 *a*：
- en: '![](img/B15558_12_059.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_059.png)'
- en: 'However, instead of using the action *a* directly, to ensure exploration, we
    add some noise ![](img/B15558_12_060.png), and so the action becomes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了确保探索，我们不是直接使用动作 *a*，而是添加一些噪声 ![](img/B15558_12_060.png)，因此动作变为：
- en: '![](img/B15558_12_061.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_061.png)'
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_016.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_072.png).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行动作 *a*，移动到下一个状态 ![](img/B15558_12_016.png)，并获得奖励 *r*。我们将这个过渡信息存储在回放缓冲区
    ![](img/B15558_12_072.png) 中。
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从回放缓冲区随机采样一个 *K* 过渡（*s*，*a*，*r*，*s'*）的小批量。这些 *K* 个过渡将用于更新评论家和演员网络。
- en: 'First, let us compute the loss of the critic network. We learned that the loss
    function of the critic network is:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算评论家网络的损失。我们了解到，评论家网络的损失函数是：
- en: '![](img/B15558_12_047.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: Where *y*[i] is the target value of the critic, that is, ![](img/B15558_12_095.png),
    and the action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_074.png).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y*[i] 是评论家的目标值，即 ![](img/B15558_12_095.png)，而动作 *a*[i] 是由演员网络生成的动作，即 ![](img/B15558_12_074.png)。
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_050.png)
    and update the critic network parameter ![](img/B15558_12_006.png) using gradient
    descent:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算评论家网络的损失后，我们计算梯度 ![](img/B15558_12_050.png)，并使用梯度下降法更新评论家网络参数 ![](img/B15558_12_006.png)：
- en: '![](img/B15558_12_099.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_099.png)'
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新演员网络。我们了解到，演员网络的目标函数是：
- en: '![](img/B15558_12_100.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_100.png)'
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). Now, we need to maximize the preceding
    objective function. Maximizing the above objective function helps the actor to
    generate actions in such a way that it maximizes the Q value produced by the critic.
    We can maximize the objective function by computing the gradients of our objective
    function ![](img/B15558_10_093.png) and update the actor network parameter ![](img/B15558_11_043.png)
    using gradient ascent:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述方程中，我们仅使用从采样的 *K* 过渡 (*s*, *a*, *r*, *s'*) 中的状态 (*s*[i])。动作 *a* 由演员网络选择，![](img/B15558_12_074.png)。现在，我们需要最大化前面的目标函数。最大化上述目标函数有助于演员以一种方式生成动作，从而最大化评论员生成的
    Q 值。我们可以通过计算目标函数 ![](img/B15558_10_093.png) 的梯度并使用梯度上升更新演员网络参数 ![](img/B15558_11_043.png)
    来最大化目标函数：
- en: '![](img/B15558_12_103.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_103.png)'
- en: 'And then, in the final step, we update the parameter of the target critic network
    ![](img/B15558_12_025.png) and the parameter of the target actor network ![](img/B15558_12_042.png)
    by soft replacement:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在最后一步，我们通过软替换更新目标评论员网络参数 ![](img/B15558_12_025.png) 和目标演员网络参数 ![](img/B15558_12_042.png)：
- en: '![](img/B15558_12_106.png)![](img/B15558_12_107.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_106.png)![](img/B15558_12_107.png)'
- en: We repeat these steps for several episodes. Thus, for each step in the episode,
    we update the parameter of our networks. Since the parameter gets updated at every
    step, our policy will also be improved at every step in the episode.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为多个回合重复这些步骤。因此，对于回合中的每一步，我们都会更新网络的参数。由于参数在每一步都会更新，我们的策略也会在每个回合的每一步得到改进。
- en: To have a better understanding of how DDPG works, let's look into the DDPG algorithm
    in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 DDPG 的工作原理，让我们在下一节中深入探讨 DDPG 算法。
- en: Algorithm – DDPG
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 - DDPG
- en: 'The DDPG algorithm is given as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 算法如下所示：
- en: Initialize the main critic network parameter ![](img/B15558_12_006.png) and
    the main actor network parameter ![](img/B15558_11_043.png)
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主评论员网络参数 ![](img/B15558_12_006.png) 和主演员网络参数 ![](img/B15558_11_043.png)。
- en: Initialize the target critic network parameter ![](img/B15558_12_025.png) by
    just copying the main critic network parameter ![](img/B15558_12_006.png)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接复制主评论员网络参数 ![](img/B15558_12_006.png) 初始化目标评论员网络参数 ![](img/B15558_12_025.png)。
- en: Initialize the target actor network parameter ![](img/B15558_12_042.png) by
    just copying the main actor network parameter ![](img/B15558_12_042.png)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过直接复制主演员网络参数 ![](img/B15558_12_042.png) 初始化目标演员网络参数 ![](img/B15558_12_042.png)。
- en: Initialize the replay buffer ![](img/B15558_12_072.png)
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化重放缓冲区 ![](img/B15558_12_072.png)。
- en: For *N* number of episodes, repeat steps 6 and 7
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 个回合，重复步骤 6 和 7。
- en: Initialize an Ornstein-Uhlenbeck random process ![](img/B15558_12_060.png) for
    an action space exploration
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 Ornstein-Uhlenbeck 随机过程 ![](img/B15558_12_060.png)，用于动作空间的探索。
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个回合的每一步，即 *t* = 0,…,*T* – 1：
- en: Select action *a* based on the policy ![](img/B15558_12_062.png) and exploration
    noise, that is, ![](img/B15558_12_117.png).
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略 ![](img/B15558_12_062.png) 和探索噪声选择动作 *a*，即 ![](img/B15558_12_117.png)。
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_016.png),
    get the reward *r*, and store this transition information in the replay buffer
    ![](img/B15558_12_072.png).
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选定的动作 *a*，移动到下一个状态 ![](img/B15558_12_016.png)，获得奖励 *r*，并将这一过渡信息存储在重放缓冲区 ![](img/B15558_12_072.png)
    中。
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_088.png).
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区 ![](img/B15558_12_088.png) 随机抽取一个 *K* 过渡的小批量。
- en: Compute the target value of the critic, that is, ![](img/B15558_12_095.png).
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员的目标值，即 ![](img/B15558_12_095.png)。
- en: Compute the loss of the critic network, ![](img/B15558_12_047.png).
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论员网络的损失，![](img/B15558_12_047.png)。
- en: Compute the gradient of the loss ![](img/B15558_12_050.png) and update the critic
    network parameter using gradient descent, ![](img/B15558_12_099.png).
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度 ![](img/B15558_12_050.png)，并使用梯度下降更新评论员网络参数，![](img/B15558_12_099.png)。
- en: Compute the gradient of the actor network ![](img/B15558_10_093.png) and update
    the actor network parameter by gradient ascent, ![](img/B15558_12_126.png).
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员网络的梯度 ![](img/B15558_10_093.png)，并通过梯度上升更新演员网络参数，![](img/B15558_12_126.png)。
- en: Update the target critic and target actor network parameter as ![](img/B15558_12_127.png)
    and ![](img/B15558_12_107.png).
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标评论员和目标演员网络参数，分别为 ![](img/B15558_12_127.png) 和 ![](img/B15558_12_107.png)。
- en: Swinging up a pendulum using DDPG
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DDPG摆动一个摆锤
- en: In this section, let's implement the DDPG algorithm to train the agent to swing
    up a pendulum. That is, we will have a pendulum that starts swinging from a random
    position and the goal of our agent is to swing the pendulum up so it stays upright.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们实现DDPG算法来训练智能体摆动一个摆锤。也就是说，我们将有一个从随机位置开始摆动的摆锤，智能体的目标是使摆锤摆动起来并保持直立。
- en: 'First, let''s import the required libraries:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库：
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating the Gym environment
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Gym环境
- en: 'Let''s create a pendulum environment using Gym:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Gym创建一个摆锤环境：
- en: '[PRE1]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Get the state shape of the environment:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的状态形状：
- en: '[PRE2]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the action shape of the environment:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的动作形状：
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the pendulum is a continuous environment, and thus our action space
    consists of continuous values. Hence, we get the bounds of our action space:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，摆锤是一个连续环境，因此我们的动作空间由连续值组成。因此，我们获取动作空间的边界：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the variables
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义变量
- en: Now, let's define some of the important variables.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些重要的变量。
- en: 'Set the discount factor, ![](img/B15558_05_056.png):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 设置折扣因子![](img/B15558_05_056.png)：
- en: '[PRE5]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Set the value of ![](img/B15558_12_056.png), which is used for soft replacement:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 设置![](img/B15558_12_056.png)的值，用于软替代：
- en: '[PRE6]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Set the size of our replay buffer:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 设置我们的重放缓冲区的大小：
- en: '[PRE7]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set the batch size:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 设置批量大小：
- en: '[PRE8]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Defining the DDPG class
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义DDPG类
- en: 'Let''s define the class called `DDPG`, where we will implement the DDPG algorithm.
    To aid understanding, let''s look into the code line by line. You can also access
    the complete code from the GitHub repository of the book:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`DDPG`的类，在其中实现DDPG算法。为了便于理解，让我们逐行查看代码。你也可以通过本书的GitHub仓库访问完整代码：
- en: '[PRE9]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Defining the init method
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义初始化方法
- en: 'First, let''s define the `init` method:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义`init`方法：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the replay buffer for storing the transitions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 定义重放缓冲区，用于存储转移：
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize `num_transitions` to `0`, which means that the number of transitions
    in our replay buffer is zero:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将`num_transitions`初始化为`0`，这意味着我们的重放缓冲区中的转移数为零：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Start the TensorFlow session:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 启动TensorFlow会话：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We learned that in DDPG, instead of selecting the action *a* directly, to ensure
    exploration, we add some noise ![](img/B15558_12_060.png) using the Ornstein-Uhlenbeck
    process. So, we first initialize the noise:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在DDPG中，为了确保探索，而不是直接选择动作*a*，我们通过使用奥恩斯坦-乌伦贝克过程添加了一些噪声![](img/B15558_12_060.png)。因此，我们首先初始化噪声：
- en: '[PRE14]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, initialize the state shape, action shape, and high action value:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，初始化状态形状、动作形状和高动作值：
- en: '[PRE15]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the placeholder for the state:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the placeholder for the next state:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 定义下一个状态的占位符：
- en: '[PRE17]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the placeholder for the reward:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 定义奖励的占位符：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With the actor variable scope:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在演员变量作用域内：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the main actor network, which is parameterized by ![](img/B15558_11_043.png).
    The actor network takes the state as an input and returns the action to be performed
    in that state:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主演员网络，该网络由![](img/B15558_11_043.png)进行参数化。演员网络以状态为输入，并返回在该状态下执行的动作：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the target actor network that is parameterized by ![](img/B15558_12_042.png).
    The target actor network takes the next state as an input and returns the action
    to be performed in that state:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标演员网络，该网络由![](img/B15558_12_042.png)进行参数化。目标演员网络以下一个状态为输入，并返回在该状态下执行的动作：
- en: '[PRE21]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With the critic variable scope:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在评论变量作用域内：
- en: '[PRE22]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the main critic network, which is parameterized by ![](img/B15558_12_006.png).
    The critic network takes the state and also the action produced by the actor in
    that state as an input and returns the Q value:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 定义主评论网络，该网络由![](img/B15558_12_006.png)进行参数化。评论网络以状态和演员在该状态下产生的动作作为输入，并返回Q值：
- en: '[PRE23]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the target critic network, which is parameterized by ![](img/B15558_12_025.png).
    The target critic network takes the next state and also the action produced by
    the target actor network in that next state as an input and returns the Q value:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标评论网络，该网络由![](img/B15558_12_025.png)进行参数化。目标评论网络以下一个状态和目标演员网络在该下一个状态下产生的动作作为输入，并返回Q值：
- en: '[PRE24]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Get the parameter of the main actor network ![](img/B15558_11_043.png):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 获取主演员网络的参数![](img/B15558_11_043.png)：
- en: '[PRE25]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Get the parameter of the target actor network ![](img/B15558_12_042.png):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标演员网络的参数![](img/B15558_12_042.png)：
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the parameter of the main critic network ![](img/B15558_12_006.png):'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 获取主评论网络的参数![](img/B15558_12_006.png)：
- en: '[PRE27]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Get the parameter of the target critic network ![](img/B15558_12_025.png):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 获取目标评论家网络的参数 ![](img/B15558_12_025.png)：
- en: '[PRE28]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Perform the soft replacement, update the parameter of the target actor network
    as ![](img/B15558_12_107.png), and update the parameter of the target critic network
    as ![](img/B15558_12_127.png):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 执行软替换，更新目标演员网络的参数为 ![](img/B15558_12_107.png)，并更新目标评论家网络的参数为 ![](img/B15558_12_127.png)：
- en: '[PRE29]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compute the target Q value. We learned that the target Q value can be computed
    as the sum of reward and the discounted Q value of the next state-action pair,
    ![](img/B15558_12_095.png):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 计算目标 Q 值。我们了解到目标 Q 值可以通过将奖励与下一个状态-动作对的折扣 Q 值相加来计算，![](img/B15558_12_095.png)：
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let''s compute the loss of the critic network. The loss of the critic
    network is the mean squared error between the target Q value and the predicted
    Q value:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算评论家网络的损失。评论家网络的损失是目标 Q 值与预测 Q 值之间的均方误差：
- en: '![](img/B15558_12_047.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_047.png)'
- en: 'So, we can define the mean squared error as:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以定义均方误差为：
- en: '[PRE31]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Train the critic network by minimizing the mean squared error using the Adam
    optimizer:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Adam 优化器最小化均方误差来训练评论家网络：
- en: '[PRE32]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We learned that the objective function of the actor is to generate an action
    that maximizes the Q value produced by the critic network, as shown here:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，演员的目标函数是生成一个动作，使评论家网络产生的 Q 值最大化，如下所示：
- en: '![](img/B15558_12_100.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_100.png)'
- en: 'Where the action ![](img/B15558_12_074.png), and we can maximize this objective
    by computing gradients and by performing gradient ascent. However, it is a standard
    convention to perform minimization rather than maximization. So, we can convert
    the preceding maximization objective into a minimization objective by just adding
    a negative sign. Hence, we can define the actor network objective as:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 动作发生在 ![](img/B15558_12_074.png)，我们可以通过计算梯度并进行梯度上升来最大化这个目标。然而，通常的约定是执行最小化而不是最大化。因此，我们可以通过仅添加一个负号，将前面的最大化目标转换为最小化目标。因此，我们可以定义演员网络目标为：
- en: '![](img/B15558_12_145.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_145.png)'
- en: 'Now, we can minimize the actor network objective by computing gradients and
    by performing gradient descent. Thus, we can write:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过计算梯度并执行梯度下降来最小化演员网络目标。因此，我们可以写出：
- en: '[PRE33]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the actor network by minimizing the loss using the Adam optimizer:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Adam 优化器最小化损失来训练演员网络：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有的 TensorFlow 变量：
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Selecting the action
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择动作
- en: 'Let''s define a function called `select_action` to select the action with the
    noise to ensure exploration:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个叫做`select_action`的函数，通过加入噪声来选择动作，以确保探索：
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Run the actor network and get the action:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 运行演员网络并获取动作：
- en: '[PRE37]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we generate a normal distribution with the mean as the action and the
    standard deviation as the noise and we randomly select an action from this normal
    distribution:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们生成一个均值为动作、标准差为噪声的正态分布，并从该正态分布中随机选择一个动作：
- en: '[PRE38]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We need to make sure that our action should not fall away from the action bound.
    So, we clip the action so that it lies within the action bound and then we return
    the action:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保我们的动作不会超出动作的边界。所以，我们将动作裁剪到动作边界内，然后返回这个动作：
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Defining the train function
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义训练函数
- en: 'Now, let''s define the train function:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义训练函数：
- en: '[PRE40]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Perform the soft replacement:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 执行软替换：
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Randomly select indices from the replay buffer with the given batch size:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 从回放缓冲区中随机选择给定批次大小的索引：
- en: '[PRE42]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Select the batch of transitions from the replay buffer with the selected indices:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从回放缓冲区中选择具有选定索引的状态转移批次：
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Get the batch of states, actions, rewards, and next states:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 获取状态、动作、奖励和下一个状态的批次：
- en: '[PRE44]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Train the actor network:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 训练演员网络：
- en: '[PRE45]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Train the critic network:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 训练评论家网络：
- en: '[PRE46]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Storing the transitions
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储状态转移
- en: 'Now, let''s store the transitions in the replay buffer:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将状态转移存储到回放缓冲区中：
- en: '[PRE47]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'First, stack the state, action, reward, and next state:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将状态、动作、奖励和下一个状态堆叠起来：
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Get the index:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 获取索引：
- en: '[PRE49]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Store the transition:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 存储状态转移：
- en: '[PRE50]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Update the number of transitions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态转移的数量：
- en: '[PRE51]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'If the number of transitions is greater than the replay buffer, train the network:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态转移的数量大于回放缓冲区，则训练网络：
- en: '[PRE52]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Building the actor network
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建演员网络
- en: 'We define a function called `build_actor_network` to build the actor network.
    The actor network takes the state and returns the action to be performed in that
    state:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个叫做`build_actor_network`的函数来构建演员网络。演员网络接收状态并返回在该状态下执行的动作：
- en: '[PRE53]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Building the critic network
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建评论员网络
- en: 'We define a function called `build_critic_network` to build the critic network.
    The critic network takes the state and the action produced by the actor in that
    state and returns the Q value:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`build_critic_network`的函数来构建评论员网络。评论员网络接收状态和由演员在该状态下产生的动作，并返回Q值：
- en: '[PRE54]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Training the network
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Now, let''s start training the network. First, let''s create an object for
    our DDPG class:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。首先，我们创建一个DDPG类的对象：
- en: '[PRE55]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Set the number of episodes:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数：
- en: '[PRE56]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Set the number of time steps in each episode:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 设置每个回合的时间步数：
- en: '[PRE57]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For each episode:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个回合：
- en: '[PRE58]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境初始化状态：
- en: '[PRE59]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Initialize the return:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化返回值：
- en: '[PRE60]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For every step:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步：
- en: '[PRE61]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Render the environment:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE62]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Select the action:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作：
- en: '[PRE63]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Perform the selected action:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选定的动作：
- en: '[PRE64]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Store the transition in the replay buffer:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 将过渡存储在回放缓冲区中：
- en: '[PRE65]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Update the return:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 更新返回值：
- en: '[PRE66]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If the state is the terminal state, then break:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果状态是终止状态，则中断：
- en: '[PRE67]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Update the state to the next state:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态更新到下一个状态：
- en: '[PRE68]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Print the return for every 10 episodes:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 打印每10个回合的返回值：
- en: '[PRE69]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'By rendering the environment, we can observe how the agent learns to swing
    up the pendulum:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 通过渲染环境，我们可以观察到代理是如何学习摆动摆钟的：
- en: '![](img/B15558_12_05.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_05.png)'
- en: 'Figure 12.5: The Gym pendulum environment'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5：Gym摆钟环境
- en: Now that we have learned how DDPG works and how to implement it, in the next
    section, we will learn about another interesting algorithm called twin delayed
    DDPG.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了DDPG是如何工作的以及如何实现它，在下一节中，我们将了解另一个有趣的算法，称为双延迟DDPG。
- en: Twin delayed DDPG
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双延迟DDPG
- en: Now, we will look into another interesting actor-critic algorithm, known as
    TD3\. TD3 is an improvement (and basically a successor) to the DDPG algorithm
    we just covered.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将深入了解另一个有趣的演员-评论员算法，称为TD3。TD3是对我们刚才讨论的DDPG算法的改进（基本上是继任者）。
- en: In the previous section, we learned how DDPG uses a deterministic policy to
    work on the continuous action space. DDPG has several advantages and has been
    successfully used in a variety of continuous action space environments.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了DDPG如何使用确定性策略在连续动作空间上工作。DDPG有几个优点，并且已经成功地应用于各种连续动作空间环境。
- en: We understood that DDPG is an actor-critic method where an actor is a policy
    network and it finds the optimal policy, while the critic evaluates the policy
    produced by the actor by estimating the Q function using a DQN.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解到DDPG是一种演员-评论员方法，其中演员是一个策略网络，负责寻找最优策略，而评论员通过使用DQN估计Q函数来评估演员产生的策略。
- en: One of the problems with DDPG is that the critic overestimates the target Q
    value. This overestimation causes several issues. We learned that the policy is
    improved based on the Q value given by the critic, but when the Q value has an
    approximation error, it causes stability issues to our policy and the policy may
    converge to local optima.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG的一个问题是评论员高估了目标Q值。这种高估导致了几个问题。我们了解到，策略是基于评论员给出的Q值来改进的，但当Q值存在近似误差时，会导致策略的不稳定，且策略可能会收敛到局部最优解。
- en: 'Thus, to combat this, TD3 proposes three important features, which are as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了解决这个问题，TD3提出了三项重要的功能，分别是：
- en: Clipped double Q learning
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪双Q学习
- en: Delayed policy updates
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 延迟策略更新
- en: Target policy smoothing
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标策略平滑
- en: First, we will understand how TD3 works intuitively, and then we will look at
    the algorithm in detail.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将直观地了解TD3是如何工作的，然后再详细查看算法。
- en: Key features of TD3
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD3的关键特点
- en: 'TD3 is essentially the same as DDPG, except that it proposes three important
    features to mitigate the problems in DDPG. In this section, let''s first get a
    basic understanding of the key features of TD3\. The three key features of TD3
    are:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: TD3本质上与DDPG相同，不同之处在于它提出了三项重要功能来缓解DDPG中的问题。在本节中，我们首先了解TD3的关键特点。TD3的三大关键特点是：
- en: '**Clipped double Q learning**:Instead of using one critic network, we use two
    main critic networks to compute the Q value and also use two target critic networks
    to compute the target value.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁剪双Q学习**：我们不再使用一个评论员网络，而是使用两个主要的评论员网络来计算Q值，同时使用两个目标评论员网络来计算目标值。'
- en: We compute two target Q values using two target critic networks and use the
    minimum value of these two while computing the loss. This helps to prevent overestimation
    of the target Q value. We will learn more about this in detail in the next section.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用两个目标Q值来计算两个目标评论网络，并在计算损失时使用这两个中的最小值。这有助于防止目标Q值的高估。我们将在下一节中更详细地学习这一点。
- en: '**Delayed policy updates**: In DDPG, we learned that we update the parameter
    of both the actor (policy network) and critic (DQN) network at every step of the
    episode. Unlike DDPG, here we delay updating the parameter of the actor network.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟策略更新**：在DDPG中，我们了解到我们在每个episode的每一步都更新演员（策略网络）和评论员（DQN）网络的参数。与DDPG不同，在这里我们延迟更新演员网络的参数。'
- en: That is, the critic network parameter is updated at every step of the episode,
    but the actor network (policy network) parameter is delayed and updated only after
    every two steps of the episode.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也就是说，评论员网络的参数在每个episode的每一步都更新，而演员网络（策略网络）的参数则延迟更新，仅在每两步之后更新一次。
- en: '**Target policy smoothing**:TheDDPG method produces different target values
    even for the same action. Hence, the variance of the target value will be high
    even for the same action, so we reduce this variance by adding some noise to the
    target action.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标策略平滑**：DDPG方法即使对于相同的动作也会产生不同的目标值。因此，即使对于相同的动作，目标值的方差也会很高，因此我们通过给目标动作添加一些噪声来减少这种方差。'
- en: Now that we have a basic idea of the key features of TD3, we will get into more
    detail and learn how exactly these three key features work and how they solve
    the problems associated with DDPG.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经对TD3的关键特性有了一个基本的了解，我们将深入探讨这三个关键特性是如何工作的，并了解它们是如何解决与DDPG相关的问题的。
- en: Clipped double Q learning
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪切双重Q学习
- en: 'Remember in *Chapter 9*, *Deep Q Network and Its Variants*, while learning
    about the DQN, we discovered that it tends to overestimate the Q value of the
    next state-action pair in the target? It is shown here:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在*第9章*，*深度Q网络及其变种*中，我们在学习DQN时发现，它倾向于高估目标状态-动作对的Q值吗？如图所示：
- en: '![](img/B15558_12_07.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_07.png)'
- en: 'In order to mitigate the overestimation, we used double Q learning. With double
    Q learning, we use two different networks, in other words, two different Q functions,
    one for selecting an action and the other to compute the Q value, as shown here:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻高估问题，我们使用了双重Q学习。通过双重Q学习，我们使用两个不同的网络，换句话说，两个不同的Q函数，一个用于选择动作，另一个用于计算Q值，如图所示：
- en: '![](img/B15558_12_146.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_146.png)'
- en: Thus, computing the target value by using the preceding equation prevents the
    overestimation of the Q value in the DQN.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过使用前面的公式计算目标值可以防止DQN中Q值的高估。
- en: We learned that in DDPG, the critic network is the DQN, and so it also suffers
    from the overestimation of the Q value in the target. Can we employ double Q learning
    in DDPG and try to solve the overestimation bias? Yes! But the problem is that
    in the actor-critic method, the policy and target network parameter updates happen
    slowly, and this will not help us in removing the overestimation bias.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到在DDPG中，评论员网络就是DQN，因此它也会遭遇目标中的Q值高估问题。那么我们能否在DDPG中应用双重Q学习来尝试解决高估偏差呢？当然可以！但问题是，在演员-评论员方法中，策略和目标网络的参数更新过程较慢，这将不会帮助我们消除高估偏差。
- en: So, we will use a slightly different version of double Q learning called *clipped
    double Q learning*. In clipped double Q learning, we use two target critic networks
    to compute the Q value.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们将使用一种稍微不同的双重Q学习版本，称为*剪切双重Q学习*。在剪切双重Q学习中，我们使用两个目标评论网络来计算Q值。
- en: We use the two target critic networks and compute the two Q values and select
    the minimum value out of these two to compute the target value. This helps to
    prevent overestimation bias. Let's understand this in more detail.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个目标评论网络并计算两个Q值，从这两个Q值中选择最小值来计算目标值。这有助于防止高估偏差。让我们更详细地理解这一点。
- en: If we need two target critic networks, then we also need two main critic networks.
    We know that the target network parameter is just a time-delayed copy of the main
    network parameter. So, we define two main critic networks with the parameters
    ![](img/B15558_12_147.png) and ![](img/B15558_12_148.png) to compute the two Q
    values, that is, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png), respectively.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要两个目标评论网络，那么我们也需要两个主评论网络。我们知道，目标网络参数只是主网络参数的延迟副本。因此，我们定义了两个主评论网络，使用参数 ![](img/B15558_12_147.png)
    和 ![](img/B15558_12_148.png) 来计算两个Q值，即 ![](img/B15558_12_149.png) 和 ![](img/B15558_12_150.png)，分别。
- en: We also define the two target critic networks with parameters ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png) to compute the two Q values of next state-action
    pair in the target, that is, ![](img/B15558_12_153.png) and ![](img/B15558_12_154.png),
    respectively. Let's understand this clearly step by step.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了两个目标评论网络，使用参数 ![](img/B15558_12_151.png) 和 ![](img/B15558_12_152.png)
    来计算目标中下一个状态-动作对的两个Q值，即 ![](img/B15558_12_153.png) 和 ![](img/B15558_12_154.png)，分别。让我们一步一步清楚地理解这一点。
- en: 'In DDPG, we learned that we compute the target value as:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDPG中，我们了解到目标值的计算方式是：
- en: '![](img/B15558_12_155.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_155.png)'
- en: 'Computing the Q value of the next state-action pair in the target in this way
    creates overestimation bias:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式计算目标中下一个状态-动作对的Q值会产生高估偏差：
- en: '![](img/B15558_12_08.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_08.png)'
- en: 'So, to avoid this, in TD3, first, we compute the Q value of the next state-action
    pair in the target using the first target critic network with a parameter ![](img/B15558_12_151.png),
    that is, ![](img/B15558_12_153.png), and then we compute the Q value of the next
    state-action pair in the target using the second target critic network with a
    parameter ![](img/B15558_12_152.png), that is, ![](img/B15558_12_154.png). Then,
    we use the minimum of these two Q values to compute the target value as expressed
    here:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免这种情况，在TD3中，首先，我们使用第一个目标评论网络计算下一个状态-动作对的Q值，使用的参数为 ![](img/B15558_12_151.png)，即
    ![](img/B15558_12_153.png)，然后我们使用第二个目标评论网络计算下一个状态-动作对的Q值，使用的参数为 ![](img/B15558_12_152.png)，即
    ![](img/B15558_12_154.png)。然后，我们使用这两个Q值的最小值来计算目标值，如下所示：
- en: '![](img/B15558_12_160.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_160.png)'
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_161.png)。
- en: 'We can express the preceding equation simply as:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地表示前面的方程为：
- en: '![](img/B15558_12_162.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_162.png)'
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 其中动作为 ![](img/B15558_12_161.png)。
- en: Computing the target value in this way prevents overestimation of the Q value
    of the next state-action pair.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式计算目标值可以防止下一个状态-动作对的Q值被高估。
- en: 'Okay, we computed the target value. How do we compute the loss and update the
    critic network parameter? We learned that we use two main critic networks, so,
    first, we compute the loss of the first main critic network, parameterized by
    ![](img/B15558_12_147.png):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们计算了目标值。我们如何计算损失并更新评论网络参数？我们了解到我们使用两个主评论网络，因此，首先，我们计算第一个主评论网络的损失，使用参数 ![](img/B15558_12_147.png)：
- en: '![](img/B15558_12_165.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_165.png)'
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_147.png)using gradient descent as ![](img/B15558_12_167.png).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失后，我们计算梯度，并使用梯度下降法以 ![](img/B15558_12_167.png) 更新参数 ![](img/B15558_12_147.png)。
- en: 'Next, we compute the loss of the second main critic network, parameterized
    by ![](img/B15558_12_148.png):'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算第二个主评论网络的损失，使用的参数为 ![](img/B15558_12_148.png)：
- en: '![](img/B15558_12_169.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_169.png)'
- en: After computing the loss, we compute the gradients and update the parameter
    ![](img/B15558_12_148.png)using gradient descent as ![](img/B15558_12_171.png).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失后，我们计算梯度，并使用梯度下降法以 ![](img/B15558_12_171.png)更新参数 ![](img/B15558_12_148.png)。
- en: 'We can simply express the preceding updates as:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地表示前面的更新为：
- en: '![](img/B15558_12_172.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_172.png)'
- en: 'After updating the two main critic network parameters, ![](img/B15558_12_147.png)and![](img/B15558_12_148.png),we
    can update the two target critic network parameters, ![](img/B15558_12_151.png)
    and ![](img/B15558_12_152.png), by soft replacement, as shown here:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新了两个主评论网络参数后，![](img/B15558_12_147.png)和![](img/B15558_12_148.png)，我们可以通过软替换来更新两个目标评论网络的参数，![](img/B15558_12_151.png)
    和 ![](img/B15558_12_152.png)，如图所示：
- en: '![](img/B15558_12_177.png)![](img/B15558_12_178.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_177.png)![](img/B15558_12_178.png)'
- en: 'We can simply express the preceding updates as:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地表示前面的更新为：
- en: '![](img/B15558_12_179.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_179.png)'
- en: Delayed policy updates
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟策略更新
- en: Delayed policy updates imply that we update the parameters of our actor network
    (policy network) less frequently than the critic networks. But why do we want
    to do that? We learned that in DDPG, actor and critic network parameters are updated
    at every step of the episode.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟策略更新意味着我们更新演员网络（策略网络）的参数频率低于评论员网络的参数更新频率。但为什么我们要这样做呢？我们了解到，在DDPG中，演员和评论员网络的参数在每个回合的每一步都会更新。
- en: When the critic network parameter is not good, then it estimates the incorrect
    Q values. If the Q value estimated by the critic network is not correct, then
    the actor network cannot update its parameter correctly. That is, we learned that
    the actor network learns based on feedback from the critic network. This feedback
    is just the Q value. When the critic network gives incorrect feedback (incorrect
    Q value), then the actor network cannot learn the correct action and cannot update
    its parameter correctly.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 当评论员网络的参数不好时，它估计的Q值就是错误的。如果评论员网络估计的Q值不正确，那么演员网络就无法正确地更新它的参数。也就是说，我们了解到，演员网络是根据评论员网络的反馈来学习的。这个反馈就是Q值。当评论员网络提供错误的反馈（错误的Q值）时，演员网络就无法学习到正确的动作，也无法正确更新它的参数。
- en: Thus, to avoid this, we hold updating the parameter of the actor network for
    a while and only update the critic network to make the critic estimate the correct
    Q value. That is, we update the parameter of the critic network at every step
    of the episode, and we delay updating the parameter of the actor network, and
    only update it for some specific steps of the episode because we don't want our
    actor to learn from the incorrect critic's feedback.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了避免这种情况，我们暂时不更新演员网络的参数，而是只更新评论员网络，以使评论员估计正确的Q值。也就是说，我们在每个回合的每一步都更新评论员网络的参数，而延迟更新演员网络的参数，仅在回合中的某些特定步骤进行更新，因为我们不希望演员从错误的评论员反馈中学习。
- en: In a nutshell, the critic network parameter is updated at every step of the
    episode, but the actor network parameter update is delayed. We generally delay
    the update by two steps.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，评论员网络的参数在每个回合的每一步都会更新，但演员网络的参数更新是延迟的。我们通常将更新延迟两步。
- en: 'Okay, in DDPG, we learned that the objective of the actor network (policy network)
    is to maximize the Q value:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在DDPG中，我们学到演员网络（策略网络）的目标是最大化Q值：
- en: '![](img/B15558_12_100.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_100.png)'
- en: The preceding objective of the actor network is the same in TD3 as well. That
    is, similar to DDPG, here, the objective of the actor is to generate actions in
    such a way that it maximizes the Q value produced by the critic. But wait! Unlike
    DDPG, here we have two Q values, ![](img/B15558_12_149.png) and ![](img/B15558_12_150.png),
    since we use two critic networks with parameters ![](img/B15558_12_147.png) and
    ![](img/B15558_12_148.png), respectively. So which Q value should our actor network
    maximize? Should it be ![](img/B15558_12_149.png) or ![](img/B15558_12_150.png)?
    We can take either of these and maximize one. So, we can take ![](img/B15558_12_149.png).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络的前述目标在TD3中也是相同的。也就是说，类似于DDPG，这里的演员目标是生成动作，使其最大化由评论员生成的Q值。但等等！与DDPG不同的是，这里我们有两个Q值，![](img/B15558_12_149.png)和![](img/B15558_12_150.png)，因为我们使用了两个评论员网络，参数分别为![](img/B15558_12_147.png)和![](img/B15558_12_148.png)。那么，我们的演员网络应该最大化哪个Q值呢？是![](img/B15558_12_149.png)还是![](img/B15558_12_150.png)？我们可以选择其中一个进行最大化。所以，我们可以选择![](img/B15558_12_149.png)。
- en: 'Thus, in TD3, the objective of the actor network is to maximize the Q value,
    ![](img/B15558_12_149.png), as shown here:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在TD3中，演员网络的目标是最大化Q值，![](img/B15558_12_149.png)，如下所示：
- en: '![](img/B15558_12_189.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_189.png)'
- en: 'Remember that in the above equation, the action *a* is selected by actor network,
    ![](img/B15558_12_074.png). In order to maximize the objective function, we compute
    the gradients of our objective function, ![](img/B15558_10_093.png), and update
    the parameter of the network using gradient ascent:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在上面的方程中，动作*a*是由演员网络选择的，![](img/B15558_12_074.png)。为了最大化目标函数，我们计算目标函数的梯度，![](img/B15558_10_093.png)，并使用梯度上升法更新网络的参数：
- en: '![](img/B15558_12_103.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_103.png)'
- en: 'Now, instead of doing this parameter update of the actor network at every time
    step of the episode, we delay the updates and update the parameter only on every
    other step (every two steps). Let *t* be the time step of the episode and *d*
    denotes the number of time steps we want to delay the update by (usually *d* is
    set to 2); then we can write the following:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再在每个时间步更新演员网络的参数，而是延迟更新，只在每隔一个步骤（即每两个步骤）更新一次参数。设 *t* 为该回合的时间步，*d* 为我们希望延迟更新的时间步数（通常
    *d* 设置为2）；那么我们可以写出如下公式：
- en: 'If *t* mod *d* =0, then:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0，则：
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_10_093.png)
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升法更新演员网络参数 ![](img/B15558_12_103.png)
- en: This will be made clearer when we look at the final algorithm.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看最终算法时，这一点会更加清晰。
- en: Target policy smoothing
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标策略平滑
- en: 'To understand this, let''s first recollect how we compute the target value
    in TD3\. We learned that in TD3, we update the target value using clipped double
    Q learning with two target critic networks:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，让我们先回顾一下在TD3中如何计算目标值。我们了解到，在TD3中，我们使用剪切双Q学习和两个目标评估网络来更新目标值：
- en: '![](img/B15558_12_162.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_162.png)'
- en: Where the action ![](img/B15558_12_161.png).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，动作为 ![](img/B15558_12_161.png)。
- en: 'As we can notice, we compute the target values with action ![](img/B15558_12_031.png)
    generated by the target actor network, ![](img/B15558_12_199.png). Instead of
    using the action given by the target actor network directly, we add some noise
    ![](img/B15558_12_200.png) to the action and modify the action to ![](img/B15558_12_201.png),
    as shown here:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所注意到的，我们使用目标演员网络生成的动作 ![](img/B15558_12_031.png) 来计算目标值，![](img/B15558_12_199.png)。我们并不直接使用目标演员网络给出的动作，而是向该动作添加一些噪声
    ![](img/B15558_12_200.png)，然后将动作修改为 ![](img/B15558_12_201.png)，如下所示：
- en: '![](img/B15558_12_202.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_202.png)'
- en: 'Here, −*c* to +*c* indicates that noise is clipped, so that we can keep the
    target close to the actual action. Thus, our target value computation now becomes:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，−*c* 到 +*c* 表示噪声被剪切，从而使目标保持接近实际的动作。因此，我们现在的目标值计算变为：
- en: '![](img/B15558_12_203.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_203.png)'
- en: In the preceding equation, the action ![](img/B15558_12_204.png).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的公式中，动作为 ![](img/B15558_12_204.png)。
- en: But why are we doing this? Why do we need to add noise to the action and use
    it to compute the target value? Similar actions should have similar target values,
    right? However, the DDPG method produces target values with high variance even
    for similar actions. This is because deterministic policies overfit to the sharp
    peaks in the value estimate. So, we can smooth out these peaks for similar actions
    by adding some noise. Thus, target policy smoothing basically acts as a regularizer
    and reduces the variance in the target values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们为什么要这样做呢？为什么需要给动作添加噪声，并用它来计算目标值呢？相似的动作应该具有相似的目标值，对吧？然而，DDPG方法即使对于相似的动作，也会产生高方差的目标值。这是因为确定性策略会对价值估计中的尖锐峰值产生过拟合。因此，我们可以通过添加一些噪声来平滑这些相似动作的峰值。如此一来，目标策略平滑基本上充当了一个正则化器，并减少了目标值的方差。
- en: Now that we have understood the key features of the TD3 algorithm, let's get
    clarity on what we have learned so far and how the TD3 algorithm works by putting
    all the concepts together.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了TD3算法的关键特性，让我们通过将所有概念结合起来，进一步澄清到目前为止学到的内容以及TD3算法是如何工作的。
- en: Putting it all together
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将这些内容整合起来
- en: 'First, let''s recollect the notations to understand TD3 better. We use six
    networks—four critic networks and two actor networks:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下符号，以更好地理解TD3。我们使用六个网络——四个评估网络和两个演员网络：
- en: The two main critic network parameters are represented by ![](img/B15558_12_205.png)
    and ![](img/B15558_12_206.png)
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个主要的评估网络参数用 ![](img/B15558_12_205.png) 和 ![](img/B15558_12_206.png) 表示。
- en: The two target critic network parameters are represented by ![](img/B15558_12_207.png)
    and ![](img/B15558_12_208.png)
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个目标评估网络参数用 ![](img/B15558_12_207.png) 和 ![](img/B15558_12_208.png) 表示。
- en: The main actor network parameter is represented by ![](img/B15558_12_209.png)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要演员网络参数用 ![](img/B15558_12_209.png) 表示。
- en: The target actor network parameter is represented by ![](img/B15558_12_210.png)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标演员网络参数用 ![](img/B15558_12_210.png) 表示。
- en: TD3 is an actor-critic method, and so the parameters of TD3 will get updated
    at every step of the episode, unlike the policy gradient method where we generate
    complete episodes and then update the parameter. Now, let's get started and understand
    how TD3 works.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: TD3 是一种演员-评论者方法，因此 TD3 的参数将在每一轮的每一步进行更新，这与策略梯度方法不同，后者需要生成完整的回合，然后再更新参数。现在，让我们开始并了解
    TD3 是如何工作的。
- en: First, we initialize the two main critic network parameters, ![](img/B15558_12_211.png)
    and ![](img/B15558_12_148.png), and the main actor network parameter ![](img/B15558_12_213.png)
    with random values. We know that the target network parameter is just a copy of
    the main network parameter. So, we initialize the two target critic network parameters
    ![](img/B15558_12_214.png) and ![](img/B15558_12_208.png) by just copying ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), respectively. Similarly, we initialize the target
    actor network parameter ![](img/B15558_12_218.png) by just copying the main actor
    network parameter ![](img/B15558_12_219.png). We also initialize the replay buffer
    ![](img/B15558_12_220.png).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化两个主要评论者网络参数，![](img/B15558_12_211.png) 和 ![](img/B15558_12_148.png)，以及主要演员网络参数
    ![](img/B15558_12_213.png)，并赋予随机值。我们知道目标网络参数只是主网络参数的副本。所以，我们通过复制 ![](img/B15558_12_216.png)
    和 ![](img/B15558_12_217.png) 来初始化两个目标评论者网络参数 ![](img/B15558_12_214.png) 和 ![](img/B15558_12_208.png)，同样，我们通过复制主要演员网络参数
    ![](img/B15558_12_219.png) 来初始化目标演员网络参数 ![](img/B15558_12_218.png)。我们还初始化回放缓冲区
    ![](img/B15558_12_220.png)。
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在每一轮中，首先，我们使用演员网络选择一个动作*a*：
- en: '![](img/B15558_12_059.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_059.png)'
- en: 'But instead of using the action *a* directly, to ensure exploration, we add
    some noise ![](img/B15558_12_222.png), where ![](img/B15558_12_223.png). Thus,
    our action now becomes:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为了确保探索，我们不直接使用动作*a*，而是添加一些噪声 ![](img/B15558_12_222.png)，其中 ![](img/B15558_12_223.png)。因此，我们的动作现在变为：
- en: '![](img/B15558_12_224.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_224.png)'
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_02_004.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_12_226.png).
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行动作*a*，移动到下一个状态 ![](img/B15558_02_004.png)，并获得奖励*r*。我们将这个转移信息存储在回放缓冲区中
    ![](img/B15558_12_226.png)。
- en: Next, we randomly sample a minibatch of *K* transitions (*s*, *a*, *r*, *s'*)
    from the replay buffer. These *K* transitions will be used for updating both our
    critic and actor network.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从回放缓冲区随机抽取一个*K*个转移(*s*，*a*，*r*，*s'*)的小批量。这些*K*个转移将用于更新评论者和演员网络。
- en: 'First, let us compute the loss of the critic networks. We learned that the
    loss function of the critic networks is:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来计算评论者网络的损失。我们已经知道评论者网络的损失函数是：
- en: '![](img/B15558_12_227.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_227.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，以下内容适用：
- en: The action *a*[i] is the action produced by the actor network, that is, ![](img/B15558_12_228.png)
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作*a*[i] 是由演员网络产生的动作，即 ![](img/B15558_12_228.png)
- en: '*y*[i] is the target value of the critic, that is, ![](img/B15558_12_230.png),
    and the action ![](img/B15558_12_231.png) is the action produced by the target
    actor network, that is, ![](img/B15558_12_232.png) where ![](img/B15558_12_233.png)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*[i] 是评论者的目标值，即 ![](img/B15558_12_230.png)，而动作 ![](img/B15558_12_231.png)
    是目标演员网络产生的动作，即 ![](img/B15558_12_232.png)，其中 ![](img/B15558_12_233.png)'
- en: 'After computing the loss of the critic network, we compute the gradients ![](img/B15558_12_234.png)
    and update the critic network parameter using gradient descent:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完评论者网络的损失后，我们计算梯度 ![](img/B15558_12_234.png) 并使用梯度下降法更新评论者网络的参数：
- en: '![](img/B15558_12_235.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_235.png)'
- en: 'Now, let us update the actor network. We learned that the objective function
    of the actor network is:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新演员网络。我们已经知道，演员网络的目标函数是：
- en: '![](img/B15558_12_189.png)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_189.png)'
- en: 'Note that in the above equation, we are only using the state (*s*[i]) from
    the sampled *K* transitions (*s*, *a*, *r*, *s''*). The action *a* is selected
    by actor network, ![](img/B15558_12_074.png). In order to maximize the objective
    function, we compute gradients of our objective function ![](img/B15558_10_093.png)
    and update the parameters of the network using gradient ascent:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述方程中，我们仅使用来自采样的*K*个转移(*s*，*a*，*r*，*s'*)的状态 (*s*[i])。动作*a* 是由演员网络选择的，![](img/B15558_12_074.png)。为了最大化目标函数，我们计算目标函数
    ![](img/B15558_10_093.png) 的梯度，并使用梯度上升法更新网络的参数：
- en: '![](img/B15558_12_103.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_103.png)'
- en: 'Instead of doing this parameter update of the actor network at every time step
    of the episode, we delay the updates. Let *t* be the time step of the episode
    and *d* denotes the number of time steps we want to delay the update by (usually
    *d* is set to 2); then we can write the following:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是在每个回合的每个时间步都更新演员网络的参数，而是延迟更新。让 *t* 表示回合的时间步，*d* 表示我们希望延迟更新的时间步数（通常 *d* 设置为
    2）；那么我们可以写出如下公式：
- en: 'If *t* mod *d* = 0, then:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0，则：
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png)
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_10_093.png)
- en: Update the actor network parameter using gradient ascent ![](img/B15558_12_103.png)
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升法更新演员网络参数 ![](img/B15558_12_103.png)。
- en: 'Finally, we update the parameter of the target critic networks ![](img/B15558_12_214.png)
    and ![](img/B15558_12_208.png) and the parameter of the target actor network ![](img/B15558_12_243.png)
    by soft replacement:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过软替换更新目标评论员网络的参数 ![](img/B15558_12_214.png) 和 ![](img/B15558_12_208.png)，以及目标演员网络的参数
    ![](img/B15558_12_243.png)：
- en: '![](img/B15558_12_244.png)![](img/B15558_12_245.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_244.png)![](img/B15558_12_245.png)'
- en: 'There is a small change in updating the parameter of the target networks. Just
    like we delay updating the actor network parameter for *d* steps, we update the
    target network parameter for every *d* step; hence, we can write:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 更新目标网络参数时有一个小的变化。就像我们延迟更新演员网络参数 *d* 步骤一样，我们每 *d* 步骤更新目标网络参数；因此，我们可以写出如下公式：
- en: 'If *t* mod *d* = 0, then:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0，则：
- en: Compute the gradient of the objective function ![](img/B15558_10_093.png) and
    update the actor network parameter using gradient ascent ![](img/B15558_12_247.png)
  id: totrans-437
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_10_093.png)，并使用梯度上升法更新演员网络的参数 ![](img/B15558_12_247.png)。
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标评论员网络参数和目标演员网络参数，分别为 ![](img/B15558_12_248.png) 和 ![](img/B15558_12_107.png)。
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how TD3 works, let's look into the TD3 algorithm
    in the next section.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复上述步骤若干回合，并改进策略。为了更好地理解 TD3 如何工作，让我们在下一节深入研究 TD3 算法。
- en: Algorithm – TD3
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – TD3
- en: The TD3 algorithm is exactly similar to the DDPG algorithm except that it includes
    the three key features we learned in the previous sections. So, before looking
    into the TD3 algorithm directly, you can revise all the key features of TD3.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: TD3 算法与 DDPG 算法完全相似，除了它包含了我们在前面部分学习的三个关键特性。所以，在直接查看 TD3 算法之前，您可以复习一下 TD3 的所有关键特性。
- en: 'The algorithm of TD3 is given as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: TD3 算法如下所示：
- en: Initialize the two main critic network parameters ![](img/B15558_12_211.png)
    and ![](img/B15558_12_217.png) and the main actor network parameter ![](img/B15558_12_252.png)
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化两个主评论员网络参数 ![](img/B15558_12_211.png) 和 ![](img/B15558_12_217.png)，以及主演员网络的参数
    ![](img/B15558_12_252.png)。
- en: Initialize the two target critic network parameters ![](img/B15558_12_214.png)
    and ![](img/B15558_12_152.png) by copying the main critic network parameters ![](img/B15558_12_255.png)
    and ![](img/B15558_12_148.png), respectively
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主评论员网络参数 ![](img/B15558_12_255.png) 和 ![](img/B15558_12_148.png)，初始化两个目标评论员网络参数
    ![](img/B15558_12_214.png) 和 ![](img/B15558_12_152.png)。
- en: Initialize the target actor network parameter ![](img/B15558_12_218.png) by
    copying the main actor network parameter ![](img/B15558_12_218.png)
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主演员网络参数 ![](img/B15558_12_218.png)，初始化目标演员网络的参数 ![](img/B15558_12_218.png)。
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化回放缓冲区 ![](img/B15558_12_259.png)
- en: For *N* number of episodes, repeat step 6
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 个回合，重复第 6 步。
- en: 'For each step in the episode, that is, for *t* = 0,…,*T* – 1:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于回合中的每个步骤，即 *t* = 0,…,*T* – 1：
- en: Select the action *a* based on the policy ![](img/B15558_12_062.png) and with
    exploration noise ![](img/B15558_12_261.png), that is, ![](img/B15558_12_224.png)
    where, ![](img/B15558_12_263.png)
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于策略 ![](img/B15558_12_062.png) 和探索噪声 ![](img/B15558_12_261.png) 选择动作 *a*，即
    ![](img/B15558_12_224.png)，其中，![](img/B15558_12_263.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_264.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_088.png)
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选定的动作 *a*，移动到下一个状态 ![](img/B15558_12_264.png)，获得奖励 *r*，并将转换信息存储到回放缓冲区 ![](img/B15558_09_088.png)。
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_12_266.png)
  id: totrans-451
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从回放缓冲区 ![](img/B15558_12_266.png) 随机抽取一个 *K* 的小批量转换。
- en: Select the action ![](img/B15558_12_267.png) to compute the target value, ![](img/B15558_12_268.png),
    where ![](img/B15558_12_269.png)
  id: totrans-452
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作 ![](img/B15558_12_267.png) 来计算目标值，![](img/B15558_12_268.png)，其中 ![](img/B15558_12_269.png)
- en: Compute the target value of the critic, that is, ![](img/B15558_12_230.png)
  id: totrans-453
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论家的目标值，即 ![](img/B15558_12_230.png)
- en: Compute the loss of the critic network, ![](img/B15558_12_227.png)
  id: totrans-454
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论家网络的损失，![](img/B15558_12_227.png)
- en: Compute the gradients of the loss ![](img/B15558_12_234.png) and minimize the
    loss using gradient descent, ![](img/B15558_12_273.png)
  id: totrans-455
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失的梯度 ![](img/B15558_12_234.png)，并使用梯度下降法最小化损失，![](img/B15558_12_273.png)
- en: 'If *t* mod *d* =0, then:'
  id: totrans-456
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* =0，则：
- en: Compute the gradient of the objective function ![](img/B15558_12_274.png) and
    update the actor network parameter using gradient ascent, ![](img/B15558_12_126.png)
  id: totrans-457
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_12_274.png)，并使用梯度上升法更新演员网络参数，![](img/B15558_12_126.png)
- en: Update the target critic network parameter and target actor network parameter
    as ![](img/B15558_12_248.png), and ![](img/B15558_12_107.png), respectively
  id: totrans-458
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标评论家网络参数和目标演员网络参数，分别为 ![](img/B15558_12_248.png) 和 ![](img/B15558_12_107.png)
- en: Now that we have learned how TD3 works, in the next section, we will learn about
    another interesting algorithm, called SAC.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 TD3 的工作原理，在接下来的章节中，我们将学习另一个有趣的算法，叫做 SAC。
- en: Soft actor-critic
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Soft Actor-Critic
- en: Now, we will look into another interesting actor-critic algorithm, called SAC.
    This is an off-policy algorithm and it borrows several features from the TD3 algorithm.
    But unlike TD3, it uses a stochastic policy ![](img/B15558_03_139.png). SAC is
    based on the concept of entropy. So first, let's understand what is meant by entropy.
    Entropy is a measure of the randomness of a variable. It basically tells us the
    uncertainty or unpredictability of the random variable and is denoted by ![](img/B15558_12_279.png).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将研究另一个有趣的演员-评论家算法，叫做 SAC。这是一个离策略算法，它借用了 TD3 算法的几个特性。但与 TD3 不同的是，它使用了一个随机策略
    ![](img/B15558_03_139.png)。SAC 基于熵的概念。那么首先，让我们理解一下什么是熵。熵是衡量变量随机性的一个指标。它基本上告诉我们随机变量的不确定性或不可预测性，表示为
    ![](img/B15558_12_279.png)。
- en: If the random variable always gives the same value every time, then we can say
    that its entropy is low because there is no randomness. But if the random variable
    gives different values, then we can say that its entropy is high.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机变量每次都给出相同的值，那么我们可以说它的熵是低的，因为没有随机性。但是如果随机变量给出不同的值，那么我们可以说它的熵是高的。
- en: For an example, consider a dice throw experiment. Every time a dice is thrown,
    if we get a different number, then we can say that the entropy is high because
    we are getting a different number every time and there is high uncertainty since
    we don't know which number will come up on the next throw. But if we are getting
    the same number, say 3, every time the dice is thrown, then we can say that the
    entropy is low, since there is no randomness here as we are getting the same number
    on every throw.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个掷骰子的实验。每次掷骰子时，如果我们得到一个不同的数字，那么我们可以说熵是高的，因为每次我们得到的数字都不同，且有较大的不确定性，因为我们不知道下一次掷骰子会出现哪个数字。但如果每次掷骰子得到的数字都是相同的，例如
    3，那么我们可以说熵是低的，因为这里没有随机性，我们每次掷骰子都得到相同的数字。
- en: We know that the policy ![](img/B15558_03_139.png) tells what action to perform
    in a given state. What happens when the entropy of the policy ![](img/B15558_12_281.png)
    is high or low? If the entropy of the policy is high, then this means that our
    policy performs different actions instead of performing the same action every
    time. But if the entropy of the policy is low, then this means that our policy
    performs the same action every time. As you may have guessed, increasing the entropy
    of a policy promotes exploration, while decreasing the entropy of the policy means
    less exploration.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道策略 ![](img/B15558_03_139.png) 指定了在给定状态下执行的动作。当策略的熵 ![](img/B15558_12_281.png)
    高或低时会发生什么？如果策略的熵高，意味着我们的策略会执行不同的动作，而不是每次都执行相同的动作。但如果策略的熵低，那么这意味着我们的策略每次都执行相同的动作。正如你可能已经猜到的，增加策略的熵有助于探索，而减少策略的熵意味着减少探索。
- en: 'We know that, in reinforcement learning, our goal is to maximize the return.
    So, we can define our objective function as shown here:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在强化学习中，我们的目标是最大化回报。因此，我们可以定义我们的目标函数，如下所示：
- en: '![](img/B15558_12_282.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_282.png)'
- en: Where ![](img/B15558_12_283.png) is the parameter of our stochastic policy ![](img/B15558_03_139.png).
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_12_283.png) 是我们随机策略 ![](img/B15558_03_139.png) 的参数。
- en: 'We know that the return of the trajectory is just the sum of rewards, that
    is:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，轨迹的回报就是奖励的总和，即：
- en: '![](img/B15558_10_115.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_115.png)'
- en: 'So, we can rewrite our objective function by expanding the return as:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以通过展开回报来重写我们的目标函数：
- en: '![](img/B15558_12_286.png)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_286.png)'
- en: 'Maximizing the preceding objective function maximizes the return. In the SAC
    method, we use a slightly modified version of the objective function with the
    entropy term as shown here:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化前述目标函数即是最大化回报。在SAC方法中，我们使用稍微修改过的带有熵项的目标函数，如下所示：
- en: '![](img/B15558_12_287.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_287.png)'
- en: As we can see, our objective function now has two terms; one is the reward and
    the other is the entropy of the policy. Thus, instead of maximizing only the reward,
    we also maximize the entropy of a policy. But what is the point of this? Maximizing
    the entropy of the policy allows us to explore new actions. But we don't want
    to explore actions that give us a bad reward. Hence, maximizing entropy along
    with maximizing reward means that we can explore new actions along with maintaining
    maximum reward. The preceding objective function is often referred to as **maximum
    entropy reinforcement learning**, or **entropy regularized reinforcement learning.**
    Adding an entropy term is also often referred to as an entropy bonus.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的目标函数现在有两个项，一个是奖励，另一个是策略的熵。因此，我们不仅仅最大化奖励，还最大化策略的熵。那么这么做的意义是什么呢？最大化策略的熵使我们能够探索新的行动。但我们不希望探索那些给我们带来不良奖励的行动。因此，最大化熵和奖励的结合意味着我们可以在保持最大奖励的同时，探索新的行动。前述目标函数通常被称为**最大熵强化学习**，或**熵正则化强化学习**。增加熵项也常常被称为熵奖励。
- en: Also, the term ![](img/B15558_09_143.png) in the objective function is called
    temperature and is used to set the importance of our entropy term, or we can say
    that it is used to control exploration. When ![](img/B15558_09_143.png) is high,
    we allow exploration in the policy, but when it is low, then we don't allow exploration.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，目标函数中的项![](img/B15558_09_143.png)叫做温度，用来设置熵项的重要性，或者我们可以说它用于控制探索。当![](img/B15558_09_143.png)较高时，我们允许策略中的探索，但当它较低时，则不允许探索。
- en: Okay, now that we have a basic idea of SAC, let's get into some more details.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们对SAC有了一个基本了解，接下来我们将深入一些细节。
- en: Understanding soft actor-critic
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解软演员-评论家方法
- en: SAC, as the name suggests, is an actor-critic method similar to DDPG and TD3
    we learned in the previous sections. Unlike DDPG and TD3, which use deterministic
    policies, SAC uses a stochastic policy. SAC works in a very similar manner to
    TD3\. We learned that in actor-critic architecture, the actor uses the policy
    gradient to find the optimal policy and the critic evaluates the policy produced
    by the actor using the Q function.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: SAC，顾名思义，是一种类似于我们在前面章节中学习的DDPG和TD3的演员-评论家方法。与使用确定性策略的DDPG和TD3不同，SAC使用的是随机策略。SAC的工作方式与TD3非常相似。我们学到，在演员-评论家架构中，演员使用策略梯度来寻找最优策略，评论家则使用Q函数来评估演员产生的策略。
- en: Similarly, in SAC, the actor uses the policy gradient to find the optimal policy
    and the critic evaluates the policy produced by the actor. However, instead of
    using only the Q function to evaluate the actor's policy, the critic uses both
    the Q function and the value function. But why exactly do we need both the Q function
    and the value function to evaluate the actor's policy? This will be explained
    in detail in the upcoming sections.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，在SAC中，演员使用策略梯度来找到最优策略，评论家则评估演员产生的策略。然而，评论家不仅使用Q函数来评估演员的策略，还同时使用Q函数和价值函数。那么，为什么我们需要Q函数和价值函数来共同评估演员的策略呢？这一点将在接下来的章节中详细解释。
- en: So, in SAC, we have three networks, one actor network (policy network) to find
    the optimal policy, and two critic networks—a value network and a Q network, to
    compute the value function and the Q function, respectively, to evaluate the policy
    produced by the actor.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在SAC中，我们有三个网络，一个是演员网络（策略网络）用来寻找最优策略，另外两个是评论家网络——价值网络和Q网络，分别用来计算价值函数和Q函数，以评估演员产生的策略。
- en: Before moving on, let's look at the modified version of the value function and
    the Q function with the entropy term.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们看看带有熵项的价值函数和Q函数的修改版本。
- en: V and Q functions with the entropy term
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有熵项的V和Q函数
- en: 'We know that the value function (state value) is the expected return of the
    trajectory starting from state *s* following a policy ![](img/B15558_03_008.png):'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，价值函数（状态值）是从状态*s*开始，遵循策略！[](img/B15558_03_008.png)的轨迹的期望回报：
- en: '![](img/B15558_12_289.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_289.png)'
- en: 'We learned that the return is the sum of rewards of the trajectory, so we can
    rewrite the preceding equation by expanding the return as:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，回报是轨迹奖励的总和，因此我们可以通过扩展回报来重写前述方程：
- en: '![](img/B15558_12_290.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_290.png)'
- en: 'Now, we can rewrite the value function by adding the entropy term as shown
    here:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过添加熵项来重写价值函数，如下所示：
- en: '![](img/B15558_12_291.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_291.png)'
- en: 'We know that the Q function (state-action value) is the expected return of
    the trajectory starting from state *s* and action *a* following a policy ![](img/B15558_03_140.png):'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，Q函数（状态-动作值）是从状态*s*和动作*a*开始，遵循策略！[](img/B15558_03_140.png)的轨迹的期望回报：
- en: '![](img/B15558_12_293.png)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_293.png)'
- en: 'Expanding the return of the trajectory, we can write the following:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展轨迹的回报，我们可以写出以下公式：
- en: '![](img/B15558_12_294.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_294.png)'
- en: 'Now, we can rewrite the Q function by adding the entropy term as shown here:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过添加熵项来重写Q函数，如下所示：
- en: '![](img/B15558_12_295.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_295.png)'
- en: 'The modified Bellman equation for the preceding Q function with the entropy
    term is given as:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 带有熵项的前述Q函数的修改版贝尔曼方程给出如下：
- en: '![](img/B15558_12_296.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_296.png)'
- en: 'Here, the value function can be computed using the relation between the Q function
    and the value function as:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，价值函数可以通过Q函数与价值函数之间的关系计算得到，如下所示：
- en: '![](img/B15558_12_297.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_297.png)'
- en: 'To learn how exactly we obtained the equations (2) and (3), you can check the
    derivation of soft policy iteration in the paper *Soft Actor-Critic: Off-Policy
    Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor*, by Tuomas
    Haarnoja et.al.: [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '要了解我们是如何精确得出公式（2）和（3）的，可以查看论文《Soft Actor-Critic: Off-Policy Maximum Entropy
    Deep Reinforcement Learning with a Stochastic Actor》中关于软策略迭代的推导，作者为Tuomas Haarnoja等人：[https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
- en: Components of SAC
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAC的组成部分
- en: Now that we have a basic idea of SAC, let's go into more detail and understand
    how exactly each component of SAC works by looking at them separately.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对SAC有了基本了解，让我们更详细地探讨并分别了解SAC中每个组件是如何工作的。
- en: Critic network
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评论网络
- en: We learned that unlike other actor-critic methods we have seen earlier, the
    critic in SAC uses both the value function and the Q function to evaluate the
    policy produced by the actor network. But why is that?
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，与之前看到的其他演员-评论家方法不同，SAC中的评论家同时使用价值函数和Q函数来评估演员网络产生的策略。但为什么会这样呢？
- en: 'In the previous algorithms, we used the critic network to compute the Q function
    for evaluating the action produced by the actor. Also, the target Q value in the
    critic is computed using the Bellman equation. We can do the same here. However,
    here we have modified the Bellman equation of the Q function due to the entropy
    term, as we learned in equation (2):'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的算法中，我们使用评论网络来计算Q函数，以评估演员产生的动作。此外，评论网络中的目标Q值是通过贝尔曼方程计算的。我们在这里也可以这样做。然而，由于熵项的存在，这里我们对Q函数的贝尔曼方程进行了修改，正如我们在公式（2）中学到的那样：
- en: '![](img/B15558_12_298.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_298.png)'
- en: From the preceding equation, we can observe that in order to compute the Q function,
    first we need to compute the value function. So, we need to compute both the Q
    function and the value function in order to evaluate the policy produced by the
    actor. We can use a single network to approximate both the Q function and the
    value function. However, instead of using a single network, we use two different
    networks, the Q network to estimate the Q function, and the value network to estimate
    the value function. Using two different networks to compute the Q function and
    value function stabilizes the training.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述方程中，我们可以观察到，为了计算Q函数，我们首先需要计算价值函数。因此，我们需要计算Q函数和价值函数，以便评估演员产生的策略。我们可以使用单个网络来逼近Q函数和价值函数。然而，我们并没有使用一个网络，而是使用了两个不同的网络，一个Q网络用来估计Q函数，另一个价值网络用来估计价值函数。使用两个不同的网络来计算Q函数和价值函数有助于稳定训练。
- en: As mentioned in the SAC paper, "*There is no need in principle to include a
    separate function approximator (neural network) for the state value since it is
    related to the Q function and policy according to Equation 2\. But in practice,
    including a separate function approximator (neural network) for the state value
    can stabilize training and is convenient to train simultaneously with the other
    networks*."
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如SAC论文中所述，"*原则上不需要为状态值包含一个独立的函数逼近器（神经网络），因为它根据方程(2)与Q函数和策略相关。但在实际操作中，包含一个独立的函数逼近器（神经网络）来处理状态值可以稳定训练，并且便于与其他网络同时训练*。"
- en: First, we will learn how the value network works and then we will learn about
    the Q network.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将学习价值网络是如何工作的，然后再学习Q网络。
- en: Value network
  id: totrans-509
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 价值网络
- en: The value network is denoted by *V*, the parameter of the value network is denoted
    by ![](img/B15558_12_299.png), and the parameter of the target value network is
    denoted by ![](img/B15558_12_300.png).
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 价值网络用*V*表示，价值网络的参数用![](img/B15558_12_299.png)表示，目标价值网络的参数用![](img/B15558_12_300.png)表示。
- en: Thus, ![](img/B15558_12_301.png) implies that we approximate the value function
    (state value) using the neural network parameterized by ![](img/B15558_12_302.png).
    Okay, how can we train the value network? We can train the network by minimizing
    the loss between the target state value and the state value predicted by our network.
    How can we obtain the target state value? We can use the value function given
    in equation (3) to compute the target state value.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/B15558_12_301.png)表示我们通过神经网络（由![](img/B15558_12_302.png)表示）来逼近价值函数（状态值）。好的，如何训练价值网络呢？我们可以通过最小化目标状态值与我们网络预测的状态值之间的损失来训练网络。我们如何得到目标状态值？我们可以使用方程（3）给出的价值函数来计算目标状态值。
- en: 'We learned that according to equation (3), the value of the state is computed
    as:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，根据方程（3），状态值是通过以下方式计算的：
- en: '![](img/B15558_12_303.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_303.png)'
- en: 'In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target state value *y*[v] using the preceding equation
    as:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们可以去掉期望值。我们将通过从重放缓冲区中采样*K*个转移来逼近期望值。因此，我们可以使用前面的方程计算目标状态值*y*[v]：
- en: '![](img/B15558_12_304.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_304.png)'
- en: 'If we look at the preceding equation, we have a Q function. In order to compute
    the Q function, we use a Q network parameterized by ![](img/B15558_09_098.png),
    and similarly, our policy is parameterized by ![](img/B15558_12_306.png), so we
    can rewrite the preceding equation with the parameterized Q function and policy
    as shown here:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下前面的方程，我们有一个Q函数。为了计算Q函数，我们使用一个Q网络，参数由![](img/B15558_09_098.png)表示，类似地，我们的策略由![](img/B15558_12_306.png)表示，因此我们可以将前面的方程用参数化的Q函数和策略重新写出，如下所示：
- en: '![](img/B15558_12_307.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_307.png)'
- en: 'But if we use the preceding equation to compute the target value, the Q value
    will overestimate. So, to avoid this overestimation, we use clipped double Q learning,
    just like we learned in TD3\. That is, we compute the two Q values using two Q
    networks parameterized by ![](img/B15558_12_211.png) and ![](img/B15558_12_217.png)
    and take the minimum value of these two, as shown here:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们使用前面的方程来计算目标值，Q值将会高估。因此，为了避免这种高估，我们使用裁剪的双Q学习，就像我们在TD3中学到的一样。也就是说，我们通过两个Q网络来计算两个Q值，这两个Q网络的参数分别由![](img/B15558_12_211.png)和![](img/B15558_12_217.png)表示，并取这两个值的最小值，如下所示：
- en: '![](img/B15558_12_310.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_310.png)'
- en: As we can observe in the preceding equation, for clipped double Q learning,
    we are using the two main Q networks parameterized by ![](img/B15558_12_311.png)
    and ![](img/B15558_12_217.png), but in TD3, we used two target Q networks parameterized
    by ![](img/B15558_12_214.png) and ![](img/B15558_12_152.png). Why is that?
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的方程中所观察到的，对于裁剪的双Q学习，我们使用了两个主要的Q网络，这两个Q网络的参数分别由![](img/B15558_12_311.png)和![](img/B15558_12_217.png)表示，而在TD3中，我们使用了两个目标Q网络，这两个目标Q网络的参数分别由![](img/B15558_12_214.png)和![](img/B15558_12_152.png)表示。为什么会这样呢？
- en: Because here, we are computing the Q value of a state-action pair ![](img/B15558_12_315.png)
    so we can use the two main Q networks parameterized by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_217.png), but in TD3, we compute the Q value of the next
    state-action pair ![](img/B15558_05_091.png), so we used the two target Q networks
    parameterized by ![](img/B15558_12_214.png) and ![](img/B15558_12_320.png). Thus,
    here, we don't need target Q networks.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这里我们正在计算状态动作对 ![](img/B15558_12_315.png) 的 Q 值，所以我们可以使用由 ![](img/B15558_12_216.png)
    和 ![](img/B15558_12_217.png) 参数化的两个主要 Q 网络，但在 TD3 中，我们计算下一个状态动作对的 Q 值 ![](img/B15558_05_091.png)，所以我们使用由
    ![](img/B15558_12_214.png) 和 ![](img/B15558_12_320.png) 参数化的两个目标 Q 网络。因此，在这里，我们不需要目标
    Q 网络。
- en: 'We can simply express the preceding equation as:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地表达上述方程为：
- en: '![](img/B15558_12_321.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_321.png)'
- en: 'Now, we can define our objective function ![](img/B15558_12_322.png) of the
    value network as the mean squared difference between the target state value and
    the state value predicted by our network, as shown here:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们的值网络的目标函数 ![](img/B15558_12_322.png) 为目标状态值与我们的网络预测的状态值之间的均方差，如下所示：
- en: '![](img/B15558_12_323.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_323.png)'
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *K* 表示我们从重播缓冲区中抽样的转换数。
- en: 'We can calculate the gradients of our objective function and then update our
    main value network parameter ![](img/B15558_12_302.png) as:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算我们的目标函数的梯度，然后更新我们的主值网络参数 ![](img/B15558_12_302.png) 如下：
- en: '![](img/B15558_12_325.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_325.png)'
- en: Note that we are using ![](img/B15558_12_326.png) to represent the learning
    rate since we are already using ![](img/B15558_07_025.png) to denote the temperature.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用 ![](img/B15558_12_326.png) 表示学习率，因为我们已经使用 ![](img/B15558_07_025.png)
    表示温度。
- en: 'We can update the parameter of the target value network ![](img/B15558_12_328.png)
    using soft replacement:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用软替换来更新目标值网络的参数 ![](img/B15558_12_328.png)：
- en: '![](img/B15558_12_329.png)'
  id: totrans-531
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_329.png)'
- en: We will learn where exactly the target value network is used in the next section.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节我们将学习目标值网络的确切应用位置。
- en: Q network
  id: totrans-533
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q 网络
- en: The Q network is denoted by *Q* and it is parameterized by ![](img/B15558_12_330.png).
    Thus, ![](img/B15558_12_331.png) implies that we approximate the Q function using
    the neural network parameterized by ![](img/B15558_09_098.png). How can we train
    the Q network? We can train the network by minimizing the loss between the target
    Q value and the Q value predicted by the network. How can we obtain the target
    Q value? Here is where we use the Bellman equation.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: Q网络由*Q*表示，并且其参数为 ![](img/B15558_12_330.png)。因此，![](img/B15558_12_331.png) 意味着我们使用由
    ![](img/B15558_09_098.png) 参数化的神经网络来近似 Q 函数。我们如何训练 Q 网络呢？我们可以通过最小化目标 Q 值与网络预测的
    Q 值之间的损失来训练网络。我们如何获取目标 Q 值呢？这就是我们使用贝尔曼方程的地方。
- en: 'We learned that according to the Bellman equation (2), the Q value can be computed as:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到根据贝尔曼方程（2），可以计算 Q 值如下：
- en: '![](img/B15558_12_298.png)'
  id: totrans-536
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_298.png)'
- en: 'We can remove the expectation in the preceding equation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer.
    So, we can compute the target Q value *y*[q] using the preceding equation as:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过抽样 *K* 个转换从重播缓冲区中近似期望。因此，我们可以使用上述方程计算目标 Q 值 *y*[q]。
- en: '![](img/B15558_12_334.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_334.png)'
- en: 'If we look at the preceding equation, we have a value of next state ![](img/B15558_12_335.png).
    In order to compute the value of next state ![](img/B15558_12_336.png), we use
    a target value network parameterized by ![](img/B15558_12_337.png), so we can
    rewrite the preceding equation with the parameterized value function as shown
    here:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一下上述方程，我们有一个下一个状态的值 ![](img/B15558_12_335.png)。为了计算下一个状态的值 ![](img/B15558_12_336.png)，我们使用由
    ![](img/B15558_12_337.png) 参数化的目标值网络，因此我们可以用参数化值函数重写上述方程，如下所示：
- en: '![](img/B15558_12_338.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_338.png)'
- en: 'Now, we can define our objective function ![](img/B15558_12_339.png) of the
    Q network as the mean squared difference between the target Q value and the Q
    value predicted by the network, as shown here:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义我们 Q 网络的目标函数 ![](img/B15558_12_339.png) 为目标 Q 值与网络预测的 Q 值之间的均方差，如下所示：
- en: '![](img/B15558_12_340.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_340.png)'
- en: Where *K* denotes the number of transitions we sample from the replay buffer.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *K* 表示我们从重播缓冲区中抽样的转换数。
- en: 'In the previous section, we learned that we use two Q networks parameterized
    by ![](img/B15558_12_216.png) and ![](img/B15558_12_217.png) to prevent overestimation
    bias. So, first, we compute the loss of the first Q network, parameterized by
    ![](img/B15558_12_211.png):'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用由 ![](img/B15558_12_216.png) 和 ![](img/B15558_12_217.png) 参数化的两个
    Q 网络来防止过估计偏差。因此，首先，我们计算由 ![](img/B15558_12_211.png) 参数化的第一个 Q 网络的损失：
- en: '![](img/B15558_12_344.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_344.png)'
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_211.png)using
    gradient descent as ![](img/B15558_12_346.png).
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算梯度并使用梯度下降法更新参数 ![](img/B15558_12_211.png)，如 ![](img/B15558_12_346.png)
    所示。
- en: 'Next, we compute the loss of the second Q network, parameterized by ![](img/B15558_12_217.png):'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算由 ![](img/B15558_12_217.png) 参数化的第二个 Q 网络的损失：
- en: '![](img/B15558_12_348.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_348.png)'
- en: Then, we compute the gradients and update the parameter ![](img/B15558_12_217.png)using
    gradient descent as ![](img/B15558_12_350.png).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算梯度并使用梯度下降法更新参数 ![](img/B15558_12_217.png)，如 ![](img/B15558_12_350.png)
    所示。
- en: 'We can simply express the preceding updates as:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将上述更新表示为：
- en: '![](img/B15558_12_351.png)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_351.png)'
- en: Actor network
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 演员网络
- en: 'The actor network (policy network) is parameterized by ![](img/B15558_11_043.png).
    Let''s recall the objective function of the actor network we learned in TD3:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络（策略网络）由 ![](img/B15558_11_043.png) 参数化。让我们回顾一下我们在 TD3 中学到的演员网络的目标函数：
- en: '![](img/B15558_12_100.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_100.png)'
- en: Where *a* is the action produced by the actor.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *a* 是由演员产生的动作。
- en: The preceding objective function means that the goal of the actor is to generate
    action in such a way that it maximizes the Q value computed by the critic.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 上述目标函数意味着，演员的目标是生成一种动作，使其最大化由评论员计算的 Q 值。
- en: 'The objective function of the actor network in SAC is the same as what we learned
    in TD3, except that here we use a stochastic policy ![](img/B15558_12_413.png),
    and also, we maximize the entropy. So, we can write the objective function of
    the actor network in SAC as:'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 中的演员网络的目标函数与我们在 TD3 中学到的相同，不同之处在于，这里我们使用了一个随机策略 ![](img/B15558_12_413.png)，同时，我们还最大化了熵。因此，我们可以将
    SAC 中演员网络的目标函数写成：
- en: '![](img/B15558_12_354.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_354.png)'
- en: 'Now, how can we compute the derivative of the preceding objective function?
    Because, unlike TD3, here, our action is computed using a stochastic policy. It
    will be difficult to apply backpropagation and compute gradients of the preceding
    objective function with the action computed using a stochastic policy. So, we
    use the reparameterization trick. The reparameterization trick guarantees that
    sampling from our policy is differentiable. Thus, we can rewrite our action as
    shown here:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何计算上述目标函数的导数呢？因为与 TD3 不同，在这里，我们的动作是使用随机策略计算的。应用反向传播并计算使用随机策略计算的动作的目标函数梯度将会很困难。因此，我们使用了重参数化技巧。重参数化技巧保证了从我们的策略中采样是可微的。因此，我们可以将我们的动作重新写为如下所示：
- en: '![](img/B15558_12_355.png)'
  id: totrans-560
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_355.png)'
- en: In the preceding equation, we can observe that we parameterize the policy with
    a neural network *f* and ![](img/B15558_12_200.png) is the noise sampled from
    a spherical Gaussian distribution.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，我们可以观察到，我们用神经网络 *f* 参数化策略，并且 ![](img/B15558_12_200.png) 是从球形高斯分布中采样的噪声。
- en: 'Thus, we can rewrite our objective function as shown here:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将目标函数重新写为如下所示：
- en: '![](img/B15558_12_358.png)'
  id: totrans-563
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_358.png)'
- en: 'Note that in the preceding equation, our action is ![](img/B15558_12_355.png).
    Remember how we used two Q functions parameterized by ![](img/B15558_12_359.png)
    and ![](img/B15558_12_360.png) to avoid overestimation bias? Now, which Q function
    should we use in the preceding objective function? We can use either of the functions
    and so we use the Q function parameterized by ![](img/B15558_12_216.png) and write
    our final objective function as:'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在上述方程中，我们的动作是 ![](img/B15558_12_355.png)。记得我们是如何使用由 ![](img/B15558_12_359.png)
    和 ![](img/B15558_12_360.png) 参数化的两个 Q 函数来避免过估计偏差的吗？现在，在上述目标函数中，我们应该使用哪个 Q 函数？我们可以使用任一函数，因此我们使用由
    ![](img/B15558_12_216.png) 参数化的 Q 函数，并将最终目标函数写为：
- en: '![](img/B15558_12_362.png)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_362.png)'
- en: Now that we have understood how the SAC algorithm works, let's recap what we
    have learned so far and how the SAC algorithm works exactly by putting all the
    concepts together.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 SAC 算法的工作原理，让我们回顾一下迄今为止所学的内容，并通过将所有概念结合起来，准确地了解 SAC 算法是如何工作的。
- en: Putting it all together
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: 'First, let''s recall the notations to understand SAC better. We use five networks—four
    critic networks (two value networks and two Q networks) and one actor network:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下符号，以便更好地理解 SAC。我们使用五个网络——四个评论员网络（两个值网络和两个 Q 网络）和一个演员网络：
- en: The main value network parameter is represented by ![](img/B15558_12_363.png)
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要值网络参数用 ![](img/B15558_12_363.png) 表示。
- en: The target value network parameter is represented by ![](img/B15558_12_364.png)
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标值网络参数用 ![](img/B15558_12_364.png) 表示。
- en: The two main Q network parameters are represented by ![](img/B15558_12_216.png)
    and ![](img/B15558_12_206.png)
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个主要 Q 网络参数分别用 ![](img/B15558_12_216.png) 和 ![](img/B15558_12_206.png) 表示。
- en: The actor network (policy network) parameter is represented by ![](img/B15558_10_152.png)
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员网络（策略网络）参数用 ![](img/B15558_10_152.png) 表示。
- en: The target state value is represented by *y*[v], and the target Q value is represented
    by *y*[q]
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标状态值用 *y*[v] 表示，目标 Q 值用 *y*[q] 表示。
- en: SAC is an actor-critic method, and so the parameters of SAC will get updated
    at every step of the episode. Now, let's get started and understand how SAC works.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 是一种演员-评论员方法，因此 SAC 的参数会在每个回合的每一步进行更新。现在，让我们开始并了解 SAC 是如何工作的。
- en: First, we initialize the main network parameter of the value network ![](img/B15558_12_363.png),
    two Q network parameters ![](img/B15558_12_216.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png). Next, we initialize
    the target value network parameter ![](img/B15558_12_364.png) by just copying
    the main network parameter ![](img/B15558_12_302.png) and then we initialize the
    replay buffer ![](img/B15558_12_374.png).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化值网络的主要网络参数 ![](img/B15558_12_363.png)，两个 Q 网络参数 ![](img/B15558_12_216.png)
    和 ![](img/B15558_12_206.png)，以及演员网络参数 ![](img/B15558_12_371.png)。接下来，我们通过复制主要网络参数
    ![](img/B15558_12_302.png) 来初始化目标值网络参数 ![](img/B15558_12_364.png)，然后初始化回放缓冲区 ![](img/B15558_12_374.png)。
- en: 'Now, for each step in the episode, first, we select an action *a* using the
    actor network:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于回合中的每一步，首先，我们使用演员网络选择一个动作 *a*：
- en: '![](img/B15558_12_375.png)'
  id: totrans-577
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_375.png)'
- en: Then, we perform the action *a*, move to the next state ![](img/B15558_12_376.png),
    and get the reward *r*. We store this transition information in a replay buffer
    ![](img/B15558_09_124.png).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们执行动作 *a*，移动到下一个状态 ![](img/B15558_12_376.png)，并获得奖励 *r*。我们将这个转移信息存储在回放缓冲区
    ![](img/B15558_09_124.png) 中。
- en: Next, we randomly sample a minibatch of *K* transitions from the replay buffer.
    These *K* transitions (*s*, *a*, *r*, *s'*) are used for updating our value, Q,
    and actor network.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从回放缓冲区随机抽取一个 *K* 的小批量转移。这个 *K* 的转移（*s*，*a*，*r*，*s'*）用于更新我们的值、Q 和演员网络。
- en: 'First, let us compute the loss of the value network. We learned that the loss
    function of the value network is:'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们计算值网络的损失。我们已经知道值网络的损失函数是：
- en: '![](img/B15558_12_323.png)'
  id: totrans-581
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_323.png)'
- en: Where ![](img/B15558_12_379.png) is the target state value and it is given as
    ![](img/B15558_12_380.png).
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_12_379.png) 是目标状态值，它表示为 ![](img/B15558_12_380.png)。
- en: 'After computing the loss, we calculate the gradients and update the parameter
    ![](img/B15558_12_302.png) of the value network using gradient descent: ![](img/B15558_12_325.png).'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失后，我们计算梯度并使用梯度下降更新值网络的参数 ![](img/B15558_12_302.png)：![](img/B15558_12_325.png)。
- en: 'Now, we compute the loss of the Q networks. We learned that the loss function
    of the Q network is:'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算 Q 网络的损失。我们已经知道 Q 网络的损失函数是：
- en: '![](img/B15558_12_383.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_383.png)'
- en: Where ![](img/B15558_12_384.png) is the target Q value and it is given as ![](img/B15558_12_385.png).
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_12_384.png) 是目标 Q 值，它表示为 ![](img/B15558_12_385.png)。
- en: 'After computing the loss, we calculate the gradients and update the parameter
    of the Q networks using gradient descent:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失后，我们计算梯度并使用梯度下降更新 Q 网络的参数：
- en: '![](img/B15558_12_386.png)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_386.png)'
- en: 'Next, we update the actor network. We learned that the objective of the actor
    network is:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们更新演员网络。我们已经知道演员网络的目标是：
- en: '![](img/B15558_12_362.png)'
  id: totrans-590
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_362.png)'
- en: 'Now, we calculate gradients and update the parameter ![](img/B15558_12_283.png)
    of the actor network using gradient ascent:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算梯度并使用梯度上升更新演员网络的参数 ![](img/B15558_12_283.png)：
- en: '![](img/B15558_12_389.png)'
  id: totrans-592
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_389.png)'
- en: 'Finally, in the end, we update the target value network parameter by soft replacement,
    as shown here:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过软替换更新目标值网络参数，如下所示：
- en: '![](img/B15558_12_390.png)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_12_390.png)'
- en: We repeat the preceding steps for several episodes and improve the policy. To
    get a better understanding of how SAC works, let's look into the SAC algorithm
    in the next section.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对前述步骤进行了多次迭代，并改进了策略。为了更好地理解SAC的工作原理，我们将在下一节深入探讨SAC算法。
- en: Algorithm – SAC
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – SAC
- en: 'The SAC algorithm is given as follows:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: SAC算法如下所示：
- en: Initialize the main value network parameter ![](img/B15558_12_302.png), the
    Q network parameters ![](img/B15558_12_211.png) and ![](img/B15558_12_206.png),
    and the actor network parameter ![](img/B15558_12_371.png)
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主值网络参数![](img/B15558_12_302.png)，Q网络参数![](img/B15558_12_211.png) 和![](img/B15558_12_206.png)，以及演员网络参数![](img/B15558_12_371.png)
- en: Initialize the target value network ![](img/B15558_12_395.png) by just copying
    the main value network parameter ![](img/B15558_12_302.png)
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过仅复制主值网络参数![](img/B15558_12_302.png)，初始化目标值网络![](img/B15558_12_395.png)
- en: Initialize the replay buffer ![](img/B15558_12_259.png)
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化回放缓冲区![](img/B15558_12_259.png)
- en: For *N* number of episodes, repeat step 5
  id: totrans-601
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*N*个回合，重复步骤5
- en: 'For each step in the episode, that is, for *t* = 0,…, *T* – 1:'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对于回合中的每个步骤，即对于*t* = 0,…, *T* – 1:'
- en: Select an action *a* based on the policy ![](img/B15558_12_398.png), that is,
    ![](img/B15558_12_399.png)
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略![](img/B15558_12_398.png)选择一个动作*a*，即![](img/B15558_12_399.png)
- en: Perform the selected action *a*, move to the next state ![](img/B15558_12_376.png),
    get the reward *r*, and store the transition information in the replay buffer
    ![](img/B15558_09_124.png)
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行选择的动作*a*，移动到下一个状态![](img/B15558_12_376.png)，获取奖励*r*，并将过渡信息存储到回放缓冲区![](img/B15558_09_124.png)
- en: Randomly sample a minibatch of *K* transitions from the replay buffer
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从回放缓冲区随机抽取一个大小为*K*的迷你批次过渡
- en: Compute the target state value ![](img/B15558_12_380.png)
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标状态值![](img/B15558_12_380.png)
- en: Compute the loss of value network ![](img/B15558_12_323.png) and update the
    parameter using gradient descent, ![](img/B15558_12_325.png)
  id: totrans-607
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的损失![](img/B15558_12_323.png)，并使用梯度下降法更新参数，![](img/B15558_12_325.png)
- en: Compute the target Q value ![](img/B15558_12_385.png)
  id: totrans-608
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标Q值![](img/B15558_12_385.png)
- en: Compute the loss of the Q networks ![](img/B15558_12_383.png) and update the
    parameter using gradient descent, ![](img/B15558_12_386.png)
  id: totrans-609
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算Q网络的损失![](img/B15558_12_383.png)，并使用梯度下降法更新参数，![](img/B15558_12_386.png)
- en: Compute gradients of the actor objective function, ![](img/B15558_10_093.png)
    and update the parameter using gradient ascent, ![](img/B15558_12_409.png)
  id: totrans-610
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算演员目标函数的梯度，![](img/B15558_10_093.png)，并使用梯度上升法更新参数，![](img/B15558_12_409.png)
- en: Update the target value network parameter as ![](img/B15558_12_410.png)
  id: totrans-611
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新目标值网络参数为![](img/B15558_12_410.png)
- en: Many congratulations on learning the several important state-of-the-art actor-critic
    algorithms, including DDPG, twin delayed DDPG, and SAC. In the next chapter, we will
    examine several state-of-the-art policy gradient algorithms.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你学习了多个重要的最先进演员-评论家算法，包括DDPG、双延迟DDPG和SAC。在下一章中，我们将研究几个最先进的策略梯度算法。
- en: Summary
  id: totrans-613
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by understanding the DDPG algorithm. We learned that
    DDPG is an actor-critic algorithm where the actor estimates the policy using policy
    gradient and the critic evaluates the policy produced by the actor using the Q
    function. We learned how DDPG uses a deterministic policy and how it is used in
    environments with a continuous action space.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解DDPG算法开始了本章的内容。我们了解到DDPG是一个演员-评论家算法，其中演员使用策略梯度估计策略，评论家使用Q函数评估演员产生的策略。我们还学习了DDPG如何使用确定性策略以及它如何应用于具有连续动作空间的环境。
- en: Later, we looked into the actor and critic components of DDPG in detail and
    understood how they work, before finally learning about the DDPG algorithm.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们详细研究了DDPG的演员和评论家组件，理解了它们是如何工作的，然后最终学习了DDPG算法。
- en: Moving on, we learned about the twin delayed DDPG, which is the successor to
    DDPG and constitutes an improvement to the DDPG algorithm. We learned the key
    features of TD3, including clipped double Q learning, delayed policy updates,
    and target policy smoothing, in detail and finally, we looked into the TD3 algorithm.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了双延迟DDPG，它是DDPG的继任者，并对DDPG算法进行了改进。我们详细学习了TD3的关键特性，包括剪切双Q学习、延迟策略更新和目标策略平滑，最后，我们深入了解了TD3算法。
- en: At the end of the chapter, we learned about the SAC algorithm. We learned that,
    unlike DDPG and TD3, the SAC method uses a stochastic policy. We also understood
    how SAC works with the entropy bonus in the objective function, and we learned
    what is meant by maximum entropy reinforcement learning.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们学习了SAC算法。我们了解到，与DDPG和TD3不同，SAC方法使用的是随机策略。我们还理解了SAC如何在目标函数中与熵奖励一起工作，并且我们了解了最大熵强化学习的含义。
- en: In the next chapter, we will learn the state-of-the-art policy gradient algorithms
    such as trust region policy optimization, proximal policy optimization, and actor-critic
    using Kronecker-factored trust region.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习最先进的策略梯度算法，如信任区域策略优化（TRPO）、近端策略优化（PPO）和使用克罗内克因子信任区域的演员-评论家方法。
- en: Questions
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s put our knowledge of actor-critic methods to the test. Try answering
    the following questions:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将演员-评论家方法的知识付诸实践。试着回答以下问题：
- en: What is the role of actor and critic networks in DDPG?
  id: totrans-621
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在DDPG中，演员和评论家网络的作用是什么？
- en: How does the critic in DDPG work?
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG中的评论家是如何工作的？
- en: What are the key features of TD3?
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TD3的关键特性是什么？
- en: Why do we need clipped double Q learning?
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要剪切双重Q学习？
- en: What is target policy smoothing?
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是目标策略平滑？
- en: What is maximum entropy reinforcement learning?
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是最大熵强化学习？
- en: What is the role of the critic network in SAC?
  id: totrans-627
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SAC中，评论家网络的作用是什么？
- en: Further reading
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, refer to the following papers:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参阅以下文献：
- en: '**Continuous Control with Deep Reinforcement Learning** by *Timothy P. Lillicrap,
    et al.*, [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用深度强化学习进行连续控制**，作者 *Timothy P. Lillicrap, 等人*, [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)'
- en: '**Addressing Function Approximation Error in Actor-Critic Methods** by *Scott
    Fujimoto, Herke van Hoof, David Meger,* [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员-评论家方法中的函数逼近误差**，作者 *Scott Fujimoto, Herke van Hoof, David Meger,* [https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf)'
- en: '**Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
    with a Stochastic Actor** by *Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey
    Levine*, [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软演员-评论家：使用随机演员的离策略最大熵深度强化学习**，作者 *Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel,
    Sergey Levine*, [https://arxiv.org/pdf/1801.01290.pdf](https://arxiv.org/pdf/1801.01290.pdf)'
