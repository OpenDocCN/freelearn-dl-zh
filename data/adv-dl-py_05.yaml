- en: Advanced Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级卷积网络
- en: In [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional
    Networks*, we discussed the building blocks of **convolutional neural networks**
    (**CNNs**) and some of their properties. In this chapter, we'll go a step further
    and talk about some of the most popular CNN architectures. These networks usually
    combine multiple primitive convolution and/or pooling operations in a novel building
    block that serves as a base for a complex architecture. This allows us to build
    very deep (and sometimes wide) networks with high representational power that
    perform well on complex tasks such as ImageNet classification, image segmentation,
    speech recognition, and so on. Many of these models were first released as participants
    in the ImageNet challenge, which they usually won. To simplify our task, we'll
    discuss all architecture within the context of image classification. We'll still
    discuss more complex tasks, but we'll do it in [Chapter 4](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml),
    *Object Detection and Image Segmentation*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，《理解卷积神经网络》中，我们讨论了**卷积神经网络**（**CNN**）的基本构件及其一些特性。在本章中，我们将更进一步，讨论一些最受欢迎的CNN架构。这些网络通常将多个基础卷积和/或池化操作结合在一个新的构件中，作为复杂架构的基础。这使得我们能够构建非常深（有时也很宽）的网络，具有较高的表示能力，能够在复杂任务中表现良好，如ImageNet分类、图像分割、语音识别等。许多这些模型最初是作为ImageNet挑战赛的参与者发布的，并且通常获得了胜利。为了简化任务，我们将在图像分类的背景下讨论所有架构。我们仍会讨论更复杂的任务，但会在[第四章](9ac88546-8662-4b23-aa96-7eb00f48fedb.xhtml)，《目标检测与图像分割》中进行讨论。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing AlexNet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍AlexNet
- en: An introduction to Visual Geometry Group
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Visual Geometry Group简介
- en: Understanding residual networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解残差网络
- en: Understanding Inception networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Inception网络
- en: Introducing Xception
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Xception
- en: Introducing MobileNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍MobileNet
- en: An introduction to DenseNets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet简介
- en: The workings of neural architecture search
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经架构搜索的工作原理
- en: Introducing capsule networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍胶囊网络
- en: Introducing AlexNet
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍AlexNet
- en: The first model we'll discuss is the winner of the 2012 **ImageNet Large Scale
    Visual Recognition Challenge** (**ILSVRC**, or simply ImageNet). It's nicknamed
    AlexNet (*ImageNet Classification with Deep Convolutional Neural Networks*, [https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)),
    after one of its authors, Alex Krizhevsky. Although this model is rarely used
    nowadays, it's an important milestone in contemporary deep learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个模型是2012年**ImageNet大规模视觉识别挑战**（**ILSVRC**，简称ImageNet）的冠军。这个模型被称为AlexNet（*通过深度卷积神经网络进行ImageNet分类*，[https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)），以其作者之一Alex
    Krizhevsky命名。虽然现在这个模型很少使用，但它是当代深度学习中的一个重要里程碑。
- en: 'The following diagram shows the network architecture:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了网络架构：
- en: '![](img/ec08175c-5282-4be2-b6e7-6f2d99272166.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec08175c-5282-4be2-b6e7-6f2d99272166.png)'
- en: The AlexNet architecture. The original model was split in two, so it can fit
    on the memory of two GPUs
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet架构。原始模型被拆分为两个部分，以便它可以适应两张GPU的内存
- en: The model has five cross-correlated convolutional layers, three overlapping
    max pooling layers, three fully connected layers, and ReLU activations. The output
    is a 1,000-way softmax (one for each ImageNet class). The first and second convolutional
    layers use local response normalization—a type of normalization, somewhat similar
    to batch normalization. The fully connected layers have a dropout rate of 0.5\.
    To prevent overfitting, the network was trained using random 227×227 crops of
    the 256×256 input images. The network achieves top-1 and top-5 test set error
    rates of 37.5% and 17.0%.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有五层交叉相关的卷积层，三层重叠的最大池化层，三层全连接层，以及ReLU激活。输出是一个1,000维的softmax（每个对应一个ImageNet类别）。第一层和第二层卷积使用局部响应归一化——这是一种归一化方法，类似于批量归一化。全连接层的dropout率为0.5。为了防止过拟合，网络使用随机227×227的裁剪图像（来自256×256的输入图像）进行训练。该网络在测试集上实现了37.5%的top-1误差率和17.0%的top-5误差率。
- en: In the next section, we'll discuss an NN architecture that was introduced by Oxford's
    Visual Geometry Group in 2014, when it became a runner-up in the ImageNet challenge
    of that year.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将讨论由牛津大学Visual Geometry Group于2014年提出的一个神经网络架构，该架构在当年的ImageNet挑战赛中获得了亚军。
- en: An introduction to Visual Geometry Group
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Visual Geometry Group（视觉几何组）简介
- en: The next architecture we're going to discuss is **Visual Geometry Group** (**VGG**)
    (from Oxford's Visual Geometry Group, *Very Deep Convolutional Networks for Large-Scale
    Ima*g*e* *Recognition*, [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)[)](https://arxiv.org/abs/1409.1556).
    The VGG family of networks remains popular today and is often used as a benchmark
    against newer architectures. Prior to VGG (for example, LeNet-5: [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/) and
    AlexNet), the initial convolutional layers of a network used filters with large
    receptive fields, such as 11×11\. Additionally, the networks usually had alternating
    single convolutional and pooling layers. The authors of the paper observed that
    a convolutional layer with a large filter size can be replaced with a stack of
    two or more convolutional layers with smaller filters (factorized convolution).
    For example, we can replace one 5×5 layer with a stack of two 3×3 layers, or a
    7×7 layer with a stack of three 3×3 layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的架构是**视觉几何组**（**VGG**）（来自牛津大学的视觉几何组，*非常深的卷积网络用于大规模图像识别*，[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)）。VGG系列网络至今仍然非常流行，且常常作为新的架构的基准。VGG之前的网络（例如，LeNet-5：[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)和AlexNet）中，网络的初始卷积层使用的是大感受野的滤波器，如11×11。此外，网络通常由交替的单一卷积层和池化层组成。论文的作者观察到，大滤波器的卷积层可以被堆叠的多个小滤波器的卷积层所替代（因式分解卷积）。例如，我们可以用两层3×3的卷积层代替一个5×5的层，或者用三层3×3的卷积层代替一个7×7的层。
- en: 'This structure has several advantages, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构有几个优点，具体如下：
- en: The neurons of the last of the stacked layers have the equivalent receptive
    field size of a single layer with a large filter.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠层的最后一个神经元具有与单层大滤波器相同的感受野大小。
- en: The number of weights and operations of stacked layers is smaller, compared
    to a single layer with a large filter size. Let's assume we want to replace one
    5×5 layer with two 3×3 layers. Let's also assume that all layers have an equal
    number of input and output channels (slices), *M*. The total number of weights
    (excluding biases) of the 5×5 layer is *5*5*M*M = 25*M²*. On the other hand, the
    total weights of a single 3×3 layer is *3*3*M*M = 9*M²*, and simply *2*(3*3*M*M)
    = 18*M²* for two layers, which makes this arrangement 28% more efficient (18/25
    = 0.72). The efficiency will increase further with larger filters.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与单一大滤波器的层相比，堆叠层的权重和操作数较少。假设我们想用两个3×3层替代一个5×5层。假设所有层的输入和输出通道数（切片）相同，记为*M*。5×5层的总权重数（不包括偏置）为*5*5*M*M
    = 25*M²*。另一方面，单个3×3层的总权重为*3*3*M*M = 9*M²*，两个层的权重总数为*2*(3*3*M*M) = 18*M²*，使得这种安排比单层大滤波器更高效28%（18/25
    = 0.72）。当滤波器更大时，效率会进一步提高。
- en: Stacking multiple layers makes the decision function more discriminative.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠多个层使得决策函数更具判别力。
- en: 'The VGG networks consist of multiple blocks of two, three, or four stacked
    convolutional layers combined with a max pooling layer. We can see the two most
    popular variants, **VGG16** and **VGG19**, in the following table:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: VGG网络由多个堆叠的两层、三层或四层卷积层和最大池化层组成。我们可以在下表中看到两种最流行的变体，**VGG16**和**VGG19**：
- en: '![](img/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab51fa0e-ccbf-4a89-b3c7-f76561637925.png)'
- en: Architecture of the VGG16 and VGG19 networks, named after the number of weighted
    layers in each network
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16和VGG19网络的架构，命名来源于每个网络中的加权层数
- en: As the depth of the VGG network increases, so does the width (the number of
    filters) in the convolutional layers. We have multiple pairs of cross-channel
    convolutions with a volume depth of 128/256/512 connected to other layers with
    the same depth. In addition, we also have two 4,096-unit fully connected layers,
    followed by a 1000-unit fully connected layer and a softmax (one for each ImageNet
    class). Because of this, the VGG networks have a large number of parameters (weights),
    which makes them memory-inefficient, as well as computationally expensive. Still,
    this is a popular and straightforward network architecture, which has been further
    improved by the addition of batch normalization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 随着VGG网络深度的增加，卷积层的宽度（滤波器的数量）也增加。我们有多个交叉通道卷积对，卷积深度为128/256/512，连接到其他相同深度的层。此外，我们还有两个4,096单元的完全连接层，后接一个1,000单元的完全连接层和一个softmax（每个ImageNet类一个）。因此，VGG网络有大量的参数（权重），这使得它们在内存使用上效率低下，且计算成本高昂。尽管如此，这仍然是一个流行且简单的网络架构，后来通过加入批归一化（batch
    normalization）得到了进一步的改进。
- en: In the next section, we'll use VGG as an example of how to load pretrained network
    models with TensorFlow and PyTorch.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用VGG作为示例，展示如何使用TensorFlow和PyTorch加载预训练的网络模型。
- en: VGG with PyTorch and TensorFlow
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch和TensorFlow的VGG
- en: Both PyTorch and TensorFlow have pretrained VGG models. Let's see how to use
    them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch和TensorFlow都有预训练的VGG模型。我们来看一下如何使用它们。
- en: 'Keras is an official part of TensorFlow 2, therefore, we''ll use it to load
    the model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是TensorFlow 2的官方部分，因此我们将使用它来加载模型：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: By setting the `weights='imagenet'` parameter, the network will be loaded with
    pretrained ImageNet weights (they will be downloaded automatically). You can set
    `include_top` to `False`, which will exclude the fully connected layers for a
    transfer learning scenario. In this case, you can also use an arbitrary input
    size by setting a tuple value to `input_shape`—the convolutional layers will automatically
    scale to match the desired input shape. This is possible because the convolution
    filter is shared along the whole feature map. Therefore, we can use the same filter on
    feature maps with different sizes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`weights='imagenet'`参数，网络将加载预训练的ImageNet权重（它们会自动下载）。你可以将`include_top`设置为`False`，以排除完全连接层，适用于迁移学习场景。在这种情况下，你还可以通过设置`input_shape`为一个元组值，来使用任意输入大小——卷积层将自动调整以匹配所需的输入形状。这之所以可行，是因为卷积滤波器在整个特征图中是共享的。因此，我们可以在具有不同大小的特征图上使用相同的滤波器。
- en: 'We''ll continue with PyTorch, where you can choose whether you want to use
    a pretrained model (again, with automatic download):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用PyTorch，你可以选择是否使用预训练模型（同样会自动下载）：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can try other pretrained models, using the same procedures we described.
    To avoid repetition, we won't include the same code examples for the other architectures
    in this section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试其他预训练模型，使用我们描述的相同流程。为了避免重复，我们不会在这一节中为其他架构提供相同的代码示例。
- en: In the next section, we'll discuss one of the most popular CNN architectures,
    which was released after VGG.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论VGG发布后最流行的卷积神经网络（CNN）架构之一。
- en: Understanding residual networks
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解残差网络
- en: Residual networks (**ResNets**, *Deep Residual Learning for Image Recognition*, [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    were released in 2015, when they won all five categories of the ImageNet challenge
    that year. In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts
    and Bolts of Neural* *Networks*, we mentioned that the layers of a neural network
    are not restricted to sequential order, but form a graph instead. This is the
    first architecture we'll learn, which takes advantage of this flexibility. This
    is also the first network architecture that has successfully trained a network
    with a depth of more than 100 layers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 残差网络（**ResNets**，*深度残差学习用于图像识别*， [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）于2015年发布，并在当年赢得了ImageNet挑战赛的五个类别的冠军。在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*中，我们提到神经网络的层并不受限于顺序排列，而是形成了一个图结构。这是我们将要学习的第一个架构，它利用了这种灵活性。这也是第一个成功训练了超过100层深度网络的网络架构。
- en: Thanks to better weight initializations, new activation functions, as well as
    normalization layers, it's now possible to train deep networks. But, the authors
    of the paper conducted some experiments and observed that a network with 56 layers
    had higher training and testing errors compared to a network with 20 layers. They
    argue that this should not be the case. In theory, we can take a shallow network
    and stack identity layers (these are layers whose output just repeats the input)
    on top of it to produce a deeper network that behaves in exactly the same way
    as the shallow one. Yet, their experiments have been unable to match the performance
    of the shallow network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更好的权重初始化、新的激活函数以及归一化层，现在可以训练深度网络了。但是，论文的作者进行了一些实验，观察到一个56层的网络在训练和测试误差上都高于一个20层的网络。他们认为这种情况不应该发生。从理论上讲，我们可以使用一个浅层网络，并在其上堆叠恒等层（这些层的输出仅重复输入），来构造一个深度网络，该网络的表现与浅层网络完全相同。然而，他们的实验未能使深度网络的表现达到浅层网络的水平。
- en: 'To solve this problem, they proposed a network constructed of residual blocks.
    A residual block consists of two or three sequential convolutional layers and
    a separate parallel identity (repeater) shortcut connection, which connects the
    input of the first layer and the output of the last one. We can see three types
    of residual blocks in the following screenshot:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，他们提出了由残差模块构成的网络。一个残差模块由两个或三个连续的卷积层和一个独立的并行恒等（重复器）快捷连接组成，它连接了第一个层的输入和最后一个层的输出。我们可以在以下截图中看到三种类型的残差模块：
- en: '![](img/9629b25c-e912-469b-83c7-9d1f4058a249.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9629b25c-e912-469b-83c7-9d1f4058a249.png)'
- en: 'From left to right: original residual block; original bottleneck residual block;
    pre-activation residual block; pre-activation bottleneck residual block'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：原始残差模块；原始瓶颈残差模块；预激活残差模块；预激活瓶颈残差模块
- en: Each block has two parallel paths. The left-hand path is similar to the other
    networks we've seen, and consists of sequential convolutional layers + batch normalization.
    The right path contains the identity shortcut connection (also known as the skip
    connection). The two paths are merged via an element-wise sum. That is, the left
    and right tensors have the same shape and an element of the first tensor is added
    to the element in the same position in the second tensor. The output is a single
    tensor with the same shape as the input. In effect, we propagate forward the features
    learned by the block, but also the original unmodified signal. In this way, we
    can get closer to the original scenario, as described by the authors. The network
    can decide to skip some of the convolutional layers thanks to the skip connections,
    in effect reducing its own depth. The residual blocks use padding in such a way
    that the input and the output of the block have the same dimensions. Thanks to
    this, we can stack any number of blocks for a network with an arbitrary depth.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块有两个并行路径。左侧路径与我们见过的其他网络类似，由连续的卷积层 + 批归一化组成。右侧路径包含了恒等快捷连接（也称为跳跃连接）。这两个路径通过逐元素相加的方式进行合并。也就是说，左侧和右侧的张量具有相同的形状，第一个张量的一个元素会加到第二个张量中相同位置的元素上。输出是一个形状与输入相同的单一张量。实际上，我们向前传播的是模块学习到的特征，但也包括了原始的未修改信号。通过这种方式，我们可以更接近原始场景，正如作者所描述的那样。网络可以通过跳跃连接决定跳过一些卷积层，实际上减少了它自身的深度。残差模块使用了填充技术，使得输入和输出的尺寸相同。得益于此，我们可以堆叠任意数量的模块，从而实现任意深度的网络。
- en: 'And now, let''s see how the blocks in the diagram differ:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看图中不同模块的区别：
- en: The first block contains two 3×3 convolutional layers. This is the original
    residual block, but if the layers are wide, stacking multiple blocks becomes computationally
    expensive.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个模块包含两个3×3的卷积层。这是原始的残差模块，但如果层数较宽，堆叠多个模块会变得计算上很昂贵。
- en: The second block is equivalent to the first, but it uses the so-called bottleneck
    layer. First, we use a 1×1 convolution to downsample the input volume depth (we
    discussed this in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),* Understanding
    Convolutional Networks*). Then, we apply a 3×3 (bottleneck) convolution to the
    reduced input. Finally, we expand the output back to the desired depth with another
    1×1 convolution. This layer is less computationally expensive than the first.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个块与第一个块相同，但它使用了所谓的瓶颈层。首先，我们使用1×1卷积来下采样输入体积的深度（我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，*理解卷积网络*中讨论了这一点）。然后，我们对缩小后的输入应用3×3（瓶颈）卷积。最后，我们使用另一个1×1卷积将输出恢复到所需的深度。该层的计算开销比第一个要小。
- en: The third block is the latest revision of the idea, published in 2016 by the
    same authors (*Identity Mappings in Deep Residual Networks*, [https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)).
    It uses pre-activations, and the batch normalization and the activation function
    come before the convolutional layer. This may seem strange at first, but thanks
    to this design, the skip connection path can run uninterrupted throughout the
    network. This is contrary to the other residual blocks, where at least one activation
    function is on the path of the skip connection. A combination of stacked residual
    blocks still has the layers in the right order.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个块是该思想的最新修订版，由同一作者于2016年发布（*深度残差网络中的身份映射*，[https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)）。它使用了预激活，批量归一化和激活函数位于卷积层之前。这一设计起初可能显得有些奇怪，但正是由于这种设计，跳跃连接路径能够在整个网络中不间断地运行。这与其他残差块不同，后者至少有一个激活函数处于跳跃连接的路径上。堆叠的残差块组合仍然保持了正确的层级顺序。
- en: The fourth block is the bottleneck version of the third layer. It follows the
    same principle as the bottleneck residual layer v1.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四个块是第三层的瓶颈版本。它遵循与瓶颈残差层v1相同的原则。
- en: 'In the following table, we can see the family of networks proposed by the authors
    of the paper:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们可以看到论文作者提出的网络系列：
- en: '![](img/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bf80ec8-ffb0-4f87-964a-3db43e6b09d0.png)'
- en: The family of the most popular residual networks. The residual blocks are represented
    by rounded rectangles
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的残差网络系列。残差块用圆角矩形表示
- en: 'Some of their properties are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它们的一些特性如下：
- en: They start with a 7×7 convolutional layer with stride 2, followed by 3×3 max-pooling.
    This layer also serves as a downsampling step—the rest of the network starts with
    a much smaller slice of 56×56, compared to 224×224 of the input.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们从一个7×7卷积层开始，步幅为2，接着是3×3的最大池化层。该层还充当了下采样步骤——网络的其余部分以一个更小的56×56切片开始，相较于输入的224×224。
- en: Downsampling in the rest of the network is implemented with a modified residual
    block with stride 2.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络其余部分的下采样是通过具有步幅2的修改残差块实现的。
- en: Average pooling downsamples the output after all residual blocks and before
    the 1,000-unit fully connected softmax layer.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均池化在所有残差块之后、1,000单元全连接softmax层之前进行下采样。
- en: The ResNet family of networks is popular not only because of their accuracy,
    but also because of their relative simplicity and the versatility of the residual
    blocks. As we mentioned, the input and output shape of the residual block can
    be the same due to the padding. We can stack residual blocks in different configurations
    to solve various problems with wide-ranging training set sizes and input dimensions.
    Because of this universality, we'll implement an example of ResNet in the next
    section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet系列网络不仅因其准确性而流行，还因为它们相对简单，并且残差块具有高度的通用性。正如我们之前提到的，残差块的输入和输出形状可以相同，这是由于填充操作。我们可以以不同的配置堆叠残差块，以解决各种问题，适应不同大小的训练集和输入维度。正因为这种通用性，我们将在下一节中实现一个ResNet示例。
- en: Implementing residual blocks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现残差块
- en: 'In this section, we''ll implement a pre-activation ResNet to classify the CIFAR-10
    images using PyTorch 1.3.1 and `torchvision` 0.4.2\. Let''s start:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个预激活ResNet，用于使用PyTorch 1.3.1和`torchvision` 0.4.2分类CIFAR-10图像。让我们开始吧：
- en: 'As usual, we''ll start with the imports. Note that we''ll use the shorthand
    `F` for the PyTorch functional module ([https://pytorch.org/docs/stable/nn.html#torch-nn-functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)):'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，我们从导入库开始。请注意，我们将使用PyTorch功能模块的简写`F`（[https://pytorch.org/docs/stable/nn.html#torch-nn-functional](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)）：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, let''s define the pre-activation regular (non-bottleneck) residual block.
    We''ll implement it as `nn.Module`—the base class for all neural network modules.
    Let''s start with the class definition and the `__init__` method:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义预激活常规（非瓶颈）残差块。我们将其实现为`nn.Module`——所有神经网络模块的基类。我们从类定义和`__init__`方法开始：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will define only the learnable block components in the `__init__` method—these
    include the convolution and batch normalization operations. Also, note the way
    we implement the `shortcut` connection. If the input dimensions are the same as
    the output dimensions, we can directly use the input tensor as the shortcut. However,
    if the dimensions differ, we have to transform the input with the help of a 1×1
    convolution with the same stride and output channels as the one in the main path.
    The dimensions may differ either by height/width (`stride != 1`) or by depth (`in_slices
    != self.expansion * slices`). `self.expansion` is a hyper-parameter, which was
    included in the original ResNet implementation. It allows us to expand the output
    depth of the residual block.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`__init__`方法中仅定义可学习的块组件——这些组件包括卷积和批量归一化操作。另外，请注意我们如何实现`shortcut`连接。如果输入维度与输出维度相同，我们可以直接使用输入张量作为捷径连接。然而，如果维度不同，我们必须借助一个1×1卷积进行转换，卷积的步幅和输出通道与主路径中的卷积相同。维度的不同可能来源于高度/宽度（`stride
    != 1`）或者深度（`in_slices != self.expansion * slices`）。`self.expansion`是一个超参数，原始ResNet实现中包含了该参数，它允许我们扩展残差块的输出深度。
- en: 'The actual data propagation is implemented in the `forward` method (please
    mind the indentation, as it''s a member of `PreActivationBlock`):'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际的数据传播在`forward`方法中实现（请注意缩进，因为它是`PreActivationBlock`的成员）：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the functional `F.relu` for the activation function, as it doesn't have
    learnable parameters. Then, if the shortcut connection is a convolution and not
    an identity (that is, the input/output dimensions of the block differ), we'll
    reuse `F.relu(self.bn_1(x))` to add non-linearity and batch normalization to the
    shortcut. Otherwise, we'll just repeat the input.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用函数式的`F.relu`作为激活函数，因为它没有可学习的参数。然后，如果捷径连接是卷积而不是恒等（也就是说，块的输入/输出维度不同），我们会重用`F.relu(self.bn_1(x))`来为捷径连接增加非线性和批量归一化。否则，我们只需重复输入。
- en: 'Then, let''s implement the bottleneck version of the residual block. We''ll
    use the same blueprint as in the non-bottleneck implementation. We''ll start with
    the class definition and the `__init__` method:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，让我们实现残差块的瓶颈版本。我们将使用与非瓶颈实现相同的蓝图。我们从类定义和`__init__`方法开始：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `expansion` parameter is `4` after the original implementation. The `self.conv_1`
    convolution operation represents the 1×1 downsampling bottleneck connection, `self.conv_2`
    is the actual convolution, and `self.conv_3` is the upsampling 1×1 convolution.
    The shortcut mechanism follows the same logic as in `PreActivationBlock`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`expansion`参数在原始实现中为`4`。`self.conv_1`卷积操作表示1×1下采样瓶颈连接，`self.conv_2`是实际的卷积，`self.conv_3`是上采样的1×1卷积。捷径机制遵循与`PreActivationBlock`中相同的逻辑。'
- en: 'Next, let''s implement the `PreActivationBottleneckBlock.forward` method. Once
    again, it follows the same logic as the one in `PreActivationBlock`:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现`PreActivationBottleneckBlock.forward`方法。它的逻辑与`PreActivationBlock`中的方法相同：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, let''s implement the residual network itself. We''ll start with the class
    definition (it inherits `nn.Module`) and the `__init__` method:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现残差网络本身。我们将从类定义开始（它继承自`nn.Module`）和`__init__`方法：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The network contains four groups of residual blocks, just like the original
    implementation. The number of blocks of each group is specified by the `num_blocks`
    parameter. The initial convolution uses a 3×3 filter with stride 1, as opposed
    to a 7×7 with stride 2 of the original implementation. This is because the 32×32 CIFAR-10
    images are much smaller than the 224×224 ImageNet ones, and the downsampling is
    unnecessary.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 网络包含四组残差块，和原始实现一样。每组中的块数由`num_blocks`参数指定。初始卷积使用3×3的滤波器，步幅为1，而原始实现中使用的是7×7，步幅为2。这是因为32×32的CIFAR-10图像比224×224的ImageNet图像要小得多，因此不需要下采样。
- en: 'Then, we''ll implement the `PreActivationResNet._make_group` method, which
    creates one residual block group. All blocks in the group have stride 1, except
    for the first, where `stride` is supplied as a parameter:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将实现`PreActivationResNet._make_group`方法，该方法创建一个残差块组。组中的所有块的步幅为1，只有第一个块的步幅由参数`stride`指定：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we''ll implement the `PreActivationResNet.forward` method, which propagates
    the data through the network. We can see the downsampling average pooling before
    the fully connected final layer:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`PreActivationResNet.forward`方法，该方法通过网络传播数据。我们可以看到全连接层前的下采样平均池化：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we''re done with the network, we can implement several ResNet configurations.
    The following is `ResNet34` with 34 convolution layers, grouped in `[3, 4, 6,
    3]` non-bottleneck residual blocks:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们完成了网络的构建，就可以实现多种ResNet配置。以下是`ResNet34`，它有34层卷积层，分组为`[3, 4, 6, 3]`非瓶颈残差块：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we can train the network. We''ll start by defining the train and test
    datasets. We won''t go into much detail about the implementation, as we''ve already
    looked at a similar scenario, in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*. We''ll augment the training set by padding
    the samples with four pixels, and then we''ll take random 32×32 crops out of it.
    The following is the implementation:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以训练网络。我们将首先定义训练和测试数据集。由于我们已经在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)
    *理解卷积网络*中看过类似的场景，这里不再详细讲解实现。我们将通过给样本填充四个像素来扩增训练集，然后从中随机裁剪出32×32的图像。以下是实现：
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we''ll instantiate the network model and the training parameters—cross-entropy
    loss and the Adam optimizer:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化网络模型和训练参数——交叉熵损失和Adam优化器：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now train the network for `EPOCHS` epochs. The `train_model`, `test_model`,
    and `plot_accuracy` functions are the same as the ones we defined in the *Implementing
    transfer learning with PyTorch* section of [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, and we won''t repeat their implementation
    here. The following is the code:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将网络训练`EPOCHS`轮。`train_model`、`test_model`和`plot_accuracy`函数与我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)
    *实现PyTorch迁移学习*部分中定义的相同，我们不会在此重复它们的实现。以下是代码：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And, in the following graph, we can see the test accuracy in 15 iterations
    (the training might take a while):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到15次迭代中的测试准确率（训练可能需要一些时间）：
- en: '![](img/ace8fd47-9786-401b-87fc-75cc65bba7e9.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ace8fd47-9786-401b-87fc-75cc65bba7e9.png)'
- en: ResNet34 CIFAR accuracy in 15 epochs
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet34 CIFAR在15轮训练中的准确率
- en: The code in this section is partially based on the pre-activation ResNet implementation
    in [https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码部分基于[https://github.com/kuangliu/pytorch-cifar](https://github.com/kuangliu/pytorch-cifar)中的预激活ResNet实现。
- en: In this section, we discussed the various types of ResNets, and then we implemented
    one with PyTorch. In the next section, we'll discuss Inception networks—yet another
    family of networks, which elevate the use of parallel connections to a new level.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们讨论了不同类型的ResNet，并用PyTorch实现了一个。在下一节中，我们将讨论Inception网络——另一类网络，它们将并行连接的使用提升到了一个新层次。
- en: Understanding Inception networks
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Inception网络
- en: Inception networks (*Going Deeper with Convolutions*, [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    were introduced in 2014, when they won the ImageNet challenge of that year (there
    seems to be a pattern here). Since then, the authors have released multiple improvements
    (versions) of the architecture.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Inception网络（*通过卷积深入*，[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）是在2014年提出的，当时它们赢得了当年的ImageNet挑战（这里似乎有个规律）。从那时起，作者们发布了该架构的多个改进版本。
- en: 'Fun fact: the name Inception comes in part from the **We need to go deeper**
    internet meme, related to the movie *Inception*.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的事实：Inception这个名字部分来源于**我们需要更深入**的网络迷因，这与电影*盗梦空间*有关。
- en: 'The idea behind Inception networks started from the basic premise that the
    objects in an image have different scales. A distant object might take up a small
    region of the image, but the same object, once nearer, might take up the majority
    of the image. This presents a difficulty for standard CNNs, where the neurons
    in the different layers have a fixed receptive field size as imposed on the input
    image. A regular network might be a good detector of objects at a certain scale,
    but could miss them otherwise. To solve this problem, the authors of the paper proposed
    a novel architecture: one composed of Inception blocks. An Inception block starts
    with a common input, and then splits it into different parallel paths (or towers).
    Each path contains either convolutional layers with a different-sized filter,
    or a pooling layer. In this way, we apply different receptive fields on the same
    input data. At the end of the Inception block, the outputs of the different paths
    are concatenated. In the next few sections, we''ll discuss the different variations
    of Inception networks.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Inception 网络背后的思想源于一个基本前提：图像中的物体具有不同的尺度。一个远处的物体可能占据图像的一个小区域，但当同一个物体靠近时，它可能占据图像的大部分。这对于标准的
    CNN 来说是一个难题，因为不同层中的神经元对输入图像的感受野大小是固定的。一个常规网络可能能很好地检测到某个尺度的物体，但在其他情况下可能会漏掉它们。为了解决这个问题，论文的作者提出了一种新型架构：由
    Inception 块组成的网络。Inception 块从一个共同的输入开始，然后将其分割成不同的并行路径（或塔）。每条路径包含不同大小滤波器的卷积层，或池化层。通过这种方式，我们对相同的输入数据应用不同的感受野。在
    Inception 块的末尾，不同路径的输出会被连接起来。在接下来的几个部分中，我们将讨论 Inception 网络的不同变种。
- en: Inception v1
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v1
- en: 'The following diagram shows the first version of the Inception block, part
    of the GoogLeNet network architecture ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)).
    GoogLeNet contains nine such Inception blocks:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Inception 块的第一个版本，它是 GoogLeNet 网络架构的一部分（[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）。GoogLeNet
    包含九个这样的 Inception 块：
- en: '![](img/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/532d7c33-b7ba-4743-a7f2-c44ef6b9c4e5.png)'
- en: Inception v1 block, inspired by https://arxiv.org/abs/1409.4842
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1 块，灵感来自于 [https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)
- en: 'The v1 block has four paths:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: v1 块有四条路径：
- en: 1×1 convolution, which acts as a kind of repeater to the input
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，作为输入的某种中继器
- en: 1×1 convolution, followed by a 3×3 convolution
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 3×3 卷积
- en: 1×1 convolution, followed by a 5×5 convolution
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1×1 卷积，后接 5×5 卷积
- en: 3×3 max pooling with stride 1
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3×3 最大池化，步幅为 1
- en: The layers in the block use padding in such a way that the input and the output
    have the same shape (but different depths). The padding is also necessary, because
    each path would produce an output with a different shape, depending on the filter
    size. This is valid for all versions of Inception blocks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该块中的层使用填充方式，使输入和输出具有相同的形状（但深度不同）。填充也是必要的，因为每条路径会根据滤波器大小产生不同形状的输出。这对所有版本的 Inception
    块都适用。
- en: The other major innovation of this Inception block is the use of downsampling
    1×1 convolutions. They are needed because the output of all paths is concatenated
    to produce the final output of the block. The result of the concatenation is an
    output with a quadrupled depth. If another Inception block followed the current
    one, its output depth would quadruple again. To avoid such exponential growth,
    the block uses 1×1 convolutions to reduce the depth of each path, which in turn
    reduces the output depth of the block. This makes it possible to create deeper
    networks, without running out of resources.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Inception 块的另一个主要创新是使用下采样的 1×1 卷积。它们是必要的，因为所有路径的输出会被连接起来生成该块的最终输出。连接的结果是输出深度增加了四倍。如果接下来的
    Inception 块继续跟随当前块，它的输出深度将再次增加四倍。为了避免这种指数增长，该块使用 1×1 卷积来减少每条路径的深度，从而降低该块的输出深度。这使得可以创建更深的网络，而不会耗尽资源。
- en: GoogLeNet also utilizes auxiliary classifiers—that is, it has two additional
    classification outputs (with the same groundtruth labels) at various intermediate
    layers. During training, the total value of the loss is a weighted sum of the auxiliary
    losses and the real loss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 还利用了辅助分类器——也就是说，它在不同的中间层有两个额外的分类输出（具有相同的真实标签）。在训练过程中，总的损失值是辅助损失和真实损失的加权和。
- en: Inception v2 and v3
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v2 和 v3
- en: Inception v2 and v3 were released together and propose several improvements
    over the original Inception block (*Rethinking the Inception Architecture for
    Computer Vision*, [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
    The first is the factorization of the 5×5 convolution in two stacked 3×3 convolutions.
    We discussed the advantages of this in the *Introduction to Visual Geometry Group* section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2和v3是一起发布的，并提出了相较于原始Inception块的若干改进（*重新思考计算机视觉中的Inception架构*，[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)）。第一个改进是将5×5卷积分解为两个堆叠的3×3卷积。我们在*视觉几何组简介*部分讨论了这种方法的优势。
- en: 'We can see the new Inception block in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到新的Inception块：
- en: '![](img/26826db8-c95e-41ee-a113-970221a8668e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26826db8-c95e-41ee-a113-970221a8668e.png)'
- en: Inception block A, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Inception块A，灵感来源于[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: 'The next improvement is the factorization of an *n*×*n* convolution in two
    stacked asymmetrical 1×*n* and *n*×1 convolutions. For example, we can split a
    single 3×3 convolution into two 1×3 and 3×1 convolutions, where the 3×1 convolution
    is applied over the output of the 1×3 convolution. In the first case, the filter
    size would be 3*3 = 9, while in the second case, we would have a combined size
    of (3*1) + (1*3) = 3 + 3 = 6, resulting in 33% efficiency, as seen in the following
    diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个改进是将*n*×*n*卷积分解为两个堆叠的不对称1×*n*和*n*×1卷积。例如，我们可以将单个3×3卷积分解为两个1×3和3×1卷积，其中3×1卷积应用于1×3卷积的输出。在第一种情况下，滤波器的大小为3*3
    = 9，而在第二种情况下，我们将得到（3*1）+（1*3）= 3 + 3 = 6的组合大小，从而实现了33%的效率，具体如图所示：
- en: '![](img/023a33c1-b7ee-4535-ab68-18ab39d21038.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/023a33c1-b7ee-4535-ab68-18ab39d21038.png)'
- en: Factorization of a 3×3 convolution in 1×3 and 3×1 convolutions. Inspired by https://arxiv.org/abs/1512.00567
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 3×3卷积的因式分解为1×3和3×1卷积。灵感来源于[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: 'The authors introduced two new blocks, which utilizes factorized convolutions.
    The first of these blocks (and the second in total) is equivalent of Inception
    block A:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作者引入了两个新块，这些块使用了因式分解卷积。第一个新块（也是第二个块）相当于Inception块A：
- en: '![](img/43e66db9-3ccb-4855-b1df-0890467a8f60.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43e66db9-3ccb-4855-b1df-0890467a8f60.png)'
- en: Inception block B. When n=3, it is equivalent to block A. Inspired by https://arxiv.org/abs/1512.00567
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Inception块B。当n=3时，它相当于块A。灵感来源于[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: 'The second (third in total) block is similar, but the asymmetrical convolutions
    are parallel, resulting in a higher output depth (more concatenated paths). The
    hypothesis here is that the more features (different filters) the network has,
    the faster it learns (we also discussed the need for more filters in [Chapter
    2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding Convolutional Networks*).
    On the other hand, the wider layers take more memory and computation time. As
    a compromise, this block is only used in the deeper part of the network, after
    the other blocks:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个（总共是第三个）块相似，但不对称的卷积是并行的，导致输出深度更高（更多的拼接路径）。这里的假设是，网络拥有的特征（不同的滤波器）越多，它学习得越快（我们也在[第二章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《理解卷积网络》中讨论了更多滤波器的需求）。另一方面，较宽的层会占用更多内存和计算时间。作为折中，这个块仅在网络的较深部分使用，在其他块之后：
- en: '![](img/e74a4669-e728-4122-ad8d-a5607c4ba63e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e74a4669-e728-4122-ad8d-a5607c4ba63e.png)'
- en: Inception block C, inspired by https://arxiv.org/abs/1512.00567
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Inception块C，灵感来源于[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: 'Using these new blocks, the authors proposed two new Inception networks: v2
    and v3\. Another major improvement in this version is the use of batch normalization,
    which was introduced by the same authors.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些新块，作者提出了两个新的Inception网络：v2和v3。此版本的另一个重大改进是采用批量归一化，由同一作者提出。
- en: Inception v4 and Inception-ResNet
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Inception v4和Inception-ResNet
- en: 'In the latest revision of Inception networks, the authors introduced three
    new streamlined Inception blocks that build upon the idea of the previous versions (*Inception-v4,
    Inception-ResNet and the Impact of Residual Connections on Learning*, [https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)).
    They introduced 7×7 asymmetric factorized convolutions, and average pooling instead
    of max pooling. More importantly, they created a residual/Inception hybrid network
    known as Inception-ResNet, where the Inception blocks also include residual connections.
    We can see the schematic of one such block in the following diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在最新修订版的Inception网络中，作者引入了三种新的精简版Inception块，这些块基于先前版本的思想（*Inception-v4，Inception-ResNet以及残差连接对学习的影响*，[https://arxiv.org/abs/1602.07261](https://arxiv.org/abs/1602.07261)）。他们引入了7×7的非对称因式分解卷积，并用平均池化代替最大池化。更重要的是，他们创建了一个残差/Inception混合网络，称为Inception-ResNet，其中Inception块也包含了残差连接。我们可以在下面的示意图中看到这样的一个块：
- en: '![](img/c073e89b-8b95-4fb6-b486-aaa22c26193b.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c073e89b-8b95-4fb6-b486-aaa22c26193b.png)'
- en: An Inception block with a residual skip connection
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 带有残差跳跃连接的Inception块
- en: In this section, we discussed different types of Inception networks and the
    different principles used in the various Inception blocks. Next, we'll talk about
    a newer CNN architecture, which takes the Inception concept to a new depth (or
    width, as it should be).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了不同类型的Inception网络以及在不同Inception块中使用的不同原理。接下来，我们将讨论一种更新的CNN架构，它将Inception概念带入了一个新的深度（或者宽度，正如它应该是的那样）。
- en: Introducing Xception
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Xception
- en: 'All Inception blocks so far start by splitting the input into several parallel
    paths. Each path continues with a dimensionality-reduction 1×1 cross-channel convolution,
    followed by regular cross-channel convolutions. On one hand, the 1×1 connection
    maps cross-channel correlations, but not spatial ones (because of the 1×1 filter
    size). On the other hand, the subsequent cross-channel convolutions map both types
    of correlations. Let''s recall that in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml), *Understanding
    Convolutional Networks*, we introduced **depthwise separable convolutions** (**DSC**),
    which combine the following two operations:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有Inception块都从将输入拆分成几个并行路径开始。每个路径接着进行维度缩减的1×1跨通道卷积，然后是常规的跨通道卷积。一方面，1×1连接映射跨通道相关性，但不映射空间相关性（因为1×1的滤波器大小）。另一方面，随后的跨通道卷积映射两种类型的相关性。让我们回忆一下在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，*理解卷积网络*中，我们介绍了**深度可分离卷积**（**DSC**），它结合了以下两种操作：
- en: '**A depthwise convolution**: In a depthwise convolution, a single input slice
    produces a single output slice, therefore it only maps spatial (and not cross-channel)
    correlations.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积**：在深度卷积中，单一的输入切片产生一个单一的输出切片，因此它仅映射空间（而非跨通道）相关性。'
- en: '**A 1×1 cross-channel convolution**: With 1×1 convolutions, we have the opposite,
    that is, they only map cross-channel correlations.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1×1跨通道卷积**：通过1×1卷积，我们得到的是相反的情况，即它们仅映射跨通道相关性。'
- en: 'The author of Xception (*Xception: Deep Learning with Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    argues that, in fact, we can think of DSC as an extreme (hence the name) version
    of an Inception block, where each depthwise input/output slice pair represents
    one parallel path. We have as many parallel paths as the number of input slices. The
    following diagram shows a simplified Inception block and its transformation to
    an Xception block:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Xception的作者（*Xception：深度学习与深度可分离卷积*，[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)）认为，事实上，我们可以把DSC看作是Inception块的极端版本（因此命名为Xception），其中每个深度输入/输出切片对代表一个并行路径。我们有与输入切片数量相等的并行路径。下图展示了一个简化的Inception块及其转化为Xception块的过程：
- en: '![](img/41ef8798-793d-4b06-8b80-d4fd09bf4d0c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41ef8798-793d-4b06-8b80-d4fd09bf4d0c.png)'
- en: 'Left: simplified Inception module. Right: Xception block. Inspired by https://arxiv.org/abs/1610.02357'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 左：简化的Inception模块。右：Xception块。灵感来源于[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)
- en: 'The Xception block and the DSC have two differences:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Xception块和DSC有两个不同点：
- en: In Xception, the 1×1 convolution comes first, instead of last as in DSC. But,
    these operations are meant to be stacked anyway, and we can assume that the order
    is of no significance.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Xception中，1×1卷积是首先进行的，而不是像DSC那样最后进行。不过，这些操作无论如何都会堆叠在一起，我们可以假设顺序并不重要。
- en: The Xception block uses ReLU activations after each convolution, while the DSC
    doesn't use non-linearity after the cross-channel convolution. According to the
    author's experiments, networks with absent non-linearity depthwise convolution
    converged faster and were more accurate.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception 块在每次卷积后使用 ReLU 激活，而 DSC 在通道间卷积后则不使用非线性激活。根据作者的实验，缺少非线性激活的深度卷积网络收敛更快且更准确。
- en: 'The following diagram depicts the architecture of the Xception network:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了 Xception 网络的架构：
- en: '![](img/c75638b5-9f2a-4871-874c-c3013d8f80ee.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c75638b5-9f2a-4871-874c-c3013d8f80ee.png)'
- en: 'From left to right: Entry flow; Middle flow, repeated eight times; Exit flow.
    Source: https://arxiv.org/abs/1610.02357'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从左到右：入口流；中间流，重复八次；出口流。来源： [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)
- en: 'It is built of linearly stacked DSCs and some of its properties are as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 它由线性堆叠的 DSC 组成，且其一些特性如下：
- en: The network contains 36 convolutional layers, structured into 14 modules, all
    of which have linear residual connections around them, except for the first and
    last modules. The modules are grouped in three sequential virtual flows—entry,
    middle, and exit.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络包含 36 个卷积层，结构分为 14 个模块，所有模块都有线性残差连接，除了第一个和最后一个模块。模块被分成三个顺序的虚拟流——入口流、中间流和出口流。
- en: Downsampling with 3×3 max pooling in the entry and exit flows; no downsampling
    in the middle flow; global average pooling before the fully connected layers.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在入口和出口流中使用 3×3 最大池化进行降采样；中间流不进行降采样；在全连接层之前使用全局平均池化。
- en: All convolutions and DSCs are followed by batch normalization.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有卷积和 DSC 都会在后面跟上批量归一化（Batch Normalization）。
- en: All DSCs have a depth multiplier of 1 (no depth expansion).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 DSC 的深度乘子为 1（没有深度扩展）。
- en: This section concludes the series of Inception-based models. In the next section,
    we'll focus on a special model, which prioritizes a small footprint and computational
    efficiency.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本节结束了基于 Inception 的模型系列。在下一节中，我们将关注一个特殊的模型，它优先考虑小的模型体积和计算效率。
- en: Introducing MobileNet
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 MobileNet
- en: 'In this section, we''ll discuss a lightweight CNN model called MobileNet (*MobileNetV2:
    Inverted Residuals and Linear Bottlenecks*, [https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)).
    We''ll focus on the second revision of this idea (MobileNetV1 was introduced in *MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications*, [https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '本节我们将讨论一种轻量级 CNN 模型，称为 MobileNet（*MobileNetV2: 倒置残差和线性瓶颈*，[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)）。我们将重点讨论该想法的第二个版本（MobileNetV1
    在 *MobileNets: 高效的卷积神经网络用于移动视觉应用* 中首次提出，[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)）。'
- en: MobileNet is aimed at devices with limited memory and computing power, such
    as mobile phones (the name kind of gives it away). To reduce its footprint, the
    network uses DSC, linear bottlenecks, and inverted residuals.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet 旨在面向内存和计算能力有限的设备，如手机（名字也透露了这一点）。为了减小网络的体积，它采用了 DSC、线性瓶颈和倒置残差结构。
- en: 'We are already familiar with DSC, so let''s discuss the other two:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经熟悉了 DSC，接下来我们讨论另外两个：
- en: '**Linear bottlenecks**: To understand this concept, we''ll quote the paper:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性瓶颈**：为了理解这一概念，我们引用论文中的内容：'
- en: '"Consider a deep neural network consisting of *n* layers *L[i]* each of which
    has an activation tensor of dimensions [![](img/684623c0-9307-4ec3-9630-699e64599285.png)].
    Throughout this section we will be discussing the basic properties of these activation
    tensors, which we will treat as containers of [![](img/d2c0077c-fecb-48f2-9e46-06527c760674.png)] "pixels"
    with *d[i]* dimensions. Informally, for an input set of real images, we say that
    the set of layer activations (for any layer *L[i]*) forms a "*manifold of interest"*.
    It has been long assumed that manifolds of interest in neural networks could be
    embedded in low-dimensional subspaces. In other words, when we look at all individual
    *d*-channel pixels of a deep convolutional layer, the information encoded in those
    values actually lie in some manifold, which can be embedded into a low-dimensional
    subspace."'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: “考虑一个由*n*层*L[i]*组成的深度神经网络，每一层都有一个维度为[![](img/684623c0-9307-4ec3-9630-699e64599285.png)]的激活张量。在本节中，我们将讨论这些激活张量的基本属性，我们将其视为包含[![](img/d2c0077c-fecb-48f2-9e46-06527c760674.png)]“像素”的容器，这些像素的维度为*d[i]*。非正式地说，对于一组真实图像输入，我们说该层激活集（对于任何一层*L[i]*）形成一个“*感兴趣的流形*”。长期以来，人们认为神经网络中的感兴趣流形可以嵌入到低维子空间中。换句话说，当我们查看深度卷积层的所有单独的*d*通道像素时，这些值编码的信息实际上位于某个流形中，可以嵌入到低维子空间。”
- en: One way to do this is with 1×1 bottleneck convolutions. But, the authors of
    the paper argue that if this convolution is followed by non-linearity like ReLU,
    this might lead to a loss of manifold information. If the ReLU input is larger
    than 0, then the output of this unit is equivalent to the linear transformation
    of the input. But, if the input is smaller, then the ReLU collapses and the information
    of that unit is lost. Because of this, MobileNet uses 1×1 bottleneck convolution
    without non-linear activation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用1×1瓶颈卷积。但论文的作者认为，如果这个卷积后接像ReLU这样的非线性激活，可能会导致流形信息的丢失。如果ReLU输入大于0，那么这个单元的输出相当于输入的线性变换。但如果输入较小，ReLU会崩溃，导致该单元的信息丢失。因此，MobileNet使用没有非线性激活的1×1瓶颈卷积。
- en: '**Inverted residuals**: In the *Residual networks* section, we introduced the
    bottleneck residual block, where the data flow in the non-shortcut path is **input
    -> 1×1 bottleneck conv -> 3×3 conv -> 1×1 unsampling conv**. In other words, it
    follows a **wide -> narrow -> wide** data representation. The authors argue that
    *the bottlenecks actually contain all the necessary information, while an expansion
    layer acts merely as an implementation detail that accompanies a non-linear transformation
    of the tensor*. Because of this, they propose having shortcut connections between
    the bottleneck connections instead.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**倒残差**：在*残差网络*部分，我们介绍了瓶颈残差块，其中非快捷路径中的数据流为**输入 -> 1×1瓶颈卷积 -> 3×3卷积 -> 1×1反采样卷积**。换句话说，它遵循**宽
    -> 窄 -> 宽**的数据表示。作者认为，*瓶颈实际上包含了所有必要的信息，而扩展层仅仅是一个实现细节，伴随着张量的非线性变换*。因此，他们提议在瓶颈连接之间使用快捷连接。'
- en: 'Based on these properties, the MobileNet model is composed of the following
    building blocks:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些属性，MobileNet模型由以下构建块组成：
- en: '![](img/f84ccba9-bb59-4d84-bcd9-3701fd3b9e62.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f84ccba9-bb59-4d84-bcd9-3701fd3b9e62.png)'
- en: 'Top: inverted residual block with stride 1\. Bottom: stride 2 block'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 上：带步幅为1的倒残差块。下：步幅为2的块。
- en: The model uses ReLU6 non-linearity: ReLU6 = min(max(input, 0),6). The maximum
    activation value is limited to 6—in this way, the non-linearity is more robust
    in low-precision floating-point computations. That's because 6 can take at most
    3 bits, leaving the rest for the floating-point portion of the number.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用ReLU6非线性：ReLU6 = min(max(输入，0)，6)。最大激活值限制为6——通过这种方式，非线性在低精度浮点计算中更加稳健。因为6最多可以占用3位，剩余的用于浮点数部分。
- en: Besides stride, the blocks are described by an expansion factor, *t*, which
    determines the expansion ratio of the bottleneck convolution.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 除了步幅外，这些块还通过扩展因子*t*进行描述，扩展因子决定了瓶颈卷积的扩展比。
- en: 'The following table shows the relationship between the input and output dimensions
    of the blocks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了块的输入和输出维度之间的关系：
- en: '![](img/deac5fcf-f98e-40f8-b066-5e2f656d142a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/deac5fcf-f98e-40f8-b066-5e2f656d142a.png)'
- en: The input and output dimensions relationship. Source: https://arxiv.org/abs/1801.04381
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出维度关系。来源：https://arxiv.org/abs/1801.04381
- en: In the preceding table, **h** and **w** are the input height and width, **s** is
    the stride, and **k** and **k'** are the input and output number of channels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前表中，**h**和**w**分别是输入的高度和宽度，**s**是步幅，**k**和**k'**是输入和输出的通道数。
- en: 'Finally, here is the full model architecture:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是完整的模型架构：
- en: '![](img/f4f0756c-e7fb-4b20-a32a-683e628747be.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4f0756c-e7fb-4b20-a32a-683e628747be.png)'
- en: The MobileNetV2 architecture. Source: https://arxiv.org/abs/1801.04381
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNetV2架构。来源：[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)
- en: Each line describes a group of one or more identical blocks, repeated *n* times.
    All layers in the same group have the same number, **c**, of output channels.
    The first layer of each sequence has a stride, **s**, and all others use stride
    1\. All spatial convolutions use 3 × 3 kernels. The expansion factor, **t**, is
    always applied to the input size, as described in the preceding table.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行描述了一组相同的块，重复*n*次。组内所有层具有相同的输出通道数，**c**。每个序列的第一层有步幅，**s**，其余层的步幅为1。所有空间卷积使用3×3的卷积核。扩展因子，**t**，总是应用于输入大小，如前表所述。
- en: The next model we'll discuss is a network model with a new type of building
    block, where all layers are interconnected.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要讨论的模型是一种具有新型构建块的网络模型，其中所有层是互相连接的。
- en: An introduction to DenseNets
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DenseNet简介
- en: 'DenseNet (*Densely Connected Convolutional Networks*, [https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)) try
    to alleviate the vanishing gradient problem and improve feature propagation, while
    reducing the number of network parameters. We''ve already seen how ResNets introduce
    residual blocks with skip connections to solve this. DenseNets take some inspiration
    from this idea and take it even further with the introduction of dense blocks.
    A dense block consists of sequential convolutional layers, where any layer has
    a direct connection to all subsequent layers. In other words, a network layer, *l*, will
    receive input, **x***[l]*, from all preceding network layers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet（*密集连接卷积网络*，[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)）旨在缓解梯度消失问题，改善特征传播，同时减少网络参数的数量。我们已经看到ResNet如何引入残差块并通过跳跃连接来解决这个问题。DenseNet从这个思路中汲取了一些灵感，并通过引入密集块将其进一步发展。一个密集块由连续的卷积层组成，其中每一层都与所有后续层直接连接。换句话说，一个网络层，*l*，将从所有前面的网络层接收输入，**x**[l]。
- en: '![](img/6c100639-f0e1-4f6e-8fd0-f7df5a9cacad.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c100639-f0e1-4f6e-8fd0-f7df5a9cacad.png)'
- en: 'Here, ![](img/72b34058-eb37-45f3-b58e-6714361f0560.png) are the **concatenated **output
    feature maps of the preceding network layers. This is unlike ResNets, where we
    combine different layers with the element-wise sum. *H[l]* is a composite function,
    which defines three types of DenseNet blocks (only two are displayed):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/72b34058-eb37-45f3-b58e-6714361f0560.png)是前面网络层的**拼接**输出特征图。这与ResNet不同，后者通过元素级加法组合不同层。*H[l]*是一个复合函数，它定义了三种类型的DenseNet块（这里只展示了两种）：
- en: '![](img/e47bdbbc-aaad-41b4-9371-2498f5bbe3c8.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e47bdbbc-aaad-41b4-9371-2498f5bbe3c8.png)'
- en: 'A dense block: the dimensionality-reduction layers (dashed lines) are part
    of the DenseNet-B architecture, while DenseNet-A doesn''t have them. DenseNet-C
    is not displayed'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一个密集块：降维层（虚线部分）是DenseNet-B架构的一部分，而DenseNet-A没有这些层。DenseNet-C未显示
- en: 'Let''s define them:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来定义它们：
- en: '**DenseNet-A**: This is the base block, where *H[l]* consists of batch normalization,
    followed by activation, and a 3×3 convolution:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DenseNet-A**：这是基础块，其中*H[l]*包括批量归一化，接着是激活函数，然后是3×3卷积：'
- en: '![](img/2934a1ea-ee3a-404e-8913-e08533fd1c18.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2934a1ea-ee3a-404e-8913-e08533fd1c18.png)'
- en: '**DenseNet-B**: The authors also introduced a second type of dense block, DenseNet-B,
    which applies a dimensionality-reduction 1×1 convolution after each concatenation:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DenseNet-B**：作者还引入了第二种类型的密集块DenseNet-B，在每次拼接之后应用一个降维的1×1卷积：'
- en: '*![](img/7f2113f2-433e-409a-bca9-51c15c338392.png)*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/7f2113f2-433e-409a-bca9-51c15c338392.png)*'
- en: '**DenseNet-C**: A further modification, which adds a downsampling 1×1 convolution
    after each dense block. The combination of B and C is referred to as DenseNet-BC.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DenseNet-C**：进一步修改，在每个密集块之后添加一个下采样的1×1卷积。B和C的组合被称为DenseNet-BC。'
- en: A dense block is specified by its number of convolutional layers and the output
    volume depth of each layer, which is called the **growth rate** in this context.
    Let's assume that the input of the dense block has a volume depth of *k[0]* and
    the output volume depth of each convolutional layer is *k*. Then, because of the
    concatenation, the input volume depth for the *l*-th layer will be *k[0]+k[x](l
    − 1)*. Although the later layers of a dense block have a large input volume depth
    (because of the many concatenations), DenseNets can work with growth rate values
    as low as 12, which reduces the total number of parameters. To understand why
    this works, let's think of the feature maps as the **collective knowledge** (or
    global state) of the network. Each layer adds its own *k* feature maps to this
    state, and the growth rate determines the amount of information the layer contributes
    to it. Because of the dense structure, the global state can be accessed from everywhere
    within the network (hence the term global). In other words, there is no need to
    replicate it from one layer to the next as in traditional network architectures,
    which allows us to start with a smaller number of feature maps.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个密集块由其卷积层的数量和每层输出的体积深度指定，在此上下文中称为 **增长率**。假设密集块的输入体积深度为 *k[0]*，每个卷积层的输出体积深度为
    *k*。然后，由于连接，*l* 层的输入体积深度将为 *k[0]+k[x](l − 1)*。尽管密集块的后续层有较大的输入体积深度（由于许多连接），DenseNet
    仍然可以使用低至 12 的增长率，这样可以减少总的参数数量。为了理解为什么这样有效，我们可以把特征图看作是网络的 **集体知识**（或全局状态）。每一层将自己的
    *k* 个特征图添加到这个状态中，增长率决定了每一层对该状态的贡献量。由于密集结构，网络内的任何地方都可以访问全局状态（因此被称为全局）。换句话说，与传统网络架构不同，不需要从一层复制到下一层，这使得我们可以从较少的特征图开始。
- en: To make concatenation possible, dense blocks use padding in such a way that
    the height and width of all output slices are the same throughout the block. But
    because of this, downsampling is not possible within a dense block. Therefore, a
    dense network consists of multiple sequential dense blocks, separated by downsampling
    pooling operations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使连接成为可能，密集块使用填充操作，确保所有输出切片的高度和宽度在整个块中保持一致。但由于这个原因，密集块内无法进行下采样。因此，密集网络由多个连续的密集块组成，块与块之间通过下采样池化操作分隔。
- en: 'The authors of the paper have proposed a family of DenseNets, whose overall
    architecture resembles ResNet:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者提出了一类 DenseNet 网络，其整体架构类似于 ResNet：
- en: '![](img/8eed3144-e67e-4370-ac24-9c13d1bdbcd1.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8eed3144-e67e-4370-ac24-9c13d1bdbcd1.png)'
- en: The family of DenseNet networks. Source: https://arxiv.org/abs/1608.06993
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 网络族。来源：[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)
- en: 'They have the following properties:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 它们具有以下特性：
- en: Start with a 7×7 stride 2 downsampling convolution.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 7×7 步幅为 2 的下采样卷积开始。
- en: A further downsampling 3×3 max pooling with stride 2.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步下采样 3×3 最大池化，步幅为 2。
- en: Four groups of DenseNet-B blocks. The family of networks differs by the number
    of dense blocks within each group.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四组 DenseNet-B 块。该网络族的区别在于每组内密集块的数量。
- en: Downsampling is handled by a transition layer of a 2×2 pooling operation with
    stride 2 between the dense groups.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下采样由过渡层处理，该层使用 2×2 池化操作，步幅为 2，位于密集块之间。
- en: The transition layer contains a further 1×1 bottleneck convolution to reduce
    the number of feature maps. The compression ratio of this convolution is specified
    by a hyper-parameter, *θ*, where *0* < *θ* ≤ *1*. If the number of input feature
    maps is *m*, then the number of output feature maps is *θm.*
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过渡层包含进一步的 1×1 瓶颈卷积，以减少特征图的数量。该卷积的压缩比由超参数 *θ* 指定，其中 *0* < *θ* ≤ *1*。如果输入特征图的数量为
    *m*，那么输出特征图的数量为 *θm*。
- en: The dense blocks end up with a 7×7 global average pooling, followed by a 1,000-unit
    fully connected softmax layer.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集块最终通过 7×7 全局平均池化，接着是一个包含 1,000 单元的全连接 softmax 层。
- en: The authors of DenseNet have also released an improved DenseNet model called
    MSDNet (*Multi-Scale Dense Networks for Resource Efficient Image Classification*, [https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)),
    which (as the name suggests) uses multi-scale dense blocks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 的作者还发布了一个改进版的 DenseNet 模型，称为 MSDNet（*多尺度密集网络用于资源高效的图像分类*，[https://arxiv.org/abs/1703.09844](https://arxiv.org/abs/1703.09844)），正如其名字所示，它使用了多尺度的密集块。
- en: With DenseNet, we conclude our discussion about conventional CNN architectures.
    In the next section, we'll discuss whether it's possible to automate the process
    of finding the optimal NN architecture.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 随着DenseNet的介绍，我们结束了对常规CNN架构的讨论。在下一节中，我们将讨论是否有可能自动化寻找最佳神经网络架构的过程。
- en: The workings of neural architecture search
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络架构搜索的工作原理
- en: The NN models we've discussed so far were designed by their authors. But, what
    if we could make the computer itself design the NN? Enter **neural architecture
    search** (**NAS**)—a technique that automates the design of NNs.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至今讨论的神经网络（NN）模型都是由其作者设计的。但是，如果我们能让计算机自己设计神经网络呢？这就是**神经网络架构搜索**（**NAS**）——一种自动化设计神经网络的技术。
- en: 'Before we continue, let''s see what the network architecture consists of:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先看看网络架构包含了什么：
- en: The graph of operations, which represents the network. As we discussed in [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and bolts of Neural
    Networks*, the operations include (but are not limited to) convolutions, activation
    functions, fully connected layers, normalization, and so on.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作图，表示网络。如我们在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中讨论的，*神经网络的基本原理*，操作包括（但不限于）卷积、激活函数、全连接层、归一化等。
- en: 'The parameters of each operation. For example, the convolution parameters are:
    type (cross-channel, depthwise, and so on), input dimensions, number of input
    and output slices, stride, and padding.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个操作的参数。例如，卷积的参数包括：类型（跨通道卷积、深度卷积等）、输入维度、输入和输出切片的数量、步幅和填充。
- en: In this section, we'll discuss gradient-based NAS with reinforcement learning
    (*Neural Architecture Search with Reinforcement Learning*, [https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)).
    At this point, we won't discuss reinforcement learning, and we'll focus on the
    algorithm instead. It starts with the premise that we can represent the network
    definition as a string (a sequence of tokens). Let's assume that we'll generate
    a sequential CNN, which consists only of convolutions.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论基于梯度的NAS与强化学习（*使用强化学习的神经网络架构搜索*，[https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)）。在这一部分，我们不会讨论强化学习，而是专注于算法本身。其前提是我们可以将网络定义表示为一个字符串（令牌序列）。假设我们将生成一个只包含卷积的顺序CNN。
- en: 'Then, part of the string definition will look like this:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，字符串定义的部分将如下所示：
- en: '![](img/dd4ee96f-aa67-4c50-b853-a98045890dbd.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd4ee96f-aa67-4c50-b853-a98045890dbd.png)'
- en: We don't have to specify the layer type, because we only use convolutions. We
    exclude padding for the sake of simplicity. The subscript text on the first line
    is included for clarity, but won't be included in the algorithmic version.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必指定层类型，因为我们只使用卷积。为了简化起见，我们排除了填充操作。第一行的下标文本是为了说明，但在算法版本中不会包含。
- en: 'We can see the algorithm overview in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图表中看到算法概述：
- en: '![](img/9c0eb578-01b6-4814-881f-79cb79301681.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c0eb578-01b6-4814-881f-79cb79301681.png)'
- en: NAS overview. Source: https://arxiv.org/abs/1611.01578
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: NAS概述。来源：[https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)
- en: Let's start with the controller. It is an RNN, whose task is to generate new
    network architectures. Although we haven't yet discussed RNNs (this honor won't
    come until [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding
    Recurrent Networks*), we'll try to explain how it works nevertheless. In [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of Neural
    Networks*, we mentioned that an RNN maintains an internal state—a summary of all
    its previous inputs. Based on that internal state and the latest input sample,
    the network generates a new output, updates its internal state, and waits for
    the next input.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从控制器开始。它是一个RNN，任务是生成新的网络架构。虽然我们还没有讨论RNN（这个话题要等到[第7章](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml)，*理解循环网络*），但我们仍会尝试解释它是如何工作的。在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基本原理*中，我们提到过RNN保持一个内部状态——所有先前输入的总结。基于该内部状态和最新的输入样本，网络生成新的输出，更新其内部状态，然后等待下一个输入。
- en: Here, the controller will generate the string sequence, which describes the
    network architecture. The controller output is a single token of the sequence.
    This could be filter height/width, stride width/height, or the number of output
    filters. The type of token depends on the length of the currently generated architecture.
    Once we have this token, we feed it back to the RNN controller as input. Then,
    the network generates the next token of the sequence.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，控制器将生成描述网络架构的字符串序列。控制器的输出是序列中的一个标记。这可能是滤波器的高度/宽度、步幅的宽度/高度，或输出滤波器的数量。标记的类型取决于当前生成的架构的长度。一旦得到这个标记，我们将它作为输入反馈给RNN控制器。然后，网络生成序列中的下一个标记。
- en: 'This process is depicted in the following diagram:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程在下图中有所描述：
- en: '![](img/36bb6544-b95c-41a3-879b-78160ed1fd28.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36bb6544-b95c-41a3-879b-78160ed1fd28.png)'
- en: Generating a network architecture with the RNN controller. The output token
    is fed back to the controller as input to generate the next token. Source: https://arxiv.org/abs/1611.01578
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN控制器生成网络架构。输出标记被反馈给控制器作为输入，以生成下一个标记。来源： [https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)
- en: The white vertical squares in the diagram represent the RNN controller, which
    consists of a two-layer **Long short-term memory** (**LSTM**) cell (along the
    *y*-axis). Although the diagram shows multiple instances of the RNN (along the
    *x*-axis), it is in fact the same network; it's just **unfolded** in time, to
    represent the process of sequence generation. That is, each step along the *x*-axis
    represents a single token of the network definition. A token prediction at step
    *t* is carried out by a softmax classifier and then fed as controller input at
    step *t+1*. We continue this process until the length of the generated network
    reaches a certain value. Initially, this value is small (a short network), but
    it gradually increases (a longer network) as the training progresses.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的白色垂直方块表示RNN控制器，包含一个两层**长短时记忆**（**LSTM**）单元（沿着*y*-轴）。尽管图中显示了多个RNN实例（沿着*x*-轴），但实际上它们是同一个网络；只是它在时间上被**展开**，以表示序列生成过程。也就是说，沿*x*-轴的每一步代表网络定义中的一个标记。第*t*步的标记预测是通过一个softmax分类器完成的，然后作为控制器输入传递到第*t+1*步。我们继续这个过程，直到生成的网络长度达到某个值。最初，这个值很小（一个短网络），但随着训练的进行，它会逐渐增加（变成一个长网络）。
- en: 'To better understand NAS, let''s see a step-by-step execution of the algorithm:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解NAS，我们来看一下算法的逐步执行：
- en: The controller generates a new architecture, *A*.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制器生成一个新的架构，*A*。
- en: It builds and **trains** a new network with said architecture until it converges.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它构建并**训练**一个新网络，使用所述架构直到它收敛。
- en: It tests the new network on a withheld part of the training set and measures
    the error, *R*.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在保留的训练集部分上测试新网络，并测量误差*R*。
- en: It uses this error to update the controller parameters, *θ[c]*. As our controller
    is RNN, this means training the network and adjusting its weights. The model parameters
    are updated in such a way as to reduce the error, *R*, of the future architectures.
    This is made possible by a reinforcement learning algorithm called REINFORCE,
    which is beyond the scope of this section.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它利用这个误差来更新控制器参数，*θ[c]*。由于我们的控制器是RNN，这意味着训练网络并调整其权重。模型参数会以减少未来架构误差*R*的方式进行更新。这个过程是通过一个名为REINFORCE的强化学习算法实现的，超出了本节的讨论范围。
- en: It repeats these steps until the error, *R*, of the generated network falls
    below a certain threshold.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它重复这些步骤，直到生成的网络的误差*R*降到某个阈值以下。
- en: The controller can generate network architectures with some restrictions. As
    we mentioned earlier in this section, the most severe is that the generated network
    only consists of convolutional layers. To simplify things, each convolutional
    layer automatically includes batch normalization and ReLU activation. But in theory,
    the controller could generate more complex architectures with other layers such
    as pooling or normalization. We could implement this by adding additional controller
    steps in the architecture sequence for the layer type.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器可以生成有一些限制的网络架构。如我们在本节前面提到的，最严格的限制是生成的网络仅由卷积层组成。为了简化，每个卷积层自动包括批量归一化和ReLU激活。但理论上，控制器可以生成更复杂的架构，包含其他层，如池化层或归一化层。我们可以通过在架构序列中添加额外的控制器步骤来实现这一点，以指定层的类型。
- en: The authors of the paper implemented a technique that allows us to add residual
    skip connections to the generated architecture. It works with a special type of
    controller step called an anchor point. The anchor point at layer *N* has content-based
    sigmoids. The output of a sigmoid *j (j = 1, 2, 3, ..., N-1)* represents the probability
    that the current layer has a residual connection to layer *j*.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者实现了一种技术，允许我们在生成的架构中添加残差跳跃连接。它通过一种特殊类型的控制步骤称为锚点来工作。第*N*层的锚点具有基于内容的Sigmoid函数。Sigmoid函数的输出
    *j (j = 1, 2, 3, ..., N-1)* 表示当前层是否与第*j*层存在残差连接的概率。
- en: 'The modified controller is depicted in the following diagram:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的控制器如下面的图所示：
- en: '![](img/6365b957-b098-48ef-b4fc-95a9c8460a5c.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6365b957-b098-48ef-b4fc-95a9c8460a5c.png)'
- en: 'RNN controller with anchor points for the residual connections. Source: https://arxiv.org/abs/1611.01578'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 带有锚点的RNN控制器，用于残差连接。来源：[https://arxiv.org/abs/1611.01578](https://arxiv.org/abs/1611.01578)
- en: 'If one layer has many input layers, all inputs are concatenated along the channel
    (depth) dimension. Skip connections could create some issues in the network design:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一层有多个输入层，所有输入会沿着通道（深度）维度进行拼接。跳跃连接可能会在网络设计中产生一些问题：
- en: The first hidden layer of the network (that is, the one not connected to any
    other input layer) uses the input image as an input layer.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的第一层隐藏层（即没有连接到任何其他输入层的层）使用输入图像作为输入层。
- en: At the end of the network, all layer outputs that have not been connected are
    concatenated in a final hidden state, which is sent to the classifier.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络的末端，所有未连接的层输出被拼接到最终的隐藏状态中，该状态被发送到分类器。
- en: It may happen that the outputs to be concatenated have different sizes. In that
    case, the smaller feature maps are padded with zeros to match the size of the
    bigger ones.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会出现待拼接的输出具有不同大小的情况。在这种情况下，较小的特征图将用零进行填充，以匹配较大特征图的大小。
- en: In their experiment, the authors used a controller with a 2-layer LSTM cell
    with 35 units in each layer. For every convolution, the controller has to select
    a filter height and width from the values {1, 3, 5, 7}, and a number of filters
    to be one of {24, 36, 48, 64}. Additionally, they performed 2 sets of experiments—one
    where the controller was allowed to select strides in {1, 2, 3} and another with
    a fixed stride of 1.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的实验中，作者使用了一个具有2层LSTM单元的控制器，每层包含35个单元。对于每次卷积，控制器必须从{1, 3, 5, 7}中选择一个滤波器的高度和宽度，并从{24,
    36, 48, 64}中选择一个滤波器的数量。此外，他们进行了两组实验——一组允许控制器选择步幅为{1, 2, 3}，另一组则固定步幅为1。
- en: Once the controller generates an architecture, the new network is trained for
    50 epochs on 45,000 images of the CIFAR-10 dataset. The remaining 5,000 images
    are used for validation. During training, the controller starts with an architecture
    depth of 6 layers and then increases the depth by 2 layers on every 1,600 iterations. The
    best performing model has a validation accuracy of 3.65%. It was discovered after
    12,800 architectures using 800 GPUs (wow!). The reason for these steep computational
    requirements is that each new network is trained from scratch just to produce
    one accuracy value, which can then be used to train the controller. More recently,
    the new ENAS algorithm (*Efficient Neural Architecture Search via Parameter Sharing*, [https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268))
    has made it possible to significantly reduce the computational resources of NAS
    by sharing the weights among the generated models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦控制器生成了一个架构，新的网络将在CIFAR-10数据集的45,000张图像上训练50个epoch。剩余的5,000张图像用于验证。在训练过程中，控制器从一个6层的架构开始，并且每进行1,600次迭代后，架构的深度增加2层。表现最佳的模型的验证准确率为3.65%。在使用800个GPU（哇！）的12,800次架构后发现了这一点。这些巨大的计算需求的原因在于每一个新的网络都从头开始训练，仅仅为了生成一个准确率值，这个值可以用于训练控制器。最近，新的ENAS算法（*Efficient
    Neural Architecture Search via Parameter Sharing*，[https://arxiv.org/abs/1802.03268](https://arxiv.org/abs/1802.03268)）使得通过在生成的模型之间共享权重，显著减少了NAS的计算资源。
- en: In the next section, we'll discuss a novel type of NN, which tries to overcome
    some of the limitations of the CNNs we talked about so far.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一种新型神经网络，它试图克服我们至今讨论的卷积神经网络的一些局限性。
- en: Introducing capsule networks
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入胶囊网络
- en: Capsule networks (*Dynamic Routing Between Capsules*, [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)) were
    introduced as a way to overcome some of the limitations of standard CNNs. To understand
    the idea behind capsule networks, we need to understand their limitations first,
    which we will see in the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊网络（*胶囊间动态路由*，[https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)）作为一种克服标准CNN某些局限性的方式被提出。为了理解胶囊网络背后的理念，我们需要首先理解它们的局限性，这将在下一节中讨论。
- en: The limitations of convolutional networks
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络的局限性
- en: 'Let''s start with a quote from professor Hinton himself:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Hinton教授的一句名言开始：
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '"卷积神经网络中使用的池化操作是一个大错误，而它之所以能如此有效，正是灾难的根源。"'
- en: As we mentioned in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, CNNs are **translation-invariant**. Let's imagine a
    picture with a face, located in the right half of the picture. Translation invariance
    means that a CNN is very good at telling us that the picture contains a face,
    but it cannot tell us whether the face is in the left or right part of the image.
    The main culprit for this behavior is the pooling layers. Every pooling layer
    introduces a little translation invariance. For example, the max pooling routes
    forward the activation of only one of the input neurons, but the subsequent layers
    don't have any knowledge of which neuron is routed.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《理解卷积神经网络》中提到的，CNN是**平移不变**的。让我们假设有一张包含面孔的图片，面孔位于图片的右半部分。平移不变性意味着CNN非常擅长告诉我们图片中包含面孔，但它无法告诉我们面孔是在图像的左侧还是右侧。这种行为的主要原因是池化层。每一层池化都会引入一点平移不变性。例如，最大池化仅将输入神经元中的一个激活传递给下一层，但后续层并不知道是哪一个神经元被传递。
- en: By stacking multiple pooling layers, we gradually increase the receptive field
    size. But, the detected object could be anywhere in the new receptive field, because
    none of the pooling layers relay such information. Therefore, we also increase
    the translation invariance. At first, this might seem to be a good thing, because
    the final labels have to be translation-invariant. But, it poses a problem, as
    CNNs cannot identify the position of one object relative to another. A CNN would
    identify both of the following images as a face, because they both contain the
    components of a face (a nose, mouth, and eyes) regardless of their relative positions
    to one another.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通过堆叠多个池化层，我们逐渐增大了感受野的大小。但由于没有任何池化层传递这类信息，检测到的物体可能出现在新的感受野中的任何位置。因此，我们也增加了平移不变性。最初，这似乎是件好事，因为最终的标签必须是平移不变的。但是，这也带来了问题，因为CNN无法识别一个物体相对于另一个物体的位置。CNN会将下列两张图片都识别为面孔，因为它们都包含面孔的组成部分（鼻子、嘴巴和眼睛），无论它们之间的相对位置如何。
- en: 'This is also known as the **Picasso problem**, as demonstrated in the following
    diagram:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为**毕加索问题**，如下面的图示所示：
- en: '![](img/2b264a39-059f-4898-8bbc-b3d5b423b1ff.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b264a39-059f-4898-8bbc-b3d5b423b1ff.png)'
- en: A convolutional network would identify both of these images as a face
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一个卷积网络会将这两张图片都识别为面孔。
- en: But, that's not all. A CNN would be confused even if the face had a different **orientation**,
    for example, if it was turned upside down. One way to overcome this is with data
    augmentation (rotation) during training. But, this only shows the limitations
    of the network. We have to explicitly show the object in different orientations
    and tell the CNN that this is, in fact, the same object.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不是全部。即使面孔的**朝向**发生了变化，例如它被倒转了，CNN也会感到困惑。克服这一问题的一种方式是在训练过程中进行数据增强（旋转）。但这只展示了网络的局限性。我们必须明确地展示物体在不同朝向下的样子，并告诉CNN，这实际上是同一个物体。
- en: So far, we've seen that a CNN discards the translation information (transitional
    invariance) and doesn't understand the orientation of an object. In computer vision,
    the combination of translation and orientation is known as the **pose**. The pose
    is enough to uniquely identify the object's properties in the coordinate system. Let's
    use computer graphics to illustrate this. A 3D object, say a cube, is entirely
    defined by its pose and the edge length. The process of transforming the representation
    of a 3D object into an image on the screen is called rendering. Knowing just its
    pose and the edge length of the cube, we can render it from any point of view
    we like.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到，CNN 会丢弃平移信息（平移不变性），并且无法理解物体的方向。在计算机视觉中，平移和方向的结合被称为**姿势**。姿势足以在坐标系中唯一标识物体的特征。我们可以通过计算机图形学来说明这一点。一个三维物体，比如一个立方体，完全由其姿势和边长来定义。将三维物体的表示转换为屏幕上的图像的过程叫做渲染。只知道立方体的姿势和边长，我们就可以从任何我们喜欢的角度渲染它。
- en: Therefore, if we can somehow train a network to understand these properties,
    we won't have to feed it with multiple augmented versions of the same object.
    A CNN cannot do that, because its internal data representation doesn't contain
    information about the object's pose (only about its type). In contrast, capsule
    networks **preserve information** for both the type and the pose of an object.
    Therefore, they can detect objects that can transform into each other, which is
    known as **equivariance**. We can also think of this as **reverse graphics**,
    that is, a reconstruction of the object's properties according to its rendered
    image.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们能够以某种方式训练网络来理解这些特性，我们就不需要为同一个物体提供多个增强版本。CNN 做不到这一点，因为它的内部数据表示不包含物体姿势的信息（仅包含物体的类型）。相反，胶囊网络**保留信息**，包括物体的类型和姿势。因此，它们能够检测出可以相互转换的物体，这就是所谓的**等变性**。我们也可以把它看作是**反向图形学**，即根据物体的渲染图像重建物体的特性。
- en: To solve these problems, the authors of the paper propose a new type of network
    building block, called a **capsule**, instead of the neuron. Let's discuss it
    in the next section.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，论文的作者提出了一种新的网络构建块，叫做**胶囊**，代替传统的神经元。我们将在下一节讨论这个概念。
- en: Capsules
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊
- en: 'The output of a capsule is a vector, compared to the output of a neuron, which
    is a scalar value. The capsule output vector carries the following meaning:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊的输出是一个向量，而神经元的输出是一个标量值。胶囊输出的向量承载以下含义：
- en: The elements of the vector represent the pose and other properties of the object.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的元素表示物体的姿势和其他特性。
- en: The length of the vector is in the (0, 1) range and represents the probability
    of detecting the feature at that location. As a reminder, the length of a vector
    is [![](img/0164c9bd-513d-44ac-a282-cf9fc9a2293c.png)], where *v[i]* are the vector
    elements.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的长度位于（0, 1）范围内，表示在该位置检测到特征的概率。提醒一下，向量的长度是 [![](img/0164c9bd-513d-44ac-a282-cf9fc9a2293c.png)]，其中
    *v[i]* 是向量元素。
- en: Let's consider a capsule that detects faces. If we start moving a face across
    an image, the values of the capsule vector will change to reflect the change in
    the position. However, its length will always stay the same, because the probability
    of the face doesn't change with the location.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个用于检测面部的胶囊。如果我们开始在图像中移动面部，胶囊向量的值将发生变化，以反映位置的变化。然而，向量的长度始终保持不变，因为面部的概率与位置无关。
- en: Capsules are organized in interconnected layers, just like a regular network. The
    capsules in one layer serve as input to the capsules in the next. And, like a
    CNN, the shallower layers detect basic features, and the deeper layers combine
    them in more abstract and complex ones. But now, the capsules also relay positional
    information, instead of just detected objects. This allows the deeper capsules
    to analyze not only the presence of features, but also their relationship. For
    example, a capsule layer may detect a mouth, face, nose, and eyes. The subsequent
    capsule layer will be able to not only verify the presence of these features,
    but also whether they have the correct spatial relationship. Only if both conditions
    are true can the subsequent layer verify that a face is present. This is a high-level
    overview of capsule networks. Now, let's see how exactly capsules work.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊被组织成互联的层，就像一个常规网络一样。一个层中的胶囊作为输入传递给下一个层的胶囊。而且，像CNN一样，浅层检测基本特征，深层则将它们组合成更抽象和复杂的特征。但现在，胶囊还传递位置相关信息，而不仅仅是检测到的对象。这使得深层胶囊不仅能够分析特征的存在，还能分析它们之间的关系。例如，一个胶囊层可能会检测到嘴巴、面部、鼻子和眼睛。随后胶囊层不仅能验证这些特征的存在，还能验证它们是否具有正确的空间关系。只有在这两个条件都满足时，后续的层才能验证面部的存在。这是胶囊网络的高级概述。现在，让我们看看胶囊到底是如何工作的。
- en: 'We can see the schematic of a capsule in the following screenshot:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下图中看到一个胶囊的示意图：
- en: '![](img/17593e60-cd8e-4ffd-8916-b875625f79c7.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17593e60-cd8e-4ffd-8916-b875625f79c7.png)'
- en: A capsule
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个胶囊
- en: 'Let''s analyze it in the following steps:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分以下几个步骤来分析：
- en: The capsule inputs are the output vectors, **u***[1],* **u***[2], ...* **u***[n]*,
    from the capsules of the previous layer.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 胶囊的输入是来自上一层胶囊的输出向量，**u***[1]*、**u***[2]*、...、**u***[n]*。
- en: We multiply each vector, **u***[i]*, by its corresponding weight matrix, *W[ij]*,
    to produce **pred****iction vectors**, [![](img/af47eced-a6b2-4d2c-9b72-529fc22462c1.png)].
    The weight matrices, **W**, encode spatial and other relationships between the
    lower-level features, coming from the capsules of the previous layer, and the
    high-level ones in the current layer. For example, imagine that the capsule in
    the current layer detects faces and the capsules from the previous layer detect
    the mouth (**u***[1]*), eyes (**u***[2]*), and nose (**u***[3]*). Then, [![](img/b4c26187-9be6-4b8e-8eb0-bc91e7ad1715.png)] is
    the predicted position of the face, given where the location of the mouth is.
    In the same way, [![](img/dcaec96e-5891-4d32-9a83-430790d1ff04.png)] predicts
    the location of the face based on the detected location of the eyes, and [![](img/309fb5d1-833a-4b43-b136-62fdd584760a.png)] predicts
    the location of the face based on the location of the nose. If all three lower-level
    capsule vectors agree on the same location, then the current capsule can be confident
    that a face is indeed present. We only used location for this example, but the
    vectors could encode other types of relationships between the features, such as
    scale and orientation. The weights, **W**, are learned with backpropagation.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将每个向量，**u***[i]*，与其对应的权重矩阵，*W[ij]*，相乘，得到**预测向量**，[![](img/af47eced-a6b2-4d2c-9b72-529fc22462c1.png)]。权重矩阵，**W**，编码了来自上一层胶囊的低层特征与当前层的高层特征之间的空间关系和其他关系。例如，假设当前层的胶囊检测的是面部，而上一层的胶囊分别检测到嘴巴（**u***[1]*）、眼睛（**u***[2]*）和鼻子（**u***[3]*）。那么，[![](img/b4c26187-9be6-4b8e-8eb0-bc91e7ad1715.png)]
    就是根据嘴巴的位置预测的面部位置。同样，[![](img/dcaec96e-5891-4d32-9a83-430790d1ff04.png)] 根据眼睛的位置预测面部的位置，而
    [![](img/309fb5d1-833a-4b43-b136-62fdd584760a.png)] 则根据鼻子的位置预测面部的位置。如果三个低层胶囊向量都在相同的位置达成一致，那么当前的胶囊就可以确信面部确实存在。我们在这个例子中只使用了位置，但向量还可以编码其他特征之间的关系，如尺度和方向。权重**W**通过反向传播进行学习。
- en: Next, we multiply the [![](img/2970c647-8b48-4b8e-960d-c8616516740c.png)] vectors
    by the scalar coupling coefficients, *c[ij]*. These coefficients are a separate
    set of parameters, apart from the weight matrices. They exist between any two
    capsules, and indicate which high-level capsules will receive input from a lower-level
    capsule. But, unlike weight matrices, which are adjusted via backpropagation,
    coupling coefficients are computed on the fly during the forward pass via a process
    called **dynamic routing**. We'll describe it in the next section.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将[![](img/2970c647-8b48-4b8e-960d-c8616516740c.png)]向量与标量耦合系数*c[ij]*相乘。这些系数是与权重矩阵不同的一组独立参数。它们存在于任何两个胶囊之间，并指示哪个高层胶囊将接收来自低层胶囊的输入。但与通过反向传播调整的权重矩阵不同，耦合系数是在前向传播过程中通过称为**动态路由**的过程动态计算的。我们将在下一节中描述它。
- en: 'Then, we perform the sum of the weighted input vectors. This step is similar to
    the weighted sum in neurons, but with vectors:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们执行加权输入向量的求和。此步骤类似于神经元中的加权和，但它处理的是向量：
- en: '![](img/ba2e77a3-d557-4965-a7cd-c66f0513762e.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba2e77a3-d557-4965-a7cd-c66f0513762e.png)'
- en: 'Finally, we''ll compute the output of the capsule, **v***[j]*, by squashing
    the vector, **s***[j]*. In this context, squashing means transforming the vector
    in such a way that its length comes in the (0, 1) range, without changing its
    direction. As mentioned, the length of the capsule vector represents the probability
    of the detected feature and squashing it in the (0, 1) range reflects that. To
    do this, the authors propose a novel formula:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将通过压缩向量**s***[j]*来计算胶囊的输出**v***[j]*。在这个上下文中，压缩意味着以一种方式转换向量，使其长度处于（0, 1）范围内，而不改变其方向。如前所述，胶囊向量的长度表示检测到的特征的概率，将其压缩到（0,
    1）范围内反映了这一点。为此，作者提出了一个新公式：
- en: '![](img/45aaa75e-ef3c-4427-8b4f-6ad373e78a0c.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45aaa75e-ef3c-4427-8b4f-6ad373e78a0c.png)'
- en: Now that we know the structure of the capsules, in the following section, we'll
    describe the algorithm to compute the coupling coefficients between capsules of
    different layers. That is, the mechanism by which they relay signals between one
    another.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了胶囊的结构，在接下来的部分，我们将描述计算不同层胶囊之间耦合系数的算法。也就是说，它们如何相互传递信号的机制。
- en: Dynamic routing
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态路由
- en: 'Let''s describe the dynamic routing process to compute the coupling coefficients, *c[ij]*,
    displayed in the following diagram:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述动态路由过程，以计算下图所示的耦合系数*c[ij]*：
- en: '![](img/3433e46b-78fc-42f1-a1f8-d187d0824843.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3433e46b-78fc-42f1-a1f8-d187d0824843.png)'
- en: Dynamic routing example. The grouped dots indicate lower-level capsules that
    agree with each other
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 动态路由示例。分组的点表示与彼此一致的低层胶囊
- en: We have a lower-level capsule, *I*, that has to decide whether to send its output
    to one of two higher-level capsules, *J* and *K*. The dark and light dots represent
    prediction vectors, ![](img/52b505fd-dae9-4348-8af6-c50be798ceab.png)and ![](img/d8f4d069-59c4-418d-8641-0644a4c1a5c5.png),
    which *J* and *K* have already received from other lower-level capsules. The arrows
    from the *I* capsule to the *J* and *K* capsules point to the ![](img/18952087-30c0-49a3-a1de-6641307e87f4.png) and
    ![](img/017be278-853c-4f1b-936c-0dfcbc6e49b3.png) prediction vectors from *I*
    to *J* and *K*. The clustered prediction vectors (lighter dots) indicate lower-level
    capsules that agree with each other with regard to the high-level feature. For
    example, if the *K* capsule describes a face, then the clustered predictions would
    indicate lower-level features, such as a mouth, nose, and eyes. Conversely, the
    dispersed (darker) dots indicate disagreement. If the *I* capsule predicts a vehicle
    tire, it would disagree with the clustered predictions in *K*.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个低层胶囊*I*，它必须决定是否将其输出发送到两个高层胶囊之一，*J*和*K*。深色和浅色的点分别代表预测向量，![](img/52b505fd-dae9-4348-8af6-c50be798ceab.png)和![](img/d8f4d069-59c4-418d-8641-0644a4c1a5c5.png)，这两个胶囊*J*和*K*已经从其他低层胶囊接收了这些向量。来自*I*胶囊的箭头指向*J*和*K*胶囊的![](img/18952087-30c0-49a3-a1de-6641307e87f4.png)和![](img/017be278-853c-4f1b-936c-0dfcbc6e49b3.png)预测向量。这些聚集的预测向量（较浅的点）表示与高层特征一致的低层胶囊。例如，如果*K*胶囊描述的是面部，那么聚集的预测将表示低层特征，如嘴巴、鼻子和眼睛。相反，分散的（较暗的）点表示不一致。如果*I*胶囊预测的是车胎，它将与*K*中的聚集预测不一致。
- en: However, if the clustered predictions in *J* represent features such as headlights,
    windshield, or fenders, then the prediction of *I* would be in agreement with
    them. The lower-level capsules have a way of determining whether they fall into
    the clustered or dispersed group of each higher-level capsule. If they fall into
    the clustered group, they will increase the corresponding coupling coefficient
    with that capsule and will route their vector in that direction. Conversely, if
    they fall into the dispersed group, the coefficient will decrease.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果聚类预测中的 *J* 代表特征，如车头灯、挡风玻璃或车翼，那么 *I* 的预测会与它们一致。低层胶囊有办法确定它们是否属于每个高层胶囊的聚类组或分散组。如果它们属于聚类组，它们将增加与该胶囊的耦合系数，并将它们的向量朝该方向路由。相反，如果它们属于分散组，则系数将减少。
- en: 'Let''s formalize this knowledge with a step-by-step algorithm, introduced by
    the authors:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个逐步的算法来形式化这些知识，算法由作者提出：
- en: For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer,
    we'll initialize [![](img/d0af0750-02cf-491b-9bcd-1965a7670f14.png)], where *b[ij]* is
    a temporary variable equivalent to *c[ij]*. The vector representation of all *b[ij]* is
    **b**[*i*]. At the start of the algorithm, the *i* capsule has an equal chance
    of routing its output to any of the capsules of the *(l + 1)* layer.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有在 *l* 层中的 *i* 胶囊和在 *(l + 1)* 层中的 *j* 胶囊，我们将初始化 [![](img/d0af0750-02cf-491b-9bcd-1965a7670f14.png)]，其中 *b[ij]* 是一个临时变量，相当于 *c[ij]*。所有 *b[ij]* 的向量表示为
    **b**[*i*]。在算法开始时， *i* 胶囊有相等的机会将输出路由到 *(l + 1)* 层的任一胶囊。
- en: 'Repeat for *r* iterations, where *r* is a parameter:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复进行 *r* 次迭代，其中 *r* 是一个参数：
- en: 'For all *i* capsules in the *l* layer: [![](img/ac9bd249-5a93-4e77-9f99-f9583cef4ac8.png)]. The
    sum of all outgoing coupling coefficients, *c[i]*, of a capsule amounts to 1 (they
    have a probabilistic nature), hence the softmax.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有在 *l* 层中的 *i* 胶囊：[![](img/ac9bd249-5a93-4e77-9f99-f9583cef4ac8.png)]。一个胶囊所有外部耦合系数的总和 *c[i]* 为1（它们具有概率性质），因此使用
    softmax。
- en: 'For all *j* capsules in the *(l + 1)* layer: [![](img/5c2be571-0a64-4f10-be1c-3c7574a40c67.png)].
    That is, we''ll compute all non-squashed output vectors of the *(l + 1)* layer.'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有在 *(l + 1)* 层中的 *j* 胶囊：[![](img/5c2be571-0a64-4f10-be1c-3c7574a40c67.png)]。也就是说，我们将计算 *(l
    + 1)* 层中所有未压缩的输出向量。
- en: 'For all *j* capsules in the *(l + 1)* layer, we''ll compute the squashed vectors:
    [![](img/c045dd46-4691-40a5-b186-a2ad864ccd04.png)].'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有在 *(l + 1)* 层中的 *j* 胶囊，我们将计算压缩向量：[![](img/c045dd46-4691-40a5-b186-a2ad864ccd04.png)]。
- en: 'For all *i* capsules in the *l* layer, and *j* capsules in the *(l + 1)* layer:
    [![](img/022f06a1-e7e3-446a-bbf5-8c55e5f72cfa.png)]. Here, [![](img/3768d012-866d-44cb-8efd-bb6d3210fa24.png)]is
    the dot product of the prediction vector of the low-level *i* capsule and the
    output vector of the high-level *j* capsule vectors. If the dot product is high,
    then the *i* capsule is in agreement with the other low-level capsules, which
    route their output to the *j* capsule, and the coupling coefficient increases.'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有在 *l* 层中的 *i* 胶囊和在 *(l + 1)* 层中的 *j* 胶囊：[![](img/022f06a1-e7e3-446a-bbf5-8c55e5f72cfa.png)]。这里，[![](img/3768d012-866d-44cb-8efd-bb6d3210fa24.png)]是低层 *i* 胶囊的预测向量与高层 *j* 胶囊输出向量的点积。如果点积较高，则说明 *i* 胶囊与其他低层胶囊一致，它们将输出传递给 *j* 胶囊，耦合系数增加。
- en: The authors have recently released an updated dynamic routing algorithm using
    a clustering technique called expectation-maximization. You can read more about
    it in the original paper,*Matrix capsules with EM routing* ([https://ai.google/research/pubs/pub46653](https://ai.google/research/pubs/pub46653)).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 作者最近发布了一个更新的动态路由算法，使用了一种名为期望最大化的聚类技术。你可以在原始论文《带有 EM 路由的矩阵胶囊》([https://ai.google/research/pubs/pub46653](https://ai.google/research/pubs/pub46653))中了解更多信息。
- en: The structure of the capsule network
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络的结构
- en: 'In this section, we''ll describe the structure of the capsule network, which
    the authors used to classify the MNIST dataset. The input of the network is the
    28×28 MNIST grayscale images, and the following are the steps:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将描述胶囊网络的结构，作者用它来对 MNIST 数据集进行分类。网络的输入是28×28的 MNIST 灰度图像，以下是步骤：
- en: We'll start with a single convolutional layer with 256 9×9 filters, stride 1,
    and ReLU activation. The shape of the output volume is (256, 20, 20).
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从一个卷积层开始，包含256个9×9的过滤器，步幅为1，激活函数为ReLU。输出体积的形状为（256, 20, 20）。
- en: We have another convolutional layer with 256 9×9 filters and stride 2\. The
    shape of the output volume is (256, 6, 6).
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有另一个卷积层，包含256个9×9的过滤器，步幅为2。输出体积的形状为（256, 6, 6）。
- en: Use the output of the layer as a foundation for the first capsule layer, called
    `PrimaryCaps`. Take the (256, 6, 6) output volume and split it into 32 separate
    (8, 6, 6) blocks. That is, each of the 32 blocks contains eight 6×6 slices. Take
    one activation value with the same coordinates from each slice and combine these
    values in a vector. For example, we can take activation (3, 7) of slice 1, (3,
    7) of slice 2, and so on and combine them in a vector with a length of 8\. We'll
    have 36 of these vectors. Then, we'll **transform** each vector into a capsule
    for a total of 36 capsules. The shape of the output volume of the `PrimaryCaps`
    layer is (32, 8, 6, 6).
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该层的输出作为第一个胶囊层的基础，称为 `PrimaryCaps`。将 (256, 6, 6) 的输出体积分成 32 个独立的 (8, 6, 6)
    块。也就是说，每个块包含 8 个 6×6 的切片。从每个切片中取一个具有相同坐标的激活值，并将这些值合并成一个向量。例如，我们可以取切片 1 的激活值 (3,
    7)，切片 2 的 (3, 7)，依此类推，并将它们合并成一个长度为 8 的向量。我们将得到 36 个这样的向量。然后，我们将**转换**每个向量为一个胶囊，总共得到
    36 个胶囊。`PrimaryCaps` 层的输出体积形状是 (32, 8, 6, 6)。
- en: The second capsule layer is called `DigitCaps`. It contains 10 capsules (1 per
    digit), whose output is a vector with length 16. The shape of the output volume
    of the `DigitCaps` layer is (10, 16). During inference, we compute the length
    of each `DigitCaps` capsule vector. We then take the capsule with the longest
    vector as the prediction result of the network.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个胶囊层称为 `DigitCaps`。它包含 10 个胶囊（每个数字一个），其输出是一个长度为 16 的向量。`DigitCaps` 层的输出体积形状是
    (10, 16)。在推理过程中，我们计算每个 `DigitCaps` 胶囊向量的长度。然后，我们取长度最长的胶囊向量作为网络的预测结果。
- en: During training, the network includes three additional, fully connected layers
    after `DigitCaps`, the last of which has 784 neurons (28×28). In the forward training
    pass, the longest capsule vector serves as input to these layers. They try to
    reconstruct the original image, starting from that vector. Then, the reconstructed
    image is compared to the original one and the difference serves as additional
    regularization loss for the backward pass.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，网络在 `DigitCaps` 后包括三个额外的全连接层，最后一层有 784 个神经元（28×28）。在前向训练过程中，最长的胶囊向量作为这些层的输入。它们尝试从该向量开始重建原始图像。然后，重建的图像与原始图像进行比较，差异作为反向传播过程中的额外正则化损失。
- en: Capsule networks are a new and promising approach to computer vision. However,
    they are not widely adopted yet and don't have an official implementation in any
    of the deep learning libraries discussed in this book, but you can find multiple
    third-party implementations.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊网络是一种新兴且有前景的计算机视觉方法。然而，它们目前尚未被广泛采用，并且在本书中讨论的任何深度学习库中都没有官方实现，但你可以找到多个第三方实现。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed some popular CNN architectures: we started with
    the classics, AlexNet and VGG. Then, we paid special attention to ResNets, as
    one of the most well-known network architectures. We also discussed the various
    incarnations of Inception networks and the Xception and MobileNetV2 models, which
    are related to them. We also talked about the exciting new ML area of neural architecture
    search. Finally, we discussed capsule networks—a new type of CV network, which
    tries to overcome some of the inherent CNN limitations.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了一些流行的 CNN 架构：首先介绍了经典的 AlexNet 和 VGG。接着，我们特别关注了 ResNet，因为它是最著名的网络架构之一。我们还讨论了
    Inception 网络的各种变种，以及与它们相关的 Xception 和 MobileNetV2 模型。我们还讲述了神经网络架构搜索这一令人兴奋的机器学习新领域。最后，我们讨论了胶囊网络——一种新的计算机视觉网络类型，它试图克服
    CNN 固有的某些局限性。
- en: We've already seen how to apply these models in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*, where we employed ResNet and MobileNet
    in a transfer learning scenario for a classification task. In the next chapter,
    we'll see how to apply some of them to more complex tasks such as object detection
    and image segmentation.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第 2 章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)中看到了如何应用这些模型，*理解卷积网络*，在该章中我们使用了
    ResNet 和 MobileNet 进行迁移学习，解决分类任务。在下一章中，我们将看到如何将其中一些模型应用于更复杂的任务，如目标检测和图像分割。
