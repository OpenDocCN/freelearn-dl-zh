- en: Data Access and Distributed Processing for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IoT 数据访问与分布式处理
- en: 'Data is everywhere: images, speech, text, weather information, the speed of
    your car, your last EMI, changing stock prices. With the integration of **Internet
    of Things** (**IoT**) systems, the amount of data produced has increased many-fold;
    an example is sensor readings, which could be taken for room temperature, soil
    alkalinity, and more. This data is stored and made available in various formats. In
    this chapter, we will learn how to read, save, and process data in some popular
    formats. Specifically, you will do the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在：图像、语音、文本、天气信息、你车速、你的最后一次 EMI、变动的股价。随着 **物联网**（**IoT**）系统的集成，产生的数据量大幅增加；例如，传感器读数可以用于房间温度、土壤碱度等。这些数据以各种格式存储并提供访问。在本章中，我们将学习如何读取、保存和处理一些流行格式的数据。具体来说，你将执行以下操作：
- en: Access data in TXT format
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 TXT 格式的数据
- en: Read and write csv-formatted data via the CSV, pandas, and NumPy modules
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 CSV、pandas 和 NumPy 模块读取和写入 csv 格式数据
- en: Access JSON data using JSON and pandas
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JSON 和 pandas 访问 JSON 数据
- en: Learn to work with the HDF5 format using PyTables, pandas, and h5py
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用 PyTables、pandas 和 h5py 处理 HDF5 格式
- en: Handle SQL databases using SQLite and MySQL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQLite 和 MySQL 处理 SQL 数据库
- en: Handle NoSQL using MongoDB
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MongoDB 处理 NoSQL 数据库
- en: Work with Hadoop's Distributed File System
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hadoop 的分布式文件系统
- en: TXT format
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TXT 格式
- en: One of the simplest and common formats for storing data is the TXT format; many
    IoT sensors log sensor readings with different timestamps in the simple `.txt`
    file format. Python provides built-in functions for creating, reading, and writing
    into TXT files.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 存储数据的最简单和常见格式之一是 TXT 格式；许多 IoT 传感器以简单的 `.txt` 文件格式记录传感器读取数据及其时间戳。Python 提供了内置函数，用于创建、读取和写入
    TXT 文件。
- en: We can access TXT files in Python itself without using any module; the data,
    in this case, is of the string type, and you will need to transform it to other
    types to use it. Alternatively, we can use NumPy or pandas.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Python 中直接访问 TXT 文件，而无需使用任何模块；在这种情况下，数据为字符串类型，你需要将其转换为其他类型以便使用。或者，我们可以使用
    NumPy 或 pandas。
- en: Using TXT files in Python
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中使用 TXT 文件
- en: Python has built-in functions that read and write into TXT files. The complete
    functionality is provided using four sets of functions: `open()`, `read()`, `write()`,
    and `close()`. As the names suggest, they are used to open a file, read from a
    file, write into a file, and finally close it. If you are dealing with string
    data (text), this is the best choice. In this section, we will use `Shakespeare` plays
    in TXT form; the file can be downloaded from the MIT site: [https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了内置函数，用于读取和写入 TXT 文件。完整的功能通过四组函数实现：`open()`、`read()`、`write()` 和 `close()`。顾名思义，它们分别用于打开文件、读取文件、写入文件和最终关闭文件。如果你处理的是字符串数据（文本），这是最佳选择。在本节中，我们将使用
    `Shakespeare` 的剧本，以 TXT 格式存储；文件可以从 MIT 网站下载：[https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt](https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt)。
- en: 'We define the following variables to access the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义以下变量来访问数据：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step here is to open the file:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一步是打开文件：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we read the whole file; we can use the `read `function, which will read
    the whole file as one single string:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取整个文件；我们可以使用 `read` 函数，它会将整个文件读取为一个单一的字符串：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This reads the whole file (consisting of 4,583,798 characters) into the `contents`
    variable. Let''s explore the contents of the `contents` variable; the following
    command will print the first `1000` characters:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这会将整个文件（包含 4,583,798 个字符）读取到 `contents` 变量中。让我们来查看 `contents` 变量的内容；以下命令将打印出前
    `1000` 个字符：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code will print the output as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出如下结果：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If the TXT files contain numeric data, it is better to use NumPy; if data is
    mixed, pandas is the best choice.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 TXT 文件包含数值数据，最好使用 NumPy；如果数据混合，pandas 是最佳选择。
- en: CSV format
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CSV 格式
- en: '**C****omma-separated value** (**CSV**) files are the most popular formats
    for storing tabular data generated by IoT systems. In a `.csv `file, the values
    of the records are stored in plain-text rows, with each row containing the values
    of the fields separated by a separator. The separator is a comma by default but
    can be configured to be any other character. In this section, we will learn how
    to use data from CSV files with Python''s `csv`, `numpy`, and `pandas` modules.
    We will use the `household_power_consumption` data file. The file can be downloaded
    from the following GitHub link: [https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv](https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv).
    To access the data files, we define the following variables:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**逗号分隔值**（**CSV**）文件是由物联网系统生成的存储表格数据的最流行格式。在 `.csv` 文件中，记录的值存储在纯文本行中，每行包含由分隔符分隔的字段值。默认情况下分隔符是逗号，但可以配置为任何其他字符。在本节中，我们将学习如何使用
    Python 的 `csv`、`numpy` 和 `pandas` 模块处理 CSV 文件中的数据。我们将使用 `household_power_consumption`
    数据文件。该文件可以从以下 GitHub 链接下载：[https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv](https://github.com/ahanse/machlearning/blob/master/household_power_consumption.csv)。为了访问数据文件，我们定义以下变量：'
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Generally, to quickly read the data from CSV files, use the Python `csv` module;
    however, if the data needs to be interpreted as a mix of date, and numeric data
    fields, it's better to use the pandas package. If the data is only numeric, NumPy
    is the most appropriate package.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，为了快速从 CSV 文件中读取数据，使用 Python 的 `csv` 模块；但是，如果数据需要被解释为日期和数字数据字段的混合，最好使用 pandas
    包。如果数据只是数字，NumPy 是最合适的包。
- en: Working with CSV files with the csv module
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `csv` 模块处理 CSV 文件
- en: 'In Python, the `csv` module provides classes and methods for reading and writing
    CSV files. The `csv.reader` method creates a reader object from which rows can
    be read iteratively. Each time a row is read from the file, the reader object
    returns a list of fields. For example, the following code demonstrates reading
    the data file and printing rows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，`csv` 模块提供了用于读写 CSV 文件的类和方法。`csv.reader` 方法从中创建一个可迭代地读取行的 reader
    对象。每次从文件中读取一行时，reader 对象返回一个字段列表。例如，以下代码演示了如何读取数据文件并打印行：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The rows are printed as a list of field values:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 行作为字段值列表打印：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `csv.writer` method returns an object that can be used to write rows to
    a file. As an example, the following code writes the first 10 rows of the file
    to a temporary file and then prints it:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`csv.writer` 方法返回一个对象，可以用来向文件写入行。例如，以下代码将文件的前 10 行写入临时文件，然后打印它：'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `delimiter` field and the `quoting `field characters are important attributes
    that you can set while creating `reader` and `writer` objects.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`delimiter` 字段和 `quoting` 字段字符是创建 `reader` 和 `writer` 对象时可以设置的重要属性。'
- en: 'By default, the `delimiter` field is `,` and the other delimiters are specified
    with the `delimiter` argument to the `reader` or `writer` functions. For example,
    the following code saves the file with `|` as `delimiter`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`delimiter` 字段为 `,`，其他分隔符可以通过 `reader` 或 `writer` 函数的 `delimiter` 参数指定。例如，以下代码将文件保存为
    `|` 作为 `delimiter`：
- en: '[PRE9]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you do not specify a `delimiter` character when the file is read, the rows
    will be read as one field and printed as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在读取文件时未指定 `delimiter` 字符，则行将作为一个字段读取并打印如下：
- en: '[PRE10]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`quotechar` specifies a character with which to surround fields. The `quoting`
    argument specifies what kind of fields can be surrounded with `quotechar`. The
    `quoting` argument can have one of the following values:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`quotechar` 指定用于包围字段的字符。`quoting` 参数指定哪种字段可以用 `quotechar` 包围。`quoting` 参数可以有以下值之一：'
- en: '`csv.QUOTE_ALL`: All the fields are quoted'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_ALL`：所有字段都加引号'
- en: '`csv.QUOTE_MINIMAL`: Only fields containing special characters are quoted'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_MINIMAL`：只有包含特殊字符的字段才加引号'
- en: '`csv.QUOTE_NONNUMERIC`: All non-numeric fields are quoted'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_NONNUMERIC`：所有非数值字段都加引号'
- en: '`csv.QUOTE_NONE`: None of the fields are quoted'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv.QUOTE_NONE`：所有字段都不加引号'
- en: 'As an example, let''s print the temp file first:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，让我们首先打印临时文件：
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s save it with all fields quoted:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用所有字段都加引号保存它：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The file gets saved with the specified quote character:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 文件将使用指定的引号字符保存：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Remember to read the file with the same arguments; otherwise, the `*` quote
    character will be treated as part of the field values and printed as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 记得使用相同的参数读取文件；否则，`*` 引号字符将被视为字段值的一部分，并打印如下：
- en: '[PRE14]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using the correct arguments with the `reader` object prints the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正确的参数与`reader`对象一起打印如下内容：
- en: '[PRE15]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now let's see how we can read CSV files with pandas, another popular Python
    library.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用pandas，另一个流行的Python库，来读取CSV文件。
- en: Working with CSV files with the pandas module
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas模块处理CSV文件
- en: 'In pandas, the `read_csv()` function returns a DataFrame after reading the
    CSV file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，`read_csv()`函数读取CSV文件后返回一个DataFrame：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The DataFrame is printed as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame将如下所示地打印：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We see in the preceding output that pandas automatically interpreted the `date`
    and `time` columns as their respective data types. The pandas DataFrame can be
    saved to a CSV file with the `to_csv()` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的输出中看到，pandas自动将`date`和`time`列解释为相应的数据类型。pandas的DataFrame可以通过`to_csv()`函数保存到CSV文件中：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'pandas, when it comes to reading and writing CSV files, offers plenty of arguments.
    Some of these are as follows, complete with how they''re used:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: pandas在读取和写入CSV文件时提供了很多参数。以下是其中的一些及其使用方法：
- en: '`header`: Defines the row number to be used as a header, or none if the file
    does not contain any headers.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`header`：定义要用作标题的行号，如果文件没有标题则为“无”。'
- en: '`sep`: Defines the character that separates fields in rows. By default, the
    value of `sep` is set to `,`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sep`：定义行中字段分隔符的字符。默认情况下，`sep`的值设置为`,`。'
- en: '`names`: Defines column names for each column in the file.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`names`：为文件中的每一列定义列名。'
- en: '`usecols`: Defines columns that need to be extracted from the CSV file. Columns
    that are not mentioned in this argument are not read.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`usecols`：定义需要从CSV文件中提取的列。没有在此参数中提到的列将不会被读取。'
- en: '`dtype`: Defines the data types for columns in the DataFrame.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`：定义DataFrame中各列的字段类型。'
- en: Many other available options are documented at the following links: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) and [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其他许多可用选项可以在以下链接中查阅：[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)
    和 [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html)。
- en: Now let's see how to read data from CSV files with the NumPy module.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何使用NumPy模块读取CSV文件中的数据。
- en: Working with CSV files with the NumPy module
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用NumPy模块处理CSV文件
- en: 'The NumPy module provides two functions for reading values from CSV files:
    `np.loadtxt()` and `np.genfromtxt()`.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy模块提供了两种从CSV文件读取值的函数：`np.loadtxt()`和`np.genfromtxt()`。
- en: 'An example of `np.loadtxt` is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.loadtxt`的一个示例如下：'
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code reads columns `3` and `4` from the file that we created
    earlier, and saves them in a 9 × 2 array as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码读取了我们之前创建的文件中的第`3`列和第`4`列，并将它们保存到一个9×2的数组中，如下所示：
- en: '[PRE20]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `np.loadtxt()` function cannot handle CSV files with missing data. For
    instances where data is missing, `np.genfromtxt()` can be used. Both of these
    functions offer many more arguments; details can be found in the NumPy documentation.
    The preceding code can be written using `np.genfromtxt()` as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.loadtxt()`函数无法处理缺失数据的CSV文件。对于缺失数据的情况，可以使用`np.genfromtxt()`。这两个函数都提供了许多其他参数；详细信息可以参考NumPy文档。以下代码可以使用`np.genfromtxt()`重写：'
- en: '[PRE21]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'NumPy arrays produced as a result of applying AI to IoT data can be saved with
    `np.savetxt()`. For example, the array we loaded previously can be saved as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作为应用AI于物联网数据后得到的NumPy数组，可以使用`np.savetxt()`保存。例如，之前加载的数组可以按如下方式保存：
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `np.savetxt()` function also accepts various other useful arguments, such
    as the format for saved fields and headers. Check the NumPy documentation for
    more details on this function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.savetxt()`函数也接受各种其他有用的参数，例如保存字段和标题的格式。有关此函数的更多详细信息，请查看NumPy文档。'
- en: CSV is the most popular data format on IoT platforms and devices. In this section,
    we learned how to read CSV data using three different packages in Python. Let's
    learn about XLSX, another popular format, in the next section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CSV是物联网平台和设备上最流行的数据格式。在本节中，我们学习了如何使用Python的三个不同包来读取CSV数据。在下一节中，我们将学习另一个流行格式——XLSX。
- en: XLSX format
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XLSX格式
- en: Excel, a component of the Microsoft Office pack, is one of the popular formats
    in which data is stored and visualized. Since 2010, Office has supported the `.xlsx` format.
    We can read XLSX files using the OpenPyXl and pandas functions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Excel 是 Microsoft Office 套件中的一个组件，是存储和可视化数据的流行格式之一。从 2010 年起，Office 开始支持 `.xlsx`
    格式。我们可以使用 OpenPyXl 和 pandas 函数来读取 XLSX 文件。
- en: Using OpenPyXl for XLSX files
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 OpenPyXl 处理 XLSX 文件
- en: 'OpenPyXl is a Python library for reading and writing Excel files. It is an
    open source project. A new `workbook` is created using the following command:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPyXl 是一个用于读取和写入 Excel 文件的 Python 库。它是一个开源项目。通过以下命令可以创建一个新的 `workbook`：
- en: '[PRE23]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can access the currently `active` sheet by using the following command:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下命令访问当前 `active` 工作表：
- en: '[PRE24]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To change the sheet name, use the `title` command:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改工作表名称，使用 `title` 命令：
- en: '[PRE25]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A single row can be added to the sheet using the `append` method:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `append` 方法将一行数据添加到工作表：
- en: '[PRE26]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'A new sheet can be created using the `create_sheet()` method. An individual
    cell in the active sheet can be created using the `column` and `row` values:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `create_sheet()` 方法创建一个新的工作表。可以通过 `column` 和 `row` 值在活动工作表中创建一个单独的单元格：
- en: '[PRE27]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: A workbook can be saved using the `save` method. To load an existing workbook,
    we can use the `load_workbook` method. The names of the different sheets in an
    Excel workbook can be accessed using `get_sheet_names()`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `save` 方法保存工作簿。要加载现有工作簿，我们可以使用 `load_workbook` 方法。可以通过 `get_sheet_names()`
    获取 Excel 工作簿中不同工作表的名称。
- en: 'The following code creates an Excel workbook with three sheets and saves it;
    later, it loads the sheet and accesses a cell. The code can be accessed from GitHub
    at `OpenPyXl_example.ipynb`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建一个包含三个工作表的 Excel 工作簿并保存；之后，它加载该工作表并访问一个单元格。代码可以在 GitHub 上通过 `OpenPyXl_example.ipynb`
    访问：
- en: '[PRE28]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can learn more about OpenPyXL from its documentation, available at [https://openpyxl.readthedocs.io/en/stable/](https://openpyxl.readthedocs.io/en/stable/).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过其文档了解更多关于 OpenPyXL 的信息，文档地址为 [https://openpyxl.readthedocs.io/en/stable/](https://openpyxl.readthedocs.io/en/stable/)。
- en: Using pandas with XLSX files
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 处理 XLSX 文件
- en: We can load existing `.xlsx` files with the help of pandas. The `read_excel`
    method is used to read Excel files as a DataFrame. This method uses an argument, `sheet_name`,
    which is used to specify the sheet we want to load. The sheet name can be specified
    either as a string or number starting from 0\. The `to_excel` method can be used
    to write into an Excel file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以借助 pandas 加载现有的 `.xlsx` 文件。`read_excel` 方法用于将 Excel 文件读取为 DataFrame。该方法使用一个参数
    `sheet_name`，用于指定我们要加载的工作表。工作表名称可以通过字符串或从 0 开始的数字来指定。`to_excel` 方法可以用来写入 Excel
    文件。
- en: 'The following code reads an Excel file, manipulates it, and saves it. The code
    can be accessed from GitHub at `Pandas_xlsx_example.ipynb`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码读取一个 Excel 文件，对其进行操作并保存。代码可以在 GitHub 上通过 `Pandas_xlsx_example.ipynb` 访问：
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Working with the JSON format
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理 JSON 格式
- en: '**JavaScript** **Object** **Notation **(**JSON**) is another popular data format
    in IoT systems. In this section, we will learn how to read JSON data with Python''s
    JSON, NumPy, and pandas packages.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaScript** **对象** **表示法**（**JSON**）是物联网系统中另一种流行的数据格式。在本节中，我们将学习如何使用 Python
    的 JSON、NumPy 和 pandas 包来读取 JSON 数据。'
- en: 'For this section, we will use the `zips.json` file, which contains US ZIP codes
    with city codes, geolocation details, and state codes. The file has JSON objects recorded
    in the following format:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 `zips.json` 文件，该文件包含美国 ZIP 代码、城市代码、地理位置详情和州代码。该文件中的 JSON 对象按以下格式记录：
- en: '[PRE30]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Using JSON files with the JSON module
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JSON 模块处理 JSON 文件
- en: 'To load and decode JSON data, use the `json.load()` or `json.loads() `functions.
    As an example, the following code reads the first 10 lines from the `zips.json`
    file and prints them nicely:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载和解码 JSON 数据，使用 `json.load()` 或 `json.loads()` 函数。作为示例，以下代码读取 `zips.json`
    文件中的前 10 行并美观地打印出来：
- en: '[PRE31]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The objects are printed as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象的打印结果如下：
- en: '[PRE32]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `json.loads()` function takes string objects as input while the `json.load()` function takes
    file objects as input. Both functions decode the JSON object and load it in the
    `json_data` file as a Python dictionary object.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`json.loads()` 函数接受字符串对象作为输入，而 `json.load()` 函数接受文件对象作为输入。两者都会解码 JSON 对象并将其加载为
    Python 字典对象，存储在 `json_data` 文件中。'
- en: The `json.dumps()` function takes an object and produces a JSON string, and
    the `json.dump()` function takes an object and writes the JSON string to the file.
    Thus, both these function do the opposite of the `json.loads()` and `json.load()`
    functions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`json.dumps()`函数接受一个对象并生成一个JSON字符串，而`json.dump()`函数接受一个对象并将JSON字符串写入文件。因此，这两个函数的作用正好与`json.loads()`和`json.load()`函数相反。'
- en: JSON files with the pandas module
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas模块操作JSON文件
- en: 'JSON strings or files can be read with the `pandas.read_json()` function, which returns
    a DataFrame or series object. For example, the following code reads the `zips.json`
    file:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: JSON字符串或文件可以使用`pandas.read_json()`函数读取，该函数返回一个DataFrame或Series对象。例如，以下代码读取`zips.json`文件：
- en: '[PRE33]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We set `lines=True` because each line contains a separate object in JSON format.
    Without this argument being set to `True`, pandas will raise `ValueError`. The
    DataFrame is printed as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置`lines=True`，因为每行包含一个单独的JSON格式对象。如果没有将这个参数设置为`True`，pandas会引发`ValueError`。DataFrame如下所示：
- en: '[PRE34]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To save the pandas DataFrame or series object to a JSON file or string, use
    the `Dataframe.to_json() `function.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要将pandas的DataFrame或Series对象保存到JSON文件或字符串中，可以使用`Dataframe.to_json()`函数。
- en: More information for both of these functions can be found at these links: [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html) and [https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数的更多信息可以在以下链接中找到：[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_json.html)和[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html)。
- en: 'While CSV and JSON remain the most popular data formats for IoT data, due to
    its large size, it is often necessary to distribute data. There are two popular distributed
    mechanisms for data storage and access: HDF5 and HDFS. Let''s first learn about
    the HDF5 format.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管CSV和JSON仍然是IoT数据中最受欢迎的数据格式，但由于其庞大的体积，通常需要分发数据。有两种流行的分布式数据存储和访问机制：HDF5和HDFS。让我们首先了解一下HDF5格式。
- en: HDF5 format
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDF5格式
- en: '**Hierarchical Data Format** (**HDF**) is a specification put together by the
    HDF Group, a consortium of academic and industry organizations ([https://support.hdfgroup.org/HDF5/](https://support.hdfgroup.org/HDF5/)).
    In HDF5 files, data is organized into groups and datasets. A group is a collection
    of **groups** or **datasets**. A dataset is a multidimensional homogeneous array.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次数据格式**（**HDF**）是由HDF集团（一个由学术界和行业组织组成的联盟）制定的规范（[https://support.hdfgroup.org/HDF5/](https://support.hdfgroup.org/HDF5/)）。在HDF5文件中，数据被组织成组和数据集。一个组是**组**或**数据集**的集合。数据集是一个多维的同质数组。'
- en: In Python, PyTables and h5py are two major libraries for handling HDF5 files.
    Both these libraries require HDF5 to be installed. For the parallel version of
    HDF5, a version of MPI is also required to be installed. Installation of HDF5
    and MPI is beyond the scope of this book. Installation instructions for parallel
    HDF5 can be found at the following link: [https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel](https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，PyTables和h5py是处理HDF5文件的两个主要库。这两个库都要求安装HDF5。对于HDF5的并行版本，还需要安装MPI版本。HDF5和MPI的安装超出了本书的范围。并行HDF5的安装说明可以在以下链接中找到：[https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel](https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel)。
- en: Using HDF5 with PyTables
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTables操作HDF5
- en: 'Let''s first create an HDF5 file from the numeric data we have in the `temp.csv` file
    with the following steps:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过以下步骤，从`temp.csv`文件中的数字数据创建一个HDF5文件：
- en: 'Get the numeric data:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数字数据：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Open the HDF5 file:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开HDF5文件：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Get the `root` node:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取`root`节点：
- en: '[PRE37]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a group with `create_group()` or a dataset with `create_array()`, and
    repeat this until all the data is stored:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`create_group()`创建一个组，或使用`create_array()`创建一个数据集，并重复此操作直到所有数据都被存储：
- en: '[PRE38]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Close the file:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭文件：
- en: '[PRE39]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s read the file and print the dataset to make sure it is properly written:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取文件并打印数据集，以确保它被正确写入：
- en: '[PRE40]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We get the NumPy array back.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了NumPy数组。
- en: Using HDF5 with pandas
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pandas操作HDF5
- en: 'We can also read and write HDF5 files with pandas. To read HDF5 files with
    pandas, they must first be created with it. For example, let''s use pandas to
    create a HDF5 file containing global power values:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 pandas 读取和写入 HDF5 文件。要使用 pandas 读取 HDF5 文件，必须先使用 pandas 创建它。例如，使用 pandas
    创建一个包含全球电力值的 HDF5 文件：
- en: '[PRE41]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let''s read the HDF5 file that we created and print the array back:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们读取我们创建的 HDF5 文件，并将数组打印出来：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The values of the DataFrame can be read in three different ways:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 的值可以通过三种不同的方式读取：
- en: '`store[''global_power'']`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`store[''global_power'']`'
- en: '`store.get(''global_power'')`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`store.get(''global_power'')`'
- en: '`store.global_power`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`store.global_power`'
- en: pandas also provides the high-level `read_hdf()` function and the `to_hdf()`
    DataFrame method for reading and writing HDF5 files.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 还提供了高级的 `read_hdf()` 函数和 `to_hdf()` DataFrame 方法，用于读取和写入 HDF5 文件。
- en: More documentation on HDF5 in pandas is available at the following link: [http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 pandas 中 HDF5 的更多文档，请访问以下链接：[http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5](http://pandas.pydata.org/pandas-docs/stable/io.html#io-hdf5)。
- en: Using HDF5 with h5py
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 h5py 操作 HDF5
- en: 'The `h5py` module is the most popular way to handle HDF5 files in Python. A
    new or existing HDF5 file can be opened with the `h5py.File()` function. After
    the file is open, its groups can simply be accessed by subscripting the file object
    as if it was a dictionary object. For example, the following code opens an HDF5
    file with `h5py` and then prints the array stored in the `/global_power `group:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`h5py` 模块是处理 Python 中 HDF5 文件的最流行方式。可以使用 `h5py.File()` 函数打开一个新的或现有的 HDF5 文件。文件打开后，可以像字典对象一样通过下标访问其组。例如，以下代码使用
    `h5py` 打开一个 HDF5 文件，然后打印 `/global_power` 组中存储的数组：'
- en: '[PRE43]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `arr` variable prints an `HDF5 dataset` type:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`arr` 变量打印出 `HDF5 数据集` 类型：'
- en: '[PRE44]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For a new `hdf5file`, datasets and groups can be created by using the `hdf5file.create_dataset()`
    function, returning the dataset object, and the `hdf5file.create_group()` function,
    returning the folder object. The `hdf5file` file object is also a folder object
    representing `/`, the root folder. Dataset objects support array style slicing
    and dicing to set or read values from them. For example, the following code creates
    an HDF5 file and stores one dataset:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个新的 `hdf5file`，可以通过使用 `hdf5file.create_dataset()` 函数来创建数据集，返回数据集对象，并通过使用
    `hdf5file.create_group()` 函数来创建组，返回文件夹对象。`hdf5file` 文件对象也是一个文件夹对象，表示根文件夹 `/`。数据集对象支持类似数组的切片操作，可以用于设置或读取它们的值。例如，以下代码创建一个
    HDF5 文件并存储一个数据集：
- en: '[PRE45]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`h5py` provides an `attrs` proxy object with a dictionary-like interface to
    store and retrieve metadata about the file, folders, and datasets. For example,
    the following code sets and then prints the dataset and file attribute:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`h5py` 提供了一个 `attrs` 代理对象，具有类似字典的接口，用于存储和检索有关文件、文件夹和数据集的元数据。例如，以下代码设置并打印数据集和文件属性：'
- en: '[PRE46]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'For more information about the `h5py` library, refer to the documentation at
    the following link: [http://docs.h5py.org/en/latest/index.html](http://docs.h5py.org/en/latest/index.html).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 `h5py` 库的信息，请参考以下文档：[http://docs.h5py.org/en/latest/index.html](http://docs.h5py.org/en/latest/index.html)。
- en: So far, we have learned about different data formats. Often, large data is stored
    commercially in databases, therefore we will explore how to access both SQL and
    NoSQL databases next.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了不同的数据格式。通常，大型数据会以商业形式存储在数据库中，因此接下来我们将探讨如何访问 SQL 和 NoSQL 数据库。
- en: SQL data
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL 数据
- en: Most databases are organized using relational models. A relational database
    consists of one or more related tables of information, and the relationship between
    information in different tables is described using keys. Conventionally, these
    databases are managed using the **Database Management System** (**DBMS**), software
    which interacts with end users, different applications, and the database itself
    to capture and analyze data. Commercially available DBMSes use **Structured Query
    Language** (**SQL**) to access and manipulate databases. We can also use Python
    to access relational databases. In this section, we will explore SQLite and MySQL,
    two very popular database engines that work with Python.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据库使用关系模型组织。关系数据库由一个或多个相关的信息表组成，表之间的信息关系通过键来描述。传统上，这些数据库通过**数据库管理系统**（**DBMS**）进行管理，DBMS
    是与最终用户、不同应用程序和数据库本身进行交互的软件，用于捕获和分析数据。商业化的 DBMS 使用**结构化查询语言**（**SQL**）来访问和操作数据库。我们也可以使用
    Python 来访问关系型数据库。在本节中，我们将探索 SQLite 和 MySQL 这两个非常流行的数据库引擎，它们可以与 Python 一起使用。
- en: The SQLite database engine
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQLite数据库引擎
- en: According to the SQLite home page ([https://sqlite.org/index.html](https://sqlite.org/index.html)),
    *SQLite is a self-contained, high-reliability, embedded, full-featured, public-domain
    SQL database engine*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SQLite主页（[https://sqlite.org/index.html](https://sqlite.org/index.html)），*SQLite是一种独立的、高可靠性的、嵌入式的、功能齐全的、公有领域的SQL数据库引擎*。
- en: SQLite is optimized for use in embedded applications. It is simple to use and
    quite fast. We need to use the `sqlite3` Python module to integrate SQLite with
    Python. The `sqlite3` module is bundled with Python 3, so there is no need to
    install it.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: SQLite专为嵌入式应用程序优化。 它使用简单且速度相当快。 我们需要使用`sqlite3` Python模块将SQLite与Python集成。 `sqlite3`模块已与Python
    3捆绑在一起，因此无需安装它。
- en: 'We will use the data from the European Soccer Database ([https://github.com/hugomathien/football-data-collection](https://github.com/hugomathien/football-data-collection))
    for demonstrative purposes. We assume that you already have a SQL server installed
    and started:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用欧洲足球数据库（[https://github.com/hugomathien/football-data-collection](https://github.com/hugomathien/football-data-collection)）进行演示。
    假设您已经安装并启动了SQL服务器：
- en: 'The first step after importing `sqlite3` is to create a connection to the database
    using the `connect` method:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`sqlite3`后的第一步是使用`connect`方法创建到数据库的连接：
- en: '[PRE47]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The European Soccer Database consists of eight tables. We can use `read_sql` to
    read the database table or SQL query into the DataFrame. This prints a list of
    all the tables in the database:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欧洲足球数据库包括八张表。 我们可以使用`read_sql`将数据库表或SQL查询读取到DataFrame中。 这将打印出数据库中所有表的列表：
- en: '[PRE48]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/94e3fff5-08d7-4fe5-ae06-5be6fdd7816a.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94e3fff5-08d7-4fe5-ae06-5be6fdd7816a.png)'
- en: 'Let''s read data from the `Country` table:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Country`表中读取数据：
- en: '[PRE49]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](img/5836ebd8-3242-4d4e-9db5-f0ac1f3b2a3b.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5836ebd8-3242-4d4e-9db5-f0ac1f3b2a3b.png)'
- en: 'We can use SQL queries on tables. In the following example, we select players
    whose height is greater than or equal to `180` and whose weight is greater than
    or equal to `170`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在表上使用SQL查询。 在以下示例中，我们选择身高大于或等于`180`且体重大于或等于`170`的球员：
- en: '[PRE50]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/4f0a7a4b-6b6f-4a6a-b613-aa60f23e8ca8.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f0a7a4b-6b6f-4a6a-b613-aa60f23e8ca8.png)'
- en: 'Finally, do not forget to close the connection using the `close` method:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，不要忘记使用`close`方法关闭连接：
- en: '[PRE51]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: If you made any changes in the database, you will need to use the `commit()`
    method.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对数据库进行了任何更改，则需要使用`commit()`方法。
- en: The MySQL database engine
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MySQL数据库引擎
- en: Though we can use SQLite for large databases, MySQL is generally preferred.
    In addition to being scalable for large databases, MySQL is also useful where
    data security is paramount. Before using MySQL, you will need to install the Python
    MySQL connector. There are many possible Python MySQL connectors such as, MySQLdb,
    PyMySQL, and MySQL; we will use `mysql-connector-python`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用SQLite处理大型数据库，但通常首选MySQL。 除了适用于大型数据库的可伸缩性外，MySQL在数据安全性至关重要的情况下也很有用。
    在使用MySQL之前，您需要安装Python MySQL连接器。 有许多可能的Python MySQL连接器，如MySQLdb、PyMySQL和MySQL；我们将使用`mysql-connector-python`。
- en: 'In all three, after making a connection using the `connect` method, we define
    the `cursor` element and use the `execute` method to run different SQL queries.
    To install MySQL, we use the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三种情况下，在使用`connect`方法建立连接后，我们定义`cursor`元素，并使用`execute`方法运行不同的SQL查询。 要安装MySQL，请使用以下方式：
- en: '[PRE52]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now that the Python MySQL connector is installed, we can start a connection
    with the SQL server. Replace the `host`, `user`, and `password` configurations
    with your SQL server configuration:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在Python MySQL连接器已安装，我们可以启动与SQL服务器的连接。 将`host`、`user`和`password`配置替换为您的SQL服务器配置：
- en: '[PRE53]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Let''s check existing databases in the server and list them. To do this, we
    use the `cursor` method:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查服务器中的现有数据库并列出它们。 为此，我们使用`cursor`方法：
- en: '[PRE54]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/303fb99e-29d7-48b9-9ef8-00eecf6b76de.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/303fb99e-29d7-48b9-9ef8-00eecf6b76de.png)'
- en: 'We can access one of the existing databases. Let''s list the tables in one
    of the databases:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以访问现有数据库之一。 让我们列出一个数据库中的表：
- en: '[PRE55]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: NoSQL data
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NoSQL数据
- en: 'The **Not Only Structured Query Language** (**NoSQL**) database is not a relational
    database; instead, data can be stored in key-value, JSON, document, columnar,
    or graph formats. They are frequently used in big data and real-time applications.
    We will learn here how to access NoSQL data using MongoDB, and we assume you have
    the MongoDB server configured properly and on:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**不仅仅是结构化查询语言**（**NoSQL**）数据库不是关系数据库；而是数据可以以键-值、JSON、文档、列或图格式存储。 它们在大数据和实时应用程序中经常使用。
    我们将在这里学习如何使用MongoDB访问NoSQL数据，并假设您已正确配置并启动了MongoDB服务器：'
- en: 'We will need to establish a connection with the Mongo daemon using the `MongoClient`
    object. The following code establishes the connection to the default host, `localhost` ,
    and port (`27017`). And it gives us access to the database:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用 `MongoClient` 对象与 Mongo 守护进程建立连接。以下代码建立与默认主机 `localhost` 和端口（`27017`）的连接，并让我们访问数据库：
- en: '[PRE56]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'In this example, we try to load the `cancer` dataset available in scikit-learn
    to the Mongo database. So, we first get the breast cancer dataset and convert
    it to a pandas DataFrame:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们尝试将 scikit-learn 中可用的 `cancer` 数据集加载到 Mongo 数据库中。因此，我们首先获取乳腺癌数据集并将其转换为
    pandas DataFrame：
- en: '[PRE57]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, we convert this into the JSON format, use the `json.loads()` function
    to decode it, and insert the decoded data into the open database:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将其转换为 JSON 格式，使用 `json.loads()` 函数进行解码，并将解码后的数据插入到开放数据库中：
- en: '[PRE58]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This will create a collection named `cancer_data` that contains the data. We
    can query the document we just created, using the `cursor` object:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将创建一个名为 `cancer_data` 的集合，包含数据。我们可以使用 `cursor` 对象查询我们刚刚创建的文档：
- en: '[PRE59]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![](img/cc5f0782-9f58-4952-b32a-fb0a42de7824.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc5f0782-9f58-4952-b32a-fb0a42de7824.png)'
- en: When it comes to distributed data on the IoT, **Hadoop Distributed File System **(**HDFS**)
    is another popular method for providing distributed data storage and access in
    IoT systems. In the next section, we study how to access and store data in HDFS.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在物联网中的分布式数据方面，**Hadoop 分布式文件系统**（**HDFS**）是提供分布式数据存储和访问的另一种流行方法。在下一节中，我们将研究如何访问和存储
    HDFS 中的数据。
- en: HDFS
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HDFS
- en: 'HDFS is a popular storage and access method for storing and retrieving data
    files for IoT solutions. The HDFS format can hold large amounts of data in a reliable
    and scalable manner. Its design is based on the **Google File System** ([https://ai.google/research/pubs/pub51](https://ai.google/research/pubs/pub51)).
    HDFS splits individual files into fixed-size blocks that are stored on machines
    across the cluster. To ensure reliability, it replicates the file blocks and distributes
    them across the cluster; by default, the replication factor is 3\. HDFS has two
    main architecture components:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS 是一种流行的存储和访问方法，用于存储和检索物联网解决方案中的数据文件。HDFS 格式能够以可靠且可扩展的方式存储大量数据。其设计基于 **Google
    文件系统**（[https://ai.google/research/pubs/pub51](https://ai.google/research/pubs/pub51)）。HDFS
    将单个文件分割成固定大小的块，这些块存储在集群中的多台机器上。为了确保可靠性，它会复制文件块并将它们分布在集群中；默认情况下，复制因子为 3。HDFS 有两个主要的架构组件：
- en: The first, **NodeName**, stores the metadata for the entire filesystem, such
    as filenames, their permissions, and the location of each block of each file.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个，**NodeName**，存储整个文件系统的元数据，例如文件名、权限以及每个文件的每个块的位置。
- en: The second, **DataNode** (one or more), is where file blocks are stored. It
    performs **Remote Procedure Calls** (**RPCs**) using protobufs.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个，**DataNode**（一个或多个），是存储文件块的地方。它使用 protobufs 执行 **远程过程调用**（**RPC**）。
- en: '**RPC** is a protocol that one program can use to request a service from a
    program located on another computer on a network without having to know the network''s
    details. A procedure call is also sometimes known as a **function call** or a
    **subroutine call**.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**RPC** 是一种协议，允许一个程序请求位于网络上另一台计算机上的程序提供服务，而无需了解网络的详细信息。过程调用有时也称为 **函数调用** 或
    **子例程调用**。'
- en: There are many options for programmatically accessing HDFS in Python, such as `snakebite`,
    `pyarrow`, `hdfs3`, `pywebhdfs`, `hdfscli`, and so on. In this section, we will
    focus mainly on libraries that provide native RPC client interfaces and work with
    Python 3.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，有许多选项可以编程方式访问 HDFS，如 `snakebite`、`pyarrow`、`hdfs3`、`pywebhdfs`、`hdfscli`
    等。在这一节中，我们将主要关注提供本地 RPC 客户端接口并与 Python 3 配合使用的库。
- en: Snakebite is a pure Python module and CLI that allows you to access HDFS from
    Python programs. At present, it only works with Python 2; Python 3 is not supported.
    Moreover, it does not yet support write operations, and so we are not including
    it in the book. However, if you are interested in knowing more about this, you
    can refer to Spotify's GitHub: [https://github.com/spotify/snakebite](https://github.com/spotify/snakebite).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Snakebite 是一个纯 Python 模块和命令行工具，允许您从 Python 程序访问 HDFS。目前，它仅支持 Python 2，不支持 Python
    3。此外，它尚不支持写操作，因此我们没有将其包含在书中。然而，如果您有兴趣了解更多，可以参考 Spotify 的 GitHub：[https://github.com/spotify/snakebite](https://github.com/spotify/snakebite)。
- en: Using hdfs3 with HDFS
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 hdfs3 访问 HDFS
- en: '`hdfs3` is a lightweight Python wrapper around the C/C++ `libhdfs3` library.
    It allows us to use HDFS natively from Python. To start, we first need to connect
    with the HDFS NameNode; this is done using the `HDFileSystem` class:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`hdfs3`是一个轻量级的Python封装，封装了C/C++的`libhdfs3`库。它允许我们从Python原生使用HDFS。首先，我们需要通过`HDFileSystem`类连接到HDFS的NameNode：'
- en: '[PRE60]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This automatically establishes a connection with the NameNode. Now, we can
    access a directory listing using the following:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这会自动与NameNode建立连接。现在，我们可以使用以下命令访问目录列表：
- en: '[PRE61]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will list all the files and directories in the `tmp` folder. You can use
    functions such as `mkdir` to make a directory and `cp` to copy a file from one
    location to another. To write into a file, we open it first using the `open` method
    and use `write`:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出`tmp`文件夹中的所有文件和目录。你可以使用诸如`mkdir`的函数来创建目录，使用`cp`来将文件从一个位置复制到另一个位置。要向文件写入数据，我们首先使用`open`方法打开文件，然后使用`write`：
- en: '[PRE62]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Data can be read from the file:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从文件中读取数据：
- en: '[PRE63]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: You can learn more about `hdfs3` from its documentation: [https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf](https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过它的文档了解更多关于`hdfs3`的信息：[https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf](https://media.readthedocs.org/pdf/hdfs3/latest/hdfs3.pdf)。
- en: Using PyArrow's filesystem interface for HDFS
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyArrow的HDFS文件系统接口
- en: 'PyArrow has a C++-based interface for HDFS. By default, it uses `libhdfs`,
    a JNI-based interface, for the Java Hadoop client. Alternatively, we can also
    use `libhdfs3`, a C++ library for HDFS. We connect to the NameNode using `hdfs.connect`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: PyArrow提供了一个基于C++的HDFS接口。默认情况下，它使用`libhdfs`，这是一个基于JNI的接口，用于Java Hadoop客户端。我们也可以选择使用`libhdfs3`，这是一个C++库，用于HDFS。我们使用`hdfs.connect`连接到NameNode：
- en: '[PRE64]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: If we change the driver to `libhdfs3`, we will be using the C++ library for
    HDFS from Pivotal Labs. Once the connection to the NameNode is made, the filesystem
    is accessed using the same methods as for hdfs3.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将驱动程序更改为`libhdfs3`，我们将使用Pivotal Labs提供的C++库来访问HDFS。一旦与NameNode建立连接，文件系统将使用与hdfs3相同的方法进行访问。
- en: HDFS is preferred when the data is extremely large. It allows us to read and
    write data in chunks; this is helpful for accessing and processing streaming data.
    A nice comparison of the three native RPC client interfaces is presented in the
    following blog post: [http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量非常庞大时，HDFS是首选。它允许我们分块读取和写入数据；这对于访问和处理流数据非常有帮助。以下博客文章提供了三种原生RPC客户端接口的良好比较：[http://wesmckinney.com/blog/python-hdfs-interfaces/](http://wesmckinney.com/blog/python-hdfs-interfaces/)。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: This chapter dealt with many different data formats, and, in the process, many
    different datasets. We started with the simplest TXT data and accessed the `Shakespeare`
    play data. We learned how to read data from CSV files using the `csv`, `numpy`,
    and `pandas` modules. We moved on to the JSON format; we used Python's JSON and
    pandas modules to access JSON data. From data formats, we progressed to accessing
    databases and covered both SQL and NoSQL databases. Next, we learned how to work
    with the Hadoop File System in Python.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了许多不同的数据格式，并且在过程中使用了许多不同的数据集。我们从最简单的TXT数据开始，并访问了`莎士比亚`的剧本数据。我们学习了如何使用`csv`、`numpy`和`pandas`模块从CSV文件中读取数据。接着我们转向了JSON格式，使用了Python的JSON和pandas模块来访问JSON数据。从数据格式的学习，我们逐步进入了数据库的访问，涵盖了SQL和NoSQL数据库。接下来，我们学习了如何在Python中使用Hadoop文件系统。
- en: Accessing data is the first step. In the next chapter, we will learn about machine
    learning tools that will help us to design, model, and make informed predictions
    on data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 访问数据是第一步。在下一章中，我们将学习一些机器学习工具，这些工具将帮助我们设计、建模，并在数据上做出明智的预测。
