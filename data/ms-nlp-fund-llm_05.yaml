- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: 'Empowering Text Classification: Leveraging Traditional Machine Learning Techniques'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活文本分类：利用传统机器学习技术
- en: In this chapter, we’ll delve into the fascinating world of text classification,
    a foundational task in **natural language processing** (**NLP**) and **machine
    learning** (**ML**) that deals with categorizing text documents into predefined
    classes. As the volume of digital text data continues to grow exponentially, the
    ability to accurately and efficiently classify text has become increasingly important
    for a wide range of applications, such as sentiment analysis, spam detection,
    and document organization. This chapter provides a comprehensive overview of the
    key concepts, methodologies, and techniques that are employed in text classification,
    catering to readers from diverse backgrounds and skill levels.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨文本分类的迷人世界，这是自然语言处理（**NLP**）和机器学习（**ML**）的基础任务，涉及将文本文档分类到预定义的类别中。随着数字文本数据量的指数级增长，准确且高效地分类文本的能力对于广泛的应用变得越来越重要，例如情感分析、垃圾邮件检测和文档组织。本章为来自不同背景和技能水平的读者提供了一个关于文本分类中使用的核心概念、方法和技术的全面概述。
- en: We’ll begin by exploring the various types of text classification tasks and
    their unique characteristics, offering insights into the challenges and opportunities
    each type presents. Next, we’ll introduce the concept of **N-grams** and discuss
    how they can be utilized as features for text classification, capturing not only
    individual words but also the local context and word sequences within the text.
    We’ll then examine the widely used **term frequency-inverse document frequency**
    (**TF-IDF**) method, which assigns weights to words based on their frequency in
    a document and across the entire corpus, showcasing its effectiveness in distinguishing
    relevant words for classification tasks.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨各种文本分类任务的类型及其独特特征，提供对每种类型所面临的挑战和机遇的见解。接下来，我们将介绍**N-gram**的概念，并讨论它们如何作为文本分类的特征被利用，不仅捕捉单个单词，还捕捉文本中的局部上下文和单词序列。然后，我们将研究广泛使用的**词频-逆文档频率**（**TF-IDF**）方法，该方法根据单词在文档中的频率和在整个语料库中的频率分配权重，展示了其在区分分类任务中相关单词方面的有效性。
- en: Following that, we’ll delve into the powerful **Word2Vec** algorithm and its
    application in text classification. We’ll discuss how **Word2Vec** creates dense
    vector representations of words that capture semantic meaning and relationships,
    and how these embeddings can be used as features to improve classification performance.
    Furthermore, we’ll cover popular architectures such as **continuous bag-of-words**
    (**CBOW**) and Skip-Gram, providing a deeper understanding of their inner workings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将深入研究强大的**Word2Vec**算法及其在文本分类中的应用。我们将讨论**Word2Vec**如何创建捕获语义意义和关系的密集向量表示，以及这些嵌入如何作为特征来提高分类性能。此外，我们还将介绍如**连续词袋模型**（**CBOW**）和
    Skip-Gram 等流行架构，提供对它们内部工作原理的更深入理解。
- en: Lastly, we’ll explore the concept of topic modeling, a technique for discovering
    hidden thematic structures within a collection of documents. We’ll examine popular
    algorithms such as **latent Dirichlet allocation** (**LDA**) and describe how
    topic modeling can be applied to text classification, enabling the discovery of
    semantic relationships between documents and improving classification performance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨主题建模的概念，这是一种在文档集合中揭示隐藏主题结构的技巧。我们将研究如**潜在狄利克雷分配**（**LDA**）等流行算法，并描述主题建模如何应用于文本分类，从而发现文档之间的语义关系并提高分类性能。
- en: Throughout this chapter, we aim to provide a thorough understanding of the underlying
    concepts and techniques that are employed in text classification, equipping you
    with the knowledge and skills needed to successfully tackle real-world text classification
    problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们旨在提供对文本分类中使用的潜在概念和技术的深入理解，为您提供解决现实世界文本分类问题的知识和技能。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Types of text classification
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类的类型
- en: Text classification based on N-grams
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 N-gram 的文本分类
- en: Text classification based on TF-IDF
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 TF-IDF 的文本分类
- en: Word2Vec and its application in text classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec 及其在文本分类中的应用
- en: Topic modeling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: Reviewing our use case – ML system design for NLP classification in a Jupyter
    notebook
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾我们的用例 - 在 Jupyter 笔记本中为 NLP 分类设计的机器学习系统
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To effectively read and understand this chapter, it is essential to have a solid
    foundation in various technical areas. A strong grasp of fundamental concepts
    in NLP, ML, and linear algebra is crucial. Familiarity with text preprocessing
    techniques, such as tokenization, stop word removal, and stemming or lemmatization,
    is necessary to comprehend the data preparation stage.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地阅读和理解这一章，对各种技术领域的坚实基础是必不可少的。对NLP、ML和线性代数的基本概念有深刻的理解至关重要。熟悉文本预处理技术，如分词、停用词去除以及词干提取或词形还原，对于理解数据准备阶段是必要的。
- en: Additionally, understanding basic ML algorithms, such as logistic regression
    and **support vector machines** (**SVMs**), is crucial for implementing text classification
    models. Finally, being comfortable with evaluation metrics such as accuracy, precision,
    recall, and F1 score, along with concepts such as overfitting, underfitting, and
    hyperparameter tuning, will enable a deeper appreciation of the challenges and
    best practices in text classification.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，理解基本的ML算法，如逻辑回归和**支持向量机**（**SVMs**），对于实现文本分类模型至关重要。最后，熟悉评估指标，如准确率、精确率、召回率和F1分数，以及过拟合、欠拟合和超参数调整等概念，将有助于更深入地理解文本分类中的挑战和最佳实践。
- en: Types of text classification
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类的类型
- en: 'Text classification is an NLP task where ML algorithms assign predefined categories
    or labels to text based on its content. It involves training a model on a labeled
    dataset to enable it to accurately predict the category of unseen or new text
    inputs. Text classification methods can be categorized into three main types –
    **supervised learning**, **unsupervised learning**, and **semi-supervised learning**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是一个NLP任务，其中ML算法根据文本的内容将其分配到预定义的类别或标签。它涉及在标记数据集上训练一个模型，以便它能够准确预测未见或新的文本输入的类别。文本分类方法可以分为三种主要类型——**监督学习**、**无监督学习**和**半监督学习**：
- en: '**Supervised learning**: This type of text classification involves training
    a model on labeled data, where each data point is associated with a target label
    or category. The model then uses this labeled data to learn the patterns and relationships
    between the input text and the target labels. Examples of supervised learning
    algorithms for text classification include naive bayes, SVMs, and neural networks
    such as **convolutional neural networks** (**CNNs**) and **recurrent neural**
    **networks** (**RNNs**).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：这种文本分类涉及在标记数据上训练模型，其中每个数据点都与一个目标标签或类别相关联。然后，该模型使用这些标记数据来学习输入文本和目标标签之间的模式和关系。文本分类的监督学习算法示例包括朴素贝叶斯、SVMs以及卷积神经网络（**CNNs**）和循环神经网络（**RNNs**）等神经网络。'
- en: '**Unsupervised learning**: This type of text classification involves clustering
    or grouping text documents into categories or topics without any prior knowledge
    of the categories or labels. Unsupervised learning is useful when there is no
    labeled data available or when the number of categories or topics is not known.
    Examples of unsupervised learning algorithms for text classification include K-means
    clustering, LDA, and **hierarchical Dirichlet** **process** (**HDP**).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：这种文本分类涉及将文本文档聚类或分组到类别或主题中，而不需要任何关于类别或标签的先验知识。当没有可用的标记数据或当类别或主题的数量未知时，无监督学习非常有用。文本分类的无监督学习算法示例包括K-means聚类、LDA和**层次狄利克雷过程**（**HDP**）。'
- en: '**Semi-supervised learning**: This type of text classification combines both
    supervised and unsupervised learning approaches. It involves using a small amount
    of labeled data to train a model and then using the model to classify the remaining
    unlabeled data. The model then uses the unlabeled data to improve its classification
    performance. Semi-supervised learning is useful when labeled data is scarce or
    expensive to obtain. Examples of semi-supervised learning algorithms for text
    classification include **self-training**, **co-training**, and **multi-view learning**.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：这种文本分类结合了监督学习和无监督学习的方法。它涉及使用少量标记数据来训练模型，然后使用该模型来分类剩余的无标签数据。然后，该模型使用无标签数据来提高其分类性能。当标记数据稀缺或难以获得时，半监督学习非常有用。文本分类的半监督学习算法示例包括**自训练**、**协同训练**和**多视角学习**。'
- en: Each of these text classification types has its strengths and weaknesses and
    is suitable for different types of applications. Understanding these types can
    help in choosing the appropriate approach for a given problem. In the following
    subsections, we’ll explain each of these methods in detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文本分类类型各有其优势和劣势，适用于不同类型的应用。了解这些类型有助于为特定问题选择适当的方法。在以下小节中，我们将详细解释这些方法。
- en: Supervised learning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: Supervised learning is a type of ML where an algorithm learns from labeled data
    to predict the label of new, unseen data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是一种机器学习类型，其中算法从标注数据中学习以预测新、未见数据的标签。
- en: 'In the context of text classification, supervised learning involves training
    a model on a labeled dataset, where each document or text sample is labeled with
    the corresponding category or class. The model then uses this training data to
    learn patterns and relationships between the text features and their associated
    labels:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类的背景下，监督学习涉及在标注数据集上训练模型，其中每个文档或文本样本都被标注为相应的类别或类别。然后，该模型使用这些训练数据来学习文本特征与其相关标签之间的模式和关系：
- en: In a supervised text classification task, the first step is to obtain a labeled
    dataset, where each text sample is annotated with its corresponding category or
    class.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在监督文本分类任务中，第一步是获取一个标注数据集，其中每个文本样本都被标注为相应的类别或类别。
- en: A labeled dataset is assumed to possess the highest level of reliability. Often,
    it is derived by having subject matter experts manually review the text and assign
    the appropriate class to each item. In other scenarios, there may be automated
    methods for deriving the labels. For instance, in cybersecurity, you may collect
    historical data and then assign labels, which may collect the outcome that followed
    each item – that is, whether the action was legitimate or not. Since such historical
    data exists in most domains, that too can serve as a reliable labeled set.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标注数据集被认为具有最高可靠性。通常，它是通过让主题专家手动审查文本并为每个项目分配适当的类别来获得的。在其他情况下，可能存在用于生成标签的自动化方法。例如，在网络安全领域，你可能收集历史数据然后分配标签，这些标签可能收集每个项目之后的成果——也就是说，该操作是否合法。由于大多数领域都存在此类历史数据，因此这些数据也可以作为可靠的标注集。
- en: The next step is to preprocess the text data to prepare it for modeling. This
    may include steps such as tokenization, stemming or lemmatization, removing stop
    words, and other text preprocessing techniques.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是对文本数据进行预处理，以准备建模。这可能包括诸如分词、词干提取或词形还原、去除停用词以及其他文本预处理技术。
- en: After preprocessing, the text data is transformed into numerical features, often
    using techniques such as bag-of-words or TF-IDF encoding.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在预处理之后，文本数据被转换为数值特征，通常使用诸如词袋模型或TF-IDF编码等技术。
- en: Then, a supervised learning algorithm such as logistic regression, SVM, or a
    neural network is trained on the labeled dataset using these numerical features.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用这些数值特征在标注数据集上训练监督学习算法，如逻辑回归、SVM或神经网络。
- en: Once the model has been trained, it can be used to predict the category or class
    of new, unseen text data based on the learned patterns and relationships between
    the text features and their associated labels.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型被训练，就可以使用它来根据学习到的文本特征与其相关标签之间的模式和关系来预测新、未见文本数据的类别或类别。
- en: Supervised learning algorithms are commonly used for text classification tasks.
    Let’s look at some common supervised learning algorithms that are used for text
    classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通常用于文本分类任务。让我们看看一些常用的用于文本分类的监督学习算法。
- en: Naive Bayes
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naive Bayes is a probabilistic algorithm that is commonly used for text classification.
    It is based on Bayes’ theorem, which states that the probability of a hypothesis
    (in this case, a document belonging to a particular class), given some observed
    evidence (in this case, the words in the document), is proportional to the probability
    of the evidence given the hypothesis times the prior probability of the hypothesis.
    Naive Bayes assumes that the features (words) are independent of each other given
    the class label, which is where the “naive” part of the name comes from.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种概率算法，常用于文本分类。它基于贝叶斯定理，该定理表明，给定一些观察到的证据（在这种情况下，文档中的单词），假设（在这种情况下，文档属于特定类别）的概率与证据在假设下给出的概率成正比，乘以假设的先验概率。朴素贝叶斯假设在类别标签给定的情况下，特征（单词）之间是相互独立的，这也是其名称中“朴素”部分的原因。
- en: Logistic regression
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is a statistical method that is used for binary classification
    problems (that is, problems where there are only two possible classes). It models
    the probability of the document belonging to a particular class using a logistic
    function, which maps any real-valued input to a value between 0 and 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种用于二元分类问题（即只有两个可能类别的問題）的统计方法。它使用逻辑函数来模拟文档属于特定类别的概率，该函数将任何实值输入映射到0到1之间的值。
- en: SVM
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SVM
- en: SVM is a powerful classification algorithm that is used in a variety of applications,
    including text classification. SVM works by finding the hyperplane that best separates
    the data into different classes. In text classification, the features are typically
    the words in the document, and the hyperplane is used to divide the space of all
    possible documents into different regions corresponding to different classes.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是一种强大的分类算法，在各种应用中使用，包括文本分类。SVM通过找到最佳分离数据的超平面来工作。在文本分类中，特征通常是文档中的词语，而超平面用于将所有可能的文档空间划分为对应不同类别的不同区域。
- en: All of these algorithms can be trained using labeled data, where the class labels
    are known for each document in the training set. Once trained, the model can be
    used to predict the class label of new, unlabeled documents. The performance of
    the model is typically evaluated using metrics such as accuracy, precision, recall,
    and F1 score.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些算法都可以使用标记数据进行训练，其中训练集中的每个文档的类别标签都是已知的。一旦训练完成，该模型就可以用来预测新、未标记文档的类别标签。模型的性能通常使用准确率、精确率、召回率和F1分数等指标进行评估。
- en: Unsupervised learning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Unsupervised learning is a type of ML where the data is not labeled and the
    algorithm is left to find patterns and structures on its own. In the context of
    text classification, unsupervised learning methods can be used when there is no
    labeled data available or when the goal is to discover hidden patterns in the
    text data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种机器学习类型，其中数据未标记，算法被留给自行寻找模式和结构。在文本分类的背景下，当没有标记数据可用或目标是发现文本数据中的隐藏模式时，可以使用无监督学习方法。
- en: One common unsupervised learning method for text classification is **clustering**.
    Clustering algorithms group similar documents together based on their content,
    without any prior knowledge of what each document is about. Clustering can be
    used to identify topics in a collection of documents or to group similar documents
    together for further analysis.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的用于文本分类的无监督学习方法是**聚类**。聚类算法根据文档的内容将相似的文档分组在一起，而不需要任何关于每个文档内容的先验知识。聚类可以用来识别文档集合中的主题，或将相似的文档分组在一起进行进一步分析。
- en: Another popular unsupervised learning algorithm for text classification is **LDA**.
    LDA is a probabilistic generative model that assumes that each document in a corpus
    is a mixture of topics, and each topic is a probability distribution over words.
    LDA can be used to discover the underlying topics in a collection of documents,
    even when the topics are not explicitly labeled.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的用于文本分类的无监督学习算法是**LDA**。LDA是一种概率生成模型，假设语料库中的每个文档都是主题的混合，每个主题是词语的概率分布。LDA可以用来发现文档集合中的潜在主题，即使主题没有明确标记。
- en: Finally, word embeddings are a popular unsupervised learning technique used
    for text classification. Word embeddings are dense vector representations of words
    that capture their semantic meaning based on the context in which they appear.
    They can be used to identify similar words and to find relationships between words,
    which can be useful for tasks such as text similarity and recommendation systems.
    Common word embedding models include Word2Vec and GloVe.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，词嵌入是一种流行的无监督学习技术，用于文本分类。词嵌入是词语的密集向量表示，根据它们出现的上下文捕获其语义意义。它们可以用来识别相似词语，并找出词语之间的关系，这对于文本相似性和推荐系统等任务可能很有用。常见的词嵌入模型包括Word2Vec和GloVe。
- en: Word2Vec is a popular algorithm that’s used to generate word embeddings, which
    are vector representations of words in a high-dimensional space. The algorithm
    was developed by a team of researchers at Google, led by Tomas Mikolov, in 2013\.
    The main idea behind Word2Vec is that words that appear in similar contexts tend
    to have similar meanings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec是一种流行的算法，用于生成词嵌入，即在高维空间中词语的向量表示。该算法由Google的研究团队在2013年开发，由Tomas Mikolov领导。Word2Vec背后的主要思想是，在相似上下文中出现的词语往往具有相似的意义。
- en: The algorithm takes in a large corpus of text as input and generates a vector
    representation for each word in the vocabulary. The vectors are typically high-dimensional
    (for example, 100 or 300 dimensions) and can be used to perform various NLP tasks,
    such as sentiment analysis, text classification, and machine translation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法以大量文本语料库作为输入，并为词汇表中的每个单词生成一个向量表示。这些向量通常是高维的（例如，100或300维），可用于执行各种NLP任务，如情感分析、文本分类和机器翻译。
- en: 'Two main architectures are used in Word2Vec: **CBOW** and **skip-gram**. In
    the CBOW architecture, the algorithm tries to predict the target word given a
    window of context words. In the skip-gram architecture, the algorithm tries to
    predict the context words given a target word. The training objective is to maximize
    the likelihood of the target word or context words given the input.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec使用了两种主要架构：**CBOW**和**skip-gram**。在CBOW架构中，算法试图根据上下文单词的窗口预测目标词。在skip-gram架构中，算法试图根据目标词预测上下文词。训练目标是最大化目标词或上下文词在给定输入下的可能性。
- en: Word2Vec has been widely adopted in the NLP community and has shown state-of-the-art
    performance on various benchmarks. It has also been used in many real-world applications,
    such as recommender systems, search engines, and chatbots.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec在NLP社区中得到广泛应用，并在各种基准测试中显示出最先进的性能。它也被用于许多实际应用中，如推荐系统、搜索引擎和聊天机器人。
- en: Semi-supervised learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Semi-supervised learning is an ML paradigm that sits between supervised and
    unsupervised learning. It utilizes a combination of labeled and unlabeled data
    for training, which is especially useful when the underlying models require labeled
    data which is expensive or time-consuming. This approach allows the model to leverage
    the information in the unlabeled data to improve its performance on the classification
    task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是介于监督学习和无监督学习之间的机器学习范式。它利用标记数据和未标记数据的组合进行训练，这在底层模型需要昂贵或耗时的标记数据时特别有用。这种方法允许模型利用未标记数据中的信息来提高其在分类任务上的性能。
- en: In the context of text classification, semi-supervised learning can be beneficial
    when we have a limited number of labeled documents but a large corpus of unlabeled
    documents. The goal is to improve the performance of the classifier by leveraging
    the information contained in the unlabeled data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本分类的背景下，当我们拥有有限的标记文档但大量未标记文档时，半监督学习可以是有益的。目标是利用未标记数据中包含的信息来提高分类器的性能。
- en: There are several common semi-supervised learning algorithms, including label
    propagation and co-training. We’ll discuss each of these in more detail next.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种常见的半监督学习算法，包括标签传播和Co-training。我们将在下一节中更详细地讨论这些算法。
- en: Label propagation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Label propagation
- en: Label propagation is a graph-based semi-supervised learning algorithm. It builds
    a graph using both labeled and unlabeled data points, with each data point represented
    as a node and edges representing the similarity between nodes. The algorithm works
    by propagating the labels from the labeled nodes to the unlabeled nodes based
    on their similarity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Label propagation是一种基于图的半监督学习算法。它使用标记和未标记数据点构建一个图，每个数据点表示为一个节点，边表示节点之间的相似性。该算法通过根据它们的相似性从标记节点传播标签到未标记节点来工作。
- en: The key idea is that similar data points should have similar labels. The algorithm
    begins by assigning initial label probabilities to the unlabeled nodes, typically
    based on their similarity to labeled nodes. Then, an iterative process propagates
    these probabilities throughout the graph until convergence. The final label probabilities
    are used to classify the unlabeled data points.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是相似的数据点应该有相似的标签。算法首先为未标记的节点分配初始标签概率，通常基于它们与标记节点的相似性。然后，一个迭代过程将把这些概率传播到整个图中，直到收敛。最终的标签概率用于对未标记的数据点进行分类。
- en: Co-training
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Co-training
- en: Co-training is another semi-supervised learning technique that trains multiple
    classifiers on different views of the data. A view is a subset of features that
    are sufficient for the learning task and are conditionally independent given the
    class label. The basic idea is to use one classifier’s predictions to label some
    of the unlabeled data, and then use that newly labeled data to train the other
    classifier. This process is performed iteratively, with each classifier improving
    the other until a stopping criterion is met.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Co-training 是另一种半监督学习技术，它在数据的不同视角上训练多个分类器。视角是特征的一个子集，对于学习任务来说是足够的，并且给定类别标签时是条件独立的。基本思想是使用一个分类器的预测来标记一些未标记的数据，然后使用这些新标记的数据来训练另一个分类器。这个过程是迭代进行的，每个分类器通过改进另一个分类器来提高性能，直到满足停止条件。
- en: To apply semi-supervised learning in a specific domain, let’s consider a medical
    domain where we want to classify scientific articles into different categories
    such as **cardiology**, **neurology**, and **oncology**. Suppose we have a small
    set of labeled articles and a large set of unlabeled articles.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要在特定领域应用半监督学习，让我们考虑一个医学领域，我们希望将科学文章分类到不同的类别中，例如**心脏病学**、**神经病学**和**肿瘤学**。假设我们有一小部分标记的文章和一大群未标记的文章。
- en: A possible approach could be to use label propagation by creating a graph of
    articles where the nodes represent the articles and the edges represent the similarity
    between the articles. The similarity could be based on various factors, such as
    the words used, the topics covered, or the citation networks between the articles.
    After propagating the labels, we can classify the unlabeled articles based on
    the final label probabilities.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的方法是创建一个文章图，其中节点代表文章，边代表文章之间的相似性。相似性可以基于各种因素，例如使用的单词、覆盖的主题或文章之间的引用网络。在传播标签后，我们可以根据最终的标签概率对未标记的文章进行分类。
- en: Alternatively, we could use co-training by splitting the features into two views,
    such as the abstract and the full text of the articles. We would train two classifiers,
    one for each view, and iteratively update the classifiers using the predictions
    made by the other classifier on the unlabeled data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过将特征分为两个视角来使用 co-training，例如文章的摘要和全文。我们将训练两个分类器，每个视角一个，并迭代地使用另一个分类器在未标记数据上的预测来更新分类器。
- en: In both cases, the goal is to leverage the information in the unlabeled data
    to improve the performance of the classifier in the specific domain.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，目标都是利用未标记数据中的信息来提高特定领域的分类器性能。
- en: In this chapter, we’ll elaborate on supervised text classification and topic
    modeling.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细阐述监督文本分类和主题建模。
- en: Sentence classification using one-hot encoding vector representation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 one-hot 编码向量表示进行句子分类
- en: One-hot encoded vector representation is a method of representing categorical
    data, such as words, as binary vectors. In the context of text classification,
    one-hot encoding can be used to represent text data as numerical input features
    for a classification model. Here’s a detailed explanation of text classification
    using one-hot encoding vectors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot 编码向量表示是一种表示分类数据（如单词）为二进制向量的方法。在文本分类的上下文中，one-hot 编码可以用来将文本数据表示为分类模型的数值输入特征。以下是使用
    one-hot 编码向量进行文本分类的详细解释。
- en: Text preprocessing
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预处理
- en: 'The first step is to preprocess the text data, as explained in the previous
    chapter. The main goal of preprocessing is to transform raw text into a more structured
    and consistent format that can be easily understood and processed by ML algorithms.
    Here are several reasons why text preprocessing is essential for one-hot encoded
    vector classification:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是预处理文本数据，如前一章所述。预处理的主要目标是将原始文本转换为更结构化和一致的格式，以便机器学习算法可以轻松理解和处理。以下是文本预处理对于
    one-hot 编码向量分类的几个原因：
- en: '**Noise reduction**: Raw text data often contains noise, such as typos, spelling
    errors, special characters, and formatting inconsistencies. Preprocessing helps
    to clean the text, reducing noise that may negatively impact the performance of
    the classification model.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声减少**：原始文本数据通常包含噪声，例如拼写错误、拼写错误、特殊字符和格式不一致。预处理有助于清理文本，减少可能对分类模型性能产生负面影响的噪声。'
- en: '**Dimensionality reduction**: One-hot encoded vector representation has a high
    dimensionality as each unique word in the dataset corresponds to a separate feature.
    Preprocessing techniques, such as stop word removal, stemming, or lemmatization,
    can help reduce the size of the vocabulary, leading to a lower-dimensional feature
    space. This can improve the efficiency of the classification algorithm and reduce
    the risk of overfitting.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降维**：one-hot 编码向量表示具有高维度，因为数据集中每个唯一的单词对应一个单独的特征。预处理技术，如停用词去除、词干提取或词形还原，可以帮助减少词汇表的大小，从而降低特征空间的维度。这可以提高分类算法的效率并降低过拟合的风险。'
- en: '**Consistent representation**: Converting all text to lowercase and applying
    stemming or lemmatization ensures that words with the same meaning or root form
    are consistently represented in the one-hot encoding vectors. This can help the
    classification model learn more meaningful patterns from the data as it will not
    treat different forms of the same word as separate features.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致的表示**：将所有文本转换为小写并应用词干提取或词形还原确保具有相同意义或词根形式的单词在 one-hot 编码向量中保持一致表示。这可以帮助分类模型从数据中学习到更有意义的模式，因为它不会将同一单词的不同形式视为不同的特征。'
- en: '**Handling irrelevant information**: Preprocessing can help remove irrelevant
    information, such as URLs, email addresses, or numbers, that may not contribute
    to the classification task. Removing such information can improve the model’s
    ability to focus on the meaningful words and patterns in the text.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理无关信息**：预处理可以帮助移除无关信息，如 URL、电子邮件地址或数字，这些信息可能不会对分类任务做出贡献。移除此类信息可以提高模型专注于文本中有意义单词和模式的能力。'
- en: '**Improving model performance**: Preprocessed text data can lead to better
    performance of the classification model as the model will learn from a cleaner
    and more structured dataset. This can result in improved accuracy and generalization
    to new, unseen text data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高模型性能**：预处理文本数据可以提高分类模型的表现，因为模型将从更干净、更有结构的数据集中学习。这可能导致准确率的提高以及对新、未见文本数据的泛化能力。'
- en: Once we preprocess the text, we can start extracting the words in the text.
    We call this task vocabulary construction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们预处理完文本，我们就可以开始提取文本中的单词了。我们称这个任务为词汇构建。
- en: Vocabulary construction
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词汇构建
- en: Construct a vocabulary containing all unique words in the preprocessed text.
    Assign a unique index to each word in the vocabulary.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包含预处理文本中所有唯一单词的词汇表。为词汇表中的每个单词分配一个唯一的索引。
- en: 'Vocabulary construction is an essential step in preparing text data for one-hot
    encoded vector classification. The vocabulary is a set of all unique words (tokens)
    in the preprocessed text data. It serves as a basis for creating one-hot-encoded
    feature vectors for each document. Here’s a detailed explanation of the vocabulary
    construction process for one-hot encoded vector classification:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇构建是准备文本数据以进行 one-hot 编码向量分类的一个关键步骤。词汇是预处理文本数据中所有唯一单词（标记）的集合。它作为创建每个文档的 one-hot
    编码特征向量的基础。以下是 one-hot 编码向量分类的词汇构建过程的详细说明：
- en: '**Create a set of unique words**: After preprocessing the text data, gather
    all the words from all documents and create a set of unique words. This set will
    represent the vocabulary. The order of the words in the vocabulary does not matter,
    but it’s crucial to keep track of the indices assigned to each word as they will
    be used to create one-hot encoded vectors later.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建一组唯一单词**：在预处理文本数据后，从所有文档中收集所有单词并创建一组唯一单词。这个集合将代表词汇表。词汇表中单词的顺序不重要，但必须跟踪分配给每个单词的索引，因为它们将用于稍后创建
    one-hot 编码向量。'
- en: 'For example, consider that the following preprocessed dataset consists of two
    documents:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，考虑以下预处理数据集由两个文档组成：
- en: '**Document 1**: “apple banana orange”'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 1**： “apple banana orange”'
- en: '**Document 2**: “banana grape apple”'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 2**： “banana grape apple”'
- en: The vocabulary for this dataset would be {“apple”, “banana”, “orange”, “grape”}.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该数据集的词汇表将是 {“apple”， “banana”， “orange”， “grape”}。
- en: '**Assign indices to the words**: Once you have the set of unique words, assign
    a unique index to each word in the vocabulary. These indices will be used to create
    one-hot-encoded vectors for each document.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为单词分配索引**：一旦你有了唯一单词的集合，为词汇表中的每个单词分配一个唯一的索引。这些索引将用于创建每个文档的 one-hot 编码向量。'
- en: 'Using the preceding example, you might assign the following indices:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用前面的示例，你可能分配以下索引：
- en: '“apple”: 0'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “apple”：0
- en: '“banana”: 1'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “banana”：1
- en: '“orange”: 2'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “orange”：2
- en: '“grape”: 3'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “grape”：3
- en: One-hot encoding
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: One-hot 编码
- en: With the constructed vocabulary and assigned indices, you can now create one-hot
    encoded vectors for each document in the dataset. One simple approach to creating
    a one-hot encoded vector is to use **bag-of-words**. For each word in a document,
    find its corresponding index in the vocabulary and set the value at that index
    to 1 in the one-hot-encoded vector. If a word appears multiple times in the document,
    its corresponding value in the one-hot-encoded vector remains 1\. All other values
    in the vector will be 0.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用构建的词汇表和分配的索引，你现在可以为数据集中的每个文档创建独热编码的向量。创建独热编码向量的一个简单方法就是使用**词袋模型**。对于文档中的每个单词，找到它在词汇表中的对应索引，并在独热编码向量中设置该索引的值为
    1。如果一个单词在文档中多次出现，其在独热编码向量中的对应值仍然为 1。向量中的所有其他值都将为 0。
- en: 'For example, using the vocabulary and indices mentioned previously, the one-hot
    encoded vectors for the documents would be as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用之前提到的词汇表和索引，文档的独热编码向量如下所示：
- en: '**Document 1**: [1, 1, 1, 0] (apple, banana, and orange are present)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 1**: [1, 1, 1, 0]（存在苹果、香蕉和橙子）'
- en: '**Document 2**: [1, 1, 0, 1] (apple, banana, and grape are present)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 2**: [1, 1, 0, 1]（存在苹果、香蕉和葡萄）'
- en: 'Once we have the corresponding values for each document, we can create a feature
    matrix with one-hot-encoded vectors as rows, where each row represents a document
    and each column represents a word from the vocabulary. This matrix will be used
    as input for the text classification model. For example, in the previous example,
    the feature vectors for two documents are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了每个文档对应的值，我们就可以创建一个特征矩阵，其中以独热编码的向量作为行，每一行代表一个文档，每一列代表词汇表中的一个单词。这个矩阵将被用作文本分类模型的输入。例如，在之前的例子中，两个文档的特征向量如下所示：
- en: '|  | Apple | Banana | Orange | Grape |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 苹果 | 香蕉 | 橙子 | 葡萄 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Document 1 | 1 | 1 | 1 | 0 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 文档 1 | 1 | 1 | 1 | 0 |'
- en: '| Document 2 | 1 | 1 | 0 | 1 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 文档 2 | 1 | 1 | 0 | 1 |'
- en: Table 5.1 – Sample one-hot-encoded vector for two documents
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 两个文档的样本独热编码向量
- en: Please note that with text preprocessing, it helps to have a smaller vocabulary
    and it gives us better model performance. Besides that, if needed, we can perform
    feature selection methods (as explained previously in this book) on the extracted
    feature vectors to improve our model performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过文本预处理，拥有一个更小的词汇表有助于提高模型性能。此外，如果需要，我们可以在提取的特征向量上执行特征选择方法（如本书之前所述），以提高我们的模型性能。
- en: While creating a one-hot encoded vector from words is useful, sometimes, we
    need to consider the existence of two words beside each other. For example, “very
    good” and “not good” can have different meanings. To achieve this goal, we can
    use N-grams.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然从单词创建独热编码向量很有用，但有时我们需要考虑两个单词相邻的存在。例如，“非常好”和“不好”可能有不同的含义。为了达到这个目标，我们可以使用 N-grams。
- en: N-grams
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-grams
- en: N-grams are a generalization of the bag-of-words model that takes into account
    the order of words by considering sequences of *n* consecutive words. An N-gram
    is a contiguous sequence of *n* items (typically words) from a given text. For
    example, in the sentence “The cat is on the mat,” the 2-grams (bigrams) would
    be “The cat,” “cat is,” “is on,” “on the,” and “the mat.”
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: N-grams 是 bag-of-words 模型的一种推广，它通过考虑连续的 *n* 个单词的顺序来考虑单词的顺序。N-gram 是从给定文本中连续的
    *n* 个项目（通常是单词）的序列。例如，在句子“The cat is on the mat”中，2-grams（双词）将是“The cat”，“cat is”，“is
    on”，“on the”，和“the mat”。
- en: Using N-grams can help capture local context and word relationships, which may
    improve the performance of the classifier. However, it also increases the dimensionality
    of the feature space, which can be computationally expensive.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 N-grams 可以帮助捕捉局部上下文和单词关系，这可能会提高分类器的性能。然而，它也增加了特征空间的维度，这可能会在计算上变得昂贵。
- en: Model training
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Train an ML model, such as logistic regression, SVM, or neural networks, on
    the feature matrix to learn the relationship between the one-hot encoded text
    features and the target labels. The model will learn to predict the class label
    based on the presence or absence of specific words in the document. Once we’ve
    decided on the training process, we need to perform the following tasks:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征矩阵上训练一个机器学习模型，如逻辑回归、SVM 或神经网络，以学习独热编码文本特征与目标标签之间的关系。该模型将学会根据文档中特定单词的存在与否来预测类别标签。一旦我们决定了训练过程，我们需要执行以下任务：
- en: '**Model evaluation**: Evaluate the performance of the model using appropriate
    evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion
    matrix, and use techniques such as cross-validation to get a reliable estimate
    of the model’s performance on unseen data.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型评估**：使用适当的评估指标评估模型性能，例如准确率、精确率、召回率、F1分数或混淆矩阵，并使用交叉验证等技术来获得模型在未见数据上的可靠性能估计。'
- en: '**Model application**: Apply the trained model to new, unseen text data. Preprocess
    and one-hot encode the new text data using the same vocabulary and use the model
    to predict the class labels.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型应用**：将训练好的模型应用于新的、未见过的文本数据。使用相同的词汇表对新的文本数据进行预处理和独热编码，然后使用模型预测类别标签。'
- en: One potential limitation of using one-hot encoded vectors for text classification
    is that they do not capture word order, context, or semantic relationships between
    words. This can lead to suboptimal performance, especially in more complex classification
    tasks. More advanced techniques, such as word embeddings (for example, Word2Vec
    or GloVe) or deep learning models (for example, CNNs or RNNs), can provide better
    representations for text data in these cases.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用独热编码向量进行文本分类的一个潜在局限性是它们没有捕捉到单词顺序、上下文或单词之间的语义关系。这可能导致性能不佳，尤其是在更复杂的分类任务中。在这种情况下，更高级的技术，如词嵌入（例如Word2Vec或GloVe）或深度学习模型（例如CNNs或RNNs），可以提供更好的文本数据表示。
- en: In summary, text classification using one-hot-encoded vectors involves preprocessing
    text data, constructing a vocabulary, representing text data as one-hot encoded
    feature vectors, training an ML model on the feature vectors, and evaluating and
    applying the model to new text data. The one-hot encoded vector representation
    is a simple but sometimes limited approach to text classification, and more advanced
    techniques may be necessary for complex tasks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用独热编码向量进行文本分类涉及预处理文本数据、构建词汇表、将文本数据表示为独热编码特征向量、在特征向量上训练机器学习模型，以及评估和应用模型到新的文本数据。独热编码向量表示是一种简单但有时有限的文本分类方法，对于复杂任务可能需要更高级的技术。
- en: So far, we’ve learned about classifying documents using N-grams. However, this
    approach has a drawback. There are a considerable number of words that occur in
    the documents frequently and do not add value to our models. To improve the models,
    text classification using TF-IDF has been proposed.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了使用N-gram对文档进行分类。然而，这种方法有一个缺点。文档中频繁出现的大量单词并不增加我们模型的价值。为了改进模型，已经提出了使用TF-IDF进行文本分类的方法。
- en: Text classification using TF-IDF
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TF-IDF进行文本分类
- en: One-hot encoded vector is a good approach to perform classification. However,
    one of its weaknesses is that it does not consider the importance of different
    words based on different documents. To solve this issue, using **TF-IDF** can
    be helpful.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码向量是进行分类的好方法。然而，它的一个弱点是它没有考虑不同文档中不同单词的重要性。为了解决这个问题，使用**TF-IDF**可能会有所帮助。
- en: TF-IDF is a numerical statistic that is used to measure the importance of a
    word in a document within a document collection. It helps reflect the relevance
    of words in a document, considering not only their frequency within the document
    but also their rarity across the entire document collection. The TF-IDF value
    of a word increases proportionally to its frequency in a document but is offset
    by the frequency of the word in the entire document collection.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是一种数值统计量，用于衡量一个词在文档集合中相对于整个文档集合的重要性。它有助于反映文档中单词的相关性，不仅考虑了单词在文档中的频率，还考虑了单词在整个文档集合中的稀有度。一个词的TF-IDF值与其在文档中的频率成正比，但会因该词在整个文档集合中的频率而抵消。
- en: 'Here’s a detailed explanation of the mathematical equations involved in calculating
    TF-IDF:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是计算TF-IDF所涉及的数学公式的详细解释：
- en: '**Term frequency (TF)**: The TF of a word, *t*, in a document, *d*, represents
    the number of times the word occurs in the document, normalized by the total number
    of words in the document. The TF can be calculated using the following equation:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频（TF）**：一个词*t*在文档*d*中的TF表示该词在文档中出现的次数，除以文档中单词的总数。TF可以使用以下公式计算：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo>′</mo><mi>t</mi><mo>′</mo><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo></mrow></mrow></mrow></math>](img/259.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>N</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mo>′</mo><mi>t</mi><mo>′</mo><mi>a</mi><mi>p</mi><mi>p</mi><mi>e</mi><mi>a</mi><mi>r</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>d</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo><mo>/</mo><mo>(</mo><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>w</mi><mi>o</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mo>′</mo><mi>d</mi><mo>′</mo><mo>)</mo></mrow></mrow></mrow></math>](img/259.png)'
- en: The TF measures the importance of a word within a specific document.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: TF 衡量一个单词在特定文档中的重要性。
- en: '**Inverse document frequency (IDF)**: The IDF of a word, *t*, reflects the
    rarity of the word across the entire document collection. IDF can be calculated
    using the following equation:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆文档频率（IDF）**：单词 *t* 的 IDF 反映了该单词在整个文档集合中的稀有度。IDF 可以使用以下公式计算：'
- en: IDF(t) = log ((Total number of documents in the collection) / (Number of documents
    containing word ′t′))
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IDF(t) = log ((集合中文档的总数) / (包含单词′t′的文档数))
- en: The logarithm is used to dampen the effect of the IDF component. If a word appears
    in many documents, its IDF value will be closer to 0, and if it appears in fewer
    documents, its IDF value will be higher.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对数用于减弱 IDF 成分的效应。如果一个单词在许多文档中出现，其 IDF 值将更接近 0，而如果一个单词在较少的文档中出现，其 IDF 值将更高。
- en: '**TF-IDF computation**: The TF-IDF value of a word, *t*, in a document, *d*,
    can be calculated by multiplying the TF of the word in the document with the IDF
    of the word across the document collection:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-IDF 计算**：文档中单词 *t* 的 TF-IDF 值可以通过将文档中该单词的 TF 与整个文档集合中该单词的 IDF 相乘来计算：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow></math>](img/260.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mo>=</mo><mi>T</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>,</mo><mi>d</mi><mo>)</mo><mi
    mathvariant="normal">*</mi><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow></math>](img/260.png)'
- en: The resulting TF-IDF value represents the importance of a word in a document,
    taking into account both its frequency within the document and its rarity across
    the entire document collection. High TF-IDF values indicate words that are more
    significant in a particular document, whereas low TF-IDF values indicate words
    that are either common across all documents or rare within the specific document.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的 TF-IDF 值表示一个单词在文档中的重要性，同时考虑其在文档中的频率和在整个文档集合中的稀有度。高 TF-IDF 值表示在特定文档中更重要的单词，而低
    TF-IDF 值表示在所有文档中都很常见或在特定文档中很罕见的单词。
- en: 'Let’s consider a simple example of classifying movie reviews into two categories:
    positive and negative. We have a small dataset with three movie reviews and their
    respective labels, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子，将电影评论分为两类：正面和负面。我们有一个包含三个电影评论及其相应标签的小数据集，如下所示：
- en: '**Document 1 (positive)**: “I loved the movie. The acting was great and the
    story was captivating.”'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 1（正面）**：“我喜欢这部电影。表演很棒，故事很吸引人。”'
- en: '**Document 2 (negative)**: “The movie was boring. I did not like the story,
    and the acting was terrible.”'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 2（负面）**：“这部电影很无聊。我不喜欢这个故事，表演也很糟糕。”'
- en: '**Document 3 (positive)**: “An amazing movie with a wonderful story and brilliant
    acting.”'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 3（正面）**： “一部令人惊叹的电影，拥有精彩的故事和出色的表演。”'
- en: 'Now, we will use TF-IDF to classify a new, unseen movie review:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用TF-IDF对一篇新的、未见过的电影评论进行分类：
- en: '**Document 4 (unknown)**: “The story was interesting, and the acting was good.”'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 4（未知）**： “故事很有趣，表演也很出色。”'
- en: 'Here are the steps that we need to perform to have the classifier predict the
    class of our document:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们需要执行的步骤，以便让分类器预测我们文档的类别：
- en: '**Step 1 – preprocess the text data**: Tokenize, lowercase, remove stop words,
    and apply stemming or lemmatization to the words in all documents:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 1 – 预处理文本数据**：对所有文档中的单词进行分词、转换为小写、去除停用词，并应用词干提取或词形还原：'
- en: '**Document 1**: “love movi act great stori captiv”'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 1**： “love movi act great stori captiv”'
- en: '**Document 2**: “movi bore not like stori act terribl”'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 2**： “movi bore not like stori act terribl”'
- en: '**Document 3**: “amaz movi wonder stori brilliant act”'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 3**： “amaz movi wonder stori brilliant act”'
- en: '**Document 4**: “stori interest act good”'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 4**： “stori interest act good”'
- en: '**Step 2 – create the vocabulary**: Combine all unique words from the preprocessed
    documents:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 2 – 创建词汇表**：将预处理文档中的所有唯一单词合并：'
- en: 'Vocabulary: {“love”, “movi”, “act”, “great”, “stori”, “captiv”, “bore”, “not”,
    “like”, “terribl”, “amaz”, “wonder”, “brilliant”, “interest”, “good”}'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词汇表：{"love", "movi", "act", "great", "stori", "captiv", "bore", "not", "like",
    "terribl", "amaz", "wonder", "brilliant", "interest", "good"}
- en: '**Step 3 – calculate the TF and IDF values**: Compute the TF and IDF for each
    word in each document.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤 3 – 计算TF和IDF值**：计算每个文档中每个单词的TF和IDF。'
- en: 'For example, for the word “stori” in Document 4, we have the following:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，对于文档 4 中的单词“stori”，我们有以下内容：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>,</mo><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mn>4</mn><mo>)</mo><mo>=</mo><mn>1</mn><mo>/</mo><mn>4</mn><mo>=</mo><mn>0.25</mn></mrow></mrow></mrow></math>](img/261.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>,</mo><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mn>4</mn><mo>)</mo><mo>=</mo><mn>1</mn><mo>/</mo><mn>4</mn><mo>=</mo><mn>0.25</mn></mrow></mrow></mrow></math>](img/261.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>4</mn><mo>/</mo><mn>3</mn><mo>)</mo><mo>≈</mo><mn>0.287</mn></mrow></mrow></mrow></math>](img/262.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mn>4</mn><mo>/</mo><mn>3</mn><mo>)</mo><mo>≈</mo><mn>0.287</mn></mrow></mrow></mrow></math>](img/262.png)'
- en: '4. **Step 4 – compute the TF-IDF values**: Calculate the TF-IDF values for
    each word in each document.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **步骤 4 – 计算TF-IDF值**：计算每个文档中每个单词的TF-IDF值。
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>,</mo><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mn>4</mn><mo>)</mo><mo>=</mo><mn>0.25</mn><mi
    mathvariant="normal">*</mi><mn>0.287</mn><mo>≈</mo><mn>0.0717</mn></mrow></mrow></mrow></math>](img/263.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>T</mi><mi>F</mi><mo>−</mo><mi>I</mi><mi>D</mi><mi>F</mi><mo>(</mo><mi
    mathvariant="normal">"</mi><mi>s</mi><mi>t</mi><mi>o</mi><mi>r</mi><mi>i</mi><mi
    mathvariant="normal">"</mi><mo>)</mo><mo>=</mo><mn>0.25</mn><mi mathvariant="normal">*</mi><mn>0.287</mn><mo>≈</mo><mn>0.0717</mn></mrow></mrow></mrow></math>](img/263.png)'
- en: Repeat this process for all words in all documents and create a feature matrix
    with the TF-IDF values.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有文档中的所有单词重复此过程，并创建一个包含TF-IDF值的特征矩阵。
- en: '5. **Step 5 – train a classifier**: Split the dataset into a training set (documents
    1 to 3) and a test set (document 4). Train a classifier, such as logistic regression
    or SVM, using the training set’s TF-IDF feature matrix and their corresponding
    labels (positive or negative).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 5. **步骤 5 – 训练分类器**：将数据集分为训练集（文档 1 到 3）和测试集（文档 4）。使用训练集的TF-IDF特征矩阵及其相应的标签（正面或负面）来训练一个分类器，例如逻辑回归或SVM。
- en: '6. **Step 6 – predict the class label**: Preprocess and compute the TF-IDF
    values for the new movie review (document 4) using the same vocabulary. Use the
    trained classifier to predict the class label for document 4 based on its TF-IDF
    feature vector.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 6. **步骤 6 – 预测类别标签**：使用相同的词汇对新的电影评论（文档 4）进行预处理和计算 TF-IDF 值。使用训练好的分类器根据文档 4 的
    TF-IDF 特征向量预测其类别标签。
- en: 'For example, if the classifier predicts a positive label for document 4, the
    classification result would be as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果分类器预测文档 4 为正面标签，分类结果如下：
- en: '**Document 4 (****Predicted)**: “Positive”'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 4（**预测**）**：“正面”'
- en: By following these steps, you can use the TF-IDF representation to classify
    text documents based on the importance of words in the documents relative to the
    entire document collection.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这些步骤，您可以使用 TF-IDF 表示来根据文档中相对于整个文档集合的词的重要性对文本文档进行分类。
- en: In summary, the TF-IDF value is calculated using the mathematical equations
    for TF and IDF. It serves as a measure of the importance of a word in a document
    relative to the entire document collection, considering both the frequency of
    the word within the document and its rarity across all documents.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TF-IDF 值是通过 TF 和 IDF 的数学公式计算得出的。它作为衡量一个词在文档中相对于整个文档集合重要性的指标，同时考虑了该词在文档中的频率以及在整个文档中的稀有度。
- en: Text classification using Word2Vec
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Word2Vec 进行文本分类
- en: One of the methods to perform text classification is to convert the words into
    embedding vectors so that you can use those vectors for classification. Word2Vec
    is a well-known method to perform this task.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 执行文本分类的一种方法是将词转换为嵌入向量，以便可以使用这些向量进行分类。Word2Vec 是执行此任务的一种知名方法。
- en: Word2Vec
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Word2Vec
- en: 'Word2Vec is a group of neural network-based models that are used to create
    word embeddings, which are dense vector representations of words in a continuous
    vector space. These embeddings capture the semantic meaning and relationships
    between words based on the context in which they appear in the text. Word2Vec
    has two main architectures. As mentioned previously, the two main architectures
    that were designed to learn word embeddings are **CBOW** and **skip-gram**. Both
    architectures are designed to learn word embeddings by predicting words based
    on their surrounding context:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是一组基于神经网络的模型，用于创建词嵌入，这些嵌入是词在连续向量空间中的密集向量表示。这些嵌入根据词在文本中出现的上下文捕获词的语义意义和关系。Word2Vec
    有两种主要架构。如前所述，设计用来学习词嵌入的两种主要架构是 **CBOW** 和 **skip-gram**。两种架构都是通过预测周围上下文中的词来学习词嵌入的：
- en: '**CBOW**: The CBOW architecture aims to predict the target word given its surrounding
    context words. It takes the average of the context word embeddings as input and
    predicts the target word. CBOW is faster to train and works well with smaller
    datasets but may be less accurate for infrequent words.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CBOW**：CBOW 架构旨在根据周围上下文词预测目标词。它以上下文词嵌入的平均值作为输入，并预测目标词。CBOW 训练速度快，适用于小型数据集，但对于不常见的词可能不够准确。'
- en: 'In the CBOW model, the objective is to maximize the average log probability
    of observing the target word given the context words:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 CBOW 模型中，目标是最大化观察目标词给定上下文词的平均对数概率：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><mrow><mi>C</mi><mi>B</mi><mi>o</mi><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow></mrow><mo>(</mo><mi>P</mi><mo>(</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/264.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><msub><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi></mrow><mrow><mi>C</mi><mi>B</mi><mi>o</mi><mi>w</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi>l</mi><mi>o</mi><mi>g</mi></mrow></mrow><mo>(</mo><mi>P</mi><mo>(</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>)</mo><mo>)</mo></mrow></mrow></mrow></math>](img/264.png)'
- en: 'Here, T is the total number of words in the text, and P(target | context) is
    the probability of observing the target word given the context words, which is
    calculated using the softmax function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，T 是文本中的总词数，P(target | context) 是在给定上下文词的情况下观察目标词的概率，它使用 softmax 函数计算：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/265.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>|</mo><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/265.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/266.png)
    is the output vector (word embedding) of the target word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/267.png)
    is the average input vector (context word embedding) of the context words, and
    the sum in the denominator runs over all words in the vocabulary.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/266.png)
    是目标词的输出向量（词嵌入），![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/267.png)
    是上下文词的平均输入向量（上下文词嵌入），分母中的求和遍历了词汇表中的所有词。
- en: '**Skip-gram**: The skip-gram architecture aims to predict the surrounding context
    words given the target word. It takes the target word embedding as input and predicts
    the context words. Skip-gram works well with larger datasets and can capture the
    meaning of infrequent words more accurately, but it may be slower to train compared
    to CBOW.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-gram**：skip-gram 架构旨在根据目标词预测周围的上下文词。它以目标词嵌入作为输入并预测上下文词。Skip-gram 在大型数据集上表现良好，并能更准确地捕捉不常见词的意义，但与
    CBOW 相比，它可能训练速度较慢。'
- en: 'In the skip-gram model, the objective is to maximize the average log probability
    of observing the context words given the target word:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 skip-gram 模型中，目标是最大化在给定目标词的情况下观察上下文词的平均对数概率：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/268.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>O</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>k</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mo>-</mml:mo><mml:mi>G</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:munder><mml:mo
    stretchy="false">∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi
    mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi
    mathvariant="normal">g</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math>](img/268.png)'
- en: 'Here, T is the total number of words in the text, and P(context | target) is
    the probability of observing the context words given the target word, which is
    calculated using the softmax function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，T 是文本中的总单词数，P(context | target) 是在给定目标词的情况下观察上下文词的概率，该概率使用 softmax 函数计算：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/269.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/269.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/270.png)is
    the output vector (context word embedding) of the context word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)is
    the input vector (word embedding) of the target word, and the sum in the denominator
    runs over all words in the vocabulary.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/270.png)是上下文词的输出向量（上下文词嵌入），![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/271.png)是目标词的输入向量（词嵌入），分母中的求和遍历词汇表中的所有词。
- en: The training process for both CBOW and skip-gram involves iterating through
    the text and updating the input and output weight matrices using **stochastic
    gradient descent** (**SGD**) and backpropagation to minimize the difference between
    the predicted words and the actual words. The learned input weight matrix contains
    the word embeddings for each word in the vocabulary.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW和skip-gram的训练过程都涉及遍历文本，并使用**随机梯度下降**（**SGD**）和反向传播来更新输入和输出权重矩阵，以最小化预测词和实际词之间的差异。学习到的输入权重矩阵包含词汇表中每个词的词嵌入。
- en: Text classification using Word2Vec
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Word2Vec进行文本分类
- en: 'Text classification using Word2Vec involves creating word embeddings using
    the Word2Vec algorithm and then training an ML model to classify text based on
    these embeddings. The following steps outline the process in detail, including
    the mathematical aspects:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Word2Vec进行文本分类涉及使用Word2Vec算法创建词嵌入，然后训练一个机器学习模型来根据这些嵌入对文本进行分类。以下步骤详细概述了该过程，包括数学方面：
- en: '**Text preprocessing**: Clean and preprocess the text data by tokenizing, lowercasing,
    removing stop words, and stemming or lemmatizing the words.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本预处理**：通过分词、小写化、去除停用词以及词干提取或词形还原来清洁和预处理文本数据。'
- en: '**Train the Word2Vec model**: Train a Word2Vec model (either CBOW or Skip-Gram)
    on the preprocessed text data to create word embeddings. The Word2Vec algorithm
    learns to predict a target word based on its context (CBOW) or predict the context
    words based on a target word (skip-gram). The training objective is to maximize
    the average log probability of observing the context words given the target word:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练Word2Vec模型**：在预处理后的文本数据上训练一个Word2Vec模型（无论是CBOW还是Skip-Gram），以创建词嵌入。Word2Vec算法学习根据上下文（CBOW）预测目标词，或者根据目标词预测上下文词（skip-gram）。训练目标是最大化在给定目标词的情况下观察上下文词的平均对数概率：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/272.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>O</mi><mi>b</mi><mi>j</mi><mi>e</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mo>=</mo><mfrac><mn>1</mn><mi>T</mi></mfrac><mrow><munder><mo>∑</mo><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow></munder><mrow><mi
    mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>)</mo></mrow></mrow></mrow></mrow></math>](img/272.png)'
- en: 'Here, *T* is the total number of words in the text, and *P(context | target)*
    is the probability of observing the context words given the target word, which
    is calculated using the softmax function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*T* 是文本中的总单词数，*P(context | target)* 是在给定目标词的情况下观察上下文词的概率，这使用softmax函数来计算：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/273.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mo>|</mo><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></mfenced><mo>=</mo><mfrac><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mrow><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi></mrow><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup><mrow><msub><mo>∑</mo><mi>i</mi></msub><msup><mi>e</mi><mrow><msubsup><mi
    mathvariant="bold">v</mi><mi>i</mi><mi>T</mi></msubsup><msub><mrow><mo>∙</mo><mi
    mathvariant="bold">v</mi></mrow><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi></mrow></msub></mrow></msup></mrow></mfrac></mrow></mrow></math>](img/273.png)'
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/274.png)is
    the output vector (context word embedding) of the context word, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/275.png)
    is the input vector (word embedding) of the target word, and the sum in the denominator
    runs over all words in the vocabulary.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msubsup><mml:mrow><mml:mi
    mathvariant="bold">v</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:math>](img/274.png)是上下文词的输出向量（上下文词嵌入），![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">v</mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mi><mml:mi>e</mml:mi><mml:mi>t</mi></mml:mrow></mml:msub></mml:math>](img/275.png)是目标词的输入向量（词嵌入），分母中的求和遍历了词汇表中的所有单词。
- en: '3. **Create document embeddings**: For each document in the dataset, calculate
    the document embedding by averaging the word embeddings of the words in the document:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 3. **创建文档嵌入**：对于数据集中的每个文档，通过计算文档中单词的词嵌入的平均值来计算文档嵌入：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>E</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi
    mathvariant="normal">W</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi
    mathvariant="normal">g</mi></mrow><mi mathvariant="normal">i</mi></msub></mrow></mrow></mrow></math>](img/276.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>D</mi><mi>o</mi><mi>c</mi><mi>u</mi><mi>m</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>E</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>d</mi><mi>d</mi><mi>i</mi><mi>n</mi><mi>g</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi
    mathvariant="normal">W</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">E</mi><mi mathvariant="normal">m</mi><mi
    mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mi
    mathvariant="normal">d</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi
    mathvariant="normal">g</mi></mrow><mi mathvariant="normal">i</mi></msub></mrow></mrow></mrow></math>](img/276.png)'
- en: Here, N is the number of words in the document, and the sum runs over all words
    in the document. Please note that based on our experience, this approach for text
    classification using Word2Vec is only useful when the document’s length is short.
    If you have longer documents or there are opposite words in the document, this
    approach won’t perform well. An alternative solution is to use Word2Vec and CNN
    together to fetch the word embeddings and then feed those embeddings as input
    of the CNN.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，N是文档中的单词数量，求和遍历文档中的所有单词。请注意，根据我们的经验，使用Word2Vec进行文本分类的方法仅在文档长度较短时才有效。如果你有较长的文档或文档中有相反的词语，这种方法表现不佳。一个替代方案是将Word2Vec和CNN结合使用来获取词嵌入，然后将这些嵌入作为CNN的输入。
- en: '4. **Model training**: Use the document embeddings as features to train an
    ML model, such as logistic regression, SVM, or a neural network, for text classification.
    The model learns to predict the class label based on the document embeddings.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 4. **模型训练**：使用文档嵌入作为特征来训练一个机器学习模型，如逻辑回归、SVM或神经网络，用于文本分类。该模型学习根据文档嵌入预测类别标签。
- en: '5. **Model evaluation**: Evaluate the performance of the model using appropriate
    evaluation metrics, such as accuracy, precision, recall, F1 score, or confusion
    matrix, and use techniques such as cross-validation to get a reliable estimate
    of the model’s performance on unseen data.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 5. **模型评估**：使用适当的评估指标（如准确率、精确率、召回率、F1分数或混淆矩阵）评估模型的性能，并使用交叉验证等技术来获得模型在未见数据上的可靠性能估计。
- en: '6. **Model application**: Apply the trained model to new, unseen text data.
    Preprocess and compute the document embeddings for the new text data using the
    same Word2Vec model and vocabulary, and use the model to predict the class labels.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 6. **模型应用**：将训练好的模型应用于新的、未见过的文本数据。使用相同的Word2Vec模型和词汇表对新的文本数据进行预处理和计算文档嵌入，并使用该模型预测类别标签。
- en: In summary, text classification using Word2Vec involves creating word embeddings
    with the Word2Vec algorithm, averaging these embeddings to create document embeddings,
    and training an ML model to classify text based on these document embeddings.
    The Word2Vec algorithm learns word embeddings by maximizing the average log probability
    of observing context words given a target word, capturing the semantic relationships
    between words in the process.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，使用Word2Vec进行文本分类涉及使用Word2Vec算法创建词嵌入，平均这些嵌入以创建文档嵌入，并训练一个机器学习模型根据这些文档嵌入对文本进行分类。Word2Vec算法通过最大化观察给定目标词的上下文词的平均对数概率来学习词嵌入，在这个过程中捕捉词语之间的语义关系。
- en: Model evaluation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'Evaluating the performance of text classification models is crucial to ensure
    that they meet the desired level of accuracy and generalizability. Several metrics
    and techniques are commonly used to evaluate text classification models, including
    accuracy, precision, recall, F1 score, and confusion matrix. Let’s discuss each
    of these in more detail:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 评估文本分类模型的性能对于确保它们达到期望的准确性和泛化水平至关重要。通常使用多种指标和技术来评估文本分类模型，包括准确率、精确率、召回率、F1分数和混淆矩阵。让我们更详细地讨论这些指标：
- en: '**Accuracy**: Accuracy is the most straightforward metric for classification
    tasks. It measures the number of correctly classified records out of all classified
    records. It is defined as follows:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：准确性是分类任务中最直接的指标。它衡量所有分类记录中正确分类的记录数量。它被定义为如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/277.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>A</mi><mi>c</mi><mi>c</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>c</mi><mi>y</mi><mo>=</mo><mfrac><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow><mrow><mo>(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>)</mo></mrow></mfrac></mrow></mrow></math>](img/277.png)'
- en: While accuracy is easy to understand, it may not be the best metric for imbalanced
    datasets, where the majority class can dominate the metric’s value.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然准确性容易理解，但它可能不是不平衡数据集的最佳指标，因为在不平衡数据集中，多数类可以主导指标的值。
- en: '**Precision**: Precision gauges the ratio of correctly identified positive
    instances to the total instances predicted as positive by the model. It is also
    referred to as **positive predictive value** (**PPV**). Precision proves valuable
    in scenarios where the expense associated with false positives is significant.
    Precision is defined as follows:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度**：精确度衡量正确识别的正例与模型预测为正的总实例的比例。它也被称为**阳性预测值**（**PPV**）。在关联到错误正例成本较高的场景中，精确度非常有价值。精确度被定义为如下：'
- en: '[[OMML-EQ-21D]]'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[[OMML-EQ-21D]]'
- en: '**Recall**: Recall, also recognized as sensitivity or the **true positive rate**
    (**TPR**), assesses the ratio of correctly identified positive instances among
    the total actual positive instances. Recall is useful when the cost of false negatives
    is high. Mathematically, it is defined as follows:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**：召回率，也称为灵敏度或**真正例率**（**TPR**），评估正确识别的正例与总实际正例的比例。当错误负例的成本很高时，召回率非常有用。数学上，它被定义为如下：'
- en: '[[OMML-EQ-22D]]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[[OMML-EQ-22D]]'
- en: '**F1 score**: The F1 score, derived as the harmonic mean of precision and recall,
    integrates both metrics into a unified value. It is an important metric in the
    context of imbalanced datasets as it considers both false positives and false
    negatives. Spanning from 0 to 1, with 1 representing the optimal outcome, the
    F1 score is mathematically expressed as follows:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 分数**：F1 分数是精确度和召回率的调和平均值，将这两个指标整合为一个统一的价值。在不平衡数据集的上下文中，它是一个重要的指标，因为它考虑了错误正例和错误负例。F1
    分数从 0 到 1 变化，1 代表最佳结果，F1 分数的数学表达式如下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mn>2</mn><mfrac><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>∙</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mrow></math>](img/278.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>F</mi><mn>1</mn><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mn>2</mn><mfrac><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi}s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>∙</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi}s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mrow></math>](img/278.png)'
- en: 'When dealing with multi-class classification, we have F1 micro and F1 macro.
    F1 micro and F1 macro are two ways to compute the F1 score for multi-class or
    multi-label classification problems. They aggregate precision and recall differently,
    leading to different interpretations of the classifier’s performance. Let’s discuss
    each in more detail:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理多类分类问题时，我们有F1微平均和F1宏平均。F1微平均和F1宏平均是计算多类或多标签分类问题F1分数的两种方法。它们以不同的方式聚合精确度和召回率，导致对分类器性能的不同解释。让我们更详细地讨论每种方法：
- en: '**F1 macro**: F1 macro computes the F1 score for each class independently and
    then takes the average of those values. This approach treats each class as equally
    important and does not consider the class imbalance. Mathematically, F1 macro
    is defined as follows:'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1宏平均**：F1宏平均独立计算每个类的F1分数，然后取这些值的平均值。这种方法将每个类视为同等重要，不考虑类不平衡。从数学上讲，F1宏平均定义为以下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>F</mi><mn>1</mn></mrow><mrow><mi>M</mi><mi>a</mi><mi>c</mi><mi>r</mi><mi>o</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi>F</mi><mn>1</mn></mrow><mi>i</mi></msub></mrow></mrow></mrow></math>](img/279.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>F</mi><mn>1</mn></mrow><mrow><mi>M</mi><mi>a</mi><mi>c</mi><mi>r</mi><mi>o</mi></mrow></msub><mo>=</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mrow><mi>F</mi><mn>1</mn></mrow><mi>i</mi></msub></mrow></mrow></mrow></math>](img/279.png)'
- en: Here, *n* is the number of classes, and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png)
    is the F1 score for the i-th class.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n* 是类的数量，而 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/280.png)
    是第i个类的F1分数。
- en: F1 macro is particularly useful when you want to evaluate the performance of
    a classifier across all classes without giving more weight to the majority class.
    However, it may not be suitable when the class distribution is highly imbalanced
    as it can provide an overly optimistic estimate of the model’s performance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: F1宏平均特别适用于当你想要评估分类器在所有类上的性能而不给多数类赋予更多权重时。然而，当类分布高度不平衡时，它可能不适用，因为它可能会提供一个过于乐观的模型性能估计。
- en: '**F1 micro**: F1 micro, on the other hand, aggregates the contributions of
    all classes to compute the F1 score. It does this by calculating the global precision
    and recall values across all classes and then computing the F1 score based on
    these global values. F1 micro takes class imbalance into account as it considers
    the number of instances in each class. Mathematically, F1 micro is defined as
    follows:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1微平均**：另一方面，F1微平均通过计算所有类的全局精确度和召回率值来聚合所有类的贡献以计算F1分数。它通过考虑每个类中的实例数量来考虑类不平衡。从数学上讲，F1微平均定义为以下：'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mrow><mi>F</mi><mn>1</mn></mrow><mrow><mi>M</mi><mi>i</mi><mi>c</mi><mi>r</mi><mi>o</mi></mrow></msub><mo>=</mo><mn>2</mn><mfrac><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>∙</mo><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>+</mo><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi></mrow></mfrac></mrow></mrow></math>](img/281.png)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![F1 微型评估器的计算公式](img/281.png)'
- en: 'Here, global precision and global recall are calculated as follows:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，全局精度和全局召回率的计算方法如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>P</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mo>∑</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/282.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![全局精度和全局召回率的计算如下](img/282.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>G</mi><mi>l</mi><mi>o</mi><mi>b</mi><mi>a</mi><mi>l</mi><mi>R</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow><mrow><mo>∑</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>e</mi><mi>P</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi><mo>+</mo><mo>∑</mo><mi>F</mi><mi>a</mi><mi>l</mi><mi>s</mi><mi>e</mi><mi>N</mi><mi>e</mi><mi>g</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>v</mi><mi>e</mi><mi>s</mi></mrow></mfrac></mrow></mrow></math>](img/283.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![全局召回率的计算公式](img/283.png)'
- en: F1 micro is useful when you want to evaluate the overall performance of a classifier
    considering the class distribution, especially when dealing with imbalanced datasets.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: F1 微型评估器在您希望评估分类器的整体性能并考虑类别分布时非常有用，尤其是在处理不平衡数据集时。
- en: In summary, F1 macro and F1 micro are two ways to compute the F1 score for multi-class
    or multi-label classification problems. F1 macro treats each class as equally
    important, regardless of the class distribution, while F1 micro takes class imbalance
    into account by considering the number of instances in each class. The choice
    between F1 macro and F1 micro depends on the specific problem and whether class
    imbalance is an important factor to consider.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，F1宏平均和F1微平均是计算多类或多标签分类问题F1分数的两种方法。F1宏平均将每个类别视为同等重要，不考虑类别分布，而F1微平均通过考虑每个类别的实例数量来考虑类别不平衡。F1宏平均和F1微平均的选择取决于具体问题以及是否将类别不平衡视为一个重要的考虑因素。
- en: Confusion matrix
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix serves as a tabular representation, showcasing the count
    of true positive, true negative, false positive, and false negative predictions
    made by a classification model. This matrix offers a nuanced perspective on the
    model’s efficacy, enabling a thorough comprehension of both its strengths and
    weaknesses.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵作为一种表格形式的表示，展示了分类模型做出的真实阳性、真实阴性、假阳性和假阴性预测的数量。这个矩阵提供了对模型有效性的细致视角，使得可以全面理解其优势和劣势。
- en: 'For a binary classification problem, the confusion matrix is arranged as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类问题，混淆矩阵的排列如下：
- en: '| **Actual/Predicted** | **(****Predicted) Positive** | **(****Predicted) Negative**
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **实际/预测** | **(预测) 阳性** | **(预测) 阴性** |'
- en: '| --- | --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| (Actual) Positive | True Positive | False Negative |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| (实际) 阳性 | 真阳性 | 假阴性 |'
- en: '| (Actual) Negative | False Positive | True Negative |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| (实际) 阴性 | 假阳性 | 真阴性 |'
- en: Table 5.2 – Confusion matrix – general view
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 – 混淆矩阵 – 通用视图
- en: For multi-class classification problems, the confusion matrix is extended to
    include the true and predicted counts for each class. The diagonal elements represent
    the correctly classified instances, while the off-diagonal elements represent
    misclassifications.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多类分类问题，混淆矩阵扩展到包括每个类别的真实和预测计数。对角线元素代表正确分类的实例，而偏对角线元素代表错误分类。
- en: In summary, evaluating text classification models involves using various metrics
    and techniques, such as accuracy, precision, recall, F1 score, and the confusion
    matrix. Selecting the appropriate evaluation metrics depends on the specific problem,
    dataset characteristics, and the trade-offs between false positives and false
    negatives. Evaluating a model using multiple metrics can provide a more comprehensive
    understanding of its performance and help guide further improvements.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，评估文本分类模型涉及使用各种指标和技术，如准确率、精确率、召回率、F1分数和混淆矩阵。选择适当的评估指标取决于具体问题、数据集特征以及假阳性和假阴性之间的权衡。使用多个指标评估模型可以提供对其性能的更全面理解，并有助于指导进一步的改进。
- en: Overfitting and underfitting
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: Overfitting and underfitting are two common issues that arise during the training
    of ML models, including text classification models. They both relate to how well
    a model generalizes to new, unseen data. This section will explain overfitting
    and underfitting, when they happen, and how to prevent them.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合是机器学习模型训练过程中，包括文本分类模型训练中常见的两个问题。它们都与模型对新、未见数据的泛化能力有关。本节将解释过拟合和欠拟合的发生情况以及如何防止它们。
- en: Overfitting
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: Overfitting arises when a model excessively tailors itself to the intricacies
    of the training data. In this case, the model captures noise and random fluctuations
    rather than discerning the fundamental patterns. Consequently, although the model
    may exhibit high performance on the training data, its effectiveness diminishes
    when applied to unseen data, such as a validation or test set.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合发生在模型过度适应训练数据的复杂性时。在这种情况下，模型捕捉到噪声和随机波动，而不是辨别基本模式。因此，尽管模型可能在训练数据上表现出高性能，但当应用于未见数据（如验证集或测试集）时，其有效性会降低。
- en: 'To avoid overfitting in text classification, consider the following strategies:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免文本分类中的过拟合，可以考虑以下策略：
- en: '**Regularization**: Introduce regularization techniques, such as ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/284.png)
    or ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi mathvariant="script">l</mi><mn>2</mn></msub></mrow></math>](img/285.png)L2
    regularization, which add a penalty to the loss function, discouraging overly
    complex models.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：引入正则化技术，例如 ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/284.png)
    或 ![<math xmlns="http://www.w3.org/1998/Math/MathML"><mrow><msub><mi mathvariant="script">l</mi><mn>2</mn></msub></mrow></math>](img/285.png)L2正则化，这会在损失函数中添加惩罚，从而阻止模型过于复杂。'
- en: '**Early stopping**: In this approach, we monitor the performance of the model
    on the validation set, and stop the training process as soon as the performance
    on the validation set starts getting worse, even though the model performance
    on the training set is getting better. It helps us to prevent overfitting.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早停法**：在此方法中，我们监控模型在验证集上的性能，一旦验证集上的性能开始变差，即使训练集上的模型性能在提高，我们也停止训练过程。这有助于我们防止过拟合。'
- en: '**Feature selection**: Reduce the number of features used for classification
    by selecting the most informative features or using dimensionality reduction techniques
    such as PCA or LSA.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：通过选择最有信息量的特征或使用PCA或LSA等降维技术来减少用于分类的特征数量。'
- en: '**Ensemble methods**: Combine multiple models, such as bagging or boosting,
    to reduce overfitting by averaging their predictions.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：通过结合多个模型，例如袋装法或提升法，通过平均它们的预测来减少过拟合。'
- en: '**Cross-validation**: Use k-fold cross-validation to get a more reliable estimate
    of model performance on unseen data and fine-tune model hyperparameters accordingly.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：使用k折交叉验证来获得对未见数据上模型性能的更可靠估计，并相应地微调模型超参数。'
- en: Next, we’ll cover underfitting.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍欠拟合问题。
- en: Underfitting
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 欠拟合
- en: Underfitting happens when a model is too simple and fails to capture the underlying
    patterns in the data. Consequently, the model performance is low on both training
    and test data. The model is too simple to represent the complexity of the data
    and can’t generalize well.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型过于简单且无法捕捉数据中的潜在模式时，就会发生欠拟合。因此，模型在训练和测试数据上的性能都较低。模型过于简单，无法表示数据的复杂性，并且无法很好地泛化。
- en: 'To avoid underfitting in text classification, consider the following strategies:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在文本分类中发生欠拟合，可以考虑以下策略：
- en: '**Increase model complexity**: Use a more complex model, such as a deeper neural
    network, to capture more intricate patterns in the data.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加模型复杂性**：使用更复杂的模型，例如更深的神经网络，以捕捉数据中的更复杂模式。'
- en: '**Feature engineering**: Create new, informative features that help the model
    better understand the underlying patterns in the text data, such as adding N-grams
    or using word embeddings.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：创建新的、有信息量的特征，帮助模型更好地理解文本数据中的潜在模式，例如添加N-gram或使用词嵌入。'
- en: '**Hyperparameter tuning**: Optimize model hyperparameters, such as the learning
    rate, number of layers, or number of hidden units, to improve the model’s ability
    to learn from the data. We’ll explain hyperparameter tuning and the different
    methods to perform this task in the next section.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调整**：优化模型超参数，例如学习率、层数或隐藏单元数，以提高模型从数据中学习的能力。我们将在下一节中解释超参数调整以及执行此任务的不同方法。'
- en: '**Increase training data**: If possible, collect more labeled data for training,
    as more examples can help the model learn the underlying patterns better.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加训练数据**：如果可能，收集更多标记数据用于训练，因为更多的例子可以帮助模型更好地学习潜在模式。'
- en: '**Reduce regularization**: If the model is heavily regularized, consider reducing
    the regularization strength, allowing the model to become more complex and better
    fit the data.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少正则化**：如果模型过度正则化，考虑减少正则化强度，使模型变得更加复杂，更好地拟合数据。'
- en: In summary, overfitting and underfitting are two common issues in text classification
    that affect a model’s ability to generalize to new data. Avoiding these issues
    involves balancing model complexity, using appropriate features, tuning hyperparameters,
    employing regularization, and monitoring model performance on a validation set.
    By addressing overfitting and underfitting, you can improve the performance and
    generalizability of your text classification models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，过拟合和欠拟合是文本分类中常见的两个问题，它们会影响模型泛化到新数据的能力。避免这些问题涉及平衡模型复杂性、使用适当的特征、调整超参数、应用正则化和监控模型在验证集上的性能。通过解决过拟合和欠拟合问题，可以提高文本分类模型的表现和泛化能力。
- en: Hyperparameter tuning
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: An important step in building an effective classification model is hyperparameter
    tuning. Hyperparameters are the model parameters that are defined before training;
    they will not change during training. These parameters determine the model architecture
    and behavior. Some of the hyperparameters that can be used are the learning rate
    and the number of iterations. They can significantly impact the model’s performance
    and generalizability.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个有效的分类模型的一个重要步骤是超参数调整。超参数是在训练之前定义的模型参数；它们在训练过程中不会改变。这些参数决定了模型架构和行为。可以使用的某些超参数包括学习率和迭代次数。它们可以显著影响模型的表现和泛化能力。
- en: 'The process of hyperparameter tuning in text classification involves the following
    steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类中超参数调整的过程涉及以下步骤：
- en: '**Define the hyperparameters and their search space**: Identify the hyperparameters
    you want to optimize and specify the range of possible values for each of them.
    Common hyperparameters in text classification include the learning rate, number
    of layers, number of hidden units, dropout rate, regularization strength, and
    feature extraction parameters such as N-grams or vocabulary size.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义超参数及其搜索空间**：确定您想要优化的超参数，并指定每个超参数的可能值范围。文本分类中常见的超参数包括学习率、层数、隐藏单元数量、dropout率、正则化强度以及特征提取参数，如N-gram或词汇大小。'
- en: '**Choose a search strategy**: Select a method to explore the hyperparameter
    search space, such as grid search, random search, or Bayesian optimization. Grid
    search systematically evaluates all combinations of hyperparameter values, while
    random search samples random combinations within the search space. Bayesian optimization
    uses a probabilistic model to guide the search, balancing exploration and exploitation
    based on the model’s predictions.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择搜索策略**：选择一种探索超参数搜索空间的方法，例如网格搜索、随机搜索或贝叶斯优化。网格搜索系统地评估所有超参数值的组合，而随机搜索在搜索空间内随机采样组合。贝叶斯优化使用概率模型来指导搜索，根据模型的预测平衡探索和利用。'
- en: '**Choose an evaluation metric and method**: Select a performance metric that
    best represents the goals of your text classification task, such as accuracy,
    precision, recall, F1 score, or area under the ROC curve. Also, choose an evaluation
    method, such as k-fold cross-validation, to get a reliable estimate of the model’s
    performance on unseen data.'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择评估指标和方法**：选择一个最能代表您文本分类任务目标的性能指标，例如准确率、精确率、召回率、F1分数或ROC曲线下的面积。同时，选择一个评估方法，如k折交叉验证，以获得模型在未见数据上的性能的可靠估计。'
- en: '**Perform the search**: For each combination of hyperparameter values, train
    a model on the training data, and evaluate its performance using the chosen metric
    and evaluation method. Keep track of the best-performing hyperparameter combination.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行搜索**：对于每个超参数值的组合，在训练数据上训练一个模型，并使用所选的指标和评估方法评估其性能。记录最佳性能的超参数组合。'
- en: '**Select the best hyperparameters**: After the search is complete, select the
    hyperparameter combination that yields the best performance on the evaluation
    metric. Retrain the model using these hyperparameters on the entire training set.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择最佳超参数**：搜索完成后，选择在评估指标上产生最佳性能的超参数组合。使用这些超参数在整个训练集上重新训练模型。'
- en: '**Evaluate on the test set**: Assess the performance of the final model with
    the optimized hyperparameters on a held-out test set to get an unbiased estimate
    of its generalizability.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在测试集上评估**：使用保留的测试集评估最终模型在优化超参数下的性能，以获得其泛化能力的无偏估计。'
- en: Hyperparameter tuning affects the performance of the model by finding the optimal
    combination of parameters that results in the best model performance on the chosen
    evaluation metric. Tuning hyperparameters can help address issues such as overfitting
    and underfitting, balance model complexity, and improve the model’s ability to
    generalize to new data.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整通过找到在所选评估指标上产生最佳模型性能的参数组合来影响模型的性能。调整超参数可以帮助解决诸如过拟合和欠拟合等问题，平衡模型复杂性，并提高模型对新数据的泛化能力。
- en: Hyperparameter tuning is a crucial process in text classification that involves
    searching for the optimal combination of model parameters to maximize performance
    on a chosen evaluation metric. By carefully tuning hyperparameters, you can improve
    the performance and generalizability of your text classification models.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是文本分类中的一个关键过程，涉及在选择的评估指标上搜索最优模型参数组合以最大化性能。通过仔细调整超参数，你可以提高文本分类模型的性能和泛化能力。
- en: Additional topics in applied text classification
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用文本分类的附加主题
- en: In the real world, applying text classification involves various practical considerations
    and challenges that arise from the nature of real-world data and problem requirements.
    Some common issues include dealing with imbalanced datasets, handling noisy data,
    and choosing appropriate evaluation metrics.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，应用文本分类涉及各种实际考虑和挑战，这些挑战源于现实世界数据的性质和问题要求。一些常见问题包括处理不平衡数据集、处理噪声数据和选择合适的评估指标。
- en: Let’s discuss each of these in more detail.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些内容。
- en: Dealing with imbalanced datasets
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: 'Text classification tasks often encounter imbalanced datasets, wherein certain
    classes boast a notably higher number of instances compared to others. This imbalance
    can result in models that are skewed, excelling in predicting the majority class
    while faltering in accurately classifying the minority class. To handle imbalanced
    datasets, consider the following strategies:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类任务经常遇到不平衡数据集的问题，其中某些类别的实例数量明显多于其他类别。这种不平衡可能导致模型偏向，擅长预测多数类，但在准确分类少数类方面表现不佳。为了处理不平衡数据集，可以考虑以下策略：
- en: '**Resampling**: You can oversample the minority class, undersample the majority
    class, or use a combination of both to balance the class distribution.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重采样**：你可以对少数类进行过采样，对多数类进行欠采样，或者使用两者的组合来平衡类别分布。'
- en: '**Weighted loss function**: Assign higher weights to the minority class in
    the loss function, making the model more sensitive to misclassifications in the
    minority class.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权损失函数**：在损失函数中为少数类分配更高的权重，使模型对少数类的误分类更加敏感。'
- en: '**Ensemble methods**: Use ensemble techniques such as bagging or boosting with
    a focus on the minority class. For example, you can use random under-sampling
    with bagging or cost-sensitive boosting algorithms.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：使用关注少数类的集成技术，如袋装或提升。例如，你可以使用带有袋装的随机欠采样或成本敏感的提升算法。'
- en: '**Evaluation metrics**: Choose evaluation metrics that are less sensitive to
    class imbalance, such as precision, recall, F1 score, or area under the ROC curve,
    instead of accuracy.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估指标**：选择对类别不平衡不那么敏感的评估指标，如精确度、召回率、F1分数或ROC曲线下的面积，而不是准确度。'
- en: '**Handling noisy data**: Real-world text data is often noisy, containing misspellings,
    grammatical errors, or irrelevant information. Noisy data can negatively impact
    the performance of text classification models.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理噪声数据**：现实世界的文本数据通常很嘈杂，包含拼写错误、语法错误或不相关信息。噪声数据可能会对文本分类模型的性能产生负面影响。'
- en: 'To handle noisy data, consider the following strategies:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理噪声数据，可以考虑以下策略：
- en: '**Preprocessing**: Clean the text data by correcting misspellings, removing
    special characters, expanding contractions, and converting text into lowercase'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：通过纠正拼写错误、删除特殊字符、扩展缩写词和将文本转换为小写来清洗文本数据。'
- en: '**Stopword removal**: Remove common words that do not carry much meaning, such
    as “the,” “is,” “and,” and so on'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词去除**：移除没有太多意义的常见词，例如“the”、“is”、“and”等。'
- en: '**Stemming or lemmatization**: Reduce words to their root form to minimize
    the impact of morphological variations'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取或词形还原**：将单词还原为其基本形式，以最小化形态变化的影响。'
- en: '**Feature selection**: Use techniques such as chi-square or mutual information
    to select the most informative features, reducing the impact of noisy or irrelevant
    features'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：使用如卡方检验或互信息等技术来选择最有信息量的特征，减少噪声或不相关特征的影响'
- en: Whether we’re working on imbalanced data or not, we always need to evaluate
    our model, and choosing the right metric to evaluate our model is important. Next,
    we’ll explain how to select the best metric to evaluate our model.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是否在处理不平衡数据，我们都需要评估我们的模型，选择合适的指标来评估我们的模型非常重要。接下来，我们将解释如何选择最佳的指标来评估我们的模型。
- en: Choosing appropriate evaluation metrics
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的评估指标
- en: Selecting the right evaluation metrics is crucial for measuring the performance
    of your text classification model and guiding model improvements.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的评估指标对于衡量您的文本分类模型性能和指导模型改进至关重要。
- en: 'Consider the following when choosing evaluation metrics:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 选择评估指标时考虑以下因素：
- en: '**Problem requirements**: Choose metrics that align with the specific goals
    of your text classification task, such as minimizing false positives or false
    negatives'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题要求**：选择与您的文本分类任务具体目标一致的指标，例如最小化误报或误判'
- en: '**Class imbalance**: For imbalanced datasets, use metrics that account for
    class imbalance, such as precision, recall, F1 score, or area under the ROC curve,
    instead of accuracy'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别不平衡**：对于不平衡的数据集，使用考虑类别不平衡的指标，如精确率、召回率、F1分数或ROC曲线下的面积，而不是准确率'
- en: '**Multi-class or multi-label problems**: For multi-class or multi-label classification
    tasks, use metrics such as micro- and macro-averaged F1 scores, which aggregate
    precision and recall differently based on the problem’s requirements'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类或多标签问题**：对于多类或多标签分类任务，使用如微平均和宏平均F1分数等指标，这些指标根据问题的要求以不同的方式聚合精确率和召回率'
- en: In summary, practical considerations in text classification include dealing
    with imbalanced datasets, handling noisy data, and choosing appropriate evaluation
    metrics. Addressing these issues can help improve the performance and generalizability
    of your text classification models and ensure that they meet the specific requirements
    of your problem.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，文本分类中的实际考虑因素包括处理不平衡数据集、处理噪声数据以及选择合适的评估指标。解决这些问题可以帮助提高文本分类模型的表现力和泛化能力，并确保它们满足问题的具体要求。
- en: Topic modeling – a particular use case of unsupervised text classification
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模——无监督文本分类的一个特定用例
- en: Topic modeling is an unsupervised ML technique that’s used to discover abstract
    topics or themes within a large collection of documents. It assumes that each
    document can be represented as a mixture of topics, and each topic is represented
    as a distribution over words. The goal of topic modeling is to find the underlying
    topics and their word distributions, as well as the topic proportions for each
    document.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是一种无监督机器学习技术，用于在大量文档集中发现抽象主题或主题。它假设每个文档可以表示为多个主题的混合，每个主题表示为词分布。主题建模的目的是找到潜在的主题及其词分布，以及每个文档的主题比例。
- en: There are several topic modeling algorithms, but one of the most popular and
    widely used is LDA. We will discuss LDA in detail, including its mathematical
    formulation.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种主题建模算法，但其中最受欢迎和最广泛使用的是LDA。我们将详细讨论LDA，包括其数学公式。
- en: LDA
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA
- en: 'LDA is a generative probabilistic model that assumes the following generative
    process for each document:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种生成概率模型，它假设每个文档的以下生成过程：
- en: Choose the number of words in the document.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择文档中的词数。
- en: Choose a topic distribution (*θ*) for the document from a Dirichlet distribution
    with parameter α.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从参数α的Dirichlet分布中选择文档的主题分布（θ）。
- en: 'For each word in the document, do the following:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档中的每个词，执行以下操作：
- en: Choose a topic (*z*) from the topic distribution (*θ*).
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从主题分布（θ）中选择一个主题（z）。
- en: Choose a word (*w*) from the word distribution of the chosen topic (*φ*), which
    is a distribution over words for that topic, drawn from a Dirichlet distribution
    with parameter *β*.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所选主题（φ）的词分布中选择一个词（w），该词分布是该主题的词分布，由参数β的Dirichlet分布抽取。
- en: The generative process is a theoretical model used by LDA to reverse-engineer
    the original documents from presumed topics.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程是LDA（Latent Dirichlet Allocation）用来从假设的主题中逆向工程原始文档的理论模型。
- en: LDA aims to find the topic-word distributions (*φ*) and document-topic distributions
    (*θ*) that best explain the observed documents.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 的目标是找到最佳解释观察到的文档的主题-单词分布 (*φ*) 和文档-主题分布 (*θ*)。
- en: 'Mathematically, LDA can be described using the following notation:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，LDA 可以用以下符号描述：
- en: '*M*: Number of documents'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*M*: 文档数量'
- en: '*N*: Number of words in a document'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*: 文档中的单词数量'
- en: '*K*: Number of topics'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K*: 主题数量'
- en: '*α*: Dirichlet before document-topic distribution, it affects the sparsity
    of topics within documents'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α*: 文档-主题分布的 Dirichlet 先验，它影响文档内主题的稀疏性'
- en: '*β*: Dirichlet before topic-word distribution, it affects the sparsity of words
    within topics'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β*: 主题-单词分布的 Dirichlet 先验，它影响主题内单词的稀疏性'
- en: '*θ*: Document-topic distributions (M × K matrix)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*θ*: 文档-主题分布（M × K 矩阵）'
- en: '*φ*: Topic-word distributions (K × V matrix, where V is the vocabulary size)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*φ*: 主题-单词分布（K × V 矩阵，其中 V 是词汇量）'
- en: '*z*: Topic assignments for each word in each document (M × N matrix)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z*: 每个文档中每个单词的主题分配（M × N 矩阵）'
- en: '*w*: Observed words in the documents (M × N matrix)'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*: 文档中观察到的单词（M × N 矩阵）'
- en: 'The joint probability of the topic assignments (*z*) and words (*w*) in the
    documents, given the topic-word distributions (*φ*) and document-topic distributions
    (*θ*), can be written as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定主题-单词分布 (*φ*) 和文档-主题分布 (*θ*) 的情况下，文档中主题分配 (*z*) 和单词 (*w*) 的联合概率可以表示如下：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>z</mi><mo>,</mo><mi>w</mi><mo>|</mo><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><mi>φ</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></mrow></mrow></mrow></math>](img/286.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><mi>z</mi><mo>,</mo><mi>w</mi><mo>|</mo><mi>θ</mi><mo>,</mo><mi>φ</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><munderover><mo>∏</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><mrow><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><mi>φ</mi><mo>,</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow></mfenced><mi>P</mi><mfenced
    open="(" close=")"><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>|</mo><msub><mi>θ</mi><mi>i</mi></msub></mrow></mfenced></mrow></mrow></mrow></mrow></mrow></math>](img/286.png)'
- en: 'The objective of LDA is to maximize the likelihood of the observed words given
    the Dirichlet priors α and β:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 的目标是最大化在 Dirichlet 先验 α 和 β 下观察到的单词的概率：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mo
    stretchy="false">∫</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>φ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/287.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:mo
    stretchy="false">∫</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">∫</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>φ</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mfenced><mml:mi>P</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>φ</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfenced><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>d</mml:mi><mml:mi>φ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math>](img/287.png)'
- en: However, computing the likelihood directly is intractable due to the integration
    over the latent variables θ and φ. Therefore, LDA uses approximate inference algorithms,
    such as Gibbs sampling or variational inference, to estimate the posterior distributions
    *P*(θ | w, α, β) and *P*(φ | w, α, β).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于对潜在变量 θ 和 φ 的积分，直接计算似然是不切实际的。因此，LDA 使用近似推理算法，如吉布斯采样或变分推理，来估计后验分布 *P*(θ
    | w, α, β) 和 *P*(φ | w, α, β)。
- en: Once the posterior distributions have been estimated, we can obtain the document-topic
    distributions (θ) and topic-word distributions (φ), which can be used to analyze
    the discovered topics and their word distributions, as well as the topic proportions
    for each document.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦估计了后验分布，我们就可以获得文档-主题分布（θ）和主题-词分布（φ），这些可以用来分析发现的主题及其词分布，以及每个文档的主题比例。
- en: Let’s consider a simple example of topic modeling.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的主题建模示例。
- en: 'Suppose we have a collection of three documents:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组三个文档：
- en: '**Document 1**: “I love playing football with my friends.”'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 1**：“我喜欢和朋友们踢足球。”'
- en: '**Document 2**: “The football match was intense and exciting.”'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 2**：“足球比赛激烈而兴奋。”'
- en: '**Document 3**: “My new laptop has an amazing battery life and performance.”'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档 3**：“我的新笔记本电脑电池寿命和性能惊人。”'
- en: 'We want to discover two topics (K = 2) in this document collection. Here are
    the steps that we need to perform:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在这个文档集中发现两个主题（K = 2）。以下是我们需要执行的步骤：
- en: '**Preprocessing**: First, we need to preprocess the text data, which typically
    involves tokenization, stopword removal, and stemming/lemmatization (which was
    explained previously in this chapter). In this example, we will skip these steps
    for simplicity and assume our documents are already preprocessed.'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：首先，我们需要预处理文本数据，这通常涉及分词、停用词去除和词干提取/词形还原（这在本章前面已经解释过）。在这个例子中，为了简单起见，我们将跳过这些步骤，并假设我们的文档已经预处理过。'
- en: '**Initialization**: Choose the initial values for the Dirichlet priors, α and
    β. For example, we can set α = [1, 1] and β = [0.1, 0.1, ..., 0.1] (assuming a
    V-dimensional vector with 0.1 for each word in the vocabulary).'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化**：选择狄利克雷先验的初始值 α 和 β。例如，我们可以设置 α = [1, 1] 和 β = [0.1, 0.1, ..., 0.1]（假设一个
    V 维向量，每个词汇的值为 0.1）。'
- en: '**Random topic assignments**: Randomly assign a topic (1 or 2) to each word
    in each document.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机主题分配**：随机为每个文档中的每个词分配一个主题（1 或 2）。'
- en: '**Iterative inference (for example, Gibbs sampling or variational inference)**:
    Iteratively update the topic assignments and the topic-word and document-topic
    distributions (φ and θ) until convergence or a fixed number of iterations. This
    process refines the assignments and distributions, ultimately revealing the underlying
    topic structure.'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代推理（例如，吉布斯采样或变分推理）**：迭代更新主题分配和主题-词分布以及文档-主题分布（φ 和 θ），直到收敛或达到固定的迭代次数。这个过程细化了分配和分布，最终揭示了潜在的主题结构。'
- en: '**Interpretation**: After the algorithm converges or reaches the maximum number
    of iterations, we can interpret the discovered topics by looking at the most probable
    words for each topic and the most probable topics for each document.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解释**：在算法收敛或达到最大迭代次数后，我们可以通过查看每个主题的最可能词和每个文档的最可能主题来解释发现的主题。'
- en: 'For our example, LDA might discover the following topics:'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们的例子，LDA 可能会发现以下主题：
- en: '**Topic 1**: {“football”, “playing”, “friends”, “match”, “intense”, “exciting”}'
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题 1**：{"足球", “踢球”, “朋友们”, “比赛”, “激烈”, “兴奋”}'
- en: '**Topic 2**: {“laptop”, “battery”, “life”, “performance”}'
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题 2**：{"笔记本电脑", “电池”, “寿命”, “性能”}'
- en: 'With these topics, the document-topic distribution (θ) might look like this:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用这些主题，文档-主题分布（θ）可能看起来像这样：
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/288.png)
    = [0.9, 0.1] (Document 1 is 90% about Topic 1 and 10% about Topic 2)'
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/288.png)
    = [0.9, 0.1]（文档 1 有 90% 关于主题 1，10% 关于主题 2）'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/289.png)
    = [0.8, 0.2] (Document 2 is 80% about Topic 1 and 20% about Topic 2)'
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/289.png)
    = [0.8, 0.2]（文档2有80%关于主题1，20%关于主题2）'
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/290.png)
    = [0.1, 0.9] (Document 3 is 10% about Topic 1 and 90% about Topic 2)'
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/290.png)
    = [0.1, 0.9]（文档3有10%关于主题1，90%关于主题2）'
- en: In this example, topic 1 seems to be related to football and sports, while topic
    2 seems to be related to technology and gadgets. The topic distributions for each
    document show that documents 1 and 2 are mostly about football, while document
    3 is about technology.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，主题1似乎与足球和体育相关，而主题2似乎与技术和小工具相关。每个文档的主题分布显示，文档1和2主要关于足球，而文档3是关于技术的。
- en: Please note that this is a simplified example, and real-world data would require
    more sophisticated preprocessing and a larger number of iterations for convergence.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个简化的例子，现实世界的数据需要更复杂的预处理和更多的迭代次数以收敛。
- en: We are now ready to discuss the paradigm for putting together a complete project
    in a work or research setting.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以讨论在工作和研究环境中构建完整项目的范式。
- en: Real-world ML system design for NLP text classification
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际世界中的NLP文本分类机器学习系统设计
- en: This section is dedicated to the practical implementation of the various methods
    we discussed. It will revolve around Python code, which serves as a complete pipeline.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专注于我们讨论的各种方法的实际实施。它将围绕Python代码展开，作为完整的管道。
- en: 'To provide a comprehensive learning experience, we will discuss the entire
    journey of a typical ML project. *Figure 5**.1* depicts the different phases of
    the ML project:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个全面的学习体验，我们将讨论典型机器学习项目的整个旅程。*图5.1*展示了机器学习项目的不同阶段：
- en: '![Figure 5.1 – The paradigm of a typical ML project](img/B18949_05_1.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 典型机器学习项目的范式](img/B18949_05_1.jpg)'
- en: Figure 5.1 – The paradigm of a typical ML project
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 典型机器学习项目的范式
- en: Let’s break the problem down in a similar fashion to a typical project in the
    industry.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以类似行业典型项目的方式分解这个问题。
- en: The business objective
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业目标
- en: An ML project, whether in a business or research setting, stems from an original
    objective, which is often qualitative rather than technical.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是商业还是研究环境中的机器学习项目，都源于一个原始目标，这个目标通常是定性的而不是技术的。
- en: 'Here’s an example:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: “We need to know which of our patients is at a higher risk.”
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们需要知道哪些患者处于更高的风险。”
- en: “We would like to maximize the engagement of our ad.”
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们希望最大化我们广告的参与度。”
- en: “We need the autonomous car to be alerted when a person is stepping in front
    of it.”
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们需要自动驾驶汽车在有人走到它前面时发出警报。”
- en: Next comes the technical objective.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是技术目标。
- en: The technical objective
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术目标
- en: 'The original objective needs to be translated into a technical objective, like
    so:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 原始目标需要被翻译成技术目标，如下所示：
- en: “We will process every patient’s medical record and build a risk estimator based
    on the history of realized risk.”
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们将处理每位患者的医疗记录，并基于实现风险的历史构建一个风险估计器。”
- en: “We will collect data about all the ads from the last year and will build a
    regressor to estimate the level of engagement based on the ad’s features.”
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们将收集去年所有广告的数据，并构建一个回归器来根据广告的特征估计参与度水平。”
- en: “We will collect a set of images taken by the car’s front camera and present
    those to our online users who are visiting our site, telling them it’s for security
    reasons and that they need to click on the parts that show a human to prove they
    are not robots. However, in practice, we’ll collect their free labels for training,
    develop a computer vision classifier for humans, and won’t give them any credit.”
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “我们将收集由汽车前摄像头拍摄的一组图像，并将这些图像展示给访问我们网站的在线用户，告诉他们这是出于安全原因，并且他们需要点击显示人类的区域以证明他们不是机器人。然而，在实践中，我们将收集他们的自由标签用于训练，开发一个用于人类的计算机视觉分类器，而不会给他们任何信用。”
- en: While the original business or research objective is somewhat of an open-ended
    question, the technical objective reflects an actionable plan. Note, however,
    that any given technical objective represents just one among several potential
    solutions aligned with the original business or research aim. It is the responsibility
    of the technical authority, such as the CTO, ML manager, or senior developer,
    to understand the original objective and translate it into a technical objective.
    Moreover, it may be that the technical objective would be refined or even replaced
    down the line. The next step after forming a technical objective is to form a
    plan for it.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原始的商业或研究目标是一个开放性问题，但技术目标反映了一个可执行的计划。然而，请注意，任何给定的技术目标只是与原始商业或研究目标一致的多项潜在解决方案之一。理解原始目标并将其转化为技术目标是技术权威（如CTO、ML经理或高级开发者）的责任。此外，技术目标可能会在后续阶段进行细化甚至替换。形成技术目标之后的下一步是为其制定计划。
- en: Tentative high-level system design
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初步的高级系统设计
- en: To realize the technical objective, we need to derive a plan to decide which
    data would be used to feed into the ML system, and what the expected output of
    the ML system is. In the first steps of a project, there may be several candidate
    sources of potential data that are believed to be indicative of the desired output.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现技术目标，我们需要制定一个计划来决定哪些数据将被用于输入到ML系统中，以及ML系统的预期输出是什么。在项目的早期阶段，可能会有几个被认为是能够指示所需输出的潜在数据来源。
- en: 'Following the set of three examples mentioned previously, here are some examples
    of data descriptions:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面提到的三个示例之后，以下是一些数据描述的示例：
- en: The input data would be columns A, B, and C of the **patient_records** SQL table
    and the risk would be assessed as *1/N*, where *N* is the number of days that
    passed from a given moment until the patient showed up in the emergency room.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据将是**patient_records** SQL表的A、B和C列，风险将评估为*1/N*，其中*N*是从给定时刻到患者在急诊室出现所经过的天数。
- en: The input data would be the geometric and color descriptions of the ads, and
    the level of engagement would be the number of clicks per day that the ad received.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据将是广告的几何和颜色描述，参与度将是广告每天收到的点击次数。
- en: The input data is the images of the car’s front camera to be fed to a computer
    vision neural network classifier, and the output data would be whether the image
    captures a person or not.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据是用于输入到计算机视觉神经网络分类器的汽车前摄像头图像，输出数据将是图像是否捕捉到人。
- en: Choosing a metric
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择度量标准
- en: When defining a potential solution approach, extra attention should be dedicated
    to identifying the best metric to focus on, also known as the objective function
    or error function. This is the metric by which the success of the solution will
    be evaluated. It is important to relate the metric to the original business or
    research objective.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义一个潜在的解决方案方法时，应额外注意确定最佳的度量标准来关注，也称为目标函数或误差函数。这是评估解决方案成功与否的度量标准。将此度量标准与原始的商业或研究目标联系起来是很重要的。
- en: 'As per the previous examples, we could have the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的示例，我们可能有以下内容：
- en: Minimize the 70th percentile confidence interval.
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化70百分位置信区间。
- en: Minimize the mean absolute error.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最小化平均绝对误差。
- en: Maximize precision while constraining on a fixed recall. This fixed recall will
    ideally be dictated by business leaders or the legal team, in the form of “the
    system must capture at least 99.9% of the cases where a person steps in front
    of a car.”
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在固定召回率的情况下最大化精度。这个固定的召回率理想情况下将由业务领导或法律团队决定，形式为“系统必须至少捕获99.9%的人站在车前的案例。”
- en: Now that we have a tentative plan, we can explore the data and evaluate the
    feasibility of the design.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个初步的计划，我们可以探索数据并评估设计的可行性。
- en: Exploration
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索
- en: Exploration is divided into two parts – exploring the data and exploring the
    feasibility of the design. Let’s take a closer look.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 探索分为两个部分——探索数据和探索设计的可行性。让我们更深入地了解一下。
- en: Data exploration
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据探索
- en: Data is not always perfect for our objective. We discussed some of the data
    shortcomings in previous chapters. In particular, free text is often notorious
    for having many abnormal phenomena, such as encodings, special characters, typos,
    and so on. When exploring our data, we want to uncover all these phenomena and
    make sure that the data can be brought to a form that serves the objective.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并不总是完美适合我们的目标。我们在前面的章节中讨论了一些数据不足之处。特别是，自由文本通常因存在许多异常现象而臭名昭著，如编码、特殊字符、拼写错误等。在探索我们的数据时，我们希望揭露所有这些现象，并确保数据可以被转换成服务于目标的形式。
- en: Feasibility study
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可行性研究
- en: Here, we want to prospectively identify proxies for whether the planned design
    is expected to succeed. While with some problems there are known proxies for expected
    success, in most problems in the business and especially research setting, it
    takes much experience and ingenuity to suggest preliminary proxies for success.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们希望前瞻性地识别计划设计预期成功的代理。虽然对于某些问题有已知的预期成功的代理，但在商业和特别是研究环境中，大多数问题需要大量的经验和独创性来提出初步的成功代理。
- en: An example of a very simple case is a simple regression problem with a single
    input variable and a single output variable. Let’s say the independent variable
    is the number of active viewers that your streaming service currently has, and
    the dependent variable is the risk that the company’s servers have for maxing
    out their capacity. The tentative design plan would be to build a regressor that
    estimates the risk at any given moment. A strong proxy for the feasibility of
    developing a successful regressor could be calculating the linear correlation
    between the historical data points. Calculating linear correlation based on sample
    data is easy and quick and if its result is close to 1 (or -1 in cases different
    than our business problem), then it means that a linear regressor is guaranteed
    to succeed, thus, making it a great proxy. However, note that if the linear correlation
    is close to 0, it doesn’t necessarily mean that a regressor would fail, only that
    a linear regression would fail. In such a case, a different proxy should be deferred
    to.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的案例是一个只有一个输入变量和一个输出变量的简单回归问题。比如说，自变量是你流媒体服务目前拥有的活跃观众数量，因变量是公司服务器达到最大容量的风险。初步的设计计划将是构建一个估算任何给定时刻风险的回归器。一个成功的回归器可行性的强大代理可能是计算历史数据点的线性相关性。基于样本数据计算线性相关性既简单又快捷，如果其结果接近1（或在我们的业务问题不同的情况下为-1），那么这意味着线性回归器肯定能成功，因此，它是一个很好的代理。然而，请注意，如果线性相关性接近0，这并不一定意味着回归器会失败，只是线性回归会失败。在这种情况下，应该推迟到不同的代理。
- en: In the *Reviewing our use case – ML system design for NLP classification in
    a Jupyter Notebook* section, we’ll review our code solution. We’ll also present
    a method to assess the feasibility of a text classifier. The method aims to mimic
    a relationship between the input text to the output class. But since we want to
    have that method suit a variable that is text and not numeric, we’ll go back to
    the origin and calculate a measure for the statistical dependency between the
    input text and the output class. Statistical dependency is the most basic measure
    for a relationship between variables and thus doesn’t require either of them to
    be numeric.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在*回顾我们的用例 - 在Jupyter Notebook中进行NLP分类的机器学习系统设计*部分，我们将回顾我们的代码解决方案。我们还将提出一种评估文本分类器可行性的方法。该方法旨在模拟输入文本与输出类别之间的关系。但由于我们希望该方法适用于文本变量而不是数值变量，我们将回到原点，计算输入文本与输出类别之间的统计依赖性度量。统计依赖性是变量之间关系的最基本度量，因此不需要它们中的任何一个必须是数值的。
- en: Assuming the **feasibility study** is successful, we can move on to implementing
    the ML solution.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 假设**可行性研究**成功，我们可以继续实施机器学习解决方案。
- en: Implementing an ML solution
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施机器学习解决方案
- en: This part is where the expertise of the ML developer comes into play. There
    are different steps for it and the developer chooses which ones are relevant based
    on the problem – whether it’s data cleaning, text segmentation, feature design,
    model comparison, or metric choice.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分是机器学习开发者的专业知识发挥作用的环节。对于这一过程有不同的步骤，开发者会根据问题选择哪些步骤是相关的——无论是数据清洗、文本分割、特征设计、模型比较还是指标选择。
- en: We will elaborate on this as we review the specific use case we’ve solved.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在回顾我们已解决的特定用例时详细阐述这一点。
- en: Evaluating the results
  id: totrans-362
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估结果
- en: We evaluate the solution given the metric that was chosen. This part requires
    some experience as ML developers tend to get better at this over time. The main
    pitfall in this task is the ability to set up an objective assessment of the result.
    That objective assessment is done by applying the finished model to data it had
    never “seen” before. But often folks who are only starting to apply ML find themselves
    improving their design after seeing what the results of that held-out set are.
    This leads to a feedback loop where the design is practically fitted to the no-longer-held-out
    set. While this may indeed improve the model and the design, it takes away from
    the ability to provide an objective forecast of how the model would perform when
    implemented in the real world. In the real world, it would see data that is truly
    held out and that it wasn’t fitted to.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据选择的指标来评估解决方案。这部分需要一些经验，因为机器学习开发者通常会随着时间的推移而变得更好。这项任务的主要陷阱是设置结果客观评估的能力。这种客观评估是通过将完成的模型应用于它之前从未“见过”的数据来完成的。但往往那些刚开始应用机器学习的人会发现，在看到保留集的结果后，他们的设计得到了改进。这导致了一个反馈循环，其中设计实际上被调整以适应不再保留的集。虽然这确实可以改进模型和设计，但它削弱了提供模型在现实世界中实施时表现客观预测的能力。在现实世界中，它将看到真正保留且未进行过拟合的数据。
- en: Done and delivered
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 已完成并交付
- en: Typically, when the design is done, the implementation is complete, and the
    results have been found satisfactory, the work is presented for business implementation,
    or in the research setting, for publication. In the business setting, implementation
    can take on different forms.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当设计完成、实施完成，并且结果令人满意时，工作就会呈现在商业实施中，或者在研究环境中，用于发表。在商业环境中，实施可以采取不同的形式。
- en: One of the simplest forms is where the output is used to provide business insights.
    Its purpose is to be presented. For instance, when looking to evaluate how much
    a marketing campaign was contributing to the growth in sales, the ML team may
    calculate an estimation for that measure of contribution and present it to leadership.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种最简单的形式是，输出被用来提供业务洞察。其目的是展示。例如，当评估营销活动对销售增长贡献了多少时，机器学习团队可能会计算该贡献度量的估计值，并将其呈现给管理层。
- en: Another form of implementation is within a dashboard in real time. For instance,
    the model calculates the predicted risk of patients coming to the emergency room,
    and it does so on a daily cadence. The results are aggregated and a graph is presented
    on the hospital dashboard to show the expected number of people who would come
    to the emergency room for every day of the next 30 days.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实施形式是在实时仪表板中。例如，模型计算患者进入急诊室的预测风险，并且它每天都会这样做。结果被汇总，并在医院仪表板上展示一个图表，以显示未来30天内每天预计进入急诊室的人数。
- en: A more advanced and common form is when the output of the data is directed so
    that it can be fed into downstream tasks. The model would then be implemented
    in production to become a microservice within a larger production pipeline. An
    example of that is when a classifier evaluates every post on your company’s Facebook
    page. When it identifies offensive language, it outputs a detection that then
    passes down the pipeline to another system that removes that post and perhaps
    blocks that user.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更高级且更常见的形式是，当数据输出被引导以便可以输入到下游任务中时。然后，模型将在生产中实施，成为更大生产管道中的一个微服务。一个例子是，当分类器评估你公司Facebook页面上每篇帖子时。当它识别出冒犯性语言时，它会输出一个检测信号，然后将其传递到管道中的另一个系统，该系统会删除该帖子并可能阻止该用户。
- en: Code design
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码设计
- en: The code’s design should suit the purpose of the code once the work is complete.
    As per the different forms of implementation mentioned previously, some implementations
    dictate a specific code structure. For instance, when the completed code is handed
    off to production within a larger, already existing pipeline, it is the production
    engineer who would dictate the constraints to the ML team. These constraints may
    be around computation and timing resources, but they would also be around code
    design. Often, basic code files, such as `.py` files, are necessary.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的设计应在工作完成后符合代码的目的。根据之前提到的不同实施形式，某些实施要求特定的代码结构。例如，当完成的代码在更大的、已经存在的管道中交付给生产时，生产工程师将决定对机器学习团队的限制。这些限制可能涉及计算和时序资源，但也会涉及代码设计。通常，基本的代码文件，如`.py`文件，是必要的。
- en: As with cases where the code is used for presentations, such as in the example
    of presenting how contributive the marketing campaign was, Jupyter Notebooks may
    be the better choice.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 就像代码用于展示的情况一样，例如在展示营销活动贡献性的例子中，Jupyter Notebook 可能是更好的选择。
- en: Jupyter Notebooks can be very informative and instructional. For that reason,
    many ML developers start their projects with Jupyter Notebooks for the exploration
    phase.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 可以提供非常丰富和有教育意义的信息。因此，许多机器学习开发者在探索阶段开始他们的项目时，会选择使用 Jupyter Notebook。
- en: Next, we will review our design in a Jupyter Notebook. This will allow us to
    encapsulate the entire process in a single coherent file that is meant to be presented
    to the reader.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在一个 Jupyter Notebook 中回顾我们的设计。这将使我们能够将整个流程封装在一个单一的连贯文件中，该文件旨在向读者展示。
- en: Reviewing our use case – ML system design for NLP classification in a Jupyter
    Notebook
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中回顾我们的用例 – NLP 分类机器学习系统设计
- en: In this section, we will walk through a hands-on example. We will follow the
    steps we presented previously for articulating the problem, designing the solution,
    and evaluating the results. This section portrays the process that an ML developer
    goes through when working on a typical project in the industry. Refer to the notebook
    at [https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing](https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing)
    for more information.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过一个动手示例进行操作。我们将遵循之前提出的步骤来阐述问题、设计解决方案和评估结果。本节描绘了机器学习开发者在行业中的典型项目中所经历的过程。有关更多信息，请参阅笔记本
    [https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing](https://colab.research.google.com/drive/1ZG4xN665le7X_HPcs52XSFbcd1OVaI9R?usp=sharing)。
- en: The business objective
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业目标
- en: In this scenario, we are working for a financial news agency. Our objective
    is to publish news about companies and products in real time.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们为一家金融新闻机构工作。我们的目标是实时发布有关公司和产品的新闻。
- en: The technical objective
  id: totrans-378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 技术目标
- en: 'The CTO derives several technical objectives from the business objective. One
    objective is for the ML team: given a stream of financial tweets in real time,
    detect those tweets that discuss information about companies or products.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: CTO 从商业目标中推导出几个技术目标。其中一个目标是针对机器学习团队的：在实时金融推文流中，检测讨论公司或产品信息的推文。
- en: The pipeline
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流程
- en: 'Let’s review the different parts of the pipeline, as shown in *Figure 5**.2*:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾流程的不同部分，如图 *图 5**.2* 所示：
- en: '![Figure 5.2 – The structure of a typical ML pipeline](img/B18949_05_2.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 典型机器学习流程的结构](img/B18949_05_2.jpg)'
- en: Figure 5.2 – The structure of a typical ML pipeline
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 典型机器学习流程的结构
- en: Note
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The phases of the pipeline in *Figure 5**.2* are explored in the following subsections
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *图 5**.2* 中的流程阶段将在以下小节中探讨
- en: Code settings
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码设置
- en: In this part of the code, we set the key parameters. We choose to have them
    as a part of the code as this is instructional code made for presentation. In
    cases where the code is expected to go to production, it may be better to host
    the parameters in a separate `.yaml` file. That would also suit heavy iterations
    during the development phase as it will allow you to iterate over different code
    parameters without having to change the code, which is often desirable.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分代码中，我们设置了关键参数。我们选择将它们作为代码的一部分，因为这是一段用于展示的指导性代码。在代码预期用于生产的情况中，可能更好的做法是将参数托管在一个单独的
    `.yaml` 文件中。这也会适应开发阶段的重度迭代，因为它将允许你在不改变代码的情况下迭代不同的代码参数，这通常是期望的。
- en: As for the choice of these values, it should be stressed that some of these
    values should be optimized to suit the optimization of the solution. We have chosen
    fixed measures here to simplify the process. For instance, the number of features
    to be used for classification is a fixed quantity here, but it should also be
    optimized to fit the training set.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些值的选择，应该强调的是，其中一些值应该优化以适应解决方案的优化。我们在这里选择了固定度量值以简化过程。例如，用于分类的特征数量在这里是一个固定数量，但它也应该优化以适应训练集。
- en: Gathering the data
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: This part loads the dataset. In our case, the loading function is simple. In
    other business cases, this part could be quite large as it may include a collection
    of SQL queries that are called. In such a case, it may be ideal to write a dedicated
    function in a separate `.py` file and source it via the imports section.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分加载数据集。在我们的案例中，加载函数很简单。在其他商业案例中，这部分可能相当大，因为它可能包括一系列被调用的SQL查询。在这种情况下，可能最好在单独的`.py`文件中编写一个专用函数，并通过导入部分调用它。
- en: Processing the data
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据处理
- en: Here, we format the data in a way that suits our work. We also observe some
    of it for the first time. This allows us to get a feel of its nature and quality.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们以适合我们工作方式格式化数据。我们也首次观察了一些数据。这使我们能够了解其性质和质量。
- en: One key action we take here is to define the classes we care about.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里采取的一个关键行动是定义我们关心的类别。
- en: Preprocessing
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: As we discussed in [*Chapter 4*](B18949_04.xhtml#_idTextAnchor113), preprocessing
    is a key part of the pipeline. For instance, we notice that many of the tweets
    have a URL, which we choose to remove.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第4章*](B18949_04.xhtml#_idTextAnchor113)中讨论的那样，预处理是管道的关键部分。例如，我们注意到许多推文中都包含URL，我们选择将其删除。
- en: Preliminary data exploration
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初步数据探索
- en: At this point, we have observed the quality of the text and the distribution
    of the classes. This is where we explore any other characteristics of the data
    that may imply either its quality or its ability to indicate the desired class.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经观察了文本的质量和类别的分布。这就是我们探索数据可能的其他特征的地方，这些特征可能意味着其质量或其指示所需类别的能力。
- en: Feature engineering
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: Next, we start processing the text. We seek to represent the text of each observation
    as a set of numerical features. The main reason for this is that traditional ML
    models are designed to accept numbers as input, not text. For instance, a common
    linear regression or logistic regression model is applied to numbers, not words,
    categories, or image pixels. Thus, we need to suggest a numeric representation
    for the text. This design constraint is lifted when working with **language models**
    such as **BERT** and **GPT**. We will see this in the coming chapters.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始处理文本。我们试图将每个观察到的文本表示为一组数值特征。这样做的主要原因是因为传统的机器学习模型是设计用来接受数字作为输入，而不是文本。例如，一个常见的线性回归或逻辑回归模型是应用于数字，而不是单词、类别或图像像素。因此，我们需要为文本提供一个数值表示。当与**语言模型**如**BERT**和**GPT**一起工作时，这种设计限制被解除。我们将在接下来的章节中看到这一点。
- en: We partition the text into N-grams, where *N* is a parameter of the code. *N*
    is fixed in this code but should be optimized to best fit the training set.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将文本分割成N-gram，其中*N*是代码的一个参数。在这个代码中*N*是固定的，但应该优化以最好地适应训练集。
- en: Once the text has been partitioned into N-grams, they are modeled as numeric
    values. When a binary (that is, **one-hot encoding**) method is chosen, the numerical
    feature that represents some N-gram gets a “1” when the observed text includes
    that N-gram, and “0” otherwise. See *Figure 5**.3* for an example. If a BOW approach
    is chosen, then the value of the feature is the number of times the N-gram appears
    in the observed text. Another common feature engineering method that isn’t implemented
    here is **TF-IDF**.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文本被分割成N-gram，它们就被建模为数值。当选择二进制（即**独热编码**）方法时，代表某些N-gram的数值特征，如果观察到的文本包含该N-gram，则得到“1”，否则得到“0”。请参见*图5.3*以获取示例。如果选择BOW方法，则特征值是N-gram在观察到的文本中出现的次数。这里没有实现的一种常见特征工程方法是**TF-IDF**。
- en: 'Here’s what we get by using unigrams only:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用单语词我们可以得到以下结果：
- en: 'Input sentence: “filing submitted.”'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 输入句子：“filing submitted。”
- en: '| **N-gram** | **“****report”** | **“****filing”** | **“****submitted”** |
    **“****product”** | **“****quarterly”** | **The rest of** **the unigrams** |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **N-gram** | **“report”** | **“filing”** | **“submitted”** | **“product”**
    | **“quarterly”** | **其他单语词** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Feature value | 0 | 1 | 1 | 0 | 0 | (0’s) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 特征值 | 0 | 1 | 1 | 0 | 0 | （0的） |'
- en: Figure 5.3 – Transforming an input text sentence into a numerical representation
    by partitioning to unigrams via one-hot encoding
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 通过单语词编码将输入文本句子转换为数值表示
- en: 'The following figure shows what we get by using both unigrams and bigrams:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了使用单语词和双语词得到的结果：
- en: '| **N-gram** | **“****report”** | **“****filing”** | **“****filing submitted”**
    | **“****report news”** | **“****submitted”** | **The rest of** **the N-grams**
    |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| **N-gram** | **“report”** | **“filing”** | **“filing submitted”** | **“report
    news”** | **“submitted”** | **其他N-gram** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Feature value | 0 | 1 | 1 | 0 | 1 | (0’s) |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 特征值 | 0 | 1 | 1 | 0 | 1 | （0的） |'
- en: Figure 5.4 – Transforming an input text sentence into a numerical representation
    by partitioning to unigrams and bigrams via one-hot encoding
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 通过将输入文本句子划分为单词和双词并通过独热编码进行转换，将其转换为数值表示
- en: Note that at this point in the code, the dataset hasn’t been partitioned into
    train and test sets, and the held-out set has not been excluded yet. This is because
    the binary and BOW feature engineering methods don’t depend on data outside of
    the underlying observation. With TF-IDF, this is different. Every feature value
    is calculated using the entire dataset for the document frequency.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在代码的这个阶段，数据集还没有被划分为训练集和测试集，预留集（也称为测试集）尚未排除。这是因为二进制和BOW特征工程方法不依赖于底层观察之外的数据。对于TF-IDF来说，情况则不同。每个特征值都是使用整个数据集的文档频率来计算的。
- en: Exploring the new numerical features
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索新的数值特征
- en: Now that our text has been represented as a feature, we can explore it numerically.
    We can look at its frequencies and statistics and get a sense of how it’s distributed.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将文本表示为特征，我们可以对其进行数值探索。我们可以查看其频率和统计信息，并了解其分布情况。
- en: Splitting into train/test sets
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 划分训练/测试集
- en: This is the part where we must pause and carve out a held-out set, also known
    as a test set, and sometimes as the validation set. Since these terms are used
    differently in different sources, it is important to explain that what we refer
    to as a test set is a held-out set. A held-out set is a data subset that we dedicate
    to evaluating our solution’s performance. It is held out to simulate the results
    that we expect to get when the system is implemented in the real world and will
    encounter new data samples.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们必须暂停并划分预留集（也称为测试集，有时也称为验证集）的部分。由于这些术语在不同的来源中有不同的用法，重要的是要解释一下，我们所说的测试集实际上是一个预留集。预留集是我们专门用于评估解决方案性能的数据子集。它被预留出来以模拟当系统在实际世界中实施并遇到新数据样本时预期的结果。
- en: How do we know when to carve out the held-out set?
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道何时划分预留集？
- en: If we carve it out “too early,” such as right after loading the data, then we
    are guaranteed to keep it held out, but we may miss discrepancies in the data
    as it won’t take part in the preliminary exploration. If we carve it out “too
    late,” our design decisions might become biased because of it. For example, if
    we choose one ML model over another based on results that include the would-be
    held-out set, then our design becomes tailored to that set, preventing us from
    offering an objective evaluation of the model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们“过早”地划分它，例如在加载数据后立即进行，那么我们保证将其保留为预留集，但可能错过数据中的差异，因为它不会参与初步探索。如果我们“过晚”地划分它，我们的设计决策可能会因为它们而变得有偏见。例如，如果我们根据包括将要预留的集的结果选择一个机器学习模型而不是另一个，那么我们的设计就会针对那个集，从而阻止我们对该模型进行客观评估。
- en: Then, we need to carry out the test set right before the first action that will
    feed into design decisions. In the next section, we’ll perform statistical analysis,
    which we can then feed into feature selection. Since that selection should be
    agnostic to the held-out set, we’ll exclude that set from this part onwards.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要在第一个将输入到设计决策中的动作之前执行测试集。在下文中，我们将进行统计分析，然后我们可以将其输入到特征选择中。由于该选择应该与预留集无关，因此我们将从这部分开始排除该集。
- en: Preliminary statistical analysis and feasibility study
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初步统计分析与可行性研究
- en: This is the second part of the exploration phase we spoke about a few pages
    ago. The first part was data exploration, and we implemented that in the previous
    parts of the code. Now that we have the text represented as numerical features,
    we can perform the feasibility study.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在几页前讨论的探索阶段的第二部分。第一部分是数据探索，我们在代码的前几部分实现了它。现在我们已经将文本表示为数值特征，我们可以进行可行性研究。
- en: We seek to measure the statistical dependence between the text inputs and the
    class values. Again, the motivation is to mimic the proxy that linear correlation
    provides with a regression problem.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图衡量文本输入和类别值之间的统计依赖性。同样，动机是模仿线性相关在回归问题中提供的代理。
- en: 'We know that for two random variables, *X* and *Y*, if they are statistically
    independent, then we get the following:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，对于两个随机变量，*X* 和 *Y*，如果它们在统计上独立，那么我们得到以下结果：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>,</mo><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>y</mi><mi>x</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>y</mi></mrow></mrow></mrow></math>](img/291.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>=</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mo>,</mo><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>y</mi><mi>x</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>y</mi></mrow></mrow></mrow></math>](img/291.png)'
- en: 'Alternatively, we get the following:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们得到以下结果：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>,</mo></mrow></mrow></math>](img/292.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>,</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>,</mo></mrow></mrow></math>](img/292.png)'
- en: This happens for every *x, y* value that yields a non-zero probability.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在每个产生非零概率的 *x, y* 值上。
- en: 'Conversely, we could use Bayes’s rules:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以使用贝叶斯定理：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/293.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mi>P</mi><mo>(</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn></mrow></mrow></math>](img/293.png)'
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>.</mo></mrow></mrow></math>](img/294.png)'
  id: totrans-431
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>Y</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>=</mo><mn>1</mn><mo>.</mo></mrow></mrow></math>](img/294.png)'
- en: Now, let’s think about any two random variables that aren’t necessarily statistically
    independent. We would like to evaluate whether there is a statistical relationship
    between the two.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑任意两个不一定相互独立的随机变量。我们想要评估这两个变量之间是否存在统计关系。
- en: Let one random variable be any of our numerical features, and the other random
    variable be the output class taking on values 0 or 1\. Let’s assume the feature
    engineering method is binary, so the feature also takes on values of 0 or 1.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 让一个随机变量代表我们的任何数值特征，另一个随机变量代表输出类别，取值为0或1。假设特征工程方法是二元的，因此特征也取值为0或1。
- en: 'Looking at the last equation, the expression on the left-hand side presents
    a very powerful measure of the ability of the relationship between *X* and *Y*:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 观察最后一个等式，左侧的表达式展示了 *X* 和 *Y* 之间关系能力的非常强大的度量：
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>o</mi><mo>{</mo><mn>0,1</mn><mo>}</mo><mo>.</mo></mrow></mrow></mrow></math>](img/295.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mrow><mfrac><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>|</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo>=</mo><mi>y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mo>=</mo><mi>x</mi><mo>)</mo></mrow></mfrac><mo>,</mo><mi>x</mi><mo>,</mo><mi>y</mi><mi>b</mi><mi>e</mi><mi>l</mi><mi>o</mi><mi>n</mi><mi>g</mi><mi>t</mi><mi>o</mi><mo>{</mo><mn>0,1</mn><mo>}</mo><mo>.</mo></mrow></mrow></mrow></math>](img/295.png)'
- en: It is powerful because if the feature is completely nonindicative of the class
    value, then in statistical terms, we say the two are statistically independent,
    and thus this measure would be equal to 1.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 它之所以强大，是因为如果特征对类别值完全没有指示性，那么在统计上，我们说这两个是统计独立的，因此这个度量将等于 1。
- en: Conversely, the bigger the difference between this measure and 1, the stronger
    the relationship is between this feature and this class. When performing a **feasibility
    study** of our design, we want to see that there are features in the data that
    have a statistical relationship with the output class.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，这个度量与 1 之间的差异越大，这个特征与这个类别之间的关系就越强。当我们对我们设计的**可行性研究**进行评估时，我们希望看到数据中存在与输出类别有统计关系的特征。
- en: For that reason, we calculate the value of this expression for every pair of
    every feature and every class.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们计算每个特征和每个类别的每一对特征的表达式的值。
- en: We present the most indicative terms for class “0,” which is the class of tweets
    that don’t indicate a company or product information, and we also present the
    terms that are most indicative of class “1,” meaning, when a tweet is discussing
    information about a company or a product.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了类别“0”的最具指示性的术语，类别“0”是指不表明公司或产品信息的推文类别，我们还展示了类别“1”中最具指示性的术语，这意味着当一条推文讨论公司或产品信息时。
- en: This proves to us that there are indeed text terms that are indicative of the
    class value. This is a definite and clear success of the feasibility study. We
    are good to go and we are expecting productive outcomes when implementing a classification
    model.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明给我们看，确实存在一些文本术语可以指示类别值。这是可行性研究的明确和清晰的成功。我们可以继续前进，并期待在实现分类模型时取得有成效的结果。
- en: As a side note, keep in mind that as with most evaluations, what we’ve just
    mentioned is just one sufficient condition for the potential of the text to predict
    the class. If it had failed, it would not necessarily indicate that there is no
    feasibility. Just like when the linear correlation between *X* and *Y* is near
    0, this doesn’t mean that *X* can’t infer *Y*. It just means that *X* cannot infer
    *Y* via a linear model. The linearity is an assumption that’s made to make things
    simple if it prevails.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个附带说明，请记住，与大多数评估一样，我们刚才提到的只是文本预测类别的潜在性的一个充分条件。如果它失败了，这并不一定意味着没有可行性。就像当 *X*
    和 *Y* 之间的线性相关性接近 0 时，这并不意味着 *X* 不能推断 *Y*。这只意味着 *X* 不能通过线性模型推断 *Y*。线性是一个为了简化问题而做出的假设，如果它成立。
- en: In the method that we’ve suggested, we make two key assumptions. First, we assume
    a very particular manner for feature design, being a certain *N* for the N-gram
    partition, and a certain quantitative method for the value – binary. The second
    is that we perform the most simple evaluation of statistical dependency, a univariate
    statistical dependency. But it could be that only a higher order, such as univariate,
    would have statistical dependence on the outcome class.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的方法中，我们做出了两个关键假设。首先，我们假设特征设计非常特别的方式，对于 N-gram 分区是某个特定的 *N*，对于值的定量方法是二进制。第二个是，我们执行最简单的统计依赖性评估，即单变量统计依赖性。但可能只有更高阶的，比如单变量，才会对结果类别有统计依赖性。
- en: With a **feasibility study** of text classification, it’s ideal if the method
    is as simple as possible while covering as much of the “signal” it is hoping to
    uncover. The approach we designed in this example was derived after years of experience
    with different sets and various problem settings. We find that it hits the target
    very well.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行文本分类的可行性研究时，如果方法尽可能简单同时覆盖尽可能多的“信号”，那么这是理想的。我们在这个例子中设计的方法是在多年的不同集合和不同问题设置的经验基础上得出的。我们发现它非常准确地击中了目标。
- en: Feature selection
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征选择
- en: With the **feasibility study**, we often kill two birds with one stone. As a
    **feasibility study** is successful, it not only helps us by confirming our plan,
    but it often hints toward the next steps that we should take. As we saw, some
    features are indicative of the class, and we learned which are the most significant.
    This allows us to reduce the feature space that the classification model will
    need to partition. We do that by keeping the most indicative features for each
    of the two classes. The number of features that we choose to keep would ideally
    be derived by computation constraints (for example, too many features would take
    too long to compute a model around), model capabilities (for example, too many
    features can’t be handled well by the model due to co-linearity), and optimization
    of the train results. In our code, we fixed this number to make things quick and
    simple.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**可行性研究**，我们经常一石二鸟。因为一个成功的**可行性研究**不仅可以帮助我们确认计划，而且经常暗示我们应该采取的下一步。正如我们所看到的，一些特征是类别指示性的，我们学会了哪些是最重要的。这使我们能够减少分类模型需要划分的特征空间。我们通过保留每个类别中最具指示性的特征来实现这一点。我们选择保留的特征数量理想情况下应由计算约束（例如，过多的特征会导致模型计算时间过长）、模型能力（例如，由于共线性，过多的特征可能无法被模型很好地处理）以及训练结果的优化来决定。在我们的代码中，我们固定了这个数字，以使事情变得快速简单。
- en: It should be stressed that in many ML models, feature selection is an inherited
    part of the model design. For instance, with the **least absolute shrinkage and
    selection operator** (**LASSO**), the hyperparameter scaler of the ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/296.png)
    norm component has an impact on which features get a zero coefficient, and thus
    get “thrown out.” It is possible and sometimes recommended to skip this part of
    the feature selection process, leave all features in, and let the model perform
    feature selection. It is advised to do so when all the models that are being evaluated
    and compared possess that characteristic.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 应该强调的是，在许多机器学习模型中，特征选择是模型设计的一个固有部分。例如，对于**最小绝对收缩和选择算子**（**LASSO**），![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="script">l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/296.png)范数的超参数缩放器会影响哪些特征得到零系数，从而被“排除”。有时可能且建议跳过这一部分的特征选择过程，保留所有特征，并让模型自行进行特征选择。当所有被评估和比较的模型都具有这一特性时，建议这样做。
- en: Remember that at this point, we are only observing the train set. Now that we
    have decided which features to keep, we need to apply that selection to the test
    set as well.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在这个阶段，我们只是在观察训练集。既然我们已经决定保留哪些特征，我们就需要将这个选择应用到测试集上。
- en: With that, our data has been prepared for ML modeling.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们的数据已经为机器学习建模做好了准备。
- en: Iterating over ML models
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遍历机器学习模型
- en: To choose which model suits this problem best, we must train several models
    and see which one of them does best.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择最适合这个问题的模型，我们必须训练几个模型，并查看哪一个表现最好。
- en: We should stress that we could do many things to try and identify the best model
    choice for a given problem. In our case, we only chose to evaluate a handful of
    models. Moreover, to make things simple and quick, we chose to not optimize the
    hyperparameters of each model in a comprehensive cross-validation approach. We
    simply fit each model to the training set with the default settings that its function
    comes with. Once we’ve identified the model we’d like to use, we optimize its
    hyperparameters for the train set via cross-validation.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调，我们可以做很多事情来尝试确定给定问题的最佳模型选择。在我们的案例中，我们只选择评估少数几个模型。此外，为了使事情简单快捷，我们选择不使用综合交叉验证方法来优化每个模型的超参数。我们只是使用每个模型默认设置将其拟合到训练集。一旦我们确定了我们想使用的模型，我们就通过交叉验证优化其在训练集上的超参数。
- en: By doing this, we identify the best model for the problem.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，我们确定了最佳模型。
- en: Generating the chosen model
  id: totrans-453
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成所选模型
- en: Here, we optimize the hyperparameters of the chosen model and fit it to our
    train set.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们优化所选模型超参数，并将其拟合到我们的训练集。
- en: Generating the train results – design choices
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成训练结果 – 设计选择
- en: At this stage, we observe the results of the model for the first time. This
    result can be used to feed insight back into the design choice and the parameters
    chosen, such as the feature engineering method, the number of features left in
    the feature selection, and even the preprocessing scheme.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们首次观察模型的性能结果。这个结果可以用来将洞察力反馈到设计选择和选择的参数中，例如特征工程方法、特征选择中保留的特征数量，甚至预处理方案。
- en: Important note
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Note that when feeding back insights from the results of the train set to the
    design of the solution, you are risking overfitting the train set. You’ll know
    whether you are by the gap between the results on the train set and the results
    on the test set.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当将训练集结果中的洞察力反馈到解决方案的设计中时，你冒着过度拟合训练集的风险。你可以通过训练集和测试集结果之间的差距来判断你是否这样做。
- en: While a gap is expected between these results in favor of the train results,
    a large gap should be treated as an alarm that the design isn’t optimal. In such
    cases, the design should be redone with systematic code-based parameters to ensure
    fair choices are made. It is possible to even carve out another semi-held-out
    set from the train set, often referred to as the validation set.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然预期这些结果之间存在差距，有利于训练结果，但较大的差距应被视为一个警报，表明设计不是最优的。在这种情况下，应使用基于代码的系统参数重新设计该设计，以确保做出公平的选择。甚至可以从训练集中划分出另一个半保留集，通常称为验证集。
- en: Generating the test results – presenting performance
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成测试结果 – 展示性能
- en: That’s it!
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！
- en: Now that the design has been optimized and we are confident that it suits our
    objective, we can apply it to our held-out set and observe the test results. These
    results are our most objective forecast of how well the system would do in the
    real world.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 现在由于设计已经优化，并且我们对其满足我们的目标有信心，我们可以将其应用于我们的保留集并观察测试结果。这些结果是我们对系统在现实世界中表现如何的最客观预测。
- en: As mentioned previously, we should avoid letting these results impact our design
    choices.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们应该避免让这些结果影响我们的设计选择。
- en: Summary
  id: totrans-464
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we embarked on a comprehensive exploration of text classification,
    an indispensable aspect of NLP and ML. We delved into various types of text classification
    tasks, each presenting unique challenges and opportunities. This foundational
    understanding sets the stage for effectively tackling a broad range of applications,
    from sentiment analysis to spam detection.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始对文本分类进行了全面的探索，这是自然语言处理（NLP）和机器学习（ML）不可或缺的方面。我们深入研究了各种文本分类任务，每个任务都提出了独特的挑战和机遇。这种基础理解为有效地解决广泛的应用奠定了基础，从情感分析到垃圾邮件检测。
- en: We walked through the role of N-grams in capturing local context and word sequences
    within text, thereby enhancing the feature set used for classification tasks.
    We also illuminated the power of the TF-IDF method, the role of Word2Vec in text
    classification, and popular architectures such as CBOW and skip-gram, giving you
    a deep understanding of their mechanics.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了N-gram在捕捉文本中的局部上下文和词序列中的作用，从而增强了用于分类任务的特性集。我们还阐明了TF-IDF方法的力量，Word2Vec在文本分类中的作用，以及CBOW和skip-gram等流行架构，为您提供了对这些机制的深入理解。
- en: Then, we introduced topic modeling and examined how popular algorithms such
    as LDA can be applied to text classification.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们介绍了主题建模，并探讨了如何将LDA等流行算法应用于文本分类。
- en: Lastly, we introduced a professional paradigm for leading an NLP-ML project
    in a business or research setting. We discussed the objectives and the project
    design aspect, and then dove into the system design. We implemented a real-world
    example in code and experimented with this.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了一种在商业或研究环境中领导NLP-ML项目的专业范式。我们讨论了项目目标和设计方面，然后深入到系统设计。我们通过代码实现了一个真实世界的示例，并对其进行了实验。
- en: In essence, this chapter has aimed to equip you with a holistic understanding
    of text classification and topic modeling by touching on the key concepts, methodologies,
    and techniques in the field. The knowledge and skills imparted will enable you
    to effectively approach and solve real-world text classification problems.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，本章旨在通过涉及该领域的核心概念、方法和技术，使你对文本分类和主题建模有一个全面的理解。所传授的知识和技能将使你能够有效地处理和解决现实世界的文本分类问题。
- en: In the next chapter, we will introduce advanced methods for text classification.
    We will review deep learning methods such as language models, discuss their theory
    and design, and present a hands-on system design in code.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍文本分类的高级方法。我们将回顾深度学习方法，如语言模型，讨论其理论和设计，并展示代码中的实际系统设计。
