- en: Validating Model Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证模型性能
- en: When you've built a deep learning model using neural networks, you are left
    with the question of how well it can predict when presented with new data. Are
    the predictions made by the model accurate enough to be usable in a real-world
    scenario? In this chapter, we will look at how to measure the performance of your
    deep learning models. We'll also dive into tooling to monitor and debug your models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建了一个深度学习模型并使用神经网络时，你面临的问题是，模型在面对新数据时的预测表现如何。模型做出的预测是否足够准确，以便在实际场景中使用？在本章中，我们将讨论如何衡量深度学习模型的性能。我们还将深入研究工具，帮助你监控和调试你的模型。
- en: By the end of this chapter, you'll have a solid understanding of different validation
    techniques you can use to measure the performance of your model. You'll also know
    how to use a tool such as TensorBoard to get into the details of your neural network.
    Finally, you will know how to apply different visualizations to debug your neural
    network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将对不同的验证技术有一个扎实的理解，这些技术可以用来衡量你的模型性能。你还将了解如何使用诸如TensorBoard之类的工具，深入探讨你的神经网络的细节。最后，你将知道如何应用不同的可视化方法来调试你的神经网络。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Choosing a good strategy to validate model performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个好的策略来验证模型性能
- en: Validating the performance of a classification model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证分类模型的性能
- en: Validating the performance of a regression model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证回归模型的性能
- en: Measuring performance of a for out-of-memory datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量大数据集的性能
- en: Monitoring your model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控你的模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We assume you have a recent version of Anaconda installed on your computer and
    have followed the steps in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, to install CNTK on your computer. The sample code
    for this chapter can be found in our GitHub repository at [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch4](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch4).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已经在电脑上安装了最新版本的Anaconda，并且已经按照[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)的步骤，*开始使用CNTK*，在电脑上安装了CNTK。本章的示例代码可以在我们的GitHub仓库中找到，地址是[https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch4](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch4)。
- en: 'In this chapter, we''ll work on a few examples stored in Jupyter Notebooks.
    To access the sample code, run the following commands inside an Anaconda prompt
    in the directory where you''ve downloaded the code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在Jupyter Notebooks中进行一些示例操作。要访问示例代码，请在你下载代码的目录中，在Anaconda提示符下运行以下命令：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We'll mention relevant notebooks in each of the sections so you can follow along
    and try out different techniques yourself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每个部分提到相关的notebook，以便你可以跟着做并亲自尝试不同的技术。
- en: 'Check out the following video to see the code in action:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，看看代码的实际运行：
- en: '[http://bit.ly/2TVuoR3](http://bit.ly/2TVuoR3)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2TVuoR3](http://bit.ly/2TVuoR3)'
- en: Choosing a good strategy to validate model performance
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个好的策略来验证模型性能
- en: Before we dive into different validation techniques for various kinds of models,
    let's talk a little bit about validating deep learning models in general.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论不同模型的验证技术之前，先简单谈一下深度学习模型的验证方法。
- en: When you build a machine learning model, you're training it with a set of data
    samples. The machine learning model learns these samples and derives general rules
    from them. When you feed the same samples to the model, it will perform pretty
    well on those samples. However, when you feed new samples to the model that you
    haven't used in training, the model will behave differently. It will most likely
    be worse at making a good prediction on those samples. This happens because your
    model will always tend to lean toward data it has seen before.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建一个机器学习模型时，你是用一组数据样本来训练它。机器学习模型从这些样本中学习并推导出一般规则。当你将相同的样本输入模型时，它将在这些样本上表现得很好。然而，当你输入一些新的、在训练时没有使用过的样本时，模型的表现会有所不同。它可能在这些样本上做得更差。这是因为你的模型总是倾向于偏向它之前见过的数据。
- en: But we don't want our model to be good at predicting the outcome for samples
    it has seen before. It needs to work well for samples that are new to the model,
    because in a production environment you will get different input that you need
    to predict an outcome for. To make sure that our model works well, we need to
    validate it using a set of samples that we didn't use for training.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不希望模型仅仅擅长预测它之前见过的样本的结果。它需要对模型之前没有见过的样本表现良好，因为在生产环境中，你将会得到不同的输入，需要预测结果。为了确保我们的模型表现良好，我们需要使用一组未用于训练的样本来验证它。
- en: Let's take a look at two different techniques for creating a dataset for validating
    a neural network. First, we'll explore how to use a hold-out dataset. After that
    we'll focus on a more complex method of creating a separate validation dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看两种不同的创建数据集用于验证神经网络的方法。首先，我们将探讨如何使用保留数据集。之后，我们将重点介绍创建单独验证数据集的更复杂方法。
- en: Using a hold-out dataset for validation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用保留数据集进行验证
- en: 'The first and easiest method to create a dataset to validate a neural network
    is to use a hold-out set. You''re holding back one set of samples from training
    and using those samples to measure the performance of your model after you''re
    done training the model:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个数据集来验证神经网络的第一个也是最简单的方法就是使用保留集。你将从训练中保留一组样本，并在训练完成后使用这些样本来衡量模型的性能：
- en: '![](img/710ddb8c-46f1-4a27-840c-ef361f390092.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/710ddb8c-46f1-4a27-840c-ef361f390092.png)'
- en: The ratio between training and validation samples is usually around 80% training
    samples versus 20% test samples. This ensures that you have enough data to train
    the model and a reasonable amount of samples to get a good measurement of the
    performance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练样本与验证样本之间的比例通常是80%的训练样本和20%的测试样本。这确保了你有足够的数据来训练模型，同时也有足够的样本来获得准确的性能度量。
- en: Usually, you choose random samples from the main dataset to include in the training
    and test set. This ensures that you get an even distribution between the sets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你会从主数据集中随机选择样本来包含在训练集和测试集中。这确保了在各个集合之间能够均匀分配样本。
- en: 'You can produce your own hold-out set using the `train_test_split` function
    from the `scikit-learn` library. It accepts any number of datasets and splits
    them into two segments based on either the `train_size` or the `test_size` keyword
    parameter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`train_test_split`函数来自`scikit-learn`库来生成你自己的保留集。该函数接受任意数量的数据集，并根据`train_size`或`test_size`关键词参数将其分成两个部分：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It is good practice to randomly split your dataset each time you run a training
    session. Deep learning algorithms, such as the ones used in CNTK, are highly influenced
    by random-number generators, and the order in which you provide samples to the
    neural network during training. So, to even out the effect of the sample order,
    you need to randomize the order of your dataset each time you train the model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每次运行训练会话时，随机划分数据集是一个好的做法。深度学习算法（例如在CNTK中使用的算法）高度依赖于随机数生成器，以及在训练过程中你为神经网络提供样本的顺序。因此，为了平衡样本顺序的影响，你需要每次训练模型时都对数据集的顺序进行随机化。
- en: Using a hold-out set works well when you want to quickly measure the performance
    of your model. It's also great when you have a large dataset or a model that takes
    a long time to train. But there are downsides to using the hold-out technique.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用保留集在你想快速衡量模型性能时非常有效。当你有一个大的数据集或训练时间较长的模型时，这种方法也非常有用。但使用保留法也有一些缺点。
- en: Your model is sensitive to the order in which samples were provided during training.
    Also, each time you start a new training session, the random-number generator
    in your computer will provide different values to initialize the parameters in
    your neural network. This can cause swings in performance metrics. Sometimes,
    you will get really good results, but sometimes you get really bad results. In
    the end, this is bad because it is unreliable.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型对训练过程中提供的样本顺序非常敏感。此外，每次你开始新的训练时，计算机中的随机数生成器会提供不同的值来初始化神经网络中的参数。这可能导致性能指标的波动。有时，你会得到非常好的结果，但有时也会得到很差的结果。最终，这种情况是不可取的，因为它不可靠。
- en: Be careful when randomizing datasets that contain sequences of samples that
    should be handled as a single input, such as when working with a time series dataset.
    Libraries such as `scikit-learn` don't handle this kind of dataset correctly and
    you may need to write your own randomization logic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在对包含应作为单个输入处理的样本序列的数据集进行随机化时要小心，例如在处理时间序列数据集时。像 `scikit-learn` 这样的库并不正确地处理这类数据集，你可能需要编写自己的随机化逻辑。
- en: Using k-fold cross-validation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k 折交叉验证
- en: 'You can increase the reliability of the performance metrics for your model
    by using a technique called k-fold cross-validation. Cross-validation performs
    the same technique as the hold-out set. But it does it a number of times—usually
    about 5 to 10 times:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用一种叫做 k 折交叉验证的技术来提高模型性能度量的可靠性。交叉验证执行的技术与留出集相同。但它执行多次—通常是 5 到 10 次：
- en: '![](img/d61537b3-a199-4972-8a7f-9378d9c2548b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d61537b3-a199-4972-8a7f-9378d9c2548b.png)'
- en: 'The process of k-fold cross-validation works like this: First, you split the
    dataset into a training and test set. You then train the model using the training
    set. Finally, you use the test set to calculate the performance metrics for your
    model. This process then gets repeated as many times as needed—usually 5 to 10
    times. At the end of the cross-validation process, the average is calculated over
    all the performance metrics, which gives you the final performance metrics. Most
    tools will also give you the individual values so you can see how much variation
    there is between different training runs.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: k 折交叉验证的过程如下：首先，将数据集划分为训练集和测试集。然后使用训练集训练模型。最后，使用测试集计算模型的性能度量。这个过程会重复进行所需的次数—通常是
    5 到 10 次。在交叉验证过程结束时，会计算所有性能度量的平均值，得到最终的性能度量。大多数工具还会给出各个值，以便你看到不同训练运行之间的差异。
- en: Cross-validation gives you a much more stable performance measurement, because
    you use a more realistic training and test scenario. The order of samples isn't
    defined in production, which is simulated by running the same training process
    a number of times. Also, we're using separate hold-out sets to simulate unseen
    data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证给你提供了更稳定的性能衡量，因为你使用了更现实的训练和测试场景。在生产中，样本的顺序是未定义的，这可以通过多次运行相同的训练过程来模拟。此外，我们使用了单独的留出集来模拟未见过的数据。
- en: Using k-fold cross-validation takes a lot of time when validating deep learning
    models, so use it wisely. If you're still experimenting with the setup of your
    model, you're better off using the basic hold-out technique. Later, when you're
    done experimenting, you can use k-fold cross-validation to make sure that the
    model performs well in a production environment.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 k 折交叉验证在验证深度学习模型时会消耗大量时间，所以要明智地使用它。如果你仍在进行模型设置的实验，最好使用基本的留出技术。稍后，当你完成实验后，可以使用
    k 折交叉验证来确保模型在生产环境中的表现良好。
- en: Note that CNTK doesn't include support for running k-fold cross-validation.
    You need to write your own scripts to do so.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，CNTK 不支持执行 k 折交叉验证。你需要编写自己的脚本来实现这一功能。
- en: What about underfitting and overfitting?
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，什么是欠拟合和过拟合呢？
- en: When you start to collect metrics for a neural network using either a hold-out
    dataset or by applying k-fold cross-validation you'll discover that the output
    for the metrics will be different for the training dataset and the validation
    dataset. In the this section, we'll take a look at how to use the information
    from the collected metrics to detect overfitting and underfitting problems for
    your model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始收集神经网络的度量时，无论是使用留出数据集还是应用 k 折交叉验证，你会发现训练数据集和验证数据集的输出度量是不同的。在本节中，我们将看看如何利用收集的度量信息来检测模型的过拟合和欠拟合问题。
- en: When a model is overfit, it performs really well on samples it has seen during
    training, but not on samples that are new. You can detect overfitting during validation
    by looking at the metrics. Your model is overfit when the metric on the test set
    is lower than the same metric on your training set.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型发生过拟合时，它在训练过程中见过的样本上表现得非常好，但在新样本上却表现不好。你可以通过查看度量来检测过拟合。当测试集上的度量低于训练集上的相同度量时，你的模型就发生了过拟合。
- en: A lot of overfitting is bad for business, since your model doesn't understand
    how to process new samples. But it is logical to have a little bit of overfitting
    in your model; this is expected, as you want to maximize the learning effort for
    your model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合对业务有很大负面影响，因为你的模型无法理解如何处理新的样本。但在模型中有一些过拟合是合乎逻辑的；这是预期的，因为你希望最大化模型的学习效果。
- en: The problem of overfitting becomes bigger when your model is trained on a dataset
    that doesn't represent the real-world environment it is used in. Then you end
    up with a model that is overfit toward the dataset. It will predict random output
    on new samples. Sadly, you can't detect this kind of overfitting. The only way
    to discover this problem is to use your model in production and use proper logging
    and user feedback to measure how well your model is doing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的问题在模型训练时所用的数据集无法代表其应用的真实世界环境时会变得更严重。这样，你最终会得到一个过度拟合数据集的模型。它会在新样本上产生随机输出。遗憾的是，你无法检测到这种类型的过拟合。发现这个问题的唯一方法是将模型投入生产，使用适当的日志记录和用户反馈来衡量模型的表现。
- en: Like overfitting, you can also have a model that is underfit. This means the
    model didn't learn enough from the training set and doesn't predict useful output.
    You can easily detect this with a performance metric. Usually, it will be lower
    than you anticipated. Actually, your model will be underfitting when you start
    training the first epoch and will become less underfit as training progresses.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与过拟合类似，你也可能遇到欠拟合的模型。这意味着模型没有从训练集学到足够的知识，无法预测有用的输出。你可以通过性能指标轻松检测到这一点。通常，性能指标会比你预期的要低。实际上，当你开始训练第一个周期时，模型会出现欠拟合，并随着训练的进行，欠拟合程度会逐渐减小。
- en: Once the model is trained, it can still be underfit. You can detect this by
    looking at the metrics for the training set and the test set. When the metric
    on the test set is higher than the metric on the training set, you have an underfit
    model. You can fix this by looking carefully at the settings of your model and
    changing them so it becomes better the next time you train the model. You can
    also try to train it for a little longer to see whether that helps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，它仍然可能会出现欠拟合。你可以通过查看训练集和测试集的指标来检测这一点。当测试集的指标高于训练集的指标时，你的模型就是欠拟合的。你可以通过仔细查看模型的设置并对其进行调整，以确保下次训练时模型表现更好。你也可以尝试训练更长时间，看看是否有帮助。
- en: Monitoring tools will help to detect underfitting and overfitting of your model.
    So, make sure you use them. We'll talk about how to use them with CNTK later in
    the section *Monitoring your model.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 监控工具有助于检测模型的欠拟合和过拟合。因此，确保你使用这些工具。我们将在后面的*监控你的模型*一节中讨论如何在CNTK中使用它们。
- en: Validating performance of a classification model
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证分类模型的性能
- en: In the previous section, *Choosing a good strategy to validate model performance*,
    we talked about choosing a good strategy for validating your neural network. In
    the following sections, we'll dive into choosing metrics for different kinds of
    models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，*选择合适的策略来验证模型性能*，我们讨论了为你的神经网络选择一个好的验证策略。在接下来的几节中，我们将深入探讨为不同类型的模型选择度量标准。
- en: When you're building a classification model, you're looking for metrics that
    express how many samples were correctly classified. You're probably also interested
    in measuring how many samples were incorrectly classified.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建分类模型时，你需要寻找能够表达正确分类样本数量的度量标准。你可能还会关心测量错误分类的样本数量。
- en: You can use a confusion matrix—a table with the predicted output versus the
    expected output—to find out a lot of detail about the performance of your model.
    This tends to get complicated, so we'll also look at a way to measure the performance
    of a model using the F-measure.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用混淆矩阵——一个将预测输出与期望输出进行对比的表格——来详细了解模型的性能。这可能变得有些复杂，所以我们也将探讨一种使用F-measure来衡量模型性能的方法。
- en: Using a confusion matrix to validate your classification model
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用混淆矩阵验证你的分类模型
- en: 'Let''s take a closer look at how you can measure the performance of a classification
    model using a confusion matrix. To understand how a confusion matrix works, let''s
    create a confusion matrix for a binary classification model that predicts whether
    a credit card transaction was normal or fraudulent:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看如何使用混淆矩阵来衡量分类模型的性能。为了理解混淆矩阵的工作原理，我们来为一个二分类模型创建一个混淆矩阵，该模型预测信用卡交易是正常的还是欺诈的：
- en: '|  | **Actual fraud** | **Actual normal** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际欺诈** | **实际正常** |'
- en: '| **Predicted fraud** | True positive | False positive |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **预测欺诈** | 真正例 | 假正例 |'
- en: '| **Predicted normal** | False negative | True negative |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **预测正常** | 假阴性 | 真阴性 |'
- en: The sample confusion matrix contains two columns and two rows. We have a column
    for the class fraud and a column for the class normal. We've added rows to the
    fraud and normal classes as well. The cells in the table will contain numbers
    that tell us how many samples were marked as true positive, true negative, false
    positive, and false negative.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 样本的混淆矩阵包含两列和两行。我们有一列表示欺诈类别，另一列表示正常类别。我们还为欺诈和正常类别添加了行。表格中的单元格将包含数字，告诉我们有多少样本被标记为真正例、真阴性、假正例和假阴性。
- en: When the model correctly predicts fraud for a transaction, we're dealing with
    a true positive. When we predict fraud but the transaction should not have been
    marked as fraud, we're dealing with a false positive.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型正确预测某笔交易为欺诈时，我们称之为真正例。当我们预测为欺诈，但该交易本应不被标记为欺诈时，我们称之为假正例。
- en: 'You can calculate a number of different things from the confusion matrix. First,
    you can calculate precision based on the values in the confusion matrix:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从混淆矩阵计算出许多不同的东西。首先，您可以根据混淆矩阵中的值计算精度：
- en: '![](img/b333c76a-10c0-432c-a7d4-e50050c28ae2.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b333c76a-10c0-432c-a7d4-e50050c28ae2.png)'
- en: Precision tells you how many samples were correctly predicted out of all the
    samples that we predicted. High precision means that your model suffers from very
    few false positives.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 精度告诉你我们预测的所有样本中有多少是正确预测的。高精度意味着你的模型很少出现假正例。
- en: 'The second metric that we can calculate based on the confusion matrix is the
    recall metric:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据混淆矩阵计算的第二个指标是召回率指标：
- en: '![](img/9816db92-39dd-4247-8093-fe2f621c0294.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9816db92-39dd-4247-8093-fe2f621c0294.png)'
- en: Recall tells you how many of the fraud cases in the dataset were actually detected
    by the model. Having a high recall means that your model is good at finding fraud
    cases in general.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率告诉你数据集中有多少欺诈案例实际上被模型检测到。高召回率意味着你的模型在发现欺诈案例方面表现良好。
- en: 'Finally, you can calculate the overall accuracy of the model:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以计算模型的整体准确度：
- en: '![](img/d8205762-7529-4899-be1e-7b0f53b535dc.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8205762-7529-4899-be1e-7b0f53b535dc.png)'
- en: 'The overall accuracy tells you how well the model does as a whole. But this
    is a dangerous metric to use when your dataset is unbalanced. For example: if
    you have 100 samples of which 5 are marked as fraud and 95 are marked as normal,
    predicting normal for all samples gives you an accuracy of *0.95*. This seems
    high, but we''re fooling ourselves.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 整体准确度告诉你模型作为整体的表现如何。但在数据集不平衡时使用这个指标是有风险的。例如：如果你有100个样本，其中5个标记为欺诈，95个标记为正常，那么如果对所有样本都预测为正常，得到的准确度为*0.95*。这看起来很高，但我们只是在自欺欺人。
- en: 'It''s much better to calculate a balanced accuracy. For this, we need to know
    the precision and specificity of the model. We already know how to calculate the
    precision of our model. We can calculate the specificity using the following formula:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平衡准确度要好得多。为此，我们需要知道模型的精度和特异性。我们已经知道如何计算模型的精度。我们可以使用以下公式计算特异性：
- en: '![](img/af157728-3148-4bf2-ba7d-024f614c0d41.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af157728-3148-4bf2-ba7d-024f614c0d41.png)'
- en: The specificity tells us how good our model is at detecting that a sample is
    normal instead of fraud. It is the perfect inverse of the precision, which tells
    us how good our model is at detecting fraud.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性告诉我们模型在检测样本是正常而非欺诈时的表现如何。它是精度的完全反向，精度告诉我们模型在检测欺诈时的表现如何。
- en: 'Once we have the specificity, we can combine it with the precision metric to
    calculate the balanced accuracy:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了特异性，我们可以将其与精度指标结合，计算平衡准确度：
- en: '![](img/af5bd5d4-7cca-43de-a5fa-e414775f78ed.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af5bd5d4-7cca-43de-a5fa-e414775f78ed.png)'
- en: 'The balanced accuracy tells us how good our model is at separating the dataset
    into fraud and normal cases, which is exactly what we want. Let''s go back to
    our previous accuracy measurement and retry it using the balanced version of the
    accuracy metric:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡准确度告诉我们模型在将数据集分为欺诈和正常案例时的表现如何，这正是我们所期望的。让我们回到之前的准确度度量，并使用平衡版本的准确度指标重新尝试：
- en: '![](img/d9a34af2-7f5e-4fc3-9e82-552413b68d97.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9a34af2-7f5e-4fc3-9e82-552413b68d97.png)'
- en: Remember, we had 100 samples, of which 5 should be marked as fraud. When we
    predict everything as normal, we end up with a precision of *0.0* because we didn't
    predict any fraud case correctly. The specificity is *0.95* because out of 100
    samples we predicted 5 incorrectly as normal. The end result is a balanced accuracy
    of *0.475*, which is not very high, for obvious reasons.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们有 100 个样本，其中 5 个应该标记为欺诈。当我们将所有样本都预测为正常时，我们的精确度为 *0.0*，因为我们没有正确预测任何欺诈案例。特异度为
    *0.95*，因为在 100 个样本中，我们错误地将 5 个预测为正常。最终结果是一个平衡准确度 *0.475*，显然这个值并不高。
- en: Now that you have a good feel for what a confusion matrix looks like and how
    it works, let's talk about the more complex cases. When you have a multi-class
    classification model with more than two classes, you will need to expand the matrix
    with more rows and columns.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对混淆矩阵的外观和工作原理有了基本了解，让我们谈谈更复杂的情况。当你有一个多类分类模型，且类别数超过两个时，你需要用更多的行和列来扩展矩阵。
- en: 'For example: when we create a confusion matrix for a model that predicts three
    possible classes, we could end up with the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：当我们为一个预测三种可能类别的模型创建混淆矩阵时，可能会得到以下结果：
- en: '|  | **Actual A** | **Actual B** | **Actual C** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | **实际 A** | **实际 B** | **实际 C** |'
- en: '| **Predicted A** | 91 | 75 | 60 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **预测为 A** | 91 | 75 | 60 |'
- en: '| **Predicted B** | 5 | 15 | 30 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **预测为 B** | 5 | 15 | 30 |'
- en: '| **Predicted C** | 4 | 10 | 10 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **预测为 C** | 4 | 10 | 10 |'
- en: 'We can still calculate the precision, recall, and specificity for this matrix.
    But it is more complex to do so, and we can only do it on a per-class basis. For
    example: when you want to calculate the precision for class A, you need to take
    the true positive rate of A, which is *91*, and divide it by the number of samples
    that were actually A but were predicted as B and C, which is *9* in total. This
    gives us the following calculation:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以计算该矩阵的精确度、召回率和特异度。但这样做更复杂，而且只能逐类进行。例如：当你想要计算 A 类的精确度时，你需要取 A 类的真正例率，*91*，然后将其除以实际为
    A 但被预测为 B 和 C 的样本数，总共有 *9* 个。这就得到了以下计算：
- en: '![](img/5749a98a-57f1-42cc-b661-c92fc95d10a2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5749a98a-57f1-42cc-b661-c92fc95d10a2.png)'
- en: The process is much the same for calculating recall, specificity, and accuracy.
    To get an overall figure for the metrics, you need to calculate the average over
    all classes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 计算召回率、特异度和准确率的过程也差不多。要获得指标的整体值，你需要对所有类进行平均计算。
- en: 'There are two strategies that you can follow to calculate the average metric,
    such as precision, recall, specificity, and accuracy. You can either choose to
    calculate the micro-average or the macro-average. Let''s first explore the macro-average
    using the precision metric:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种策略可以用来计算平均指标，比如精确度、召回率、特异度和准确率。你可以选择计算微平均或宏平均。我们首先来探讨使用精确度指标的宏平均：
- en: '![](img/f73d6ac4-bcf7-4df9-aa07-42c975b4e40a.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f73d6ac4-bcf7-4df9-aa07-42c975b4e40a.png)'
- en: 'To get the macro-average for the precision metric, we first add up the precision
    values for all classes and then divide them over the number of classes, *k*. The
    macro-average doesn''t take into account any class imbalances. For example: there
    could be 100 samples for class A while there are only 20 samples for class B.
    Calculating the macro-average gives you a skewed picture.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算精确度指标的宏平均，我们首先将所有类别的精确度值相加，然后除以类别的数量，*k*。宏平均不考虑类别的不平衡。例如：A 类可能有 100 个样本，而
    B 类只有 20 个样本。计算宏平均会导致结果失衡。
- en: 'When you work on multi-class classification models, it''s better to use the
    micro-average for the different metrics—precision, recall, specificity, and accuracy.
    Let''s take a look at how to calculate the micro-average precision:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理多类分类模型时，最好为不同的指标—精确度、召回率、特异度和准确率—使用微平均。我们来看看如何计算微平均精确度：
- en: '![](img/c26b804c-fcaf-4010-acdf-8418edf329fa.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c26b804c-fcaf-4010-acdf-8418edf329fa.png)'
- en: First, we'll add up all the true positives for every class. We then divide them
    by the sum of all true positives and false negatives for every class. This will
    give us a much more balanced view of the different metrics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将每个类别的所有真正例相加，然后将其除以每个类别的真正例和假阴性的总和。这样我们就能更平衡地看待不同的指标。
- en: Using the F-measure as an alternative to the confusion matrix
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 F-measure 作为混淆矩阵的替代
- en: 'While using precision and recall give you a good idea of how your model performs,
    they can''t be maximized at the same time. There''s a strong relationship between
    the two metrics:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用精确度和召回率可以帮助你了解模型的性能，但它们不能同时最大化。这两个度量之间有着密切的关系：
- en: '![](img/9f37bee7-0b13-42e5-bdf1-cf29e7afdcdb.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f37bee7-0b13-42e5-bdf1-cf29e7afdcdb.png)'
- en: Let's see how this relationship between precision and recall plays out. Let's
    say you want to use a deep learning model to classify cell samples as cancerous
    or normal. In theory, to reach maximum precision in your model, you need to reduce
    the number of predictions to 1\. This gives you the maximum chance to reach 100%
    precision, but recall becomes really low, as you're missing a lot of possible
    cases of cancer. When you want to reach maximum recall to detect as many cases
    of cancer as possible, you need to make as many predictions as possible. But this
    reduces precision, as you increase the chance that you get false positives.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看精确度和召回率之间的关系是如何体现的。假设你想使用深度学习模型将细胞样本分类为癌症或正常。理论上，要实现模型的最大精确度，你需要将预测数量减少到1。这给你提供了最大机会达到100%精确度，但召回率会变得非常低，因为你错过了很多可能的癌症病例。当你想要最大化召回率，尽可能多地检测到癌症病例时，你需要进行尽可能多的预测。但这会降低精确度，因为增加了出现假阳性的可能性。
- en: 'In practice, you will find yourself balancing between precision and recall.
    Whether you should go primarily for precision or recall is dependent on what you
    want your model to predict. Often, you will need to talk to the user of your model
    to determine what they find most important: a low number of false positives or
    a high chance of finding that one patient who has a deadly disease.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你会发现自己在精确度和召回率之间进行平衡。你应该主要追求精确度还是召回率，取决于你希望模型预测的内容。通常，你需要与模型的用户进行沟通，以确定他们认为最重要的是什么：低假阳性数量，还是高概率发现那一个患有致命疾病的病人。
- en: 'Once you have made a choice between precision and recall, you need a way to
    express this in a metric. The F-measure allows you to do this. The F-measure expresses
    a harmonic average between precision and recall:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在精确度和召回率之间做出选择，你需要找到一种方法来用度量表达这一选择。F-度量允许你做到这一点。F-度量表示精确度和召回率之间的调和平均值：
- en: '![](img/369bf086-6bc9-4050-b4fe-883f66ff9da0.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/369bf086-6bc9-4050-b4fe-883f66ff9da0.png)'
- en: The full formula for the F-measure includes an extra term, *B*, which is set
    to 1 to get an equal ratio of precision and recall. This is called the F1-measure
    and is the standard in almost all tools you will come across. It gives equal weight
    to recall and precision. When you want to emphasize recall, you can set the *B*
    factor to 2\. Alternatively, when you want to emphasize precision in your model,
    you can set the *B* factor to 0.5\.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: F-度量的完整公式包括一个额外的参数 *B*，它被设置为1，以使精确度和召回率的比例相等。这就是所谓的F1-度量，也是你几乎在所有工具中会遇到的标准。它给召回率和精确度赋予相等的权重。当你想强调召回率时，可以将
    *B* 值设置为2。或者，当你想强调精确度时，可以将 *B* 值设置为0.5。
- en: In the next section, we'll see how to use the confusion matrix and f-measure
    in CNTK to measure the performance of a classification model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用CNTK中的混淆矩阵和F-度量来衡量分类模型的性能。
- en: Measuring classification performance in CNTK
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CNTK中衡量分类性能
- en: Let's take a look at how you can use the CNTK metrics functions to create a
    confusion matrix for the flower classification model that we used in Chapter 2,
    *Building Neural Networks with CNTK*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用CNTK的度量函数为我们在第二章《使用CNTK构建神经网络》中使用的花卉分类模型创建一个混淆矩阵。
- en: You can follow along with the code in this section by opening the `Validating
    performance of classification models.ipynb` notebook file from the sample files
    for this chapter. We'll focus on the validation code in this section. The sample
    code contains more detail on how to preprocess the data for the model as well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过打开本章示例文件中的 `Validating performance of classification models.ipynb` 笔记本文件来跟随这一节的代码。我们将集中讨论这一节的验证代码。示例代码还包含了如何为模型预处理数据的更多细节。
- en: 'Before we can train and validate the model, we''ll need to prepare the dataset
    for training. We''ll split the dataset into separate training and test sets to
    ensure that we get a proper performance measurement for our model:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练和验证模型之前，我们需要准备好训练数据集。我们将把数据集拆分为单独的训练集和测试集，以确保能够正确评估模型的性能：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, we'll import the `train_test_split` function from the `sklearn.model_selection`
    package. We then take the features, `X`, and the labels, `y`, and run them through
    the function to split them. We'll use 20% of the samples for testing.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从`sklearn.model_selection`包中导入`train_test_split`函数。然后，我们将特征`X`和标签`y`传递给该函数进行拆分。我们将使用20%的样本用于测试。
- en: Note that we're using the `stratify` keyword parameter. Because we're validating
    a classification model, we want to have a good balance between classes in the
    test and training set. Each class should ideally be equally represented in both
    the test and training set. When you feed a list of classes or labels to the `stratify`
    keyword, `scikit-learn` will take them to evenly distribute the samples over the
    training and test set.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在使用`stratify`关键字参数。因为我们正在验证一个分类模型，所以我们希望测试集和训练集中的各类别之间能够保持良好的平衡。理想情况下，每个类别在测试集和训练集中的分布应当相同。当你将类别或标签列表传递给`stratify`关键字时，`scikit-learn`会基于这些标签均匀地分配样本到训练集和测试集中。
- en: 'Now that we have the training and test set, let''s train the model:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练集和测试集，让我们开始训练模型：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We'll run the whole dataset through the training function for a total of 15
    epochs of training. We've included a progress writer to visualize the training
    process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整个数据集通过训练函数运行，总共进行15个周期的训练。我们还包括了一个进度条，以便可视化训练过程。
- en: 'At this point, we don''t know what the performance is like. We know that the
    loss decreased nicely over 15 epochs of training. But the question is this, is
    it enough? Let''s find out by running the validation samples through the model
    and create a confusion matrix:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们还不知道模型的表现如何。我们知道在15个训练周期内，损失值有所下降。但问题是，这个下降程度足够吗？让我们通过将验证样本输入模型并创建一个混淆矩阵来找出答案：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We''re using the `confusion_matrix` function from `scikit-learn` to create
    the confusion matrix. This function needs the true labels and the predicted labels.
    Both need to be stored as a numpy array with numeric values representing the labels.
    We don''t have those numbers. We have a binary representation of the labels because
    that is what is required by the model. To fix this, we need to convert the binary
    representation of the labels into a numeric one. You can do this by invoking the
    `argmax` function from the `numpy` package. The output of the `confusion_matrix`
    function is a numpy array and looks like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`scikit-learn`中的`confusion_matrix`函数来创建混淆矩阵。这个函数需要真实标签和预测标签。这两个标签需要以numpy数组的形式存储，并且数组中的数值表示标签。我们没有这些数值，我们只有标签的二进制表示，因为模型要求的是这个格式。为了修正这个问题，我们需要将标签的二进制表示转换为数字表示。你可以通过调用`numpy`包中的`argmax`函数来实现。`confusion_matrix`函数的输出是一个numpy数组，类似于这样：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We get three rows and three columns in the confusion matrix because we have
    three possible classes that our model can predict. The output itself isn''t very
    pleasant to read. You can convert this table into a heat map using another package
    called `seaborn`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵有三行三列，因为我们模型可以预测三种可能的类别。输出结果本身不太容易阅读。你可以使用另一个叫做`seaborn`的包将这个表格转换为热力图：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First, we create a new heat map based on the confusion matrix. We pass in the
    classes of the `label_encoder` used while preprocessing the dataset for the row
    and column labels.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们基于混淆矩阵创建一个新的热力图。我们传入在数据集预处理时使用的`label_encoder`的类别，用于设置行和列的标签。
- en: 'The standard heat map needs some tweaks to be easily readable. We''re using
    a custom colormap for the heat map. We''re also using custom labels on the X and
    Y axes. Finally, we add a title and display the graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的热力图需要一些调整才能更易读。我们使用了一个自定义的热力图配色方案。我们还在X轴和Y轴上使用了自定义标签。最后，我们添加了标题并显示了图表：
- en: '![](img/de395e15-62fb-4392-ac54-be08052c5b8b.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de395e15-62fb-4392-ac54-be08052c5b8b.png)'
- en: Standard heat map
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 标准热力图
- en: Looking at the confusion matrix, you can quickly see how the model is doing.
    In this case, the model is missing quite a few cases for the Iris-versicolor class.
    Only 60% of the flowers of this species were correctly classified.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵来看，你可以迅速了解模型的表现。在这个例子中，模型漏掉了相当多的Iris-versicolor类别的样本。仅有60%的该物种的花朵被正确分类。
- en: While the confusion matrix gives a lot of detail about how the model performs
    on different classes, it may be useful to get a single performance figure for
    your model so you can easily compare different experiments.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然混淆矩阵提供了很多关于模型在不同类别上表现的详细信息，但有时获取一个单一的性能指标可能更有用，这样你就可以轻松地比较不同的实验。
- en: One way to get a single performance figure is to use the `classification_error`
    metric from CNTK. It calculates the fraction of samples that were misclassified.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 获取单一性能指标的一种方法是使用CNTK中的`classification_error`度量。它计算被误分类样本的比例。
- en: 'To use it, we need to modify the training code. Instead of just having a `loss`
    function to optimize the model, we''re going to include a metric as well. Previously
    we created just a `loss` function instance, this time we''re going to have to
    write a `criterion` function that produces a combined `loss` and `metric` function
    that we can use during training. The following code demonstrates how to do this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用它，我们需要修改训练代码。我们不再只是拥有一个`loss`函数来优化模型，而是还要包含一个指标。之前我们只创建了一个`loss`函数实例，这一次我们需要编写一个`criterion`函数，生成一个组合的`loss`和`metric`函数，以便在训练过程中使用。以下代码演示了如何操作：
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Follow the given steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, create a new Python function that takes our model as the `output` argument
    and the target that we want to optimize for as the `output` argument.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个新的Python函数，将我们的模型作为`output`参数，并将我们想要优化的目标作为`output`参数。
- en: Within the function, create a `loss` function and provide it the `output` and
    `target`.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内，创建一个`loss`函数，并为其提供`output`和`target`。
- en: Next, create a `metric` function and provide it the `output` and `target` as
    well.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个`metric`函数，并为其提供`output`和`target`。
- en: At the end of the function, return both as a tuple, where the first element
    is the `loss` function and the second element is the `metric` function.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数的最后，返回一个元组，元组的第一个元素是`loss`函数，第二个元素是`metric`函数。
- en: Mark the function with `@cntk.Function`. This will wrap the loss and metric
    so we can call the `train` method on it to train the model and call the `test`
    method to validate the model.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`@cntk.Function`标记该函数。这将包装`loss`和`metric`，以便我们可以在其上调用`train`方法训练模型，并调用`test`方法来验证模型。
- en: 'Once we have the combined `loss` and `metric` function factory, we can use
    it during training:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了组合的`loss`和`metric`函数工厂，就可以在训练过程中使用它：
- en: '[PRE8]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Follow the given steps:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the `cross_entropy_with_softmax` function from the `losses` module
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`losses`模块导入`cross_entropy_with_softmax`函数。
- en: Next, import the `sgd` learner from the `learners` module
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从`learners`模块导入`sgd`学习器。
- en: Also, import the `ProgressPrinter` from the `logging` module so you can log
    the output of the training process
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，从`logging`模块导入`ProgressPrinter`，以便记录训练过程的输出。
- en: Then, create a new instance `progress_writer` to log the output of the training
    process
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的实例`progress_writer`，用于记录训练过程的输出。
- en: Afer that, create a `loss` using the newly created `criterion_factory` function
    and feed it the model variable `z` and the `labels` variable
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，使用新创建的`criterion_factory`函数创建`loss`，并将模型变量`z`和`labels`变量作为输入。
- en: Next, create the `learner` instance using the `sgd` function and feed it the
    parameters and a learning rate of `0.1`
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`sgd`函数创建`learner`实例，并为其提供参数以及学习率`0.1`。
- en: Finally, call the `train` method with the training data, the `learner` and the
    `progress_writer`
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用训练数据、`learner`和`progress_writer`调用`train`方法。
- en: 'When we call train on the `loss` function, we get a slightly different output.
    Instead of just the loss, we also get to see the output of the `metric` function
    during training. In our case, the value of the metric should increase over time:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用`loss`函数时，我们会得到略微不同的输出。除了损失值，我们还可以看到训练过程中`metric`函数的输出。在我们的例子中，`metric`的值应该随着时间的推移而增加：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, when we''re done training, you can use the `test` method on the `loss`/`metric`
    combination function to calculate classification error using the test set we created
    earlier:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当我们完成训练时，可以使用`test`方法来计算分类错误，该方法基于我们之前创建的测试集，使用`loss`/`metric`组合函数：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When you execute the `test` method on the `loss` function with a dataset, CNTK
    will take the samples you provide as input for this function and make a prediction
    based on the input features, `X_test`. It then takes the predictions and the values
    stored in `y_test` and runs them through the `metric` function that we created
    in the `criterion_factory` function. This produces a single scalar value expressing
    the metric.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在`loss`函数上执行`test`方法，并使用数据集时，CNTK会将你提供的样本作为该函数的输入，并基于输入特征`X_test`进行预测。然后，它会将预测结果和存储在`y_test`中的值一起传入我们在`criterion_factory`函数中创建的`metric`函数。这会生成一个表示指标的单一标量值。
- en: The `classification_error` function that we used in this sample measures the
    difference between the real labels and predicted labels. It returns a value that
    expresses the percentage of samples that were incorrectly classified.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个示例中使用的`classification_error`函数衡量了真实标签和预测标签之间的差异。它返回一个表示被错误分类的样本百分比的值。
- en: 'The output of the `classification_error` function should confirm what we saw
    when we created the confusion matrix, and will look similar to this:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_error`函数的输出应该与我们在创建混淆矩阵时看到的结果一致，类似于下面的样子：'
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The results may differ because of the random-number generator that is used
    to initialize the model. You can set a fixed random seed for the random-number
    generator using the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会有所不同，因为初始化模型时使用的随机数生成器。你可以通过以下代码为随机数生成器设置固定的随机种子：
- en: '[PRE12]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will fix some variances in output but not all of them. There are a few
    components in CNTK that ignore the fixed random seed and will still generate different
    results each time you run the training code.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这将修正一些输出的变异性，但并不能修正所有变异。CNTK中有一些组件会忽略固定的随机种子，每次运行训练代码时仍然会生成不同的结果。
- en: 'CNTK 2.6 includes the `fmeasure` function, which implements the F-measure we
    discussed in the section, *Using the F-measure as an alternative to the confusion
    matrix*. You can use the `fmeasure` in the training code by replacing the call
    to `cntk.metrics.classification_error` with a call to `cntk.losses.fmeasure` when
    defining the `criterion factory` function:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK 2.6包括了`fmeasure`函数，它实现了我们在“*使用F值作为替代混淆矩阵*”章节中讨论的F值。你可以通过在定义`criterion factory`函数时，将对`cntk.metrics.classification_error`的调用替换为对`cntk.losses.fmeasure`的调用，来在训练代码中使用`fmeasure`：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the training code again will give generate different output for the
    `loss.test` method call:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 再次运行训练代码将生成不同的`loss.test`方法调用输出：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As with the previous samples, the output may vary because of how the random-number
    generator is used to initialize the model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例一样，输出可能会有所不同，因为随机数生成器在初始化模型时的使用方式不同。
- en: Validating performance of a regression model
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证回归模型的性能
- en: In the previous section, *Validating performance of a classification model,*
    we talked about validating the performance of a classification model. Now let's
    look at validating the performance of a regression model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节“*验证分类模型性能*”中，我们讨论了如何验证分类模型的性能。现在让我们来看一下如何验证回归模型的性能。
- en: Regression models are different in that there's no binary measure of right or
    wrong for individual samples. Instead, you want to measure how close the prediction
    is to the actual value. The closer we are to the expected output, the better the
    model performs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型与分类模型不同，因为对于个体样本没有二进制的对错衡量标准。相反，你希望衡量预测值与实际值之间的接近程度。我们越接近期望的输出，模型的表现就越好。
- en: In this section, we'll discuss three methods to measure the performance of a
    neural network that is used for regression. We'll first talk about how to measure
    the performance using different error-rate functions. We'll then talk about how
    to use the coefficient of determination to further validate your regression model.
    Finally, we'll use a residual plot to get down to a very detailed level of how
    our model is doing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论三种衡量用于回归的神经网络性能的方法。我们将首先讨论如何使用不同的错误率函数来衡量性能。然后，我们将讨论如何使用决定系数进一步验证回归模型。最后，我们将使用残差图来详细了解我们的模型表现。
- en: Measuring the accuracy of your predictions
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量预测准确性
- en: Let's first look at the basic concept of validating a regression model. As we
    mentioned before, you can't really say whether a prediction is right or wrong
    when validating a regression model. You want the prediction to be as close to
    the real value as possible, but a small error margin is acceptable.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来看看验证回归模型的基本概念。正如我们之前提到的，在验证回归模型时，你无法真正判断预测是对还是错。你希望预测值尽可能接近实际值，但一个小的误差范围是可以接受的。
- en: 'You can calculate the error margin on predictions made by a regression model
    by looking at the distance between the predicted value and the expected value.
    This can be expressed as a single formula, like so:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看回归模型的预测值与实际值之间的距离来计算预测的误差范围。这个误差可以用一个简单的公式表示，像这样：
- en: '![](img/39ce6970-235e-4e52-aaa3-d2f701b0cdd0.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39ce6970-235e-4e52-aaa3-d2f701b0cdd0.png)'
- en: First, we calculate the distance between the predicted value, *y* indicated
    by a hat, and the real value, *y*, and square it. To get an overall error rate
    for the model, we'll need to sum these squared distances and calculate the average.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算预测值（带帽的*y*）与实际值（*y*）之间的距离并对其进行平方处理。为了得到模型的总体误差率，我们需要将这些平方的距离相加并计算平均值。
- en: 'The square operator is needed to turn negative distances between the predicted
    value, *y* with a hat, and the real value, *y*, into positive distances. Without
    this, we would run into problems: when you have a distance of +100 and another
    of -100 in the next sample, you''ll end up with an error rate of exactly 0\. This
    is, of course, not what we want. The square operator solves this for us.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 需要平方运算符将预测值（带帽的*y*）和实际值（*y*）之间的负距离转换为正距离。如果没有这个步骤，我们会遇到问题：例如，在一个样本中距离为+100，接下来的样本中距离为-100，那么最终的错误率将正好是0。显然，这并不是我们想要的。平方运算符为我们解决了这个问题。
- en: Because we square the distance between the prediction and actual values, we
    punish the computer more for large errors.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们对预测值与实际值之间的距离进行了平方处理，所以我们对计算机的大错误进行了更多的惩罚。
- en: The `mean squared` error function can be used as a metric for validation and
    as a loss function during training. Mathematically, there's no difference between
    the two. This makes it easier to see the performance of a regression model during
    training. You only need to look at the loss to get an idea of the performance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`均方`误差函数可以作为验证的指标，也可以作为训练过程中的损失函数。在数学上，这两者没有区别。这样可以让我们在训练过程中更容易看到回归模型的性能。你只需要看损失值，就能大致了解模型的表现。'
- en: 'It''s important to understand that you get a distance back from the mean squared
    error function. This is not an absolute measure of how well your model performs.
    You have to make a decision what maximum distance between the predicted value
    and expected value is acceptable to you. For example: you could specify that 90%
    of the predictions should have a maximum of 5% difference between the actual and
    the predicted value. This is very valuable for the users of your model. They typically
    want some form of assurance that the model predicts within certain limits.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 需要理解的是，使用均方误差函数你得到的是一个距离值。这并不是衡量模型表现好坏的绝对标准。你需要决定预测值与实际值之间的最大可接受距离。例如，你可以规定90%的预测结果与实际值之间的最大差距应为5%。这对模型的用户来说非常重要，因为他们通常希望得到某种形式的保证，确保模型的预测在某些限制范围内。
- en: 'If you''re looking for performance figures that express an error margin, you''re
    not going to find much use for the `mean squared` error function. Instead, you
    need a formula that expresses the absolute error. This can be done using the `mean
    absolute` error function:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻找表达误差范围的性能数据，那么你可能不会发现`均方`误差函数有什么用处。相反，你需要一个表达绝对误差的公式。你可以使用`平均绝对`误差函数来做到这一点：
- en: '![](img/8bf6778d-a641-4b26-9c10-9a72343647b2.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bf6778d-a641-4b26-9c10-9a72343647b2.png)'
- en: 'This takes the absolute distance between the predicted and the real value,
    sums them, and then takes the average. This will give you a number that is much
    more readable. For example: when you''re talking about house prices, it''s much
    more understandable to present users with a $5,000 error margin than a $25,000
    squared-error margin. The latter seems rather large, but it really isn''t because
    it is a squared value.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法计算预测值与实际值之间的绝对距离，并将其求和，然后取平均值。这将给你一个更容易理解的数值。例如，当你讨论房价时，向用户展示一个5000美元的误差范围比展示一个25000美元的平方误差范围要容易理解得多。后者看起来似乎很大，但实际上并不是，因为它是一个平方值。
- en: We're going to be purely looking at how to use the metrics from CNTK to validate
    a regression model. But it's good to remember to talk to the users of your model
    to determine what performance will be good enough.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于如何使用CNTK中的指标来验证回归模型。但也要记得与模型的用户进行沟通，了解什么样的性能表现才算足够好。
- en: Measuring regression model performance in CNTK
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在CNTK中衡量回归模型性能
- en: 'Now that we''ve seen how to validate regression models in theory, let''s take
    a look at how to use the different metrics we just discussed in combination with
    CNTK. For this section, we''ll be working with a model that predicts miles per
    gallon for cars using the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何从理论上验证回归模型，接下来让我们看看如何结合使用我们刚刚讨论的各种指标与CNTK。在本节中，我们将使用以下代码处理一个预测汽车油耗（每加仑多少英里）的模型：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Follow the given steps:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, import the required components from the `cntk` package
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`cntk`包中导入所需的组件。
- en: Next, define a default activation function using the `default_options` function.
    We're using the `relu` function for this example
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`default_options`函数定义默认激活函数。在本例中，我们使用`relu`函数。
- en: Create a new `Sequential` layer set and provide two `Dense` layers with `64`
    neurons each
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`Sequential`层集，并提供两个具有`64`个神经元的`Dense`层。
- en: Add an additional `Dense` layer to the `Sequential` layer set and give it `1`
    neuron without an activation. This layer will serve as the output layer
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向`Sequential`层集添加额外的`Dense`层，并设置为`1`个神经元且不带激活函数。这一层将作为输出层。
- en: After you've created the network, create an input variable for the input features
    and make sure it has the same shape as the features that we're going to be using
    for training
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建网络后，在输入特征上创建一个输入变量，并确保其形状与我们将用于训练的特征相同。
- en: Create another `input_variable` with size 1 to store the expected value for
    the neural network.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建另一个大小为1的`input_variable`，用于存储神经网络的预期值。
- en: The output layer doesn't have an activation function assigned because we want
    it to be linear. When you leave out the activation function for a layer, CNTK
    will use an identity function instead and the layer will not apply a non-linearity
    to the data. This is useful for regression scenarios since we don't want to limit
    the output to a specific range of values.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层没有分配激活函数，因为我们希望它是线性的。当你在层中留出激活函数时，CNTK会使用一个恒等函数，该层不会对数据应用非线性变换。这对于回归场景很有用，因为我们不希望限制输出在特定值范围内。
- en: 'To train the model, we''re going to need to split the dataset and perform some
    preprocessing:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，我们需要拆分数据集并进行一些预处理：
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Follow the given steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定步骤操作：
- en: First, take the dataset and drop the `mpg` column using the `drop` method. This
    will produce a copy of the original dataset from which we can get the numpy vectors
    from the `values` property.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，获取数据集并使用`drop`方法删除`mpg`列。这将从原始数据集中创建一个副本，我们可以从`values`属性获取numpy向量。
- en: Next, scale the data using a `StandardScaler` so we get values between -1 and
    + 1\. Doing this will help against exploding gradient problems in the neural network.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`StandardScaler`缩放数据，以便在神经网络中获得-1到+1之间的值。这样做有助于避免神经网络中的梯度爆炸问题。
- en: Finally, split the dataset into a training and validation set using the `train_test_split`
    function.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`train_test_split`函数将数据集拆分为训练集和验证集。
- en: 'Once we have split and preprocessed the data, we can train the neural network.
    To train the model, we''re going to define a combination of a `loss` and `metric`
    function to train the model:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们拆分和预处理数据，就可以训练神经网络。要训练模型，我们将定义一个组合`loss`和`metric`函数： '
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Follow the given steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定步骤操作：
- en: Define a new function named `absolute_error`
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为`absolute_error`的新函数。
- en: In the `absolute_error` function calculate the mean absolute difference between
    the output and target
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`absolute_error`函数中计算输出和目标之间的平均绝对差。
- en: Return the result
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回结果。
- en: Next, create another function called `criterion_factory`
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，创建另一个名为`criterion_factory`的函数。
- en: Mark this function with `@cntk.Function` to tell CNTK to include the `train`
    and `test` method on the function
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`@cntk.Function`标记此函数，告诉CNTK在函数上包含`train`和`test`方法。
- en: Within the function, create the `loss` using the `squared_loss` function
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内部，使用`squared_loss`函数创建`loss`。
- en: Then, create the metric using the `absolute_error` function
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用`absolute_error`函数创建度量。
- en: Return both the `loss` and the `metric` as a tuple
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`loss`和`metric`作为一个元组返回。
- en: CNTK will automatically combine these into a single callable function. When
    you invoke the `train` method, the loss is used to optimize the parameters in
    the neural network. When you invoke the `test` method, the metric is used to measure
    the performance of the previously-trained neural network.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK会自动将这些组合成一个单独的可调用函数。当你调用`train`方法时，损失用于优化神经网络中的参数。当你调用`test`方法时，度量用于衡量先前训练的神经网络的性能。
- en: If we want to measure the absolute error for our model we need to write our
    own metric, since the `mean absolute` error function isn't included in the framework.
    This can be done by combining the standard operators included in CNTK.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要为我们的模型测量绝对误差，我们需要编写自己的度量，因为框架中没有包含`mean absolute error`函数。这可以通过结合CNTK中包含的标准运算符来完成。
- en: 'Now that we have a way to create a combined `loss` and `metric` function, let''s
    take a look at how to use it to train the model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了创建组合`loss`和`metric`函数的方法，让我们看看如何使用它来训练模型：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We''ll use `criterion_factory` as the `loss` and `metric` combination for our
    model. When you train the model, you will see that the loss is going down quite
    nicely over time. We can also see that the mean absolute error is going down as
    well:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`criterion_factory`作为模型的`loss`和`metric`组合。当你训练模型时，你会看到损失值随着时间的推移不断下降。我们还可以看到平均绝对误差也在下降：
- en: '[PRE19]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now we need to make sure that our model handles new data just as well is it
    does the training data. To do this, we need to invoke the `test` method on the
    `loss`/`metric` combination with the test dataset:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要确保我们的模型能够像处理训练数据一样有效地处理新数据。为此，我们需要在`loss`/`metric`组合上调用`test`方法，并使用测试数据集。
- en: '[PRE20]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This gives us the value for the performance metric and the number of samples
    it was run on. The output should look similar to the following. It''s low and
    tells us that the model has a small error margin:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了性能指标的值以及运行时的数据样本数。输出应该类似于以下内容。它较低，告诉我们模型的误差范围很小：
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When we call the `test` method on the loss with the test set, it will take the
    test data, `X_test`, and run it through the model to obtain predictions for each
    of the samples. It then runs these through the `metric` function together with
    the expected output, `y_test`. This will result in a single scalar value as the
    output.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在测试集上调用`test`方法时，它会将测试数据`X_test`传递给模型，通过模型获取每个样本的预测值。然后，它将这些预测值与期望的输出`y_test`一起传入`metric`函数。这将产生一个单一的标量值作为输出。
- en: Measuring performance for out-of-memory datasets
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量内存不足数据集的性能
- en: We've talked a lot about different methods to validate the performance of your
    neural networks. So far, we've only had to deal with datasets that fit in memory.
    But this is almost never the case in production scenarios, since you need a lot
    of data to train a neural network. In this section, we'll discuss how to use the
    different metrics on out-of-memory datasets.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了很多不同的方法来验证神经网络的性能。到目前为止，我们只需要处理适合内存的数据集。但在生产场景中几乎从不这样，因为训练神经网络需要大量数据。在本节中，我们将讨论如何在内存不足的数据集上使用不同的指标。
- en: Measuring performance when working with minibatch sources
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量使用小批量数据源时的性能
- en: 'When you use a minibatch data source, you need a slightly different setup for
    the loss and metric. Let''s go back and review how you can set up training using
    a minibatch source and extend it with metrics to validate the model. First, we
    need to set up a way to feed data to the trainer of the model:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用小批量数据源时，你需要为损失函数和指标设置稍微不同的配置。让我们回过头来，回顾一下如何使用小批量数据源设置训练，并通过指标扩展它来验证模型。首先，我们需要设置一种方法，将数据提供给模型的训练器：
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Follow the given steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行操作：
- en: First, import the components needed to create a minibatch source.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入创建小批量数据源所需的组件。
- en: Next, define a new function `create_datasource` with two parameters, `filename`
    and `limit` with a default value of `INFINITELY_REPEAT`.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个新的函数`create_datasource`，该函数有两个参数，`filename`和`limit`，`limit`的默认值为`INFINITELY_REPEAT`。
- en: Within the function, create a `StreamDef` for the labels that reads from the
    labels field that has three features. Set the `is_sparse` keyword argument to
    `False`.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内，为标签创建一个`StreamDef`，它从具有三个特征的标签字段读取数据。将`is_sparse`关键字参数设置为`False`。
- en: Create another `StreamDef` for the features that reads from the features field
    that has four features. Set the `is_sparse` keyword argument to `False`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为特征创建另一个`StreamDef`，它从具有四个特征的特征字段中读取数据。将`is_sparse`关键字参数设置为`False`。
- en: Next, initialize a new instance of `CTFDeserializer` class and specify the filename
    and streams that you want to deserialize.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，初始化一个新的`CTFDeserializer`类实例，并指定要反序列化的文件名和数据流。
- en: Finally, Create a minibatch source using the `deserializer` and configure it
    to shuffle the dataset and specify the `max_sweeps` keyword argument with the
    configured amount of sweeps.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`deserializer`创建一个小批量数据源，并配置它以随机打乱数据集，并指定`max_sweeps`关键字参数，设置为配置的遍历次数。
- en: Remember from [Chapter 3](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml), *Getting
    Data into Your Neural Network*, that to use a minibatch source, you need to have
    a compatible file format. For the classification model in [Chapter 3](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml),
    *Getting Data into Your Neural Network,* we used the CTF file format as input
    for the MinibatchSource. We've included the data files in the sample code for
    this chapter. Check out the `Validating with a minibatch source.ipynb` file for
    more details.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在[第3章](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml)，*将数据导入神经网络*中提到，为了使用MinibatchSource，您需要有兼容的文件格式。对于[第3章](f7cd9148-99e8-427c-acf4-d74c3e52df58.xhtml)，*将数据导入神经网络*中的分类模型，我们使用CTF文件格式作为MinibatchSource的输入。我们已将数据文件包含在本章的示例代码中。有关更多详细信息，请查看`Validating
    with a minibatch source.ipynb`文件。
- en: 'Once we have the data source, we can create the model same model as we used
    in the earlier section, *Validating performance of a classification model*, and
    initialize a training session for it:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有数据源，就可以创建与前面章节中使用的模型相同的模型，*验证分类模型性能*，并为其初始化训练会话：
- en: '[PRE23]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Follow the given steps:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the `ProgressPrinter` to log information about the training process
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`ProgressPrinter`导入，以记录有关训练过程的信息。
- en: Additionally, import the `Trainer` and `training_session` component from the
    `train` module
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，从`train`模块导入`Trainer`和`training_session`组件。
- en: Next, define a mapping between the input variables of the model and the data
    streams from the minibatch source
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义模型的输入变量与MinibatchSource数据流之间的映射。
- en: Then, create a new instance of the `ProgressWriter` to log the output of the
    training progress
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的`ProgressWriter`实例，以记录训练进度的输出。
- en: After, initialize the `trainer` and provide it with the model, the `loss`, the
    `metric`, the `learner` and the `progress_writer`
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，初始化`trainer`并为其提供模型、`loss`、`metric`、`learner`和`progress_writer`。
- en: Finally, invoke the `training_session` function to start the training process.
    Provide the function with the `training_source`, the settings and the mapping
    between the input variables and the data streams from the minibatch source.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，调用`training_session`函数以启动训练过程。为该函数提供`training_source`、设置以及输入变量与MinibatchSource数据流之间的映射。
- en: 'To add validation to this setup, you need to use a `TestConfig` object and
    assign it to the `test_config` keyword argument of the `train_session` function.
    The `TestConfig` object doesn''t have a lot of settings that you need to configure:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要在此设置中添加验证，您需要使用一个`TestConfig`对象，并将其分配给`train_session`函数的`test_config`关键字参数。`TestConfig`对象没有太多您需要配置的设置：
- en: '[PRE24]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Follow the given steps:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the `TestConfig` class from the `train` module
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`train`模块导入`TestConfig`类。
- en: Then, create a new instance of the `TestConfig` with the `test_source`, which
    we created earlier, as input
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的`TestConfig`实例，并将我们之前创建的`test_source`作为输入。
- en: You can use this test configuration during training by specifying the `test_config`
    keyword argument for in the `train` method.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您可以通过在`train`方法中指定`test_config`关键字参数来使用此测试配置。
- en: 'When you run the training session, you will get output that is similar to this:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行训练会话时，您将得到类似于以下的输出：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: First, you'll see that the model is trained using the data from the training
    MinibatchSource. Because we configured the progress printer as a callback for
    the training session, we can see how the loss is progressing. Additionally, you
    will see a metric increasing in value. This metric output comes from the fact
    that we gave the `training_session` function a trainer that had both a loss and
    a metric configured.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将看到模型使用来自训练MinibatchSource的数据进行训练。由于我们将进度打印机配置为训练会话的回调，因此我们可以看到损失的变化。此外，您还会看到一个指标值的增加。这个指标输出来自于我们为`training_session`函数提供了一个既配置了损失又配置了指标的训练器。
- en: When the training finishes, a test pass will be performed over the model using
    the data coming from the MinibatchSource that you configured in the `TestConfig`
    object.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成时，将使用在`TestConfig`对象中配置的MinibatchSource提供的数据对模型进行测试。
- en: What's cool is that not only is your training data now loaded in memory in small
    batches to prevent memory issues, the test data is also loaded in small batches.
    This is really useful if you're working on models with large datasets, even for
    testing purposes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷的是，不仅您的训练数据现在以小批次的方式加载到内存中，以避免内存问题，测试数据也以小批次加载。这在处理大型数据集的模型时特别有用，即使是测试时也是如此。
- en: Measuring performance when working with a manual minibatch loop
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用手动小批量循环时如何衡量性能。
- en: Using metrics when training with the regular APIs in CNTK is the easiest way
    to measure the performance of your model during and after training. Things will
    be more difficult when you work with a manual minibatch loop. This is the point
    where you get the most control though.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNTK中的常规API进行训练时，使用度量来衡量模型在训练过程中和训练后的性能是最简单的方式。但是，当你使用手动小批量循环时，事情就变得更加困难了。不过，这正是你获得最大控制力的地方。
- en: Let's first go back and review how to train a model using a manual minibatch
    loop. We're going to be working on the classification model we used in the section
    *Validating performance of a classification model*. You can find it in the `Validating
    with a manual minibatch loop.ipynb` file in the sample code for this chapter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下如何使用手动小批量循环训练模型。我们将使用在*验证分类模型性能*一节中使用的分类模型。你可以在本章示例代码中的`Validating
    with a manual minibatch loop.ipynb`文件中找到它。
- en: 'The loss for the model is defined as a combination of the `cross-entropy` loss
    function and the F-measure metric that we saw in the section *Using the F-Measure
    as an alternative to the confusion matrix*. You can use the function object combination
    that we used before in the section *Measuring classification performance in CNTK,*
    with a manual training process, which is a nice touch:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的损失定义为`cross-entropy`损失函数与我们在*使用F-measure作为混淆矩阵的替代方法*一节中看到的F-measure度量的组合。你可以使用我们在*测量分类性能（在CNTK中）*一节中使用过的函数对象组合，并使用手动训练过程，这是一个很好的补充：
- en: '[PRE26]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Once we have a loss defined, we can use it in the trainer to set up a manual
    training session. As you might expect, this requires a bit more work to write
    it in Python code:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了损失，就可以在训练器中使用它来设置手动训练会话。正如你所预期的那样，这需要更多的工作来用Python代码编写：
- en: '[PRE27]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Follow the given steps:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定步骤操作：
- en: To get started, import the `numpy` and `pandas` packages to load and preprocess
    the data
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`numpy`和`pandas`包，以便加载和预处理数据。
- en: Next, import the `ProgressPrinter` class to log information during training
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入`ProgressPrinter`类，以便在训练过程中记录信息。
- en: Then, import the `Trainer` class from the `train` module
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从`train`模块导入`Trainer`类。
- en: After importing all the necessary components, create a new instance of the `ProgressPrinter`
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的组件后，创建一个`ProgressPrinter`的实例。
- en: Then, initialize the `trainer` and provide it with the model, the `loss`, the
    `learner` and the `progress_writer`
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，初始化`trainer`并提供模型、`loss`、`learner`和`progress_writer`。
- en: To train the model, create a loop that iterates over the dataset thirty times.
    This will be our outer training loop
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要训练模型，创建一个循环，迭代数据集三十次。这将是我们的外部训练循环。
- en: Next, load the data from disk using `pandas` and set the `chunksize` keyword
    argument to `16` so the dataset is loaded in mini-batches
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`pandas`从磁盘加载数据，并将`chunksize`关键字参数设置为`16`，这样数据集就会以小批量的形式加载。
- en: Iterate over each of the mini-batches using a `for` loop, this will be our inner
    training loop
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环迭代每个小批量，这将是我们的内部训练循环。
- en: Within the `for` loop, read the first four columns using the `iloc` indexer
    as the `features` to train from and convert them to `float32`
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`for`循环中，使用`iloc`索引器读取前四列作为训练特征，并将其转换为`float32`。
- en: Next, read the last column as the label to train from
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，读取最后一列作为训练标签。
- en: The labels are stored as strings, but we one-hot vectors, so convert the label
    strings to their numeric representation
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签以字符串形式存储，但我们需要使用one-hot向量，所以将标签字符串转换为其数字表示。
- en: Then, take the numeric representation of the labels and convert them to a numpy
    array so its easier to work with them
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将标签的数字表示转换为numpy数组，以便更容易处理。
- en: After that, create a new numpy array that has the same number of rows as the
    label values that we just converted. But with `3` columns, representing the number
    of possible classes that the model can predict
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，创建一个新的numpy数组，其行数与我们刚刚转换的标签值相同。但它有`3`列，表示模型可以预测的可能类别数量。
- en: Now, select the columns based on the numeric label values and set it them `1`,
    to create one-hot encoded labels
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，基于数字标签值选择列，并将其设置为`1`，以创建one-hot编码标签。
- en: Finally, invoke the `train_minibatch` method on the `trainer` and feed it the
    processed features and labels for the minibatch
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，调用`trainer`上的`train_minibatch`方法，并将处理后的特征和标签作为小批量输入。
- en: 'When you run the code you swill see output similar to this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行代码时，你将看到类似这样的输出：
- en: '[PRE28]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Because we combined a metric and loss in a function object and used a progress
    printer in the trainer configuration, we get both the output for the loss and
    the metric during training.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们在函数对象中结合了度量和损失，并在训练器配置中使用了进度打印机，所以我们在训练过程中会得到损失和度量的输出。
- en: 'To evaluate the model performance, you need to perform a similar task as with
    training the model. Only this time, we need to use an `Evaluator` instance to
    test the model:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型性能，你需要执行与训练模型类似的任务。只不过这次我们需要使用`Evaluator`实例来测试模型：
- en: '[PRE29]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Follow the given steps:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤进行操作：
- en: First, import the `Evaluator` from the `cntk` package
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`cntk`包中导入`Evaluator`。
- en: Then, create a new instance of the `Evaluator` and provide it the second output
    of the `loss` function
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建`Evaluator`的一个新实例，并提供`loss`函数的第二个输出。
- en: After initializing the `Evalutator`, load the CSV file containing the data and
    provide the `chunksize` parameter so we load the data in batches
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`Evaluator`后，加载包含数据的CSV文件，并提供`chunksize`参数，以便我们按批次加载数据。
- en: Now, iterate over the batches returned by the `read_csv` function to process
    the items in the dataset
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，遍历`read_csv`函数返回的批次，以处理数据集中的项。
- en: Within this loop, read the first four columns as the `features` and convert
    them to `float32`
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个循环中，读取前四列作为`features`并将它们转换为`float32`。
- en: After that, read the labels column
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，读取标签列。
- en: Since the labels are stored as string we need to convert them to a numeric representation
    first
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于标签是以字符串形式存储的，我们需要首先将它们转换为数值表示。
- en: After that, take the underlying numpy array, for easier processing
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，获取底层的numpy数组，方便处理。
- en: Next, create a new array using the `np.zeros` function
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`np.zeros`函数创建一个新数组。
- en: Set the elements at the label indices we obtained in step 7 to `1` to create
    the one-hot encoded vectors for the labels
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第7步中获取的标签索引处的元素设置为`1`，以创建标签的独热编码向量。
- en: Then, invoke the `test_minibatch` method on the `evaluator` and provide it the
    features and encoded labels
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在`evaluator`上调用`test_minibatch`方法，并提供特征和编码后的标签。
- en: Finally, use the `summarize_test_progress` method on the `evaluator` to obtain
    the final performance metrics
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`evaluator`的`summarize_test_progress`方法来获取最终的性能指标。
- en: 'When you run the `evaluator`, you will get output similar to this:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行`evaluator`时，你会得到类似这样的输出：
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: While the manual minibatch loop is a lot more work to set up for both training
    and evaluation, it is one of the most powerful. You can change everything and
    even run evaluation at different intervals during training. This is especially
    useful if you have a model that takes a long time to train. By using testing at
    regular intervals, you can monitor when your model starts to overfit, and you
    can stop the training if you need to.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然手动的迷你批次循环在设置训练和评估时需要更多的工作，但它是最强大的之一。你可以改变一切，甚至在训练过程中以不同的间隔进行评估。如果你有一个需要长时间训练的模型，这尤其有用。通过定期测试，你可以监控模型何时开始过拟合，并在需要时停止训练。
- en: Monitoring your model
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控你的模型。
- en: 'Now that we''ve done some validation on our models, it''s time to talk about
    monitoring your model during training. You saw some of this before in the section
    *Measuring classification performance in CNTK* and the previous [Chapter 2](4c9da7a9-6873-4de9-99a9-43de693d65f8.xhtml),
    *Building Neural Networks with CNTK,* through the use of the `ProgressWriter`
    class, but there are more ways to monitor your model. For example: you can use
    `TensorBoardProgressWriter`. Let''s take a closer look at how monitoring in CNTK
    works and how you can use it to detect problems in your model.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对模型进行了验证，接下来是谈论在训练过程中如何监控模型。你之前在*在CNTK中测量分类性能*一节以及前面[第2章](4c9da7a9-6873-4de9-99a9-43de693d65f8.xhtml)《用CNTK构建神经网络》中，通过使用`ProgressWriter`类看到了一些，但还有更多监控模型的方法。例如：你可以使用`TensorBoardProgressWriter`。让我们更仔细地看看CNTK中的监控是如何工作的，以及如何利用它来检测模型中的问题。
- en: Using callbacks during training and validation
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在训练和验证过程中使用回调。
- en: 'CNTK allows you to specify callbacks in several spots in the API. For example:
    when you call train on a `loss` function, you can specify a set of callbacks through
    the callbacks argument:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK允许你在API的多个位置指定回调。例如：当你在`loss`函数上调用train时，你可以通过回调参数指定一组回调：
- en: '[PRE31]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you''re working with minibatch sources or using a manual minibatch loop,
    you can specify callbacks for monitoring purposes when you create the `Trainer`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用迷你批次源或手动迷你批次循环，可以在创建`Trainer`时指定回调，用于监控目的：
- en: '[PRE32]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'CNTK will invoke these callbacks at set moments:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK将在特定的时刻调用这些回调函数：
- en: When a minibatch is completed
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个小批次完成时
- en: When a full sweep over the dataset is completed during training
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在训练过程中完成整个数据集的遍历时
- en: When a minibatch of testing is completed
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个小批次的测试完成时
- en: When a full sweep over the dataset is completed during testing
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当在测试过程中完成整个数据集的遍历时
- en: A callback in CNTK can be a callable function or a progress writer instance.
    The progress writers use a specific API that corresponds to the four times during
    which logging data is written. We'll leave the implementation of the progress
    writers for your own exploration. Instead, we'll look at how you can use the different
    progress writers during training to monitor your model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNTK中，回调函数可以是一个可调用函数或一个进度写入实例。进度写入器使用特定的API，按四个不同的时间点写入数据日志。我们将留给你自己探索进度写入器的实现。相反，我们将重点看看如何在训练过程中使用不同的进度写入器来监控模型。
- en: Using ProgressPrinter
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`ProgressPrinter`
- en: One of the monitoring tools you will find yourself using quite a lot is `ProgressPrinter`.
    This class implements basic console-based logging to monitor your model. It can
    also log to disk should you want it to. This is especially useful if you're working
    in a distributed training scenario or in a scenario where you can't log in on
    the console to see the output of your Python program.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 你会经常使用的监控工具之一是`ProgressPrinter`。这个类实现了基本的基于控制台的日志记录，用于监控你的模型。如果需要，它也可以将日志写入磁盘。如果你在分布式训练场景下工作，或者无法登录控制台查看Python程序输出，这将特别有用。
- en: 'You can create a `ProgressPrinter` instance like so:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像这样创建一个`ProgressPrinter`实例：
- en: '[PRE33]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You can configure quite a few things in `ProgressPrinter`, but we'll limit ourselves
    to the most-used arguments. You can, however, find more information about `ProgressPrinter`
    on the CNTK website should you want something more exotic.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`ProgressPrinter`中配置很多选项，但我们将仅限于最常用的参数。不过，如果你需要更多的信息，可以访问CNTK网站，了解更多关于`ProgressPrinter`的内容。
- en: When you configure `ProgressPrinter`, you can specify the frequency as the first
    argument to configure how often data should be printed to the output. When you
    specify a value of zero, it will print status messages every other minibatch (1,2,4,6,8,...).
    You can change this setting to a value greater than zero to create a custom schedule.
    For example, when you enter 3 as the frequency, the logger will write status data
    after every 3 minibatches.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当你配置`ProgressPrinter`时，你可以指定频率作为第一个参数，配置数据多长时间输出一次。当你指定零作为值时，它将在每隔一个小批次（1,2,4,6,8,...）后输出状态信息。你可以将这个设置更改为大于零的值，创建一个自定义的调度。例如，当你输入3作为频率时，日志记录器将在每三个小批次后写入状态数据。
- en: 'The `ProgressPrinter` class also accepts a `log_to_file` argument. This is
    where you can specify a filename to write the log data to. The output of the file
    will look similar to this when used:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProgressPrinter`类还接受一个`log_to_file`参数。在这里你可以指定一个文件名，将日志数据写入该文件。当使用时，文件的输出将类似于这样：'
- en: '[PRE34]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This is quite similar to what you've seen before in this chapter when we used
    the `ProgressPrinter` class.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在本章中使用`ProgressPrinter`类时看到的非常相似。
- en: Finally, you can specify how the metrics should be displayed by the `ProgressPrinter`
    class using the `metric_is_pct` setting. Set this to `False` to print the raw
    value instead of the default strategy to print the metric as a percentage.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用`ProgressPrinter`类的`metric_is_pct`设置指定如何显示度量。将其设置为`False`，以便打印原始值，而不是默认策略将度量值打印为百分比。
- en: Using TensorBoard
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard
- en: 'While `ProgressPrinter` can be useful to monitor training progress inside a
    Python notebook, it certainly leaves a lot to be desired. For example: getting
    a good view of how the loss and metric progress over time is hard with `ProgressPrinter`.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`ProgressPrinter`在Python笔记本中监控训练进度时很有用，但它肯定还有很多不足之处。例如：使用`ProgressPrinter`很难清楚地看到损失和度量随着时间的变化。
- en: There's a great alternative to the `ProgressPrinter` class in CNTK. You can
    use `TensorBoardProgressWriter` to log data in a native TensorBoard format.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNTK中，有一个很好的替代`ProgressPrinter`类的工具。你可以使用`TensorBoardProgressWriter`以原生的TensorBoard格式记录数据。
- en: 'TensorBoard is a tool that was invented by Google to be used with TensorFlow.
    It can visualize all sorts of metrics from your model during and after training.
    You can download this tool manually by installing it using PIP:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是Google发明的一个工具，用于与TensorFlow配合使用。它可以在训练过程中和训练后可视化你模型的各种度量。你可以通过手动安装PIP来下载这个工具：
- en: '[PRE35]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To use TensorBoard, you need to set up `TensorBoardProgressWriter` in your
    training code first:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TensorBoard，首先需要在你的训练代码中设置`TensorBoardProgressWriter`：
- en: '[PRE36]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: First, import the `time` package
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`time`包
- en: Next, Import the `TensorBoardProgressWriter`
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入`TensorBoardProgressWriter`
- en: Finally, create a new `TensorBoardProgressWriter` and provide a timestamped
    directory to log to. Make sure to provide the model as a keyword argument so it
    gets sent to TensorBoard during training.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建一个新的`TensorBoardProgressWriter`并提供带时间戳的目录来进行日志记录。确保将模型作为关键字参数提供，以便在训练过程中将其发送到TensorBoard。
- en: We opted to use a separate log `dir` for each run by parameterizing the log
    directory setting with a timestamp. This ensures that multiple runs on the same
    model are logged separately and can be viewed and compared. Finally, you can specify
    the model that you're using the TensorBoard progress writer with.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择为每次运行使用单独的日志`dir`，通过为日志目录设置一个时间戳来参数化。这确保了在同一模型上的多次运行被单独记录，并且可以进行查看和比较。最后，你可以指定在训练过程中使用TensorBoard进度写入器的模型。
- en: Once you've trained your model, make sure you call the `close` method on your
    `TensorboardProgressWriter` instance to ensure that the log files are fully written.
    Without this, you're likely to miss a few, if not all, metrics collected during
    training.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练完模型，确保调用`TensorboardProgressWriter`实例上的`close`方法，以确保日志文件完全写入。没有这个步骤，你可能会漏掉训练过程中收集的某些（如果不是全部）指标。
- en: 'You can visualize the TensorBoard logging data by starting TensorBoard using
    the command in your Anaconda prompt:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在Anaconda提示符中使用命令启动TensorBoard，来可视化TensorBoard的日志数据：
- en: '[PRE37]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `--logdir` argument should match the root `dir` where all runs are logged.
    In this case, we're using the `logs` `dir` as the input source for TensorBoard.
    Now you can open TensorBoard in your browser by going to the URL indicated in
    the console where you started TensorBoard.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '`--logdir`参数应该与所有运行的根`dir`匹配。在本例中，我们使用`logs` `dir`作为TensorBoard的输入源。现在，你可以在浏览器中通过控制台中显示的URL打开TensorBoard。'
- en: 'The TensorBoard web page looks like this, with the SCALARS tab as the default
    page:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard网页如下所示，默认页面为SCALARS标签页：
- en: '![](img/c969bf59-e1d1-4416-8ccb-5b858c9626af.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c969bf59-e1d1-4416-8ccb-5b858c9626af.png)'
- en: Tensorboard web page, with the scalars tab as the default page
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard网页，默认页面为SCALARS标签页
- en: You can view multiple runs by selecting them on the left of the screen. This
    allows you to compare different runs to see how much things have changed between
    the runs. In the middle of the screen, you can check out different charts that
    depict the loss and metrics over time. CNTK will log the metrics per minibatch
    and per epoch, and both can be used to see how metrics have changed over time.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过选择屏幕左侧的多个运行来查看不同的运行。这使你能够比较不同的运行，看看在这些运行之间发生了哪些变化。在屏幕中间，你可以查看描绘损失和指标随时间变化的不同图表。CNTK会按小批次和每个周期记录指标，二者都可以用来查看指标随时间的变化。
- en: 'There are more ways in which TensorBoard helps to monitor your model. When
    you go to the GRAPHS tab, you can see what your model looks like in a nice graphical
    map:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard还有更多方式来帮助你监控模型。当你进入GRAPH标签页时，你可以看到你的模型以精美的图形地图展示出来：
- en: '![](img/06fc5bc5-ac3b-4235-85c9-fb035af05dcb.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06fc5bc5-ac3b-4235-85c9-fb035af05dcb.png)'
- en: Display of your model in a graphical map
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 以图形地图的形式显示你的模型
- en: This is especially useful for technically-complex models with a lot of layers.
    It helps you understand how layers are connected, and it has saved many developers
    from a headache because they were able to find their disconnected layers through
    this tab.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于具有许多层的技术复杂型模型尤其有用。它有助于你理解各层之间的连接，并且通过这个标签页，许多开发人员能够避免头疼，因为他们可以发现哪些层是断开的。
- en: TensorBoard contains many more ways to monitor your model, but sadly CNTK uses
    only the SCALARS and GRAPH tabs by default. You can also log images to TensorBoard
    should you work with them. We'll talk about this later in [Chapter 5](9d91a0e4-3870-4a2f-b483-82fdb8849bc2.xhtml),
    *Working with Images*, when we start to work on images.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard有更多监控模型的方式，但遗憾的是，CNTK默认只使用SCALARS和GRAPH标签页。你也可以将图像记录到TensorBoard中，如果你在工作中使用它们的话。我们将在[第五章](9d91a0e4-3870-4a2f-b483-82fdb8849bc2.xhtml)《*图像处理*》中讨论这一点，届时我们将开始处理图像。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to validate different types of deep learning
    models and how you can use metrics in CNTK to implement validation logic for your
    models. We also explored how to use TensorBoard to visualize training progress
    and the structure of the model so you can easily debug your models.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何验证不同类型的深度学习模型，以及如何使用CNTK中的指标来实现模型的验证逻辑。我们还探讨了如何使用TensorBoard来可视化训练进度和模型结构，以便你可以轻松调试模型。
- en: Monitoring and validating your model early and often will ensure that you end
    up with neural networks that work very well on production and do what your client
    expects them to. It is the only way to detect underfitting and overfitting of
    your model.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 早期并频繁地监控和验证你的模型，将确保你最终得到的神经网络在生产环境中表现良好，并能按客户的期望工作。这是检测模型欠拟合和过拟合的唯一方法。
- en: Now that you know how to build and validate basic neural networks, we'll dive
    into more interesting deep learning scenarios. In the next chapter, we will explore
    how you can use images with neural networks to perform image detection, and in
    [Chapter 6](a5da9ef2-399a-4c30-b751-318d64939369.xhtml), *Working with Time Series
    Data*, we will take a look at how to build and validate deep learning models that
    work on time series data, such as financial market data. You will need all of
    the techniques described in this and previous chapters in the next chapters to
    make the most of the more advanced deep learning scenarios.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何构建和验证基本的神经网络，我们将深入探讨更有趣的深度学习场景。在下一章中，我们将探索如何使用图像与神经网络进行图像检测，另外在[第六章](a5da9ef2-399a-4c30-b751-318d64939369.xhtml)，*时间序列数据的处理*，我们将探讨如何构建和验证适用于时间序列数据的深度学习模型，比如金融市场数据。你将需要本章以及前面章节中描述的所有技术，以便在接下来的章节中充分利用更高级的深度学习场景。
