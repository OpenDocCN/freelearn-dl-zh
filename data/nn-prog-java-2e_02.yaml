- en: Chapter 2. Getting Neural Networks to Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：让神经网络学习
- en: 'Now that you have been introduced to neural networks, it is time to learn about
    their learning process. In this chapter, we''re going to explore the concepts
    involved with neural network learning, along with their implementation in Java.
    We will make a review on the foundations and inspirations for the neural learning
    process that will guide us in implementation of learning algorithms in Java to
    be applied on our neural network code. In summary, these are the concepts addressed
    in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了神经网络，是时候学习它们的 学习过程了。在本章中，我们将探讨神经网络学习所涉及的概念，以及它们在Java中的实现。我们将回顾神经网络学习过程的基础和灵感，这将指导我们在Java中实现学习算法，并将其应用于我们的神经网络代码。总的来说，本章涉及以下概念：
- en: Learning ability
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习能力
- en: How learning helps
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何帮助
- en: Learning paradigms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习范式
- en: Supervised
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The learning process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习过程
- en: Optimization foundations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化基础
- en: The cost function
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数
- en: Error measurement
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误度量
- en: Learning algorithms
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习算法
- en: Delta rule
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Δ规则
- en: Hebbian rule
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫布规则
- en: Adaline/perceptron
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adaline/感知器
- en: Training, test, and validation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、测试和验证
- en: Dataset splitting
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集拆分
- en: Overfitting and overtraining
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合和过训练
- en: Generalization
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泛化
- en: Learning ability in neural networks
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的学习能力
- en: What is really amazing in neural networks is their capacity to learn from the
    environment, just like brain-gifted beings are able to do so. We, as humans, experience
    the learning process through observations and repetitions, until some task, or
    concept is completely mastered. From the physiological point of view, the learning
    process in the human brain is a reconfiguration of the neural connections between
    the nodes (neurons), which results in a new thinking structure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中真正令人惊叹的是它们从环境中学习的能力，就像天赋异禀的生物能够做到的那样。作为人类，我们通过观察和重复来体验学习过程，直到某个任务或概念完全掌握。从生理学的角度来看，人脑中的学习过程是节点（神经元）之间神经连接的重新配置，这导致了一种新的思维结构。
- en: While the connectionist nature of neural networks distributes the learning process
    all over the entire structure, this feature makes this structure flexible enough
    to learn a wide variety of knowledge. As opposed to ordinary digital computers
    that can execute only tasks they are programmed to do, neural systems are able
    to improve and perform new activities according to some satisfaction criteria.
    In other words, neural networks don't need to be programmed; they learn the program
    by themselves.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然神经网络连接的本质将学习过程分布在整个结构中，但这一特性使得该结构足够灵活，可以学习各种知识。与只能执行它们被编程执行的任务的普通数字计算机不同，神经网络能够根据某些满意标准改进和执行新的活动。换句话说，神经网络不需要被编程；它们通过自己学习程序。 '
- en: How learning helps solving problems
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何帮助解决问题
- en: Considering that every task to solve may have a huge number of theoretically
    possible solutions, the learning process seeks to find an optimal solution that
    can produce a satisfying result. The use of structures such as **artificial neural
    networks** (**ANN**) is encouraged due to their ability to acquire knowledge of
    any type, strictly by receiving input stimuli, that is, data relevant to the task/problem.
    At first, the ANN will produce a random result and an error, and based on this
    error, the ANN parameters are adjusted.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每个要解决的问题可能有大量的理论上的可能解决方案，学习过程旨在找到一种最优解，以产生令人满意的结果。由于它们能够通过接收输入刺激（即与任务/问题相关的数据）严格地获取任何类型的知识，因此鼓励使用诸如**人工神经网络**（**ANN**）之类的结构。最初，ANN将产生一个随机结果和一个错误，然后基于这个错误，ANN参数进行调整。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: We can then think of the ANN parameters (weights) as the components of a solution.
    Let's imagine that each weight corresponds to a dimension and one single solution
    represents a single point in the solution hyperspace. For each single solution,
    there is an error measure informing how far that solution is from the satisfaction
    criteria. The learning algorithm then iteratively seeks a solution closer to the
    satisfaction criteria.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将ANN参数（权重）视为解决方案的组成部分。让我们想象每个权重对应一个维度，一个单一的解决方案代表了解决超空间中的一个点。对于每个单一的解决方案，都有一个误差度量，它告知该解决方案距离满意标准的距离。学习算法随后迭代地寻找一个更接近满意标准的解决方案。
- en: Learning paradigms
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习范式
- en: There are basically two types of learning for neural networks, namely supervised,
    and unsupervised. The learning in the human mind, for example, also works in this
    way. We are able to build knowledge from observations without any target (unsupervised)
    or we can have a teacher who shows us the right pattern to follow (supervised).
    The difference between these two paradigms relies mainly on the relevancy of a
    target pattern, and varies from problem to problem.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络基本上有两种学习类型，即监督学习和无监督学习。例如，人类大脑中的学习也是这样进行的。我们能够从观察中构建知识，而不需要目标（无监督）或者我们可以有一个老师向我们展示正确的模式来遵循（监督）。这两种范式之间的区别主要在于目标模式的相关性，并且因问题而异。
- en: Supervised learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'This learning type deals with pairs of *xs* (independent values), and *ys*
    (dependent values) with the objective to map them in a function . Here the Y data
    is the *supervisor*, the target desired outputs, and the X are the source independent
    data that jointly generate the Y data. It is analogous to a teacher who is teaching
    somebody a certain task to be performed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习类型处理的是*xs*（独立值）和*ys*（依赖值）的成对，目标是将它们映射到一个函数中。在这里，Y数据是*监督者*，即期望的输出目标，而X是生成Y数据的源独立数据。这类似于一个正在教授某人执行特定任务的老师：
- en: '![Supervised learning](img/B05964_02_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/B05964_02_01.jpg)'
- en: One particular feature of this learning paradigm is that there is a direct error
    reference which is just the comparison between the target and the current actual
    result. The network parameters are fed into a cost function which quantifies the
    mismatch between desired and actual outputs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习范式的一个特定特征是存在一个直接的错误参考，即目标与当前实际结果之间的比较。网络参数被输入到一个成本函数中，该函数量化了期望输出和实际输出之间的不匹配。
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A cost function is just a measurement to be minimized in an optimization problem.
    That means one seeks to find the parameters that drive the cost function to the
    lowest possible value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数只是优化问题中要最小化的一个度量。这意味着一个人寻求找到驱动成本函数达到最低可能值的参数。
- en: The cost function will be covered in detail later in this chapter
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数将在本章后面详细介绍
- en: The supervised learning is suitable for tasks having a defined pattern to be
    reproduced. Some examples include classification of images, speech recognition,
    function approximation, and forecasting. Note that the neural network should be
    provided a previous knowledge of both input independent values (X) and the output
    dependent values (Y). The presence of a dependent output value is a necessary
    condition for the learning to be supervised.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习适用于具有定义模式要重现的任务。一些例子包括图像分类、语音识别、函数逼近和预测。请注意，神经网络应该提供输入独立值（X）和输出依赖值（Y）的先验知识。存在依赖输出值是学习成为监督的必要条件。
- en: Unsupervised learning
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'In unsupervised learning, we deal only with data without any labeling or classification.
    Instead, one tries to make an inference and extract knowledge by taking into account
    only the independent data X:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，我们只处理没有标签或分类的数据。相反，一个人试图通过只考虑独立数据X来做出推断和提取知识：
- en: '![Unsupervised learning](img/B05964_02_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/B05964_02_02.jpg)'
- en: This is analogous to self-learning, when someone takes into account his/her
    own experience and a set of supporting criteria. In unsupervised learning, we
    don't have a defined desired pattern; instead, we use the provided data to infer
    a dependent output Y without any supervision.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于自我学习，当某人考虑自己的经验和一组支持标准时。在无监督学习中，我们没有定义的期望模式；相反，我们使用提供的数据来推断依赖输出Y，而不需要任何监督。
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In unsupervised learning, the closer the independent data is, more similar the
    generated output should be, and this should be considered in the cost function,
    as opposed to the supervised paradigm.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，独立数据越接近，生成的输出应该越相似，这应该在成本函数中考虑，与监督范式相反。
- en: Examples of tasks that unsupervised learning can be applied to are clustering,
    data compression, statistical modeling, and language modeling. This learning paradigm
    will be covered in more detail in [Chapter 4](ch04.xhtml "Chapter 4. Self-Organizing
    Maps"), *Self-Organizing Maps*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以应用于的任务示例包括聚类、数据压缩、统计建模和语言建模。这种学习范式将在[第4章](ch04.xhtml "第4章. 自组织映射")*自组织映射*中更详细地介绍。
- en: The learning process
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习过程
- en: So far, we have theoretically defined the learning process and how it is carried
    out. But in practice, we must dive a little bit deeper into the mathematical logic,
    in order to implement the learning algorithm itself. For simplicity, in this chapter,
    we are basically covering the supervised learning case; however, we will present
    here a rule for updating weights in unsupervised learning. A learning algorithm
    is a procedure that drives the learning process of neural networks, and it is
    strongly determined by the neural network architecture. From the mathematical
    point of view, one wishes to find the optimal weights W that can drive the cost
    function C(X, Y) to the lowest possible value. However, sometimes the learning
    process cannot find a good set of weights capable of meeting the acceptance criteria,
    but a stop condition must be set to prevent the neural network from learning forever
    and thereby causing the Java program to freeze.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在理论上定义了学习过程及其执行方式。但在实践中，我们必须对数学逻辑进行更深入的探讨，以便实现学习算法本身。为了简化，在本章中，我们基本上涵盖了监督学习的情况；然而，我们在这里将介绍一个无监督学习更新权重的规则。学习算法是驱动神经网络学习过程的程序，它强烈依赖于神经网络架构。从数学的角度来看，人们希望找到最优权重
    W，以驱动成本函数 C(X, Y) 达到尽可能低的值。然而，有时学习过程可能找不到一组能够满足接受标准的良好权重，但必须设置一个停止条件，以防止神经网络永远学习，从而造成
    Java 程序冻结。
- en: 'In general, this process is carried out in the fashion presented in the following
    flowchart:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这个过程按照以下流程图所示的方式进行：
- en: '![The learning process](img/B05964_02_03.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![学习过程](img/B05964_02_03.jpg)'
- en: The cost function finding the way down to the optimum
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找成本函数下降到最优路径
- en: 'Now let''s find out in detail what role the cost function plays. Let''s think
    of cost function as a two-variable function whose shape is represented by a hypersurface.
    For simplicity, let''s consider for now only two weights (two-dimensional space
    plus height representing cost function). Suppose our cost function has the following
    shape:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细探讨成本函数所起的作用。让我们将成本函数想象成一个由超曲面表示的双变量函数的形状。为了简化，我们目前只考虑两个权重（二维空间加上表示成本函数的高度）。假设我们的成本函数具有以下形状：
- en: '![The cost function finding the way down to the optimum](img/B05964_02_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![成本函数下降到最优路径](img/B05964_02_04.jpg)'
- en: 'Visually, we can see that there is an optimum, by which the cost function roughly
    approaches zero. But how can we make this programmatically? The answer lies in
    the mathematical optimization, whereby the cost function is defined as an optimization
    problem:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们可以看到存在一个最优解，通过这个最优解，成本函数大致接近零。但我们如何程序化地实现这一点呢？答案在于数学优化，其中成本函数被定义为优化问题：
- en: '![The cost function finding the way down to the optimum](img/B05964_02_04_01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![成本函数下降到最优路径](img/B05964_02_04_01.jpg)'
- en: By recalling the optimization Fermat's theorems, the optimal solution lies in
    a place where the surface slope should be zero at all dimensions, that is, the
    partial derivative should be zero, and it should be convex (for the minimum case).
    Considering that one starts with an arbitrary solution W, the search for the optimum
    should take into account the direction to which the surface height is going down.
    This is the so-called gradient method.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回忆费马优化定理，最优解位于所有维度上表面斜率应为零的位置，即偏导数应为零，并且应该是凸的（对于最小值情况）。考虑到从一个任意解 W 开始，寻找最优解应考虑表面高度下降的方向。这就是所谓的梯度法。
- en: Learning in progress - weight update
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习进行中 - 权重更新
- en: 'According to the cost function used, an update rule will dictate how the weights,
    the neural flexible parameters, should be changed, so the cost function will have
    a lower value at the new weights:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 根据使用的成本函数，更新规则将决定权重、神经网络可变参数应该如何改变，以便在新的权重下成本函数的值更低：
- en: '![Learning in progress - weight update](img/B05964_02_04_02.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![学习进行中 - 权重更新](img/B05964_02_04_02.jpg)'
- en: Here, k refers to the kth iteration and *W(k)* refers to the neural weights
    at the kth iteration, and subsequently *k+1* refers to the next iteration.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，k 表示第 k 次迭代，*W(k)* 表示第 k 次迭代的神经网络权重，随后 *k+1* 表示下一次迭代。
- en: The weight update operation can be performed in online or batch mode. Online
    here implies that the weights are updated after every single record from the dataset.
    Batch update means that first all the records from the dataset are presented to
    the neural network before it starts updating its weights. This will be explored
    in detail in the code at the end of this chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新操作可以在在线或批量模式下执行。这里的“在线”意味着在数据集的每一条记录之后更新权重。批量更新意味着首先将数据集中的所有记录呈现给神经网络，然后它开始更新其权重。这将在本章末尾的代码中详细探讨。
- en: Calculating the cost function
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算成本函数
- en: 'When a neural network learns, it receives data from an environment and adapts
    its weights according to the objective. This data is referred to as the training
    dataset and has several samples. The idea behind the word training lies in the
    process of adapting the neural weights, as if they were *training* to give the
    desired response in the neural network. While the neural network is still learning,
    there is an error between the target outputs (Y) and the neural outputs (), in
    the supervised case:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络学习时，它从环境中接收数据并根据目标调整其权重。这些数据被称为训练数据集，包含多个样本。单词“训练”背后的理念在于调整神经网络权重的过程，就像它们在“训练”一样，以便在神经网络中给出期望的响应。当神经网络仍在学习时，在监督情况下，目标输出（Y）和神经网络输出（）之间存在误差：
- en: '![Calculating the cost function](img/B05964_02_04_03.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![计算成本函数](img/B05964_02_04_03.jpg)'
- en: Tip
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Some literature about neural networks identifies the target variable with the
    letter T, and the neural output as Y, while in this book we are going to the denote
    it as Y and, to not confuse the reader, since it was presented initially as Y.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于神经网络文献中用字母T表示目标变量，将神经网络输出表示为Y，而在这本书中，我们将用Y来表示它，为了避免读者混淆，因为它最初被表示为Y。
- en: 'Well, given that the training dataset has multiple values, there will be N
    values of errors for each single record. So, how to get an overall error? One
    intuitive approach is to get an average of all errors, but this is misleading.
    The error vector can take on both positive and negative values, therefore an average
    of all error values is very likely to be closer to zero, regardless of how big
    the error measurements may be. Using the absolute value to generate an average
    seems to be a smarter approach, but this function has a discontinuity at the origin,
    what is awkward in calculating its derivative:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，鉴于训练数据集具有多个值，对于每一条记录都会有N个误差值。那么，如何得到总体误差呢？一个直观的方法是取所有误差的平均值，但这可能会误导。误差向量可以取正值和负值，因此所有误差值的平均值很可能接近零，无论误差测量值有多大。使用绝对值来生成平均值似乎是一个更明智的方法，但这个函数在原点处有一个不连续性，这在计算其导数时是尴尬的：
- en: '![Calculating the cost function](img/B05964_02_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![计算成本函数](img/B05964_02_05.jpg)'
- en: 'So, the reasonable option we have is to use the average of a quadratic sum
    of the error, also known as **mean squared error (MSE)**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们合理的选项是使用误差的二次和的平均值，也称为**均方误差（MSE）**：
- en: '![Calculating the cost function](img/B05964_02_05_01.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![计算成本函数](img/B05964_02_05_01.jpg)'
- en: General error and overall error
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般误差和总体误差
- en: 'We need to clarify one thing before going further. The neural network being
    a multiple output structure, we have to deal with the multiple output case, when
    instead of an error vector, we will have an error matrix:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，我们需要明确一点。由于神经网络是一个多输出结构，我们必须处理多输出情况，即当我们有一个误差矩阵而不是误差向量时：
- en: '![General error and overall error](img/B05964_02_05_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![一般误差和总体误差](img/B05964_02_05_02.jpg)'
- en: Well, in such cases, there may be a huge number of errors to work with, whether
    regarding one specific output, a specific record, or the whole dataset. To facilitate
    understanding, let's call the specific-to-record error the general error, by which
    all output errors are given one scalar for the general output error; and the error
    referring to the whole data as overall error.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在这种情况下，可能需要处理大量的错误，无论是关于一个特定的输出、特定的记录还是整个数据集。为了便于理解，让我们将特定到记录的误差称为一般误差，通过它为所有输出误差提供一个标量值，用于一般输出误差；而指整个数据的误差称为总体误差。
- en: 'The general error for single output network is a mere difference between target
    and output, but in the multiple output case, it needs be composed of each output
    error. As we saw, the squared error is a suitable approach to summarize error
    measures, therefore the general error can be calculated using the square of each
    output error:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单输出网络，一般误差仅仅是目标值和输出值之间的差异，但在多输出情况下，它需要由每个输出误差组成。正如我们所见，平方误差是总结误差测量的合适方法，因此一般误差可以通过每个输出误差的平方来计算：
- en: '![General error and overall error](img/B05964_02_05_03.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![一般误差和整体误差](img/B05964_02_05_03.jpg)'
- en: As for the overall error, it actually considers the general error but for all
    records in the dataset. Since the dataset can be huge, it is better to calculate
    the overall error using the MSE of the quadratic general errors.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 至于整体误差，它实际上考虑的是一般误差，但针对数据集中的所有记录。由于数据集可能非常大，因此最好使用二次一般误差的均方误差（MSE）来计算整体误差。
- en: Can the neural network learn forever? When is it good to stop?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络能否永远学习？何时停止学习是合适的？
- en: 'As the learning process is run, the neural network must give results closer
    and closer to the expectation, until finally it reaches the acceptation criteria
    or one limitation in learning iterations, that we''ll call epochs. The learning
    process is then considered to be finished when one of these conditions is met:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着学习过程的进行，神经网络必须给出越来越接近预期的结果，直到最终达到接受标准或学习迭代中的一个限制，我们称之为迭代次数。当满足以下条件之一时，学习过程被认为是完成的：
- en: '**Satisfaction criterion**: minimum overall error or minimum weight distance,
    according to the learning paradigm'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**满意标准**：根据学习范式，最小整体误差或最小权重距离'
- en: '**Maximum number of epochs**'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大迭代次数**'
- en: Examples of learning algorithms
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习算法的示例
- en: Let's now merge the theoretical content presented so far together into simple
    examples of learning algorithms. In this chapter, we are going to explore a couple
    of learning algorithms in single layer neural networks; multiple layers will be
    covered in the next chapter.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将到目前为止所提出的理论内容合并到学习算法的简单示例中。在本章中，我们将探讨单层神经网络中的几个学习算法；多层将在下一章中介绍。
- en: In the Java code, we will create one new superclass `LearningAlgorithm` in a
    new package `edu.packt.neural.learn`. Another useful package called `edu.packt.neural.data`
    will be created to handle datasets that will be processed by the neural network,
    namely the classes `NeuralInputData`, and `NeuralOutputData`, both referenced
    by the `NeuralDataSet` class. We recommend the reader takes a glance at the code
    documentation to understand how these classes are organized, to save text space
    here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Java代码中，我们将在新的包`edu.packt.neural.learn`中创建一个新的超类`LearningAlgorithm`。另一个有用的包名为`edu.packt.neural.data`，它将被创建来处理神经网络将处理的数据集，即`NeuralInputData`和`NeuralOutputData`这两个类，它们都由`NeuralDataSet`类引用。我们建议读者浏览一下代码文档，以了解这些类的组织结构，以节省这里的文本空间。
- en: 'The `LearningAlgorithm` class has the following attributes and methods:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`LearningAlgorithm`类具有以下属性和方法：'
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `neuralNet` object is a reference to the neural network that will be trained
    by this learning algorithm. The `enums` define the learning mode and learning
    paradigm. The learning executing parameters are defined (MaxEpochs, MinOverallError,
    LearningRate), and the datasets that will be taken into account during the learning
    process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralNet`对象是这个学习算法将要训练的神经网络的引用。`enums`定义了学习模式和学习的范式。学习执行参数被定义（MaxEpochs，MinOverallError，LearningRate），以及在学习过程中将考虑的数据集。'
- en: The method `train( )` should be overridden by each learning algorithm implementation.
    All the training process will occur in this method. The methods `forward( )` and
    `forward(int k)` process the neural network with all input data and with the kth
    input data record, respectively. And finally, the method `calcNewWeight( )` will
    perform the weight update for the weight connecting an input to a neuron in a
    specific layer. A variation in the `calcNewWeight( )` method allows providing
    a specific error to be taken in the update operation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 方法`train( )`应该由每个学习算法实现重写。所有的训练过程都将在这个方法中发生。方法`forward( )`和`forward(int k)`分别处理包含所有输入数据的神经网络和第k个输入数据记录。最后，方法`calcNewWeight(
    )`将对连接输入到特定层中神经元的权重进行更新。`calcNewWeight( )`方法的变化允许在更新操作中提供一个特定的误差。
- en: The delta rule
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: delta规则
- en: 'This algorithm updates the weights according to the cost function. Following
    the gradient approach, one wants to know which weights can drive the cost function
    to a lower value. Note that we can find the direction by computing the partial
    derivative of the cost function to each of the weights. To help in understanding,
    let''s consider one simple approach with only one neuron, one weight, and one
    bias, and therefore one input. The output will be as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法根据成本函数更新权重。遵循梯度方法，人们想知道哪些权重可以驱动成本函数达到更低的值。请注意，我们可以通过计算成本函数对每个权重的偏导数来找到方向。为了帮助理解，让我们考虑一个只有一个神经元、一个权重和一个偏差、因此只有一个输入的简单方法。输出将如下所示：
- en: '![The delta rule](img/B05964_02_05_04.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![delta规则](img/B05964_02_05_04.jpg)'
- en: 'Here, g is the activation function, X is the vector containing x values, and
    Y is the output vector generated by the neural network. The general error for
    the kth sample is quite simple:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，g是激活函数，X是包含x值的向量，Y是由神经网络生成的输出向量。第k个样本的一般误差相当简单：
- en: '![The delta rule](img/B05964_02_05_05.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![delta规则](img/B05964_02_05_05.jpg)'
- en: 'However, it is possible to define this error as square error, N-degree error,
    or MSE. But, for simplicity, let''s consider the simple error difference for the
    general error. Now the overall error, that will be the cost function, should be
    computed as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可以将这个误差定义为平方误差、N次方误差或均方误差。但为了简单起见，让我们考虑一般误差的简单误差差。现在，整体误差，也就是成本函数，应该按照以下方式计算：
- en: '![The delta rule](img/B05964_02_05_06.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![delta规则](img/B05964_02_05_06.jpg)'
- en: 'The weight and bias are updated according to the delta rule, that considers
    the partial derivatives ![The delta rule](img/B05964_02_05_07.jpg) with respect
    to the weight and the bias, respectively. For the batch training mode, X and E
    are vectors:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏差根据delta规则进行更新，该规则考虑了权重和偏差的偏导数![delta规则](img/B05964_02_05_07.jpg)。对于批量训练模式，X和E是向量：
- en: '![The delta rule](img/B05964_02_05_08.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![delta规则](img/B05964_02_05_08.jpg)'
- en: 'If the training mode is online, we don''t need to perform dot product:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练模式是在线的，我们不需要执行点积：
- en: '![The delta rule](img/B05964_02_05_09.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![delta规则](img/B05964_02_05_09.jpg)'
- en: The learning rate
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: 'Note in the preceding equations the presence of the term α that indicates the
    learning rate. It plays an important role in weight update, because it can drive
    faster or slower to the minimum cost value. Let''s see a cost error surface in
    relation to two weights:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在先前的方程中存在表示学习率的项α。它在权重更新中起着重要作用，因为它可以更快或更慢地达到最小成本值。让我们看看与两个权重相关的成本误差表面：
- en: '![The learning rate](img/B05964_02_06.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![学习率](img/B05964_06.jpg)'
- en: Implementing the delta rule
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现delta规则
- en: 'We will implement the delta rule in a class called `DeltaRule`, that will extend
    the `LearningAlgorithm` class:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在名为`DeltaRule`的类中实现delta规则，该类将扩展`LearningAlgorithm`类：
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The errors discussed in the error measurement section (general and overall errors)
    are implemented in the `DeltaRule` class, because the delta rule learning algorithm
    considers these errors during the training. They are arrays because there will
    be a general error for each dataset record, and there will be an overall error
    for each output. An attribute `overallGeneralError` takes on the cost function
    result, or namely the overall error for all outputs and records. A matrix called
    error, stores the errors for each output record combination.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在误差测量部分（一般误差和整体误差）中讨论的误差在`DeltaRule`类中实现，因为delta规则学习算法在训练期间考虑了这些误差。它们是数组，因为每个数据集记录都会有一般误差，每个输出也会有整体误差。一个名为`overallGeneralError`的属性承担成本函数的结果，即所有输出和记录的整体误差。一个名为error的矩阵存储每个输出记录组合的误差。
- en: This class also allows multiple ways of calculating the overall and general
    errors. The attributes `generalErrorMeasurement` and `overallErrorMeasurement`
    can take on one of the input values for simple error, square error calculation,
    Nth degree error (cubic, quadruple, and so on), or the MSE. The default will be
    simple error for the general error and MSE for the overall.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此类还允许以多种方式计算整体和一般误差。`generalErrorMeasurement`和`overallErrorMeasurement`属性可以接受简单误差、平方误差计算、N次方误差（立方、四次方等）或均方误差的输入值之一。默认情况下，一般误差将使用简单误差，整体误差将使用均方误差。
- en: 'Two important attributes are worth noting in this code: `currentRecord` refers
    to the index of the record being fed into the neural network during training,
    and the `newWeights` cubic matrix is a collection of all new values of weights
    that will be updated in the neural network. The `currentRecord` attribute is useful
    in the online training, and the `newWeights` matrix helps the neural network to
    keep all of its original weights until all new weights calculation is finished,
    preventing new weights to be updated during the forward processing stage, what
    could compromise the training quality significantly.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，有两个重要的属性值得注意：`currentRecord`指的是在训练过程中被输入到神经网络的记录的索引，而`newWeights`立方矩阵是所有将要更新的神经网络新权重的集合。`currentRecord`属性在在线训练中很有用，而`newWeights`矩阵帮助神经网络在其所有原始权重计算完成之前保持所有原始权重，防止在正向处理阶段更新新权重，这可能会严重影响训练质量。
- en: The core of the delta rule learning - train and calcNewWeight methods
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度规则学习的核心 - 训练和calcNewWeight方法
- en: 'To save space, we will not detail here the implementation of the forward methods.
    As described in the previous section, forward means that neural dataset records
    should be fed into the neural network and then the error values are calculated:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，我们在此不详细说明前向方法的实现。如前所述，前向意味着应该将神经网络数据集记录输入到神经网络中，然后计算误差值：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We note that in the `train( )` method, there is a loop with a condition to continue
    training. This means that while the training will stop when this condition no
    longer holds true. The condition checks the `epoch` number and the overall error.
    When the `epoch` number reaches the maximum or the error reaches the minimum,
    the training is finished. However, there are some cases in which the overall error
    fails to meet the minimum requirement, and the neural network needs to stop training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到在`train( )`方法中，有一个带有继续训练条件的循环。这意味着当这个条件不再成立时，训练将停止。该条件检查`epoch`数量和总体误差。当`epoch`数量达到最大值或误差达到最小值时，训练完成。然而，在某些情况下，总体误差未能达到最小要求，神经网络需要停止训练。
- en: 'The new weight is calculated using the `calcNewWeight( )` method:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 新的权重是通过`calcNewWeight( )`方法计算的：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that in the weight update, there a call to the derivative of the activation
    function of the given neuron. This is needed to meet the delta rule. In the activation
    function interface, we've added this method `derivative( )` to be overridden in
    each of the implementing classes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在权重更新中，有一个调用给定神经元的激活函数的导数。这是满足梯度规则所需的。在激活函数接口中，我们添加了此`derivative( )`方法，以便在每个实现类中重写。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note: For the batch mode the call to the `derivativeBatch( )`, that receives
    and returns an array of values, instead of a single scalar.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：对于批量模式，调用`derivativeBatch( )`时，接收并返回一个值数组，而不是单个标量。
- en: In the `train( )` method, we've seen that new weights are stored in the `newWeights`
    attribute, to not influence the current learning process, and are only applied
    after the training iteration has finished.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train( )`方法中，我们看到了新权重被存储在`newWeights`属性中，以避免影响当前的学习过程，并且仅在训练迭代完成后才应用。
- en: Another learning algorithm - Hebbian learning
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一种学习算法 - 海布学习算法
- en: 'In the 1940s, the neuropsychologist Donald Hebb postulated that the connections
    between neurons that activate or fire simultaneously, or using his words, repeatedly
    or persistently, should be increased. This is one approach of unsupervised learning,
    since no target output is specified for Hebbian learning:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪40年代，神经心理学家唐纳德·海布提出了这样的假设：同时激活或放电的神经元之间的连接，或者用他的话说，反复或持续地，应该被增强。这是无监督学习的一种方法，因为海布学习没有指定目标输出：
- en: '![Another learning algorithm - Hebbian learning](img/B05964_02_07.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![另一种学习算法 - 海布学习算法](img/B05964_02_07.jpg)'
- en: 'In summary, the weight update rule for Hebbian learning takes into account
    only the input and outputs of the neuron. Given a neuron j whose connection to
    neuron i (weight ij) is to be updated, the update is given by the following equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，海布学习算法的权重更新规则只考虑神经元的输入和输出。给定一个要更新的神经元j，其与神经元i（权重ij）的连接，更新由以下方程给出：
- en: '![Another learning algorithm - Hebbian learning](img/B05964_02_07_01.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![另一种学习算法 - 海布学习算法](img/B05964_02_07_01.jpg)'
- en: Here, α is a learning rate, oj is the output of the neuron j, and oi is the
    output of the neuron i, also the input i for the neuron j. For the batch training
    case, oi and oj will be vectors, and we'll need to perform a dot product.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，α是学习率，oj是神经元j的输出，oi是神经元i的输出，也是神经元j的输入i。对于批量训练案例，oi和oj将是向量，我们需要执行点积。
- en: Since we don't include error measurement in Hebbian learning, a stop condition
    can be determined by the maximum number of epochs or the increase in the overall
    average of neural outputs. Given N records, we compute the expectancy or average
    of all outputs produced by the neural network. When this average increases over
    a certain level, it is time to stop the training, to prevent the neural outputs
    from blowing up.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有在Hebbian学习中包含错误测量，可以通过最大epoch数或神经网络输出的整体平均值的增加来确定停止条件。给定N条记录，我们计算神经网络产生的所有输出的期望值或平均值。当这个平均值超过一定水平时，就是停止训练的时候了，以防止神经输出的爆炸。
- en: 'We''ll develop a new class for Hebbian learning, also inheriting from `LearningAlgorithm`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个新的类用于Hebbian学习，它也继承自`LearningAlgorithm`：
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'All parameters except for the absent error measures and the new measures of
    mean are identical to the `DeltaRule` class. The methods are quite similar, except
    for the `calcNewWeight( )`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缺失的错误测量和新的平均测量之外，所有参数都与`DeltaRule`类相同。方法相当相似，除了`calcNewWeight( )`：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Adaline
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Adaline
- en: Adaline is an architecture standing for Adaptive Linear Neuron, developed by
    Bernard Widrow and Ted Hoff, based on the McCulloch, and Pitts neuron. It has
    only one layer of neurons and can be trained similarly to the delta rule. The
    main difference lies in the fact that the update rule is given by the error between
    the weighted sum of inputs and biases and the target output, instead of updating
    based on the neuron output after the activation function. This may be desirable
    when one wants to perform continuous learning for classification problems, which
    tend to use discrete values instead of continuous.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline是一种代表自适应线性神经元的架构，由Bernard Widrow和Ted Hoff开发，基于McCulloch和Pitts神经元。它只有一层神经元，可以类似于delta规则进行训练。主要区别在于更新规则是由输入加权和偏差与目标输出的误差给出，而不是基于激活函数后的神经元输出进行更新。当想要对分类问题进行连续学习时，这可能是所希望的，因为分类问题倾向于使用离散值而不是连续值。
- en: 'The following figure illustrates how Adaline learns:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了Adaline是如何学习的：
- en: '![Adaline](img/B05964_02_08.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Adaline](img/B05964_02_08.jpg)'
- en: 'So the weights are updated by the following equation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，权重是通过以下方程更新的：
- en: '![Adaline](img/B05964_02_08_01.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![Adaline](img/B05964_02_08_01.jpg)'
- en: 'In order to implement Adaline, we create a class called **Adaline** with the
    following overridden `weight calcNewWeight`. To save space, we''re presenting
    only the online case:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现Adaline，我们创建了一个名为**Adaline**的类，其中包含以下重写的`calcNewWeight`方法。为了节省空间，我们只展示在线案例：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note the method `getOutputBeforeActivation( )`; we mentioned in the last chapter
    that this property would be useful in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在上一章中提到的`getOutputBeforeActivation( )`方法；这个属性在将来会很有用。
- en: Time to see the learning in practice!
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现在是时候看到学习在实际中的应用了！
- en: 'Let''s work on a very simple yet illustrative example. Suppose you want a single
    neuron neural network to learn how to fit a simple linear function such as the
    following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们处理一个非常简单但具有说明性的例子。假设你想要一个单神经元神经网络学习如何拟合以下简单的线性函数：
- en: '| ![Time to see the learning in practice!](img/B05964_02_09_01.jpg) | ![Time
    to see the learning in practice!](img/B05964_02_09.jpg) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ![现在是时候看到学习在实际中的应用了！](img/B05964_02_09_01.jpg) | ![现在是时候看到学习在实际中的应用了！](img/B05964_02_09.jpg)
    |'
- en: This is quite easy even for those who have little math background, so guess
    what? It is a nice start for our simplest neural network to prove its learning
    ability!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对数学背景薄弱的人来说也很简单，所以猜猜看？这是我们最简单的神经网络证明其学习能力的一个好起点！
- en: Teaching the neural network – the training dataset
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教授神经网络 – 训练数据集
- en: 'We;re going to structure the dataset for the neural network to learn using
    the following code, which you can find in the main method of the file `NeuralNetDeltaRuleTest`:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来结构化神经网络的学习数据集，你可以在文件`NeuralNetDeltaRuleTest`的主方法中找到它：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `funcTest` function is defined as the function we mentioned:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`funcTest`函数定义为我们在上一章中提到的函数：'
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that we''re using the class `NeuralDataSet` to structure all this data
    in such a way that they will be fed into the neural network the right way. Now
    let''s link this dataset to the neural network. Remember that this network has
    a single neuron in the output. Let''s use a nonlinear activation function such
    as hyperbolic tangent at the output with a coefficient `0.85`:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在使用`NeuralDataSet`类来以正确的方式组织所有这些数据。现在让我们将这个数据集链接到神经网络。记住，这个网络在输出端只有一个神经元。让我们使用一个非线性激活函数，比如输出端的双曲正切函数，系数为`0.85`：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s now instantiate the `DeltaRule` object and link it to the neural network
    created. Then we''ll set the learning parameters such as learning rate, minimum
    overall error, and maximum number of epochs:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实例化`DeltaRule`对象并将其链接到创建的神经网络。然后我们将设置学习参数，如学习率、最小总体误差和最大迭代次数：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s see the first neural output of the untrained neural network, after
    calling the method `forward( )` of the `deltaRule` object:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看未训练的神经网络的第一批神经输出，这是在调用`deltaRule`对象的`forward()`方法之后：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Teaching the neural network – the training dataset](img/B05964_02_10.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![教授神经网络 – 训练数据集](img/B05964_02_10.jpg)'
- en: 'Plotting a chart, we find that the output generated by the neural network is
    a little bit different:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制图表，我们发现神经网络生成的输出略有不同：
- en: '![Teaching the neural network – the training dataset](img/B05964_02_11.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![教授神经网络 – 训练数据集](img/B05964_02_11.jpg)'
- en: 'We will start training the neural network in the online mode. We''ve set the
    `printTraining` attribute as true, so we will receive in the screen an update.
    The following piece of code will produce the subsequent screenshot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始以在线模式训练神经网络。我们已经将`printTraining`属性设置为true，因此我们将在屏幕上收到更新。以下代码将生成后续的截图：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Teaching the neural network – the training dataset](img/B05964_02_12.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![教授神经网络 – 训练数据集](img/B05964_02_12.jpg)'
- en: 'The training begins and the overall error information is updated after every
    weight update. Note the error is decreasing:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 训练开始，并在每次权重更新后更新总体错误信息。注意错误正在减少：
- en: '![Teaching the neural network – the training dataset](img/B05964_02_13.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![教授神经网络 – 训练数据集](img/B05964_02_13.jpg)'
- en: 'After five epochs, the error reaches the minimum; now let''s see the neural
    outputs and the plot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 经过五个迭代周期后，错误达到最小值；现在让我们看看神经输出和图表：
- en: '| ![Teaching the neural network – the training dataset](img/B05964_02_14.jpg)
    | ![Teaching the neural network – the training dataset](img/B05964_02_15.jpg)
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![教授神经网络 – 训练数据集](img/B05964_02_14.jpg) | ![教授神经网络 – 训练数据集](img/B05964_02_15.jpg)
    |'
- en: 'Quite amazing, isn''t it? The target and the neural output are practically
    the same. Now let''s take a look at the final `weight` and `bias`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 非常令人惊讶，不是吗？目标和神经输出几乎相同。现在让我们看看最终的`权重`和`偏差`：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Amazing, it learned! Or, did it really? A further step – testing
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惊人的是，它已经学习到了！或者，它真的学习到了吗？下一步——测试
- en: 'Well, we might ask now: so the neural network has already learned from the
    data; how can we attest it has effectively learned? Just like in exams students
    are subjected to, we need to check the network response after training. But wait!
    Do you think it is likely a teacher would put in an exam the same questions he/she
    has presented in class? There is no sense in evaluating somebody''s learning with
    examples that are already known, or a suspecting teacher would conclude the student
    might have memorized the content, instead of having learned it.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们现在可能会问：那么神经网络已经从数据中学习到了；我们如何证明它已经有效地学习了呢？就像学生在考试中会遇到的，我们需要检查训练后的网络响应。但是等等！你认为老师会在考试中出与课堂上所讲相同的问题吗？用已知例子来评估某人的学习是没有意义的，或者怀疑老师可能会得出学生可能只是记住了内容而没有真正学习的结论。
- en: Okay, let's now explain this part. What we are talking about here is testing.
    The learning process we have covered is called training. After training a neural
    network, we should test whether it has really learnt. For testing, we must present
    to the neural network another fraction of data from the same environment it has
    learnt from. This is necessary because, just like the student, the neural network
    could respond properly to only the data points it has been exposed to; this is
    called overtraining. To check whether the neural network has not passed on overtraining,
    we must check its response to other data points.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们解释这部分。我们在这里讨论的是测试。我们之前介绍的学习过程被称为训练。训练神经网络后，我们应该测试它是否真的学到了东西。为了测试，我们必须向神经网络展示来自它学习过的相同环境的另一部分数据。这是必要的，因为，就像学生一样，神经网络只能正确响应它接触过的数据点；这被称为过训练。为了检查神经网络是否没有过度训练，我们必须检查它对其他数据点的响应。
- en: 'The following figure illustrates the overtraining problem. Imagine that our
    network is designed to approximate some function f(x) whose definition is unknown.
    The neural network was fed with some data from that function and produced the
    result shown on the left in the following figure. But when expanding to a wider
    domain, for example, adding a testing dataset, we note the neural response does
    not follow the data (on the right in the figure):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了过训练问题。想象一下，我们的网络被设计用来逼近某个函数 f(x)，其定义是未知的。神经网络被喂食了该函数的一些数据，并在下图的左侧产生了结果。但当扩展到更广泛的领域时，例如，添加测试数据集，我们注意到神经网络的响应没有遵循数据（在图的右侧）：
- en: '![Amazing, it learned! Or, did it really? A further step – testing](img/B05964_02_16.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![它学会了！还是真的学会了？进一步的一步——测试](img/B05964_02_16.jpg)'
- en: 'In this case, we see that the neural network failed to learn the whole environment
    (the function *f(x)*). This happens because of a number of reasons:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们看到神经网络未能学习整个环境（函数 *f(x)*）。这发生是因为以下多个原因：
- en: The neural network didn't receive enough information from the environment
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络没有从环境中获得足够的信息
- en: The data from the environment is nondeterministic
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自环境的数据是非确定性的
- en: The training and testing datasets are poorly defined
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试数据集定义得不好
- en: The neural network has learnt so much on the training data, it has *forgotten*
    about the testing data
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络在训练数据上学习得太多，以至于它已经“忘记”了测试数据
- en: Throughout this book, we are going to cover the process to prevent this and
    other issues that may arise during training.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将介绍防止这种情况以及其他在训练过程中可能出现的问题的过程。
- en: Overfitting and overtraining
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和过训练
- en: 'In our previous example, the neural network seemed to have learned amazingly
    well. However, there is a risk of overfitting and overtraining. The difference
    between these two concepts is very subtle. An overfitting occurs when the neural
    network memorizes the problem''s behavior, so that it can provide good values
    only on training points, therefore losing a generalization capacity. Overtraining,
    which can be a cause for overfitting, occurs when the training error becomes much
    smaller than the testing error, or actually, the testing error starts to increase
    as the neural network continues (over)training:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的例子中，神经网络似乎学得非常好。然而，存在过拟合和过训练的风险。这两个概念之间的区别非常微妙。过拟合发生在神经网络记住问题的行为，因此它只能在训练点上提供良好的值，从而失去了泛化能力。过训练，这可能是过拟合的原因之一，发生在训练误差远小于测试误差的情况下，或者实际上，随着神经网络继续（过度）训练，测试误差开始增加：
- en: '![Overfitting and overtraining](img/B05964_02_17.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![过拟合和过训练](img/B05964_02_17.jpg)'
- en: One of ways to prevent overtraining and overfitting is checking the testing
    error when the training goes on. When the testing error starts to increase, it
    is time to stop. This will be covered more in detail in the next chapters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过训练和过拟合的一种方法是在训练过程中检查测试误差。当测试误差开始增加时，就是停止的时候了。这将在下一章中更详细地介绍。
- en: 'Now, let''s see if there is the case in our example. Let''s now add some more
    data and test it:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在我们的例子中是否存在这种情况。现在让我们添加一些更多数据并对其进行测试：
- en: '[PRE14]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '| ![Overfitting and overtraining](img/B05964_02_18.jpg) | ![Overfitting and
    overtraining](img/B05964_02_19.jpg) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ![过拟合和过训练](img/B05964_02_18.jpg) | ![过拟合和过训练](img/B05964_02_19.jpg) |'
- en: As can be seen, the neural network presents a generalization capacity in this
    case. In spite of the simplicity of this example, we can still see the learning
    skill of the neural network.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，在这种情况下，神经网络表现出了一般化能力。尽管这个例子很简单，我们仍然可以看到神经网络的学习能力。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter presented the reader with the whole learning process of which neural
    networks are capable. We presented the very basic foundations of learning, inspired
    by human learning itself. To illustrate this process in practice, we have implemented
    two learning algorithms in Java, and applied them in two examples. With this,
    the reader can have a basic but useful understanding on how neural networks learn
    and even how one can systematically describe the process of learning. This will
    be the foundation for the next chapters, which will present more complex examples.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向读者展示了神经网络能够实现的整体学习过程。我们介绍了学习的基础知识，这些知识灵感来源于人类自身的学习。为了在实践中说明这一过程，我们已经在Java中实现了两种学习算法，并在两个示例中应用了它们。通过这种方式，读者可以对神经网络的学习方式有一个基本但有用的理解，甚至可以系统地描述学习过程。这将是下一章的基础，下一章将展示更复杂的示例。
