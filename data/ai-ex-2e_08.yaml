- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Solving the XOR Problem with a Feedforward Neural Network
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用前馈神经网络解决 XOR 问题
- en: In the course of a corporate project, there always comes the point when a problem that
    seems impossible to solve hits you. At that point, you try everything you've learned,
    but it doesn't work for what's asked of you. Your team or customer begins to look
    elsewhere. It's time to react.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业项目的过程中，总会有某一时刻，遇到一个看似无法解决的问题。此时，你会尝试所有学到的东西，但对方的要求还是无法满足。你的团队或客户开始寻找其他解决方案。是时候采取行动了。
- en: In this chapter, an impossible-to-solve business case regarding material optimization
    will be resolved successfully with a hand-made version of a **feedforward neural
    network** (**FNN**) with backpropagation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，关于材料优化的一个无法解决的商业案例将通过手工制作的**前馈神经网络**（**FNN**）与反向传播成功解决。
- en: 'Feedforward networks are one of the key building blocks of deep learning. The
    battle around the XOR function perfectly illustrates how deep learning regained
    popularity in corporate environments. XOR is an exclusive OR function that we
    will explore later in this chapter. The XOR FNN illustrates one of the critical
    functions of neural networks: **classification**. Once information becomes classified
    into subsets, it opens the doors to **prediction** and many other functions of
    neural networks, such as representation learning.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络是深度学习的关键构建模块之一。围绕 XOR 函数的争论完美地说明了深度学习如何在企业环境中重新获得流行。XOR 是一种排他性“或”函数，我们将在本章后面探讨。XOR
    FNN 说明了神经网络的一个关键功能：**分类**。一旦信息被分类为子集，它就为**预测**和神经网络的许多其他功能打开了大门，如表征学习。
- en: An XOR FNN will be built from scratch to demystify deep learning from the start.
    A vintage, start-from-scratch method will be applied, blowing the deep learning
    hype off the table.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 XOR FNN 将从零开始构建，以从一开始就解密深度学习。我们将应用一种复古的、从头开始的方法，揭开深度学习的炒作面纱。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Explaining the XOR problem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释 XOR 问题
- en: How to hand-build an FNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何手动构建 FNN
- en: Solving XOR with an FNN
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 FNN 解决 XOR 问题
- en: Classification
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Backpropagation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: A cost function
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数
- en: Cost function optimization
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数优化
- en: Error loss
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误损失
- en: Convergence
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收敛
- en: Before we begin building an FNN, we'll first introduce XOR and its limitations
    in the first artificial neural model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建 FNN 之前，我们首先会介绍 XOR 及其在第一个人工神经模型中的局限性。
- en: The original perceptron could not solve the XOR function
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始感知器无法解决 XOR 函数
- en: The original perceptron was designed in the 1950s and improved in the late 1970s.
    The original perceptron contained one neuron that could not solve the XOR function.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 原始感知器是在1950年代设计的，并在1970年代末期得到了改进。原始感知器包含一个神经元，无法解决 XOR 函数。
- en: An XOR function means that you have to choose an exclusive OR (XOR).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 函数意味着你必须选择一个排他性的“或” (XOR)。
- en: 'This can be difficult to grasp, as we''re not used to thinking about the way
    in which we use *or* in our everyday lives. In truth, we use *or* interchangeably
    as either inclusive or exclusive all of the time. Take this simple example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很难理解，因为我们不习惯以我们日常生活中使用 *或* 的方式来思考。事实上，我们总是交替使用 *或*，既有包容性也有排他性。举个简单的例子：
- en: If a friend were to come and visit me, I may ask them, "Would you like tea or
    coffee?" This is basically the offer of tea XOR coffee; I would not expect my
    friend to ask for both tea and coffee! My friend will choose one or the other.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个朋友来拜访我，我可能会问他们：“你想要茶还是咖啡？”这基本上是提供茶 XOR 咖啡；我不会期望我的朋友同时要求茶和咖啡！我的朋友会选择其中之一。
- en: I may follow up my question with, "Would you like milk or sugar?" In this case,
    I would not be surprised if my friend wanted both. This is an inclusive *or*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我可能会跟进我的问题，问：“你想要加牛奶还是加糖？”在这种情况下，如果我的朋友想要两者，我不会感到惊讶。这是一个包容性的 *或*。
- en: XOR, therefore, means "You can have one or the other, but not both."
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，XOR 意味着“你可以选择一个或另一个，但不能同时选择两者。”
- en: We will develop these concepts in the chapter through more examples.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章通过更多示例来展开这些概念。
- en: To solve this XOR function, we will build an FNN.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个 XOR 函数，我们将构建一个 FNN。
- en: Once the feedforward network for solving the XOR problem is built, it will be
    applied to an optimization example. The material optimizing example will choose
    the best combinations of dimensions among billions to minimize the use of corporate
    resources with the generalization of the XOR function.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了解决XOR问题的前馈网络，它将被应用于优化示例。该优化示例将选择数十亿个维度中的最佳组合，以最小化企业资源的使用，同时实现XOR函数的泛化。
- en: First, a solution to the XOR limitation of a perceptron must be clarified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，必须明确一个感知机XOR限制的解决方案。
- en: XOR and linearly separable models
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XOR和线性可分模型
- en: In the late 1960s, it was mathematically proven that a perceptron could *not*
    solve an XOR function. Fortunately, today, the perceptron and its neocognitron
    version form the core model for neural networking.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在1960年代末期，数学上证明了感知机*不能*解决XOR函数。幸运的是，今天感知机及其神经认知网络（neocognitron）版本形成了神经网络的核心模型。
- en: You may be tempted to think, *so what?* However, the entire field of neural
    networks relies on solving problems such as this to classify patterns. Without
    pattern classification, images, sounds, and words mean nothing to a machine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会忍不住想，*那又怎样？* 然而，整个神经网络领域依赖于解决类似这样的问题来进行模式分类。如果没有模式分类，图像、声音和文字对机器来说毫无意义。
- en: Linearly separable models
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性可分模型
- en: The McCulloch-Pitts 1943 neuron (see *Chapter 2*, *Building a Reward Matrix
    – Designing Your Datasets*) led to Rosenblatt's 1957-59 perceptron and the 1960
    Widrow-Hoff adaptive linear element (Adaline).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: McCulloch-Pitts 1943年的神经元（参见*第2章*，*建立奖励矩阵 - 设计你的数据集*）导致了Rosenblatt 1957-59年的感知机以及1960年Widrow-Hoff的自适应线性元件（Adaline）。
- en: These models are linear models based on an *f*(*x*, *w*) function that requires
    a line to separate results. A perceptron cannot achieve this goal and thus cannot
    classify many objects it faces.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型是基于*f*(*x*, *w*)函数的线性模型，需要一条线来分隔结果。一个感知机无法实现这一目标，因此无法对许多对象进行分类。
- en: 'A standard linear function can separate values. **Linear separability** can
    be represented in the following graph:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个标准的线性函数可以分隔数值。**线性可分性**可以通过下图表示：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_01-1.png](img/B15438_08_01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_01-1.png](img/B15438_08_01.png)'
- en: 'Figure 8.1: Linearly separable patterns'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：线性可分模式
- en: Imagine that the line separating the preceding dots and the part under it represent
    a picture that needs to be represented by a machine learning or deep learning
    application. The dots above the line represent *clouds* in the sky; the dots below
    the line represent *trees* on a hill. The line represents the slope of that hill.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，分隔前述点和其下部分的那条线代表一个需要通过机器学习或深度学习应用来表示的图像。线上方的点代表*天空中的云*；线下方的点代表*山坡上的树*。这条线代表山坡的斜率。
- en: To be linearly separable, a function must be able to separate the *clouds* from
    the *trees* to classify them. The prerequisite to classification is **separability**
    of some sort, linear or nonlinear.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现线性可分，一个函数必须能够将*云*从*树*中分隔出来进行分类。分类的前提是某种形式的**可分性**，无论是线性的还是非线性的。
- en: The XOR limit of a linear model, such as the original perceptron
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性模型的XOR限制，如原始感知机
- en: 'A linear model cannot solve the XOR problem expressed as follows in a table:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个线性模型无法解决以下表格表示的XOR问题：
- en: '| **Value of x1** | **Value of x2** | **Output** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **x1的值** | **x2的值** | **输出** |'
- en: '| 1 | 1 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: '| 0 | 0 | 0 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 |'
- en: '| 1 | 0 | 1 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 |'
- en: '| 0 | 1 | 1 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 |'
- en: 'Lines 3 and 4 show an exclusive OR (XOR). Imagine that you are offering a child
    a piece of cake OR a piece of candy (1 or 1):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第3行和第4行展示了排他性或（XOR）。想象一下，你给一个孩子提供一块蛋糕或者一块糖果（1或1）：
- en: '**Case 1**: The child answers: "I want candy or nothing at all!" (0 or 1).
    That''s exclusive OR (XOR)!'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例1**：孩子回答：“我想要糖果或者什么都不要！”（0或1）。这就是排他性或（XOR）！'
- en: '**Case 2**: The child answers: "I want a cake or nothing at all!" (1 or 0).
    That''s an exclusive OR (XOR) as well!'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例2**：孩子回答：“我想要蛋糕或者什么都不要！”（1或0）。这也是排他性或（XOR）！'
- en: 'The following graph shows the linear inseparability of the XOR function represented
    by one perceptron:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了由一个感知机表示的XOR函数的线性不可分性：
- en: '![](img/B15438_08_02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_08_02.png)'
- en: 'Figure 8.2: Linearly inseparable patterns'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：线性不可分模式
- en: The values of the table represent the Cartesian coordinates in this graph. The
    circles with a cross at (1, 1) and (0, 0) cannot be separated from the circles
    at (1, 0) and (0, 1). That's a huge problem. It means that Frank Rosenblatt's
    *f*(*x*, *w*) perceptron cannot separate, and thus can not classify, these dots
    into *clouds* and *trees*. Thus, in many cases, the perceptron cannot identify
    values that require linear separability.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的值表示该图中的笛卡尔坐标。位于（1, 1）和（0, 0）的交叉圆点，无法与位于（1, 0）和（0, 1）的圆点分开。这是一个巨大的问题。它意味着弗兰克·罗森布拉特的*f*（*x*，*w*）感知机无法分离这些点，因此无法将它们分类成*云*和*树*。因此，在许多情况下，感知机无法识别需要线性可分的值。
- en: Having invented the most powerful neural concept of the twentieth century—a
    neuron that can learn—Frank Rosenblatt had to bear with this limitation through
    the 1960s.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 弗兰克·罗森布拉特发明了二十世纪最强大的神经概念——一种可以学习的神经元——但他在1960年代必须忍受这个限制。
- en: As explained with the preceding cake-OR-candy example, the absence of an XOR
    function limits applications in which you must choose exclusively between two
    options. There are many "it's-either-that-or-nothing" situations in real-life
    applications. For a self-driving car, it could be either turn left or turn right,
    but don't swerve back and forth while making the decision!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面用蛋糕或糖果的例子所解释的，缺乏XOR函数限制了你必须在两个选项之间排他选择的应用场景。在现实生活中，有许多“只能是那个，不能是别的”的情境。对于一辆自动驾驶汽车来说，它要么左转，要么右转，但不能在做决策时来回摇摆！
- en: We will solve this limitation with a vintage solution, starting by building,
    and later implementing, an FNN.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一种复古的解决方案来解决这个限制，从构建开始，最终实现一个FNN。
- en: Building an FNN from scratch
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始构建一个前馈神经网络（FNN）
- en: Let's perform a mind experiment. Imagine we are in 1969\. We have today's knowledge
    but nothing to prove it. We know that a perceptron cannot implement the exclusive
    OR function XOR.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行一个思维实验。想象一下，我们处在1969年。我们拥有今天的知识，但没有任何证据来证明它。我们知道感知机无法实现异或（XOR）功能。
- en: We have an advantage because we now know a solution exists. To start our experiment,
    we only have a pad, a pencil, a sharpener, and an eraser waiting for us. We're
    ready to solve the XOR problem from scratch on paper before programming it. We
    have to find a way to classify those dots with a neural network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个优势，因为我们现在知道解决方案存在。为了开始我们的实验，我们只有一张便签、一支铅笔、一把削笔刀和一块橡皮擦等着我们。我们已经准备好从零开始在纸上解决XOR问题，然后再进行编程。我们必须找到一种方法，用神经网络对这些点进行分类。
- en: Step 1 – defining an FNN
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 定义一个前馈神经网络（FNN）
- en: We have to be unconventional to solve this problem. We must forget the complicated
    words and theories of the twenty-first century.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须采取非常规的方法来解决这个问题。我们必须忘记21世纪复杂的词汇和理论。
- en: 'We can write a neural network layer in high-school format. A hidden layer will
    be:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用高中格式写一个神经网络层。一个隐藏层将是：
- en: '*h*[1] = *x* * *w*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *x* * *w*'
- en: 'OK. Now we have one layer. A layer is merely a function. This function can
    be expressed as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。现在我们有了一个层。一个层只是一个函数。这个函数可以表示为：
- en: '*f*(*x*, *w*)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *w*)'
- en: In which *x* is the input value, and *w* is some value to multiply *x* by. Hidden
    means that the computation is not visible, just as *x* = 2 and *x* + 2 is the
    hidden layer that leads to 4.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是输入值，*w* 是乘以 *x* 的某个值。隐藏层意味着计算过程是不可见的，就像 *x* = 2 和 *x* + 2 是导致4的隐藏层一样。
- en: 'At this point, we have defined a neural network in three lines:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经用三行定义了一个神经网络：
- en: Input *x*.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入 *x*。
- en: Some function that changes its value, like 2 × 2 = 4, which transformed 2\.
    That is a layer. And if the result is superior to 2, for example, then great!
    The output is 1, meaning yes or true. Since we don't see the computation, this
    is the *hidden* layer.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些函数会改变它的值，比如 2 × 2 = 4，它转化了 2。这就是一个层。如果结果大于 2，比如说，那么太棒了！输出是 1，意味着“是”或“真”。由于我们看不到计算过程，这就是*隐藏*层。
- en: An output.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出。
- en: '*f*(*x*, *w*) is the building block of any neural network. "Feedforward" means
    that we will be going from layer 1 to layer 2, moving forward in a sequence.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*(*x*, *w*) 是任何神经网络的构建块。“前馈”意味着我们将从第1层走到第2层，按顺序向前移动。'
- en: Now that we know that basically any neural network is built with values transformed
    by an operation to become an output of something, we need some logic to solve
    the XOR problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道，基本上任何神经网络都是通过某个操作转化的值来生成某种输出的，我们需要一些逻辑来解决XOR问题。
- en: Step 2 – an example of how two children can solve the XOR problem every day
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 举一个两个孩子如何每天解决XOR问题的例子
- en: An example follows of how two children can solve the XOR problem using a straightforward
    everyday example. I strongly recommend this method. I have taken very complex
    problems, broken them down into small parts to a child's level, and often solved
    them in a few minutes. Then, you get the sarcastic answer from others such as
    "Is that all you did?" But, the sarcasm vanishes when the solution works over
    and over again in high-level corporate projects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是两个孩子如何通过一个简单的日常例子来解决 XOR 问题的示例。我强烈推荐这种方法。我将非常复杂的问题分解成小部分，使其适应孩子的理解水平，通常几分钟就能解决。然后，你会得到别人讽刺的回答：“这就是你做的全部吗？”但当这个解决方案在高层次的企业项目中反复奏效时，讽刺就消失了。
- en: 'First, let''s convert the XOR problem into a candy problem in a store. Two
    children go to the store and want to buy candy. However, they only have enough
    money to buy one pack of candy. They have to agree on a choice between two packs
    of different candy. Let''s say pack one is chocolate and the other is chewing
    gum. Then, during the discussion between these two children, 1 means yes, 0 means
    no. Their budget limits the options of these two children:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们把 XOR 问题转化为一个商店里的糖果问题。两个孩子去商店，想买糖果。但是他们只有足够的钱买一包糖果。他们必须在两包不同的糖果之间达成一致。假设一包是巧克力，另一包是口香糖。那么，在这两个孩子的讨论中，1表示是，0表示否。他们的预算限制了这两个孩子的选择：
- en: Going to the store and not buying any chocolate **or** chewing gum = (no, no)
    = (0, 0). That's not an option for these children! So the answer is false.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去商店但不买任何巧克力 **或** 口香糖 = (no, no) = (0, 0)。这不是这些孩子的选择！所以答案是错误的。
- en: Going to the store and buying both chocolate **and** chewing gum = (yes, yes)
    = (1, 1). That would be fantastic, but that's not possible. It's too expensive.
    So, the answer is, unfortunately, false.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去商店并且购买巧克力 **和** 口香糖 = (是，是) = (1, 1)。那将是非常棒的，但这不可能。太贵了。所以，答案是，不幸的是，错误的。
- en: Going to the store and either buying chocolate **or** chewing gum = (1, 0 or
    0, 1) = (yes or no) or (no or yes). That's possible. So, the answer is true.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去商店并且买巧克力 **或** 口香糖 = (1, 0 或 0, 1) = (是或否) 或 (否或是)。这是可能的。所以，答案是正确的。
- en: Imagine the two children. The eldest one is reasonable. The younger one doesn't
    really know how to count yet and wants to buy both packs of candy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这两个孩子。长子是理智的。较小的孩子还不太会算数，想买两包糖果。
- en: 'We express this on paper:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其写在纸上：
- en: '*x*[1] (eldest child''s decision, yes or no, 1 or 0) * *w*[1] (what the elder
    child thinks). The elder child is thinking this, or:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[1]（长子的决定，是或否，1或0） * *w*[1]（长子的想法）。长子是这样想的，或者：'
- en: '*x*[1] * *w*[1] or *h*[1] = *x*[1] * *w*[1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*x*[1] * *w*[1] 或 *h*[1] = *x*[1] * *w*[1]'
- en: The elder child weighs a decision like we all do every day, such as purchasing
    a car (*x* = 0 or 1) multiplied by the cost (*w*[1]).
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长子做出决定，就像我们每天做的那样，比如购买一辆车（*x* = 0 或 1）乘以成本（*w*[1]）。
- en: '*x*[2] (the younger child''s decision, yes or no, 1 or 0) * *w*[3] (what the
    younger child thinks). The younger child is also thinking this, or:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[2]（较小的孩子的决定，是或否，1或0） * *w*[3]（较小的孩子的想法）。较小的孩子也在思考这个，或者：'
- en: '*x*[2] * *w*[3] or *h*[2] = *x*[2] * *w*[3]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*x*[2] * *w*[3] 或 *h*[2] = *x*[2] * *w*[3]'
- en: '**Theory**: *x*[1] and *x*[2] are the inputs. *h*[1] and *h*[2] are neurons
    (the result of a calculation). Since *h*[1] and *h*[2] contain calculations that
    are not visible during the process, they are hidden neurons. *h*[1] and *h*[2]
    thus form a hidden layer. *w*[1] and *w*[3] are weights that represent how we
    "weigh" a decision, stating that something is more important than something else.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**理论**：*x*[1] 和 *x*[2] 是输入。*h*[1] 和 *h*[2] 是神经元（计算结果）。由于 *h*[1] 和 *h*[2] 包含在过程中的计算，且不可见，因此它们是隐藏神经元。*h*[1]
    和 *h*[2] 形成了一个隐藏层。*w*[1] 和 *w*[3] 是表示我们如何“衡量”决策的权重，表明某些事情比其他事情更重要。'
- en: Now imagine the two children talking to each other.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象这两个孩子在互相交谈。
- en: 'Hold it a minute! This means that now, each child is communicating with the
    other:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 等一下！这意味着现在，每个孩子都在和对方沟通：
- en: '*x*[1] (the elder child) says *w*[2] to the younger child. Thus, *w*[2] = this
    is what I think and am telling you:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[1]（长子）对较小的孩子说 *w*[2]。因此，*w*[2] = 这是我想的并告诉你的：'
- en: '*x*[1] * *w*[2]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*x*[1] * *w*[2]'
- en: '*x*[2] (the younger child) says, "please add my views to your decision," which
    is represented by *w*[4][:]'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[2]（较小的孩子）说：“请将我的观点加入到你的决定中，”这表示为 *w*[4][:]'
- en: '*x*[2] * *w*[4]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*x*[2] * *w*[4]'
- en: 'We now have the first two equations expressed in high-school-level code. It''s
    what one thinks plus what one says to the other, asking the other to take that
    into account:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了前两个方程式，以高中水平的代码表示。这就是一个人想法加上他说给另一个人听，请另一个人考虑这一点：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`h1` sums up what is going on in one child''s mind: personal opinion + the
    other child''s opinion.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`h1`总结了一个孩子的想法：个人意见 + 另一个孩子的意见。'
- en: '`h2` sums up what is going on in the other child''s mind and conversation:
    personal opinion + the other child''s opinion.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`h2`总结了另一个孩子的想法和对话：个人意见 + 另一个孩子的意见。'
- en: '**Theory**: The calculation now contains two input values and one hidden layer.
    Since, in the next step, we are going to apply calculations to `h1` and `h2`,
    we are in a feedforward neural network. We are moving from the input to another
    layer, which will lead us to another layer, and so on. This process of going from
    one layer to another is the basis of deep learning. The more layers you have,
    the deeper the network is. The reason `h1` and `h2` form a hidden layer is that
    their output is just the input of another layer.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**理论**：现在的计算包含两个输入值和一个隐藏层。因为在下一步中，我们将对`h1`和`h2`进行计算，所以我们处于一个前馈神经网络中。我们从输入层移动到另一个层，这将引导我们到下一个层，以此类推。从一个层到另一个层的这个过程是深度学习的基础。层数越多，网络就越深。`h1`和`h2`形成隐藏层的原因是它们的输出正是另一个层的输入。'
- en: 'For this example, we don''t need complicated numbers in an activation function
    such as logistic sigmoid, so we state whether the output values are less than
    1 or not:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们不需要复杂的激活函数中的数字，比如逻辑sigmoid函数，所以我们只是判断输出值是否小于1：
- en: if *h*[1] + *h*[2] >= 1 then *y*[1] = 1
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *h*[1] + *h*[2] >= 1，则 *y*[1] = 1
- en: if *h*[1] + *h*[2] < 1 then *y*[2] = 0
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *h*[1] + *h*[2] < 1，则 *y*[2] = 0
- en: '**Theory**: *y*[1] and *y*[2] form a second hidden layer. These variables can
    be scalars, vectors, or matrices. They are neurons.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**理论**：*y*[1]和*y*[2]构成第二个隐藏层。这些变量可以是标量、向量或矩阵。它们是神经元。'
- en: Now, a problem comes up. Who is right? The elder child or the younger child?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题来了。谁是对的？年长的孩子还是年幼的孩子？
- en: The only way seems to be to play around, with the weights *W* representing all
    the weights. Weights in a neural network work like weights in our everyday lives.
    We *weigh* decisions all the time. For example, there are two books to purchase,
    and we will "weigh" our decisions. If one is interesting and cheaper, it will
    weigh more or less in our decision, for example.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的办法似乎是进行尝试，其中权重*W*代表所有的权重。神经网络中的权重就像我们日常生活中的权重。我们总是在*权衡*决策。例如，有两本书可以购买，我们将“权衡”我们的决策。如果一本书有趣且便宜，它在我们的决策中所占的权重就会更多或更少。
- en: The children in our case agree on purchasing at least something, so from now
    on, *w*[3] = *w*[2], *w*[4] = *w*[1]. The younger and elder child will thus share
    some of the decision weights.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，孩子们同意至少购买一些东西，因此从现在开始，*w*[3] = *w*[2]，*w*[4] = *w*[1]。这样，年长的孩子和年幼的孩子将共享一些决策权重。
- en: Now, somebody has to be an influencer. Let's leave this hard task to the elder
    child. The elder child, being more reasonable, will continuously deliver the bad
    news. You have to subtract something from your choice, represented by a minus
    (–) sign.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有人得当“影响者”。让我们把这个困难的任务交给年长的孩子。年长的孩子更为理智，会不断地传达坏消息。你必须从你的选择中减去一些东西，用负号（–）表示。
- en: Each time they reach the point *h*[i], the eldest child applies a critical negative
    view on purchasing packs of candy. It's –*w* of everything comes up to be sure
    not to go over the budget. The opinion of the elder child is biased, so let's
    call the variable a bias, *b*[1]. Since the younger child's opinion is biased
    as well, let's call this view a bias too, *b*[2]. Since the eldest child's view
    is always negative, –*b*[1] will be applied to all of the eldest child's thoughts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每当他们到达 *h*[i] 时，年长的孩子就会对购买糖果包提出关键的负面看法。它是–*w*，以确保不会超过预算。年长孩子的看法有偏差，因此我们将变量称为偏差，*b*[1]。由于年幼孩子的观点也有偏差，我们也将其视为偏差，*b*[2]。由于年长孩子的观点始终是负面的，因此–*b*[1]将应用于年长孩子的所有想法。
- en: 'When we apply this decision process to their view, we obtain:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这个决策过程应用到他们的观点时，我们得到：
- en: '*h*[1] = *y*[1] * –*b*[1]'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *y*[1] * –*b*[1]'
- en: '*h*[2] = *y*[2] * *b*[2]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[2] = *y*[2] * *b*[2]'
- en: 'Then, we just have to use the same result. If the result is >=1, then the threshold
    has been reached. The threshold is calculated as shown in the following function:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们只需使用相同的结果。如果结果>=1，那么阈值就达到了。阈值的计算如下所示：
- en: '*y* = *h*[1] + *h*[2]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *h*[1] + *h*[2]'
- en: 'We will first start effectively finding the weights, starting by setting the
    weights and biases to 0.5, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先开始有效地寻找权重，从将权重和偏置设置为0.5开始，如下所示：
- en: '*w*[1] = 0.2; *w*[2] = 0.5; *b*[1] = 0.5'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[1] = 0.2; *w*[2] = 0.5; *b*[1] = 0.5'
- en: '*w*[3] = *w*[2]; *w*[4] = *w*[1]; *b*[2] = *b*[1]'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[3] = *w*[2]; *w*[4] = *w*[1]; *b*[2] = *b*[1]'
- en: It's not a full program yet, but its theory is done.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这还不是一个完整的程序，但其理论已经完成。
- en: Only the communication going on between the two children is making the difference;
    we will focus on only modifying *w*[2] and *b*[1] after a first try. It works
    on paper after a few tries.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 只有两个孩子之间的交流在起作用；我们将在第一次尝试后，专注于只修改 *w*[2] 和 *b*[1]。经过几次尝试后，这在纸面上是可行的。
- en: 'We now write the basic mathematical function, which is, in fact, the program
    itself on paper:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在写下基本的数学函数，事实上，这就是纸面上的程序本身：
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let's go from the solution on paper to Python.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从纸面上的解决方案转到Python代码。
- en: Why wasn't this deceivingly simple solution found in 1969? Because *it seems
    simple today but wasn't so at that time*, like all inventions found by our genius
    predecessors. Nothing is easy at all in artificial intelligence and mathematics.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么1969年没有找到这个看似简单的解决方案？因为*今天看起来简单，但当时并非如此*，就像所有我们天才前辈发明的东西一样。人工智能和数学中没有任何事情是容易的。
- en: In the next section, we'll stick with the solution proposed here, and implement
    it in Python.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将坚持这里提出的解决方案，并在Python中实现它。
- en: Implementing a vintage XOR solution in Python with an FNN and backpropagation
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用FNN和反向传播在Python中实现复古XOR解决方案
- en: To stay in the spirit of a 1969 vintage solution, we will not use NumPy, TensorFlow,
    Keras, or any other high-level library. Writing a vintage FNN with backpropagation
    written in high-school mathematics is fun.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持1969年复古解决方案的精神，我们将不会使用NumPy、TensorFlow、Keras或任何其他高级库。用高中数学编写复古FNN和反向传播是很有趣的。
- en: If you break a problem down into very elementary parts, you understand it better
    and provide a solution to that specific problem. You don't need to use a huge
    truck to transport a loaf of bread.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将一个问题分解成非常基础的部分，你会更好地理解它，并提供对这个特定问题的解决方案。你不需要用一辆大卡车来运输一条面包。
- en: Furthermore, by thinking through the minds of children, we went against running
    20,000 or more episodes in modern CPU-rich solutions to solve the XOR problem.
    The logic used proves that both inputs can have the same parameters as long as
    one bias is negative (the elder reasonable critical child) to make the system
    provide a reasonable answer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过从孩子们的思维方式出发，我们避免了在现代CPU丰富的解决方案中运行20,000次或更多的训练来解决XOR问题。所使用的逻辑证明了只要一个偏置是负的（年长的、理性的批判性孩子），两者的输入就可以拥有相同的参数，从而使系统给出合理的答案。
- en: 'The basic Python solution quickly reaches a result in a few iterations, approximately
    10 iterations (epochs or episodes), depending on how we think it through. An epoch
    can be related to a try. Imagine looking at somebody practicing basketball:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的Python解决方案在几次迭代后快速得出结果，大约10次迭代（时期或回合），具体取决于我们如何思考它。一周期可以与一次尝试相关联。想象一下，看着某人在练习篮球：
- en: The person throws the ball toward the hoop but misses. That was an epoch (an
    episode can be used as well).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个人朝着篮筐投掷篮球，但投偏了。那是一个时代（也可以用一集来描述）。
- en: The person thinks about what happened and changes the way the ball will be thrown.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个人思考发生了什么，并改变了投篮的方式。
- en: This improvement is what makes it a learning epoch (or episode). It is not a
    simple memoryless try. Something is really happening to improve performance.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种改进就是使其成为学习周期（或回合）的原因。这不是一次简单的无记忆尝试。确实发生了一些事情，提升了性能。
- en: The person throws the ball again (next epoch) and again (next epochs) until
    the overall performance has improved. This is how a neural network improves over
    epochs.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个人再次投篮（下一个周期），又一次（下一个周期）直到整体表现有所提升。这就是神经网络如何在周期中改进的方式。
- en: '`FNN_XOR_vintage_tribute.py` contains (at the top of the code) a result matrix
    with four columns.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`FNN_XOR_vintage_tribute.py`（在代码的顶部）包含一个四列的结果矩阵。'
- en: 'Each element of the matrix represents the status (`1` = correct, `0` = false)
    of the four predicates to solve:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的每个元素表示四个谓词的状态（`1` = 正确，`0` = 错误）：
- en: '[PRE2]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `train` variable is the number of predicates to solve: (0, 0), (1, 1),
    (1, 0), (0, 1). The variable of the predicate to solve is `pred`.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 变量是要解决的谓词数量：(0, 0)，(1, 1)，(1, 0)，(0, 1)。要解决的谓词变量是 `pred`。'
- en: 'The core of the program is practically a copy of the sheet of paper we wrote,
    as in the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的核心实际上是我们所写的纸面内容的一个副本，如以下代码所示：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`pred` is an argument of the function from `1` to `4`. The four predicates
    are represented in the following table:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`pred`是一个函数的参数，范围是从`1`到`4`。这四个谓词在下表中表示：'
- en: '| **Predicate (pred)** | **x[1]** | **x[2]** | **Expected result** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **谓词 (pred)** | **x[1]** | **x[2]** | **期望结果** |'
- en: '| 0 | 1 | 1 | 0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 | 0 |'
- en: '| 1 | 0 | 0 | 0 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 |'
- en: '| 2 | 1 | 0 | 1 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | 1 |'
- en: '| 3 | 0 | 1 | 1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 1 | 1 |'
- en: That is why *y* must be <1 for predicates 0 and 1\. Then, *y* must be >=1 for
    predicates 2 and 3.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么对于谓词0和1，*y*必须小于1。而对于谓词2和3，*y*必须大于等于1。
- en: 'Now, we have to call the following function limiting the training to 50 epochs,
    which are more than enough:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要调用以下函数，将训练限制为50个周期，这已经足够：
- en: '[PRE4]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At the first epoch, the weights and biases are all set to `0.5`. No use thinking!
    Let the program do the job. As explained previously, the weight and bias of `x2`
    are equal.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，所有的权重和偏差都设置为`0.5`。别想太多！让程序来做这件事。正如前面所解释的，`x2`的权重和偏差是相等的。
- en: 'Now, the hidden layers and `y` calculation function are called four times,
    one for each predicate to train, as shown in the following code snippet:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，隐藏层和`y`计算函数被调用四次，每次针对一个谓词进行训练，如下代码片段所示：
- en: '[PRE5]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, the system must train. To do that, we need to measure the number of predictions,
    1 to 4, that are correct at each iteration and decide how to change the weights/biases
    until we obtain proper results. We'll do that in the following section.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，系统必须进行训练。为此，我们需要在每次迭代中衡量1到4的预测结果中有多少是正确的，并决定如何调整权重/偏差，直到获得合适的结果。我们将在接下来的部分中完成这项工作。
- en: A simplified version of a cost function and gradient descent
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简化版的代价函数和梯度下降
- en: 'Slightly more complex gradient descent will be described in the next chapter.
    In this chapter, only a one-line equation will do the job. The only thing to bear
    in mind as an unconventional thinker is: *so what?* The concept of gradient descent
    is minimizing loss or errors between the present result and a goal to attain.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微复杂一些的梯度下降将在下一章中介绍。在本章中，只有一行方程就能完成工作。作为一名非传统的思考者，你需要记住的唯一一点是：*那又怎样？* 梯度下降的概念是最小化当前结果与目标之间的损失或误差。
- en: First, a cost function is needed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要一个代价函数。
- en: There are four predicates (0-0, 1-1, 1-0, 0-1) to train correctly. We need to
    find out how many are correctly trained at each epoch.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个谓词（0-0，1-1，1-0，0-1）需要正确训练。我们需要找出在每个训练周期中有多少是正确训练的。
- en: The cost function will measure the difference between the training goal (4)
    and the result of this epoch or training iteration (result).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 代价函数将衡量训练目标（4）与本次迭代或训练周期（result）的结果之间的差异。
- en: When 0 convergence is reached, it means the training has succeeded.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当收敛为0时，意味着训练已经成功。
- en: '`result[0,0,0,0]` contains a `0` for each value if none of the four predicates
    have been trained correctly. `result[1,0,1,0]` means two out of the four predicates
    are correct. `result[1,1,1,1]` means that all four predicates have been trained
    and that the training can stop. `1`, in this case, means that the correct training
    result was obtained. It can be `0` or `1`. The `result` array is the result counter.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`result[0,0,0,0]`表示如果四个谓词都没有被正确训练，每个值都会是`0`。`result[1,0,1,0]`表示四个谓词中有两个是正确的。`result[1,1,1,1]`表示四个谓词都已经训练完成，训练可以停止。在这种情况下，`1`表示正确的训练结果已获得，它可以是`0`或`1`。`result`数组是结果计数器。'
- en: The cost function will express this training by having a value of `4`, `3`,
    `2`, `1`, or `0` as the training goes down the slope to 0.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代价函数将通过将训练值降到`4`、`3`、`2`、`1`或`0`来表示这次训练，直到降到0。
- en: 'Gradient descent measures the value of the descent to find the direction of
    the slope: up, down, or 0\. Then, once you have that slope and the steepness of
    it, you can optimize the weights. A derivative is a way to know whether you are
    going up or down a slope.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降测量下降的值，以找到坡度的方向：上坡、下坡或平坦。然后，一旦你得到了坡度及其陡峭度，你就可以优化权重。导数是了解你是否在上坡或下坡的一种方法。
- en: Each time we move up or down the slope, we check to see whether we are moving
    in the right direction. We will assume that we will go one step at a time. So
    if we change directions, we will change our pace by one step. That one step value
    is our **learning rate**. We will measure our progression at each step. However,
    if we feel comfortable with our results, we might walk 10 steps at a time and
    only check to see if we are on the right track every 10 steps. Our learning rate
    will thus have increased to 10 steps.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们沿着坡道上升或下降时，我们都会检查是否朝着正确的方向前进。我们假设我们会一步一步地走。所以，如果我们改变方向，我们会以一步的速度改变步伐。这个一步的值就是我们的**学习率**。我们将在每一步衡量我们的进展。然而，如果我们对结果感到满意，我们可能会一次走10步，只有每走10步才检查是否走在正确的轨道上。这样我们的学习率就增加到了10步。
- en: 'In this case, we hijacked the concept and used it to set the learning rate
    to `0.05` with a one-line function. Why not? It helped to solve gradient descent
    optimization in one line:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们劫持了这个概念，并用它来设置学习率为`0.05`，只用了一个函数行。为什么不呢？它帮助我们用一行代码解决了梯度下降优化问题：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By applying the vintage children-buying-candy logic to the whole XOR problem,
    we found that only `w2` needed to be optimized. That's why `b1=w2`. That's because
    `b1` is doing the tough job of saying something negative (`-`) all the time, which
    completely changes the course of the resulting outputs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将复古的儿童买糖果逻辑应用于整个XOR问题，我们发现只有`w2`需要优化。这就是为什么`b1=w2`。因为`b1`在做一个艰难的工作——一直输出负值（`-`），这完全改变了最终输出的结果。
- en: 'The rate is set at `0.05`, and the program finishes training in 10 epochs:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率设置为`0.05`，程序在10个周期内完成了训练：
- en: '[PRE7]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a logical *yes* or *no* problem. The way the network is built is pure
    logic. Nothing can stop us from using whatever training rates we wish. In fact,
    that's what gradient descent is about. There are many gradient descent methods.
    If you invent your own and it works for your solution, that is fine.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个逻辑性的*是*或*否*问题。网络的构建方式纯粹是逻辑性的。没有什么能阻止我们使用任何我们想要的训练率。实际上，这就是梯度下降的核心。有很多梯度下降方法。如果你自己发明了一种方法，并且它适用于你的解决方案，那也是可以的。
- en: 'This one-line code is enough, in this case, to see whether the slope is going
    down. As long as the slope is negative, the function is going downhill to *cost*
    = 0:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这一行代码足够判断坡度是否正在下降。只要坡度为负值，函数就会朝着*成本* = 0的方向下行：
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following diagram sums up the whole process:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了整个过程：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_03.png](img/B15438_08_03.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_03.png](img/B15438_08_03.png)'
- en: 'Figure 8.3: A feedforward neural network model (FNN)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：前馈神经网络模型（FNN）
- en: We can see that all of the arrows of the layers go forward in this "feedforward"
    neural network. However, the arrow that stems from the *y* node and goes backward
    can seem confusing. This line represents a change in weights to train the model.
    This means that we go back to changing the weights and running the network for
    another epoch (or episode). The system is adjusting its weights epoch by epoch
    until the overall result is correct.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在这个“前馈”神经网络中，所有层的箭头都是朝前的。然而，从*y*节点向后延伸的箭头可能会让人感到困惑。这个箭头代表了训练模型时权重的变化。这意味着我们会回到更改权重并再次运行网络进行另一个周期（或回合）。系统会在每个周期中调整其权重，直到整体结果正确为止。
- en: Too simple? Well, it works, and that's all that counts in real-life development.
    If your code is bug-free and does the job, then that's what matters.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 太简单了吗？嗯，这个方法有效，这就是现实开发中最重要的。只要你的代码没有错误，并且完成了任务，那才是最关键的。
- en: Finding a simple development tool means nothing more than that. It's just another
    tool in the toolbox. We can get this XOR function to work on a neural network
    and generate income.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找一个简单的开发工具无非就是如此。它只是工具箱中的另一个工具。我们可以让这个XOR函数在神经网络中运行并产生收益。
- en: Companies are not interested in how smart you are but how efficient (profitable)
    you can be.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 公司关心的不是你有多聪明，而是你能多高效（有利可图）。
- en: 'A company''s survival relies on multiple constraints: delivering on time, offering
    good prices, providing a product with a reasonable quality level, and many more
    factors besides.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一家公司的生存依赖于多种约束条件：按时交付、提供合理价格、提供合理质量水平的产品等等。
- en: When we come up with a solution, it is useless to show how smart we can be writing
    tons of code. Our company or customers expect an efficient solution that will
    run well and is easy to maintain. In short, focus on efficiency. Once we have
    a good solution, we need to show that it works. In this case, we proved that linear
    separability was achieved.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提出解决方案时，展示我们写了大量代码有多聪明是没有意义的。我们的公司或客户期望一个高效的解决方案，这个解决方案运行良好且易于维护。简而言之，要关注效率。一旦我们有了一个好的解决方案，我们需要证明它是有效的。在这种情况下，我们证明了线性可分性已经实现。
- en: Linear separability was achieved
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现了线性可分性
- en: Bear in mind that the whole purpose of this feedforward network with backpropagation
    through a cost function was to transform a linear non-separable function into
    a linearly separable function to implement the classification of features presented
    to the system. In this case, the features had a `0` or `1` value.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个带有反向传播和代价函数的前馈网络的全部目的是将一个线性不可分的函数转化为一个线性可分的函数，以实现对系统呈现的特征的分类。在这种情况下，这些特征的值为`0`或`1`。
- en: One of the core goals of a layer in a neural network is to make the input make
    sense, meaning to be able to separate one kind of information from another.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中每一层的核心目标之一是使输入变得有意义，即能够将一种信息与另一种信息区分开来。
- en: '`h1` and `h2` will produce the Cartesian coordinate linear separability training
    axis, as implemented in the following code:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`h1`和`h2`将生成笛卡尔坐标线性可分训练轴，如以下代码实现所示：'
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Running the program provides a view of the nonlinear input values once the
    hidden layers have trained them. The nonlinear values then become linear values
    in a linearly separable function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 运行程序时，隐藏层训练后的非线性输入值将显示出来。然后，非线性值变成线性值，成为一个线性可分的函数：
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The intermediate result and goal are not a bunch of numbers on a screen to
    show that the program works. The result is a set of Cartesian values that can
    be represented in the following linearly separated graph:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 中间结果和目标不是一堆显示程序运行是否正常的数字。结果是一组可以通过以下线性分隔图表示的笛卡尔坐标值：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_04-1.png](img/B15438_08_04.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_04_04-1.png](img/B15438_08_04.png)'
- en: 'Figure 8.4: Linearly separable patterns'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：线性可分模式
- en: We have now obtained a separation between the top values, representing the intermediate
    values of the (1, 0) and (0, 1) inputs, and the bottom values, representing the
    (1, 1) and (0, 0) inputs. The top values are separated from the bottom values
    by a clear line. We now have *clouds* on top and *trees* below the line that separates
    them.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在顶部值之间获得了区分，这些值代表了（1，0）和（0，1）输入的中间值，而底部值则代表（1，1）和（0，0）输入。顶部值与底部值之间通过一条清晰的线分开。我们现在在这条分隔线的顶部有*云*，在底部有*树*。
- en: 'The layers of the neural network have transformed nonlinear values into linearly
    separable values, making classification possible through standard separation equations,
    such as the one in the following code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的各层将非线性值转换为线性可分的值，从而使得通过标准分隔方程（如以下代码所示）进行分类成为可能：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability of a neural network to make non-separable information separable
    and classifiable represents one of the core powers of deep learning. From this
    technique, many operations can be performed on data, such as subset optimization.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络将不可分的信息转化为可分且可分类的信息，代表了深度学习的一项核心能力。基于这一技术，可以对数据执行许多操作，例如子集优化。
- en: In the next section, we'll look at a practical application for our FNN XOR solution.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨FNN XOR解决方案的实际应用。
- en: Applying the FNN XOR function to optimizing subsets of data
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将FNN XOR函数应用于优化数据子集
- en: There are more than 7.5 billion people breathing air on this planet. In 2050,
    there might be 2.5 billion more of us. All of these people need to wear clothes
    and eat. Just those two activities involve classifying data into subsets for industrial
    purposes.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 目前地球上有超过75亿人呼吸着空气。到2050年，可能会有25亿人加入我们的行列。所有这些人都需要穿衣和吃饭。这两项活动本身就涉及到将数据分类为工业用途的子集。
- en: '**Grouping** is a core concept for any production. Production relating to producing
    clothes and food requires grouping to optimize production costs. Imagine not grouping
    and delivering one T-shirt at a time from one continent to another instead of
    grouping T-shirts in a container and grouping many containers (not just two on
    a ship). Let''s focus on clothing, for example.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**分组**是任何生产过程中的核心概念。涉及服装和食品的生产需要分组来优化生产成本。想象一下，如果不进行分组，而是将一件T恤从一个大陆运送到另一个大陆，而不是将T恤按批次装进集装箱，并且将多个集装箱（而不仅仅是两个集装箱在一艘船上）一起运输。以服装为例，我们来进行分析。'
- en: A chain of stores needs to replenish the stock of clothing in each store as
    the customers purchase their products. In this case, the corporation has 10,000
    stores. The brand produces jeans, for example. Their average product is a faded
    jean. This product sells a slow 50 units a month per store. That adds up to 10,000
    stores × 50 units = 500,000 units or stock-keeping units (SKUs) per month. These
    units are sold in all sizes, grouped into average, small, and large. The sizes
    sold per month are random.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一家连锁商店需要根据顾客购买产品的情况补充每家店的库存。在这种情况下，该公司拥有10,000家店。品牌例如生产牛仔裤。它们的平均产品是褪色牛仔裤。这款产品每月在每家店销售50件，销售速度较慢。总共是10,000家店
    × 50件 = 500,000件，即每月500,000个库存单位（SKU）。这些单位的销售涵盖了所有尺码，分为平均码、小码和大码。每月销售的尺码是随机的。
- en: 'The main factory for this product has about 2,500 employees producing those
    jeans at an output of about 25,000 jeans per day. The employees work in the following
    main fields: cutting, assembling, washing, lasering, packaging, and warehousing.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该产品的主要工厂大约有2,500名员工，每天生产约25,000条牛仔裤。员工主要在以下领域工作：剪裁、组装、洗涤、激光处理、包装和仓储。
- en: The first difficulty arises with the purchase and use of fabric. The fabric
    for this brand is not cheap. Large amounts are necessary. Each pattern (the form
    of pieces of the pants to be assembled) needs to be cut by wasting as little fabric
    as possible.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个难题出现在面料的购买和使用上。该品牌的面料并不便宜，需要大量采购。每种图案（即组装裤子的各个部件）都需要尽可能减少面料的浪费进行剪裁。
- en: Imagine you have an empty box you want to fill up to optimize the volume. If
    you only put soccer balls in it, there will be a lot of space. If you slip tennis
    balls in the empty spaces, space will decrease. If, on top of that, you fill the
    remaining empty spaces with ping pong balls, you will have optimized the available
    space in the box.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你有一个空箱子，想要通过优化体积来填充它。如果只放足球，箱子里会有很多空隙。如果你把网球塞进空隙里，空间会减少。如果再把剩余的空隙用乒乓球填满，你就优化了箱子里的空间。
- en: Building optimized subsets can be applied to containers, warehouse flows and
    storage, truckload optimizing, and almost all human activities.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 构建优化的子集可以应用于集装箱、仓库流动和存储、货车装载优化以及几乎所有人类活动。
- en: In the apparel business, if 1% to 10% of the fabric is wasted while manufacturing
    jeans, the company will survive the competition. At over 10%, there is a real
    problem to solve. Losing 20% of all the fabric consumed in manufacturing jeans
    can bring the company down and force it into bankruptcy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在服装行业中，如果制造牛仔裤时有1%到10%的面料浪费，企业仍能在竞争中生存。如果超过10%，就需要解决实际问题。损失20%的面料可能会导致公司倒闭，迫使其破产。
- en: The main rule is to combine larger pieces and smaller pieces to make optimized
    cutting patterns.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 主要规则是将大件和小件结合起来，以制作优化的剪裁图案。
- en: Optimization of space through larger and smaller objects can be applied to cutting
    the forms, which are the patterns of the jeans, for example. Once they are cut,
    they will be assembled at the sewing stations.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通过大件和小件优化空间的使用，可以应用于剪裁形式（例如牛仔裤的图案）。一旦剪裁完成，它们将被送往缝纫站进行组装。
- en: 'The problem can be summed up as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可以总结为：
- en: Creating subsets of the 500,000 SKUs to optimize the cutting process for the
    month to come in a given factory
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建500,000个SKU的子集，以优化接下来一个月在特定工厂的剪裁过程。
- en: Making sure that each subset contains smaller sizes and larger sizes to minimize
    the loss of fabric by choosing 6 sizes per day to build 25,000 unit subsets per
    day
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保每个子集包含小号和大号，以通过每天选择6个尺码来构建25,000个单位的子集，从而最小化面料的浪费。
- en: Generating cut plans of an average of 3 to 6 sizes per subset per day for a
    production of 25,000 units per day
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成平均3到6个尺码的剪裁计划，每个子集每天生产25,000件产品。
- en: In mathematical terms, this means trying to find subsets of sizes among 500,000
    units for a given day.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，这意味着要在50万个单位中寻找特定日期的尺码子集。
- en: 'The task is to find 6 well-matched sizes among 500,000 units, as shown in the
    following combination formula:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是从50万个单元中找到6个匹配的尺码，如下所示的组合公式：
- en: '![](img/B15438_08_001.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_08_001.png)'
- en: At this point, most people abandon the idea and find some easy way out of this,
    even if it means wasting fabric.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，大多数人都会放弃这个想法，找到一些简单的解决办法，即使这意味着浪费面料。
- en: The first reaction we all have is that this is more than the number of stars
    in the universe and all that hype. However, that's not the right way to look at
    it at all. The right way is to look exactly in the opposite direction.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有人的第一反应是，这比宇宙中的星星还多，所有的炒作也是如此。然而，这根本不是正确的看法。正确的看法是要从完全相反的方向来考虑。
- en: The key to this problem is to observe the particle at a microscopic level, at
    the **bits of information** level. Analyzing detailed data is necessary to obtain
    reliable results. This is a fundamental concept of machine learning and deep learning.
    Translated into our field, it means that to process an image, ML and DL process
    pixels.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的关键是从微观层面观察粒子，即**信息位**层面。分析详细数据是获得可靠结果的必要条件。这是机器学习和深度学习的基本概念。翻译到我们的领域，它意味着要处理一张图片，ML和DL处理的是像素。
- en: 'So, even if the pictures to process represent large quantities, it will come
    down to small units of information to analyze:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使要处理的图片代表了大量数据，最终也会降至对小单位信息的分析：
- en: '| yottabyte (YB) | 10^(24) | yobibyte (YiB) | 2^(80) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| yottabyte (YB) | 10^(24) | yobibyte (YiB) | 2^(80) |'
- en: It might be surprising to see these large numbers appear suddenly! However,
    when trying to combine thousands of elements, the combinations become exponential.
    When you extend this to the large population that major apparel brands have to
    deal with, it becomes rapidly exponential as well.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这些大数字突然出现可能会让人感到惊讶！然而，当试图将成千上万的元素组合在一起时，组合的方式变得是指数级增长。当你将其扩展到大型服装品牌所需处理的大量人口时，它也会迅速呈指数增长。
- en: Today, Google, Facebook, Amazon, and others have yottabytes of data to classify
    and make sense of. Using the term **big data** doesn't mean much. It's just a
    lot of data, and so what?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，谷歌、Facebook、亚马逊等公司拥有yottabyte级的数据需要分类和理解。使用**大数据**这一术语其实并没有太大意义。它只是大量的数据，那又怎样呢？
- en: You do not need to analyze the individual positions of each data point in a
    dataset but use the probability distribution.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要分析数据集中的每个数据点的具体位置，而是要使用概率分布。
- en: To understand that, let's go to a store to buy some jeans for a family. One
    of the parents wants a pair of jeans, and so does a teenager in that family. They
    both go and try to find their size in the pair of jeans they want. The parent
    finds 10 pairs of jeans in size *x*. All of the jeans are part of the production
    plan. The parent picks one at *random*, and the teenager does the same. Then they
    pay for them and take them home.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，让我们去商店为一家人买些牛仔裤。家里的一个父母需要一条牛仔裤，家里的一个青少年也需要一条。两人都去寻找他们想要的牛仔裤尺码。父母找到10条尺码为*x*的牛仔裤。所有牛仔裤都是生产计划的一部分。父母随机挑选一条，青少年也做同样的事。然后他们支付并带着牛仔裤回家。
- en: 'Some systems work fine with random choices: random transportation (taking jeans
    from the store to home) of particles (jeans, other product units, pixels, or whatever
    is to be processed), making up that fluid (a dataset).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统在随机选择时运作良好：粒子的随机运输（将牛仔裤从商店带回家）（牛仔裤、其他产品单元、像素或任何需要处理的物品），形成流体（一个数据集）。
- en: Translated into our factory, this means that a stochastic (random) process can
    be introduced to solve the problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译到我们的工厂，这意味着可以引入一个随机过程来解决这个问题。
- en: 'All that was required is that small and large sizes were picked at random among
    the 500,000 units to produce. If 6 sizes from 1 to 6 were to be picked per day,
    the sizes could be classified as follows in a table:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的只是从50万个单元中随机选取大号和小号尺码进行生产。如果每天选取从1到6的6个尺码，那么可以按如下方式在表格中对这些尺码进行分类：
- en: '*Smaller sizes* = *S* = {1, 2, 3}'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*小号尺码* = *S* = {1, 2, 3}'
- en: '*Larger sizes* = *L* = {4, 5, 6}'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*大号尺码* = *L* = {4, 5, 6}'
- en: 'Converting this into numerical subset names, *S* = 1 and *L* = 6\. By selecting
    large and small sizes to produce at the same time, the fabric will be optimized,
    as shown in the following table:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 将其转换为数字子集名称，*S* = 1 和 *L* = 6。通过同时选择大号和小号尺码进行生产，面料将得到优化，如下表所示：
- en: '| **Size of choice 1** | **Size of choice 2** | **Output** |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| **选择 1 的大小** | **选择 2 的大小** | **输出** |'
- en: '| 6 | 6 | 0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6 | 0 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: '| 1 | 6 | 1 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 1 |'
- en: '| 6 | 1 | 1 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | 1 |'
- en: You will notice that the first two lines contain the same value. This will not
    optimize fabric consumption. If you put only large size 6 products together, there
    will be "holes" in the pattern. If you only put small size 1 products together,
    then they will fill up all of the space and leave no room for larger products.
    Fabric cutting is optimal when large and small sizes are present on the same roll
    of fabric.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到前两行包含相同的值。这不会优化面料的消耗。如果你只将大号 6 尺寸的产品放在一起，图案中会有“空洞”。如果只将小号 1 尺寸的产品放在一起，它们会填满所有空间，根本没有空位给较大的产品。如果大尺寸和小尺寸的产品能同时出现在同一卷面料上，裁剪就最为优化。
- en: Doesn't this sound familiar? It looks exactly like our vintage FNN, with 1 instead
    of 0 and 6 instead of 1.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来是不是很熟悉？它看起来和我们的经典 FNN 完全一样，只是 1 替代了 0，6 替代了 1。
- en: All that has to be done is to stipulate that subset *S* = *value* 0, and subset
    *L* = *value* 1; and the previous code can be generalized.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所要做的就是规定子集 *S* = *value* 0，子集 *L* = *value* 1；然后之前的代码可以进行泛化。
- en: '`FFN_XOR_generalization.py` is the program that generalizes the previous code,
    as shown in the following snippet.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`FFN_XOR_generalization.py` 是用于泛化之前代码的程序，如下所示的代码片段。'
- en: 'If this works, then smaller and larger sizes will be chosen to send to the
    cut planning department, and the fabric will be optimized. Applying the randomness
    concept of Bellman''s equation, a stochastic process is applied, choosing customer
    unit orders at random (each order is one size and a unit quantity of 1):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这有效，那么较小和较大的尺寸将被选择并发送到裁剪计划部门，面料将得到优化。通过应用贝尔曼方程的随机性概念，采用随机过程，随机选择客户的单位订单（每个订单是一个尺寸，数量为
    1）：
- en: '[PRE12]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The weights and bias are now constants obtained by the result of the XOR training
    FNN. The training is over; the FNN is now used to provide results. Bear in mind
    that the word *learning* in machine learning and deep learning doesn't mean you
    have to train systems forever. In stable environments, training is run only when
    the datasets change. At one point in a project, you are hopefully using deep *trained*
    systems and not simply exploring the training phase of a deep *learning* process.
    The goal is not to spend all corporate resources on learning but on using trained
    models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，权重和偏差是通过 XOR 训练 FNN 得到的常数。训练已经结束；FNN 现在用于提供结果。请记住，机器学习和深度学习中的“学习”一词并不意味着你必须永远训练系统。在稳定的环境中，只有在数据集发生变化时才需要运行训练。在项目的某个阶段，你希望使用的是经过深度*训练*的系统，而不是仅仅在探索深度*学习*过程的训练阶段。目标不是将所有企业资源都投入到学习中，而是将资源用于使用已经训练好的模型。
- en: Deep learning architecture must rapidly become deep trained models to produce
    a profit.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构必须迅速转变为深度训练的模型，以产生利润。
- en: 'For this prototype validation, the size of a given order is random. `0` means
    the order fits in the *S* subset; `1` means the order fits in the *L* subset.
    The data generation function reflects the random nature of consumer behavior in
    the following six-size jeans consumption model:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个原型验证，给定订单的尺寸是随机的。`0` 表示该订单符合 *S* 子集；`1` 表示该订单符合 *L* 子集。数据生成函数反映了消费者行为在以下六种尺寸牛仔裤消费模型中的随机性：
- en: '[PRE13]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once two customer orders have been chosen at random in the correct size category,
    the FNN is activated and runs like the previous example. Only the `result` array
    has been changed since we are using the same core program. Only a yes (`1`) or
    no (`0`) is expected, as shown in the following code:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在正确的尺寸类别中随机选择了两个客户订单，FNN 就会激活并像之前的示例一样运行。只改变了 `result` 数组，因为我们使用的是相同的核心程序。只需期望返回一个“是”（`1`）或“否”（`0`），如下所示的代码：
- en: '[PRE14]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The number of subsets to produce needs to be calculated to determine the volume
    of positive results required.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 需要计算出要生产的子集数量，以确定所需的正面结果的数量。
- en: The choice is made of 6 sizes among 500,000 units. But, the request is to produce
    a daily production plan for the factory. The daily production target is 25,000\.
    Also, each subset can be used about 20 times. There is always, on average, 20
    times the same size in a given pair of jeans available.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 选择是从 500,000 个单位中挑选出 6 种尺寸。然而，要求是为工厂制定每日生产计划。每日生产目标是 25,000。此外，每个子集可以使用约 20
    次。平均而言，在一条牛仔裤中，每种尺寸总会有大约 20 次相同的尺寸。
- en: 'Six sizes are required to obtain good fabric optimization. This means that
    after three choices, the result represents one subset of potential optimized choices:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得良好的面料优化，需要六种尺寸。这意味着，在做出三次选择后，结果代表了潜在优化选择的一个子集：
- en: '*R* = 120 × 3 subsets of two sizes = 360'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '*R* = 120 × 3 个两尺寸的子集 = 360'
- en: The magic number has been found. For every 3 choices, the goal of producing
    6 sizes multiplied by a repetition of 20 will be reached.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 找到了魔数。对于每3个选择，通过20次重复生成6个尺寸的目标将会达成。
- en: 'The production-per-day request is 25,000:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 每天的生产请求为25,000：
- en: The number of subsets requested = 25000/3=8333\. 333
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请求的子集数量 = 25000/3 = 8333\. 333
- en: 'The system can run 8,333 products as long as necessary to produce the volume
    of subsets requested. In this case, the range is set to a sample of 1,000,000
    products. It can be extended or reduced when needed. The system is filtering the
    correct subsets through the following function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 系统可以运行8,333个产品，直到生成所需子集的数量。在这种情况下，范围设置为1,000,000个产品的样本。根据需要可以扩展或缩减。系统通过以下函数筛选正确的子集：
- en: '[PRE15]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When the 8,333 subsets have been found respecting the smaller-larger size distribution,
    the system stops, as shown in the following output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当找到了符合较小-较大尺寸分布的8,333个子集时，系统停止运行，如下输出所示：
- en: '[PRE16]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This example proves the point. *Simple solutions can solve very complex problems.*
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例证明了这一点。*简单的解决方案可以解决非常复杂的问题。*
- en: 'Two main functions, among some minor ones, must be added:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 必须添加两个主要功能，以及一些次要功能：
- en: After each choice, the orders chosen must be removed from the 500,000-order
    dataset. When an order has been selected, processing it again will generate errors
    in the global results. This will preclude choosing the same order twice and reduce
    the number of choices to be made.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次选择后，必须从500,000订单数据集中删除选择的订单。当选择了一个订单后，再次处理它将会在全局结果中生成错误。这将排除两次选择相同订单的可能性，并减少要做出的选择次数。
- en: An optimization function to regroup the results for production purposes, for
    example. The idea is not to run through the records randomly, but to organize
    them by sets. This way, each set can be controlled independently.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，优化函数用于重新组织生产目的的结果。其思想不是随机遍历记录，而是按组织成集合进行控制。这样每个集合可以独立控制。
- en: 'Application information:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 应用信息：
- en: The core calculation part of the application is fewer than 50 lines long.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序的核心计算部分不超过50行。
- en: With a few control functions and arrays, the program might reach 200 lines maximum.
    The goal of the control functions is to check and see whether the results reach
    the overall goal. For example, every 1,000 records, a local result could be checked
    to see whether it fits the overall goal.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过少数控制功能和数组，程序最多可能达到200行。控制功能的目标是检查结果是否达到总体目标。例如，每1,000条记录，可以检查本地结果是否符合总体目标。
- en: This results in easy maintenance for a team.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这样可以轻松维护团队。
- en: Optimizing the number of lines of code to create a powerful application can
    prove to be very efficient for many business problems.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 优化代码行数以创建强大的应用程序，对于许多业务问题来说，这可能非常高效。
- en: Summary
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Building a small neural network from scratch provides a practical view of the
    elementary properties of a neuron. We saw that a neuron requires an input that
    can contain many variables. Then, weights are applied to the values with biases.
    An activation function then transforms the result and produces an output.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建一个小型神经网络提供了神经元基本属性的实际视图。我们看到神经元需要一个可能包含许多变量的输入。然后，将权重应用于带有偏差的值。接着，激活函数转换结果并生成输出。
- en: Neural networks, even one- or two-layer networks, can provide real-life solutions
    in a corporate environment. A real-life business case was implemented using complex
    theory broken down into small functions. Then, these components were assembled
    to be as minimal and profitable as possible.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在企业环境中，即使是一层或两层的神经网络，也可以提供现实生活中的解决方案。使用复杂理论分解成小函数实施了一个真实的业务案例。然后，这些组件被组装成尽可能简单和有利可图的形式。
- en: It takes talent to break a problem down into elementary parts and find a simple,
    powerful solution. It requires more effort than just typing hundreds to thousands
    of lines of code to make things work. A well-thought through algorithm will always
    be more profitable, and software maintenance will prove more cost-effective.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 把问题分解成基本部分，并找到简单而强大的解决方案需要才智。这比仅仅输入数百到数千行代码更需要努力来使事情运行。一个经过深思熟虑的算法将始终更有利可图，软件维护也将更加经济实惠。
- en: Customers expect quick-win solutions. Artificial intelligence provides a large
    variety of tools that satisfy that goal. When solving a problem for a customer,
    do not look for the best theory, but the simplest and fastest way to implement
    a profitable solution, no matter how unconventional it seems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 客户期望快速解决方案。人工智能提供了多种工具，能够满足这一目标。在为客户解决问题时，不要追求最完美的理论，而是寻找最简单、最快速的方式来实施一个有利可图的解决方案，无论它看起来多么不传统。
- en: In this case, an enhanced FNN perceptron solved a complex business problem.
    In the next chapter, we will explore a convolutional neural network (CNN). We
    will build a CNN with TensorFlow 2.x, layer by layer, to classify images.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，增强型FNN感知机解决了一个复杂的商业问题。在下一章中，我们将探索卷积神经网络（CNN）。我们将逐层使用TensorFlow 2.x构建一个CNN来分类图像。
- en: Questions
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can the perceptron alone solve the XOR problem? (Yes | No)
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 感知机是否能单独解决异或问题？（是 | 否）
- en: Is the XOR function linearly non-separable? (Yes | No)
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 异或函数是线性不可分的吗？（是 | 否）
- en: One of the main goals of layers in a neural network is classification. (Yes
    | No)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中层的主要目标之一是分类。（是 | 否）
- en: Is deep learning the only way to classify data? (Yes | No)
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深度学习是唯一的分类数据方式吗？（是 | 否）
- en: A cost function shows the increase in the cost of a neural network. (Yes | No)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 成本函数显示神经网络成本的增加。（是 | 否）
- en: Can simple arithmetic be enough to optimize a cost function? (Yes | No)
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 简单的算术运算是否足以优化成本函数？（是 | 否）
- en: A feedforward network requires inputs, layers, and an output. (Yes | No)
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络需要输入、层和输出。（是 | 否）
- en: A feedforward network always requires training with backpropagation. (Yes |
    No)
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络总是需要通过反向传播进行训练。（是 | 否）
- en: In real-life applications, solutions are only found by following existing theories.
    (Yes | No)
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实际应用中，解决方案往往是通过遵循现有理论找到的。（是 | 否）
- en: Further reading
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Linear separability: [http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性可分性：[http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html](http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html)
