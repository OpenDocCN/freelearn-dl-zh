- en: 1\. Introduction to Machine Learning with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 使用 TensorFlow 进行机器学习介绍
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: 'In this chapter, you will learn how to create, utilize, and apply linear transformations
    to the fundamental building blocks of programming with TensorFlow: tensors. You
    will then utilize tensors to understand the complex concepts associated with neural
    networks, including tensor reshaping, transposition, and multiplication.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何创建、利用和应用线性变换到 TensorFlow 编程的基本构建块——张量。然后，你将利用张量来理解与神经网络相关的复杂概念，包括张量重塑、转置和乘法。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Machine learning** (**ML**) has permeated various aspects of daily life that
    are unknown to many. From the recommendations of your daily social feeds to the
    results of your online searches, they are all powered by machine learning algorithms.
    These algorithms began in research environments solving niche problems, but as
    their accessibility broadened, so too have their applications for broader use
    cases. Researchers and businesses of all types recognize the value of using models
    to optimize every aspect of their respective operations. Doctors can use machine
    learning to decide diagnosis and treatment options, retailers can use ML to get
    the right products to their stores at the right time, and entertainment companies
    can use ML to provide personalized recommendations to their customers.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）已经渗透到许多人未曾察觉的日常生活的各个方面。从日常社交媒体推荐到在线搜索结果，它们都由机器学习算法驱动。这些算法最初在研究环境中解决特定问题，但随着其可访问性不断扩展，它们的应用场景也变得越来越广泛。各类研究人员和企业都认识到，使用模型来优化各自运营的每个环节具有重要价值。医生可以使用机器学习来决定诊断和治疗方案，零售商可以利用
    ML 在合适的时间将合适的产品送到店铺，而娱乐公司可以使用 ML 向客户提供个性化的推荐。'
- en: In the age of data, machine learning models have proven to be valuable assets
    to any data-driven company. The large quantities of data available allow powerful
    and accurate models to be created to complete a variety of tasks, from regression
    to classification, recommendations to time series analysis, and even generative
    art, many of which will be covered in this workshop. And all can be built, trained,
    and deployed with TensorFlow.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据时代，机器学习模型已被证明是任何数据驱动型公司的宝贵资产。海量数据使得可以创建强大且准确的模型来完成各种任务，从回归到分类，从推荐到时间序列分析，甚至是生成艺术，这些内容将在本次研讨会中介绍。所有这些任务都可以通过
    TensorFlow 构建、训练和部署。
- en: The TensorFlow API has a huge amount of functionality that has made it popular
    among all machine learning practitioners building machine learning models or working
    with tensors, which are multidimensional numerical arrays. For researchers, TensorFlow
    is an appropriate choice to create new machine learning applications due to its
    advanced customization and flexibility. For developers, TensorFlow is an excellent
    choice of machine learning library due to its ease in terms of deploying models
    from development to production environments. Combined, TensorFlow's flexibility
    and ease of deployment make the library a smart choice for many practitioners
    looking to build performant machine learning models using a variety of different
    data sources and to replicate the results of that learning in production environments.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow API 拥有大量功能，使其在所有构建机器学习模型或处理张量（多维数值数组）的机器学习从业者中广受欢迎。对于研究人员来说，TensorFlow
    是创建新机器学习应用的合适选择，因为它提供了高度的定制和灵活性。对于开发者而言，TensorFlow 是一个优秀的机器学习库选择，因为它在模型从开发到生产环境的部署方面非常方便。综合来看，TensorFlow
    的灵活性和易于部署使得它成为许多从业者的智能选择，他们希望使用各种数据源构建高效的机器学习模型，并在生产环境中复制这些学习成果。
- en: This chapter provides a practical introduction to TensorFlow's API. You will
    learn how to perform mathematical operations pertinent to machine learning that
    will give you a firm foundation for building performant ML models using TensorFlow.
    You will first learn basic operations such as how to create variables with the
    API. Following that, you will learn how to perform linear transformations such
    as addition before moving on to more advanced tasks, including tensor multiplication.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了 TensorFlow API 的实用介绍。你将学习如何执行与机器学习相关的数学操作，这些操作将为你使用 TensorFlow 构建高效 ML
    模型奠定坚实的基础。你将首先学习基本操作，例如如何使用 API 创建变量。接下来，你将学习如何执行线性变换，如加法，然后再深入到更高级的任务，包括张量乘法。
- en: Implementing Artificial Neural Networks in TensorFlow
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现人工神经网络
- en: The advanced flexibility that TensorFlow offers lends itself well to creating
    **artificial neural networks** (**ANNs**). ANNs are algorithms that are inspired
    by the connectivity of neurons in the brain and are intended to replicate the
    process in which humans learn. They consist of layers through which information
    propagates from the input to the output.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供的高级灵活性非常适合构建**人工神经网络**（**ANNs**）。ANNs 是受到大脑神经元连接启发的算法，旨在复制人类学习的过程。它们由多个层组成，信息通过这些层从输入传播到输出。
- en: '*Figure 1.1* shows a visual representation of an ANN. An input layer is on
    the left-hand side, which, in this example, has two features (`X`1 and `X`2).
    The input layer is connected to the first hidden layer, which has three units.
    All the data from the previous layer gets passed to each unit in the first hidden
    layer. The data is then passed to the second hidden layer, which also has three
    units. Again, the information from each unit of the prior layer is passed to each
    unit of the second hidden layer. Finally, all the information from the second
    hidden layer is passed to the output layer, which has one unit, representing a
    single number for each set of input features.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.1* 展示了一个人工神经网络（ANN）的可视化表示。输入层位于左侧，在这个示例中，输入层有两个特征（`X`1 和 `X`2）。输入层与第一个隐藏层连接，后者有三个单元。来自前一层的所有数据会传递给第一个隐藏层的每个单元。然后，数据被传递到第二个隐藏层，第二个隐藏层同样有三个单元。再次地，来自上一层每个单元的信息会传递给第二个隐藏层的每个单元。最后，所有来自第二个隐藏层的信息都会传递到输出层，输出层有一个单元，表示每组输入特征的单个数字。'
- en: '![Figure 1.1: A visual representation of an ANN with two hidden layers'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：一个具有两个隐藏层的人工神经网络（ANN）的可视化表示'
- en: '](img/B16341_01_01.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_01.jpg)'
- en: 'Figure 1.1: A visual representation of an ANN with two hidden layers'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：一个具有两个隐藏层的人工神经网络（ANN）的可视化表示
- en: ANNs have proven to be successful in learning complex and nonlinear relationships
    with large, unstructured datasets, such as audio, images, and text data. While
    the results can be impressive, there is a lot of variability in how ANNs can be
    configured. For example, the number of layers, the size of each layer, and which
    nonlinear function should be used are some of the factors that determine the configuration
    of ANNs. Not only are the classes and functions that TensorFlow provides well-suited
    to building and training ANNs, but the library also supplies a suite of tools
    to help visualize and debug ANNs during the training process.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs 已被证明能够成功学习具有复杂和非线性关系的大规模非结构化数据集，如音频、图像和文本数据。尽管结果可能令人印象深刻，但ANNs的配置存在很大的可变性。例如，层数、每层的大小以及应该使用哪种非线性函数是决定ANNs配置的一些因素。TensorFlow
    提供的类和函数不仅非常适合构建和训练ANNs，此外，该库还提供了一套工具，帮助在训练过程中可视化和调试ANNs。
- en: Compared with traditional machine learning algorithms, such as linear and logistic
    regression, ANNs can outperform them when provided with large amounts of data.
    ANNs are advantageous since they can be fed unstructured data and feature engineering
    is not necessarily required. Data pre-processing can be a time-intensive process.
    Therefore, many practitioners prefer ANNs if there is a large amount of data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习算法（如线性回归和逻辑回归）相比，人工神经网络（ANNs）在数据量较大的情况下能够超越这些算法。ANNs的优势在于它们能够处理非结构化数据，并且不一定需要特征工程。数据预处理可能是一个耗时的过程。因此，如果数据量较大，许多实践者更倾向于选择ANNs。
- en: Many companies from all sectors utilize TensorFlow to build ANNs for their applications.
    Since TensorFlow is backed by Google, the company utilizes the library for much
    of its research, development, and production of machine learning applications.
    However, there are many other companies that also use the library. Companies such
    as Airbnb, Coca-Cola, Uber, and GE Healthcare all utilize the library for a variety
    of tasks. The use of ANNs is particularly appealing since they can achieve remarkable
    accuracy if provided with sufficient data and trained appropriately. For example,
    GE Healthcare uses TensorFlow to build ANNs to identify specific anatomy regardless
    of orientation from magnetic resonance images to improve speed and accuracy. By
    using ANNs, they can achieve over 99% accuracy in identifying anatomy in seconds,
    regardless of head rotation, which would otherwise take a trained professional
    much more time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来自各行各业的许多公司都利用 TensorFlow 构建人工神经网络（ANN）用于他们的应用。由于 TensorFlow 得到了 Google 的支持，该公司将该库应用于大部分机器学习应用的研究、开发和生产。然而，许多其他公司也使用该库。像
    Airbnb、可口可乐、Uber 和 GE Healthcare 等公司，都在进行各种任务时使用该库。人工神经网络的使用尤其具有吸引力，因为只要提供足够的数据并进行适当训练，它们就能实现显著的准确性。例如，GE
    Healthcare 使用 TensorFlow 构建 ANN，从磁共振图像中识别特定的解剖结构，不论其朝向如何，以提高速度和准确性。通过使用 ANN，他们能够在几秒钟内识别出解剖结构，准确率超过
    99%，无论头部如何旋转，而这在没有 ANN 的情况下需要经过训练的专业人员花费更多时间才能完成。
- en: 'While the number of companies utilizing ANNs is vast, ANNs may not be the most
    appropriate choice for solving all business problems. In such an environment,
    you must answer the following questions to determine whether ANNs are the most
    appropriate choice:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多公司都在使用人工神经网络，但 ANN 可能不是解决所有业务问题的最佳选择。在这种环境下，你需要回答以下问题，以确定 ANN 是否是最合适的选择：
- en: '**Does the problem have a numerical solution?** Machine learning algorithms,
    ANNs included, generate predicted numerical results based on input data. For example,
    machine learning algorithms may predict a given number, such as the temperature
    of a city given the location and previous weather conditions, or the stock price
    given previous stock prices, or label images into a given number of categories.
    In each of these examples, a numerical output is generated based on the data provided
    and, given enough labeled data, models can perform well. However, when the desired
    result is more abstract, or creativity is needed, such as creating a new song,
    then machine learning algorithms may not be the most appropriate choice, since
    a well-defined numerical solution may not be available.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题是否有数值解决方案？** 包括人工神经网络（ANN）在内的机器学习算法，基于输入数据生成预测的数值结果。例如，机器学习算法可能会预测一个给定的数字，如根据城市的位置和先前的天气条件预测温度，或者根据先前的股票价格预测股价，或者将图像分类为某个给定的类别。在这些例子中，基于提供的数据生成数值输出，并且只要有足够的标注数据，模型就能表现得很好。然而，当期望的结果更为抽象，或需要创意时，比如创作一首新歌，那么机器学习算法可能不是最合适的选择，因为可能没有明确的数值解决方案可供参考。'
- en: '**Is there enough appropriately labeled data to train a model?** For a supervised
    learning task, you must have at least some labeled data to train a model. For
    example, if you want to build a model to predict financial stock data for a given
    company, you will first need historical training data. If the company in question
    has not been public for very long, there may not be adequate training data. ANNs
    can often require a lot of data. When working with images, ANNs often need millions
    of training examples to develop accurate, robust models. This may be a determining
    factor for consideration when deciding which algorithm is appropriate for a given
    task.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**是否有足够的合适标注数据来训练模型？** 对于监督学习任务，你必须至少拥有一些标注数据来训练模型。例如，如果你想构建一个模型来预测某个公司的金融股票数据，你首先需要历史训练数据。如果该公司尚未上市很长时间，可能就没有足够的训练数据。人工神经网络通常需要大量的数据。当处理图像时，ANN
    通常需要数百万个训练样本来开发出准确且稳健的模型。这可能是决定哪个算法适合给定任务的一个关键因素。'
- en: Now that you are aware of what TensorFlow is, consider the following advantages
    and disadvantages of TensorFlow.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了什么是 TensorFlow，那么请考虑以下 TensorFlow 的优缺点。
- en: Advantages of TensorFlow
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 的优点
- en: 'The following are a few of the main advantages of using TensorFlow that many
    practitioners consider when deciding whether to pursue the library for machine
    learning purposes:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是许多从业者在决定是否使用 TensorFlow 进行机器学习时所考虑的几个主要优点：
- en: '**Library Management**: There is a large community of practitioners that maintain
    the TensorFlow library to keep it up to date with frequent new releases to help
    fix bugs, add new functions and classes to reflect current advances in the field,
    and add support for multiple programming languages.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**库管理**：TensorFlow库有一个庞大的开发者社区，他们不断更新库，发布新版本以修复bug，增加新的函数和类，以反映当前领域的进展，并支持多种编程语言。'
- en: '**Pipelining**: TensorFlow supports end-to-end model production, from model
    development in highly parallelizable environments that support GPU processing
    to a suite of model deployment tools. Also, there are lightweight libraries in
    TensorFlow that are used for deploying trained TensorFlow models on mobile and
    embedded devices, such as **Internet of Things** (**IoT**) devices.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流水线**：TensorFlow支持端到端的模型生产，从在支持GPU处理的高并行环境中开发模型，到一整套模型部署工具。此外，TensorFlow还有轻量级的库，用于将训练好的TensorFlow模型部署到移动设备和嵌入式设备上，例如**物联网**（**IoT**）设备。'
- en: '**Community Support**: The community of practitioners that use and support
    the library is vast and they support each other, because of which those practitioners
    who are new to the library achieve the results they are looking for easily.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社区支持**：使用并支持该库的开发者社区庞大，他们互相支持，正因如此，初学者能轻松地获得他们想要的结果。'
- en: '**Open Source**: TensorFlow is an open source library, and its code base is
    available for anyone to use and modify for their own applications.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：TensorFlow是一个开源库，其代码库对任何人开放，允许用户根据自己的需求使用和修改。'
- en: '**Works with Multiple Languages**: While the library is natively designed for
    Python, models can now be trained and deployed in JavaScript.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持多种语言**：虽然该库本身是为Python设计的，但现在也可以在JavaScript中训练和部署模型。'
- en: Disadvantages of TensorFlow
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow的缺点
- en: 'The following are a few of the disadvantages of using TensorFlow:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用TensorFlow的一些缺点：
- en: '**Computational Speed**: Since the primary programming language of TensorFlow
    is Python, the library is not as computationally fast as it could be if it were
    native to other languages, such as C++.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算速度**：由于TensorFlow的主要编程语言是Python，因此它的计算速度不如其他语言（如C++）的原生实现。'
- en: '**Steep Learning Curve**: Compared to other machine learning libraries, such
    as Keras, the learning curve is steeper, and this can make it challenging for
    new practitioners to create their own models outside of given example code.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**陡峭的学习曲线**：与其他机器学习库（如Keras）相比，TensorFlow的学习曲线较为陡峭，这可能会使得新手在没有现成示例代码的情况下，创建自己的模型变得具有挑战性。'
- en: Now that you have understood what TensorFlow is, the next section will demonstrate
    how to use the TensorFlow library using Python.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了TensorFlow是什么，接下来的部分将演示如何使用Python来操作TensorFlow库。
- en: The TensorFlow Library in Python
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的TensorFlow库
- en: 'TensorFlow can be used in Python by importing certain libraries. You can import
    libraries in Python using the `import` statement:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过导入特定的库在Python中使用TensorFlow。你可以使用`import`语句在Python中导入库：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding command, you have imported the TensorFlow library and used
    the shorthand `tf`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的命令中，你已导入了TensorFlow库并使用了简写`tf`。
- en: In the next exercise, you will learn how to import the TensorFlow library and
    check its version so that you can utilize the classes and functions supplied by
    the library, which is an important and necessary first step when utilizing the
    library.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，你将学习如何导入TensorFlow库并检查其版本，以便你能够利用库提供的类和函数，这是使用该库时非常重要和必要的第一步。
- en: 'Exercise 1.01: Verifying Your Version of TensorFlow'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.01：验证你的TensorFlow版本
- en: In this exercise, you will load TensorFlow and check which version is installed
    on your system.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你将加载TensorFlow并检查系统中安装的版本。
- en: 'Perform the following steps:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Open a Jupyter notebook to implement this exercise by typing `jupyter notebook`
    in the terminal.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Jupyter笔记本来实现这个练习，在终端中输入`jupyter notebook`。
- en: 'Import the TensorFlow library by entering the following code in the Jupyter
    cell:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在Jupyter单元格中输入以下代码来导入TensorFlow库：
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Verify the version of TensorFlow using the following command:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令验证TensorFlow的版本：
- en: '[PRE2]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will result in the following output:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE3]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from the preceding output, the version of TensorFlow is `2.6.0`.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从之前的输出中可以看到，TensorFlow的版本是`2.6.0`。
- en: Note
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The version may vary on your system if you have not set up the environment using
    the steps provided in *Preface*.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你没有按照*前言*中的步骤设置环境，版本可能会在你的系统上有所不同。
- en: In this exercise, you successfully imported TensorFlow. You have also checked
    which version of TensorFlow is installed on your system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你成功导入了TensorFlow。你还检查了系统上安装的TensorFlow版本。
- en: This task can be done for any imported library in Python and is useful for debugging
    and referencing documentation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务可以在Python中对任何导入的库执行，并且对于调试和参考文档非常有用。
- en: The potential applications of using TensorFlow are numerous, and it has already
    achieved impressive results, as evidenced by the results from companies such as
    Airbnb, which uses TensorFlow to classify images on their platform, to GE Healthcare,
    which uses TensorFlow to identify anatomy on MRIs of the brain. To learn how to
    create powerful models for your own applications, you first must learn the basic
    mathematical principles and operations that make up the machine learning models
    that can be achieved in TensorFlow. The mathematical operations can be intimidating
    to new users, but a comprehensive understanding of how they operate is key to
    making performant models.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorFlow的潜在应用非常广泛，它已经取得了令人印象深刻的成果，像Airbnb这样的公司利用TensorFlow对平台上的图像进行分类，GE
    Healthcare则使用TensorFlow在大脑的MRI图像中识别解剖结构。要学习如何为自己的应用创建强大的模型，你首先需要学习构成TensorFlow中可实现的机器学习模型的基本数学原理和运算。数学运算可能会让新用户感到畏惧，但对它们的全面理解是构建高效模型的关键。
- en: Introduction to Tensors
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量简介
- en: Tensors can be thought of as the core components of ANNs—the input data, output
    predictions, and weights that are learned throughout the training process are
    all tensors. Information propagates through a series of linear and nonlinear transformations
    to turn the input data into predictions. This section demonstrates how to apply
    linear transformations such as additions, transpositions, and multiplications
    to tensors. Other linear transformations, such as rotations, reflections, and
    shears, also exist. However, their applications as they pertain to ANNs are less common.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以被看作是人工神经网络（ANNs）的核心组件——输入数据、输出预测以及在训练过程中学习到的权重都是张量。信息通过一系列线性和非线性变换传播，将输入数据转化为预测结果。本节展示了如何对张量应用线性变换，如加法、转置和乘法。其他线性变换，如旋转、反射和剪切，也存在。然而，它们在人工神经网络中的应用较为少见。
- en: Scalars, Vectors, Matrices, and Tensors
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标量、向量、矩阵和张量
- en: 'Tensors can be represented as multi-dimensional arrays. The number of dimensions
    a tensor spans is known as the tensor''s rank. Tensors with ranks `0`, `1`, and
    `2` are used often and have their own names, which are **scalars**, **vectors**,
    and **matrices**, respectively, although the term *tensors* can be used to describe
    each of them. *Figure 1.2* shows some examples of tensors of various ranks. From
    left to right are a scalar, vector, matrix, and a 3-dimensional tensor, where
    each element represents a different number, and the subscript represents the location
    of the element in the tensor:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以表示为多维数组。张量所跨越的维度数量称为张量的阶数。阶数为`0`、`1`和`2`的张量常常使用，并且各自有独立的名称，分别是**标量**、**向量**和**矩阵**，尽管术语*张量*可以用来描述它们中的任何一个。*图
    1.2*展示了不同阶张量的一些示例。从左到右分别是标量、向量、矩阵和一个三维张量，每个元素表示一个不同的数字，下标表示该元素在张量中的位置：
- en: '![Figure 1.2: A visual representation of a scalar, vector, matrix, and tensor'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2：标量、向量、矩阵和张量的可视化表示'
- en: '](img/B16341_01_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_02.jpg)'
- en: 'Figure 1.2: A visual representation of a scalar, vector, matrix, and tensor'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：标量、向量、矩阵和张量的可视化表示
- en: 'The formal definitions of a scalar, vector, matrix, and tensor are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 标量、向量、矩阵和张量的正式定义如下：
- en: '**Scalar**: A scalar consists of a single number, making it a zero-dimensional
    array. It is an example of zero-order tensors. Scalars do not have any axes. For
    instance, the width of an object is a scalar.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：标量由一个单一的数字组成，因此它是一个零维数组，是零阶张量的例子。标量没有任何轴。例如，物体的宽度就是一个标量。'
- en: '**Vector**: Vectors are one-dimensional arrays and are an example of first-order
    tensors. They can be considered lists of values. Vectors have one axis. The size
    of a given object denoted by the width, height, and depth is an example of a vector field.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：向量是一维数组，是一阶张量的例子。它们可以被看作是值的列表。向量有一个轴。给定物体的宽度、高度和深度的大小是一个向量场的例子。'
- en: '**Matrix**: Matrices are two-dimensional arrays with two axes. They are an
    example of second-order tensors. Matrices might be used to store the size of several
    objects. Each dimension of the matrix comprises the size of each object (width,
    height, depth) and the other matrix dimension is used to differentiate between objects.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：矩阵是具有两个轴的二维数组，是二阶张量的一个例子。矩阵可能用于存储多个对象的大小。矩阵的每个维度包含每个对象的大小（宽度、高度、深度），另一个矩阵维度则用于区分对象。'
- en: '`3` or more. A tensor can be used to store the size of many objects and their
    locations over time. The first dimension of the matrix comprises the size of each
    object (width, height, depth), the second dimension is used to differentiate between
    the objects, and the third dimension describes the location of these objects over time.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`或更多。张量可以用来存储多个对象的大小及其随时间变化的位置。矩阵的第一维包含每个对象的大小（宽度、高度、深度），第二维用于区分不同的对象，第三维描述这些对象随时间变化的位置。'
- en: 'Tensors can be created using the `Variable` class present in the TensorFlow
    library and passing in a value representing the tensor. A float or integer can
    be passed for scalars, a list of floats or integers can be passed for vectors,
    a nested list of floats or integers for matrices, and so on. The following command
    demonstrates the use of the `Variable` class where a list of the intended values
    for the tensor as well as any other attributes that are required to be explicitly
    defined are passed:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以使用TensorFlow库中的`Variable`类来创建，并传入一个表示张量的值。标量可以传入浮动值或整数，向量可以传入浮动值或整数的列表，矩阵可以传入浮动值或整数的嵌套列表，依此类推。以下命令演示了如何使用`Variable`类，其中传入了张量的预期值列表以及任何需要显式定义的其他属性：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The resultant `Variable` object has several attributes that may be commonly
    called, and these are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果`Variable`对象具有几个常用的属性，具体如下：
- en: '`dtype`: The datatype of the `Variable` object (for the tensor defined above,
    the datatype is `tf.int32`). The default value for this attribute is determined
    from the values passed.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtype`：`Variable`对象的数据类型（对于上面定义的张量，数据类型是`tf.int32`）。该属性的默认值由传入的值决定。'
- en: '`shape`: The number of dimensions and length of each dimension of the `Variable`
    object (for the tensor defined above, the shape is `[3]`). The default value for
    this attribute is also determined from the values passed.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shape`：`Variable`对象的维度数和每个维度的长度（对于上面定义的张量，形状是`[3]`）。该属性的默认值也由传入的值决定。'
- en: '`name`: The name of the `Variable` object (for the tensor defined above, the
    name of the tensor is defined as `''my_tensor''`). The default for this attribute
    is `Variable`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：`Variable`对象的名称（对于上面定义的张量，张量的名称定义为`''my_tensor''`）。该属性的默认值为`Variable`。'
- en: '`trainable`: This attribute indicates whether the `Variable` object can be
    updated during model training (for the tensor defined above, the `trainable` parameter
    is set to `true`). The default for this attribute is `true`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainable`：该属性指示`Variable`对象是否可以在模型训练过程中更新（对于上面定义的张量，`trainable`参数设置为`true`）。该属性的默认值是`true`。'
- en: Note
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about the attributes of the `Variable` object here: [https://www.tensorflow.org/api_docs/python/tf/Variable](https://www.tensorflow.org/api_docs/python/tf/Variable).'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于`Variable`对象属性的内容：[https://www.tensorflow.org/api_docs/python/tf/Variable](https://www.tensorflow.org/api_docs/python/tf/Variable)。
- en: 'The `shape` attribute of the `Variable` object can be called as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`Variable`对象的`shape`属性可以按如下方式调用：'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `shape` attribute gives the shape of the tensor, that is, is it a scalar,
    vector, matrix, and so on. The output of the preceding command will be `[3]` since
    the tensor has a single dimension with three values along that dimension.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`shape`属性给出了张量的形状，即它是标量、向量、矩阵等。前面命令的输出将是`[3]`，因为该张量只有一个维度，且该维度上有三个值。'
- en: 'The rank of a tensor can be determined in TensorFlow using the `rank` function.
    It can be used by passing the tensor as the single argument to the function and
    the result will be an integer value:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用TensorFlow中的`rank`函数来确定张量的阶数。通过将张量作为唯一的参数传递给该函数，结果将是一个整数值：
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The output of the following command will be a zero-dimensional integer tensor
    representing the rank of the input. In this case, the rank of `tensor1` will be
    `1` as the tensor has only one dimension.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令的输出将是一个零维整数张量，表示输入张量的阶数。在此情况下，`tensor1`的阶数为`1`，因为该张量只有一个维度。
- en: In the following exercise, you will learn how to create tensors of various ranks
    using TensorFlow's `Variable` class.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，您将学习如何使用 TensorFlow 的 `Variable` 类创建各种秩的张量。
- en: 'Exercise 1.02: Creating Scalars, Vectors, Matrices, and Tensors in TensorFlow'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.02：在 TensorFlow 中创建标量、向量、矩阵和张量
- en: 'The votes cast for different candidates of three different political parties
    in districts A and B are as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 三个不同政党在 A 和 B 两个选区的不同候选人所获得的选票如下：
- en: '![Figure 1.3: Votes cast for different candidates of three different political'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3：三种不同政治派别候选人的选票（不同政治派别和不同选区的选票分布）](img/B16341_01_03.jpg)'
- en: parties in districts A and B](img/B16341_01_03.jpg)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 A 和 B 两个选区的各个政党的候选人选票分布](img/B16341_01_03.jpg)
- en: 'Figure 1.3: Votes cast for different candidates of three different political
    parties in districts A and B'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：三种不同政党在 A、B 两个选区的不同候选人获得的选票数
- en: 'You are required to do the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要执行以下操作：
- en: Create a scalar to store the votes cast for `Candidate 1` of political party
    `X` in district `A`, that is, `4113`, and check its shape and rank.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个标量来存储选区 A 中 `X` 政党 `候选人 1` 的选票数，即 `4113`，并检查其形状和秩。
- en: Create a vector to represent the proportion of votes cast for three different
    candidates of political party `X` in district `A` and check its shape and rank.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个向量来表示 `X` 政党在 A 选区中三名候选人所获得选票的比例，并检查其形状和秩。
- en: Create a matrix to represent the votes cast for three different candidates of
    political parties `X` and `Y` and check its shape and rank.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个矩阵来表示 `X` 和 `Y` 政党在三个不同候选人中的选票，并检查其形状和秩。
- en: Create a tensor to represent the votes cast for three different candidates in
    two different districts, for three political parties, and check its shape and
    rank.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个张量来表示三名候选人在两个不同选区、三种不同政党中的选票，并检查其形状和秩。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成本练习：
- en: 'Import the TensorFlow library:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库：
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create an integer variable using TensorFlow''s `Variable` class and pass `4113`
    to represent the number of votes cast for a particular candidate. Also, pass `tf.int16`
    as a second argument to ensure that the input number is an integer datatype. Print
    the result:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建一个整数变量，并传入 `4113` 来表示某个候选人获得的选票数。同时，传入 `tf.int16`
    作为第二个参数，以确保输入的数字是整数数据类型。打印结果：
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will result in the following output:'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, you can see the attributes of the variable created, including the name,
    `Variable:0`, the shape, datatype, and the NumPy representation of the tensor.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，您可以看到所创建变量的属性，包括名称 `Variable:0`、形状、数据类型以及张量的 NumPy 表示。
- en: 'Use TensorFlow''s `rank` function to print the rank of the variable created:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `rank` 函数打印创建的变量的秩：
- en: '[PRE10]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the following output:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE11]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see that the rank of the integer variable that was created is `0` from
    the NumPy representation of the tensor.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 NumPy 对张量的表示中可以看到，创建的整数变量的秩是 `0`。
- en: 'Access the integer variable of the rank by calling the `numpy` attribute:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `numpy` 属性访问秩的整数变量：
- en: '[PRE12]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will result in the following output:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The rank of the scalar is `0`.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标量的秩是 `0`。
- en: Note
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: All attributes of the result of the `rank` function can be called, including
    the `shape` and `dtype` attributes.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rank` 函数的所有属性都可以被调用，包括 `shape` 和 `dtype` 属性。'
- en: 'Call the `shape` attribute of the integer to find the shape of the tensor:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用整数的 `shape` 属性以查看张量的形状：
- en: '[PRE14]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will result in the following output:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE15]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding output signifies that the shape of the tensor has no size, which
    is representative of a scalar.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的输出表明张量的形状没有大小，代表它是一个标量。
- en: 'Print the `shape` of the scalar variable as a Python list:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印标量变量的 `shape`，以 Python 列表的形式表示：
- en: '[PRE16]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will result in the following output:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE17]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create a `vector` variable using TensorFlow''s `Variable` class. Pass a list
    for the vector to represent the proportion of votes cast for three different candidates,
    and pass in a second argument for the datatype as `tf.float32` to ensure that
    it is a `float` datatype. Print the result:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建一个 `vector` 变量。传入一个列表来表示三名候选人所获得选票的比例，并传入第二个参数
    `tf.float32` 以确保它是一个 `float` 数据类型。打印结果：
- en: '[PRE18]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will result in the following output:'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE19]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You can see that the shape and NumPy attributes are different from the scalar
    variable created earlier. The shape is now `(3,)`, indicating that the tensor
    is one-dimensional with three elements along that dimension.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以看到，形状和 NumPy 属性与之前创建的标量变量不同。现在的形状是 `(3,)`，表示张量是一个一维的，沿该维度有三个元素。
- en: 'Print the rank of the `vector` variable using TensorFlow''s `rank` function
    as a NumPy variable:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `rank` 函数打印 `vector` 变量的秩，作为 NumPy 变量：
- en: '[PRE20]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will result in the following output:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, you can see that the rank of the vector variable is `1`, confirming that
    this variable is one-dimensional.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，您可以看到，向量变量的秩为 `1`，确认该变量是一维的。
- en: 'Print the shape of the `vector` variable as a Python list:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `vector` 变量的形状作为 Python 列表：
- en: '[PRE22]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will result in the following output:'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create a matrix variable using TensorFlow''s `Variable` class. Pass a list
    of lists of integers for the matrix to represent the votes cast for three different
    candidates in two different districts. This matrix will have three columns representing
    the candidates, and two rows representing the districts. Pass in a second argument
    for the datatype as `tf.int32` to ensure that it is an integer datatype. Print
    the result:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建一个矩阵变量。传入一个整数的列表列表，表示在两个不同选区中，三个不同候选人的投票结果。该矩阵将有三列，表示候选人，和两行，表示选区。传入第二个参数，数据类型为
    `tf.int32`，以确保其为整数数据类型。打印结果：
- en: '[PRE24]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will result in the following output:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 1.4: The output of the TensorFlow variable'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.4：TensorFlow 变量的输出'
- en: '](img/B16341_01_04.jpg)'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_04.jpg)'
- en: 'Figure 1.4: The output of the TensorFlow variable'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.4：TensorFlow 变量的输出
- en: 'Print the rank of the matrix variable as a NumPy variable:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印矩阵变量的秩作为 NumPy 变量：
- en: '[PRE25]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will result in the following output:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE26]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, you can see that the rank of the matrix variable is `2`, confirming that
    this variable is two-dimensional.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，您可以看到矩阵变量的秩为`2`，确认该变量是二维的。
- en: 'Print the shape of the matrix variable as a Python list:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印矩阵变量的形状作为 Python 列表：
- en: '[PRE27]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will result in the following output:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE28]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Create a tensor variable using TensorFlow''s `Variable` class. Pass in a triple
    nested list of integers for the tensor to represent the votes cast for three different
    candidates in two different districts, for three political parties. Print the result:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建一个张量变量。传入一个三重嵌套的整数列表，表示三个不同候选人在两个不同选区、三个政党中的投票结果。打印结果：
- en: '[PRE29]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will result in the following output:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 1.5: The output of the TensorFlow variable'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.5：TensorFlow 变量的输出'
- en: '](img/B16341_01_05.jpg)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_05.jpg)'
- en: 'Figure 1.5: The output of the TensorFlow variable'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.5：TensorFlow 变量的输出
- en: 'Print the rank of the tensor variable as a NumPy variable:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印张量变量的秩作为 NumPy 变量：
- en: '[PRE30]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will result in the following output:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE31]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, you can see that the rank of the tensor variable is `3`, confirming that
    this variable is three-dimensional.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，您可以看到，张量变量的秩为 `3`，确认该变量是三维的。
- en: 'Print the shape of the tensor variable as a Python list:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印张量变量的形状作为 Python 列表：
- en: '[PRE32]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will result in the following output:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE33]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The result shows that the shape of the resulting tensor is a list object.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，生成的张量的形状是一个列表对象。
- en: In this exercise, you have successfully created tensors of various ranks from
    political voting data using TensorFlow's `Variable` class. First, you created
    scalars, which are tensors that have a rank of `0`. Next, you created vectors,
    which are tensors with a rank of `1`. Matrices were then created, which are tensors
    of rank `2`. Finally, tensors were created that have rank `3` or more. You confirmed
    the rank of the tensors you created by using TensorFlow's `rank` function and
    verified their shape by calling the tensor's `shape` attribute.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，您已经成功地使用 TensorFlow 的 `Variable` 类从政治投票数据中创建了各种秩的张量。首先，您创建了标量，这是秩为 `0`
    的张量。接下来，您创建了向量，这是秩为 `1` 的张量。然后，创建了矩阵，这是秩为 `2` 的张量。最后，创建了秩为 `3` 或更高的张量。您通过使用 TensorFlow
    的 `rank` 函数确认了所创建的张量的秩，并通过调用张量的 `shape` 属性验证了它们的形状。
- en: In the next section, you will combine tensors to create new tensors using tensor addition.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，您将结合张量，通过张量加法创建新张量。
- en: Tensor Addition
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量加法
- en: Tensors can be added together to create new tensors. You will use the example
    of matrices in this chapter, but the concept can be extended to tensors with any
    rank. Matrices may be added to scalars, vectors, and other matrices under certain
    conditions in a process known as broadcasting. Broadcasting refers to the process
    of array arithmetic on tensors of different shapes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 张量可以相加以创建新的张量。在本章中，你将使用矩阵的例子，但这一概念可以扩展到任何秩的张量。矩阵在特定条件下可以与标量、向量和其他矩阵相加，这个过程被称为广播。广播是指对不同形状的张量进行数组算术运算的过程。
- en: Two matrices may be added (or subtracted) together if they have the same shape.
    For such matrix-matrix addition, the resultant matrix is determined by the element-wise
    addition of the input matrices. The resultant matrix will therefore have the same
    shape as the two input matrices. You can define the matrix `Z = [Z`ij`]` as the
    matrix sum `Z = X + Y`, where `z`ij = `x`ij `+` `y`ij and each element in `Z`
    is the sum of the same element in `X` and `Y`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个矩阵形状相同，则可以将它们相加（或相减）。对于这种矩阵-矩阵加法，结果矩阵由输入矩阵逐元素相加得到。因此，结果矩阵的形状与两个输入矩阵相同。你可以定义矩阵`Z
    = [Z_ij]`为矩阵和`Z = X + Y`，其中`z_ij = x_ij + y_ij`，并且`Z`中的每个元素是`X`和`Y`中相同位置元素的和。
- en: Matrix addition is commutative, which means that the order of `X` and `Y` does
    not matter, that is, `X + Y = Y + X`. Matrix addition is also associative, which means
    that the same result is achieved even when the order of additions is different
    or even if the operation is applied more than once, that is, `X + (Y + Z) = (X
    + Y) + Z`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵加法是交换律的，这意味着`X`和`Y`的顺序无关，即`X + Y = Y + X`。矩阵加法也是结合律的，这意味着即使加法顺序不同，或者即使运算应用多次，结果也相同，即`X
    + (Y + Z) = (X + Y) + Z`。
- en: 'The same matrix addition principles apply to scalars, vectors, and tensors.
    An example is shown in the following figure:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的矩阵加法原则适用于标量、向量和张量。以下图为例：
- en: '![Figure 1.6: A visual example of matrix-matrix addition'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.6：矩阵-矩阵加法的视觉示例'
- en: '](img/B16341_01_06.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_06.jpg)'
- en: 'Figure 1.6: A visual example of matrix-matrix addition'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6：矩阵-矩阵加法的视觉示例
- en: 'Scalars can also be added to matrices. Here, each element of the matrix is
    added to the scalar individually, as shown in *Figure 1.7*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标量也可以与矩阵相加。在这里，矩阵的每个元素都与标量逐个相加，如*图1.7*所示：
- en: '![Figure 1.7: A visual example of matrix-scalar addition'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.7：矩阵-标量加法的视觉示例'
- en: '](img/B16341_01_07.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_07.jpg)'
- en: 'Figure 1.7: A visual example of matrix-scalar addition'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：矩阵-标量加法的视觉示例
- en: Addition is an important transformation that can be applied to tensors since
    the transformation occurs so frequently. For example, a common transformation
    in developing ANNs is to add a bias to a layer. This is when a constant tensor
    array of the same size of the ANN layer is added to that layer. Therefore, it
    is important to know how and when this seemingly simple transformation can be
    applied to tensors.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 加法是一个重要的变换，因其在张量中应用非常频繁。例如，在开发人工神经网络（ANN）时，一个常见的变换是为层添加偏置。这是将与ANN层大小相同的常量张量数组加到该层上。因此，了解如何以及何时应用这个看似简单的变换是很重要的。
- en: 'Tensor addition can be performed in TensorFlow by using the `add` function
    and passing in the tensors as arguments, or simply by using the `+` operator as
    follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 张量加法可以通过使用`add`函数并传递张量作为参数来在TensorFlow中执行，或者简单地使用`+`运算符，如下所示：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the following exercise, you will perform tensor addition on scalars, vectors,
    and matrices in TensorFlow.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，你将执行TensorFlow中标量、向量和矩阵的张量加法。
- en: 'Exercise 1.03: Performing Tensor Addition in TensorFlow'
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习1.03：在TensorFlow中执行张量加法
- en: 'The votes cast for different candidates of three different political parties
    in districts A and B are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: A区和B区三种不同政治党派不同候选人投票数如下：
- en: '![Figure 1.8: Votes cast for different candidates of three different political'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.8：三种不同政治党派在三个不同候选人中的投票情况'
- en: parties in districts A and B
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: A区和B区的政党
- en: '](img/B16341_01_08.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_08.jpg)'
- en: 'Figure 1.8: Votes cast for different candidates of three different political
    parties in districts A and B'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：A区和B区三种不同政治党派不同候选人投票数
- en: 'Your requisite tasks are as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务如下：
- en: Store the total number of votes cast for political party X in district A.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储A区政治党派X获得的总票数。
- en: Store the total number of votes cast for each political party in district A.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储A区每个政党获得的总票数。
- en: Store the total number of votes cast for each political party in both districts.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储两个选区中每个政党所投的总票数。
- en: 'Perform the following steps to complete the exercise:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤完成本次练习：
- en: 'Import the TensorFlow library:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库：
- en: '[PRE35]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create three scalar variables using TensorFlow''s `Variable` class to represent
    the votes cast for three candidates of political party X in district A:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建三个标量变量，表示在选区 A 中政党 X 的三位候选人所投的票数：
- en: '[PRE36]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Create a new variable to store the total number of votes cast for political
    party X in district A:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量，用来存储政党 X 在选区 A 中的总票数：
- en: '[PRE37]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Print the result of the sum of the two variables as a NumPy variable:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印两个变量之和作为 NumPy 变量的结果：
- en: '[PRE38]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will result in the following output:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE39]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create three vectors to represent the number of votes cast for different political
    parties in district A, each with one row and three columns:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个向量，表示在选区 A 中不同政党的投票数量，每个向量有一行三列：
- en: '[PRE40]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a new variable to store the total number of votes for each political
    party in district A:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量，用来存储每个政党在选区 A 中的总票数：
- en: '[PRE41]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Print the result of the sum of the two variables as a NumPy array:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印两个变量之和作为 NumPy 数组的结果：
- en: '[PRE42]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This will result in the following output:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE43]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Verify that the vector addition is as expected by performing the addition of
    each element of the vector:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行对向量中每个元素的加法，验证向量加法是否符合预期：
- en: '[PRE44]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This will result in the following output:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE45]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: You can see that the `+` operation on three vectors is simply element-wise addition
    of the vectors.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，三个向量上的`+`操作仅仅是向量的逐元素加法。
- en: 'Create three matrices to store the votes cast for candidates of each political
    party in each district:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个矩阵，用来存储每个选区中各政党的候选人所投的票数：
- en: '[PRE46]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Verify that the three tensors have the same shape:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证这三个张量是否具有相同的形状：
- en: '[PRE47]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This will result in the following output:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE48]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Create a new variable to store the total number of votes cast for each political
    party in both districts:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量，用来存储两个选区中每个政党所投的总票数：
- en: '[PRE49]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print the result of the sum of the two variables as a NumPy array:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印两个变量之和作为 NumPy 数组的结果：
- en: '[PRE50]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This will result in the following output representing the total votes for each
    candidate and each party across districts:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出，表示每个选区中每个候选人和每个政党的总票数：
- en: '![Figure 1.9: The output of the matrix summation as a NumPy variable'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.9：作为 NumPy 变量的矩阵求和输出'
- en: '](img/B16341_01_09.jpg)'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_09.jpg)'
- en: 'Figure 1.9: The output of the matrix summation as a NumPy variable'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.9：作为 NumPy 变量的矩阵求和输出
- en: 'Verify that the tensor addition is as expected by performing the addition of
    each element of the vector:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行对向量中每个元素的加法，验证张量加法是否符合预期：
- en: '[PRE51]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This will result in the following output:'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE52]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: You can see that the `+` operation is equivalent to the element-wise addition
    of the three matrices created.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以看到，`+` 操作等同于对创建的三个矩阵进行逐元素加法。
- en: In this exercise, you successfully performed tensor addition on data representing
    votes cast for political candidates. The transformation can be applied by using
    the `+` operation. You also verified that addition is performed element by element,
    and that one way to ensure that the transformation is valid is for the tensors
    to have the same rank and shape.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，你成功地对代表政治候选人选票的数据进行了张量加法。这个转换可以通过使用`+`操作来实现。你还验证了加法是逐元素进行的，并且确保转换有效的一种方式是确保张量具有相同的秩和形状。
- en: In the following activity, you will further practice tensor addition in TensorFlow.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，你将进一步练习在 TensorFlow 中进行张量加法。
- en: 'Activity 1.01: Performing Tensor Addition in TensorFlow'
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.01：在 TensorFlow 中进行张量加法
- en: You work in a company that has three locations, each with two salespersons and
    each location sells three products. You are required to sum the tensors to represent
    the total revenue for each product across locations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你在一家公司工作，该公司有三个地点，每个地点有两位销售人员，并且每个地点销售三种产品。你需要对张量进行求和，以表示各地点每个产品的总收入。
- en: '![Figure 1.10: Number of different products sold by each salesperson at different
    locations'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：每个销售人员在不同地点销售的不同产品数量'
- en: '](img/B16341_01_10.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_10.jpg)'
- en: 'Figure 1.10: Number of different products sold by each salesperson at different
    locations'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：每个销售人员在不同地点销售的不同产品数量
- en: 'The steps you will take are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 你将采取的步骤如下：
- en: Import the TensorFlow library.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库。
- en: Create two scalars to represent the total revenue for `Product A` by all salespeople
    at `Location X` using TensorFlow's `Variable` class. The first variable will have
    a value of `2706` and the second will have a value of `2386`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类，创建两个标量来表示 `Location X` 上所有销售人员的 `Product A` 总收入。第一个变量的值为
    `2706`，第二个变量的值为 `2386`。
- en: Create a new variable as the sum of the scalars and print the result.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量作为标量的和并打印结果。
- en: 'You should get the following output:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE53]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Create a vector with values `[2706, 2799, 5102]` and a scalar with the value
    `95` using TensorFlow's `Variable` class.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建一个值为 `[2706, 2799, 5102]` 的向量和一个值为 `95` 的标量。
- en: Create a new variable as the sum of the scalar with the vector to represent
    the sales goal for `Salesperson 1` at `Location X` and print the result.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量，将标量与向量相加，表示 `Salesperson 1` 在 `Location X` 的销售目标并打印结果。
- en: 'You should get the following output:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 1.11: The output of the integer-vector summation as a NumPy variable'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.11：整数向量求和的输出，作为 NumPy 变量'
- en: '](img/B16341_01_11.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_11.jpg)'
- en: 'Figure 1.11: The output of the integer-vector summation as a NumPy variable'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.11：整数向量求和的输出，作为 NumPy 变量
- en: Create three tensors with a rank of 2 representing the revenue for each salesperson,
    product, and location using TensorFlow's `Variable` class. The first tensor will
    have the value `[[2706, 2799, 5102], [2386, 4089, 5932]]`, the second will have
    the value `[[5901, 1208, 645], [6235, 1098, 948]]`, and the third will have `[[3908,
    2339, 5520], [4544, 1978, 4729]]`.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Variable` 类创建三个秩为 2 的张量，表示每个销售人员、产品和位置的收入。第一个张量的值为 `[[2706,
    2799, 5102], [2386, 4089, 5932]]`，第二个张量的值为 `[[5901, 1208, 645], [6235, 1098, 948]]`，第三个张量的值为
    `[[3908, 2339, 5520], [4544, 1978, 4729]]`。
- en: 'Create a new variable as the sum of the matrices and print the result:![Figure
    1.12: The output of the matrix summation as a NumPy variable'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新变量作为矩阵的和并打印结果：![图 1.12：矩阵求和的输出，作为 NumPy 变量
- en: '](img/B16341_01_12.jpg)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_12.jpg)'
- en: 'Figure 1.12: The output of the matrix summation as a NumPy variable'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12：矩阵求和的输出，作为 NumPy 变量
- en: Note
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor250).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor250)找到。
- en: In the following section, you will learn how to change a tensor's shape and
    rank.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何改变张量的形状和秩。
- en: Reshaping
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 形状重塑
- en: Some operations, such as addition, can only be applied to tensors if they meet
    certain conditions. Reshaping is one method for modifying the shape of tensors
    so that such operations can be performed. Reshaping takes the elements of a tensor
    and rearranges them into a tensor of a different size. A tensor of any size can
    be reshaped so long as the number of total elements remains the same.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作，如加法，只有在满足特定条件时才能应用于张量。重塑是修改张量形状的一种方法，使得这些操作可以执行。重塑将张量的元素重新排列成一个不同大小的张量。只要总元素的数量保持不变，任何大小的张量都可以进行重塑。
- en: For example, a `(4x3)` matrix can be reshaped into a `(6x2)` matrix since they
    both have a total of `12` elements. The rank, or number, of dimensions, can also
    be changed in the reshaping process. For instance, a `(4x3)` matrix that has a
    rank equal to `2` can be reshaped into a `(3x2x2)` tensor that has a rank equal
    to `3`. The `(4x3)` matrix can also be reshaped into a `(12x1)` vector in which
    the rank has changed from `2` to `1`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 `(4x3)` 矩阵可以被重塑为一个 `(6x2)` 矩阵，因为它们都有 `12` 个元素。秩，即维度的数量，也可以在重塑过程中发生变化。例如，一个秩为
    `2` 的 `(4x3)` 矩阵可以被重塑为一个秩为 `3` 的 `(3x2x2)` 张量。该 `(4x3)` 矩阵也可以被重塑为一个 `(12x1)` 向量，其秩从
    `2` 变为 `1`。
- en: '*Figure 1.13* illustrates tensor reshaping. On the left is a tensor with shape
    `(3x2)`, which can be reshaped to a tensor of shape equal to either `(2x3)`, `(6)`,
    or `(6x1)`. Here, the number of elements, that is, six, has remained constant,
    though the shape and rank of the tensor have changed:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.13* 说明了张量的重塑。左侧是一个形状为 `(3x2)` 的张量，它可以被重塑为形状为 `(2x3)`、`(6)` 或 `(6x1)` 的张量。在这里，元素的数量，即六，保持不变，尽管张量的形状和秩发生了变化：'
- en: '![Figure 1.13: Visual representation of reshaping a (3x2) tensor to tensors
    of different shapes'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：将 (3x2) 张量重塑为不同形状张量的可视化表示'
- en: '](img/B16341_01_13.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_13.jpg)'
- en: 'Figure 1.13: Visual representation of reshaping a (3x2) tensor to tensors of
    different shapes'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：将 (3x2) 张量重塑为不同形状张量的可视化表示
- en: 'Tensor reshaping can be performed in TensorFlow by using the `reshape` function
    and passing in the tensor and the desired shape of the new tensor as the arguments:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 张量的形状重塑可以通过 TensorFlow 的 `reshape` 函数来实现，并将张量和新张量的目标形状作为参数传入：
- en: '[PRE54]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Here, a new tensor is created that has the same elements as the original; however,
    the shape is `[3,2]` instead of `[6]`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，创建了一个新的张量，其元素与原张量相同；然而，形状是`[3,2]`，而不是`[6]`。
- en: The next section introduces tensor transposition, which is another method for
    modifying the shape of a tensor.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分介绍了张量转置，这是另一种修改张量形状的方法。
- en: Tensor Transposition
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量转置
- en: 'When a tensor is transposed, the elements in the tensor are rearranged in a
    specific order. The transpose operation is usually denoted as a `T` superscript
    on the tensor. The new position of each element in the tensor can be determined
    by `(x`12…k`)`T = `x`k…21\. For a matrix or tensor of rank equal to `2`, the rows
    become the columns and vice versa. An example of matrix transposition is shown
    in *Figure 1.14*. Tensors of any rank can be transposed, and often the shape changes
    as a result:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个张量被转置时，张量中的元素会按特定顺序重新排列。转置操作通常用张量上的 `T` 上标表示。张量中每个元素的新位置可以通过 `(x12…k`)`T
    = `xk…21`来确定。对于秩为`2`的矩阵或张量，行变为列，反之亦然。矩阵转置的示例如*图 1.14*所示。任何秩的张量都可以进行转置，且通常会导致形状的变化：
- en: '![Figure 1.14: A visual representation of tensor transposition on a (3x2) matrix'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：在 (3x2) 矩阵上的张量转置的可视化表示'
- en: '](img/B16341_01_14.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_14.jpg)'
- en: 'Figure 1.14: A visual representation of tensor transposition on a (3x2) matrix'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：在 (3x2) 矩阵上的张量转置的可视化表示
- en: 'The following diagram shows the matrix transposition properties of matrices
    `A` and `B`:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了矩阵`A`和`B`的转置特性：
- en: '![Figure 1.15: Tensor transposition properties where X and Y are tensors'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：张量转置特性，其中 X 和 Y 是张量'
- en: '](img/B16341_01_15.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_15.jpg)'
- en: 'Figure 1.15: Tensor transposition properties where X and Y are tensors'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：张量转置特性，其中 X 和 Y 是张量
- en: A tensor is said to be symmetrical if the transpose of a tensor is equivalent
    to the original tensor.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个张量的转置等于原张量，则称该张量为对称的。
- en: 'Tensor transposition can be performed in TensorFlow by using its `transpose`
    function and passing in the tensor as the only argument:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 张量转置可以通过 TensorFlow 的 `transpose` 函数来实现，并将张量作为唯一参数传入：
- en: '[PRE55]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: When transposing a tensor, there is only one possible result; however, reshaping
    a tensor has multiple possible results depending on the desired shape of the output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行张量转置时，只有一个可能的结果；然而，改变张量形状则有多个可能的结果，这取决于输出的目标形状。
- en: In the following exercise, reshaping and transposition are demonstrated on tensors
    using TensorFlow.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下练习中，使用 TensorFlow 演示张量的形状重塑和转置操作。
- en: 'Exercise 1.04: Performing Tensor Reshaping and Transposition in TensorFlow'
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.04：在 TensorFlow 中执行张量重塑和转置
- en: In this exercise, you will learn how to perform tensor reshaping and transposition
    using the TensorFlow library.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将学习如何使用 TensorFlow 库进行张量形状重塑和转置。
- en: 'Perform the following steps:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Import the TensorFlow library and create a matrix with two rows and four columns
    using TensorFlow''s `Variable` class:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库，并使用 TensorFlow 的 `Variable` 类创建一个具有两行四列的矩阵：
- en: '[PRE56]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Verify the shape of the matrix by calling the `shape` attribute of the matrix
    as a Python list:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用矩阵的`shape`属性，将矩阵作为 Python 列表来验证矩阵的形状：
- en: '[PRE57]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This will result in the following output:'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE58]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: You see that the shape of the matrix is `[2,4]`.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会看到矩阵的形状是`[2,4]`。
- en: 'Use TensorFlow''s `reshape` function to change the matrix to a matrix with
    four rows and two columns by passing in the matrix and the desired new shape:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `reshape` 函数，通过传入矩阵和所需的新形状，将矩阵更改为具有四行两列的矩阵：
- en: '[PRE59]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You should get the following output:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 1.16: The reshaped matrix'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.16：重塑后的矩阵'
- en: '](img/B16341_01_16.jpg)'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_16.jpg)'
- en: 'Figure 1.16: The reshaped matrix'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.16：重塑后的矩阵
- en: 'Verify the shape of the reshaped matrix by calling the `shape` attribute as
    a Python list:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `shape` 属性并将其作为 Python 列表来验证重塑后的矩阵的形状：
- en: '[PRE60]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will result in the following output:'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE61]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Here, you can see that the shape of the matrix has changed to your desired shape,
    `[4,2]`.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你可以看到矩阵的形状已经改变为你期望的形状`[4,2]`。
- en: 'Use TensorFlow''s `reshape` function to convert the matrix into a matrix with
    one row and eight columns. Pass the matrix and the desired new shape as parameters
    to the `reshape` function:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `reshape` 函数将矩阵转换为一行八列的矩阵。将矩阵和期望的新形状作为参数传递给 `reshape` 函数：
- en: '[PRE62]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You should get the following output:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE63]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Verify the shape of the reshaped matrix by calling the `shape` attribute as
    a Python list:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `shape` 属性作为 Python 列表来验证重新塑形后的矩阵形状：
- en: '[PRE64]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This will result in the following output:'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE65]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: The preceding output confirms the shape of the reshaped matrix as `[1, 8]`.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出确认了重新塑形矩阵的形状为 `[1, 8]`。
- en: 'Use TensorFlow''s `reshape` function to convert the matrix into a matrix with
    eight rows and one column, passing the matrix and the desired new shape as parameters
    to the `reshape` function:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `reshape` 函数将矩阵转换为八行一列的矩阵，将矩阵和期望的新形状作为参数传递给 `reshape` 函数：
- en: '[PRE66]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You should get the following output:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 1.17: Reshaped matrix of shape (8, 1)'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.17：形状为 (8, 1) 的重新塑形矩阵'
- en: '](img/B16341_01_17.jpg)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_17.jpg)'
- en: 'Figure 1.17: Reshaped matrix of shape (8, 1)'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.17：形状为 (8, 1) 的重新塑形矩阵
- en: 'Verify the shape of the reshaped matrix by calling the `shape` attribute as
    a Python list:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `shape` 属性作为 Python 列表来验证重新塑形后的矩阵形状：
- en: '[PRE67]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This will result in the following output:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE68]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The preceding output confirms the shape of the reshaped matrix as `[8, 1]`.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出确认了重新塑形矩阵的形状为 `[8, 1]`。
- en: 'Use TensorFlow''s `reshape` function to convert the matrix to a tensor of size
    `2x2x2`. Pass the matrix and the desired new shape as parameters to the reshape
    function:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `reshape` 函数将矩阵转换为大小为 `2x2x2` 的张量。将矩阵和期望的新形状作为参数传递给 reshape
    函数：
- en: '[PRE69]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'You should get the following output:'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 1.18: Reshaped matrix of shape (2, 2, 2)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.18：形状为 (2, 2, 2) 的重新塑形矩阵'
- en: '](img/B16341_01_18.jpg)'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_18.jpg)'
- en: 'Figure 1.18: Reshaped matrix of shape (2, 2, 2)'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.18：形状为 (2, 2, 2) 的重新塑形矩阵
- en: 'Verify the shape of the reshaped matrix by calling the `shape` attribute as
    a Python list:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `shape` 属性作为 Python 列表来验证重新塑形后的矩阵形状：
- en: '[PRE70]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'This will result in the following output:'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE71]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The preceding output confirms the shape of the reshaped matrix as `[2, 2, 2]`.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出确认了重新塑形矩阵的形状为 `[2, 2, 2]`。
- en: 'Verify that the rank has changed using TensorFlow''s `rank` function and print
    the result as a NumPy variable:'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `rank` 函数验证秩是否已更改，并将结果作为 NumPy 变量打印：
- en: '[PRE72]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'This will result in the following output:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE73]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Use TensorFlow''s `transpose` function to convert the matrix of size `2X4`
    to a matrix of size `4x2`:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `transpose` 函数将大小为 `2X4` 的矩阵转换为大小为 `4x2` 的矩阵：
- en: '[PRE74]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You should get the following output:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 1.19: Transposed matrix'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.19：转置矩阵'
- en: '](img/B16341_01_19.jpg)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_19.jpg)'
- en: 'Figure 1.19: Transposed matrix'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.19：转置矩阵
- en: 'Verify that the `reshape` function and the `transpose` function create different
    resulting matrices when applied to the given matrix:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 `reshape` 函数和 `transpose` 函数在应用于给定矩阵时，是否会产生不同的结果：
- en: '[PRE75]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '![Figure 1.20: Verification that transposition and reshaping produce different
    results'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.20：验证转置和重新塑形产生不同结果'
- en: '](img/B16341_01_20.jpg)'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_20.jpg)'
- en: 'Figure 1.20: Verification that transposition and reshaping produce different
    results'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.20：验证转置和重新塑形产生不同结果
- en: 'Use TensorFlow''s `transpose` function to transpose the reshaped matrix in
    *step 9*:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `transpose` 函数转置 *步骤 9* 中的重新塑形矩阵：
- en: '[PRE76]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This will result in the following output:'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 1.21: The output of the transposition of the reshaped tensor'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.21：转置后的重新塑形张量的输出'
- en: '](img/B16341_01_21.jpg)'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_21.jpg)'
- en: 'Figure 1.21: The output of the transposition of the reshaped tensor'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：转置后的重新塑形张量的输出
- en: This result shows how the resulting tensor appears after reshaping and transposing
    a tensor.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果展示了在重新塑形和转置张量后，得到的张量的形状。
- en: In this exercise, you have successfully modified the shape of a tensor either
    through reshaping or transposition. You studied how the shape and rank of the
    tensor changes following the reshaping and transposition operation.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，你已成功通过重新塑形或转置操作修改了张量的形状。你研究了在重新塑形和转置操作后，张量的形状和秩如何变化。
- en: In the following activity, you will test your knowledge on how to reshape and
    transpose tensors using TensorFlow.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的活动中，你将测试如何使用 TensorFlow 重新塑形和转置张量的知识。
- en: 'Activity 1.02: Performing Tensor Reshaping and Transposition in TensorFlow'
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动1.02：在TensorFlow中执行张量的重塑和转置
- en: In this activity, you are required to simulate the grouping of 24 school children
    for class projects. The dimensions of each resulting reshaped or transposed tensor
    will represent the size of each group.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，你需要模拟将24名学校儿童分组进行课堂项目。每个重新塑形或转置后的张量的维度将表示每个小组的大小。
- en: 'Perform the following steps:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Import the TensorFlow library.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入TensorFlow库。
- en: Create a one-dimensional tensor with 24 monotonically increasing elements using
    the `Variable` class to represent the IDs of the school children. Verify the shape
    of the matrix.
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Variable`类创建一个包含24个单调递增元素的单维张量，用来表示学校儿童的ID。验证矩阵的形状。
- en: 'You should get the following output:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE77]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Reshape the matrix so that it has 12 rows and 2 columns using TensorFlow's `reshape`
    function representing 12 pairs of school children. Verify the shape of the new
    matrix.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow的`reshape`函数将矩阵重塑为12行2列，表示12对学校儿童。验证新矩阵的形状。
- en: 'You should get the following output:'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE78]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Reshape the original matrix so that it has a shape of `3x4x2` using TensorFlow's
    `reshape` function representing 3 groups of 4 sets of pairs of school children.
    Verify the shape of the new tensor.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow的`reshape`函数将原始矩阵重塑为`3x4x2`的形状，表示3组由4对学校儿童组成的小组。验证新张量的形状。
- en: 'You should get the following output:'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE79]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: Verify that the rank of this new tensor is `3`.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证新张量的秩为`3`。
- en: Transpose the tensor created in *step 3* to represent 2 groups of 12 students
    using TensorFlow's `transpose` function. Verify the shape of the new tensor.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*第3步*中创建的张量进行转置，使用TensorFlow的`transpose`函数表示12名学生的2组。验证新张量的形状。
- en: 'You should get the following output:'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE80]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Note
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor252).
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor252)找到。
- en: In this section, you were introduced to some of the basic components of ANNs—tensors.
    You also learned about some basic manipulation of tensors, such as addition, transposition,
    and reshaping. You implemented these concepts by using functions in the TensorFlow
    library.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你被介绍了一些人工神经网络（ANNs）的基本组件——张量。你还学习了张量的一些基本操作，如加法、转置和重新塑形。你通过使用TensorFlow库中的函数实现了这些概念。
- en: In the next topic, you will extend your understanding of linear transformations
    by covering another important transformation related to ANNs—tensor multiplication.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一主题中，你将通过学习与人工神经网络（ANNs）相关的另一个重要变换——张量乘法，来扩展你对线性变换的理解。
- en: Tensor Multiplication
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量乘法
- en: Tensor multiplication is another fundamental operation that is used frequently
    in the process of building and training ANNs since information propagates through
    the network from the inputs to the result via a series of additions and multiplications.
    While the rules for addition are simple and intuitive, the rules for tensors are
    more complex. Tensor multiplication involves more than simple element-wise multiplication
    of the elements. Rather, a more complicated procedure is implemented that involves
    the dot product between the entire rows/columns of each of the tensors to calculate
    each element of the resulting tensor. This section will explain how multiplication
    works for two-dimensional tensors or matrices. However, tensors of higher orders
    can also be multiplied.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 张量乘法是另一个在构建和训练人工神经网络（ANNs）过程中常用的基本操作，因为信息通过网络从输入传递到结果，过程中涉及一系列加法和乘法。加法的规则简单且直观，但张量的规则则更加复杂。张量乘法不仅仅是元素之间的逐元素相乘，而是通过计算每个张量的整行/整列的点积来计算结果张量的每个元素。这一部分将解释二维张量或矩阵的乘法是如何工作的。但更高阶的张量也可以进行乘法操作。
- en: Given a matrix, `X = [x`ij`]`m x n, and another matrix, `Y = [y`ij`]`n x p,
    the product of the two matrices is `Z = XY = [z`ij`]`m x p, and each element,
    `z`ij, is defined element-wise as ![Formula](img/B16341_01_21a.png). The shape
    of the resultant matrix is the same as the outer dimensions of the matrix product,
    or the number of rows of the first matrix and the number of columns of the second
    matrix. For the multiplication to work, the inner dimensions of the matrix product
    must match, or the number of columns in the first matrix and the number of columns
    in the second matrix must correspond.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 给定矩阵 `X = [x`ij`]`m x n`，和另一个矩阵 `Y = [y`ij`]`n x p`，这两个矩阵的乘积是 `Z = XY = [z`ij`]`m
    x p`，每个元素 `z`ij` 都按元素定义如 ![Formula](img/B16341_01_21a.png)。结果矩阵的形状与矩阵乘积的外部维度相同，即第一个矩阵的行数和第二个矩阵的列数。为了使乘法有效，矩阵乘积的内部维度必须匹配，即第一个矩阵的列数和第二个矩阵的行数必须相对应。
- en: 'The concept of inner and outer dimensions of matrix multiplication is shown
    in the following diagram, where `X` represents the first matrix and `Y` represents
    the second matrix:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法的内部和外部维度的概念显示在下图中，其中 `X` 表示第一个矩阵，`Y` 表示第二个矩阵：
- en: '![Figure 1.22: A visual representation of inner and outer dimensions in matrix
    multiplication'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.22: 矩阵乘法中内部和外部维度的可视化表示'
- en: '](img/B16341_01_22.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_22.jpg)'
- en: 'Figure 1.22: A visual representation of inner and outer dimensions in matrix
    multiplication'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1.22: 矩阵乘法中内部和外部维度的可视化表示'
- en: 'Unlike matrix addition, matrix multiplication is not commutative, which means
    that the order of the matrices in the product matters:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 与矩阵加法不同，矩阵乘法不是交换的，这意味着矩阵在乘积中的顺序很重要：
- en: '![Figure 1.23: Matrix multiplication is non-commutative'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.23: 矩阵乘法是非交换的'
- en: '](img/B16341_01_23.jpg)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_23.jpg)'
- en: 'Figure 1.23: Matrix multiplication is non-commutative'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1.23: 矩阵乘法是非交换的'
- en: 'For example, say you have the following two matrices:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有以下两个矩阵：
- en: '![Figure 1.24: Two matrices, X and Y'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.24: 两个矩阵 X 和 Y'
- en: '](img/B16341_01_24.jpg)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_24.jpg)'
- en: 'Figure 1.24: Two matrices, X and Y'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1.24: 两个矩阵 X 和 Y'
- en: 'One way to construct the product is to have matrix `X` first, multiplied by
    `Y`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 构建乘积的一种方式是首先有矩阵 `X`，然后乘以 `Y`：
- en: '![Figure 1.25: Visual representation of matrix X multiplied by Y, X•Y'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.25: 矩阵 X 乘以 Y 的可视化表示，X•Y'
- en: '](img/B16341_01_25.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_25.jpg)'
- en: 'Figure 1.25: Visual representation of matrix X multiplied by Y, X•Y'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1.25: 矩阵 X 乘以 Y 的可视化表示，X•Y'
- en: 'This results in a `2x2` matrix. Another way to construct the product is to
    have `Y` first, multiplied by `X`:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致一个 `2x2` 的矩阵。构建乘积的另一种方式是首先有 `Y`，然后乘以 `X`：
- en: '![Figure 1.26: Visual representation of matrix Y multiplied by X, Y•X'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 1.26: 矩阵 Y 乘以 X 的可视化表示，Y•X'
- en: '](img/B16341_01_26.jpg)'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_26.jpg)'
- en: 'Figure 1.26: Visual representation of matrix Y multiplied by X, Y•X'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 1.26: 矩阵 Y 乘以 X 的可视化表示，Y•X'
- en: Here you can see that the matrix formed from the product `YX` is a `3x3` matrix
    and is very different from the matrix formed from the product `XY`.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你可以看到从乘积 `YX` 形成的矩阵是一个 `3x3` 的矩阵，与从乘积 `XY` 形成的矩阵非常不同。
- en: 'Tensor multiplication can be performed in TensorFlow by using the `matmul`
    function and passing in the tensors to be multiplied in the order in which they
    are to be multiplied as the arguments:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，可以通过使用 `matmul` 函数执行张量乘法，并按照它们需要被乘的顺序将张量作为参数传递进去：
- en: '[PRE81]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Tensor multiplication can also be achieved by using the `@` operator as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 张量乘法也可以通过使用 `@` 运算符来实现，如下所示：
- en: '[PRE82]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Scalar-tensor multiplication is much more straightforward and is simply the
    product of every element in the tensor multiplied by the scalar so that `λX =
    [λx`ij…k`]`, where `λ` is a scalar and `X` is a tensor.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 标量-张量乘法要简单得多，只是张量中每个元素乘以标量，因此 `λX = [λx`ij…k`]`，其中 `λ` 是标量，`X` 是张量。
- en: 'Scalar multiplication can be achieved in TensorFlow either by using the `matmul`
    function or by using the `*` operator:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，可以通过使用 `matmul` 函数或使用 `*` 运算符来实现标量乘法：
- en: '[PRE83]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: In the following exercise, you will perform tensor multiplication using the
    TensorFlow library.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的练习中，您将使用 TensorFlow 库执行张量乘法。
- en: 'Exercise 1.05: Performing Tensor Multiplication in TensorFlow'
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 1.05：在 TensorFlow 中执行张量乘法
- en: In this exercise, you will perform tensor multiplication in TensorFlow using
    TensorFlow's `matmul` function and the `@` operator. In this exercise, you will
    use the example of data from a sandwich retailer representing the ingredients
    of various sandwiches and the costs of different ingredients. You will use matrix
    multiplication to determine the costs of each sandwich.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你将使用 TensorFlow 的 `matmul` 函数和 `@` 运算符执行张量乘法。本练习将以三明治零售商的数据为例，表示不同三明治的配料及配料的成本。你将使用矩阵乘法来确定每个三明治的成本。
- en: '**Sandwich recipe**:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '**三明治食谱**：'
- en: '![Figure 1.27: Sandwich recipe'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27：三明治食谱'
- en: '](img/B16341_01_27.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_27.jpg)'
- en: 'Figure 1.27: Sandwich recipe'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27：三明治食谱
- en: '**Ingredient details**:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '**配料详情**：'
- en: '![Figure 1.28: Ingredient details'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.28：配料详情'
- en: '](img/B16341_01_28.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_28.jpg)'
- en: 'Figure 1.28: Ingredient details'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.28：配料详情
- en: '**Sales projections**:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '**销售预测**：'
- en: '![Figure 1.29: Sales projections'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.29：销售预测'
- en: '](img/B16341_01_29.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_29.jpg)'
- en: 'Figure 1.29: Sales projections'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.29：销售预测
- en: 'Perform the following steps:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Import the TensorFlow library:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库：
- en: '[PRE84]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Create a matrix representing the different sandwich recipes, with the rows
    representing the three different sandwich offerings and the columns representing
    the combination and number of the five different ingredients using the `Variable`
    class:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个矩阵表示不同的三明治食谱，行代表三种不同的三明治种类，列代表五种不同配料的组合和数量，使用`Variable`类：
- en: '[PRE85]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'You should get the following output:'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 1.30: Matrix representing the number of ingredients needed to make
    sandwiches'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.30：表示制作三明治所需配料数量的矩阵'
- en: '](img/B16341_01_30.jpg)'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_30.jpg)'
- en: 'Figure 1.30: Matrix representing the number of ingredients needed to make sandwiches'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.30：表示制作三明治所需配料数量的矩阵
- en: 'Verify the shape of the matrix by calling the `shape` attribute of the matrix
    as a Python list:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用矩阵的`shape`属性作为 Python 列表，验证矩阵的形状：
- en: '[PRE86]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'This will result in the following output:'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE87]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Create a second matrix representing the cost and weight of each individual
    ingredient in which the rows represent the five ingredients, and the columns represent
    the cost and weight of the ingredients in grams:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个第二个矩阵，表示每种配料的成本和重量，其中行代表五种配料，列代表配料的成本和重量（单位：克）：
- en: '[PRE88]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'You should get the following result:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下结果：
- en: '![Figure 1.31: A matrix representing the cost and weight of each ingredient'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.31：表示每种配料的成本和重量的矩阵'
- en: '](img/B16341_01_31.jpg)'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_31.jpg)'
- en: 'Figure 1.31: A matrix representing the cost and weight of each ingredient'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.31：表示每种配料的成本和重量的矩阵
- en: 'Use TensorFlow''s `matmul` function to perform the matrix multiplication of
    `matrix1` and `matrix2`:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的`matmul`函数执行`matrix1`和`matrix2`的矩阵乘法：
- en: '[PRE89]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'This will result in the following output:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 1.32: The output of the matrix multiplication'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.32：矩阵乘法的输出'
- en: '](img/B16341_01_32.jpg)'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_32.jpg)'
- en: 'Figure 1.32: The output of the matrix multiplication'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.32：矩阵乘法的输出
- en: 'Create a matrix to represent the sales projections of five different stores
    for each of the three sandwiches:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个矩阵来表示五家不同商店对三种三明治的销售预测：
- en: '[PRE90]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Multiply `matrix3` by the result of the matrix multiplication of `matrix1`
    and `matrix2` to give the expected cost and weight for each of the five stores:'
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`matrix3`与`matrix1`和`matrix2`相乘的结果相乘，以给出每家五家商店的预期成本和重量：
- en: '[PRE91]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'This will result in the following output:'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 1.33: The output of matrix multiplication'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.33：矩阵乘法的输出'
- en: '](img/B16341_01_33.jpg)'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_33.jpg)'
- en: 'Figure 1.33: The output of matrix multiplication'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.33：矩阵乘法的输出
- en: The resulting tensor from the multiplication shows the expected cost of sandwiches
    and the expected weight of the total ingredients for each of the stores.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法得到的张量显示了每家商店三明治的预期成本以及总配料的预期重量。
- en: In this exercise, you have successfully learned how to perform matrix multiplication
    in TensorFlow using several operators. You used TensorFlow's `matmul` function,
    as well as the shorthand `@` operator. Each will perform the multiplication; however,
    the `matmul` function has several different arguments that can be passed into
    the function that make it more flexible.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，你已经成功学会了如何使用几个操作符在TensorFlow中进行矩阵乘法。你使用了TensorFlow的`matmul`函数以及简写符号`@`操作符。它们都能执行乘法操作；不过，`matmul`函数有多个不同的参数，可以传入，使其更加灵活。
- en: Note
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about the `matmul` function here: [https://www.tensorflow.org/api_docs/python/tf/linalg/matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于`matmul`函数的内容：[https://www.tensorflow.org/api_docs/python/tf/linalg/matmul](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul)。
- en: In the next section, you will explore some other mathematical concepts that
    are related to ANNs. You will explore forward and backpropagation, as well as
    activation functions.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将探索一些与ANN相关的其他数学概念。你将探索前向传播和反向传播，以及激活函数。
- en: Optimization
  id: totrans-469
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化
- en: In this section, you will learn about some optimization approaches that are
    fundamental to training machine learning models. Optimization is the process by
    which the weights of the layers of an ANN are updated such that the error between
    the predicted values of the ANN and the true values of the training data is minimized.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习一些对于训练机器学习模型至关重要的优化方法。优化是通过更新ANN各层的权重，使得ANN预测值与训练数据的真实值之间的误差最小化的过程。
- en: Forward Propagation
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: Forward propagation is the process by which information propagates through ANNs.
    Operations such as a series of tensor multiplications and additions occur at each
    layer of the network until the final output. Forward propagation is explained
    in *Figure 1.37*, showing a single hidden layer ANN. The input data has two features,
    while the output layer has a single value for each input record.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播是信息在人工神经网络（ANN）中传播的过程。在网络的每一层都会发生一系列张量乘法和加法操作，直到最终输出。前向传播在*图1.37*中进行了说明，展示了一个单隐层的ANN。输入数据有两个特征，而输出层对每个输入记录只有一个值。
- en: The weights and biases for the hidden layer and output are shown as matrices
    and vectors with the appropriate indexes. For the hidden layer, the number of
    rows in the weight matrix is equal to the number of features of the input, and
    the number of columns is equal to the number of units in the hidden layer. Therefore,
    `W1` has two rows and three columns because the input, `X`, has two features.
    Likewise, `W2` has three rows and one column, the hidden layer has three units,
    and the output has the size one. The bias, however, is always a vector with a
    size equal to the number of nodes in that layer and is added to the product of
    the input and weight matrix.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层和输出层的权重和偏差显示为具有适当索引的矩阵和向量。对于隐藏层，权重矩阵的行数等于输入特征的数量，列数等于隐藏层单元的数量。因此，`W1`有两行三列，因为输入`X`有两个特征。同样，`W2`有三行一列，隐藏层有三个单元，输出层的大小为一。然而，偏差始终是一个大小等于该层节点数量的向量，并与输入和权重矩阵的乘积相加。
- en: '![Figure 1.34: A single-layer artificial neural network'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.34：单层人工神经网络'
- en: '](img/B16341_01_34.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_34.jpg)'
- en: 'Figure 1.34: A single-layer artificial neural network'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.34：单层人工神经网络
- en: 'The steps to perform forward propagation are as follows:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前向传播的步骤如下：
- en: '`X` is the input to the network and the input to the hidden layer. First, the
    input matrix, `X`, is multiplied by the weight matrix for the hidden layer, `W1`,
    and then the bias, `b1`, is added:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`X`是网络的输入，也是隐藏层的输入。首先，输入矩阵`X`与隐藏层的权重矩阵`W1`相乘，然后加上偏差`b1`：'
- en: '`z1 = X*W1 + b1`'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`z1 = X*W1 + b1`'
- en: Here is an example of what the shape of the resulting tensor will be after the
    operation. If the input is size `nX2`, where `n` is the number of input examples,
    `W1` is of size `2X3`, and `b1` is of size `1X3`, the resulting matrix, `z1`,
    will have a size of `nX3`.
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是操作后结果张量的形状示例。如果输入的大小为`nX2`，其中`n`是输入示例的数量，`W1`的大小为`2X3`，`b1`的大小为`1X3`，则结果矩阵`z1`的大小将为`nX3`。
- en: '`z1` is the output of the hidden layer, which is the `W2`, and the bias, `b2`,
    is added:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`z1`是隐藏层的输出，它是`W2`与偏差`b2`相加后的结果：'
- en: '`Y = z1 * W2 + b2`'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`Y = z1 * W2 + b2`'
- en: To understand the shape of the resulting tensor, consider the following example.
    If the input to the output layer, `z1`, is of size `nX3`, `W2` is of size `3X1`,
    and `b1` is of size `1X1`, the resulting matrix, `Y`, will have a size of `nX1`,
    representing one result for each training example.
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了理解最终张量的形状，考虑以下示例。如果输出层的输入`z1`大小为`nX3`，`W2`的大小为`3X1`，`b1`的大小为`1X1`，则结果矩阵`Y`的大小将为`nX1`，表示每个训练样本的一个结果。
- en: The total number of parameters in this model is equal to the sum of the number
    of elements in `W1`, `W2`, `b1`, and `b2`. Therefore, the number of parameters
    can be calculated by summing the elements in each of the parameters in weight
    matrices and biases, which is equal to `6 + 3 + 3 + 1 = 13`. These are the parameters
    that need to be learned in the process of training the ANN.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型中的总参数数量等于`W1`、`W2`、`b1`和`b2`中元素的总和。因此，可以通过求和每个权重矩阵和偏置参数中的元素来计算参数的数量，计算结果为`6
    + 3 + 3 + 1 = 13`。这些是需要在训练人工神经网络（ANN）过程中学习的参数。
- en: Following the forward propagation step, you must evaluate your model and compare
    it to the real target values. This is achieved using a loss function. Mean squared
    error, that is, the mean value of the squared difference between true and predicted
    values, is one of the examples of the loss function of the regression task. Once
    the loss is calculated, the weights must be updated to reduce the loss, and the
    amount and direction that the weights should be updated are found using backpropagation.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播步骤之后，你必须评估你的模型，并将其与实际目标值进行比较。这是通过使用损失函数来实现的。均方误差（Mean Squared Error，MSE），即真实值和预测值之间的平方差的均值，是回归任务中损失函数的一个例子。一旦计算出损失，就需要更新权重以减少损失，并且通过反向传播找出更新权重的量和方向。
- en: Backpropagation
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: '`loss` function to the predicted outputs as follows:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss` 函数对预测输出的影响如下：'
- en: '`loss = L(y_predicted)`'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss = L(y_predicted)`'
- en: The derivative of the loss with respect to the model parameters will inform
    you if increasing or decreasing the model parameter will result in increasing
    or decreasing the loss. The process of backpropagation is achieved by applying
    the chain rule of calculus from the output layer to the input layer of a neural
    network, at each layer computing the derivatives of the `loss` function with respect
    to the model parameters.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数对模型参数的导数可以告诉你，增加或减少模型参数是否会导致损失增加或减少。反向传播的过程通过应用微积分的链式法则，从神经网络的输出层到输入层，在每一层计算损失函数对模型参数的导数来实现。
- en: 'The chain rule of calculus is a technique used to compute the derivative of
    a composite function via intermediate functions. A generalized version of the
    function can be written as follows:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分的链式法则是一种通过中间函数计算复合函数导数的技术。该函数的广义版本可以写作如下：
- en: '`dz/dx = dz/dy * dy/dx`'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '`dz/dx = dz/dy * dy/dx`'
- en: Here, `dz/dx` is the composite function and `y` is the intermediate function.
    In the case of ANNs, the composite function is the loss as a function of the model
    parameters and the intermediate functions represent the hidden layers. Therefore,
    the derivative of the loss with respect to the model parameters can be computed
    by multiplying the derivative of the loss with respect to the predicted output
    by the derivative of the predicted output with respect to the model parameters.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`dz/dx`是复合函数，`y`是中间函数。在人工神经网络中，复合函数是损失函数与模型参数的关系，中间函数则表示隐藏层。因此，损失函数对模型参数的导数可以通过将损失对预测输出的导数与预测输出对模型参数的导数相乘来计算。
- en: In the next section, you will learn how the weight parameters are updated given
    the derivatives of the loss function with respect to each of the weights so that
    the loss is minimized.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将学习如何根据损失函数对每个权重的导数来更新权重参数，以最小化损失。
- en: Learning Optimal Parameters
  id: totrans-494
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习最优参数
- en: In this section, you will see how optimal weights are iteratively chosen. You
    know that forward propagation transfers information through the network via a
    series of tensor additions and multiplications, and that backpropagation is the
    process of understanding the change in loss with respect to each model weight.
    The next step is to use the results from backpropagation to update the weights
    so that they reduce the error according to the loss function. This process is
    known as learning the parameters and is achieved using an optimization algorithm.
    A common optimization algorithm often utilized is called **gradient descent**.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将看到如何迭代选择最优权重。您知道前向传播通过一系列张量加法和乘法在网络中传输信息，而反向传播是理解每个模型权重变化对损失的过程。下一步是使用反向传播的结果更新权重，以便根据损失函数减少误差。这个过程称为学习参数，使用优化算法实现。一个常用的优化算法通常称为**梯度下降**。
- en: In learning the optimal parameters, you apply the optimization algorithm until
    a minimum in the loss function is reached. You usually stop after a given number
    of steps or when there is a negligible change in the loss function. If you plot
    the loss as a function of each model parameter, the shape of the loss function
    resembles a convex shape, having only one minimum, and it is the goal of the optimization
    function to find this minimum.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习最优参数时，应用优化算法直到损失函数达到最小值。通常在给定步数之后停止或在损失函数变化微不足道时停止。如果将损失作为每个模型参数的函数绘制，损失函数的形状类似于一个凸形，只有一个最小值，优化函数的目标是找到这个最小值。
- en: 'The following figure shows the loss function of a particular feature:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了特定特征的损失函数：
- en: '![Figure 1.35: A visual representation of the gradient descent algorithm finding'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.35：梯度下降算法找到的可视化表示'
- en: the optimal parameter to minimize the loss
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 最优参数以最小化损失
- en: '](img/B16341_01_35.jpg)'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_35.jpg)'
- en: 'Figure 1.35: A visual representation of the gradient descent algorithm finding
    the optimal parameter to minimize the loss'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.35：梯度下降算法找到的最优参数以最小化损失
- en: This is achieved, first, by randomly setting parameters for each weight, indicated
    by `p`1 in the diagram. The loss is then calculated for that model parameter,
    `l`1\. The backpropagation step determines the derivative of the loss with respect
    to the model parameter and will determine in which direction the model should
    be updated. The next model parameter, `p`2, is equal to the current model parameter
    minus the learning rate (`α`) multiplied by the derivative value. The learning
    rate is a hyperparameter that is set before the model training process. By multiplying
    by the derivative value, larger steps will be taken when the parameter is far
    from the minimum where the absolute value for the derivative is larger. The loss,
    `l`2, is then calculated and the process continues until the minimum loss is reached,
    `l`m, with the optimal parameter, `p`m.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过随机设置每个权重的参数（在图中表示为 `p`1）。然后计算该模型参数的损失，记为 `l`1\. 后向传播步骤确定了损失相对于模型参数的导数，并确定了模型应更新的方向。下一个模型参数
    `p`2 等于当前模型参数减去学习率（`α`）乘以导数值。学习率是在模型训练过程之前设置的超参数。通过导数值的乘积，当参数远离导数绝对值较大的最小值时，会采取更大的步长。然后计算损失
    `l`2，并继续该过程，直到达到最小损失 `l`m，并找到最优参数 `p`m。
- en: 'To summarize, these are the iterative steps that the optimization algorithm
    performs to find the optimal parameters:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这些是优化算法执行以找到最优参数的迭代步骤：
- en: Use forward propagation and current parameters to predict the outputs for the
    entire dataset.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用前向传播和当前参数预测整个数据集的输出。
- en: Apply the loss function to compute the loss over all the examples from the predicted
    output.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用损失函数计算预测输出中所有示例的损失。
- en: Use backpropagation to compute the derivatives of the loss with respect to the
    weights and biases at each layer.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播计算每层权重和偏差对损失的导数。
- en: Update the weights and biases using the derivative values and the learning rate.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用导数值和学习率更新权重和偏差。
- en: Optimizers in TensorFlow
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 中的优化器
- en: 'There are several different optimizers readily available within TensorFlow.
    Each is based on a different optimization algorithm that aims to reach a global
    minimum for the loss function. They are all based on the gradient descent algorithm,
    although they differ slightly in implementation. The available optimizers in TensorFlow
    include the following:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中有几种不同的优化器，每个优化器都基于不同的优化算法，旨在找到损失函数的全局最小值。它们都基于梯度下降算法，尽管在实现上有所不同。TensorFlow中可用的优化器包括以下几种：
- en: '**Stochastic Gradient Descent** (**SGD**): The SGD algorithm applies gradient
    descent to small batches of training data. A momentum parameter is also available
    when using the optimizer in TensorFlow that applies exponential smoothing to the
    computed gradient to speed up training.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）：SGD算法将梯度下降应用于小批次的训练数据。在使用TensorFlow中的优化器时，还可以使用一个动量参数，它通过对计算出的梯度进行指数平滑，从而加速训练过程。'
- en: '**Adam**: This optimization is an SGD method that is based on the continuous
    adaptive estimation of first and second-order moments.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**：这是一种基于连续自适应估计一阶和二阶矩的SGD方法。'
- en: '**Root Mean Squared Propagation** (**RMSProp**): This is an unpublished, adaptive
    learning rate optimizer. RMSprop divides the learning rate by an average of the
    squared gradients when finding the loss minimum after each step, which results
    in a learning rate that exponentially decays.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根传播**（**RMSProp**）：这是一种未公开的自适应学习率优化器。RMSProp在每步找到损失最小值时，将学习率除以梯度平方的平均值，这样得到的学习率呈指数衰减。'
- en: '**Adagrad**: This optimizer has parameter-specific learning rates that are
    updated depending on how frequently the parameter is updated during the training
    process. As the parameter receives more updates, each subsequent update is smaller
    in value.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adagrad**：该优化器具有参数特定的学习率，学习率会根据参数在训练过程中更新的频率进行调整。当参数收到更多更新时，每次更新的幅度会变小。'
- en: 'The choice of optimizer will affect training time and model performance. Each optimizer
    also has hyperparameters, such as the initial learning rate, that must be selected
    before training, and tuning of these hyperparameters will also affect training
    time and model performance. While other optimizers available in TensorFlow are
    not explicitly stated here (and can be found here: [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)),
    those stated above perform well both in terms of training time and model performance
    and are a safe first choice when selecting an optimizer for your model. The optimizers
    available in TensorFlow are located in the `tf.optimizers` module; for example,
    an Adam optimizer with a learning rate equal to `0.001` can be initialized as
    follows:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器的选择会影响训练时间和模型性能。每个优化器都有超参数，比如初始学习率，这些超参数必须在训练前进行选择，而调优这些超参数也会影响训练时间和模型性能。虽然TensorFlow中还有其他优化器未在此明确列出（可以在此处找到：[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)），但上述优化器在训练时间和模型性能上表现良好，是选择优化器时的一个安全的首选。TensorFlow中可用的优化器位于`tf.optimizers`模块中；例如，一个学习率为`0.001`的Adam优化器可以如下初始化：
- en: '[PRE92]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: In this topic, you have seen the steps taken in achieving gradient descent to
    compute the optimal parameters for model training. In gradient descent, every
    single training example is used to learn the parameters. However, when working
    with large volume datasets, such as with images and audio, you will often work
    in batches and make updates after learning from each batch. When using gradient
    descent on batch data, the algorithm is known as SGD. The SGD optimizer, along
    with a suite of other performant optimizers, is readily available in TensorFlow,
    including the Adam, RMSProp, and Adagrad optimizers, and more.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已经看到了实现梯度下降的步骤，用以计算模型训练的最优参数。在梯度下降中，每一个训练样本都会用于学习这些参数。然而，在处理大规模数据集时，比如图像和音频数据，通常会采用批量训练，并在每次学习完一个批次后进行更新。当使用梯度下降进行批量数据训练时，这个算法被称为SGD。TensorFlow中提供了SGD优化器以及一系列其他高效的优化器，包括Adam、RMSProp、Adagrad优化器等。
- en: In the next section, you will explore different activation functions, which
    are generally applied to the output of each layer.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将探索不同的激活函数，这些函数通常应用于每一层的输出。
- en: Activation functions
  id: totrans-518
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Activation functions are mathematical functions that are generally applied
    to the outputs of ANN layers to limit or bound the values of the layer. The reason
    that values may want to be bounded is that without activation functions, the value
    and corresponding gradients can either explode or vanish, thereby making the results
    unusable. This is because the final value is the cumulative product of the values
    from each subsequent layer. As the number of layers increases, the likelihood
    of values and gradients exploding to infinity or vanishing to zero increases.
    This concept is known as the **exploding and vanishing gradient problem**. Deciding
    whether a node in a layer should be *activated* is another use of activation functions,
    hence their name. Common activation functions and their visual representation
    in *Figure 1.36* are as follows:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是应用于ANN层输出的数学函数，通常用于限制或约束层的值。值可能需要被约束的原因是，如果没有激活函数，值和相应的梯度可能会爆炸或消失，从而导致结果不可用。这是因为最终值是每个后续层值的累积乘积。随着层数的增加，值和梯度爆炸到无穷大或消失到零的可能性增加。这个概念被称为**梯度爆炸和消失问题**。决定一个层中的节点是否应被*激活*是激活函数的另一个用途，这也就是它们的名字的由来。常见的激活函数及其在*图
    1.36*中的可视化表示如下：
- en: '**Step** function: The value is non-zero if it is above a certain threshold,
    otherwise it is zero. This is shown in *Figure 1.36a*.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶跃**函数：如果值超过某一阈值，则值为非零，否则为零。此内容如*图 1.36a*所示。'
- en: '**Linear** function: ![Formula](img/B16341_01_35a.png), which is a scalar multiplication
    of the input value. This is shown in *Figure 1.36b*.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**函数：![公式](img/B16341_01_35a.png)，它是输入值的标量乘积。此内容如*图 1.36b*所示。'
- en: '**Sigmoid** function: ![Formula](img/B16341_01_35b.png), like a smoothed-out
    step function with smooth gradients. This activation function is useful for classification
    since the values are bound from zero to one. This is shown in *Figure 1.36c*.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**函数：![公式](img/B16341_01_35b.png)，类似于一个平滑的阶跃函数，具有平滑的梯度。此激活函数对于分类非常有用，因为其值被限制在0到1之间。此内容如*图
    1.36c*所示。'
- en: '`x=0`. This is shown in *Figure 1.36d*.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x=0`。此内容如*图 1.36d*所示。'
- en: '`0`. This is shown in *Figure 1.36e*.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`。此内容如*图 1.36e*所示。'
- en: '**ELU** (**Exponential Linear Unit**) function: ![Formula](img/B16341_01_35e.png),
    otherwise ![Formula](img/B16341_01_35f.png), where ![Formula](img/B16341_01_35g.png)
    is a constant.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ELU**（**指数线性单元**）函数：![公式](img/B16341_01_35e.png)，否则![公式](img/B16341_01_35f.png)，其中![公式](img/B16341_01_35g.png)是常数。'
- en: '**SELU** (**Scaled Exponential Linear Unit**) function: ![Formula](img/B16341_01_35h.png),
    otherwise ![Formula](img/B16341_01_35i.png), where ![Formula](img/B16341_01_35j.png)are
    constants. This is shown in *Figure 1.36f*.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SELU**（**缩放指数线性单元**）函数：![公式](img/B16341_01_35h.png)，否则![公式](img/B16341_01_35i.png)，其中![公式](img/B16341_01_35j.png)是常数。此内容如*图
    1.36f*所示。'
- en: '**Swish** function: ![Formula](img/B16341_01_35k.png). This is shown in *Figure
    1.36g*:'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swish**函数：![公式](img/B16341_01_35k.png)。此内容如*图 1.36g*所示：'
- en: '![Figure 1.36: A visual representation of the common activation functions'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.36：常见激活函数的可视化表示'
- en: '](img/B16341_01_36.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_36.jpg)'
- en: 'Figure 1.36: A visual representation of the common activation functions'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.36：常见激活函数的可视化表示
- en: 'An activation function can be applied to any tensor by utilizing the activation
    functions in the `tf.keras.activations` module. For example, a sigmoid activation
    function can be applied to a tensor as follows:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过利用`tf.keras.activations`模块中的激活函数，将激活函数应用于任何张量。例如，sigmoid激活函数可以应用于一个张量，如下所示：
- en: '[PRE93]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Now, let's test the knowledge that you have gained so far in the following activity.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过以下活动测试你到目前为止所获得的知识。
- en: 'Activity 1.03: Applying Activation Functions'
  id: totrans-534
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 1.03：应用激活函数
- en: In this activity, you will recall many of the concepts used throughout the chapter
    as well as apply activation functions to tensors. You will use example data of
    car dealership sales, apply these concepts, show the sales records of various
    salespeople, and highlight those with net positive sales.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，你将回顾本章中使用的许多概念，并将激活函数应用于张量。你将使用汽车销售数据示例，应用这些概念，展示各个销售人员的销售记录，并突出显示那些有净正销售的记录。
- en: '**Sales records**:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '**销售记录**：'
- en: '![Figure 1.37: Sales records'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.37：销售记录'
- en: '](img/B16341_01_37.jpg)'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_37.jpg)'
- en: 'Figure 1.37: Sales records'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.37：销售记录
- en: '**Vehicle MSRPs**:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '**车辆MSRP**：'
- en: '![Figure 1.38: Vehicle MSRPs'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.38：车辆MSRP'
- en: '](img/B16341_01_38.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_38.jpg)'
- en: 'Figure 1.38: Vehicle MSRPs'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.38：车辆MSRP
- en: '**Fixed costs**:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '**固定成本**：'
- en: '![Figure 1.39: Fixed costs'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.39：固定成本'
- en: '](img/B16341_01_39.jpg)'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16341_01_39.jpg)'
- en: 'Figure 1.39: Fixed costs'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.39：固定成本
- en: 'Perform the following steps:'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: Import the TensorFlow library.
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 库。
- en: Create a `3x4` tensor as an input with the values `[[-0.013, 0.024, 0.06, 0.022],
    [0.001, -0.047, 0.039, 0.016], [0.018, 0.030, -0.021, -0.028]]`. The rows in this
    tensor represent the sales of various sales representatives, the columns represent
    various vehicles available at the dealership, and values represent the average
    percentage difference from MSRP. The values are positive or negative depending
    on whether the salesperson was able to sell for more or less than the MSRP.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `3x4` 的张量作为输入，值为 `[[-0.013, 0.024, 0.06, 0.022], [0.001, -0.047, 0.039,
    0.016], [0.018, 0.030, -0.021, -0.028]]`。这个张量的行表示不同销售代表的销售情况，列表示经销商提供的各种车辆，而值则表示与建议零售价（MSRP）的平均百分比差异。根据销售员是否能以高于或低于
    MSRP 的价格售出车辆，数值为正或负。
- en: Create a `4x1` weights tensor with the shape `4x1` with the values `[[19995.95],
    [24995.50], [36745.50], [29995.95]]` representing the MSRP of the cars.
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `4x1` 的权重张量，形状为 `4x1`，值为 `[[19995.95], [24995.50], [36745.50], [29995.95]]`，表示汽车的
    MSRP。
- en: Create a bias tensor of size `3x1` with the values `[[-2500.0], [-2500.0], [-2500.0]]`
    representing the fixed costs associated with each salesperson.
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个大小为 `3x1` 的偏置张量，值为 `[[-2500.0], [-2500.0], [-2500.0]]`，表示每个销售员的固定成本。
- en: Matrix multiply the input by the weight to show the average deviation from the
    MSRP on all cars and add the bias to subtract the fixed costs of the salesperson.
    Print the result.
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入与权重进行矩阵乘法，以展示所有车辆的 MSRP 平均偏差，并加上偏置，减去销售员的固定成本。打印结果。
- en: 'You should get the following result:'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下结果：
- en: '![Figure 1.40: The output of the matrix multiplication'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.40：矩阵乘法的输出'
- en: '](img/B16341_01_40.jpg)'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_40.jpg)'
- en: 'Figure 1.40: The output of the matrix multiplication'
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.40：矩阵乘法的输出
- en: Apply a ReLU activation function to highlight the net-positive salespeople and
    print the result.
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 ReLU 激活函数以突出净销售为正的销售员，并打印结果。
- en: 'You should get the following result:'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该得到以下结果：
- en: '![Figure 1.41: The output after applying the activation function'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.41：应用激活函数后的输出'
- en: '](img/B16341_01_41.jpg)'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16341_01_41.jpg)'
- en: 'Figure 1.41: The output after applying the activation function'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.41：应用激活函数后的输出
- en: Note
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor253).
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以通过[此链接](B16341_Solution_ePub.xhtml#_idTextAnchor253)查看。
- en: In subsequent chapters, you will see how to add activation functions to your
    ANNs, either between layers or applied directly after a layer when layers are
    defined. You will learn how to choose which activation functions are most appropriate,
    which is often by hyperparameter optimization techniques. The activation function
    is one example of a hyperparameter, a parameter set before the learning process
    begins, that can be tuned to find the optimal values for model performance.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续的章节中，您将学习如何为您的 ANN 添加激活函数，激活函数可以插入到层与层之间，或在定义层后直接应用。您将学习如何选择最适合的激活函数，这通常是通过超参数优化技术来实现的。激活函数是超参数的一个例子，超参数是学习过程开始前设定的参数，它可以进行调整以找到模型性能的最佳值。
- en: Summary
  id: totrans-566
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you were introduced to the TensorFlow library. You learned
    how to use it in the Python programming language. You created the building blocks
    of ANNs (tensors) with various ranks and shapes, performed linear transformations
    on tensors using TensorFlow, and implemented addition, reshaping, transposition,
    and multiplication on tensors—all of which are fundamental for understanding the
    underlying mathematics of ANNs.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已了解了 TensorFlow 库。您学习了如何在 Python 编程语言中使用它。您创建了具有不同秩和形状的 ANN 构建块（张量），并使用
    TensorFlow 对张量进行线性变换，实施了张量的加法、重塑、转置和乘法——这些都是理解 ANN 基础数学原理的基础。
- en: In the next chapter, you will improve your understanding of tensors and learn
    how to load data of various types and pre-process it such that it is appropriate
    for training ANNs in TensorFlow. You will work with tabular, visual, and textual
    data, all of which must be pre-processed differently. By working with visual data
    (that is, images), you will also learn how to use training data in which the size
    of the training data cannot fit into memory.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将提升对张量的理解，并学习如何加载各种类型的数据并进行预处理，以使其适合在 TensorFlow 中训练人工神经网络（ANN）。你将处理表格数据、视觉数据和文本数据，所有这些数据必须以不同的方式进行预处理。通过处理视觉数据（即图像），你还将学习如何使用那些无法全部加载到内存中的训练数据。
