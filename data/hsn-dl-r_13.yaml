- en: Reinforcement Learning for Gaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于游戏的强化学习
- en: In this chapter, we will learn about reinforcement learning. As the name suggests,
    with this method, optimal strategies are discovered through reinforcing or rewarding
    certain behavior and penalizing other behavior. The basic idea for this type of
    machine learning is to use an agent that performs actions towards a goal in an
    environment. We will explore this machine learning technique by using the `ReinforcementLearning`
    package in R to compute a policy for the agent to win a game of tic-tac-toe.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习强化学习。顾名思义，通过这种方法，最佳策略是通过强化或奖励某些行为并惩罚其他行为来发现的。这种类型的机器学习的基本思想是使用一个代理，在环境中执行朝着目标的动作。我们将通过使用R中的`ReinforcementLearning`包来计算代理的策略，从而帮助它在井字游戏中获胜，来探索这种机器学习技术。
- en: While this may seem like a simple game, it is a good environment for investigating
    reinforcement learning. We will learn how to structure input data for reinforcement
    learning, which is the same format for tic-tac-toe as for more complex games.
    We will learn how to compute a policy using the input data to provide the agent
    with the optimal strategy for the environment. We will also look at the hyperparameters
    available with this type of machine learning and the effect of adjusting these
    values.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来像是一个简单的游戏，但它是一个非常适合研究强化学习的环境。我们将学习如何为强化学习构建输入数据结构，这对于井字游戏和更复杂的游戏来说是相同的格式。我们将学习如何利用输入数据计算策略，以为代理提供环境的最佳策略。我们还将了解这种类型的机器学习中可用的超参数，以及调整这些值的效果。
- en: 'Throughout this chapter, we will complete the following tasks:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将完成以下任务：
- en: Understanding the concept of reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解强化学习的概念
- en: Preparing and preprocessing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备和预处理数据
- en: Configuring a reinforcement learning agent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置强化学习代理
- en: Tuning hyperparameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files of this chapter at GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub链接 [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R)
    上找到本章的代码文件。
- en: Understanding the concept of reinforcement learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解强化学习的概念
- en: Reinforcement learning is the last of the three most broad categories of machine
    learning. We have already studied supervised learning and unsupervised learning.
    Reinforcement learning is the third broad category and differs from the other
    two types in significant ways. Reinforcement learning neither trains on labeled
    data nor adds labels to data. Instead, it seeks to find an optimal solution for
    an agent to receive the highest reward.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习三大类别中的最后一个。我们已经学习过监督学习和无监督学习。强化学习是第三大类别，并在许多方面与其他两种类型有所不同。强化学习既不依赖标记数据进行训练，也不会为数据添加标签。相反，它旨在找到一个最优解，使得代理能获得最高的奖励。
- en: The environment is the space where the agent completes its task. In our case,
    the environment will be the 3 x 3 grid used to play the game tic-tac-toe. The
    agent performs tasks within the environment. In this case, the agent places the
    X's or O's on the grid. The environment also contains rewards and penalties—that
    is, the agent needs to be rewarded for certain actions and penalized for others.
    In tic-tac-toe, if a player places marks (X or O) in three consecutive spaces
    either horizontally, vertically, or diagonally, then they win and conversely the
    other player loses. This is the simple reward and penalty structure for this game.
    The policy is the strategy that dictates which actions the agent should take to
    lead to the greatest probability for success given any set of previous actions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是代理完成任务的空间。在我们的案例中，环境将是用于玩井字游戏的3 x 3网格。代理在环境内执行任务。在这种情况下，代理在网格上放置X或O。环境还包含奖励和惩罚——也就是说，代理需要因某些行为而获得奖励，而因其他行为而受到惩罚。在井字游戏中，如果一方将标记（X或O）放在三格连续的空间内，不论是水平、垂直还是对角线，那么该玩家获胜，反之，另一方玩家失败。这就是该游戏的简单奖励和惩罚结构。策略是决定代理在给定一系列先前行为的情况下应采取哪些行动，以最大概率成功的策略。
- en: To determine the optimal policy, we will be using Q-learning. The Q in Q-learning
    stands for quality. It involves developing a quality matrix to determine the best
    course of action. This involves using the Bellman equation. The interior of the
    equation calculates the reward value plus the discounted maximum value of future
    moves minus the current quality score. This calculated value is then multiplied
    by the learning rate and added to the current quality score. Later, we will see
    how to write this equation using R.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最优策略，我们将使用 Q-learning。Q-learning 中的 Q 代表质量。它涉及开发一个质量矩阵来确定最佳的行动路线。这包括使用贝尔曼方程。方程的内部计算奖励值，加上未来动作的折扣最大值，再减去当前的质量评分。这个计算值会乘以学习率并加到当前的质量评分上。稍后，我们将看到如何使用
    R 编写这个方程。
- en: In this chapter, we are using Q-learning; however, there are other ways to perform
    reinforcement learning. Another popular algorithm is called **actor–critic** and
    it differs from Q-learning in significant ways. The following paragraph is a comparison
    of the two to better show the different approaches to pursuing the same type of
    machine learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 Q-learning；然而，还有其他执行强化学习的方法。另一个流行的算法叫做**演员-评论家**，它与 Q-learning 在许多方面有显著的不同。以下段落对这两者进行比较，以更好地展示它们在追求同一类型的机器学习时采取的不同方法。
- en: Q-learning computes a value function, so it requires a finite set of actions,
    such as the game tic-tac-toe. Actor–critic works with a continuous environment
    and seeks to optimize the policy without a value function like Q-learning does.
    Instead, actor–critic has two models. One of them, the actor, performs actions
    while the other, the critic, calculates the value function. This takes place for
    each action, and over numerous iterations, the actor learns the best set of actions.
    While Q-learning works well for solving a game like tic-tac-toe, which has a finite
    space and set of moves, actor–critic works well for environments that are not
    constrained or that change dynamically.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 计算一个价值函数，因此它需要一个有限的动作集合，例如井字棋。演员-评论家则适用于连续环境，并力求优化策略，而不像 Q-learning
    那样使用价值函数。相反，演员-评论家有两个模型，其中一个是演员，执行动作，而另一个是评论家，计算价值函数。这一过程会针对每个动作进行，并在多个迭代中，演员学习到最佳的动作集合。虽然
    Q-learning 适用于解决像井字棋这样的游戏，这类游戏有有限的空间和动作集合，但演员-评论家适用于不受约束或动态变化的环境。
- en: In this section, we quickly reviewed the different methods for performing reinforcement
    learning. Next, we will begin to implement Q-learning on our tic-tac-toe data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要回顾了执行强化学习的不同方法。接下来，我们将开始在我们的井字棋数据上实现 Q-learning。
- en: Preparing and processing data
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备和处理
- en: 'For our first task, we will use the tic-tac-toe dataset from the `ReinforcementLearning`
    package. In this case, the dataset is built for us; however, we will investigate
    how it is made to understand how to get data into the proper format for reinforcement
    learning:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的第一个任务，我们将使用 `ReinforcementLearning` 包中的井字棋数据集。在这种情况下，数据集已经为我们构建好了；然而，我们将调查它是如何构建的，以理解如何将数据转换为适合强化学习的正确格式：
- en: 'First, let''s load the tic-tac-toe data. To load the dataset, we first load
    the `ReinforcementLearning` library and then call the `data` function with `"tictactoe"` as
    the argument. We load our data by running the following code:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们加载井字棋数据。要加载数据集，我们首先加载 `ReinforcementLearning` 库，然后调用 `data` 函数，并将 `"tictactoe"`
    作为参数传入。我们通过运行以下代码来加载数据：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After running these lines, you will see the data object in the data **Environment**
    pane. Its current type is `<Promise>`; however, we will change that in the next
    step to see what is contained in this object. For now, your Environment pane will
    look like the following screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这些代码后，你将在数据**环境**面板中看到数据对象。它当前的类型是 `<Promise>`；然而，我们将在下一步中将其更改，以查看此对象包含的内容。现在，你的环境面板将显示如下截图：
- en: '![](img/f4b61246-8a29-4e82-a853-0dc2adb682da.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4b61246-8a29-4e82-a853-0dc2adb682da.png)'
- en: 'Now, let''s look at the first few rows to evaluate what the dataset contains.
    We will use the `head` function to print the first few rows to the console and
    this will also convert the object in our **Environment** pane from `<Promise>`
    to an object that we can interact with and explore. We print the first five rows
    to our console using the following code:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下前几行，评估数据集的内容。我们将使用 `head` 函数将前几行打印到控制台，这还会将我们**环境**面板中的对象从 `<Promise>`
    转换为我们可以互动并探索的对象。我们使用以下代码将前五行打印到控制台：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After running the code, your console will look like the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，您的控制台将显示如下内容：
- en: '![](img/62c086d5-bbd8-491a-8bc5-8ce1426fa4c8.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62c086d5-bbd8-491a-8bc5-8ce1426fa4c8.png)'
- en: 'In addition, the object in the Environment pane will now look like the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，环境窗格中的对象现在将显示如下内容：
- en: '![](img/288e4e5f-2255-4d45-817c-fd07e7fd2681.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/288e4e5f-2255-4d45-817c-fd07e7fd2681.png)'
- en: As we look at these images, we can see the way that this data is set up. In
    order to conduct reinforcement learning, we need our data to be in the format
    where one column is the current state, another is the action, and then the subsequent
    state, and lastly the reward. Let's take the first row and explain exactly what
    the value means.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看这些图像时，我们可以看到数据是如何设置的。为了进行强化学习，我们需要将数据设置为一种格式，其中一列是当前状态，另一列是动作，然后是随后的状态，最后是奖励。让我们以第一行作为例子，详细解释这些值的含义。
- en: The `State` is `"........."`. The dots indicate that the space on the 3 x 3
    grid is blank, so this character string represents a blank tic-tac-toe board.
    The `Action` is `"c7"`. This means that the agent who is playing as X will place
    an X in the seventh spot, which is the bottom-left corner. The `NextState` is
    `"......X.B"`, which means that in this scenario, for this row, the opponent has
    placed an O in the bottom-right corner. The `Reward` is `0` because the game has
    not ended and the value of `0` for `Reward` indicates a neutral state where the
    game will continue. A row like this will exist for every possible combination
    of values for `State`, `Action`, `NextState`, and `Reward`**.**
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`State`是`"........."`。这些点表示3x3棋盘上的空格，所以这个字符串代表一个空的井字游戏棋盘。`Action`是`"c7"`，意味着扮演X的代理将在第七个位置放置一个X，即左下角。`NextState`是`"......X.B"`，这意味着在这个场景中，对于这一行，对手已经在右下角放置了一个O。`Reward`是`0`，因为游戏尚未结束，`0`的奖励值表示游戏将继续的中立状态。像这样的行将存在于每个可能的`State`、`Action`、`NextState`和`Reward`的组合中**。'
- en: 'Using only the top five rows, we can see that all possible moves are nonterminal,
    which is to say that the game continues after the move. Let''s now look at the
    moves that lead to the conclusion of the game:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅使用前五行，我们可以看到所有可能的动作都是非终结的，也就是说，游戏在执行这些动作后会继续。现在让我们看一下导致游戏结束的动作：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After running the preceding code, we will see the following rows printed to
    our console for actions that lead to victory:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码后，我们将在控制台看到以下几行，表示导致胜利的动作：
- en: '![](img/92c76c14-d5f1-48d0-a0ef-be0d36d4f1ea.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92c76c14-d5f1-48d0-a0ef-be0d36d4f1ea.png)'
- en: 'We will also see these rows printed to our console for moves that lead to defeat:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会看到这些行被打印到控制台，表示导致失败的动作：
- en: '![](img/d8480855-a816-4849-aec6-24c5b36fc69f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8480855-a816-4849-aec6-24c5b36fc69f.png)'
- en: Let's look at the first row from the subset that leads to victory. In this case,
    the agent already has an X in the top-right corner and the center of the game
    board. Here, the agent places an X in the bottom-left corner, and this results
    in three consecutive X's along a diagonal, which means the agent has won the game,
    which we see reflected in the `Reward` column.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下子集中的第一行，该行导致了胜利。在这种情况下，代理已经在游戏板的右上角和中心放置了一个X。这里，代理将在左下角放置一个X，这样就形成了一个斜线连续的三个X，这意味着代理赢得了游戏，我们可以在`Reward`列中看到这一点。
- en: 'Next, let''s look at a given state and see all possible moves:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们来看一个给定的状态，并查看所有可能的动作：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Subsetting our data this way, we start from a given state and see all possible
    options. The printout to your console will look like the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式对数据进行子集选择，我们从一个给定的状态开始，并查看所有可能的选项。您的控制台输出将如下所示：
- en: '![](img/1418b1c2-3821-4406-a80b-14830fc71ab7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1418b1c2-3821-4406-a80b-14830fc71ab7.png)'
- en: In this case, there are only three spaces left on the game board. We can see
    that two moves lead to victory for the agent. If the agent selects the other space
    on the game board, then there are two remaining spaces on the board, and we can
    see that, regardless of which one the opponent chooses, the game continues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，游戏板上只剩下三个空位。我们可以看到两种动作会导致代理胜利。如果代理选择游戏板上的另一个空位，那么游戏板上还剩下两个空位，我们可以看到，无论对手选择哪个，游戏都会继续。
- en: 'From this investigation, we can see how to prepare a dataset for reinforcement
    learning. Even though this one was done for us, we could see just how we would
    make one ourselves. If we wanted to code our tic-tac-toe board differently, we
    could use the values from the game Number Scramble. Number Scramble is isomorphic
    to tic-tac-toe, but involves choosing numbers rather than placing marks on a grid;
    however, the number values match perfectly to the grid, so the values can be swapped.
    The game of Number Scramble involves selecting numbers between 1 and 15 between
    two players, where no number can be selected twice and the winner is the first
    to select numbers that sum up to 15\. With this in mind, we could rewrite the
    first row that we looked at like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从这次调查中，我们可以看到如何为强化学习准备数据集。尽管这个数据集是由别人为我们准备的，但我们可以看到如何自己制作一个。如果我们想以不同的方式编码我们的井字棋棋盘，我们可以使用游戏《数字拼图》的值。《数字拼图》与井字棋同构，但它涉及选择数字而不是在网格上放置标记；然而，数字值与网格完美匹配，因此可以互换。游戏《数字拼图》涉及两位玩家在1到15之间选择数字，每个数字只能选择一次，获胜者是第一个选择出使数字之和为15的数字的人。考虑到这一点，我们可以像这样重写我们查看的第一行：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running this, we would get the following printed to our console:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，我们将在控制台看到以下输出：
- en: '![](img/5a61c3b8-8679-4458-9820-fef283216fc5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a61c3b8-8679-4458-9820-fef283216fc5.png)'
- en: From this, we can see that the values for `State`, `Action`, and `NextState`
    can be encoded in any way that we like as long as a consistent convention is used
    so that the reinforcement learning process can traverse from state to state to
    discover the optimal path to reward.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从中，我们可以看到`State`、`Action`和`NextState`的值可以以我们喜欢的任何方式进行编码，只要使用一致的约定，以便强化学习过程可以从一个状态遍历到另一个状态，从而发现通向奖励的最优路径。
- en: Now that we know how to set up our data, let's move on to looking at exactly
    how our agent will find the best way to reach the reward.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何设置数据，接下来我们来看一下我们的智能体将如何找到最佳的奖励路径。
- en: Configuring the reinforcement agent
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置强化学习智能体
- en: 'Let''s go into the details of what is happening to configure a reinforcement
    agent using Q-learning. The goal of Q-learning is to create a state–action matrix
    where a value is assigned for all state–action combinations—that is, if our agent
    is at any given state, then the values provided determine the action the agent
    will take to obtain maximum value. We are going to enable the computation of the
    best policy for our agent by creating a value matrix that provides a calculated
    value for every possible move:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解如何使用Q学习配置一个强化学习智能体。Q学习的目标是创建一个状态-动作矩阵，其中为所有状态-动作组合分配一个值——也就是说，如果我们的智能体处于某个状态，那么提供的值将决定智能体采取的行动，以获取最大值。我们将通过创建一个值矩阵来启用智能体最佳策略的计算，该矩阵为每一个可能的移动提供一个计算值：
- en: 'To start, we need a set of state and action pairs that all have a value of
    0\. As a best practice, we will use hashing here, which is a more efficient alternative
    to large lists for scaling up to more complex environments. To begin, we will
    load the hash library and then we will use a `for` loop to populate the hash environment.
    The `for` loop starts by getting every unique state from the data, and for every
    unique state, it then appends every unique action to create all possible state–action
    pairs and assigns all pairs a value of 0\. We generate this hash environment that
    will hold the values calculated during the Q-learning phase by running the following
    code:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要一组所有值为0的状态和动作对。作为最佳实践，我们将在这里使用哈希，这是一种比大型列表更高效的替代方案，可以扩展到更复杂的环境。首先，我们将加载哈希库，然后使用`for`循环填充哈希环境。`for`循环首先从数据中获取每个唯一状态，对于每个唯一状态，它将附加每个唯一动作，创建所有可能的状态-动作对，并为所有对分配一个值0。通过运行以下代码，我们生成了这个哈希环境，它将在Q学习阶段保存计算出的值：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After running the code, we will see that our **Environment** pane now looks
    like the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们会看到**环境**面板现在看起来像下图所示：
- en: '![](img/f802eb4e-3660-45a5-8529-8aadb4fac82b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f802eb4e-3660-45a5-8529-8aadb4fac82b.png)'
- en: We have a hash environment, `Q`, that contains every state-action pair.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个哈希环境，`Q`，它包含每个状态-动作对。
- en: 'The next step is to define the hyperparameters. For now, we will use the default
    values; however, we will soon tune these to see the impact. We set the hyperparameters
    to their default values by running the following code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义超参数。现在，我们将使用默认值；然而，我们很快就会调整这些值，以查看它们的影响。通过运行以下代码，我们将超参数设置为其默认值：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the code, we can now see that we have a list with our hyperparameter
    values in our Environment pane, which now looks like the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们现在可以看到环境面板中显示了我们的超参数值列表，面板现在看起来如下：
- en: '![](img/a3bfbff9-c5ef-4fb1-8820-875de3044620.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3bfbff9-c5ef-4fb1-8820-875de3044620.png)'
- en: 'Next, we begin to populate our Q matrix. This again takes place within a `for`
    loop; however, we will look at one isolated iteration. We start by taking a row
    and moving the elements from this row to discrete data objects using the following
    code:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们开始填充我们的 Q 矩阵。这同样是在一个 `for` 循环内进行的；不过，我们将仅查看其中的一次独立迭代。我们首先通过以下代码将一行数据的元素提取到离散的数据对象中：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After running the code, we can see the changes to our **Environment** pane,
    which now contains the discrete elements from the first row. The Environment pane
    will look like the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们可以看到 **环境** 面板中的变化，其中现在包含了第一行中的离散元素。环境面板将如下所示：
- en: '![](img/9884af2f-f684-429c-8ce8-29ea8628f8c5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9884af2f-f684-429c-8ce8-29ea8628f8c5.png)'
- en: 'Next, we get a value for the current Q-learning score if there is one. If there
    isn''t a value, then `0` is stored as the current value. We set this initial quality
    value score by running the following code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们获取当前 Q 学习分数的值（如果有的话）。如果没有值，那么就将 `0` 存储为当前值。我们通过运行以下代码来设置这个初始的质量分数：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After running this code, we now have a value for `currentQ`, which is `0` in
    this case because all values in Q for the state `'......X.B'` are `0`, as we have
    set all values to `0`; however, in the next step, we will begin to update the
    Q values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们现在得到 `currentQ` 的值，在这种情况下它是 `0`，因为状态 `'......X.B'` 的所有 Q 值都为 `0`，因为我们已将所有值设置为
    `0`；然而，在下一步中，我们将开始更新 Q 值。
- en: 'Lastly, we update the Q value by using the Bellman equation. This is also called
    **temporal difference** **learning**. We write out this step for computing values
    with this equation for R using the following code:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过使用 Bellman 方程更新 Q 值。这也叫做 **时序差分** **学习**。我们通过以下代码写出这一计算 R 值的步骤：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After running the following code, we can pull out the updated value for this
    state–action pair; we can see it in the field labeled `q_value`. Your **Environment**
    pane will look like the following screenshot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码后，我们可以提取这个状态-动作对的更新值；我们可以在标记为 `q_value` 的字段中看到它。您的 **环境** 面板将如下所示：
- en: '![](img/7ad7e1e0-875a-4992-b1fe-3f3bed94b04e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ad7e1e0-875a-4992-b1fe-3f3bed94b04e.png)'
- en: We note here that the `q_value` is still `0`. Why is this the case? If we look
    at our equation, we will see that the reward is part of the equation and our reward
    is `0`, which makes the entire calculated value `0`. As a result, we will not
    begin to see updated Q values until our code encounters a row with a nonzero reward.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里注意到 `q_value` 仍然是 `0`。为什么会是这样呢？如果我们看一下我们的公式，我们会发现奖励是公式的一部分，而我们的奖励是 `0`，这使得整个计算值为
    `0`。因此，直到我们的代码遇到具有非零奖励的行时，才会开始看到更新后的 Q 值。
- en: 'We can now put all of these steps together and run them over every row to create
    our Q matrix. We create the matrix of values that we will use to select the policy
    for optimal strategy by running the following code, which wraps all the previous
    code together in a `for` loop:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以将所有这些步骤结合起来，针对每一行运行它们，来创建我们的 Q 矩阵。通过运行以下代码，我们创建了一个包含所有值的矩阵，这些值将用于选择最佳策略的决策：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After looping through all the rows, we see that some state–action pairs do
    now have a value in the Q matrix. Running the following code, we will see the
    following value printed to our console:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环遍历所有行后，我们看到某些状态-动作对现在已经有了 Q 矩阵中的值。运行以下代码时，我们将在控制台上看到以下输出：
- en: '![](img/e2db8f77-2cb1-483f-84bc-8455f72227c4.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2db8f77-2cb1-483f-84bc-8455f72227c4.png)'
- en: At this point, we have now created our matrix for Q-learning. In this case,
    we are storing the value in a hash environment with values for every key-value
    pairing; however, this is equivalent to storing the values in a matrix—it is just
    more efficient for scaling up later. Now that we have these values, we can compute
    a policy for our agent that will provide the best path to a reward; however, before
    we compute this policy, we will make one last set of modifications, and that is
    to tune the hyperparameters that we set earlier to their default values.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经为 Q-learning 创建了矩阵。在这种情况下，我们将值存储在一个哈希环境中，每个键值对都有对应的值；然而，这等同于将值存储在矩阵中——只不过这种方式在之后的扩展中更为高效。现在我们有了这些值，我们可以为智能体计算一个策略，以提供通向奖励的最佳路径；然而，在我们计算这个策略之前，我们将做最后一组修改，那就是将之前设置的超参数调整回默认值。
- en: Tuning hyperparameters
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: We have now defined our environment and iterated over all possible actions and
    results from any given state to calculate the quality value of every move and
    stored these values in our Q object. At this point, we can now begin to tune the
    options for this model to see how it impacts performance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了环境，并遍历了从任何给定状态下可能采取的所有行动及其结果，以计算每一步的质量值，并将这些值存储在我们的 Q 对象中。到此为止，我们现在可以开始调整这个模型的选项，看看这些调整如何影响性能。
- en: 'If we recall, there are three parameters for reinforcement learning, and these
    are alpha, gamma, and epsilon. The following list describes the role of each parameter
    and the impact of adjusting their value:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回顾一下，强化学习有三个参数，它们分别是 alpha、gamma 和 epsilon。下面的列表描述了每个参数的作用以及调整其值的影响：
- en: '**Alpha**: The alpha rate for reinforcement learning is the same as the learning
    rate for many other machine learning models. It is the constant value used to
    control how quickly probabilities are updated as calculations are made based on
    exploring rewards for the agent taking certain actions.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Alpha**：强化学习中的 alpha 值与许多其他机器学习模型的学习率相同。它是用于控制在基于智能体采取某些行动探索奖励时，更新概率的速度的常量值。'
- en: '**Gamma**: Adjusting gamma adjusts how much the model values future rewards.
    When gamma is set to `1`, then all rewards current and future are valued equally.
    This means that a reward that is several steps away is worth as much as a reward
    that is earned at the next step. Practically, this is almost never what we want
    since we want future rewards to be more valuable since it takes more effort to
    earn them. By contrast, setting gamma to `0` means that only rewards from the
    next action will have any value. Future rewards have no value at all. Again, aside
    from special cases, this is not desirable. When adjusting gamma, you have to seek
    the balance in weighting among future rewards that leads the agent to make an
    optimal selection of actions.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gamma**：调整 gamma 值可以改变模型对未来奖励的重视程度。当 gamma 设置为 `1` 时，当前和未来的所有奖励被赋予相同的权重。这意味着距离当前步骤几步之遥的奖励与下一步获得的奖励具有相同的价值。实际上，这几乎从来不是我们想要的效果，因为我们希望未来的奖励更有价值，因为获得它们需要更多的努力。相比之下，设置
    gamma 为 `0` 意味着只有来自下一步行动的奖励才会有任何价值，未来的奖励完全没有价值。同样，除了特殊情况外，这通常不是我们想要的效果。在调整 gamma
    时，你需要在未来奖励的加权平衡中找到一种平衡，使得智能体能够做出最优的行动选择。'
- en: '**Epsilon**: The epsilon parameter is used to introduce randomness when selecting
    future actions. Setting epsilon to `0` is referred to as greedy learning. In this
    case, the agent will always take the path with the highest probability of success;
    however, in this case, as with other machine learning, it is easy for an agent
    to get lost in some local minima and never discover an optimal strategy. By introducing
    some randomness, different actions will be pursued over different iterations.
    Adjusting this value optimizes the exploration to exploit balance. We want the
    model to exploit what has been learned to choose the best future action; however,
    we also want the model to explore and continue learning.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Epsilon**：epsilon 参数用于在选择未来行动时引入随机性。将 epsilon 设置为 `0` 被称为贪婪学习。在这种情况下，智能体将始终选择成功概率最高的路径；然而，正如其他机器学习方法一样，智能体很容易陷入某些局部最小值，永远无法发现最优策略。通过引入一些随机性，在不同的迭代中将会采取不同的行动。调整此值有助于优化探索与利用之间的平衡。我们希望模型能够利用已学到的知识，选择最佳的未来行动；但我们也希望模型继续探索并不断学习。'
- en: 'Using what we know about these hyperparameters, let''s see how the values change
    as we make adjustments to these parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们对这些超参数的理解，接下来我们来看一下在调整这些参数时，值是如何变化的：
- en: 'First, we will make adjustments to the value of `alpha`. As noted, the `alpha`
    value is the learning rate value, which we may be familiar with from learning
    about other machine learning topics. It is just the constant value that controls
    how quickly the algorithm makes adjustments. Currently, we have an `alpha` rate
    set at `0.1`; however, let''s set our `alpha` rate at `0.5`. This is higher than
    we would usually ever want it to be in practice and is used here simply to explore
    the impact of changing these values. We will need to reset Q to all zeroes and
    restart the learning process to see what happens. The following code block takes
    everything we just did previously and runs it all again with the one adjustment
    to `alpha`. We tune the `alpha` value and see the effect by running the following
    code:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将调整`alpha`的值。如前所述，`alpha`值是学习率值，这是我们在学习其他机器学习主题时可能熟悉的内容。它只是一个常数值，用来控制算法调整的速度。目前，我们将`alpha`的值设置为`0.1`；然而，我们将`alpha`的值设为`0.5`。这个值高于我们通常实际希望的值，主要是为了探索更改这些值的影响。我们将需要将Q值重置为零并重新启动学习过程，以查看发生了什么。以下代码块将我们之前所做的一切再次执行，只不过对`alpha`进行了调整。我们调整`alpha`值并通过运行以下代码来查看其效果：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can see from this tuning that we get a different value for the Q value at
    `234543`. You will see the following printed out to your console:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个调整中，我们可以看到在`234543`处，Q值发生了变化。你将在控制台上看到以下输出：
- en: '![](img/1c9cb469-4878-4e9c-bb0f-64e28f731d82.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c9cb469-4878-4e9c-bb0f-64e28f731d82.png)'
- en: As we might have expected, since we increased the value for alpha, we, as a
    result, ended up with a larger value for the Q value at the same point that we
    looked at previously. In other words, our algorithm learned faster and the quality
    value received a greater amount of weight.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的那样，由于我们提高了`alpha`的值，因此结果是，在我们之前查看的相同点上，Q值变得更大。换句话说，我们的算法学习得更快，质量值获得了更大的权重。
- en: 'Next, let''s tune the value for `gamma`. If we recall, adjusting this value
    for gamma will alter how much the agent values future rewards. Our value is currently
    set at `0.1`, which means that future rewards do have value, but the level at
    which they are valued is relatively small. Let''s boost this up to `0.9` and see
    what happens. We go through the same operation as we did when we adjusted alpha.
    We start by resetting the Q hash environment so that all state–action pairs have
    a value of `0` and then we repopulate this hash environment by looping through
    all options, applying the Bellman equation by making our own changes to the `gamma`
    value. We assess what happens when we change the `gamma` value by running the
    following code:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们调整`gamma`的值。如果我们记得的话，调整`gamma`的值会改变代理对未来奖励的重视程度。我们当前的值设置为`0.1`，这意味着未来的奖励是有价值的，但它们被重视的程度相对较小。让我们将其提高到`0.9`，看看会发生什么。我们进行与调整`alpha`时相同的操作。首先重置Q哈希环境，使所有状态–动作对的值为`0`，然后通过循环遍历所有选项，应用贝尔曼方程，并对`gamma`值进行更改来重新填充该哈希环境。我们通过运行以下代码来评估更改`gamma`值时会发生什么：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After running this code, you will see the following code printed on your console:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完这段代码后，你将在控制台看到以下代码输出：
- en: '![](img/d7316a87-4159-4839-b6f9-001edd3a9989.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7316a87-4159-4839-b6f9-001edd3a9989.png)'
- en: 'From this, we can make the following observations:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从这点出发，我们可以得出以下观察结论：
- en: We can see that our value increased quite significantly
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以看到我们的值显著增加了
- en: From this state and taking this action, there is value, but when considering
    future rewards there is even more value
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从这个状态并采取这个动作，会有一定的价值，但在考虑未来奖励时，价值会更大
- en: However, with a game like tic-tac-toe, we need to consider that there are never
    many steps between any state and a reward; however, we can see that from this
    state and this action, there will be a good probability of getting a reward
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，对于像井字游戏这样的游戏，我们需要考虑到，从任何状态到奖励之间的步骤通常很少；然而，我们可以看到，从这个状态和这个动作出发，获得奖励的概率会很高
- en: 'For our final adjustment, we will adjust the `epsilon`. The value of `epsilon`
    relies on how much previous knowledge is used compared with how much exploration
    is done to gather knowledge. For this adjustment, we will go back to using the
    function from the `ReinforcementLearning` package, since it relies on not only
    looping through the Bellman equation, but also storing these values for reference
    over multiple iterations. To adjust `epsilon`, we use the following code:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于最后的调整，我们将调整`epsilon`。`epsilon`的值取决于相对于探索知识的程度，我们使用多少以前的知识。为了这个调整，我们将回到使用`ReinforcementLearning`包中的函数，因为它不仅依赖于通过贝尔曼方程循环计算，还需要在多次迭代中存储这些值以供参考。要调整`epsilon`，我们使用以下代码：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After running this code, we will see that our Q value has changed. You will
    see the following value printed to the console:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们会看到我们的Q值发生了变化。你将看到以下值打印到控制台：
- en: '![](img/8db9a1a2-7374-447f-bbb0-f2e1d57862bb.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8db9a1a2-7374-447f-bbb0-f2e1d57862bb.png)'
- en: 'From this, we can make the following observations:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以得出以下观察结论：
- en: Our value is similar to the value that we had when we were using our default
    values for parameters, but slightly larger
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数值类似于使用默认参数值时的数值，但稍微大一些
- en: In this case, we have introduced a relatively large amount of randomness to
    force our agent to continue exploring; as a result, we can see that we have not
    lost so much value with this randomness, and this action retains a similar value
    even when it leads to different sets of subsequent actions
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们引入了相对较大的随机性，以强迫我们的智能体继续探索；结果，我们可以看到，在这种随机性下，我们的价值损失并不大，而且即使它导致了不同的后续行动集，这个动作仍然保持相似的价值
- en: 'After tuning the parameters to the desired settings, we can now view the policy
    for a given state. First, let''s take a look at the model object in our **Environment**
    pane. Your **Environment** pane will look like the following:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将参数调整到理想设置之后，我们现在可以查看给定状态的策略。首先，让我们看看**Environment**面板中的模型对象。你的**Environment**面板将如下所示：
- en: '![](img/62c571c6-90d6-41df-8db1-3223277e7e82.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62c571c6-90d6-41df-8db1-3223277e7e82.png)'
- en: 'Let''s look more in-depth at every element in the model object:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解模型对象中的每个元素：
- en: '`Q_hash`: The hash environment, just like we created earlier, which includes
    every state–action pair, along with a Q value'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Q_hash`：哈希环境，就像我们之前创建的那样，包含每个状态–动作对及其Q值'
- en: '`Q`: A named matrix that contains the same data as the hash environment, except
    in the form of a named matrix'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Q`：一个命名矩阵，包含与哈希环境相同的数据，只是以命名矩阵的形式呈现'
- en: '`States`: The named rows from our matrix'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`States`：我们矩阵中的命名行'
- en: '`Actions`: The named columns from our matrix'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Actions`：我们矩阵中的命名列'
- en: '`Policy`: A named vector that contains the optimal action that the agent should
    take from any state'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Policy`：一个命名向量，包含智能体应该从任何状态采取的最优动作'
- en: '`Reward` and `RewardSequence`: These are the number of rows from the dataset
    that lead to a reward less than the number that lead to a penalty'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Reward` 和 `RewardSequence`：这些是数据集中导致奖励的行数，少于导致惩罚的行数'
- en: 'We can use the values from here to see the value of all actions at any given
    state and judge which is the best move to make. Let''s start with a brand new
    game and see which move has the most value. We can see the value of every action
    from this state and note which action is best by running the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这里的数值来查看在任何给定状态下所有动作的价值，并判断哪个是最好的行动。让我们从一个全新的游戏开始，看看哪个动作的价值最高。我们可以从这个状态看到每个动作的价值，并通过运行以下代码来标记哪个动作是最好的：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After running this code, we will see the following printed to the console:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们将看到以下内容打印到控制台：
- en: '![](img/7aa1b28b-40f0-4a49-a323-f6d0eff91e52.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aa1b28b-40f0-4a49-a323-f6d0eff91e52.png)'
- en: 'We can see that our first line lists every possible action along with their
    respective values in descending order. We can see that from this named vector,
    the move `"c5"`, which is a mark at the center of the grid, has the highest value.
    Consequently, when we view the policy for our agent when it is at that state,
    we see that it is `"c5"`. In this way, we can now use the results of reinforcement
    learning to choose the optimal move from any given state:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，第一行列出了所有可能的动作以及它们按降序排列的各自价值。我们可以看到，在这个命名向量中，动作`"c5"`，即网格中心的标记，具有最高的价值。因此，当我们查看智能体处于该状态时的策略时，我们看到它是`"c5"`。通过这种方式，我们现在可以利用强化学习的结果，从任何给定状态中选择最优的行动：
- en: We just adjusted all the parameters to note the effects of changing these variables
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们刚刚调整了所有参数，以便注意这些变量变化的影响
- en: Then, in the last step, we saw how to select the best policy based on the grid
    being in any state
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，在最后一步中，我们看到如何根据网格处于任何状态来选择最佳策略
- en: Through trying every possible combination of actions, we calculated the value
    of moves based on immediate and future rewards
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过尝试每种可能的行动组合，我们根据即时和未来的奖励计算了移动的价值
- en: We decided to weigh the Q value by adjusting our parameters and decided upon
    a method to solve games
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们决定通过调整参数来权衡Q值，并决定一种解决游戏的方法
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we have coded a reinforcement-learning system using Q-learning.
    We defined our environment or playing surface and then looked at the dataset containing
    every possible combination of states, actions, and future states. Using the dataset,
    we calculated the value of every state–action pair, which we stored in a hash
    environment and also as a matrix. We then used this matrix of values as the basis
    of our policy, which selects the move with the most value.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Q-learning编码了一个强化学习系统。我们定义了我们的环境或游戏表面，然后查看了包含每种可能状态、动作和未来状态的数据集。使用数据集，我们计算了每个状态-动作对的价值，将其存储在哈希环境和矩阵中。然后，我们将这个值矩阵作为我们策略的基础，选择具有最大价值的移动。
- en: In our next chapter, we will expand on Q-learning by adding neural networks
    to create deep Q-learning networks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一章中，我们将通过将神经网络添加到Q-learning中来扩展深度Q-learning网络。
