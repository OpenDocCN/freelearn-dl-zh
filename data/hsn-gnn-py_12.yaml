- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Learning from Heterogeneous Graphs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从异质图中学习
- en: In the previous chapter, we tried to generate realistic molecules that contain
    different types of nodes (atoms) and edges (bonds). We also observe this kind
    of behavior in other applications, such as recommender systems (users and items),
    social networks (followers and followees), or cybersecurity (routers and servers).
    We call these kinds of graphs **heterogeneous**, as opposed to homogeneous graphs,
    which only involve one type of node and one type of edge.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们尝试生成包含不同类型节点（原子）和边（键）的真实分子。我们在其他应用中也观察到了这种行为，比如推荐系统（用户和物品）、社交网络（关注者和被关注者）、或者网络安全（路由器和服务器）。我们称这些图为**异质图**，与只涉及一种类型的节点和一种类型的边的同质图相对。
- en: In this chapter, we will recap everything we know about homogeneous GNNs. We
    will introduce the message passing neural network framework to generalize the
    architectures we have seen so far. This summary will allow us to understand how
    to expand our framework to heterogeneous networks. We will start by creating our
    own heterogeneous dataset. Then, we will transform homogeneous architectures into
    heterogeneous ones.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回顾关于同质 GNN 的所有知识。我们将引入消息传递神经网络框架，以概括迄今为止我们所看到的架构。这个总结将帮助我们理解如何扩展我们的框架到异质网络。我们将从创建我们自己的异质数据集开始。然后，我们将同质架构转化为异质架构。
- en: In the last section, we will take a different approach and discuss an architecture
    specifically designed to process heterogeneous networks. We will describe how
    it works to understand better the difference between this architecture and a classic
    GAT. Finally, we will implement it in PyTorch Geometric and compare our results
    with the previous techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将采取不同的方法，讨论一种专门为处理异质网络设计的架构。我们将描述它的工作原理，以更好地理解这种架构与经典 GAT 之间的差异。最后，我们将在
    PyTorch Geometric 中实现它，并将我们的结果与前面的方法进行比较。
- en: By the end of this chapter, you will have a strong understanding of the differences
    between homogeneous and heterogeneous graphs. You will be able to create your
    own heterogeneous datasets and convert traditional models to use them in this
    context. You will also be able to implement architectures specifically designed
    to make the most of heterogeneous networks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将深入理解同质图与异质图之间的差异。您将能够创建自己的异质数据集，并将传统模型转换为适用于此情境的模型。您还将能够实现专门为最大化异质网络优势而设计的架构。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要内容：
- en: The message passing neural network framework
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息传递神经网络框架
- en: Introducing heterogeneous graphs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入异质图
- en: Transforming homogeneous GNNs to heterogeneous GNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将同质 GNN 转换为异质 GNN
- en: Implementing a hierarchical self-attention network
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现分层自注意力网络
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter12](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter12).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码示例可以在 GitHub 上找到，网址是 [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter12](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter12)。
- en: The installation steps required to run the code on your local machine can be
    found in the *Preface* of this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*中，您可以找到在本地机器上运行代码所需的安装步骤。
- en: The message passing neural network framework
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息传递神经网络框架
- en: Before exploring heterogeneous graphs, let’s recap what we have learned about
    homogeneous GNNs. In the previous chapters, we saw different functions for aggregating
    and combining features from different nodes. As seen in [*Chapter 5*](B19153_05.xhtml#_idTextAnchor064),
    the simplest GNN layer consists of summing the linear combination of features
    from neighboring nodes (including the target node itself) with a weight matrix.
    The output of the previous sum then replaces the previous target node embedding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索异质图之前，让我们回顾一下我们对同质 GNN 的了解。在前几章中，我们看到了一些用于聚合和组合来自不同节点的特征的不同函数。如在 [*第五章*](B19153_05.xhtml#_idTextAnchor064)中所示，最简单的
    GNN 层由对邻近节点（包括目标节点本身）特征的线性组合求和，并使用权重矩阵。前述求和的输出会替代之前的目标节点嵌入。
- en: 'The node-level operator can be written as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 节点级别的操作符可以写作如下：
- en: '![](img/Formula_B19153_12_001.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_001.jpg)'
- en: '![](img/Formula_B19153_12_002.png) is the set of neighboring nodes of the ![](img/Formula_B19153_12_003.png)
    node (including itself), ![](img/Formula_B19153_12_004.png) is the embedding of
    the ![](img/Formula_B19153_12_005.png) node, and ![](img/Formula_B19153_12_006.png)
    is a weight matrix:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B19153_12_002.png) 是 ![](img/Formula_B19153_12_003.png) 节点的邻居节点集合（包括自身），![](img/Formula_B19153_12_004.png)
    是 ![](img/Formula_B19153_12_005.png) 节点的嵌入，![](img/Formula_B19153_12_006.png)
    是一个权重矩阵：'
- en: 'GCN and GAT layers added fixed and dynamic weights to node features but kept
    the same idea. Even GraphSAGE’s LSTM operator or GIN’s max aggregator did not
    change the main concept of a GNN layer. If we look at all these variants, we can
    generalize GNN layers into a common framework called the **Message Passing Neural
    Network** (**MPNN** or **MP-GNN**). Introduced in 2017 by Gilmer et al. [1], this
    framework consists of three main operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GCN 和 GAT 层为节点特征添加了固定和动态权重，但保持了相同的思想。即便是 GraphSAGE 的 LSTM 操作符或 GIN 的最大聚合器，也没有改变
    GNN 层的主要概念。如果我们查看这些变种，可以将 GNN 层概括为一个通用框架，称为 **消息传递神经网络**（**MPNN** 或 **MP-GNN**）。这个框架由
    Gilmer 等人于 2017 年提出[1]，它包含三个主要操作：
- en: '**Message**: Every node uses a function to create a message for each neighbor.
    It can simply consist of its own features (as in the previous example) or also
    consider the neighboring node’s features and edge features.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Message**：每个节点使用一个函数为每个邻居创建一个消息。这个消息可以简单地由它自己的特征组成（如前面的例子），也可以考虑邻居节点的特征和边的特征。'
- en: '**Aggregate**: Every node aggregates messages from its neighbors using a permutation-equivariant
    function, such as the sum in the previous example.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Aggregate**：每个节点使用一个置换等变函数（如前面的例子中的求和）来聚合来自邻居的消息。'
- en: '**Update**: Every node updates its features using a function to combine its
    current features and the aggregated messages. In the previous example, we introduced
    a self-loop to aggregate the current features of the ![](img/Formula_B19153_12_007.png)
    node, such as a neighbor.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Update**：每个节点使用一个函数更新其特征，将其当前特征与聚合的消息结合。在前面的例子中，我们引入了自环来聚合 ![](img/Formula_B19153_12_007.png)
    节点的当前特征，就像一个邻居一样。'
- en: 'These steps can be summarized in a single equation:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以总结为一个公式：
- en: '![](img/Formula_B19153_12_008.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_008.jpg)'
- en: 'Here, ![](img/Formula_B19153_12_009.png) is the node embedding of the ![](img/Formula_B19153_12_010.png)
    node, ![](img/Formula_B19153_12_011.png) is the edge embedding of the ![](img/Formula_B19153_12_012.png)
    link, ![](img/Formula_B19153_12_013.png) is the message function, ![](img/Formula_B19153_12_014.png)
    is the aggregation function, and ![](img/Formula_B19153_12_015.png) is the update
    function. You can find an illustrated version of this framework in the following
    figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B19153_12_009.png) 是 ![](img/Formula_B19153_12_010.png) 节点的嵌入，![](img/Formula_B19153_12_011.png)
    是 ![](img/Formula_B19153_12_012.png) 链接的边嵌入，![](img/Formula_B19153_12_013.png)
    是消息函数，![](img/Formula_B19153_12_014.png) 是聚合函数，![](img/Formula_B19153_12_015.png)
    是更新函数。你可以在下图中找到这个框架的示意图：
- en: '![Figure 12.1 – The MPNN framework](img/B19153_12_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – MPNN 框架](img/B19153_12_001.jpg)'
- en: Figure 12.1 – The MPNN framework
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – MPNN 框架
- en: 'PyTorch Geometric directly implements this framework with the `MessagePassing`
    class. For instance, here is how to implement the GCN layer using this class:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Geometric 直接通过 `MessagePassing` 类实现了这个框架。例如，以下是如何使用这个类实现 GCN 层：
- en: 'First, we import the required libraries:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库：
- en: '[PRE0]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We declare the GCN class that inherits from `MessagePassing`:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们声明继承自 `MessagePassing` 的 GCN 类：
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This takes two parameters – the input dimensionality and the output (hidden)
    dimensionality. `MessagePassing` is initialized with the “add” aggregation. We
    define a single PyTorch linear layer without bias:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这需要两个参数 – 输入的维度和输出（隐藏）维度。`MessagePassing` 被初始化为“加法”聚合。我们定义了一个没有偏置的单个 PyTorch
    线性层：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `forward()` function contains the logic. First, we add self-loops to the
    adjacency matrix to consider target nodes:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`forward()` 函数包含了逻辑。首先，我们向邻接矩阵中添加自环以考虑目标节点：'
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we apply a linear transformation using the linear layer we previously
    defined:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用之前定义的线性层进行线性变换：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We compute the normalization factor – ![](img/Formula_B19153_12_016.png):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算归一化因子 – ![](img/Formula_B19153_12_016.png)：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We call the `propagate()` method with our updated `edge_index` (including self-loops)
    and our normalization factors, stored in the `norm` tensor. Internally, this method
    calls `message()`, `aggregate()`, and `update()`. We do not need to redefine `update()`
    because we already included the self-loops. The `aggregate()` function is already
    specified in *step 3* with `aggr=''add''`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们调用`propagate()`方法，传入更新后的`edge_index`（包括自环）和存储在`norm`张量中的归一化因子。内部，该方法会调用`message()`、`aggregate()`和`update()`。我们不需要重新定义`update()`，因为我们已经包括了自环。`aggregate()`函数已在*步骤
    3*中通过`aggr='add'`进行指定：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We redefine the `message()` function to normalize the neighboring node features
    `x` with `norm`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重新定义了`message()`函数，以便使用`norm`对邻接节点特征`x`进行归一化：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now initialize and use this object as a GCN layer:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以初始化并使用这个对象作为GCN层：
- en: '[PRE8]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This example shows how you can create your own GNN layers in PyTorch Geometric.
    You can also read how the GCN or GAT layers are implemented in the source code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例展示了如何在PyTorch Geometric中创建自己的GNN层。你还可以查看GCN或GAT层在源代码中的实现方式。
- en: The MPNN framework is an important concept that will help us to transform our
    GNNs into heterogeneous models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MPNN框架是一个重要的概念，它将帮助我们将GNN转化为异构模型。
- en: Introducing heterogeneous graphs
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入异构图
- en: Heterogeneous graphs are a powerful tool to represent general relationships
    between different entities. Having different types of nodes and edges creates
    graph structures that are more complex but also more difficult to learn. In particular,
    one of the main problems with heterogeneous networks is that features from different
    types of nodes or edges do not necessarily have the same meaning or dimensionality.
    Therefore, merging different features would destroy a lot of information. This
    is not the case with homogeneous graphs, where each dimension has the exact same
    meaning for every node or edge.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 异构图是一个强大的工具，可以表示不同实体之间的一般关系。拥有不同类型的节点和边创建了更复杂的图结构，但同时也更难以学习。特别是，异构网络的一个主要问题是，来自不同类型的节点或边的特征未必具有相同的含义或维度。因此，合并不同特征会破坏大量信息。而在同质图中，每个维度对每个节点或边都有相同的含义。
- en: Heterogeneous graphs are a more general kind of network that can represent different
    types of nodes and edges. Formally, it is defined as a graph, ![](img/Formula_B19153_12_017.png),
    comprising ![](img/Formula_B19153_12_018.png), a set of nodes, and ![](img/Formula_B19153_12_019.png),
    a set of edges. In the heterogeneous setting, it is associated with a node-type
    mapping function, ![](img/Formula_B19153_12_020.png) (where ![](img/Formula_B19153_12_021.png)
    denotes the set of node types), and a link-type mapping function, ![](img/Formula_B19153_12_022.png)
    (where ![](img/Formula_B19153_12_023.png) denotes the set of edge types).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 异构图是一种更通用的网络，可以表示不同类型的节点和边。形式上，它被定义为一个图，![](img/Formula_B19153_12_017.png)，包含![](img/Formula_B19153_12_018.png)，一组节点，和![](img/Formula_B19153_12_019.png)，一组边。在异构设置中，它与一个节点类型映射函数![](img/Formula_B19153_12_020.png)相关联（其中![](img/Formula_B19153_12_021.png)表示节点类型的集合），以及一个链接类型映射函数![](img/Formula_B19153_12_022.png)（其中![](img/Formula_B19153_12_023.png)表示边类型的集合）。
- en: The following figure is an example of a heterogeneous graph.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图为异构图的示例。
- en: '![Figure 12.2 – An example of a heteregeneous graph with three types of nodes
    and three types of edges](img/B19153_12_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.2 – 一个包含三种节点类型和三种边类型的异构图示例](img/B19153_12_002.jpg)'
- en: Figure 12.2 – An example of a heteregeneous graph with three types of nodes
    and three types of edges
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 一个包含三种节点类型和三种边类型的异构图示例
- en: In this graph, we see three types of nodes (users, games, and developers) and
    three types of edges (**follows**, **plays**, and **develops**). It represents
    a network involving people (users and developers) and games that could be used
    for various applications, such as recommending games. If this graph contained
    millions of elements, it could be used as a graph-structured knowledge database,
    or a knowledge graph. Knowledge graphs are used by Google or Bing to answer queries
    such as, “Who plays games developed by **Dev 1**?”
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，我们看到三种类型的节点（用户、游戏和开发者）和三种类型的边（**关注**、**玩**和**开发**）。它表示一个涉及人（用户和开发者）和游戏的网络，可以用于各种应用，例如游戏推荐。如果这个图包含数百万个元素，它可以用作图结构化的知识数据库，或者知识图谱。知识图谱被Google或Bing用于回答类似“谁玩**Dev
    1**开发的游戏？”这样的问题。
- en: Similar queries can extract useful homogeneous graphs. For example, we might
    want only to consider users who play **Game 1**. The output would be **User 1**
    and **User 2**. We can create more complex queries, such as, “Who are the users
    who play games developed by **Dev 1**?” The result is the same, but we traversed
    two relations to obtain our users. This kind of query is called a meta-path.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的查询可以提取有用的同质图。例如，我们可能只想考虑玩**Game 1**的用户。输出将是**User 1**和**User 2**。我们还可以创建更复杂的查询，例如，“谁是玩由**Dev
    1**开发的游戏的用户？”结果是一样的，但我们通过两种关系来获取我们的用户。这种查询被称为元路径（meta-path）。
- en: In the first example, our meta-path was *User → Game → User* (commonly denoted
    as **UGU**), and in the second one, our meta-path was *User → Game → Dev → Game
    → User* (or **UGDGU**). Note that the start node type and the end node type are
    the same. Meta-paths are an essential concept in heterogeneous graphs, often used
    to measure the similarity of different nodes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，我们的元路径是*User → Game → User*（通常表示为**UGU**），在第二个例子中，我们的元路径是*User → Game
    → Dev → Game → User*（或**UGDGU**）。请注意，起始节点类型和终止节点类型是相同的。元路径是异质图中的一个重要概念，常用于度量不同节点之间的相似性。
- en: 'Now, let’s see how to implement the previous graph with PyTorch Geometric.
    We will use a special data object called `HeteroData`. The following steps create
    a data object to store the graph from *Figure 12**.2*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用PyTorch Geometric实现前面的图。我们将使用一个叫做`HeteroData`的特殊数据对象。以下步骤创建一个数据对象，用于存储*图12.2*中的图：
- en: 'We import the `HeteroData` class from `torch_geometric.data` and create a `data`
    variable:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`torch_geometric.data`导入`HeteroData`类，并创建一个`data`变量：
- en: '[PRE9]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'First, let’s store node features. We can access user features with `data[''user''].x`,
    for instance. We feed it a tensor with the `[num_users, num_features_users]` dimensions.
    The content does not matter in this example, so we will create feature vectors
    filled with ones for `user 1`, twos for `user 2`, and threes for `user 3`:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们来存储节点特征。我们可以访问用户特征，例如`data['user'].x`。我们将其传入一个具有`[num_users, num_features_users]`维度的张量。内容在这个例子中不重要，因此我们为`user
    1`创建填充了1的特征向量，为`user 2`创建填充了2的特征向量，为`user 3`创建填充了3的特征向量：
- en: '[PRE10]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We repeat this process with `games` and `devs`. Note that the dimensionality
    of feature vectors is not the same; this is an important benefit of heterogeneous
    graphs when handling different representations:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`games`和`devs`重复这个过程。请注意，特征向量的维度不相同；这是异质图在处理不同表示时的一个重要优势：
- en: '[PRE11]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s create connections between our nodes. Links have different meanings,
    which is why we will create three sets of edge indices. We can declare each set
    using a triplet ![](img/Formula_B19153_12_024.png), such as `data[''user'', ''follows'',
    ''user''].edge_index`. Then, we store the connections in a tensor with the `[2,
    number of` `edges]` dimensions:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在节点之间创建连接。连接具有不同的含义，因此我们将创建三组边的索引。我们可以通过一个三元组声明每一组！[](img/Formula_B19153_12_024.png)，例如`data['user',
    'follows', 'user'].edge_index`。然后，我们将连接存储在一个具有`[2, number of edges]`维度的张量中：
- en: '[PRE12]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Edges can also have features – for example, the `plays` edges could include
    the number of hours the user played the corresponding game. In the following,
    we assume that `user 1` played `game 1` for 2 hours, `user 2` played `game 1`
    for half an hour and `game 2` for 10 hours, and `user 3` played `game 2` for 12
    hours:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 边也可以具有特征——例如，`plays`边可以包括用户玩对应游戏的时长。下面假设`user 1`玩了`game 1` 2小时，`user 2`玩了`game
    1` 30分钟，`game 2`玩了10小时，`user 3`玩了`game 2` 12小时：
- en: '[PRE13]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can print the `data` object to see the result:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以打印`data`对象以查看结果：
- en: '[PRE14]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see in this implementation, different types of nodes and edges do
    not share the same tensors. In fact, it is impossible because they don’t share
    the same dimensionality either. This raises a new issue – how do we aggregate
    information from multiple tensors using GNNs?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个实现中，不同类型的节点和边并不共享相同的张量。实际上，这是不可能的，因为它们的维度也不同。这引出了一个新问题——我们如何使用GNN从多个张量中聚合信息？
- en: So far, we have only focused our efforts on a single type. In practice, our
    weight matrices have the right size to be multiplied with a predefined dimension.
    However, how do we implement GNNs when we get inputs with different dimensionalities?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只关注单一类型。实际上，我们的权重矩阵已经具有适当的大小，可以与预定义维度相乘。但是，当我们得到不同维度的输入时，如何实现GNN？
- en: Transforming homogeneous GNNs to heterogeneous GNNs
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将同质GNN转化为异质GNN
- en: To better understand the problem, let’s take a real dataset as an example. The
    DBLP computer science bibliography offers a dataset, `[2-3]`, that contains four
    types of nodes – `papers` (14,328), `terms` (7,723), `authors` (4,057), and `conferences`
    (20). This dataset’s goal is to correctly classify the authors into four categories
    – database, data mining, artificial intelligence, and information retrieval. The
    authors’ node features are a bag-of-words (“`0`” or “`1`”) of 334 keywords they
    might have used in their publications. The following figure summarizes the relations
    between the different node types.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解问题，让我们以真实数据集为例。DBLP计算机科学文献提供了一个数据集，`[2-3]`，包含四种类型的节点 – `论文`（14,328），`术语`（7,723），`作者`（4,057），和`会议`（20）。该数据集的目标是将作者正确分类为四个类别
    – 数据库、数据挖掘、人工智能和信息检索。作者的节点特征是一组词袋（“`0`”或“`1`”），包含他们在出版物中可能使用的334个关键字。以下图表总结了不同节点类型之间的关系。
- en: '![Figure 12.3 – Relationships between node types in the DBLP dataset](img/B19153_12_003.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – DBLP数据集中节点类型之间的关系](img/B19153_12_003.jpg)'
- en: Figure 12.3 – Relationships between node types in the DBLP dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – DBLP数据集中节点类型之间的关系
- en: These node types do not have the same dimensionalities and semantic relationships.
    In heterogeneous graphs, relations between nodes are essential, which is why we
    want to consider node pairs. For example, instead of feeding author nodes to a
    GNN layer, we would consider a pair such as (`author`, `paper`). It means we now
    need a GNN layer per relation; in this case, the “to” relations are bidirectional,
    so we would get six layers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些节点类型的维度和语义关系并不相同。在异构图中，节点之间的关系至关重要，这就是为什么我们要考虑节点对。例如，我们不再将作者节点直接馈送到GNN层，而是考虑一对，如(`作者`,
    `论文`)。这意味着现在我们需要根据每一种关系创建一个GNN层；在这种情况下，“to”关系是双向的，所以我们会得到六层。
- en: These new layers have independent weight matrices with the right size for each
    node type. Unfortunately, we have only solved half of the problem. Indeed, we
    now have six distinct layers that do not share any information. We can fix that
    by introducing **skip-connections**, **shared layers**, **jumping knowledge**,
    and so on [4].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新层具有独立的权重矩阵，适合每种节点类型的大小。不幸的是，我们只解决了问题的一半。实际上，现在我们有六个不同的层，它们不共享任何信息。我们可以通过引入**跳跃连接**、**共享层**、**跳跃知识**等方法来解决这个问题
    [4]。
- en: Before we transform a homogeneous model into a heterogeneous one, let’s implement
    a classic GAT on the DBLP dataset. The GAT cannot take into account different
    relations; we have to give it a unique adjacency matrix that connects authors
    to each other. Fortunately, we now have a technique to generate this adjacency
    matrix easily – we can create a meta-path, such as `author-paper-author`, that
    will connect authors from the same papers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在将同质模型转换为异构模型之前，让我们在DBLP数据集上实现一个经典的GAT。GAT不能考虑不同的关系；我们必须为它提供一个连接作者的唯一邻接矩阵。幸运的是，我们现在有一种技术可以轻松生成这个邻接矩阵
    – 我们可以创建一个元路径，如`作者-论文-作者`，它将连接同一篇论文中的作者。
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We can also build a good adjacency matrix through random walks. Even if the
    graph is heterogeneous, we can explore it and connect nodes that often appear
    in the same sequences.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过随机游走，我们还可以构建一个良好的邻接矩阵。即使图形是异构的，我们也可以探索并连接在相同序列中经常出现的节点。
- en: 'The code is a little more verbose, but we can implement a regular GAT as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 代码有点冗长，但我们可以按以下方式实现常规的GAT：
- en: 'We import the required libraries:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入所需的库：
- en: '[PRE15]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We define the meta-path we will use using this specific syntax:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用特定语法定义我们将使用的元路径：
- en: '[PRE16]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We use the `AddMetaPaths` transform function to automatically calculate our
    meta-path. We use `drop_orig_edge_types=True` to remove the other relations from
    the dataset (the GAT can only consider one):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`AddMetaPaths`转换函数自动计算我们的元路径。我们使用`drop_orig_edge_types=True`从数据集中删除其他关系（GAT只能考虑一个）：
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We load the `DBLP` dataset and print it:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载`DBLP`数据集并打印它：
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We obtain the following output. Note the `(author, metapath_0, author)` relation
    that was created with our transform function:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得了以下输出。请注意我们的转换函数创建的`(作者, metapath_0, 作者)`关系：
- en: '[PRE19]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We directly create a one-layer GAT model with `in_channels=-1` to perform lazy
    initialization (the model will automatically calculate the value) and `out_channels=4`
    because we need to classify the author nodes into four categories:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们直接创建一个带有`in_channels=-1`的单层GAT模型来进行惰性初始化（模型将自动计算值），并且`out_channels=4`，因为我们需要将作者节点分类为四类：
- en: '[PRE20]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We implement the `Adam` optimizer and store the model and the data on a GPU
    if possible:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实现了`Adam`优化器，并将模型和数据存储在GPU上（如果可能的话）：
- en: '[PRE21]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `test()` function measures the accuracy of the prediction:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test()`函数衡量预测的准确性：'
- en: '[PRE22]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We create a classic training loop, where the node features (`author`) and edge
    indexes (`author`, `metapath_0`, and `author`) are carefully selected:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个经典的训练循环，在这个循环中，节点特征（`author`）和边索引（`author`、`metapath_0`和`author`）经过精心选择：
- en: '[PRE23]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We test it on the test set with the following output:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在测试集上进行测试，输出如下：
- en: '[PRE24]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We reduced our heterogeneous dataset into a homogeneous one using a meta-path
    and applied a traditional GAT. We obtained a test accuracy of 73.29%, which provides
    a good baseline to compare it to other techniques.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过元路径将异构数据集转化为同构数据集，并应用了传统的GAT。我们得到了73.29%的测试准确率，这为与其他技术的比较提供了一个良好的基准。
- en: 'Now, let’s create a heterogeneous version of this GAT model. Following the
    method we described previously, we need six GAT layers instead of one. We don’t
    have to do it manually, since PyTorch Geometric can do it automatically using
    the `to_hetero()` or `to_hetero_bases()` functions. The `to_hetero()` function
    takes three important parameters:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个异构版本的GAT模型。按照我们之前描述的方法，我们需要六个GAT层，而不是一个。我们不必手动完成这项工作，因为PyTorch Geometric可以通过`to_hetero()`或`to_hetero_bases()`函数自动完成。`to_hetero()`函数有三个重要参数：
- en: '`module`: The homogeneous model we want to convert'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`module`：我们想要转换的同构模型'
- en: '`metadata`: Information about the heterogeneous nature of the graph, represented
    by a tuple, `(``node_types, edge_types)`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metadata`：关于图的异构性质的信息，通过元组表示，`(``node_types, edge_types)`'
- en: '`aggr`: The aggregator to combine node embeddings generated by different relations
    (for instance, `sum`, `max`, or `mean`)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aggr`：用于合并由不同关系生成的节点嵌入的聚合器（例如，`sum`、`max`或`mean`）'
- en: The following figure shows our homogeneous GAT (left) and its heterogeneous
    version (right), obtained with `to_hetero()`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了我们的同构GAT（左）及其异构版本（右），是通过`to_hetero()`获得的。
- en: '![Figure 12.4 – Architecture of a homogeneous GAT (left) and a heterogeneous
    GAT (right) on the DBLP dataset](img/B19153_12_004.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.4 – 同构GAT（左）和异构GAT（右）在DBLP数据集上的架构](img/B19153_12_004.jpg)'
- en: Figure 12.4 – Architecture of a homogeneous GAT (left) and a heterogeneous GAT
    (right) on the DBLP dataset
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – 同构GAT（左）和异构GAT（右）在DBLP数据集上的架构
- en: 'As shown in the following steps, the heterogeneous GAT’s implementation is
    similar:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下步骤所示，异构GAT的实现类似：
- en: 'First, we import GNN layers from PyTorch Geometric:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从PyTorch Geometric导入GNN层：
- en: '[PRE25]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We load the `DBLP` dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了`DBLP`数据集：
- en: '[PRE26]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When we printed information about this dataset, you might have noticed that
    conference nodes do not have any features. This is an issue because our architecture
    assumes that each node type has its own features. We can fix this problem by generating
    zero values as features, as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们打印出这个数据集的信息时，您可能已经注意到会议节点没有任何特征。这是一个问题，因为我们的架构假设每种节点类型都有自己的特征。我们可以通过生成零值作为特征来解决这个问题，方法如下：
- en: '[PRE27]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We create our own GAT class with a GAT and linear layers. Note that we use
    lazy initialization again with the `(-1, -``1)` tuple:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了自己的GAT类，包含GAT和线性层。请注意，我们再次使用懒初始化，采用`(-1, -1)`元组：
- en: '[PRE28]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We `instantiate` the model and convert it using `to_hetero()`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们`实例化`模型并通过`to_hetero()`进行转换：
- en: '[PRE29]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We implement the `Adam` optimizer and store the model and data on a GPU if
    possible:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实现了`Adam`优化器，并将模型和数据存储在GPU上（如果可能的话）：
- en: '[PRE30]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The test process is very similar. This time, we don’t need to specify any relation,
    since the model considers all of them:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试过程非常相似。这次，我们不需要指定任何关系，因为模型会考虑所有关系：
- en: '[PRE31]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The same is true for the training loop:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练循环来说也是如此：
- en: '[PRE32]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We obtain the following test accuracy:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获得了以下的测试准确率：
- en: '[PRE33]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The heterogeneous GAT obtains a test accuracy of 78.42%. This is a good improvement
    (+5.13%) over the homogeneous version, but can we do better? In the next section,
    we will explore an architecture that is specifically designed to process heterogeneous
    networks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 异构GAT获得了78.42%的测试准确率。这比同构版本提高了5.13%，但我们能做得更好吗？在接下来的部分，我们将探讨一种专门设计用于处理异构网络的架构。
- en: Implementing a hierarchical self-attention network
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个层次化自注意力网络
- en: 'In this section, we will implement a GNN model designed to handle heterogeneous
    graphs – the **hierarchical self-attention network** (**HAN**). This architecture
    was introduced by Liu et al. in 2021 [5]. HAN uses self-attention at two different
    levels:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个设计用于处理异构图的GNN模型——**层次自注意力网络**（**HAN**）。该架构由Liu等人于2021年提出[5]。HAN在两个不同的层次上使用自注意力机制：
- en: '**Node-level attention** to understand the importance of neighboring nodes
    in a given meta-path (such as a GAT in a homogeneous setting).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点级别注意力**用于理解在给定元路径中邻近节点的重要性（如同质设置下的GAT）。'
- en: '`game-user-game` might be more relevant than `game-dev-game` in some tasks,
    such as predicting the number of players.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些任务中，`game-user-game` 可能比 `game-dev-game` 更相关，例如预测玩家数量。
- en: In the following section, we will detail the three main components – node-level
    attention, semantic-level attention, and the prediction module. This architecture
    is illustrated in *Figure 12**.5*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将详细介绍三个主要组件——节点级别注意力、语义级别注意力和预测模块。该架构如*图 12.5*所示。
- en: '![Figure 12.5 – HAN’s architecture with its three main modules](img/B19153_12_005.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – HAN的架构及其三个主要模块](img/B19153_12_005.jpg)'
- en: Figure 12.5 – HAN’s architecture with its three main modules
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – HAN的架构及其三个主要模块
- en: 'Like in a GAT, the first step consists of projecting nodes into a unified feature
    space for each meta-path. We then calculate the weight of a node pair (concatenation
    of two projected nodes) in the same meta-path, with a second weight matrix. A
    nonlinear function is applied to this result, which is then normalized with the
    softmax function. The normalized attention score (importance) of the ![](img/Formula_B19153_12_025.png)
    node to the ![](img/Formula_B19153_12_026.png) node is calculated as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GAT，第一步是将节点映射到每个元路径的统一特征空间。接着，我们使用第二个权重矩阵计算在同一元路径中节点对（两个投影节点的连接）的权重。然后对该结果应用非线性函数，并通过softmax函数进行归一化。归一化后的注意力分数（重要性）表示为从![](img/Formula_B19153_12_025.png)节点到![](img/Formula_B19153_12_026.png)节点的计算方式如下：
- en: '![](img/Formula_B19153_12_027.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_027.jpg)'
- en: Here, ![](img/Formula_B19153_12_028.png) denotes the features of the ![](img/Formula_B19153_12_029.png)
    node, ![](img/Formula_B19153_12_030.png) is a shared weight matrix for the ![](img/Formula_B19153_12_031.png)
    meta-path, ![](img/Formula_B19153_12_032.png) is the attention weight matrix for
    the ![](img/Formula_B19153_12_033.png) meta-path, ![](img/Formula_B19153_12_034.png)
    is a nonlinear activation function (such as LeakyReLU), and ![](img/Formula_B19153_12_035.png)
    is the set of neighbors of the ![](img/Formula_B19153_12_036.png) node (including
    itself) in the ![](img/Formula_B19153_12_031.png) meta-path.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B19153_12_028.png)表示![](img/Formula_B19153_12_029.png)节点的特征，![](img/Formula_B19153_12_030.png)是![](img/Formula_B19153_12_031.png)元路径的共享权重矩阵，![](img/Formula_B19153_12_032.png)是![](img/Formula_B19153_12_033.png)元路径的注意力权重矩阵，![](img/Formula_B19153_12_034.png)是非线性激活函数（如LeakyReLU），而![](img/Formula_B19153_12_035.png)是![](img/Formula_B19153_12_036.png)节点在![](img/Formula_B19153_12_031.png)元路径中的邻居集合（包括其自身）。
- en: 'Multi-head attention is also performed to obtain the final embedding:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 也会执行多头注意力机制以获得最终的嵌入表示：
- en: '![](img/Formula_B19153_12_038.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_038.jpg)'
- en: 'With semantic-level attention, we repeat a similar process for the attention
    score of every meta-path (denoted ![](img/Formula_B19153_12_039.png)). Every node
    embedding in a given meta-path (denoted as ![](img/Formula_B19153_12_040.png))
    is fed to an MLP that applies a nonlinear transformation. We compare this result
    to a new attention vector, ![](img/Formula_B19153_12_041.png), as a similarity
    measure. We average this result to calculate the importance of a given meta-path:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通过语义级别注意力，我们对每个元路径的注意力分数（记作![](img/Formula_B19153_12_039.png)）执行类似的处理。每个给定元路径中的节点嵌入（记作![](img/Formula_B19153_12_040.png)）输入到MLP中，MLP应用非线性变换。我们将此结果与一个新的注意力向量![](img/Formula_B19153_12_041.png)进行比较，作为相似度度量。然后我们对该结果求平均，以计算给定元路径的重要性：
- en: '![](img/Formula_B19153_12_042.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_042.jpg)'
- en: Here, ![](img/Formula_B19153_12_043.png) (the MLP’s weight matrix), ![](img/Formula_B19153_12_044.png)
    (the MLP’s bias), and ![](img/Formula_B19153_12_045.png) (the semantic-level attention
    vector) are shared across the meta-paths.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B19153_12_043.png)（MLP的权重矩阵）、![](img/Formula_B19153_12_044.png)（MLP的偏置）和![](img/Formula_B19153_12_045.png)（语义级别的注意力向量）在所有元路径之间共享。
- en: 'We must normalize this result to compare the different semantic-level attention
    scores. We use the softmax function to obtain our final weights:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须对这个结果进行归一化，以比较不同的语义级注意力分数。我们使用 softmax 函数来获取我们的最终权重：
- en: '![](img/Formula_B19153_12_046.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_046.jpg)'
- en: 'The final embedding, ![](img/Formula_B19153_12_047.png), that combines node-level
    and semantic-level attention is obtained as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 结合节点级和语义级注意力的最终嵌入，如下所示：![](img/Formula_B19153_12_047.png)
- en: '![](img/Formula_B19153_12_048.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_12_048.jpg)'
- en: A final layer, like an MLP, is used to fine-tune the model for a particular
    downstream task, such as node classification or link prediction.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层，例如 MLP，用于微调模型以完成特定的下游任务，如节点分类或链接预测。
- en: 'Let’s implement this architecture in PyTorch Geometric on the `DBLP` dataset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 PyTorch Geometric 上的 `DBLP` 数据集中实现这个架构：
- en: 'First, we import the HAN layer:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入 HAN 层：
- en: '[PRE34]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We load the `DBLP` dataset and introduce dummy features for conference nodes:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载 `DBLP` 数据集并为会议节点引入虚拟特征：
- en: '[PRE35]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We create the `HAN` class with two layers – a `HAN` convolution using `HANConv`
    and a `linear` layer for the final classification:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了 `HAN` 类，包括两层 - 使用 `HANConv` 的 `HAN` 卷积和用于最终分类的 `linear` 层：
- en: '[PRE36]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the `forward()` function, we have to specify that we are interested in the
    authors:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `forward()` 函数中，我们必须指定我们对作者感兴趣：
- en: '[PRE37]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We initialize our model with lazy initialization (`dim_in=-1`), so PyTorch
    Geometric automatically calculates the input size for each node type:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用懒初始化（`dim_in=-1`）初始化我们的模型，因此 PyTorch Geometric 自动计算每个节点类型的输入尺寸：
- en: '[PRE38]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We choose the `Adam` optimizer and transfer our data and model to the GPU if
    possible:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择了 `Adam` 优化器，并在可能的情况下将数据和模型转移到 GPU：
- en: '[PRE39]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `test()` function calculates the accuracy of the classification task:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`test()` 函数计算分类任务的准确率：'
- en: '[PRE40]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We train the model for 100 epochs. The only difference with a training loop
    for a homogeneous GNN is that we need to specify we’re interested in the author
    node type:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将模型训练了 100 个 epochs。与同质 GNN 的训练循环唯一的区别在于，我们需要指定我们对作者节点类型感兴趣：
- en: '[PRE41]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The training gives us the following output:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练给出了以下输出：
- en: '[PRE42]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we test our solution on the test set:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在测试集上测试我们的解决方案：
- en: '[PRE43]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: HAN obtains a test accuracy of 81.58%, which is higher than what we got with
    the heterogeneous GAT (78.42%) and the classic GAT (73.29%). It shows the importance
    of building good representations that aggregate different types of nodes and relations.
    Heterogeneous graphs’ techniques are highly application-dependent, but it is worth
    trying different options, especially when the relationships described in the network
    are meaningful.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: HAN 在测试中获得了 81.58% 的准确率，比我们在异质 GAT（78.42%）和经典 GAT（73.29%）中获得的要高。这显示了构建良好的表示形式以聚合不同类型的节点和关系的重要性。异质图技术高度依赖于应用程序，但尝试不同的选项是值得的，特别是当网络中描述的关系具有意义时。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the MPNN framework to generalize GNN layers using
    three steps – message, aggregate, and update. In the rest of the chapter, we expanded
    this framework to consider heterogeneous networks, composed of different types
    of nodes and edges. This particular kind of graph allows us to represent various
    relations between entities, which are more insightful than a single type of connection.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 MPNN 框架，以三个步骤 - 消息、聚合和更新，来推广 GNN 层。在本章的其余部分，我们扩展了这个框架以考虑异质网络，由不同类型的节点和边组成。这种特殊类型的图允许我们表示实体之间的各种关系，这比单一类型的连接更具见解。
- en: Moreover, we saw how to transform homogeneous GNNs into heterogeneous ones thanks
    to PyTorch Geometric. We described the different layers in our heterogeneous GAT,
    which take node pairs as inputs to model their relations. Finally, we implemented
    a heterogeneous-specific architecture with `HAN` and compared the results of three
    techniques on the `DBLP` dataset. It proved the importance of exploiting the heterogeneous
    information that is represented in this kind of network.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们看到如何通过 PyTorch Geometric 将同质 GNN 转换为异质 GNN。我们描述了在我们的异质 GAT 中使用的不同层，这些层将节点对作为输入来建模它们的关系。最后，我们使用
    `HAN` 实现了一种特定于异质的架构，并在 `DBLP` 数据集上比较了三种技术的结果。这证明了利用这种网络中所表示的异质信息的重要性。
- en: In [*Chapter 13*](B19153_13.xhtml#_idTextAnchor153), *Temporal Graph Neural
    Networks*, we will see how to consider time in GNNs. This chapter will unlock
    a lot of new applications thanks to temporal graphs, such as traffic forecasting.
    It will also introduce PyG’s extension library called PyTorch Geometric Temporal,
    which will help us to implement new models specifically designed to handle time.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第13章*](B19153_13.xhtml#_idTextAnchor153) *时间图神经网络* 中，我们将看到如何在图神经网络中考虑时间。这个章节将开启很多新的应用，得益于时间图，比如交通预测。它还将介绍PyG的扩展库——PyTorch
    Geometric Temporal，帮助我们实现专门设计用于处理时间的新模型。
- en: Further reading
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[1] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. *Neural
    Message Passing for Quantum Chemistry*. arXiv, 2017\. DOI: 10.48550/ARXIV.1704.01212\.
    Available: [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, 和 G. E. Dahl. *量子化学的神经消息传递*.
    arXiv, 2017\. DOI: 10.48550/ARXIV.1704.01212\. 可用： [https://arxiv.org/abs/1704.01212](https://arxiv.org/abs/1704.01212).'
- en: '[2] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. *ArnetMiner:
    Extraction and Mining of Academic Social Networks*. In Proceedings of the Fourteenth
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD’2008).
    pp.990–998\. Available: [https://dl.acm.org/doi/abs/10.1145/1401890.1402008](https://dl.acm.org/doi/abs/10.1145/1401890.1402008).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, 和 Zhong Su. *ArnetMiner：学术社交网络的提取与挖掘*.
    载于第十四届ACM SIGKDD国际知识发现与数据挖掘大会论文集(SIGKDD''2008). pp.990–998\. 可用： [https://dl.acm.org/doi/abs/10.1145/1401890.1402008](https://dl.acm.org/doi/abs/10.1145/1401890.1402008).'
- en: '[3] X. Fu, J. Zhang, Z. Meng, and I. King. *MAGNN: Metapath Aggregated Graph
    Neural Network for Heterogeneous Graph Embedding*. Apr. 2020\. DOI: 10.1145/3366423.3380297\.
    Available: [https://arxiv.org/abs/2002.01680](https://arxiv.org/abs/2002.01680).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] X. Fu, J. Zhang, Z. Meng, 和 I. King. *MAGNN：用于异构图嵌入的元路径聚合图神经网络*. 2020年4月\.
    DOI: 10.1145/3366423.3380297\. 可用： [https://arxiv.org/abs/2002.01680](https://arxiv.org/abs/2002.01680).'
- en: '[4] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and
    M. Welling. *Modeling Relational Data with Graph Convolutional Networks*. arXiv,
    2017\. DOI: 10.48550/ARXIV.1703.06103\. Available: [https://arxiv.org/abs/1703.06103](https://arxiv.org/abs/1703.06103).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, 和 M.
    Welling. *用图卷积网络建模关系数据*. arXiv, 2017\. DOI: 10.48550/ARXIV.1703.06103\. 可用： [https://arxiv.org/abs/1703.06103](https://arxiv.org/abs/1703.06103).'
- en: '[5] J. Liu, Y. Wang, S. Xiang, and C. Pan. *HAN: An Efficient Hierarchical
    Self-Attention Network for Skeleton-Based Gesture Recognition*. arXiv, 2021\.
    DOI: 10.48550/ARXIV.2106.13391\. Available: [https://arxiv.org/abs/2106.13391](https://arxiv.org/abs/2106.13391).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Liu, Y. Wang, S. Xiang, 和 C. Pan. *HAN：一种高效的层次化自注意力网络，用于基于骨架的手势识别*.
    arXiv, 2021\. DOI: 10.48550/ARXIV.2106.13391\. 可用： [https://arxiv.org/abs/2106.13391](https://arxiv.org/abs/2106.13391).'
