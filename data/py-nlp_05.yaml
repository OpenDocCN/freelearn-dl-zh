- en: Feature Engineering and NLP Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程与NLP算法
- en: Feature engineering is the most important part of developing NLP applications.
    Features are the input parameters for **machine learning** (**ML**) algorithms.
    These ML algorithms generate output based on the input features. Feature engineering
    is a kind of art and skill because it generates the best possible features, and
    choosing the best algorithm to develop NLP application requires a lot of effort
    and understanding about feature engineering as well as NLP and ML algorithms.
    In [Chapter 2](837b4260-d60f-474a-a208-68a1eaa8e1bb.xhtml), *Practical Understanding
    of Corpus and Dataset,* we saw how data is gathered and what the different formats
    of data or corpus are. In [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences,* we touched on some of the basic but important
    aspects of NLP and linguistics. We will use these concepts to derive features
    in this chapter. In [Chapter 4](59dc896d-b0de-44c9-9cbe-18c63e510897.xhtml), *Preprocessing,*
    we looked preprocessing techniques. Now, we will work on the corpus that we preprocessed
    and will generate features from that corpus.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是开发NLP应用中最重要的部分。特征是**机器学习**（**ML**）算法的输入参数。这些ML算法会根据输入特征生成输出。特征工程是一种艺术和技能，因为它生成最佳的特征，选择最合适的算法来开发NLP应用需要大量的努力，并且需要理解特征工程、NLP和ML算法。在[第2章](837b4260-d60f-474a-a208-68a1eaa8e1bb.xhtml)，*语料库和数据集的实用理解*中，我们看到数据是如何收集的，以及数据或语料库的不同格式是什么。在[第3章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)，*理解句子结构*中，我们讨论了一些NLP和语言学的基础但重要的方面。我们将在本章中使用这些概念来导出特征。在[第4章](59dc896d-b0de-44c9-9cbe-18c63e510897.xhtml)，*预处理*中，我们介绍了预处理技术。现在，我们将处理我们已经预处理过的语料库，并从中生成特征。
- en: 'Refer to *Figure 5.1*, which will help you understand all the stages that we
    have covered so far, as well as all the focus points of this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 参考*图5.1*，这将帮助你理解到目前为止我们所覆盖的所有阶段，以及本章的所有重点：
- en: '![](img/5e23efc2-689b-4d33-a782-6761259fe6bf.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e23efc2-689b-4d33-a782-6761259fe6bf.png)'
- en: 'Figure 5.1: An overview of the features generation process'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：特征生成过程概览
- en: You can refer to *Figure 1.4* in [Chapter 1](238411c1-88cf-4377-a6c6-7451e0f48daa.xhtml),
    *Introduction*. We covered the first four stages in the preceding three chapters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考[第1章](238411c1-88cf-4377-a6c6-7451e0f48daa.xhtml)中的*图1.4*，*介绍部分*。我们在前面的三章中已经覆盖了前四个阶段。
- en: 'In this chapter, we will mostly focus on the practical aspect of NLP applications.
    We will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注NLP应用的实践方面。我们将涵盖以下主题：
- en: What is feature engineering?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是特征工程？
- en: Understanding basic features of NLP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解NLP的基本特征
- en: Basic statistical features of NLP
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP的基本统计特征
- en: As well as this, we will explore topics such as how various tools or libraries
    are developed to generate features, what the various libraries that we can use
    are, and how you can tweak open source libraries or open source tools as and when
    needed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将探讨如何开发各种工具或库来生成特征，哪些库是我们可以使用的，以及在需要时如何调整开源库或开源工具。
- en: We will also look at the challenges for each concept. Here, we will not develop
    tools from scratch as it is out of the scope of this book, but we will walk you
    through the procedure and algorithms that are used to develop the tools. So if
    you want to try and build customized tools, this will help you, and will give
    you an idea of how to approach those kind of problem statements.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨每个概念的挑战。在这里，我们不会从零开始开发工具，因为这超出了本书的范围，但我们会带你了解开发这些工具所使用的过程和算法。所以，如果你想尝试构建定制化工具，这将对你有所帮助，并且会让你了解如何处理这类问题陈述。
- en: Understanding feature engineering
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: Before jumping into the feature generation techniques, we need to understand
    feature engineering and its purpose.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入特征生成技术之前，我们需要理解特征工程及其目的。
- en: What is feature engineering?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是特征工程？
- en: Feature engineering is the process of generating or deriving features (attributes
    or an individual measurable property of a phenomenon) from raw data or corpus
    that will help us develop NLP applications or solve NLP-related problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是从原始数据或语料库中生成或导出特征（现象的属性或可衡量的单独特性）的过程，这些特征将帮助我们开发NLP应用或解决NLP相关问题。
- en: A feature can be defined as a piece of information or measurable property that
    is useful when building NLP applications or predicting the output of NLP applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征可以定义为在构建NLP应用程序或预测NLP应用程序输出时有用的信息或可衡量的属性。
- en: We will use ML techniques to process the natural language and develop models
    that will give us the final output. This model is called the **machine learning
    model** (**ML model**). We will feed features for machine learning algorithms
    as input and to generate the machine learning model. After this, we will use the
    generated machine learning model to produce an appropriate output for an NLP application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用机器学习（ML）技术来处理自然语言，并开发模型以提供最终输出。这个模型被称为**机器学习模型**（**ML模型**）。我们将为机器学习算法提供特征作为输入，以生成机器学习模型。之后，我们将使用生成的机器学习模型为自然语言处理（NLP）应用程序生成适当的输出。
- en: If you're wondering what information can be a feature, then the answer is that
    any attribute can be a feature as long as it is useful in order to generate a
    good ML model that will produce the output for NLP applications accurately and
    efficiently. Here, your input features are totally dependent on your dataset and
    the NLP application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在想哪些信息可以成为特征，那么答案是：任何属性都可以成为特征，只要它在生成一个能够准确有效地产生NLP应用程序输出的优秀机器学习模型时是有用的。在这里，你的输入特征完全取决于你的数据集和NLP应用程序。
- en: Features are derived using domain knowledge for NLP applications. This is the
    reason we have explored the basic linguistics aspect of natural language, so that
    we can use these concepts in feature engineering.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是利用领域知识为NLP应用程序推导出的。这也是为什么我们要探索自然语言的基本语言学方面，以便能够在特征工程中使用这些概念。
- en: What is the purpose of feature engineering?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程的目的是什么？
- en: 'In this section, we will look at the major features that will help us to understand
    feature engineering:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看有助于我们理解特征工程的主要特征：
- en: We have raw data in natural language that the computer can't understand, and
    algorithms don't have the ability to accept the raw natural language and generate
    the expected output for an NLP application. Features play an important role when
    you are developing NLP applications using machine learning techniques.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有的是计算机无法理解的自然语言原始数据，算法也没有能力接受这些原始自然语言并生成NLP应用程序所期望的输出。当你使用机器学习技术开发NLP应用程序时，特征起着至关重要的作用。
- en: We need to generate the attributes that are representative for our corpus as
    well as those attributes that can be understood by machine learning algorithms.
    ML algorithms can understand only the language of feature for communication, and
    coming up with appropriate attributes or features is a big deal. This is the whole
    purpose of feature engineering.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要生成那些能代表我们的语料库的属性，并且能够被机器学习算法理解的属性。机器学习算法只能理解特征的语言进行沟通，而提出适当的属性或特征是一项重要任务。这就是特征工程的全部目的。
- en: Once we have generated the feature, we then need to feed them to the machine
    learning algorithm as input, and after processing these input features, we will
    get the ML model. This ML model will be used to predict or generate the output
    for new features. The ML models, accuracy and efficiency is majorly dependent
    on features, which is why we say that features engineering is a kind of art and
    skill.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们生成了特征，就需要将其作为输入传递给机器学习算法，经过处理这些输入特征后，我们将得到机器学习模型。这个机器学习模型将用于预测或生成新的特征输出。机器学习模型的准确性和效率主要依赖于特征，这也是为什么我们说特征工程是一种艺术和技能。
- en: Challenges
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'The following are the challenges involved in feature engineering:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是特征工程中涉及的挑战：
- en: Coming up with good features is difficult and sometimes complex.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提出好的特征是困难的，有时是复杂的。
- en: After generating features, we need to decide which features we should select
    this selection of features also plays a major role when we perform machine learning
    techniques on top of that. The process of selecting appropriate feature is called
    **feature selection**.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成特征后，我们需要决定应该选择哪些特征，这些特征的选择在我们进行机器学习技术时也起着重要作用。选择适当特征的过程被称为**特征选择**。
- en: Sometimes, during the feature selection, we need to eliminate some of the less
    important features, and this elimination of features is also a critical part of
    the feature engineering.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，在特征选择过程中，我们需要剔除一些不太重要的特征，这些特征的剔除也是特征工程中的一个关键部分。
- en: Manual feature engineering is time-consuming.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动特征工程非常耗时。
- en: Feature engineering requires domain expertise or, at least, basic knowledge
    about domains.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程需要领域专业知识，或者至少需要一些基础的领域知识。
- en: Basic feature of NLP
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的基本特征
- en: Apart from the challenges, NLP applications heavily rely on feature that are
    manually crafted based on various NLP concepts. From this point onwards, we will
    explore the basic features that are available in the NLP world. Let's dive in!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了挑战之外，NLP应用程序在很大程度上依赖于基于各种NLP概念手工制作的特征。从现在开始，我们将探讨NLP领域中可用的基本特征。让我们深入了解吧！
- en: Parsers and parsing
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析器和解析
- en: By parsing sentences, you can derive some of the most important features that
    can be helpful for almost every NLP application.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解析句子，你可以推导出一些对几乎所有NLP应用都非常重要的特征。
- en: We will explore the concept of parser and parsing. Later, we will understand
    **context-free grammar** (**CFG**) and **probabilistic context-free grammar**
    (**PCFG**). We will see how statistical parsers are developed. If you want to
    make your own parser, then we will explain the procedure to do so, or if you want
    to tweak the existing parser, then what steps you should follow. We will also
    do practical work using the available parser tools. We will look at the challenges
    later in this same section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨解析器和解析的概念。稍后，我们将理解**上下文无关文法**（**CFG**）和**概率上下文无关文法**（**PCFG**）。我们将了解统计解析器是如何开发的。如果你想创建自己的解析器，我们会解释如何进行，或者如果你想调整现有的解析器，我们也会告诉你应该遵循哪些步骤。我们还将使用现有的解析工具进行实际操作。稍后我们将在同一章节中讨论遇到的挑战。
- en: Understanding the basics of parsers
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解解析器的基础
- en: Here, I'm going to explain parser in terms of the NLP domain. The parser concept
    is also present in other computer science domains, but let's focus on the NLP
    domain and start understanding parser and what it can do for us.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我将从NLP领域的角度解释解析器。解析器的概念也出现在其他计算机科学领域，但我们将聚焦于NLP领域，开始理解解析器及其能为我们做什么。
- en: In the NLP domain, a parser is the program or, more specifically, tool that
    takes natural language in the form of a sentence or sequence of tokens. It breaks
    the input stream into smaller chunks. This will help us understand the syntactic
    role of each element present in the stream and the basic syntax-level meaning
    of the sentence. In NLP, a parser actually analyzes the sentences using the rules
    of context-free grammar or probabilistic context-free grammar. We have seen CFG
    in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure
    of Sentences*.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP领域，解析器是一个程序，或者更具体地说，是一个工具，它接受以句子或一系列标记的形式呈现的自然语言。它将输入流拆分成更小的部分。这有助于我们理解流中每个元素的句法作用以及句子的基本语法层次含义。在NLP中，解析器实际上是使用上下文无关文法或概率上下文无关文法的规则来分析句子的。我们在[第3章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)中已经介绍了CFG，*理解句子的结构*。
- en: A parser usually generates output in the form of a parser tree or abstract syntax
    tree. Let's see some of the example parser trees here. There are certain grammar
    rules that parsers use to generate the parse tree with single words or lexicon
    items.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器通常生成解析树或抽象语法树形式的输出。我们将在这里看一些解析树的例子。解析器使用某些文法规则生成包含单词或词汇项的解析树。
- en: 'See the grammar rules in *Figure 5.2*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见*图5.2*中的文法规则：
- en: '![](img/3ea61a64-940d-4a7e-9c11-0d302aec1cbc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ea61a64-940d-4a7e-9c11-0d302aec1cbc.png)'
- en: 'Figure 5.2: Grammar rules for parser'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：解析器的文法规则
- en: 'Let''s discuss the symbols first:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先讨论一下符号：
- en: '**S** stands for sentence'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S** 代表句子'
- en: '**NP** stands for noun phrase'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NP** 代表名词短语'
- en: '**VP** stands for verb phrase'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VP** 代表动词短语'
- en: '**V** stands for verb'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V** 代表动词'
- en: '**N** stand for noun'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N** 代表名词'
- en: '**ART** stands for article a, an, or the'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ART** 代表冠词 a、an 或 the'
- en: 'See a parse tree generated using the grammar rules in *Figure 5.3*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见使用*图5.3*中的文法规则生成的解析树：
- en: '![](img/6e4b587a-30e3-459f-be47-c84cc79ef02b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e4b587a-30e3-459f-be47-c84cc79ef02b.png)'
- en: 'Figure 5.3: A parse tree as per the grammar rules defined in Figure 5.2'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：根据图5.2中定义的文法规则生成的解析树
- en: Here in F*igure 5.3,* we converted our sentence into the parse tree format,
    and as you can see, each word of the sentence is expressed by the symbols of the
    grammar that we already defined in F*igure 5.2.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.3*中，我们将句子转换为解析树格式，正如你所见，句子的每个单词都通过我们在*图5.2*中定义的文法符号表示。
- en: 'There are two major types of parsers. We are not going to get into the technicality
    of each type of parser here because it''s more about the compiler designing aspect.
    Instead, we will explore the different types of parsers, so you can get some clarity
    on which type of parser we generally use in NLP. Refer to *Figure 5.4*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器主要有两种类型。我们不会深入探讨每种解析器的技术细节，因为它更多涉及到编译器设计方面。相反，我们将探索不同类型的解析器，以便你能清楚地了解在自然语言处理（NLP）中我们通常使用哪种解析器。请参阅*图
    5.4*：
- en: '![](img/d32ee1e7-1041-486a-a143-46e2dced8b8e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d32ee1e7-1041-486a-a143-46e2dced8b8e.png)'
- en: 'Figure 5.4: Types of parser'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：解析器类型
- en: We will also look at the differences between top-down parsers and bottom-up
    parsers in the next section, as the difference is related to the process; this
    will be followed by each of the parsers so that we understand the difference.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们还将比较自顶向下解析器和自底向上解析器之间的区别，因为这种区别与过程相关，接下来将介绍每种解析器，以便我们理解它们之间的不同。
- en: Let's jump into the concept of parsing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解解析的概念。
- en: Understanding the concept of parsing
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解解析的概念
- en: 'First of all, let''s discuss what parsing is. Let''s define the term parsing.
    Parsing is a formal analysis or a process that uses a sentence or the stream of
    tokens, and with the help of defined formal grammar rules, we can understand the
    sentence structure and meaning. So, parsing uses each of the words in the sentence
    and determines its structure using a constituent structure. What is a consistent
    structure? A constituent structure is based on the observation of which words
    combine with other words to form a sensible sentence unit. So, in the English
    language, the subject mostly comes first in the sentence; the sentence **He is
    Tom** makes sense to us, whereas the sentence **is Tom he** doesn''t make sense.
    By parsing, we actually check as well as try to obtain a sensible constituent
    structure. These are the following points that will explain what parser and parsing
    does for us:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来讨论什么是解析。我们来定义一下“解析”这个术语。解析是一个形式化分析过程，利用句子或符号流，并借助定义的正式语法规则，我们可以理解句子的结构和含义。所以，解析使用句子中的每一个单词，并通过构成结构来确定其结构。什么是构成结构呢？构成结构是基于观察哪些单词与其他单词结合形成合理的句子单位。在英语中，主语通常位于句子的开头；句子**He
    is Tom**对我们来说是有意义的，而句子**is Tom he**则没有意义。通过解析，我们实际上是检查并尝试获得一个合理的构成结构。以下是解析器和解析为我们所做的几点说明：
- en: The parser tool performs the process of parsing as per the grammar rules and
    generates a parse tree. This parse tree structure is used to verify the syntactical
    structure of the sentence. If a parse tree of the sentence follows the grammar
    rules as well as generates a meaningful sentence, then we say that the grammar
    as well as the sentence generated using that grammar is valid.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析器工具根据语法规则执行解析过程，并生成一个解析树。这个解析树结构用于验证句子的句法结构。如果句子的解析树遵循语法规则并生成一个有意义的句子，那么我们就说该语法以及使用该语法生成的句子是有效的。
- en: At the end of the parsing, a parse tree is generated as output that will help
    you to detect ambiguity in the sentence because. Ambiguous sentences often result
    in multiple parse trees.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解析的最后，生成一个解析树作为输出，它将帮助你检测句子中的歧义，因为歧义句子往往会产生多个解析树。
- en: 'Let''s see the difference between a top-down parser and a bottom-up parser:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看自顶向下解析器和自底向上解析器之间的区别：
- en: '| **Top-down parsing** | **Bottom-up parsing** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **自顶向下解析** | **自底向上解析** |'
- en: '| Top-down parsing is hypothesis-driven. | Bottom-up parsing is data-driven.
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 自顶向下解析是由假设驱动的。 | 自底向上解析是由数据驱动的。 |'
- en: '| At each stage of parsing, the parser assumes a structure and takes a word
    sequentially from the sentence and tests whether the taken word or token fulfills
    the hypothesis or not. | In this type of parsing, the first words are taken from
    the input string, then the parser checks whether any predefined categories are
    there in order to generate a valid sentence structure, and lastly, it tries to
    combine them into acceptable structures in the grammar. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 在每个解析阶段，解析器假设一个结构，并按顺序从句子中取出一个单词，测试该单词或符号是否符合假设。 | 在这种解析方法中，首先从输入字符串中取出单词，然后解析器检查是否存在任何预定义的类别，以生成有效的句子结构，最后尝试将它们组合成语法中可接受的结构。
    |'
- en: '| It scans a sentence in a left-to-right manner. When grammar production rules
    derive lexical items, the parser usually checks with the input to see whether
    the right sentence is being derived or not. | This kind of parsing starts with
    the input string of terminals. This type of parsing searches for substrings of
    the working string because if any string or substring matches the right-hand side
    production rule of grammar, then it substitutes the left-hand side non-terminal
    for the matching right-hand side rule. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 它以从左到右的方式扫描句子。当语法生成规则推导出词汇项时，解析器通常会检查输入，以查看是否正在推导出正确的句子。 | 这种解析通常从终结符的输入字符串开始。这种类型的解析会查找工作字符串的子字符串，因为如果任何字符串或子字符串与语法的右侧生成规则匹配，它就会用匹配的右侧规则替换左侧非终结符。
    |'
- en: '| It includes a backtracking mechanism. When it is determined that the wrong
    rule has been used, it backs up and tries another rule. | It usually doesn''t
    include a backtracking mechanism. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 它包括回溯机制。当确定使用了错误的规则时，它会回退并尝试另一个规则。 | 它通常不包括回溯机制。 |'
- en: You will get to know how this parser has been built in the following section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在接下来的章节中了解这个解析器是如何构建的。
- en: Developing a parser from scratch
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始开发一个解析器
- en: In this section, we will try to understand the procedure of the most famous
    Stanford parser, and which algorithm has been used to develop the most successful
    statistical parser.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试理解最著名的斯坦福解析器的过程，以及开发最成功的统计解析器时使用了哪些算法。
- en: In order to get an idea about the final procedure, we need to first understand
    some building blocks and concepts. Then, we will combine all the concepts to understand
    the overall procedure of building a statistical parser such as the Stanford parser.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解最终的过程，我们需要首先理解一些构建块和概念。然后，我们将结合所有这些概念，理解构建统计解析器（如斯坦福解析器）的整体过程。
- en: Types of grammar
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语法的类型
- en: In this section, we will see two types of grammar that will help us understand
    the concept of how a parser works. As a prerequisite, we will simply explain them
    and avoid getting too deep into the subject. We will make them as simple as possible
    and we will explore the basic intuition of the concepts that will be used to understand
    the procedure to develop the parser. Here we go!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到两种语法类型，这些语法类型将帮助我们理解解析器如何工作的概念。作为先决条件，我们将简单地解释它们，避免过于深入地探讨该主题。我们会尽量简化它们，并探讨一些基本直觉，以帮助理解开发解析器的过程。开始吧！
- en: 'There are two types of grammar. You can refer to *Figure 5.5*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种语法类型。你可以参考*图5.5*：
- en: Context-free grammar
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文无关文法
- en: Probabilistic context-free grammar
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率上下文无关文法
- en: '![](img/7bd04ffa-e515-45d9-ad77-e24594b9a306.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7bd04ffa-e515-45d9-ad77-e24594b9a306.png)'
- en: 'Figure 5.5: Types of grammar'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：语法的类型
- en: Context-free grammar
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文无关文法
- en: We have seen the basic concept of context-free grammar in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences*. We have already seen the formal definition
    of the CFG to find a moment and recall it. Now we will see how the rules of grammar
    are important when we build a parser.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)《理解句子结构》中已经看过上下文无关文法的基本概念。我们已经看过CFG的正式定义，可以稍微回忆一下。现在，我们将了解在构建解析器时，语法规则有多么重要。
- en: 'CFG is also referred to as phrase structure grammar. So, CFG and phrase structure
    grammar are two terms but refer to one concept. Now, let''s see some examples
    related to this type of grammar and then talk about the conventions that are followed
    in order to generate a more natural form of grammar rules. Refer to the grammar
    rules, lexicons, and sentences in *Figure 5.6*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: CFG也被称为短语结构语法。所以，CFG和短语结构语法是两个术语，但指代同一个概念。现在，让我们看一些与这种语法类型相关的例子，然后讨论为了生成更自然的语法规则形式所遵循的约定。参考*图5.6*中的语法规则、词汇和句子：
- en: '![](img/420f73a9-3604-49b4-a6bf-9f4ac11b87b3.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/420f73a9-3604-49b4-a6bf-9f4ac11b87b3.png)'
- en: 'Figure 5.6: CFG rules, lexicon, and sentences'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：CFG规则、词汇和句子
- en: Here, **S** is the starting point of grammar. **NP** stands for noun phrase
    and **VP** stands for verb phrase. Now we will apply top-down parsing and try
    to generate the given sentence by starting from the rule with the right-hand side
    non-terminal **S** and substitute **S** with **NP** and **VP**. Now substitute
    **NP** with **N** and **VP** with **V** and **NP**, and then substitute **N**
    with people. Substitute **V** with **fish**, **NP** with **N**, and **N** with
    **tank**. You can see the pictorial representation of the given process in *Figure
    5.7:*
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**S** 是语法的起点。**NP** 代表名词短语，**VP** 代表动词短语。现在我们将应用自上而下的解析法，从右侧非终结符 **S** 的规则开始，并用
    **NP** 和 **VP** 替代 **S**。接着，用 **N** 替代 **NP**，用 **V** 和 **NP** 替代 **VP**，然后用 people
    替代 **N**。将 **V** 替代为 **fish**，**NP** 替代为 **N**，并将 **N** 替代为 **tank**。你可以在*图 5.7*
    中看到这一过程的示意图：
- en: '![](img/22964f07-06d4-4c80-854a-804aa52f7962.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22964f07-06d4-4c80-854a-804aa52f7962.png)'
- en: 'Figure 5.7: A parse tree representation of one of the sentences generated by
    the given grammar'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：由给定语法生成的一个句子的解析树表示
- en: 'Now try to generate a parse tree for the second sentence on your own. If you
    play around with this grammar rule for a while, then you will soon discover that
    it is a very ambiguous one. As well as this we will also discuss the more practical
    aspect of CFG that is used by linguists in order to derive the sentence structure.
    This is a more natural form of CFG and is very similar to the formal definition
    of CFG with one minor change, that is, we define the preterminal symbols in this
    grammar. If you refer to *Figure 5.7*, you will see that symbols such as **N,
    V** are called **preterminal symbols**. Now look at the definition of the natural
    form of CFG in *Figure 5.8*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，试着为第二个句子自己生成一个解析树。如果你玩一下这个语法规则，你很快就会发现它是一个非常模糊的规则。除此之外，我们还将讨论语言学家用来推导句子结构的更实际的上下文无关文法（CFG）。这是一种更自然的CFG形式，非常类似于CFG的正式定义，唯一的不同之处是，我们在这种语法中定义了预终结符符号。如果你参考*图
    5.7*，你会看到像 **N, V** 这样的符号被称为**预终结符号**。现在，看看*图 5.8* 中自然形式的CFG定义：
- en: '![](img/c26804c6-9b02-4957-9056-1ef8fa1843c4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c26804c6-9b02-4957-9056-1ef8fa1843c4.png)'
- en: 'Figure 5.8: A formal representation of a more natural form of CFG'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：更自然形式的CFG的正式表示
- en: Here, the ***** symbol includes the existence of an empty sequence. We are starting
    from the **S** symbol, but in a statistical parser we add one more stage, which
    is TOP or ROOT. Therefore, when we generate the parse tree, the main top most
    node is indicated by **S.** Please refer to *Figure 5.7* for more information*.*
    Now we will put an extra node with the symbol ROOT or TOP before **S.**
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*****符号表示空序列的存在。我们从 **S** 符号开始，但在统计解析器中，我们增加了一个额外的阶段，即 TOP 或 ROOT。因此，当我们生成解析树时，最顶层的主节点由
    **S** 表示。更多信息请参见*图 5.7*。现在，我们将在 **S** 之前加上一个带有 ROOT 或 TOP 符号的额外节点。
- en: You may have noticed one weird rule in *Figure 5.6*. **NP** can be substituted
    using **e**, which stands for an empty string, so let's see what the use of that
    empty string rule is. We will first look at see an example to get a detailed idea
    about this type of grammar as well as the empty string rule. We will begin with
    the concept of a preterminal because it may be new to you. Take a noun phrase
    in the English language--any phrase containing a determiner such as a, an, or
    the, along with the noun itself. When you substitute **NP** with the symbol **DT**
    and **NN**, you enter actual lexical terminals; where we substitute **NP** with
    **DT** and **NN**, it is called the preterminal symbol. Now let's talk about the
    empty string rule. We have included this rule because, in real life, you will
    find various instances where there are missing parts to a sentence. To handle
    these kinds of instances, we put this empty string rule in grammar. We will now
    give you an example that will help you.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到在*图 5.6* 中有一个奇怪的规则。**NP** 可以使用 **e** 进行替代，**e** 代表空字符串。让我们来看看这个空字符串规则的用途。我们将首先通过一个例子来详细了解这种语法和空字符串规则的作用。我们将从预终结符的概念开始，因为这对你来说可能是新的。以英语中的名词短语为例——任何包含限定词（如
    a、an 或 the）以及名词本身的短语。当你将 **NP** 替代为符号 **DT** 和 **NN** 时，你进入了实际的词汇终结符；而当我们将 **NP**
    替代为 **DT** 和 **NN** 时，这被称为预终结符符号。现在，让我们谈谈空字符串规则。我们引入这个规则是因为在实际生活中，你会遇到许多句子缺失部分的情况。为了处理这些情况，我们在语法中加入了这个空字符串规则。接下来，我们将给你一个例子，帮助你理解。
- en: 'We have seen the word sequence, **people fish tank**. From this, you can extract
    two phrases: one is **fish tank** and the second is **people fish**. In both examples,
    there are missing nouns. We will represent these phrases as **e fish tank** and
    **people fish e**. Here, **e** stands for empty string. You will notice that in
    the first phrase, there is a missing noun at the start of the phrase; more technically,
    there is a missing subject. In the second case, there is a missing noun at the
    end of the phrase; more technically, there is a missing object. These kinds of
    situations are very common when dealing with real **natural language** (**NL**)
    data.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了词序列**people fish tank**。从中，你可以提取出两个短语：一个是**fish tank**，另一个是**people fish**。在这两个例子中，都缺少名词。我们将这些短语表示为**e
    fish tank**和**people fish e**。这里，**e**代表空字符串。你会注意到，在第一个短语中，短语的开头缺少一个名词；更准确地说，缺少的是主语。在第二个例子中，短语的末尾缺少一个名词；更准确地说，缺少的是宾语。这类情况在处理真实的**自然语言**（**NL**）数据时非常常见。
- en: There is one last thing we need to describe, which we will use in the topic
    on **grammar transformation**. Refer to *Figure 5.6*, where you will find the
    rules. Keep referring to these grammar rules as you go. The rule that has only
    an empty string on its right-hand side is called an **empty rule**. You can see
    that there are some rules that have just one symbol on their right side as well
    as on their left side; they are called **unary rules** because you can rewrite
    one category into another category, for example, **NP -> N**. There are also some
    other rules that have two symbols on their right side, such as **VP -> V NP**.
    These kinds of rules are called **binary rules.** There are also some rules that
    have three symbols on their right-hand side; we certain apply some techniques
    to get rid of the kind of rules that have more than two symbols on the right-hand
    side. We will look at these shortly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事我们需要描述，这将在**语法转换**主题中用到。请参考*图 5.6*，你将看到相关规则。在阅读时，请继续参考这些语法规则。右侧只有空字符串的规则被称为**空规则**。你可以看到有些规则的右侧和左侧都有一个符号，它们被称为**一元规则**，因为你可以将一个类别转换为另一个类别，例如**NP
    -> N**。还有一些规则，它们的右侧有两个符号，比如**VP -> V NP**。这些规则被称为**二元规则**。另外，也有一些规则，它们的右侧有三个符号；我们会应用一些技巧来去除右侧有超过两个符号的规则。我们将很快讨论这些。
- en: Now we have looked at CFG, as well as the concepts needed to understand it.
    You will be able to connect those dots in the following sections. It's now time
    to move on to the next section, which will give you an idea about probabilistic
    CFG.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了CFG以及理解它所需的概念。你将在接下来的章节中将这些知识串联起来。现在是时候进入下一部分，它将让你对概率CFG有个更清晰的了解。
- en: Probabilistic context-free grammar
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率上下文无关语法
- en: In probabilistic grammar, we add the concept of probability. Don't worry - it's
    one of the most simple extensions of CFG that we've seen so far. We will now look
    at **probabilistic context-free grammar** (**PCFG**).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率语法中，我们加入了概率的概念。别担心——这是我们到目前为止看到的最简单的上下文无关语法（CFG）扩展。接下来，我们将看一下**概率上下文无关语法**（**PCFG**）。
- en: 'Let''s define PCFG formally and then explore a different aspect of it. Refer
    to *Figure 5.9*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式定义概率上下文无关语法（PCFG），然后探讨它的另一个方面。请参考*图 5.9*：
- en: '![](img/5bb01103-db44-4d79-8ea5-3076319ed1da.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bb01103-db44-4d79-8ea5-3076319ed1da.png)'
- en: 'Figure 5.9: PCFGs formal definition'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：PCFG的正式定义
- en: 'Here, *T*, *N*, *S*, and *R* are similar to CFG; the only new thing here is
    the probability function, so let''s look at that here, the probability function
    takes each grammar rule and gives us the probability value of each rule. This
    probability maps to a real number, *R*. The range for *R* is [0,1]. We are not
    blindly taking any probability value. We enter one constraint where we have defined
    that the sum of the probability for any non-terminal should add up to 1\. Let''s
    look at an example to understand things. You can see the grammar rules with probability
    in *Figure 5.10*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*T*、*N*、*S* 和 *R* 与CFG类似；唯一的新内容是概率函数，所以让我们在这里看看，概率函数接受每个语法规则，并给出每个规则的概率值。这个概率映射为一个实数，*R*。*R*的范围是[0,1]。我们并不是盲目地取任意的概率值。我们设定了一个约束，规定任何非终结符的概率总和应该等于1。让我们看一个例子来理解这些内容。你可以在*图
    5.10*中看到带有概率的语法规则：
- en: '![](img/e01d4e08-6d4e-4144-9d03-46402eabf20c.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e01d4e08-6d4e-4144-9d03-46402eabf20c.png)'
- en: 'Figure 5.10: Probabilities for grammar rules'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：语法规则的概率
- en: 'You can see the lexical grammar rules with probability in *Figure 5.11*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.11*中看到带有概率的词汇语法规则：
- en: '![](img/6c4be3fe-94eb-4509-9a67-a2f9808fe44d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c4be3fe-94eb-4509-9a67-a2f9808fe44d.png)'
- en: 'Figure 5.11: Probabilities for lexical rules'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：词汇规则的概率
- en: 'As you can see, *Figure 5.10* has three NP rules, and if you look at the probability
    distribution, you will notice the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，*图 5.10* 中有三个 NP 规则，如果你看一下概率分布，你会注意到以下几点：
- en: Its probability adds up to 1 (0.1 + 0.2 + 0.7 = 1.0)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的概率相加为 1（0.1 + 0.2 + 0.7 = 1.0）
- en: It is likely that NP is further rewritten as a noun as its probability is 0.7
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能 NP 会进一步重写为名词，因为它的概率是 0.7
- en: In the same way, you can see that the first rule at the start of the sentence
    has a value of 1.0 because of a certain event that occurred first. If you look
    carefully, you'll notice that we have removed the empty string rule to make our
    grammar less ambiguous.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你可以看到，句子开头的第一个规则值为 1.0，这是由于某个先发生的事件。如果你仔细观察，你会注意到我们去除了空字符串规则，以使我们的语法更少歧义。
- en: So, how are we going to use these probability values? This question leads us
    to the description of calculating the probability of trees and strings.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何使用这些概率值呢？这个问题引导我们进入计算树和字符串概率的描述。
- en: Calculating the probability of a tree
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算树的概率
- en: If we want to calculate the probability of a tree, it is quite easy because
    you need to multiply the probability values of lexicons and grammar rules. This
    will give us the probability of a tree.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要计算一棵树的概率，那是相当简单的，因为你只需要将词汇和语法规则的概率值相乘。这将给我们树的概率。
- en: Let's look at an example to understand this calculation. Here, we will take
    two trees and the sentence for which we have generated trees, **people fish tank
    with rods**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子来理解这个计算。这里，我们将选择两棵树和为其生成的句子，**people fish tank with rods**。
- en: 'Refer to *Figure 5.12* and *Figure 5.13* for tree structures with their respective
    probability values before we calculate the probability of each tree:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 *图 5.12* 和 *图 5.13* 中的树结构及其相应的概率值，然后再计算每棵树的概率：
- en: '![](img/3640290e-0d53-48e2-81c2-8f0bc269dba3.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3640290e-0d53-48e2-81c2-8f0bc269dba3.png)'
- en: 'Figure 5.12: Parse Tree'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：解析树
- en: 'If we want calculate the probability for the parse tree given in *Figure 5.12*,
    then the steps of obtaining the probability is given as follows. We start scanning
    the tree from the top, so our string point is **S**, the top most node of the
    parse tree. Here, the preposition modifies the verb:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要计算图 *5.12* 中给出的解析树的概率，获取该概率的步骤如下所示。我们从树的顶部开始扫描，所以我们的字符串点是 **S**，解析树的最顶端节点。在这里，介词修饰动词：
- en: '*P(t1)* = 1.0 * 0.7 * 0.4 * 0.5 * 0.6 * 0.7 * 1.0 * 0.2 * 1.0 * 0.7 * 0.1 =
    0.0008232'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(t1)* = 1.0 * 0.7 * 0.4 * 0.5 * 0.6 * 0.7 * 1.0 * 0.2 * 1.0 * 0.7 * 0.1 =
    0.0008232'
- en: 'The value 0.0008232 is the probability of the tree. Now you can calculate the
    same for another parse tree given in *Figure 5.13*. In this parse tree, the preposition
    modifies the noun. Calculate the tree probability for this parse tree:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 0.0008232 是树的概率。现在，你可以为另一个解析树（如图 *5.13* 所示）计算相同的概率。在这个解析树中，介词修饰名词。计算这个解析树的树概率：
- en: '![](img/b83b9f10-0d93-4d19-8163-32e20cc1a3ef.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b83b9f10-0d93-4d19-8163-32e20cc1a3ef.png)'
- en: 'Figure 5.13: Second parse tree'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：第二棵解析树
- en: If you calculate the parse tree probability, the value should be 0.00024696.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算解析树的概率，值应该是 0.00024696。
- en: Now let's look at the calculation of the probability of string that uses the
    concept of the probability of a tree.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看使用树的概率概念计算字符串概率。
- en: Calculating the probability of a string
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算字符串的概率
- en: Calculating the probability of a string is more complex compared to calculating
    the probability of a tree. Here, we want to calculate the probability of strings
    of words, and for that we need to consider all the possible tree structures that
    generate the string for which we want to calculate the probability. We first need
    to consider all the trees that have the string as part of the tree and then calculate
    the final probability by adding the different probabilities to generate the final
    probability value.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于计算树的概率，计算字符串的概率要复杂得多。在这里，我们想要计算由词组成的字符串的概率，为此我们需要考虑所有可能生成该字符串的树结构。我们首先需要考虑所有包含该字符串的树，然后通过将不同的概率值加在一起，计算最终的概率值。
- en: 'Let''s revisit *Figure 5.12* and *Figure 5.13*, which we used to calculate
    the tree probability. Now, in order to calculate the probability of the string,
    we need to consider both the tree and the tree probability and then add those.
    Calculate the probability of the string as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下我们用来计算树概率的*图 5.12*和*图 5.13*。现在，为了计算字符串的概率，我们需要同时考虑树和树的概率，然后将其加起来。按照以下步骤计算字符串的概率：
- en: '*P(S) = P(t1) +P(t2)*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(S) = P(t1) +P(t2)*'
- en: '*= 0.0008232 + 0.00024696*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.0008232 + 0.00024696*'
- en: '*= 0.00107016*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.00107016*'
- en: Here, *t1* tree has a high probability, so a **VP**-attached sentence structure
    is more likely to be generated compared to *t2*, which has **NP** attached to
    it. The reason behind this is that *t1* has a **VP** node with *0.4*, whereas
    *t2* has two nodes, **VP** with a probability of *0.6* and **NP** with a probability
    of *0.2* probability. When you multiply this, you will get *0.12*, which is less
    than *0.4*. So, the *t1* parse tree is the most likely structure.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t1* 树有很高的概率，所以比起附有**NP**的 *t2*，更可能生成附有**VP**的句子结构。原因在于，*t1* 有一个带有*0.4*的**VP**节点，而
    *t2* 有两个节点，一个是**VP**，其概率为*0.6*，另一个是**NP**，其概率为*0.2*。当你将这两个概率相乘时，你得到*0.12*，这比*0.4*小。所以，*t1*
    解析树是最有可能的结构。
- en: You should now understand the different types of grammar. Now, it's time to
    explore the concept of grammar transformation for efficient parsing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该理解了不同类型的语法。接下来，是时候探索语法转换的概念，以提高解析效率了。
- en: Grammar transformation
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语法转换
- en: Grammar transformation is a technique used to make grammar more restrictive,
    which makes the parsing process more efficient. We will use **Chomsky Normal Form**
    (**CNF**) to transform grammar rules. Let's explore CNF before looking at an example.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 语法转换是一种使语法更加限制的技术，从而提高解析过程的效率。我们将使用**查姆斯基范式**（**CNF**）来转换语法规则。在看一个例子之前，让我们先探讨一下
    CNF。
- en: 'Let''s see CNF first. It states that all rules should follow the following
    rules:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看 CNF。它说明所有规则应遵循以下规则：
- en: '*X-> Y Z* or *X-> w* where *X, Y, Z* *ε N* and *w* *ε T*'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*X-> Y Z* 或 *X-> w* 其中 *X, Y, Z* 属于 N，*w* 属于 T'
- en: 'The meaning of the rule is very simple. You should not have more than two non-terminals
    on the right-hand side of any grammar rule; you can include the rules where the
    right-hand side of the rule has a single terminal. To transform the existing grammar
    into CNF, there is a basic procedure that you can follow:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规则的意思非常简单。任何语法规则的右侧不能有超过两个非终结符；你可以包括那些右侧只有一个终结符的规则。为了将现有的语法转化为 CNF（查姆斯基范式），你可以遵循以下基本步骤：
- en: Empty rules and unary rules can be removed using recursive functions.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空规则和单一规则可以通过递归函数去除。
- en: '*N*-ary rules are divided by introducing new non-terminals in the grammar rules.
    This applies to rules that have more than two non-terminals on the right-hand
    side. When you use CNF, you can get the same string using new transform rules,
    but its parse structure may differ. The newly generated grammar after applying
    CNF is also CFG.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N*元规则通过在语法规则中引入新的非终结符来划分。这适用于右侧有两个以上非终结符的规则。当你使用 CNF 时，你可以通过新的转换规则得到相同的字符串，但它的解析结构可能会有所不同。应用
    CNF 后生成的新语法仍然是 CFG（上下文无关文法）。'
- en: 'Let''s look at the intuitive example. We take the grammar rules that we defined
    earlier in *Figure 5.6* and apply CNF to transform those grammar rules. Let''s
    begin. See the following steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个直观的例子。我们采用之前在*图 5.6*中定义的语法规则，并应用 CNF 来转换这些语法规则。让我们开始吧。请看以下步骤：
- en: We first remove the empty rules. When you have **NP** on the right-hand side,
    you can have two rules such as **S -> NP VP**, and when you put an empty value
    for **NP**, you will get **S -> VP**. By applying this method recursively, you
    will get rid of the empty rule in the grammar.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先去除空规则。当右侧有**NP**时，你可以有两个规则，例如**S -> NP VP**，而当你为**NP**设置空值时，你会得到**S -> VP**。通过递归应用这种方法，你将去除语法中的空规则。
- en: Then, we must try to remove unary rules. So, in this case, if you try to remove
    the first unary rule **S -> VP**, then you need to consider all the rules that
    have **VP** on their left-hand side. When you do this, you need to introduce new
    rules because **S** will immediately go to **VP**. We will introduce the rule,
    **S -> V NP**. You need to keep doing this until you get rid of the unary rules.
    When you remove all the unary rules, such as **S -> V**, then you also need to
    change your lexical entries.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须尝试去除单一规则。因此，在这种情况下，如果你尝试去除第一个单一规则**S -> VP**，那么你需要考虑所有左边是**VP**的规则。当你这样做时，你需要引入新规则，因为**S**将立即转到**VP**。我们将引入规则**S
    -> V NP**。你需要继续这样做，直到去除所有的单一规则。当你去掉所有的单一规则，如**S -> V**，你还需要改变词汇项。
- en: 'Refer to *Figure 5.14* for the CNF process:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图 5.14*，了解 CNF 过程：
- en: '![](img/e586546b-4f32-4676-9efa-b238a673835f.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e586546b-4f32-4676-9efa-b238a673835f.png)'
- en: 'Figure 5.14: CNF steps 1 to 5'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：CNF 步骤 1 到 5
- en: You can see the final result of the CNF process in *Figure 5.15:*
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.15*中看到 CNF 过程的最终结果。
- en: '![](img/87990088-3a73-46d7-9fc3-816c1da8070c.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87990088-3a73-46d7-9fc3-816c1da8070c.png)'
- en: 'Figure 5.15: Step 6 - Final grammar rules after applying CNF'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：步骤 6 - 应用 CNF 后的最终语法规则
- en: In real life, it is not necessary to apply full CNF, and it can often be quite
    painful to do so. It just makes parsing more efficient and your grammar rules
    cleaner. In real-life applications, we keep unary rules as our grammar rules because
    they tell us whether a word is treated as a verb or noun, as well as the non-terminal
    symbol information, which means that we have the information of the POS tag.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，并不一定需要应用完整的 CNF（范式），而且这样做往往会相当痛苦。它只是让解析更高效，使你的语法规则更简洁。在实际应用中，我们保留单一规则作为语法规则，因为它们告诉我们一个词是作为动词还是名词使用，以及非终结符号信息，这意味着我们拥有词性标签（POS）的信息。
- en: That's enough of the boring conceptual part. Now it's time to combine all the
    basic concepts of parsers and parsing to learn the algorithm that is used to develop
    a parser.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，够了，别再讲那些无聊的概念部分了。现在是时候将所有解析器和解析的基本概念结合起来，学习开发解析器所用的算法了。
- en: Developing a parser with the Cocke-Kasami-Younger Algorithm
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Cocke-Kasami-Younger 算法开发解析器
- en: For the English language, there are plenty of parsers that you can use, CNF
    if you want to build a parser for any other language, you can use the **Cocke-Kasami-Younger**
    (**CKY**) algorithm. Here, we will look at some information that will be useful
    to you in terms of making a parser. We will also look at the main logic of the
    CKY algorithm.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于英语语言，有许多你可以使用的解析器，若要为其他语言构建解析器，可以使用**Cocke-Kasami-Younger**（**CKY**）算法。在这里，我们将看看一些对你开发解析器有帮助的信息。我们还将看看
    CKY 算法的主要逻辑。
- en: We need to look at the assumption that we are considering before we start with
    the algorithm. Our technical assumption is that, here, each of the parser subtrees
    is independent. This means that if we have a tree node NP, then we just focus
    on this NP node and not on the node its, derived from; each of the subtrees act
    independently. The CKY algorithm can give us the result in cubic time.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在开始算法之前，先了解我们所考虑的假设。我们的技术假设是，在这里，每个解析器子树都是独立的。这意味着，如果我们有一个树节点 NP，那么我们只关注这个
    NP 节点，而不是它派生出来的节点；每个子树都是独立作用的。CKY 算法可以在立方时间内给出结果。
- en: 'Now let''s look at the logic of the CKY algorithm. This algorithm takes words
    from the sentences and tries to generate a parse tree using bottom-up parsing.
    Here, we will define a data structure that is called a **parse triangle** or **chart**.
    Refer to *Figure 5.16*:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下 CKY 算法的逻辑。该算法从句子中提取单词，并尝试使用自下而上的解析方法生成解析树。在这里，我们将定义一个被称为**解析三角形**或**图表**的数据结构。参见*图
    5.16*：
- en: '![](img/59c8c4e8-4528-4ba0-ba97-e1933d264000.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59c8c4e8-4528-4ba0-ba97-e1933d264000.png)'
- en: 'Figure 5.16: Parse triangle for the CKY algorithm'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：CKY 算法的解析三角形
- en: 'Its bottom cells represent single words such as **fish**, **people**, **fish**,
    and **tanks**. The cells in the middle row represent the overlapped word pairs
    such as **Fish people**, **People fish**, and **fish tanks**. Its third row represents
    the pair of two words without overlapping such as **Fish people** and **fish tanks**.
    The last row represents the top or root of the sentence. To understand the algorithm,
    we first need the grammar rules of rule probability. To understand the algorithm,
    we should refer to *Figure 5.17*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 它的底部单元格表示单个单词，如**fish**、**people**、**fish**和**tanks**。中间行的单元格表示重叠的单词对，如**Fish
    people**、**People fish**和**fish tanks**。第三行表示没有重叠的两个单词的对，如**Fish people**和**fish
    tanks**。最后一行表示句子的顶部或根节点。为了理解这个算法，我们首先需要语法规则的规则概率。为了理解算法，我们应该参考*图5.17*：
- en: '![](img/7e868283-8d6c-44ab-b678-3576643852a0.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e868283-8d6c-44ab-b678-3576643852a0.png)'
- en: 'Figure 5.17: To understand the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com/nlp/slides/Parsing-Probabilistic.pdf
    page no: 36)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17：理解CKY算法（图片来源：http://spark-public.s3.amazonaws.com/nlp/slides/Parsing-Probabilistic.pdf
    第36页）
- en: 'As shown in *Figure 5.17*, to explain the algorithm logic, we have entered
    the basic probability values in the bottom-most cells. Here, we need to find all
    the combinations that the fulfill grammar rules. Follow the given steps:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图5.17*所示，为了解释算法逻辑，我们已将基本概率值输入到最底部的单元格中。在这里，我们需要找到所有符合语法规则的组合。按照给定的步骤继续操作：
- en: We will first take **NP** from the **people** cell and **VP** from the **fish**
    cell. In the grammar rules, check whether there is any grammar rule present that
    takes the sequence, **NP VP**, which you will need to find on the right-hand side
    of the grammar rule. Here, we found that the rule is **S -> NP VP** with a probability
    of 0.9.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从“people”单元格中取**NP**，从“fish”单元格中取**VP**。在语法规则中，检查是否有任何语法规则包含序列**NP VP**，你需要在语法规则的右侧找到这个序列。在这里，我们发现规则是**S
    -> NP VP**，概率为0.9。
- en: Now, calculate the probability value, and to find this, you need to multiply
    the probability value of **NP** given in the people cell, the probability value
    of **VP** in the **fish** cell, and the probability value of the grammar rule
    itself. So here, the probability value of **NP** placed in the **people** cell
    *=* 0.35, the probability value of **VP** placed in the **fish** cell *=0.06*
    and the probability of the grammar rule **S -> NP VP** *=* 0.9.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，计算概率值，为此你需要将**NP**在“people”单元格中的概率值、**VP**在“fish”单元格中的概率值以及语法规则本身的概率值相乘。因此，**NP**在“people”单元格中的概率值*=*
    0.35，**VP**在“fish”单元格中的概率值*=* 0.06，语法规则**S -> NP VP**的概率值*=* 0.9。
- en: We then multiply 0.35 (the probability of **NP** placed in the **people** cell)
    * 0.06 (the probability of **VP** in the **fish** cell ) * 0.9 (the probability
    of the grammar rule **S -> NP VP**). Therefore, the final multiplication value
    *= 0.35 * 0.06 * 0.9 = 0.0189*. *0.0189* is the final probability for the grammar
    rule if we expand **S** into the **NP VP** grammar rule.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将0.35（**NP**在“people”单元格中的概率）* 0.06（**VP**在“fish”单元格中的概率）* 0.9（语法规则**S
    -> NP VP**的概率）相乘。因此，最终的乘积值*= 0.35 * 0.06 * 0.9 = 0.0189*。*0.0189*是我们将**S**扩展为**NP
    VP**语法规则时的最终概率。
- en: In the same way, you can calculate other combinations, such as **NP** from the
    **people** cell and **NP** from the **fish** cell, and find the grammar rule,
    that is, **NP NP** on the right-hand side. Here, the **NP - NP NP** rule exists.
    So we calculate the probability value, *0.35 * 0.14 * 0.2 = 0.0098*. We continue
    with this process until we generate the probability value for all the combinations,
    and then we will see for which combination we have generated the maximum probability.
    The process of finding the maximum probability is called **Viterbi max score**.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样的方式，你可以计算其他组合，如**NP**来自“people”单元格和**NP**来自“fish”单元格，并找到语法规则，即右侧的**NP NP**。在这里，存在**NP
    - NP NP**规则。因此，我们计算概率值，*0.35 * 0.14 * 0.2 = 0.0098*。我们继续这个过程，直到为所有组合生成概率值，然后我们可以看到哪一个组合生成了最大概率。找到最大概率的过程称为**Viterbi最大得分**。
- en: For the combination **S -> NP VP**, we will get the maximum probability when
    the cells generate the left-hand side non-terminal on its upward cell. So, those
    two cells generate **S**, which is a sentence.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于组合**S -> NP VP**，当单元格生成其向上的非终结符时，我们将获得最大概率。因此，这两个单元格生成**S**，即句子。
- en: 'This is the core logic of the CKY algorithm. Let''s look at one concrete example
    for this concept. For writing purposes, we will rotate the parse triangle 90 degrees
    clockwise. Refer to *Figure 5.18*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 CKY 算法的核心逻辑。我们来看一个具体的例子来说明这个概念。为了便于书写，我们将解析三角形顺时针旋转 90 度。参见 *图 5.18*：
- en: '![](img/e6680024-6a6e-416e-889e-f6de607e9c37.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6680024-6a6e-416e-889e-f6de607e9c37.png)'
- en: 'Figure 5.18: Step 1 for the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18：CKY 算法的第一步（图片来源：http://spark-public.s3.amazonaws.com）
- en: Here, *cell (0,1)* is for **fish** and it fills using lexical rules. We have
    put **N** -> **fish** with *0.2* probability because this is defined in our grammar
    rule. We have put **V -> fish** with *0.6* probability. Now we focus on some unary
    rules that have **N** or **V** only on the right-hand side. We have the rules
    that we need to calculate the probability by considering the grammar rules probability
    and lexical probability. So, for rule **NP -> N**, the probability is *0.7* and
    **N -> fish** has the probability value *0.2*. We need to multiply this value
    and generate the probability of the grammar rule **NP -> N** *= 0.14*. In the
    same way, we generate the probability for the rule **VP -> V**, and its value
    is *0.1 * 0.6 = 0.6*. This way, you need to fill up all the four cells.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*单元格 (0,1)* 用于 **fish**，并通过词汇规则进行填充。我们为 **N** -> **fish** 赋予 *0.2* 的概率，因为这是我们语法规则中定义的内容。我们为
    **V -> fish** 赋予 *0.6* 的概率。现在我们专注于一些只有 **N** 或 **V** 在右侧的单一规则。我们需要通过考虑语法规则的概率和词汇概率来计算概率。因此，对于规则
    **NP -> N**，概率是 *0.7*，而 **N -> fish** 的概率是 *0.2*。我们需要将这些值相乘，得到语法规则 **NP -> N**
    的概率 *= 0.14*。同样，我们为规则 **VP -> V** 计算概率，结果是 *0.1 * 0.6 = 0.6*。这样，你需要填充所有四个单元格。
- en: 'In the next stage, we follow the same procedure to get the probability for
    each combination generated from the grammar rules. Refer to *Figure 5.19*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个阶段，我们遵循相同的过程，计算从语法规则生成的每个组合的概率。参见 *图 5.19*：
- en: '![](img/04383ecf-5951-4e09-abc1-e8f34884570c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04383ecf-5951-4e09-abc1-e8f34884570c.png)'
- en: 'Figure 5.19 : Stage 2 of the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19：CKY 算法的第二阶段（图片来源：http://spark-public.s3.amazonaws.com）
- en: 'In *Figure 5.20*, you can see the final probability values, using which you
    can decide the best parse tree for the given data:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.20* 中，你可以看到最终的概率值，使用这些概率值，你可以为给定数据选择最佳的解析树：
- en: '![](img/d12f49a6-e874-475a-b56b-5d6fd3063327.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d12f49a6-e874-475a-b56b-5d6fd3063327.png)'
- en: 'Figure 5.20: The final stage of the CKY algorithm (Image credit: http://spark-public.s3.amazonaws.com)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20：CKY 算法的最终阶段（图片来源：http://spark-public.s3.amazonaws.com）
- en: Now you know how the parse tree has been generated we want to share with you
    some important facts regarding the Stanford parser. It is built based on this
    CKY algorithm. There are a couple of technical assumptions and improvisations
    applied to the Stanford parser, but the following are the core techniques used
    to build the parser.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道了如何生成解析树，我们想与您分享一些关于斯坦福解析器的重要信息。它是基于这个 CKY 算法构建的。斯坦福解析器应用了一些技术假设和改进，但以下是用于构建解析器的核心技术。
- en: Developing parsers step-by-step
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分步开发解析器
- en: 'Here, we will look at the steps required to build your own parser with the
    help of the CKY algorithm. Let''s begin summarizing:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将介绍构建你自己解析器的步骤，并借助 CKY 算法来完成。让我们开始总结：
- en: 'You should have tagged the corpus that has a human-annotated parse tree: if
    it is tagged as per the Penn Treebank annotation format, then you are good to
    go.'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该已经标注了带有人类注释的解析树的语料库：如果它是按照 Penn Treebank 注释格式标注的，那么你就可以继续了。
- en: With this tagged parse corpus, you can derive the grammar rules and generate
    the probability for each of the grammar rules.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这个标注过的解析语料库，你可以推导出语法规则，并为每个语法规则生成概率。
- en: You should apply CNF for grammar transformation.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该应用 CNF 进行语法转换。
- en: Use the grammar rules with probability and apply them to the large corpus; use
    the CKY algorithm with the Viterbi max score to get the most likely parse structure.
    If you are providing a large amount of data, then you can use the ML learning
    technique and tackle this problem as a multiclass classifier problem. The last
    stage is where you get the best parse tree for the given data as per the probability
    value.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用具有概率的语法规则，并将其应用于大型语料库；使用带有 Viterbi 最大分数的 CKY 算法来获得最可能的解析结构。如果你提供了大量数据，那么可以使用
    ML 学习技术，将此问题视为一个多类分类问题。最后阶段是根据概率值得到给定数据的最佳解析树。
- en: That's enough theory; let's now use some of the famous existing parser tools
    practically and also check what kind of features you can generate from the parse
    tree.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 理论讲解到此为止；现在让我们实际使用一些著名的现有分析工具，并查看你能从解析树中生成哪些特征。
- en: Existing parser tools
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有的分析器工具
- en: In this section, we will look at some of the existing parsers and how you can
    generate some cool features that can be used for ML algorithms or in rule-based
    systems.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将看一下现有的一些分析器，并讨论如何生成一些可以用于机器学习算法或基于规则的系统中的酷特征。
- en: 'Here, we will see two parsers:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到两个分析器：
- en: The Stanford parser
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福分析器
- en: The spaCy parser
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy分析器
- en: The Stanford parser
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福分析器
- en: 'Let''s begin with the Stanford parser. You can download it from [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/).
    After downloading it, you just need to extract it to any location you like. The
    prerequisite of running the Stanford parser is that you should have a Java-run
    environment installed in your system. Now you need to execute the following command
    in order to start the Stanford parser service:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从斯坦福分析器开始。你可以从[https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)下载它。下载后，你只需将其解压到你喜欢的任何位置。运行斯坦福分析器的前提是你应该在系统中安装了Java运行环境。现在，你需要执行以下命令来启动斯坦福分析器服务：
- en: '[PRE0]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, you can change the memory from `-mx4g` to `-mx3g`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以将内存从`-mx4g`更改为`-mx3g`。
- en: Let's look at the concept of dependency in the parser before can fully concentrating
    on the coding part.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全专注于编码部分之前，我们先来看一下分析器中的依赖关系概念。
- en: The dependency structure in the parser shows which words depend on other words
    in the sentence. In the sentence, some words modify the meaning of other words;
    on the other hand, some act as an argument for other words. All of these kinds
    of relationships are described using dependencies. There are several dependencies
    in the Stanford parser. We will go through some of them. Let's take an example
    and we will explain things as we go.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 分析器中的依赖结构显示了句子中哪些词依赖于其他词。在句子中，一些词修饰其他词的含义；另一方面，有些词作为其他词的论元。所有这些关系都通过依赖来描述。在斯坦福分析器中有几种依赖关系，我们将逐一讲解。我们来举个例子，一边讲解一边进行。
- en: 'The sentence is: The boy put the tortoise on the rug.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 句子是：The boy put the tortoise on the rug.
- en: 'For this given sentence, the **head** of the sentence is *put* and it modifies
    three sections: *boy*, *tortoise*, and *on the rug*. How do you find the head
    word of the sentence?Find out by asking the following questions:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个给定的句子，句子的**中心词**是*put*，它修饰了三个部分：*boy*、*tortoise*和*on the rug*。如何找到句子的中心词？通过以下问题来找出答案：
- en: 'Who put it down? You get the answer: *boy*'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁把它放下的？你会得到答案：*boy*
- en: 'Then, what thing did he put down? You get the answer: *tortoise*'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，他放下的是什么东西？你会得到答案：*tortoise*
- en: 'Where is it put? You get the answer: *on the rug*'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它被放在哪里？你会得到答案：*on the rug*
- en: 'So, the word **put** modifies three things. Now look at the word *boy*, and
    check whether there is any modifier for it. Yes, it has a modifier: **the**. Then,
    check whether there is any modifier for *tortoise*. Yes, it has a modifier: **the**.
    For the phrase *on the rug* *on* complements *rug* and *rug* acts as the head
    for this phrase, taking a modifier, *the*. Refer to *Figure 5.21*:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，单词**put**修饰了三样东西。现在看看单词*boy*，检查它是否有任何修饰词。是的，它有一个修饰词：**the**。接着，检查*the tortoise*是否有修饰词。是的，它有一个修饰词：**the**。对于短语*on
    the rug*，*on*是对*rug*的补充，而*rug*是这个短语的中心词，接受修饰词*the*。参考*图5.21*：
- en: '![](img/5bf923f7-b71b-4d89-a5af-6bc2a28137eb.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bf923f7-b71b-4d89-a5af-6bc2a28137eb.png)'
- en: 'Figure 5.21: Dependency structure of the sentence'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21：句子的依赖结构
- en: 'The Stanford parser has dependencies like `nsubjpass` (passive nominal subject)
    , `auxpass` (passive auxiliary) , `prep` (prepositional modifier), `pobj` (object
    of preposition), `conj` (conjunct), and so on. We won''t go into more detail regarding
    this, but is worth mentioning that dependency parsing also follows the tree structure
    and it''s linked by binary asymmetric relations called **dependencies**. You can
    find more details about each of the dependencies by accessing the Stanford parser
    document here: [https://nlp.stanford.edu/software/dependencies_manual.pdf](https://nlp.stanford.edu/software/dependencies_manual.pdf).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福解析器有诸如`nsubjpass`（被动名词主语）、`auxpass`（被动助动词）、`prep`（介词修饰语）、`pobj`（介词宾语）、`conj`（连接词）等依赖关系。我们不会详细讨论这些内容，但值得一提的是，依赖解析也遵循树形结构，并通过称为**依赖关系**的二元不对称关系进行链接。你可以通过访问斯坦福解析器文档了解每个依赖关系的更多细节：[https://nlp.stanford.edu/software/dependencies_manual.pdf](https://nlp.stanford.edu/software/dependencies_manual.pdf)。
- en: 'You can see the basic example in *Figure 5.22*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.22*中看到基本示例：
- en: '![](img/026b30ae-54c5-46fc-bae7-dd3a088116a5.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/026b30ae-54c5-46fc-bae7-dd3a088116a5.png)'
- en: 'Figure 5.22: Dependency parsing of the sentence'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.22：句子的依赖解析
- en: Now if you want to use the Stanford parser in Python, you have to use the dependency
    named `pycorenlp`. We will use it to generate the output from the Stanford parser.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想在 Python 中使用斯坦福解析器，你需要使用名为`pycorenlp`的依赖包。我们将使用它来生成斯坦福解析器的输出。
- en: 'You can see the sample code in which we have used the Stanford parser to parse
    the sentence. You can parse multiple sentences as well. You can find the code
    at the following GitHub link:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们使用斯坦福解析器解析句子的示例代码。你也可以解析多个句子。你可以在以下 GitHub 链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample](https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample](https://github.com/jalajthanaki/NLPython/tree/master/ch5/parserexample)。'
- en: 'You can see the code snippet in *Figure 5.23*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.23*中看到代码片段：
- en: '![](img/157cb610-6435-4dea-a3ea-32aa9fd8e12c.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/157cb610-6435-4dea-a3ea-32aa9fd8e12c.png)'
- en: 'Figure 5.23: Code snippet for the Stanford parser demo'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.23：斯坦福解析器演示的代码片段
- en: 'You can see the output of this code in *Figure 5.24*:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.24*中看到此代码的输出：
- en: '![](img/8e8ae2b9-6d21-4567-8d1b-284e810a0ccc.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e8ae2b9-6d21-4567-8d1b-284e810a0ccc.png)'
- en: 'Figure 5.24: Output of the Stanford parser'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：斯坦福解析器的输出
- en: The spaCy parser
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 解析器
- en: 'This parser helps you generate the parsing for the sentence. This is a dependency
    parser. You can find the code at the following GitHub link:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解析器帮助你生成句子的解析结果。这是一个依赖解析器。你可以在以下 GitHub 链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/parserexample/scpacyparserdemo.py)。'
- en: 'You can find the code snippet of this parser in *Figure 5.25*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.25*中找到该解析器的代码片段：
- en: '![](img/8e0fcafd-dc68-49c7-82d9-a91fa5011eeb.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e0fcafd-dc68-49c7-82d9-a91fa5011eeb.png)'
- en: 'Figure 5.25: spaCy dependency parser code'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25：spaCy 依赖解析器代码
- en: 'You can see the output of the spaCy parser in *Figure 5.26*:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.26*中看到 spaCy 解析器的输出：
- en: '![](img/8589e6a8-17ff-4f81-a2a4-1085618b98d7.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8589e6a8-17ff-4f81-a2a4-1085618b98d7.png)'
- en: 'Figure 5.26: The spaCy parser output'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26：spaCy 解析器输出
- en: People used the Stanford parser because it provides good accuracy as well as
    a lot of flexibility in terms of generating output. Using the Stanford parser,
    you can generate the output in a JSON format, XML format, or text format. You
    may think that we get the parse tree using the preceding code, but the kind of
    features that we can derive from the parsing result will be discussed in the next
    section.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 人们使用斯坦福解析器，因为它提供了良好的准确性，并且在生成输出方面具有很大的灵活性。使用斯坦福解析器，您可以以JSON格式、XML格式或文本格式生成输出。你可能认为我们通过前面的代码获取了解析树，但我们将在下一节讨论从解析结果中可以派生的特征类型。
- en: Extracting and understanding the features
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取和理解特征
- en: 'Generally, using the parse result, you can derive many features such as generating
    noun phrases and POS tags inside the noun phrase; you can also derive the head
    word from phrases. You can use each word and its tag. You can use the dependency
    relationships as features. You can see the code snippet in *Figure 5.27*:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用解析结果，你可以派生出许多特征，例如生成名词短语和名词短语中的词性标签；你还可以从短语中提取主词。你可以使用每个单词及其标签，也可以将依赖关系作为特征。你可以在*图
    5.27*中看到代码片段：
- en: '![](img/2082d31e-85fd-4bd0-a6b0-5f78955069cf.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2082d31e-85fd-4bd0-a6b0-5f78955069cf.png)'
- en: 'Figure 5.27: Code to get NP from sentences'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27：从句子中获取NP的代码
- en: 'The output snippet is in *Figure 5.28*:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出片段见*图 5.28*：
- en: '![](img/68cef596-0bdf-4b4c-a534-8579fd709b34.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68cef596-0bdf-4b4c-a534-8579fd709b34.png)'
- en: 'Figure 5.28: Output all NP from sentences'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.28：从句子中输出所有的NP
- en: You can generate the stem as well as lemma from each word, as we saw in [Chapter
    3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml), *Understanding Structure of Sentences*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从每个单词中生成词干和词元，正如我们在[第3章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)，*理解句子结构*中所看到的那样。
- en: In real life, you can generate the features easily using libraries, but which
    features you need to use is critical and depends on your NLP application. Let's
    assume that you are making a grammar correction system; in that case, you need
    to consider all the phrases of the sentence as well as the POS tags of each word
    present in the phrase. If you are developing a question-answer system, then noun
    phases and verb phrases are the important features that you can select.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，你可以通过库轻松生成特征，但选择哪些特征至关重要，这取决于你的NLP应用。假设你正在开发一个语法修正系统；在这种情况下，你需要考虑句子的所有短语以及每个单词在短语中的词性标签。如果你正在开发一个问答系统，那么名词短语和动词短语是你可以选择的重要特征。
- en: Features selection is a bit tricky and you will need to do some iteration to
    get an idea of which features are good for your NLP application. Try to dump your
    features in a `.csv` file so you can use the `.csv` file later on in your processing.
    Each and every feature can be a single column of the `.csv` file. For example,
    you have NP words stored in one column, lemma in the other column for all words
    in NP, and so on. Now, suppose you have more than 100 columns; in that case, you
    would need to find out which are the important columns (features) and which are
    not. Based on the problem statement and features, you can decide what the most
    important features are that help us to solve our problem. In [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Problems,* we will look at features selection in more
    detail.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择有点棘手，你需要做一些迭代来了解哪些特征适合你的NLP应用。尝试将你的特征存储在一个`.csv`文件中，这样你可以在后续处理中使用该文件。每个特征都可以是`.csv`文件中的一列。例如，你可以将NP词汇存储在一列，将每个词在NP中的词元存储在另一列，等等。现在，假设你有超过100列，在这种情况下，你需要找出哪些列（特征）是重要的，哪些是不重要的。根据问题陈述和特征，你可以决定哪些特征是最重要的，有助于我们解决问题。在[第8章](97808151-90d2-4034-8d53-b94123154265.xhtml)，*面向NLP问题的机器学习*中，我们将更详细地讨论特征选择。
- en: Customizing parser tools
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义解析器工具
- en: In real life, datasets are quite complex and messy. In that case, it may be
    that the parser is unable to give you a perfect or accurate result. Let's take
    an example.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，数据集通常非常复杂且凌乱。在这种情况下，解析器可能无法给出完美或准确的结果。让我们来看一个例子。
- en: Let's assume that you want to parse a dataset that has text content of research
    papers, and these research papers belong to the chemistry domain. If you are using
    the Stanford parser in order to generate a parse tree for this dataset, then sentences
    that contain chemical symbols and equations may not get parsed properly. This
    is because the Stanford parser has been trained on the Penn TreeBank corpus, so
    it's accuracy for the generation of a parse tree for chemical symbols and equation
    is low. In this case, you have two options - either you search for a parser that
    can generate parsing for symbols and equations accurately or if you have a corpus
    that has been tagged, you have the flexibility of retraining the Stanford parser
    using your tagged data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想解析一个包含研究论文文本内容的数据集，这些研究论文属于化学领域。如果你使用斯坦福解析器生成该数据集的解析树，那么包含化学符号和方程式的句子可能无法正确解析。这是因为斯坦福解析器是在Penn
    TreeBank语料库上训练的，因此它在生成化学符号和方程式的解析树时准确性较低。在这种情况下，你有两个选择——要么你可以寻找一个能准确生成符号和方程式解析的解析器，要么如果你有一个已经标注好的语料库，你可以利用这个标注数据重新训练斯坦福解析器。
- en: 'You can follow the same tagging notation given in the Penn TreeBank data for
    your dataset, then use the following command to retrain the Stanford parser on
    your dataset, save the trained model, and use it later on. You can use the following
    command to retrain the Stanford Parser:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照Penn TreeBank数据中给出的标注符号为你的数据集进行标注，然后使用以下命令在你的数据集上重新训练斯坦福解析器，保存训练好的模型，并在之后使用它。你可以使用以下命令来重新训练斯坦福解析器：
- en: '[PRE1]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Challenges
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Here are some of the challenges related to parsers:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些与分析器相关的挑战：
- en: To generate a parser for languages such as Hebrew, Gujarati, and so on is difficult
    and the reason is that we don't have a tagged corpus.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为希伯来语、古吉拉特语等语言生成分析器非常困难，原因是我们没有标注过的语料库。
- en: Developing parsers for fusion languages is difficult. A fusion language means
    that you are using another language alongside the English language, with a sentence
    including more than one language. Processing these kinds of sentences is difficult.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为融合语言开发分析器非常困难。融合语言意味着你在使用英语的同时，还使用另一种语言，句子中包含多种语言。处理这种句子非常困难。
- en: Now that we have understood some features of the parser, we can move on to our
    next concept, which is POS tagging. This is one of the essential concepts of NLP.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了分析器的一些特点，接下来我们可以讨论下一个概念，即词性标注。这是自然语言处理（NLP）中的一个基本概念。
- en: POS tagging and POS taggers
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注与词性标注器
- en: In this section, we will discuss the long-awaited topic of POS tags.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论备受期待的词性标注话题。
- en: Understanding the concept of POS tagging and POS taggers
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词性标注和词性标注器的概念
- en: POS tagging is defined as the process of marking words in the corpus corresponding
    to their particular part of speech. The POS of the word is dependent on both its
    definition and its context. It is also called **grammatical tagging** or **word-category
    disambiguation**. POS tags of words are also dependent on their relationship with
    adjacent and related words in the given phrase, sentence, and paragraph.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注（POS tagging）定义为在语料库中标记单词并对应其特定词性的过程。一个单词的词性取决于其定义和上下文。它也被称为**语法标注**或**词类消歧**。单词的词性标注还依赖于它与给定短语、句子和段落中相邻和相关单词的关系。
- en: 'POS tagger is the tool that is used to assign POS tags for the given data.
    Assigning the POS tags is not an easy task because POS of words is changed as
    per the sentence structure and meaning. Let''s take an example. Let''s take the
    word dogs; generally, we all know that dogs is a plural noun, but in some sentences
    it acts as a verb. See the sentence: **The sailor dogs the hatch**. Here, the
    correct POS for *dogs* is the verb and not the plural noun. Generally, many POS
    taggers use the POS tags that are generated by the University of Pennsylvania.
    You can find word-level POS tags and definitions at the following link:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注器（POS tagger）是用于为给定数据分配词性标签的工具。分配词性标注并不是一项简单的任务，因为单词的词性会根据句子结构和语义发生变化。我们来看一个例子。假设我们有单词"dogs"；一般来说，我们都知道dogs是复数名词，但在某些句子中它充当动词。看看这个句子：**The
    sailor dogs the hatch**。在这里，*dogs*的正确词性应该是动词，而不是复数名词。通常，许多词性标注器使用由宾夕法尼亚大学生成的词性标签。你可以通过以下链接找到单词级别的词性标签及其定义：
- en: '[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)。'
- en: 'We will now touch on some of the POS tags. There are 36 POS tags in the Penn
    Treebank POS list, such as **NN** indicates noun, **DT** for determiner words,
    and **FW** for foreign words. The word that is new to POS tags is generally assigned
    the **FW** tag. Latin names and symbols often get the **FW** tag by POS tagger.
    So, if you have a (lambda) symbol then POS may tagger suggest the **FW** POS tag
    for it. You can see some word-level POS tags in *Figure 5.29*:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论一些词性标注。Penn Treebank词性列表中有36个词性标注，例如**NN**表示名词，**DT**表示限定词，**FW**表示外来词。对于新出现的词，一般会分配**FW**标注。拉丁语名称和符号通常会被词性标注工具分配**FW**标签。因此，如果你有一个（λ）符号，词性标注器可能会为它推荐**FW**标签。你可以在*图5.29*中看到一些单词级别的词性标注：
- en: '![](img/8084df2c-6e1f-43d9-a1ef-b21563bff63f.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8084df2c-6e1f-43d9-a1ef-b21563bff63f.png)'
- en: 'Figure 5.29: Some word-level POS tags'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29：一些单词级别的词性标注
- en: 'There are POS tags available at the phrase-level as well as the clause-level.
    All of these tags can be found at the following GitHub link:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 也有在短语级别和从句级别的词性标注。所有这些标签可以通过以下GitHub链接找到：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/POS_tags.txt)。'
- en: See each of the tags given in the file that we have specified as they are really
    useful when you evaluating your parse tree result. POS tags and their definitions
    are very straightforward, so if you know basic English grammar, then you can easily
    understand them.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 查看我们在文件中指定的每个标签，它们在评估解析树结果时非常有用。词性标签及其定义非常直观，如果你了解基础的英语语法，就能轻松理解它们。
- en: You must be curious to know how POS taggers are built. Let's find out the procedure
    of making your own POS tagger.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定很好奇词性标注器是如何构建的。让我们一起看看如何制作自己的词性标注器的过程。
- en: Developing POS taggers step-by-step
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤化开发词性标注器
- en: 'To build your own POS tagger, you need to perform the following steps:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建自己的词性标注器，你需要执行以下步骤：
- en: You need a tagged corpus.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要一个标注好的语料库。
- en: Select features.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特征。
- en: Perform training using a decision tree classifier available in the Python library,
    `scikit-learn`.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python库中可用的决策树分类器`scikit-learn`进行训练。
- en: Check your accuracy.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查你的准确性。
- en: Try to predict the POS tags using your own trained model.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用你自己训练的模型来预测词性标注（POS tags）。
- en: A great part of this section is that we will code our own POS tagger in Python,
    so you guys will get an idea of how each of the preceding stages are performed
    in reality. If you don't know what a decision tree algorithm is, do not worry
    - we will cover this topic in more detail in [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Applications*.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的一个重要部分是，我们将用Python编写自己的词性标注器，这样你就能理解前述每个阶段是如何实际执行的。如果你不知道什么是决策树算法，不用担心——我们将在[第8章](97808151-90d2-4034-8d53-b94123154265.xhtml)，*NLP应用的机器学习*中详细介绍这个话题。
- en: 'Here, we will see a practical example that will help you understand the process
    of developing POS taggers. You can find the code snippet for each stage and you
    can access the code at the following GitHub link:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到一个实用的示例，帮助你理解开发词性标注器的过程。你可以在每个阶段找到相应的代码片段，也可以通过以下GitHub链接访问代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger](https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger](https://github.com/jalajthanaki/NLPython/tree/master/ch5/CustomPOStagger)。'
- en: 'See the code snippet of getting the Pann TreeBank corpus in *Figure 5.30*:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 查看获取Pann TreeBank语料库的代码片段，如*图5.30*所示：
- en: '![](img/259c9478-af35-4f67-91e1-b4b747bdc541.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/259c9478-af35-4f67-91e1-b4b747bdc541.png)'
- en: 'Figure 5.30: Load the Penn TreeBank data from NLTK'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30：从NLTK加载Penn TreeBank数据
- en: 'You can see the feature selection code snippet in *Figure 5.31*:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.31*中看到特征选择的代码片段：
- en: '![](img/8342709c-f9ea-4b81-9b00-5564421c08ad.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8342709c-f9ea-4b81-9b00-5564421c08ad.png)'
- en: 'Figure 5.31: Extracting features of each word'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31：提取每个单词的特征
- en: 'We have to extract the features for each word. You can see the code snippet
    for some basic transformation such as splitting the dataset into training and
    testing. Refer to *Figure 5.32*:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为每个单词提取特征。你可以在*图5.32*中看到一些基本转换的代码片段，例如将数据集拆分为训练集和测试集：
- en: '![](img/4c8cb7a1-beac-41ae-bd89-e4d16ae656f0.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c8cb7a1-beac-41ae-bd89-e4d16ae656f0.png)'
- en: 'Figure 5.32: Splitting the data into training and testing'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32：将数据分为训练集和测试集
- en: 'See the code to train the model using a decision tree algorithm in *Figure
    5.33*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 查看使用决策树算法训练模型的代码，如*图5.33*所示：
- en: '![](img/a2d9f363-6bb4-43fd-acd6-ca74c40c98c2.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2d9f363-6bb4-43fd-acd6-ca74c40c98c2.png)'
- en: 'Figure 5.33: Actual training using a decision tree algorithm'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.33：使用决策树算法的实际训练
- en: 'See the output prediction of POS tags for the sentence that you provided in
    *Figure 5.34*:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 查看你提供的句子的词性标注输出，如*图5.34*所示：
- en: '![](img/6956da11-f9bd-4be9-b762-b947e839c581.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6956da11-f9bd-4be9-b762-b947e839c581.png)'
- en: 'Figure 5.34: Output of a custom POS tagger'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.34：自定义词性标注器的输出
- en: You should have now understood the practical aspect of making your own POS tags,
    but you can still also use some of the cool POS taggers.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该已经理解了制作自己词性标注器的实际操作，但你仍然可以使用一些很棒的现成词性标注器。
- en: Plug and play with existing POS taggers
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 即插即用现有的词性标注器
- en: 'There are many POS taggers available nowadays. Here, we will use the POS tagger
    available in the Stanford CoreNLP and polyglot library. There are others such
    as the Tree tagger; NLTK also has a POS tagger that you can use. You can find
    the code at the following GitHub link:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有很多词性标注器可用。在这里，我们将使用斯坦福CoreNLP和polyglot库中的词性标注器。还有其他如Tree tagger，NLTK也有一个词性标注器可以使用。你可以通过以下GitHub链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo](https://github.com/jalajthanaki/NLPython/blob/master/ch5/POStagdemo/).'
- en: A Stanford POS tagger example
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福词性标注器示例
- en: 'You can see the code snippet for the Stanford POS tagger in *Figure 5.35*:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.35*中看到斯坦福词性标注器的代码片段：
- en: '![](img/6133b79c-6dd9-463d-ae4d-05bcb4a45de9.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6133b79c-6dd9-463d-ae4d-05bcb4a45de9.png)'
- en: 'Figure 5.35: Stanford POS tagger code'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.35：斯坦福词性标注器代码
- en: 'The output from the Stanford POS tagger can be found in *Figure 5.36*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.36*中看到来自斯坦福词性标注器的输出：
- en: '![](img/98fd3b01-e8cd-457a-ab3a-7143a5a0c890.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98fd3b01-e8cd-457a-ab3a-7143a5a0c890.png)'
- en: 'Figure 5.36: POS tags generated by the Stanford POS tagger'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.36：斯坦福词性标注器生成的词性标签
- en: Using polyglot to generate POS tagging
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用polyglot生成词性标注
- en: 'You can see the code snippet for the `polyglot` POS tagger in *Figure 5.37*:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.37*中看到`polyglot`词性标注器的代码片段：
- en: '![](img/71fdba9e-3649-476b-8b26-6aad00d4a5b4.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71fdba9e-3649-476b-8b26-6aad00d4a5b4.png)'
- en: 'Figure 5.37: Polyglot POS tagger'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.37：Polyglot词性标注器
- en: 'The output from the `polyglot` POS tagger can be found in *Figure 5.38*:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.38*中看到来自`polyglot`词性标注器的输出：
- en: '![](img/b28aea32-579b-4a97-ab1d-38b5e61215f0.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b28aea32-579b-4a97-ab1d-38b5e61215f0.png)'
- en: Figure 5.38:The polyglot POS tagger output
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.38：Polyglot词性标注器输出
- en: Exercise
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Try using the TreeTagger library to generate POS tagging. You can find the
    installation details at this link:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用TreeTagger库来生成词性标注。你可以在以下链接找到安装详情：
- en: '[http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/.](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/.](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)'
- en: Using POS tags as features
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词性标签作为特征
- en: Now that we have generated POS tags for our text data using the POS tagger,
    where can we use them? We will now look at NLP applications that can use these
    POS tags as features.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用词性标注器为我们的文本数据生成了词性标签，那么我们可以在哪里使用它们呢？接下来我们将看看可以将这些词性标签作为特征的NLP应用。
- en: POS tags are really important when you are building a chatbot with machine learning
    algorithms. POS tag sequences are quite useful when a machine has to understand
    various sentence structures. It is also useful if you are building a system that
    identifies **multiword express** (**MWE**). Some examples of MWE phrases are be
    able to, a little bit about, you know what, and so on.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标签在你使用机器学习算法构建聊天机器人时非常重要。当机器需要理解不同的句子结构时，词性标签序列非常有用。如果你正在构建一个识别**多词表达式**（**MWE**）的系统，它也会很有用。MWE短语的示例包括：be
    able to、a little bit about、you know what 等。
- en: 'If you have a sentence: **He backed off from the tour plan of Paris**. Here,
    *backed off* is the MWE. To identify these kinds of MWEs in sentences, you can
    use POS tags and POS tag sequences as features. You can use a POS tag in sentiment
    analysis, and there are other applications as well.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个句子：**He backed off from the tour plan of Paris**。在这里，*backed off* 就是MWE。为了识别句子中的这类MWE，你可以使用词性标签和词性标签序列作为特征。你还可以在情感分析中使用词性标签，除此之外还有其他应用。
- en: Challenges
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'The following are some challenges for POS tags:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是词性标签的一些挑战：
- en: Identifying the right POS tag for certain words in an ambiguous syntax structure
    is difficult, and if the word carries a very different contextual meaning, then
    the POS tagger may generate the wrong POS tags.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在模棱两可的句法结构中识别某个单词的正确词性标签是很困难的，如果该单词在上下文中的含义非常不同，那么词性标注器可能会生成错误的词性标签。
- en: Developing a POS tagger for Indian languages is a bit difficult because, for
    some languages, you cannot find the tagged dataset.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发适用于印度语言的词性标注器有点困难，因为对于某些语言，你找不到已标注的数据集。
- en: Now let's move on to the next section, where we will learn how to find the different
    entities in sentences.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一部分，我们将在其中学习如何在句子中找到不同的实体。
- en: Name entity recognition
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: In this section, we will look at a tool called **name entity recognition** (**NER**).
    The use of this tool is as follows. If you have a sentence, such as **Bank of
    America announced its earning today**, we as humans can understand that the *Bank
    of America* is the name of a financial institution and should be referred to as
    a single entity. However, for machine to handle and recognize that entity is quite
    challenging. There is where NER tools come into the picture to rescue us.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一个名为 **命名实体识别** (**NER**) 的工具。这个工具的使用方法如下：如果你有一句话，例如 **美国银行今天宣布了其财报**，我们作为人类能够理解
    *美国银行* 是一个金融机构的名字，应该作为一个整体来识别。然而，对于机器来说，要处理和识别这个实体是相当具有挑战性的。这时，NER 工具就派上了用场，帮助我们解决这个问题。
- en: With the NER tool, you can find out entities like person name, organization
    name, location, and so on. NER tools have certain classes in which they classify
    the entities. Here, we are considering the words of the sentence to find out the
    entities, and if there are any entities present in the sentence. Let's get some
    more details about what kind of entities we can find in our sentence using some
    of the available NER tools.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NER 工具，你可以识别出像人名、组织名、地点等实体。NER 工具有一些类别，用于对实体进行分类。在这里，我们考虑的是句子的词汇来查找实体，以及是否存在任何实体。让我们深入了解一下，通过一些可用的
    NER 工具，我们能在句子中找到哪些类型的实体。
- en: Classes of NER
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NER 类别
- en: 'NER tools generally segregate the entities into some predefined classes. Different
    NER tools have different types of classes. The Stanford NER tool has three different
    versions based on the NER classes:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: NER 工具通常将实体划分为一些预定义的类别。不同的 NER 工具有不同类型的类别。斯坦福 NER 工具有三个不同的版本，基于 NER 类别的不同：
- en: The first version is the three-class NER tool that can identify the entities
    - whether it's Location, Person, or Organization.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个版本是三类 NER 工具，可以识别地点、人物或组织这三类实体。
- en: The second version is the four-class NER tool that can identify the Location,
    person, Organization, and Misc. Misc is referred to as a miscellaneous entity
    type. If an entity doesn't belong to Location, Person, or Organization and is
    still an entity, then you can tag it as Misc.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个版本是四类 NER 工具，可以识别地点、人物、组织和杂项。杂项指的是一种多种类型的实体。如果一个实体既不属于地点、人物或组织，但仍然是一个实体，那么你可以将其标记为杂项。
- en: The third version is a seven-class tool that can identify Person, Location,
    Organization, Money, Percent, Date, and Time.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个版本是一个七类工具，可以识别人物、地点、组织、金钱、百分比、日期和时间。
- en: The spaCy parser also has an NER package available with the following classes.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 解析器还提供了一个包含以下类别的 NER 包。
- en: '`PERSON` class identifies the name of a person'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PERSON` 类别用于识别人物的名字'
- en: '`NORP` class meaning Nationality, Religious or Political groups'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NORP` 类别表示国籍、宗教或政治团体'
- en: '`FACILITY` class including buildings, airports, highways, and so on'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FACILITY` 类别包括建筑物、机场、高速公路等'
- en: '`ORG` class for organization, institution and so on'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ORG` 类别用于组织、机构等'
- en: '`GPE` class for cities, countries and so on'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GPE` 类别用于城市、国家等'
- en: '`LOC` class for non-GPE locations such as mountain ranges and bodies of water'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LOC` 类别用于非 GPE 的地点，如山脉和水体'
- en: '`PRODUCT` that includes objects, vehicles, food, and so on, but not services'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PRODUCT` 类别包括物体、车辆、食物等，但不包括服务'
- en: '`EVENT` class for sports events, wars, named hurricanes, and so on'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EVENT` 类别用于运动事件、战争、命名的飓风等'
- en: '`WORK_OF_ART` class for titles of books, songs, and so on'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WORK_OF_ART` 类别用于书籍、歌曲等作品的标题'
- en: '`LANGUAGE` that tags any named language'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LANGUAGE` 类别用于标记任何命名的语言'
- en: Apart from this, spaCy's NER package has classes such as date, time, percent,
    money, quantity, ordinal, and cardinal
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除此之外，spaCy 的 NER 包还包括日期、时间、百分比、金钱、数量、序数和基数等类别
- en: Now it's time to do some practical work. We will use the Stanford NER tool and
    spaCy NER in our next section.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候做一些实践操作了。在下一节中，我们将使用斯坦福 NER 工具和 spaCy NER 工具。
- en: Plug and play with existing NER tools
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与现有 NER 工具即插即用
- en: 'In this section, we will look at the coding part as well as information on
    how to practically use these NER tools. We will begin with the Stanford NER tool
    and then the Spacy NER. You can find the code at the following GitHub link:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论编码部分以及如何实际使用这些 NER 工具的信息。我们将从斯坦福 NER 工具开始，然后介绍 Spacy NER。你可以通过以下 GitHub
    链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/NERtooldemo)。'
- en: A Stanford NER example
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个斯坦福 NER 示例
- en: You can find the code and output snippet as follows. You need to download the
    Stanford NER tool at [https://nlp.stanford.edu/software/CRF-NER.shtml#Download](https://nlp.stanford.edu/software/CRF-NER.shtml#Download).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下方式找到代码和输出片段。你需要下载斯坦福NER工具，访问[https://nlp.stanford.edu/software/CRF-NER.shtml#Download](https://nlp.stanford.edu/software/CRF-NER.shtml#Download)。
- en: 'You can see the code snippet in *Figure 5.39*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.39*中看到代码片段：
- en: '![](img/0892d055-ab16-411e-8557-d21b1114c73f.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0892d055-ab16-411e-8557-d21b1114c73f.png)'
- en: 'Figure 5.39: Stanford NER tool code'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.39：斯坦福NER工具代码
- en: 'You can see the output snippet in *Figure 5.40*:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.40*中看到输出片段：
- en: '![](img/427a4a7c-7cfc-4f25-bc2f-c0267a910e83.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/427a4a7c-7cfc-4f25-bc2f-c0267a910e83.png)'
- en: 'Figure 5.40: Output of Stanford NER'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.40：斯坦福NER输出
- en: A Spacy NER example
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个Spacy NER示例
- en: 'You can find the code and output snippet as follows. You can see the code snippet
    in *Figure 5.41*:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下方式找到代码和输出片段。你可以在*图5.41*中看到代码片段：
- en: '![](img/58328dac-aebe-4c68-9d52-1abe10e77d4a.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58328dac-aebe-4c68-9d52-1abe10e77d4a.png)'
- en: 'Figure 5.41: spaCy NER tool code snippet'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.41：spaCy NER工具代码片段
- en: 'You can see the output snippet in *Figure 5.42*:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.42*中看到输出片段：
- en: '![](img/fe8df511-f2dd-474a-9913-bf0535b1b44c.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe8df511-f2dd-474a-9913-bf0535b1b44c.png)'
- en: 'Figure 5.42: Output of the spaCy tool'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.42：spaCy工具的输出
- en: Extracting and understanding the features
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取和理解特征
- en: NER tags are really important because they help you to understand sentence structure
    and help machines or NLP systems to understand the meaning of certain words in
    a sentence.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: NER标签非常重要，因为它们帮助你理解句子结构，并帮助机器或自然语言处理（NLP）系统理解句子中某些词语的含义。
- en: Let's take an example. If you are building a proofreading tool, then this NER
    tool is very useful because NER tools can find a person's name, an organizations'
    name, currency-related symbols, numerical formats, and so on that will help your
    proofreading tool identify exceptional cases present in text. Then, according
    to the NER tag, the system can suggest the necessary changes. Take the sentence,
    **Bank of America announced its earning today morning**. In this case, the NER
    tool gives the tag organization for *Bank of America*, which helps our system
    better understand the meaning of the sentence and the structure of the sentence.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子。如果你正在构建一个校对工具，那么这个命名实体识别（NER）工具非常有用，因为NER工具可以识别出一个人的名字、一个组织的名字、与货币相关的符号、数字格式等，这些都能帮助你的校对工具识别文本中存在的特殊情况。然后，根据NER标签，系统可以建议必要的修改。以这句话为例，**美国银行今天早上宣布了其财报**。在这种情况下，NER工具为*美国银行*赋予了组织标签，这帮助我们的系统更好地理解句子的含义和结构。
- en: NER tags are also very important if you are building a question-answer system
    as it is very crucial to extract entities in this system. Once you have generated
    the entities, you can use a syntactic relationship in order to understand questions.
    After this stage, you can process the question and generate the answer.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在构建一个问答系统，NER标签也非常重要，因为提取实体在这个系统中至关重要。一旦生成了实体，你可以利用句法关系来理解问题。在这个阶段之后，你就可以处理问题并生成答案。
- en: Challenges
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'There are certain challenges for the NER system, which are as follows:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NER系统来说，存在一些挑战，具体如下：
- en: NER tools train on a closed domain dataset. So, an NER system developed for
    one domain does not typically perform well on an other domain. This requires a
    universal NER tool that can work for all domains, and after training it should
    able to generalize enough to deal with unseen situations.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NER工具是在封闭领域的数据集上进行训练的。因此，为某一领域开发的NER系统通常无法在其他领域表现良好。这就需要一个通用的NER工具，能够适用于所有领域，并且在训练后应具备足够的泛化能力来应对未见过的情况。
- en: Sometimes you will find words which are the names of locations as well as the
    name of a person. The NER tool can't handle a case where one word can be expressed
    as the location name, person name, and organization name. This is a very challenging
    case for all NER tools. Suppose you have word TATA hospital; the single the words
    TATA can be the name of a person as well as the name of an organization. In this
    case, the NER tool can't decide whether TATA is the name of a person or the name
    of an organization.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时你会发现一些词语既是地点的名字，也可能是人的名字。NER工具无法处理某个词既可以表示地点名称、也可以表示人名或组织名称的情况。这对于所有的NER工具来说都是一个非常具有挑战性的案例。假设你有词语TATA医院；单词TATA既可以是一个人的名字，也可以是一个组织的名字。在这种情况下，NER工具无法判断TATA是一个人的名字还是一个组织的名字。
- en: To build an NER tool specifically for microblogging web platforms is also a
    challenging task.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为微博平台专门构建一个NER工具也是一个具有挑战性的任务。
- en: Let's move on to the next section, which is about n-gram algorithms. You will
    get to learn some very interesting stuff.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入下一部分，讲解n-gram算法。你将学习一些非常有趣的内容。
- en: n-grams
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: n-gram
- en: n-gram is a very popular and widely used technique in the NLP domain. If you
    are dealing with text data or speech data, you can use this concept.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram是NLP领域中一种非常流行且广泛使用的技术。如果你处理的是文本数据或语音数据，你可以使用这个概念。
- en: Let's look at the formal definition of n-grams. An n-gram is a continuous sequence
    of n items from the given sequence of text data or speech data. Here, items can
    be phonemes, syllables, letters, words, or base pairs according to the application
    that you are trying to solve.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看n-gram的正式定义。n-gram是从给定的文本数据或语音数据序列中提取的n个项的连续序列。在这里，项可以是音素、音节、字母、单词或根据你正在解决的应用，可能是碱基对。
- en: 'There are some versions of n-grams that you will find very useful. If we put
    n=1, then that particular n-gram is referred to as a unigram. If we put n=2, then
    we get the bigram. If we put n=3, then that particular n-gram is referred to as
    a trigram, and if you put n=4 or n=5, then these versions of n-grams are referred
    to as four gram and five gram, respectively. Now let''s take some examples from
    different domains to get a more detailed picture of n-grams. See examples from
    NLP and computational biology to understand a unigram in *Figure 5.43*:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些版本的n-gram对你来说非常有用。如果我们将n=1，那么这个n-gram被称为单元语法。如果我们将n=2，那么我们得到的是双元语法。如果我们将n=3，那么这个n-gram被称为三元语法，而如果你将n=4或n=5，那么这些n-gram版本分别称为四元语法和五元语法。现在让我们从不同领域中举一些例子，以更详细地了解n-gram。请参阅NLP和计算生物学的示例，以了解*图
    5.43*中的单元语法：
- en: '![](img/22d797e8-3c08-4f7b-a130-a9910ea82a98.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22d797e8-3c08-4f7b-a130-a9910ea82a98.png)'
- en: 'Figure 5.43: Unigram example sequence'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.43：单元语法示例序列
- en: 'You have seen unigrams. Now we will look at the bigram. With bigrams, we are
    considering overlapped pairs, as you can see in the following example. We have
    taken the same NLP and computational biology sequences to understand bigrams.
    See *Figure 5.44*:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看过单元语法。现在我们来看双元语法。双元语法考虑的是重叠对，正如你在以下示例中看到的那样。我们使用相同的NLP和计算生物学序列来理解双元语法。见*图
    5.44*：
- en: '![](img/8b9aa773-e7aa-4bfb-876a-082eed4ee00d.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b9aa773-e7aa-4bfb-876a-082eed4ee00d.png)'
- en: 'Figure 5.44: Bigram example sequence'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.44：双元语法示例序列
- en: 'If you understood the bigram overlapped pairing concept from the example, then
    a trigram will be easier for you to understand. A trigram is just an extension
    of the bigram, but if you are still confused, then let''s explain it for you in
    laymans'' terms. In the first three rows of *Figure 5.44*, we generated a character-based
    bigram and the fourth row is a word-based bigram. We will start from the first
    character and consider the very next character because we are considering n=2
    and the same is applicable to words as well. See the first row where we are considering
    a bigram such as *AG* as the first bigram. Now, in the next iteration, we are
    considering *G* again and generate *GC*. In the next iteration, we are considering
    *C* again and so on. For generating a trigram, see the same examples that we have
    looked at previously for. Refer to *Figure 5.45*:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你理解了双元语法重叠配对的概念，那么三元语法对你来说会更容易理解。三元语法只是双元语法的扩展，但如果你仍然感到困惑，那么让我们用通俗的语言为你解释。在*图
    5.44*的前三行中，我们生成了基于字符的双元语法，第四行是基于词的双元语法。我们从第一个字符开始，考虑下一个字符，因为我们考虑的是n=2，词语也是如此。请看第一行，我们考虑了如*AG*这样的双元语法作为第一个双元语法。然后，在下一次迭代中，我们再次考虑*G*并生成*GC*。在接下来的迭代中，我们再次考虑*C*，依此类推。要生成三元语法，请参阅我们之前看的相同示例。参考*图
    5.45*：
- en: '![](img/e6cf426f-2386-4eb9-bc01-a593893c4c0e.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6cf426f-2386-4eb9-bc01-a593893c4c0e.png)'
- en: 'Figure 5.45: Trigram example sequence'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.45：三元语法示例序列
- en: The preceding examples are very much self-explanatory. You can figure out how
    we are taking up the sequencing from the number of n. Here, we are taking the
    overlapped sequences, which means that if you are taking a trigram and taking
    the words **this**, **is**, and **a** as a single pair, then next time, you are
    considering **is**, **a**, and **pen**. Here, the word *is* overlaps, but these
    kind of overlapped sequences help store context. If we are using large values
    for n-five-gram or six-gram, we can store large contexts but we still need more
    space and more time to process the dataset.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子是非常自解释的。你可以根据n的数量理解我们是如何进行序列化的。在这里，我们取的是重叠的序列，这意味着如果你正在取三元组，并将单词**this**、**is**和**a**作为一个单独的对，下一次你会考虑**is**、**a**和**pen**。这里，单词*is*重叠了，但这种重叠的序列有助于存储上下文。如果我们使用较大的n值——五元组或六元组——我们可以存储更多的上下文，但我们仍然需要更多的空间和时间来处理数据集。
- en: Understanding n-gram using a practice example
  id: totrans-377
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过实践例子理解n-gram
- en: 'Now we are going to implement n-gram using the `nltk` library. You can see
    the code at this GitHub link:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用`nltk`库实现n-gram。你可以在这个GitHub链接中查看代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram](https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram).'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram](https://github.com/jalajthanaki/NLPython/tree/master/ch5/n_gram)'
- en: 'You can see the code snippet in *Figure 5.46*:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.46*中看到代码片段：
- en: '![](img/06f637d3-27a3-490c-b05e-33e2b85e2a31.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06f637d3-27a3-490c-b05e-33e2b85e2a31.png)'
- en: 'Figure 5.46: NLTK n-gram code'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.46：NLTK n-gram代码
- en: 'You can see the output code snippet in *Figure 5.47*:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.47*中看到输出代码片段：
- en: '![](img/ee9c7255-a8f7-4d7f-ac85-0e3036b5a6f4.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee9c7255-a8f7-4d7f-ac85-0e3036b5a6f4.png)'
- en: 'Figure 5.47: Output of the n-gram'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.47：n-gram的输出
- en: Application
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'In this section, we will see what kinds of applications n-gram has been used
    in:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到n-gram已经被应用于哪些方面：
- en: If you are making a plagiarism tool, you can use n-gram to extract the patterns
    that are copied, because that's what other plagiarism tools do to provide basic
    features
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在制作一个抄袭检测工具，你可以使用n-gram来提取被复制的模式，因为其他抄袭工具就是这样做的，以提供基本功能。
- en: Computational biology has been using n-grams to identify various DNA patterns
    in order to recognize any unusual DNA pattern; based on this, biologists decide
    what kind of genetic disease a person may have
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算生物学使用n-gram来识别各种DNA模式，以便识别任何异常的DNA模式；基于此，生物学家决定一个人可能患有什么样的遗传疾病。
- en: 'Now let''s move on to the next concept, which is an easy but very useful concept
    for NLP applications: Bag of words.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一个概念，这是一个简单但对NLP应用非常有用的概念：词袋模型。
- en: Bag of words
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋模型
- en: '**Bag of words** (**BOW**) is the technique that is used in the NLP domain.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '**词袋模型**（**BOW**）是NLP领域中使用的一种技术。'
- en: Understanding BOW
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解BOW
- en: This BOW model makes our life easier because it simplifies the representation
    used in NLP. In this model, the data is in the form of text and is represented
    as the bag or multiset of its words, disregarding grammar and word order and just
    keeping words. Here, text is either a sentence or document. Let's an example to
    give you a better understanding of BOW.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个BOW模型让我们的生活变得更简单，因为它简化了NLP中使用的表示方法。在这个模型中，数据以文本的形式存在，并被表示为其单词的集合或多重集合，忽略了语法和单词顺序，只保留单词。这里，文本可以是一个句子或文档。让我们通过一个例子更好地理解BOW。
- en: 'Let''s take the following sample set of documents:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看以下这组样本文档：
- en: 'Text document 1: John likes to watch cricket. Chris likes cricket too.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文档1：John喜欢看板球。Chris也喜欢板球。
- en: 'Text document 2: John also likes to watch movies.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文档2：John也喜欢看电影。
- en: 'Based on these two text documents, you can generate the following list:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这两个文本文档，你可以生成以下列表：
- en: '[PRE2]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This list is called **BOW**. Here, we are not considering the grammar of the
    sentences. We are also not bothered about the order of the words. Now it''s time
    to see the practical implementation of BOW. BOW is often used to generate features;
    after generating BOW, we can derive the term-frequency of each word in the document,
    which can later be fed to a machine learning algorithm. For the preceding documents,
    you can generate the following frequency list:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表被称为**BOW**。在这里，我们不考虑句子的语法，也不关心单词的顺序。现在是时候看看BOW的实际实现了。BOW通常用于生成特征；生成BOW后，我们可以推导出文档中每个单词的词频，随后可以将其输入到机器学习算法中。对于上述文档，你可以生成以下频率列表：
- en: 'Frequency count for Document 1: [1, 2, 1, 1, 2, 1, 1, 0, 0]'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 文档1的频率计数：[1, 2, 1, 1, 2, 1, 1, 0, 0]
- en: 'Frequency count for Document 2: [1, 1, 1, 1, 0, 0, 0, 1, 1]'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 文档2的频率计数：[1, 1, 1, 1, 0, 0, 0, 1, 1]
- en: 'So, how did we generate the list of frequency counts? In order to generate
    the frequency count of Document 1, consider the list of words and check how many
    times each of the listed words appear in Document 1\. Here, we will first take
    the word, *John*, which appears in Document 1 once; the frequency count for Document
    1 is 1\. **Frequency count for Document 1: [1]**. For the second entry, the word
    *like* appears twice in Document 1, so the frequency count is 2\. **Frequency
    count for Document 1: [1, 2].** Now, we will take the third word from our list
    and the word is *to***.** This word appears in Document 1 once, so we make the
    third entry in the frequency count as 1\. **Frequency count for Document 1: [1,
    2, 1].** We have generated the frequency count for Document 1 and Document 2 in
    the same way. We will learn more about frequency in the upcoming section, TF-IDF,
    in this chapter.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是如何生成频率计数列表的呢？为了生成文档1的频率计数，考虑一下单词列表并检查每个单词在文档1中出现的次数。这里，我们首先拿出单词*John*，它在文档1中出现了一次；文档1的频率计数为1。**文档1的频率计数：[1]**。对于第二个条目，单词*like*在文档1中出现了两次，所以频率计数是2。**文档1的频率计数：[1,
    2]**。现在，我们将取出列表中的第三个单词，单词是*to*。这个单词在文档1中出现了一次，所以我们将频率计数的第三个条目设为1。**文档1的频率计数：[1,
    2, 1]**。我们以相同的方式生成了文档1和文档2的频率计数。我们将在本章接下来的部分——TF-IDF中深入学习频率。
- en: Understanding BOW using a practical example
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用实际示例理解BOW
- en: 'In this section, we will look at the practical implementation of BOW using
    `scikit-learn`. You can find the code at this GitHub link:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过`scikit-learn`来看BOW的实际实现。你可以在这个GitHub链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py).'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/master/ch5/bagofwordsdemo/BOWdemo.py)。'
- en: 'See the code snippet in *Figure 5.48*:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 参见*图5.48*中的代码片段：
- en: '![](img/77214fed-ef02-4e3a-99ce-7c5a3cfd8791.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77214fed-ef02-4e3a-99ce-7c5a3cfd8791.png)'
- en: 'Figure 5.48: BOW scikit-learn implementation'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.48：BOW的scikit-learn实现
- en: 'The first row of the output belongs to the first document with the word, `words`,
    and the second row belongs to the document with the word, `wprds`. You can see
    the output in *Figure 5.49*:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一行属于包含单词`words`的第一个文档，第二行属于包含单词`wprds`的文档。你可以在*图5.49*中看到输出：
- en: '![](img/2f298bed-8b6e-40c8-b447-4fdc6d00033a.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f298bed-8b6e-40c8-b447-4fdc6d00033a.png)'
- en: 'Figure 5.49: BOW vector representation'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.49：BOW向量表示
- en: Comparing n-grams and BOW
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较n-grams和BOW
- en: We have looked at the concepts of n-grams and BOW. So, let's now see how n-grams
    and BOW are different or related to each other.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了n-grams和BOW的概念。现在，让我们看看n-grams和BOW是如何不同或相关的。
- en: Let's first discuss the differences. Here, the difference is in terms of their
    usage in NLP applications. In n-grams, word order is important, whereas in BOW
    it is not important to maintain word order. During the NLP application, n-gram
    is used to consider words in their real order so we can get an idea about the
    context of the particular word; BOW is used to build vocabulary for your text
    dataset.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先讨论它们的不同之处。这里，区别在于它们在NLP应用中的使用方式。在n-grams中，单词的顺序很重要，而在BOW中，保持单词顺序并不重要。在NLP应用中，n-gram用于考虑单词的实际顺序，以便我们能够了解某个特定单词的上下文；BOW用于为文本数据集构建词汇表。
- en: Now let's look at some meaningful relationships between n-grams and BOW that
    will give you an idea of how n-grams and BOW are related to each other. If you
    are considering n-gram as a feature, then BOW is the text representation derived
    using a unigram. So, in that case, an n-gram is equal to a feature and BOW is
    equal to a representation of text using a unigram (one-gram) contained within.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看n-grams和BOW之间的一些有意义的关系，这将帮助你理解n-grams和BOW是如何相互关联的。如果你将n-gram作为特征，那么BOW是通过单一词法（unigram）得出的文本表示。所以，在这种情况下，n-gram等同于一个特征，而BOW等同于使用其中包含的单一词法（one-gram）表示的文本。
- en: Now, let's check out an application of BOW.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看BOW的一个应用。
- en: Applications
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'In this section, we will look at which applications use BOW as features in
    the NLP domain:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看看哪些应用使用BOW作为NLP领域的特征：
- en: If you want to make an NLP application that classifies documents in different
    categories, then you can use BOW.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想制作一个可以将文档分类到不同类别的NLP应用，那么你可以使用BOW。
- en: BOW is also used to generate frequency count and vocabulary from a dataset.
    These derived attributes are then used in NLP applications such as sentiment analysis,
    Word2vec, and so on.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BOW（词袋模型）还用于从数据集中生成词频统计和词汇表。这些衍生出的属性随后被用于自然语言处理应用中，如情感分析、Word2vec等。
- en: Now it's time to look at some of the semantic tools that we can use if we want
    to include semantic-level information in our NLP applications.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们可以使用的一些语义工具，如果我们想在自然语言处理应用中加入语义级别的信息。
- en: Semantic tools and resources
  id: totrans-423
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义工具和资源
- en: Trying to get the accurate meaning of a natural language is still a challenging
    task in the NLP domain, although we do have some techniques that have been recently
    developed and resources that we can use to get semantics from natural language.
    In this section, we will try to understand these techniques and resources.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们确实有一些最近开发的技术和资源可以用来从自然语言中获取语义，但在自然语言处理领域，准确理解自然语言的含义仍然是一个具有挑战性的任务。在这一部分中，我们将尝试理解这些技术和资源。
- en: The latent semantic analysis algorithm uses t**erm frequency - inverse document
    Frequency** (**tf-idf**) and the concept of linear algebra, such as cosine similarity
    and Euclidean distance, to find words with similar meanings. These techniques
    are a part of distributional semantics. The other one is word2vec. This is a recent
    algorithm that has been developed by Google and can help us find the semantics
    of words and words that have similar meanings. We will explore word2vec and other
    techniques in [Chapter 6](c4861b9e-2bcf-4fce-94d4-f1e2010831de.xhtml), *Advance
    Features Engineering and NLP Algorithms*.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在语义分析算法使用**词频 - 逆文档频率**（**tf-idf**）以及线性代数中的概念，如余弦相似度和欧几里得距离，来找到具有相似含义的词汇。这些技术是分布式语义的一部分。另一种方法是word2vec。这是由谷歌最近开发的算法，可以帮助我们找到词汇的语义以及具有相似含义的词汇。我们将在[第六章](c4861b9e-2bcf-4fce-94d4-f1e2010831de.xhtml)，*高级特征工程与NLP算法*中探讨word2vec和其他技术。
- en: Apart from Word2vec, another powerful resource is `WordNet`, which is the largest
    corpus available to us and it's tagged by humans. It also contains sense tags
    for each word. These databases are really helpful for finding out the semantics
    of a particular word.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Word2vec，另一个强大的资源是`WordNet`，这是我们能使用的最大语料库，并且由人工标注。它还包含了每个词的词义标签。这些数据库对于找出特定词汇的语义非常有帮助。
- en: 'You can have a look at `WordNet` at the following link: [https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下链接查看`WordNet`：[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)
- en: Here, we have listed some of the most useful resources and tools for generating
    semantics. There is a lot of room for improvement in this area.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们列出了一些用于生成语义的最有用的资源和工具。这个领域还有很大的改进空间。
- en: We have seen most of the NLP domain-related concepts and we have also seen how
    we can derive features using these concepts and available tools. Now it's time
    to jump into the next section, which will give us information about statistical
    features.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了大多数与自然语言处理领域相关的概念，并且已经看到如何利用这些概念和现有工具来提取特征。现在是时候进入下一个部分，这一部分将为我们提供关于统计特征的信息。
- en: Basic statistical features for NLP
  id: totrans-430
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的基本统计特征
- en: In the last section, we looked at most of the NLP concepts, tools, and algorithms
    that can be used to derive features. Now it's time to learn about some statistical
    features as well. Here, we will explore the statistical aspect. You will learn
    how statistical concepts help us derive some of the most useful features.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们介绍了大多数可用于提取特征的自然语言处理（NLP）概念、工具和算法。现在是学习一些统计特征的时刻了。在这里，我们将探讨统计方面的内容。你将了解统计概念如何帮助我们提取一些最有用的特征。
- en: Before we jump into statistical features, as a prerequisite, you need to understand
    basic mathematical concepts, linear algebra concepts, and probabilistic concepts.
    So here, we will seek to understand these concepts first and then understand the
    statistical features.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们跳入统计特征之前，作为先决条件，你需要理解基本的数学概念、线性代数概念和概率概念。因此，在这里，我们将首先理解这些概念，然后再理解统计特征。
- en: Basic mathematics
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础数学
- en: We will begin with the basics of linear algebra and probability; this is because
    we want you to recall and memorize the necessary concepts so it will help you
    in this chapter as well as the upcoming chapters. We will explain the necessary
    math concepts as and when needed.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从线性代数和概率的基础知识开始；这是因为我们希望你能回忆并记住必要的概念，以便它们在本章以及接下来的章节中能对你有所帮助。我们将在需要时解释必要的数学概念。
- en: Basic concepts of linear algebra for NLP
  id: totrans-435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中的线性代数基本概念
- en: In this section, we will not look at all the linear algebra concepts in great
    detail. The purpose of this section is to get familiar with the basic concepts.
    Apart from the given concepts, there are many other concepts that can be used
    in NLP applications. Here, we will cover only the much needed concepts. We will
    give you all the necessary details about algorithms and their mathematical aspects
    in upcoming chapters. Let's get started with the basics.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们不会详细讨论所有线性代数的概念。本部分的目的是让你熟悉基本概念。除了给出的概念外，还有许多其他概念可以用于NLP应用。在这里，我们只会介绍必要的概念。关于算法及其数学方面的所有必要细节将在接下来的章节中提供。让我们从基础开始。
- en: 'There are four main terms that you will find consistently in NLP and ML:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP和机器学习中，你将经常遇到以下四个术语：
- en: '**Scalars**: They are just single, the real number'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标量**：它们只是单一的实数'
- en: '**Vectors**: They are a one-dimensional array of the numbers'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：它们是一个一维的数字数组'
- en: '**Matrices**: They are two-dimensional arrays of the numbers'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：它们是二维的数字数组'
- en: '**Tensors**: They are n-dimensional arrays of the numbers'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量**：它们是n维的数字数组'
- en: 'The pictorial representation is given in *Figure 5.50*:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图示表示见*图5.50*：
- en: '![](img/967508ae-8029-42fc-8ef2-1eaa80af81c7.png)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![](img/967508ae-8029-42fc-8ef2-1eaa80af81c7.png)'
- en: 'Figure 5.50: A pictorial representation of scalar, vector, matrix, and tensor
    (Image credit: http://hpe-cct.github.io/programmingGuide/img/diagram1.png)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.50：标量、向量、矩阵和张量的图示表示（图片来源：http://hpe-cct.github.io/programmingGuide/img/diagram1.png）
- en: Matrix manipulation operations are available in the `NumPy` library. You can
    perform vector-related operations using the `SciPy` and `scikit-learn` libraries.
    We will suggest certain libraries because their sources are written to give you
    optimal solutions and provide you with a high-level API so that you don't need
    to worry about what's going on behind the scenes. However, if you want to develop
    a customized application, then you need to know the math aspect of each manipulation.
    We will also look at the concept of linear regression, gradient descent, and linear
    algebra. If you really want to explore math that is related to machine learning
    and deep learning, then the following learning materials can help you.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵操作可以通过`NumPy`库进行。你可以使用`SciPy`和`scikit-learn`库进行与向量相关的操作。我们推荐某些库，因为它们的源代码旨在为你提供最佳解决方案，并为你提供高层次的API，这样你就不需要担心背后发生了什么。但是，如果你想开发自定义的应用程序，你就需要了解每个操作的数学方面。我们还会讨论线性回归、梯度下降和线性代数的概念。如果你真的想深入探讨与机器学习和深度学习相关的数学，以下的学习资料会对你有帮助。
- en: 'Part one of this book will really help you:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的第一部分将对你大有帮助：
- en: '[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)'
- en: 'A cheat sheet of statistics, linear algebra, and calculus can be found at this
    link:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在此链接找到统计学、线性代数和微积分的备忘单：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math](https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math).'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math](https://github.com/jalajthanaki/NLPython/tree/master/Appendix2/Cheatsheets/11_Math)。'
- en: 'If you are new to math, we recommend that you check out these videos:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对数学比较陌生，我们建议你观看这些视频：
- en: '[https://www.khanacademy.org/math/linear-algebra](https://www.khanacademy.org/math/linear-algebra)
    [https://www.khanacademy.org/math/probability](https://www.khanacademy.org/math/probability)
    [https://www.khanacademy.org/math/calculus-home](https://www.khanacademy.org/math/calculus-home)'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.khanacademy.org/math/linear-algebra](https://www.khanacademy.org/math/linear-algebra)
    [https://www.khanacademy.org/math/probability](https://www.khanacademy.org/math/probability)
    [https://www.khanacademy.org/math/calculus-home](https://www.khanacademy.org/math/calculus-home)'
- en: '[https://www.khanacademy.org/math/calculus-home/multivariable-calculus](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [https://www.khanacademy.org/math](https://www.khanacademy.org/math) [If you want
    to see the various vector similarity concepts, then this article will help you:](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.khanacademy.org/math/calculus-home/multivariable-calculus](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [https://www.khanacademy.org/math](https://www.khanacademy.org/math) [如果你想了解各种向量相似性概念，那么这篇文章将帮助你：](https://www.khanacademy.org/math/calculus-home/multivariable-calculus)
    [http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/](http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)'
- en: Now let's jump into the next section, which is all about probability. This is
    one of the core concepts of probabilistic theory.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入下一部分，讲解概率的相关内容。这是概率论中的核心概念之一。
- en: Basic concepts of the probabilistic theory for NLP
  id: totrans-454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理中的概率论基本概念
- en: In this section, we will look at some of the concepts of probabilistic theory.
    We will also look at some examples of them so that you can understand what is
    going on. We will start with probability, then the concept of an independent event,
    and then conditional probability. At the end, we will look at the Bayes rule.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨概率论的一些概念。我们还会看一些例子，以帮助你理解这些概念的实际应用。我们将从概率开始，然后讲解独立事件的概念，接着是条件概率。最后，我们会介绍贝叶斯定理。
- en: Probability
  id: totrans-456
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率
- en: 'Probability is a measure of the likelihood that a particular event will occur.
    Probability is quantified as a number and the range of probability is between
    0 and 1\. 0 means that the particular event will never occur and 1 indicates that
    the particular event will definitely occur. Machine learning techniques use the
    concept of probability widely. Let''s look at an example just to refresh the concept.
    Refer to *Figure 5\. 51*:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是衡量某个特定事件发生可能性的指标。概率以数字的形式量化，概率的范围介于0和1之间。0表示该事件永远不会发生，而1则表示该事件必定会发生。机器学习技术广泛应用概率的概念。我们来看一个例子，以帮助复习这个概念。参见*图5.51*：
- en: '![](img/8da1e11f-fd1e-4f0e-ba03-93ddc50419ae.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8da1e11f-fd1e-4f0e-ba03-93ddc50419ae.png)'
- en: 'Figure 5.51: Probability example (Image credit: http://www.algebra-class.com/image-files/examples-of-probability-3.gif)'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.51：概率示例（图片来源：http://www.algebra-class.com/image-files/examples-of-probability-3.gif）
- en: Now let's see what dependent and independent events are.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下什么是相关事件和独立事件。
- en: Independent event and dependent event
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立事件与相关事件
- en: In this section, we will look at what dependent events and independent events
    are. After that, we will see how to decide if an event is dependent or not. First,
    let's begin with definitions.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨什么是相关事件与独立事件。之后，我们将学习如何判断一个事件是否是相关事件。首先，让我们从定义开始。
- en: If the probability of one event doesn't affect the probability of the other
    event, then this kind of event is called an independent event. So technically,
    if you take two events, A and B, and if the fact that A occurs does not affect
    the probability of B occurring, then it's called an independent event. Flipping
    a fair coin is an independent event because it doesn't depend on any other previous
    events.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个事件的发生概率不影响另一个事件的发生概率，那么这种事件被称为独立事件。严格来说，如果你有两个事件A和B，且A发生的事实不影响B发生的概率，那么这两个事件就叫做独立事件。掷公正的硬币是一个独立事件，因为它不依赖于其他任何事件。
- en: Sometimes, some events affect other events. Two events are said to be dependent
    when the probability of one event occurring influences the other event's occurrence.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，某些事件会影响其他事件。当一个事件发生的概率会影响另一个事件发生时，这两个事件被认为是相关的。
- en: For example, if you were to draw two cards from a deck of 52 cards, and on your
    first draw you had an ace, the probability of drawing another ace on the second
    draw has changed because you drew an ace the first time. Let's calculate these
    different probabilities to see what's going on.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你从一副52张牌中抽取两张牌，而你第一次抽到的是一张王牌，那么第二次抽到王牌的概率就会发生变化，因为你第一次抽到了王牌。让我们计算一下这些不同的概率，看看发生了什么。
- en: 'There are four aces in a deck of 52 cards. See *Figure 5.52*:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 一副52张牌中有四张王牌。参见*图5.52*：
- en: '![](img/e3321a03-afb3-4ccd-9bf5-ce59dccbdb36.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3321a03-afb3-4ccd-9bf5-ce59dccbdb36.png)'
- en: 'Figure 5.52: Equation of probability'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.52：概率方程
- en: 'On your first draw, the probability of getting an ace is in *Figure 5.53*:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在你第一次抽取时，得到一张王牌的概率见于*图 5.53*：
- en: '![](img/93b3d6c4-1664-4737-8ad8-7a254bd3c42b.png)'
  id: totrans-470
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93b3d6c4-1664-4737-8ad8-7a254bd3c42b.png)'
- en: 'Figure 5.53: Calculation step (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/02cec729-378c-4293-8a5c-3873e0b06942.gif)'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.53：计算步骤（图片来源： https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/02cec729-378c-4293-8a5c-3873e0b06942.gif）
- en: 'Now if you don''t return this drawn card to the deck, the probability of drawing
    an ace on the second round is given in the following equation. See *Figure 5.54*:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你不把抽中的卡片放回牌堆，那么第二轮抽到王牌的概率在以下公式中给出。见*图 5.54*：
- en: '![](img/881756b3-18bf-4985-b5ff-559bddc014a0.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/881756b3-18bf-4985-b5ff-559bddc014a0.png)'
- en: 'Figure 5.54: Dependent event probability equation (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/7a45b393-0275-47ac-93e1-9669f5c31caa.gif)'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.54：依赖事件概率公式（图片来源： https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/7a45b393-0275-47ac-93e1-9669f5c31caa.gif）
- en: 'See the calculation step in *Figure 5.55*:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*图 5.55*中的计算步骤：
- en: '![](img/f5d8f123-48bf-4717-b544-97232a6d1084.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5d8f123-48bf-4717-b544-97232a6d1084.png)'
- en: 'Figure 5.55: Calculation step (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/11221e29-96ea-44fb-b7b5-af614f1bec96.gif)'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.55：计算步骤（图片来源： https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/11221e29-96ea-44fb-b7b5-af614f1bec96.gif）
- en: 'See the final answer in *Figure 5.56*:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 查看最终答案见于*图 5.56*：
- en: '![](img/5bf68296-740a-45d5-9aec-1c60bd3a475b.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bf68296-740a-45d5-9aec-1c60bd3a475b.png)'
- en: 'Figure 5.56: Final answer of the example (Image credit: https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/78fdf71e-fc1c-41d2-8fb8-bc2baeb25c25.gif)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.56：示例的最终答案（图片来源： https://dj1hlxw0wr920.cloudfront.net/userfiles/wyzfiles/78fdf71e-fc1c-41d2-8fb8-bc2baeb25c25.gif）
- en: As you can see, the preceding two probability values are different, so we say
    that the two events are dependent because the second event depends on the first
    event.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，前面两个概率值是不同的，所以我们说这两个事件是相关的，因为第二个事件依赖于第一个事件。
- en: 'The mathematical condition to check whether the events are dependent or independent
    is given as: events A and B are independent events if, and only if, the following
    condition will be satisfied:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 检查事件是否相关或独立的数学条件如下：如果且仅当以下条件满足时，事件A和事件B是独立事件：
- en: '*P(A ∩ B) = P(A) * P(B)*'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A ∩ B) = P(A) * P(B)*'
- en: Otherwise, *A* and *B* are called dependent events.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，*A* 和 *B* 被称为依赖事件。
- en: Now let's take an example to understand the defined condition.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们举个例子来理解定义的条件。
- en: '**Example**: A poll finds that 72% of the population of Mumbai consider themselves
    football fans. If you randomly pick two people from the population, what is the
    probability that the first person is a football fan and the second is as well?
    That the first one is and the second one isn''t?'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：一项调查发现，孟买市72%的人口认为自己是足球迷。如果你从中随机挑选两个人，第一人是足球迷且第二人也是足球迷的概率是多少？第一人是足球迷而第二人不是的概率是多少？'
- en: '**Solution**: The first person being a football fan doesn''t have any impact
    on whether the second randomly selected person is a football fan or not. Therefore,
    the events are independent.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**解答**：第一人是足球迷并不会影响第二个随机选择的人是否是足球迷。因此，事件是独立的。'
- en: The probability can be calculated by multiplying the individual probabilities
    of the given events together. If the first person and second person both are football
    fans, then P(A∩B) = P(A) P(B) = .72 * .72 = .5184.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率可以通过将给定事件的各自概率相乘来计算。如果第一人和第二人都是足球迷，那么 P(A∩B) = P(A) P(B) = .72 * .72 = .5184。
- en: 'For the second question: The first one is a football fan, the second one isn''t:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个问题：第一人是足球迷，第二人不是：
- en: '*P(A∩ not B) = P(A) P( B'' ) = .72 * ( 1 - 0.72) = 0.202*.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A∩非B) = P(A) P( B'' ) = .72 * ( 1 - 0.72) = 0.202*。'
- en: In this part of the calculation, we multiplied by the complement.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分计算中，我们乘以了补集的概率。
- en: Here, events *A* and *B* are independent because the equation *P(A∩B) = P(A)
    P(B)* holds true.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，事件 *A* 和 *B* 是独立的，因为公式 *P(A∩B) = P(A) P(B)* 成立。
- en: Now it's time to move on to the next concept called conditional probability.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候继续讲解下一个概念——条件概率了。
- en: Conditional probability
  id: totrans-494
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件概率
- en: In this section, we will look at a concept called conditional probability. We
    will use the concept of a dependent event and independent event to understand
    the concept of conditional probability.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论一个名为条件概率的概念。我们将运用依赖事件和独立事件的概念来理解条件概率。
- en: 'The conditional probability of an event *B* is the probability that the event
    will occur given the knowledge that an event, *A*, has already occurred. This
    probability is written as *P(B|A)*, the notation for the probability of *B* given
    *A*. Now let''s see how this conditional probability turns out when events are
    independent. Where events *A* and *B* are independent, the conditional probability
    of event *B* given event *A* is simply the probability of event B, that is, *P(B)*.
    What if events *A* and *B* are not independent? Then, the probability of the intersection
    of *A* and *B* means that the probability that both events occur is defined by
    the following equation:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 事件 *B* 的条件概率是给定事件 *A* 已经发生的情况下，事件 *B* 发生的概率。这个概率写作 *P(B|A)*，即给定 *A* 的情况下 *B*
    发生的概率。现在让我们看看当事件是独立时，这个条件概率会是什么样。若事件 *A* 和 *B* 独立，那么给定事件 *A* 的情况下事件 *B* 的条件概率就是事件
    B 的概率，即 *P(B)*。如果事件 *A* 和 *B* 不是独立的呢？那么，事件 *A* 和 *B* 的交集的概率意味着两个事件都发生的概率，由下列公式定义：
- en: '*P(A and B) = P(A) * P(B|A)*'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A 和 B) = P(A) * P(B|A)*'
- en: Now we will look at an example.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看一个例子。
- en: '**Example**: Jalaj''s two favorite food items are tea and pizza. Event A represents
    the event that I drink tea for my breakfast. B represents the event that I eat
    pizza for lunch. On randomly selected days, the probability that I drink tea for
    breakfast, P(A), is 0.6\. The probability that I eat pizza for lunch, P(B), is
    0.5 and the conditional probability that I drink tea for breakfast, given that
    I eat pizza for lunch, P(A|B) is 0.7\. Based on this, please calculate the conditional
    probability of P(B|A). P(B|A) will indicate the probability that I eat pizza for
    lunch, given that I drink tea for breakfast. In layman''s terms, find out the
    probability of having pizza for lunch when drinking tea for breakfast.'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例**：Jalaj 最喜欢的两样食物是茶和比萨饼。事件 A 表示我早餐喝茶，B 表示我午餐吃比萨饼。在随机选择的日子里，我早餐喝茶的概率 P(A)
    是 0.6。午餐吃比萨饼的概率 P(B) 是 0.5，而给定我午餐吃比萨的情况下我早餐喝茶的条件概率 P(A|B) 是 0.7。根据这些信息，请计算 P(B|A)
    的条件概率。P(B|A) 表示给定我早餐喝茶的情况下，我午餐吃比萨的概率。通俗地说，就是找出早餐喝茶时午餐吃比萨的概率。'
- en: '**Solution**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**解答**'
- en: '*P(A) = 0.6 , P(B) =0.5 , P(A|B) =0.7*'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A) = 0.6 , P(B) = 0.5 , P(A|B) = 0.7*'
- en: Here, two events are dependent because the probability of B being true has changed
    the probability of A being true. Now we need to calculate P(B|A).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，两个事件是依赖的，因为事件 B 发生的概率改变了事件 A 发生的概率。现在我们需要计算 P(B|A)。
- en: 'See the equation *P(A and B) = P(A) * P(B|A)*. To find out P(B|A), we first
    need to calculate P(A and B):'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 看到公式 *P(A 和 B) = P(A) * P(B|A)*。要找出 P(B|A)，我们首先需要计算 P(A 和 B)：
- en: '*P(A and B) = P(B) * P(A|B) = P(A) * P(B|A)*'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A 和 B) = P(B) * P(A|B) = P(A) * P(B|A)*'
- en: Here, we know that *P(B) = 0.5 and P(A|B) =0.7*
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们知道 *P(B) = 0.5 且 P(A|B) = 0.7*
- en: '*P(A and B) = 0.5 * 0.7 = 0.35*'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A 和 B) = 0.5 * 0.7 = 0.35*'
- en: '*P(B|A) = P(A and B) / P(A) = 0.35 / 0.6 = 0.5833*'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(B|A) = P(A 和 B) / P(A) = 0.35 / 0.6 = 0.5833*'
- en: So, we have found the conditional probability for dependent events.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经找到了依赖事件的条件概率。
- en: Now we have seen the basics of probability that we will use in upcoming chapters
    to understand ML algorithms. We will define additional concepts as we go. The
    `scikit-learn`, TensorFlow, SparkML, and other libraries already implement major
    probability calculation, provide us with high-level APIs, and have options that
    can change the predefined parameter and set values according to your application.
    These parameters are often called **hyperparameters**. To come up with the best
    suited values for each of the parameters is called **hyperparameter tuning**.
    This process helps us optimize our system. We will look at hyperparameter tuning
    and other major concepts in [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Applications*.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了概率的基础知识，这些将在接下来的章节中帮助我们理解机器学习算法。我们会随着学习的深入定义更多的概念。`scikit-learn`、TensorFlow、SparkML
    等库已经实现了主要的概率计算，提供了高级 API，并且有选项可以根据你的应用更改预定义参数并设置值。这些参数通常被称为 **超参数**。找到每个参数最适合的值称为
    **超参数调优**。这个过程有助于我们优化系统。我们将在 [第 8 章](97808151-90d2-4034-8d53-b94123154265.xhtml)
    *《自然语言处理应用中的机器学习》* 中讨论超参数调优和其他主要概念。
- en: This is the end of our prerequisite section. From this section onwards, we look
    at see some statistical concepts that help us extract features from the text.
    Many NLP applications also use them.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们前提部分的结束。从这一部分开始，我们将看到一些有助于从文本中提取特征的统计概念。许多 NLP 应用也使用这些概念。
- en: TF-IDF
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TF-IDF
- en: The concept TF-IDF stands for **term frequency-inverse document frequency**.
    This is in the field of numerical statistics. With this concept, we will be able
    to decide how important a word is to a given document in the present dataset or
    corpus.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF的概念代表**词频-逆文档频率**。这是数值统计学中的一个概念。通过这个概念，我们能够决定某个单词在当前数据集或语料库中对给定文档的重要性。
- en: Understanding TF-IDF
  id: totrans-513
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TF-IDF
- en: This is a very simple but useful concept. It actually indicates how many times
    a particular word appears in the dataset and what the importance of the word is
    in order to understand the document or dataset. Let's give you an example. Suppose
    you have a dataset where students write an essay on the topic, My Car. In this
    dataset, the word **a** appears many times; it's a high frequency word compared
    to other words in the dataset. The dataset contains other words like **car**,
    **shopping**, and so on that appear less often, so their frequency are lower and
    they carry more information compared to the word, **a**. This is the intuition
    behind TF-IDF.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单但有用的概念。它实际上表示某个特定单词在数据集中出现的次数，以及该单词在理解文档或数据集中的重要性。我们来举个例子。假设你有一个数据集，学生们就“我的车”这一主题写作文。在这个数据集中，**a**
    这个词出现了很多次，相较于数据集中的其他单词，它是一个高频词。数据集中还包含其他词汇，如**car**、**shopping**等，它们出现得较少，因此它们的频率较低，相比于**a**这个词，它们承载了更多的信息。这就是TF-IDF的直观理解。
- en: 'Let''s explain this concept in detail. Let''s also look at its mathematical
    aspect. TF-IDF has two parts: Term Frequency and Inverse Document Frequency. Let''s
    begin with the term frequency. The term is self-explanatory but we will walk through
    the concept. The term frequency indicates the frequency of each of the words present
    in the document or dataset. So, its equation is given as follows:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释这个概念。我们也来看看它的数学方面。TF-IDF有两个部分：词频（Term Frequency）和逆文档频率（Inverse Document
    Frequency）。我们从词频开始。词频显而易见，但我们还是来走一遍这个概念。词频表示文档或数据集中每个词汇的出现频率。所以，它的公式如下：
- en: '*TF(t) = (Number of times term t appears in a document) / (Total number of
    terms in the document)*'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(t) = （词t在文档中出现的次数） / （文档中的总词数）*'
- en: 'Now let''s look at the second part - inverse document frequency. IDF actually
    tells us how important the word is to the document. This is because when we calculate
    TF, we give equal importance to every single word. Now, if the word appears in
    the dataset more frequently, then its term frequency (TF) value is high while
    not being that important to the document. So, if the word **the** appears in the
    document 100 times, then it''s not carrying that much information compared to
    words that are less frequent in the dataset. Thus, we need to define some weighing
    down of the frequent terms while scaling up the rare ones, which decides the importance
    of each word. We will achieve this with the following equation:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看第二部分——逆文档频率。IDF实际上告诉我们一个单词对文档的重要性。这是因为在我们计算TF时，我们对每个单词赋予了相等的重要性。现在，如果一个词在数据集中出现得更频繁，那么它的词频（TF）值会很高，但它对文档的实际重要性却不高。所以，如果**the**这个词在文档中出现了100次，那么它相较于数据集中出现较少的单词，承载的信息就少了。因此，我们需要定义一种方式，对频繁出现的词汇进行“降权”，同时对稀有词汇进行“加权”，从而决定每个单词的重要性。我们将通过以下公式来实现：
- en: '*IDF(t) = log[10](Total number of documents / Number of documents with term
    t in it).*'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF(t) = log[10](文档总数 / 包含词t的文档数)。*'
- en: So, our equation is calculate TF-IDF is as follows.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们计算TF-IDF的公式如下。
- en: '*TF * IDF = [ (Number of times term t appears in a document) / (Total number
    of terms in the document) ] * log10(Total number of documents / Number of documents
    with term t in it).*'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF * IDF = [（词t在文档中出现的次数） / （文档中的总词数）] * log10（文档总数 / 包含词t的文档数）。*'
- en: Note that in TF-IDF, - is hyphen, not the minus symbol. In reality, TF-IDF is
    the multiplication of TF and IDF, such as *TF * IDF*.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在TF-IDF中，- 是连字符，而不是减号。实际上，TF-IDF是TF和IDF的乘积，比如 *TF * IDF*。
- en: 'Now, let''s take an example where you have two sentences and are considering
    those sentences as different documents in order to understand the concept of TF-IDF:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们举个例子，假设你有两个句子，并考虑将这两个句子视为不同的文档，以理解TF-IDF的概念：
- en: 'Document 1: This is a sample.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 文档1：这是一个示例。
- en: 'Document 2: This is another example.'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 文档2：这是另一个例子。
- en: 'Now to calculate TF-IDF, we will follow these steps:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了计算TF-IDF，我们将按照以下步骤进行：
- en: We first calculate the frequency of each word for each document.
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算每个文档中每个词汇的频率。
- en: We calculate IDF.
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算IDF。
- en: We multiply TF and IDF.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将TF和IDF相乘。
- en: 'Refer to *Figure 5.57*:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 参见*图5.57*：
- en: '![](img/8e8559f0-b3ed-4ffe-b474-6016d574dfa5.png)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e8559f0-b3ed-4ffe-b474-6016d574dfa5.png)'
- en: 'Figure 5.57: TF-IDF example'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.57：TF-IDF示例
- en: 'Now, let''s see the calculation of IDF and TF * IDF in *Figure 5.58*:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在*图5.58*中IDF和TF * IDF的计算：
- en: '![](img/c1278a75-815d-417b-b884-4e54c9344ce7.png)'
  id: totrans-533
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1278a75-815d-417b-b884-4e54c9344ce7.png)'
- en: 'Figure 5.58: TF-IDF example'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.58：TF-IDF示例
- en: Understanding TF-IDF with a practical example
  id: totrans-535
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过实际示例理解TF-IDF
- en: 'Here, we will use two libraries to calculate TF-IDF - textblob and scikit-learn.
    You can see the code at this GitHub link:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用两个库来计算TF-IDF——textblob和scikit-learn。你可以在这个GitHub链接中看到代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/TFIDFdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/TFIDFdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo)。'
- en: Using textblob
  id: totrans-538
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用textblob
- en: 'You can see the code snippet in *Figure 5.59*:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.59*中看到代码片段：
- en: '![](img/a5fbe677-d716-4aab-9325-fd83780e4757.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5fbe677-d716-4aab-9325-fd83780e4757.png)'
- en: 'Figure 5.59: TF-IDF using textblob'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.59：使用textblob的TF-IDF
- en: 'The output of the code is in *Figure 5.60*:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出如*图5.60*所示：
- en: '![](img/0c7946ba-5a96-4aa1-9638-93c26969e884.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c7946ba-5a96-4aa1-9638-93c26969e884.png)'
- en: 'Figure 5.60: Output of TF-IDF for the word short'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.60：短词的TF-IDF输出
- en: Using scikit-learn
  id: totrans-545
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn
- en: 'We will try to generate the TF-IDF model using a small Shakespeare dataset.
    For a new given document with a TF-IDF score model, we will suggest the top three
    keywords for the document. You can see the code snippet in *Figure 5.61*:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试使用一个小型的莎士比亚数据集生成TF-IDF模型。对于一个新的文档及其TF-IDF得分模型，我们将为该文档推荐前三个关键词。你可以在*图5.61*中看到代码片段：
- en: '![](img/15efd78d-2e2a-493a-ad28-797f7ca62476.png)'
  id: totrans-547
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15efd78d-2e2a-493a-ad28-797f7ca62476.png)'
- en: 'Figure 5.61: Using scikit-learn to generate a TF-IDF model'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.61：使用scikit-learn生成TF-IDF模型
- en: 'You can see the output in *Figure 5.62*:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图5.62*中看到输出：
- en: '![](img/be479c79-eef3-4acc-b3a9-874c2e4480b6.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be479c79-eef3-4acc-b3a9-874c2e4480b6.png)'
- en: 'Figure 5.62: Output of the TF-IDF model'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.62：TF-IDF模型的输出
- en: Now it's time to see where we can use this TF-IDF concept, so let's look at
    some applications.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们可以在哪里使用TF-IDF概念了，让我们来看看一些应用。
- en: Application
  id: totrans-553
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'In this section, we will look at some cool applications that use TF-IDF:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看看一些使用TF-IDF的酷应用：
- en: In general, text data analysis can be performed by TF-IDF easily. You can get
    information about the most accurate keywords for your dataset.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，TF-IDF可以轻松地进行文本数据分析。你可以获取关于数据集中最准确关键词的信息。
- en: If you are developing a text summarization application where you have a selected
    statistical approach, then TF-IDF is the most important feature for generating
    a summary for the document.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在开发一个文本摘要应用程序，并且选择了某种统计方法，那么TF-IDF是生成文档摘要时最重要的特征。
- en: Variations of the TF-IDF weighting scheme are often used by search engines to
    find out the scoring and ranking of a document's relevance for a given user query.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF加权方案的变体通常被搜索引擎用来确定文档与给定用户查询的相关性评分和排名。
- en: Document classification applications use this technique along with BOW.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类应用程序结合BOW使用这项技术。
- en: Now let's look at the concept of vectorization for an NLP application.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下NLP应用中的向量化概念。
- en: Vectorization
  id: totrans-560
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: Vectorization is an important aspect of feature extraction in the NLP domain.
    Transforming the text into a vector format is a major task.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化是NLP领域特征提取的一个重要方面。将文本转换为向量格式是一个主要任务。
- en: 'Vectorization techniques try to map every possible word to a specific integer.
    There are many available APIs that make your life easier. `scikit-learn` has `DictVectorizer`
    to convert text to a one-hot encoding form. The other API is the `CountVectorizer`,
    which converts the collection of text documents to a matrix of token counts. Last
    but not least, there are a couple of other APIs out there. We can also use word2vec
    to convert text data to the vector form. Refer to this link''s *From text* section
    for more details:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化技术尝试将每个可能的单词映射到一个特定的整数。有许多现成的API可以让你的工作更加轻松。`scikit-learn`提供了`DictVectorizer`，它将文本转换为独热编码形式。另一个API是`CountVectorizer`，它将文本集合转换为一个标记计数矩阵。最后，还有一些其他的API可供使用。我们还可以使用word2vec将文本数据转换为向量形式。有关更多详细信息，请参考此链接的*From
    text*部分：
- en: '[http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text).'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction.text)。'
- en: Now let's look at the concept of one-hot encoding for an NLP application. This
    one-hot encoding is considered as part of vectorization.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看NLP应用中一热编码的概念。一热编码被认为是向量化的一部分。
- en: Encoders and decoders
  id: totrans-565
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器和解码器
- en: The concept of encoding in NLP is quite old as well as useful. As we mentioned
    earlier, it is not easy to handle categorical data attributes present in our dataset.
    Here, we will explore the encoding technique named one-hot encoding, which helps
    us convert our categorical features in to a numerical format.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，编码的概念相当古老且有用。正如我们之前提到的，处理数据集中的类别数据属性并不容易。在这里，我们将探讨一种名为一热编码的编码技术，它帮助我们将类别特征转换为数值格式。
- en: One-hot encoding
  id: totrans-567
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一热编码
- en: In an NLP application, you always get categorical data. The categorical data
    is mostly in the form of words. There are words that form the vocabulary. The
    words from this vocabulary cannot turn into vectors easily.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）应用中，你总是会遇到类别数据。类别数据大多数以单词的形式存在。这些单词组成了词汇表，而这些词汇表中的单词并不容易直接转化为向量。
- en: Consider that you have a vocabulary with the size N. The way to approximate
    the state of the language is by representing the words in the form of one-hot
    encoding. This technique is used to map the words to the vectors of length n,
    where the n^(th) digit is an indicator of the presence of the particular word.
    If you are converting words to the one-hot encoding format, then you will see
    vectors such as 0000...001, 0000...100, 0000...010, and so on. Every word in the
    vocabulary is represented by one of the combinations of a binary vector. Here,
    the nth bit of each vector indicates the presence of the nth word in the vocabulary.
    So, how are these individual vectors related to sentences or other words in the
    corpus? Let's look at an example that will help you understand this concept.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个大小为N的词汇表。通过将单词表示为一热编码的形式，我们可以近似表示语言的状态。这种技术用于将单词映射到长度为n的向量，其中第n位是特定单词存在的指示符。如果你将单词转换为一热编码格式，你会看到像0000...001、0000...100、0000...010等向量。词汇表中的每个单词由一个二进制向量的组合表示。这里，每个向量的第n位表示词汇表中第n个单词的存在。那么，这些单独的向量如何与语料库中的句子或其他单词相关呢？让我们通过一个例子来帮助你理解这个概念。
- en: For example, you have one sentence, *Jalaj likes NLP*. Suppose after applying
    one-hot encoding, this sentence becomes 00010 00001 10000\. This vector is made
    based on the vocabulary size and encoding schema. Once we have this vector representation,
    then we can perform the numerical operation on it. Here, we are turning words
    into vectors and sentences into matrices.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你有一个句子，*Jalaj likes NLP*。假设应用一热编码后，这个句子变成了00010 00001 10000。这种向量是基于词汇表的大小和编码方案生成的。一旦我们有了这个向量表示，就可以对它进行数值运算。在这里，我们将单词转换为向量，将句子转换为矩阵。
- en: Understanding a practical example for one-hot encoding
  id: totrans-571
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解一热编码的实际例子
- en: 'In this section, we will use `scikit-learn` to generate one-hot encoding for
    a small dataset. You can find the code at this GitHub link:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`scikit-learn`为一个小型数据集生成一热编码。你可以通过这个GitHub链接查看代码：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo).'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo](https://github.com/jalajthanaki/NLPython/tree/master/ch5/onehotencodingdemo)。'
- en: 'You can see the code snippet in *Figure 5.63*:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.63*中看到代码片段：
- en: '![](img/79dd41bf-e6ce-4af1-8601-716404d78099.png)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79dd41bf-e6ce-4af1-8601-716404d78099.png)'
- en: 'Figure 5.63: Pandas and scikit-learn to generate one-hot encoding'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.63：使用Pandas和scikit-learn生成一热编码
- en: 'You can see the output in *Figure 5.64*:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 5.64*中看到输出：
- en: '![](img/9131b126-45aa-4853-a880-8f0b1fb7c3a5.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9131b126-45aa-4853-a880-8f0b1fb7c3a5.png)'
- en: 'Figure 5.64: Output of one-hot encoding'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.64：一热编码的输出
- en: Application
  id: totrans-580
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'These techniques are very useful. Let''s see some of the basic applications
    for this mapping technique:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术非常有用。让我们看看这种映射技术的一些基本应用：
- en: Many artificial neural networks accept input data in the one-hot encoding format
    and generate output vectors that carry the sematic representation as well
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多人工神经网络接受一热编码格式的输入数据，并生成包含语义表示的输出向量
- en: The word2vec algorithm accepts input data in the form of words and these words
    are in the form of vectors that are generated by one-hot encoding
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec算法接受以单词形式存在的输入数据，而这些单词则是通过一热编码生成的向量
- en: Now it's time to look at the decoding concept. Decoding concepts are mostly
    used in deep learning nowadays. So here, we will define the decoder in terms of
    deep learning because we will use this encoding and decoding architecture in [Chapter
    9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning for NLU and NLG
    Problems*, to develop a translation system.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看解码概念了。解码概念如今大多用于深度学习中。因此，在这里，我们将根据深度学习来定义解码器，因为我们将在[第9章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)，*用于自然语言理解和自然语言生成问题的深度学习*，中使用这个编码和解码架构来开发翻译系统。
- en: An encoder maps input data to a different feature representation; we are using
    one-hot encoding for the NLP domain. A decoder maps the feature representation
    back to the input data space. In deep learning, a decoder knows which vector represents
    which words, so it can decode words as per the given input schema. We will see
    the detailed concept of encoder-decoder when we cover the sequence-to-sequence
    model.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器将输入数据映射到不同的特征表示；我们在NLP领域使用的是独热编码。解码器将特征表示映射回输入数据空间。在深度学习中，解码器知道哪个向量表示哪个单词，因此它可以根据给定的输入架构解码单词。我们将在讲解序列到序列模型时详细讨论编码器-解码器的概念。
- en: Now, let's look at the next concept called **normalization**.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看下一个概念——**归一化**。
- en: Normalization
  id: totrans-587
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: Here, we will explain normalization in terms of linguistics as well as statistics.
    Even though they are different, the word normalization can create a lot of confusion.
    Let's resolve this confusion.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从语言学和统计学的角度解释归一化。尽管它们不同，但归一化这个词常常会引起许多困惑。我们来解决这个困惑。
- en: The linguistics aspect of normalization
  id: totrans-589
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化的语言学方面
- en: The linguistics aspect of normalization includes the concept text normalization.
    Text normalization is the process of transforming the given text into a single
    canonical form. Let's take an example to understand text normalization properly.
    If you are making a search application and you want the user to enter **John**,
    then John becomes a search string and all the strings that contain the word **John**
    should also pop up. If you are preparing data to search, then people prefer to
    take the stemmed format; even if you search **flying** or **flew**, ultimately
    these are forms that are derived from the word **fly**. So, the search system
    uses the stemmed form and other derived forms are removed. If you recall Chapter
    3, *Understanding Structure of Sentences*, then you will remember that we have
    already discussed how to derive lemma, stem, and root.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化的语言学方面包括文本归一化的概念。文本归一化是将给定文本转化为单一标准形式的过程。让我们通过一个例子来正确理解文本归一化。如果你正在开发一个搜索应用，并且希望用户输入**John**，那么John就成了搜索字符串，所有包含**John**的字符串也应该显示出来。如果你正在准备搜索数据，通常人们会选择词干化格式；即使你搜索**flying**或**flew**，它们最终都是由**fly**这个词派生出来的形式。所以，搜索系统使用词干形式，其他派生形式则被移除。如果你回想一下第3章，*理解句子的结构*，你就会记得我们已经讨论过如何推导词形、词干和词根。
- en: The statistical aspect of normalization
  id: totrans-591
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化的统计学方面
- en: The statistical aspect of normalization is used to do features scaling. If you
    have a dataset where one data attribute's ranges are too high and the other data
    attributes' ranges are too small, then generally we need to apply statistical
    techniques to bring all the data attributes or features into one common numerical
    range. There are many ways to perform this transformation, but here we will illustrate
    the most common and easy method of doing this called **min-max scaling**. Let's
    look at equation and mathematical examples to understand the concept.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化的统计学方面用于特征缩放。如果你的数据集中，某一数据属性的取值范围过大，而其他数据属性的取值范围过小，那么通常我们需要应用统计技术，将所有数据属性或特征带入一个统一的数值范围。执行这种转换的方法有很多种，但在这里我们将介绍最常见且简单的方式——**最小-最大缩放**。让我们通过方程式和数学示例来理解这个概念。
- en: 'Min-max scaling brings the features in the range of [0,1]. The general formula
    is given in *Figure 5.65*:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放将特征带入[0,1]范围。通用公式如*图 5.65*所示：
- en: '![](img/4d1a6acf-7595-4eea-bb16-6323a59ef1b9.jpg)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d1a6acf-7595-4eea-bb16-6323a59ef1b9.jpg)'
- en: 'Figure 5.65: The min-max normalization equation'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.65：最小-最大归一化方程式
- en: Suppose you have features values such as *[1, 8, 22, 25]*; i you apply the preceding
    formula and calculate the value for each of the elements, then you will get the
    feature with a range of [0,1]. For the first element, *z = 1 - 1/ 25 -1 = 0*,
    for the second element, *z =8 -1 /25-1 = 0.2917*, and so on. The `scikit-learn`
    library has an API that you can use for min-max scaling on the dataset.
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些特征值，如 *[1, 8, 22, 25]*；如果你应用前面的公式并计算每个元素的值，那么你将得到一个范围在 [0,1] 之间的特征值。对于第一个元素，*z
    = 1 - 1 / 25 -1 = 0*，对于第二个元素，*z =8 -1 /25-1 = 0.2917*，以此类推。`scikit-learn` 库提供了一个API，你可以使用它对数据集进行最小-最大缩放。
- en: In the next section, we will cover the language model.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论语言模型。
- en: Probabilistic models
  id: totrans-598
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率模型
- en: We will discuss one of the most famous probabilistic models in NLP, which has
    been used for a variety of applications - the language model. We will look at
    the basic idea of the language model. We are not going to dive deep into this,
    but we will get an intuitive idea on how the language model works and where we
    can use it.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论自然语言处理中最著名的概率模型之一，它已被应用于多种任务——语言模型。我们将了解语言模型的基本思想。我们不会深入探讨这个问题，但我们会对语言模型的工作原理以及它的应用场景有一个直观的了解。
- en: Understanding probabilistic language modeling
  id: totrans-600
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解概率语言建模
- en: 'There are two basic goals of the **language model** (**LM**):'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（**LM**）有两个基本目标：
- en: The goal of LM is to assign probability to a sentence or sequence of words
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型（LM）的目标是为一句话或单词序列分配概率。
- en: LM also tells us about the probability of the upcoming word, which means that
    it indicates which is the next most likely word by observing the previous word
    sequence
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型（LM）还告诉我们下一个单词的概率，这意味着它通过观察前一个单词序列来指示下一个最可能的单词。
- en: 'If any model can compute either of the preceding tasks, it is called a language
    model. LM uses the conditional probability chain rule. The chain rule of conditional
    probability is just an extension of conditional probability. We have already seen
    the equation:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何模型能够计算前面提到的任务中的任何一个，那么它就被称为语言模型（LM）。语言模型使用条件概率链式法则。条件概率的链式法则实际上是条件概率的一种扩展。我们已经见过这个方程：
- en: '*P(A|B) = P(A and B) / P(B)*'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A|B) = P(A and B) / P(B)*'
- en: '*P(A and B) = P(A,B) = P(A|B) P(B)*'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A and B) = P(A,B) = P(A|B) P(B)*'
- en: 'Here, P(A,B) is called **joint probability**. Suppose you have multiple events
    that are dependent, then the equation to compute joint probability becomes more
    general:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，P(A,B) 被称为**联合概率**。假设你有多个相互依赖的事件，那么计算联合概率的方程变得更加一般化：
- en: '*P(A,B,C,D) = P(A) P(B|A)P(C|A,B)P(D|A,B,C)*'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(A,B,C,D) = P(A) P(B|A)P(C|A,B)P(D|A,B,C)*'
- en: '*P(x1,x2,x3,...,xn) =P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,x2,x3,...xn-1)*'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(x1,x2,x3,...,xn) =P(x1)P(x2|x1)P(x3|x1,x2)...P(xn|x1,x2,x3,...xn-1)*'
- en: 'The preceding equation is called chain rule for conditional probability. LM
    uses this to predict the probability of upcoming words. We often calculate probability
    by counting the number of times a particular event occurs and dividing it by the
    total number of possible combinations, but we can''t apply this to language because,
    with certain words, you can generate millions of sentences. So, we are not going
    to use the probability equation; we are using an assumption called **Markov Assumption**
    to calculate probability instead. Let''s understand the concept intuitively before
    looking at a technical definition of it. If you have very a long sentence and
    you are trying to predict what the next word in the sentence sequence will be,
    then you actually need to consider all the words that are already present in the
    sentence to calculate the probability for the upcoming word. This calculation
    is very tedious, so we consider only the last one, two or three words to compute
    the probability for the upcoming word; this is called the Markov assumption. The
    assumption is that you can calculate the probability of the next word that comes
    in a sequence of the sentence by looking at the last word two. Let''s take an
    example to understand this. If you want to calculate the probability of a given
    word, then it is only dependent on the last word. You can see the equation here:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程式称为条件概率的链式法则。语言模型使用此法则来预测即将出现的词的概率。我们通常通过计算特定事件发生的次数，并将其除以所有可能组合的总数来计算概率，但在语言中我们无法应用此方法，因为对于某些词，你可以生成数百万个句子。因此，我们不使用概率方程式，而是使用一种叫做**马尔可夫假设**的假设来计算概率。让我们在查看技术定义之前，先直观地理解这个概念。如果你有一个非常长的句子，并且你正在尝试预测句子序列中的下一个词，那么你实际上需要考虑句子中已经出现的所有词来计算下一个词的概率。这个计算非常繁琐，因此我们只考虑最后一个、两个或三个词来计算下一个词的概率；这就是所谓的马尔可夫假设。这个假设是，通过查看句子中最后一个或两个词，你可以计算下一个词的概率。让我们通过一个例子来理解这一点。如果你想计算给定词的概率，那么它只依赖于前一个词。你可以看到这里的方程式：
- en: '*P(the | its water is so transparent that) = P(the | that) or you can consider
    last two words P(the | its water is so transparent that) = P(the | transparent
    that)*'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(the | its water is so transparent that) = P(the | that) 或者你也可以考虑前两个词 P(the
    | its water is so transparent that) = P(the | transparent that)*'
- en: 'A simple LM uses a unigram, which means that we are just considering the word
    itself and calculating the probability of an individual word; you simply take
    the probability of individual words and generate a random word sequence. If you
    take a bigram model, then you consider that one previous word will decide the
    next word in the sequence. You can see the result of the bigram model in *Figure
    5.66*:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的语言模型使用的是单元模型（unigram），意味着我们仅考虑单词本身并计算单个词的概率；你只是取单个词的概率并生成随机的词序列。如果你使用二元模型（bigram），则考虑前一个词将决定序列中的下一个词。你可以在*图
    5.66*中看到二元模型的结果：
- en: '![](img/cca09d81-e3d0-4671-912d-29684c6e2954.png)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cca09d81-e3d0-4671-912d-29684c6e2954.png)'
- en: 'Figure 5.66: Output using a bigram LM'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.66：使用二元语言模型的输出
- en: 'How can we count the n-gram probability that is a core part of LM? Let''s look
    at the bigram model. We will see the equation and then go through the example.
    See the equation in *Figure 5.67*:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何计算语言模型中作为核心部分的n-gram概率？让我们来看一下二元模型。我们将看到方程式，然后通过例子来讲解。请参见*图 5.67*中的方程式：
- en: '![](img/56ac8ba9-9e7d-4a60-a2cf-bf8afa430ee2.png)'
  id: totrans-616
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ac8ba9-9e7d-4a60-a2cf-bf8afa430ee2.png)'
- en: 'Figure 5.67: Equation to find out the next most likely word in the sequence'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.67：计算序列中下一个最可能词的方程式
- en: 'The equation is easy to understand. We need to calculate how many times the
    words *wi-1* and *wi* occurred together, and we also need to count how many times
    the word *wi-1* occurred. See the example in *Figure 5.68*:'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式很容易理解。我们需要计算*wi-1*和*wi*一起出现的次数，还需要计算*wi-1*出现的次数。请参见*图 5.68*中的例子：
- en: '![](img/1d194dbd-69b3-4b63-961b-a462a8056fb5.png)'
  id: totrans-619
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d194dbd-69b3-4b63-961b-a462a8056fb5.png)'
- en: 'Figure 5.68: Example to find out the most likely word using LM'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.68：使用语言模型找出最可能的词的例子
- en: As you can see, we is followed by *<s>* twice in three given sentences, so we
    have *P(I|<s>) =2/3*, and for every word, we will calculate the probability. Using
    LM, we can come to know how the word pairs are described in the corpus as well
    as what the more popular word pairs that occur in the corpus are. If we use a
    four-gram or five-gram model, it will give us a good result for LM because some
    sentences have a long-dependency relationship in their syntax structure with subject
    and verbs. So, with a four-gram and five-gram model, you can build a really good
    LM.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在三句话中，“we”后面跟着两次“*<s>*”，所以我们有 *P(I|<s>) = 2/3*，对于每个单词，我们都会计算其概率。通过使用语言模型（LM），我们可以了解单词对在语料库中的描述，以及语料库中出现的更常见的单词对。如果我们使用四元组或五元组模型，它会为语言模型提供一个良好的结果，因为有些句子在语法结构中具有长依赖关系，尤其是主语和动词之间的关系。因此，使用四元组或五元组模型，你可以构建一个非常好的语言模型。
- en: Application of LM
  id: totrans-622
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型的应用
- en: 'LM has a lot of great applications in the NLP domain. Most NLP applications
    use LM at some point. Let''s see them:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型（LM）在NLP领域有很多优秀的应用。大多数NLP应用都会在某个阶段使用语言模型。让我们来看一下这些应用：
- en: Machine translation systems use LM to find out the probability for each of the
    translated sentences in order to decide which translated sentence is the best
    possible translation for the given input sentence
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译系统使用语言模型来计算每个翻译句子的概率，从而决定哪个翻译句子是给定输入句子的最佳翻译。
- en: To spell the correct application, we can use a bigram LM to provide the most
    likely word suggestion
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了拼写正确的应用，我们可以使用二元语言模型（bigram LM）来提供最可能的单词建议。
- en: We can use LM for text summarization
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用语言模型进行文本摘要。
- en: We can use LM in a question answering system to rank the answers as per their
    probability
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在问答系统中使用语言模型，根据概率对答案进行排名。
- en: Indexing
  id: totrans-628
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 索引化
- en: '**Indexing** is quite a useful technique. This is used to convert the categorical
    data to its numerical format. In an NLP application, you may find that the data
    attributes are categorical and you want to convert them to a certain numerical
    value. In such cases, this indexing concept can help you. We can use the SparkML
    library, which has a variety of APIs to generate indexes. SparkML has an API named
    StringIndexer that uses the frequency of the categorical data and assigns the
    index as per the frequency count. So, the most frequent category gets an index
    value of 0\. This can sometimes be a naïve way of generating indexing, but in
    some analytical applications, you may find this technique useful. You can see
    the example at this link:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引化**是一种非常有用的技术。它用于将类别数据转换为数字格式。在NLP应用中，你可能会发现数据属性是类别型的，你希望将其转换为某个数字值。在这种情况下，索引化概念可以帮到你。我们可以使用SparkML库，它提供了多种API来生成索引。SparkML有一个名为StringIndexer的API，它利用类别数据的频率并根据频率计数分配索引。所以，出现最频繁的类别会得到索引值0。虽然这有时可能是一种简单的索引生成方式，但在某些分析应用中，你可能会发现这种技术非常有用。你可以在以下链接查看示例：'
- en: '[https://spark.apache.org/docs/latest/ml-features.html#stringindexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer).'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/ml-features.html#stringindexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer)。'
- en: 'SparkML has the API, IndexToString, which you can use when you need to convert
    your numerical values back to categorical values. You can find the example at
    this link:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: SparkML有一个名为IndexToString的API，当你需要将数值转换回类别值时可以使用它。你可以在以下链接找到示例：
- en: '[https://spark.apache.org/docs/latest/ml-features.html#indextostring](https://spark.apache.org/docs/latest/ml-features.html#indextostring).'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/ml-features.html#indextostring](https://spark.apache.org/docs/latest/ml-features.html#indextostring)。'
- en: Application
  id: totrans-633
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: 'Here are some applications which use indexing for extracting features:'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些使用索引化提取特征的应用：
- en: When we are dealing with a multiclass classifier and our target classes are
    in the text format and we want to convert our target class labels to a numerical
    format, we can use StingIndexer
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们处理一个多类别分类器，并且目标类别是文本格式时，我们想将目标类别标签转换为数字格式，可以使用StringIndexer。
- en: We can also generate the text of the `target` class using the IndexToString
    API
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以使用IndexToString API生成`target`类别的文本。
- en: Now it's time to learn about a concept called ranking.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候了解一个叫做排名的概念了。
- en: Ranking
  id: totrans-638
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排名
- en: In many applications, ranking plays a key role. The concept of **ranking** is
    used when you search anything on the web. Basically, the ranking algorithm is
    used to find the relevance of the given input and generated output.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，排名扮演着关键角色。当你在网页上搜索任何内容时，就会使用到**排名**的概念。基本上，排名算法用于找出给定输入和生成输出之间的相关性。
- en: Let's look at an example. When you search the web, the search engine takes your
    query, processes it, and generates some result. It uses the ranking algorithm
    to find the most relevant link according to your query and displays the most relevant
    link or content at the top and least relevant at the end. The same thing happens
    when you visit any online e-commerce website; when you search for a product, they
    display the relevant product list to you. To make their customer experience enjoyable,
    they display those products that are relevant to your query, whose reviews are
    good, and that have affordable prices. These all are the parameters given to the
    ranking algorithm in order to generate the most relevant products.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子。当你在网上搜索时，搜索引擎会处理你的查询并生成一些结果。它使用排序算法根据你的查询找到最相关的链接，并将最相关的链接或内容显示在顶部，而最不相关的则显示在底部。当你访问任何在线电子商务网站时也是如此；当你搜索某个产品时，它们会向你展示相关的产品列表。为了提升客户体验，它们会展示那些与你的查询相关、评价良好并且价格合理的产品。这些都作为参数传递给排序算法，用以生成最相关的产品。
- en: 'The implementation of the ranking algorithm is not a part of this book. You
    can find more information that will be useful here:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 排序算法的实现不属于本书的内容。你可以在这里找到更多有用的信息：
- en: '[https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f](https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f).'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f](https://medium.com/towards-data-science/learning-to-rank-with-python-scikit-learn-327a5cfd81f)。'
- en: Indexing and ranking are not frequently used in the NLP domain, but they are
    much more important when you are trying to build an application related to analytics
    using machine learning. It is mostly used to learn the user's preferences. If
    you are making a Google News kind of NLP application, where you need to rank certain
    news events, then ranking and indexing plays a major role. In a question answering
    system, generating ranks for the answers is the most critical task, and you can
    use indexing and ranking along with a language model to get the best possible
    result Applications such as grammar correction, proofreading, summarization systems,
    and so on don't use this concept.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 索引和排序在 NLP 领域并不常用，但当你尝试构建一个与分析相关的机器学习应用时，它们变得尤为重要。它们主要用于学习用户的偏好。如果你正在制作类似 Google
    News 的 NLP 应用，在其中需要对特定新闻事件进行排序，那么排序和索引就起到了至关重要的作用。在问答系统中，为答案生成排名是最关键的任务，你可以结合使用索引、排序和语言模型，以获得最佳的结果。像语法修正、校对、摘要系统等应用则不使用这一概念。
- en: We have seen most of the basic features that we can use in NLP applications.
    We will be using most of them in Chapter 8, *Machine Learning for NLP Application*,
    where we will build some real-life NLP applications with ML algorithms. In the
    upcoming section, we will explain the advantages and challenges of features engineering.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了在 NLP 应用中可以使用的大部分基本特性。我们将在第八章《*NLP 应用中的机器学习*》中使用它们，届时我们将使用 ML 算法构建一些实际的
    NLP 应用。在接下来的章节中，我们将解释特征工程的优势和挑战。
- en: Advantages of features engineering
  id: totrans-645
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程的优势
- en: 'Features engineering is the most important aspect of the NLP domain when you
    are trying to apply ML algorithms to solve your NLP problems. If you are able
    to derive good features, then you can have many advantages, which are as follows:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是 NLP 领域最重要的方面，当你尝试应用 ML 算法来解决 NLP 问题时。如果你能够提取出好的特征，那么你将拥有以下几个优势：
- en: Better features give you a lot of flexibility. Even if you choose a less optimal
    ML algorithm, you will get a good result. Good features provide you with the flexibility
    of choosing an algorithm; even if you choose a less complex model, you get good
    accuracy.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的特征会给你带来更大的灵活性。即使你选择了一个不太理想的 ML 算法，仍然能得到不错的结果。好的特征让你在选择算法时更具灵活性；即使选择了一个较简单的模型，依然可以得到较好的准确度。
- en: If you choose good features, then even simple ML algorithms do well.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你选择了好的特征，即使是简单的 ML 算法也能取得好效果。
- en: Better features will lead you to better accuracy. You should spend more time
    on features engineering to generate the appropriate features for your dataset.
    If you derive the best and appropriate features, you have won most of the battle.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的特征将带来更好的准确度。你应该花更多的时间进行特征工程，为你的数据集生成合适的特征。如果你能提取出最佳且合适的特征，那么你就赢得了大部分的胜利。
- en: Challenges of features engineering
  id: totrans-650
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程的挑战
- en: 'Here, we will discuss the challenges of features engineering for NLP applications.
    You must be thinking that we have a lot of options available in terms of tools
    and algorithms, so what is the most challenging part? Let''s find out:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论NLP应用中的特征工程挑战。你一定在想，我们有很多工具和算法的选择，那么最具挑战性的部分是什么呢？让我们来找出答案：
- en: In the NLP domain, you can easily derive the features that are categorical features
    or basic NLP features. We have to convert these features into a numerical format.
    This is the most challenging part.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在NLP领域，你可以轻松地推导出类别特征或基本的NLP特征。我们必须将这些特征转换为数字格式。这是最具挑战性的部分。
- en: An effective way of converting text data into a numerical format is quite challenging.
    Here, the trial and error method may help you.
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本数据转换为数字格式的有效方法相当具有挑战性。在这里，试错法可能会对你有所帮助。
- en: Although there are a couple of techniques that you can use, such as TF-IDF,
    one-hot encoding, ranking, co-occurrence matrix, word embedding, Word2Vec, and
    so on to convert your text data into a numerical format, there are not many ways,
    so people find this part challenging.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管有一些技术可以用于将文本数据转换为数字格式，例如TF-IDF、独热编码、排名、共现矩阵、词嵌入、Word2Vec等，但方法并不多，因此人们觉得这一部分具有挑战性。
- en: Summary
  id: totrans-655
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen many concepts and tools that are widely used in
    the NLP domain. All of these concepts are the basic building blocks of features
    engineering. You can use any of these techniques when you want to generate features
    in order to generate NLP applications. We have looked at how parse, POS taggers,
    NER, n-grams, and bag-of-words generate Natural Language-related features. We
    have also explored the how they are built and what the different ways to tweak
    some of the existing tools are in case you need custom features to develop NLP
    applications. Further, we have seen basic concepts of linear algebra, statistics,
    and probability. We have also seen the basic concepts of probability that will
    be used in ML algorithms in the future. We have looked at some cool concepts such
    as TF-IDF, indexing, ranking, and so on, as well as the language model as part
    of the probabilistic model.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了NLP领域中广泛使用的许多概念和工具。所有这些概念都是特征工程的基本构建块。当你想生成NLP应用时，可以使用这些技术中的任何一种来生成特征。我们已经了解了如何通过解析、词性标注、命名实体识别（NER）、n-gram和词袋模型生成与自然语言相关的特征。我们还探索了它们的构建方式，以及在需要定制特征来开发NLP应用时，如何调整一些现有工具。此外，我们已经学习了线性代数、统计学和概率论的基本概念。我们还了解了将来在机器学习算法中使用的概率论基本概念。我们还研究了一些有趣的概念，如TF-IDF、索引、排名等，以及作为概率模型一部分的语言模型。
- en: In the next chapter, we will look at advanced features such as word2vec, Doc2vec,
    Glove, and so on. All of these algorithms are part of word embedding techniques.
    These techniques will help us convert our text features into a numerical format
    efficiently; especially when we need to use semantics. The next chapter will provide
    you with much more detailed information about the word2Vec algorithm. We will
    cover each and every technicality behind the word2vec model. We will also understand
    how an **artificial neural network** (**ANN**) is used to generate the semantic
    relationship between words, and then we will explore an extension of this concept
    from word level, sentence level, document level, and so on. We will build an application
    that includes some awesome visualization for word2vec. We will also discuss the
    importance of vectorization, so keep reading!
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍一些高级特征，如word2vec、Doc2vec、Glove等。所有这些算法都是词嵌入技术的一部分。这些技术将帮助我们有效地将文本特征转换为数字格式，尤其是当我们需要使用语义时。下一章将为你提供关于word2Vec算法的更详细信息。我们将覆盖word2vec模型背后的每一个技术细节。我们还将了解如何使用**人工神经网络**（**ANN**）来生成单词之间的语义关系，然后我们将从单词级别、句子级别、文档级别等方面探索这一概念的扩展。我们将构建一个包含一些精彩的word2vec可视化的应用程序。我们还将讨论向量化的重要性，所以请继续阅读！
