- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Imitation Learning and Inverse RL
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模仿学习与逆向强化学习
- en: Learning from demonstration is often called imitation learning. In the imitation
    learning setting, we have expert demonstrations and train our agent to mimic those
    expert demonstrations. Learning from demonstrations has many benefits, including
    helping an agent to learn more quickly. There are several approaches to perform
    imitation learning, and two of them are **supervised imitation learning** and
    **Inverse Reinforcement Learning** (**IRL**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从示范学习通常被称为模仿学习。在模仿学习的设置中，我们拥有专家的示范，并训练我们的智能体模仿这些专家的示范。通过示范学习有很多好处，包括帮助智能体更快地学习。执行模仿学习有几种方法，其中两种是**监督式模仿学习**和**逆向强化学习**（**IRL**）。
- en: First, we will understand how we can perform imitation learning using supervised
    learning, and then we will learn about an algorithm called **Dataset Aggregation**
    (**DAgger**). Next, we will learn how to use demonstration data in a DQN using
    an algorithm called **Deep Q Learning from Demonstrations** (**DQfD**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解如何使用监督学习来执行模仿学习，然后我们将学习一种叫做**数据集聚合**（**DAgger**）的算法。接下来，我们将学习如何在 DQN
    中使用示范数据，这里有一种叫做**深度 Q 学习从示范**（**DQfD**）的算法。
- en: Moving on, we will learn about IRL and how it differs from reinforcement learning.
    We will learn about one of the most popular IRL algorithms called **maximum entropy
    IRL**. Toward the end of the chapter, we will understand how **Generative Adversarial
    Imitation Learning** (**GAIL**) works.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解 IRL 以及它如何与强化学习不同。我们将学习一种非常流行的 IRL 算法——**最大熵逆向强化学习**。在本章的最后，我们将了解**生成对抗模仿学习**（**GAIL**）是如何工作的。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Supervised imitation learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督式模仿学习
- en: DAgger
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAgger
- en: Deep Q learning from demonstrations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 Q 学习与示范
- en: Inverse reinforcement learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逆向强化学习
- en: Maximum entropy inverse reinforcement learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大熵逆向强化学习
- en: Generative adversarial imitation learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗模仿学习
- en: Let's begin our chapter by understanding how supervised imitation learning works.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过理解监督式模仿学习如何工作来开始本章。
- en: Supervised imitation learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式模仿学习
- en: In the imitation learning setting, our goal is to mimic the expert. Say, we
    want to train our agent to drive a car. Instead of training the agent from scratch
    by having them interact with the environment, we can train them with expert demonstrations.
    Okay, what are expert demonstrations? An expert demonstrations are a set of trajectories
    consisting of state-action pairs where each action is performed by the expert.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习的设置中，我们的目标是模仿专家。假设，我们想要训练我们的智能体开车。我们可以通过专家的示范来训练智能体，而不是让它从头开始与环境互动。好，那么什么是专家示范呢？专家示范是一组包含状态-动作对的轨迹，其中每个动作都是由专家执行的。
- en: We can train an agent to mimic the actions performed by the expert in various
    respective states. Thus, we can view expert demonstrations as training data used
    to train our agent. The fundamental idea of imitation learning is to imitate (learn)
    the behavior of an expert.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以训练智能体模仿专家在各种相应状态下执行的动作。因此，我们可以将专家的示范视为训练数据，用来训练我们的智能体。模仿学习的基本思想是模仿（学习）专家的行为。
- en: One of the simplest and most naive ways to perform imitation learning is to
    treat the imitation learning task as a supervised learning task. First, we collect
    a set of expert demonstrations, and then we train a classifier to perform the
    same action performed by the expert in the respective states. We can view this
    as a big multiclass classification problem and train our agent to perform the
    action performed by the expert in the respective states.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 执行模仿学习的一种最简单和最直接的方法是将模仿学习任务视为监督学习任务。首先，我们收集一组专家示范，然后训练一个分类器，在相应的状态下执行专家所执行的相同动作。我们可以将其视为一个大的多类分类问题，并训练我们的智能体在相应状态下执行专家的动作。
- en: Our goal is to minimize the loss ![](img/B15558_15_001.png) where ![](img/B15558_04_122.png)
    is the expert action and ![](img/B15558_15_003.png) denotes the action performed
    by our agent.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是最小化损失 ![](img/B15558_15_001.png)，其中 ![](img/B15558_04_122.png) 是专家的动作，![](img/B15558_15_003.png)
    表示我们智能体执行的动作。
- en: 'Thus, in supervised imitation learning, we perform the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在监督式模仿学习中，我们执行以下步骤：
- en: Collect the set of expert demonstrations
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集一组专家示范
- en: Initialize a policy ![](img/B15558_10_044.png)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个策略 ![](img/B15558_10_044.png)
- en: Learn the policy by minimizing the loss function ![](img/B15558_15_001.png)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过最小化损失函数 ![](img/B15558_15_001.png) 来学习策略
- en: However, there exist several challenges and drawbacks with this method. The
    knowledge of the agent is limited only to the expert demonstrations (training
    data), so if the agent comes across a new state that is not present in the expert
    demonstrations, then the agent will not know what action to perform in that state.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在一些挑战和缺点。智能体的知识仅限于专家的示范（训练数据），因此如果智能体遇到一个在专家示范中不存在的新状态，它将不知道在该状态下该执行什么动作。
- en: Say, we train an agent to drive a car using supervised imitation learning and
    let the agent perform in the real world. If the training data has no state where
    the agent encounters a traffic signal, then our agent will have no clue about
    the traffic signal.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们使用监督模仿学习训练一个智能体驾驶汽车，并让智能体在现实世界中执行任务。如果训练数据中没有智能体遇到交通信号灯的状态，那么我们的智能体将对交通信号灯一无所知。
- en: Also, the accuracy of the agent is highly dependent on the knowledge of the
    expert. If the expert demonstrations are poor or not optimal, then the agent cannot
    learn correct actions or the optimal policy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，智能体的准确性高度依赖于专家的知识。如果专家的示范不佳或不最优，那么智能体就无法学习到正确的动作或最优策略。
- en: To overcome the challenges in supervised imitation learning, we introduce a
    new algorithm called DAgger. In the next section, we will learn how DAgger works
    and how it overcomes the limitations of supervised imitation learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服监督模仿学习中的挑战，我们引入了一种新的算法，叫做 DAgger。在下一节中，我们将学习 DAgger 如何工作以及它如何克服监督模仿学习的局限性。
- en: DAgger
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DAgger
- en: DAggeris one of the most-used imitation learning algorithms. Let's understand
    how DAgger works with an example. Let's revisit our example of training an agent
    to drive a car. First, we initialize an empty dataset ![](img/B15558_09_124.png).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DAgger 是最常用的模仿学习算法之一。让我们通过一个例子来理解 DAgger 是如何工作的。我们重新回顾一下训练一个智能体驾驶汽车的例子。首先，我们初始化一个空数据集
    ![](img/B15558_09_124.png)。
- en: '**In the first iteration**, we start off with some policy ![](img/B15558_03_153.png)
    to drive the car. Thus, we generate a trajectory ![](img/B15558_14_143.png) using
    the policy ![](img/B15558_03_153.png). We know that the trajectory consists of
    a sequence of states and actions—that is, states visited by our policy ![](img/B15558_15_010.png)
    and actions made in those states using our policy ![](img/B15558_15_010.png).
    Now, we create a new dataset ![](img/B15558_15_012.png) by taking only the states
    visited by our policy ![](img/B15558_03_153.png) and we use an expert to provide
    the actions for those states. That is, we take all the states from the trajectory
    and ask the expert to provide actions for those states.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**在第一次迭代中**，我们使用某个策略 ![](img/B15558_03_153.png) 来驾驶汽车。因此，我们利用该策略 ![](img/B15558_03_153.png)
    生成一条轨迹 ![](img/B15558_14_143.png)。我们知道，这条轨迹由一系列的状态和动作组成——即由我们的策略 ![](img/B15558_15_010.png)
    访问的状态以及在这些状态下由我们的策略 ![](img/B15558_15_010.png) 执行的动作。现在，我们通过只取我们的策略 ![](img/B15558_03_153.png)
    访问的状态来创建一个新数据集 ![](img/B15558_15_012.png)，并请专家为这些状态提供相应的动作。也就是说，我们取轨迹中的所有状态，并请专家为这些状态提供动作。'
- en: 'Now, we combine the new dataset ![](img/B15558_15_014.png) with our initialized
    empty dataset ![](img/B15558_12_259.png) and update ![](img/B15558_12_259.png)
    as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将新数据集 ![](img/B15558_15_014.png) 与初始化的空数据集 ![](img/B15558_12_259.png) 结合，并更新
    ![](img/B15558_12_259.png)，如：
- en: '![](img/B15558_15_017.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_017.png)'
- en: Next, we train a classifier on this updated dataset ![](img/B15558_12_259.png)
    and learn a new policy ![](img/B15558_03_159.png).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在这个更新后的数据集上训练一个分类器 ![](img/B15558_12_259.png)，并学习一个新的策略 ![](img/B15558_03_159.png)。
- en: '**In the second iteration**, we use the new policy ![](img/B15558_03_158.png)
    to generate trajectories, create a new dataset ![](img/B15558_15_021.png) by taking
    only the states visited by the new policy ![](img/B15558_03_159.png), and ask
    the expert to provide the actions for those states.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**在第二次迭代中**，我们使用新的策略 ![](img/B15558_03_158.png) 生成轨迹，创建一个新数据集 ![](img/B15558_15_021.png)，只取新的策略
    ![](img/B15558_03_159.png) 访问的状态，并请专家为这些状态提供相应的动作。'
- en: 'Now, we combine the dataset ![](img/B15558_15_023.png) with ![](img/B15558_09_088.png)
    and update ![](img/B15558_12_259.png) as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集 ![](img/B15558_15_023.png) 与 ![](img/B15558_09_088.png) 结合，并更新 ![](img/B15558_12_259.png)，如：
- en: '![](img/B15558_15_026.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_026.png)'
- en: Next, we train a classifier on this updated dataset ![](img/B15558_15_027.png)
    and learn a new policy ![](img/B15558_15_028.png).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在这个更新后的数据集上训练一个分类器 ![](img/B15558_15_027.png)，并学习一个新的策略 ![](img/B15558_15_028.png)。
- en: '**In the third iteration**, we use the new policy ![](img/B15558_04_094.png)
    to generate trajectories and create a new dataset ![](img/B15558_15_030.png) by
    taking only the states visited by the new policy ![](img/B15558_15_031.png), and
    then we ask the expert to provide the actions for those states.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**在第三次迭代中**，我们使用新的策略 ![](img/B15558_04_094.png) 来生成轨迹，并通过只采集由新策略 ![](img/B15558_15_031.png)
    访问的状态，创建一个新的数据集 ![](img/B15558_15_030.png)，然后我们要求专家提供这些状态的动作。'
- en: 'Now, we combine the dataset ![](img/B15558_15_030.png) with ![](img/B15558_09_075.png)
    and update ![](img/B15558_09_075.png) as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集 ![](img/B15558_15_030.png) 与 ![](img/B15558_09_075.png) 合并，并更新 ![](img/B15558_09_075.png)
    为：
- en: '![](img/B15558_15_035.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_035.png)'
- en: Next, we train a classifier on this updated dataset ![](img/B15558_15_036.png)
    and learn a new policy ![](img/B15558_15_037.png). In this way, DAgger works in
    a series of iterations until it finds the optimal policy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在这个更新后的数据集 ![](img/B15558_15_036.png) 上训练一个分类器，并学习一个新的策略 ![](img/B15558_15_037.png)。通过这种方式，DAgger
    通过一系列的迭代工作，直到找到最优策略。
- en: Now that we have a basic understanding of Dagger; let's go into more detail
    and learn how DAgger finds the optimal policy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Dagger 有了基本的理解；接下来，我们将深入探讨，了解 DAgger 如何找到最优策略。
- en: Understanding DAgger
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解 DAgger
- en: Let's suppose we have a human expert, and let's denote the expert policy with
    ![](img/B15558_15_038.png). We initialize an empty dataset ![](img/B15558_09_075.png)
    and also a novice policy ![](img/B15558_15_040.png).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个人类专家，并且用 ![](img/B15558_15_038.png) 表示专家策略。我们初始化一个空的数据集 ![](img/B15558_09_075.png)，并且初始化一个初学者策略
    ![](img/B15558_15_040.png)。
- en: '**Iteration 1**:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 1**：'
- en: 'In the first iteration, we create a new policy ![](img/B15558_15_041.png) as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代中，我们创建一个新的策略 ![](img/B15558_15_041.png)，表示为：
- en: '![](img/B15558_15_042.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_042.png)'
- en: 'The preceding equation implies that we create a new policy ![](img/B15558_04_086.png)
    by taking some amount of expert policy ![](img/B15558_15_044.png) and some amount
    of novice policy ![](img/B15558_15_045.png). How much of the expert policy and
    novice policy we take is decided by the parameter ![](img/B15558_06_030.png).
    The value of ![](img/B15558_15_047.png) is given as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程意味着我们通过结合一定量的专家策略 ![](img/B15558_15_044.png) 和一定量的初学者策略 ![](img/B15558_15_045.png)
    来创建新的策略 ![](img/B15558_04_086.png)。我们取多少专家策略和初学者策略是由参数 ![](img/B15558_06_030.png)
    决定的。![](img/B15558_15_047.png) 的值为：
- en: '![](img/B15558_15_048.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_048.png)'
- en: 'The value of *p* is chosen between 0.1 and 0.9\. Since we are in iteration
    1, substituting *i* = 1, we can write:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* 的值在 0.1 到 0.9 之间选择。由于我们处于第一次迭代，代入 *i* = 1，我们可以写成：'
- en: '![](img/B15558_15_049.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_049.png)'
- en: 'Thus, substituting ![](img/B15558_15_050.png) in equation (1), we can write:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，代入方程 (1) 中的 ![](img/B15558_15_050.png)，我们可以写成：
- en: '![](img/B15558_15_051.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_051.png)'
- en: As we can observe, in the first iteration, the policy ![](img/B15558_15_052.png)
    is just an expert policy ![](img/B15558_15_053.png). Now, we use this policy ![](img/B15558_15_041.png)
    and generate trajectories. Next, we create a new dataset ![](img/B15558_15_055.png)
    by collecting all the states visited by our policy ![](img/B15558_03_153.png)
    and ask the expert to provide actions of those states. So, our dataset will consist
    of ![](img/B15558_15_057.png).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在第一次迭代中，策略 ![](img/B15558_15_052.png) 只是一个专家策略 ![](img/B15558_15_053.png)。现在，我们使用这个策略
    ![](img/B15558_15_041.png) 来生成轨迹。接下来，我们通过收集我们策略 ![](img/B15558_03_153.png) 访问的所有状态，创建一个新的数据集
    ![](img/B15558_15_055.png)，并要求专家提供这些状态的动作。因此，我们的数据集将包括 ![](img/B15558_15_057.png)。
- en: 'Now, we combine the dataset ![](img/B15558_15_058.png) with our initialized
    empty dataset ![](img/B15558_09_124.png) and update ![](img/B15558_12_259.png) as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集 ![](img/B15558_15_058.png) 与初始化的空数据集 ![](img/B15558_09_124.png) 合并，并更新
    ![](img/B15558_12_259.png) 为：
- en: '![](img/B15558_15_017.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_017.png)'
- en: Now that we have an updated dataset ![](img/B15558_12_259.png), we train a classifier
    on this new dataset and extract a new policy. Let the new policy be ![](img/B15558_15_063.png).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了更新后的数据集 ![](img/B15558_12_259.png)，我们在这个新数据集上训练一个分类器并提取一个新的策略。设新策略为 ![](img/B15558_15_063.png)。
- en: '**Iteration 2**:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代 2**：'
- en: 'In the second iteration, we create a new policy ![](img/B15558_15_064.png)
    as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次迭代中，我们创建一个新的策略 ![](img/B15558_15_064.png)，表示为：
- en: '![](img/B15558_15_065.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_065.png)'
- en: 'The preceding equation implies that we create a new policy ![](img/B15558_03_159.png)
    by taking some amount of expert policy ![](img/B15558_15_053.png) and some amount
    of policy ![](img/B15558_15_068.png) that we obtained in the previous iteration.
    We know that the value of beta is chosen as: ![](img/B15558_15_069.png). Thus,
    we have ![](img/B15558_15_070.png).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程意味着我们通过结合一定量的专家策略 ![](img/B15558_15_053.png) 和我们在前一次迭代中获得的策略 ![](img/B15558_15_068.png)
    来创建一个新的策略 ![](img/B15558_03_159.png)。我们知道，beta 的值选取为： ![](img/B15558_15_069.png)。因此，我们得到
    ![](img/B15558_15_070.png)。
- en: Now, we use this policy ![](img/B15558_15_071.png) and generate trajectories.
    Next, we create a new dataset ![](img/B15558_15_072.png) by collecting all the
    states visited by our policy ![](img/B15558_15_073.png) and ask the expert to
    provide actions of those states. So, our dataset will consist of ![](img/B15558_15_074.png).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用这个策略 ![](img/B15558_15_071.png) 并生成轨迹。接下来，我们通过收集我们的策略 ![](img/B15558_15_073.png)
    访问过的所有状态来创建一个新的数据集 ![](img/B15558_15_072.png)，并请求专家为这些状态提供动作。因此，我们的数据集将由 ![](img/B15558_15_074.png)
    组成。
- en: 'Now, we combine the dataset ![](img/B15558_15_075.png) with ![](img/B15558_15_036.png)
    and update ![](img/B15558_12_259.png) as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将数据集 ![](img/B15558_15_075.png) 与 ![](img/B15558_15_036.png) 结合，并更新 ![](img/B15558_12_259.png)
    如下：
- en: '![](img/B15558_15_026.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_026.png)'
- en: Now that we have an updated dataset ![](img/B15558_09_075.png), we train a classifier
    on this new dataset and extract a new policy. Let that new policy be ![](img/B15558_15_080.png).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了更新的数据集 ![](img/B15558_09_075.png)，我们在这个新数据集上训练分类器并提取新的策略。让我们将这个新策略命名为
    ![](img/B15558_15_080.png)。
- en: We repeat these steps for several iterations to obtain the optimal policy. As
    we can observe in each iteration, we aggregate our dataset ![](img/B15558_09_088.png)
    and train a classifier to obtain the new policy. Notice that the value of ![](img/B15558_09_151.png)
    is decaying exponentially. This makes sense as over a series of iterations, our
    policy will become better and so we can reduce the importance of the expert policy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这些步骤进行多次迭代，以获得最优策略。如我们在每次迭代中所观察到的，我们聚合数据集 ![](img/B15558_09_088.png) 并训练分类器以获得新的策略。请注意，值
    ![](img/B15558_09_151.png) 正在指数衰减。这个现象是合理的，因为在多次迭代过程中，我们的策略会变得更好，因此可以减少专家策略的重要性。
- en: Now that we have understood how DAgger works, in the next section, we will look
    into the algorithm of DAgger for a better understanding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 DAgger 的工作原理，在接下来的章节中，我们将进一步研究 DAgger 算法以加深理解。
- en: Algorithm – DAgger
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – DAgger
- en: 'The algorithm of DAgger is given as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: DAgger 算法如下所示：
- en: Initialize an empty dataset ![](img/B15558_12_259.png)
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空数据集 ![](img/B15558_12_259.png)。
- en: Initialize a policy ![](img/B15558_15_045.png)
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略 ![](img/B15558_15_045.png)。
- en: 'For iterations *i* = 1 to *N*:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迭代 *i* = 1 到 *N*：
- en: Create a policy ![](img/B15558_15_085.png).
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建策略 ![](img/B15558_15_085.png)。
- en: Generate a trajectory using the policy ![](img/B15558_15_086.png).
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_15_086.png) 生成轨迹。
- en: Create a dataset ![](img/B15558_15_087.png) by collecting states visited by
    the policy ![](img/B15558_15_086.png) and actions of those states provided by
    the expert ![](img/B15558_15_089.png). Thus, ![](img/B15558_15_090.png).
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过收集策略 ![](img/B15558_15_086.png) 访问过的状态以及专家提供的这些状态的动作，创建一个数据集 ![](img/B15558_15_087.png)。因此，
    ![](img/B15558_15_090.png)。
- en: Aggregate the dataset as ![](img/B15558_15_091.png).
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集聚合为 ![](img/B15558_15_091.png)。
- en: Train a classifier on the updated dataset ![](img/B15558_12_259.png) and extract
    a new policy ![](img/B15558_15_093.png).
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新后的数据集上训练分类器 ![](img/B15558_12_259.png) 并提取新的策略 ![](img/B15558_15_093.png)。
- en: Now that we have learned the DAgger algorithm, in the next section, we will
    learn about DQfD.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 DAgger 算法，在接下来的章节中，我们将学习 DQfD。
- en: Deep Q learning from demonstrations
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来自演示的深度 Q 学习
- en: We learned that in imitation learning, we try to learn from expert demonstrations.
    Can we make use of expert demonstrations in DQN and perform better? Yes! In this section,
    we will learn how to make use of expert demonstrations in DQN using an algorithm
    calledDQfD.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在模仿学习中，我们尝试从专家演示中学习。我们能否在 DQN 中利用专家演示并获得更好的表现呢？答案是肯定的！在这一节中，我们将学习如何使用一种名为
    DQfD 的算法，在 DQN 中利用专家演示。
- en: In the previous chapters, we have learned about several types of DQN. We started
    off with vanilla DQN, and then we explored various improvements to the DQN, such
    as double DQN, dueling DQN, prioritized experience replay, and more. In all these
    methods, the agent tries to learn from scratch by interacting with the environment.
    The agent interacts with the environment and stores their interaction experience
    in a buffer called a replay buffer and learns based on their experience.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了几种类型的DQN。我们从基础的vanilla DQN开始，然后探索了各种对DQN的改进，如双重DQN、对战DQN、优先经验重放等。在所有这些方法中，智能体都试图通过与环境互动从零开始学习。智能体与环境互动，并将他们的互动经验存储在一个叫做重放缓冲区的地方，然后基于这些经验进行学习。
- en: In order for the agent to perform better, it has to gather a lot of experience
    from the environment, add it to the replay buffer, and train itself. However,
    this method costs us a lot of training time. In all the previous methods we have
    learned so far, we have trained our agent in a simulator, so the agent gathers
    experience in the simulator environment to perform better. To learn the optimal
    policy, the agent has to perform a lot of interactions with the environment, and
    some of these interactions give the agent a very bad reward. This is tolerable
    in a simulator environment. But how can we train the agent in a real-world environment?
    We can't train the agent by directly interacting with the real-world environment
    and by making a lot of bad actions in the real-world environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让智能体表现得更好，它必须从环境中获取大量经验，将其添加到重放缓冲区并进行自我训练。然而，这种方法会消耗大量的训练时间。在我们迄今为止学习的所有方法中，我们一直是在模拟器中训练智能体，因此智能体在模拟器环境中积累经验以提高表现。为了学习最优策略，智能体必须与环境进行大量的互动，其中一些互动可能会给智能体带来很差的奖励。在模拟器环境中，这种情况是可以容忍的。但是我们如何在真实环境中训练智能体呢？我们不能通过直接与真实环境互动并在真实环境中进行大量不良操作来训练智能体。
- en: So, in those cases, we can train the agent in a simulator that corresponds to
    the particular real-world environment. But the problem is that it is hard to find
    an accurate simulator corresponding to the real-world environment for most use
    cases. However, we can easily obtain expert demonstrations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这些情况下，我们可以在一个对应于特定真实环境的模拟器中训练智能体。但是问题是，对于大多数用例，很难找到一个准确的模拟器来对应真实环境。不过，我们可以轻松地获得专家示范。
- en: For instance, let's suppose we want to train an agent to play chess. Let's assume
    we don't find an accurate simulator to train the agent to play chess. But we can
    easily obtain good expert demonstrations of an expert playing chess.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们要训练一个智能体来下棋。假设我们找不到一个准确的模拟器来训练智能体下棋。但是我们可以轻松地获得一个专家下棋的良好示范。
- en: Now, can we make use of these expert demonstrations and train our agent? Yes!
    Instead of learning from scratch by interacting with the environment, if we add
    the expert demonstrations directly to the replay buffer and pre-train our agent
    based on these expert demonstrations, then the agent can learn better and faster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们能否利用这些专家示范来训练我们的智能体？可以！与其通过与环境互动从零开始学习，不如直接将专家示范添加到重放缓冲区，并基于这些专家示范预训练智能体，这样智能体可以更好、更快地学习。
- en: This is the fundamental idea behind DQfD. We fill the replay buffer with expert
    demonstrations and pre-train the agent. Note that these expert demonstrations
    are used only for pre-training the agent. Once the agent is pre-trained, the agent
    will interact with the environment and gather more experience and make use of
    it for learning. Thus DQfD consists of two phases, which are pre-training and
    training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是DQfD背后的基本理念。我们将专家示范填充到重放缓冲区中并预训练智能体。请注意，这些专家示范仅用于预训练智能体。一旦智能体完成预训练，它将与环境互动并获取更多经验，利用这些经验进行学习。因此，DQfD包括两个阶段，分别是预训练和训练。
- en: First, we pre-train the agent based on the expert demonstrations, and then we
    train the agent by interacting with the environment. When the agent interacts
    with the environment, it collects some experience, and the agent's experience
    (self-generated data) also gets added to the replay buffer. The agent makes use
    of both the expert demonstrations and also the self-generated data for learning.
    We use a prioritized experience replay buffer and give more priority to the expert
    demonstrations than the self-generated data. Now that we have a basic understanding
    of DQfD, let's go into detail and learn how exactly it works.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们基于专家示范对智能体进行预训练，然后通过与环境的交互来训练智能体。当智能体与环境进行交互时，它收集一些经验，并且智能体的经验（自生成数据）也被加入到回放缓冲区中。智能体利用专家示范和自生成的数据进行学习。我们使用优先经验回放缓冲区，给专家示范数据的优先级高于自生成数据。现在我们对DQfD有了基本的了解，让我们深入探讨并了解它是如何工作的。
- en: Phases of DQfD
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQfD的各个阶段
- en: 'DQfD consists of two phases:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: DQfD由两个阶段组成：
- en: Pre-training phase
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练阶段
- en: Training phase
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练阶段
- en: Pre-training phase
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练阶段
- en: In the pre-training phase, the agent does not interact with the environment.
    We directly add the expert demonstrations to the replay buffer and the agent learns
    by sampling the expert demonstrations from the replay buffer.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练阶段，智能体不会与环境进行交互。我们直接将专家示范添加到回放缓冲区中，智能体通过从回放缓冲区中抽取专家示范进行学习。
- en: The agent learns from expert demonstrations by minimizing the loss *J*(*Q*)
    using gradient descent. However, pre-training with expert demonstrations alone
    is not sufficient for the agent to perform better because the expert demonstrations
    will not contain all possible transitions. But the pretraining with expert demonstrations
    acts as a good starting point to train our agent. Once the agent is pre-trained
    with demonstrations, then during the training phase, the agent will perform better
    actions in the environment from the initial iteration itself instead of performing
    random actions, and so the agent can learn quickly.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体通过最小化损失*J*(*Q*)来学习专家示范，使用的是梯度下降法。然而，仅仅通过专家示范进行预训练并不足以使智能体表现得更好，因为专家示范无法包含所有可能的转移。然而，通过专家示范进行预训练为训练我们的智能体提供了一个良好的起点。一旦智能体通过示范完成了预训练，在训练阶段，智能体将能够从初始迭代开始就执行更好的动作，而不是执行随机动作，因此智能体能够更快地学习。
- en: Training phase
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练阶段
- en: Once the agent is pre-trained, we start the training phase, where the agent
    interacts with the environment and learns based on its experience. Since the agent
    has already learned some useful information from the expert demonstrations in
    the pre-training phase, it will not perform random actions in the environment.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦智能体完成预训练，我们就进入训练阶段，在此阶段智能体与环境进行交互，并根据其经验进行学习。由于智能体已经在预训练阶段从专家示范中学到了一些有用的信息，它在环境中将不再执行随机动作。
- en: During the training phase, the agent interacts with the environment and stores
    its transition information (experience) in the replay buffer. We learned that
    our replay buffer will be pre-filled with the expert demonstrations data. So,
    now, our replay buffer will consist of a mixture of both expert demonstrations
    and the agent's experience (self-generated data). We sample a minibatch of experience
    from the replay buffer and train the agent. Note that here we use a prioritized
    replay buffer, so while sampling, we give more priority to the expert demonstrations
    than the agent-generated data. In this way, we train the agent by sampling experience
    from the replay buffer and minimize the loss using gradient descent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，智能体与环境进行交互，并将其转移信息（经验）存储在回放缓冲区中。我们了解到，回放缓冲区将预先填充专家示范数据。所以，现在我们的回放缓冲区将由专家示范和智能体的经验（自生成数据）混合组成。我们从回放缓冲区中抽取一个小批量经验并训练智能体。注意，这里我们使用的是优先回放缓冲区，所以在抽样时，我们会优先选择专家示范数据而非智能体生成的数据。通过这种方式，我们通过从回放缓冲区抽样经验并使用梯度下降来最小化损失，从而训练智能体。
- en: We learned that the agent interacts with the environment and stores the experience
    in the replay buffer. If the replay buffer is full, then we overwrite the buffer
    with new transition information generated by the agent. However, we won't overwrite
    the expert demonstrations. So, the expert demonstrations will always remain in
    the replay buffer so that the agent can make use of expert demonstrations for
    learning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，智能体与环境进行交互，并将经验存储在重放缓冲区中。如果重放缓冲区已满，则我们会用智能体生成的新转移信息覆盖缓冲区。然而，我们不会覆盖专家示范。因此，专家示范将始终保留在重放缓冲区中，以便智能体可以利用这些专家示范进行学习。
- en: Thus, we have learned how to pre-train and train an agent with expert demonstrations.
    In the next section, we will learn about the loss function of DQfD.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学习了如何使用专家示范对智能体进行预训练和训练。在下一节中，我们将了解 DQfD 的损失函数。
- en: Loss function of DQfD
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQfD 的损失函数
- en: 'The loss function of DQfD comprises the sum of four losses:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DQfD 的损失函数由四个损失的总和组成：
- en: Double DQN loss
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双重 DQN 损失
- en: N-step double DQN loss
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N 步双重 DQN 损失
- en: Supervised classification loss
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督分类损失
- en: L2 loss
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L2 损失
- en: Now, we will look at each of these losses.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将逐个查看这些损失。
- en: '**Double DQN loss** – ![](img/B15558_15_094.png) represents the 1-step double
    DQN loss.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '**双重 DQN 损失** – ![](img/B15558_15_094.png) 代表 1 步双重 DQN 损失。'
- en: '**N-step double DQN loss** – ![](img/B15558_15_095.png) represents the n-step
    double DQN loss.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**N 步双重 DQN 损失** – ![](img/B15558_15_095.png) 代表 n 步双重 DQN 损失。'
- en: '**Supervised classification loss** – ![](img/B15558_15_096.png) represents
    the supervised classification loss. It is expressed as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督分类损失** – ![](img/B15558_15_096.png) 代表监督分类损失。其表达式为：'
- en: '![](img/B15558_15_097.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_097.png)'
- en: 'Where:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*a*[E] is the action taken by the expert.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*[E] 是专家采取的动作。'
- en: '*l*(*a*[E], *a*) is known as the margin function or margin loss. It will be
    0 when the action taken is equal to the expert action *a* = *a*[E]; else, it is
    positive.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l*(*a*[E], *a*) 被称为边际函数或边际损失。当所采取的动作等于专家动作 *a* = *a*[E] 时，它为 0；否则，它为正值。'
- en: '**L2 regularization loss** – ![](img/B15558_15_098.png) represents the L2 regularization
    loss. It prevents the agent from overfitting to the demonstration data.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 正则化损失** – ![](img/B15558_15_098.png) 代表 L2 正则化损失。它可以防止智能体过度拟合示范数据。'
- en: 'Thus, the final loss function will be the sum of all the preceding four losses:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终的损失函数将是前面四个损失的总和：
- en: '![](img/B15558_15_099.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_099.png)'
- en: Where the value of ![](img/B15558_15_100.png) acts as a weighting factor and
    helps us to control the importance of the respective loss.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B15558_15_100.png) 的值作为加权因子，帮助我们控制各个损失的相对重要性。
- en: Now that we have learned how DQfD works, we will look into the algorithm of
    DQfD in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 DQfD 的工作原理，接下来我们将在下一节中查看 DQfD 的算法。
- en: Algorithm – DQfD
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – DQfD
- en: 'The algorithm of DQfD is given as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: DQfD 的算法如下所示：
- en: Initialize the main network parameter ![](img/B15558_10_037.png)
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化主网络参数 ![](img/B15558_10_037.png)
- en: Initialize the target network parameter ![](img/B15558_15_102.png) by copying
    the main network parameter ![](img/B15558_09_123.png)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_123.png) 来初始化目标网络参数 ![](img/B15558_15_102.png)
- en: Initialize the replay buffer ![](img/B15558_12_259.png) with the expert demonstrations
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用专家示范初始化重放缓冲区 ![](img/B15558_12_259.png)
- en: 'Set *d*: the number of time steps we want to delay updating the target network
    parameter'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *d*：我们希望延迟更新目标网络参数的时间步数
- en: '**Pre-training phase**: For steps ![](img/B15558_15_105.png):'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练阶段**：对于步骤 ![](img/B15558_15_105.png)：'
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区中采样一个小批量经验 ![](img/B15558_12_259.png)
- en: Compute the loss *J*(*Q*)
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失 *J*(*Q*)
- en: Update the parameter of the network using gradient descent
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新网络参数
- en: 'If *t* mod *d* = 0:'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0：
- en: Update the target network parameter ![](img/B15558_09_084.png) by copying the
    main network parameter ![](img/B15558_09_118.png)
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_118.png) 来更新目标网络参数 ![](img/B15558_09_084.png)
- en: '**Training phase**: For steps *t* = 1, 2, ..., *T*:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练阶段**：对于步骤 *t* = 1, 2, ..., *T*：'
- en: Select an action
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个动作
- en: Perform the selected action and move to the next state, observe the reward,
    and store this transition information in the replay buffer ![](img/B15558_09_075.png)
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所选动作，进入下一个状态，观察奖励，并将该转移信息存储在重放缓冲区中 ![](img/B15558_09_075.png)
- en: Sample a minibatch of experience from the replay buffer ![](img/B15558_12_259.png)
    with prioritization
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从重放缓冲区中采样一个小批量经验，带有优先级 ![](img/B15558_12_259.png)
- en: Compute the loss *J*(*Q*)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失 *J*(*Q*)
- en: Update the parameter of the network using gradient descent
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新网络参数
- en: 'If *t* mod *d* = 0:'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *t* mod *d* = 0：
- en: Update the target network parameter ![](img/B15558_09_059.png) by copying the
    main network parameter ![](img/B15558_09_118.png)
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过复制主网络参数 ![](img/B15558_09_118.png)，更新目标网络参数 ![](img/B15558_09_059.png)
- en: That's it! In the next section, we will learn about a very interesting concept
    called IRL.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！在接下来的章节中，我们将学习一个非常有趣的概念——逆强化学习（IRL）。
- en: Inverse reinforcement learning
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆强化学习
- en: '**Inverse Reinforcement Learning (IRL)** is one of the most exciting fields
    of reinforcement learning. In reinforcement learning, our goal is to learn the
    optimal policy. That is, our goal is to find the optimal policy that gives the
    maximum return (sum of rewards of the trajectory). In order to find the optimal
    policy, first, we should know the reward function. A reward function tells us
    what reward we obtain by performing an action *a* in the state *s*. Once we have
    the reward function, we can train our agent to learn the optimal policy that gives
    the maximum reward. But the problem is that designing the reward function is not
    that easy for complex tasks.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**逆强化学习（IRL）**是强化学习中最令人兴奋的领域之一。在强化学习中，我们的目标是学习最优策略。也就是说，我们的目标是找到能够提供最大回报（轨迹奖励总和）的最优策略。为了找到最优策略，首先，我们应该知道奖励函数是什么。奖励函数告诉我们，在状态
    *s* 中执行一个动作 *a* 时，我们获得什么奖励。一旦我们拥有奖励函数，就可以训练我们的智能体学习能够提供最大奖励的最优策略。但问题是，对于复杂任务，设计奖励函数并不是一件容易的事。'
- en: Consider designing the reward function for tasks such as an agent learning to
    walk, self-driving cars, and so on. In these cases, designing the reward function
    is not that handy and involves assigning rewards to a variety of agent behaviors.
    For instance, consider designing the reward function for an agent learning to
    drive a car. In this case, we need to assign a reward for every behavior of the
    agent. For example, we can assign a high reward if the agent follows the traffic
    signal, avoids pedestrians, doesn't hit any objects, and so on. But designing
    the reward function in this way is not optimal, and there is also a good chance
    that we might miss out on several behaviors of an agent.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑为一些任务设计奖励函数，比如智能体学习走路、自驾车等。在这些情况下，设计奖励函数并不简单，涉及为各种智能体行为分配奖励。例如，考虑为一个学习驾驶的智能体设计奖励函数。在这种情况下，我们需要为智能体的每个行为分配奖励。例如，如果智能体遵守交通信号、避开行人、没有撞到任何物体，我们可以为这些行为分配较高的奖励。但是以这种方式设计奖励函数并不是最优的，并且很可能会遗漏一些智能体的行为。
- en: Okay, now the question is can we learn the reward function? Yes! If we have
    expert demonstrations, then we can learn the reward function from the expert demonstrations.
    But how can we do that exactly? Here is where IRL helps us. As the name suggests,
    IRL is the inverse of reinforcement learning.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在问题是我们能否学习到奖励函数？当然可以！如果我们有专家演示，那么我们可以从专家演示中学习奖励函数。但是，我们具体该如何做呢？这就是IRL帮助我们的地方。顾名思义，IRL是强化学习的逆过程。
- en: In RL, we try to find the optimal policy given the reward function, but in IRL,
    we try to learn the reward function given the expert demonstrations. Once we have
    derived the reward function from the expert demonstrations using IRL, we can use
    the reward function to train our agent to learn the optimal policy using any reinforcement
    learning algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，我们试图在给定奖励函数的情况下找到最优策略，但在逆强化学习（IRL）中，我们试图根据专家演示学习奖励函数。一旦我们通过IRL从专家演示中推导出奖励函数，就可以使用该奖励函数训练我们的智能体，通过任何强化学习算法学习最优策略。
- en: IRL consists of several interesting algorithms. In the next section, we will
    learn one of the most popular IRL algorithms, called maximum entropy IRL.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: IRL包含几个有趣的算法。在接下来的章节中，我们将学习其中一个最流行的IRL算法——最大熵逆强化学习（maximum entropy IRL）。
- en: Maximum entropy IRL
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大熵逆强化学习
- en: In this section, we will learn how to extract a reward function from the given
    set of expert demonstrations using an IRL algorithm called maximum entropy IRL.
    Before diving into maximum entropy IRL, let's learn some of the important terms
    that are required to understand how maximum entropy IRL works.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将学习如何使用一种名为最大熵逆强化学习（maximum entropy IRL）的IRL算法，从给定的专家演示集中提取奖励函数。在深入了解最大熵逆强化学习之前，让我们先学习一些理解最大熵逆强化学习运作所必需的重要术语。
- en: Key terms
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键术语
- en: '**Feature vector** – We can represent the state by a feature vector *f*. Let''s
    say we have a state *s*; its feature vector can then be defined as *f*[s].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征向量** – 我们可以通过特征向量 *f* 来表示状态。假设我们有一个状态 *s*，那么它的特征向量可以定义为 *f*[s]。'
- en: '**Feature count** –Say we have a trajectory ![](img/B15558_14_164.png); the
    feature count of the trajectory is then defined as the sum of the feature vectors
    of all the states in the trajectory:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征计数** – 假设我们有一条轨迹 ![](img/B15558_14_164.png)；那么该轨迹的特征计数定义为轨迹中所有状态的特征向量之和：'
- en: '![](img/B15558_15_114.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_114.png)'
- en: Where ![](img/B15558_15_115.png) denotes the feature count of the trajectory
    ![](img/B15558_14_164.png).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_15_115.png) 表示轨迹 ![](img/B15558_14_164.png) 的特征计数。
- en: '**Reward function** – The reward function can be defined as the linear combination
    of the features, that is, the sum of feature vectors multiplied by a weight ![](img/B15558_09_118.png):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**奖励函数** – 奖励函数可以定义为特征的线性组合，即特征向量乘以权重的和 ![](img/B15558_09_118.png)：'
- en: '![](img/B15558_15_118.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_118.png)'
- en: Where ![](img/B15558_09_123.png) denotes the weight and *f*[s] denotes the feature
    vector. Note that this ![](img/B15558_09_118.png) is what we are trying to learn.
    When we obtain the optimal ![](img/B15558_15_121.png), then we will have a correct
    reward function. We will learn how we can find the optimal ![](img/B15558_09_106.png)
    in the next section.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_123.png) 表示权重，*f*[s] 表示特征向量。注意，这个 ![](img/B15558_09_118.png)
    就是我们要学习的内容。当我们获得最优的 ![](img/B15558_15_121.png) 时，我们将拥有正确的奖励函数。我们将在下一节学习如何找到最优的
    ![](img/B15558_09_106.png)。
- en: 'We can represent the preceding equation in sigma notation as:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 Sigma 表示法将前面的方程式表示为：
- en: '![](img/B15558_15_123.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_123.png)'
- en: 'We know that the feature count is the sum of feature vectors of all the states
    in the trajectory, so from (2), we can rewrite the preceding equation as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道特征计数是轨迹中所有状态的特征向量之和，因此根据（2），我们可以将前面的方程式重写为：
- en: '![](img/B15558_15_124.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_124.png)'
- en: Thus, the reward of the trajectory is just the weight multiplied by the feature
    count of the trajectory.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，轨迹的奖励就是权重与轨迹的特征计数相乘。
- en: Back to maximum entropy IRL
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回到最大熵逆向强化学习
- en: Now, let's learn how maximum entropy IRL works. Consider that we have expert
    demonstrations ![](img/B15558_15_125.png). Our goal is to learn the reward function
    from the given expert demonstrations. How can we do that?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解最大熵逆向强化学习（IRL）是如何工作的。假设我们有专家演示 ![](img/B15558_15_125.png)。我们的目标是从给定的专家演示中学习奖励函数。我们该如何做呢？
- en: We have already learned that the reward function is given as ![](img/B15558_15_126.png).
    Finding the optimal parameter ![](img/B15558_09_098.png) helps us to learn the
    correct reward function. So, we will sample a trajectory ![](img/B15558_14_140.png)
    from the expert demonstrations ![](img/B15558_15_036.png) and try to find the
    reward function by finding the optimal parameter ![](img/B15558_09_106.png).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到奖励函数是 ![](img/B15558_15_126.png)。找到最优参数 ![](img/B15558_09_098.png) 帮助我们学习正确的奖励函数。所以，我们将从专家演示中采样一条轨迹
    ![](img/B15558_14_140.png) 并通过找到最优参数 ![](img/B15558_09_106.png) 来学习奖励函数。
- en: 'The probability of a trajectory being sampled from the expert demonstrations
    is directly proportional to the exponential of the reward function. That is, trajectories
    that obtain more rewards are more likely to be sampled from our demonstrations
    than those trajectories that obtain less rewards:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从专家演示中采样轨迹的概率与奖励函数的指数成正比。也就是说，获得更多奖励的轨迹比获得更少奖励的轨迹更有可能从我们的演示中被采样：
- en: '![](img/B15558_15_131.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_131.png)'
- en: 'The probability should be in the range of 0 to 1, right? But the value of ![](img/B15558_15_132.png)
    will not be in the range of 0 to 1\. So, in order to normalize that, we introduce
    *z*, which acts as the normalization constant and is given as ![](img/B15558_15_133.png).
    We can rewrite our preceding equation with *z* as:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 概率应该在0到1之间，对吗？但是 ![](img/B15558_15_132.png) 的值不会在0到1之间。所以，为了归一化这个值，我们引入了 *z*，它作为归一化常数，并定义为
    ![](img/B15558_15_133.png)。我们可以用 *z* 重写前面的方程式：
- en: '![](img/B15558_15_134.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_134.png)'
- en: 'Now, our objective is to maximize ![](img/B15558_15_135.png), that is, to maximize
    the log probability of selecting trajectories that give more rewards. So, we can
    define our objective function as:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的目标是最大化 ![](img/B15558_15_135.png)，即最大化选择那些获得更多奖励的轨迹的对数概率。因此，我们可以定义我们的目标函数为：
- en: '![](img/B15558_15_136.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_136.png)'
- en: Where *M* denotes the number of demonstrations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *M* 表示演示次数。
- en: 'Substituting (3) in (4), we can write:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (3) 代入 (4)，我们可以写成：
- en: '![](img/B15558_15_137.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_137.png)'
- en: 'Based on the log rule, ![](img/B15558_15_138.png), we can write:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对数规则，![](img/B15558_15_138.png)，我们可以写成：
- en: '![](img/B15558_15_139.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_139.png)'
- en: 'The logarithmic and exponential terms cancel each other out, so the preceding
    equation becomes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对数项和指数项相互抵消，所以前面的方程变为：
- en: '![](img/B15558_15_140.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_140.png)'
- en: 'We know that ![](img/B15558_15_133.png); substituting the value of *z*, we
    can rewrite the preceding equation as:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 ![](img/B15558_15_133.png)；将 *z* 的值代入后，我们可以重写前面的方程为：
- en: '![](img/B15558_15_142.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_142.png)'
- en: 'We know that ![](img/B15558_15_126.png); substituting the value of ![](img/B15558_15_144.png),
    our final simplified objective function is given as:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 ![](img/B15558_15_126.png)；将 ![](img/B15558_15_144.png) 的值代入后，最终简化后的目标函数为：
- en: '![](img/B15558_15_145.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_145.png)'
- en: To find the optimal parameter ![](img/B15558_09_087.png), we compute the gradient
    of the preceding objective function ![](img/B15558_15_147.png) and update the
    value of ![](img/B15558_09_106.png) as ![](img/B15558_15_149.png). In the next
    section, we will learn how to compute the gradient ![](img/B15558_09_043.png).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最优参数 ![](img/B15558_09_087.png)，我们计算前述目标函数 ![](img/B15558_15_147.png) 的梯度，并将
    ![](img/B15558_09_106.png) 的值更新为 ![](img/B15558_15_149.png)。在下一部分，我们将学习如何计算梯度
    ![](img/B15558_09_043.png)。
- en: Computing the gradient
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算梯度
- en: 'We learned that our objective function is given as:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到我们的目标函数为：
- en: '![](img/B15558_15_151.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_151.png)'
- en: 'Now, we compute the gradient of the objective function with respect to ![](img/B15558_15_152.png).
    After computation, our gradient is given as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算关于 ![](img/B15558_15_152.png) 的目标函数梯度。经过计算，我们得到的梯度为：
- en: '![](img/B15558_15_153.png)![](img/B15558_15_154.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_153.png)![](img/B15558_15_154.png)'
- en: 'The average of the feature count is just the feature expectation ![](img/B15558_15_155.png),
    so we can substitute ![](img/B15558_15_156.png) and rewrite the preceding equation
    as:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 特征计数的平均值即为特征期望 ![](img/B15558_15_155.png)，因此我们可以代入 ![](img/B15558_15_156.png)
    并将前面的方程重写为：
- en: '![](img/B15558_15_157.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_157.png)'
- en: 'We can rewrite the preceding equation by combining all the states of the trajectories
    as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将轨迹的所有状态结合起来，重写前面的方程：
- en: '![](img/B15558_15_158.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_158.png)'
- en: Thus, using the preceding equation, we compute gradients and update the parameter
    ![](img/B15558_13_233.png). If you look at the preceding equation, we can easily
    compute the first term, which is just the feature expectation ![](img/B15558_15_160.png),
    but what about the ![](img/B15558_15_161.png) in the second term? ![](img/B15558_15_162.png)
    is called the state visitation frequency and it represents the probability of
    being in a given state. Okay, how can we compute ![](img/B15558_15_163.png)?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用前面的方程，我们计算梯度并更新参数 ![](img/B15558_13_233.png)。如果你看前面的方程，我们可以轻松计算出第一个项，它就是特征期望
    ![](img/B15558_15_160.png)，但是第二项中的 ![](img/B15558_15_161.png) 怎么办？![](img/B15558_15_162.png)
    被称为状态访问频率，它表示处于给定状态的概率。那么，我们如何计算 ![](img/B15558_15_163.png) 呢？
- en: If we have a policy ![](img/B15558_03_082.png), then we can use the policy to
    compute the state visitation frequency. But we don't have any policy yet. So,
    we can use a dynamic programming method, say, value iteration, to compute the
    policy. However, in order to compute the policy using the value iteration method,
    we require a reward function. So, we just feed our reward function ![](img/B15558_15_165.png)
    and extract the policy using the value iteration. Then, using the extracted policy,
    we compute the state visitation frequency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个策略 ![](img/B15558_03_082.png)，那么我们可以使用该策略计算状态访问频率。但我们还没有任何策略。因此，我们可以使用动态规划方法，比如值迭代，来计算策略。然而，为了通过值迭代方法计算策略，我们需要一个奖励函数。因此，我们只需输入我们的奖励函数
    ![](img/B15558_15_165.png)，然后通过值迭代提取策略。接着，使用提取的策略计算状态访问频率。
- en: 'The steps involved in computing the state visitation frequency using the policy
    ![](img/B15558_03_008.png) are as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用策略 ![](img/B15558_03_008.png) 计算状态访问频率的步骤如下：
- en: 'Let the probability of visiting a state *s* at a time *t* be ![](img/B15558_15_167.png).
    We can write the probability of visiting the initial state *s*[1] at a first time
    step *t* = 1 as: ![](img/B15558_15_168.png)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设在时间 *t* 访问状态 *s* 的概率为 ![](img/B15558_15_167.png)。我们可以写出在第一次时间步 *t* = 1 访问初始状态
    *s*[1] 的概率为： ![](img/B15558_15_168.png)
- en: 'Then for time steps *t* = 1 to *T*:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后对于时间步 *t* = 1 到 *T*：
- en: Compute ![](img/B15558_15_169.png)
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算 ![](img/B15558_15_169.png)
- en: Compute the state visitation frequency as ![](img/B15558_15_170.png).
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算状态访问频率为 ![](img/B15558_15_170.png)。
- en: To get a clear understanding of how maximum entropy IRL works, let's look into
    the algorithm in the next section.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚理解最大熵 IRL 的工作原理，让我们在下一节中深入研究该算法。
- en: Algorithm – maximum entropy IRL
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – 最大熵 IRL
- en: 'The algorithm of maximum entropy IRL is given as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵 IRL 算法如下所示：
- en: Initialize the parameter ![](img/B15558_09_118.png) and gather the demonstrations
    ![](img/B15558_15_125.png)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化参数 ![](img/B15558_09_118.png) 并收集演示 ![](img/B15558_15_125.png)
- en: 'For *N* number of iterations:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 次迭代：
- en: Compute the reward function ![](img/B15558_15_126.png)
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算奖励函数 ![](img/B15558_15_126.png)
- en: Compute the policy using value iteration with the reward function obtained in
    the previous step
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一阶段获得的奖励函数通过价值迭代计算策略
- en: Compute state visitation frequency ![](img/B15558_15_174.png) using the policy
    obtained in the previous step
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一阶段获得的策略计算状态访问频率 ![](img/B15558_15_174.png)
- en: Compute the gradient with respect to ![](img/B15558_14_004.png), that is, ![](img/B15558_15_176.png)
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于 ![](img/B15558_14_004.png) 的梯度，即 ![](img/B15558_15_176.png)
- en: Update the value of ![](img/B15558_09_054.png) as ![](img/B15558_13_286.png)
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_09_054.png) 的值为 ![](img/B15558_13_286.png)
- en: Thus, over a series of iterations, we will find an optimal parameter ![](img/B15558_09_054.png).
    Once we have ![](img/B15558_09_087.png), we can use it to define the correct reward
    function ![](img/B15558_15_181.png). In the next section, we will learn about
    GAIL.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，经过一系列迭代，我们将找到一个最优的参数 ![](img/B15558_09_054.png)。一旦得到 ![](img/B15558_09_087.png)，我们就可以使用它来定义正确的奖励函数
    ![](img/B15558_15_181.png)。在下一节中，我们将学习 GAIL。
- en: Generative adversarial imitation learning
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗模仿学习
- en: '**Generative Adversarial Imitation Learning (GAIL)** is another very popular
    IRL algorithm. As the name suggests, it is based on **Generative Adversarial Networks
    (GANs)**, which we learned about in *Chapter 7*, *Deep Learning Foundations*.
    To understand how GAIL works, we should first recap how GANs work.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗模仿学习 (GAIL)** 是另一个非常流行的 IRL 算法。顾名思义，它基于 **生成对抗网络 (GANs)**，这是我们在 *第 7
    章*《*深度学习基础*》中学习过的内容。要理解 GAIL 的工作原理，我们首先应该回顾 GAN 的工作原理。'
- en: 'In a GAN, we have two networks: one is the generator and the other is the discriminator.
    The role of the generator is to generate new data points by learning the distribution
    of the input dataset. The role of the discriminator is to classify whether a given
    data point is generated by the generator (learned distribution) or whether it
    is from the real data distribution.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 中，我们有两个网络：一个是生成器，另一个是判别器。生成器的作用是通过学习输入数据集的分布生成新的数据点。判别器的作用是分类给定的数据点是由生成器（学习的分布）生成的，还是来自真实的数据分布。
- en: Minimizing the loss function of a GAN implies minimizing the **Jensen Shannon
    (JS)** divergence between the real data distribution and the fake data distribution
    (learned distribution). The JS divergence is used to measure how two probability
    distributions differ from each other. Thus, when the JS divergence between the
    real and fake distributions is zero, it means that the real and fake data distributions
    are equal, that is, our generator network has successfully learned the real distribution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化 GAN 的损失函数意味着最小化 **Jensen Shannon (JS)** 散度，JS 散度用于度量两个概率分布之间的差异。因此，当真实数据分布和假数据分布（学习的分布）之间的
    JS 散度为零时，表示真实和假数据分布是相等的，即我们的生成器网络成功地学习到了真实分布。
- en: Now, let's learn how to make use of GANs in an IRL setting. First, let's introduce
    a new term called **occupancy measure**.It is defined as the distribution of the
    states and actions that our agent comes across while exploring the environment
    with some policy ![](img/B15558_03_008.png). In simple terms, it is basically
    the distribution of state-action pairs following a policy ![](img/B15558_03_084.png).
    The occupancy measure of a policy ![](img/B15558_03_008.png) is denoted by ![](img/B15558_15_185.png).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何在 IRL 设置中使用 GAN。首先，我们引入一个新的术语叫做 **占据度量**。它定义为我们的智能体在使用某个策略 ![](img/B15558_03_008.png)
    探索环境时，遇到的状态和动作的分布。简单来说，它基本上是按照策略 ![](img/B15558_03_084.png) 跟随的状态-动作对的分布。策略 ![](img/B15558_03_008.png)
    的占据度量用 ![](img/B15558_15_185.png) 表示。
- en: In the imitation learning setting, we have an expert policy, and let's denote
    the expert policy by ![](img/B15558_15_186.png). Similarly, let's denote the agent's
    policy by ![](img/B15558_10_111.png). Now, our goal is to make our agent learn
    the expert policy. How can we do that? If we make the occupancy measure of the
    expert policy and the agent policy equal, then it implies that our agent has successfully
    learned the expert policy. That is, the occupancy measure is the distribution
    of the state-action pairs following a policy. If we can make the distribution
    of state-action pairs of the agent's policy equal to the distribution of state-action
    pairs of the expert's policy, then it means that our agent has learned the expert
    policy. Let's explore how we can do this using GANs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在模仿学习设置中，我们有一个专家策略，记作 ![](img/B15558_15_186.png)。同样，记智能体的策略为 ![](img/B15558_10_111.png)。现在，我们的目标是让智能体学习专家策略。我们怎么做呢？如果我们使专家策略和智能体策略的占用度量相等，那么就意味着智能体已经成功学习了专家策略。也就是说，占用度量是遵循某个策略的状态-动作对的分布。如果我们能使智能体策略的状态-动作对分布等于专家策略的状态-动作对分布，那么就意味着我们的智能体学到了专家策略。接下来，我们将探讨如何使用GAN实现这一目标。
- en: We can perceive the occupancy measure of the expert policy as the real data
    distribution and the occupancy measure of the agent policy as the fake data distribution.
    Thus, minimizing the JS divergence between the occupancy measure of the expert
    policy ![](img/B15558_15_188.png) and the occupancy measure of the agent policy
    ![](img/B15558_15_189.png) implies that the agent will learn the expert policy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将专家策略的占用度量看作真实数据分布，将智能体策略的占用度量看作伪数据分布。因此，最小化专家策略 ![](img/B15558_15_188.png)
    和智能体策略 ![](img/B15558_15_189.png) 的占用度量之间的JS散度，就意味着智能体会学习专家策略。
- en: With GANs, we know that the role of the generator is to generate new data points
    by learning the distribution of a given dataset. Similarly, in GAIL, the role
    of the generator is to generate a new policy by learning the distribution (occupancy
    measure) of the expert policy. The role of the discriminator is to classify whether
    the given policy is the expert policy or the agent policy.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN中，我们知道生成器的作用是通过学习给定数据集的分布来生成新的数据点。同样，在GAIL中，生成器的作用是通过学习专家策略的分布（占用度量）来生成新的策略。判别器的作用是分类给定的策略是专家策略还是智能体策略。
- en: With GANs, we know that, for a generator, the optimal discriminator is the one
    that is not able to distinguish between the real and fake data distributions;
    similarly, in GAIL, the optimal discriminator is the one that is unable to distinguish
    whether the generated state-action pair is from the agent policy or the expert
    policy.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在GAN中，我们知道，对于生成器来说，最优的判别器是无法区分真实数据和伪数据分布的；同样，在GAIL中，最优的判别器是无法区分生成的状态-动作对是来自智能体策略还是来自专家策略。
- en: 'To make ourselves clear, let''s understand the terms we use in GAIL by relating
    to GAN terminology:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解，我们通过将GAIL中的术语与GAN术语进行对比，来了解我们在GAIL中使用的术语：
- en: '**Real data distribution** – Occupancy measure of the expert policy'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实数据分布** – 专家策略的占用度量'
- en: '**Fake data distribution** – Occupancy measure of the agent policy'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪数据分布** – 智能体策略的占用度量'
- en: '**Real data** – State-action pair generated by the expert policy'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实数据** – 专家策略生成的状态-动作对'
- en: '**Fake data** – State-action pair generated by the agent policy'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伪数据** – 智能体策略生成的状态-动作对'
- en: 'In a nutshell, we use the generator to generate the state-action pair in a
    way that the discriminator is not able to distinguish whether the state-action
    pair is generated using the expert policy or the agent policy. Both the generator
    and discriminator are neural networks. We train the generator to generate a policy
    similar to the expert policy using TRPO. The discriminator is a classifier, and
    it is optimized using Adam. Thus, we can define the objective function of GAIL
    as:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们使用生成器生成状态-动作对，目的是让判别器无法分辨该状态-动作对是通过专家策略还是通过智能体策略生成的。生成器和判别器都是神经网络。我们通过TRPO训练生成器，生成一个与专家策略相似的策略。判别器是一个分类器，并使用Adam优化。因此，我们可以定义GAIL的目标函数如下：
- en: '![](img/B15558_15_190.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_190.png)'
- en: Where ![](img/B15558_09_054.png) is the parameter of the generator and ![](img/B15558_15_192.png)
    is the parameter of the discriminator.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_054.png) 是生成器的参数，![](img/B15558_15_192.png) 是判别器的参数。
- en: Now that we have an understanding of how GAIL works, let's get into more detail
    and learn how the preceding equation is derived.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了GAIL是如何工作的，接下来让我们更详细地学习如何推导前面的方程。
- en: Formulation of GAIL
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAIL的公式化
- en: 'In this section, we explore the math of GAIL and see how it works. You can
    skip this section if you are not interested in math. We know that in reinforcement
    learning, our objective is to find the optimal policy that gives the maximum reward.
    It can be expressed as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了GAIL的数学原理，并看它是如何工作的。如果你对数学不感兴趣，可以跳过这一部分。我们知道，在强化学习中，我们的目标是找到一个给出最大奖励的最优策略。它可以表示为：
- en: '![](img/B15558_15_193.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_193.png)'
- en: 'We can rewrite our objective function by adding the entropy of a policy as
    shown here:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加策略的熵来重新定义我们的目标函数，如下所示：
- en: '![](img/B15558_15_194.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_194.png)'
- en: The preceding equation tells us that we can maximize the entropy of the policy
    while also maximizing the reward. Instead of defining the objective function in
    terms of the reward, we can also define the objective function in terms of cost.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程告诉我们，我们可以在最大化奖励的同时，也最大化策略的熵。我们可以将目标函数定义为成本，而不是奖励。
- en: 'That is, we can define our RL objective function in terms of cost, as our objective
    is to find an optimal policy that minimizes the cost; this can be expressed as:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们可以将我们的RL目标函数定义为成本，因为我们的目标是找到一个最小化成本的最优策略；这可以表示为：
- en: '![](img/B15558_15_195.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_195.png)'
- en: Where *c* is the cost. Thus, given the cost function, our goal is to find the
    optimal policy that minimizes the cost.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*c*是成本。因此，给定成本函数，我们的目标是找到最小化成本的最优策略。
- en: 'Now, let''s talk about IRL. We learned that in IRL, our objective is to find
    the reward function from the given set of expert demonstrations. We can also define
    the objective of IRL in terms of cost instead of reward. That is, we can define
    our IRL objective function in terms of cost, as our objective is to find the cost
    function under which the expert demonstration is optimal. The objective can be
    expressed using maximum causal entropy IRL as:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们谈谈IRL。我们了解到，在IRL中，我们的目标是从给定的专家示范集中找出奖励函数。我们也可以将IRL的目标定义为成本，而不是奖励。也就是说，我们可以将IRL的目标函数定义为成本，因为我们的目标是找到一个使专家示范最优的成本函数。该目标可以通过最大因果熵IRL表示为：
- en: '![](img/B15558_15_196.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_196.png)'
- en: What does the preceding equation imply? In the IRL setting, our goal is to learn
    the cost function given the expert demonstrations (expert policy). We know that
    the expert policy performs better than the other policy, so we try to learn the
    cost function *c*, which assigns low cost to the expert policy and high cost to
    other policies. Thus, the preceding objective function implies that we try to
    find a cost function that assigns low cost to the expert policy and high cost
    to other policies.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程意味着什么？在IRL设置中，我们的目标是根据专家示范（专家策略）学习成本函数。我们知道专家策略的表现优于其他策略，因此我们尝试学习成本函数*c*，该函数将低成本分配给专家策略，并将高成本分配给其他策略。因此，前面的目标函数意味着我们尝试找到一个成本函数，该函数将低成本分配给专家策略，并将高成本分配给其他策略。
- en: 'To reduce overfitting, we introduce the regularizer ![](img/B15558_15_197.png)
    and rewrite the preceding equation as:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合，我们引入正则化项！[](img/B15558_15_197.png)，并将前面的方程改写为：
- en: '![](img/B15558_15_198.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_198.png)'
- en: From equation (6), we learned that in a reinforcement learning setting, given
    a cost, we obtain the optimal policy, and from (7), we learned that in an IRL
    setting, given an expert policy (expert demonstration), we obtain the cost. Thus,
    from (6) and (7), we can observe that the output of IRL can be sent as an input
    to the RL. That is, IRL results in the cost function and we can use this cost
    function as an input in RL to learn the optimal policy. Thus, we can write ![](img/B15558_15_199.png),
    which implies that the result of IRL is fed as an input to the RL.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程（6）我们了解到，在强化学习设置中，给定一个成本，我们可以获得最优策略；而从（7）我们了解到，在逆向强化学习（IRL）设置中，给定专家策略（专家示范），我们可以获得成本。因此，从（6）和（7）可以观察到，IRL的输出可以作为RL的输入。也就是说，IRL的结果是成本函数，我们可以将该成本函数作为输入，在RL中学习最优策略。因此，我们可以写出！[](img/B15558_15_199.png)，这意味着IRL的结果作为输入传递给RL。
- en: 'We can express this in a functional composition form as ![](img/B15558_15_200.png):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其表示为函数组合形式：![](img/B15558_15_200.png)：
- en: '![](img/B15558_15_201.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_201.png)'
- en: 'In equation (8), the following applies:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程（8）中，适用以下内容：
- en: '![](img/B15558_15_202.png) is the convex conjugate of the regularizer ![](img/B15558_15_203.png)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_15_202.png) 是正则化器 ![](img/B15558_15_203.png) 的凸共轭。'
- en: '![](img/B15558_15_204.png) is the occupancy measure of the agent policy'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_15_204.png) 是智能体策略的占用度量。'
- en: '![](img/B15558_15_205.png) is the occupancy measure of the expert policy'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_15_205.png) 是专家策略的占用度量。'
- en: 'To learn how exactly the equation (8) is derived, you can refer to the GAIL
    paper given in the *Further reading* section at the end of the chapter. The objective
    function (equation (8)) implies that we try to find the optimal policy whose occupancy
    measure is close to the occupancy measure of the expert policy. The occupancy
    measure between the agent policy and the expert policy is measured by ![](img/B15558_15_206.png).
    There are several choices for the regularizer ![](img/B15558_15_207.png). We use
    a generative adversarial regularizer ![](img/B15558_15_208.png) and write our
    equation as:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解公式 (8) 是如何推导出来的，可以参考本章末尾的*进一步阅读*部分中的 GAIL 论文。目标函数（公式 (8)）意味着我们试图找到一个最优策略，其占用度量接近专家策略的占用度量。智能体策略和专家策略之间的占用度量通过
    ![](img/B15558_15_206.png) 进行度量。对于正则化器 ![](img/B15558_15_207.png)，有几种选择。我们使用生成对抗正则化器
    ![](img/B15558_15_208.png)，并将我们的方程写为：
- en: '![](img/B15558_15_209.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_209.png)'
- en: 'Thus, minimizing ![](img/B15558_15_210.png) basically implies that we minimize
    the JS divergence between the occupancy measure of the agent policy ![](img/B15558_15_211.png)
    and the expert policy ![](img/B15558_15_212.png). Thus, we can rewrite the RHS
    of the equation (9) as:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最小化 ![](img/B15558_15_210.png) 基本上意味着我们最小化智能体策略 ![](img/B15558_15_211.png)
    和专家策略 ![](img/B15558_15_212.png) 之间的 JS 散度。因此，我们可以将方程 (9) 的右侧重写为：
- en: '![](img/B15558_15_213.png)![](img/B15558_15_214.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_213.png)![](img/B15558_15_214.png)'
- en: 'Where ![](img/B15558_12_326.png) is just the policy regularizer. We know that
    the JS divergence between the occupancy measure of the agent policy ![](img/B15558_15_185.png)
    and the expert policy ![](img/B15558_15_217.png) is minimized using the GAN, so
    we can just replace ![](img/B15558_15_218.png) in the preceding equation with
    the GAN objective function as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_12_326.png) 只是策略正则化器。我们知道，通过 GAN 最小化智能体策略 ![](img/B15558_15_185.png)
    和专家策略 ![](img/B15558_15_217.png) 之间的 JS 散度，因此我们可以将前面的方程中的 ![](img/B15558_15_218.png)
    替换为 GAN 目标函数，如下所示：
- en: '![](img/B15558_15_219.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_219.png)'
- en: Where ![](img/B15558_09_087.png) is the parameter of the generator and ![](img/B15558_15_221.png)
    is the parameter of the discriminator.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_09_087.png) 是生成器的参数，![](img/B15558_15_221.png) 是判别器的参数。
- en: 'Thus, our final objective function of GAIL becomes:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的 GAIL 最终目标函数变为：
- en: '![](img/B15558_15_222.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_15_222.png)'
- en: The objective equation implies that we can find the optimal policy by minimizing
    the occupancy measure of the expert policy and the agent policy, and we minimize
    that using GANs.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 目标方程意味着我们可以通过最小化专家策略和智能体策略的占用度量来找到最优策略，我们通过使用 GAN 来最小化它。
- en: The role of the generator is to generate a policy by learning the occupancy
    measure of the expert policy, and the role of the discriminator is to classify
    whether the generated policy is from the expert policy or the agent policy. So,
    we train the generator using TRPO and the discriminator is basically a neural
    network that tells us whether the policy generated by the generator is the expert
    policy or the agent policy.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器的作用是通过学习专家策略的占用度量来生成一个策略，而判别器的作用是判断生成的策略是来自专家策略还是智能体策略。因此，我们使用 TRPO 训练生成器，而判别器基本上是一个神经网络，用来告诉我们生成的策略是专家策略还是智能体策略。
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding what imitation learning is and how supervised
    imitation learning works. Next, we learned about the DAgger algorithm, where we
    aggregate the dataset obtained over a series of iterations and learn the optimal
    policy.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过了解模仿学习是什么以及监督模仿学习如何运作来开始本章内容。接下来，我们学习了 DAgger 算法，在该算法中，我们将通过一系列迭代获得的数据集进行聚合，并学习最优策略。
- en: After looking at DAgger, we learned about DQfD, where we prefill the replay
    buffer with expert demonstrations and pre-train the agent with expert demonstrations
    before the training phase.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在看完 DAgger 后，我们学习了 DQfD，其中我们通过专家演示预填充回放缓冲区，并在训练阶段之前用专家演示预训练智能体。
- en: Moving on, we learned about IRL. We understood that in reinforcement learning,
    we try to find the optimal policy given the reward function, but in IRL, we try
    to learn the reward function given the expert demonstrations. When we have derived
    the reward function from the expert demonstrations using IRL, we can use the reward
    function to train our agent to learn the optimal policy using any reinforcement
    learning algorithm. We then explored how to learn the reward function using the
    maximum entropy IRL algorithm.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了逆向强化学习（IRL）。我们理解到，在强化学习中，我们尝试根据奖励函数寻找最优策略，而在IRL中，我们则是根据专家示范来学习奖励函数。当我们通过IRL从专家示范中推导出奖励函数时，就可以使用该奖励函数来训练我们的智能体，借助任何强化学习算法来学习最优策略。然后，我们探索了如何使用最大熵IRL算法来学习奖励函数。
- en: At the end of the chapter, we learned about GAIL, where we used GANs to learn
    the optimal policy. In the next chapter, we will explore a reinforcement learning
    library called Stable Baselines.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们了解了GAIL，其中我们使用生成对抗网络（GAN）来学习最优策略。在下一章，我们将探索一个名为Stable Baselines的强化学习库。
- en: Questions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s assess our understanding of imitation learning and IRL. Try answering
    the following questions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估一下我们对模仿学习和逆向强化学习（IRL）的理解。尝试回答以下问题：
- en: How does supervised imitation learning work?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监督式模仿学习是如何工作的？
- en: How does DAgger differ from supervised imitation learning?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAgger与监督式模仿学习有何不同？
- en: Explain the different phases of DQfD.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释DQfD的不同阶段。
- en: Why do we need IRL?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为什么需要逆向强化学习（IRL）？
- en: What is a feature vector?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是特征向量？
- en: How does GAIL work?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GAIL是如何工作的？
- en: Further reading
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, refer to the following papers:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参考以下论文：
- en: '**A Reduction of Imitation Learning and Structured Prediction to No-Regret
    Online Learning** by *Stephane Ross*, *Geoffrey J. Gordon*, *J. Andrew Bagnell*,
    [https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将模仿学习和结构化预测归结为无悔在线学习的简化** 由 *Stephane Ross*、*Geoffrey J. Gordon*、*J. Andrew
    Bagnell* 撰写，[https://arxiv.org/pdf/1011.0686.pdf](https://arxiv.org/pdf/1011.0686.pdf)'
- en: '**Deep Q-learning from Demonstrations** by *Todd Hester*, *et al*., [https://arxiv.org/pdf/1704.03732.pdf](https://arxiv.org/pdf/1704.03732.pdf)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度Q学习与示范** 由 *Todd Hester*、*等人* 撰写，[https://arxiv.org/pdf/1704.03732.pdf](https://arxiv.org/pdf/1704.03732.pdf)'
- en: '**Maximum Entropy Inverse Reinforcement Learning** by *Brian D. Ziebart*, *Andrew
    Maas*, *J.Andrew Bagnell*, *Anind K. Dey*, [https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf](https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大熵逆向强化学习** 由 *Brian D. Ziebart*、*Andrew Maas*、*J.Andrew Bagnell*、*Anind
    K. Dey* 撰写，[https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf](https://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf)'
- en: '**Generative Adversarial Imitation Learning** by *Jonathan Ho*, *Stefano Ermon*,
    [https://arxiv.org/pdf/1606.03476.pdf](https://arxiv.org/pdf/1606.03476.pdf)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗模仿学习** 由 *Jonathan Ho*、*Stefano Ermon* 撰写，[https://arxiv.org/pdf/1606.03476.pdf](https://arxiv.org/pdf/1606.03476.pdf)'
