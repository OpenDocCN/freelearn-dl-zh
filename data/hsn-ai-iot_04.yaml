- en: Deep Learning for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的深度学习
- en: 'In the last chapter, we learned about different **machine learning** (**ML**)
    algorithms. The focus of this chapter is neural networks based on multiple layered
    models, also known as deep learning models. They have become a buzzword in the
    last few years and an absolute favorite of investors in the field of artificial-intelligence-based
    startups. Achieving above human level accuracy in the task of object detection
    and defeating the world''s Dan Nine Go master are some of the feats possible by
    **deep** **learning** (**DL**). In this chapter and a few subsequent chapters,
    we will learn about the different DL models and how to use DL on our IoT generated
    data. In this chapter, we will start with a glimpse into the journey of DL, and
    learn about four popular models, the **multilayered perceptron** (**MLP**), the
    **convolutional neural network** (**CNN**), **recurrent neural network** (**RNN**),
    and autoencoders. Specifically, you will learn about the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了不同的**机器学习**（**ML**）算法。本章的重点是基于多层模型的神经网络，也被称为深度学习模型。在过去几年中，它们成为了一个热门词汇，并成为了人工智能初创企业投资者的绝对最爱。在目标检测任务中实现超越人类水平的准确率，并击败世界顶尖的九段围棋大师，是深度学习可能实现的一些壮举。在本章和几个随后的章节中，我们将学习不同的深度学习模型以及如何在我们的物联网生成数据上使用深度学习。在本章中，我们将先来一窥深度学习的历程，并学习四个流行模型，即**多层感知器**（**MLP**）、**卷积神经网络**（**CNN**）、**递归神经网络**（**RNN**）和自编码器。具体而言，您将学习以下内容：
- en: The history of DL and the factors responsible for its present success
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的历史及其目前成功的因素
- en: Artificial neurons and how they can be connected to solve non-linear problems
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经元及其如何连接以解决非线性问题
- en: The backpropagation algorithm and using it to train the MLP model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播算法及其用于训练 MLP 模型
- en: The different optimizers and activation functions available in TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 中可用的不同优化器和激活函数
- en: How the CNN works and the concept behind kernel, padding, and strides
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 的工作原理及卷积核、填充和步幅背后的概念
- en: Using CNN model for classification and recognition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 CNN 模型进行分类和识别
- en: RNNs and modified RNN and long short-term memory and gated recurrent units
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 及其修改版、长短期记忆和门控循环单元
- en: The architecture and functioning of autoencoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的架构和功能
- en: Deep learning 101
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习 101
- en: The human mind has always intrigued philosophers, scientists, and engineers
    alike. The desire to imitate and replicate the intelligence of the human brain
    by man has been written about over many years; Galatea by Pygmalion of Cyprus
    in Greek mythology, Golem in Jewish folklore, and Maya Sita in Hindu mythology
    are just a few examples. Robots with **Artificial Intelligence** (**AI**) are
    a favorite of (science) fiction writers since time immemorial.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类思维一直令哲学家、科学家和工程师着迷。人类通过仿效和复制大脑智能的愿望已经被多年写入文献；比如希腊神话中的塞浦路斯的毕格马利翁的加拉特亚、犹太民间传说中的歌勒姆和印度神话中的玛雅·希塔，只是少数例子。拥有**人工智能**（**AI**）的机器人自古以来就是（科幻）作家钟爱的对象。
- en: AI, as we know today, was conceived parallel with the idea of computers. The
    seminal paper, *A Logical Calculus Of The Ideas Immanent In Nervous Activity*,
    in the year 1943 by McCulloch and Pitts proposed the first neural network model—the
    threshold devices that could perform logical operations such as AND, OR, AND-NOT.
    In his pioneering work, *Computing Machinery and Intelligence,* published in the
    year 1950, Alan Turing proposed a **Turing test**; a test to identify whether
    a machine has intelligence or not. Rosenblatt, in 1957, laid the base for networks
    that could learn from experience in his report, *The Perceptron—a perceiving and
    recognizing automaton*. These ideas were far ahead of their time; while the concepts
    looked theoretically possible, computational resources at that time severely limited
    the performance you could get through these models that could do logic and learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能，如今我们所知的，与计算机的概念是平行的。1943年，McCulloch和Pitts在其开创性论文《神经活动中内在思想的逻辑演算》中提出了第一个神经网络模型——阈值设备，可以执行逻辑运算如AND、OR、AND-NOT。1950年，Alan
    Turing在其开创性工作《计算机器与智能》中提出了**图灵测试**；一种识别机器是否具有智能的测试方法。1957年，Rosenblatt在其报告《感知和识别自动机——感知器》中奠定了可以从经验中学习的网络基础。这些想法当时遥遥领先；虽然这些概念在理论上看似可能，但当时的计算资源严重限制了通过这些能够执行逻辑和学习的模型获得的性能。
- en: 'While these papers seem old and irrelevant, they are very much worth reading
    and give great insight into the vision these initial thinkers had. Following,
    are the links to these papers for interested readers:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些论文看起来有些陈旧且不太相关，但它们非常值得阅读，并能深入洞察这些早期思想家的视野。以下是这些论文的链接，供感兴趣的读者参考：
- en: '*A Logical Calculus Of The Ideas Immanent In Nervous Activity*, McCulloch and
    Pitts: [https://link.springer.com/article/10.1007%2FBF02478259](https://link.springer.com/article/10.1007%2FBF02478259)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经活动中固有思想的逻辑演算*，麦卡洛克和皮茨：[https://link.springer.com/article/10.1007%2FBF02478259](https://link.springer.com/article/10.1007%2FBF02478259)'
- en: '*Computing Machinery and Intelligence*, Alan Turing: [http://phil415.pbworks.com/f/TuringComputing.pdf](http://phil415.pbworks.com/f/TuringComputing.pdf)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*计算机机械与智能*，艾伦·图灵：[http://phil415.pbworks.com/f/TuringComputing.pdf](http://phil415.pbworks.com/f/TuringComputing.pdf)'
- en: '*The Perceptron—a perceiving and recognizing automaton*, Rosenblatt: [https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*感知机——一种感知和识别的自动机*，罗森布拉特：[https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
- en: 'Another interesting paper one by Wang and Raj from Carnegie Melon University,
    *On the Origin of Deep Learning*; the 72-page paper covers in detail the history
    of DL, starting from the McCulloch Pitts model to the latest attention models:
    [https://arxiv.org/pdf/1702.07800.pdf](https://arxiv.org/pdf/1702.07800.pdf).'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一篇有趣的论文是来自卡内基梅隆大学的王和Raj的论文，*深度学习的起源*；这篇72页的论文详细介绍了深度学习的历史，从麦卡洛克·皮茨模型到最新的注意力模型：[https://arxiv.org/pdf/1702.07800.pdf](https://arxiv.org/pdf/1702.07800.pdf)。
- en: 'Two AI winters and a few successes later (with the breakthrough in 2012, when
    Alex Krizhvesky, Ilya Sutskever, and Geoffrey Hinton''s AlexNet entry in the annual
    ImageNet challenge achieved an error rate of 16%), today we stand at a place where
    DL has outperformed most of the existing AI techniques. The following screenshot
    from Google Trends shows that, roughly around 2014, **Deep** **Learning** became
    popular and had been growing since then:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 经历了两次AI寒冬和一些成功（2012年突破，当亚历克斯·克里日夫斯基、伊利亚·苏茨克维尔和杰弗里·辛顿的AlexNet在年度ImageNet挑战赛中取得了16%的错误率），今天我们站在了一个地方，深度学习已经超越了大多数现有的AI技术。以下是来自Google
    Trends的截图，显示大约在2014年，**深度学习**开始变得流行，并且自那时起一直在增长：
- en: '![](img/902e63f9-5ec8-4568-b10e-9f18e2a3a5f0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/902e63f9-5ec8-4568-b10e-9f18e2a3a5f0.png)'
- en: Deep learning in Google Trends from 2004 to April 2018
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Google Trends中的深度学习，从2004年到2018年4月
- en: Let's see the reasons behind this growing trend and analyze whether it's just
    hype or whether there's more to it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这一增长趋势背后的原因，分析一下这是不是只是炒作，还是背后还有更多内容。
- en: Deep learning—why now?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习——为什么是现在？
- en: 'Most of the core concepts in the field of DL were already in place by the 80s
    and 90s, and therefore, the question arises why suddenly we see an increase in
    the applications of DL to solve different problems from image classification and
    image inpainting, to self-driving cars and speech generation. The major reason
    is twofold, outlined as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的大多数核心概念在80年代和90年代就已经基本成型，因此，问题来了：为什么我们现在突然看到深度学习被应用于解决从图像分类、图像修复到自动驾驶汽车和语音生成等不同问题？主要原因有两个，具体如下：
- en: '**Availability of large high-quality dataset**: The internet resulted in the
    generation of an enormous amount of datasets in terms of images, video, text,
    and audio. While most of it''s unlabeled, by the effort of many leading researchers
    (for example, Fei Fei Li creating the ImageNet dataset), we finally have access
    to large labeled datasets. If DL is a furnace lighting your imagination, data
    is the fuel burning it. The greater the amount and variety of the data, the better
    the performance of the model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模高质量数据集的可用性**：互联网的出现产生了大量的图像、视频、文本和音频数据集。虽然大部分数据没有标签，但在许多领先研究人员的努力下（例如，李飞飞创建了ImageNet数据集），我们终于可以获得大量标注数据集。如果深度学习是一炉点燃你想象力的火炉，那么数据就是燃烧它的燃料。数据量和种类越多，模型的表现就越好。'
- en: '**Availability of parallel computing using graphical processing units**: In
    DL models, there are mainly two mathematical matrix operations that play a crucial
    role, namely, matrix multiplication and matrix addition. The possibility of parallelizing
    these processes for all the neurons in a layer with the help of **graphical processing
    units** (**GPUs**) made it possible to train the DL models in reasonable time.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用图形处理单元进行并行计算的可用性**：在DL模型中，主要有两种数学矩阵操作发挥着至关重要的作用，即矩阵乘法和矩阵加法。通过**图形处理单元**（**GPUs**）对这些过程进行并行化，使得在合理的时间内训练DL模型成为可能。'
- en: Once the interest in DL grew, people came up with further improvements, like
    better optimizers for the gradient descent (the necessary algorithm used to calculate
    weight and bias update in DL models), for example, Adam and RMSprop; new regularization
    techniques such as dropout and batch normalization that help, not only in overfitting,
    but can also reduce the training time, and last, but not the least, availability
    of DL libraries such as TensorFlow, Theano, Torch, MxNet, and Keras, which made
    it easier to define and train complex architectures.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习（DL）兴趣的增长，人们提出了进一步的改进，例如更好的优化器，用于梯度下降（在DL模型中用于计算权重和偏差更新的必要算法），例如Adam和RMSprop；新的正则化技术，如dropout和批量归一化（batch
    normalization），不仅有助于解决过拟合问题，还能减少训练时间；最后但同样重要的是，DL库的出现，如TensorFlow、Theano、Torch、MxNet和Keras，这些都让定义和训练复杂架构变得更加容易。
- en: According to Andrew Ng, founder of [deeplearning.ai](https://www.deeplearning.ai/),
    despite plenty of hype and frantic investment, we won't see another AI winter,
    because improvements in the computing devices *will keep the performance advances
    and breakthroughs coming for the foreseeable future*, Andrew Ng said this at EmTech
    Digital in 2016, and true to his prediction, we have seen advancements in the
    processing hardware with Google's **Tensor Processing Unit** (**TPUs**), Intel
    Movidius, and NVIDIA's latest GPUs. Moreover, there are cloud computing GPUs that
    are available today at as low as 0.40 cents per hour, making it affordable for
    all.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[deeplearning.ai](https://www.deeplearning.ai/)的创始人Andrew Ng的说法，尽管有大量炒作和疯狂的投资，但我们不会再看到AI冬天的到来，因为计算设备的改进*将持续带来性能提升和突破*。Andrew
    Ng在2016年的EmTech Digital上做出了这一预测，并且正如他所预言的那样，我们已经看到了处理硬件的进步，如谷歌的**张量处理单元**（**TPUs**）、Intel
    Movidius以及NVIDIA最新的GPU。此外，今天有云计算GPU服务，其价格低至每小时0.40美元，使其对所有人都变得更加可负担。
- en: 'You can read the complete article *AI Winter Isn''t Coming*, published in MIT
    Technology Review: [https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/](https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/).
    Here Andrew Ng answers different queries regarding the future of AI.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以阅读MIT Technology Review上发表的完整文章*《AI冬天不会到来》*：[https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/](https://www.technologyreview.com/s/603062/ai-winter-isnt-coming/)。在这篇文章中，Andrew
    Ng回答了有关人工智能未来的各种问题。
- en: 'For DL, GPU processing power is a must; there are a large number of companies
    offering cloud computing services for the same. But in case you are starting in
    the field, you can use one of the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习（DL），GPU计算能力是必须的；有大量公司提供云计算服务来满足这一需求。但如果你是刚刚开始涉足这一领域，可以使用以下一些服务：
- en: '**Google Colaboratory**: It provides a browser-based, GPU enabled Jupyter Notebook—like
    interface. It gives free access to the GPU computing power for 12 continuous hours.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colaboratory**：它提供了一个基于浏览器的、启用GPU的Jupyter Notebook风格界面，并且为用户提供12小时的GPU计算能力。'
- en: '**Kaggle**: Kaggle too provides a Jupyter Notebook style interface with GPU
    computing power for roughly six continuous hours free of cost.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kaggle**：Kaggle同样提供了一个Jupyter Notebook风格的界面，并且为用户提供大约六小时的GPU计算能力，完全免费。'
- en: Artificial neuron
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: The fundamental component of all DL models is an artificial neuron. The artificial
    neuron is inspired by the working of biological neurons. It consists of some inputs
    connected via weights (also called **synaptic connections**), the weighted sum
    of all the inputs goes through a processing function (called the **activation
    function**) and generates a non-linear output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有DL模型的基本组成部分是人工神经元。人工神经元的灵感来源于生物神经元的工作方式。它由一些通过权重连接的输入（也称为**突触连接**）组成，所有输入的加权和通过一个处理函数（称为**激活函数**）进行处理，生成一个非线性输出。
- en: 'The following screenshot shows **A biological Neuron** and **An Artificial
    Neuron**:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了**生物神经元**与**人工神经元**：
- en: '![](img/533935b8-b03c-47b0-835f-3315a1a9361a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/533935b8-b03c-47b0-835f-3315a1a9361a.png)'
- en: A biological neuron and an artificial neuron
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元与人工神经元
- en: 'If *X[i]* is the *i*^(th) input to the artificial neuron (*j*) connected via
    the synaptic connection *w[ij]*, then, the net input to the neuron, commonly called
    the **activity of the neuron**, can be defined as the weighted sum of all its
    contains, and is given by the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*X[i]*是连接到人工神经元(*j*)的第*i*输入，且通过突触连接*w[ij]*连接，则该神经元的净输入，通常称为**神经元的活动**，可以定义为所有输入的加权和，表示为：
- en: '![](img/2d7fd5ac-b237-47c6-ba7a-c7d2a56f0415.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d7fd5ac-b237-47c6-ba7a-c7d2a56f0415.png)'
- en: 'In the preceding equation, *N* is the total number of inputs to the *j*^(th)
    neuron, and *θ[j]* is the threshold of the *j*^(th) neuron; the output of the
    neuron is then given by the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述方程中，*N*是输入到第*j*个神经元的总数，*θ[j]*是第*j*个神经元的阈值；神经元的输出由下式给出：
- en: '![](img/94f64405-a749-4631-8e50-1bf6d36bca10.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94f64405-a749-4631-8e50-1bf6d36bca10.png)'
- en: 'In the preceding, *g* is the activation function. The following point lists
    different activation functions used in different DL models, along with their mathematical
    and graphical representations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述表达式中，*g*是激活函数。以下列出了不同深度学习模型中使用的不同激活函数，并给出了它们的数学和图形表示：
- en: Sigmoid: ![](img/3a88f5a9-2bb0-4f1e-89b3-117b96da4945.png)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid: ![](img/3a88f5a9-2bb0-4f1e-89b3-117b96da4945.png)
- en: '![](img/c64270c0-bc9b-4917-81cf-3f050fb76f44.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c64270c0-bc9b-4917-81cf-3f050fb76f44.png)'
- en: Hyperbolic Tangent: *g(h[j])= tanh(h[j])*
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切：*g(h[j]) = tanh(h[j])*
- en: '![](img/a8bc9f89-bf33-4b53-aae5-edf3fe12c45e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8bc9f89-bf33-4b53-aae5-edf3fe12c45e.png)'
- en: ReLU: *g(h[j])= max(0,h[j])*
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU: *g(h[j])= max(0,h[j])*
- en: '![](img/1d0fe159-f070-4193-bb05-61dc1f5045d2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d0fe159-f070-4193-bb05-61dc1f5045d2.png)'
- en: Softmax: ![](img/7c124da7-6c92-4b9b-9d4c-d79ccae0cc89.png)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax: ![](img/7c124da7-6c92-4b9b-9d4c-d79ccae0cc89.png)
- en: '![](img/4197a960-7e58-4b01-890f-e611b8ee4062.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4197a960-7e58-4b01-890f-e611b8ee4062.png)'
- en: Leaky ReLU: ![](img/71459d3e-9c52-47bf-b038-9db5e584f5a8.png)
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU: ![](img/71459d3e-9c52-47bf-b038-9db5e584f5a8.png)
- en: '![](img/e2167991-26bb-4f98-8325-3663bb75cdac.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2167991-26bb-4f98-8325-3663bb75cdac.png)'
- en: ELU: ![](img/1ae102ff-1354-456c-a111-6c2f99642980.png)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ELU: ![](img/1ae102ff-1354-456c-a111-6c2f99642980.png)
- en: '![](img/9854bc26-8a85-49c1-b321-c25993a7f369.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9854bc26-8a85-49c1-b321-c25993a7f369.png)'
- en: Threshold: ![](img/b6ede6c5-55ce-4340-8fdb-cfecc66880ad.png)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值：![](img/b6ede6c5-55ce-4340-8fdb-cfecc66880ad.png)
- en: '![](img/7585376a-d5a9-4212-9916-d07b03419737.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7585376a-d5a9-4212-9916-d07b03419737.png)'
- en: Modelling single neuron in TensorFlow
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中建模单一神经元
- en: 'Can we use this single neuron and make it learn? The answer is yes, the process
    of learning involves adapting the weights such that a predefined loss function
    (*L*) reduces. If we update the weights in the direction opposite to the gradient
    of the loss function with respect to weights, it will ensure that loss function
    decreases with each update. This algorithm is called the **gradient descent**
    algorithm, and is at the heart of all DL models. Mathematically, if *L* is the
    loss function and *η* the learning rate, then the weight *w[ij]* is updated and
    represented as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否使用这个单一的神经元并让它学习？答案是肯定的，学习的过程涉及调整权重，使得预定义的损失函数(*L*)减小。如果我们按照损失函数关于权重的梯度的反方向更新权重，它将确保每次更新时损失函数减少。这个算法被称为**梯度下降**算法，是所有深度学习模型的核心。数学上，如果*L*是损失函数，*η*是学习率，那么权重*w[ij]*的更新表示为：
- en: '![](img/d0ba28af-252f-4fd5-9a6c-77ac7ac240de.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0ba28af-252f-4fd5-9a6c-77ac7ac240de.png)'
- en: 'If we have to model the single artificial neuron, we need first to decide the
    following parameters:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要建模单一的人工神经元，首先需要决定以下参数：
- en: '**Learning rate parameter**: Learning rate parameter determines how fast we
    descent the gradient. Conventionally, it lies between *0* and *1*. If learning
    rate is too high, the network may either oscillate around the correct solution
    or completely diverge from the solution. On the other hand, when learning rate
    is too low, it will take a long time to converge to the solution finally.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率参数**：学习率参数决定了我们梯度下降的速度。通常，它的值介于*0*和*1*之间。如果学习率过高，网络可能会围绕正确的解振荡，或者完全偏离解。另一方面，当学习率过低时，最终收敛到解所需的时间会很长。'
- en: '**Activation function**: The activation function decides how the output of
    the neuron varies with its activity. Since the weight update equation involves
    a derivative of the loss function, which in turn will depend on the derivative
    of the activation function, we prefer a continuous-differentiable function as
    the activation function for the neuron. Initially, sigmoid and hyperbolic tangent
    were used, but they suffered from slow convergence and vanishing gradients (the
    gradient becoming zero, and hence, no learning, while the solution hasn''t been
    reached). In recent years, **rectified linear units** (**ReLU**) and its variants
    such as leaky ReLU and ELU are preferred since they offer fast convergence and
    at the same time, help in overcoming the vanishing gradient problem. In ReLU,
    we sometimes have a problem of **dead neurons**, that is some neurons never fire
    because their activity is always less than zero, and hence, they never learn.
    Both leaky ReLU and ELU overcome the problem of dead neurons by ensuring a non-zero
    neuron output, even when the activity is negative. The lists of the commonly used
    activation functions, and their mathematical and graphical representations is
    explained before this section. (You can play around with the `activation_functions.ipynb` code ,
    which uses TensorFlow defined activation functions.)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：激活函数决定了神经元的输出如何随其活动而变化。由于权重更新方程涉及损失函数的导数，而这个导数又依赖于激活函数的导数，因此我们更倾向于选择一个连续可微的函数作为神经元的激活函数。最初，使用的是sigmoid和双曲正切函数，但它们存在收敛慢和梯度消失的问题（梯度变为零，从而没有学习，而解决方案还未达到）。近年来，**修正线性单元**（**ReLU**）及其变体如leaky
    ReLU和ELU更受欢迎，因为它们提供了快速收敛，并且有助于克服梯度消失问题。在ReLU中，我们有时会遇到**死神经元**的问题，即一些神经元永远不会激活，因为它们的活动总是小于零，因此它们永远无法学习。Leaky
    ReLU和ELU通过确保神经元输出不为零，即使活动为负，也解决了死神经元的问题。常用激活函数的列表以及它们的数学和图形表示在本节之前已经解释。（你可以尝试使用`activation_functions.ipynb`代码，其中使用了TensorFlow定义的激活函数。）'
- en: '**Loss function**: Loss function is the parameter our network tries to minimize,
    and so choosing the right loss function is crucial for the learning. As you will
    delve deep into DL, you will find many cleverly defined loss functions. You will
    see how, by properly defining loss functions, we can make our DL model create
    new images, visualize dreams, or give a caption to an image, and much more. Conventionally,
    depending on the type of task regression or classification, people use **mean
    square error** (**MSE**) or **categorical-cross entropy** loss function. You will
    learn these loss functions as we progress through the book.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：损失函数是我们网络试图最小化的参数，因此选择正确的损失函数对于学习至关重要。随着你深入了解深度学习，你会发现许多巧妙定义的损失函数。你会看到，通过正确地定义损失函数，我们可以让我们的深度学习模型生成新的图像、可视化梦境、为图像添加标题等等。传统上，根据任务的类型（回归或分类），人们使用**均方误差**（**MSE**）或**类别交叉熵**损失函数。你将在本书的后续内容中学习这些损失函数。'
- en: 'Now that we know the basic elements needed to model an artificial neuron, let''s
    start with the coding. We will presume a regression task, and so we will use MSE
    loss function. If *y[j]* is the output of our single neuron for the input vector
    *X* and ![](img/ae1b66ec-7c89-4339-b04e-9c4f506272e1.png) is the output we desire
    for output neuron *j*, then the MSE error is mathematically expressed as (mean
    of the square of the error ![](img/a6adaa8f-16fa-49c4-af70-e3844c7cd9d8.png)),
    shown as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了建模人工神经元所需的基本元素，让我们开始编写代码。我们假设是一个回归任务，因此我们将使用均方误差（MSE）损失函数。如果 *y[j]*
    是我们单个神经元对输入向量 *X* 的输出，而 ![](img/ae1b66ec-7c89-4339-b04e-9c4f506272e1.png) 是我们期望输出神经元
    *j* 的输出，那么MSE误差在数学上可以表示为（误差的平方的均值 ![](img/a6adaa8f-16fa-49c4-af70-e3844c7cd9d8.png)），如下所示：
- en: '![](img/3b1d1c78-aff6-46b5-bdd6-05c9058ff9f0.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b1d1c78-aff6-46b5-bdd6-05c9058ff9f0.png)'
- en: In the preceding, *M* is the total number of training sample (input-output pair).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*M* 是训练样本的总数（输入-输出对）。
- en: Note that if you were to implement this artificial neuron without using TensorFlow
    (to be specific without using any of the DL libraries mentioned earlier), then
    you will need to calculate the gradient yourself, for example, you will write
    a function or a code that will first compute the gradient of loss function, and
    then you will have to write a code to update all of the weights and biases. For
    a single neuron with the MSE loss function, calculating derivative is still straightforward,
    but as the complexity of the network increases, calculating the gradient for the
    specific loss function, implementing it in code, and then finally updating weights
    and biases can become a very cumbersome act.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您要实现这个人工神经元而不使用 TensorFlow（具体而言是不使用前面提到的任何 DL 库），那么您将需要自己计算梯度，例如，您将编写一个函数或代码，首先计算损失函数的梯度，然后您将必须编写代码来更新所有权重和偏差。对于具有
    MSE 损失函数的单个神经元，计算导数仍然很简单，但随着网络复杂性的增加，计算特定损失函数的梯度、在代码中实现它，然后最终更新权重和偏差可能变得非常繁琐。
- en: TensorFlow makes this whole process easier by using automatic differentiation.
    TensorFlow specifies all the operations in a TensorFlow graph; this allows it
    to use the chain rule and go complicated in the graph assigning the gradients.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 通过使用自动微分使整个过程变得更加简单。TensorFlow 指定了 TensorFlow 图中的所有操作；这使它能够使用链式法则并在图中复杂地分配梯度。
- en: And so, in TensorFlow we build the execution graph, and define our loss function,
    then it calculates the gradient automatically, and it supports many different
    gradients, calculating algorithms (optimizers), which we can conveniently use.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 TensorFlow 中，我们构建执行图，并定义我们的损失函数，然后它会自动计算梯度，并支持许多不同的梯度计算算法（优化器），我们可以方便地使用它们。
- en: 'You can learn more about the concept of automatic differentiation through this
    link: [http://www.columbia.edu/~ahd2125/post/2015/12/5/](http://www.columbia.edu/~ahd2125/post/2015/12/5/).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过此链接了解更多关于自动微分的概念：[http://www.columbia.edu/~ahd2125/post/2015/12/5/](http://www.columbia.edu/~ahd2125/post/2015/12/5/)。
- en: 'Now with all this basic information, we build our single neuron in TensorFlow
    with the following steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了所有这些基本信息，我们通过以下步骤在 TensorFlow 中构建我们的单个神经元：
- en: 'The first step, in every Python code, is always importing the modules one will
    need in the rest of the program. We will import TensorFlow to build the single
    artificial neuron. Numpy and pandas are there for any supporting mathematical
    calculations and for reading the data files. Beside this, we are also importing
    some useful functions (for normalization of data, splitting it into train, validation,
    and shuffling the data) from scikit-learn, we have already used these functions
    in the earlier chapters and know that normalization and shuffling is an important
    step in any AI pipeline:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个 Python 代码的第一步始终是导入程序中将需要的模块。我们将导入 TensorFlow 来构建单个人工神经元。Numpy 和 pandas 则用于任何支持的数学计算和读取数据文件。此外，我们还从
    scikit-learn 中导入一些有用的函数（用于数据归一化、将数据分割为训练集和验证集以及对数据进行洗牌），我们在早期章节已经使用过这些函数，并且知道归一化和洗牌在任何
    AI 流水线中都是重要的步骤：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As explained earlier, validation helps in knowing if the model has learned or
    it's overfitting or underfitting
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所解释的，验证有助于了解模型是否学习，以及是否出现了过拟合或欠拟合的情况。
- en: 'In TensorFlow, we first build a model graph and then execute it. This might,
    when starting, seem complicated, but once you get the hang of it, it''s very convenient
    and allows us to optimize the code for production. So, let''s first define our
    single neuron graph. We define `self.X` and `self.y` as placeholders to pass on
    the data to the graph, as shown in the following code:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，我们首先构建一个模型图，然后执行它。一开始可能会觉得复杂，但一旦掌握了它，它就非常方便，并允许我们优化用于生产的代码。因此，让我们首先定义我们的单个神经元图。我们将
    `self.X` 和 `self.y` 定义为占位符，以便将数据传递给图，如以下代码所示：
- en: '[PRE1]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The weights and biases are defined as variables so that the automatic differentiation
    automatically updates them. TensorFlow provides a graphical interface to support
    TensorBoard to see the graph structure, as well as different parameters, and how
    they change during training. It''s beneficial for debugging and understanding
    how your model is behaving. In the following code, we, therefore, add code lines
    to create histogram summaries for both weights and biases:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重和偏置被定义为变量，以便自动求导自动更新它们。TensorFlow提供了一个图形界面，支持TensorBoard查看图的结构、不同参数以及它们在训练过程中的变化。这对于调试和理解模型的行为非常有帮助。在以下代码中，我们因此添加了代码行来为权重和偏置创建直方图摘要：
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we perform the mathematical operations, the matrix multiplication, between
    input and weights, add the bias, and calculate the activity of the neuron and
    its output, denoted by `self.y_hat` shown as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们执行数学运算，即输入和权重之间的矩阵乘法，加入偏差，计算神经元的激活值及其输出，记为`self.y_hat`，如下面所示：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We define the loss function that we want our model to minimize, and use the
    TensorFlow optimizer to minimize it, and update weights and biases using the gradient
    descent optimizer, as shown in the following code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了我们希望模型最小化的损失函数，并使用TensorFlow优化器最小化该损失，利用梯度下降优化器更新权重和偏置，如下代码所示：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We complete the `init` function by defining a TensorFlow Session and initializing
    all the variables. We also add code to ensure that TensorBoard writes all the
    summaries at the specified place, shown as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过定义一个TensorFlow会话并初始化所有变量来完成`init`函数。我们还添加了代码，确保TensorBoard将所有摘要写入指定位置，如下所示：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We define the `train` function where the graph we previously built is executed,
    as shown in the following code:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了`train`函数，在其中执行之前构建的图，如下代码所示：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To make a prediction, we also include a `predict` method, as shown in the following
    code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进行预测，我们还包括了一个`predict`方法，如下代码所示：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, like in the previous chapter, we read the data, normalize it using scikit-learn
    functions, and split it into training and validation set, shown as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，像在上一章中一样，我们读取数据，使用scikit-learn函数对其进行归一化，并将其划分为训练集和验证集，如下所示：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We use the artificial neuron we created to make the energy output prediction.
    `Training Loss` and `Validation Loss` are plotted as the artificial neuron learns,
    as shown in the following:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用创建的人工神经元来进行能量输出预测。随着人工神经元学习，`训练损失`和`验证损失`被绘制，如下所示：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](img/d4f9e5f6-53e1-4a90-8053-4dc1f12be2f0.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4f9e5f6-53e1-4a90-8053-4dc1f12be2f0.png)'
- en: Mean square error for training and validation data as the single artificial
    neuron learns to predict the energy output
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 单个人工神经元学习预测能量输出时，训练和验证数据的均方误差
- en: The complete code with data reading, data normalization, training, and so on
    is given in the `single_neuron_tf.ipynb` Jupyter Notebook.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 包含数据读取、数据归一化、训练等完整代码在`single_neuron_tf.ipynb` Jupyter Notebook中给出。
- en: Multilayered perceptrons for regression and classification
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器用于回归和分类
- en: In the last section, you learned about a single artificial neuron and used it
    to predict the energy output. If we compare it with the linear regression result
    of [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml), *Machine Learning
    for IoT*, we can see that though the single neuron did a good job, it was not
    as good as linear regression. The single neuron architecture had an MSE value
    of 0.078 on the validation dataset as compared 0.01 of linear regression. Can
    we make it better, with maybe more epochs, or different learning rate, or perhaps
    more single neurons. Unfortunately not, single neurons can solve only linearly
    separable problems, for example, they can provide a solution only if there exists
    a straight line separating the classes/decision.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，你了解了单个人工神经元并使用它来预测能量输出。如果我们将其与[第3章](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml)《物联网的机器学习》中的线性回归结果进行比较，我们会发现，尽管单神经元表现不错，但仍不如线性回归。单神经元架构在验证数据集上的均方误差（MSE）为0.078，而线性回归为0.01。我们能否通过增加更多的训练周期、调整学习率或增加更多的单一神经元来改善结果呢？不行，单神经元只能解决线性可分的问题。例如，只有当类别/决策之间存在一条直线时，单神经元才能提供解决方案。
- en: The network with a single layer of neurons is called **simple perceptron**.
    The perceptron model was given by Rosenblatt in 1958 ([htt](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)[p://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)).
    The paper created lots of ripples in the scientific community and lots of research
    was initiated in the field. It was first implemented in hardware for the task
    of image recognition. Although perceptron seemed very promising initially, the
    book *Perceptrons* by Marvin Minsky and Seymour Papert proved that simple perceptron
    can solve only linearly separable problems ([https://books.google.co.in/books?hl=en&amp;lr=&amp;id=PLQ5DwAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Perceptrons:+An+Introduction+to+Computational+Geometry&amp;ots=zyEDwMrl__&amp;sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&amp;q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;f=false](https://books.google.co.in/books?hl=en&lr=&id=PLQ5DwAAQBAJ&oi=fnd&pg=PR5&dq=Perceptrons:+An+Introduction+to+Computational+Geometry&ots=zyEDwMrl__&sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&f=false)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一层神经元的网络被称为**简单感知机**。感知机模型由Rosenblatt于1958年提出（[htt](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)[p://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&rep=rep1&type=pdf)）。该论文在科学界引起了广泛的反响，并促使该领域展开了大量的研究。它首次被应用于硬件中进行图像识别任务。尽管感知机在初期看起来非常有前景，但Marvin
    Minsky和Seymour Papert的书籍《感知机》证明了简单的感知机只能解决线性可分的问题（[https://books.google.co.in/books?hl=en&amp;lr=&amp;id=PLQ5DwAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Perceptrons:+An+Introduction+to+Computational+Geometry&amp;ots=zyEDwMrl__&amp;sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&amp;q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&amp;f=false](https://books.google.co.in/books?hl=en&lr=&id=PLQ5DwAAQBAJ&oi=fnd&pg=PR5&dq=Perceptrons:+An+Introduction+to+Computational+Geometry&ots=zyEDwMrl__&sig=DfDDbbj3es52hBJU9znCercxj3M#v=onepage&q=Perceptrons%3A%20An%20Introduction%20to%20Computational%20Geometry&f=false))。
- en: 'So what do we do? We can use multiple layers of single neurons, in other words,
    use MLP. Just as in real life, we solve a complex problem by breaking it into
    small problems, each neuron in the first layer of the MLP breaks the problem into
    small linearly separable. Since the information flows here in one direction from
    the input layer to the output layer via hidden layers, this network is also called
    a **feedforward** network. In the following diagram, we see how the **XOR** problem
    is solved using two neurons in the first layer, and a single neuron in the **Output
    Layer**. The network breaks the non-linearly separable problem into three linearly
    separable problems:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们该怎么办呢？我们可以使用多个单一神经元的层，换句话说，使用MLP。就像在现实生活中，我们通过将复杂问题分解为小问题来解决问题，MLP的第一层中的每个神经元将问题分解为小的线性可分问题。由于信息在这里是单向流动的，从输入层通过隐藏层流向输出层，这个网络也被称为**前馈**网络。在下图中，我们可以看到如何通过第一层的两个神经元和**输出层**中的一个神经元来解决**XOR**问题。网络将不可线性分割的问题分解为三个线性可分的问题：
- en: '![](img/91008d5d-b096-4775-a7e0-778e0a396226.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91008d5d-b096-4775-a7e0-778e0a396226.png)'
- en: Previous diagram can be explained as XOR solved using MLP with one hidden layer
    with neurons and one neuron in the output layer. Red points represent zero and
    blue points represent one. We can see that the hidden neurons separate the problem
    into two linearly separable problems (AND and OR), the output neuron then implements
    another linearly separable logic the AND-NOT logic, combining them together we
    are able to solve the XOR, which is not linearly separable
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 上图可以通过使用具有一个隐藏层和一个输出层神经元的MLP来解释XOR的解决方案。红色点表示零，蓝色点表示一。我们可以看到，隐藏层神经元将问题分解为两个线性可分的问题（与、或），然后输出神经元实现另一个线性可分的逻辑——与非逻辑。将它们结合在一起，我们能够解决XOR问题，而XOR是不可线性分割的。
- en: The hidden neurons transform the problem into a form that output layer can use.
    The idea of multiple layers of neurons was given by McCulloch and Pitts earlier,
    but while Rosenblatt gave the learning algorithm for simple perceptrons, he had
    no way of training multiple layered percetrons. The major difficulty was that,
    while for the output neurons we know what should be the desired output and so
    can calculate the error, and hence, the loss function and weight updates using
    gradient descent, there was no way to know the desired output of hidden neurons.
    Hence, in the absence of any learning algorithm, MLPs were never explored much.
    This changed in 1982 when Hinton proposed the backpropagation algorithm ([https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf](https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf)),
    which can be used to calculate the error, and hence, the weight updates for the
    hidden neurons. They employed a neat and straightforward mathematical trick of
    differentiation using the chain rule, and solved the problem of passing the errors
    at the output layer back to the hidden neurons, and in turn, boosted life back
    to neural networks. Today, backpropagation algorithm is at the heart of almost
    all DL models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层神经元将问题转换成输出层可以使用的形式。多层神经元的思想最早由McCulloch和Pitts提出，但虽然Rosenblatt为简单的感知器提供了学习算法，但他无法训练多层感知器。主要的困难在于，对于输出神经元，我们知道应该期望的输出是什么，因此可以计算误差，从而使用梯度下降法来计算损失函数和权重更新，但是对于隐藏层神经元却没有办法知道期望的输出。因此，在没有任何学习算法的情况下，MLP（多层感知器）从未被广泛探索。这一情况在1982年发生了变化，当时Hinton提出了反向传播算法（[https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf](https://www.researchgate.net/profile/Yann_Lecun/publication/2360531_A_Theoretical_Framework_for_Back-Propagation/links/0deec519dfa297eac1000000/A-Theoretical-Framework-for-Back-Propagation.pdf)），该算法可以用来计算误差，从而计算隐藏层神经元的权重更新。他们采用了链式法则的一个简单直接的数学技巧，解决了将输出层的误差传递回隐藏层神经元的问题，从而给神经网络带来了新的生机。今天，反向传播算法几乎是所有深度学习模型的核心。
- en: The backpropagation algorithm
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: 'Let''s first gain a little understanding of the technique behind the backpropagation
    algorithm. If you remember from the previous section, the loss function at the
    output neuron is as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们对反向传播算法背后的技术有一个简单的了解。如果你还记得上一节的内容，输出神经元的损失函数如下：
- en: '![](img/cfa9f5dc-ecb4-45ef-9369-63e2229da5ee.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfa9f5dc-ecb4-45ef-9369-63e2229da5ee.png)'
- en: 'You can see that it''s unchanged, and so the weight connecting hidden neuron
    *k* to the output neuron *j* would be given as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到它保持不变，因此连接隐藏层神经元*k*到输出神经元*j*的权重如下所示：
- en: '![](img/078b9093-0555-493a-af10-c031448c46a4.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/078b9093-0555-493a-af10-c031448c46a4.png)'
- en: 'Applying the chain rule of differentiation, this reduces to the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则，这简化为以下形式：
- en: '![](img/26448358-62bd-4f0b-9a6d-c89e12e8c77e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26448358-62bd-4f0b-9a6d-c89e12e8c77e.png)'
- en: 'In the preceding equation, *O[k]* is the output of the hidden neuron *k*. Now
    the weight update connecting input neuron *i* to the hidden neuron *k* of hidden
    layer *n* can be written as the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*O[k]*是隐藏神经元*k*的输出。现在，连接输入神经元*i*到隐藏层*n*中隐藏神经元*k*的权重更新可以写成以下形式：
- en: '![](img/13803de4-5171-4668-ad06-bf08334d91ae.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13803de4-5171-4668-ad06-bf08334d91ae.png)'
- en: 'Again applying the chain rule, it reduces to the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用链式法则，它简化为以下形式：
- en: '![](img/bd1d50af-50cd-43a9-b6bf-97fd3f629ef1.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd1d50af-50cd-43a9-b6bf-97fd3f629ef1.png)'
- en: Here, *O[i]* is the output of the hidden neuron *i* in the *n-1^(th)* hidden
    layer. Since we are using TensorFlow, we need not bother with calculating these
    gradients, but still, it's a good idea to know the expressions. From these expressions,
    you can see why it's important that the activation function is differentiable.
    The weight updates depend heavily on the derivative of the activation function,
    as well as the inputs to the neurons. Therefore, a smooth derivative function
    like that in the case of ReLU and ELU result in faster convergence. If the derivative
    becomes too large, we have the problem of exploding gradients, and if the derivative
    becomes almost zero, we have the problem of vanishing gradients. In both cases,
    the network does not learn optimally.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*O[i]* 是 *n-1^(th)* 隐藏层中隐藏神经元 *i* 的输出。由于我们使用的是 TensorFlow，因此无需担心计算这些梯度，但了解这些表达式还是很有帮助的。从这些表达式中，你可以看到为什么激活函数的可微性非常重要。权重更新在很大程度上依赖于激活函数的导数以及神经元的输入。因此，像
    ReLU 和 ELU 这样的平滑导数函数会导致更快的收敛。如果导数变得太大，我们会遇到梯度爆炸的问题；如果导数接近零，我们会遇到梯度消失的问题。在这两种情况下，网络都无法进行最佳学习。
- en: 'Universal approximation theorem: in 1989 Hornik et al. and George Cybenko independently
    proved the universal approximation theorem. The theorem, in its simplest form,
    states that a large enough feedforward multilayered perceptron, under mild assumptions
    on activation function, with a single hidden layer, can approximate any Borel
    measurable function with any degree of accuracy we desire.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逼近定理：1989年，Hornik 等人和 George Cybenko 独立证明了通用逼近定理。该定理以最简单的形式表述为，在激活函数满足温和假设的前提下，足够大的前馈多层感知器具有单个隐藏层，可以以我们所需的任何精度逼近任何
    Borel 可测函数。
- en: 'In simpler words, it means that the neural network is a universal approximator,
    and we can approximate any function, listed as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 用更简单的话来说，这意味着神经网络是一个通用逼近器，我们可以逼近任何函数，列举如下：
- en: We can do so using a single hidden layer feedforward network.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用单个隐藏层前馈网络来实现这一点。
- en: We can do so provided the network is large enough (that is add more hidden neurons
    if needed).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只要网络足够大（即如果需要，可以增加更多的隐藏神经元），我们就能做到这一点。
- en: Cybenko proved it for sigmoid activation function at the hidden layer, and linear
    activation function at the output layer. Later, Hornik et al showed that it's
    actually the property of MLPs and can be proved for other activation functions
    too
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cybenko 证明了在隐藏层使用 sigmoid 激活函数，在输出层使用线性激活函数的情况。后来，Hornik 等人证明了这实际上是 MLP 的一个性质，并且可以证明对于其他激活函数也是成立的
- en: The theorem gives a guarantee that MLP can solve any problem, but does not give
    any measure on how large the network should be. Also, it does not guarantee learning
    and convergence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 该定理保证了 MLP 可以解决任何问题，但没有给出网络应有多大的衡量标准。此外，它也不保证学习和收敛。
- en: 'You can refer to the papers using the following links:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接中的论文：
- en: 'Hornik et al.: [https://www.sciencedirect.com/science/article/pii/0893608089900208](https://www.sciencedirect.com/science/article/pii/0893608089900208)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hornik 等人: [https://www.sciencedirect.com/science/article/pii/0893608089900208](https://www.sciencedirect.com/science/article/pii/0893608089900208)'
- en: 'Cybenko: [https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cybenko: [https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf](https://pdfs.semanticscholar.org/05ce/b32839c26c8d2cb38d5529cf7720a68c3fab.pdf)'
- en: 'Now we can describe the steps involved in the backpropagation algorithm, listed
    as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以描述反向传播算法中涉及的步骤，列举如下：
- en: Apply the input to the network
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入应用到网络中
- en: Propagate the input forward and calculate the output of the network
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入向前传播并计算网络的输出
- en: Calculate the loss at the output, and then using the preceding expressions,
    calculate weight updates for output layer neuron
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出的损失，然后使用前面的表达式，计算输出层神经元的权重更新
- en: Using the weighted errors at output layers, calculate the weight updates for
    hidden layer
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用输出层的加权误差，计算隐藏层的权重更新
- en: Update all the weights
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新所有的权重
- en: Repeat the steps for other training examples
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他训练示例重复这些步骤
- en: Energy output prediction using MLPs in TensorFlow
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MLP 在 TensorFlow 中进行能量输出预测
- en: Let's now see how good an MLP is for predicting energy output. This will be
    a regression problem. We will be using a single hidden layer MLP and will predict
    the net hourly electrical energy output from a combined cycle power plant. The
    description of the dataset is provided in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and foundations of IoT and AI*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看MLP在预测能量输出方面的表现如何。这将是一个回归问题。我们将使用一个单一的隐藏层MLP，预测联合循环发电厂的每小时净电能输出。数据集的描述见于[第1章](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml)《物联网与人工智能的原理与基础》。
- en: 'Since it''s a regression problem, our loss function remains the same as before.
    The complete code implementing the `MLP` class is given as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个回归问题，我们的损失函数与之前相同。实现`MLP`类的完整代码如下：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Before using it, let''s see the differences between previous code and the code
    we made earlier for the single artificial neuron. Here the dimensions of weights
    of hidden layer is `#inputUnits × #hiddenUnits`; the bias of the hidden layer
    will be equal to the number of hidden units (`#hiddenUnits`). The output layer
    weights have the dimensions `#hiddenUnits × #outputUnits`; the bias of output
    layer is of the dimension of the number of units in the output layer (`#outputUnits`).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '在使用之前，我们来看看之前的代码和我们为单个人工神经元编写的代码之间的区别。这里，隐藏层的权重维度是`#inputUnits × #hiddenUnits`；隐藏层的偏置将等于隐藏单元的数量（`#hiddenUnits`）。输出层的权重维度是`#hiddenUnits
    × #outputUnits`；输出层的偏置维度与输出层单元的数量（`#outputUnits`）相同。'
- en: In defining the bias, we have used only the column dimensions, not row. This
    is because TensorFlow like `numpy` broadcasts the matrices according to the operation
    to be performed. And by not fixing the row dimensions of bias, we are able to
    maintain the flexibility of the number of input training samples (batch-size)
    we present to the network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义偏置时，我们只使用了列维度，而没有使用行维度。这是因为像`numpy`一样，TensorFlow根据执行的操作对矩阵进行广播。通过不固定偏置的行维度，我们能够保持输入训练样本数（批量大小）对网络的灵活性。
- en: 'Following screenshot shows  matrix multiplication and addition dimensions while
    calculating activity:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了计算活动时的矩阵乘法和加法维度：
- en: '![](img/5f59276a-59a7-490a-8c0b-09153b41bb2b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f59276a-59a7-490a-8c0b-09153b41bb2b.png)'
- en: The matrix multiplication and addition dimensions while calculating activity
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 计算活动时的矩阵乘法和加法维度
- en: 'The second difference that you should note is in the definition of loss, we
    have added here the `l2` regularization term to reduce overfitting as discussed
    in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml), *Machine Learning
    for IoT*, shown as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要注意的第二个区别是在损失的定义中，我们在这里加入了`l2`正则化项，以减少过拟合，如在[第3章](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml)《物联网的机器学习》中讨论的那样，具体如下：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After reading the data from the `csv` file and separating it into training
    and validation like before, we define the `MLP` class object with `4` neurons
    in the input layer, `15` neurons in the hidden layer, and `1` neuron in the output
    layer:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在从`csv`文件中读取数据并像之前一样将其分为训练集和验证集后，我们定义了一个`MLP`类对象，输入层有`4`个神经元，隐藏层有`15`个神经元，输出层有`1`个神经元：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the following code we train the model on training dataset for `6000` epochs:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将在训练数据集上训练模型`6000`个周期：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This trained network gives us an MSE of 0.016 and an *R²* value of 0.67\. Both
    are better than what we obtained from a single neuron, and comparable to the ML
    methods we studied in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*. The complete code can be accessed in the file named
    `MLP_regresssion.ipynb`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练好的网络给出了0.016的均方误差（MSE）和0.67的*R²*值。两者都优于我们从单个神经元得到的结果，并且与我们在[第3章](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml)《物联网的机器学习》中研究的机器学习方法相当。完整代码可以在名为`MLP_regresssion.ipynb`的文件中找到。
- en: 'You can play around with hyperparameters namely: the number of hidden neurons,
    the activation functions, the learning rate, the optimizer, and the regularization
    coefficient, and can obtain even better results.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调整超参数，包括：隐藏神经元的数量、激活函数、学习率、优化器和正则化系数，并能获得更好的结果。
- en: Wine quality classification using MLPs in TensorFlow
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLP在TensorFlow中进行葡萄酒质量分类
- en: MLP can be used to do classification tasks as well. We can reuse the MLP class
    from the previous section with minor modifications to perform the task of classification.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: MLP也可以用于分类任务。我们可以在之前的基础上对MLP类进行少量修改来执行分类任务。
- en: 'We will need to make the following two major changes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做以下两个主要的修改：
- en: The target in the case of classification will be one-hot encoded
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分类任务中，目标将会是 one-hot 编码的。
- en: 'The loss function will now be categorical cross-entropy loss: `tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat,
    labels=self.y))`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数现在将是类别交叉熵损失：`tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_hat,
    labels=self.y))`
- en: 'So let''s now see the complete code, which is also available at GitHub in the
    file `MLP_classification`. We will be classifying the red wine quality, to make
    it convenient, we use only two wine classes:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看完整的代码，这个代码也可以在 GitHub 上的 `MLP_classification` 文件中找到。我们将对红酒质量进行分类，为了方便起见，我们只使用两类红酒：
- en: 'We import the necessary modules namely: TensorFlow, Numpy, Matplotlib, and
    certain functions from scikit-learn, as shown in the following code:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了必要的模块，即：TensorFlow、Numpy、Matplotlib，以及来自 scikit-learn 的某些函数，代码如下所示：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We defined our `MLP` class, it''s very similar to the `MLP` class you saw earlier,
    the only difference is in the definition of the loss function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了 `MLP` 类，它与您之前看到的 `MLP` 类非常相似，唯一的不同是在损失函数的定义上：
- en: '[PRE15]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we read the data, normalize it, and preprocess it so that wine quality
    is one-hot encoded with two labels. We also divide the data into training and
    validation set, shown as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们读取数据，对其进行标准化和预处理，以便将红酒质量进行 one-hot 编码，并使用两个标签。我们还将数据划分为训练集和验证集，如下所示：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We define an `MLP` object and train it, demonstrated in the following code:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义一个`MLP`对象并对其进行训练，代码如下所示：
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following, you can see the results of training, the cross-entropy loss decreases
    as the network learns:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以看到训练结果，随着网络的学习，交叉熵损失逐渐减少：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/83e8116c-41c8-4e3e-a3c8-d1fb3a3f30cb.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83e8116c-41c8-4e3e-a3c8-d1fb3a3f30cb.png)'
- en: 'The trained network, when tested on the validation dataset, provides an accuracy
    of 77.8%. The `confusion_matrix` on the validation set is shown as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证数据集上测试训练后的网络，准确率为77.8%。验证集上的`confusion_matrix`如下所示：
- en: '[PRE19]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/a480aca2-4bdf-4a12-96cd-1916ece2f3aa.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a480aca2-4bdf-4a12-96cd-1916ece2f3aa.png)'
- en: These results are again comparable to the results we obtained using ML algorithms.
    We can make it even better by playing around with the hyperparameters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与我们使用 ML 算法获得的结果相当。通过调整超参数，我们可以进一步改善结果。
- en: Convolutional neural networks
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: MLPs were fun, but as you must have observed while playing with MLP codes in
    the previous section, the time to learn increases as the complexity of input space
    increases; moreover, the performance of MLPs is just second to the ML algorithms.
    Whatever you can do with MLP, there's a high probability you can do it slightly
    better using ML algorithms you learned in [Chapter 3](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml),
    *Machine Learning for IoT*. Precisely for this reason, despite backpropagation
    algorithm being available in the 1980s, we observed the second AI winter roughly
    from 1987 to 1993.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 很有趣，但正如您在上一节中使用 MLP 代码时所观察到的那样，随着输入空间复杂度的增加，学习时间也会增加；此外，MLP 的表现仅次于 ML 算法。无论您能在
    MLP 中做什么，使用[第三章](09538353-bf5b-4035-8b98-cc131bcfcf24.xhtml)《物联网的机器学习》中的 ML 算法，您都有很大可能做得更好。正因如此，尽管反向传播算法早在1980年代就已出现，但我们还是经历了第二次人工智能寒冬，大约从1987年到1993年。
- en: This all changed, and the neural networks stopped playing the second fiddle
    to ML algorithms, in the 2010s with the development of deep neural networks. Today
    DL has achieved human level or more than human level performance in varied tasks
    of computer vision like recognizing traffic signals ([http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)),
    faces ([https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf)),
    handwritten digits, ([https://cs.nyu.edu/~wanli/dropc/dropc.pdf](https://cs.nyu.edu/~wanli/dropc/dropc.pdf))
    and so on. The list is continuously growing.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切发生了变化，神经网络不再在 2010 年代的深度神经网络发展中屈居 ML 算法的“配角”。如今，深度学习在计算机视觉的多种任务中，像是识别交通信号（[http://people.idsia.ch/~juergen/cvpr2012.pdf](http://people.idsia.ch/~juergen/cvpr2012.pdf)）、人脸（[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf)）、手写数字（[https://cs.nyu.edu/~wanli/dropc/dropc.pdf](https://cs.nyu.edu/~wanli/dropc/dropc.pdf)）等，已经达到了人类水平或超越人类水平的表现。这个名单还在不断增长。
- en: CNN has been a major part of this success story. In this section, you will learn
    about CNN, the maths behind CNN, and some of the popular CNN architectures.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是这个成功故事的重要组成部分。在本节中，您将学习CNN、CNN背后的数学原理以及一些流行的CNN架构。
- en: Different layers of CNN
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的不同层
- en: 'CNN consists of three main types of neuron layers: convolution layers, pooling
    layers, and fully connected layers. Fully connected layers are nothing but layers
    of MLP, they are always the last few layers of the CNN, and perform the final
    task of classification or regression. Let''s see how the convolution layer and
    max pooling layers work.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由三种主要类型的神经元层组成：卷积层、池化层和全连接层。全连接层就是MLP层，它们总是CNN的最后几层，执行分类或回归的最终任务。让我们看看卷积层和最大池化层是如何工作的。
- en: The convolution layer
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: This is the core building block of CNNs. It performs the mathematical operation
    similar to convolution (cross-correlation to be precise) on its input, normally
    a 3D image. It's defined by kernels (filters). The basic idea is that these filters
    stride through the entire image and extract specific features from the image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是CNN的核心构建模块。它对输入数据（通常是3D图像）执行类似卷积的数学操作（更精确地说是交叉相关）。它由卷积核（滤波器）定义。基本的思路是，这些滤波器遍历整个图像，并从中提取特定的特征。
- en: 'Before going into further details, let''s first see the convolution operation
    on a two-dimensional matrix for simplicity. The following diagram shows the operation
    when one pixel placed at position [2, 2] of a 5×5 **2D image** matrix is convolved
    with a 3×3 filter:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入细节之前，首先让我们看一下卷积操作在二维矩阵上的应用，简化问题。以下图示展示了当一个像素位于5×5**二维图像**矩阵的[2, 2]位置，并与3×3滤波器进行卷积时的操作：
- en: '![](img/73305444-5f38-4e55-b2e1-d901a0aacfc8.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73305444-5f38-4e55-b2e1-d901a0aacfc8.png)'
- en: Convolution operation at a single pixel
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 单像素上的卷积操作
- en: The convolution operation involves placing the filter with the pixel at the
    center, then performing element-wise multiplication between the filter elements
    and the pixel, along with its neighbors. Finally, summing the product. Since convolution
    operation is performed on a pixel, the filters are conventionally odd-sized like
    5×5, 3×3, or 7×7, and so on. The size of the filters specify how much neighboring
    area it's covering.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作包括将滤波器放置在像素的中心位置，然后对滤波器元素与该像素及其邻居之间进行逐元素相乘，并最终求和。由于卷积操作是针对单个像素进行的，因此滤波器通常是奇数大小，如5×5、3×3或7×7等。滤波器的大小指定了它覆盖的邻域范围。
- en: 'The important parameters when designing the convolution layers are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 设计卷积层时的重要参数如下：
- en: The size of the filters (k×k).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器的大小（k×k）。
- en: The number of filters in the layer, also called **channels**. The input color
    image is present in the three RGB channels. The number of channels are conventionally
    increased in the higher layers. Resulting in deeper information in higher layers.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层中的滤波器数量，也称为**通道**。输入的彩色图像包含三个RGB通道。通道的数量通常在更高的层次中增加，从而使得高层具有更深的信息。
- en: The number of pixels the filter strides (s) through the image. Conventionally,
    the stride is of one pixel so that the filter covers the entire image starting
    from top-left to bottom-right.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器在图像中滑动的像素数（s）。通常，步幅为一个像素，以便滤波器从左上角到右下角覆盖整个图像。
- en: 'The padding to be used while convolving. Traditionally, there are two options,
    either valid or same. In **valid** padding, there''s no padding at all, and thus
    the size of the convolved image is less than that of the original. In **same**,
    the padding of zeros is done around the boundary pixels, so that the size of the
    convolved image is the same as that of the original image. The following screenshot
    shows the complete **Convolved Image**. The green square of size 3×3 is the result
    when padding is valid, the complete 5×5 matrix on the right will be the result
    when padding is the same:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积时使用的填充。传统上，有两种选项，分别是 valid 或 same。在**valid**填充中，完全没有填充，因此卷积后的图像大小比原图小。在**same**填充中，围绕边界像素进行零填充，以确保卷积后图像的大小与原图相同。以下截图展示了完整的**卷积图像**。当使用
    valid 填充时，绿色的3×3方框是结果，右侧的完整5×5矩阵是使用 same 填充时的结果：
- en: '![](img/180e138e-d181-4379-9469-50ec5fddde35.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/180e138e-d181-4379-9469-50ec5fddde35.png)'
- en: Convolution operation applied on a 5×5 image
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在5×5图像上应用的卷积操作
- en: The green square on the right will be the result of **valid** padding. For the
    **same** padding, we will get the complete 5×5 matrix shown on the right-hand
    side.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的绿色方块将是**有效**填充的结果。对于**相同**填充，我们将得到右侧显示的完整5×5矩阵。
- en: Pooling layer
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: The convolution layer is followed conventionally by a pooling layer. The purpose
    of the pooling layer is to progressively reduce the size of the representation,
    and thus, reduce the number of parameters and computations in the network. Thus,
    it down samples the information as it propagates through the network in feed forward
    manner.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层后面通常跟着一个池化层。池化层的目的是逐步减小表示的大小，从而减少网络中的参数和计算量。因此，它以前馈的方式对信息进行下采样，随着信息在网络中的传播。
- en: 'Here again, we have a filter, traditionally people prefer a filter of size
    2×2, and it moves with a stride of two pixels in both directions. The pooling
    process replaces the four elements under the 2×2 filter by either the maximum
    value of the four (**Max Pooling**) or the average value of the four (**Average
    Pooling**). In the following diagram, you can see the result of pooling operation
    on a **2D single channel slice of an image**:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们再次使用一个滤波器，传统上人们偏好使用大小为2×2的滤波器，并且它在两个方向上以两个像素的步长进行移动。池化过程将2×2滤波器下的四个元素替换为四个中的最大值（**最大池化**）或四个中的平均值（**平均池化**）。在下图中，你可以看到池化操作对**二维单通道图像切片**的结果：
- en: '![](img/d703e932-41f1-4053-bd23-c56664cc31db.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d703e932-41f1-4053-bd23-c56664cc31db.png)'
- en: Max pooling and average pooling operation on a two-dimensional single depth
    slice of an image
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的二维单通道切片上的最大池化和平均池化操作
- en: 'Multiple convolution pooling layers are stacked together to form a deep CNN.
    As the image propagates through the CNN, each convolutional layer extracts specific
    features. The lower layers extract the gross feature like shape, curves, lines,
    and so on, while the higher layers extract more abstract features like eyes, lips,
    and so on. The image, as it propagates through the network, reduces in dimensions,
    but increases in depth. The output from the last convolutional layer is flattened
    and passed to fully connected layers, as shown in the following diagram:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多个卷积池化层被堆叠在一起形成深度CNN。当图像通过CNN传播时，每个卷积层提取特定的特征。较低的层提取粗略的特征，如形状、曲线、线条等，而较高的层提取更抽象的特征，如眼睛、嘴唇等。随着图像在网络中的传播，尺寸逐渐减小，但深度逐渐增加。最后一层卷积层的输出被展平并传递到全连接层，如下图所示：
- en: '![](img/c80f02d4-6331-41f7-9fbb-df3ea4545697.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c80f02d4-6331-41f7-9fbb-df3ea4545697.png)'
- en: The basic architecture of a CNN network
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: CNN网络的基本架构
- en: The values of filter matrix are also called **weights** and they are shared
    by the whole image. This sharing reduces the number of training parameters. The
    weights are learned by the network using the backpropagation algorithm. Since
    we will be using the auto-differentiation feature of TensorFlow, we are not calculating
    the exact expression for weight update for convolution layers.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器矩阵的值也称为**权重**，并且它们在整个图像中是共享的。这种共享减少了训练参数的数量。权重通过反向传播算法由网络学习。由于我们将使用TensorFlow的自动微分功能，因此我们不需要计算卷积层的权重更新的精确表达式。
- en: Some popular CNN model
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些常见的CNN模型
- en: 'The following is a list of some of the popular CNN models available:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的CNN模型列表：
- en: '**LeNet**: LeNet was the first successful CNN applied to recognize handwritten
    digits. It was developed by Yann LeCun in the 1990s. You can know more about LeNet
    architecture and its related publications at Yann LeCun''s home page ([http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LeNet**：LeNet是第一个成功应用于识别手写数字的CNN。它由Yann LeCun在1990年代开发。你可以在Yann LeCun的主页上了解更多关于LeNet架构及相关出版物的信息（[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)）。'
- en: '**VGGNet**: This was the runner-up in ILSVRC 2014, developed by Karen Simonyan
    and Andrew Zisserman. Its first version contains 16 Convolution+FC layers and
    was called **VGG16**, later they brought VGG19 with 19 layers. The details about
    its performance and publications can be accessed from the University of Oxford
    site ([http://www.robots.ox.ac.uk/~vgg/research/very_deep/](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VGGNet**：这是ILSVRC 2014的亚军，由Karen Simonyan和Andrew Zisserman开发。它的第一个版本包含16个卷积+全连接层，被称为**VGG16**，后来他们推出了包含19层的VGG19。有关其性能和出版物的详细信息可以从牛津大学网站访问（[http://www.robots.ox.ac.uk/~vgg/research/very_deep/](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)）。'
- en: '**ResNet**: Developed by Kaiming He et al., ResNet was the winner of ILSVRC
    2015\. It made use of new feature called **residual learning** and **batch normalization**.
    It''s a very deep network with more than 100 layers. It''s known that adding more
    layers will improve the performance, but adding layers also introduced the problem
    of vanishing gradients. ResNet solved this issue by making use of identity shortcut
    connection, where the signal skips one or more layers. You can read the original
    paper for more information ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ResNet**：由Kaiming He等人开发，ResNet是ILSVRC 2015年的获胜者。它使用了一个新的特性——**残差学习**和**批量归一化**。它是一个非常深的网络，层数超过100层。众所周知，增加更多的层可以提高性能，但增加层数也会引入梯度消失问题。ResNet通过使用身份快捷连接解决了这个问题，在这种连接中，信号跳过一个或多个层。你可以阅读原始论文以了解更多信息（[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)）。'
- en: '**GoogleNet**: This was the winning architecture of ILSVRC 2014\. It has 22
    layers, and introduced the idea of inception layer. The basic idea is to cover
    a bigger area, while at the same time, keep a fine resolution for small information
    on the images. As a result instead of one size filters, at each layer, we have
    filter ranging from 1×1 (for fine detailing) to 5×5\. The result of all the filters
    are concatenated and passed to next layer, the process is repeated in the next
    inception layer.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GoogleNet**：这是ILSVRC 2014年的获胜架构。它有22层，并引入了inception层的概念。基本思路是覆盖更大的区域，同时保持对图像上小信息的高分辨率。因此，在每一层中，我们使用从1×1（用于精细细节）到5×5的不同大小的滤波器，而不是单一大小的滤波器。所有滤波器的结果将连接在一起并传递到下一层，这一过程会在下一个inception层中重复。'
- en: LeNet to recognize handwritten digits
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LeNet识别手写数字
- en: In the chapters ahead, we will be using some of these popular CNNs and their
    variants to solve image and video processing tasks. Right now, let's use the LeNet
    architecture proposed by Yann LeCun to recognize handwritten digits. This architecture
    was used by the US Postal Service to recognize handwritten ZIP codes on the letters
    they received ([http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf](http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用一些流行的CNN及其变体来解决图像和视频处理任务。现在，让我们使用Yann LeCun提出的LeNet架构来识别手写数字。这个架构曾被美国邮政服务用于识别他们收到的信件上的手写邮政编码（[http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf](http://yann.lecun.com/exdb/publis/pdf/jackel-95.pdf)）。
- en: 'LeNet consists of five layers with two convolutional max pool layers and three
    fully connected layers. The network also uses dropout feature, that is while training,
    some of the weights are turned off. This forces the other interconnections to
    compensate for them, and hence helps in overcoming overfitting:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet包含五个层，其中有两个卷积最大池化层和三个全连接层。该网络还使用了dropout特性，即在训练时，某些权重会被关闭。这迫使其他连接来补偿这些权重，从而有助于克服过拟合：
- en: We import the necessary modules, shown as follows
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的模块，如下所示：
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we create the class object `LeNet`, which will have the necessary CNN
    architecture and modules to train and make the prediction. In the `__init__` method,
    we define all the needed placeholders to hold input images and their output labels.
    We also define the loss, since this is a classification problem, we use cross-entropy
    loss, as shown in the following code:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`LeNet`类对象，它将包含必要的CNN架构和模块来进行训练和预测。在`__init__`方法中，我们定义了所有需要的占位符来保存输入图像及其输出标签。我们还定义了损失函数，由于这是一个分类问题，我们使用交叉熵损失，如以下代码所示：
- en: '[PRE21]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `model` method is the one where the convolutional network architecture
    graph is actually build. We use the TensorFlow `tf.nn.conv2d` function to build
    the convolutional layers. The function takes an argument the filter matrix defined
    as weights and computes the convolution between the input and the filter matrix.
    We also use biases to give us a high degree of freedom. After the two convolution
    layers, we flatten the output and pass it to the fully connected layers, shown
    as follows:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model`方法是实际构建卷积网络架构图的方法。我们使用TensorFlow的`tf.nn.conv2d`函数来构建卷积层。该函数接受一个参数，即定义为权重的滤波器矩阵，并计算输入与滤波器矩阵之间的卷积。我们还使用偏置来提供更大的自由度。在两个卷积层之后，我们将输出展平并传递到全连接层，如下所示：'
- en: '[PRE22]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `fit` method performs the batch-wise training, and `predict` method provides
    the output for given input, as shown in the following code:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit`方法执行按批次的训练，而`predict`方法为给定输入提供输出，如下所示：'
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We use the handwritten digits dataset and download it from Kaggle ([https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data)).
    The dataset is available in `.csv` format. We load the `.csv` files and preprocess
    the data. The following are the sample training diagrams:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用手写数字数据集，并从 Kaggle 下载（[https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data)）。数据集以
    `.csv` 格式提供。我们加载 `.csv` 文件并对数据进行预处理。以下是样本训练图：
- en: '[PRE24]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/1c22399b-ebc8-4748-a2d9-961e33840c5f.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c22399b-ebc8-4748-a2d9-961e33840c5f.png)'
- en: 'Here we will be training the model:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将训练模型：
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We create the `LeNet` object and train it on the training data. The obtain
    is 99.658% on the training dataset and 98.607% on the validation dataset:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建 `LeNet` 对象并在训练数据上进行训练。训练数据集的准确率为 99.658%，验证数据集的准确率为 98.607%：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Impressive! You can predict the output for the test dataset and make a submission
    at Kaggle.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 印象深刻！你可以预测测试数据集的输出并在 Kaggle 上提交。
- en: Recurrent neural networks
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: The models that we have studied till now respond only present input. You present
    them an input, and based on what they have learned, they give you a corresponding
    output. But this is not the way we humans work. When you are reading a sentence,
    you do not interpret each word individually, you take the previous words into
    account to conclude its semantic
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们学习的模型仅响应当前输入。你给它一个输入，基于它所学的知识，它会给出相应的输出。但这并不是我们人类的工作方式。当你读一个句子时，你不会单独理解每个词，而是会考虑前面的词汇来推断它的语义。
- en: meaning.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 意思是。
- en: 'RNNs are able to address this issue. They use the feedback loops, which preserves
    the information. The feedback loop allows the information to be passed from the
    previous steps to the present. The following diagram shows the basic architecture
    of an RNN and how the feedback allows the passing of information from one step
    of the network to the next (**Unroll**):'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 能够解决这个问题。它们利用反馈回路来保留信息。反馈回路允许信息从前一个步骤传递到当前步骤。下图展示了 RNN 的基本架构，以及反馈如何使信息从网络的一个步骤传递到下一个步骤（**展开**）：
- en: '![](img/3053a864-2e59-4b74-af9e-b0baa5861d37.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3053a864-2e59-4b74-af9e-b0baa5861d37.png)'
- en: Recurrent neural network
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'In the preceding diagram, *X* represents the inputs. It''s connected to the
    neurons in the hidden layer by weights *W[hx]*, the output of the hidden layer,
    *h*, is fed back to the hidden layer via weights *W[hh]*, and also contributes
    to the output, *O,* via weights *W[yh]*. We can write the mathematical relationships
    as the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，*X* 代表输入。它通过权重 *W[hx]* 连接到隐藏层的神经元，隐藏层的输出 *h* 通过权重 *W[hh]* 反馈到隐藏层，同时也通过权重
    *W[yh]* 影响输出 *O*。我们可以将这些数学关系写成如下形式：
- en: '![](img/08a55795-65bb-469e-9035-07107aa07d3d.png)![](img/2dfa68b4-ad21-402e-ad0a-dc306c307622.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08a55795-65bb-469e-9035-07107aa07d3d.png)![](img/2dfa68b4-ad21-402e-ad0a-dc306c307622.png)'
- en: Where *g* is the activation function, *b[h]* and *b[y]* are the biases of hidden
    and output neurons, respectively. In the a preceding relation all *X*, *h*, and
    *O* are vectors; *W[hx]*, *W[hh],* and *W[yh]* are matrices. The dimensions of
    the input *X* and the output *O* depends upon the dataset you are working on,
    and the number of units in hidden layer *h* are decided by you; you will find
    many papers where researchers have used 128 number of hidden units. The preceding
    architecture shows only one hidden layer, but we can have as many hidden layers
    as we want. RNNs have been applied in the field of natural language processing,
    they have also been applied to analyze the time series data, like stock prices.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *g* 是激活函数，*b[h]* 和 *b[y]* 分别是隐藏层和输出层神经元的偏置项。在前述关系中，*X*、*h* 和 *O* 都是向量；*W[hx]*、*W[hh]*
    和 *W[yh]* 都是矩阵。输入 *X* 和输出 *O* 的维度取决于你使用的数据集，隐藏层 *h* 的单元数由你决定；你会发现许多论文中，研究者使用了
    128 个隐藏单元。前述架构只展示了一个隐藏层，但我们可以根据需要添加任意数量的隐藏层。RNN 已被应用于自然语言处理领域，也用于分析时间序列数据，比如股价。
- en: 'RNNs learn via an algorithm called as **backpropagation** **through time**
    (**BPTT**), it''s a modification of backpropagation algorithm that takes into
    account the time series nature of data. Here, the loss is defined as the sum of
    all the loss functions at times *t*=*1* to *t*=*T* (number of time steps to be
    unrolled), for example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通过一种叫做 **时间反向传播** (**BPTT**) 的算法进行学习，这是一种反向传播算法的修改版，考虑了数据的时间序列特性。这里，损失函数被定义为从
    *t*=*1* 到 *t*=*T*（要展开的时间步数）的所有损失函数之和，例如：
- en: '![](img/5edc1524-7b79-44e4-8ef6-097685d2548d.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5edc1524-7b79-44e4-8ef6-097685d2548d.png)'
- en: Where *L^((t))* is the loss at time *t*, we apply the chain rule of differentiation
    like before, and derive the weight updates for weights *W[hx]*, *W[hh]**,* and
    *W[yh.]*
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*L^((t))*是时间*t*的损失，我们像之前一样应用链式法则进行微分，并推导出权重*W[hx]*、*W[hh]*和*W[yh]*的更新。
- en: 'We are not deriving the expression for weight updates in this book, because
    we will not be coding it. TensorFlow provides an implementation for RNN and BPTT.
    But for the readers interested in going into the mathematical details, following
    are some references:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不推导权重更新的表达式，因为我们不会实现代码。TensorFlow提供了RNN和BPTT的实现。但是对于那些有兴趣深入数学细节的读者，以下是一些参考文献：
- en: '*On the difficulty of training Recurrent Neural Networks,* Razvan Pascanu,
    Tomas Mikolov, and Yoshua Bengio ([https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf))'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于训练递归神经网络的困难*，Razvan Pascanu、Tomas Mikolov 和 Yoshua Bengio（[https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)）'
- en: '*Learning Long-Term Dependencies with Gradient Descent is Difficult*, Yoshua
    Bengio, Patrice Simard, and Paolo Frasconi ([www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf))'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习长期依赖关系与梯度下降的困难*，Yoshua Bengio、Patrice Simard 和 Paolo Frasconi（[www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/ieeetrnn94.pdf)）'
- en: Also, it will be incomplete not to mention Colah's blog ([http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/))
    and Andrej Karpathy's blog ([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))
    for an excellent explanation of RNNs and some of their cool applications
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，不提及Colah的博客（[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)）和Andrej
    Karpathy的博客（[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)）对于RNN及其一些酷应用的精彩解释，实在是不完整的。
- en: We present the RNN with one input each timestep and predict the corresponding
    output. BPTT works by unrolling all input timesteps. The errors are calculated
    and accumulated for each timestep, later the network is rolled back to update
    the weights. One of the disadvantages of BPTT is that when the number of time
    steps increases, the computation also increases. This makes the overall model
    computationally expensive. Moreover, due to multiple gradient multiplications,
    the network is prone to the vanishing gradient problem.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们呈现了每个时间步有一个输入的RNN，并预测相应的输出。BPTT通过展开所有输入时间步来工作。误差在每个时间步计算并积累，之后网络会回滚以更新权重。BPTT的一个缺点是当时间步数增加时，计算量也会增加，这使得整个模型的计算代价较高。此外，由于多次梯度乘法，网络容易出现梯度消失问题。
- en: To solve this issue, a modified version of BPTT, the truncated-BPTT is often
    used. In the truncated-BPTT, the data is processed one timestep at a time and
    the BPTT weight update is performed periodically for a fixed number of time steps.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，通常使用BPTT的修改版——截断BPTT。在截断BPTT中，数据一次处理一个时间步，BPTT的权重更新定期在固定的时间步数内执行。
- en: 'We can enumerate the steps of the truncated-BPTT algorithm as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下步骤列举截断BPTT算法：
- en: Present the sequence of *K[1]* time steps of input and output pairs to the network
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向网络呈现*K[1]*时间步的输入和输出对的序列
- en: Calculate and accumulate the errors across *K[2]* time steps by unrolling the
    network
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过展开网络，计算并积累*K[2]*时间步的误差
- en: Update the weights by rolling up the network
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过回滚网络来更新权重
- en: The performance of the algorithm depends on two hyperparameters *K[1]* and *K[2]*.
    The number of forwarding pass timesteps between updates is represented by *K[1]*,
    it affects how fast or slow the training will be training and the frequency of
    the weight updates. *K[2]* on the other hand, represents the number of timesteps
    that apply to BPTT, it should be large enough to capture the temporal structure
    of the input data.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的表现取决于两个超参数*K[1]*和*K[2]*。更新之间的前向传递时间步数由*K[1]*表示，它影响训练的速度和权重更新的频率。而*K[2]*则表示应用于BPTT的时间步数，它应该足够大，以捕捉输入数据的时间结构。
- en: Long short-term memory
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: Hochreiter and Schmidhuber in 1997 proposed a modified RNN model, called the
    **long short-term memory** (**LSTM**) as a solution to overcome the vanishing
    gradient problem. The hidden layer in the RNNs is replaced by an LSTM cell.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber 在 1997 年提出了一种修改版的 RNN 模型，称为 **长短期记忆** (**LSTM**)，用以解决梯度消失问题。RNN
    中的隐藏层被 LSTM 单元取代。
- en: 'The LSTM cell consists of three gates: forget gate, input gate, and the output
    gate. These gates control the amount of long-term memory and the short-term memory
    generated and retained by the cell. The gates all have the `sigmoid` function,
    which squashes the input between *0* and *1*. Following, we see how the outputs
    from various gates are calculated, in case the expressions seem daunting to you,
    do not worry, we will be using the TensorFlow `tf.contrib.rnn.BasicLSTMCell` and
    `tf.contrib.rnn.static_rnn` to implement the LSTM cell, shown in the following
    diagram:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元由三个门组成：忘记门、输入门和输出门。这些门控制着单元生成和保持的长期记忆和短期记忆的量。这些门都使用 `sigmoid` 函数，将输入压缩到
    *0* 和 *1* 之间。接下来，我们将看到各种门的输出是如何计算的。如果这些表达式看起来有些难以理解，不用担心，我们将使用 TensorFlow 中的 `tf.contrib.rnn.BasicLSTMCell`
    和 `tf.contrib.rnn.static_rnn` 来实现 LSTM 单元，见下图：
- en: '![](img/200afd47-761e-48f2-b1f0-96ae371de2c6.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/200afd47-761e-48f2-b1f0-96ae371de2c6.png)'
- en: The basic LSTM cell, *x* is the input to the cell, *h* the short-term memory
    and *c* the long-term memory. The subscript refers to the time
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 LSTM 单元，*x* 是输入，*h* 是短期记忆，*c* 是长期记忆。下标表示时间步。
- en: 'At each time step, *t*, the LSTM cell takes three inputs: the input *x[t]*,
    the short-term memory *h*[*t-1*], and the long-term memory *c[t-1]*, and outputs
    the long-term memory *c[t]* at and short-term memory *h[t]*. The subscript to
    *x*, *h,* and *c* refer to the timestep.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步 *t*，LSTM 单元接收三个输入：输入 *x[t]*、短期记忆 *h*[*t-1*] 和长期记忆 *c[t-1]*，并输出长期记忆 *c[t]*
    和短期记忆 *h[t]*。*x*、*h* 和 *c* 的下标表示时间步。
- en: 'The **Forget Gate** *f*(.) controls the amount of short-term memory, *h*, to
    be remembered for further flow in the present time step. Mathematically we can
    represent Forget Gate *f(.)* as:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**忘记门** *f*(.) 控制当前时间步中要记住的短期记忆 *h* 的量，以便在后续步骤中继续流动。从数学角度，我们可以表示忘记门 *f(.)*
    如下：'
- en: '![](img/ca432207-ce53-40bd-8444-41504c254c53.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca432207-ce53-40bd-8444-41504c254c53.png)'
- en: Where σ represents the sigmoid activation function, *W[fx]* and *W[fh]* are
    the weights controlling the influence of input *x[t],* short-term memory *h*[*t*-1],
    and *b[f]* the bias of the forget gate.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 σ 代表 sigmoid 激活函数，*W[fx]* 和 *W[fh]* 是控制输入 *x[t]*、短期记忆 *h*[*t*-1] 和 *b[f]*
    忘记门偏置的权重。
- en: 'The **Input Gate** *i*(.) controls the amount of input and working memory influencing
    the output of the cell. We can express it as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入门** *i*(.) 控制输入和工作记忆对单元输出的影响。我们可以将其表示如下：'
- en: '![](img/1dacd13e-4362-4ae5-ae0a-2fc1ddd995d8.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1dacd13e-4362-4ae5-ae0a-2fc1ddd995d8.png)'
- en: 'The **Output Gate** *o*(.) controls the amount of information that''s used
    for updating the short-term memory, and given by the following:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出门** *o*(.) 控制用于更新短期记忆的信息量，表达式如下：'
- en: '![](img/4a136d33-ba2b-4d30-9184-10a4e4258714.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a136d33-ba2b-4d30-9184-10a4e4258714.png)'
- en: 'Beside these three gates, the LSTM cell also computes the candidate hidden
    state ![](img/9e3aa387-e3c4-413f-a38a-1ffc05da350b.png) , which along with the
    input and forget gate, is used to compute the amount of long term memory *c[t]*:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三个门，LSTM 单元还计算候选隐藏状态 ![](img/9e3aa387-e3c4-413f-a38a-1ffc05da350b.png)，它与输入门和忘记门一起用于计算长期记忆
    *c[t]* 的量：
- en: '![](img/b0d751d8-781b-4a9c-a4a7-e4a5707b2960.png)![](img/55ff027e-d862-415c-b288-e72b31beaf81.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0d751d8-781b-4a9c-a4a7-e4a5707b2960.png)![](img/55ff027e-d862-415c-b288-e72b31beaf81.png)'
- en: 'The circle represents the element wise multiplication. The new value of the
    short-term memory is then computed as the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 圆圈表示逐元素相乘。短期记忆的新值接下来按如下方式计算：
- en: '![](img/e3f46d9b-122e-4d86-a254-2be3800cd613.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3f46d9b-122e-4d86-a254-2be3800cd613.png)'
- en: 'Let''s now see how we can implement LSTM in TensorFlow in the following steps:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看如何在 TensorFlow 中实现 LSTM，按以下步骤操作：
- en: 'We are using the following modules:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下模块：
- en: '[PRE27]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We define a class LSTM where we construct the graph and define the LSTM layer
    with the help of TensorFlow `contrib`. To take care of memory, we first clear
    the default graph stack and reset the global default graph using `tf.reset_default_graph()`.
    The input goes directly to the LSTM layer with `num_units` number of hidden units.
    It''s followed by a fully connected output layer with the `out_weights` weights
    and `out_bias` bias. Create the placeholders for input `self.x` and `self.y` label.
    The input is reshaped and fed to the LSTM cell. To create the LSTM layer, we first
    define the LSTM cell with `num_units` hidden units and forget bias set to `1.0`.
    This adds the biases to the forget gate in order to reduce the scale of forgetting
    in the beginning of the training. Reshape the output from the LSTM layer and feed
    it to the fully connected layer, shown as follows:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个LSTM类，在其中构建了图并使用TensorFlow的`contrib`定义了LSTM层。为了处理内存，我们首先清除默认的图堆栈并使用`tf.reset_default_graph()`重置全局默认图。输入直接传递到具有`num_units`个隐藏单元的LSTM层。随后是一个带有`out_weights`权重和`out_bias`偏置的全连接输出层。创建输入`self.x`和标签`self.y`的占位符。输入被重塑并馈送到LSTM单元。为了创建LSTM层，我们首先定义具有`num_units`隐藏单元和遗忘偏置设置为`1.0`的LSTM单元。这会在遗忘门中添加偏置，以减少训练开始时的遗忘规模。将来自LSTM层的输出重塑并馈送到全连接层，如下所示：
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We create the methods to train and predict, as shown in the following code:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了训练和预测的方法，如下所示的代码：
- en: '[PRE29]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the coming chapters, we will be using the RNN for handling time series production
    and text processing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将使用RNN处理时间序列生成和文本处理。
- en: Gated recurrent unit
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: '**Gated recurrent unit** (**GRU**) is another modification of RNN. It has a
    simplified architecture compared to LSTM and overcomes the vanishing gradient
    problem. It takes only two inputs, the input *x[t]* at time *t* and memory *h[t-1]*
    from time *t*-1\. There are only two gates, **Update G****ate** and **Reset Gate**,
    shown in the following diagram:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRU**）是对RNN的另一种修改。它相对于LSTM有简化的架构，并且解决了梯度消失问题。它只接受两个输入，时间*t*处的输入*x[t]*和来自时间*t*-1处的记忆*h[t-1]*。它有两个门，**更新门**和**重置门**，如下图所示：'
- en: '![](img/914f4d89-33d5-4db6-950b-ba58385b38a9.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/914f4d89-33d5-4db6-950b-ba58385b38a9.png)'
- en: The architecture of a basic GRU cell
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的GRU单元的结构
- en: 'The update gate controls how much previous memory to keep, and the reset gate
    determines how to combine the new input with previous memory. We can define the
    complete GRU cell by the following four equations:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门控制要保留多少以前的记忆，重置门确定如何将新输入与以前的记忆结合。我们可以通过以下四个方程定义完整的GRU单元：
- en: '![](img/ae7dba9c-7ee0-4ff2-ada7-42a0a0eec746.png)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/ae7dba9c-7ee0-4ff2-ada7-42a0a0eec746.png)'
- en: '![](img/2acac066-bd31-4b34-b736-ba63b17fc36c.png)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/2acac066-bd31-4b34-b736-ba63b17fc36c.png)'
- en: '![](img/ec5aa1a8-224d-4ed5-98c7-27ffa72ed61a.png)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/ec5aa1a8-224d-4ed5-98c7-27ffa72ed61a.png)'
- en: '![](img/37499e4d-e92a-404a-b41c-f4951d582cee.png)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/37499e4d-e92a-404a-b41c-f4951d582cee.png)'
- en: Both GRU and LSTM give a comparable performance, but GRU has fewer training
    parameters.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GRU和LSTM表现相当，但GRU的训练参数更少。
- en: Autoencoders
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: The models we have learned up to now were learning using supervised learning.
    In this section, we will learn about autoencoders. They are feedforward, non-recurrent
    neural network, and learn through unsupervised learning. They are the latest buzz,
    along with generative adversarial networks, and we can find applications in image
    reconstruction, clustering, machine translation, and much more. They were initially
    proposed in the 1980s by Geoffrey E. Hinton and the PDP group ([http://www.cs.toronto.edu/~fritz/absps/clp.pdf](http://www.cs.toronto.edu/~fritz/absps/clp.pdf)).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止学习的模型都是使用监督学习来学习的。在这一节中，我们将学习自编码器。它们是前馈非递归神经网络，通过无监督学习进行学习。它们和生成对抗网络一样是最新的热点，我们可以在图像重建、聚类、机器翻译等领域找到应用。最初由Geoffrey
    E. Hinton和PDP小组在1980年代提出（[http://www.cs.toronto.edu/~fritz/absps/clp.pdf](http://www.cs.toronto.edu/~fritz/absps/clp.pdf)）。
- en: 'The autoencoder basically consists of two cascaded neural networks—the first
    network acts as an encoder; it takes the input *x* and encodes it using a transformation
    *h* to encoded signal *y*, shown in the following equation:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器基本上由两个级联的神经网络组成——第一个网络充当编码器；它接受输入*x*并使用变换*h*编码为编码信号*y*，如下方程所示：
- en: '![](img/03a3d50f-7635-4a27-ac5d-9ed153780372.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03a3d50f-7635-4a27-ac5d-9ed153780372.png)'
- en: 'The second neural network uses the encoded signal *y* as its input and performs
    another transformation *f* to get a reconstructed signal *r*, shown as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个神经网络使用编码信号*y*作为输入，并执行另一个变换*f*以获得重建信号*r*，如图所示：
- en: '![](img/e8c72478-363b-4d88-98d8-8247494a6503.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8c72478-363b-4d88-98d8-8247494a6503.png)'
- en: The loss function is the MSE with error *e* defined as the difference between
    the original input *x* and the reconstructed signal *r:*
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是均方误差（MSE），其中误差*e*定义为原始输入*x*与重建信号*r*之间的差异：
- en: '![](img/2ac31af7-19b0-4f8b-b7a8-e1f7d6b3e542.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ac31af7-19b0-4f8b-b7a8-e1f7d6b3e542.png)'
- en: '![](img/6b841e32-381c-4f57-9cd7-47229f08c5f7.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b841e32-381c-4f57-9cd7-47229f08c5f7.png)'
- en: Basic architecture of an autoencoder
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的基本架构
- en: 'The preceding diagram shows an autoencoder with **Encoder** and **Decoder**
    highlighted separately. Autoencoders may have weight sharing, that is, weights
    of decoder and encoder are shared. This is done by simply making them a transpose
    of each other; this helps the network learn faster as the number of training parameters
    is less. There are a large variety of autoencoders for example: sparse autoencoders,
    denoising autoencoders, convolution autoencoders, and variational autoencoders.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示展示了一个自编码器，其中**编码器**和**解码器**被分别突出显示。自编码器可能会使用权重共享，即解码器和编码器的权重是共享的。这是通过简单地使它们互为转置来实现的；这有助于加速网络的学习，因为训练参数的数量较少。自编码器有很多种类，例如：稀疏自编码器、去噪自编码器、卷积自编码器和变分自编码器等。
- en: Denoising autoencoders
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: A denoising autoencoder learns from a corrupted (noisy) input; we feed the encoder
    network the noisy input and the reconstructed image from the decoder is compared
    with the original denoised input. The idea is that this will help the network
    learn how to denoise an input. The network does not just make a pixel-wise comparison,
    instead, in order to denoise the image, the network is forced to learn the information
    of neighboring pixels as well.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自编码器从被破坏（有噪声）的输入中学习；我们将有噪声的输入送入编码器网络，解码器重建的图像与原始去噪后的输入进行比较。其核心思想是，这样可以帮助网络学会如何去噪。网络不仅仅进行逐像素比较，相反，为了去噪，网络被迫学习邻近像素的信息。
- en: Once the autoencoder has learned the encoded features *y*, we can remove the
    decoder part of the network and use only the encoder part to achieve dimensionality
    reduction. The dimensionally reduced input can be fed to some other classification
    or regression model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦自编码器学会了编码特征*y*，我们就可以去掉网络中的解码器部分，仅使用编码器部分来实现降维。降维后的输入可以送入其他分类或回归模型中。
- en: Variational autoencoders
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: 'Another very popular autoencoder is **variational autoencoders** (**VAE**).
    They are a mix of the best of both worlds: DL and the Bayesian inference.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种非常流行的自编码器是**变分自编码器**（**VAE**）。它们结合了深度学习和贝叶斯推理的优点。
- en: VAEs have an additional stochastic layer; this layer, after the encoder network,
    samples the data using a Gaussian distribution, and the one after the decoder
    network samples the data using Bernoulli's distribution.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）有一个额外的随机层；该层在编码器网络之后，使用高斯分布对数据进行采样，而解码器网络之后的层使用伯努利分布对数据进行采样。
- en: VAEs can be used to generate images. VAEs allow one to set complex priors in
    the latent and learn powerful latent representations. We will learn more about
    them in a later chapter.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE）可以用于生成图像。VAE允许在潜在空间中设定复杂的先验分布，并学习强大的潜在表示。我们将在后续章节中深入了解它们。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered some basic and useful deep neural network models.
    We started with a single neuron, saw its power and its limitations. The multilayered
    perceptron was built for both regression and classification tasks. The backpropagation
    algorithm was introduced. The chapter progressed to CNN, with an introduction
    to the convolution layers and pooling layers. We learned about some of the successful
    CNN and used the first CNN LeNet to perform handwritten digits recognition. From
    the feed forward MLPs and CNNs, we moved forward to RNNs. LSTM and GRU networks
    were introduced. We made our own LSTM network in TensorFlow and finally learned
    about autoencoders.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些基础且实用的深度神经网络模型。我们从单个神经元开始，了解了它的强大之处以及局限性。多层感知机被构建用于回归和分类任务，反向传播算法也在此时引入。接着我们深入探讨了卷积神经网络（CNN），并介绍了卷积层和池化层。我们学习了一些成功的CNN应用，并使用第一个CNN模型LeNet进行手写数字识别。从前馈型多层感知机（MLP）和卷积神经网络（CNN）出发，我们继续深入到循环神经网络（RNN）。随后介绍了LSTM和GRU网络。我们在TensorFlow中实现了自己的LSTM网络，最终学习了自编码器。
- en: In the next chapter, we will start with a totally new type of AI model genetic
    algorithms. Like neural networks, they too are inspired by nature. We will be
    using what we learned in this chapter and the coming few chapters in the case
    studies we'll do in later chapters.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将从一种全新的人工智能模型——遗传算法开始。像神经网络一样，它们也受到自然的启发。我们将在后续章节的案例研究中，运用本章以及接下来的几章所学的内容。
