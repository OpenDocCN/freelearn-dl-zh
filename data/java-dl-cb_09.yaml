- en: Using RL4J for Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RL4J 进行强化学习
- en: Reinforcement learning is a goal-oriented machine learning algorithm that trains
    an agent to make a sequence of decisions. In the case of deep learning models,
    we train them on existing data and apply the learning on new or unseen data. Reinforcement
    learning exhibits dynamic learning by adjusting its own actions based on continuous
    feedback in order to maximize the reward. We can introduce deep learning into
    a reinforcement learning system, which is known as deep reinforcement learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种以目标为导向的机器学习算法，它训练智能体做出一系列决策。对于深度学习模型，我们在现有数据上训练它们，并将学习应用于新数据或未见过的数据。强化学习通过根据持续反馈调整自己的行为来展现动态学习，以最大化奖励。我们可以将深度学习引入强化学习系统，这就是深度强化学习。
- en: 'RL4J is a reinforcement learning framework integrated with DL4J. RL4J supports
    two reinforcement algorithms: deep Q-learning and A3C (short for **Asynchronous
    Actor-Critic Agents**). Q-learning is an off-policy reinforcement learning algorithm
    that seeks the best action for the given state. It learns from actions outside
    the ones mentioned in the current policy by taking random actions. In deep Q-learning,
    we use a deep neural network to find the optimal Q-value rather than value iteration
    in regular Q-learning. In this chapter, we will set up a gaming environment powered
    by reinforcement learning using Project Malmo. Project Malmo is a platform for
    reinforcement learning experiments built on top of Minecraft.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: RL4J 是一个与 DL4J 集成的强化学习框架。RL4J 支持两种强化学习算法：深度 Q 学习和 A3C（即**异步演员-评论家智能体**）。Q 学习是一种离策略强化学习算法，旨在为给定的状态寻求最佳动作。它通过采取随机动作从当前策略之外的动作中学习。在深度
    Q 学习中，我们使用深度神经网络来找到最佳的 Q 值，而不是像常规 Q 学习那样使用值迭代。在本章中，我们将使用 Project Malmo 设置一个由强化学习驱动的游戏环境。Project
    Malmo 是一个基于 Minecraft 的强化学习实验平台。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Setting up the Malmo environment and respective dependencies
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 Malmo 环境及相关依赖
- en: Setting up the data requirements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置数据要求
- en: Configuring and training a Deep Q-Network (DQN) agent
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置和训练一个深度 Q 网络（DQN）智能体
- en: Evaluating a Malmo agent
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 Malmo 智能体
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The source code for this chapter can be found here:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在这里找到：
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/java/MalmoExample.java)'
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/09_Using_RL4J_for_Reinforcement
    learning/sourceCode` directory. Then, import the `cookbookapp` project as a Maven
    project by importing `pom.xml`.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的 GitHub 仓库后，导航到`Java-Deep-Learning-Cookbook/09_Using_RL4J_for_Reinforcement
    learning/sourceCode`目录。然后，通过导入`pom.xml`将`cookbookapp`项目作为 Maven 项目导入。
- en: 'You need to set up a Malmo client to run the source code. First, download the
    latest Project Malmo release as per your OS ([https://github.com/Microsoft/malmo/releases](https://github.com/Microsoft/malmo/releases)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要设置一个 Malmo 客户端来运行源代码。首先，根据你的操作系统下载最新的 Project Malmo 版本：[https://github.com/Microsoft/malmo/releases](https://github.com/Microsoft/malmo/releases)
- en: For Linux OS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_linux.md](https://github.com/microsoft/malmo/blob/master/doc/install_linux.md).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Linux 操作系统，按照此处的安装说明操作：[https://github.com/microsoft/malmo/blob/master/doc/install_linux.md](https://github.com/microsoft/malmo/blob/master/doc/install_linux.md)
- en: For Windows OS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_windows.md](https://github.com/microsoft/malmo/blob/master/doc/install_windows.md).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Windows 操作系统，按照此处的安装说明操作：[https://github.com/microsoft/malmo/blob/master/doc/install_windows.md](https://github.com/microsoft/malmo/blob/master/doc/install_windows.md)
- en: For macOS, follow the installation instructions here: [https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 macOS 操作系统，按照此处的安装说明操作：[https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/install_macosx.md)
- en: 'To launch the Minecraft client, navigate to the Minecraft directory and run
    the client script:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动Minecraft客户端，请导航到Minecraft目录并运行客户端脚本：
- en: Double-click on `launchClient.bat` (on Windows).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双击`launchClient.bat`（在Windows上）。
- en: Run `./launchClient.sh` on the console (either on Linux or macOS).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在控制台上运行`./launchClient.sh`（无论是在Linux还是macOS上）。
- en: If you're in Windows and are facing issues while launching the client, you can
    download the dependency walker here: [https://lucasg.github.io/Dependencies/](https://lucasg.github.io/Dependencies/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Windows上遇到启动客户端时的问题，可以在这里下载依赖关系查看工具：[https://lucasg.github.io/Dependencies/](https://lucasg.github.io/Dependencies/)。
- en: 'Then, follow these steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按照以下步骤操作：
- en: Extract and run `DependenciesGui.exe`.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压并运行`DependenciesGui.exe`。
- en: 'Select `MalmoJava.dll` in the `Java_Examples` directory to see the missing
    dependencies like the ones shown here:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Java_Examples`目录中选择`MalmoJava.dll`，查看类似下面所示的缺失依赖项：
- en: '![](img/63ab736b-704c-47ac-909e-fe828aad702c.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63ab736b-704c-47ac-909e-fe828aad702c.png)'
- en: In the case of any issues, the missing dependencies will be marked on the list.
    You will need to add the missing dependencies in order to relaunch the client
    successfully. Any missing libraries/files should be present in the `PATH` environment
    variable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现任何问题，缺失的依赖项将在列表中标记出来。你需要添加缺失的依赖项，以便成功重新启动客户端。任何缺失的库/文件应该存在于`PATH`环境变量中。
- en: 'You may refer to OS-specific build instructions here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考此处的操作系统特定构建说明：
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_linux.md](https://github.com/microsoft/malmo/blob/master/doc/build_linux.md)
    (Linux)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/microsoft/malmo/blob/master/doc/build_linux.md](https://github.com/microsoft/malmo/blob/master/doc/build_linux.md)
    (Linux)'
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_windows.md](https://github.com/microsoft/malmo/blob/master/doc/build_windows.md)
    (Windows)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/microsoft/malmo/blob/master/doc/build_windows.md](https://github.com/microsoft/malmo/blob/master/doc/build_windows.md)
    (Windows)'
- en: '[https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md)
    (macOS)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md](https://github.com/microsoft/malmo/blob/master/doc/build_macosx.md)
    (macOS)'
- en: 'If everything goes well, you should see something like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该能看到类似下面的画面：
- en: '![](img/8c9d23f2-2f13-4e81-9e5d-4a2628282ce4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c9d23f2-2f13-4e81-9e5d-4a2628282ce4.png)'
- en: Additionally, you need to create a mission schema to build blocks for the gaming
    window. The complete mission schema can be found in this chapter's project directory
    at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你需要创建一个任务架构来构建游戏窗口的模块。完整的任务架构可以在本章节的项目目录中找到：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/09_Using_RL4J_for_Reinforcement%20learning/sourceCode/cookbookapp/src/main/resources/cliff_walking_rl4j.xml)。
- en: Setting up the Malmo environment and respective dependencies
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Malmo环境及其相关依赖
- en: We need to set up RL4J Malmo dependencies to run the source code. Just like
    any other DL4J application, we also need to add ND4J backend dependencies as well
    depending upon your hardware (CPU/GPU). In this recipe, we will add the required
    Maven dependencies and set up the environment to run the application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置RL4J Malmo依赖项来运行源代码。就像任何其他DL4J应用一样，我们还需要根据硬件（CPU/GPU）添加ND4J后端依赖。在本教程中，我们将添加所需的Maven依赖并设置环境来运行应用程序。
- en: Getting ready
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: The Malmo client should be up and running before we run the Malmo example source
    code. Our source code will communicate with the Malmo client in order to create
    and run the missions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行Malmo示例源代码之前，Malmo客户端应该已经启动并正常运行。我们的源代码将与Malmo客户端进行通信，以便创建并执行任务。
- en: How to do it...
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Add the RL4J core dependency:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加RL4J核心依赖：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the RL4J Malmo dependency:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加RL4J Malmo依赖：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add a dependency for the ND4J backend:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为ND4J后端添加依赖：
- en: 'For CPU, you can use the following:'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于CPU，你可以使用以下配置：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For GPU, you can use the following:'
  id: totrans-44
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于GPU，你可以使用以下配置：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add Maven dependency for `MalmoJavaJar`:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`MalmoJavaJar`添加Maven依赖：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we added RL4J core dependencies to bring in RL4J DQN libraries in
    our application. RL4J Malmo dependencies are added in step 2 to construct the
    Malmo environment and build missions in RL4J.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们添加了RL4J核心依赖项，将RL4J DQN库引入到我们的应用程序中。在第2步中，添加了RL4J Malmo依赖项，以构建Malmo环境并在RL4J中构建任务。
- en: We need to add CPU/GPU-specific ND4J backend dependencies as well (step 3).
    Finally, in step 4, we added dependencies for `MalmoJavaJar` (step 4), which acts
    as a communication interface for the Java program to interact with Malmo.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要添加特定于CPU/GPU的ND4J后端依赖项（第3步）。最后，在第4步中，我们添加了`MalmoJavaJar`的依赖项（第4步），它作为Java程序与Malmo交互的通信接口。
- en: Setting up the data requirements
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置数据要求
- en: 'The data for the Malmo reinforcement learning environment includes the image
    frames that the agent is moving in. A sample gaming window for Malmo will look
    like the following. Here, the agent dies if they step over the lava:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Malmo强化学习环境的数据包括代理正在移动的图像帧。Malmo的示例游戏窗口如下所示。这里，如果代理走过熔岩，它会死亡：
- en: '![](img/7f9aed8d-f3a5-408c-9f95-57dbf0fa9577.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7f9aed8d-f3a5-408c-9f95-57dbf0fa9577.png)'
- en: Malmo requires developers to specify the XML schema in order to generate the
    mission. We will need to create mission data for both the agent and the server
    to create blocks in the world (that is, the gaming environment). In this recipe,
    we will create an XML schema to specify the mission data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Malmo要求开发者指定XML模式以生成任务。我们需要为代理和服务器创建任务数据，以便在世界中创建方块（即游戏环境）。在本示例中，我们将创建一个XML模式来指定任务数据。
- en: How to do it...
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行此操作...
- en: 'Define the initial conditions of the world using the `<ServerInitialConditions>`
    tag:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<ServerInitialConditions>`标签定义世界的初始条件：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Navigate to [http://www.minecraft101.net/superflat/](http://www.minecraft101.net/superflat/)
    and create your own preset string for the super-flat world:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[http://www.minecraft101.net/superflat/](http://www.minecraft101.net/superflat/)并为超平坦世界创建您自己的预设字符串：
- en: '![](img/fe296849-0271-413d-80e9-92aa424094ca.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe296849-0271-413d-80e9-92aa424094ca.png)'
- en: 'Generate a super-flat world with the specified preset string using the `<FlatWorldGenerator>`
    tag:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<FlatWorldGenerator>`标签生成具有指定预设字符串的超平坦世界：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Draw structures in the world using the `<DrawingDecorator>` tag:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<DrawingDecorator>`标签在世界中绘制结构：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Specify a time limit for all agents using the `<ServerQuitFromTimeUp>` tag:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<ServerQuitFromTimeUp>`标签为所有代理指定时间限制：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Add all mission handlers to the block using the `<ServerHandlers>` tag:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<ServerHandlers>`标签将所有任务处理器添加到方块中：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Add `<ServerHandlers>` and `<ServerInitialConditions>` under the `<ServerSection>` tag:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`<ServerSection>`标签下添加`<ServerHandlers>`和`<ServerInitialConditions>`：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the agent name and starting position:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义代理的名称和起始位置：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the block types using the `<ObservationFromGrid>` tag:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<ObservationFromGrid>`标签定义方块类型：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Configure the video frames using the `<VideoProducer>` tag:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<VideoProducer>`标签配置视频帧：
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Mention the reward points to be received when an agent comes into contact with
    a block type using the `<RewardForTouchingBlockType>` tag:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提到当代理与使用`<RewardForTouchingBlockType>`标签的方块类型接触时将获得的奖励点数：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Mention the reward points to issue a command to the agent using the `<RewardForSendingCommand>`
    tag:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提到奖励点数以向代理发出命令，使用`<RewardForSendingCommand>`标签：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Specify the mission endpoints for the agent using the `<AgentQuitFromTouchingBlockType>`
    tag:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`<AgentQuitFromTouchingBlockType>`标签为代理指定任务终点：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add all agent handler functions under the `<AgentHandlers>` tag:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`<AgentHandlers>`标签下添加所有代理处理器函数：
- en: '[PRE17]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add all agent handlers to `<AgentSection>`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有代理处理器添加到`<AgentSection>`中：
- en: '[PRE18]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create a `DataManager` instance to record the training data:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`DataManager`实例来记录训练数据：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, the following configurations are added as the initial conditions
    for the world:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，以下配置被添加为世界的初始条件：
- en: '`StartTime`: This specifies the time of day at the start of the mission, in
    thousandths of an hour. 6,000 refers to noontime.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StartTime`：这指定了任务开始时的时间，以千分之一小时为单位。6000表示中午12点。'
- en: '`AllowPassageOfTime`: If set to `false`, then it will stop the day-night cycle.
    The weather and the sun position will remain constant during the mission.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AllowPassageOfTime`：如果设置为`false`，则会停止昼夜循环。在任务期间，天气和太阳位置将保持不变。'
- en: '`Weather`: This specifies the type of weather at the start of the mission.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Weather`：这指定了任务开始时的天气类型。'
- en: '`AllowSpawning`: If set to `true`, then it will produce animals and hostiles
    during the mission.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AllowSpawning`：如果设置为`true`，则在任务期间将生成动物和敌对生物。'
- en: In *step 2*, we created a preset string to represent the super-flat type that
    is being used in step 3\. A super-flat type is nothing but the type of surface
    seen in the mission.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 2 步*中，我们创建了一个预设字符串来表示在第 3 步中使用的超平面类型。超平面类型指的就是任务中看到的表面类型。
- en: In step 4, we drew structures into the world using `DrawCuboid` and `DrawBlock`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 4 步，我们使用 `DrawCuboid` 和 `DrawBlock` 向世界中绘制了结构。
- en: We follow three-dimensional space `(x1,y1,z1)` -> `(x2,y2,z2)` to specify the
    boundaries. The `type` attribute is used to represent block types. You may add
    any of the available 198 blocks for your experiments.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循三维空间 `(x1,y1,z1)` -> `(x2,y2,z2)` 来指定边界。`type` 属性用于表示块类型。你可以为实验添加任何 198
    个可用的块。
- en: In step 6, we add all mission handlers specific to world creation under the `<ServerHandlers>`
    tag. Then, we add them to the `<ServerSection>` parent tag in step 7.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 6 步，我们将所有与世界创建相关的任务处理程序添加到`<ServerHandlers>`标签下。然后，在第 7 步，我们将它们添加到`<ServerSection>`父标签中。
- en: In step 8, the `<Placement>` tag is used to specify the player's starting position.
    The starting point will be chosen randomly if it is not specified.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 步，`<Placement>`标签用于指定玩家的起始位置。如果未指定起始点，它将随机选择。
- en: 'In step 9, we specified the position of the floor block in the gaming window.
    In step 10, `viewpoint` sets the camera viewpoint:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9 步，我们指定了游戏窗口中地面块的位置。在第 10 步，`viewpoint` 设置了相机的视角：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In step 13, we specify the block types in which agent movement is stopped once
    the step is over. In the end, we add all agent-specific mission handlers in the `AgentSection` tag
    at step 15\. Mission schema creation will end at step 15.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 13 步，我们指定了智能体移动在步骤结束后会停止的块类型。最后，我们在第 15 步的 `AgentSection` 标签中添加了所有特定于智能体的任务处理程序。任务架构创建将在第
    15 步结束。
- en: Now, we need to store the training data from the mission. We use `DataManager`
    to handle the recording of training data. It creates the `rl4j-data` directory if
    it does not exist and stores the training data as the reinforcement learning training
    progresses. We passed `false` as an attribute while creating `DataManager` in
    step 16\. This means that we are not persisting the training data or the model.
    Pass `true` if the training data and model are to be persisted. Note that we are
    going to need the data manager instance while configuring DQN.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要存储来自任务的训练数据。我们使用 `DataManager` 来处理训练数据的记录。如果`rl4j-data`目录不存在，它会创建该目录，并随着强化学习训练的进行存储训练数据。我们在第
    16 步创建 `DataManager` 时传递了 `false` 作为属性。这意味着我们不会持久化训练数据或模型。如果要持久化训练数据和模型，请传递 `true`。请注意，在配置
    DQN 时，我们需要用到数据管理器实例。
- en: See also
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Refer to the following documentation to create your own custom XML schema for
    the Minecraft world:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请参考以下文档，创建你自己的 Minecraft 世界自定义 XML 架构：
- en: '[http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html](http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html)'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html](http://microsoft.github.io/malmo/0.14.0/Schemas/Mission.html)'
- en: '[http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html](http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html)'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html](http://microsoft.github.io/malmo/0.30.0/Schemas/MissionHandlers.html)'
- en: Configuring and training a DQN agent
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置和训练 DQN 智能体
- en: DQN refers to an important class of reinforcement learning, called value learning.
    Here, we use a deep neural network to learn the optimal Q-value function. For
    every iteration, the network approximates Q-value and evaluates them against the
    Bellman equation in order to measure the agent accuracy. Q-value is supposed to
    be optimized while the agent makes movements in the world. So, how we configure
    the Q-learning process is important. In this recipe, we will configure DQN for
    a Malmo mission and train the agent to achieve the task.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 是一种强化学习的重要类别，称为价值学习。在这里，我们使用深度神经网络来学习最优 Q 值函数。在每次迭代中，网络会近似 Q 值，并根据贝尔曼方程对其进行评估，以衡量智能体的准确性。Q
    值应该在智能体在世界中进行动作时得到优化。因此，如何配置 Q-learning 过程非常重要。在这个教程中，我们将配置 DQN 以进行 Malmo 任务，并训练智能体完成任务。
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Basic knowledge on the following are prerequisites for this recipe:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容的基础知识是此教程的先决条件：
- en: Q-learning
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning
- en: DQN
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN
- en: Q-learning basics will help while configuring the Q-learning hyperparameters
    for the DQN.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 基础将有助于在配置 DQN 的 Q-learning 超参数时。
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create an action space for the mission:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为任务创建一个动作空间：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create an observation space for the mission:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为任务创建一个观测空间：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a Malmo consistency policy:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Malmo 一致性策略：
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create an MDP (short for **Markov Decision Process**) wrapper around the Malmo
    Java client:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Malmo Java客户端周围创建一个MDP（**马尔可夫决策过程**）包装器：
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create a DQN using `DQNFactoryStdConv`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DQNFactoryStdConv`创建一个DQN：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Use `HistoryProcessor` to scale the pixel image input:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`HistoryProcessor`对像素图像输入进行缩放：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a Q-learning configuration by specifying hyperparameters:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定超参数创建Q学习配置：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create the DQN model using `QLearningDiscreteConv` by passing MDP wrapper and `DataManager`:
    within the `QLearningDiscreteConv` constructor:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`QLearningDiscreteConv`创建DQN模型，通过传递MDP包装器和`DataManager`：在`QLearningDiscreteConv`构造函数中：
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Train the DQN:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练DQN：
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: How it works...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we defined an action space for the agent by specifying a defined
    set of Malmo actions. For example, `movenorth 1` means moving the agent one block
    north. We passed in a list of strings to `MalmoActionSpaceDiscrete` indicating
    an agent's actions on Malmo space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们通过指定一组定义好的Malmo动作，为代理定义了一个动作空间。例如，`movenorth 1`表示将代理向北移动一个区块。我们将一个字符串列表传递给`MalmoActionSpaceDiscrete`，指示代理在Malmo空间中的动作。
- en: In step 2, we created an observation space from the bitmap size (mentioned by
    `xSize` and `ySize`) of input images(from the Malmo space). Also, we assumed three
    color channels (R, G, B). The agent needs to know about observation space before
    they run. We used `MalmoObservationSpacePixels` because we target observation
    from pixels.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们根据输入图像（来自Malmo空间）的位图大小（由`xSize`和`ySize`指定）创建了一个观察空间。此外，我们假设有三个颜色通道（R、G、B）。代理需要在运行之前了解观察空间。我们使用了`MalmoObservationSpacePixels`，因为我们目标是从像素中获取观察。
- en: In step 3, we have created a Malmo consistency policy using `MalmoDescretePositionPolicy` to
    ensure that the upcoming observation is in a consistent state.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3步中，我们使用`MalmoDescretePositionPolicy`创建了一个Malmo一致性策略，以确保即将到来的观察处于一致的状态。
- en: A MDP is an approach used in reinforcement learning in grid-world environments.
    Our mission has states in the form of grids. MDP requires a policy and the objective of
    reinforcement learning is to find the optimal policy for the MDP. `MalmoEnv` is
    an MDP wrapper around a Java client.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: MDP是强化学习中在网格世界环境中使用的一种方法。我们的任务有网格形式的状态。MDP需要一个策略，强化学习的目标是为MDP找到最优策略。`MalmoEnv`是一个围绕Java客户端的MDP包装器。
- en: In step 4, we created an MDP wrapper using the mission schema, action space,
    observation space, and observation policy. Note that the observation policy is
    not the same as the policy that an agent wants to form at the end of the learning
    process.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们使用任务架构、动作空间、观察空间和观察策略创建了一个MDP包装器。请注意，观察策略与代理在学习过程结束时希望形成的策略不同。
- en: In step 5, we used **`DQNFactoryStdConv`** to build the DQN by adding convolutional
    layers.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们使用**`DQNFactoryStdConv`**通过添加卷积层构建了DQN。
- en: In step 6, we configured `HistoryProcessor` to scale and remove pixels that
    were not needed. The actual intent of `HistoryProcessor` is to perform an experience
    replay, where the previous experience from the agent will be considered while
    deciding the action on the current state. With the use of `HistoryProcessor`,
    we can change the partial observation of states to a fully-observed state, that
    is, when the current state is an accumulation of the previous states.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们配置了`HistoryProcessor`来缩放并移除不需要的像素。`HistoryProcessor`的实际目的是执行经验回放，在这种回放中，代理的过去经验将在决定当前状态的动作时考虑。通过使用`HistoryProcessor`，我们可以将部分状态观察转变为完全观察状态，也就是说，当当前状态是先前状态的积累时。
- en: 'Here are the hyperparameters used in step 7 while creating Q-learning configuration:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是第7步中在创建Q学习配置时使用的超参数：
- en: '`maxEpochStep`: The maximum number of steps allowed per epoch.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxEpochStep`：每个周期允许的最大步数。'
- en: '`maxStep`: The maximum number of steps that are allowed. Training will finish
    when the iterations exceed the value specified for `maxStep`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxStep`：允许的最大步数。当迭代次数超过`maxStep`指定的值时，训练将结束。'
- en: '`expRepMaxSize`: The maximum size of experience replay. Experience replay refers
    to the number of past transitions based on which the agent can decide on the next
    step to take.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expRepMaxSize`：经验回放的最大大小。经验回放是指基于过去的过渡数量，代理可以决定下一步要采取的行动。'
- en: '`doubleDQN`: This decides whether double DQN is enabled in the configuration
    (true if enabled).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`doubleDQN`：这决定了是否在配置中启用了双重DQN（如果启用，则为true）。'
- en: '`targetDqnUpdateFreq`: Regular Q-learning can overestimate the action values
    under certain conditions. Double Q-learning adds stability to the learning. The
    main idea of double DQN is to freeze the network after every *M* number of updates
    or smoothly average for every *M* number of updates. The value of M is referred
    to as `targetDqnUpdateFreq`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`targetDqnUpdateFreq`：常规的 Q 学习在某些条件下可能会高估动作值。双 Q 学习通过增加学习的稳定性来解决这个问题。双 DQN
    的主要思想是在每 `M` 次更新后冻结网络，或者在每 `M` 次更新后平滑平均。`M` 的值被称为 `targetDqnUpdateFreq`。'
- en: '`updateStart`: The number of no-operation (do nothing) moves at the beginning
    to ensure the Malmo mission starts with a random configuration. If the agent starts
    the game in the same way every time, then the agent will memorize the sequence
    of actions, rather than learning to take the next action based on the current
    state.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`updateStart`：在开始时进行无操作（什么也不做）的步骤数，以确保 Malmo 任务以随机配置开始。如果代理每次都以相同的方式开始游戏，代理将记住行动序列，而不是根据当前状态学习采取下一个行动。'
- en: '`gamma`: This is also known as the discount factor. A discount factor is multiplied
    by future rewards to prevent the agent from being attracted to high rewards, rather
    than learning the actions. A discount factor close to 1 indicates that the rewards
    from the distant future are considered. On the other hand, a discount factor close
    to 0 indicates that the rewards from the immediate future are being considered.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：这也被称为折扣因子。折扣因子会乘以未来的奖励，防止代理被高奖励吸引，而不是学习如何采取行动。接近 1 的折扣因子表示考虑来自远期的奖励，而接近
    0 的折扣因子表示考虑来自近期的奖励。'
- en: '`rewardFactor`: This is a reward-scaling factor to scale the reward for every
    single step of training.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rewardFactor`：这是一个奖励缩放因子，用于缩放每一步训练的奖励。'
- en: '`errorClamp`: This will clip the gradient of loss function with respect to
    output during backpropagation. For `errorClamp = 1`, the gradient component is
    clipped to the range *(-1, 1)*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`errorClamp`：这会在反向传播期间截断损失函数相对于输出的梯度。对于 `errorClamp = 1`，梯度分量会被截断到范围 *(-1,
    1)*。'
- en: '`minEpsilon`: Epsilon is the derivative of the loss function with respect to
    the output of the activation function. Gradients for every activation node for
    backpropagation are calculated from the given epsilon value.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minEpsilon`：Epsilon 是损失函数相对于激活函数输出的导数。每个激活节点的梯度会根据给定的 epsilon 值计算，以用于反向传播。'
- en: '`epsilonNbStep`: Th epsilon value is annealed to `minEpsilon` over an `epsilonNbStep` number
    of steps.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilonNbStep`：Epsilon 值将在 `epsilonNbStep` 步骤中退火至 `minEpsilon`。'
- en: There's more...
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We can make the mission even harder by putting lava onto the agent''s path
    after a certain number of actions are performed. First, start by creating a mission
    specification using the schema XML:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在代理路径上放置熔岩来让任务变得更加困难，放置熔岩的条件是执行了一定数量的动作。首先，使用 XML 模式创建一个任务规范：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, setting the lava challenge on the mission is as simple as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，设置熔岩挑战任务变得非常简单，如下所示：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`MissionSpec` is a class file included in the `MalmoJavaJar` dependency,which
    we can use to set missions in the Malmo space.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`MissionSpec` 是一个类文件，包含在 `MalmoJavaJar` 依赖项中，我们可以用它来设置 Malmo 空间中的任务。'
- en: Evaluating a Malmo agent
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 Malmo 代理
- en: We need to evaluate the agent to see how well it has learned to play the game.
    We just trained our agent to navigate through the world to reach the target. In
    this recipe, we will evaluate the trained Malmo agent.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要评估代理，看看它在游戏中的表现如何。我们刚刚训练了我们的代理让它在世界中导航并达到目标。在这个过程中，我们将评估训练好的 Malmo 代理。
- en: Getting ready
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: As a prerequisite, we will need to persist the agent policies and reload them
    back during evaluation.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前提，我们需要持久化代理的策略，并在评估期间重新加载它们。
- en: 'The final policy (policy to make movements in Malmo space) used by the agent
    after training can be saved as shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在训练后使用的最终策略（在 Malmo 空间中进行移动的策略）可以如下面所示保存：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`dql` refers to the DQN model. We retrieve the final policies and store them
    as a `DQNPolicy`. A DQN policy provides actions that have the highest Q-value
    estimated by the model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`dql` 指的是 DQN 模型。我们获取最终的策略，并将它们存储为 `DQNPolicy`。DQN 策略提供模型估计的具有最高 Q 值的动作。'
- en: 'It can be restored later for evaluation/inference:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以在稍后进行恢复以进行评估/推理：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: How to do it...
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create an MDP wrapper to load the mission:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 MDP 封装器来加载任务：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Evaluate the agent:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估代理：
- en: '[PRE35]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How it works...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The Malmo mission/world is launched in step 1\. In step 2, `MALMO_HPROC` is
    the history processor configuration. You can refer to step 6 of the previous recipe
    for the sample configuration. Once the agent is subjected to evaluation, you should
    see the results as shown here:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Malmo 任务/世界在步骤 1 中启动。在步骤 2 中，`MALMO_HPROC` 是历史处理器配置。你可以参考之前食谱中的第 6 步，查看示例配置。一旦代理被评估，你应该能看到如下结果：
- en: '![](img/0ea45d95-90e1-4b49-afd0-58da36176ad4.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ea45d95-90e1-4b49-afd0-58da36176ad4.png)'
- en: For every mission evaluation, we calculate the reward score. A positive reward
    score indicates that the agent has reached the target. At the end, we calculated
    the average reward score of the agent.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次任务评估，我们都会计算奖励分数。正向奖励分数表示代理已到达目标。最后，我们会计算代理的平均奖励分数。
- en: '![](img/763ff236-143a-4198-b2ad-8a59b3ef743d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/763ff236-143a-4198-b2ad-8a59b3ef743d.png)'
- en: In the preceding screenshot, we can see that the agent has reached the target.
    This is the ideal target position, no matter how the agent decides to move across
    the block. After the training session, the agent will form a final policy, which
    the agent can use to reach the target without falling into lava. The evaluation
    process will ensure that the agent is trained enough to play the Malmo game on
    its own.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的截图中，我们可以看到代理已经到达了目标。这是理想的目标位置，无论代理如何决定在方块中移动。训练结束后，代理将形成最终的策略，代理可以利用该策略到达目标，而不会掉入岩浆。评估过程将确保代理已经足够训练，能够独立进行
    Malmo 游戏。
