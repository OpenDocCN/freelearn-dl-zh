- en: Assessment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Chapter 1
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章
- en: A replay buffer is required for off-policy RL algorithms. We sample from the
    replay buffer a mini-batch of experiences and use it to train the *Q(s,a)* state-value
    function in DQN and the actor's policy in a DDPG.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离策略强化学习算法需要一个重放缓冲区。我们从重放缓冲区中抽取一个小批量的经验，用它来训练 DQN 中的*Q(s,a)* 状态值函数以及 DDPG 中的演员策略。
- en: We discount rewards, as there is more uncertainty about the long-term performance
    of the agent. So, immediate rewards have a higher weight, a reward earned in the
    next time step has a relatively lower weight, a reward earned in the subsequent
    time step has an even lower weight, and so on.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对奖励进行折扣，因为关于智能体的长期表现存在更多不确定性。因此，立即奖励具有更高的权重，下一时间步获得的奖励的权重相对较低，再下一时间步的奖励权重更低，依此类推。
- en: The training of the agent will not be stable if *γ > 1*. The agent will fail
    to learn an optimal policy.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*γ > 1*，智能体的训练将不稳定，智能体将无法学习到最优策略。
- en: A model-based RL agent has the potential to perform well, but there is no guarantee
    that it will perform better than a model-free RL agent, as the model of the environment
    we are constructing need not always be a good one. It is also very hard to build
    an accurate enough model of the environment.
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于模型的强化学习智能体有潜力表现得很好，但不能保证它一定会比基于无模型的强化学习智能体表现得更好，因为我们构建的环境模型未必总是一个好的模型。而且，构建一个足够准确的环境模型也非常困难。
- en: In deep RL, deep neural networks are used for the *Q(s,a)* and the actor's policy
    (the latter is true in an Actor-Critic setting). In the traditional RL algorithms,
    a tabular *Q(s, a)* is used but is not possible when the number of states is very
    large, as is usually the case in most problems.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度强化学习中，深度神经网络用于表示*Q(s,a)* 和演员的策略（在 Actor-Critic 设置中是这样的）。在传统的强化学习算法中，使用的是表格型的*Q(s,a)*，但当状态的数量非常大时，这种方式无法应用，而这通常是大多数问题的情况。
- en: Chapter 3
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章
- en: A replay buffer is used in DQN in order to store past experiences, sample a
    mini-batch of data from it, and use it to train the agent.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 DQN 中，重放缓冲区用于存储过去的经验，从中抽取一个小批量的数据，并用来训练智能体。
- en: Target networks help in the stability of the training. This is achieved by keeping
    an additional neural network whose weights are updated using an exponential moving
    average of the weights of the main neural network. Alternatively, another approach
    that is also widely used is to copy the weights of the main neural network to
    the target network once every few thousand steps or so.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目标网络有助于提高训练的稳定性。这是通过保持一个额外的神经网络来实现的，该网络的权重通过使用主神经网络权重的指数移动平均来更新。或者，另一种广泛使用的方法是每隔几千步就将主神经网络的权重复制到目标网络中。
- en: One frame as the state will not help in the Atari Breakout problem. This is
    because no temporal information is deductible from one frame only. For instance,
    in one frame alone, the direction of motion of the ball cannot be obtained. If,
    however, we stack up multiple frames, the velocity and acceleration of the ball
    can be ascertained.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Atari Breakout 问题中，单一帧作为状态是无法提供帮助的。这是因为单一帧无法提取时间信息。例如，单一帧中无法得出球的运动方向。但如果我们将多帧叠加起来，就可以确定球的速度和加速度。
- en: L2 loss is known to overfit to outliers. Hence, the Huber loss is preferred,
    as it combines both L2 and L1 losses. See Wikipedia: [https://en.wikipedia.org/wiki/Huber_loss](https://en.wikipedia.org/wiki/Huber_loss).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L2 损失已知会对异常值产生过拟合。因此，更倾向使用 Huber 损失，因为它结合了 L2 和 L1 损失。详见维基百科：[https://en.wikipedia.org/wiki/Huber_loss](https://en.wikipedia.org/wiki/Huber_loss)。
- en: RGB images can also be used. However, we will need extra weights for the first
    hidden layer of the neural network, as we now have three channels in each of the
    four frames in the state stack. This much finer detail for the state space is
    not required for Atari. However, RGB images can help in other applications, for
    example, in autonomous driving and/or robotics.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 也可以使用 RGB 图像。不过，我们需要为神经网络的第一隐藏层增加额外的权重，因为现在在状态堆栈中的四帧每一帧都有三个通道。这种对于状态空间的更精细的细节在
    Atari 中并不是必须的。然而，RGB 图像可以在其他应用中提供帮助，例如在自动驾驶和/或机器人技术中。
- en: Chapter 4
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章
- en: DQN is known to overestimate the state-action value function, *Q(s,a)*. To overcome
    this, DDQN was introduced. DDQN has fewer problems than DQN regarding the overestimation
    of *Q(s,a)*.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DQN 被认为会高估状态-动作值函数*Q(s,a)*。为了解决这个问题，引入了 DDQN。DDQN 在高估*Q(s,a)*方面比 DQN 少遇到问题。
- en: Dueling network architecture has separate streams for the advantage function
    and the state-value function. These are then combined to obtain *Q(s,a)*. This
    branching out and then combining is observed to result in a more stable training
    of the RL agent.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗网络架构为优势函数和状态价值函数提供了独立的流。然后，这些流被组合起来得到*Q(s,a)*。这种分支后再合并的方式被观察到有助于RL代理的训练更加稳定。
- en: '**Prioritized Experience Replay** (**PER**) gives more importance to experience
    samples where the agent performs poorly, and so these samples are sampled more
    frequently than other samples where the agent performed well. By frequently using
    samples where the agent performed poorly, the agent is able to work on its weakness
    more often, and so PER speeds up the training.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优先经验回放**（**PER**）赋予代理表现较差的经验样本更高的重要性，因此这些样本比代理表现良好的其他样本被采样的频率更高。通过频繁使用表现较差的样本，代理能够更频繁地解决自身的弱点，从而PER加速了训练。'
- en: In some computer games, such as Atari Breakout, the simulator has too many frames
    per second. If a separate action is sampled from the policy in each of these time
    steps, the state of the agent may not change enough in one time step, as it is
    too small. Hence, sticky actions are used where the same action is repeated over
    a finite but fixed number of time steps, say *n*, and the total reward accrued
    over these n time steps is used as the reward for the action performed. In these
    n time steps, the state of the agent has changed sufficiently enough to be able
    to evaluate the efficacy of the action taken. Too small a value for n can prevent
    the agent from learning a good policy; likewise, too large a value can also be
    a problem. You must choose the right number of time steps over which the same
    action is taken, and this depends on the simulator used.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一些计算机游戏中，例如Atari Breakout，模拟器的帧率过高。如果在每个时间步长中都从策略中采样一个独立的动作，代理的状态可能在一个时间步长内变化不够，因为这个时间步长太小。因此，使用了粘性动作，其中相同的动作会在有限但固定的时间步数内重复，例如*n*，并且在这些n个时间步内累计的总奖励将作为执行该动作的奖励。在这些n个时间步内，代理的状态已经发生了足够的变化，可以评估所采取的动作的有效性。n值过小会阻止代理学习到良好的策略；同样，n值过大也会成为一个问题。必须选择合适的时间步长数，在这些时间步长内执行相同的动作，这取决于所使用的模拟器。
- en: Chapter 5
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章
- en: DDPG is an off-policy algorithm, as it uses a replay buffer.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG是一种基于策略外的算法，因为它使用了回放缓冲区。
- en: In general, the same number of hidden layers and the number of neurons per hidden
    layer is used for the actor and the critic, but this is not required. Note that
    the output layer will be different for the actor and the critic, with the actor
    having the number of outputs equal to the number of actions; the critic will have
    only one output.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般来说，演员和评论家的隐藏层数量以及每个隐藏层的神经元数量是相同的，但这不是强制要求。请注意，演员和评论家的输出层是不同的，演员的输出数量等于动作的数量；评论家只有一个输出。
- en: DDPG is used for continuous control, that is, when the actions are continuous
    and real-valued. Atari Breakout has discrete actions, and so DDPG is not suitable
    for Atari Breakout.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG用于连续控制，即当动作是连续且为实数值时。Atari Breakout有离散动作，因此DDPG不适用于Atari Breakout。
- en: We use the `relu` activation function, and so the biases are initialized to
    small positive values so that they fire at the beginning of the training and allow
    gradients to back-propagate.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`relu`激活函数，因此偏置初始化为小的正值，以便它们在训练开始时就能够激活并允许梯度回传。
- en: This is an exercise. See [https://gym.openai.com/envs/InvertedDoublePendulum-v2/](https://gym.openai.com/envs/InvertedDoublePendulum-v2/).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个练习。请参见[https://gym.openai.com/envs/InvertedDoublePendulum-v2/](https://gym.openai.com/envs/InvertedDoublePendulum-v2/)。
- en: This is also an exercise. Notice what happens to the learning when the number
    of neurons is decreased in the first layer sequentially. In general, information
    bottlenecks are observed not only in an RL setting, but for any DL problem.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这也是一个练习。注意当第一层的神经元数量逐渐减少时，学习会发生什么变化。一般来说，信息瓶颈不仅在强化学习（RL）环境中观察到，任何深度学习（DL）问题中也会出现。
- en: Chapter 6
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章
- en: '**Asynchronous Advantage Actor-Critic Agents** (**A3C**) is an on-policy algorithm,
    as we do not use a replay buffer to sample data from. However, a temporary buffer
    is used to collect immediate samples, which are used to train once, after which
    the buffer is emptied.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**异步优势演员-评论家代理**（**A3C**）是一种基于策略的算法，因为我们并没有使用回放缓冲区来采样数据。然而，使用了一个临时缓冲区来收集即时样本，这些样本会用来训练一次，然后缓冲区会被清空。'
- en: The Shannon entropy term is used as a regularizer—the higher the entropy, the
    better the policy is.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shannon 熵项被用作正则化器——熵越高，策略越好。
- en: When too many worker threads are used, the training can slow down and can crash,
    as memory is limited. If, however, you have access to a large cluster of processors,
    then using a large number of worker threads/processes helps.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当使用过多工作线程时，训练可能会变慢并崩溃，因为内存有限。然而，如果你可以访问大量的处理器集群，那么使用大量的工作线程/进程将会有所帮助。
- en: Softmax is used in the policy network to obtain probabilities of different actions.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Softmax 被用于策略网络中，以获取不同动作的概率。
- en: An advantage function is widely used, as it decreases the variance of the policy
    gradient. *Section 3* of the A3C paper ([https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf))
    has more regarding this.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优势函数被广泛使用，因为它降低了策略梯度的方差。《A3C 论文》第 *3 节*（[https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)）对此有更多说明。
- en: This is an exercise.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个练习。
- en: Chapter 7
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章
- en: '**Trust Region Policy Optimization** (**TRPO**) has an objective function and
    a constraint. It hence requires a second order optimization such as a conjugate
    gradient. SGD and Adam are not applicable in TRPO.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**信任区域策略优化**（**TRPO**）具有目标函数和约束条件。因此，它需要二阶优化方法，如共轭梯度法。SGD 和 Adam 不适用于 TRPO。'
- en: The entropy term helps in regularization. It allows the agent to explore more.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 熵项有助于正则化，它允许智能体进行更多探索。
- en: We clip the policy ratio to limit the amount by which one update step will change
    the policy. If this clipping parameter epsilon is large, the policy can change
    drastically in each update, which can result in a sub-optimal policy, as the agent's
    policy is noisier and has too many fluctuations.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们剪切策略比率，以限制每次更新步骤对策略的变化量。如果这个剪切参数 epsilon 设置得较大，则每次更新中策略可能发生剧烈变化，这可能导致一个次优策略，因为智能体的策略变得更加嘈杂并且波动过大。
- en: The action is bounded between a negative and a positive value, and so the `tanh`
    activation function is used for `mu`. For sigma, the `softplus` is used as sigma
    and is always positive. The `tanh` function cannot be used for sigma, as `tanh`
    can result in negative values for sigma, which is meaningless!
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作的取值范围在负值和正值之间，因此对 `mu` 使用了 `tanh` 激活函数。对于 sigma，使用 `softplus` 作为 sigma，它始终为正。不能对
    sigma 使用 `tanh` 函数，因为 `tanh` 可能会导致 sigma 为负值，这是没有意义的！
- en: Reward shaping generally helps with the training. But if it is done poorly,
    it will not help with the training. You must ensure that the reward shaping is
    done to keep the `reward` function dense as well as in appropriate ranges.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励塑形通常有助于训练。但如果操作不当，它将无法帮助训练。你必须确保奖励塑形能够保持 `reward` 函数的密度并处于适当的范围内。
- en: No, reward shaping is used only in the training.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，奖励塑形仅在训练过程中使用。
- en: Chapter 8
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章
- en: TORCS is a continuous control problem. DQN works only for discrete actions,
    and so it cannot be used in TORCS.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TORCS 是一个连续控制问题。DQN 仅适用于离散动作，因此不能用于 TORCS。
- en: The initialization is another initialization strategy; you can also use a random
    uniform initialization with the `min` and `max` values of the range specified;
    another approach is to sample from a Gaussian with a zero mean and a specified
    sigma value. The interested reader must try these different initializers and compare
    the agent's performance.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化是另一种初始化策略；你也可以使用指定范围内的 `min` 和 `max` 值进行随机均匀初始化；另一种方法是从一个均值为零、sigma 值已指定的高斯分布中采样。感兴趣的读者应尝试这些不同的初始化方法，并比较智能体的表现。
- en: The `abs()` function is used in the `reward` function, as we penalize lateral
    drift from the center equally on either side (left or right). The first term is
    the longitudinal speed, and so no `abs()` function is required.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`abs()` 函数在 `reward` 函数中使用，因为我们对偏离中心的横向漂移进行平等惩罚（无论是左侧还是右侧）。第一个项是纵向速度，因此不需要
    `abs()` 函数。'
- en: The Gaussian noise added to the actions for exploration can be tapered down
    with episode count, and this can result in smoother driving. Surely, there are
    many other tricks you can do!
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了探索而加入的高斯噪声可以随着回合数的增加逐渐减少，这可以导致更平稳的驾驶体验。当然，你还可以尝试许多其他技巧！
- en: DDPG is off-policy, but **Proximal Policy Optimization** (**PPO**) is the on-policy
    RL algorithm. Hence, DDPG requires a replay buffer to store past-experience samples,
    but PPO does not require a reply buffer.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DDPG 是一个脱离策略的算法，但**近端策略优化**（**PPO**）是一个基于策略的强化学习算法。因此，DDPG 需要一个回放缓冲区来存储过去的经验样本，而
    PPO 则不需要回放缓冲区。
