- en: Chapter 1. Simple Classifiers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章：简单分类器
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Deserializing and running a classifier
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反序列化和运行分类器
- en: Getting confidence estimates from a classifier
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分类器中获取置信度估计
- en: Getting data from the Twitter API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Twitter API 获取数据
- en: Applying a classifier to a `.csv` file
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分类器应用于 `.csv` 文件
- en: Evaluation of classifiers – the confusion matrix
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器的评估 – 混淆矩阵
- en: Training your own language model classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练你自己的语言模型分类器
- en: How to train and evaluate with cross validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用交叉验证进行训练和评估
- en: Viewing error categories – false positives
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看错误类别 – 假阳性
- en: Understanding precision and recall
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解精准度和召回率
- en: How to serialize a LingPipe object – classifier example
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何序列化 LingPipe 对象 – 分类器示例
- en: Eliminate near duplicates with the Jaccard distance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jaccard 距离消除近似重复项
- en: How to classify sentiment – simple version
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何分类情感 – 简单版
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter introduces the LingPipe toolkit in the context of its competition
    and then dives straight into text classifiers. Text classifiers assign a category
    to text, for example, they assign the language to a sentence or tell us if a tweet
    is positive, negative, or neutral in sentiment. This chapter covers how to use,
    evaluate, and create text classifiers based on language models. These are the
    simplest machine learning-based classifiers in the LingPipe API. What makes them
    simple is that they operate over characters only—later, classifiers will have
    notions of words/tokens and even more. However, don't be fooled, character-language
    models are ideal for language identification, and they were the basis of some
    of the world's earliest commercial sentiment systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 LingPipe 工具包，并将其与同类工具进行比较，然后直接深入到文本分类器的内容。文本分类器将一个类别分配给文本，例如，它们可以确定一句话的语言，或者告诉我们一条推文的情感是积极、消极还是中立。本章讲解了如何使用、评估和创建基于语言模型的文本分类器。这些是
    LingPipe API 中最简单的基于机器学习的分类器。它们之所以简单，是因为它们只处理字符——稍后，分类器将引入单词/标记等概念。然而，不要被迷惑，字符语言模型在语言识别方面是理想的，它们曾是世界上一些最早的商业情感系统的基础。
- en: This chapter also covers crucial evaluation infrastructure—it turns out that
    almost everything we do turns out to be a classifier at some level of interpretation.
    So, do not skimp on the power of cross validation, definitions of precision/recall,
    and F-measure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了至关重要的评估基础设施——事实证明，我们所做的几乎所有事情在某种层次的解释中都可以看作是分类器。因此，不要忽视交叉验证、精准度/召回率定义和
    F-measure 的强大作用。
- en: The best part is that you will learn how to programmatically access Twitter
    data to train up and evaluate your own classifiers. There is a boring bit concerning
    the mechanics of reading and writing LingPipe objects from/to disk, but other
    than that, this is a fun chapter. The goal of this chapter is to get you up and
    running quickly with the basic care and feeding of machine-learning techniques
    in the domain of **natural language processing** (**NLP**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最棒的部分是你将学习如何以编程方式访问 Twitter 数据，来训练和评估你自己的分类器。虽然有一部分内容涉及到从磁盘读取和写入 LingPipe 对象的机制，这部分有点枯燥，但除此之外，这一章还是很有趣的。本章的目标是让你快速上手，掌握机器学习技术在**自然语言处理**（**NLP**）领域的基本使用方法。
- en: LingPipe is a Java toolkit for NLP-oriented applications. This book will show
    you how to solve common NLP problems with LingPipe in a problem/solution format
    that allows developers to quickly deploy solutions to common tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 是一个面向 NLP 应用的 Java 工具包。本书将展示如何通过问题/解决方案的形式，使用 LingPipe 解决常见的 NLP 问题，使开发者能够快速部署解决方案来完成常见任务。
- en: LingPipe and its installation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LingPipe 及其安装
- en: LingPipe 1.0 was released in 2003 as a dual-licensed open source NLP Java library.
    At the time of writing this book, we are coming up on 2000 hits on Google Scholar
    and have thousands of commercial installs, ranging from universities to government
    agencies to Fortune 500 companies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 1.0 于 2003 年发布，作为一个双重许可的开源 NLP Java 库。在本书写作时，我们即将达到 Google Scholar
    上 2000 次点击，且已有成千上万的商业安装，用户包括大学、政府机构以及财富 500 强公司。
- en: Current licensing is either AGPL ([http://www.gnu.org/licenses/agpl-3.0.html](http://www.gnu.org/licenses/agpl-3.0.html))
    or our commercial license that offers more traditional features such as indemnification
    and non-sharing of code as well as support.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的许可协议是 AGPL（[http://www.gnu.org/licenses/agpl-3.0.html](http://www.gnu.org/licenses/agpl-3.0.html)）或者我们的商业许可，后者提供更多传统的功能，如赔偿、代码不共享以及支持。
- en: Projects similar to LingPipe
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类似于 LingPipe 的项目
- en: Nearly all NLP projects have awful acronyms so we will lay bare our own. **LingPipe**
    is the short form for **linguistic pipeline**, which was the name of the `cvs`
    directory in which Bob Carpenter put the initial code.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的 NLP 项目都有糟糕的缩写，我们会公开自己的缩写。**LingPipe** 是 **语言处理管道**（linguistic pipeline）的缩写，这也是
    Bob Carpenter 放置初始代码的 `cvs` 目录的名称。
- en: 'LingPipe has lots of competition in the NLP space. The following are some of
    the more popular ones with a focus on Java:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 在 NLP 领域有很多竞争者。以下是一些受欢迎的、专注于 Java 的竞争者：
- en: '**NLTK**: This is the dominant Python library for NLP processing.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK**：这是主流的 Python 库，用于 NLP 处理。'
- en: '**OpenNLP**: This is an Apache project built by a bunch of smart folks.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenNLP**：这是一个 Apache 项目，由一群聪明的人构建。'
- en: '**JavaNLP**: This is a rebranding of Stanford NLP tools, again built by a bunch
    of smart folks.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaNLP**：这是斯坦福 NLP 工具的重新品牌化，也由一群聪明的人构建。'
- en: '**ClearTK**: This is a University of Boulder toolkit that wraps lots of popular
    machine learning frameworks.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ClearTK**：这是科罗拉多大学博尔德分校的一个工具包，它封装了许多流行的机器学习框架。'
- en: '**DkPro**: Technische Universität Darmstadt from Germany produced this UIMA-based
    project that wraps many common components in a useful manner. UIMA is a common
    framework for NLP.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DkPro**：来自德国达姆施塔特工业大学的这个基于 UIMA 的项目以有用的方式封装了许多常见的组件。UIMA 是一个常见的 NLP 框架。'
- en: '**GATE**: GATE is really more of a framework than competition. In fact, LingPipe
    components are part of their standard distribution. It has a nice graphical "hook
    the components up" capability.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GATE**：GATE 更像是一个框架，而不是竞争对手。实际上，LingPipe 的组件是它的标准分发包的一部分。它具有很好的图形化“连接组件”功能。'
- en: '**Learning Based Java** (**LBJ**): LBJ is a special-purpose programming language
    based on Java, and it is geared toward machine learning and NLP. It was developed
    at the Cognitive Computation Group of the University of Illinois at Urbana Champaign.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于学习的 Java**（**LBJ**）：LBJ 是一种基于 Java 的专用编程语言，面向机器学习和自然语言处理（NLP）。它是在伊利诺伊大学香槟分校的认知计算小组开发的。'
- en: '**Mallet**: This name is the short form of **MAchine Learning for LanguagE
    Toolkit**. Apparently, reasonable acronym generation is short in supply these
    days. Smart folks built this too.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mallet**：这个名字是 **机器学习语言工具包**（MAchine Learning for LanguagE Toolkit）的缩写。显然，如今生成合理的缩写非常困难。聪明的人也构建了这个工具。'
- en: 'Here are some pure machine learning frameworks that have broader appeal but
    are not necessarily tailored for NLP tasks:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些纯粹的机器学习框架，它们有更广泛的吸引力，但不一定是专门为 NLP 任务量身定制的：
- en: '**Vowpal Wabbit**: This is very focused on scalability around Logistic Regression,
    Latent Dirichelet Allocation, and so on. Smart folks drive this.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vowpal Wabbit**：这个项目非常专注于围绕逻辑回归、潜在狄利克雷分配（Latent Dirichlet Allocation）等方面的可扩展性。聪明的人在推动这个项目。'
- en: '**Factorie**: It is from UMass, Amherst and an alternative offering to Mallet.
    Initially it focused primarily on graphic models, but now it also supports NLP
    tasks.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Factorie**：它来自马萨诸塞大学阿默斯特分校，是 Mallet 的一个替代方案。最初，它主要集中在图形模型上，但现在它也支持 NLP 任务。'
- en: '**Support Vector Machine** (**SVM**): SVM light and `libsvm` are very popular
    SVM implementations. There is no SVM implementation in LingPipe, because logistic
    regression does this as well.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）：SVM light 和 `libsvm` 是非常流行的 SVM 实现。LingPipe 没有 SVM 实现，因为逻辑回归也可以实现这一功能。'
- en: So, why use LingPipe?
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 那么，为什么要使用 LingPipe？
- en: 'It is very reasonable to ask why choose LingPipe with such outstanding free
    competition mentioned earlier. There are a few reasons:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 询问为什么选择 LingPipe 而不是上述提到的出色的免费竞争对手是非常合理的。原因有几个：
- en: '**Documentation**: The class-level documentation in LingPipe is very thorough.
    If the work is based on academic work, that work is cited. Algorithms are laid
    out, the underlying math is explained, and explanations are precise. What the
    documentation lacks is a "how to get things done" perspective; however, this is
    covered in this book.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档**：LingPipe 的类级文档非常详尽。如果工作是基于学术研究的，相关研究会被引用。算法清晰列出，底层数学解释详细，说明精确。文档缺少的是一种“如何完成任务”的视角；不过，这本书会覆盖这一内容。'
- en: '**Enterprise/server optimized**: LingPipe is designed from the ground up for
    server applications, not for command-line usage (though we will be using the command
    line extensively throughout the book).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面向企业/服务器优化**：LingPipe 从一开始就为服务器应用而设计，而不是为命令行使用（尽管我们将在本书中广泛使用命令行）。'
- en: '**Coded in the Java dialect**: LingPipe is a native Java API that is designed
    according to standard Java class design principles (Joshua Bloch''s *Effective
    Java*, by Addison-Wesley), such as consistency checks on construction, immutability,
    type safety, backward-compatible serializability, and thread safety.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 Java 方言编写**：LingPipe 是一个本地的 Java API，设计遵循标准的 Java 类设计原则（Joshua Bloch 的《*Effective
    Java*》，由 Addison-Wesley 出版），例如在构造时进行一致性检查、不可变性、类型安全、向后兼容的可序列化性以及线程安全性。'
- en: '**Error handling**: Considerable attention is paid to error handling through
    exceptions and configurable message streams for long-running processes.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误处理**：对于通过异常和可配置的消息流处理长时间运行的进程，LingPipe 给予了相当多的关注。'
- en: '**Support**: LingPipe has paid employees whose job is to answer your questions
    and make sure that LingPipe is doing its job. The rare bug gets fixed in under
    24 hours typically. They respond to questions very quickly and are very willing
    to help people.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持**：LingPipe 有专职员工负责回答你的问题，并确保 LingPipe 执行其功能。罕见的 bug 通常在 24 小时内得到修复。他们对问题响应非常迅速，并且非常愿意帮助他人。'
- en: '**Consulting**: You can hire experts in LingPipe to build systems for you.
    Generally, they teach developers how to build NLP systems as a byproduct.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**咨询服务**：你可以聘请 LingPipe 的专家为你构建系统。通常，他们会作为副产品教授开发人员如何构建 NLP 系统。'
- en: '**Consistency**: The LingPipe API was designed by one person, Bob Carpenter,
    with an obsession of consistency. While it is not perfect, you will find a regularity
    and eye to design that can be missing in academic efforts. Graduate students come
    and go, and the resulting contributions to university toolkits can be quite varied.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：LingPipe 的 API 由一个人，Bob Carpenter 设计，他非常注重一致性。虽然它并不完美，但你会发现它在设计上的规律性和眼光，这是学术界的工作中可能缺乏的。研究生来来去去，大学工具包中的贡献可能会有所不同。'
- en: '**Open source**: There are many commercial providers, but their software is
    a black box. The open source nature of LingPipe provides transparency and confidence
    that the code is doing what we ask it to do. When the documentation fails, it
    is a huge relief to have access to code to understand it better.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源**：虽然有许多商业供应商，但他们的软件是一个黑盒子。LingPipe 的开源性质提供了透明性，并且让你确信代码按我们要求的方式运行。当文档无法解释时，能够访问源代码来更好地理解它是一个巨大的安慰。'
- en: Downloading the book code and data
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载书籍代码和数据
- en: 'You will need to download the source code for this cookbook, with supporting
    models and data from [http://alias-i.com/book.html](http://alias-i.com/book.html).
    Untar and uncompress it using the following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要从 [http://alias-i.com/book.html](http://alias-i.com/book.html) 下载本书的源代码，支持的模型和数据。使用以下命令解压和解压它：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小提示
- en: '**Downloading the example code**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: You can download the example code files for all Packt books you have purchased
    from your account at [http://www.packtpub.com](http://www.packtpub.com). If you
    purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从你的帐户在 [http://www.packtpub.com](http://www.packtpub.com) 下载你所购买的所有 Packt
    书籍的示例代码文件。如果你在其他地方购买了本书，可以访问 [http://www.packtpub.com/support](http://www.packtpub.com/support)
    并注册，直接将文件通过电子邮件发送给你。
- en: Alternatively, your operating system might provide other ways of extracting
    the archive. All recipes assume that you are running the commands in the resulting
    cookbook directory.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你的操作系统可能提供了其他方式来提取这个压缩包。所有示例假设你是在解压后的书籍目录中运行命令。
- en: Downloading LingPipe
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载 LingPipe
- en: Downloading LingPipe is not strictly necessary, but you will likely want to
    be able to look at the source and have a local copy of the Javadoc.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下载 LingPipe 并不是绝对必要的，但你可能希望能够查看源代码，并拥有本地的 Javadoc 副本。
- en: The download and installation instructions for LingPipe can be found at [http://alias-i.com/lingpipe/web/install.html](http://alias-i.com/lingpipe/web/install.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 的下载和安装说明可以在 [http://alias-i.com/lingpipe/web/install.html](http://alias-i.com/lingpipe/web/install.html)
    找到。
- en: The examples from this chapter use command-line invocation, but it is assumed
    that the reader has sufficient development skills to map the examples to their
    preferred IDE/ant or other environment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了命令行调用，但假设读者具有足够的开发技能，将示例映射到自己偏好的 IDE/ant 或其他环境中。
- en: Deserializing and running a classifier
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反序列化并运行分类器
- en: 'This recipe does two things: introduces a very simple and effective language
    ID classifier and demonstrates how to deserialize a LingPipe class. If you find
    yourself here from a later chapter, trying to understand deserialization, I encourage
    you to run the example program anyway. It will take 5 minutes, and you might learn
    something useful.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱做了两件事：介绍了一个非常简单且有效的语言ID分类器，并演示了如何反序列化LingPipe类。如果你是从后面的章节来到这里，试图理解反序列化，我鼓励你还是运行这个示例程序。它只需要5分钟，或许你会学到一些有用的东西。
- en: Our language ID classifier is based on character language models. Each language
    model gives you the probability of the text, given that it is generated in that
    language. The model that is most familiar with the text is the first best fit.
    This one has already been built, but later in the chapter, you will learn to make
    your own.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言ID分类器基于字符语言模型。每个语言模型会给出文本在该语言中生成的概率。最熟悉该文本的模型是最佳匹配的第一个。这个模型已经构建好了，但在本章稍后的部分，你将学习如何自己构建一个。
- en: How to do it...
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Perform the following steps to deserialize and run a classifier:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤反序列化并运行分类器：
- en: 'Go to the `cookbook` directory for the book and run the command for OSX, Unix,
    and Linux:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 `cookbook` 目录并运行适用于OSX、Unix和Linux的命令：
- en: '[PRE1]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For Windows invocation (quote the classpath and use `;` instead of `:`):'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于Windows调用（请引用类路径并使用 `;` 代替 `:`）：
- en: '[PRE2]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will use the Unix style command line in this book.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本书中我们将使用Unix风格的命令行。
- en: 'The program reports the model being loaded and a default, and prompts for a
    sentence to classify:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序报告加载的模型和默认模型，并提示输入一个待分类的句子：
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The classifier is trained on English, Spanish, and Japanese. We have entered
    an example of each—to get some Japanese, go to [http://ja.wikipedia.org/wiki/](http://ja.wikipedia.org/wiki/).
    These are the only languages it knows about, but it will guess on any text. So,
    let''s try some Arabic:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该分类器已在英语、西班牙语和日语上进行训练。我们已经输入了每种语言的一个示例——要获取一些日语内容，可以访问 [http://ja.wikipedia.org/wiki/](http://ja.wikipedia.org/wiki/)。这些是它所知道的唯一语言，但它会对任何文本进行猜测。所以，让我们试试一些阿拉伯语：
- en: '[PRE4]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It thinks it is Japanese because this language has more characters than English
    or Spanish. This in turn leads that model to expect more unknown characters. All
    the Arabic characters are unknown.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它认为这是日语，因为这种语言比英语或西班牙语有更多字符。这反过来导致该模型预计会有更多未知字符。所有的阿拉伯字符都是未知的。
- en: If you are working with a Windows terminal, you might encounter difficulty entering
    UTF-8 characters.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在使用 Windows 终端，可能会遇到输入 UTF-8 字符的问题。
- en: How it works...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The code in the jar is `cookbook/src/com/lingpipe/cookbook/chapter1/ RunClassifierFromDisk.java`.
    What is happening is that a pre-built model for language identification is deserialized
    and made available. It has been trained on English, Japanese, and Spanish. The
    training data came from Wikipedia pages for each language. You can see the data
    in `data/3LangId.csv`. The focus of this recipe is to show you how to deserialize
    the classifier and run it—training is handled in the *Training your own language
    model classifier* recipe in this chapter. The entire code for the `RunClassifier
    FromDisk.java` class starts with the package; then it imports the start of the
    `RunClassifierFromDisk` class and the start of `main()`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: JAR包中的代码是 `cookbook/src/com/lingpipe/cookbook/chapter1/ RunClassifierFromDisk.java`。这里发生的事情是一个预构建的语言识别模型被反序列化并可用。该模型已在英语、日语和西班牙语上进行训练。训练数据来自每种语言的维基百科页面。你可以在
    `data/3LangId.csv` 中看到这些数据。本示例的重点是向你展示如何反序列化分类器并运行它——训练部分内容请参见本章的*训练你自己的语言模型分类器*食谱。`RunClassifierFromDisk.java`
    类的完整代码从包开始；接着导入 `RunClassifierFromDisk` 类的起始部分以及 `main()` 方法的起始部分：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code is a very standard Java code, and we present it without
    explanation. Next is a feature in most recipes that supplies a default value for
    a file if the command line does not contain one. This allows you to use your own
    data if you have it, otherwise it will run from files in the distribution. In
    this case, a default classifier is supplied if there is no argument on the command
    line:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是非常标准的 Java 代码，我们将其展示而不做解释。接下来是大多数食谱中都有的一个功能，如果命令行中没有提供文件，它会为文件提供一个默认值。这样，如果你有自己的数据，可以使用自己的数据，否则它将从分发版中的文件运行。在这种情况下，如果命令行没有提供参数，则会提供一个默认的分类器：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will see how to deserialize a classifier or another LingPipe object
    from disk:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何从磁盘反序列化一个分类器或其他LingPipe对象：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding code snippet is the first LingPipe-specific code, where the classifier
    is built using the static `AbstractExternalizable.readObject` method.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段是第一个LingPipe特定的代码，其中分类器是使用静态的`AbstractExternalizable.readObject`方法构建的。
- en: This class is employed throughout LingPipe to carry out a compilation of classes
    for two reasons. First, it allows the compiled objects to have final variables
    set, which supports LingPipe's extensive use of immutables. Second, it avoids
    the messiness of exposing the I/O methods required for externalization and deserialization,
    most notably, the no-argument constructor. This class is used as the superclass
    of a private internal class that does the actual compilation. This private internal
    class implements the required `no-arg` constructor and stores the object required
    for `readResolve()`.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类在LingPipe中被广泛使用，用于执行类的编译，原因有两个。首先，它允许编译后的对象设置最终变量，这支持LingPipe对不可变对象的广泛使用。其次，它避免了暴露外部化和反序列化所需的I/O方法，最显著的是无参数构造函数。这个类被用作一个私有内部类的父类，后者执行实际的编译。这个私有内部类实现了所需的`no-arg`构造函数，并存储了`readResolve()`所需的对象。
- en: Note
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The reason we use `Externalizable` instead of `Serializable` is to avoid breaking
    backward compatibility when changing any method signatures or member variables.
    `Externalizable` extends `Serializable` and allows control of how the object is
    read or written. For more information on this, refer to the excellent chapter
    on serialization in Josh Bloch's book, *Effective Java, 2nd Edition*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`Externalizable`而不是`Serializable`的原因是为了避免在更改任何方法签名或成员变量时破坏向后兼容性。`Externalizable`扩展了`Serializable`，并允许控制对象的读写方式。关于这一点的更多信息，请参阅Josh
    Bloch的书籍《*Effective Java, 第二版*》中关于序列化的精彩章节。
- en: '`BaseClassifier<E>` is the foundational classifier interface, with `E` being
    the type of object being classified in LingPipe. Look at the Javadoc to see the
    range of classifiers that implements the interface—there are 10 of them. Deserializing
    to `BaseClassifier<E>` hides a good bit of complexity, which we will explore later
    in the *How to serialize a LingPipe object – classifier example* recipe in this
    chapter.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseClassifier<E>`是基础的分类器接口，其中`E`是LingPipe中被分类的对象类型。查看Javadoc可以看到实现此接口的分类器范围——有10个。反序列化到`BaseClassifier<E>`隐藏了不少复杂性，我们将在本章的《*如何序列化LingPipe对象——分类器示例*》食谱中进一步探讨。'
- en: 'The last line calls a utility method, which we will use frequently in this
    book:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行调用了一个实用方法，我们将在本书中频繁使用：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This method handles interactions with the command line. The code is in `src/com/lingpipe/cookbook/Util.java`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法处理与命令行的交互。代码位于`src/com/lingpipe/cookbook/Util.java`：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the string is read in from the console, then `classifier.classify(input)`
    is called, which returns `Classification`. This, in turn, provides a `String`
    label that is printed out. That's it! You have run a classifier.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从控制台读取字符串，就会调用`classifier.classify(input)`，该方法返回`Classification`。然后，它提供一个`String`标签并打印出来。就这样！你已经运行了一个分类器。
- en: Getting confidence estimates from a classifier
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从分类器获取信心估算
- en: Classifiers tend to be a lot more useful if they give more information about
    how confident they are of the classification—this is usually a score or a probability.
    We often threshold classifiers to help fit the performance requirements of an
    installation. For example, if it was vital that the classifier never makes a mistake,
    then we could require that the classification be very confident before committing
    to a decision.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类器能够提供更多关于其分类信心的信息，通常会更有用——这通常是一个分数或概率。我们经常对分类器进行阈值设置，以帮助满足安装的性能要求。例如，如果分类器绝不能出错，那么我们可能要求分类结果必须非常有信心，才能做出最终决定。
- en: 'LingPipe classifiers exist on a hierarchy based on the kinds of estimates they
    provide. The backbone is a series of interfaces—don''t freak out; it is actually
    pretty simple. You don''t need to understand it now, but we do need to write it
    down somewhere for future reference:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe分类器存在于一个基于它们提供的估算类型的层级结构中。核心是一个接口系列——别慌，它其实非常简单。你现在不需要理解它，但我们确实需要在某个地方写下来，以备将来参考：
- en: '`BaseClassifier<E>`: This is just your basic classifier of objects of type
    `E`. It has a `classify()` method that returns a classification, which in turn
    has a `bestCategory()` method and a `toString()` method that is of some informative
    use.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BaseClassifier<E>`：这只是一个基本的对象分类器，类型为`E`。它有一个`classify()`方法，该方法返回一个分类结果，而分类结果又有一个`bestCategory()`方法和一个具有一定信息用途的`toString()`方法。'
- en: '`RankedClassifier<E> extends BaseClassifier<E>`: The `classify()` method returns
    `RankedClassification`, which extends `Classification` and adds methods for `category(int
    rank)` that says what the 1st to *n*th classifications are. There is also a `size()`
    method that indicates how many classifications there are.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RankedClassifier<E> extends BaseClassifier<E>`：`classify()`方法返回`RankedClassification`，它扩展了`Classification`并添加了`category(int
    rank)`方法，说明第1至*n*个分类是什么。还有一个`size()`方法，表示分类的数量。'
- en: '`ScoredClassifier<E> extends RankedClassifier<E>`: The returned `ScoredClassification`
    adds a `score(int rank)` method.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ScoredClassifier<E> extends RankedClassifier<E>`：返回的`ScoredClassification`添加了一个`score(int
    rank)`方法。'
- en: '`ConditionalClassifier<E> extends RankedClassifier<E>`: `ConditionalClassification`
    produced by this has the property that the sum of scores for all categories must
    sum to 1 as accessed via the `conditionalProbability(int rank)` method and `conditionalProbability(String
    category)`. There''s more; you can read the Javadoc for this. This classification
    will be the work horse of the book when things get fancy, and we want to know
    the confidence that the tweet is English versus the tweet is Japanese versus the
    tweet is Spanish. These estimates will have to sum to 1.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConditionalClassifier<E> extends RankedClassifier<E>`：由此生成的`ConditionalClassification`具有一个特性，即所有类别的分数之和必须为
    1，这可以通过`conditionalProbability(int rank)`方法和`conditionalProbability(String category)`方法访问。还有更多内容；你可以阅读
    Javadoc 了解详细信息。当事情变得复杂时，这种分类方法将成为本书的核心工具，我们希望知道推文是英语、日语还是西班牙语的信心值。这些估算值必须加起来为
    1。'
- en: '`JointClassifier<E> extends ConditionalClassifier<E>`: This provides `JointClassification`
    of the input and category in the space of all the possible inputs, and all such
    estimates sum to 1\. This is a sparse space, so values are log based to avoid
    underflow errors. We don''t see a lot of use of this estimate directly in production.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JointClassifier<E> extends ConditionalClassifier<E>`：这提供了输入和类别在所有可能输入空间中的`JointClassification`，所有这些估算值之和为
    1。由于这是一个稀疏空间，因此值是基于对数的，以避免下溢错误。我们在生产中很少直接使用这种估算值。'
- en: It is obvious that there has been a great deal of thought put into the classification
    stack presented. This is because huge numbers of industrial NLP problems are handled
    by a classification system in the end.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，分类栈的设计考虑了很多因素。这是因为大量的工业 NLP 问题最终都由分类系统处理。
- en: It turns out that our simplest classifier—in some arbitrary sense of simple—produces
    the richest estimates, which are joint classifications. Let's dive in.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证明，我们最简单的分类器——在某种任意的意义上简单——产生了最丰富的估算值，这些估算值是联合分类。让我们深入了解一下。
- en: Getting ready
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In the previous recipe, we blithely deserialized to `BaseClassifier<String>`
    that hid all the details of what was going on. The reality is a bit more complex
    than suggested by the hazy abstract class. Note that the file on disk that was
    loaded is named `3LangId.LMClassifier`. By convention, we name serialized models
    with the type of object it will deserialize to, which, in this case, is `LMClassifier`,
    and it extends `BaseClassifier`. The most specific typing for the classifier is:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们轻松地反序列化为`BaseClassifier<String>`，这隐藏了所有正在发生的细节。事实上，实际情况比模糊的抽象类所暗示的要复杂一些。请注意，加载的磁盘文件名为`3LangId.LMClassifier`。根据约定，我们将序列化的模型命名为它将反序列化为的对象类型，在这种情况下是`LMClassifier`，它扩展了`BaseClassifier`。分类器的最具体类型是：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The cast to `LMClassifier<CompiledNGramBoundaryLM, MultivariateDistribution>`
    specifies the type of distribution to be `MultivariateDistribution`. The Javadoc
    for `com.aliasi.stats.MultivariateDistribution` is quite explicit and helpful
    in describing what this is.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对`LMClassifier<CompiledNGramBoundaryLM, MultivariateDistribution>`的强制转换指定了分布类型为`MultivariateDistribution`。`com.aliasi.stats.MultivariateDistribution`的
    Javadoc 非常明确并且有帮助，详细描述了它是什么。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`MultivariateDistribution` implements a discrete distribution over a finite
    set of outcomes, numbered consecutively from zero.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultivariateDistribution` 实现了一个离散分布，覆盖从零开始连续编号的有限结果集。'
- en: The Javadoc goes into a lot of detail about `MultivariateDistribution`, but
    it basically means that we can have an n-way assignment of probabilities that
    sum to 1.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc详细介绍了`MultivariateDistribution`，但基本上意味着我们可以进行 n 维的概率分配，这些概率之和为 1。
- en: 'The next class in the cast is for `CompiledNGramBoundaryLM`, which is the "memory"
    of the `LMClassifier`. In fact, each language gets its own. This means that English
    will have a separate language model from Spanish and so on. There are eight different
    kinds of language models that could have been used as this part of the classifier—consult
    the Javadoc for the `LanguageModel` interface. Each **language model** (**LM**)
    has the following properties:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要介绍的类是`CompiledNGramBoundaryLM`，它是`LMClassifier`的“记忆”。实际上，每种语言都有自己的记忆模型。这意味着，英语将有一个与西班牙语不同的语言模型，依此类推。分类器的这一部分可以使用八种不同类型的语言模型——请参阅`LanguageModel`接口的Javadoc。每个**语言模型**（**LM**）具有以下属性：
- en: The LM will provide a probability that it generated the text provided. It is
    robust against data that it has not seen before, in the sense that it won't crash
    or give a zero probability. Arabic just comes across as a sequence of unknown
    characters for our example.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LM会提供一个概率，表示它生成了给定的文本。它对之前未见过的数据具有鲁棒性，意味着它不会崩溃或给出零概率。对于我们的例子，阿拉伯语仅仅表现为一串未知字符。
- en: The sum of all the possible character sequence probabilities of any length is
    1 for boundary LMs. Process LMs sum the probability to 1 over all sequences of
    the same length. Look at the Javadoc for how this bit of math is done.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于边界语言模型（LM），任何长度的所有可能字符序列概率的总和为1。过程型LM则将相同长度所有序列的概率总和为1。查看Javadoc以了解这部分数学如何进行。
- en: Each language model has no knowledge of data outside of its category.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个语言模型对于其类别外的数据没有任何了解。
- en: The classifier keeps track of the marginal probability of the category and factors
    this into the results for the category. Marginal probability is saying that we
    tend to see two-thirds English, one-sixth Spanish, and one-sixth Japanese in Disney
    tweets. This information is combined with the LM estimates.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器会跟踪类别的边际概率，并将其纳入该类别的结果中。边际概率意味着我们通常会看到三分之二是英语，一六分之一是西班牙语，一六分之一是日语的迪士尼推文。这些信息与LM的估计结果结合在一起。
- en: The LM is a compiled version of `LanguageModel.Dynamic` that we will cover in
    the later recipes that discuss training.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LM是`LanguageModel.Dynamic`的编译版本，我们将在后续的配方中讨论训练时如何使用。
- en: '`LMClassifier` that is constructed wraps these components into a classifier.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 构造的`LMClassifier`将这些组件封装成一个分类器。
- en: 'Luckily, the interface saves the day with a more aesthetic deserialization:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，接口通过更具美学感的反序列化解决了这个问题：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The interface hides the guts of the implementation nicely and this is what we
    are going with in the example program.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该接口巧妙地隐藏了实现的内部细节，这是我们在示例程序中所采用的方式。
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'This recipe is the first time we start peeling away from what classifiers can
    do, but first, let''s play with it a bit:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤是我们第一次开始从分类器的功能中剥离出来，但首先，让我们玩一玩：
- en: 'Get your magic shell genie to conjure a command prompt with a Java interpreter
    and type:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让你的魔法shell精灵召唤一个命令提示符，并输入：
- en: '[PRE12]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will enter the same data as we did earlier:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将输入与之前相同的数据：
- en: '[PRE13]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As described, `JointClassification` carries through all the classification
    metrics in the hierarchy rooted at `Classification`. Each level of classification
    shown as follows adds to the classifiers preceding it:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`JointClassification`会将所有分类指标从根分类`Classification`传递下去。如下所示的每一层分类都会增加前面分类器的内容：
- en: '`Classification` provides the first best category as the rank 0 category.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Classification`提供了第一个最佳类别，作为排名0的类别。'
- en: '`RankedClassification` adds an ordering of all the possible categories with
    a lower rank corresponding to greater likelihood of the category. The `rank` column
    reflects this ordering.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RankedClassification`按所有可能类别的顺序添加，其中较低的排名对应更高的类别可能性。`rank`列反映了这种排序。'
- en: '`ScoredClassification` adds a numeric score to the ranked output. Note that
    scores might or might not compare well against other strings being classified
    depending on the type of classifier. This is the column labeled `Score`. To understand
    the basis of this score, consult the relevant Javadoc.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ScoredClassification`为排名输出添加了一个数字分数。请注意，根据分类器的类型，分数可能与其他正在分类的字符串进行比较时表现不佳。该列为`Score`。要理解该分数的依据，请参阅相关的Javadoc。'
- en: '`ConditionalClassification` further refines the score by making it a category
    probability conditioned on the input. The probabilities of all categories will
    sum up to 1\. This is the column labeled `P(Category|Input)`, which is the traditional
    way to write *probability of the category given the input*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ConditionalClassification`通过将其设为基于输入的类别概率来进一步细化分数。所有类别的概率将加起来为1。这个列被标记为`P(Category|Input)`，这是传统的写法，表示*给定输入的类别概率*。'
- en: '`JointClassification` adds the log2 (log base 2) probability of the input and
    the category—this is the joint probability. The probabilities of all categories
    and inputs will sum up to 1, which is a very large space indeed with very low
    probabilities assigned to any pair of category and string. This is why log2 values
    are used to prevent numerical underflow. This is the column labeled `log 2 P(Category,
    Input)`, which is translated as *the log**2* *probability of the category and
    input*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JointClassification`增加了输入和类别的log2（以2为底的对数）概率——这是联合概率。所有类别和输入的概率将加起来为1，这实际上是一个非常大的空间，任何类别和字符串对的概率都非常低。这就是为什么使用log2值来防止数值下溢的原因。这一列被标记为`log
    2 P(Category, Input)`，它被翻译为*类别和输入的log**2* *概率*。'
- en: Look at the Javadoc for the `com.aliasi.classify` package for more information
    on the metrics and classifiers that implement them.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`com.aliasi.classify`包的Javadoc，了解实现这些度量和分类器的更多信息。
- en: How it works…
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'The code is in `src/com/lingpipe/cookbook/chapter1/RunClassifierJoint.java`,
    and it deserializes to a `JointClassifier<CharSequence>`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于`src/com/lingpipe/cookbook/chapter1/RunClassifierJoint.java`，它反序列化为`JointClassifier<CharSequence>`：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It makes a call to `Util.consoleInputPrintClassification(classifier)`, which
    minimally differs from `Util.consoleInputBestCategory(classifier)`, in that it
    uses the `toString()` method of classification to print. The code is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用`Util.consoleInputPrintClassification(classifier)`，这个方法与`Util.consoleInputBestCategory(classifier)`只有最小的区别，区别在于它使用分类的`toString()`方法来打印。代码如下：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We got a richer output than we expected, because the type is `Classification`,
    but the `toString()` method will be applied to the runtime type `JointClassification`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了比预期更丰富的输出，因为类型是`Classification`，但`toString()`方法将应用于运行时类型`JointClassification`。
- en: See also
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: There is detailed information in [Chapter 6](ch06.html "Chapter 6. String Comparison
    and Clustering"), *Character Language Models* of *Text Analysis with LingPipe
    4*, by *Bob Carpenter* and *Breck Baldwin*, *LingPipe Publishing* ([http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf](http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf))
    on language models.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[第六章](ch06.html "第六章：字符串比较与聚类")中，有详细信息，*《使用LingPipe 4进行文本分析》*中，*Bob Carpenter*和*Breck
    Baldwin*编写的*字符语言模型*部分，*LingPipe出版*（[http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf](http://alias-i.com/lingpipe-book/lingpipe-book-0.5.pdf)）介绍了语言模型。
- en: Getting data from the Twitter API
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Twitter API获取数据
- en: We use the popular `twitter4j` package to invoke the Twitter Search API, and
    search for tweets and save them to disk. The Twitter API requires authentication
    as of Version 1.1, and we will need to get authentication tokens and save them
    in the `twitter4j.properties` file before we get started.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用流行的`twitter4j`包来调用Twitter搜索API，搜索推文并将其保存到磁盘。Twitter API从版本1.1开始要求身份验证，我们需要获取认证令牌并将其保存在`twitter4j.properties`文件中，然后才能开始。
- en: Getting ready
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'If you don''t have a Twitter account, go to [twitter.com/signup](http://twitter.com/signup)
    and create an account. You will also need to go to [dev.twitter.com](http://dev.twitter.com)
    and sign in to enable yourself for the developer account. Once you have a Twitter
    login, we''ll be on our way to creating the Twitter OAuth credentials. Be prepared
    for this process to be different from what we are presenting. In any case, we
    will supply example results in the `data` directory. Let''s now create the Twitter
    OAuth credentials:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有Twitter账号，去[twitter.com/signup](http://twitter.com/signup)创建一个账号。你还需要访问[dev.twitter.com](http://dev.twitter.com)并登录，以启用你的开发者账号。一旦你有了Twitter登录，我们就可以开始创建Twitter
    OAuth凭证。请准备好这个过程可能与你所看到的不同。无论如何，我们会在`data`目录提供示例结果。现在让我们来创建Twitter OAuth凭证：
- en: Log in to [dev.twitter.com](http://dev.twitter.com).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到[dev.twitter.com](http://dev.twitter.com)。
- en: Find the little pull-down menu next to your icon on the top bar.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到顶部栏上你图标旁边的小下拉菜单。
- en: Choose **My Applications**.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**我的应用**。
- en: Click on **Create a new application**.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建一个新应用**。
- en: Fill in the form and click on **Create a Twitter application**.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写表单并点击**创建Twitter应用**。
- en: The next page contains the OAuth settings.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一页包含OAuth设置。
- en: Click on the **Create my access token** link.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建我的访问令牌**链接。
- en: You will need to copy **Consumer key** and **Consumer secret**.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要复制**消费者密钥**和**消费者密钥密钥**。
- en: You will also need to copy **Access token** and **Access token secret**.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还需要复制**访问令牌**和**访问令牌密钥**。
- en: 'These values should go into the `twitter4j.properties` file in the appropriate
    locations. The properties are as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些值应放入`twitter4j.properties`文件中的适当位置。属性如下：
- en: '[PRE16]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行操作...
- en: 'Now, we''re ready to access Twitter and get some search data using the following
    steps:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好通过以下步骤访问Twitter并获取一些搜索数据：
- en: 'Go to the directory of this chapter and run the following command:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入本章节的目录并运行以下命令：
- en: '[PRE17]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The code displays the output file (in this case, a default value). Supplying
    a path as an argument will write to this file. Then, type in your query at the
    prompt:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码显示输出文件（在本例中为默认值）。提供路径作为参数将写入此文件。然后，在提示符下键入您的查询：
- en: '[PRE18]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The code then queries Twitter and reports every 100 tweets found (output truncated):'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，代码查询Twitter，并报告每找到100条推文的结果（输出被截断）：
- en: '[PRE19]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This program uses the search query, searches Twitter for the term, and writes
    the output (limited to 1500 tweets) to the `.csv` file name that you specified
    on the command line or uses a default.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序使用搜索查询，搜索Twitter中的相关术语，并将输出（限制为1500条推文）写入您在命令行中指定的`.csv`文件名，或者使用默认值。
- en: How it works...
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The code uses the `twitter4j` library to instantiate `TwitterFactory` and searches
    Twitter using the user-entered query. The start of `main()` at `src/com/lingpipe/cookbook/chapter1/TwitterSearch.java`
    is:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用`twitter4j`库实例化`TwitterFactory`并使用用户输入的查询搜索Twitter。`main()`方法的开始部分位于`src/com/lingpipe/cookbook/chapter1/TwitterSearch.java`中：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding code gets the outfile, supplying a default if none is provided,
    and takes the query from the command line.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码获取输出文件，如果没有提供，则使用默认值，并从命令行获取查询。
- en: 'The following code sets up the query according to the vision of the twitter4j
    developers. For more information on this process, read their Javadoc. However,
    it should be fairly straightforward. In order to make our result set more unique,
    you''ll notice that when we create the query string, we will filter out retweets
    using the `-filter:retweets` option. This is only somewhat effective; see the
    *Eliminate near duplicates with the Jaccard distance* recipe later in this chapter
    for a more complete solution:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码根据twitter4j开发者的设计设置查询。有关此过程的更多信息，请阅读他们的Javadoc。然而，这应该是相当直接的。为了使我们的结果集更加唯一，您会注意到在创建查询字符串时，我们会使用`-filter:retweets`选项来过滤掉转发。这只是一种部分有效的方法；有关更完整的解决方案，请参阅本章后面的*通过Jaccard距离消除近重复项*一节：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will get the following result:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下结果：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding snippet is a pretty standard code slinging, albeit without the
    usual hardening for external facing code—try/catch, timeouts, and retries. One
    potentially confusing bit is the use of `query` to handle paging through the search
    results—it returns `null` when no more pages are available. The current Twitter
    API allows a maximum of 100 results per page, so in order to get 1500 results,
    we need to rerun the search until there are no more results, or until we get 1500
    tweets. The next step involves a bit of reporting and writing:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段是相当标准的代码实现，尽管没有通常的面向外部代码的加固——try/catch、超时和重试。一个可能让人困惑的地方是使用`query`来处理搜索结果的分页——当没有更多页面时，它会返回`null`。当前的Twitter
    API每页最多返回100个结果，因此为了获得1500个结果，我们需要重新运行搜索，直到没有更多结果，或者直到我们获得1500条推文。下一步涉及一些报告和写入：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The list of tweets is then written to a `.csv` file using the `Util.writeCsvAddHeader`
    method:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用`Util.writeCsvAddHeader`方法将推文列表写入`.csv`文件：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will be using this `.csv` file to run the language ID test in the next section.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节使用这个`.csv`文件进行语言识别测试。
- en: See also
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'For more details on using the Twitter API and twitter4j, please go to their
    documentation pages:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关使用Twitter API和twitter4j的更多详细信息，请访问他们的文档页面：
- en: '[http://twitter4j.org/javadoc/](http://twitter4j.org/javadoc/)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://twitter4j.org/javadoc/](http://twitter4j.org/javadoc/)'
- en: '[https://dev.twitter.com/docs](https://dev.twitter.com/docs)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://dev.twitter.com/docs](https://dev.twitter.com/docs)'
- en: Applying a classifier to a .csv file
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对`.csv`文件应用分类器
- en: Now, we can test our language ID classifier on the data we downloaded from Twitter.
    This recipe will show you how to run the classifier on the `.csv` file and will
    set the stage for the evaluation step in the next recipe.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在从Twitter下载的数据上测试我们的语言ID分类器。本方案将向你展示如何在`.csv`文件上运行分类器，并为下一个方案中的评估步骤打下基础。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Applying a classifier to the `.csv` file is straightforward! Just perform the
    following steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类器应用于`.csv`文件是非常简单的！只需执行以下步骤：
- en: 'Get a command prompt and run:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取命令提示符并运行：
- en: '[PRE25]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will use the default CSV file from the `data/disney.csv` distribution,
    run over each line of the CSV file, and apply a language ID classifier from `models/
    3LangId.LMClassifier` to it:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将使用`data/disney.csv`分发中的默认CSV文件，逐行处理CSV文件，并应用来自`models/ 3LangId.LMClassifier`的语言ID分类器：
- en: '[PRE26]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You can also specify the input as the first argument and the classifier as the
    second one.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以将输入指定为第一个参数，将分类器指定为第二个参数。
- en: How it works…
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We will deserialize a classifier from the externalized model that was described
    in the previous recipes. Then, we will iterate through each line of the `.csv`
    file and call the classify method of the classifier. The code in `main()` is:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从之前的方案中描述的外部模型反序列化一个分类器。然后，我们将遍历每一行`.csv`文件，并调用分类器的分类方法。`main()`中的代码是：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code builds on the previous recipes with nothing particularly
    new. `Util.readCsvRemoveHeader`, shown as follows, just skips the first line of
    the `.csv` file before reading from disk and returning the rows that have non-null
    values and non-empty strings in the `TEXT_OFFSET` position:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码基于之前的方案，没有特别新的内容。`Util.readCsvRemoveHeader`，如下所示，它在从磁盘读取并返回具有非空值和非空字符串的行之前，会跳过`.csv`文件的第一行，并将`TEXT_OFFSET`位置的数据返回：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Evaluation of classifiers – the confusion matrix
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器的评估 —— 混淆矩阵
- en: Evaluation is incredibly important in building solid NLP systems. It allows
    developers and management to map a business need to system performance, which,
    in turn, helps communicate system improvement to vested parties. "Well, uh, the
    system seems to be doing better" does not hold the gravitas of "Recall has improved
    20 percent, and the specificity is holding well with 50 percent more training
    data".
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 评估在构建稳固的自然语言处理系统中至关重要。它使开发人员和管理层能够将业务需求与系统性能进行映射，从而帮助向利益相关方传达系统改进的情况。“嗯，系统似乎做得更好”并不如“召回率提高了20％，特异性在增加50％的训练数据下仍然保持良好”那样有分量。
- en: This recipe provides the steps for the creation of truth or *gold standard*
    data and tells us how to use this data to evaluate the performance of our precompiled
    classifier. It is as simple as it is powerful.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本方案提供了创建真值或*黄金标准*数据的步骤，并告诉我们如何使用这些数据来评估我们预编译分类器的性能。它既简单又强大。
- en: Getting ready
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You might have noticed the headers from the output of the CSV writer and the
    suspiciously labeled column, `TRUTH`. Now, we get to use it. Load up the tweets
    we provided earlier or convert your data into the format used in our `.csv` format.
    An easy way to get novel data is to run a query against Twitter with a multilingual
    friendly query such as `Disney`, which is our default supplied data.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到CSV写入器输出中的表头和那个标记为`TRUTH`的列。现在，我们可以开始使用它了。加载我们之前提供的推文，或者将你的数据转换为我们`.csv`格式使用的格式。获取新数据的简单方法是通过Twitter运行一个多语言友好的查询，例如`Disney`，这是我们默认提供的数据。
- en: 'Open the CSV file and annotate the language you think the tweet is in for at
    least 10 examples each of *e* for English and *n* for non-English. There is a
    `data/disney_e_n.csv` file in the distribution; you can use this if you don''t
    want to deal with annotating data. If you are not sure about a tweet, feel free
    to ignore it. Unannotated data is ignored. Have a look at the following screenshot:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 打开CSV文件，并为至少10个示例标注你认为推文所用的语言，*e*代表英语，*n*代表非英语。如果你不想手动标注数据，分发包中有一个`data/disney_e_n.csv`文件，你可以使用这个文件。如果你对某条推文不确定，可以忽略它。未标注的数据会被忽略。请看下面的截图：
- en: '![Getting ready](img/4672OS_01_01.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/4672OS_01_01.jpg)'
- en: Screenshot of the spreadsheet with human annotations for English 'e' and non-English
    'n'. It is known as truth data or gold standard data because it represents the
    phenomenon correctly.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是包含英文'e'和非英文'n'的人类标注的电子表格截图。它被称为真值数据或黄金标准数据，因为它正确地表示了该现象。
- en: Often, this data is called **gold standard data**, because it represents the
    truth. The "gold" in "gold standard" is quite literal. Back it up and store it
    with longevity in mind—it is most likely that it is the single-most valuable collection
    of bytes on your hard drive, because it is expensive to produce in any quantity
    and the cleanest articulation of what is being done. Implementations come and
    go; evaluation data lives on forever. The John Smith corpus from the *The John
    Smith problem* recipe, in [Chapter 7](ch07.html "Chapter 7. Finding Coreference
    Between Concepts/People"), *Finding Coreference Between Concepts/People*, is the
    canonical evaluation corpus for that particular problem and lives on as the point
    of comparison for a line of research that started in 1997\. The original implementation
    is long forgotten.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些数据被称为**黄金标准数据**，因为它代表了真相。"黄金标准"中的"黄金"是字面意思。备份并长期存储这些数据——它很可能是你硬盘上最宝贵的一组字节，因为它的生产成本很高，而且最清晰地表达了所做的工作。实现方式会不断变化；而评估数据则永远存在。来自*The
    John Smith problem*案例的约翰·史密斯语料库，在[第7章](ch07.html "第7章. 查找概念/人物之间的共指关系")，*查找概念/人物之间的共指关系*，是该特定问题的权威评估语料库，并成为1997年开始的研究线的比较标准。最初的实现早已被遗忘。
- en: How to do it...
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Perform the following steps to evaluate the classifiers:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来评估分类器：
- en: 'Enter the following in the command prompt; this will run the default classifier
    on the texts in the default gold standard data. Then, it will compare the classifier''s
    best category against what was annotated in the `TRUTH` column:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符中输入以下内容；这将运行默认分类器，对默认黄金标准数据中的文本进行分类。然后，它将比较分类器的最佳类别与在`TRUTH`列中标注的内容。
- en: '[PRE29]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This class will then produce the confusion matrix:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该类将生成混淆矩阵：
- en: '[PRE30]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The confusion matrix is aptly named since it confuses almost everyone initially,
    but it is, without a doubt, the best representation of classifier output, because
    it is very difficult to hide bad classifier performance with it. In other words,
    it is an excellent BS detector. It is the unambiguous view of what the classifier
    got right, what it got wrong, and what it thought was the right answer.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵恰如其名，因为它最初几乎让每个人都感到困惑，但毫无疑问，它是分类器输出的最佳表示方式，因为它很难掩盖分类器的差劲表现。换句话说，它是一个优秀的虚假检测器。它清晰地展示了分类器做对了什么，做错了什么，以及它认为正确的答案是什么。
- en: The sum of each row represents the items that are known by truth/reference/gold
    standard to belong to the category. For English (e) there were 11 tweets. Each
    column represents what the system thought was in the same labeled category. For
    English (e), the system thought 11 tweets were English and none were non-English
    (n). For the non-English category (n), there are 10 cases in truth, of which the
    classifier thought 1 was English (incorrectly) and 9 were non-English (correctly).
    Perfect system performance will have zeros in all the cells that are not located
    diagonally, from the top-left corner to the bottom-right corner.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行的总和表示由真相/参考/黄金标准所知的属于该类别的项目。对于英语（e），共有11条推文。每一列表示系统认为属于相同标签类别的内容。对于英语（e），系统认为这11条推文都是英语，且没有非英语（n）。对于非英语类别（n），在真相中有10个案例，其中分类器错误地认为1个是英语（错误），并正确地认为9个是非英语（正确）。完美的系统表现会使所有非对角线位置的单元格值为零，从左上角到右下角。
- en: The real reason it is called a confusion matrix is that it is relatively easy
    to see categories that the classifier is confusing. For example, British English
    and American English would likely be highly confusable. Also, confusion matrices
    scale to multiple categories quite nicely, as will be seen later. Visit the Javadoc
    for a more detailed explanation of the confusion matrix—it is well worth mastering.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 它之所以被称为混淆矩阵的真正原因是，因为它很容易看出分类器混淆的类别。例如，英式英语和美式英语可能会高度混淆。此外，混淆矩阵能够很好地扩展到多个类别，稍后会看到。访问Javadoc以获取更详细的混淆矩阵说明——它值得深入掌握。
- en: How it works...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Building on the code from the previous recipes in this chapter, we will focus
    on what is novel in the evaluation setup. The entirety of the code is in the distribution
    at `src/com/lingpipe/cookbook/chapter1/RunConfusionMatrix.java`. The start of
    `main()` is shown in the following code snippet. The code starts by reading from
    the arguments that look for non-default CSV data and serialized classifiers. Defaults,
    which this recipe uses, are shown here:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 基于本章前面配方中的代码，我们将重点介绍评估设置中的新颖之处。完整的代码位于`src/com/lingpipe/cookbook/chapter1/RunConfusionMatrix.java`。`main()`方法的开头显示在下面的代码片段中。代码首先从命令行参数中读取，寻找非默认的CSV数据和序列化分类器。此配方使用的默认值如下所示：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, the language model and the `.csv` data will be loaded. The method differs
    slightly from the `Util.CsvRemoveHeader` explanation, in that it only accepts
    rows that have a value in the `TRUTH` column—see `src/com/lingpipe/cookbook/Util.java`
    if this is not clear:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将加载语言模型和`.csv`数据。该方法与`Util.CsvRemoveHeader`的解释略有不同，它仅接受在`TRUTH`列中有值的行—如果不清楚，请参阅`src/com/lingpipe/cookbook/Util.java`：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, the categories will be found:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将找到类别：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The method will accumulate all the category labels from the `TRUTH` column.
    The code is simple and is shown here:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将累积`TRUTH`列中的所有类别标签。代码很简单，下面显示了代码：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The code will be useful when we run arbitrary data, where the labels are not
    known at compile time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行任意数据且标签在编译时未知时，这段代码将非常有用。
- en: 'Then, we will set up `BaseClassfierEvaluator`. This requires the classifier
    to be evaluated. The categories and a `boolean` value that controls whether inputs
    are stored in the classifier for construction will also be set up:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将设置`BaseClassfierEvaluator`。这需要评估的分类器。类别和控制分类器是否为构造存储输入的`boolean`值也将进行设置：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that the classifier can be null and specified at a later time; the categories
    must exactly match those produced by the annotation and the classifier. We will
    not bother configuring the evaluator to store the inputs, because we are not going
    to use this capability in this recipe. See the *Viewing error categories – false
    positives* recipe for an example in which the inputs are stored and accessed.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，分类器可以为null，并且可以在稍后的时间指定；类别必须与注释和分类器产生的类别完全匹配。我们不会配置评估器来存储输入，因为在此配方中我们不打算使用此功能。请参阅*查看错误类别
    – 假阳性*配方，其中存储并访问了输入。
- en: 'Next, we will do the actual evaluation. The loop will iterate over each row
    of the information in the `.csv` file, build a `Classified<CharSequence>`, and
    pass it off to the evaluator''s `handle()` method:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行实际评估。循环将遍历`.csv`文件中的每一行信息，构建一个`Classified<CharSequence>`对象，并将其传递给评估器的`handle()`方法：
- en: '[PRE36]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The fourth line will create a classification object with the value from the
    truth annotation—*e* or *n* in this case. This is the same type as the one `BaseClassifier<E>`
    returns for the `bestCategory()` method. There is no special type for truth annotations.
    The next line adds in the text that the classification applies to and we get a
    `Classified<CharSequence>` object.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第四行将使用真值注释中的值创建一个分类对象—在这种情况下是*e*或*n*。这与`BaseClassifier<E>`为`bestCategory()`方法返回的类型相同。真值注释没有特殊类型。下一行添加了分类所应用的文本，我们得到了一个`Classified<CharSequence>`对象。
- en: The last line of the loop will apply the handle method to the created classified
    object. The evaluator assumes that data supplied to its handle method is a truth
    annotation, which is handled by extracting the data being classified, applying
    the classifier to this data, getting the resulting `firstBest()` classification,
    and finally noting whether the classification matches that of what was just constructed
    with the truth. This happens for each row of the `.csv` file.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 循环的最后一行将对创建的分类对象应用`handle`方法。评估器假设其`handle`方法所提供的数据是一个真值注释，该数据通过提取待分类数据、应用分类器进行分类、获得结果中的`firstBest()`分类，然后标记分类是否与刚刚构造的真值匹配。对于`.csv`文件中的每一行都会发生这种情况。
- en: 'Outside the loop, we will print out the confusion matrix with `Util.createConfusionMatrix()`:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环外，我们将使用`Util.createConfusionMatrix()`打印出混淆矩阵：
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Examining this code is left to the reader. That's it; we have evaluated our
    classifier and printed out the confusion matrix.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 本段代码的详细分析留给读者自行阅读。就是这样；我们已经评估了分类器并打印出了混淆矩阵。
- en: There's more...
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: The evaluator has a complete `toString()` method that is a bit of a fire hose
    for information on just how well your classifier did. Those aspects of the output
    will be covered in later recipes. The Javadoc is quite extensive and well worth
    reading.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器有一个完整的`toString()`方法，可以提供大量的信息，说明你的分类器表现如何。输出中的这些方面将在后续配方中讲解。Javadoc非常详尽，值得一读。
- en: Training your own language model classifier
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练你自己的语言模型分类器
- en: The world of NLP really opens up when classifiers are customized. This recipe
    provides details on how to customize a classifier by collecting examples for the
    classifier to learn from—this is called training data. It is also called gold
    standard data, truth, or ground truth. We have some from the previous recipe that
    we will use.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当分类器被定制时，自然语言处理的世界真正开始展开。本配方提供了如何通过收集分类器学习的示例来定制分类器的详细信息——这叫做训练数据。它也叫做黄金标准数据、真实值或地面真相。我们有一些来自前面配方的数据，将用它们。
- en: Getting ready
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will create a customized language ID classifier for English and other languages.
    Creation of training data involves getting access to text data and then annotating
    it for the categories of the classifier—in this case, annotation is the language.
    Training data can come from a range of sources. Some possibilities include:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为英语和其他语言创建一个定制的语言识别分类器。训练数据的创建涉及获取文本数据，并为分类器的类别进行标注——在这个例子中，标注的就是语言。训练数据可以来自多种来源。以下是一些可能性：
- en: Gold standard data such as the one created in the preceding evaluation recipe.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如在前面评估配方中创建的黄金标准数据。
- en: Data that is somehow already annotated for the categories you care about. For
    example, Wikipedia has language-specific versions, which make easy pickings to
    train up a language ID classifier. This is how we created the `3LangId.LMClassifier`
    model.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经以某种方式注释过的数据，针对你关心的类别。例如，维基百科有语言特定版本，方便用来训练语言识别分类器。这就是我们如何创建`3LangId.LMClassifier`模型的方式。
- en: Be creative—where is the data that helps guide a classifier in the right direction?
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要有创意——数据在哪里能帮助引导分类器朝正确方向发展？
- en: Language ID doesn't require much data to work well, so 20 tweets per language
    will start to reliably distinguish strongly different languages. The amount of
    training data will be driven by evaluation—more data generally improves performance.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 语言识别不需要太多数据就能很好地工作，因此每种语言20条推文就能开始可靠地区分出不同的语言。训练数据的数量将由评估结果驱动——一般来说，更多的数据能提高性能。
- en: The example assumes that around 10 tweets of English and 10 non-English tweets
    have been annotated by people and put in `data/disney_e_n.csv`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例假设大约10条英文推文和10条非英文推文已由人工注释并存放在`data/disney_e_n.csv`中。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In order to train your own language model classifier, perform the following
    steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练你自己的语言模型分类器，请执行以下步骤：
- en: 'Fire up a terminal and type the following:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个终端并输入以下内容：
- en: '[PRE38]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, type some English in the command prompt, perhaps, a Kurt Vonnegut quotation,
    to see the resulting `JointClassification`. See the *Getting confidence estimates
    from a classifier* recipe for the explanation of the following output:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在命令提示符中输入一些英文文本，或许是库尔特·冯内古特的名言，来查看生成的`JointClassification`。有关以下输出的解释，请参见*从分类器获取置信度估计*的配方：
- en: '[PRE39]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Type in some non-English, such as the Spanish title of Borge''s *The Garden
    of the Forking Paths*:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入一些非英语文本，例如博尔赫斯的《*分岔小径*》的西班牙语标题：
- en: '[PRE40]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The program is in `src/com/lingpipe/cookbook/chapter1/TrainAndRunLMClassifier.java`;
    the contents of the `main()` method start with:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 程序位于`src/com/lingpipe/cookbook/chapter1/TrainAndRunLMClassifier.java`；`main()`方法的内容如下：
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The preceding code gets the contents of the `.csv` file and then extracts the
    list of categories that were annotated; these categories will be all the non-empty
    strings in the annotation column.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码获取`.csv`文件的内容，然后提取已注释的类别列表；这些类别将是注释列中所有非空的字符串。
- en: 'The following `DynamicLMClassifier` is created using a static method that requires
    the array of categories and `int`, which is the order of the language models.
    With an order of 3, the language model will be trained on all 1 to 3 character
    sequences of the text training data. So "I luv Disney" will produce training instances
    of "I", "I ", "I l", " l", " lu", "u", "uv", "luv", and so on. The `createNGramBoundary`
    method appends a special token to the beginning and end of each text sequence;
    this token can help if the beginnings or ends are informative for classification.
    Most text data is sensitive to beginnings/ends, so we will choose this model:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`DynamicLMClassifier`是通过一个静态方法创建的，该方法需要一个类别数组和`int`类型的语言模型顺序。顺序为3时，语言模型将在文本训练数据的所有1至3字符序列上进行训练。因此，“I
    luv Disney”将生成如“I”，“I ”，“I l”，“ l”，“ lu”，“u”，“uv”，“luv”等训练实例。`createNGramBoundary`方法会在每个文本序列的开始和结束处附加一个特殊符号；这个符号如果开头或结尾对于分类有帮助的话，会很有用。大多数文本数据对开头/结尾是敏感的，所以我们会选择这个模型：
- en: '[PRE42]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The following code iterates over the rows of training data and creates `Classified<CharSequence>`
    in the same way as shown in the *Evaluation of classifiers – the confusion matrix*
    recipe for evaluation. However, instead of passing the `Classified` object to
    an evaluation handler, it is used to train the classifier.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码遍历训练数据的行，并以与*分类器评估 – 混淆矩阵*食谱中相同的方式创建`Classified<CharSequence>`。然而，它不是将`Classified`对象传递给评估处理器，而是用来训练分类器。
- en: '[PRE43]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'No further steps are necessary, and the classifier is ready for use by the
    console:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要进一步的步骤，分类器已经准备好可以通过控制台使用：
- en: '[PRE44]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: There's more...
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Training and using the classifier can be interspersed for classifiers based
    on `DynamicLM`. This is generally not the case with other classifiers such as
    `LogisticRegression`, because they use all the data to compile a model that can
    carry out classifications.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于`DynamicLM`的分类器，训练和使用可以交替进行。这通常不是其他分类器（如`LogisticRegression`）的情况，因为后者使用所有数据来编译一个模型，进行分类。
- en: 'There is another method for training the classifier that gives you more control
    over how the training goes. The following is the code snippet for this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种训练分类器的方法，可以让你更好地控制训练过程。以下是这种方法的代码片段：
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Alternatively, we can have the same effect with:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过以下方式实现相同的效果：
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `train()` method allows an extra degree of control for training, because
    it allows for the count to be explicitly set. As we explore LingPipe classifiers,
    we will often see an alternate way of training that allows for some additional
    control beyond what the `handle()` method provides.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`train()`方法提供了更多的训练控制，因为它允许显式设置计数。在我们探讨LingPipe分类器时，我们通常会看到一种替代的训练方法，提供了一些额外的控制，超出了`handle()`方法所提供的控制。'
- en: Character-language model-based classifiers work very well for tasks where character
    sequences are distinctive. Language identification is an ideal candidate for this,
    but it can also be used for tasks such as sentiment, topic assignment, and question
    answering.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 基于字符语言模型的分类器在字符序列具有独特性的任务中表现非常好。语言识别是一个理想的候选任务，但它也可以用于情感分析、主题分配和问答等任务。
- en: See also
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The Javadoc for LingPipe's classifiers are quite extensive on the underlying
    math that drives the technology.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe分类器的Javadoc在其底层数学方面非常详细，解释了技术背后的原理。
- en: How to train and evaluate with cross validation
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用交叉验证进行训练和评估
- en: 'The earlier recipes have shown how to evaluate classifiers with truth data
    and how to train with truth data but how about doing both? This great idea is
    called cross validation, and it works as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的食谱展示了如何使用真实数据评估分类器，以及如何使用真实数据训练分类器，但如果要同时进行这两者呢？这个好主意叫做交叉验证，其工作原理如下：
- en: Split the data into *n* distinct sets or folds—the standard *n* is 10.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为*n*个不同的集合或折叠——标准的*n*是10。
- en: 'For *i* from 1 to *n*:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*i*从1到*n*：
- en: Train on the *n - 1* folds defined by the exclusion of fold *i*
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在通过排除第*i*折叠定义的*n - 1*折叠上进行训练
- en: Evaluate on fold *i*
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第*i*折上进行评估
- en: Report the evaluation results across all folds *i*.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告所有折叠*i*的评估结果。
- en: 'This is how most machine-learning systems are tuned for performance. The work
    flow is as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大多数机器学习系统调优性能的方式。工作流程如下：
- en: See what the cross validation performance is.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看交叉验证的性能。
- en: Look at the error as determined by an evaluation metric.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看通过评估指标确定的错误。
- en: Look at the actual errors—yes, the data—for insights into how the system can
    be improved.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看实际错误——是的，就是数据——以洞察系统如何改进。
- en: Make some changes
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 做一些修改
- en: Evaluate it again.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次评估它。
- en: Cross validation is an excellent way to compare different approaches to a problem,
    try different classifiers, motivate normalization approaches, explore feature
    enhancements, and so on. Generally, a system configuration that shows increased
    performance on cross validation will also show increased performance on new data.
    What cross validation does not do, particularly with active learning strategies
    discussed later, is reliably predict performance on new data. Always apply the
    classifier to new data before releasing production systems as a final sanity check.
    You have been warned.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是比较不同问题解决方法、尝试不同分类器、激励归一化方法、探索特征增强等的优秀方式。通常，显示出在交叉验证上提高性能的系统配置，也会在新数据上表现出更好的性能。但交叉验证做不到的是，特别是在后面讨论的主动学习策略中，它无法可靠地预测新数据上的性能。在发布生产系统之前，始终将分类器应用于新数据，以作为最终的理智检查。你已经被警告了。
- en: Cross validation also imposes a negative bias compared to a classifier trained
    on all possible training data, because each fold is a slightly weaker classifier,
    in that it only has 90 percent of the data on 10 folds.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于使用所有可能的训练数据训练的分类器，交叉验证也会带来一定的负偏差，因为每个折叠都是一个略微较弱的分类器，因为它仅使用了10折数据中的90%。
- en: '*Rinse, lather, and repeat* is the mantra of building state-of-the-art NLP
    systems.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '*冲洗、涂抹并重复*是构建最先进NLP系统的座右铭。'
- en: Getting ready
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好
- en: Note how different this approach is from other classic computer-engineering
    approaches that focus on developing against a functional specification driven
    by unit tests. This process is more about refining and adjusting the code to work
    better as determined by the evaluation metrics.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种方法与其他经典的计算机工程方法有何不同，后者侧重于根据单元测试驱动的功能规范进行开发。这个过程更多的是关于通过评估指标来完善和调整代码，使其表现更好。
- en: How to do it...
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To run the code, perform the following steps:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行代码，请执行以下步骤：
- en: 'Get to a command prompt and type:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令提示符，输入：
- en: '[PRE47]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The result will be:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将是：
- en: '[PRE48]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The preceding output will make more sense in the following section.
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的输出将在以下部分更具意义。
- en: How it works…
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'This recipe introduces an `XValidatingObjectCorpus` object that manages cross
    validation. It is used heavily in training classifiers. Everything else should
    be familiar from the previous recipes. The `main()` method starts with:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱介绍了一个`XValidatingObjectCorpus`对象，用于管理交叉验证。在训练分类器时，这个对象被广泛使用。其他部分的内容应该和前面的食谱类似。`main()`方法从以下内容开始：
- en: '[PRE49]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding code gets us the data from the default or a user-entered file.
    The next two lines introduce `XValidatingObjectCorpus`—the star of this recipe:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将从默认文件或用户输入的文件中获取数据。接下来的两行引入了`XValidatingObjectCorpus`——这个食谱的主角：
- en: '[PRE50]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `numFolds` variable controls how the data that is just loaded will be partitioned—it
    will be in four partitions in this case. Now, we will look at the `Util.loadXValCorpus(truthData,
    numfolds)` subroutine:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`numFolds`变量控制刚加载的数据如何被划分——在这种情况下，它将被划分为四个部分。现在，我们来看一下`Util.loadXValCorpus(truthData,
    numfolds)`子例程：'
- en: '[PRE51]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`XValidatingObjectCorpus<E>` constructed will contain all the truth data in
    the form of `Objects E`. In this case, we are filling the corpus with the same
    object used to train and evaluate in the previous recipes in this chapter—`Classified<CharSequence>`.
    This will be handy, because we will be using the objects to both train and test
    our classifier. The `numFolds` parameter specifies how many partitions of the
    data to make. It can be changed later.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 构建的`XValidatingObjectCorpus<E>`将包含所有真实数据，以`Objects E`的形式。在本例中，我们将使用前面食谱中训练和评估的相同对象——`Classified<CharSequence>`来填充语料库。这将非常方便，因为我们将同时使用这些对象来训练和测试分类器。`numFolds`参数指定了数据的划分数量，可以稍后进行更改。
- en: The following `for` loop should be familiar, in that, it should iterate over
    all the annotated data and creates the `Classified<CharSequence>` object before
    applying the `corpus.handle()` method, which adds it to the corpus. Finally, we
    will return the corpus. It is worth taking a look at the Javadoc for `XValidatingObjectCorpus<E>`
    if you have any questions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`for`循环应该是熟悉的，它应该遍历所有标注数据，并在应用`corpus.handle()`方法之前创建`Classified<CharSequence>`对象，该方法将它添加到语料库中。最后，我们将返回语料库。如果你有任何问题，查看`XValidatingObjectCorpus<E>`的Javadoc值得一看。
- en: 'Returning to the body of `main()`, we will permute the corpus to mix the data,
    get the categories, and set up `BaseClassifierEvaluator<CharSequence>` with a
    null value where we supplied a classifier in a previous recipe:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`main()`方法体时，我们将打乱语料库以混合数据，获取类别，并使用空值初始化`BaseClassifierEvaluator<CharSequence>`，替代之前食谱中的分类器：
- en: '[PRE52]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, we are ready to do the cross validation:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备做交叉验证：
- en: '[PRE53]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: On each iteration of the `for` loop, we will set which fold is being used, which,
    in turn, will select the training and testing partition. Then, we will construct
    `DynamicLMClassifier` and train it by supplying the classifier to `corpus.visitTrain(classifier)`.
    Next, we will set the evaluator's classifier to the one we just trained. The evaluator
    is passed to the `corpus.visitTest(evaluator)` method where the contained classifier
    is applied to the test data that it was not trained on. With four folds, 25 percent
    of the data will be test data at any given iteration, and 75 percent of the data
    will be training data. Data will be in the test partition exactly once and three
    times in the training. The training and test partitions will never contain the
    same data unless there are duplicates in the data.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次`for`循环迭代时，我们将设置当前使用的折叠，这将选择训练和测试分区。然后，我们将构建`DynamicLMClassifier`并通过将分类器传递给`corpus.visitTrain(classifier)`来训练它。接下来，我们将把评估器的分类器设置为刚刚训练好的分类器。评估器将传递给`corpus.visitTest(evaluator)`方法，在这里，所包含的分类器将应用于它没有训练过的测试数据。对于四个折叠，任何给定的迭代中，25%的数据将是测试数据，75%的数据将是训练数据。数据将在测试分区中出现一次，在训练分区中出现三次。训练和测试分区中的数据永远不会重复，除非数据中有重复项。
- en: 'Once the loop has finished all iterations, we will print a confusion matrix
    discussed in the *Evaluation of classifiers – the confusion matrix* recipe:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 循环完成所有迭代后，我们将打印在*评估分类器—混淆矩阵*食谱中讨论的混淆矩阵：
- en: '[PRE54]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: There's more…
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'This recipe introduces quite a few moving parts, namely, cross validation and
    a corpus object that supports it. The `ObjectHandler<E>` interface is also used
    a lot; this can be confusing to developers not familiar with the pattern. It is
    used to train and test the classifier. It can also be used to print the contents
    of the corpus. Change the contents of the `for` loop to `visitTrain` with `Util.corpusPrinter`:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱引入了相当多的动态元素，即交叉验证和支持交叉验证的语料库对象。`ObjectHandler<E>`接口也被广泛使用；对于不熟悉该模式的开发人员来说，可能会感到困惑。该接口用于训练和测试分类器，也可以用于打印语料库的内容。将`for`循环中的内容改为`visitTrain`，并使用`Util.corpusPrinter`：
- en: '[PRE55]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, you will get an output that looks like:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将得到如下输出：
- en: '[PRE56]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The text is followed by `:` and the category. Printing the training/test folds
    is a good sanity check for whether the corpus is properly populated. It is also
    a nice glimpse into how the `ObjectHandler<E>` interface works—here, the source
    is from `com/lingpipe/cookbook/Util.java`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 文本后面跟着`:`和类别。打印训练/测试折叠是检查语料库是否正确填充的一个好方法。这也是一个很好的示例，展示了`ObjectHandler<E>`接口是如何工作的——这里的源代码来自`com/lingpipe/cookbook/Util.java`：
- en: '[PRE57]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: There is not much to the returned class. There is a single `handle()`method
    that just prints the `toString()` method of `Classified<CharSequence>`. In the
    context of this recipe, the classifier instead invokes `train()` on the text and
    classification, and the evaluator takes the text, runs it past the classifier,
    and compares the result to the truth.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的类没有太多内容。它只有一个`handle()`方法，单纯地打印`Classified<CharSequence>`的`toString()`方法。在本食谱的上下文中，分类器会调用`train()`方法来处理文本和分类，而评估器则接受文本，将其传递给分类器，并将结果与真实值进行比较。
- en: Another good experiment to run is to report performance on each fold instead
    of all folds. For small datasets, you will see very large variations in performance.
    Another worthwhile experiment is to permute the corpus 10 times and see the variations
    in performance that come from different partitioning of the data.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不错的实验是报告每个折叠的性能，而不是所有折叠的性能。对于小数据集，你将看到性能的巨大波动。另一个有意义的实验是将语料库打乱10次，观察不同数据划分方式对性能的影响。
- en: Another issue is how data is selected for evaluation. To text process applications,
    it is important to not leak information between test data and training data. Cross
    validation over 10 days of data will be much more realistic if each day is a fold
    rather than a 10-percent slice of all 10 days. The reason is that a day's data
    will likely be correlated, and this correlation will produce information about
    that day in training and testing, if days are allowed to be in both train and
    test. When evaluating the final performance, always select data from after the
    training data epoch if possible, to better emulate production environments where
    the future is not known.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是如何选择数据进行评估。对于文本处理应用程序，重要的是不要在测试数据和训练数据之间泄露信息。如果将每一天的数据作为一个折叠进行交叉验证，而不是将所有10天的数据切分成10%的样本，那么跨越10天的数据将更具现实性。原因是，一天的数据可能会相关联，如果允许某些天的数据同时出现在训练和测试集中，那么这种相关性将在训练和测试中产生关于那一天的信息。在评估最终性能时，尽可能从训练数据周期之后选择数据，以更好地模拟生产环境，因为未来是不可知的。
- en: Viewing error categories – false positives
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查看错误类别——假阳性
- en: We can achieve the best possible classifier performance by examining the errors
    and making changes to the system. There is a very bad habit among developers and
    machine-learning folks to not look at errors, particularly as systems mature.
    Just to be clear, at the end of a project, the developers responsible for tuning
    the classifier should be very familiar with the domain being classified, if not
    expert in it, because they have looked at so much data while tuning the system.
    If the developer cannot do a reasonable job of emulating the classifiers that
    you are tuning, then you are not looking at enough data.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查错误并对系统进行修改，我们可以实现最佳的分类器性能。开发人员和机器学习人员有一个非常不好的习惯，那就是不去查看错误，尤其是当系统逐渐成熟时。为了明确说明，项目结束时，负责调整分类器的开发人员应该对所分类的领域非常熟悉，甚至如果不是专家，也是因为在调整系统时已经查看了大量的数据。如果开发人员无法合理地模拟你正在调整的分类器，那么你就没有查看足够的数据。
- en: This recipe performs the most basic form of looking at what the system got wrong
    in the form of false positives, which are examples from training data that the
    classifier assigned to a category, but the correct category was something else.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方执行了最基本的形式，即查看系统在假阳性形式中犯的错误，这些假阳性是训练数据中分类器分配到一个类别的例子，但正确的类别应该是另一个。
- en: How to do it...
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行...
- en: 'Perform the following steps in order to view error categories using false positives:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，以便通过假阳性查看错误类别：
- en: 'This recipe extends the previous *How to train and evaluate with cross validation*
    recipe by accessing more of what the evaluation class provides. Get a command
    prompt and type:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个配方通过访问评估类提供的更多功能，扩展了之前的*如何进行交叉验证训练与评估*配方。打开命令提示符并输入：
- en: '[PRE58]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This will result in:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将导致：
- en: '[PRE59]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The output starts with a confusion matrix. Then, we will see the actual six
    instances of false positives for `p` from the lower left-hand side cell of the
    confusion matrix labeled with the category that the classifier guessed. Then,
    we will see false positives for `n`, which is a single example. The true category
    is appended with `:`, which is helpful for classifiers that have more than two
    categories.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出从混淆矩阵开始。然后，我们将看到混淆矩阵左下角单元格中标记为分类器猜测的类别的`p`的实际六个假阳性实例。接着，我们将看到`n`的假阳性，它是一个单一的例子。正确的类别后面附带有`:`，这对于具有多个类别的分类器非常有帮助。
- en: How it works…
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This recipe is based on the previous one, but it has its own source in `com/lingpipe/cookbook/chapter1/ReportFalsePositivesOverXValidation.java`.
    There are two differences. First, `storeInputs` is set to `true` for the evaluator:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方基于前一个配方，但它有自己的来源，位于`com/lingpipe/cookbook/chapter1/ReportFalsePositivesOverXValidation.java`。有两个不同之处。首先，`storeInputs`被设置为`true`，以供评估器使用：
- en: '[PRE60]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Second, a `Util` method is added to print false positives:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，添加了一个`Util`方法来打印假阳性：
- en: '[PRE61]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding code works by identifying a category of focus—`e` or English
    tweets—and extracting all the false positives from the classifier evaluator. For
    this category, false positives are tweets that are non-English in truth, but the
    classifier thought they were English. The referenced `Util` method is as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码通过识别一个关注的类别——`e`或英文推文——并提取分类器评估器中的所有假阳性来工作。对于这个类别，假阳性是那些实际上是非英语的推文，但分类器认为它们是英语的。引用的`Util`方法如下：
- en: '[PRE62]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The preceding code takes the corpus that contains all the truth data and populates
    `Map<E,Classification>` to allow for lookup of the truth annotation, given the
    input. If the same input exists in two categories, then this method will not be
    robust but will record the last example seen:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码获取包含所有真实数据的语料库，并填充`Map<E,Classification>`，以便在给定输入时查找真实注释。如果相同的输入存在于两个类别中，那么此方法将不够健壮，而是记录最后一个看到的示例：
- en: '[PRE63]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: The code gets the false positives from the evaluator and then iterates over
    all them with a lookup into `truthMap` built in the preceding code and prints
    out the relevant information. There are also methods to get false negatives, true
    positives, and true negatives in `evaluator`.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 代码从评估器中获取假阳性，并通过查找前面代码中构建的`truthMap`，对所有这些进行迭代，并打印出相关信息。`evaluator`中也有方法可以获取假阴性、真阳性和真阴性。
- en: The ability to identify mistakes is crucial to improving performance. The advice
    seems obvious, but it is very common for developers to not look at mistakes. They
    will look at system output and make a rough estimate of whether the system is
    good enough; this does not result in top-performing classifiers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 识别错误的能力对于提高性能至关重要。这个建议看起来显而易见，但开发者很常忽略错误。他们会查看系统输出，并粗略估计系统是否足够好；但这样不会产生表现最好的分类器。
- en: The next recipe works through more evaluation metrics and their definition.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个配方通过更多的评估指标及其定义来进行说明。
- en: Understanding precision and recall
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解精确度和召回率
- en: 'The false positive from the preceding recipe is one of the four possible error
    categories. All the categories and their interpretations are as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 前面配方中的假阳性是四种可能错误类别之一。所有类别及其解释如下：
- en: 'For a given category X:'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的类别X：
- en: '**True positive**: The classifier guessed X, and the true category is X'
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**：分类器猜测X，且真实类别是X。'
- en: '**False positive**: The classifier guessed X, but the true category is a category
    that is different from X'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**：分类器猜测X，但真实类别是与X不同的类别。'
- en: '**True negative**: The classifier guessed a category that is different from
    X, and the true category is different from X'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**：分类器猜测的类别与X不同，且真实类别也与X不同。'
- en: '**False negative**: The classifier guessed a category different from X, but
    the true category is X'
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**：分类器猜测的类别不同于X，但真实类别是X。'
- en: 'With these definitions in hand, we can define the additional common evaluation
    metrics as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些定义，我们可以按如下方式定义额外的常见评估指标：
- en: Precision for a category X is true positive / (false positive + true positive)
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别X的精确度为真阳性 / （假阳性 + 真阳性）
- en: The degenerate case is to make one very confident guess for 100 percent precision.
    This minimizes the false positives but will have a horrible recall.
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退化案例是做出一个非常自信的猜测，以获得100%的精度。这可以最小化假阳性，但会导致召回率非常差。
- en: Recall or sensitivity for a category X is true positive / (false negative +
    true positive)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别X的召回率或灵敏度为真阳性 / （假阴性 + 真阳性）
- en: The degenerate case is to guess all the data as belonging to category X for
    100 percent recall. This minimizes false negatives but will have horrible precision.
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退化案例是将所有数据猜测为属于类别X，以获得100%的召回率。这最小化了假阴性，但会导致精确度极差。
- en: Specificity for a category X is true negative / (true negative + false positive)
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别X的特异性为真阴性 / （真阴性 + 假阳性）
- en: The degenerate case is to guess that all data is not in category X.
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 退化案例是猜测所有数据都不属于类别X。
- en: The degenerate cases are provided to make clear what the metric is focused on.
    There are metrics such as f-measure that balance precision and recall, but even
    then, there is no inclusion of true negatives, which can be highly informative.
    See the Javadoc at `com.aliasi.classify.PrecisionRecallEvaluation` for more details
    on evaluation.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了退化案例，以便清楚地说明该度量聚焦的内容。像F-measure这样的指标平衡了精确度和召回率，但即便如此，仍然没有包括真阴性，而真阴性往往是非常有价值的信息。有关评估的更多细节，请参见`com.aliasi.classify.PrecisionRecallEvaluation`的Javadoc。
- en: 'In our experience, most business needs map to one of the three scenarios:'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据我们的经验，大多数业务需求可以映射到以下三种场景中的一种：
- en: '**High precision** / **high recall**: The language ID needs to have both good
    coverage and good accuracy; otherwise, lots of stuff will go wrong. Fortunately,
    for distinct languages where a mistake will be costly (such as Japanese versus
    English or English versus Spanish), the LM classifiers perform quite well.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高精度** / **高召回率**：语言ID需要同时具有良好的覆盖率和准确性；否则，很多事情会出错。幸运的是，对于区分度高的语言（例如日语与英语或英语与西班牙语），错误代价高昂，LM分类器表现得相当不错。'
- en: '**High precision** / **usable recall**: Most business use cases have this shape.
    For example, a search engine that automatically changes a query if it is misspelled
    better not make lots of mistakes. This means it looks pretty bad to change "Breck
    Baldwin" to "Brad Baldwin", but no one really notices if "Bradd Baldwin" is not
    corrected.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高精度** / **可用召回率**：大多数商业用例都是这种形式。例如，如果搜索引擎自动更正拼写错误，最好不要犯太多错误。这意味着将“Breck Baldwin”更改为“Brad
    Baldwin”会显得很糟糕，但如果“Bradd Baldwin”没有被更正，几乎没人会注意到。'
- en: '**High recall** / **usable precision**: Intelligence analysis looking for a
    particular needle in a haystack will tolerate a lot of false positives in support
    of finding the intended target. This was an early lesson from our DARPA days.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高召回率** / **可用精度**：智能分析在寻找稻草堆中的某根针时，会容忍大量的假阳性结果，以支持找到目标。这是我们在DARPA时期的早期经验教训。'
- en: How to serialize a LingPipe object – classifier example
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何序列化一个LingPipe对象——分类器示例
- en: In a deployment situation, trained classifiers, other Java objects with complex
    configuration, or training are best accessed by deserializing them from a disk.
    The first recipe did exactly this by reading in `LMClassifier` from the disk with
    `AbstractExternalizable`. This recipe shows how to get the language ID classifier
    written out to the disk for later use.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署环境中，训练好的分类器、其他具有复杂配置的Java对象或训练，最好通过从磁盘反序列化来访问。第一种方法正是通过使用`AbstractExternalizable`从磁盘读取`LMClassifier`来实现的。这个方法展示了如何将语言ID分类器写入磁盘以便以后使用。
- en: Serializing `DynamicLMClassiﬁer` and reading it back in results in a different
    class, which is an instance of `LMClassifier` that performs the same as the one
    just trained except that it can no longer accept training instances because counts
    have been converted to log probabilities and the backoff smoothing arcs are stored
    in suffix trees. The resulting classifier is much faster.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 对`DynamicLMClassifier`进行序列化并重新读取时，会得到一个不同的类，它是一个`LMClassifier`的实例，功能与刚刚训练的分类器相同，只是它不再接受训练实例，因为计数已经转换为对数概率，且回退平滑弧已存储在后缀树中。最终得到的分类器运行速度更快。
- en: In general, most of the LingPipe classifiers, language models, and **hidden
    Marcov models** (**HMM**) implement both the `Serializable` and `Compilable` interfaces.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，大多数LingPipe分类器、语言模型和**隐马尔可夫模型**（**HMM**）都实现了`Serializable`和`Compilable`接口。
- en: Getting ready
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will work with the same data as we did in the *Viewing error categories –
    false positives* recipe.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与*查看错误类别——假阳性*方法中相同的数据。
- en: How to do it...
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Perform the following steps to serialize a LingPipe object:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来序列化一个LingPipe对象：
- en: 'Go to the command prompt and convey:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令提示符并输入以下命令：
- en: '[PRE64]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The program will respond with the default file values for input/output:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序将响应输入/输出的默认文件值：
- en: '[PRE65]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Test if the model works by invoking the *Deserializing and running a classifier*
    recipe while specifying the classifier file to be read in:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用*反序列化并运行分类器*的方法，并指定要读取的分类器文件，来测试模型是否有效：
- en: '[PRE66]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The usual interaction follows:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常的交互流程如下：
- en: '[PRE67]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: How it works…
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The contents of `main()` from `src/com/lingpipe/cookbook/chapter1/ TrainAndWriteClassifierToDisk.java`
    start with the materials covered in the previous recipes of the chapter to read
    the `.csv` files, set up a classifier, and train it. Please refer back to it if
    any code is unclear.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`src/com/lingpipe/cookbook/chapter1/ TrainAndWriteClassifierToDisk.java`中`main()`方法的内容，从本章之前的配方中开始，读取`.csv`文件，设置分类器并对其进行训练。如果有任何代码不清楚，请参考先前的内容。'
- en: 'The new bit for this recipe happens when we invoke the `AbtractExternalizable.compileTo()`
    method on `DynamicLMClassifier`, which compiles the model and writes it to a file.
    This method is used like the `writeExternal` method from Java''s `Externalizable`
    interface:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 本方法的新内容在于，当我们调用`DynamicLMClassifier`的`AbtractExternalizable.compileTo()`方法时，它会编译模型并将其写入文件。这个方法的使用方式类似于Java的`Externalizable`接口中的`writeExternal`方法：
- en: '[PRE68]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This is all you need to know folks to write a classifier to a disk.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你需要了解的所有内容，以将分类器写入磁盘。
- en: There's more…
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'There is an alternate way to serialize that is amenable to more variations
    of data sources for serializations that are not based on the `File` class. An
    alternate way to write a classifier is:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种可用于更多数据源变种的序列化方法，这些数据源的序列化方式并不基于`File`类。写一个分类器的替代方法是：
- en: '[PRE69]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Additionally, `DynamicLM` can be compiled without involving the disk with a
    static `AbstractExternalizable.compile()` method. It will be used in the following
    fashion:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`DynamicLM`可以在不涉及磁盘的情况下通过静态的`AbstractExternalizable.compile()`方法进行编译。它将按以下方式使用：
- en: '[PRE70]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The compiled version is a lot faster but does not allow further training instances.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 编译后的版本速度更快，但不允许进一步的训练实例。
- en: Eliminate near duplicates with the Jaccard distance
  id: totrans-390
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Jaccard距离去除近似重复项
- en: 'It often happens that the data has duplicates or near duplicates that should
    be filtered. Twitter data has lots of duplicates that can be quite frustrating
    to work with even with the `-filter:retweets` option available for the search
    API. A quick way to see this is to sort the text in the spreadsheet, and tweets
    with common prefixes will be neighbors:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中经常会有重复项或近似重复项，这些都应该被过滤。Twitter数据有很多重复项，即使使用了`-filter:retweets`选项来搜索API，这也可能让人非常头痛。一个快速的方式来查看这些重复项是将文本在电子表格中排序，共享相同前缀的推文将会排在一起：
- en: '![Eliminate near duplicates with the Jaccard distance](img/4672OS_01_02.jpg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![使用Jaccard距离消除近似重复项](img/4672OS_01_02.jpg)'
- en: Duplicate tweets that share a prefix
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 共享前缀的重复推文
- en: This sort only reveals shared prefixes; there are many more that don't share
    a prefix. This recipe will allow you to find other sources of overlap and threshold,
    the point at which duplicates are removed.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个排序只揭示了共享前缀的内容；还有很多没有共享前缀的内容。这个方法将帮助你找到其他的重叠源和阈值，即去除重复项的临界点。
- en: How to do it…
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Perform the following steps to eliminate near duplicates with the Jaccard distance:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来使用Jaccard距离消除近似重复项：
- en: 'Type in the command prompt:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令提示符下输入：
- en: '[PRE71]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'You will be overwhelmed with a torrent of text:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被一大堆文本淹没：
- en: '[PRE72]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Two example outputs are shown—the first is a near-exact duplicate with only
    a difference in a final `?`. It has a proximity of `1.0`; the next example has
    proximity of `0.50`, and the tweets are different but have a good deal of word
    overlap. Note that the second case does not share a prefix.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示了两个示例输出——第一个是几乎完全相同的重复项，唯一的区别在于最后一个`?`。它的相似度为`1.0`；下一个示例的相似度为`0.50`，推文不同但有很多单词重叠。请注意，第二个例子没有共享前缀。
- en: How it works…
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: This recipe jumps a bit ahead of the sequence, using a tokenizer to drive the
    deduplication process. It is here because the following recipe, for sentiment,
    really needs deduplicated data to work well. [Chapter 2](ch02.html "Chapter 2. Finding
    and Working with Words"), *Finding and Working with Words*, covers tokenization
    in detail.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法跳过了序列的部分步骤，使用了分词器来驱动去重过程。之所以这样做，是因为接下来的情感分析方法确实需要去重后的数据才能更好地工作。[第二章](ch02.html
    "第二章. 查找与处理单词")，*查找与处理单词*，详细介绍了分词化的内容。
- en: 'The source for `main()` is:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`的源代码如下：'
- en: '[PRE73]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'There is nothing new in the preceding code snippet, but the following code
    snippet has `TokenizerFactory`:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段没有新内容，但下面的代码片段包含了`TokenizerFactory`：
- en: '[PRE74]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Briefly, the tokenizer breaks the text into text sequences defined by matching
    the regular expression `\w+` (the first `\` escapes the second one in the preceding
    code—it is a Java thing). It matches contiguous word characters. The string "Hi,
    you here??" produces tokens "Hi", "you", and "here". The punctuation is ignored.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，分词器通过匹配正则表达式`\w+`将文本分割成文本序列（前面的代码中的第一个`\`用来转义第二个`\`——这是Java的一个特点）。它匹配连续的单词字符。字符串"Hi,
    you here??"将生成"Hi"、"you"和"here"三个标记，标点符号被忽略。
- en: 'Next up, `Util.filterJaccard` is called with a cutoff of `.5`, which roughly
    eliminates tweets that overlap with half their words. Then, the filter data is
    written to disk:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`Util.filterJaccard`，设定截止值为`.5`，大致去除了那些与自己一半单词重叠的推文。然后，过滤后的数据被写入磁盘：
- en: '[PRE75]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The `Util.filterJaccard()` method''s source is as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`Util.filterJaccard()`方法的源代码如下：'
- en: '[PRE76]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: In the preceding snippet, a `JaccardDistance` class is constructed with a tokenizer
    factory. The Jaccard distance divides the intersection of tokens from the two
    strings over the union of tokens from both strings. Look at the Javadoc for more
    information.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，使用分词器工厂构建了一个`JaccardDistance`类。Jaccard距离通过两个字符串的标记交集与它们的标记并集之比来计算。请查看Javadoc了解更多信息。
- en: 'The nested `for` loops in the following example explore each row with every
    other row until a higher threshold proximity is found or until all data has been
    looked at. Do not use this for large datasets because it is the O(n²)algorithm.
    If no row is above proximity, then the row is added to `filteredTexts`:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中的嵌套`for`循环遍历每一行与其他每一行，直到找到更高的阈值接近度或直到所有数据都被检查过。如果数据集很大，尽量避免使用这种方法，因为它是O(n²)算法。如果没有行的接近度超过阈值，则该行会被添加到`filteredTexts`中：
- en: '[PRE77]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: There are much better ways to efficiently filter the texts at a cost of extra
    complexity—a simple reverse-word lookup index to compute an initial covering set
    will be vastly more efficient—search for a shingling text lookup for O(n) to O(n
    log(n)) approaches.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多更好的方法可以高效过滤文本，虽然这些方法会增加复杂度——通过建立一个简单的反向词汇查找索引来计算初步的覆盖集，效率会高得多——你可以搜索一个用于O(n)到O(n
    log(n))方法的文本查找滑动窗口（shingling）技术。
- en: Setting the threshold can be a bit tricky, but looking a bunch of data should
    make the appropriate cutoff fairly clear for your needs.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 设置阈值可能有些棘手，但查看大量数据应该能让你清楚地了解适合自己需求的分割点。
- en: How to classify sentiment – simple version
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行情感分类——简易版
- en: Sentiment has become the classic business-oriented classification task—what
    executive can resist an ability to know on a constant basis what positive and
    negative things are being said about their business? Sentiment classifiers offer
    this capability by taking text data and classifying it into positive and negative
    categories. This recipe addresses the process of creating a simple sentiment classifier,
    but more generally, it addresses how to create classifiers for novel categories.
    It is also a 3-way classifier, unlike the 2-way classifiers we have been working
    with.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析已成为经典的面向商业的分类任务——哪个高管能抵挡住实时了解关于自己企业的正面和负面言论的能力呢？情感分类器通过将文本数据分类为正面和负面类别，提供了这种能力。本篇讲解了如何创建一个简单的情感分类器，但更广泛地说，它探讨了如何为新类别创建分类器。它还是一个三分类器，与我们之前使用的二分类器不同。
- en: Our first sentiment system was built for BuzzMetrics in 2004 using language
    model classifiers. We tend to use logistic regression classifiers now, because
    they tend to perform better. [Chapter 3](ch03.html "Chapter 3. Advanced Classifiers"),
    *Advanced Classifiers*, covers logistic regression classifiers.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个情感分析系统是在2004年为BuzzMetrics构建的，使用的是语言模型分类器。我们现在倾向于使用逻辑回归分类器，因为它们通常表现得更好。[第3章](ch03.html
    "第3章：高级分类器")，*高级分类器*，讲解了逻辑回归分类器。
- en: How to do it…
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'The previous recipes focused on language ID—how do we shift the classifier
    over to the very different task of sentiment? This will be much simpler than one
    might think—all that needs to change is the training data, believe it or not.
    The steps are as follows:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方关注的是语言识别——我们如何将分类器转到截然不同的情感分析任务呢？这比想象的要简单——所需要改变的只是训练数据，信不信由你。步骤如下：
- en: Use the Twitter search recipe to download tweets about a topic that has positive/negative
    tweets about it. A search on `disney` is our example, but feel free to branch
    out. This recipe will work with the supplied CSV file, `data/disneySentiment_annot.csv`.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Twitter搜索配方下载关于某个话题的推文，寻找其中有正面/负面评价的推文。我们使用`disney`作为示例，但你可以自由扩展。这个配方将与提供的CSV文件`data/disneySentiment_annot.csv`兼容。
- en: Load the created `data/disneySentiment_annot.csv` file into your spreadsheet
    of choice. There are already some annotations done.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将创建的`data/disneySentiment_annot.csv`文件加载到你选择的电子表格中。文件中已经有一些标注。
- en: 'As in the *Evaluation of classifiers – the confusion matrix* recipe, annotate
    the `true class` column for one of the three categories:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同*分类器评估——混淆矩阵*配方一样，标注`true class`列为三种类别之一：
- en: 'The `p` annotation stands for positive. The example is "Oh well, I love Disney
    movies. #hateonit".'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`标注代表“正面”。示例是“哎呀，我爱迪士尼电影。#讨厌它”。'
- en: The `n` annotation stands for negative. The example is "Disney really messed
    me up yo, this is not the way things are suppose to be".
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`标注代表“负面”。示例是“迪士尼真让我崩溃，事情本不该是这样的。”'
- en: The `o` annotation stands for other. The example is "Update on Downtown Disney.
    [http://t.co/SE39z73vnw](http://t.co/SE39z73vnw).
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`o`标注代表“其他”。示例是“关于迪士尼市中心的更新。[http://t.co/SE39z73vnw](http://t.co/SE39z73vnw)。'
- en: Leave blank tweets that are not in English, irrelevant, both positive and negative,
    or you are unsure about.
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于不使用英语、不相关、包含正负两面内容或不确定的推文，留空。
- en: Keep annotating until the smallest category has at least 10 examples.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续标注，直到最小类别至少有10个示例。
- en: Save the annotations.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存标注。
- en: 'Run the previous recipe for cross validation, providing the annotated file''s
    name:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前面的交叉验证配方，并提供注释文件的名称：
- en: '[PRE78]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The system will then run a four-fold cross validation and print a confusion
    matrix. Look at the *How to train and evaluate with cross validation* recipe if
    you need further explanation:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统接着会进行四折交叉验证并打印出混淆矩阵。如果需要更多解释，请参考*如何通过交叉验证训练和评估*的配方：
- en: '[PRE79]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: That's it! Classifiers are entirely dependent on training data for what they
    classify. More sophisticated techniques will bring richer features into the mix
    than character ngrams, but ultimately, the labels imposed by training data are
    the knowledge being imparted to the classifier. Depending on your view, the underlying
    technology is magical or astoundingly simple minded.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！分类器完全依赖于它们所分类的训练数据。更复杂的技术将比字符n-gram引入更丰富的特征，但最终，由训练数据施加的标签就是传授给分类器的知识。根据你的观点，底层技术可能是神奇的，或者令人惊讶地简单。
- en: How it works...
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Most developers are surprised that the only difference between language ID and
    sentiment is the labeling applied to the data for training. The language model
    classifier is applying an individual language model for each category and also
    noting the marginal distribution of the categories in the estimates.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数开发者会惊讶地发现，语言识别和情感分析之间唯一的区别在于为训练数据所应用的标签。语言模型分类器会为每个类别应用单独的语言模型，并在估计中记录各类别的边际分布。
- en: There's more…
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Classifiers are pretty dumb but very useful if they are not expected to work
    outside their capabilities. Language ID works great as a classification problem
    because the observed events are tightly tied to the classification being done—the
    words and characters of a language. Sentiment is more difficult because the observed
    events, in this case, are exactly the same as the language ID and are less strongly
    associated with the end classification. For example, the phrase "I love" is a
    good predictor of the sentence being English but not as clear a predictor that
    the sentiment is positive, negative, or other. If the tweet is "I love Disney",
    then we have a positive statement. If the tweet is "I love Disney, not", then
    it is negative. Addressing the complexities of sentiment and other more complex
    phenomenon tends to be resolved in the following ways:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器非常简单，但如果不期望它们超出自身的能力范围，它们是非常有用的。语言识别作为分类问题表现很好，因为观察到的事件与正在进行的分类密切相关——即一种语言的单词和字符。情感分析则更为复杂，因为在这种情况下，观察到的事件与语言识别完全相同，并且与最终分类的关联性较弱。例如，“I
    love”这个短语可以很好地预测该句子是英语，但不能明确预测情感是正面、负面还是其他。如果推文是“I love Disney”，那么它是一个正面陈述。如果推文是“I
    love Disney, not”，那么它就是负面。处理情感及其他更复杂现象的复杂性通常会通过以下方式解决：
- en: Create more training data. Even relatively dumb techniques such as language
    model classifiers can perform very well given enough data. Humanity is just not
    that creative in ways to gripe about, or praise, something. The *Train a little,
    learn a little – active learning* recipe of [Chapter 3](ch03.html "Chapter 3. Advanced
    Classifiers"), *Advanced Classifiers*, presents a clever way to do this.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建更多的训练数据。即使是相对简单的技术，例如语言模型分类器，只要有足够的数据，也能表现得非常好。人类在抱怨或称赞某件事物时并没有那么富有创造力。*训练一点，学习一点——主动学习*的[第3章](ch03.html
    "第3章. 高级分类器")，*高级分类器*，介绍了一种巧妙的方法来实现这一点。
- en: Use fancier classifiers that in turn use fancier features (observations) about
    the data to get the job done. Look at the logistic regression recipes for more
    information. For the negation case, a feature that looked for a negative phrase
    in the tweet might help. This could get arbitrarily sophisticated.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更复杂的分类器，这些分类器又使用更复杂的特征（关于数据的观察）来完成任务。有关更多信息，请查看逻辑回归配方。在否定的情况下，可能通过查找推文中的负面短语来帮助解决问题。这可能会变得非常复杂。
- en: Note that a more appropriate way to take on the sentiment problem can be to
    create a binary classifier for *positive* and *not positive* and a binary classifier
    for *negative* and *not negative*. The classifiers will have separate training
    data and will allow for a tweet to be both positive and negative.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，更合适的处理情感问题的方式是为*正面*和*非正面*创建一个二分类器，为*负面*和*非负面*创建另一个二分类器。这些分类器将有独立的训练数据，并允许推文同时是正面和负面。
- en: Common problems as a classification problem
  id: totrans-444
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为分类问题的常见问题
- en: Classifiers form the foundations of many industrial NLP problems. This recipe
    goes through the process of encoding some common problems into a classification-based
    solution. We will pull from real-world examples that we have built whenever possible.
    You can think of them as mini recipes.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器是许多工业级自然语言处理（NLP）问题的基础。本解决方案将通过将一些常见问题编码为基于分类的解决方案的过程进行介绍。我们将在可能的情况下，提取我们实际构建的真实世界示例，你可以将它们视为小型解决方案。
- en: Topic detection
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主题检测
- en: 'Problem: Take footnotes from financial documents (10Qs and 10Ks) and determine
    whether an **eXtensible Business Reporting Language** (**XBRL**) category is applied
    like "forward looking financial statements". Turns out that foot notes are where
    all the action happens. For example, is the footnote referring to retired debt?
    Performance needed to be greater than 90 percent precision with acceptable recall.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：从财务文档（如10Qs和10Ks）中提取脚注，并确定是否应用了**可扩展商业报告语言**（**XBRL**）类别，如“前瞻性财务报表”。事实证明，脚注是所有信息的核心。例如，脚注是否指的是已退休的债务？性能需要达到超过90%的精度，并保持可接受的召回率。
- en: 'Solution: This problem closely mirrors how we approached language ID and sentiment.
    The actual solution involves a sentence recognizer that detects the footnotes—see
    [Chapter 5](ch05.html "Chapter 5. Finding Spans in Text – Chunking"), *Finding
    Spans in Text – Chunking*—and then creates training data for each of the XBRL
    categories. We used the confusion matrix output to help refine the XBRL categories
    that the system was struggling to distinguish. Merging categories was a possibility,
    and we did merge them. This system is based on language model classifiers. If
    done now, we would use logistic regression.'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：这个问题与我们处理语言识别和情感分析时的方法非常相似。实际解决方案包括一个句子识别器，能够检测脚注——见[第5章](ch05.html "第5章.
    文本中跨度的查找——分块")，*文本中跨度的查找——分块*——然后为每个XBRL类别创建训练数据。我们使用混淆矩阵的输出帮助优化系统难以区分的XBRL类别。合并类别是一个可能的方案，我们确实进行了合并。该系统基于语言模型分类器。如果现在来做，我们将使用逻辑回归。
- en: Question answering
  id: totrans-449
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问答
- en: 'Problem: Identify FAQs in a large dataset of text-based customer support data
    and develop the answers and ability to automatically deliver answers with 90 percent
    precision.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在大量基于文本的客户支持数据中识别常见问题，并开发回答能力，实现90%的精确度自动回答。
- en: 'Solution: Perform clustering analysis over logs to find FAQs—see [Chapter 6](ch06.html
    "Chapter 6. String Comparison and Clustering"), *String Comparison and Clustering*.
    This will result in a very large set of FAQs that are really **Infrequently Asked
    Questions** (**IAQs**); this means that the prevalence of an IAQ can be as low
    as 1/20000\. Positive data is fairly easy to find for a classifier, but negative
    data is too expensive to create on any kind of balanced distribution—for every
    positive case, one will expect 19999 negative case. The solution is to assume
    that any random sample of a large size will contain very few positives and to
    just use this as negative data. A refinement is to run a trained classifier over
    the negatives to find high-scoring cases and annotate them to pull out the positives
    that might be found.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：对日志进行聚类分析以找出常见问题——见[第6章](ch06.html "第6章. 字符串比较与聚类")，*字符串比较与聚类*。这将生成一个非常大的常见问题集合，实际上是**很少被提问的问题**（**IAQs**）；这意味着IAQ的出现频率可能低至1/20000。对分类器来说，正向数据相对容易找到，但负向数据在任何平衡分布中都难以获取——每当出现一个正向案例时，通常会有19999个负向案例。解决方案是假设任何大规模的随机样本都会包含极少的正向数据，并将其作为负向数据使用。一种改进方法是对负向数据运行训练好的分类器，以找出高分案例，并为可能找到的正向数据进行注释。
- en: Degree of sentiment
  id: totrans-452
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 情感程度
- en: 'Problem: Classify a sentiment on a scale of 1 to 10 based on the degree of
    negativeness to positiveness.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：根据负面到正面情感的程度，将情感分类为1到10之间的等级。
- en: 'Solution: Even though our classifiers provide a score that can be mapped on
    a 1-to-10 scale, this is not what the background computation is doing. To correctly
    map to a degree scale, one will have to annotate the distinction in training data—this
    tweet is a 1, this tweet is a 3, and so on. We will then train a 10-way classifier,
    and the first best category should, in theory, be the degree. We write *in theory*
    because despite regular customer requests for this, we have never found a customer
    that was willing to support the required annotation.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：尽管我们的分类器提供了一个可以映射到1到10的评分，这并不是后台计算所做的工作。为了正确映射到程度量表，必须在训练数据中注释出这些区别——这条推文是1分，那条推文是3分，依此类推。然后，我们将训练一个10分类器，理论上，第一个最佳分类应该是这个程度。我们写*理论上*是因为尽管客户经常要求这种功能，我们从未找到一个愿意支持所需注释的客户。
- en: Non-exclusive category classification
  id: totrans-455
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非互斥类别分类
- en: 'Problem: The desired classifications are not mutually exclusive. A tweet can
    say both positive and negative things, for example, "Loved Mickey, hated Pluto".
    Our classifiers assume that categories are mutually exclusive.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所需的分类不是互斥的。例如，一条推文可以同时表达正面和负面内容，如“喜欢米奇，讨厌普鲁托”。我们的分类器假设各个类别是互斥的。
- en: 'Solution: We regularly use multiple binary classifiers in place of one *n*-way
    or multinomial classifiers. The classifiers will be trained for positive/non-positive
    and negative/non-negative. A tweet can then be annotated `n` and `p`.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：我们经常使用多个二分类器来代替一个*n*分类器或多项分类器。分类器将被训练为正面/非正面和负面/非负面。然后，可以将推文标注为`n`和`p`。
- en: Person/company/location detection
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人物/公司/地点检测
- en: 'Problem: Detect mentions of people in text data.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在文本数据中检测人物的提及。
- en: 'Solution: Believe it or not, this breaks down into a word classification problem.
    See [Chapter 6](ch06.html "Chapter 6. String Comparison and Clustering"), *String
    Comparison and Clustering*.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：信不信由你，这实际上是一个词汇分类问题。请参见[第六章](ch06.html "第六章：字符串比较与聚类")，*字符串比较与聚类*。
- en: It is generally fruitful to look at any novel problem as a classification problem,
    even if classifiers don't get used as the underlying technology. It can help clarify
    what the underlying technology actually needs to do.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通常将任何新问题视为分类问题，即使分类器并不是作为底层技术使用，这也是有益的。它有助于澄清底层技术实际需要做什么。
