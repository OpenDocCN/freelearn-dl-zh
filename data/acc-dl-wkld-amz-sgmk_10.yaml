- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Operationalizing Inference Workloads
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作化推理工作负载
- en: In [*Chapter 8*](B17519_08.xhtml#_idTextAnchor121), *Considering Hardware for
    Inference*, and [*Chapter 9*](B17519_09.xhtml#_idTextAnchor137), *Implementing
    Model Servers*, we discussed how to engineer your **deep learning** (**DL**) inference
    workloads on Amazon SageMaker. We also reviewed how to select appropriate hardware
    for inference workloads, optimize model performance, and tune model servers based
    on specific use case requirements. In this chapter, we will focus on how to operationalize
    your DL inference workloads once they have been deployed to test and production
    environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B17519_08.xhtml#_idTextAnchor121)《考虑推理硬件》和[*第9章*](B17519_09.xhtml#_idTextAnchor137)《实现模型服务器》中，我们讨论了如何在Amazon
    SageMaker上设计你的**深度学习**（**DL**）推理工作负载。我们还回顾了如何为推理工作负载选择适当的硬件、优化模型性能并根据特定的使用案例要求调整模型服务器。在本章中，我们将重点讨论如何在推理工作负载部署到测试和生产环境后，进行操作化。
- en: In this chapter, we will start by reviewing advanced model hosting options such
    as **multi-model**, **multi-container**, and **Serverless Inference** endpoints
    to optimize your resource utilization and workload costs. Then, we will cover
    the **Application Auto Scaling** service for SageMaker, which provides another
    mechanism to improve resource utilization. Auto Scaling allows you to dynamically
    match your inference traffic requirements with provisioned inference resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先回顾一些高级模型托管选项，如**多模型**、**多容器**和**无服务器推理**端点，以优化你的资源利用率和工作负载成本。接着，我们将介绍SageMaker的**应用自动扩展**服务，它提供了另一种提高资源利用率的机制。自动扩展使你能够动态地将推理流量需求与预置的推理资源匹配。
- en: After that, we will discuss how to continuously promote models and model versions
    without this impacting your end users. We will also cover some advanced deployment
    patterns required for A/B testing and quality assurance of model candidates. For
    this, we will review SageMaker’s **Model Variant** and **Deployment Guardrails**
    capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何在不影响终端用户的情况下，持续推广模型和模型版本。我们还将介绍一些用于A/B测试和模型候选质量保证的高级部署模式。为此，我们将回顾SageMaker的**模型变体**和**部署安全边界**功能。
- en: Then, we review how to monitor model and inference data quality using SageMaker
    **Model Monitor**. We will close this chapter by discussing how to select an optimal
    inference workload configuration based on your use case type, its business, and
    technical requirements.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将回顾如何使用SageMaker的**模型监控器**来监控模型和推理数据的质量。本章的最后，我们将讨论如何根据你的使用案例类型、业务需求和技术要求选择最佳的推理工作负载配置。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Managing inference deployments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理推理部署
- en: Monitoring inference workloads
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控推理工作负载
- en: Selecting your workload configuration
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择工作负载配置
- en: By the end of this chapter, you will have an understanding and practical skills
    on how to operationalize SageMaker inference workloads.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将掌握如何操作化SageMaker推理工作负载的理解和实践技能。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will provide code samples so that you can develop practical
    skills. The full code examples are available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供代码示例，帮助你开发实践技能。完整的代码示例可以在这里查看：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/)。
- en: 'To follow along with this code, you will need the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本代码，你需要以下条件：
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个AWS账户和具有管理Amazon SageMaker资源权限的IAM用户。
- en: A SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible
    environment established.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个SageMaker笔记本、SageMaker Studio笔记本或已建立的本地SageMaker兼容环境。
- en: Access to GPU training instances in your AWS account. Each example in this chapter
    will provide the recommended instance types to use. You may need to increase your
    compute quota for *SageMaker Training Job* to have GPU instances enabled. In this
    case, please follow the instructions at [https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的AWS账户中访问GPU训练实例。本章中的每个示例将提供推荐使用的实例类型。你可能需要增加你的计算配额，以便启用GPU实例进行*SageMaker训练作业*。在这种情况下，请按照[https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml)上的说明进行操作。
- en: You will need to install the required Python libraries by running `pip install
    -r requirements.txt`. The file that contains the required libraries can be found
    in the `chapter10` directory.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要通过运行`pip install -r requirements.txt`来安装所需的Python库。包含所需库的文件可以在`chapter10`目录中找到。
- en: In this chapter, we will provide examples of compiling models for inference,
    which requires access to specific accelerator types. Please review the instance
    recommendations as part of the model server examples.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供编译模型以进行推理的示例，这需要访问特定的加速器类型。请作为模型服务器示例的一部分，查看实例推荐。
- en: Managing inference deployments
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理推理部署
- en: 'In [*Chapter 1*](B17519_01.xhtml#_idTextAnchor013), *Introducing Deep Learning
    with Amazon SageMaker*, we discussed that SageMaker provides several options when
    it comes to running your inference workloads, depending on your use case’s requirements,
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B17519_01.xhtml#_idTextAnchor013)，*与Amazon SageMaker一起介绍深度学习*中，我们讨论了SageMaker在运行推理工作负载时提供的几种选项，这些选项取决于你的用例需求，具体如下：
- en: '**Real-time endpoints** are designed for inference use cases with low latency
    requirements. It comes with certain limitations on payload size (up to 5 MB) and
    response latency (up to 60 seconds).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时端点**设计用于具有低延迟要求的推理用例。它在有效载荷大小（最大5MB）和响应延迟（最多60秒）上有一定的限制。'
- en: '**Batch transform jobs** are an option for processing large-scale batched inference
    requests in an offline fashion.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量转换作业**是处理大规模批量推理请求的离线选项。'
- en: '**Asynchronous endpoints** allow you to queue and process inference requests
    in near-real time. It also has a much higher limit on inference payload size (up
    to 1 GB) compared to real-time endpoints.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步端点**允许你排队并以接近实时的方式处理推理请求。与实时端点相比，它对推理有效载荷大小有更高的限制（最大1GB）。'
- en: So far in this book, we have covered how to deploy a **single model** for your
    inference workload. This is supported by all three inference options listed previously.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们已经介绍了如何为推理工作负载部署**单一模型**。所有三种推理选项都支持这一方法。
- en: However, for real-time endpoints, it’s possible to package and deploy several
    models and model versions (known as **production** **variants**) behind a single
    endpoint. In this section, we will dive deeper into these model deployment strategies
    and highlight implementation details, their advantages, and certain limitations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于实时端点，有可能将多个模型和模型版本（称为**生产** **变体**）打包并部署在一个单一端点后面。在本节中，我们将深入探讨这些模型部署策略，并重点介绍实施细节、其优势及某些限制。
- en: Additionally, we will review the recently introduced **Serverless Inference**
    endpoints. Like real-time endpoints, serverless endpoints are designed to serve
    users in real time. However, in the case of serverless endpoints, you will have
    access to compute resources without the need to choose provision and scale inference
    instances.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将回顾最近推出的**无服务器推理**端点。与实时端点类似，无服务器端点旨在实时服务用户。然而，在无服务器端点的情况下，你可以使用计算资源，而无需选择并扩展推理实例。
- en: Considering model deployment options
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑模型部署选项
- en: In many situations, hosting a single model behind a dedicated SageMaker real-time
    endpoint can lead to sub-optimal resource utilization and additional costs that
    can be avoided. For example, when you need to simultaneously host a fleet of models,
    each with low resource requirements, hosting each model behind an individual endpoint
    would be a major avoidable cost.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，将单个模型托管在专用的SageMaker实时端点后面可能导致资源利用率低下并产生额外的费用，这些费用是可以避免的。例如，当你需要同时托管一批模型，每个模型的资源需求都很低时，将每个模型托管在单独的端点后面将是一项重大且可避免的成本。
- en: SageMaker provides a range of model deployment options that can address more
    complex use cases. In the following subsections, we will discuss their target
    use cases, advantages, and limitations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker提供了一系列模型部署选项，可以解决更复杂的用例。在以下小节中，我们将讨论它们的目标用例、优点和限制。
- en: Multi-model endpoints
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模型端点
- en: A **multi-model endpoint** (**MME**) is a special type of SageMaker model endpoint
    that allows you to host thousands of models behind a single endpoint simultaneously.
    This type of endpoint is suitable for scenarios for similarly sized models with
    relatively low resource requirements that can be served from the same inference
    container.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**多模型端点**（**MME**）是一种特殊类型的SageMaker模型端点，允许您在单个端点后同时托管数千个模型。这种类型的端点适用于模型大小相似、资源要求较低、可以从同一推理容器中提供服务的场景。'
- en: MMEs and their underlying model servers manage resource allocation, such as
    unloading infrequently used models and loading requested ones when an instance
    runs out of memory. This leads to additional inference latency when the user requests
    a model that is currently not loaded into memory. Hence, MMEs may not be a good
    fit for scenarios where consistently low latency is required. This additional
    latency can increase when hosting large models with evenly distributed traffic
    patterns as this will lead to frequent unloading and loading of models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: MME及其底层模型服务器管理资源分配，例如在实例内存不足时卸载不常用的模型并加载请求的模型。这会导致当用户请求当前未加载到内存中的模型时，出现额外的推理延迟。因此，MME可能不适用于对低延迟要求持续较低的场景。当托管大型模型且流量模式均匀分布时，这种额外的延迟可能会增加，因为这将导致频繁地卸载和加载模型。
- en: To provision an MME, you need to package each model (model artifacts and inference
    code) into a separate archive and upload it to Amazon S3\. Once the MME instance
    has been provisioned, it is downloaded from the S3 location to the instance disk,
    which loads the models into instance memory. By default, if the MME runs out of
    instance disk space and/or instance memory, SageMaker deletes the least recently
    used models from the local disk and/or unloads models from memory to accommodate
    the requested models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置MME，您需要将每个模型（模型工件和推理代码）打包成单独的归档文件，并将其上传到Amazon S3。MME实例配置完毕后，它将从S3位置下载到实例磁盘，将模型加载到实例内存中。默认情况下，如果MME的实例磁盘空间和/或实例内存不足，SageMaker会从本地磁盘中删除最不常用的模型和/或从内存中卸载模型，以便为请求的模型腾出空间。
- en: 'The following diagram shows the MME architecture:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了MME架构：
- en: '![Figure 10.1 – MME architecture ](img/B17519_10_001.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – MME架构](img/B17519_10_001.jpg)'
- en: Figure 10.1 – MME architecture
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – MME架构
- en: MMEs are supported by PyTorch and TensorFlow inference containers. You can also
    automatically scale MMEs in and out to match your inference traffic. MMEs allow
    you to directly invoke models as well as inference pipelines comprising several
    models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: MME支持PyTorch和TensorFlow推理容器。您还可以根据推理流量自动扩展和缩减MME。MME允许您直接调用模型以及由多个模型组成的推理管道。
- en: 'When selecting an instance type and family, consider the following aspects:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择实例类型和系列时，请考虑以下几个方面：
- en: Instance memory defines how many models can be loaded simultaneously
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例内存定义了可以同时加载多少个模型。
- en: Instance disk size defines how many models can be cached locally to avoid expensive
    download procedures from S3
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例磁盘大小定义了可以在本地缓存多少个模型，从而避免从S3进行昂贵的下载操作。
- en: The number of vCPUs defines how many inference requests can be handled simultaneously
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vCPU的数量定义了可以同时处理多少个推理请求。
- en: Note that GPU-based instances are not supported for MMEs, which limits what
    model architectures can be served using MMEs within reasonable SLAs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，基于GPU的实例不支持MME，这限制了可以在合理的服务水平协议（SLA）内使用MME托管的模型架构。
- en: Now, let’s learn how to implement an MME.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何实现MME。
- en: Implementing an MME
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现MME
- en: 'In this code sample, we will learn how to deploy two NLP models simultaneously
    using an MME. One model analyzes the sentiment of German text, while the other
    analyzes the sentiment of English text. We will use the HuggingFace PyTorch container
    for this. The full code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，我们将学习如何使用MME同时部署两个NLP模型。一个模型分析德语文本的情感，另一个模型分析英语文本的情感。我们将使用HuggingFace
    PyTorch容器来完成此任务。完整的代码可以在此处查看：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/1_Multi_Model_Endpoint.ipynb)。
- en: 'For this task, we will use two models, trained to predict the sentiment of
    English and German texts: `distilbert-base-uncased-finetuned-sst-2-english` and
    `oliverguhr/german-sentiment-bert`, respectively. Follow these steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此任务，我们将使用两个模型，分别训练用于预测英语和德语文本的情感：`distilbert-base-uncased-finetuned-sst-2-english`和`oliverguhr/german-sentiment-bert`。请按照以下步骤进行：
- en: 'We will start by fetching the models from the HuggingFace Model hub and saving
    them locally. The following code shows the English model:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先从HuggingFace模型库中获取模型并将其保存在本地。以下代码展示了英语模型：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As a result, the following artifacts are downloaded:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，将下载以下工件：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These model artifacts will be added to the model data package later. But first,
    we need to develop the inference script.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型工件稍后将被添加到模型数据包中。但首先，我们需要开发推理脚本。
- en: 'An MME has the same requirements as those for the inference scripts of single-model
    endpoints. The following code shows the inference script for the English model,
    which implements the required methods for model loading, inference, and data pre-/post-processing:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MME与单模型端点的推理脚本具有相同的要求。以下代码展示了英语模型的推理脚本，脚本实现了模型加载、推理以及数据的前后处理所需的方法：
- en: '[PRE2]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we need to package the model and inference code for the MME. SageMaker
    requests a specific directory structure that varies for PyTorch and TensorFlow
    containers. For PyTorch containers, the model and code should be packaged into
    a single `tar.gz` archive and have the following structure:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为MME打包模型和推理代码。SageMaker要求一个特定的目录结构，该结构在PyTorch和TensorFlow容器中有所不同。对于PyTorch容器，模型和代码应打包成一个单独的`tar.gz`压缩包，并具有以下结构：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each model should have a model package. Once the packages have been prepared
    locally, we need to upload them to Amazon S3 and save the respective URI:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型应有一个模型包。一旦包在本地准备好，我们需要将它们上传到Amazon S3，并保存相应的URI：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the data has been uploaded, we need to define the respective serving container
    and configure it to be used for the MME. The following code locates the PyTorch
    container based on the desired runtime configuration and task (inference):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据上传完成，我们需要定义相应的服务容器，并配置它以供MME使用。以下代码根据所需的运行时配置和任务（推理）定位PyTorch容器：
- en: '[PRE5]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we need to configure the MME parameters. Specifically, we must define
    the `MultiModel` mode. Note that we provide two specific environment variables
    – `SAGEMAKER_PROGRAM` and `SAGEMAKER_SUBMIT_DIRECTORY` – so that the SageMaker
    inference framework knows how to register the model handler:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要配置MME参数。具体来说，我们必须定义`MultiModel`模式。请注意，我们提供了两个特定的环境变量 – `SAGEMAKER_PROGRAM`
    和 `SAGEMAKER_SUBMIT_DIRECTORY` – 以便SageMaker推理框架知道如何注册模型处理器：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The last step of configuring the MME is to create a SageMaker model instance,
    endpoint configuration, and the endpoint itself. When creating the model, we must
    provide the `MultiModel`-enabled container from the preceding step. We have omitted
    the creation of the endpoint configuration and endpoint for brevity:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置MME的最后一步是创建SageMaker模型实例、端点配置和端点本身。在创建模型时，我们必须提供前一步中启用了`MultiModel`的容器。为了简便起见，我们省略了端点配置和端点的创建：
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the endpoint has been created, we can run and invoke our models. For this,
    in the invocation request, we need to supply a special parameter called `TargetModel`,
    as follows:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点创建完成，我们就可以运行并调用我们的模型。为此，在调用请求中，我们需要提供一个名为`TargetModel`的特殊参数，如下所示：
- en: '[PRE8]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While the MME capability provides a convenient way to optimize your inference
    costs when running multiple similar models, it requires models to have the same
    runtime environment (in other words, they must use the same inference container).
    To address scenarios where you need to host multiple models within different inference
    containers, SageMaker supports **multi-container endpoints** (**MCEs**), as shown
    in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MME功能提供了一种便捷的方式来优化运行多个相似模型时的推理成本，但它要求模型具有相同的运行时环境（换句话说，它们必须使用相同的推理容器）。为了解决需要在不同推理容器中托管多个模型的场景，SageMaker支持**多容器端点**（**MCEs**），如下一节所示。
- en: Multi-Container Endpoints
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多容器端点
- en: An MCE allows you to host up to 15 inference containers simultaneously. In this
    case, each container would serve its own model. MCEs are a good fit for use cases
    where models require different runtime environments/containers but not every single
    model can fully utilize the available instance resources. Another scenario is
    when models are called at different times.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MCE允许你同时托管最多15个推理容器。在这种情况下，每个容器将服务于自己的模型。MCE非常适合用于需要不同运行时环境/容器的模型场景，但并非每个模型都能充分利用可用的实例资源。另一种场景是当模型在不同时间被调用时。
- en: 'Unlike an MME, an MCE doesn’t cache or unload containers based on their invocation
    patterns. Hence, you need to ensure that the inference containers will collectively
    have enough resources to run on the endpoint instance. If the instance resources
    (for example, instance memory) are not enough to run all containers, you may see
    an error during MCE creation time. Hence, you need to consider the total resource
    requirements of all the inference containers when choosing an instance configuration.
    Each inference container will have a proportional amount of resources available
    for it. The following diagram shows the MCE architecture:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与MME不同，MCE不会根据容器的调用模式缓存或卸载容器。因此，你需要确保推理容器总共拥有足够的资源来在端点实例上运行。如果实例资源（例如，实例内存）不足以运行所有容器，可能会在创建MCE时出现错误。因此，在选择实例配置时，你需要考虑所有推理容器的总资源需求。每个推理容器将为其提供按比例分配的资源。以下图显示了MCE架构：
- en: '![Figure 10.2 – MCE architecture ](img/B17519_10_002.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – MCE架构](img/B17519_10_002.jpg)'
- en: Figure 10.2 – MCE architecture
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – MCE架构
- en: You can automatically scale an MCE. It supports **Direct** mode (invoking inference
    containers directly) or **Serial** mode (invoking several containers sequentially).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自动扩展MCE。它支持**Direct**模式（直接调用推理容器）或**Serial**模式（按顺序调用多个容器）。
- en: At the time of writing this book, MCEs don’t support GPU-based instances.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在写这本书时，MCEs不支持基于GPU的实例。
- en: Now, let’s learn how to create an MCE by using a simple example of running TensorFlow
    and PyTorch models simultaneously. This will give you some practical skills in
    terms of how to create and use an MCE.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个简单的例子来学习如何创建MCE，运行TensorFlow和PyTorch模型并行工作。这将让你获得一些实际技能，了解如何创建和使用MCE。
- en: Implementing an MCE
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现MCE
- en: 'In this example, we will run an inference workload with two NLP models using
    different runtime environments: TensorFlow and PyTorch. We will host the Q&A model
    in a TensorFlow container and the text summarization model in a PyTorch container.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用不同运行时环境的两个NLP模型运行推理工作负载：TensorFlow和PyTorch。我们将在TensorFlow容器中托管问答模型，在PyTorch容器中托管文本摘要模型。
- en: 'Creating an MCE is very similar to creating an MME with a few notable exceptions,
    which we will highlight in the following steps:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建MCE与创建MME非常相似，除了几个显著的不同之处，我们将在以下步骤中进行突出说明：
- en: 'Fetching the model data, inference scripts, and model packaging is identical
    to what we did for the MME. Note that since one of our endpoints will run the
    TensorFlow container, the Q&A model should comply with the following directory
    structure:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取模型数据、推理脚本和模型包装的过程与我们为MME所做的一致。请注意，由于我们的一个端点将运行TensorFlow容器，因此问答模型应遵循以下目录结构：
- en: '[PRE9]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will configure the container and create the model package. Note that
    we provide two containers and endpoint mode, `Direct`, while creating the model
    package:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将配置容器并创建模型包。请注意，在创建模型包时，我们提供了两个容器和端点模式`Direct`：
- en: '[PRE10]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then, we will create the endpoint configuration and endpoint. This step is similar
    to that for the MME, so we have omitted the code snippet for brevity.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建端点配置和端点。这个步骤类似于MME，因此我们省略了代码片段以简化内容。
- en: 'Once the endpoint has been deployed, we are ready to send inference traffic.
    Note that we supply the `TargetContainerHostname` header so that SageMaker knows
    where to route our inference request:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点部署完成，我们就可以开始发送推理流量。请注意，我们提供了 `TargetContainerHostname` 头信息，以便 SageMaker
    知道将推理请求路由到哪里：
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So far, we have discussed how to host multiple models on SageMaker. Next, we
    will discuss how to safely promote a new version of the model (or a different
    model altogether) while keeping the endpoint operational for end users. For this,
    we will review SageMaker multi-variant endpoints.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何在 SageMaker 上托管多个模型。接下来，我们将讨论如何在保持端点对最终用户可用的同时，安全地推广新版本的模型（或完全不同的模型）。为此，我们将回顾
    SageMaker 多变体端点。
- en: Multi-variant endpoints
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多变体端点
- en: 'A production variant is a SageMaker-specific concept that defines a combination
    of the model, its container, and the resources required to run this model. As
    such, this is an extremely flexible concept that can be used for different use
    cases, such as the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 生产变体是 SageMaker 特有的概念，它定义了模型、其容器以及运行该模型所需资源的组合。因此，这是一个极其灵活的概念，可以用于不同的使用场景，例如以下情况：
- en: Different model versions with the same runtime and resource requirements
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有相同运行时和资源需求的不同模型版本
- en: Different models with different runtimes and/or resource requirements
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有不同运行时和/或资源需求的不同模型
- en: The same model with different runtimes and/or resource requirements
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同模型与不同的运行时和/或资源需求
- en: 'Additionally, as part of the variant configuration, you also define its traffic
    weights, which can be then updated without them having any impact on endpoint
    availability. Once deployed, the production variant can be invoked directly (so
    you can bypass SageMaker traffic shaping) or as part of the SageMaker endpoint
    call (then, SageMaker traffic shaping is not bypassed). The following diagram
    provides more details:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作为变体配置的一部分，你还需要定义其流量权重，这些权重可以在不影响端点可用性的情况下进行更新。部署后，生产变体可以直接调用（从而绕过 SageMaker
    流量控制），也可以作为 SageMaker 端点调用的一部分（此时不会绕过 SageMaker 流量控制）。以下图示提供了更多细节：
- en: '![Figure 10.3 – Using production variants with traffic shaping (left) and with
    a direct invocation (right) ](img/B17519_10_003.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 使用流量控制的生产变体（左）和直接调用的生产变体（右）](img/B17519_10_003.jpg)'
- en: Figure 10.3 – Using production variants with traffic shaping (left) and with
    a direct invocation (right)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 使用流量控制的生产变体（左）和直接调用的生产变体（右）
- en: When updating production variants, the real-time endpoint stays available and
    no interruption occurs for the end users. This also means that you will incur
    additional costs, as each production variant will have an associated cost.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新生产变体时，实时端点保持可用，并且不会对最终用户造成中断。这也意味着你将产生额外的费用，因为每个生产变体都会有一个关联的费用。
- en: Now, let’s see how we can use production variants to test a new production variant.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用生产变体来测试一个新的生产变体。
- en: Using production variants for A/B testing
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用生产变体进行 A/B 测试
- en: 'In this example, we will register two different models for the same Q&A NLP
    task. Then, we will shape the inference traffic using the production variant weights
    and invoke the models directly. The full code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb).
    Follow these steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将为相同的问答 NLP 任务注册两个不同的模型。然后，我们将使用生产变体权重对推理流量进行流量控制，并直接调用模型。完整代码可在此处获取：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/4_AB_Testing.ipynb)。请按照以下步骤进行操作：
- en: We will start by creating two HuggingFace models using the `HuggingFaceModel`
    class. We have omitted this for brevity.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从使用 `HuggingFaceModel` 类创建两个 HuggingFace 模型开始。为了简洁起见，我们省略了这部分。
- en: 'Then, we will create two different endpoint variants. We start with the equal
    weights parameter, which tells SageMaker that inference traffic should split evenly
    between model variants:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建两个不同的端点变体。我们从相等权重参数开始，这告诉 SageMaker 推理流量应该在模型变体之间均匀分配：
- en: '[PRE12]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After that, we create the endpoint based on our configured production variants:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们根据配置的生产变体创建端点：
- en: '[PRE13]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once the endpoint has been deployed, we can run inference against the newly
    created endpoint. Once you run the following code, the resulting statistics should
    show that each production variant served ~50% of inference traffic:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点部署完成，我们就可以对新创建的端点进行推理。运行以下代码后，结果统计应显示每个生产变体大约服务了 ~50% 的推理流量：
- en: '[PRE14]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we can update the weights of our endpoint variants. Re-running the previous
    inference test loop should now show that only ~10% of traffic is served by `"Variant1"`,
    which is expected based on the provided variant traffic weights:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以更新端点变体的权重。重新运行先前的推理测试循环，现在应该显示只有 ~10% 的流量由 `"Variant1"` 服务，这与提供的变体流量权重一致：
- en: '[PRE15]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also bypass SageMaker traffic shaping and directly invoke a specific
    variant by using the `TargetVariant` parameter, as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以绕过 SageMaker 的流量整形，直接通过使用 `TargetVariant` 参数调用特定变体，具体如下：
- en: '[PRE16]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: SageMaker’s production variants provide you with a flexible mechanism to operate
    your inference workloads in production or production-like environments.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 的生产变体为您提供了一种灵活的机制，用于在生产或类生产环境中运行推理工作负载。
- en: Serverless inference endpoints
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无服务器推理端点
- en: Using **serverless inference endpoints** (**SIEs**) is another deployment option
    available on SageMaker. It allows you to provision real-time inference endpoints
    without the need to provision and configure the underlying endpoint instances.
    SageMaker automatically provisions and scales the underlying available compute
    resources based on your inference traffic. Your SIE can scale them down to 0 in
    cases where there is no inference traffic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**无服务器推理端点**（**SIEs**）是 SageMaker 提供的另一种部署选项。它允许您在无需预配置和配置底层端点实例的情况下，提供实时推理端点。SageMaker
    会根据推理流量自动配置和扩展底层可用计算资源。如果没有推理流量，您的 SIE 可以将其缩减为 0。
- en: SIEs are a good fit for scenarios where there’s an uneven traffic pattern and
    you can tolerate short periods of elevated latency during a **cold start**. A
    cold start period specifies the time needed to provision new serverless resources
    and deploy your model runtime environment. Since larger models generally have
    longer deployment times than smaller ones, they will have longer cold start periods
    too. One potential use case for Serverless Inference is using it in test and sandbox
    environments. With an SIE, you pay only for the time that the SIE takes to process
    the inference request.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: SIEs 非常适合流量模式不均匀的场景，且您可以容忍在**冷启动**期间出现短暂的延迟增加。冷启动时长指定了配置新的无服务器资源并部署模型运行时环境所需的时间。由于较大的模型通常比小模型需要更长的部署时间，因此它们的冷启动时长也会更长。无服务器推理的一个潜在用例是在测试和沙盒环境中使用它。使用
    SIE，您只需为 SIE 处理推理请求所花费的时间付费。
- en: 'Serverless Inference is functionally similar to SageMaker real-time inference.
    It supports many types of inference containers, including PyTorch and TensorFlow
    inference containers. However, Serverless Inference also has several limitations,
    including the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器推理在功能上类似于 SageMaker 实时推理。它支持多种推理容器类型，包括 PyTorch 和 TensorFlow 推理容器。然而， 无服务器推理也有一些限制，主要包括以下几点：
- en: No GPU resources are available
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有可用的 GPU 资源
- en: The instance disk size is 5 GB
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例磁盘大小为 5 GB
- en: The maximum concurrency for the endpoint is 200; requests beyond this limit
    will be throttled by SageMaker
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端点的最大并发量为 200；超出此限制的请求将被 SageMaker 限流。
- en: The cold start period depends on your model size and inference container start
    time
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冷启动时长取决于您的模型大小和推理容器的启动时间
- en: When creating SIE resources, you can choose from the list of available memory
    options, and SageMaker will automatically assign the proportional number of vCPUs.
    During the memory configuration, you will need the size of the memory to be at
    least slightly higher than your model size, and the minimum memory size must be
    1,024 MB; the maximum is 6,144 MB. If your model performance is CPU-bound, you
    may choose a bigger memory configuration to have more vCPU resources.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 SIE 资源时，您可以从可用的内存选项列表中进行选择，SageMaker 会自动分配相应数量的 vCPU。在配置内存时，您需要确保内存大小至少略大于模型大小，并且最小内存大小必须为
    1,024 MB；最大内存为 6,144 MB。如果您的模型性能受 CPU 限制，您可以选择更大的内存配置，以获得更多的 vCPU 资源。
- en: Now, let’s see how we can deploy a serverless endpoint using the SageMaker Python
    SDK.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用 SageMaker Python SDK 部署一个无服务器端点。
- en: Deploying a serverless endpoint
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署无服务器端点
- en: 'In this example, we will deploy the Q&A NLP model from the HuggingFace Model
    hub. The full code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb).
    Follow these steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将部署来自 HuggingFace 模型库的问答 NLP 模型。完整代码可见这里：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/3_Serverless_Inference.ipynb)。按照以下步骤操作：
- en: 'We will start by defining the SageMaker model to deploy. For this, we will
    fetch a CPU version of the HuggingFace inference container using the built-in
    `image_uris` method, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从定义要部署的 SageMaker 模型开始。为此，我们将使用内置的 `image_uris` 方法获取 HuggingFace 推理容器的 CPU
    版本，如下所示：
- en: '[PRE17]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we will use the `HuggingFaceModel` instance to configure the model architecture
    and target NLP task:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `HuggingFaceModel` 实例来配置模型架构和目标 NLP 任务：
- en: '[PRE18]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we will define the serverless configuration and deploy our first endpoint.
    Here, the `memory_size_in_mb` parameter defines the initial memory behind your
    endpoint and the `max_concurrency` parameter defines the maximum number of concurrent
    invocations your endpoint can handle before inference traffic gets throttled by
    SageMaker:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义无服务器配置并部署我们的第一个端点。此处，`memory_size_in_mb` 参数定义了端点背后的初始内存，`max_concurrency`
    参数定义了端点在推理流量被 SageMaker 限流之前可以处理的最大并发调用数：
- en: '[PRE19]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: That’s it! In several minutes, your endpoint will be deployed. After that, you
    can use it as any other real-time endpoint.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！几分钟后，你的端点将被部署。之后，你可以像使用任何其他实时端点一样使用它。
- en: With Serverless Inference, SageMaker automatically scales in and out of your
    endpoints without much input from your side other than the memory sizing and concurrency.
    In the next section, we will review the endpoint autoscaling capability, which
    provides you more with fine-grained control over scaling behavior.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无服务器推理时，SageMaker 会在无需太多输入的情况下自动扩展端点，除了内存大小和并发性设置。在下一节中，我们将回顾端点自动扩展功能，它为你提供了更多细粒度的扩展行为控制。
- en: Advanced model deployment techniques
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级模型部署技巧
- en: In this section, we will discuss some advanced techniques for managing your
    SageMaker inference resources, namely autoscaling and blue/green deployments.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些管理 SageMaker 推理资源的高级技巧，即自动扩展和蓝绿部署。
- en: Autoscaling endpoints
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自动扩展端点
- en: 'SageMaker allows you to automatically scale out (increase the number of instances)
    and scale in (decrease the number of instances) for real-time endpoints and asynchronous
    endpoints. When inference traffic increases, scaling out maintains steady endpoint
    performance while keeping costs to a minimum. When inference traffic decreases,
    scaling in allows you to minimize the inference costs. For real-time endpoints,
    the minimum instance size is 1; asynchronous endpoints can scale to 0 instances.
    The following diagram shows this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 允许你自动扩展（增加实例数量）和收缩（减少实例数量）实时端点和异步端点。当推理流量增加时，扩展可以保持端点性能稳定，同时将成本控制在最低。流量减少时，收缩可以帮助你降低推理成本。对于实时端点，最小实例大小为
    1；而异步端点则可以扩展到 0 实例。下图展示了这一点：
- en: '![Figure 10.4 – Autoscaling concepts ](img/B17519_10_004.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 自动扩展概念](img/B17519_10_004.jpg)'
- en: Figure 10.4 – Autoscaling concepts
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 自动扩展概念
- en: During scaling events, SageMaker endpoints remain fully available for end users.
    In the case of downsizing an endpoint, SageMaker automatically drains traffic
    from instances so that they can be removed. To ensure additional resiliency, SageMaker
    places instances in different **availability zones**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展事件期间，SageMaker 端点始终保持完全可用，对终端用户可用。在收缩端点的情况下，SageMaker 会自动将流量从实例中排空，以便可以移除这些实例。为了确保更高的可用性，SageMaker
    会将实例分布在不同的 **可用区**。
- en: 'To autoscale your endpoint, you need to create a production variant for your
    model. After that, you must define the desired scaling behavior in the **autoscaling
    policy**. SageMaker supports four types of scaling policies, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要自动扩展你的端点，你需要为模型创建一个生产变体。之后，你必须在 **自动扩展策略** 中定义期望的扩展行为。SageMaker 支持四种类型的扩展策略，如下所示：
- en: '`TargetTrackingScaling`) allows you to scale endpoints based on the value of
    specific Amazon CloudWatch metrics. SageMaker supports several endpoint metrics
    out of the box, but you can also use your own custom metrics. The **CPUUtilization**,
    **GPUUtilization**, and **SageMakerVariantInvocationsPerInstance** metrics are
    usually good starting choices.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TargetTrackingScaling`）允许你根据特定Amazon CloudWatch指标的值来扩展端点。SageMaker默认支持多种端点指标，但你也可以使用自己的自定义指标。**CPUUtilization**、**GPUUtilization**和**SageMakerVariantInvocationsPerInstance**指标通常是一个不错的起点。'
- en: '**Step scaling** is a more advanced scaling policy that allows you to have
    finer control over how many instances are provisioned based on the size of the
    metric value change. This policy requires careful configuration and testing with
    various load profile values.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阶梯扩展**是一种更高级的扩展策略，允许你根据指标值变化的大小更精细地控制实例的配置数量。此策略需要根据不同的负载配置文件值进行仔细的配置和测试。'
- en: '**Scheduled scaling** allows you to scale endpoints based on a predefined schedule.
    For instance, you can scale in after hours, and scale out during peak work hours.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定时扩展**允许你根据预定的时间表扩展端点。例如，你可以在非工作时间缩减扩展，在工作高峰时段进行扩展。'
- en: '**On-Demand scaling** changes the endpoint instance count based on explicit
    user requests.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按需扩展**根据用户的明确请求来改变端点实例的数量。'
- en: When selecting and configuring autoscaling policies, you may start by analyzing
    your traffic patterns and how they correlate with your endpoint metrics. Load
    profiles define which type of scaling policy to choose, while correlating to endpoint
    metrics allows you to select good tracking metrics. It’s recommended that you
    start with a simple baseline (for example, simple scaling with the CPUUtilization
    tracking metric). Then, you can fine-tune it over time as you observe other traffic
    patterns and how autoscaling reacts to them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择和配置自动扩展策略时，你可以通过分析流量模式以及它们与端点指标的相关性来开始。负载配置文件定义了选择哪种类型的扩展策略，而与端点指标的相关性使你能够选择合适的跟踪指标。建议你从一个简单的基准开始（例如，使用CPUUtilization跟踪指标的简单扩展）。然后，你可以随着观察到其他流量模式以及自动扩展对它们的反应，逐步进行调整。
- en: In the following example, we will learn how to apply autoscaling policies to
    a SageMaker real-time endpoint.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将学习如何将自动扩展策略应用于SageMaker实时端点。
- en: Implementing autoscaling for inference endpoints
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为推理端点实现自动扩展
- en: 'In this example, we will learn how to apply the target tracking autoscaling
    policy to a real-time endpoint. The full code is available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb).
    Follow these steps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将学习如何将目标跟踪自动扩展策略应用于实时端点。完整的代码可以在这里找到：[https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter10/5_AutoScaling.ipynb)。按照以下步骤操作：
- en: We will start by creating a regular SageMaker real-time endpoint. We have omitted
    this code for brevity.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建一个常规的SageMaker实时端点。为简便起见，我们省略了此部分代码。
- en: 'Next, we will create two autoscaling resources: a **scalable target** and a
    **scaling policy**. The scalable target defines a specific AWS resource that we
    want to scale using the Application Auto Scaling service.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建两个自动扩展资源：**可扩展目标**和**扩展策略**。可扩展目标定义了我们希望通过应用程序自动扩展服务进行扩展的特定AWS资源。
- en: 'In the following code snippet, we are instantiating the client for the Application
    Auto Scaling service and registering our SageMaker endpoint as a scalable target.
    Note that the `ResourceId` parameter defines a reference to a specific endpoint
    and production variant. The `ScalableDimension` parameter for SageMaker resources
    always references the number of instances behind the production variant. `MinCapacity`
    and `MaxCapacity` define the instance scaling range:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们实例化了应用程序自动扩展服务的客户端，并将我们的SageMaker端点注册为可扩展目标。请注意，`ResourceId`参数定义了对特定端点和生产变体的引用。SageMaker资源的`ScalableDimension`参数始终引用生产变体后面的实例数量。`MinCapacity`和`MaxCapacity`定义了实例扩展的范围：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we will create a policy for our scalable target. Here, we chose to use
    the target tracking policy type with the following parameters:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将为可扩展目标创建一个策略。在这里，我们选择使用目标跟踪策略类型，并设置以下参数：
- en: '[PRE21]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Once the policy is in place, we can test it. For this, we need to generate sufficient
    inference traffic to breach the target metric value for a duration longer than
    the scale-out cooldown period. For this purpose, we can use the Locust.io load
    testing framework ([https://locust.io/](https://locust.io/)), which provides a
    simple mechanism to mimic various load patterns. Follow the instructions in the
    notebook to create a Locust configuration for your endpoint and provide your AWS
    credentials for authorization purposes.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦策略就绪，我们可以进行测试。为此，我们需要生成足够的推理流量，超出目标指标值，并持续时间超过扩展冷却期。为此，我们可以使用Locust.io负载测试框架([https://locust.io/](https://locust.io/))，它提供了一种简单的机制来模拟各种负载模式。按照笔记本中的说明创建Locust配置，为您的端点提供AWS凭证进行授权。
- en: 'Once the configuration is complete, you can start your Locust client to generate
    load using the following terminal command. It generates an inference load of up
    to 20 concurrent users for 5 minutes. This load profile should trigger a scaling-out
    event for our endpoint:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置完成后，您可以启动Locust客户端，通过以下终端命令生成负载。它会在5分钟内生成最多20个并发用户的推理负载。这个负载配置应该会触发端点的扩展事件：
- en: '[PRE22]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'During the load test, you can observe your endpoint status as well as the associated
    scaling alerts in the Amazon CloudWatch console. First, you can see that scale-out
    and scale-in alerts have been configured based on the provided cooldown periods
    and target metric value:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在负载测试期间，您可以在Amazon CloudWatch控制台中观察端点状态以及相关的扩展警报。首先，您可以看到已经根据提供的冷却期和目标指标值配置了扩展和收缩警报：
- en: '![Figure 10.5 – Autoscaling alerts for SageMaker endpoints ](img/B17519_10_005.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – SageMaker端点的自动扩展警报](img/B17519_10_005.jpg)'
- en: Figure 10.5 – Autoscaling alerts for SageMaker endpoints
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – SageMaker端点的自动扩展警报
- en: 'After the initial scale-out cooldown period has passed, the scale-out alert
    switches to the **In alarm** state, which causes the endpoint to scale out. Note
    that in the following screenshot, the red line is the desired value of the tracking
    metric, while the blue line is the number of invocations per endpoint instance:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始扩展冷却期结束后，扩展警报会切换到**告警中**状态，从而导致端点进行扩展。请注意，在以下截图中，红线是跟踪指标的目标值，而蓝线是每个端点实例的调用次数：
- en: '![Figure 10.6 – Triggered a scaling-out alert ](img/B17519_10_006.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 触发了扩展警报](img/B17519_10_006.jpg)'
- en: Figure 10.6 – Triggered a scaling-out alert
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 触发了扩展警报
- en: 'After triggering scaling out, your endpoint status will change from `in Service`
    to `Updating`. Now, we can run the `describe_endpoint()` method to confirm that
    the number of instances has been increased. Since we are generating a sufficiently
    large concurrent load in a short period, SageMaker immediately scaled our endpoint
    to the maximum number of instances. The following code is for the `describe_endpoint()`
    method:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在触发扩展后，您的端点状态将从`in Service`变为`Updating`。现在，我们可以运行`describe_endpoint()`方法来确认实例数量已经增加。由于我们在短时间内生成了足够大的并发负载，SageMaker立即将端点扩展到最大实例数。以下代码用于`describe_endpoint()`方法：
- en: '[PRE23]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Since we are no longer running an inference traffic generator, we should expect
    our endpoint to scale in once the scale-in cooldown period has passed.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不再运行推理流量生成器，一旦扩展收缩冷却期结束，我们应该预期端点会收缩。
- en: In the next section, we will review how to securely and reliably deploy model
    candidates using SageMaker Deployment Guardrails.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾如何使用SageMaker部署护栏安全可靠地部署模型候选。
- en: Using blue/green deployment patterns
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用蓝绿部署模式
- en: So far, we have discussed how to deploy and update SageMaker endpoints via APIs
    or SDK calls. However, this approach may not fit when you’re updating mission-critical
    workloads in production, where you need to have additional checks to ensure smooth
    production rollout.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何通过API或SDK调用部署和更新SageMaker端点。然而，这种方法可能不适用于更新生产中的关键任务工作负载，因为在这种情况下，你需要额外的检查来确保生产部署的顺利进行。
- en: SageMaker **Deployment Guardrails** is a fully managed endpoint promotion mechanism.
    Guardrails follows the blue/green deployment concept, which is common for DevOps
    practices. Here, the blue fleet is the old deployment (the production variant
    in the case of SageMaker endpoints), while the green fleet is the new version
    to be deployed. SageMaker provisions a green fleet next to the blue fleet. Once
    the green fleet is ready and healthy, SageMaker starts shifting traffic according
    to the predefined rules from the blue fleet to the green fleet.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker **部署保护措施**是一个完全托管的端点推广机制。保护措施遵循蓝绿部署概念，这是DevOps实践中常见的一种方式。在这里，蓝色舰队是旧的部署（在SageMaker端点中是生产变体），而绿色舰队是要部署的新版本。SageMaker会在蓝色舰队旁边配置绿色舰队。一旦绿色舰队准备就绪且健康，SageMaker就会按照预定义的规则将流量从蓝色舰队转移到绿色舰队。
- en: 'Deployment Guardrails supports several modes of traffic shifting:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 部署保护措施支持多种流量转移模式：
- en: '**All at once** mode shifts all traffic from blue to green in one step once
    the green fleet is up and healthy. At this point, SageMaker decommissions the
    blue fleet.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**All at once**模式一旦绿色舰队启动并健康，就一次性将所有流量从蓝色转移到绿色。在这一点上，SageMaker会停用蓝色舰队。'
- en: '**Canary** mode shifts a small portion of traffic to the green fleet. Then,
    if the canaries are healthy, SageMaker shifts the remainder of the traffic to
    the green fleet. After that, SageMaker decommissions the blue fleet.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Canary**模式将一小部分流量转移到绿色舰队。如果金丝雀（canaries）健康，SageMaker将剩余的流量转移到绿色舰队。之后，SageMaker会停用蓝色舰队。'
- en: '**Linear** mode gradually shifts the traffic from the blue fleet to the green
    fleet.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Linear**模式逐步将流量从蓝色舰队转移到绿色舰队。'
- en: Note that during a blue-green deployment, you will incur costs for both the
    blue and green fleets while they are running. If, during the rollout, the green
    fleet becomes unhealthy, SageMaker will execute an automatic rollback to the initial
    deployment to avoid any impact on the end user experience.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在蓝绿部署期间，您将为蓝色和绿色舰队的运行同时产生费用。如果在部署过程中绿色舰队变得不健康，SageMaker会执行自动回滚到初始部署，以避免对最终用户体验产生影响。
- en: 'Deployment Guardrails doesn’t support the following features:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 部署保护措施不支持以下功能：
- en: Marketplace containers
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场容器
- en: Multi-container endpoints
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多容器端点
- en: Multi-model endpoints
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模型端点
- en: Multi-variant endpoints
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变体端点
- en: Endpoints that use Inferentia-based instances
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Inferentia基础实例的端点
- en: Endpoints that use Amazon SageMaker Model Monitor (with data capture enabled)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker模型监控器（启用数据捕获）的端点
- en: Practicing setting up deployment guardrails is outside the scope of this book,
    as these types of tasks are typically performed by dedicated DevOps/MLOps teams.
    However, it’s important to understand that SageMaker supports such capabilities
    out of the box.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 部署保护措施的设置实践超出了本书的范围，因为这些任务通常由专门的DevOps/MLOps团队执行。然而，重要的是要了解SageMaker原生支持这些功能。
- en: Monitoring inference workloads
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控推理工作负载
- en: In this section, we will cover the available mechanisms for monitoring inference
    workloads.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍用于监控推理工作负载的可用机制。
- en: Using Amazon CloudWatch
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Amazon CloudWatch
- en: 'Throughout this book, we have frequently referenced Amazon CloudWatch. SageMaker
    relies on it for all monitoring needs, specifically the following:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们多次提到Amazon CloudWatch。SageMaker依赖它来满足所有的监控需求，具体包括以下内容：
- en: Uses CloudWatch Logs to collect, organize, and manage SageMaker logs (for example,
    your model server logs)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CloudWatch日志来收集、组织和管理SageMaker日志（例如，您的模型服务器日志）。
- en: Uses CloudWatch Metrics to measure endpoint characteristics such as latency,
    resource utilization, and others
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CloudWatch指标来衡量端点特征，例如延迟、资源利用率等。
- en: Uses CloudWatch alarms to trigger autoscaling events
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CloudWatch警报触发自动扩展事件。
- en: 'SageMaker inference workloads support several metrics out of the box. Depending
    on the chosen inference workload option and deployment pattern, your default SageMaker
    metrics may vary. For instance, for an MME, you will have additional default metrics
    to measure some specific characteristics, such as the model’s performance and
    loading time. We recommend that you refer to the SageMaker documentation for the
    most up-to-date information on default SageMaker metrics: [https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker推断工作负载支持多种出厂指标。根据选择的推断工作负载选项和部署模式，您的默认SageMaker指标可能会有所不同。例如，对于MME，您将获得额外的默认指标以衡量一些特定特征，如模型性能和加载时间。我们建议您参考SageMaker文档，获取关于默认SageMaker指标的最新信息：[https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.xhtml)。
- en: 'If, for some reason, the out-of-the-box metrics are not sufficient for your
    use case, you can always create custom metrics. Some scenarios where custom metrics
    can be useful are as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果由于某种原因，现有的出厂指标不足以满足您的用例需求，您始终可以创建自定义指标。一些适用自定义指标的场景如下：
- en: Your model and model server require custom metrics for appropriate scaling.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的模型和模型服务器需要自定义指标以进行适当的扩展。
- en: You need a higher resolution of metrics. Note that SageMaker default metrics
    to a 1-second resolution.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要更高分辨率的指标。请注意，SageMaker默认指标为1秒分辨率。
- en: You need to do custom metrics pre-processing. For instance, you may need to
    apply a sliding window average that’s not supported by CloudWatch.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要进行自定义指标预处理。例如，您可能需要应用CloudWatch不支持的滑动窗口平均值。
- en: You can also create custom CloudWatch alarms. Note that you can create alarms
    for both metrics and logs. CloudWatch alarms can be used to notify you about specific
    events via email or text notifications (this will require integrating your alarms
    with the Amazon SNS service).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以创建自定义CloudWatch警报。请注意，您可以为指标和日志创建警报。CloudWatch警报可用于通过电子邮件或短信通知您特定事件（这将需要将您的警报与Amazon
    SNS服务集成）。
- en: Another popular use case for CloudWatch alarms is to perform actions once an
    alarm is triggered. We have already seen how CloudWatch alarms are used to scale
    your SageMaker endpoint in and out. However, you can use alarms for any other
    custom logic. For example, you may integrate your custom alarm with an Amazon
    Lambda serverless function. Your function and its custom logic (for example, an
    endpoint update action) will be executed once the alarms is triggered.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个CloudWatch警报的热门用例是在触发警报后执行操作。我们已经看到CloudWatch警报如何用于调整您的SageMaker端点的大小。但是，您可以将警报用于任何其他自定义逻辑。例如，您可以将自定义警报与Amazon
    Lambda无服务器函数集成。一旦触发警报，将执行您的功能及其自定义逻辑（例如，端点更新操作）。
- en: Monitoring inference workload quality
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控推断工作质量
- en: SageMaker Model Monitor is a purpose-built capability for measuring and continuously
    monitoring the quality of your inference. It allows you to calculate baseline
    statistics for your inference inputs and model outputs and then monitor how your
    models perform against baseline statistics in near-real time. In the case of significant
    deviations from the predefined statistical constraints, SageMaker Model Monitor
    will generate an alert to notify you that your model may be not performing according
    to the desired quality metrics.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker模型监控是专为衡量和持续监控推断质量而设计的能力。它允许您计算推断输入和模型输出的基线统计数据，然后实时监控模型在与基线统计数据的比较中的表现。在显著偏离预定义统计约束条件的情况下，SageMaker模型监控将生成警报，通知您模型可能未按期望的质量指标执行。
- en: 'Model Monitor comprises several components for monitoring different aspects
    of your inference quality:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Model Monitor包括多个组件，用于监控推断质量的不同方面：
- en: '**Data quality monitoring** allows you to detect **data drift** between data
    used to train your model and real inference traffic against the deployed model.
    Data drift usually results in a lower-than-expected quality of your model predictions.
    To detect data drift, Model Monitor calculates statistics for the training data
    (baseline), captures the inference traffic, and continuously compares these statistics
    to the baseline.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量监控** 允许您检测用于训练模型的数据与部署模型的实际推断流量之间的**数据漂移**。数据漂移通常会导致模型预测质量低于预期。为了检测数据漂移，Model
    Monitor计算训练数据（基线）的统计数据，捕获推断流量，并持续比较这些统计数据与基线。'
- en: '**Model quality monitoring** allows you to compare your model predictions to
    the predefined ground truth labels. If your model predictions violate the ground
    truth predictions by predefined constraints, Model Monitor will generate an alert.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型质量监控**使你能够将模型预测与预定义的真实标签进行比较。如果模型预测违反了预定义的真实标签约束，Model Monitor 会生成警报。'
- en: '**Bias drift monitoring** allows you to detect bias in your model predictions
    and how it changes over time. Model bias can be introduced when inference traffic
    is different from the data used for model training. To detect bias, Model Monitor
    calculates a specific bias metric called **Difference in Positive Proportions
    in Predicted Label** (**DPPL**). When the DPPL metric violates a predefined range
    of values, an alert is generated.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差漂移监控**使你能够检测模型预测中的偏差以及它随时间的变化。当推理流量与用于模型训练的数据不同，可能会引入模型偏差。为了检测偏差，Model
    Monitor 计算一个特定的偏差指标，称为**预测标签中正比例差异**（**DPPL**）。当 DPPL 指标违反预定义的数值范围时，将生成警报。'
- en: '**Model feature attribution monitoring** is another way to ensure that new
    bias is not introduced during model deployment. Feature attribution drift means
    that the influence that a specific feature has over an inference result changes
    over time.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型特征归因监控**是确保在模型部署过程中不会引入新偏差的另一种方式。特征归因漂移意味着某一特定特征对推理结果的影响随着时间变化。'
- en: Note
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Model Monitor only supports tabular data as inference inputs. This limits its
    applicability to DL inference since in most cases, DL models are used to perform
    inference on unstructured data, such as images or text.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Model Monitor 仅支持表格数据作为推理输入。这限制了其在深度学习推理中的适用性，因为在大多数情况下，深度学习模型用于对非结构化数据（例如图像或文本）进行推理。
- en: 'There are several scenarios where Model Monitor can apply to DL inference:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种场景可以将 Model Monitor 应用于深度学习推理：
- en: If you are using DL models to run classification or regression inference tasks.
    In practice, this rarely happens since classical **machine learning** (**ML**)
    algorithms (for example, XGBoost) often outperform DL models on such tasks and
    require a fraction of the resources for training and inference compared to more
    expensive DL models.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用深度学习模型进行分类或回归推理任务，实际上这种情况很少发生，因为经典的**机器学习**（**ML**）算法（例如 XGBoost）通常在此类任务上超越深度学习模型，并且相较于更昂贵的深度学习模型，它们在训练和推理时需要的资源也较少。
- en: If your inference input can be converted from an unstructured format into a
    structured format before it’s sent to the SageMaker inference resource – for example,
    if you convert your unstructured text into a tokenized input and send it for inference.
    In this case, the tokenized input can be represented as a tabular dataset so that
    it can be used with Model Monitor.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的推理输入可以在发送到 SageMaker 推理资源之前从非结构化格式转换为结构化格式——例如，如果你将非结构化文本转换为分词输入并发送进行推理。在这种情况下，分词后的输入可以表示为一个表格数据集，从而可以与
    Model Monitor 一起使用。
- en: Note that you can still use Model Monitor to ensure your model accuracy with
    DL workloads for scenarios where your model has either classification or regression
    outputs.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你仍然可以使用 Model Monitor 来确保你的深度学习工作负载在分类或回归输出场景中的模型精度。
- en: Selecting your workload configuration
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择工作负载配置
- en: 'In the previous three chapters, we reviewed the different capabilities Amazon
    SageMaker provides to engineer and operate inference workloads: from selecting
    optimal compute instances and runtime environments to configuring model servers
    and managing and monitoring deployed models.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面三章中，我们回顾了 Amazon SageMaker 提供的不同功能，用于设计和操作推理工作负载：从选择最佳计算实例和运行时环境到配置模型服务器以及管理和监控已部署的模型。
- en: In this section, we will summarize various selection criteria that you can use
    when selecting inference workload configurations. Then, we will suggest a simple
    algorithm that will guide the decision-making process when you’re choosing your
    inference configuration.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将总结在选择推理工作负载配置时可以使用的各种选择标准。接着，我们将建议一个简单的算法来指导你在选择推理配置时的决策过程。
- en: 'When engineering your inference workload, you may consider the following selection
    criteria:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计推理工作负载时，你可以考虑以下选择标准：
- en: '**Business use case**: This allows you to understand your business opportunity
    and end user experience by using your inference service. Analyzing your use case
    drives important decisions such as selecting the right SageMaker inference option
    and end user SLAs.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务应用场景**：这让你能够通过使用推理服务来了解你的商业机会和最终用户体验。分析你的应用场景能够推动一些重要的决策，比如选择合适的 SageMaker
    推理选项和最终用户 SLA（服务水平协议）。'
- en: '**Inference SLAs**: We have discussed two key inference SLAs in this book:
    latency and throughput. Understanding the desired SLAs drives decisions such as
    which instance type to use, model server configuration, and others.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理 SLA**：本书中我们讨论了两个关键的推理 SLA：延迟和吞吐量。了解期望的 SLA 可以推动一系列决策，比如选择使用哪种实例类型、模型服务器配置等。'
- en: '**Budget and cost**: It’s important to forecast both the inference budget and
    the setup mechanisms to monitor for budget usage (the running cost of inference).
    In the case of a budget overrun, you may want to have a mechanism to react to
    such an event (for example, sending a notification, scaling down an endpoint,
    and so on).'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预算和成本**：预测推理预算和设置监控预算使用的机制（推理运行成本）是非常重要的。如果预算超支，你可能需要有机制来应对此类事件（例如，发送通知、缩减端点规模等）。'
- en: '**Compute instances**: When you choose compute instances, you need to consider
    multiple factors, such as which model architecture you intend to use, your SLAs,
    and others. The process of selecting an instance type is called rightsizing and
    requires load tests and benchmarking to be performed.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算实例**：选择计算实例时，你需要考虑多个因素，如你打算使用的模型架构、你的 SLA 等。选择实例类型的过程称为合理化（rightsizing），需要进行负载测试和基准测试。'
- en: '**Input data and inference traffic**: You need to understand your data size
    (for offline inference) and inference traffic patterns (for online inference).
    For instance, if your traffic is seasonality patterns, you may be able to use
    endpoint autoscaling to minimize your inference costs.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入数据和推理流量**：你需要了解你的数据大小（针对离线推理）和推理流量模式（针对在线推理）。例如，如果你的流量具有季节性模式，你可能能够利用端点自动扩展来最小化推理成本。'
- en: '**Model runtime and deployment**: Depending on your model characteristics,
    inference traffic patterns, and chosen compute instances, you need to choose specific
    SageMaker containers and model packaging configurations (single model versus several
    models behind an endpoint). Another aspect to explore is the model promotion strategy
    and quality assurance in productions. For instance, earlier in this chapter, we
    discussed how to organize A/B testing on live SageMaker endpoints using production
    variants.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型运行时和部署**：根据你的模型特性、推理流量模式以及选择的计算实例，你需要选择特定的 SageMaker 容器和模型打包配置（单一模型与多个模型通过一个端点进行部署）。另一个需要探索的方面是模型推广策略和生产环境中的质量保证。例如，在本章早些时候，我们讨论了如何使用生产变体组织在
    SageMaker 实时端点上的 A/B 测试。'
- en: 'The following table highlights the key characteristics of the available SageMaker
    inference options:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格突出显示了可用的 SageMaker 推理选项的关键特性：
- en: '|  | **Real-Time Inference** | **Batch Transform** | **Asynchronous Inference**
    | **Serverless Inference** |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | **实时推理** | **批量转换** | **异步推理** | **无服务器推理** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Inference Type | Online(real-time response). | Offline. | Online(near-real-time
    inference, cold start during scale out from 0). | Online(cold start during scale
    out from 0). |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 推理类型 | 在线（实时响应） | 离线 | 在线（接近实时推理，冷启动时从 0 扩展） | 在线（冷启动时从 0 扩展） |'
- en: '| Resource Scaling | 1 to hundreds of instances behind a single endpoint. |
    1 to hundreds of instances in one inference job. | 0 to hundreds of instances.
    | 0 to 200 concurrent inference requests. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 资源扩展 | 单个端点下的 1 到数百个实例 | 单个推理任务中的 1 到数百个实例 | 0 到数百个实例 | 0 到 200 个并发推理请求 |'
- en: '| Payload Size | Up to 6 MB. | Up to 100 MB. | Up to 1 GB. | Up to 4 MB. |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 负载大小 | 最大 6 MB | 最大 100 MB | 最大 1 GB | 最大 4 MB |'
- en: '| Inference Timeout | 60 seconds. | No. | Up to 15 minutes. | 60 seconds. |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 推理超时 | 60 秒 | 无限制 | 最长 15 分钟 | 60 秒 |'
- en: '| Multi-Model/Multi-Container Support | Yes. | No. | No. | No. |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 多模型/多容器支持 | 是 | 否 | 否 | 否 |'
- en: '| Target Use Case | When you need to have consistent real-time inference latency.
    Supports a wide range of compute instances and model servers. | Offline inference
    or processing when the input dataset is available upfront. | When you need to
    handle larger payload sizes and/or processing times and additional inference latency
    is acceptable. There is a cost-saving opportunity to scale to 0 when there is
    no inference traffic. | When you need to have real-time inference with the lowest
    management overhead and associated costs. Pay only for served inference requests.
    You can scale to 0. |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 目标使用案例 | 当您需要具有一致的实时推理延迟时。支持广泛的计算实例和模型服务器。 | 当输入数据集可预先提供时，进行离线推理或处理。 | 当您需要处理较大的负载大小和/或处理时间，并且可以接受额外的推理延迟时。在没有推理流量时，可以进行零扩展来节省成本。
    | 当您需要最低的管理开销和相关成本的实时推理时。仅为服务的推理请求付费，且可以扩展至零。 |'
- en: Figure 10.7 – Comparing SageMaker inference options
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 比较 SageMaker 推理选项
- en: 'In the following diagram, we have organized several decision points you need
    to be aware of when selecting your inference workload implementation:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们整理了在选择推理工作负载实现时，您需要注意的几个决策点：
- en: '![Figure 10.8 – Selection algorithm for inference options ](img/B17519_10_008.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 推理选项的选择算法 ](img/B17519_10_008.jpg)'
- en: Figure 10.8 – Selection algorithm for inference options
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 推理选项的选择算法
- en: 'Note that your workload configuration is not static. Some non-extensive examples
    that may require you to reconsider your workload configuration choices include
    the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您的工作负载配置不是静态的。以下是一些可能需要您重新考虑工作负载配置选择的非全面示例：
- en: Traffic pattern changes may result in changes in your scaling policies
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量模式的变化可能会导致扩展策略的变化
- en: Changes in user SLAs may result in changes in the selected compute instances
    and/or updates in scaling policies
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户服务水平协议（SLA）的变化可能会导致所选计算实例的变化和/或扩展策略的更新
- en: New versions of the model architecture and/or available compute instances may
    require benchmarking against the baseline to measure potential accuracy or performance
    gains
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新版本的模型架构和/或可用的计算实例可能需要与基准进行基准测试，以衡量潜在的准确性或性能提升
- en: Hence, you should plan and budget for continuous monitoring and workload optimizations
    as part of your initial workload design.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您应该计划并预算用于持续监控和工作负载优化的费用，作为初始工作负载设计的一部分。
- en: Using SageMaker Inference Recommender
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker 推理推荐器
- en: Choosing an optimal inference configuration requires considerable engineering
    and testing efforts. To simplify this process, AWS recently introduced **SageMaker
    Inference Recommender**, which provides you with a simple way to assess your inference
    performance and costs for real-time endpoints in different configurations.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的推理配置需要相当多的工程和测试工作。为简化这一过程，AWS 最近推出了 **SageMaker 推理推荐器**，它为您提供了一种简便的方式，评估不同配置下实时端点的推理性能和成本。
- en: 'Inference Recommender deploys your model to real-time endpoints with different
    configurations, runs load testing against those endpoints, and then provides latency
    and throughput measures, as well as associated costs. Based on the generated measures,
    you can select the most appropriate configuration based on your SLAs and cost
    budget. SageMaker Inference Recommender provides the following benchmarks:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 推理推荐器将您的模型部署到具有不同配置的实时端点，进行负载测试，然后提供延迟和吞吐量度量，以及相关成本。根据生成的度量，您可以根据您的 SLA 和成本预算选择最合适的配置。SageMaker
    推理推荐器提供以下基准：
- en: End-to-end **model latency** in milliseconds
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端到端 **模型延迟**（毫秒级）
- en: '**Maximum invocations** per minute'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每分钟最大调用次数**'
- en: '**Cost per hour** and **cost per inference**'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每小时成本** 和 **每次推理成本**'
- en: 'SageMaker Inference Recommender is well suited for the following use cases:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 推理推荐器非常适合以下使用案例：
- en: Finding the optimal instance type. Note that you can either provide your own
    list of instance types you are interested in benchmarking or let SageMaker benchmark
    this list across all supported instances.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最佳实例类型。请注意，您可以提供自己感兴趣的实例类型列表进行基准测试，或者让 SageMaker 在所有支持的实例上对这个列表进行基准测试。
- en: Benchmarking compiled by SageMaker Neo models. Here, you can compare the performance
    of your original model to the performance of the compiled model variant.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 SageMaker Neo 模型编制的基准。在这里，您可以比较原始模型与编译后的模型变体的性能。
- en: Running a custom load test. Inference Recommender supports modeling different
    traffic patterns to benchmark your endpoint performance under different conditions.
    Hence, you can use SageMaker Inference Recommender to benchmark and fine-tune
    your model server configurations, different model versions, and more.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行自定义负载测试。推理推荐器支持模拟不同的流量模式，以在不同条件下基准测试你的端点性能。因此，你可以使用SageMaker推理推荐器来基准测试和微调你的模型服务器配置、不同的模型版本等。
- en: Note that at the time of writing, Inference Recommender only supports real-time
    endpoints. So, if you need to benchmark different inference options (for instance,
    Serverless Inference), you may need to use the custom benchmarking and load testing
    facilities. Also, benchmark statistics by Inference Recommender as well as the
    supported traffic patterns are limited.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在撰写时，推理推荐器仅支持实时端点。因此，如果你需要基准测试不同的推理选项（例如，无服务器推理），你可能需要使用自定义基准测试和负载测试功能。此外，推理推荐器提供的基准统计信息以及支持的流量模式是有限的。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed how to operationalize and optimize your inference
    workloads. We covered various inference options offered by Amazon SageMaker and
    model hosting options, such as multi-model, multi-container, and Serverless Inference.
    Then, we reviewed how to promote and test model candidates using the Production
    Variant capability.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使推理工作负载实现和优化。我们介绍了亚马逊SageMaker提供的各种推理选项和模型托管选项，例如多模型、多容器和无服务器推理。然后，我们回顾了如何使用生产变体功能来推广和测试模型候选。
- en: After that, we provided a high-level overview of advanced model deployment strategies
    using SageMaker Deployment Guardrails, as well as workload monitoring using the
    Amazon CloudWatch service and SageMaker’s Model Monitor capability. Finally, we
    summarized the key selection criteria and algorithms you should use when defining
    your inference workload configuration.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们提供了使用SageMaker部署保护措施的高级模型部署策略概述，以及使用亚马逊CloudWatch服务和SageMaker的模型监控功能进行工作负载监控。最后，我们总结了在定义推理工作负载配置时应使用的关键选择标准和算法。
