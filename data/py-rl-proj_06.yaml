- en: Learning to Play Go
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习围棋
- en: 'When considering the capabilities of AI, we often compare its performance for
    a particular task with what humans can achieve. AI agents are now able to surpass
    human-level competency in more complex tasks. In this chapter, we will build an
    agent that learns how to play what is considered the most complex board game of
    all time: Go. We will become familiar with the latest deep reinforcement learning
    algorithms that achieve superhuman performances, namely AlphaGo, and AlphaGo Zero,
    both of which were developed by Google''s DeepMind. We will also learn about Monte
    Carlo tree search, a popular tree-searching algorithm that is an integral component
    of turn-based game agents.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑AI的能力时，我们常常将其在特定任务中的表现与人类能达到的水平进行比较。如今，AI代理已经能够在更复杂的任务中超越人类水平。在本章中，我们将构建一个能够学习如何下围棋的代理，而围棋被认为是史上最复杂的棋盘游戏。我们将熟悉最新的深度强化学习算法，这些算法能够实现超越人类水平的表现，即AlphaGo和AlphaGo
    Zero，这两者都是由谷歌的DeepMind开发的。我们还将了解蒙特卡罗树搜索（Monte Carlo tree search），这是一种流行的树搜索算法，是回合制游戏代理的核心组成部分。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Introduction to Go and relevant research in AI
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 围棋介绍及AI相关研究
- en: Overview of AlphaGo and AlphaGo Zero
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo与AlphaGo Zero概述
- en: The Monte Carlo tree search algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒙特卡罗树搜索算法
- en: Implementation of AlphaGo Zero
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo Zero的实现
- en: A brief introduction to Go
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 围棋简介
- en: Go is a board game that was first recorded in China two millennia ago. Similar
    to other common board games, such as chess, shogi, and Othello, Go involves two
    players alternately placing black and white stones on a 19x19 board with the objective
    of capturing as much territory as possible by surrounding a larger total area
    of the board. One can capture their opponent's pieces by surrounding the opponent's
    pieces with their own pieces. Captured stones are removed from the board, thereby
    creating a void in which the opponent can no longer place stones unless the territory
    is captured back.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 围棋是一种最早在两千年前中国有记载的棋盘游戏。与象棋、将棋和黑白棋等其他常见棋盘游戏类似，围棋有两位玩家轮流在19x19的棋盘上放置黑白棋子，目标是通过围住尽可能多的区域来捕获更多的领土。玩家可以通过用自己的棋子围住对方的棋子来捕获对方的棋子。被捕获的棋子会从棋盘上移除，从而形成一个空白区域，除非对方的领土被重新夺回，否则对方无法在该区域放置棋子。
- en: A game ends when both players refuse to place a stone or either player resigns.
    Upon the termination of a game, the winner is decided by counting each player's
    territory and the number of captured stones.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当双方玩家都拒绝落子或其中一方认输时，比赛结束。比赛结束时，胜者通过计算每位玩家的领土和捕获的棋子数量来决定。
- en: Go and other board games
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 围棋及其他棋盘游戏
- en: Researchers have already created AI programs that outperform the best human
    players in board games such as chess and backgammon. In 1992, researchers from
    IBM developed TD-Gammon, which used classic reinforcement learning algorithms
    and an artificial neural network to play backgammon at the level of a top player.
    In 1997, Deep Blue, a chess-playing program developed by IBM and Carnegie Mellon
    University, defeated then world champion Garry Kasparov in a six-game face off.
    This was the first time that a computer program defeated the world champion in
    chess.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经创建出能够超越最佳人类选手的AI程序，用于象棋、跳棋等棋盘游戏。1992年，IBM的研究人员开发了TD-Gammon，采用经典的强化学习算法和人工神经网络，在跳棋比赛中达到了顶级玩家的水平。1997年，由IBM和卡内基梅隆大学开发的国际象棋程序Deep
    Blue，在六局对战中击败了当时的世界冠军加里·卡斯帕罗夫。这是第一次计算机程序在国际象棋中击败世界冠军。
- en: Developing Go playing agents is not a new topic, and hence one may wonder what
    took so long for researchers to replicate such successes in Go. The answer is
    simple—Go, despite its simple rules, is a far more complex game than chess. Imagine
    representing a board game as a tree, where each node is a snapshot of the board
    (which we also refer to as the **board state**) and its child nodes are possible
    moves the opponent can make. The height of the tree is essentially the number
    of moves a game lasts. A typical chess game lasts 80 moves, whereas a game in
    Go lasts 150; almost twice as long. Moreover, while the average number of possible
    moves in a chess turn is 35, a Go player has 250 possible plays per move. Based
    on these numbers, Go has 10^(761) total possible games, compared to 10^(120) games
    in chess. It is impossible to enumerate every possible state in Go in a computer,
    and the sheer complexity of the game has made it difficult for researchers to
    develop an agent that can play the game at a world-class level.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 开发围棋下棋智能体并不是一个新话题，因此人们可能会想，为什么研究人员花了这么长时间才在围棋领域复制出这样的成功。答案很简单——围棋，尽管规则简单，却远比国际象棋复杂。试想将一个棋盘游戏表示为一棵树，每个节点是棋盘的一个快照（我们也称之为**棋盘状态**），而它的子节点则是对手可能的下一步落子。树的高度本质上是游戏持续的步数。一场典型的国际象棋比赛大约进行80步，而一场围棋比赛则持续150步；几乎是国际象棋的两倍。此外，国际象棋每一步的平均可选步数为35，而围棋每步可下的棋局则多达250种可能。根据这些数字，围棋的总游戏可能性为10^(761)，而国际象棋的则为10^(120)。在计算机中枚举围棋的每一种可能状态几乎是不可能的，这使得研究人员很难开发出能够在世界级水平上进行围棋对弈的智能体。
- en: Go and AI research
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 围棋与人工智能研究
- en: In 2015, researchers from Google's DeepMind published a paper in Nature that
    detailed a novel reinforcement learning agent for Go called **AlphaGo**. In October
    of that year, AlphaGo beat Fan Hui, the European champion, 5-0\. In 2016, AlphaGo
    challenged Lee Sedol, who, with 18 world championship titles, is considered one
    of the greatest players in modern history. AlphaGo won 4-1, marking a watershed
    moment in deep learning research and the game's history. In the following year,
    DeepMind published an updated version of AlphaGo, AlphaGo Zero, which defeated
    its predecessor 100 times in 100 games. In just a matter of days of training,
    AlphaGo and AlphaGo Zero were able to learn and surpass the wisdom that mankind
    has accumulated over the thousands of years of the game's existence.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，谷歌DeepMind的研究人员在《自然》杂志上发表了一篇论文，详细介绍了一种新型的围棋强化学习智能体——**AlphaGo**。同年10月，AlphaGo以5-0战胜了欧洲冠军范辉（Fan
    Hui）。2016年，AlphaGo挑战了拥有18次世界冠军头衔的李世石，李世石被认为是现代围棋史上最伟大的选手之一。AlphaGo以4-1获胜，标志着深度学习研究和围棋历史的一个分水岭。次年，DeepMind发布了AlphaGo的更新版本——AlphaGo
    Zero，并在100场比赛中以100战全胜的成绩击败了其前身。在仅仅几天的训练后，AlphaGo和AlphaGo Zero就学会并超越了人类数千年围棋智慧的积累。
- en: The following sections will discuss how AlphaGo and AlphaGo Zero work, including
    the algorithms and techniques that they use to learn and play the game. This will
    be followed by an implementation of AlphaGo Zero. Our exploration begins with
    Monte Carlo tree search, an algorithm that is integral to both AlphaGo and AlphaGo
    Zero for making decisions on where to place stones.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将讨论AlphaGo和AlphaGo Zero的工作原理，包括它们用于学习和下棋的算法和技术。紧接着将介绍AlphaGo Zero的实现。我们的探索从蒙特卡洛树搜索算法开始，这一算法对AlphaGo和AlphaGo
    Zero在做出落子决策时至关重要。
- en: Monte Carlo tree search
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索
- en: In games such as Go and chess, players have perfect information, meaning they
    have access to the full game state (the board and the positions of the pieces).
    Moreover, there lacks an element of chance that can affect the game state; only
    the players' decisions can affect the board. Such games are also referred to as perfect-information
    games. In perfect-information games, it is theoretically possible to enumerate
    all possible game states. As discussed earlier, this would look such as a tree,
    where each child node (a game state) is a possible outcome of the parent. In two-player
    games, alternating levels of this tree represent moves produced by the two competitors.
    Finding the best possible move for a given state is simply a matter of traversing
    the tree and finding which sequence of moves leads to a win. We can also store
    the value, or the expected outcome or reward (a win or a loss) of a given state,
    at each node.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在围棋和国际象棋等游戏中，玩家拥有完美的信息，这意味着他们可以访问完整的游戏状态（棋盘和棋子的摆放位置）。此外，游戏状态不受随机因素的影响；只有玩家的决策能影响棋盘。这类游戏通常被称为**完全信息游戏**。在完全信息游戏中，理论上可以枚举所有可能的游戏状态。如前所述，这些状态可以表现为一棵树，其中每个子节点（游戏状态）是父节点的可能结果。在两人对弈的游戏中，树的交替层次表示两个竞争者所做的步棋。为给定状态找到最佳的步棋，实际上就是遍历树并找到哪一系列步棋能够导致胜利。我们还可以在每个节点存储给定状态的价值，或预期结果或奖励（胜利或失败）。
- en: 'However, constructing a perfect tree is impractical in practice for games such
    as Go. So how can an agent learn how to play the game without such knowledge?
    The **Monte Carlo tree-search** (**MCTS**) algorithm provides an efficient approximation
    of this complete tree. In a nutshell, MCTS involves playing a game iteratively,
    keeping statistics on states that were visited, and learning which moves are more
    favorable/likely to lead to a win. The goal of MCTS is to build a tree that approximates
    the aforementioned perfect tree as much as possible. Each move in a game corresponds
    to an iteration of the MCTS algorithm. There are four main steps in the algorithm:
    Selection, Expansion, Simulation, and Update (also known as **backpropagation**).
    We will briefly detail each procedure.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于围棋等游戏来说，构建一个完美的树是不现实的。那么，代理如何在没有这种知识的情况下学会如何下棋呢？**蒙特卡洛树搜索**（**MCTS**）算法提供了一个高效的近似完美树的方法。简而言之，MCTS涉及反复进行游戏，记录访问过的状态，并学习哪些步骤更有利/更可能导致胜利。MCTS的目标是尽可能构建一个近似前述完美树的树。游戏中的每一步对应MCTS算法的一次迭代。该算法有四个主要步骤：选择、扩展、模拟和更新（也称为**反向传播**）。我们将简要说明每个过程。
- en: Selection
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择
- en: 'The first step of MCTS involves playing the game intelligently. That means
    the algorithm has enough experience to determine the next move given a state.
    One method for determining the next move is called **Upper Confidence Bound 1
    Applied to Trees** (**UCT**). In short, this formula rates moves based on the
    following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS的第一步是智能地进行游戏。这意味着算法具有足够的经验来根据状态确定下一步棋。确定下一步棋的方法之一叫做**上置信界限1应用于树**（**UCT**）。简而言之，这个公式根据以下内容对步棋进行评分：
- en: The mean reward of games where a given move was made
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个棋局中某一步棋所获得的平均奖励
- en: How often the move was selected
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该步棋被选择的频率
- en: 'Each node''s rating can be expressed as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点的评分可以表示如下：
- en: '![](img/cecb71cb-1964-4f40-8585-975998119ce6.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cecb71cb-1964-4f40-8585-975998119ce6.png)'
- en: 'Where:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](img/d7a90243-d2e5-4856-a8bc-7be435a50380.png): Is the mean reward for choosing
    move ![](img/1159d5da-b57a-4100-bc50-0ced1d628aa7.png) (for example, the win-rate)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/d7a90243-d2e5-4856-a8bc-7be435a50380.png)：是选择步棋 ![](img/1159d5da-b57a-4100-bc50-0ced1d628aa7.png)的平均奖励（例如，胜率）'
- en: '![](img/f01de5bf-f163-448a-9240-44f4d726173e.png): Is the number of times the
    algorithm selected move ![](img/4e52a294-85a6-468c-b216-55895ca828fc.png)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/f01de5bf-f163-448a-9240-44f4d726173e.png)：是算法选择步棋 ![](img/4e52a294-85a6-468c-b216-55895ca828fc.png)的次数'
- en: '![](img/924f7580-5cb7-4e68-9357-81c4368bbe22.png): Is the total number of moves
    made after the current state (including move ![](img/071de01b-39e6-43cb-aa23-fb2e9a87c9fe.png))'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/924f7580-5cb7-4e68-9357-81c4368bbe22.png)：是当前状态下所有已做步棋的总数（包括步棋 ![](img/071de01b-39e6-43cb-aa23-fb2e9a87c9fe.png)）'
- en: '![](img/993dec34-0edf-4931-9cdd-c418896d1933.png): Is an exploration parameter'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/993dec34-0edf-4931-9cdd-c418896d1933.png)：是一个探索参数'
- en: 'The following diagram shows an example of selecting the next node. In each
    node, the left number represents the node''s rating, and the right number represents
    the number of times the node was visited. The color of the node indicates which
    player''s turn it is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了选择下一个节点的示例。在每个节点中，左边的数字代表节点的评分，右边的数字代表该节点的访问次数。节点的颜色表示轮到哪位玩家：
- en: '![](img/722ce601-b57d-4f8d-bb83-180e5deeff89.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/722ce601-b57d-4f8d-bb83-180e5deeff89.png)'
- en: 'Figure 1: Selection in MCTS'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：MCTS中的选择
- en: In selection, the algorithm chooses the move that has the highest value for
    the preceding expression. The keen reader may notice that, while moves with a
    high mean reward, ![](img/d7a90243-d2e5-4856-a8bc-7be435a50380.png), are rated
    highly, so too are moves with fewer numbers of visits, ![](img/f01de5bf-f163-448a-9240-44f4d726173e.png). Why
    is this so? In MCTS, we not only want the algorithm to choose moves that most
    likely result in wins but also to try less-often-selected moves. This is commonly
    referred to as the balance between exploitation and exploration. If the algorithm
    solely resorted to exploitation, the resulting tree would be very narrow and ill-experienced.
    Encouraging exploration allows the algorithm to learn from a broader set of experiences
    and simulations. In the preceding example, we simply select the node with a rating
    of 7 and subsequently the node with a rating of 4.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择过程中，算法会选择对前一个表达式具有最高价值的动作。细心的读者可能会注意到，虽然高平均奖励的动作 ![](img/d7a90243-d2e5-4856-a8bc-7be435a50380.png)
    得到高度评价，但访问次数较少的动作 ![](img/f01de5bf-f163-448a-9240-44f4d726173e.png) 也同样如此。这是为什么呢？在MCTS中，我们不仅希望算法选择最有可能带来胜利的动作，还希望它尝试那些不常被选择的动作。这通常被称为开发与探索之间的平衡。如果算法仅仅依赖开发，那么结果树将会非常狭窄且经验不足。鼓励探索可以让算法从更广泛的经验和模拟中学习。在前面的例子中，我们简单地选择了评分为7的节点，然后是评分为4的节点。
- en: Expansion
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: 'We apply selection to decide moves until the algorithm can no longer apply
    UCT to rate the next set of moves. In particular, we can no longer apply UCT when
    not all of the child nodes of a given state have records (number of visits, mean
    reward). This is when the second phase of MCTS, expansion, occurs. Here, we simply
    look at all possible new moves (unvisited child nodes) of a given state and randomly
    choose one. We then update the tree to record this new child node. The following
    diagram illustrates this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用选择方法来决定动作，直到算法无法再应用UCT来评估下一组动作。特别是，当某一状态的所有子节点没有记录（访问次数、平均奖励）时，我们就无法再应用UCT。这时，MCTS的第二阶段——扩展阶段就会发生。在这个阶段，我们简单地查看给定状态下所有可能的未访问子节点，并随机选择一个。然后，我们更新树结构以记录这个新的子节点。下图说明了这一过程：
- en: '![](img/2956a245-b179-4673-918e-8d25522514a3.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2956a245-b179-4673-918e-8d25522514a3.png)'
- en: 'Figure 2: Expansion'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：扩展
- en: You may be wondering from the preceding diagram why we initialize the visit
    count as zero rather than one. The visit count of this new node as well as the
    statistics of the nodes we have traversed so far will be incremented during the
    update step, which is the final step of an MCTS iteration.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会好奇，为什么在前面的图示中，我们初始化访问次数为零，而不是一。这个新节点的访问次数以及我们已经遍历过的节点的统计数据将在更新步骤中增加，这是MCTS迭代的最后一步。
- en: Simulation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模拟
- en: 'After expansion, the rest of the game is played by randomly choosing subsequent
    moves. This is also commonly referred to as the **playout** or **rollout**. Depending
    on the game, some heuristics may be applied to choose the next move. For example,
    in DeepBlue, simulations rely on handcrafted heuristics to select the next move
    intelligently rather than randomly. This is also called **heavy rollouts**. While
    such rollouts provide more realistic games, they are often computationally expensive,
    which can slow down the learning of the MCTS tree:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展后，游戏的其余部分通过随机选择后续动作来进行。这也通常被称为**游戏展开（playout）**或**回滚（rollout）**。根据不同的游戏，可能会应用一些启发式方法来选择下一步动作。例如，在DeepBlue中，模拟依赖于手工制作的启发式方法来智能地选择下一步动作，而不是随机选择。这也被称为**重度回滚（heavy
    rollouts）**。虽然这种回滚提供了更真实的游戏体验，但它们通常计算开销较大，可能会减慢MCTS树的学习进程。
- en: '![](img/632c53ab-457e-4ac8-aaed-59ffba21fb7e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/632c53ab-457e-4ac8-aaed-59ffba21fb7e.png)'
- en: 'Figure 3: Simulation'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：模拟
- en: In our preceding toy example, we expand a node and play until the very end of
    the game (represented by the dotted line), which results in either a win or loss.
    Simulation yields a reward, which in this case is either 1 or 0.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前面的示例中，我们扩展一个节点并进行游戏，直到游戏结束（由虚线表示），最终得出胜利或失败的结果。模拟过程会产生奖励，在这个案例中，奖励为1或0。
- en: Update
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新
- en: 'Finally, the update step happens when the algorithm reaches a terminal state,
    or when either player wins or the game culminates in a draw. For each node/state
    of the board that was visited during this iteration, the algorithm updates the
    mean reward and increments the visit count of that state. This is also called **backpropagation**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，更新步骤发生在算法达到终止状态时，或者当任一玩家获胜或游戏以平局结束时。在这一轮迭代过程中，算法会更新每个访问过的节点/状态的平均奖励，并增加该状态的访问计数。这也被称为**反向传播**：
- en: '![](img/0dfe452d-2e94-4838-8ac8-8998c1683ae9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dfe452d-2e94-4838-8ac8-8998c1683ae9.png)'
- en: 'Figure 4: Update'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：更新
- en: In the preceding diagram, since we reached a terminal state that returned 1
    (a win), we increment the visit count and reward accordingly for each node along
    the path from the root node accordingly.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，由于我们到达了一个返回1（胜利）的终止状态，因此我们会相应地为每个沿路径从根节点到达的节点增加访问计数和奖励。
- en: That concludes the four steps that occur in one MCTS iteration. As the name
    Monte Carlo suggests, we conduct this search multiple times before we decide the
    next move to take. The number of iterations is configurable, and often depends
    on time/resources available. Over time, the tree learns a structure that approximates
    a perfect tree and can be used to guide agents to make decisions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是一次MCTS迭代中的四个步骤。正如蒙特卡洛方法的名字所示，我们会进行多次搜索，然后决定下一步走哪步。迭代次数是可配置的，通常取决于可用的时间或资源。随着时间的推移，树会学习出一种接近完美树的结构，进而可以用来引导智能体做出决策。
- en: AlphaGo and AlphaGo Zero, DeepMind's revolutionary Go playing agents, rely on
    MCTS to select moves. In the next section, we will explore the two algorithms
    to understand how they combine neural networks and MCTS to play Go at a superhuman
    level of proficiency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo和AlphaGo Zero，DeepMind的革命性围棋对弈智能体，依赖MCTS来选择棋步。在接下来的部分，我们将探讨这两种算法，了解它们如何将神经网络和MCTS结合起来，以超人的水平下围棋。
- en: AlphaGo
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo
- en: 'AlphaGo''s main innovation is how it combines deep learning and Monte Carlo
    tree search to play Go. The AlphaGo architecture consists of four neural networks:
    a small supervised learning policy network, a large supervised-learning policy
    network, a reinforcement learning policy network, and a value network. We train
    all four of these networks plus the MCTS tree. The following sections will cover
    each training step.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo的主要创新在于它如何将深度学习和蒙特卡洛树搜索相结合来下围棋。AlphaGo架构由四个神经网络组成：一个小型的监督学习策略网络，一个大型的监督学习策略网络，一个强化学习策略网络和一个价值网络。我们训练这四个网络以及MCTS树。接下来的章节将详细介绍每个训练步骤。
- en: Supervised learning policy networks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习策略网络
- en: The first step in training AlphaGo involves training policy networks on games
    played by two professionals (in board games such as chess and Go, it is common
    to keep records of historical games, the board state, and the moves made by each
    player at every turn). The main idea is to make AlphaGo learn and understand how
    human experts play Go. More formally, given a board state, ![](img/55d32f25-c70f-44f0-9035-4a54c2afda6c.png), and
    set of actions, ![](img/65c7bea2-8bc3-4a04-93cd-ffc0fafed904.png), we would like
    a policy network, ![](img/b2ac07b7-e049-481e-b9b8-8b7e30fbd8ab.png), to predict
    the next move the human makes. The data consists of pairs of ![](img/5dc1ab94-90ee-4e0b-85ad-3c3e9ab3ca19.png) sampled
    from over 30,000,000 historical games from the KGS Go server. The input to the
    network consists of the board state as well as metadata. AlphaGo has two supervised
    learning policy networks of varying sizes. The large network is a 13-layer convolutional
    neural network with ReLU activation functions in the hidden layers, while the
    smaller one is a single-layer softmax network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo训练的第一步涉及对两位职业选手下的围棋进行训练（在棋类游戏如国际象棋和围棋中，通常会记录历史比赛、棋盘状态和每一步棋的玩家动作）。主要思路是让AlphaGo学习并理解人类专家如何下围棋。更正式地说，给定一个棋盘状态，![](img/55d32f25-c70f-44f0-9035-4a54c2afda6c.png)，和一组动作，![](img/65c7bea2-8bc3-4a04-93cd-ffc0fafed904.png)，我们希望一个策略网络，![](img/b2ac07b7-e049-481e-b9b8-8b7e30fbd8ab.png)，预测人类的下一步棋。数据由从KGS围棋服务器上30,000,000多场历史比赛中采样得到的棋盘状态对组成。网络的输入包括棋盘状态以及元数据。AlphaGo有两个不同大小的监督学习策略网络。大型网络是一个13层的卷积神经网络，隐藏层使用ReLU激活函数，而较小的网络是一个单层的softmax网络。
- en: Why do we train two similar networks? The larger policy network initializes
    the weights of the reinforcement learning policy network, which gets further refined
    through an RL approach called **policy gradients**. The smaller network is used
    during the simulation step of MCTS. Remember, while most simulations in MCTS rely
    on the randomized selection of moves, one can also utilize light or heavy heuristics
    to have more intelligent simulations. The smaller network, which lacks the accuracy
    of the larger supervised network yet yields much faster inference, provides light
    heuristics for rollout.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们训练两个相似的网络？较大的策略网络初始化强化学习策略网络的权重，后者通过一种叫做**策略梯度**的RL方法进一步优化。较小的网络在MCTS的仿真步骤中使用。记住，虽然MCTS中的大多数仿真依赖于随机选择动作，但也可以利用轻度或重度启发式方法来进行更智能的仿真。较小的网络虽然缺乏较大监督网络的准确性，但推理速度更快，为回滚提供轻度启发式。
- en: Reinforcement learning policy networks
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习策略网络
- en: 'Once the larger supervised learning policy network is trained, we further improve
    the model by having the RL policy network play against a previous version of itself.
    The weights of the network are updated using a method called **policy gradients**,
    which is a variant of gradient descent for vanilla neural networks. Formally speaking,
    the gradient update rule for the weights of our RL policy network can be expressed
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦较大的监督学习策略网络训练完成，我们通过让RL策略网络与自己之前的版本进行对抗，进一步改进模型。网络的权重通过一种叫做**策略梯度**的方法进行更新，这是一种用于普通神经网络的梯度下降变种。从形式上来说，我们的RL策略网络的权重更新规则可以表示如下：
- en: '![](img/0dfc1d79-0fe2-45ac-9778-c5dcc3c7aeff.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dfc1d79-0fe2-45ac-9778-c5dcc3c7aeff.png)'
- en: Here, ![](img/6e728664-0539-498d-bc13-7174e4fa7568.png) are the weights of the
    RL policy network, ![](img/b804e251-7fe6-4692-ade3-371872ad9257.png), and ![](img/5597f3a0-4409-453c-a487-aac612537815.png) is
    the expected reward at timestep ![](img/1b2a4aa3-d94d-45d7-a7c5-9d8ab755638c.png).
    The reward is simply the outcome of the game, where a win results in +1 and a
    loss results in -1\. Herein lies the main difference between the supervised learning
    policy network and the reinforcement learning policy network. For the former network,
    the objective is to maximize the likelihood of choosing a particular action given
    a state, or, in other words, to simply mimic the moves of the historical games.
    Since there is no reward function involved, it does not care about the eventual
    outcome of the game.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/6e728664-0539-498d-bc13-7174e4fa7568.png) 是RL策略网络的权重，![](img/b804e251-7fe6-4692-ade3-371872ad9257.png)，和
    ![](img/5597f3a0-4409-453c-a487-aac612537815.png) 是在时间步 ![](img/1b2a4aa3-d94d-45d7-a7c5-9d8ab755638c.png)
    的预期奖励。奖励就是游戏的结果，胜利得 +1，失败得 -1。在这里，监督学习策略网络和强化学习策略网络的主要区别在于：前者的目标是最大化给定状态下选择某个特定动作的概率，换句话说，就是简单地模仿历史游戏中的动作。由于没有奖励函数，它并不关心游戏的最终结果。
- en: On the other hand, the reinforcement learning policy network incorporates the
    final outcome when updating the weights. More specifically, it is trying to maximize
    the log likelihood of the moves that contribute to higher rewards (that is, winning
    moves). This is because we are multiplying the gradient of the log-likelihood
    with the reward (either +1 or -1), which essentially determines the direction
    in which to move the weights. The weights of a poor move will be moved in the
    opposite direction, for we will likely be multiplying the gradients with -1\.
    To summarize, the network not only tries to figure out the most likely move, but
    also one that helps it win. According to DeepMind's paper, the reinforcement learning
    policy network won the vast majority (80%~85%) of its games against its supervised
    counterpart and other Go playing programs, such as Pachi.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强化学习策略网络在更新权重时考虑了最终结果。更具体地说，它尝试最大化那些有助于获得更高奖励（即获胜动作）的动作的对数似然性。这是因为我们将对数似然的梯度与奖励（+1或-1）相乘，从而决定了调整权重的方向。若某个动作不好，其权重会朝相反方向调整，因为我们可能会将梯度与-1相乘。总结来说，网络不仅试图找出最可能的动作，还试图找出能够帮助它获胜的动作。根据DeepMind的论文，强化学习策略网络在与其监督学习对手及其他围棋程序（如Pachi）对抗时，赢得了绝大多数（80%~85%）的比赛。
- en: Value network
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值网络
- en: 'The last step of the pipeline involves training a value network to evaluate
    the board state, or in other words, to determine how favorable a particular board
    state is for winning the game. Formally speaking, given a particular policy, ![](img/58974947-3e71-4976-8efc-db7dd55a5c42.png), and
    state, ![](img/73b29631-b660-499b-a22f-8f143b56c3b0.png), we would like to predict
    the expected reward, ![](img/82c5ab93-2ac7-4f69-952b-8a47b220d1d8.png). The network
    is trained by minimizing the **mean-squared error** (**MSE**) between the predicted
    value, ![](img/07eab5f4-2e49-46e9-bea2-6d0a8e36c28e.png), and the final outcome:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的最后一步涉及训练一个价值网络来评估棋盘状态，换句话说，就是确定某一特定棋盘状态对赢得游戏的有利程度。严格来说，给定特定的策略，![](img/58974947-3e71-4976-8efc-db7dd55a5c42.png)
    和状态，![](img/73b29631-b660-499b-a22f-8f143b56c3b0.png)，我们希望预测预期奖励，![](img/82c5ab93-2ac7-4f69-952b-8a47b220d1d8.png)。通过最小化**均方误差**（**MSE**）来训练网络，其中预测值，![](img/07eab5f4-2e49-46e9-bea2-6d0a8e36c28e.png)，与最终结果之间的差异：
- en: '![](img/072044d5-075f-42ad-83e2-098429dae709.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/072044d5-075f-42ad-83e2-098429dae709.png)'
- en: Where ![](img/a231bccc-4eeb-4129-9a6a-96928ad23a2a.png) are the parameters of
    the network. In practice, the network is trained on 30,000,000 state-reward pairs,
    each coming from a distinct game. The dataset is constructed in this way because
    the board states from the same game can be highly correlated, potentially leading
    to overfitting.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/a231bccc-4eeb-4129-9a6a-96928ad23a2a.png) 是网络的参数。实际上，网络是在 30,000,000
    对状态-奖励对上训练的，每一对都来自于一局独立的游戏。数据集是这样构建的，因为同一游戏中的棋盘状态可能高度相关，可能导致过拟合。
- en: Combining neural networks and MCTS
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合神经网络和蒙特卡洛树搜索（MCTS）
- en: 'In AlphaGo, the policy and value networks are combined with MCTS to provide
    a look-ahead search when selecting actions in a game. Previously, we discussed
    how MCTS keeps track of the mean reward and number of visits made to each node.
    In AlphaGo, we have a few more values to keep track of:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AlphaGo 中，策略网络和价值网络与 MCTS 相结合，在选择游戏中的行动时提供前瞻性搜索。之前，我们讨论了 MCTS 如何追踪每个节点的平均奖励和访问次数。在
    AlphaGo 中，我们还需要追踪以下几个值：
- en: '![](img/41233493-8215-4815-97a8-376962fd52f7.png): Which is the mean action
    value of choosing a particular action'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/41233493-8215-4815-97a8-376962fd52f7.png)：选择特定动作的平均行动价值'
- en: '![](img/52b25710-26ed-4cdb-bdea-c51bb2815c41.png): The probability of taking
    an action for a given board state given by the larger supervised learning policy
    network'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/52b25710-26ed-4cdb-bdea-c51bb2815c41.png)：由较大的监督学习策略网络给定的特定棋盘状态下采取某个动作的概率'
- en: '![](img/1245e894-e2e0-404d-b1fc-d123dde71296.png): The value evaluation of
    a state that is not explored yet (a leaf node)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/1245e894-e2e0-404d-b1fc-d123dde71296.png)：尚未探索的状态（叶节点）的价值评估'
- en: '![](img/656ba1c3-6860-4c72-a5e6-49fe02f998f5.png): The number of times a particular
    action was chosen given a state'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/656ba1c3-6860-4c72-a5e6-49fe02f998f5.png)：给定状态下选择特定动作的次数'
- en: 'During a single simulation of our tree search, the algorithm selects an action, ![](img/91a9e2e5-c0c5-4a2e-9762-b5287ffd8c8a.png),
    for a given state, ![](img/32013526-a1b9-4f4e-9af7-1d02f257b604.png), at a particular
    timestep, ![](img/d0e8dd3c-9f2d-4d7c-932a-d837b365612f.png), according to the
    following formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们树搜索的单次模拟过程中，算法为给定的状态，![](img/32013526-a1b9-4f4e-9af7-1d02f257b604.png)，在特定的时间步，![](img/d0e8dd3c-9f2d-4d7c-932a-d837b365612f.png)，选择一个动作，![](img/91a9e2e5-c0c5-4a2e-9762-b5287ffd8c8a.png)，根据以下公式：
- en: '![](img/be45ffe6-9f2f-4c75-b709-fd02fb21f9a2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be45ffe6-9f2f-4c75-b709-fd02fb21f9a2.png)'
- en: Where
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](img/095003e3-4162-4d5d-9e10-2dc9b2f02489.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/095003e3-4162-4d5d-9e10-2dc9b2f02489.png)'
- en: Hence ![](img/2baa0841-0bac-4d33-a16c-205f1bf23094.png) is a value that favors
    moves determined to be more likely by the larger policy network, but also supports
    exploration by penalizing those that have been visited more frequently.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此 ![](img/2baa0841-0bac-4d33-a16c-205f1bf23094.png) 是一个值，偏向于由较大的策略网络判定为更可能的走法，但也通过惩罚那些被更频繁访问的走法来支持探索。
- en: 'During expansion, when we don''t have the preceding statistics for a given
    board state and move, we use the value network and the simulation to evaluate
    the leaf node. In particular, we take a weighted sum of the expected value given
    by the value network and outcome of the rollout:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展过程中，当我们没有给定棋盘状态和棋步的前置统计信息时，我们使用价值网络和模拟来评估叶子节点。特别地，我们对价值网络给出的预期值和模拟结果进行加权求和：
- en: '![](img/13475d29-0672-4385-9fbf-0d0d84fbcafc.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13475d29-0672-4385-9fbf-0d0d84fbcafc.png)'
- en: Where ![](img/3920ccc3-88bf-431a-ba9d-4e0254796706.png) is the evaluation of
    the value network, ![](img/63d5932c-a701-4f61-a14d-d3fc26ecbe4d.png) is the eventual
    reward of the search, and ![](img/17a3ae17-897c-41d6-bbae-bc37fe27647d.png) is
    the weighting term that is often referred to as the mixing parameter. ![](img/92e980c9-584c-42e2-aeea-58255101c7af.png) is
    obtained after rollout, where the simulations are conducted using the smaller
    and faster supervised learning policy network. Having fast rollouts is important,
    especially in situations where decisions are time-boxed, hence the need for the
    smaller policy network.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/3920ccc3-88bf-431a-ba9d-4e0254796706.png) 是价值网络的评估，![](img/63d5932c-a701-4f61-a14d-d3fc26ecbe4d.png)
    是搜索的最终奖励，![](img/17a3ae17-897c-41d6-bbae-bc37fe27647d.png) 是通常称为混合参数的权重项。![](img/92e980c9-584c-42e2-aeea-58255101c7af.png)
    是在展开后获得的，其中的模拟是通过使用较小且更快速的监督学习策略网络进行的。快速展开非常重要，尤其是在决策时间有限的情况下，因此需要较小的策略网络。
- en: 'Finally, during the update step of MCTS, visit counts for each node are updated.
    Moreover, the action values are recalculated by taking the mean reward of all
    simulations that included a given node:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 MCTS 的更新步骤中，每个节点的访问计数会更新。此外，行动值通过计算所有包含给定节点的模拟的平均奖励来重新计算：
- en: '![](img/61660467-4290-470b-90f8-844776c2edc1.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61660467-4290-470b-90f8-844776c2edc1.png)'
- en: Where ![](img/7987e8fa-4eba-4e2c-a3fa-614c174658d8.png) is the total reward
    across the ![](img/3e5634d9-6a12-41bd-963e-585d0ed1d738.png) times MCTS took action ![](img/7f52dd4f-a2be-40a4-857d-53c7c1db0517.png) at
    node ![](img/58c868cd-4e72-47fb-b2bc-a4edee1333dc.png). After the MCTS search,
    the model chooses the most frequently-visited move when actually playing the game.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/7987e8fa-4eba-4e2c-a3fa-614c174658d8.png) 是在![](img/3e5634d9-6a12-41bd-963e-585d0ed1d738.png)
    轮次中 MCTS 所采取的总奖励，![](img/7f52dd4f-a2be-40a4-857d-53c7c1db0517.png) 是在节点 ![](img/58c868cd-4e72-47fb-b2bc-a4edee1333dc.png)
    采取的行动。经过 MCTS 搜索后，模型在实际对弈时选择最常访问的棋步。
- en: And that concludes a rudimentary overview of AlphaGo. While an in-depth exposition
    of the architecture and methodology is beyond the scope of this book, this hopefully
    serves as an introductory guide to what makes AlphaGo work.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 AlphaGo 的基本概述。虽然对其架构和方法论的深入讲解超出了本书的范围，但希望这能作为介绍 AlphaGo 工作原理的入门指南。
- en: AlphaGo Zero
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AlphaGo Zero
- en: We will cover AlphaGo Zero, the upgraded version of its predecessor before we
    finally get into some coding. The main features of AlphaGo Zero address some of
    the drawbacks of AlphaGo, including its dependency on a large corpus of games
    played by human experts.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始编写代码之前，我们将介绍 AlphaGo Zero，这一其前身的升级版。AlphaGo Zero 的主要特点解决了 AlphaGo 一些缺点，包括它对大量人类专家对弈数据的依赖。
- en: 'The main differences between AlphaGo Zero and AlphaGo are the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo Zero 和 AlphaGo 之间的主要区别如下：
- en: AlphaGo Zero is trained solely with self-play reinforcement learning, meaning
    it does not rely on any human-generated data or supervision that is used to train
    AlphaGo
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlphaGo Zero 完全通过自我对弈强化学习进行训练，这意味着它不依赖于任何人类生成的数据或监督，而这些通常用于训练 AlphaGo。
- en: Policy and value networks are represented as one network with two heads rather
    than two separate ones
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略和价值网络合并为一个网络，并通过两个输出头表示，而不是两个独立的网络。
- en: The input to the network is the board itself as an image, such as a 2D grid;
    the network does not rely on heuristics and instead uses the raw board state itself
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的输入是棋盘本身，作为图像输入，比如二维网格；该网络不依赖于启发式方法，而是直接使用原始的棋盘状态。
- en: In addition to finding the best move, Monte Carlo tree search is also used for
    policy iteration and evaluation; moreover, AlphaGo Zero does not conduct rollouts
    during a search
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了寻找最佳走法外，蒙特卡洛树搜索还用于策略迭代和评估；此外，AlphaGo Zero 在搜索过程中不进行展开。
- en: Training AlphaGo Zero
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 AlphaGo Zero
- en: Since we don't use human-generated data for training or supervision, how does
    AlphaGo Zero learn at all? The novel reinforcement learning algorithm developed
    by DeepMind involves using MCTS as a teacher for the neural network, which represents
    both policy and value functions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在训练或监督过程中不使用人类生成的数据，那么 AlphaGo Zero 是如何学习的呢？DeepMind 开发的这一新型强化学习算法涉及使用 MCTS
    作为神经网络的教师，而该网络同时表示策略和价值函数。
- en: 'In particular, the outputs of MCTS are 1) probabilities, ![](img/e48da246-32de-4d29-b9d0-3a65e7588e9d.png),
    for each selecting move during the simulation, and 2) the final outcome of the
    game, ![](img/897a684f-b5a8-4dbe-9014-1863ce47bf8c.png). The neural network, ![](img/79b7bae6-1046-4d6a-b961-ea90110b351d.png), takes
    in a board state, ![](img/4d6629c8-5c12-4d6d-8e30-0e1b28b860af.png), and also
    outputs a tuple of ![](img/f9317109-5594-4918-b190-83b7c3b2ee4d.png), where ![](img/878389a6-6a3f-4251-becf-9b44229edc92.png) is
    a vector of move probabilities and ![](img/a39cf6dc-f0bb-494e-9c41-3fd3fc62dd18.png) is
    the value of ![](img/c08e5d8e-5979-4d19-9180-d0a7f38267b2.png). Given these outputs,
    we want to train our network such that the network''s policy, ![](img/decbf228-08ff-4b95-bc2c-f3bab09d6567.png), moves
    closer to the policy, ![](img/68757487-064a-4362-91d5-d7a3929882b3.png), that
    is produced by MCTS, and the network''s value, ![](img/a9fcbc11-93db-43da-a80e-5e190e5f2409.png), moves
    closer to the eventual outcome, ![](img/ccdb6cfe-0bc7-4695-bb97-2767238495d6.png), of
    the search. Note that in MCTS, the algorithm does not conduct rollouts, but instead
    relies on ![](img/380539fe-4b9d-4287-8c85-a27b1c37538b.png) for expansion and
    simulating the whole game until termination. Hence by the end of MCTS, the algorithm
    improves the policy from ![](img/f482b125-f84b-4abd-9d79-f8cad3b93f82.png) to ![](img/3ceede97-6c8b-42af-a1db-a7ebe7a45499.png) and
    is able to act as a teacher for ![](img/f7031eb4-7f9a-4d2f-bd44-a3ad3715ffae.png).
    The loss function for the network consists of two parts: one is the cross-entropy
    between ![](img/27c7bb97-71b9-45a5-a51f-5b7771099f6f.png) and ![](img/eb6e827b-6d49-4227-af78-ee15d7694596.png),
    and the other is the mean-squared error between ![](img/d87448fc-eecf-4a64-b380-457b29317672.png) and ![](img/3e5d6fd5-c669-48f9-9e1d-0b427543cad5.png).
    This joint loss function looks as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，MCTS 的输出包括 1）每次在模拟过程中选择移动的概率，![](img/e48da246-32de-4d29-b9d0-3a65e7588e9d.png)，以及
    2）游戏的最终结果，![](img/897a684f-b5a8-4dbe-9014-1863ce47bf8c.png)。神经网络，![](img/79b7bae6-1046-4d6a-b961-ea90110b351d.png)，接受一个棋盘状态，![](img/4d6629c8-5c12-4d6d-8e30-0e1b28b860af.png)，并输出一个元组，![](img/f9317109-5594-4918-b190-83b7c3b2ee4d.png)，其中，![](img/878389a6-6a3f-4251-becf-9b44229edc92.png)
    是一个表示移动概率的向量，![](img/a39cf6dc-f0bb-494e-9c41-3fd3fc62dd18.png) 是![](img/c08e5d8e-5979-4d19-9180-d0a7f38267b2.png)的值。根据这些输出，我们希望训练我们的网络，使得网络的策略，![](img/decbf228-08ff-4b95-bc2c-f3bab09d6567.png)，向由
    MCTS 生成的策略，![](img/68757487-064a-4362-91d5-d7a3929882b3.png)，靠近，并且网络的值，![](img/a9fcbc11-93db-43da-a80e-5e190e5f2409.png)，向最终结果，![](img/ccdb6cfe-0bc7-4695-bb97-2767238495d6.png)，靠近。请注意，在
    MCTS 中，算法不进行滚动扩展，而是依赖于![](img/380539fe-4b9d-4287-8c85-a27b1c37538b.png) 来进行扩展，并模拟整个游戏直到结束。因此，在
    MCTS 结束时，算法将策略从![](img/f482b125-f84b-4abd-9d79-f8cad3b93f82.png)改进为![](img/3ceede97-6c8b-42af-a1db-a7ebe7a45499.png)，并能够作为![](img/f7031eb4-7f9a-4d2f-bd44-a3ad3715ffae.png)的教师。网络的损失函数由两部分组成：一部分是![](img/27c7bb97-71b9-45a5-a51f-5b7771099f6f.png)与![](img/eb6e827b-6d49-4227-af78-ee15d7694596.png)之间的交叉熵，另一部分是![](img/d87448fc-eecf-4a64-b380-457b29317672.png)与![](img/3e5d6fd5-c669-48f9-9e1d-0b427543cad5.png)之间的均方误差。这个联合损失函数如下所示：
- en: '![](img/75b49656-2540-45e0-8980-c91de70527a4.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75b49656-2540-45e0-8980-c91de70527a4.png)'
- en: Where ![](img/79027252-9b97-46a5-8e84-c198ebc1c929.png) is network parameters
    and ![](img/bb1f525f-491c-420e-85cd-3e9577cc9a9d.png) is a parameter for L2-regularization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/79027252-9b97-46a5-8e84-c198ebc1c929.png)是网络参数，![](img/bb1f525f-491c-420e-85cd-3e9577cc9a9d.png)是
    L2 正则化的参数。
- en: Comparison with AlphaGo
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 AlphaGo 的对比
- en: According to DeepMind's paper, AlphaGo Zero was able to outperform AlphaGo in
    36 hours, whereas the latter took months to train. In a head-to-head competition
    with the version of AlphaGo that defeated Lee Sedol, AlphaGo Zero won 100 games
    out of 100\. What's significant about these results is that, even without initial
    human supervision, a Go playing program can reach superhuman-level proficiency
    more efficiently and is able to discover much of the knowledge and wisdom that
    humanity spent thousands of years and millions of games cultivating.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 DeepMind 的论文，AlphaGo Zero 能在 36 小时内超越 AlphaGo，而后者则需要数月时间进行训练。在与击败李世石版本的 AlphaGo
    进行的一对一比赛中，AlphaGo Zero 赢得了 100 场比赛中的 100 场。值得注意的是，尽管没有初始的人类监督，这个围棋程序能够更加高效地达到超越人类的水平，并发现人类在数千年的时间里通过数百万局游戏培养出的大量知识和智慧。
- en: In the following sections, we will finally implement this powerful algorithm.
    Additional technical details of AlphaGo Zero will be covered as we go through
    the code.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将最终实现这个强大的算法。我们将在代码实现过程中涵盖 AlphaGo Zero 的其他技术细节。
- en: Implementing AlphaGo Zero
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 AlphaGo Zero
- en: At last, we will implement AlphaGo Zero in this section. In addition to achieving
    better performance than AlphaGo, it is in fact relatively easier to implement.
    This is because, as discussed, AlphaGo Zero only relies on `selfplay` data for
    learning, and thus relieves us from the burden of searching for large amounts
    of historical data. Moreover, we only need to implement one neural network that
    serves as both the policy and value function. The following implementation makes
    some further simplifications—for example, we assume that the Go board size is
    9 instead of 19\. This is to allow for faster training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在这一部分实现AlphaGo Zero。除了实现比AlphaGo更好的性能外，实际上它的实现相对容易一些。这是因为，如前所述，AlphaGo
    Zero仅依赖`selfplay`数据进行学习，从而减轻了我们寻找大量历史数据的负担。此外，我们只需要实现一个神经网络，它既作为策略函数，也作为价值函数。以下实现做了一些进一步的简化——例如，我们假设围棋棋盘的大小是9，而不是19，以便加速训练。
- en: 'The directory structure of our implementation looks such as the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的目录结构如下所示：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will especially pay attention to `network.py` and `mcts.py`, which contain
    the implementations for the dual network and the MCTS algorithm. Moreover, `alphagozero_agent.py`
    contains the implementation for combining the dual network and MCTS to create
    a Go playing agent.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将特别关注`network.py`和`mcts.py`，它们包含了双网络和MCTS算法的实现。此外，`alphagozero_agent.py`包含了将双网络与MCTS结合以创建围棋对弈代理的实现。
- en: Policy and value networks
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略和价值网络
- en: Let's get started with implementing the dual network, which we will call `PolicyValueNetwork`.
    First, we will create a few modules that contain configurations and constants
    that our `PolicyValueNetwork` will use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实现双网络，我们将其称为`PolicyValueNetwork`。首先，我们将创建一些模块，其中包含我们的`PolicyValueNetwork`将使用的配置和常量。
- en: preprocessing.py
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: preprocessing.py
- en: The `preprocessing.py` module mainly deals with reading from and writing to `TFRecords`
    files, which is TensorFlow's native data-representation file format. When training
    AlphaGo Zero, we store MCTS self-play results and moves. As discussed, these then
    become the ground truths from which `PolicyValueNetwork` learns. `TFRecords` provides
    a convenient way to save historical moves and results from MCTS. When reading
    these from disk, `preprocessing.py` turns `TFRecords` into `tf.train.Example`,
    an in-memory representation of data that can be directly fed into `tf.estimator.Estimator`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessing.py`模块主要处理从`TFRecords`文件的读取和写入，`TFRecords`是TensorFlow的原生数据表示文件格式。在训练AlphaGo
    Zero时，我们存储了MCTS自对弈结果和棋步。如前所述，这些数据成为`PolicyValueNetwork`学习的**真实数据**。`TFRecords`提供了一种方便的方式来保存来自MCTS的历史棋步和结果。当从磁盘读取这些数据时，`preprocessing.py`将`TFRecords`转换为`tf.train.Example`，这是一种内存中的数据表示，可以直接输入到`tf.estimator.Estimator`中。'
- en: '`tf_records` usually have filenames that end with `*.tfrecord.zz`.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf_records`通常以`*.tfrecord.zz`为文件名后缀。'
- en: 'The following function reads from a `TFRecords` file. We first turn a given
    list of `TFRecords` into `tf.data.TFRecordDataset`, an intermediate representation
    before we turn them into `tf.train.Example`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数用于从`TFRecords`文件中读取数据。我们首先将给定的`TFRecords`列表转换为`tf.data.TFRecordDataset`，这是在将其转换为`tf.train.Example`之前的中间表示：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step involves parsing this dataset so that we can feed the values
    into `PolicyValueNetwork`. There are three values we care about: the input, which
    we call either `x` or `board_state` throughout the implementation, the policy, `pi`,
    and the outcome, `z`, both of which are outputted by the MCTS algorithm:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是解析这个数据集，以便将数值输入到`PolicyValueNetwork`中。我们关心的有三个数值：输入，整个实现过程中我们称之为`x`或`board_state`，策略`pi`，以及输出结果`z`，这两个值都是由MCTS算法输出的：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding two functions are combined in the following function to construct
    the input tensors to be fed into the network:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的两个函数将在以下函数中结合，构造要输入网络的输入张量：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, the following functions are used to write self-play results to disk:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下函数用于将自对弈结果写入磁盘：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Some of these functions will be used later when we generate training data from
    self-play results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数中的一些将在后续生成自对弈结果的训练数据时使用。
- en: features.py
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: features.py
- en: 'This module contains helper code for turning Go board representations into
    proper TensorFlow tensors, which can be provided to `PolicyValueNetwork`. The
    main function, `extract_features`, takes `board_state`, which is our representation
    of a Go board, and turns it into a tensor of the `[batch_size, N, N, 17]` shape,
    where `N` is the shape of the board (which is by default `9`), and `17` is the
    number of feature channels, representing the past moves as well as the color to
    play:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块包含将围棋棋盘表示转化为适当TensorFlow张量的辅助代码，这些张量可以提供给`PolicyValueNetwork`。主要功能`extract_features`接收`board_state`（即围棋棋盘的表示），并将其转换为形状为`[batch_size,
    N, N, 17]`的张量，其中`N`是棋盘的形状（默认为`9`），`17`是特征通道的数量，表示过去的着棋和当前要下的颜色：
- en: '[PRE5]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `extract_features` function will be used by both the `preprocessing.py`
    and `network.py` modules to construct the feature tensors to be either written
    to a `TFRecord` file or fed into a neural network.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_features`函数将被`preprocessing.py`和`network.py`模块使用，以构建特征张量，这些张量要么写入`TFRecord`文件，要么输入到神经网络中。'
- en: network.py
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: network.py
- en: 'This file contains our implementation of `PolicyValueNetwork`. In short, we
    construct a `tf.estimator.Estimator` that is trained using board states, policies,
    and self-play outcomes produced by MCTS self-play. The network has two heads:
    one acting as a value function, and the other acting as a policy network.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本文件包含我们对`PolicyValueNetwork`的实现。简而言之，我们构建一个`tf.estimator.Estimator`，该估算器使用围棋棋盘状态、策略和通过MCTS自对弈生成的自对弈结果进行训练。该网络有两个头：一个作为价值函数，另一个作为策略网络。
- en: 'First, we define some layers that will be used by `PolicyValueNetwork`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一些将被`PolicyValueNetwork`使用的层：
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we have a function that is used to create `tf.estimator.Estimator`. While
    TensorFlow provides several prebuilt estimators, such as `tf.estimator.DNNClassifier`,
    our architecture is rather unique, which is why we need to build our own `Estimator`.
    This can be done by creating `tf.estimator.EstimatorSpec`, a skeleton class where
    we can define things such as the output tensors, network architecture, the loss
    functions, and the evaluation metrics:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有一个函数，用于创建`tf.estimator.Estimator`。虽然TensorFlow提供了几个预构建的估算器，如`tf.estimator.DNNClassifier`，但我们的架构相当独特，这就是我们需要构建自己的`Estimator`的原因。这可以通过创建`tf.estimator.EstimatorSpec`来完成，它是一个骨架类，我们可以在其中定义输出张量、网络架构、损失函数和评估度量等：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our `generate_network_specifications` function takes several input:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`generate_network_specifications`函数接收多个输入：
- en: '`features`: The tensor representation of the Go board (with the `[batch_size,
    9, 9, 17]` shape)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`features`：围棋棋盘的张量表示（形状为`[batch_size, 9, 9, 17]`）'
- en: '`labels`: Our `pi` and `z` tensors'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`：我们的`pi`和`z`张量'
- en: '`mode`: Here, we can specify whether our network is being instantiated in train
    or test mode'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`：在这里，我们可以指定我们的网络是处于训练模式还是测试模式'
- en: '`params`: Additional parameters to specify the network structure (for example,
    convolutional filter size)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`params`：指定网络结构的附加参数（例如，卷积滤波器大小）'
- en: 'We then implement the shared portion of the network, the policy output head,
    the value output head, and then the loss functions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实现网络的共享部分，策略输出头、价值输出头以及损失函数：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then specify the optimization algorithm. Here, we use `tf.train.MomentumOptimizer`.
    We also adjust the learning rate during training; because we can''t directly alter
    the learning rate once we create `Estimator`, we turn the learning rate update
    into a TensorFlow operation as well. We also log several metrics to TensorBoard:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们指定优化算法。这里，我们使用`tf.train.MomentumOptimizer`。我们还会在训练过程中调整学习率；由于一旦创建了`Estimator`我们不能直接更改学习率，因此我们将学习率更新转化为TensorFlow操作。我们还将多个度量记录到TensorBoard中：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we create a `tf.estmator.EstimatorSpec` object and return it. There
    are several parameters we need to specify when creating one:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`tf.estimator.EstimatorSpec`对象并返回。创建时我们需要指定几个参数：
- en: '`mode`: Train or test, as specified earlier.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mode`：训练模式或测试模式，如前所述。'
- en: '`predictions`: A dictionary that maps a string (name) to the output operation
    of the network. Note that we can specify multiple output operations.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predictions`：一个字典，将字符串（名称）映射到网络的输出操作。注意，我们可以指定多个输出操作。'
- en: '`loss`: The loss function operation.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：损失函数操作。'
- en: '`train_op`: The optimization operation.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_op`：优化操作。'
- en: '`eval_metrics_op`: Operations that are run to store several metrics, such as
    loss, accuracy, and variable weight values.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_metrics_op`：运行以存储多个度量的操作，如损失、准确率和变量权重值。'
- en: 'For the `predictions` argument, we provide outputs of both the policy and value
    networks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`predictions`参数，我们提供政策网络和值网络的输出：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the very first step of training AlphaGo Zero, we must initialize a model
    with random weights. The following function implements this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练AlphaGo Zero的第一步中，我们必须用随机权重初始化模型。以下函数实现了这一点：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We use the following function to create the `tf.estimator.Estimator` object
    based on a given set of parameters:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下函数根据给定的一组参数创建`tf.estimator.Estimator`对象：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`tf.estimator.Estimator` expects a function that provides `tf.estimator.EstimatorSpec`,
    which is our `generate_network_specifications` function. Here, `estimator_dir`
    refers to a directory in which our network stores checkpoints. By providing this
    parameter, our `tf.estimator.Estimator` object can load weights from a previous
    iteration of training.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.estimator.Estimator`期望一个提供`tf.estimator.EstimatorSpec`的函数，这就是我们的`generate_network_specifications`函数。这里，`estimator_dir`指的是存储我们网络检查点的目录。通过提供此参数，我们的`tf.estimator.Estimator`对象可以加载之前训练迭代的权重。'
- en: 'We also implement functions for training and validating a model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了用于训练和验证模型的函数：
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `tf.estimator.Estimator.train` function expects a function that provides
    the training data in batches (`input_fn`). `input_data` uses our `get_input_tensors`
    function from the `preprocessing.py` module to parse `TFRecords` data and turn
    them into input tensors. The `tf.estimator.Estimator.evaluate` function expects
    the same input function.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.estimator.Estimator.train`函数期望一个提供训练数据批次的函数（`input_fn`）。`input_data`使用我们在`preprocessing.py`模块中的`get_input_tensors`函数解析`TFRecords`数据并将其转化为输入张量。`tf.estimator.Estimator.evaluate`函数也期望相同的输入函数。'
- en: 'We finally encapsulate our estimator into our `PolicyValueNetwork`. This class uses
    the path to a network (`model_path`) and loads its weights. It uses the network
    to predict the value and most probable next moves of a given board state:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将估算器封装到我们的`PolicyValueNetwork`中。这个类使用网络的路径（`model_path`）并加载其权重。它使用该网络来预测给定棋盘状态的价值和最可能的下一步棋：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `model_path` argument passed to the constructor is the directory of a particular
    version of the model. When this is `None`, we initialize random weights. The following
    functions are used to predict the probabilities of the next action and the value
    of a given board state:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给构造函数的`model_path`参数是模型特定版本的目录。当此参数为`None`时，我们初始化随机权重。以下函数用于预测下一步动作的概率和给定棋盘状态的价值：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Do check the GitHub repository for the full implementation of the module.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查GitHub仓库，以获取该模块的完整实现。
- en: Monte Carlo tree search
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛树搜索（Monte Carlo tree search）
- en: The second component of our AlphaGo Zero agent is the MCTS algorithm. In our `mcts.py`
    module, we implement an `MCTreeSearchNode` class, which represents each node in
    an MCTS tree during a search. This is then used by the agent implemented in `alphagozero_agent.py`
    to perform MCTS using `PolicyValueNetwork`, which we implemented just now.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的AlphaGo Zero代理的第二个组件是MCTS算法。在我们的`mcts.py`模块中，我们实现了一个`MCTreeSearchNode`类，该类表示MCTS树中每个节点在搜索过程中的状态。然后，该类被`alphagozero_agent.py`中实现的代理使用，利用我们刚才实现的`PolicyValueNetwork`来执行MCTS。
- en: mcts.py
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mcts.py
- en: '`mcts.py` contains our implementation of Monte Carlo tree search. Our first
    class is `RootNode`, which is meant to represent the root node of the MCTS tree
    at the start of a simulation. By definition, the root node does not have a parent.
    Having a separate class for the root node is not absolutely necessary, but it
    does keep the code cleaner:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`mcts.py`包含了我们对蒙特卡洛树搜索的实现。我们的第一个类是`RootNode`，它用于表示模拟开始时MCTS树的根节点。根据定义，根节点没有父节点。为根节点创建一个单独的类并非绝对必要，但这样可以使代码更清晰：'
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we implement the `MCTreeSearchNode` class. This class has several attributes,
    the most important ones being the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现`MCTreeSearchNode`类。该类具有多个属性，其中最重要的几个如下：
- en: '`parent_node`: The parent node'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parent_node`: 父节点'
- en: '`previous_move`: The previous move that led to this node''s board state'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`previous_move`: 导致此节点棋盘状态的上一步棋'
- en: '`board_state`: The current board state'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`board_state`: 当前棋盘状态'
- en: '`is_visited`: Whether the leaves (child nodes) are expanded or not; this is
    `False` when the node is initialized'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_visited`: 是否展开了叶子（子节点）；当节点初始化时，这个值为`False`。'
- en: '`child_visit_counts`: A `numpy.ndarray` representing the visit counts of each
    child node'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`child_visit_counts`: 一个`numpy.ndarray`，表示每个子节点的访问次数'
- en: '`child_cumulative_rewards`: A `numpy.ndarray` representing the cumulative reward
    of each child node'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`child_cumulative_rewards`: 一个`numpy.ndarray`，表示每个子节点的累计奖励'
- en: '`children_moves`: A dictionary of children moves'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`children_moves`: 子节点走法的字典'
- en: We also have parameters such as `loss_counter`, `original_prior`, and `child_prior`.
    These are related to advanced MCTS techniques that AlphaGo Zero implements, such
    as paralleling the search process as well as adding noise to the search. For the
    sake of brevity, we won't cover these techniques, so you can ignore them for now.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一些参数，比如 `loss_counter`、`original_prior` 和 `child_prior`。这些与 AlphaGo Zero
    实现的高级 MCTS 技术相关，例如并行搜索过程以及向搜索中加入噪声。为了简洁起见，我们不会详细讨论这些技术，因此现在可以忽略它们。
- en: 'Here''s the `__init__` function of `MCTreeSearchNode`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `MCTreeSearchNode` 类的 `__init__` 函数：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Each node keeps track of the mean reward and action value of every child node.
    We set these as properties:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点会跟踪每个子节点的平均奖励和动作值。我们将这些设置为属性：
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And of course, we keep track of the action value, visit count, and cumulative
    reward of the node itself. Remember, `child_mean_rewards` is the mean reward,
    `child_visit_counts` is the number of times a child node was visited, and `child_cumulative_rewards` is
    the total reward of a node. We implement getters and setters for each attribute
    by adding the `@property` and `@*.setter` decorators:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们还会跟踪节点自身的动作值、访问次数和累积奖励。请记住，`child_mean_rewards` 是平均奖励，`child_visit_counts`
    是子节点被访问的次数，`child_cumulative_rewards` 是节点的总奖励。我们通过添加 `@property` 和 `@*.setter`
    装饰器为每个属性实现 getter 和 setter 方法：
- en: '[PRE19]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'During the selection step of MCTS, the algorithm chooses the child node with
    the greatest action value. This can be easily done by calling `np.argmax` on the
    matrix of child action scores:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MCTS 的选择步骤中，算法会选择具有最大动作值的子节点。这可以通过对子节点动作得分矩阵调用 `np.argmax` 来轻松完成：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As discussed in our section about AlphaGo Zero, `PolicyValueNetwork` is used
    to conduct simulations in an MCTS iteration. Again, the output of the network
    are the probabilities and the predicted value of the node, which we then reflect
    in the MCTS tree itself. In particular, the predicted value is propagated throughout
    the tree via the `back_propagate_result` function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在讨论 AlphaGo Zero 部分中提到的，`PolicyValueNetwork` 用于在 MCTS 迭代中进行模拟。同样，网络的输出是节点的概率和预测值，然后我们将其反映在
    MCTS 树中。特别地，预测值通过 `back_propagate_result` 函数在树中传播：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Refer to the GitHub repository for a full implementation of our `MCTreeSearchNode`
    class and its functions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 GitHub 仓库，查看我们 `MCTreeSearchNode` 类及其函数的完整实现。
- en: Combining PolicyValueNetwork and MCTS
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合 PolicyValueNetwork 和 MCTS
- en: We combine our `PolicyValueNetwork` and MCTS implementations in `alphagozero_agent.py`.
    This module implements `AlphaGoZeroAgent`, which is the main AlphaGo Zero that
    conducts MCTS search and inference using `PolicyValueNetwork` to play games.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `PolicyValueNetwork` 和 MCTS 实现结合在 `alphagozero_agent.py` 中。该模块实现了 `AlphaGoZeroAgent`，它是主要的
    AlphaGo Zero 代理，使用 `PolicyValueNetwork` 进行 MCTS 搜索和推理，以进行游戏。
- en: alphagozero_agent.py
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: alphagozero_agent.py
- en: 'Finally, we implement the agent that acts as the interface between the Go games
    and the algorithms. The main class we will implement is called `AlphaGoZeroAgent`.
    Again, this class combines `PolicyValueNetwork` with our MCTS module, as is done
    in AlphaGo Zero, to select moves and simulate games. Note that any missing modules
    (for example, `go.py`, which implements the game of Go itself) can be found in
    the main GitHub repository:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现了一个代理，作为围棋游戏和算法之间的接口。我们将实现的主要类是 `AlphaGoZeroAgent`。同样，这个类将 `PolicyValueNetwork`
    与我们的 MCTS 模块结合，正如 AlphaGo Zero 中所做的那样，用于选择走法并模拟游戏。请注意，任何缺失的模块（例如，`go.py`，它实现了围棋本身）可以在主
    GitHub 仓库中找到：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We start a Go game by initializing our agent and the game itself. This is done
    via the `initialize_game` method, which initializes `MCTreeSearchNode` and buffers
    that keep track of move probabilities and action values outputted by the network:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过初始化我们的代理和游戏本身来开始围棋游戏。这是通过 `initialize_game` 方法完成的，该方法初始化了 `MCTreeSearchNode`
    和用于跟踪网络输出的走法概率和动作值的缓冲区：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In each turn, our agent conducts MCTS and picks a move using the `select_move`
    function. Notice that we allow for some exploration in the early stages of the
    game by selecting a random node.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一回合中，我们的代理会进行 MCTS 并使用 `select_move` 函数选择一个走法。注意，在游戏的早期阶段，我们允许一定的探索，通过选择一个随机节点来进行。
- en: 'The `play_move(coordinates)` method takes in a coordinate returned by `select_move`
    and updates the MCTS tree and board states:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`play_move(coordinates)` 方法接受由 `select_move` 返回的坐标，并更新 MCTS 树和棋盘状态：'
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'These functions are encapsulated in the `search_tree` method, which conducts
    an iteration of MCTS using the network to select the next move:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数被封装在`search_tree`方法中，该方法使用网络进行MCTS迭代，以选择下一步棋：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Notice that once we have leaf nodes (where we can no longer select a node based
    on visit count), we use the `PolicyValueNetwork.predict_on_multiple_board_states(board_states)`
    function to output the next move probabilities and value of each leaf node. This `AlphaGoZeroAgent` is
    then used for either playing against another network or against itself for self-play.
    We implement separate functions for each. For `play_match`, we first start by
    initializing an agent each for black and white pieces:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一旦我们拥有叶子节点（在这些节点上无法根据访问次数选择节点），我们使用`PolicyValueNetwork.predict_on_multiple_board_states(board_states)`函数输出每个叶子节点的下一步概率和价值。然后，使用这个`AlphaGoZeroAgent`来进行与另一个网络的对弈或与自身的自对弈。我们为每种情况实现了独立的函数。对于`play_match`，我们首先为黑白棋子各初始化一个代理：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'During the game, we keep track of the number of moves made, which also informs
    us which agent''s turn it is. During each agent''s turn, we use MCTS and the network
    to choose the next move:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏中，我们跟踪每一步的着棋数量，这也帮助我们判断当前轮到哪个代理。每当代理轮到时，我们使用MCTS和网络来选择下一步棋：
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once the tree search is done, we see whether the agent has resigned or the
    game has ended by other means. If so, we write the results and end the game itself:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦树搜索完成，我们查看代理是否已投降或游戏是否已通过其他方式结束。如果是，我们记录结果并结束游戏：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `make_sgf` method writes the outcome of the game in a format that is commonly
    used in other Go AIs and computer programs. In other words, the output of this
    module are compatible with other Go software! Although we won't delve into the
    technicalities, this would help you create a Go playing bot that can play other
    agents and even human players.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_sgf`方法将游戏结果写入一种在其他围棋AI和计算机程序中常用的格式。换句话说，这个模块的输出与其他围棋软件兼容！虽然我们不会深入技术细节，但这将帮助您创建一个可以与其他代理甚至人类玩家对弈的围棋机器人。'
- en: '**SGF** stands for **Smart Game Format**, and is a popular way of storing the
    results of board games such as Go. You can find more information here: [https://senseis.xmp.net/?SmartGameFormat](https://senseis.xmp.net/?SmartGameFormat).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**SGF**代表**智能棋局格式**，是一种流行的存储围棋等棋类游戏结果的格式。您可以在此了解更多信息：[https://senseis.xmp.net/?SmartGameFormat](https://senseis.xmp.net/?SmartGameFormat)。'
- en: The `play_against_self()` is used during the self-play simulations of training,
    while `play_match()` is used to evaluate the latest model against an earlier version
    of the model. Again, for a full implementation of the module, please refer to
    the codebase.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`play_against_self()`用于训练中的自对弈模拟，而`play_match()`则用于将最新模型与早期版本进行对比评估。同样，关于模块的完整实现，请参考代码库。'
- en: Putting everything together
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将一切整合在一起
- en: 'Now that we have implemented the two main components of AlphaGo Zero—the `PolicyValueNetwork`
    and the MCTS algorithm—we can build the controller that handles training. At the
    very beginning of the training procedure, we initialize a model with random weights.
    Next, we generate 100 self-play games. Five percent of those games and their results
    are held out for validation. The rest are kept for training the network. After
    the first initialization and self-play iteration, we essentially loop through
    the following steps:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了AlphaGo Zero的两个主要组件——`PolicyValueNetwork`和MCTS算法——我们可以构建处理训练的控制器。在训练过程的最开始，我们用随机权重初始化一个模型。接下来，我们生成100个自对弈游戏。其中5%的游戏及其结果会用于验证，其余的则用于训练网络。在首次初始化和自对弈迭代之后，我们基本上会循环执行以下步骤：
- en: Generate self-play data
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成自对弈数据
- en: Collate self-play data to create `TFRecords`
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整理自对弈数据以创建`TFRecords`
- en: Train network using collated self-play data
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用整理后的自对弈数据训练网络
- en: Validate on `holdout` dataset
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`holdout`数据集上进行验证
- en: After every step 3, the resulting model is stored in a directory as the latest
    version. The training procedure and logic are handled by `controller.py`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 每执行完第3步后，结果模型会存储在目录中，作为最新版本。训练过程和逻辑由`controller.py`处理。
- en: controller.py
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: controller.py
- en: 'First, we start with some import statements and helper functions that help
    us check directory paths and find the latest model version:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从一些导入语句和帮助函数开始，这些帮助函数帮助我们检查目录路径并找到最新的模型版本：
- en: '[PRE29]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The first step of every training run is to initialize a random model. Note
    that we store model definitions and weights in the `PATHS.MODELS_DIR` directory,
    while checkpoint results outputted by the estimator object are stored in `PATHS.ESTIMATOR_WORKING_DIR`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 每次训练运行的第一步是初始化一个随机模型。请注意，我们将模型定义和权重存储在`PATHS.MODELS_DIR`目录中，而由估算器对象输出的检查点结果则存储在`PATHS.ESTIMATOR_WORKING_DIR`：
- en: '[PRE30]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We next implement the function for executing self-play simulations. As mentioned
    earlier, the output of a self-play consist of each board state and the associated
    moves and game outcomes produced by the MCTS algorithm. Most output are stored
    in `PATHS.SELFPLAY_DIR`, while some are stored in `PATHS.HOLDOUT_DIR` for validation.
    Self-play involves initializing one `AlphaGoZeroAgent` and having it play against
    itself. This is where we use the `play_against_self` function that we implemented
    in `alphagozero_agent.py`. In our implementation, we conduct self-play games according
    to the `GLOBAL_PARAMETER_STORE.NUM_SELFPLAY_GAMES` parameter specified. More self-play
    games allow our neural network to learn from more experience, but do bear in mind
    that the training time increases accordingly:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现执行自对弈模拟的函数。如前所述，自对弈的输出包括每个棋盘状态及由MCTS算法生成的相关棋步和游戏结果。大多数输出存储在`PATHS.SELFPLAY_DIR`，而一些存储在`PATHS.HOLDOUT_DIR`以供验证。自对弈涉及初始化一个`AlphaGoZeroAgent`，并让它与自己对弈。这时我们使用了在`alphagozero_agent.py`中实现的`play_against_self`函数。在我们的实现中，我们根据`GLOBAL_PARAMETER_STORE.NUM_SELFPLAY_GAMES`参数执行自对弈游戏。更多的自对弈游戏能让我们的神经网络从更多经验中学习，但请记住，训练时间也会相应增加：
- en: '[PRE31]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'During self-play, we instantiate an agent with weights of a previously-generated
    model and make it play against itself for a number of games defined by `GLOBAL_PARAMETER_STORE.NUM_SELFPLAY_GAMES`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在自对弈过程中，我们实例化一个带有之前生成的模型权重的代理，并让它与自己对弈，游戏的数量由`GLOBAL_PARAMETER_STORE.NUM_SELFPLAY_GAMES`定义：
- en: '[PRE32]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After the agent plays against itself, we store the moves it has generated as
    game data, which we use to train our policy and value networks:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与自己对弈后，我们将其生成的棋步存储为游戏数据，用来训练我们的策略网络和价值网络：
- en: '[PRE33]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Notice that we reserve a percentage of the games played as the validation set.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们保留了一部分对弈作为验证集。
- en: 'After generating self-play data, we expect roughly five percent of the self-play
    games to be in the `holdout` directory, to be used in validation. The majority
    of self-play data is used to train the neural network. We add another step, called
    **aggregate**, which takes the latest model version and its self-play data to
    construct `TFRecords` with the format that our neural network specifies. This
    is where we use the functions we implemented in `preprocessing.py`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成自对弈数据后，我们预计大约5%的自对弈游戏会存储在`holdout`目录中，用于验证。大多数自对弈数据用于训练神经网络。我们添加了另一个步骤，叫做**aggregate**，它将最新的模型版本及其自对弈数据用于构建`TFRecords`，这是我们神经网络所要求的格式。这里我们使用了在`preprocessing.py`中实现的函数。
- en: '[PRE34]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After we generate the training data, we train a new version of the neural network.
    We search for the latest version of the model, load an estimator using the weights
    of the latest version, and execute another iteration of training:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成训练数据后，我们训练一个新的神经网络版本。我们搜索最新版本的模型，加载使用最新版本权重的估算器，并执行另一次训练迭代：
- en: '[PRE35]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, after every training iteration, we would like to validate the model
    with the `holdout` dataset. When enough data is available, we take the `holdout`
    data from the last five versions:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每次训练迭代后，我们希望用`holdout`数据集验证模型。当有足够的数据时，我们会取最后五个版本的`holdout`数据：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Lastly, we implement the `evaluate` function, which has one model play multiple
    games against another:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现了`evaluate`函数，该函数让一个模型与另一个模型进行多场对弈：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `evaluate` method takes two parameters, `black_model` and `white_model`,
    where each argument refers to the path of the agent used to play a game. We use `black_model`
    and `white_model` to instantiate two `PolicyValueNetworks`. Typically, we want
    to evaluate the latest model version, which would play as either black or white.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate`方法接受两个参数，`black_model`和`white_model`，每个参数都指向用于对弈的代理路径。我们使用`black_model`和`white_model`来实例化两个`PolicyValueNetworks`。通常，我们希望评估最新的模型版本，它会作为黑方或白方进行对弈。'
- en: train.py
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: train.py
- en: 'Finally, `train.py` is where all the functions we implemented in the controller
    are called and coordinated. More specifically, we execute each step as `subprocess`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`train.py`是我们在控制器中实现的所有函数的调用和协调的地方。更具体地说，我们通过`subprocess`执行每一步：
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Assuming that no model has been trained yet, we initialize a model with random
    weights and make it play against itself to generate some data for our policy and
    value networks. After rewards, we repeat the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 假设尚未训练任何模型，我们使用随机权重初始化一个模型，并让它与自己对弈，从而为我们的策略网络和价值网络生成一些数据。奖励后，我们重复以下步骤：
- en: Aggregate data self-play data
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汇总数据自我对弈数据
- en: Train networks
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络
- en: Make the agent play against itself
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让代理与自己对弈
- en: Validate on validation data
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证数据上进行验证
- en: 'This is implemented as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 实现方法如下：
- en: '[PRE39]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Finally, since this is the main module, we add the following at the end of
    the file:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于这是主模块，我们在文件末尾添加以下内容：
- en: '[PRE40]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: And at long last, we're done!
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 终于，我们完成了！
- en: 'To run the training of AlphaGo Zero, all you need to do is call this command:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行AlphaGo Zero的训练，你所需要做的就是调用这个命令：
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: If everything has been implemented correctly, you should start to see the model
    train. However, the reader is to be warned that training will take a long, long
    time. To put things into perspective, DeepMind used 64 GPU workers and 19 CPU
    servers to train AlphaGo Zero for 40 days. If you wish to see your model attain
    a high level of proficiency, expect to wait a long time.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切实现正确，你应该开始看到模型训练的过程。然而，读者需要注意，训练将需要很长的时间。为了让你有个大致的概念，DeepMind使用了64个GPU工作节点和19个CPU服务器，花费了40天时间训练AlphaGo
    Zero。如果你希望看到你的模型达到高水平的熟练度，预计需要等待很长时间。
- en: Note that training AlphaGo Zero takes a very long time. Do not expect the model
    to reach professional-level proficiency any time soon!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练AlphaGo Zero需要非常长的时间。不要期望模型很快达到职业级水平！
- en: 'You should be able to see output that looks such as the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到如下所示的输出：
- en: '[PRE42]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You will also be able to see the board state as the agent plays against itself
    or against other agents:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将能够看到棋盘状态，当代理与自己或其他代理对弈时：
- en: '[PRE43]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'If you want to play one model against another, you can run the following command
    (assuming that the models are stored in `models/`):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让一个模型与另一个模型对弈，可以运行以下命令（假设模型存储在`models/`目录下）：
- en: '[PRE44]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we studied reinforcement learning algorithms for one of the
    most complex and difficult games in the world, Go. In particular, we explored
    Monte Carlo tree search, a popular algorithm that learns the best moves over time.
    In AlphaGo, we observed how MCTS can be combined with deep neural networks to
    make learning more efficient and powerful. Then we investigated how AlphaGo Zero
    revolutionized Go agents by learning solely and entirely from self-play experience
    while outperforming all existing Go software and players. We then implemented
    this algorithm from scratch.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们研究了强化学习算法，专门用于世界上最复杂、最困难的游戏之一——围棋。特别是，我们探索了蒙特卡洛树搜索（MCTS），一种流行的算法，它通过时间积累学习最佳棋步。在AlphaGo中，我们观察到MCTS如何与深度神经网络结合，使学习变得更加高效和强大。然后，我们研究了AlphaGo
    Zero如何通过完全依赖自我对弈经验而彻底改变围棋代理，且超越了所有现有的围棋软件和玩家。接着，我们从零开始实现了这个算法。
- en: We also implemented AlphaGo Zero, which is the lighter version of AlphaGo since
    it does not depend on human game data. However, as noted, AlphaGo Zero requires
    enormous amounts of computational resources. Moreover, as you may have noticed,
    AlphaGo Zero depends on a myriad of hyperparameters, all of which require fine-tuning.
    In short, training AlphaGo Zero fully is a prohibitive task. We don't expect the
    reader to implement a state-of-the-art Go agent; rather, we hope that through
    this chapter, the reader has a better understanding of how Go playing deep reinforcement
    learning algorithms work. A firmer comprehension of these techniques and algorithms
    is already a valuable takeaway and outcome from this chapter. But of course, we
    encourage the reader to continue their exploration on this topic and build an
    even better version of AlphaGo Zero.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了AlphaGo Zero，它是AlphaGo的简化版，因为它不依赖于人类的游戏数据。然而，如前所述，AlphaGo Zero需要大量的计算资源。此外，正如你可能已经注意到的，AlphaGo
    Zero依赖于众多超参数，所有这些都需要进行精细调优。简而言之，完全训练AlphaGo Zero是一项极具挑战的任务。我们并不期望读者实现最先进的围棋代理；相反，我们希望通过本章，读者能够更好地理解围棋深度强化学习算法的工作原理。对这些技术和算法的更深理解，已经是本章一个有价值的收获和成果。当然，我们鼓励读者继续探索这一主题，并构建出一个更好的AlphaGo
    Zero版本。
- en: 'For more in-depth information and resources on the topics we covered in this
    chapter, please refer to the following links:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取更多关于我们在本章中涉及主题的深入信息和资源，请参考以下链接：
- en: '**AlphaGo home page**: [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/%E2%80%8B)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo 主页**: [https://deepmind.com/research/alphago/](https://deepmind.com/research/alphago/%E2%80%8B)'
- en: '**AlphaGo paper**: [https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo 论文**: [https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)'
- en: '**AlphaGo Zero paper**: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo Zero 论文**: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)'
- en: '**AlphaGo Zero blog post by DeepMind**: [https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DeepMind 发布的 AlphaGo Zero 博客文章**: [https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)'
- en: '**A survey of MCTS methods**: [http://mcts.ai/pubs/mcts-survey-master.pdf](http://mcts.ai/pubs/mcts-survey-master.pdf)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MCTS 方法调查**: [http://mcts.ai/pubs/mcts-survey-master.pdf](http://mcts.ai/pubs/mcts-survey-master.pdf)'
- en: Now that computers have surpassed human performance in board games, one may
    ask, What's next? What are the implications of these results? There remains much
    to be done; Go, which has complete information and is played turn by turn, is
    still considered simple compared to many real-life situations. One can imagine
    that the problem of self-driving cars poses a more difficult challenge given the
    lack of complete information and a larger number of variables. Nevertheless, AlphaGo
    and AlphaGo Zero have provided a crucial step toward achieving these tasks, and
    one can surely be excited about further developments in this field.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在计算机在棋盘游戏中超越了人类表现，人们可能会问，接下来是什么？这些结果有什么影响？仍然有很多工作要做；围棋作为一个完全信息且逐轮进行的游戏，与许多现实生活中的情况相比仍然被认为是简单的。可以想象，自动驾驶汽车的问题由于信息不完全和更多的变量而面临更大的挑战。尽管如此，AlphaGo
    和 AlphaGo Zero 已经迈出了实现这些任务的关键一步，人们对这一领域的进一步发展肯定是兴奋的。
- en: References
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche,
    G., ... and Dieleman, S. (2016). M*astering the game of Go with deep neural networks
    and tree search*. Nature, 529(7587), 484.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche,
    G., ... 和 Dieleman, S. (2016). *使用深度神经网络和树搜索掌握围棋*. 自然, 529(7587), 484.
- en: Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
    A., ... and Chen, Y. (2017). *Mastering the game of Go without human knowledge*. Nature, 550(7676),
    354.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
    A., ... 和 Chen, Y. (2017). *不借助人类知识掌握围棋*. 自然, 550(7676), 354.
- en: Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen,
    P., ... and Colton, S. (2012). *A survey of Monte Carlo tree search methods*. IEEE
    Transactions on Computational Intelligence and AI in games, 4(1), 1-43.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Browne, C. B., Powley, E., Whitehouse, D., Lucas, S. M., Cowling, P. I., Rohlfshagen,
    P., ... 和 Colton, S. (2012). *蒙特卡洛树搜索方法调查*. IEEE 计算智能与AI在游戏中的应用, 4(1), 1-43.
