- en: Video and Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频与递归神经网络
- en: So far in this book, we have only considered still images. However, in this
    chapter, we will introduce the techniques that are applied to video analysis.
    From self-driving cars to video streaming websites, computer vision techniques
    have been developed to enable sequences of images to be processed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书只讨论了静态图像。然而，在这一章中，我们将介绍应用于视频分析的技术。从自动驾驶汽车到视频流网站，计算机视觉技术已经发展出来，用以处理图像序列。
- en: We will introduce a new type of neural network—**recurrent neural networks**
    (**RNNs**), which are designed specifically for sequential inputs such as video.
    As a practical application, we will combine them with **convolutional neural networks**
    (**CNNs**) to detect actions included in short video clips.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍一种新的神经网络类型——**递归神经网络**（**RNNs**），它们专门为处理视频等序列输入而设计。作为一个实际应用，我们将它们与**卷积神经网络**（**CNNs**）结合，用来检测短视频片段中的动作。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: Introduction to RNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络简介
- en: Inner workings of long short-term memory networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短时记忆网络的内部工作原理
- en: Applications of computer vision models to videos
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉模型在视频中的应用
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Commented code in the form of Jupyter notebooks is available in this book's
    GitHub repository at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的 GitHub 仓库中提供了以 Jupyter notebooks 形式的注释代码，网址为[https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08)。
- en: Introducing RNNs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 RNNs
- en: RNNs are a type of neural network that are suited for *sequential* (or *recurrent*)
    data. Examples of sequential data include sentences (sequences of words), time
    series (sequences of stock prices, for instance), or videos (sequences of frames).
    They qualify as recurrent data as each time step is related to the previous ones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs 是一种适用于*序列*（或*递归*）数据的神经网络。序列数据的例子包括句子（词语序列）、时间序列（例如股票价格序列）或视频（帧序列）。它们属于递归数据，因为每个时间步都与前面的步骤相关。
- en: While RNNs were originally developed for time series analysis and natural language
    processing tasks, they are now applied to various computer vision tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RNNs 最初是为时间序列分析和自然语言处理任务而开发的，但现在它们已被应用于各种计算机视觉任务。
- en: We will first introduce the basic concepts behind RNNs, before trying to get
    a general understanding of how they work. We will then describe how their weights
    can be learned.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍 RNNs 的基本概念，然后尝试对它们的工作原理进行一般性的理解。接着，我们将描述它们的权重如何学习。
- en: Basic formalism
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本形式
- en: To introduce RNNs, we will use the example of video recognition. A video is
    composed of *N* frames. The naive method to classify a video would be to apply
    a CNN to each frame, and then take the average of the outputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍 RNNs，我们将以视频识别为例。一个视频由 *N* 帧组成。分类一个视频的简单方法是对每一帧应用 CNN，然后对输出结果取平均值。
- en: While this would provide decent results, it does not reflect the fact that some
    parts of the video are more important than others. Moreover, the important parts
    do not always take more frames than the meaningless ones. The risk of averaging
    the output would be to lose important information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法能提供不错的结果，但它并未反映出视频中的某些部分比其他部分更为重要。而且，重要部分并不总是需要比无意义的部分更多的帧。将输出进行平均的风险在于可能会丢失重要信息。
- en: To circumvent this problem, an RNN is applied to all the frames of the video,
    one after the other, from the first one to the last one. The main attribute of
    RNNs is adequately combining features from all the frames in order to generate
    meaningful results.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，RNN 会将视频的所有帧依次处理，从第一帧到最后一帧。RNN 的主要特点是能够有效地结合所有帧的特征，从而生成有意义的结果。
- en: We do not apply the RNN directly to the raw pixels of the frame. As described
    later in the chapter, we first use a CNN to generate a feature volume (a stack
    of feature maps). The concept of feature volume was detailed in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml),
    *Modern Neural Networks*. As a reminder, a feature volume is the output of a CNN
    and usually represents the input with a smaller dimensionality.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不会直接将 RNN 应用于帧的原始像素。如本章稍后所述，我们首先使用 CNN 生成一个特征体积（一个特征图的堆叠）。特征体积的概念在[第 3 章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)，*现代神经网络*中有详细介绍。提醒一下，特征体积是
    CNN 的输出，通常表示的是具有较小维度的输入。
- en: To do so, RNNs introduce a new concept called the **state**. State can be pictured
    as the memory of the RNN. In practice, *state* is a float matrix. The *state*
    starts as a zero matrix and is updated with each frame of the video. At the end
    of the process, the final state is used to generate the output of the RNN.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，RNN 引入了一个新的概念，叫做**状态**。状态可以看作是 RNN 的记忆。在实践中，*状态*是一个浮动矩阵。*状态*最初是一个零矩阵，并在每一帧视频中进行更新。过程结束时，最终状态被用来生成
    RNN 的输出。
- en: 'The main component of an RNN is the **RNN cell**, which we will apply to every
    frame. A cell receives as inputs both the *current frame* and the *previous state*.
    For a video composed of *N* frames, an unfolded representation of a simple recurrent
    network is depicted in *Figure 8-1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的主要组件是 **RNN 单元**，我们将对每一帧应用该单元。一个单元接受的输入包括 *当前帧* 和 *上一个状态*。对于由 *N* 帧组成的视频，简单递归网络的展开表示如图
    *8-1* 所示：
- en: '![](img/5169459a-a619-4a53-9a8b-4c3a7fe2928e.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5169459a-a619-4a53-9a8b-4c3a7fe2928e.png)'
- en: 'Figure 8-1: Basic RNN cell'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-1：基本 RNN 单元
- en: In detail, we start with a null state (*h^(<0>)*). As a first step, the cell
    combines the current state (*h^(<0>)*) with the current frame (frame[1]) to generate
    a new state (*h^(<1>)*). Then, the same process is applied to the next frames.
    At the end of this process, we end up with the final state (*h^(<n>)*).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们从一个空状态 (*h^(<0>)* ) 开始。第一步，单元将当前状态 (*h^(<0>)* ) 与当前帧 (frame[1]) 结合生成新的状态
    (*h^(<1>)* )。然后，相同的过程会应用于接下来的帧。过程结束时，我们会得到最终状态 (*h^(<n>)* )。
- en: Note the vocabulary here—*RNN* refers to the component that accepts an image
    and returns a final output. An *RNN cell* refers to the sub-component that combines
    a frame as well as a current state, and returns the next state.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意这里的术语——*RNN* 指的是接收图像并返回最终输出的组件。*RNN 单元* 指的是将一帧和当前状态结合起来，并返回下一个状态的子组件。
- en: 'In practice, the cell combines the current state and the frame to generate
    a new state. This combination happens according to the following formula:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，单元将当前状态和帧结合起来生成新的状态。这个组合是根据以下公式进行的：
- en: '![](img/a963e0e1-c4b2-4417-ace0-01520200eb48.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a963e0e1-c4b2-4417-ace0-01520200eb48.png)'
- en: 'In the formula, the following applies:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，以下内容适用：
- en: '*b* is the bias.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是偏置。'
- en: '*W[rec]* is the recurrent weight matrix, and *W[input]* is the weight matrix.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W[rec]* 是递归权重矩阵，*W[input]* 是权重矩阵。'
- en: '*x^(<t>)* is the input.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x^(<t>)* 是输入。'
- en: '*h*^(*<t-1*)^> is the current state, and *h^(<t>)* is the new state.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h*^(*<t-1>*)^> 是当前状态，*h*^(<t>)* 是新状态。'
- en: 'The hidden state is not used as is. A weight matrix, *V*, is used to compute
    the final prediction:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态不是直接使用的。会使用一个权重矩阵 *V* 来计算最终的预测：
- en: '![](img/dbf994e9-96b7-42f6-876a-4dc1d6022356.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dbf994e9-96b7-42f6-876a-4dc1d6022356.png)'
- en: Throughout this chapter, we will make use of chevrons (*< >*) to denote temporal
    information. Other sources may use different conventions. Note, however, that
    the *y* with a hat (![](img/4d9d459e-d52e-4560-ad87-0679ff1768e4.png)) commonly
    represents the prediction of a neural network, while *y* represents the ground
    truth.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用尖括号 (*< >*) 来表示时间信息。其他来源可能使用不同的约定。然而，请注意，带有帽子的 *y*（![](img/4d9d459e-d52e-4560-ad87-0679ff1768e4.png)）通常表示神经网络的预测，而
    *y* 则表示真实值。
- en: When applied to videos, RNNs can be used to classify the whole video or every
    single frame. In the former case, for instance, when predicting whether a video
    is violent, only the final prediction, ![](img/9a1a7978-6dd9-4bba-b4e5-b677fb274ad6.png),
    will be used. In the latter case, for instance, to detect which frames may contain
    nudity, predictions for each time step will be used.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用于视频时，RNN 可以用于对整个视频或每一帧进行分类。在前一种情况下，例如，在预测视频是否暴力时，只有最终的预测 ![](img/9a1a7978-6dd9-4bba-b4e5-b677fb274ad6.png)
    会被使用。在后一种情况下，例如，为了检测哪些帧可能包含裸露内容，每个时间步的预测都会被使用。
- en: General understanding of RNNs
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 的一般理解
- en: Before we detail how the network learns the weights of *W[input]*, *W[rec]*,
    and *V*, let's try to get a broad understanding of how a basic RNN works. The
    general idea is that *W[input]* will influence the results if some of the features
    from the input make it into the hidden state, and *W[rec]* will influence the
    results if some features stay in the hidden state.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细说明网络如何学习 *W[input]*、*W[rec]* 和 *V* 的权重之前，我们先尝试大致了解基本的 RNN 是如何工作的。一般来说，*W[input]*
    会影响结果，如果输入中的某些特征进入了隐藏状态，*W[rec]* 会影响结果，如果某些特征停留在隐藏状态中。
- en: Let's use specific examples—classifying a violent video and a dance video.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用具体的例子来说明——分类暴力视频和舞蹈视频。
- en: As a gunshot can be quite sudden, it would represent only a few frames among
    all the frames of the video. Ideally, the network will learn W[input], so that
    when *x^(<t>)* contains the information of a gunshot, the concept of *violent
    video* would be added to the state. Moreover, *W[rec]* (defined in the previous
    equation) must be learned in a way that prevents the concept of *violent* from
    disappearing from the state. This way, even if the gunshot appears only in the
    first few frames, the video would still be classified as violent (see *Figure
    8-2*).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于枪声通常是非常突然而短暂的，它只会出现在视频中的少数几帧中。理想情况下，网络将学习 *W[input]*，以便当 *x^(<t>)* 包含枪声信息时，*暴力视频*
    的概念会被加入到状态中。此外，*W[rec]*（在前面方程中定义）必须以一种防止 *暴力* 概念从状态中消失的方式进行学习。这样，即使枪声只出现在视频的前几帧，视频仍然会被分类为暴力视频（见
    *图 8-2*）。
- en: 'However, to classify dance videos, we would adopt another behavior. Ideally,
    the network would learn *W[input]* so that, for example, when *x^(<t> )*contains
    people who appear to be dancing, the concept of *dance* would only be lightly
    incremented in the state (see *Figure 8-2*):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了对舞蹈视频进行分类，我们需要采用另一种行为。理想情况下，网络应该学习 *W[input]*，因此，例如，当 *x^(<t>)* 包含看起来像在跳舞的人时，*舞蹈*
    的概念只会轻微地增加到状态中（见 *图 8-2*）：
- en: '![](img/b9eb6930-ae06-4161-9841-7c6e18e6696a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9eb6930-ae06-4161-9841-7c6e18e6696a.png)'
- en: 'Figure 8-2: Simplified representation of how the hidden state should evolve,
    depending on the video content'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-2：隐藏状态应如何根据视频内容演变的简化表示
- en: Indeed, if the input is a sport video, we would not want a single frame mistakenly
    classified as *dancing people* to change our state to *dancing*. As a dancing
    video is mostly made of frames containing dancing people, by incrementing the
    state little by little, we would avoid misclassification.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，如果输入的是一段体育视频，我们不希望某一帧被错误地分类为 *跳舞的人*，进而改变我们的状态为 *跳舞*。由于舞蹈视频大多由包含跳舞人物的帧组成，通过逐步递增状态，我们可以避免误分类。
- en: Moreover, *W[rec]* must be learned in order to make *dance* gradually disappear
    from the state. This way, if the introduction of the video is about dance, but
    the whole video is not, it would not be classified as such.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*W[rec]* 必须被学习，以使 *舞蹈* 概念逐渐从状态中消失。这样，如果视频的开头是关于舞蹈的，但整个视频并不是舞蹈视频，它就不会被分类为舞蹈视频。
- en: Learning RNN weights
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 RNN 权重
- en: In practice, the state of the network is much more complex than a vector containing
    a weight for each class, as in the previous example. The weights of *W[input]*, *W[rec]*,
    and *V* cannot be engineered by hand. Thankfully, they can be learned through
    **backpropagation**. This technique was detailed in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*. The general idea is to learn the weights
    by correcting them based on the errors that the network makes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，网络的状态比前面例子中仅包含每个类别的权重的向量要复杂得多。*W[input]*、*W[rec]* 和 *V* 的权重无法手工设计。幸运的是，它们可以通过
    **反向传播** 进行学习。该技术在[第 1 章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)，*计算机视觉与神经网络*
    中有详细介绍。一般的思路是通过根据网络所犯的错误来修正权重，从而学习这些权重。
- en: Backpropagation through time
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过时间的反向传播
- en: 'For RNNs, however, we not only backpropagate the error through the depth of
    the network, but also through time. First of all, we compute the total loss by
    summing the individual loss (*L*) over all the time steps:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于 RNN，我们不仅通过网络的深度进行反向传播误差，还需要通过时间进行反向传播。首先，我们通过对所有时间步的个体损失（*L*）求和来计算总损失：
- en: '![](img/dcef82b0-e784-40a7-9d94-5230a6aec74a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dcef82b0-e784-40a7-9d94-5230a6aec74a.png)'
- en: 'This means that we can compute the gradient for each time step separately.
    To greatly simplify the calculations, we will assume that *tanh* = *identity*
    (that is, we assume that there is no activation function). For instance, at *t*
    = *4*, we will compute the gradient by applying the chain rule:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以单独计算每个时间步的梯度。为了大大简化计算，我们将假设 *tanh* = *identity*（也就是说，我们假设没有激活函数）。例如，在
    *t* = *4* 时，我们将通过应用链式法则来计算梯度：
- en: '![](img/2dc89d41-4b50-41c7-9652-606b9839cec7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dc89d41-4b50-41c7-9652-606b9839cec7.png)'
- en: Here, we stumble upon a complexity—the third term (in bold) on the right-hand
    side of the equation cannot be easily derived. Indeed, to take the derivative
    of *h^(<4>)* with respect to *W[rec]*, all other terms must not depend on *W[rec]*.
    However, *h^(<4>)* also depends on *h^(<3>)*. And *h^(<3>)* depends on *W[rec]*,
    since *h^(<3>)*= *tanh* (*W**[rec] h^(<2>)* + *W**[input] x^(<3>)*+*b*), and so
    on and so forth until we reach *h^(<0>)*, which is entirely composed of zeros.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们遇到了一种复杂性——方程右侧的第三项（加粗部分）无法轻易求导。实际上，要对 *h^(<4>)* 关于 *W[rec]* 求导，所有其他项不能依赖于
    *W[rec]*。然而，*h^(<4>)* 也依赖于 *h^(<3>)*，而 *h^(<3>)* 又依赖于 *W[rec]*，因为 *h^(<3>)* =
    *tanh* (*W**[rec] h^(<2>)* + *W**[input] x^(<3>)* + *b*)，以此类推，直到我们到达 *h^(<0>)*，它完全由零组成。
- en: 'To properly derive this term, we apply the total derivative formula on this
    partial derivative:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确地推导这个项，我们在这个偏导数上应用全微分公式：
- en: '![](img/2301b12c-ff98-425b-8cc0-5384c6a2a653.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2301b12c-ff98-425b-8cc0-5384c6a2a653.png)'
- en: It might seem weird that a term is equal to itself plus other (non-null) terms.
    However, since we are taking the total derivative of a partial derivative, we
    need to take into account all the terms in order to generate the gradient.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，似乎有些奇怪：一个项等于它自己加上其他（非零）项。然而，由于我们正在对一个偏导数进行全微分，我们需要考虑所有项，以生成梯度。
- en: 'By noticing that all the other terms are remaining constant, we obtain the
    following equations:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注意到其他所有项保持不变，我们可以得到以下方程：
- en: '![](img/0a6c7dd6-ae58-4567-b5fc-81350c329cf6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a6c7dd6-ae58-4567-b5fc-81350c329cf6.png)'
- en: 'Therefore, the partial derivative presented before can be expressed as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，之前呈现的偏导数可以表示为如下：
- en: '![](img/165d3572-8480-436e-a48e-35b45fcc897e.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/165d3572-8480-436e-a48e-35b45fcc897e.png)'
- en: In conclusion, we notice that the gradient will depend on all the previous states
    as well as *W[rec]*. This concept is called **backpropagation through time** (**BPTT**).
    Since the latest state depends on all the states before it, it only makes sense
    to consider them to compute the error. As we sum the gradient of each time step
    to compute the total gradient, and since, for each time step, we have to go back
    to the first time step to compute the gradient, a large amount of computation
    is implied. For this reason, RNNs are notoriously slow to train.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们注意到梯度将依赖于所有之前的状态以及 *W[rec]*。这个概念被称为 **时间反向传播**（**BPTT**）。由于最新的状态依赖于它之前的所有状态，因此考虑这些状态来计算误差是有意义的。当我们将每个时间步的梯度相加以计算总梯度时，并且由于对于每个时间步，我们必须回到第一个时间步来计算梯度，因此涉及大量的计算。因此，RNN
    在训练时往往非常慢。
- en: Moreover, we can generalize the previous formula to show that ![](img/1fd22468-8206-4bb5-bd9b-35c90b6f2489.png)depends
    on *W[rec]* to the power of (*t-2*). This is very problematic when *t* is large.
    Indeed, if the terms of *W[rec]* are below one, with the high exponent, they become
    very small. Worse, if the terms are above one, the gradient tends toward infinity.
    These phenomena are called **gradient vanishing** and **gradient explosion**,
    respectively (they were previously described in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*). Thankfully, workarounds exist to avoid this
    problem.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以将之前的公式推广，表明 ![](img/1fd22468-8206-4bb5-bd9b-35c90b6f2489.png) 依赖于 *W[rec]*
    的 *t-2* 次方。当 *t* 较大时，这非常棘手。事实上，如果 *W[rec]* 的项小于 1，随着指数的增大，它们变得非常小。更糟糕的是，如果这些项大于
    1，梯度将趋向于无穷大。这些现象分别被称为 **梯度消失** 和 **梯度爆炸**（它们在[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)中有描述，*影响力分类工具*）。幸运的是，存在一些解决方法可以避免这个问题。
- en: Truncated backpropagation
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 截断反向传播
- en: To circumvent the long training time, it is possible to compute the gradient
    every *k[1]* time step instead of every step. This divides the number of gradient
    operations by *k[1]*, making the training of the network faster.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免长时间的训练过程，可以选择每 *k[1]* 个时间步计算一次梯度，而不是每一步计算一次。这将梯度计算的次数除以 *k[1]*，使得网络的训练更加迅速。
- en: Instead of backpropagating throughout all the time steps, we can also limit
    the propagation to *k[2]* steps in the past. This effectively limits the gradient
    vanishing, since the gradient will depend on *W^(k[2])* at most. This also limits
    the computations that are necessary to compute the gradient. However, the network
    will be less likely to learn long-term temporal relations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将反向传播限制在 *k[2]* 步之前，而不是遍历所有时间步。这有效地限制了梯度消失，因为梯度最多只会依赖于 *W^(k[2])*。这也限制了计算梯度所需的计算量。然而，网络将不太可能学习长期的时间关系。
- en: The combination of those two techniques is called **truncated backpropagation**,
    with its two parameters commonly referred to as *k[1]* and *k[2]*. They must be
    tuned to ensure a good trade-off between training speed and model performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术的结合被称为 **截断反向传播**，其两个参数通常被称为 *k[1]* 和 *k[2]*。它们必须进行调整，以确保在训练速度和模型性能之间达到良好的平衡。
- en: This technique—while powerful—remains a workaround for a fundamental RNN problem.
    In the next section, we will introduce a change of architecture that can be used
    to solve this issue in its entirety.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技术——尽管强大——仍然是解决一个基本 RNN 问题的权宜之计。在接下来的章节中，我们将介绍一种架构的改变，可以用来彻底解决这个问题。
- en: Long short-term memory cells
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆单元
- en: As we saw previously, regular RNNs suffer from gradient explosion. As such,
    it can sometimes be hard to teach them long-term relations in sequences of data.
    Moreover, they store information in a single-state matrix. For instance, if a
    gunshot happens at the very beginning of a very long video, it will be unlikely
    that the hidden state of the RNNs will not be overridden by noise by the time
    it reaches the end of the video. The video might not be classified as violent.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，常规 RNN 存在梯度爆炸问题。因此，有时候它们很难学习数据序列中的长期关系。此外，它们将信息存储在单一的状态矩阵中。例如，如果枪声发生在一个非常长的视频的开始部分，到视频结束时，RNN
    的隐藏状态很可能已经被噪声覆盖。这个视频可能不会被分类为暴力内容。
- en: To circumvent those two problems, Sepp Hochreiter and Jürgen Schmidhuber proposed,
    in their paper (*Long Short-Term Memory*, *Neural Computation*, 1997), a variant
    of the basic RNN—the **Long Short-Term Memory** (**LSTM**) cell. This has improved
    markedly over the years, with many variants being introduced. In this section,
    we will give an overview of its inner workings, and we will show why gradient
    vanishing is less of an issue.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这两个问题，Sepp Hochreiter 和 Jürgen Schmidhuber 在他们的论文（*Long Short-Term Memory*，*Neural
    Computation*，1997）中提出了基本 RNN 的一种变种——**长短期记忆**（**LSTM**）单元。多年来，这一方法有了显著的改进，并引入了许多变体。在本节中，我们将概述其内部工作原理，并展示为什么梯度消失问题不再那么严重。
- en: LSTM general principles
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 的基本原理
- en: Before we detail the mathematics behind the LSTM cell, let's try to get a general
    understanding of how it works. To do so, we will use the example of a live classification
    system that is applied to the Olympic Games. The system has to detect, for every
    frame, which sport is being played during a long video from the Olympics.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍 LSTM 单元背后的数学原理之前，我们先尝试理解它是如何工作的。为此，我们将以一个应用于奥林匹克运动会的实时分类系统为例。该系统必须检测每一帧中正在进行的运动项目。
- en: If the network sees people standing in line, can it infer what sport it is?
    Is it soccer players singing the anthem, or is it athletes preparing to run a
    100-meter race? Without information about what happened in the frames just prior
    to this, the prediction will not be accurate. The basic RNN architecture we presented
    earlier would be able to store this information in the hidden state. However,
    if the sports are alternating one after the other, it would be much harder. Indeed,
    the state is used to generate the current predictions. The basic RNN is unable
    to store information that it will not use immediately.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络看到人们站成一排，它能推测是什么运动吗？是足球运动员在唱国歌，还是运动员准备跑 100 米比赛？如果没有关于前面几帧发生了什么的信息，预测就不会准确。我们之前介绍的基本
    RNN 架构能够将这些信息存储在隐藏状态中。然而，如果运动项目是一个接一个交替出现的，那么这将变得更加困难。事实上，状态被用来生成当前的预测。基本 RNN
    无法存储那些它不会立即使用的信息。
- en: The LSTM architecture solves this by storing a memory matrix, which is called
    the **cell state** and is referred to as *C^(<t>)*. At every time step, *C^(<t>)*
    contains information about the current state. But this information will not be
    used directly to generate the output. Instead, it will be filtered by a *gate*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 架构通过存储一个被称为**单元状态**的记忆矩阵来解决这个问题，它被表示为 *C^(<t>)*。在每个时间步，*C^(<t>)* 包含当前状态的信息。但这些信息不会直接用于生成输出。相反，它将通过一个*门*进行过滤。
- en: Note that the LSTM's cell state is different from the simple RNN's state, as
    outlined by the following equations. The LSTM's cell state is filtered before
    being transformed into the final state.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LSTM 的单元状态与简单 RNN 的状态不同，如下方程所示。LSTM 的单元状态在转化为最终状态之前会经过过滤。
- en: Gates are the core idea of LSTM's cell. A gate is a matrix that will be multiplied
    term by term to another element in the LSTM. If all the values of the gate are
    *0*, none of the information from the other element will pass through. On the
    other hand, if the gate values are all around *1*, all the information of the
    other element will pass through.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 门是 LSTM 单元的核心思想。一个门是一个矩阵，将与 LSTM 中的另一个元素逐项相乘。如果门的所有值为*0*，则其他元素的信息将无法通过。另一方面，如果门的值接近*1*，则所有其他元素的信息都将通过。
- en: 'As a reminder, an example of term-by-term multiplication (also called **element-wise
    multiplication** or the **Hadamard product**) can be depicted as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，逐项相乘的示例（也称为**元素级乘法**或**哈达玛积**）可以如下表示：
- en: '![](img/98e6f482-4d5e-4a76-a4c2-bf147d661b8f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98e6f482-4d5e-4a76-a4c2-bf147d661b8f.png)'
- en: 'At each time step, three gate matrices are computed using the current input
    and the previous output:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步长，利用当前输入和前一输出计算三个门矩阵：
- en: '**The input gate**: Applied to the input to decide which information gets through.
    In our example, if the video is showing members of the audience, we would not
    want to use this input to generate predictions. The gate would be mostly zeros.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：应用于输入，以决定哪些信息能通过。在我们的示例中，如果视频显示的是观众成员，我们可能不希望使用此输入来生成预测。此时，门的值大多为零。'
- en: '**The forget gate**: Applied to the cell state to decide which information
    to forget. In our example, if the video is showing presenters talking, we would
    want to forget about the current sport, as we are probably going to see a new
    sport next.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：应用于单元状态，以决定忘记哪些信息。在我们的示例中，如果视频展示的是讲解员在讲话，我们可能希望忘记当前的运动项目，因为接下来可能会展示新的运动项目。'
- en: '**The output gate**: This will be multiplied by the cell state to decide which
    information to output. We might want to keep in the cell state the fact that the
    previous sport was soccer, but this information will not be useful for the current
    frame. Outputting this information would perturb the upcoming time steps. By setting
    the gate around zero, we would effectively keep this information for later.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：将与单元状态相乘，以决定哪些信息被输出。我们可能希望在单元状态中保留先前的体育项目为足球这一事实，但这对于当前帧并没有用处。输出此信息可能会扰乱接下来的时间步。通过将门的值设为接近零，我们可以有效地将此信息保留到后面。'
- en: In the next section, we will cover how the gates and the candidate state are
    computed and demonstrate why LSTMs suffer less from gradient vanishing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍如何计算这些门和候选状态，并展示为什么 LSTM 在梯度消失问题上影响较小。
- en: LSTM inner workings
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 内部工作原理
- en: 'First, let''s detail how the gates are computed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们详细说明门是如何计算的：
- en: '![](img/b5777da1-74f4-4480-941d-d114c7f886f5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5777da1-74f4-4480-941d-d114c7f886f5.png)'
- en: As detailed in the previous equations, the three gates are computed using the
    same principle—by multiplying a weight matrix (*W*) by the previous output (*h^(<t-1>)*)
    and the current input (*x^(<t>)*). Notice that the activation function is the
    sigmoid (σ). As a consequence, the gate values are always between *0* and *1*.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面方程所述，三个门是通过相同的原理计算的——将权重矩阵 (*W*) 与前一输出 (*h^(<t-1>)*) 和当前输入 (*x^(<t>)*）相乘。请注意，激活函数是
    sigmoid（σ）。因此，门的值始终介于*0*和*1*之间。
- en: 'The candidate state (![](img/729d7117-1700-4f15-b009-bc5a52b11a51.png)) is
    computed in a similar fashion. However, the activation function used is a hyperbolic
    tangent instead of the sigmoid:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 候选状态（![](img/729d7117-1700-4f15-b009-bc5a52b11a51.png)）以类似方式计算。然而，所使用的激活函数是双曲正切函数，而非
    sigmoid 函数：
- en: '![](img/8c27adaf-69d5-4c6e-a822-9cb219f86b97.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c27adaf-69d5-4c6e-a822-9cb219f86b97.png)'
- en: 'Notice that this formula is exactly the same as the one used to compute *h^(<t>)*
    in the basic RNN architecture. However, *h^(<t>)* was the *hidden state* while,
    in this case, we are computing the **candidate cell state**. To compute the new
    cell state, we combine the previous one with the candidate cell state. Both states
    are gated by the forget and input gates, respectively:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个公式与在基本 RNN 架构中用于计算*h^(<t>)*的公式完全相同。然而，*h^(<t>)* 是*隐藏状态*，而在这种情况下，我们计算的是**候选单元状态**。为了计算新的单元状态，我们将前一个单元状态与候选单元状态结合。两个状态分别通过遗忘门和输入门进行控制：
- en: '![](img/1d704d09-e5d0-48fc-8c44-c5077a57ef37.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d704d09-e5d0-48fc-8c44-c5077a57ef37.png)'
- en: 'Finally, the LSTM hidden state (output) will be computed from the cell state
    as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LSTM 隐藏状态（输出）将从单元状态计算如下：
- en: '![](img/90069c9e-8d6f-4409-a98b-4a850ff902a7.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90069c9e-8d6f-4409-a98b-4a850ff902a7.png)'
- en: 'The simplified representation of the LSTM cell is depicted in *Figure 8-3*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元的简化表示如*图 8-3*所示：
- en: '![](img/84f1e8a6-d776-431d-80d8-7c7fb6571eb3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84f1e8a6-d776-431d-80d8-7c7fb6571eb3.png)'
- en: 'Figure 8-3: Simplified representation of the LSTM cell. Gate computation is
    omitted'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8-3：LSTM 单元的简化表示。门控计算已省略
- en: 'LSTM weights are also computed using backpropagation through time. Due to the
    numerous information paths in LSTM cells, gradient computation is even more complex.
    However, we can observe that if the terms of the forget gate, *f^(<t>)*, are close
    to *1*, information can be passed from one cell state to the other, as shown in
    the following equation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 权重同样通过时间反向传播进行计算。由于 LSTM 单元中存在众多信息路径，梯度计算变得更加复杂。然而，我们可以观察到，如果遗忘门的项*f^(<t>)*接近*1*，则信息可以从一个单元状态传递到另一个单元状态，如下方公式所示：
- en: '![](img/63f36f34-b96e-48d6-8bfd-6c5e73186690.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63f36f34-b96e-48d6-8bfd-6c5e73186690.png)'
- en: For this reason, by initializing the forget gate bias to a vector of ones, we
    can ensure that the information backpropagates through numerous time steps. As
    such, LSTMs suffer less from gradient vanishing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过将遗忘门偏置初始化为全为 1 的向量，我们可以确保信息能够通过多个时间步进行反向传播。这样，LSTM 在梯度消失问题上表现得较少。
- en: This concludes our introduction to RNNs; we can now begin with the hands-on
    classification of a video.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对 RNN 的介绍结束；我们现在可以开始进行视频的实际分类。
- en: Classifying videos
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频分类
- en: From television to web streaming, the video format is getting more and more
    popular. Since the inception of computer vision, researchers have attempted to
    apply computer vision to more than one image at a time. While limited by computing
    power at first, they more recently have developed powerful techniques for video
    analysis. In this section, we will introduce video-related tasks and detail one
    of them—video classification.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从电视到网络流媒体，视频格式越来越受欢迎。自计算机视觉诞生以来，研究人员一直尝试将计算机视觉应用于多张图像的处理。尽管最初受到计算能力的限制，但最近他们已经开发出了强大的视频分析技术。在本节中，我们将介绍与视频相关的任务，并详细介绍其中之一——视频分类。
- en: Applying computer vision to video
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将计算机视觉应用于视频
- en: At 30 frames per second, processing every frame of a video implies analyzing
    *30 × 60* = *180* frames per minute. This problem was faced really early in computer
    vision, before the rise of deep learning. Techniques were then devised to analyze
    videos efficiently.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每秒 30 帧的情况下，处理每一帧视频意味着每分钟需要分析*30 × 60* = *180* 帧。这一问题在计算机视觉早期阶段就已经出现，在深度学习兴起之前。当时，开发出了高效的视频分析技术。
- en: The most obvious technique is **sampling**. We can analyze only one or two frames
    per second instead of all the frames. While more efficient, we may lose information
    if an important scene appears very briefly, such as in the case of a gunshot,
    which was mentioned earlier.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最明显的技术是**采样**。我们可以每秒分析一到两帧，而不是分析所有帧。虽然这样更高效，但如果一个重要场景短暂出现，例如之前提到的枪声，我们可能会失去信息。
- en: A more advanced technique is **scene extraction**. This is particularly popular
    for analyzing movies. An algorithm detects when the video is changing from one
    scene to another. For instance, if the camera goes from a close-up view to a wide
    view, we would analyze a frame from each framing. Even if the close-up is really
    short and the wide view occurs over many frames, we would extract only one frame
    from each shot. *Scene extraction* can be done by using fast and efficient algorithms.
    They process the pixels of images and evaluate the variation between two consecutive
    frames. A large variation indicates a scene change.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 更高级的技术是**场景提取**。这对于分析电影特别流行。算法检测视频何时从一个场景切换到另一个场景。例如，如果镜头从特写镜头切换到广角镜头，我们将分析每个镜头中的一帧。即使特写镜头非常短，而广角镜头跨越多个帧，我们也只会从每个镜头中提取一帧。*场景提取*可以通过使用快速高效的算法来完成。它们处理图像的像素，并评估两个连续帧之间的变化。大的变化意味着场景切换。
- en: 'In addition, all the image-related tasks described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*, also apply to video. For instance, super-resolution,
    segmentation, and style transfer are commonly targeted at video. However, the
    temporal aspect of a video creates new applications in the form of the following
    video-specific tasks:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有在[第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)中描述的与图像相关的任务，*计算机视觉与神经网络*，也适用于视频。例如，超分辨率、分割和风格迁移通常针对视频进行。但视频的时间维度创造了新的应用形式，具体体现在以下特定于视频的任务中：
- en: '**Action detection**: A variant of video classification, the goal here is to
    classify what actions a person is accomplishing. Actions range from running to
    playing soccer, but can also be as precise as the kind of dance being performed,
    or the musical instrument being played.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作检测**：这是视频分类的一个变种，目标是分类一个人正在完成的动作。动作从跑步到踢足球不等，甚至可以精确到表演的舞蹈类型或演奏的乐器。'
- en: '**Next-frame prediction**: Given *N* consecutive frames, this predicts how
    frame *N+1* is going to look.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一帧预测**：给定*N*连续帧，预测第*N+1*帧的样子。'
- en: '**Ultra slow motion**: This is also called **frame interpolation**. The model
    has to generate intermediate frames to make slow motion look less jerky.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超慢动作**：这也叫做**帧插值**。模型需要生成中间帧，以使慢动作看起来不那么卡顿。'
- en: '**Object tracking**: This was executed historically using classical computer
    vision techniques such as descriptors. However, deep learning is now applied to
    track objects in videos.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体跟踪**：这通常使用传统的计算机视觉技术，如描述符来执行。然而，现在已经采用深度学习来跟踪视频中的物体。'
- en: Of these video-specific tasks, we will focus on action detection. In the next
    section, we will introduce an action video dataset and cover how to apply an LSTM
    cell to videos.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些特定于视频的任务中，我们将重点介绍动作检测。在下一节中，我们将介绍一个动作视频数据集，并介绍如何将LSTM单元应用于视频。
- en: Classifying videos with an LSTM
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM对视频进行分类
- en: 'We will make use of the *UCF1**01* dataset ([https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)),
    which was put together by K. Soomro et al. (refer to *UCF101: A Dataset of 101
    Human Actions Classes From Videos in The Wild*, CRCV-TR-12-01, 2012). Here are
    a few examples from the dataset:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*UCF101*数据集（[https://www.crcv.ucf.edu/data/UCF101.php](https://www.crcv.ucf.edu/data/UCF101.php)），该数据集由K.
    Soomro等人编制（请参阅*UCF101：来自野外视频的101个人类动作类别数据集*，CRCV-TR-12-01，2012）。以下是数据集中的一些示例：
- en: '![](img/deca8b2d-956c-4dae-ad97-7409d5742783.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/deca8b2d-956c-4dae-ad97-7409d5742783.png)'
- en: 'Figure 8-4: Example images from the UCF101 dataset'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-4：UCF101数据集中的示例图像
- en: The dataset is composed of 13,320 segments of video. Each segment contains a
    person performing one of 101 possible actions.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含13,320个视频片段。每个片段包含一个人执行101种可能动作中的一种。
- en: 'To classify the video, we will use a two-step process. Indeed, a recurrent
    network is not fed the raw pixel images. While it could technically be fed with
    full images, CNN feature extractors are used beforehand in order to reduce the
    dimensionality, and to reduce the computations done by LSTMs. Therefore, our network
    architecture can be represented by *Figure 8-5*:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对视频进行分类，我们将采用两步过程。事实上，递归网络并不会直接输入原始的像素图像。虽然理论上可以直接输入完整图像，但在此之前使用CNN特征提取器来减少维度，并减少LSTM的计算量。因此，我们的网络架构可以通过*图8-5*表示：
- en: '![](img/d30c5edf-b57d-4a39-911e-8ee2255bfc67.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d30c5edf-b57d-4a39-911e-8ee2255bfc67.png)'
- en: 'Figure 8-5: Combination of a CNN and an RNN to categorize videos. In this simplified
    example, the sequence length is 3'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-5：结合CNN和RNN进行视频分类。在这个简化的示例中，序列长度为3
- en: As stated earlier, backpropagating errors through RNNs is difficult. While we
    could train the CNN from scratch, it would take a tremendous amount of time for
    sub-par results. Therefore, we use a pretrained network, applying the transfer
    learning technique that was introduced in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml),
    *Influential Classification Tools*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，通过RNN反向传播错误是困难的。虽然我们可以从头开始训练CNN，但这将花费大量时间并得到不理想的结果。因此，我们使用预训练网络，应用[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《有影响力的分类工具》中介绍的迁移学习技术。
- en: For the same reason, it is also common practice not to fine-tune the CNN and
    to keep its weights untouched, as this does not bring any performance improvement.
    Since the CNN will stay unchanged throughout all the epochs of training, a specific
    frame will always return the same feature vector. This allows us to *cache* the
    feature vectors. As the CNN step is the most time-consuming, caching the results
    means computing the feature vector only once instead of at each epoch, saving
    us a tremendous amount of training time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，通常不对CNN进行微调，并保持其权重不变，因为这样不会带来性能提升。由于CNN在整个训练周期中保持不变，因此特定帧始终返回相同的特征向量。这使我们能够*缓存*特征向量。由于CNN步骤是最耗时的，缓存结果意味着只需计算一次特征向量，而不是每个周期都计算，从而节省大量训练时间。
- en: Therefore, we will classify the videos in two steps. First, we will extract
    the features and cache them. Once this is done, we will train the LSTM on extracted
    features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将视频分为两个步骤进行分类。首先，我们将提取特征并缓存它们。一旦完成此操作，我们将在提取的特征上训练LSTM。
- en: Extracting features from videos
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视频中提取特征
- en: To generate feature vectors, we will use a pretrained inception network trained
    on the ImageNet dataset to categorize images in different categories.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成特征向量，我们将使用在ImageNet数据集上训练过的预训练Inception网络来对图像进行分类。
- en: We will remove the last layer (the fully connected layer) and only keep the
    feature vector that is generated after a max-pooling operation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将移除最后一层（全连接层），仅保留最大池化操作后生成的特征向量。
- en: Another option would be to keep the output of the layer just before average-pooling,
    that is, the higher-dimensional feature maps. However, in our example, we will
    not need spatial information—whether the action takes place in the middle of the
    frame or in the corner, the predictions will be the same. Therefore, we will use
    the output of the two-dimensional max-pooling layer. This will make the training
    faster, since the input of the LSTM will be 64 times smaller (*64* = *8* × *8*
    = the size of a feature map for an input image of size *299* × *299*).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是保留平均池化层之前的输出，即高维特征图。然而，在我们的示例中，我们不需要空间信息——无论动作发生在画面中央还是角落，预测结果都会相同。因此，我们将使用二维最大池化层的输出。这将加速训练，因为LSTM的输入将比原来小64倍（*64*
    = *8* × *8* = 输入图像大小为 *299* × *299* 的特征图大小）。
- en: 'TensorFlow allows us to access a pretrained model with a single line, as described
    in [Chapter 4](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml), *Influential Classification
    Tools*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow允许我们通过一行代码访问预训练模型，如[第4章](061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml)《有影响力的分类工具》中所描述：
- en: '[PRE0]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We add the max-pooling operation to transform the *8* × *8* × *2,048* feature
    map into a *1* × *2,048* vector:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加最大池化操作，将*8* × *8* × *2,048* 的特征图转换为*1* × *2,048* 的向量：
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will use the `tf.data` API to load the frames from the video. An initial
    problem arises—all the videos have different lengths. Here is the distribution
    of the number of frames:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`tf.data` API从视频中加载帧。一个初步问题是——所有视频的长度不同。以下是帧数的分布情况：
- en: '![](img/d45fba54-3893-4f07-908b-7854257096e0.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d45fba54-3893-4f07-908b-7854257096e0.png)'
- en: 'Figure 8-6: Distribution of the number of frames per video in the UCF101 dataset'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-6：UCF101数据集中每个视频的帧数分布
- en: It is always good practice to run a quick analysis on data before using it.
    Manually reviewing it and plotting the distributions can save a lot of experimenting
    time.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据之前，最好进行快速分析。手动检查数据并绘制分布图可以节省大量实验时间。
- en: With TensorFlow, as with most deep learning frameworks, all examples in a batch
    must be of the same length. The most common solution to fit this requirement is
    *padding*—we fill the first temporal time steps with actual data and the last
    ones with zeros.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，与大多数深度学习框架一样，批次中的所有示例必须具有相同的长度。为满足此要求，最常见的解决方案是*padding*（填充）——我们用实际数据填充前几个时间步，将最后几个时间步填充为零。
- en: 'In our case, we will not use all the frames from the video. At 25 frames per
    second, most of the frames look alike. By using only a subset of the frames, we
    will reduce the size of our input, and therefore speed up the training process.
    To select this subset, we can use any of the following options:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们不会使用视频中的所有帧。每秒25帧的情况下，大多数帧看起来相似。通过只使用一部分帧，我们可以减少输入的大小，从而加快训练过程。为了选择这个子集，我们可以使用以下任意一种选项：
- en: Extract *N* frames per second.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒提取*N*帧。
- en: Sample *N* frames out of all the frames.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从所有帧中采样*N*帧。
- en: 'Segment the videos in scenes and extract *N* frames per scene, as shown in
    the following diagram:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将视频分段为场景并从每个场景中提取*N*帧，如下图所示：
- en: '![](img/1ba1d585-f268-460d-b199-f2773f7e4606.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ba1d585-f268-460d-b199-f2773f7e4606.png)'
- en: 'Figure 8-7: Comparison of two sampling techniques. Dotted rectangles indicate
    zero padding'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-7：两种采样技术的比较。虚线矩形表示零填充
- en: Due to the large variation in video length, extracting *N* frames per second
    would also result in a large variation in input length. Although this could be
    solved with padding, we would end up with some inputs mostly composed of zeros—this
    could lead to poor training performance. We will, therefore, sample *N* images
    per video.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视频长度的巨大变化，每秒提取*N*帧也会导致输入长度的巨大变化。虽然可以通过填充解决这个问题，但我们最终会得到一些几乎全部由零组成的输入——这可能导致训练性能不佳。因此，我们将从每个视频中采样*N*张图像。
- en: 'We will use the TensorFlow dataset API to feed the input to our feature extraction
    network:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow数据集API将输入馈送到我们的特征提取网络：
- en: '[PRE2]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the previous code, we specify the input type and the input shape. Our generator
    will return images of shape *299* × *299* with three channels, as well as a string
    representing the filename. The filename will be used to group the frames by video
    later on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们指定了输入类型和输入形状。我们的生成器将返回形状为*299* × *299*的图像，并且具有三个通道，还会返回一个表示文件名的字符串。文件名稍后将用于根据视频对帧进行分组。
- en: 'The role of `frame_generator` is to select the frames that will be processed
    by the network. We use the OpenCV library to read from the video file. For each
    video, we will sample an image every *N* frames, where *N* equals `num_frames
    / SEQUENCE_LENGTH` and `SEQUENCE_LENGTH` is the size of the input sequence of
    the LSTM. A simplified version of this generator looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`frame_generator`的作用是选择将由网络处理的帧。我们使用OpenCV库从视频文件中读取数据。对于每个视频，我们每隔*N*帧采样一张图像，其中*N*等于`num_frames
    / SEQUENCE_LENGTH`，而`SEQUENCE_LENGTH`是LSTM输入序列的大小。该生成器的简化版本如下所示：'
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We iterate over the frames of the video, processing only a subset. At the end
    of the video, the OpenCV library will return `success` as `False` and the loop
    will terminate.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历视频的帧，只处理其中的一部分。在视频结束时，OpenCV库将返回`success`为`False`，循环将终止。
- en: Note that just like in any Python generator, instead of using the `return` keyword,
    we use the `yield` keyword. This allows us to start returning frames before the
    end of the loop. This way, the network can start training without waiting for
    all the frames to be preprocessed.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与任何Python生成器一样，我们不是使用`return`关键字，而是使用`yield`关键字。这样我们可以在循环结束之前就开始返回帧。这样，网络可以在不等待所有帧预处理完成的情况下开始训练。
- en: 'Finally, we iterate over the dataset to generate video features:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们遍历数据集以生成视频特征：
- en: '[PRE4]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the previous code, note that we iterate over the batch output and compare
    video filenames. We do so because the batch size is not necessarily the same as
    *N* (the number of frames we sample per video). Therefore, a batch may contain
    frames from multiple consecutive sequences:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，注意我们迭代批次输出并比较视频文件名。我们这样做是因为批次大小不一定与*N*（我们每个视频采样的帧数）相同。因此，一个批次可能包含多个连续序列的帧：
- en: '![](img/f5dd6d25-9070-4070-a4ee-7b942f513eee.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5dd6d25-9070-4070-a4ee-7b942f513eee.png)'
- en: 'Figure 8-8: Representation of the input for a batch size of four and three
    sampled frames per video'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图8-8：批次大小为四，每个视频采样三帧的输入表示
- en: We read the output of the network, and when we reach a different filename, we
    save the video features to file. Note that this technique will only work if the
    frames are in the correct order. If the dataset is shuffled, it would no longer
    work. Video features are saved at the same location as the video, but with a different
    extension (`.npy` instead of `.avi`).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们读取网络的输出，当遇到不同的文件名时，我们将视频特征保存到文件中。请注意，这种技术只有在帧的顺序正确时才有效。如果数据集被打乱，它将无法正常工作。视频特征会保存到与视频相同的位置，但扩展名不同（`.npy`而不是`.avi`）。
- en: This step iterates over the 13,320 videos of the dataset and generates features
    for every single one of them. Sampling 40 frames per video takes about one hour
    on a modern GPU.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步会迭代数据集中13,320个视频，并为每个视频生成特征。每个视频采样40帧，使用现代GPU大约需要一个小时。
- en: Training the LSTM
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练LSTM
- en: Now that the video features are generated, we can use them to train an LSTM.
    This step is very similar to the training steps described earlier in the book—we
    define a model and an input pipeline, and launch the training.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在视频特征已生成，我们可以用它们来训练LSTM。这个步骤与本书前面描述的训练步骤非常相似——我们定义模型和输入管道，并启动训练。
- en: Defining the model
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'Our model is a simple sequential model, defined using Keras layers:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型是一个简单的顺序模型，使用Keras层定义：
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We apply a dropout, a concept introduced in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml),
    *Modern Neural Networks.* The `dropout` parameter of the LSTM controls how much
    dropout is applied to the input weight matrix. The `recurrent_dropout` parameter
    controls how much dropout is applied to the previous state. Similar to a mask,
    `recurrent_dropout` randomly ignores part of the previous state activations in
    order to avoid overfitting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了一个dropout，这是在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)中介绍的概念，*现代神经网络*。LSTM的`dropout`参数控制输入权重矩阵上应用的dropout量。`recurrent_dropout`参数控制对前一个状态应用的dropout量。与mask类似，`recurrent_dropout`会随机忽略部分前一状态的激活值，以避免过拟合。
- en: The very first layer of our model is a `Masking` layer. As we padded our image
    sequences with empty frames in order to batch them, our LSTM cell would needlessly
    iterate over those added frames. Adding the `Masking` layer ensures the LSTM layer
    stops at the actual end of the sequence, before it encounters a zero matrix.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的第一层是一个`Masking`层。由于我们将图像序列填充了空帧以便批处理，因此我们的LSTM单元会不必要地迭代这些添加的帧。添加`Masking`层可以确保LSTM层在遇到零矩阵之前停止在实际的序列末尾。
- en: The model will categorize videos in 101 categories, such as *kayaking*, *rafting,*
    or *fencing*. However, it will only predict a vector representing the predictions.
    We need a way to convert those 101 categories into vector form. We will use a
    technique called **one-hot encoding**, described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*. Since we have 101 different labels, we
    will return a vector of size 101\. For *kayaking*, the vector will be full of
    zeros except for the first item, which is set to *1*. For *rafting*, it will be
    *0* except for the second element, which is set to *1*, and so on for the other
    categories.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型将把视频分为101个类别，比如*kayaking*、*rafting*或*fencing*。然而，它只会预测一个表示预测的向量。我们需要一种方法将这101个类别转换成向量形式。我们将使用一种叫做**独热编码**的技术，详细说明见[第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)，*计算机视觉与神经网络*。由于我们有101个不同的标签，我们将返回一个大小为101的向量。对于*kayaking*，该向量将除了第一个元素外全为零，第一个元素设置为*1*。对于*rafting*，除了第二个元素外其余全为*0*，第二个元素设置为*1*，其他类别以此类推。
- en: Loading the data
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'We will load the `.npy` files that are produced when generating frame features
    using a generator. The code ensures that all the input sequences have the same
    length, padding them with zeros if necessary:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用生成器加载生成帧特征时产生的`.npy`文件。代码确保所有输入序列具有相同的长度，必要时会用零进行填充：
- en: '[PRE6]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the previous code, we defined a Python **closure function**—a function that
    returns another function. This technique allows us to create `train_dataset`,
    returning training data, and `validation_dataset`, returning validation data with
    just one generator function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们定义了一个Python **闭包函数**——一个返回另一个函数的函数。这个技术使我们能够通过一个生成器函数创建`train_dataset`（返回训练数据）和`validation_dataset`（返回验证数据）：
- en: '[PRE7]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We also batch and prefetch the data according to the best practices that were
    described in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), **Training
    on Complex and Scarce Datasets**.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还根据[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)中描述的最佳实践，对数据进行批处理和预取，**在复杂和稀缺数据集上训练**。
- en: Training the model
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: The training procedure is very similar to those previously described in the
    book, and we invite the readers to refer to the notebook attached to this chapter.
    Using the model described previously, we reach a precision level of 72% on the
    validation set.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程与书中先前描述的非常相似，我们邀请读者参考本章附带的笔记本。使用之前描述的模型，我们在验证集上达到了72%的精度。
- en: This result can be compared to the state-of-the-art precision level of 94%,
    which is obtained when using more advanced techniques. Our simple model could
    be enhanced by improving frame sampling, using data augmentation, using a different
    sequence length, or by optimizing the size of the layers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果可以与使用更先进技术时获得的94%的最先进精度水平进行比较。我们的简单模型可以通过改善帧采样、使用数据增强、采用不同的序列长度，或通过优化层的大小来增强。
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We expanded our knowledge of neural networks by describing the general principles
    of RNNs. After covering the inner workings of the basic RNN, we extended backpropagation
    to apply it to recurrent networks. As presented in this chapter, BPTT suffers
    from gradient vanishing when applied to RNNs. This can be worked around by using
    truncated backpropagation, or by using a different type of architecture—LSTM networks.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过描述RNN的基本原理扩展了对神经网络的理解。在介绍了基本RNN的内部工作原理后，我们将反向传播扩展到递归网络的应用。正如本章所介绍的，当应用于RNN时，BPTT会遭遇梯度消失问题。可以通过使用截断反向传播，或采用不同类型的架构——LSTM网络来解决这个问题。
- en: We applied those theoretical principles to a practical problem—action recognition
    in videos. By combining CNNs and LSTMs, we successfully trained a network to classify
    videos in 101 categories, introducing video-specific techniques such as frame
    sampling and padding.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些理论原则应用于一个实际问题——视频中的动作识别。通过结合CNN和LSTM，我们成功地训练了一个网络，将视频分类为101个类别，并引入了视频特有的技术，如帧采样和填充。
- en: In the next chapter, we will broaden our knowledge of neural network applications
    by covering new platforms—mobile devices and web browsers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过介绍新平台——移动设备和网页浏览器，扩展我们对神经网络应用的了解。
- en: Questions
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the main advantages of LSTMs over the simple RNN architecture?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM相较于简单RNN架构的主要优势是什么？
- en: What is the CNN used for when it is applied before the LSTM?
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当CNN应用于LSTM之前，它的用途是什么？
- en: What is gradient vanishing and why does it occur? Why is it a problem?
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是梯度消失，为什么会发生？它为什么是个问题？
- en: What are some of the workarounds for gradient vanishing?
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 梯度消失的一些解决方法有哪些？
- en: Further reading
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*RNNs with Python Quick Start Guide* ([https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide](https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide)),
    by Simeon Kostadinov: This book details RNN architectures, and applies them to
    examples using TensorFlow 1.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Python快速入门指南中的RNN* ([https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide](https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide))，作者：Simeon
    Kostadinov：本书详细介绍了RNN架构，并通过使用TensorFlow 1的示例进行应用。'
- en: '*A Critical Review of RNNs for Sequence Learning* ([https://arxiv.org/abs/1506.00019](https://arxiv.org/abs/1506.00019)),
    by Zachary C. Lipton et al.: This survey reviews and synthesizes three decades
    of RNN architectures.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RNN在序列学习中的批判性回顾* ([https://arxiv.org/abs/1506.00019](https://arxiv.org/abs/1506.00019))，作者：Zachary
    C. Lipton等人：本文综述并综合了三十年的RNN架构。'
- en: '*Empirical Evaluation of Gated RNNs on Sequence Modeling* ([https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)),
    by Junyoung Chung et al.: This paper compares the performance of different RNN
    architectures.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*门控RNN在序列建模中的经验评估* ([https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555))，作者：Junyoung
    Chung等人：本文比较了不同RNN架构的性能。'
