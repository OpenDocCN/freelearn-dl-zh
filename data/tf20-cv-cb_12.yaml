- en: '*Chapter 12*: Boosting Performance'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：提升性能'
- en: More often than not, the leap between good and great doesn't involve drastic
    changes, but instead subtle tweaks and fine-tuning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 更多时候，从好到优秀的跃升并不涉及剧烈的变化，而是细微的调整和微调。
- en: It is often said that 20% of the effort can get you 80% of the results (this
    is known as the **Pareto principle**). But what about that gap between 80% and
    100%? What do we need to do to exceed expectations, to improve our solutions,
    to squeeze as much performance out of our computer vision algorithms as possible?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常说，20%的努力可以带来80%的成果（这就是**帕累托原则**）。但是80%和100%之间的差距呢？我们需要做什么才能超越预期，改进我们的解决方案，最大限度地提升计算机视觉算法的性能？
- en: Well, as with all things deep learning, the answer is a mixture of art and science.
    The good news is that in this chapter, we'll focus on simple tools you can use
    to boost the performance of your neural networks!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，和所有深度学习相关的事情一样，答案是艺术与科学的结合。好消息是，本章将专注于一些简单的工具，你可以用它们来提升神经网络的性能！
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下方法：
- en: Using convolutional neural network ensembles to improve accuracy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络集成来提高准确性
- en: Using test time augmentation to improve accuracy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试时数据增强来提高准确性
- en: Using rank-N accuracy to evaluate performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用排名-N准确率来评估性能
- en: Using label smoothing to increase performance
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标签平滑提高性能
- en: Checkpointing models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点模型
- en: Customizing the training process using `tf.GradientTape`
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tf.GradientTape`自定义训练过程
- en: Visualizing class activation maps to better understand your network
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化类激活图以更好地理解你的网络
- en: Let's get started!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'As usual, you''ll get the most out of these recipes if you can access a GPU,
    given that some of the examples in this chapter are quite resource-intensive.
    Also, if there are any preparatory steps you''ll need to perform in order to complete
    a recipe, you''ll find them in the *Getting ready* sections provided. As a last
    remark, the code for this chapter is available in the companion repository on
    GitHub: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，如果你能使用 GPU，你将能从这些方法中获得最大收益，因为本章中的某些示例对资源的需求相当高。此外，如果有任何你需要执行的准备工作以完成某个方法，你会在*准备工作*部分找到相关内容。最后，关于本章的代码可以在
    GitHub 的附带仓库中找到：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch12)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看代码实战视频：
- en: '[https://bit.ly/2Ko3H3K](https://bit.ly/2Ko3H3K).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/2Ko3H3K](https://bit.ly/2Ko3H3K)。'
- en: Using convolutional neural network ensembles to improve accuracy
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络集成来提高准确性
- en: In machine learning, one of the most robust classifiers is, in fact, a meta-classifier,
    known as an ensemble. An ensemble is comprised of what's known as weak classifiers,
    predictive models just a tad better than random guessing. However, when combined,
    they result in a rather robust algorithm, especially against high variance (overfitting).
    Some of the most famous examples of ensembles we may encounter include Random
    Forest and Gradient Boosting Machines.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，最强大的分类器之一，实际上是一个元分类器，叫做集成。集成由所谓的弱分类器组成，弱分类器是指比随机猜测稍微好一点的预测模型。然而，当它们结合在一起时，结果是一个相当强大的算法，特别是在面对高方差（过拟合）时。我们可能遇到的一些最著名的集成方法包括随机森林和梯度提升机。
- en: The good news is that we can leverage the same principle when it comes to neural
    networks, thus creating a whole that's more than the sum of its parts. Do you
    want to learn how? Keep reading!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，当涉及到神经网络时，我们可以利用相同的原理，从而创造出一个整体效果，超越单独部分的总和。你想知道怎么做吗？继续阅读吧！
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'This recipe depends on `Pillow` and `tensorflow_docs`, which can be easily
    installed like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本方法依赖于`Pillow`和`tensorflow_docs`，可以通过以下方式轻松安装：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''ll also be using the famous `Caltech 101` dataset, available here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to your preferred location.
    For the purposes of this recipe, we''ll place it in `~/.keras/datasets/101_ObjectCategories`.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用著名的`Caltech 101`数据集，可以在这里找到：[http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)。下载并解压`101_ObjectCategories.tar.gz`到你选择的位置。为了这个配方，我们将它放在`~/.keras/datasets/101_ObjectCategories`中。
- en: 'The following are some sample images:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例图像：
- en: '![Figure 12.1 – Caltech 101 sample images'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – Caltech 101样本图像](img/B14768_12_001.jpg)'
- en: '](img/B14768_12_001.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_001.jpg)'
- en: Figure 12.1 – Caltech 101 sample images
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – Caltech 101样本图像
- en: Let's start this recipe, shall we?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个配方吧？
- en: How to do it…
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps to create an ensemble of **Convolutional Neural Networks**
    (**CNNs**):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建**卷积神经网络**（**CNN**）的集成：
- en: 'Import all the required modules:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必需的模块：
- en: '[PRE1]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `load_images_and_labels()` function, which reads the images and
    categories of the `Caltech 101` dataset and returns them as NumPy arrays:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`load_images_and_labels()`函数，读取`Caltech 101`数据集中的图像和类别，并将它们作为NumPy数组返回：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the `build_model()` function, which is in charge of building a VGG-like
    convolutional neural network:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_model()`函数，负责构建一个类似VGG的卷积神经网络：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `plot_model_history()` function, which we''ll use to plot the training
    and validation curves of the networks in the ensemble:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`plot_model_history()`函数，我们将用它来绘制集成中各个网络的训练和验证曲线：
- en: '[PRE5]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To enhance reproducibility, set a random seed:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高可复现性，设置一个随机种子：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Compile the paths to the images of `Caltech 101`, as well as the classes:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译`Caltech 101`图像的路径以及类别：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像和标签，同时对图像进行归一化，并对标签进行独热编码：
- en: '[PRE8]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Reserve 20% of the data for test purposes and use the rest to train the models:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留20%的数据用于测试，其余用于训练模型：
- en: '[PRE9]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the batch size, the number of epochs, and the number of batches per
    epoch:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批次大小、训练轮次以及每个轮次的批次数：
- en: '[PRE10]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We''ll use data augmentation here to perform a series of random transformations,
    such as horizontal flipping, rotations, and zooming:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在这里使用数据增强，执行一系列随机变换，如水平翻转、旋转和缩放：
- en: '[PRE11]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our ensemble will be comprised of `5` models. We''ll save the predictions of
    each network in the ensemble in the `ensemble_preds` list:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的集成将包含`5`个模型。我们会将每个网络在集成中的预测保存到`ensemble_preds`列表中：
- en: '[PRE12]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We''ll train each model in a similar fashion. We''ll start by creating and
    compiling the network itself:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将以类似的方式训练每个模型。首先创建并编译网络本身：
- en: '[PRE13]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we''ll fit the model using data augmentation:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用数据增强来拟合模型：
- en: '[PRE14]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Compute the accuracy of the model on the test set, plot its training and validation
    accuracy curves, and store its predictions in `ensemble_preds`:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在测试集上的准确率，绘制训练和验证准确率曲线，并将其预测结果存储在`ensemble_preds`中：
- en: '[PRE15]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The last step consists of averaging the predictions of each member of the ensemble,
    effectively producing a joint prediction for the whole meta-classifier, and then
    computing the accuracy on the test set:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是对每个集成成员的预测进行平均，从而有效地为整个元分类器产生联合预测，然后计算测试集上的准确率：
- en: '[PRE16]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Because we are training five networks, this program can take a while to complete.
    When it does, you should see accuracies similar to the following for each member
    of the ensemble:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为我们训练的是五个网络，所以这个程序可能需要一段时间才能完成。完成后，你应该能看到每个集成网络成员的准确率类似于以下内容：
- en: '[PRE17]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here, we can observe the accuracy ranges between 65% and 67.5%. The following
    figure shows the training and validation curves for models 1 to 5 (from left to
    right, models 1, 2, and 3 on the top row; models 4 and 5 on the bottom row):'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到准确率在65%到67.5%之间。下图展示了模型1到5的训练和验证曲线（从左到右，上排为模型1、2、3，下排为模型4、5）：
- en: '![Figure 12.2 – Curves for the training and validation accuracy for the five
    models in the ensemble'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.2 – 五个模型在集成中的训练和验证准确率曲线](img/B14768_12_002.jpg)'
- en: '](img/B14768_12_002.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_002.jpg)'
- en: Figure 12.2 – Curves for the training and validation accuracy for the five models
    in the ensemble
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 五个模型在集成中的训练和验证准确率曲线
- en: 'However, the most interesting result is the accuracy of the ensemble, which
    is the result of averaging the predictions of each model:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最有趣的结果是集成模型的准确性，这是通过平均每个模型的预测结果得出的：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Truly impressive! Just by combining the predictions of the five networks, we
    bumped our accuracy all the way to 72.2%, on a very challenging dataset – `Caltech
    101`! Let's discuss this a bit further in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 真是令人印象深刻！仅仅通过结合五个网络的预测，我们就把准确率提升到了72.2%，并且是在一个非常具有挑战性的数据集——`Caltech 101`上！我们将在下一部分进一步讨论这一点。
- en: How it works…
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we leveraged the power of ensembles by training five neural
    networks on the challenging `Caltech 101` dataset. It must be noted that our process
    was pretty straightforward and unremarkable. We started by loading and shaping
    the data in a format suitable for training and then using the same template to
    train several copies of a VGG-inspired architecture.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们通过在具有挑战性的`Caltech 101`数据集上训练五个神经网络，利用了集成的力量。必须指出的是，我们的过程相当简单而不起眼。我们首先加载并整理数据，使其适合训练，然后使用相同的模板训练多个VGG风格的架构副本。
- en: To create more robust classifiers, we used data augmentation and trained each
    network for 40 epochs. Besides these details, we didn't change the architecture
    of the networks, nor did we tweak each particular member. The result is that each
    model was between 65% and 67% accurate on the test set. However, when combined,
    they reached a decent 72%!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建更强大的分类器，我们使用了数据增强，并对每个网络进行了40个周期的训练。除了这些细节之外，我们没有改变网络的架构，也没有调整每个特定成员。结果是，每个模型在测试集上的准确率介于65%和67%之间。然而，当它们结合起来时，达到了一个不错的72%！
- en: Why did this happen, though? The rationale behind ensemble learning is that
    each model develops its own biases during the training process, which is a consequence
    of the stochastic nature of deep learning. However, when combining their decisions
    through a voting process (which is basically what averaging their predictions
    does), these differences smooth out and give far more robust results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么会发生这种情况呢？集成学习的原理是每个模型在训练过程中都会形成自己的偏差，这是深度学习随机性质的结果。然而，通过投票过程（基本上就是平均它们的预测结果）结合它们的决策时，这些差异会被平滑掉，从而给出更强健的结果。
- en: Of course, training several models is a resource-intensive task, and depending
    on the size and complexity of the problem, it might be outright impossible to
    do so. Nevertheless, it's a very useful tool that can boost your predicting power
    just by creating and combining multiple copies of the same network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，训练多个模型是一项资源密集型任务，根据问题的规模和复杂性，这可能完全不可能实现。然而，这仍然是一个非常有用的工具，通过创建并结合多个相同网络的副本，可以提高预测能力。
- en: Not bad, huh?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不错吧？
- en: See also
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'If you want to understand the mathematical basis behind ensembles, read this
    article about **Jensen''s Inequality**: [https://en.wikipedia.org/wiki/Jensen%27s_inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解集成背后的数学原理，可以阅读这篇关于**詹森不等式**的文章：[https://en.wikipedia.org/wiki/Jensen%27s_inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality)。
- en: Using test time augmentation to improve accuracy
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试时增强提高准确性
- en: Most of the time, when we're testing the predictive power of a network, we use
    a test set to do so. This test set is comprised of images the model has never
    seen. Then, we present them to the model and ask it what class each belongs to.
    The thing is… we do it *once*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，当我们测试一个网络的预测能力时，我们会使用一个测试集。这个测试集包含了模型从未见过的图像。然后，我们将它们呈现给模型，并询问模型每个图像属于哪个类别。问题是……我们只做了*一次*。
- en: What if we were more forgiving and gave the model multiple chances to do this?
    Would its accuracy improve? Well, more often than not, it does!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更宽容一点，给模型多次机会去做这个任务呢？它的准确性会提高吗？嗯，往往会提高！
- en: This technique is known as **Test Time Augmentation** (**TTA**), and it's the
    focus of this recipe.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术被称为**测试时增强**（**TTA**），它是本教程的重点。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In order to load the images in the dataset, we need `Pillow`. Install it using
    the following command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载数据集中的图像，我们需要`Pillow`。可以使用以下命令安装：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, download the `Caltech 101` dataset, which is available here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to a location of your choosing.
    For the rest of this recipe, we''ll work under the assumption that the dataset
    is in `~/.keras/datasets/101_ObjectCategories`.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，下载`Caltech 101`数据集，地址如下：[http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)。下载并解压`101_ObjectCategories.tar.gz`到你选择的位置。在本食谱的其余部分中，我们将假设数据集位于`~/.keras/datasets/101_ObjectCategories`。
- en: 'Here''s a sample of what you can find inside `Caltech 101`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`Caltech 101`中可以找到的一个示例：
- en: '![Figure 12.3 – Caltech 101 sample images'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – Caltech 101样本图像]'
- en: '](img/B14768_12_003.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_003.jpg)'
- en: Figure 12.3 – Caltech 101 sample images
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – Caltech 101样本图像
- en: We are ready to begin!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备开始了！
- en: How to do it…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Follow these steps to learn the benefits of TTA:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤学习TTA的好处：
- en: 'Import the dependencies we need:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们需要的依赖项：
- en: '[PRE20]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define the `load_images_and_labels()` function in order to read the data from
    `Caltech 101` (in NumPy format):'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`load_images_and_labels()`函数，以便从`Caltech 101`（NumPy格式）中读取数据：
- en: '[PRE21]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the `build_model()` function, which returns a network based on the famous
    **VGG** architecture:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_model()`函数，该函数基于著名的**VGG**架构返回一个网络：
- en: '[PRE22]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE23]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `flip_augment()` function is the basis of our **TTA** scheme. It takes
    an image and produces copies of it that can be randomly flipped (horizontally)
    with a 50% probability:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`flip_augment()`函数是我们**TTA**方案的基础。它接收一张图像，并生成其副本，这些副本可以随机水平翻转（50%的概率）：'
- en: '[PRE24]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To ensure reproducibility, set a random seed:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保可重复性，请设置随机种子：
- en: '[PRE25]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译`Caltech 101`图像的路径及其类别：
- en: '[PRE26]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像和标签，同时对图像进行标准化，并对标签进行独热编码：
- en: '[PRE27]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用20%的数据进行测试，剩余部分用于训练模型：
- en: '[PRE28]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the batch size and the number of epochs:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批次大小和周期数：
- en: '[PRE29]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We''ll randomly horizontally flip the images in the train set:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将随机地水平翻转训练集中的图像：
- en: '[PRE30]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Build and compile the network:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译网络：
- en: '[PRE31]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Fit the model:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE32]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Make predictions on the test set and use them to compute the accuracy of the
    model:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集进行预测，并利用预测结果计算模型的准确性：
- en: '[PRE33]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we''ll use **TTA** on the test set. We''ll store the predictions for each
    copy of an image in the test set in the predictions list. We''ll create 10 copies
    of each image:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将在测试集上使用**TTA**。我们将把每个图像副本的预测结果存储在预测列表中。我们将创建每个图像的10个副本：
- en: '[PRE34]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we will iterate over each image of the test set, creating a batch of
    copies of it and passing it through the model:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对测试集中的每个图像进行迭代，创建其副本的批次，并将其传递通过模型：
- en: '[PRE35]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The final prediction of each image will be the most predicted class in the
    batch of copies:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每张图像的最终预测将是该批次中预测最多的类别：
- en: '[PRE36]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, we will compute the accuracy on the predictions made by the model
    using TTA:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用TTA计算模型预测的准确性：
- en: '[PRE37]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After a while, we''ll see results similar to these:'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稍等片刻，我们将看到类似于这些的结果：
- en: '[PRE38]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The network achieves an accuracy of 64.4% without TTA, while it increases to
    65.3% if we give the model more chances to generate correct predictions. Cool,
    right?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在没有TTA的情况下达到64.4%的准确率，而如果我们给模型更多机会生成正确预测，准确率将提高到65.3%。很酷，对吧？
- en: Let's move on to the *How it works…* section.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入*如何工作……*部分。
- en: How it works…
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作……
- en: In this recipe, we learned that **test time augmentation** is a simple technique
    that entails only a few changes once the network has been trained. The reasoning
    behind this is that if we present the network with copies of images in the test
    set that have been altered in a similar way to the ones it saw during training,
    the network should do better.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们学习到**测试时增强**是一种简单的技术，一旦网络训练完成，只需要做少量修改。其背后的原因是，如果我们在测试集中给网络呈现一些与训练时图像相似的变化图像，网络的表现会更好。
- en: However, the key is that these transformations, which are done during the evaluation
    phase, should match the ones that were done during the training period; otherwise,
    we would be feeding the model incongruent data!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，关键在于，这些变换应当在评估阶段进行，且应该与训练期间的变换相匹配；否则，我们将向模型输入不一致的数据！
- en: 'There''s a caveat, though: TTA is really, really slow! After all, we are multiplying
    the size of the test set by the augmentation factor, which in our case was 10\.
    This means that instead of evaluating one image at a time, the network must process
    10 instead.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有一个警告：TTA 实际上非常非常慢！毕竟，我们是在将测试集的大小乘以增强因子，在我们的例子中是 10。这意味着网络不再一次处理一张图像，而是必须处理
    10 张图像。
- en: Of course, TTA is not suitable for real-time or speed-constrained applications,
    but it can be useful when time or speed are not an issue.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，TTA 不适合实时或速度受限的应用，但当时间或速度不是问题时，它仍然可以非常有用。
- en: Using rank-N accuracy to evaluate performance
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 rank-N 准确度来评估性能
- en: Most of the time, when we're training deep learning-based image classifiers,
    we care about the accuracy, which is a binary measure of a model's performance,
    based on a one-on-one comparison between its predictions and the ground-truth
    labels. When the model says there's a *leopard* in a photo, is there actually
    a *leopard* there? In other words, we measure how *precise* the model is.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，当我们训练基于深度学习的图像分类器时，我们关心的是准确度，它是模型性能的一个二元度量，基于模型的预测与真实标签之间的一对一比较。当模型说照片中有一只
    *豹* 时，照片中真的有 *豹* 吗？换句话说，我们衡量的是模型的 *精确度*。
- en: However, for more complex datasets, this way of assessing a network's learning
    might be counterproductive and even unfair, because it's too restrictive. What
    if the model didn't classify the feline in the picture as a *leopard* but as a
    *tiger*? Moreover, what if the second most probable class was, indeed, a *leopard*?
    This means the model has some more learning to do, but it's getting there! That's
    valuable!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于更复杂的数据集，这种评估网络学习的方法可能适得其反，甚至不公平，因为它过于限制。假如模型没有将图片中的猫科动物识别为 *豹*，而是误识别为 *老虎*
    呢？更重要的是，如果第二可能性最大的类别确实是 *豹* 呢？这意味着模型还需要进一步学习，但它正在逐步接近目标！这很有价值！
- en: This is the reasoning behind **rank-N accuracy**, a more lenient and fairer
    way of measuring a predictive model's performance, which counts a prediction as
    correct if the ground-truth label is in the top-N most probable classes output
    by the model. In this recipe, we'll learn how to implement it and use it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 **rank-N 准确度** 的原理，它是一种更宽容、更公平的评估预测模型性能的方法，当真实标签出现在模型输出的前 N 个最可能类别中时，该预测会被视为正确。在本教程中，我们将学习如何实现并使用这种方法。
- en: Let's get started.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Install `Pillow`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `Pillow`：
- en: '[PRE39]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, download and unzip the `Caltech 101` dataset, which is available here:
    [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Make sure to click on the `101_ObjectCategories.tar.gz` file. Once downloaded,
    place it in a location of your choosing. For the rest of this recipe, we''ll work
    under the assumption that the dataset is in `~/.keras/datasets/101_ObjectCategories`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载并解压 `Caltech 101` 数据集，数据集可以在这里找到：[http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)。确保点击下载
    `101_ObjectCategories.tar.gz` 文件。下载后，将其放在你选择的位置。在本教程的后续部分，我们将假设数据集位于 `~/.keras/datasets/101_ObjectCategories`
    目录下。
- en: 'Here''s a sample of `Caltech 101`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `Caltech 101` 的一个样本：
- en: '![Figure 12.4 – Caltech 101 sample images'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.4 – Caltech 101 样本图像'
- en: '](img/B14768_12_004.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_004.jpg)'
- en: Figure 12.4 – Caltech 101 sample images
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 – Caltech 101 样本图像
- en: Let's implement this recipe!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始实现这个教程吧！
- en: How to do it…
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to implement and use **rank-N accuracy**:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤实现并使用 **rank-N 准确度**：
- en: 'Import the necessary modules:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块：
- en: '[PRE40]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define the `load_images_and_labels()` function in order to read the data from
    `Caltech 101`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `load_images_and_labels()` 函数，用于从 `Caltech 101` 读取数据：
- en: '[PRE41]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the `build_model()` function to create a **VGG**-inspired network:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `build_model()` 函数，创建一个 **VGG** 风格的网络：
- en: '[PRE42]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE43]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the `rank_n()` function, which computes the **rank-N accuracy** based
    on the predictions and ground-truth labels. Notice that it produces a value between
    0 and 1, where a "hit" or correct prediction is accounted for when the ground-truth
    label is in the N most probable categories:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `rank_n()` 函数，它根据预测结果和真实标签计算 **rank-N 准确度**。请注意，它会输出一个介于 0 和 1 之间的值，当真实标签出现在模型输出的前
    N 个最可能的类别中时，才会算作“命中”或正确预测：
- en: '[PRE44]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For the sake of reproducibility, set a random seed:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可复现性，设置随机种子：
- en: '[PRE45]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译 `Caltech 101` 的图像路径，以及它的类别：
- en: '[PRE46]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像和标签，同时对图像进行归一化处理并对标签进行独热编码：
- en: '[PRE47]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将20%的数据用于测试，剩下的用于训练模型：
- en: '[PRE48]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the batch size and the number of epochs:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批次大小和训练轮数：
- en: '[PRE49]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define an `ImageDataGenerator()` to augment the images in the training set
    with random flips, rotations, and other transformations:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`ImageDataGenerator()`，以随机翻转、旋转和其他转换来增强训练集中的图像：
- en: '[PRE50]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Build and compile the network:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译网络：
- en: '[PRE51]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Fit the model:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE52]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Make predictions on the test set:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测：
- en: '[PRE53]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Compute rank-1 (regular accuracy), rank-3, rank-5, and rank-10 accuracies:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算排名-1（常规准确率）、排名-3、排名-5和排名-10的准确率：
- en: '[PRE54]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Here are the results:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE55]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Here, we can observe that 64.29% of the time, the network produces an exact
    match. However, 78.05% of the time, the correct prediction is in the top 3, 83.01%
    of the time it's in the top 5, and almost 90% of the time it's in the top 10\.
    These are pretty interesting and encouraging results, considering our dataset
    is comprised of 101 classes that are very different from each other.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到，64.29%的时间，网络产生了完全匹配的结果。然而，78.05%的时间，正确的预测出现在前3名，83.01%的时间出现在前5名，几乎90%的时间出现在前10名。这些结果相当有趣且令人鼓舞，考虑到我们的数据集包含了101个彼此差异很大的类别。
- en: We'll dig deeper in the *How it works…* section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*如何工作...*部分深入探讨。
- en: How it works…
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作…
- en: In this recipe, we learned about the existence and utility of rank-N accuracy.
    We also implemented it with a simple function, `rank_n()`, which we then tested
    on a network that had been trained on the challenging `Caltech-101` dataset.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们了解了排名-N准确率的存在和实用性。我们还通过一个简单的函数`rank_n()`实现了它，并在一个已经训练过具有挑战性的`Caltech-101`数据集的网络上进行了测试。
- en: Rank-N, particularly the rank-1 and rank-5 accuracies, are common in the literature
    of networks that have been trained on massive, challenging datasets, such as COCO
    or ImageNet, where even humans have a hard time discerning between categories.
    It is particularly useful when we have fine-grained classes that share a common
    parent or ancestor, such as *Pug* and *Golden Retriever*, both being *Dog* breeds.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 排名-N，尤其是排名-1和排名-5的准确率，在训练过大型且具有挑战性数据集的网络文献中很常见，例如COCO或ImageNet，这些数据集即使人类也很难区分不同类别。它在我们有细粒度类别且这些类别共享一个共同的父类或祖先时尤为有用，例如*巴哥犬*和*金毛猎犬*，它们都是*狗*的品种。
- en: The reason why rank-N is meaningful is a well-trained model that has truly learned
    to generalize will produce contextually similar classes in its top-N predictions
    (typically, the top 5).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 排名-N之所以有意义，是因为一个训练良好的模型，能够真正学会泛化，将在其前N个预测中产生语境上相似的类别（通常是前5名）。
- en: Of course, we can take rank-N accuracy too far, to the point where it loses
    its meaning and utility. For instance, a rank-5 accuracy on `MNIST`, a dataset
    comprised of 10 categories, would be almost useless.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们也可以把排名-N准确率使用得过头，直到它失去意义和实用性。例如，在`MNIST`数据集上进行排名-5准确率评估，该数据集仅包含10个类别，这几乎是没有意义的。
- en: See also
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Want to see rank-N being used in the wild? Take a look at the results section
    of this paper: [https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 想看到排名-N在实际应用中的效果吗？请查看这篇论文的结果部分：[https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)。
- en: Using label smoothing to increase performance
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用标签平滑提高性能
- en: One of the constant battles we have to fight against in machine learning is
    overfitting. There are many techniques we can use to prevent a model from losing
    generalization power, such as dropout, L1 and L2 regularization, and even data
    augmentation. A recent addition to this group is **label smoothing**, a more forgiving
    alternative to one-hot encoding.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中我们必须不断应对的一个常见问题是过拟合。我们可以使用多种技术来防止模型丧失泛化能力，例如dropout、L1和L2正则化，甚至数据增强。最近加入这个组的一个新技术是**标签平滑**，它是独热编码的一个更宽容的替代方案。
- en: Whereas in one-hot encoding we represent each category as a binary vector where
    the only non-zero element corresponds to the class that's been encoded, with **label
    smoothing**, we represent each label as a probability distribution where all the
    elements have a non-zero probability. The one with the highest probability, of
    course, is the one that corresponds to the encoded class.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 而在独热编码中，我们通过二进制向量表示每个类别，其中唯一非零元素对应被编码的类别，使用**标签平滑**时，我们将每个标签表示为一个概率分布，其中所有元素都有非零的概率。最高概率对应的类别，当然是与编码类别相符的。
- en: For instance, a smoothed version of the *[0, 1, 0]* vector would be *[0.01,
    0.98, 0.01]*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，平滑后的* [0, 1, 0] *向量会变成* [0.01, 0.98, 0.01] *。
- en: In this recipe, we'll learn how to use **label smoothing**. Keep reading!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将学习如何使用**标签平滑**。继续阅读！
- en: Getting ready
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Install `Pillow`, which we''ll need to manipulate the images in the dataset:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`Pillow`，我们需要它来处理数据集中的图像：
- en: '[PRE56]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Head to the `Caltech 101` website: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and unzip the file named `101_ObjectCategories.tar.gz` in a location
    of your preference. From now on, we''ll assume the data is in `~/.keras/datasets/101_ObjectCategories`.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 访问`Caltech 101`官网：[http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/)。下载并解压名为`101_ObjectCategories.tar.gz`的文件到你喜欢的目录。从现在开始，我们假设数据位于`~/.keras/datasets/101_ObjectCategories`。
- en: 'Here''s a sample from `Caltech 101`:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是`Caltech 101`的一个样本：
- en: '![Figure 12.5 – Caltech 101 sample images'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5 – Caltech 101样本图像'
- en: '](img/B14768_12_005.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_005.jpg)'
- en: Figure 12.5 – Caltech 101 sample images
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – Caltech 101样本图像
- en: Let's begin!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成这个教程：
- en: 'Import the necessary dependencies:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE57]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Create the `load_images_and_labels()` function in order to read the data from
    `Caltech 101`:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`load_images_and_labels()`函数，用来从`Caltech 101`读取数据：
- en: '[PRE58]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Implement the `build_model()` function to create a **VGG**-based network:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`build_model()`函数，创建一个基于**VGG**的网络：
- en: '[PRE59]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE60]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Set a random seed to enhance reproducibility:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个随机种子以增强可复现性：
- en: '[PRE61]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Compile the paths to the images of `Caltech 101`, as well as its classes:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译`Caltech 101`的图像路径以及其类别：
- en: '[PRE62]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Load the images and labels while normalizing the images and one-hot encoding
    the labels:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像和标签，同时对图像进行归一化，并对标签进行独热编码：
- en: '[PRE63]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Use 20% of the data for test purposes and leave the rest to train the models:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用20%的数据作为测试数据，其余数据用于训练模型：
- en: '[PRE64]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Define the batch size and the number of epochs:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批次大小和训练轮次：
- en: '[PRE65]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define an `ImageDataGenerator()` to augment the images in the training set
    with random flips, rotations, and other transformations:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`ImageDataGenerator()`来通过随机翻转、旋转和其他变换增强训练集中的图像：
- en: '[PRE66]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We''ll train two models: one with and an other without **label smoothing**.
    This will allow us to compare their performance and assess whether **label smoothing**
    has an impact on performance. The logic is pretty much the same in both cases,
    starting with the model creation process:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将训练两个模型：一个使用**标签平滑**，另一个不使用。这将让我们比较它们的表现，评估**标签平滑**是否对性能有影响。两个案例的逻辑几乎相同，从模型创建过程开始：
- en: '[PRE67]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'If `with_label_smoothing` is `True`, then we''ll set the smoothing factor to
    0.1\. Otherwise, the factor will be 0, which implies we''ll use regular one-hot
    encoding:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`with_label_smoothing`为`True`，则我们将平滑因子设置为0.1。否则，平滑因子为0，这意味着我们将使用常规的独热编码：
- en: '[PRE68]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We apply `loss` function – in this case, `CategoricalCrossentropy()`:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应用`损失`函数——在这种情况下是`CategoricalCrossentropy()`：
- en: '[PRE69]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Compile and fit the model:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并训练模型：
- en: '[PRE70]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Make predictions on the test set and compute the accuracy:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测并计算准确率：
- en: '[PRE71]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The script will train two models: one without `loss` function. Here are the
    results:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 脚本将训练两个模型：一个不使用`损失`函数。以下是结果：
- en: '[PRE72]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Just by using **label smoothing**, we improved our test score by almost 0.7%,
    a non-negligible boost considering the size of our dataset and its complexity.
    We'll dive deeper in the next section.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅通过使用**标签平滑**，我们提高了测试分数接近0.7%，这是一个不可忽视的提升，考虑到我们的数据集大小和复杂性。在下一部分我们将深入探讨。
- en: How it works…
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned how to apply `CategoricalCrossentropy()` loss function,
    which is used to measure the network's learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们学习了如何应用`CategoricalCrossentropy()`损失函数，用于衡量网络的学习效果。
- en: Why does label smoothing work, though? Despite its widespread use in many areas
    of deep learning, including **Natural Language Processing** (**NLP**) and, of
    course, **computer vision**, **label smoothing** is still poorly understood. However,
    what many have observed (including ourselves, in this example) is that by softening
    the targets, the generalization and learning speed of a network often improves
    significantly, preventing it from becoming overconfident, thus shielding us against
    the harmful effects of overfitting.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么标签平滑有效呢？尽管在许多深度学习领域中被广泛应用，包括**自然语言处理**（**NLP**）和当然的**计算机视觉**，**标签平滑**仍然没有被完全理解。然而，许多人（包括我们在这个示例中的研究者）观察到，通过软化目标，网络的泛化能力和学习速度通常显著提高，防止其过于自信，从而保护我们免受过拟合的负面影响。
- en: For a very interesting insight into **label smoothing**, read the paper mentioned
    in the *See also* section.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关**标签平滑**的有趣见解，请阅读*另见*部分提到的论文。
- en: See also
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 另见
- en: 'This paper explores the reasons why **label smoothing** helps, as well as when
    it does not. It''s a worthy read! You can download it here: [https://arxiv.org/abs/1906.02629](https://arxiv.org/abs/1906.02629).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文探讨了**标签平滑**有效的原因，以及它何时无效。值得一读！你可以在这里下载：[https://arxiv.org/abs/1906.02629](https://arxiv.org/abs/1906.02629)。
- en: Checkpointing model
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查点模型
- en: Training a deep neural network is an expensive process in terms of time, storage,
    and resources. Retraining a network each time we want to use it is preposterous
    and impractical. The good news is that we can use a mechanism to automatically
    save the best versions of a network during the training process.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个深度神经网络是一个耗时且消耗存储和资源的过程。每次我们想要使用网络时重新训练它是不合理且不切实际的。好消息是，我们可以使用一种机制，在训练过程中自动保存网络的最佳版本。
- en: In this recipe, we'll talk about such a mechanism, known as checkpointing.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将讨论一种机制，称为检查点。
- en: How to do it…
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'Follow these steps to learn about the different modalities of checkpointing
    you have at your disposal in TensorFlow:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤了解您在 TensorFlow 中可以使用的不同检查点方式：
- en: 'Import the modules we will be using:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们将使用的模块：
- en: '[PRE73]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define a function that will load `Fashion-MNIST` into `tf.data.Datasets`:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将`Fashion-MNIST`加载到`tf.data.Datasets`中：
- en: '[PRE74]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Use 20% of the training data to validate the dataset:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 20% 的训练数据来验证数据集：
- en: '[PRE75]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Convert the train, test, and validation subsets into `tf.data.Datasets`:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集、测试集和验证集转换为`tf.data.Datasets`：
- en: '[PRE76]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the `build_network()` method, which, as its name suggests, creates the
    model we''ll train on `Fashion-MNIST`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_network()`方法，顾名思义，它创建我们将在`Fashion-MNIST`上训练的模型：
- en: '[PRE77]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE78]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Define the `train_and_checkpoint()` function, which loads the dataset and then
    builds, compiles, and fits the network, saving the checkpoints according to the
    logic established by the `checkpointer` parameter:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train_and_checkpoint()`函数，它加载数据集，然后根据`checkpointer`参数中设定的逻辑构建、编译并训练网络，同时保存检查点。
- en: '[PRE79]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Define the batch size, the number of epochs to train the model for, and the
    buffer size of each subset of data:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批量大小、训练模型的轮次数以及每个数据子集的缓冲区大小：
- en: '[PRE80]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The first way to generate checkpoints is by just saving a different model after
    each iteration. To do this, we must pass `save_best_only=False` to `ModelCheckpoint()`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成检查点的第一种方式是每次迭代后保存一个不同的模型。为此，我们必须将`save_best_only=False`传递给`ModelCheckpoint()`：
- en: '[PRE81]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Notice that we save all the checkpoints in the `save_all` folder, with the epoch,
    the loss, and the validation loss in the checkpointed model name.
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，我们将所有的检查点保存在`save_all`文件夹中，检查点模型名称中包含了轮次、损失和验证损失。
- en: 'A more efficient way of checkpointing is to just save the best model so far.
    We can achieve this by setting `save_best_only` to `True` in `ModelCheckpoint()`:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种更高效的检查点方式是仅保存迄今为止最好的模型。我们可以通过在`ModelCheckpoint()`中将`save_best_only`设置为`True`来实现：
- en: '[PRE82]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: We'll save the results in the `best_only` directory.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将把结果保存在`best_only`目录中。
- en: 'A leaner way to generate checkpoints is to just save one, corresponding to
    the best model so far, instead of storing each incrementally improved model. To
    achieve this, we can remove any parameters from the checkpoint name:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种更简洁的生成检查点的方式是只保存一个与当前最佳模型相对应的检查点，而不是存储每个逐步改进的模型。为了实现这一点，我们可以从检查点名称中删除任何参数：
- en: '[PRE83]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'After running these three experiments, we can examine each output folder to
    see how many checkpoints were generated. In the first experiment, we saved a model
    after each epoch, as shown in the following screenshot:'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行这三个实验后，我们可以检查每个输出文件夹，看看生成了多少个检查点。在第一个实验中，我们在每个 epoch 后保存了一个模型，如下图所示：
- en: '![Figure 12.6 – Experiment 1 results'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.6 – 实验 1 结果'
- en: '](img/B14768_12_006.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_006.jpg)'
- en: Figure 12.6 – Experiment 1 results
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 – 实验 1 结果
- en: 'The downside of this approach is that we end up with a lot of useless snapshots.
    The upside is that, if we want, we can resume training from any epoch by loading
    the corresponding epoch. A better approach is to save only the best model so far,
    which, as the following screenshot shows, produces fewer models. By inspecting
    the checkpoint names, we can see that each one has a validation loss that''s lower
    than the one before it:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，我们会得到很多无用的快照。优点是，如果需要的话，我们可以通过加载相应的 epoch 恢复训练。更好的方法是只保存到目前为止最好的模型，正如以下截图所示，这样生成的模型会更少。通过检查检查点名称，我们可以看到每个检查点的验证损失都低于前一个：
- en: '![Figure 12.7 – Experiment 2 results'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.7 – 实验 2 结果'
- en: '](img/B14768_12_007.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_007.jpg)'
- en: Figure 12.7 – Experiment 2 results
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 – 实验 2 结果
- en: 'Lastly, we can just save the best model, as shown in the following screenshot:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以只保存最好的模型，如下图所示：
- en: '![Figure 12.8 – Experiment 3 results'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.8 – 实验 3 结果'
- en: '](img/B14768_12_008.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_008.jpg)'
- en: Figure 12.8 – Experiment 3 results
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8 – 实验 3 结果
- en: Let's move on to the next section.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入下一部分。
- en: How it works…
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned how to checkpoint models, which saves us a huge amount
    of time as we don't need to retrain a model from scratch. Checkpointing is great
    because we can save the best model according to our own criteria, such as the
    validation loss, training accuracy, or any other measurement.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们学习了如何进行模型检查点保存，这为我们节省了大量时间，因为我们不需要从头开始重新训练模型。检查点保存非常棒，因为我们可以根据自己的标准保存最好的模型，例如验证损失、训练准确度或任何其他度量标准。
- en: By leveraging the `ModelCheckpoint()` callback, we can save a snapshot of the
    network after each completed epoch, thus keeping only the best model or a history
    of the best models produced during training.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 `ModelCheckpoint()` 回调，我们可以在每个完成的 epoch 后保存网络的快照，从而仅保留最好的模型或训练过程中产生的最佳模型历史。
- en: Each strategy has its pros and cons. For instance, generating models after each
    epoch has the upside of allowing us to resume training from any epoch, but at
    the cost of occupying lots of space on disk, while saving the best model only
    preserves space but reduces our flexibility to experiment.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 每种策略都有其优缺点。例如，在每个 epoch 后生成模型的好处是我们可以从任何 epoch 恢复训练，但这会占用大量磁盘空间，而仅保存最好的模型则能节省空间，但会降低我们的实验灵活性。
- en: What strategy will you use in your next project?
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一个项目中使用什么策略？
- en: Customizing the training process using tf.GradientTape
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf.GradientTape 自定义训练过程
- en: 'One of the biggest competitors of TensorFlow is another well-known framework:
    PyTorch. What made PyTorch so attractive until the arrival of TensorFlow 2.x was
    the level of control it gives to its users, particularly when it comes to training
    neural networks.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的最大竞争对手之一是另一个著名框架：PyTorch。直到 TensorFlow 2.x 发布之前，PyTorch 的吸引力在于它给用户提供的控制程度，尤其是在训练神经网络时。
- en: If we are working with somewhat traditional neural networks to solve common
    problems, such as image classification, we don't need that much control over how
    to train a model, and therefore can rely on TensorFlow's (or the Keras API's)
    built-in capabilities, loss functions, and optimizers without a problem.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在处理一些传统的神经网络以解决常见问题，如图像分类，我们不需要对如何训练模型进行过多控制，因此可以依赖 TensorFlow（或 Keras
    API）的内置功能、损失函数和优化器，而没有问题。
- en: But what if we are researchers that are exploring new ways to do things, as
    well as new architectures and novel strategies to solve challenging problems?
    That's when, in the past, we had to resort to PyTorch, due to it being considerably
    easier to customize the training models than using TensorFlow 1.x, but not anymore!
    TensorFlow 2.x's `tf.GradientTape` allows us to create custom training loops for
    models implemented in Keras and low-level TensorFlow more easily, and in this
    recipe, we'll learn how to use it.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们是那些探索新方法、新架构以及解决挑战性问题的新策略的研究人员呢？过去，正因为PyTorch在自定义训练模型方面比TensorFlow 1.x容易得多，我们才不得不使用它，但现在情况已经不同了！TensorFlow
    2.x的`tf.GradientTape`使得我们能够更加轻松地为Keras和低级TensorFlow实现的模型创建自定义训练循环，在本章节中，我们将学习如何使用它。
- en: How to do it…
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成本章节：
- en: 'Import the modules we will be using:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们将使用的模块：
- en: '[PRE84]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Define a function that will load and prepare `Fashion-MNIST`:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来加载和准备`Fashion-MNIST`：
- en: '[PRE85]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Define the `build_network()` method, which, as its name suggests, creates the
    model we''ll train on `Fashion-MNIST`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`build_network()`方法，顾名思义，它创建我们将在`Fashion-MNIST`上训练的模型：
- en: '[PRE86]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, build the fully connected part of the network:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，构建网络的全连接部分：
- en: '[PRE87]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'To demonstrate how to use `tf.GradientTape`, we''ll implement the `training_step()`
    function, which obtains the gradients for a batch of data and then backpropagates
    them using an optimizer:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了演示如何使用`tf.GradientTape`，我们将实现`training_step()`函数，该函数获取一批数据的梯度，然后通过优化器进行反向传播：
- en: '[PRE88]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Define the batch size and the number of epochs to train the model for:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批次大小和训练模型的轮次：
- en: '[PRE89]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Load the dataset:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE90]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Create the optimizer and the network:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建优化器和网络：
- en: '[PRE91]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Now, we''ll create our custom training loop. First, we''ll go over each epoch,
    measuring the time it takes to complete:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建自定义训练循环。首先，我们将遍历每个轮次，衡量完成的时间：
- en: '[PRE92]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Now, we''ll iterate over each batch of data and pass them, along with the network
    and the optimizer, to our `training_step()` function:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将遍历每个数据批次，并将它们与网络和优化器一起传递给`training_step()`函数：
- en: '[PRE93]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Then, we''ll print the epoch''s elapsed time:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将打印当前轮次的时间：
- en: '[PRE94]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Lastly, evaluate the network on the test set to make sure it learned without
    any problems:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在测试集上评估网络，确保它没有出现任何问题：
- en: '[PRE95]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Here are the results:'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: '[PRE96]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Let's move on to the next section.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一个部分。
- en: How it works…
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In this recipe, we learned how to create our own custom training loop. Although
    we didn''t do anything particularly interesting in this instance, we highlighted
    the components (or ingredients, if you will) to cook up a custom deep learning
    training loop with `tf.GradientTape`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们学习了如何创建自己的自定义训练循环。虽然我们在这个实例中没有做任何特别有趣的事情，但我们重点介绍了如何使用`tf.GradientTape`来“烹饪”一个自定义深度学习训练循环的组件（或者说是食材）：
- en: The network architecture itself
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构本身
- en: The loss function used to compute the model loss
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于计算模型损失的损失函数
- en: The optimizer used to update the model weights based on the gradients
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于根据梯度更新模型权重的优化器
- en: The step function, which implements a forward pass (compute the gradients) and
    a backward pass (apply the gradients through the optimizers)
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤函数，它实现了前向传播（计算梯度）和反向传播（通过优化器应用梯度）
- en: If you want to study more realistic and appealing uses of `tf.GradientTape`,
    you can refer to [*Chapter 6*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214),
    *Generative Models and Adversarial Attacks*; [*Chapter 7*](B14768_07_Final_JM_ePub.xhtml#_idTextAnchor248),
    *Captioning Images with CNNs and RNNs*; and [*Chapter 8*](B14768_08_Final_JM_ePub.xhtml#_idTextAnchor270),
    *Fine-Grained Understanding of Images through Segmentation*. However, you can
    just read the next recipe, where we'll learn how to visualize class activation
    maps in order to debug deep neural networks!
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想研究`tf.GradientTape`的更真实和吸引人的应用，可以参考[*第6章*](B14768_06_Final_JM_ePub.xhtml#_idTextAnchor214)，*生成模型与对抗攻击*；[*第7章*](B14768_07_Final_JM_ePub.xhtml#_idTextAnchor248)，*用CNN和RNN进行图像描述*；以及[*第8章*](B14768_08_Final_JM_ePub.xhtml#_idTextAnchor270)，*通过分割实现细粒度图像理解*。不过，你也可以直接阅读下一篇章节，在那里我们将学习如何可视化类别激活图来调试深度神经网络！
- en: Visualizing class activation maps to better understand your network
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化类别激活图，以更好地理解你的网络
- en: Despite their incontestable power and usefulness, one of the biggest gripes
    about deep neural networks is their mysterious nature. Most of the time, we use
    them as black boxes, where we know they work but not why they do.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度神经网络具有无可争议的强大能力和实用性，但关于它们的最大抱怨之一就是它们的神秘性。大多数时候，我们将它们视为黑箱，知道它们能工作，但不知道为什么。
- en: In particular, it's truly challenging to say why a network arrived at a particular
    result, which neurons were activated and why, or where the network is looking
    at to figure out the class or nature of an object in an image.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，真正具有挑战性的是解释为什么一个网络会得到特定的结果，哪些神经元被激活了，为什么会被激活，或者网络在看哪里，以确定图像中物体的类别或性质。
- en: In other words, how can we trust something we don't understand? How can we improve
    it or fix it if it breaks?
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们如何信任我们不理解的东西？如果它坏了，我们又该如何改进或修复它？
- en: Fortunately, in this recipe, we'll study a novel method to shine some light
    on these topics, known as **Gradient Weighted Class Activation Mapping**, or **Grad-CAM**
    for short.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在这个配方中，我们将研究一种新方法，来揭示这些话题，称为**梯度加权类激活映射**，或简称**Grad-CAM**。
- en: Are you ready? Let's get going!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了吗？我们开始吧！
- en: Getting ready
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For this recipe, we need `OpenCV`, `Pillow`, and `imutils`. You can install
    them in one go like this:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个配方，我们需要`OpenCV`、`Pillow`和`imutils`。你可以像这样一次性安装它们：
- en: '[PRE97]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Now, we are ready to implement this recipe.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好实现这个配方了。
- en: How to do it…
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to complete this recipe:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤完成这个配方：
- en: 'Import the modules we will be using:'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们将要使用的模块：
- en: '[PRE98]'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Define the `GradCAM` class, which will encapsulate the **Grad-CAM** algorithm,
    allowing us to produce a heatmap of the activation maps of a given layer. Let''s
    start by defining the constructor:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`GradCAM`类，它将封装**Grad-CAM**算法，使我们能够生成给定层的激活图热力图。让我们从定义构造函数开始：
- en: '[PRE99]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Here, we are receiving the `class_index` of a class we want to inspect, and
    the `layer_name` of a layer whose activations we want to visualize. If we don''t
    receive a `layer_name`, we''ll take the outermost output layer of our `model`
    by default. Finally, we create `grad_model` by relying on the `_create_grad_model()`
    method, as defined here:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们接收的是我们想要检查的类的`class_index`，以及我们希望可视化其激活的层的`layer_name`。如果我们没有接收到`layer_name`，我们将默认使用模型的最外层输出层。最后，我们通过调用这里定义的`_create_grad_model()`方法创建`grad_model`：
- en: '[PRE100]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: This model takes the same inputs as `model`, but outputs both the activations
    of the layer of interest, and the predictions of `model` itself.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个模型与`model`的输入相同，但输出既包含兴趣层的激活，也包含`model`本身的预测。
- en: 'Next, we must define the `compute_heatmap()` method. First, we must pass the
    input image to `grad_model`, obtaining both the activation map of the layer of
    interest and the predictions:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须定义`compute_heatmap()`方法。首先，我们需要将输入图像传递给`grad_model`，以获取兴趣层的激活图和预测：
- en: '[PRE101]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'We can calculate the gradients based on the loss corresponding to the `class_index`:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以根据与`class_index`对应的损失来计算梯度：
- en: '[PRE102]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We can compute guided gradients by, basically, finding positive values in both
    `float_conv_outputs` and `float_grads`, and multiplying those by the gradients,
    which will enable us to visualize what neurons are activating:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过基本上在`float_conv_outputs`和`float_grads`中找到正值并将其与梯度相乘来计算引导梯度，这样我们就可以可视化哪些神经元正在激活：
- en: '[PRE103]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now, we can compute the gradient weights by averaging the guided gradients,
    and then use those weights to add the pondered maps to our **Grad-CAM** visualization:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以通过平均引导梯度来计算梯度权重，然后使用这些权重将加权映射添加到我们的**Grad-CAM**可视化中：
- en: '[PRE104]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Then, we take the **Grad-CAM** visualization, resize it to the dimensions of
    the input image, and min-max normalize it before returning it:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将**Grad-CAM**可视化结果调整为输入图像的尺寸，进行最小-最大归一化后再返回：
- en: '[PRE105]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The last method of the `GradCAM` class overlays a heatmap onto the original
    image. This lets us get a better sense of the visual cues the network is looking
    at when making predictions:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`GradCAM`类的最后一个方法将热力图叠加到原始图像上。这使我们能够更好地了解网络在做出预测时注视的视觉线索：'
- en: '[PRE106]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Let''s instantiate a **ResNet50** trained on ImageNet:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们实例化一个在ImageNet上训练过的**ResNet50**模型：
- en: '[PRE107]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Load the input image, resize it to the dimensions expected by ResNet50, turn
    it into a NumPy array, and preprocess it:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载输入图像，将其调整为ResNet50所期望的尺寸，将其转换为NumPy数组，并进行预处理：
- en: '[PRE108]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Pass the image through the model and extract the index of the most probable
    class:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像通过模型并提取最可能类别的索引：
- en: '[PRE109]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Instantiate a **GradCAM** object and calculate the heatmap:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个**GradCAM**对象并计算热力图：
- en: '[PRE110]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Overlay the heatmap on top of the original image:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将热力图叠加在原始图像上：
- en: '[PRE111]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Decode the predictions to make it human-readable:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码预测结果，使其可供人类读取：
- en: '[PRE112]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Label the overlaid heatmap with the class and its associated probability:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用类别及其关联的概率标注覆盖的热力图：
- en: '[PRE113]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Lastly, merge the original image, the heatmap, and the labeled overlay into
    a single image and save it to disk:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将原始图像、热力图和标注覆盖层合并为一张图像并保存到磁盘：
- en: '[PRE114]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Here is the result:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 12.9 – Visualization of Grad-CAM'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.9 – Grad-CAM 可视化'
- en: '](img/B14768_12_009.jpg)'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_12_009.jpg)'
- en: Figure 12.9 – Visualization of Grad-CAM
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9 – Grad-CAM 可视化
- en: As we can see, the network classified my dog as a Pug, which is correct, with
    a confidence of 85.03%. Moreover, the heatmap reveals the network activates around
    the nose and eyes of my dog's face, which means these are important features and
    the model is behaving as expected.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，网络将我的狗分类为巴哥犬，这是正确的，置信度为 85.03%。此外，热力图显示网络在我的狗的鼻子和眼睛周围激活，这意味着这些是重要特征，模型的表现符合预期。
- en: How it works…
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we learned and implemented **Grad-CAM**, a very useful algorithm
    for visually inspecting the activations of a neural network. This can be an effective
    way of debugging its behavior as it ensures it's looking at the right parts of
    an image.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们学习并实现了**Grad-CAM**，这是一种非常有用的算法，用于可视化检查神经网络的激活情况。这是调试网络行为的有效方法，它确保网络关注图像的正确部分。
- en: This is a very important tool because the high accuracy or performance of our
    model may have less to do with the actual learning, and more to do with factors
    that have been unaccounted for. For instance, if we are working on a pet classifier
    to distinguish between dogs and cats, we should use **Grad-CAM** to verify that
    the network looks at features inherent to these animals in order to properly classify
    them, and not at the surroundings, background noise, or less important elements
    in the images.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常重要的工具，因为我们模型的高准确率或性能可能与实际学习的关系不大，而与一些未考虑到的因素有更多关系。例如，如果我们正在开发一个宠物分类器来区分狗和猫，我们应该使用**Grad-CAM**来验证网络是否关注这些动物固有的特征，以便正确分类，而不是关注周围环境、背景噪音或图像中的次要元素。
- en: See also
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can expand your knowledge of **Grad-CAM** by reading the following paper:
    [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过阅读以下论文来扩展你对**Grad-CAM**的知识：[https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)。
