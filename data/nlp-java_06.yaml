- en: Chapter 3. Advanced Classifiers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。高级分类器
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: A simple classifier
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的分类器
- en: Language model classifier with tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有标记的语言模型分类器
- en: Naïve Bayes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Feature extractors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取器
- en: Logistic regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Multithreaded cross validation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程交叉验证
- en: Tuning parameters in logistic regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归中的调参
- en: Customizing feature extraction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义特征提取
- en: Combining feature extractors
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合特征提取器
- en: Classifier-building life cycle
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器构建生命周期
- en: Linguistic tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言调优
- en: Thresholding classifiers
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值分类器
- en: Train a little, learn a little – active learning
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一点，学习一点——主动学习
- en: Annotation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释
- en: Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This chapter introduces more sophisticated classifiers that use different learning
    techniques as well as richer observations about the data (features). We will also
    address the best practices for building machine-learning systems as well as data
    annotation and approaches that minimize the amount of training data needed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了使用不同学习技术以及关于数据（特征）的更丰富观察的更复杂的分类器。我们还将讨论构建机器学习系统的最佳实践，以及数据注释和减少所需训练数据量的方法。
- en: A simple classifier
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的分类器
- en: This recipe is a thought experiment that should help make clear what machine
    learning does. Recall the *Training your own language model classifier* recipe
    in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    to train your own sentiment classifier in the recipe. Consider what a conservative
    approach to the same problem might be—build `Map<String,String>` from the inputs
    to the correct class. This recipe will explore how this might work and what its
    consequences might be.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法是一个思想实验，应该有助于清楚地了解机器学习的作用。回顾[第1章](ch01.html "第1章。简单分类器")中的*训练你自己的语言模型分类器*，在该方法中训练自己的情感分类器。考虑一下针对同一问题的保守方法——从输入构建`Map<String,String>`到正确的类别。这个方法将探讨这种方式如何工作以及可能带来的后果。
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: Brace yourself; this will be spectacularly stupid but hopefully informative.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 做好准备；这将是极其愚蠢的，但希望能带来一些信息。
- en: 'Enter the following in the command line:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入以下内容：
- en: '[PRE0]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The usual anemic prompt appears, with some user input:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 出现了一个常见的无力提示，并伴随一些用户输入：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It correctly gets the language as `e` or English. However, everything else
    is about to fail. Next, we will use the following code:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它正确地将语言识别为`e`或英语。然而，其他部分即将失败。接下来，我们将使用以下代码：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We just dropped the final `y` on `#Disney`, and as a result, we got a big confused
    classifier. What happened?
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚刚去掉了`#Disney`的最后一个`y`，结果得到了一个大混乱的分类器。发生了什么？
- en: How it works...
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section should really be called *How it doesn't work*, but let's dive into
    the details anyway.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本节实际上应该叫做*它是如何不工作的*，不过让我们还是深入探讨一下细节。
- en: Just to be clear, this recipe is not recommended as an actual solution to a
    classification problem that requires any flexibility at all. However, it introduces
    a minimal example of how to work with LingPipe's `Classification` class as well
    as makes clear what an extreme case of overfitting looks like; this in turn, helps
    demonstrate how machine learning is different from most of standard computer engineering.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，这个方法并不推荐作为解决任何需要灵活性的分类问题的实际方案。然而，它介绍了如何使用LingPipe的`Classification`类的一个最小示例，并清晰地展示了过拟合的极端情况；这反过来有助于展示机器学习与大多数标准计算机工程的不同之处。
- en: 'Starting with the `main()` method, we will get into standard code slinging
    that should be familiar to you from [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从`main()`方法开始，我们将进入一些标准代码操作，这些应该是你从[第1章](ch01.html "第1章。简单分类器")，*简单分类器*中熟悉的内容：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Nothing novel is going on here—we are just training up a classifier, as shown
    in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    and then supplying the classifier to the `Util.consoleInputBestCategory()` method.
    Looking at the class code reveals what is going on:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么新奇的内容——我们只是在训练一个分类器，如在[第1章](ch01.html "第1章。简单分类器")中所示的*简单分类器*，然后将该分类器提供给`Util.consoleInputBestCategory()`方法。查看类代码可以揭示发生了什么：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'So, the `handle()` method takes the `text` and `classification` pair and stuffs
    them in `HashMap`. The classifier does nothing else to learn from the data so
    training amounts to memorization of the data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`handle()`方法接受`text`和`classification`对，并将它们存入`HashMap`。分类器没有其他操作来从数据中学习，因此训练仅仅是对数据的记忆：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `classify()` method just does a lookup into `Map` and returns the value
    if there is one, otherwise, we will get the category `n` as the return classification.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`classify()`方法只是进行一次`Map`查找，如果找到对应的值，则返回该值，否则我们将返回类别`n`作为分类结果。'
- en: What is good about the preceding code is that you have a minimalist example
    of a `BaseClassifier` implementation, and you can see how the `handle()` method
    adds data to the classifier.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的优点是，你有一个`BaseClassifier`实现的最简示例，并且可以看到`handle()`方法是如何将数据添加到分类器中的。
- en: What is bad about the preceding code is the utter rigidity of the mapping from
    training data to categories. If the exact example is not seen in training, then
    the `n` category is assumed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的缺点是训练数据与类别之间的映射完全僵化。如果训练中没有看到确切的示例，那么就会假定为`n`类别。
- en: This is an extreme example of overfitting, but it essentially conveys what it
    means to have an overfit model. An overfit model is tailored too close to the
    training data and cannot generalize well to new data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是过拟合的极端示例，但它本质上传达了过拟合模型的含义。过拟合模型过于贴合训练数据，无法很好地对新数据进行泛化。
- en: 'Let''s think a bit more about what is so wrong about the preceding classifier
    for language identification—the issue is that entire sentences/tweets are the
    wrong unit of processing. Words/tokens are a much better measure of what language
    is being used. Some improvements that will be borne out in the later recipes are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再想想前面语言识别分类器到底有什么问题——问题在于，整个句子/推文是错误的处理单位。单词/标记才是衡量使用何种语言的更好方式。一些将在后续方法中体现的改进包括：
- en: Break the text into words/tokens.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本分解为单词/标记。
- en: Instead of a match/no-match decision, consider a more nuanced approach. A simple
    *which language matches more words* will be a huge improvement.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不仅仅是匹配/不匹配的决策，考虑一种更微妙的方法。简单的*哪个语言匹配更多单词*将是一个巨大的改进。
- en: As languages get closer, for example, British versus American English, probabilities
    can be called for that. Pay attention to likely discriminating words.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着语言的接近，例如英式英语与美式英语，可以为此调用概率。注意可能的区分词。
- en: While this recipe might be comically inappropriate for the task at hand, consider
    trying a sentiment for an even more ludicrous example. It embodies a core assumption
    of much of computer science that the world of inputs is discrete and finite. Machine
    learning can be viewed as a response to a world where this is not the case.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个方法可能对眼前的任务来说有些滑稽不合适，但考虑尝试情感分析来做一个更荒谬的例子。它体现了计算机科学中的一个核心假设，即输入的世界是离散且有限的。机器学习可以被视为对这个假设不成立的世界的回应。
- en: There's more…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Oddly enough, we often have a need for such a classifier in commercial systems—we
    call it the management classifier; it runs preemptively on data. It has happened
    that a senior VP is unhappy with the system output for some example. This classifier
    then can be trained with the exact case that allows for immediate system fixing
    and satisfaction of the VP.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，我们在商业系统中经常需要这种分类器——我们称之为管理分类器；它会预先在数据上运行。曾经发生过某个高级副总裁对系统输出中的某个示例不满。然后可以使用这个分类器训练精确的案例，从而立即修复系统并让副总裁满意。
- en: Language model classifier with tokens
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有标记的语言模型分类器
- en: '[Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    covered classification without knowing what tokens/words were, with a language
    model per category—we used character slices or ngrams to model the text. [Chapter
    2](ch02.html "Chapter 2. Finding and Working with Words"), *Finding and Working
    with Words*, discussed at length the process of finding tokens in text, and now
    we can use them to build a classifier. Most of the time, we use tokenized input
    to classifiers, so this recipe is an important introduction to the concept.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html "第1章. 简单分类器")，*简单分类器*，讲解了在不知道标记/单词是什么的情况下进行分类，每个类别都有一个语言模型——我们使用字符切片或n-gram来建模文本。[第2章](ch02.html
    "第2章. 寻找和处理单词")，*寻找和处理单词*，详细讨论了在文本中寻找标记的过程，现在我们可以利用这些标记来构建分类器。大多数时候，我们将标记化输入提供给分类器，因此这个方法是对概念的一个重要介绍。'
- en: How to do it...
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'This recipe will tell us how to train and use a tokenized language model classifier,
    but it will ignore issues such as evaluation, serialization, deserialization,
    and so on. You can refer to the recipes in [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, for examples. This code of this recipe is
    in `com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将告诉我们如何训练和使用一个分词的语言模型分类器，但它会忽略评估、序列化、反序列化等问题。你可以参考[第1章](ch01.html "第1章 简单分类器")中的食谱，*简单分类器*，获取示例。本食谱的代码在`com.lingpipe.cookbook.chapter3.TrainAndRunTokenizedLMClassifier`中：
- en: 'The exception of the following code is the same as found in the *Training your
    own language model classifier* recipe in [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*. The `DynamicLMClassifier` class provides
    a static method for the creation of a tokenized LM classifier. Some setup is required.
    The `maxTokenNgram` variable sets the largest size of token sequences used in
    the classifier—smaller datasets usually benefit from lower order (number of tokens)
    ngrams. Next, we will set up a `tokenizerFactory` method, selecting the workhorse
    tokenizer from [Chapter 2](ch02.html "Chapter 2. Finding and Working with Words"),
    *Finding and Working with Words*. Finally, we will specify the categories that
    the classifier uses:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码的例外情况与[第1章](ch01.html "第1章 简单分类器")中的*训练你自己的语言模型分类器*食谱中的内容相同，*简单分类器*。`DynamicLMClassifier`类提供了一个静态方法，用于创建一个分词的语言模型分类器。需要一些设置。`maxTokenNgram`变量设置了分类器中使用的最大令牌序列大小——较小的数据集通常从较低阶（令牌数量）n-gram中受益。接下来，我们将设置一个`tokenizerFactory`方法，选择[第2章](ch02.html
    "第2章 查找和使用词汇")中所用的主力分词器，*查找和使用词汇*。最后，我们将指定分类器使用的类别：
- en: '[PRE6]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, the classifier is constructed:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，构建分类器：
- en: '[PRE7]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the code from the command line or your IDE:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行或你的 IDE 中运行代码：
- en: '[PRE8]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: There's more...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In application, the `DynamicLMClassifier` classifier does not see a great deal
    of use in commercial application. This classifier might be a good choice for an
    author-identification classifier (that is, one that classifies whether a given
    piece of text is written by an author or by someone else) that was highly sensitive
    to turns of phrase and exact word usage. The Javadoc is well worth consulting
    to better understand what this class does.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，`DynamicLMClassifier`分类器在商业应用中并没有得到广泛使用。这个分类器可能是进行作者识别分类的一个不错选择（即用于判断给定文本是某个作者写的，还是其他人写的），该分类器对于措辞和精确用词非常敏感。建议查阅Javadoc，了解该类的具体功能。
- en: Naïve Bayes
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Naïve Bayes is probably the world's most famous classification technology, and
    just to keep you on your toes, we provide two separate implementations with lots
    of configurability. One of the most well-known applications of a Naïve Bayes classifier
    is for spam filtering in an e-mail.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯可能是世界上最著名的分类技术，为了让你保持警觉，我们提供了两个独立的实现，具有很高的可配置性。朴素贝叶斯分类器的一个最著名应用是用于电子邮件中的垃圾邮件过滤。
- en: 'The reason the word *naïve* is used is that the classifier assumes that words
    (features) occur independent of one another—this is clearly a naïve assumption,
    but lots of useful and not-so-useful technologies have been based on the approach.
    Some notable features of the traditional naïve Bayes include:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*naïve*（天真）一词的原因是，该分类器假设词汇（特征）是相互独立的——这一假设显然是天真的，但许多有用或不那么有用的技术都是基于这一方法的。一些传统朴素贝叶斯的显著特征包括：
- en: Character sequences are converted to bags of tokens with counts. No whitespaces
    are considered, and the order of the tokens does not matter.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符序列被转换为带有计数的词袋。空格不被考虑，词项的顺序不重要。
- en: Naïve Bayes classifiers require two or more categories into which input texts
    are categorized. These categories must be both exhaustive and mutually exclusive.
    This indicates that a document used for training must only belong to one category.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器需要两个或更多类别，用于将输入文本分类。这些类别必须是完整的且互相排斥的。这意味着用于训练的文档必须只属于一个类别。
- en: 'The math is very simple: `p(category|tokens) = p(category,tokens)/p(tokens)`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数学非常简单：`p(category|tokens) = p(category,tokens)/p(tokens)`。
- en: The class is configurable for various kinds of unknown token models.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该类可以根据各种未知令牌模型进行配置。
- en: A naïve Bayes classifier estimates two things. First, it estimates the probability
    of each category, independent of any tokens. This is carried out based on the
    number of training examples presented for each category. Second, for each category,
    it estimates the probability of seeing each token in that category. Naïve Bayes
    is so useful and important that we will show you exactly how it works and plug
    through the formulas. The example we have is to classify hot and cold weather
    based on the text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器估计两个方面的内容。首先，它估计每个类别的概率，独立于任何标记。这是根据每个类别提供的训练示例数量来进行的。其次，对于每个类别，它估计在该类别中看到每个标记的概率。朴素贝叶斯如此有用且重要，以至于我们将向你展示它如何工作，并逐步讲解公式。我们使用的例子是基于文本分类热天气和冷天气。
- en: First, we will work out the math to calculate the probability of a category
    given a word sequence. Second, we will plug in an example and then verify it using
    the classifier we build.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算出给定单词序列时，类别的概率。其次，我们将插入一个例子，并使用我们构建的分类器进行验证。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'Let''s lay out the basic formula to calculate the probability of a category
    given a text input. A token-based naïve Bayes classifier computes the joint token
    count and category probabilities as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出计算给定文本输入时类别概率的基本公式。基于标记的朴素贝叶斯分类器通过以下方式计算联合标记计数和类别概率：
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Conditional probabilities are derived by applying Bayes''s rule to invert the
    probability calculation:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 条件概率是通过应用贝叶斯规则来逆转概率计算得到的：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we will get to expand all these terms. If we look at `p(tokens|cat)`,
    this is where the naïve assumption comes into play. We assume that each token
    is independent, and thus, the probability of all the tokens is the product of
    the probability of each token:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将扩展所有这些术语。如果我们看一下`p(tokens|cat)`，这是朴素假设发挥作用的地方。我们假设每个标记是独立的，因此所有标记的概率是每个标记概率的乘积：
- en: '[PRE11]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The probability of the tokens themselves, that is, `p(tokens)`, the denominator
    in the preceding equation. This is just the sum of their probability in each category
    weighted by the probability of the category itself:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记本身的概率，即`p(tokens)`，是前面方程中的分母。这只是它们在每个类别中的概率总和，并根据类别本身的概率加权：
- en: '[PRE12]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: When building a naïve Bayes classifier, `p(tokens)` doesn't need to be explicitly
    calculated. Instead, we can use `p(tokens|cat) * p(cat)` and assign the tokens
    to the category with the higher product.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在构建朴素贝叶斯分类器时，`p(tokens)`不需要显式计算。相反，我们可以使用`p(tokens|cat) * p(cat)`，并将标记分配给具有更高乘积的类别。
- en: Now that we have laid out each element of our equation, we can look at how these
    probabilities are calculated. We can calculate both these probabilities using
    simple frequencies.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经列出了方程的每个元素，可以看看这些概率是如何计算的。我们可以通过简单的频率来计算这两个概率。
- en: 'The probability of a category is calculated by counting the number of times
    the category showed up in the training instances divided by the total number of
    training instances. As we know that Naïve Bayes classifiers have exhaustive and
    mutually-exclusive categories, the sum of the frequency of each category must
    equal the total number of training instances:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别的概率是通过计算该类别在训练实例中出现的次数除以训练实例的总数来计算的。我们知道，朴素贝叶斯分类器具有穷尽且互斥的类别，因此每个类别的频率总和必须等于训练实例的总数：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The probability of a token in a category is computed by counting the number
    of times the token appeared in a category divided by the number of times all the
    other tokens appeared in this category:'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类别中标记的概率是通过计算标记在该类别中出现的次数除以所有其他标记在该类别中出现的总次数来计算的：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These probabilities are calculated to provide what is called the **maximum likelihood
    estimate** of the model. Unfortunately, these estimates provide zero probability
    for tokens that were not seen during the training. You can see this very easily
    in the calculation of an unseen token probability. Since it wasn't seen, it will
    have a frequency count of 0, and the numerator of our original equation goes to
    0.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些概率的计算提供了所谓的**最大似然估计**模型。不幸的是，这些估计对于训练中未出现的标记提供零概率。你可以很容易地通过计算一个未见过的标记的概率看到这一点。由于它没有出现，它的频率计数为0，因此原始方程的分子也变为0。
- en: 'In order to overcome this, we will use a technique known as **smoothing** that
    assigns a prior and then computes a maximum a posteriori estimate rather than
    a maximum likelihood estimate. A very common smoothing technique is called additive
    smoothing, and it just involves adding a prior count to every count in the training
    data. Two sets of counts are added: the first is a token count added to all the
    token frequency calculations, and the second is a category count, which is added
    to all the category count calculations.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了克服这个问题，我们将使用一种称为**平滑**的技术，它分配一个先验，然后计算最大后验估计，而不是最大似然估计。一种非常常见的平滑技术叫做加法平滑，它只是将一个先验计数加到训练数据中的每个计数上。有两个计数集合被加上：第一个是加到所有标记频率计算中的标记计数，第二个是加到所有类别计数计算中的类别计数。
- en: 'This obviously changes the `p(cat)` and the `p(token|cat)` values. Let''s call
    the `alpha` prior that is added to the category count and the `beta` prior that
    is added to the token count. When we call the `alpha` prior, our previous calculations
    will change to:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显然会改变`p(cat)`和`p(token|cat)`的值。我们将添加到类别计数的`alpha`先验和添加到标记计数的`beta`先验称为先验。当我们调用`alpha`先验时，之前的计算会变成：
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When we call the `beta` prior, the calculations will change to:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们调用`beta`先验时，计算会变成：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we have set up our equations, let's look at a concrete example.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了方程式，让我们来看一个具体的例子。
- en: 'We''ll build a classifier to classify whether the forecast calls for hot or
    cold weather based on a set of phrases:'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将构建一个分类器，基于一组短语来分类天气预报是热还是冷。
- en: '[PRE17]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'There are a total of seven tokens in these five training items:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这五个训练项中总共有七个标记：
- en: '`super`'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`super`'
- en: '`steamy`'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steamy`'
- en: '`today`'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`today`'
- en: '`boiling`'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boiling`'
- en: '`out`'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out`'
- en: '`freezing`'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`freezing`'
- en: '`icy`'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`icy`'
- en: Of these, all the tokens appear once, except `steamy`, which appears twice in
    the `hot` category and `out`, which appears once in each category. This is our
    training data. Now, let's calculate the probability of an input text being in
    the `hot` or `cold` category . Let's say our input is the word `super`. Let's
    set the category prior `alpha` to `1` and the token prior `beta` also to `1`.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这些数据中，所有的标记都出现一次，除了`steamy`，它在`hot`类别中出现了两次，而`out`则在每个类别中各出现了一次。这就是我们的训练数据。现在，让我们计算输入文本属于`hot`类别或`cold`类别的概率。假设我们的输入是单词`super`。我们将类别先验`alpha`设置为`1`，标记先验`beta`也设置为`1`。
- en: 'So, we will calculate the probabilities of `p(hot|super)` and `p(cold|super)`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，我们将计算`p(hot|super)`和`p(cold|super)`的概率：
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We will take into consideration all the tokens, including the ones that haven''t
    been seen in the `hot` category:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将考虑所有标记，包括那些在`hot`类别中没有出现的标记：
- en: '[PRE19]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give us a denominator equal to a sum of these inputs:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将给我们一个分母，等于这些输入的总和：
- en: '[PRE20]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, `p(super|hot) = 2/13` is one part of the equation. We still need to calculate
    `p(hot)` and `p (super)`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，`p(super|hot) = 2/13`是方程的一部分。我们仍然需要计算`p(hot)`和`p(super)`：
- en: '[PRE21]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For the `hot` category, we have three documents or cases, and for the `cold`
    category, we have two documents in our training data. So, `freq(hot) = 3` and
    `freq(cold) = 2`:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于`hot`类别，我们的训练数据中有三个文档或案例，而对于`cold`类别，我们有两个文档。所以，`freq(hot) = 3`，`freq(cold)
    = 2`：
- en: '[PRE22]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To calculate `p(super|cold)`, we need to repeat the same steps:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要计算`p(super|cold)`，我们需要重复相同的步骤：
- en: '[PRE23]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This gives us the probability of the token `super`:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会给我们标记`super`的概率：
- en: '[PRE24]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We now have all the pieces together to calculate `p(hot|super)` and `p(cold|super)`:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们已经将所有部分整合在一起，来计算`p(hot|super)`和`p(cold|super)`：
- en: '[PRE25]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we want to repeat this for the input stream `super super`, the following
    calculations can be used:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果我们想对输入流`super super`重复此过程，可以使用以下计算：
- en: '[PRE26]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Remember our naïve assumption: the probability of the tokens is the product
    of the probabilities, since we assume that they are independent of each other.'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住我们的朴素假设：标记的概率是概率的乘积，因为我们假设它们彼此独立。
- en: Let's verify our calculations by training up the naïve Bayes classifier and
    using the same input.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过训练朴素贝叶斯分类器并使用相同的输入来验证我们的计算。
- en: How to do it...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s verify some of these calculations in code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在代码中验证这些计算：
- en: 'In your IDE, run the `TrainAndRunNaiveBayesClassifier` class in the code package
    of this chapter, or using the command line, type the following command:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中，运行本章代码包中的`TrainAndRunNaiveBayesClassifier`类，或者使用命令行输入以下命令：
- en: '[PRE27]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the prompt, let''s use our first example, `super`:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提示中，我们使用第一个例子，`super`：
- en: '[PRE28]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As we can see, our calculations were correct. For the case of a word, `hello`,
    that doesn''t exist in our training; we will fall back to the prevalence of the
    categories modified by the category''s prior counts:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们的计算是正确的。对于一个在我们的训练集中不存在的单词`hello`，我们将回退到由类别的先验计数修正的类别的普遍性：
- en: '[PRE29]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Again, for the case of `super super`, our calculations were correct.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，对于`super super`的情况，我们的计算是正确的。
- en: '[PRE30]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The source that generates the preceding output is in `src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java`.
    The code should be straightforward, so we will not covering it in this recipe.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成前述输出的源代码位于`src/com/lingpipe/chapter3/TrainAndRunNaiveBays.java`。这段代码应该很直观，因此我们在本食谱中不会详细讲解。
- en: See also
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: For more details on configuring naïve Bayes, including length normalizing, refer
    to the Javadoc at [http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html](http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关配置朴素贝叶斯的更多细节，包括长度归一化，请参考Javadoc：[http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html](http://alias-i.com/lingpipe/docs/api/index.html?com/aliasi/classify/TradNaiveBayesClassifier.html)
- en: You can refer to the expectation maximization tutorial at [http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以参考期望最大化教程，网址为[http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/em/read-me.html)
- en: Feature extractors
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取器
- en: Up until now, we have been using characters and words to train our models. We
    are about to introduce a classifier (logistic regression) that allows for other
    observations about the data to inform the classifier—for example, whether a word
    is actually a date. Feature extractors are used in CRF taggers and K-means clustering.
    This recipe will introduce feature extractors independent of any technology that
    uses them.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用字符和单词来训练我们的模型。接下来，我们将引入一个分类器（逻辑回归），它可以让数据的其他观察结果来影响分类器——例如，一个单词是否实际上是一个日期。特征提取器在CRF标注器和K-means聚类中都有应用。本食谱将介绍独立于任何使用它们的技术的特征提取器。
- en: How to do it...
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: There is not much to this recipe, but the upcoming *Logistic regression* recipe
    has many moving parts, and this is one of them.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱不复杂，但接下来的*逻辑回归*食谱有许多动态部分，而这正是其中之一。
- en: 'Fire up your IDE or type in the command line:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的IDE或在命令行中输入：
- en: '[PRE32]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Type a string into our standard I/O loop:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的标准I/O循环中输入一个字符串：
- en: '[PRE33]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Features are then produced:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后生成特征：
- en: '[PRE34]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that there is no order information here. Does it keep a count or not?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，这里没有顺序信息。它是否保持计数？
- en: '[PRE35]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The feature extractor keeps count with `my=2`, and it does not normalize the
    case (`My` is different from `my`). Refer to the later recipes in this chapter
    on how to modify feature extractors—they are very flexible.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器使用`my=2`来保持计数，并且不规范化大小写（`My`与`my`是不同的）。有关如何修改特征提取器的更多信息，请参阅本章稍后的食谱——它们非常灵活。
- en: How it works…
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'LingPipe provides solid infrastructure for the creation of feature extractors.
    The code for this recipe is in `src/com/lingipe/chapter3/SimpleFeatureExtractor.java`:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe为创建特征提取器提供了坚实的基础设施。本食谱的代码位于`src/com/lingipe/chapter3/SimpleFeatureExtractor.java`：
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The preceding code constructs `TokenFeatureExtractor` with `TokenizerFactory`.
    It is one of the 13 `FeatureExtractor` implementations provided in LingPipe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码使用`TokenizerFactory`构建了`TokenFeatureExtractor`。这是LingPipe中提供的13种`FeatureExtractor`实现之一。
- en: Next, we will apply the I/O loop and print out the feature, which is `Map<String,
    ? extends Number>`. The `String` element is the feature name. In this case, the
    actual token is the name. The second element of the map is a value that extends
    `Number`, in this case, the count of how many times the token was seen in the
    text.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用I/O循环并打印出特征，它是`Map<String, ? extends Number>`。`String`元素是特征名称。在这种情况下，实际的标记是名称。映射的第二个元素是一个扩展了`Number`的值，在这种情况下，就是标记在文本中出现的次数。
- en: '[PRE37]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The feature name needs to only be a unique name—we could have prepended each
    feature name with `SimpleFeatExt_` to keep track of where the feature came from,
    which is helpful in complex feature-extraction scenarios.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 特征名称只需是唯一的——我们本可以在每个特征名称前加上`SimpleFeatExt_`，以跟踪特征的来源，这在复杂的特征提取场景中很有帮助。
- en: Logistic regression
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Logistic regression is probably responsible for the majority of industrial classifiers,
    with the possible exception of naïve Bayes classifiers. It almost certainly is
    one of the best performing classifiers available, albeit at the cost of slow training
    and considerable complexity in configuration and tuning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可能是大多数工业分类器的基础，唯一的例外可能是朴素贝叶斯分类器。它几乎肯定是性能最好的分类器之一，尽管代价是训练过程较慢且配置和调优较为复杂。
- en: Logistic regression is also known as maximum entropy, neural network classification
    with a single neuron, and others. So far in this book, the classifiers have been
    based on the underlying characters or tokens, but logistic regression uses unrestricted
    feature extraction, which allows for arbitrary observations of the situation to
    be encoded in the classifier.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归也被称为最大熵、单神经元的神经网络分类等。到目前为止，本书中的分类器基于底层的字符或词元，但逻辑回归使用的是不受限的特征提取，这允许在分类器中编码任意的情况观察。
- en: This recipe closely follows a more complete tutorial at [http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程与[http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/logistic-regression/read-me.html)中的一个更完整的教程非常相似。
- en: How logistic regression works
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归如何工作
- en: All that logistic regression does is take a vector of feature weights over the
    data, apply a vector of coefficients, and do some simple math, which results in
    a probability for each class encountered in training. The complicated bit is in
    determining what the coefficients should be.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归所做的就是对数据进行特征权重的向量运算，应用系数向量，并进行一些简单的数学计算，最终得出每个训练类的概率。复杂的部分在于如何确定系数。
- en: 'The following are some of the features produced by our training recipe for
    21 tweets annotated for English `e` and non-English `n`. There are relatively
    few features because feature weights are being pushed to `0.0` by our prior, and
    once a weight is `0.0`, then the feature is removed. Note that one category, `n`,
    is set to `0.0` for all the features of the `n-1` category—this is a property
    of the logistic regression process that fixes once categories features to `0.0`
    and adjust all other categories features with respect to that:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们为21条推文（标注为英文`e`和非英文`n`）的训练结果所生成的一些特征。由于我们的先验会将特征权重推向`0.0`，因此特征相对较少，一旦某个权重为`0.0`，该特征就会被移除。请注意，类别`n`的所有特征都被设置为`0.0`，这与逻辑回归过程的特性有关，它将一类特征固定为`0.0`，并根据此调整其他类别的特征：
- en: '[PRE38]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Take the string, `I luv Disney`, which will only have two non-zero features:
    `I=0.37` and `Disney=0.15` for `e` and zeros for `n`. Since there is no feature
    that matches `luv`, it is ignored. The probability that the tweet is English breaks
    down to:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以字符串`I luv Disney`为例，它将只具有两个非零特征：`I=0.37`和`Disney=0.15`（对于`e`），`n`类的特征全为零。由于没有与`luv`匹配的特征，它会被忽略。该推文为英文的概率可以分解为：
- en: '*vectorMultiply(e,[I,Disney]) = exp(.37*1 + .15*1) = 1.68*'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*vectorMultiply(e,[I,Disney]) = exp(.37*1 + .15*1) = 1.68*'
- en: '*vectorMultiply(n,[I,Disney]) = exp(0*1 + 0*1) = 1*'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*vectorMultiply(n,[I,Disney]) = exp(0*1 + 0*1) = 1*'
- en: 'We will rescale to a probability by summing the outcomes and dividing it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过求和结果并进行归一化，得到最终的概率：
- en: '*p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(e|,[I,Disney]) = 1.68/(1.68 +1) = 0.62*'
- en: '*p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38*'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(e|,[I,Disney]) = 1/(1.68 +1) = 0.38*'
- en: This is how the math works on running a logistic regression model. Training
    is another issue entirely.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是运行逻辑回归模型时数学运算的原理。训练则是完全不同的问题。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe assumes the same framework that we have been using all along to
    get training data from `.csv` files, train the classifier, and run it from the
    command line.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程假设使用与我们一直以来相同的框架，从`.csv`文件中获取训练数据，训练分类器，并通过命令行运行它。
- en: Setting up to train the classifier is a bit complex because of the number of
    parameters and objects used in training. We will discuss all the 10 arguments
    to the training method as found in `com.lingpipe.cookbook.chapter3.TrainAndRunLogReg`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 设置分类器的训练有点复杂，因为训练过程中使用了大量的参数和对象。我们将讨论在`com.lingpipe.cookbook.chapter3.TrainAndRunLogReg`中训练方法的10个参数。
- en: 'The `main()` method starts with what should be familiar classes and methods—if
    they are not familiar, have a look at *How to train and evaluate with cross validation*
    and *Introduction to Introduction to tokenizer Factories – finding words in a
    character stream*, recipes from [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, and [Chapter 2](ch02.html "Chapter 2. Finding and Working
    with Words"), *Finding and Working with Words*, respectively:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`方法从应该熟悉的类和方法开始——如果它们不熟悉，可以看看*如何通过交叉验证进行训练和评估*以及*引入分词器工厂——在字符流中查找单词*，这些都是[第1章](ch01.html
    "第1章 简单分类器") *简单分类器*和[第2章](ch02.html "第2章 查找与处理单词") *查找与处理单词*中的配方：'
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that we are using `XValidatingObjectCorpus` when a simpler implementation
    such as `ListCorpus` will do. We will not take advantage of any of its cross-validation
    features, because the `numFolds` param as `0` will have training visit the entire
    corpus. We are trying to keep the number of novel classes to a minimum, and we
    tend to always use this implementation in real-world gigs anyway.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是`XValidatingObjectCorpus`，而使用像`ListCorpus`这样更简单的实现就足够了。我们不会利用它的任何交叉验证功能，因为`numFolds`参数为`0`时训练会遍历整个语料库。我们试图将新的类别数量保持在最小，并且在实际工作中，我们总是倾向于使用这种实现。
- en: 'Now, we will start to build the configuration for our classifier. The `FeatureExtractor<E>`
    interface provides a mapping from data to features; this will be used to train
    and run the classifier. In this case, we are using a `TokenFeatureExtractor()`
    method, which creates features based on the tokens found by the tokenizer supplied
    during construction. This is similar to what naïve Bayes reasons over. The previous
    recipe goes into more detail about what the feature extractor is doing if this
    is not clear:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始为我们的分类器构建配置。`FeatureExtractor<E>`接口提供了从数据到特征的映射；这将用于训练和运行分类器。在本例中，我们使用`TokenFeatureExtractor()`方法，它基于构造时提供的分词器找到的标记来创建特征。这与朴素贝叶斯的推理方式类似。如果不清楚，前面的配方会更详细地说明特征提取器的作用：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `minFeatureCount` item is usually set to a number higher than 1, but with
    small training sets, this is needed to get any performance. The thought behind
    filtering feature counts is that logistic regression tends to overfit low-count
    features that, just by chance, exist in one category of training data. As training
    data grows, the `minFeatureCount` value is adjusted usually by paying attention
    to cross-validation performance:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`minFeatureCount`项通常设置为大于1的数字，但对于小的训练集，这个设置是提高性能所必需的。过滤特征计数的思路是，逻辑回归往往会过拟合低频特征，这些特征仅因偶然出现在某个类别的训练数据中而存在。随着训练数据量的增加，`minFeatureCount`值通常会根据交叉验证性能进行调整：'
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `addInterceptFeature` Boolean controls whether a category feature exists
    that models the prevalence of the category in training. The default name of the
    intercept feature is `*&^INTERCEPT%$^&**`, and you will see it in the weight vector
    output if it is being used. By convention, the intercept feature is set to `1.0`
    for all inputs. The idea is that if a category is just very common or very rare,
    there should be a feature that captures just this fact, independent of other features
    that might not be as cleanly distributed. This models the category probability
    in naïve Bayes in some way, but the logistic regression algorithm will decide
    how useful it is as it does with all other features:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`addInterceptFeature`布尔值控制是否存在一个类别特征，表示该类别在训练数据中的普遍性。默认的截距特征名称是`*&^INTERCEPT%$^&**`，如果它被使用，你会在权重向量输出中看到它。根据惯例，截距特征对于所有输入的值被设置为`1.0`。其理念是，如果某个类别非常常见或非常稀有，则应有一个特征专门捕捉这一事实，而不受其他可能分布不均的特征的影响。这某种程度上建模了朴素贝叶斯中的类别概率，但逻辑回归算法会像对待其他特征一样决定它的有用性：'
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: These Booleans control what happens to the intercept feature if it is used.
    Priors, in the following code, are typically not applied to the intercept feature;
    this is the result if this parameter is true. Set the Boolean to `false`, and
    the prior will be applied to the intercept.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些布尔值控制截距特征被使用时会发生什么。如果此参数为真，通常不会将先验应用于截距特征；如果将布尔值设置为`false`，则先验将应用于截距。
- en: Next is the `RegressionPrior` instance, which controls how the model is fit.
    What you need to know is that priors help prevent logistic regression from overfitting
    the data by pushing coefficients towards 0\. There is a non-informative prior
    that does not do this with the consequence that if there is a feature that applies
    to just one category it will be scaled to infinity, because the model keeps fitting
    better as the coefficient is increased in the numeric estimation. Priors, in this
    context, function as a way to not be over confident in observations about the
    world.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`RegressionPrior`实例，它控制模型的拟合方式。你需要知道的是，先验帮助防止逻辑回归对数据的过拟合，通过将系数推向0。这里有一个非信息性先验，它不会这样做，结果是如果有特征仅适用于一个类别，它将被缩放到无穷大，因为模型在系数增加时会不断更好地拟合数值估计。先验，在这个上下文中，作为一种方式，避免对世界的观察过于自信。
- en: Another dimension in the `RegressionPrior` instance is the expected variance
    of the features. Low variance will push coefficients to zero more aggressively.
    The prior returned by the static `laplace()` method tends to work well for NLP
    problems. For more information on what is going on here, consult the relevant
    Javadoc and the logistic regression tutorial referenced at the beginning of the
    recipe—there is a lot going on, but it can be managed without a deep theoretical
    understanding. Also, see the *Tuning parameters in logistic regression* recipe
    in this chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegressionPrior`实例中的另一个维度是特征的期望方差。低方差会更积极地将系数推向零。由静态`laplace()`方法返回的先验在NLP问题中通常效果很好。关于这里发生了什么，更多信息请参考相关的Javadoc和在本配方开始时提到的逻辑回归教程——虽然有很多内容，但无需深入理论理解也能管理。此外，见本章中的*逻辑回归中的参数调优*配方。'
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we will control how the algorithm searches for an answer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将控制算法如何搜索答案。
- en: '[PRE44]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`AnnealingSchedule` is best understood by consulting the Javadoc, but what
    it does is change how much the coefficients are allowed to vary when fitting the
    model. The `minImprovement` parameter sets the amount the model fit has to improve
    to not terminate the search, because the algorithm has converged. The `minEpochs`
    parameter sets a minimal number of iterations, and `maxEpochs` sets an upper limit
    if the search does not converge as determined by `minImprovement`.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`AnnealingSchedule`最好通过查阅Javadoc来理解，但它的作用是改变拟合模型时允许系数变化的程度。`minImprovement`参数设置模型拟合必须改进的量，才不会终止搜索，因为算法已经收敛。`minEpochs`参数设置最小迭代次数，`maxEpochs`设置一个上限，如果搜索没有收敛（根据`minImprovement`判断）。'
- en: 'Next is some code that allows for basic reporting/logging. `LogLevel.INFO`
    will report a great deal of information about the progress of the classifier as
    it tries to converge:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是一些允许基本报告/日志记录的代码。`LogLevel.INFO`将报告关于分类器尝试收敛过程中的大量信息：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Here ends the *Getting ready* section of one of our most complex classes—next,
    we will train and run the classifier.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们最复杂类之一的*准备就绪*部分的结束——接下来，我们将训练并运行分类器。
- en: How to do it...
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'It has been a bit of work setting up to train and run this class. We will just
    go through the steps to get it up and running; the upcoming recipes will address
    its tuning and evaluation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 设置训练和运行这个类确实有一些工作。我们将逐步介绍如何启动它；接下来的配方将涉及其调优和评估：
- en: 'Note that there is a more complex 14-argument train method as well the one
    that extends configurability. This is the 10-argument version:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，还有一个更复杂的14参数训练方法，以及一个扩展配置能力的方法。这是10参数版本：
- en: '[PRE46]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `train()` method, depending on the `LogLevel` constant, will produce from
    nothing with `LogLevel.NONE` to the prodigious output with `LogLevel.ALL`.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train()`方法，根据`LogLevel`常量的不同，会从`LogLevel.NONE`的空输出到`LogLevel.ALL`的巨大输出。'
- en: 'While we are not going to use it, we show how to serialize the trained model
    to disk. The *How to serialize a LingPipe object – classifier example* recipe
    in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*,
    explains what is going on:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然我们不会使用它，但我们展示如何将训练好的模型序列化到磁盘。*如何序列化LingPipe对象——分类器示例*配方在[第1章](ch01.html "第1章：简单分类器")，*简单分类器*中解释了发生了什么：
- en: '[PRE47]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Once trained, we will apply the standard classification loop with:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们将应用标准分类循环，包含：
- en: '[PRE48]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Run the preceding code in the IDE of your choice or use the command-line command:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你选择的IDE中运行前面的代码，或使用命令行命令：
- en: '[PRE49]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The result is a big dump of information about the training:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果是关于训练的一个大量信息输出：
- en: '[PRE50]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `epoch` reporting goes on until either the number of epochs is met or the
    search converges. In the following case, the number of epochs was met:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`epoch`报告会一直进行，直到达到设定的周期数或者搜索收敛。在以下情况下，周期数已满足：'
- en: '[PRE51]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, we can play with the classifier a bit:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以稍微玩一下分类器：
- en: '[PRE52]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This should look familiar; it is exactly the same result as the worked example
    at the start of the recipe.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这应该看起来很熟悉；它与食谱开始时的示例结果完全相同。
- en: That's it! You have trained up and used the world's most relevant industrial
    classifier. However, there's a lot more to harnessing the power of this beast.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你已经训练并使用了世界上最相关的工业分类器。不过，要充分利用这个“猛兽”的力量，还有很多内容要掌握。
- en: Multithreaded cross validation
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多线程交叉验证
- en: Cross validation (refer to the *How to train and evaluate with cross validation*
    recipe in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*)
    can be very slow, which interferes with tuning systems. This recipe will show
    you a simple but effective way to access all the available cores on your system
    to more quickly process each fold.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证（请参阅[第1章](ch01.html "第1章。简单分类器")中的*如何使用交叉验证进行训练和评估*食谱，*简单分类器*）可能非常慢，这会干扰系统的调优。这个食谱将展示一种简单但有效的方法，帮助你利用系统上的所有可用核心，更快速地处理每个折叠。
- en: How to do it...
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: This recipe explains multi-threaded cross validation in the context of the next
    recipe, so don't be confused by the fact that the same class is repeated.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱在下一个食谱的背景下解释了多线程交叉验证，所以不要被相同类名的重复所困惑。
- en: 'Engage your IDE or type in the command line:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的IDE或在命令行中输入：
- en: '[PRE53]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The system then responds with the following output (you might have to scroll
    to the top of the window):'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统随后会返回以下输出（你可能需要滚动到窗口顶部）：
- en: '[PRE54]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The default training data is 21 tweets annotated for English `e` and non-English
    `n`. In the preceding output, we saw a report of each fold that runs as a thread
    and the resulting confusion matrix. That's it! We just did multithreaded cross
    validation. Let's see how this works.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认的训练数据是21条带有英语`e`和非英语`n`标注的推文。在之前的输出中，我们看到每个作为线程运行的折叠报告和结果混淆矩阵。就是这样！我们刚刚完成了多线程交叉验证。让我们来看看它是如何工作的。
- en: How it works…
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'All the action happens in the `Util.xvalLogRegMultiThread()` method, which
    we invoke from `src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java`. The
    details of `TuneLogRegParams` are covered in the next recipe. This recipe will
    focus on the `Util` method:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的操作都发生在`Util.xvalLogRegMultiThread()`方法中，我们从`src/com/lingpipe/cookbook/chapter3/TuneLogRegParams.java`中调用它。`TuneLogRegParams`的细节将在下一个食谱中讲解。本食谱将重点介绍`Util`方法：
- en: '[PRE55]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: All 10 parameters used to configure logistic regression are controllable (you
    can refer to the previous recipe for explanation), with the addition of `numFolds`,
    which controls how many folds there will be, `numThreads`, which controls how
    many threads can be run at the same time, and finally, `categories`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 用于配置逻辑回归的所有10个参数都是可控的（你可以参考前一个食谱了解解释），此外还新增了`numFolds`，用于控制有多少个折叠，`numThreads`，控制可以同时运行多少个线程，最后是`categories`。
- en: 'If we look at the relevant method in `src/com/lingpipe/cookbook/Util.java`,
    we see:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`src/com/lingpipe/cookbook/Util.java`中的相关方法，我们会看到：
- en: '[PRE56]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The method starts with the matching arguments for configuration information
    of logistic regression and running cross validation. Since cross validation is
    most often used in system tuning, all the relevant bits are exposed to modification.
    Everything is final because we are using an anonymous inner class to create threads.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法首先匹配逻辑回归的配置参数以及运行交叉验证。由于交叉验证最常用于系统调优，因此所有相关部分都暴露出来以供修改。由于我们使用了匿名内部类来创建线程，所以所有内容都是最终的。
- en: 'Next, we will set up `crossFoldEvaluator` that will collect the results from
    each thread:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置`crossFoldEvaluator`来收集每个线程的结果：
- en: '[PRE57]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we will get down to the business of creating threads for each fold, `i`:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将开始为每个折叠`i`创建线程的工作：
- en: '[PRE58]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The `XValidatingObjectCorpus` class is set up for multithreaded access by creating
    a thread-safe version of the corpus for reads with the `itemView()` method. This
    method returns a corpus that can have the fold set, but no data can be added.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`XValidatingObjectCorpus`类通过创建一个线程安全的语料库版本来设置多线程访问，该版本用于读取，方法为`itemView()`。此方法返回一个可以设置折叠的语料库，但无法添加数据。'
- en: 'Each thread is a `runnable` object, where the actual work of training and evaluating
    the fold is handled in the `run()` method:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个线程是一个`runnable`对象，实际的训练和评估工作在`run()`方法中完成：
- en: '[PRE59]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In this code, we started with training the classifier, which, in turn, requires
    a `try/catch` statement to handle `IOException` thrown by the `LogisticRegressionClassifier.train()`
    method. Next, we will create `withinFoldEvaluator` that will be populated within
    the thread without a synchronization issue:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先训练分类器，而这又需要一个 `try/catch` 语句来处理由 `LogisticRegressionClassifier.train()`
    方法抛出的 `IOException`。接下来，我们将创建 `withinFoldEvaluator`，它将在没有同步问题的情况下在线程中填充：
- en: '[PRE60]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'It is important that `storeInputs` be `true` so that the fold results can be
    added to `crossFoldEvaluator`:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要的是，`storeInputs` 必须为 `true`，这样才能将折叠结果添加到 `crossFoldEvaluator` 中：
- en: '[PRE61]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This method, also in `Util`, iterates over all the true positives and false
    negatives for each category and adds them to `crossFoldEvaluator`. Note that this
    is synchronized: this means that only one thread can access the method at a time,
    but given that classification has already been done, it should not be much of
    a bottleneck:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个方法，位于 `Util` 中，遍历每个类别的真正例和假阴性，并将它们添加到 `crossFoldEvaluator` 中。请注意，这是同步的：这意味着一次只有一个线程可以访问这个方法，但由于分类已经完成，所以这不应成为瓶颈：
- en: '[PRE62]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The method takes the true positives and false negatives from each category and
    adds them to the `crossFoldEval` evaluator. These are essentially copy operations
    that do not take long to compute.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法从每个类别中获取真正例和假阴性，并将它们添加到 `crossFoldEval` 评估器中。这些本质上是复制操作，计算时间非常短。
- en: 'Returning to `xvalLogRegMultiThread`, we will handle the exception and add
    the completed `Runnable` to our list of `Thread`:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到 `xvalLogRegMultiThread`，我们将处理异常并将完成的 `Runnable` 添加到我们的 `Thread` 列表中：
- en: '[PRE63]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'With all the threads set up, we will invoke `runThreads()` as well as print
    the confusion matrix that results. We will not go into the source of `runThreads()`,
    because it is a straightforward Java management of threads, and `printConfusionMatrix`
    has been covered in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple
    Classifiers*:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置好所有线程后，我们将调用 `runThreads()` 并打印出结果的混淆矩阵。我们不会深入讨论 `runThreads()` 的源码，因为它是一个简单的
    Java 线程管理，而 `printConfusionMatrix` 已在[第 1 章](ch01.html "第 1 章 简单分类器")中讲解过了，*简单分类器*：
- en: '[PRE64]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: That's it for really speeding up cross validation on multicore machines. It
    can make a big difference when tuning systems.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在多核机器上真正加速交叉验证的所有内容。调优系统时，它能带来很大的差异。
- en: Tuning parameters in logistic regression
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整逻辑回归中的参数
- en: Logistic regression presents an intimidating array of parameters to tweak for
    better performance, and working with it is a bit of black art. Having built thousands
    of these classifiers, we are still learning how to do it better. This recipe will
    point you in the general right direction, but the topic probably deserves its
    own book.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归提供了一系列令人头疼的参数，用于调整以提高性能，处理它有点像黑魔法。虽然我们已经构建了数千个此类分类器，但我们仍在学习如何做得更好。这个食谱会为你指引一个大致的方向，但这个话题可能值得单独一本书来讲解。
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: This recipe involves extensive changes to the source of `src/com/lingpipe/chapter3/TuneLogRegParams.java`.
    We will just run one configuration of it here, with most of the exposition in
    the *How it works…* section.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱涉及对 `src/com/lingpipe/chapter3/TuneLogRegParams.java` 源代码的大量修改。我们这里只运行它的一个配置，大部分内容都在
    *它是如何工作的……* 部分中阐述。
- en: 'Engage your IDE or type the following in the command line:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动你的 IDE 或在命令行中输入以下命令：
- en: '[PRE65]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The system then responds with cross-validation output confusion matrix for
    our default data in `data/disney_e_n.csv`:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统随后会响应并返回我们在 `data/disney_e_n.csv` 中默认数据的交叉验证输出混淆矩阵：
- en: '[PRE66]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we will report on false positives for each category—this will cover all
    the mistakes made:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将报告每个类别的假阳性——这将涵盖所有错误：
- en: '[PRE67]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This output is followed by the features, their coefficients, and a count—remember
    that we will see `n-1` categories, because one of the category''s features is
    set to `0.0` for all features:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该输出后面是特征、它们的系数以及一个计数——记住，我们将看到 `n-1` 类别，因为其中一个类别的特征对所有特征都设置为 `0.0`：
- en: '[PRE68]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, we have our standard I/O that allows for examples to be tested:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们有了标准的输入/输出，允许测试示例：
- en: '[PRE69]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This is the basic structure that we will work with. In the upcoming sections,
    we will explore the impact of varying parameters more closely.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是我们将要使用的基本结构。在接下来的章节中，我们将更仔细地探讨调整参数的影响。
- en: How it works…
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'This recipe assumes that you are familiar with logistic regression training
    and configuration from two recipes back and cross validation, which is the previous
    recipe. The overall structure of the code is presented in an outline form, with
    the tuning parameters retained. Modifying each parameter will be discussed later
    in the recipe—below we start with the `main()` method ignoring some code as indicated
    by ''`...`'' and the tunable code shown for tokenization and feature extraction:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设你已经熟悉两道食谱前的逻辑回归训练和配置，以及交叉验证，即前一篇食谱。代码的整体结构以大纲形式呈现，并保留了调优参数。每个参数的修改将在本食谱后续讨论——下面我们从`main()`方法开始，忽略了一些代码，如标记的`...`，并显示了用于分词和特征提取的可调代码：
- en: '[PRE70]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next the priors are set up:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来设置先验：
- en: '[PRE71]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Priors have a strong influence on the behavior coefficient assignment:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 先验对行为系数的分配有很大影响：
- en: '[PRE72]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The preceding code controls the search space of logistic regression:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码控制了逻辑回归的搜索空间：
- en: '[PRE73]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The preceding code runs cross validation to see how the system is doing—note
    the elided parameters with `...`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码运行交叉验证，以查看系统的表现——请注意省略的参数`...`。
- en: 'In the following code, we will set the number of folds to `0`, which will have
    the train method visit the entire corpus:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将折叠数设置为`0`，这将使训练方法遍历整个语料库：
- en: '[PRE74]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Then, for each category, we will print out the features and their coefficients
    for the just trained classifier:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个类别，我们将打印出刚刚训练的分类器的特征及其系数：
- en: '[PRE75]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Finally, we will have the usual console classifier I/O:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将进行常规的控制台分类器输入输出：
- en: '[PRE76]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Tuning feature extraction
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调优特征提取
- en: 'The features that are fed into logistic regression have a huge impact on the
    performance of the system. We will cover feature extraction in greater detail
    in the later recipes, but we will bring to bear one of the most useful and somewhat
    counter-intuitive approaches here, because it is very easy to execute—use character
    ngrams instead of words/tokens. Let''s look at an example:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到逻辑回归中的特征对系统性能有着巨大的影响。我们将在后续的食谱中详细讨论特征提取，但在这里我们将运用一种非常有用且有些反直觉的方法，因为它非常容易执行——使用字符n-gram而不是单词/标记。让我们来看一个例子：
- en: '[PRE77]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: This output indicates that the classifier is tied between `e` English and `n`
    non-English as a decision. Scrolling back through the features, we will see that
    there are no matches for any of the words in the input. There are some substring
    matches on the English side. `The` has the substring `he` for the feature word
    `the`. For language ID, it makes sense to consider subsequences, but as a matter
    of experience, it can be a big help for sentiment and other problems as well.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出表示分类器在`e`英语和`n`非英语之间做出决策时出现了纠结。回顾特征，我们会发现输入中的任何词汇都没有匹配项。英文学方面，有一些子串匹配。`The`包含了`he`，这是特征词`the`的子串。对于语言识别，考虑子序列是合理的，但根据经验，这对情感分析和其他问题也会有很大帮助。
- en: 'Modifying the tokenizer to be two-to-four-character ngrams is done as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 修改分词器为二到四字符的n-gram可以按如下方式进行：
- en: '[PRE78]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This results in the proper distinction being made:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就能正确地区分：
- en: '[PRE79]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The overall performance on cross validation drops a bit. For very small training
    sets, such as 21 tweets, this is not unexpected. Generally, the cross-validation
    performance with a consultation of what the mistakes look like and a look at the
    false positives will help guide this process.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中的总体表现略有下降。对于非常小的训练集，如21条推文，这是意料之中的。通常，通过查看错误的样子并观察误报，交叉验证的表现将有助于引导这个过程。
- en: 'In looking at the false positives, it is clear that `Disney` is a source of
    problems, because the coefficients on features show it to be evidence for English.
    Some of the false positives are:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 观察误报时，很明显`Disney`是问题的来源，因为特征上的系数表明它是英语的证据。部分误报包括：
- en: '[PRE80]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The following are the features for `e`:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`e`的特征：
- en: '[PRE81]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: In the absence of more training data, the features `!`, `Disney`, and `"` should
    be removed to help this classifier perform better, because none of these features
    are language specific, whereas `I` and `to` are, although not unique to English.
    This can be done by filtering the data or creating the appropriate tokenizer factory,
    but the best move is to probably get more data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏更多训练数据的情况下，特征`!`、`Disney`和`"`应当移除，以帮助分类器更好地表现，因为这些特征并不具有语言特异性，而`I`和`to`则有，尽管它们并不独特于英语。可以通过过滤数据或创建合适的分词器工厂来实现这一点，但最好的做法可能是获取更多数据。
- en: The `minFeature` count becomes useful when there is much more data, and you
    don't want logistic regression focusing on a very-low-count phenomenon because
    it tends to lead to overfitting.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据量大得多时，`minFeature`计数变得有用，因为你不希望逻辑回归专注于一个非常少量的现象，因为这往往会导致过拟合。
- en: 'Setting the `addInterceptFeature` parameter to `true` will add a feature that
    always fires. This will allow logistic regression to have a feature sensitive
    to the number of examples for each category. It is not the marginal likelihood
    of the category, as logistic regression will adjust the weight like any other
    feature—but the following priors show how it can be further tuned:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将`addInterceptFeature`参数设置为`true`将添加一个始终触发的特征。这将使逻辑回归具有一个对每个类别示例数量敏感的特征。它不是该类别的边际似然，因为逻辑回归会像对待其他特征一样调整权重——但以下的先验展示了如何进一步调优：
- en: '[PRE82]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The intercept is the strongest feature for `n` in the end, and the overall cross-validation
    performance suffered in this case.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，截距是最强的特征，而整体交叉验证性能在这种情况下受到了影响。
- en: Priors
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先验
- en: 'The role of priors is to restrict the tendency of logistic regression to perfectly
    fit the training data. The ones we use try in varying degrees to push coefficients
    to zero. We will start with the `nonInformativeIntercept` prior, which controls
    whether the intercept feature is subject to the normalizing influences of the
    prior—if true, then the intercept is not subject to the prior, which was the case
    in the preceding example. Setting it to `false` moved it much closer to zero from
    `-0.17`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 先验的作用是限制逻辑回归完美拟合训练数据的倾向。我们使用的先验在不同程度上尝试将系数推向零。我们将从`nonInformativeIntercept`先验开始，它控制截距特征是否受到先验的归一化影响——如果为`true`，则截距不受先验的影响，这在前面的例子中是这样的。将其设置为`false`后，截距从`-0.17`接近零：
- en: '[PRE83]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Next, we will adjust the variance of the prior. This sets an expected variation
    for the weights. A low variance means that coefficients are expected not to vary
    much from zero. In the preceding code, the variance was set to `2`. This is the
    result of setting it to `.01`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调整先验的方差。这为权重设置了一个预期的变异值。较低的方差意味着系数预期不会与零有太大变化。在前面的代码中，方差被设置为`2`。将其设置为`.01`后，结果如下：
- en: '[PRE84]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: This is a drop from 104 features with variance `2` to one feature for variance
    `.01`, because once a feature has dropped to `0`, it is removed.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从方差为`2`的104个特征减少到方差为`.01`时的一个特征，因为一旦某个特征的值降为`0`，它将被移除。
- en: 'Increasing the variance changes our top `e` features from `2` to `4`:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 增加方差将我们的前`e`个特征从`2`变为`4`：
- en: '[PRE85]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This is a total of 119 features.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这是119个特征的总数。
- en: 'Consider a variance of `2` and a `gaussian` prior:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 假设方差为`2`，并且使用`高斯`先验：
- en: '[PRE86]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We will get the following output:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE87]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Oddly, we spend very little time worrying about which prior we use, but variance
    has a big role in performance, because it can cut down the feature space quickly.
    Laplace is a commonly accepted prior for NLP applications.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，我们很少担心使用哪种先验，但方差在性能中起着重要作用，因为它可以迅速减少特征空间。拉普拉斯先验是自然语言处理应用中常见的先验。
- en: Consult the Javadoc and logistic regression tutorial for more information.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考Javadoc和逻辑回归教程获取更多信息。
- en: Annealing schedule and epochs
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 退火计划和迭代次数
- en: 'As logistic regression converges, the annealing schedule controls how the search
    space is explored and terminated:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 随着逻辑回归的收敛，退火计划控制了搜索空间的探索和终止：
- en: '[PRE88]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: When tuning, we will increase the first parameter to the annealing schedule
    by order of magnitude (`.0025,.025,..`) if the search is taking too long—often,
    we can increase the training speed without impacting the cross-validation performance.
    Also, the `minImprovement` value can be increased to have the convergence end
    earlier, which can both increase the training speed and prevent the model from
    overfitting—this is called **early stopping**. Again, your guiding light in this
    situation is to look at the cross-validation performance when making changes.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整时，如果搜索时间过长，我们将按数量级增大退火计划的第一个参数（`.0025, .025, ..`）——通常，我们可以在不影响交叉验证性能的情况下提高训练速度。此外，`minImprovement`值可以增加，以便更早结束收敛，这可以同时提高训练速度并防止模型过拟合——这被称为**早停**。在这种情况下，你的指导原则是在做出改变时查看交叉验证性能。
- en: The epochs required to achieve convergence can get quite high, so if the classifier
    is iterating to `maxEpochs -1`, this means that more epochs are required to converge.
    Be sure to set the `reporter.setLevel(LogLevel.INFO);` property or a more informative
    level to get the convergence report. This is another way to additionally force
    early stopping.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 达到收敛所需的训练轮次可能会相当高，因此如果分类器迭代到`maxEpochs -1`，这意味着需要更多的轮次来收敛。确保设置`reporter.setLevel(LogLevel.INFO);`属性或更详细的级别，以获取收敛报告。这是另外一种强制早停的方法。
- en: Parameter tuning is a black art that can only be learned through practice. The
    quality and quantity of training data is probably the dominant factor in classifier
    performance, but tuning can make a big difference as well.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 参数调优是一门黑艺术，只能通过实践来学习。训练数据的质量和数量可能是分类器性能的主要因素，但调优也能带来很大差异。
- en: Customizing feature extraction
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义特征提取
- en: 'Logistic regression allows for arbitrary features to be used. Features are
    any observations that can be made about data being classified. Some examples are
    as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归允许使用任意特征。特征是指可以对待分类数据进行的任何观察。一些例子包括：
- en: Words/tokens from the text.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中的单词/标记。
- en: We found that character ngrams work very well in lieu of words or stemmed words.
    For small data sets of less than 10,000 words of training, we will use 2-4 grams.
    Bigger training data can merit a longer gram, but we have never had good results
    above 8-gram characters.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现字符n-gram比单词或词干化后的单词效果更好。对于小数据集（少于10,000个训练单词），我们将使用2-4克。更大的训练数据集可以适合使用更长的gram，但我们从未在8-gram字符以上获得过好结果。
- en: Output from another component can be a feature, for example, a part-of-speech
    tagger.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自另一个组件的输出可以是一个特征，例如，词性标注器。
- en: Metadata known about the text, for example, the location of a tweet or time
    of the day it was created.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于文本的元数据，例如，推文的位置或创建时间。
- en: Recognition of dates and numbers abstracted from the actual value.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从实际值中抽象出的日期和数字的识别。
- en: How to do it…
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: The source for this recipe is in `src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java`.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱的来源在`src/com/lingpipe/cookbook/chapter3/ContainsNumberFeatureExtractor.java`中。
- en: 'Feature extractors are straightforward to build. The following is a feature
    extractor that returns a `CONTAINS_NUMBER` feature with weight `1`:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器非常容易构建。以下是一个返回`CONTAINS_NUMBER`特征，权重为`1`的特征提取器：
- en: '[PRE89]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'By adding a `main()` method, we can test the feature extractor:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加`main()`方法，我们可以测试特征提取器：
- en: '[PRE90]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Now run the following command:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行以下命令：
- en: '[PRE91]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The preceding code yields the following output:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码产生以下输出：
- en: '[PRE92]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: That's it. The next recipe will show you how to combine feature extractors.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。下一个食谱将展示如何组合特征提取器。
- en: There's more…
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Designing features is a bit of an art. Logistic regression is supposed to be
    robust in the face of irrelevant features, but overwhelming it with really dumb
    features will likely detract from performance.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 设计特征有点像艺术创作。逻辑回归应该能够应对无关特征，但如果用非常低级的特征来压倒它，可能会影响性能。
- en: One way to think about what features you need is to wonder what evidence from
    the text or environment helps you, the human, decide what the correct classification
    is. Try and ignore your world knowledge when looking at the text. If world knowledge,
    that is, France is a country, is important, then try and model this world knowledge
    with a gazetteer to generate `CONTAINS_COUNTRY_MENTION`.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过思考需要哪些特征来决定文本或环境中的哪些证据帮助你（人类）做出正确的分类决定。查看文本时，尽量忽略你的世界知识。如果世界知识（例如，法国是一个国家）很重要，那么尝试用地名词典来建模这种世界知识，以生成`CONTAINS_COUNTRY_MENTION`。
- en: Be aware that features are strings, and the only notion of equivalence is the
    exact string match. The `12:01pm` feature is completely distinct from `12:02pm`,
    although, to a human, these strings are very close, because we understand time.
    To get the similarity of these two features, you must have something like a `LUNCH_TIME`
    feature that is computed using time.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征是字符串，等价的唯一标准是完全匹配的字符串。`12:01pm`特征与`12:02pm`特征完全不同，尽管对人类而言，这两个字符串非常接近，因为我们理解时间。要获得这两个特征的相似性，必须有类似`LUNCH_TIME`的特征，通过时间计算得出。
- en: Combining feature extractors
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合特征提取器
- en: Feature extractors can be combined in much the same way as tokenizers in [Chapter
    2](ch02.html "Chapter 2. Finding and Working with Words"), *Finding and Working
    with Words*.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取器可以像[第2章](ch02.html "第2章. 查找与处理单词")中讲解的分词器一样组合使用，*查找与处理单词*。
- en: How to do it…
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: This recipe will show you how to combine the feature extractor from the previous
    recipe with a very common feature extractor over character ngrams.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将向你展示如何将前一个食谱中的特征提取器与一个常见的字符n-gram特征提取器结合使用。
- en: 'We will start with a `main()` method in `src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java`
    that we will use to run the feature extractor. The following lines set up features
    that result from the tokenizer using the LingPipe class, `TokenFeatureExtractor`:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从`src/com/lingpipe/cookbook/chapter3/CombinedFeatureExtractor.java`中的`main()`方法开始，使用它来运行特征提取器。以下行设置了通过LingPipe类`TokenFeatureExtractor`使用分词器产生的特征：
- en: '[PRE93]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Then, we will construct the feature extractor from the previous recipe.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将构建前一个食谱中的特征提取器。
- en: '[PRE94]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, the LingPipe class joining feature extractors, `AddFeatureExtractor`,
    joins the two into a third:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，LingPipe类`AddFeatureExtractor`将两个特征提取器结合成第三个：
- en: '[PRE95]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'The remaining code gets the features and prints them out:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩下的代码获取特征并打印出来：
- en: '[PRE96]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: Run the following command
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令
- en: '[PRE97]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The output looks like this:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE98]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: There's more…
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: The Javadoc references a broad range of feature extractors and combiners/filters
    to help manage the task of feature extraction. One slightly confusing aspect of
    the class is that the `FeatureExtractor` interface is in the `com.aliasi.util`
    package, and the implementing classes are all in `com.aliasi.features`.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc引用了广泛的特征提取器和组合器/过滤器，以帮助管理特征提取任务。这个类的一个稍微令人困惑的方面是，`FeatureExtractor`接口位于`com.aliasi.util`包中，而实现类都在`com.aliasi.features`中。
- en: Classifier-building life cycle
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类器构建生命周期
- en: 'At the top-level building, a classifier usually proceeds as follows:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶层构建中，分类器通常按以下步骤进行：
- en: Create training data—refer to the following recipe for more about this.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练数据—有关更多信息，请参阅以下食谱。
- en: Build training and evaluation infrastructure with sanity check.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建训练和评估基础设施并进行合理性检查。
- en: Establish baseline performance.
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立基准性能。
- en: Select optimization metric for classifier—this is what the classifier is trying
    to do and will guide tuning.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为分类器选择优化指标——这是分类器的目标，将引导调优过程。
- en: 'Optimize classifier via techniques such as:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下技术优化分类器：
- en: Parameter tuning
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调优
- en: Thresholding
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值处理
- en: Linguistic tuning
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言调优
- en: Adding training data
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加训练数据
- en: Refining classifier definition
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精细化分类器定义
- en: This recipe will present the first four steps in concrete terms, and there are
    recipes in this chapter for the optimization step.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将具体呈现前四个步骤，并且本章有优化步骤的食谱。
- en: Getting ready
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Nothing happens without training data for classifiers. Look at the *Annotation*
    recipe at the end of the chapter for tips on creating training data. You can also
    use an active learning framework to incrementally generate a training corpus (covered
    later in this chapter), which is the data used in this recipe.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 没有训练数据，分类器什么也做不了。请参考本章末尾的*注解*食谱，获取创建训练数据的提示。你也可以使用主动学习框架逐步生成训练语料库（稍后在本章中介绍），这就是本食谱中使用的数据。
- en: Next, reduce the risk by starting with the dumbest possible implementation to
    make sure that the problem being solved is scoped correctly, and that the overall
    architecture makes sense. Connect the assumed inputs to assumed outputs with simple
    code. We promise that most of the time, one or the other will not be what you
    thought it would be.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过从最简单的实现开始，减少风险，确保所解决的问题被正确界定，并且整体架构合理。用简单的代码将假设的输入与假设的输出连接起来。我们保证大多数情况下，输入或输出之一不会是你预期的。
- en: This recipe assumes that you are familiar with the evaluation concepts in [Chapter
    1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*, such as cross
    validation and confusion matrices, in addition to the logistic regression recipes
    covered so far.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱假设你已经熟悉[第1章](ch01.html "第1章 简单分类器")中的评估概念，如交叉验证和混淆矩阵，以及目前为止介绍的逻辑回归食谱。
- en: The entire source is at `src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 整个源代码位于`src/com/lingpipe/cookbook/chapter3/ClassifierBuilder.java`。
- en: This recipe also assumes that you can compile and run the code within your preferred
    development environment. The result of all the changes we are making is in `src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java`.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱还假设你可以在你首选的开发环境中编译和运行代码。我们所做的所有更改的结果位于`src/com/lingpipe/cookbook/chapter3/ClassifierBuilderFinal.java`。
- en: Note
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Big caveat in this recipe—we are using a tiny dataset to make basic points on
    classifier building. The sentiment classifier we are trying to build would benefit
    from 10 times more data.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中的一个重要警告——我们使用的是一个小型数据集来阐明分类器构建的基本要点。我们尝试构建的情感分类器如果有10倍的数据，将会受益匪浅。
- en: How to do it…
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行操作…
- en: 'We start with a collection of tweets that have been deduplicated and are the
    result of the *Train a little, learn a little – active learning* recipe that will
    follow this recipe. The starting point of the recipe is the following code:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一组已经去重的推文开始，这些推文是*训练一点，学习一点——主动学习*食谱的结果，并且会遵循本食谱。食谱的起始点是以下代码：
- en: '[PRE99]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Sanity check – test on training data
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理智检查——在训练数据上进行测试
- en: 'The first thing to do is get the system running and test on training data:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是让系统运行起来，并在训练数据上进行测试：
- en: 'We have left a print statement that advertises what is going on:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们留下了一个打印语句，用来展示正在发生的事情：
- en: '[PRE100]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Running `ClassifierBuilder` will yield the following:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`ClassifierBuilder`将得到以下结果：
- en: '[PRE101]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: The preceding confusion matrix is nearly a perfect system output, which validates
    that the system is basically working. This is the best system output you will
    ever see; never let management see it, or they will think this level of performance
    is either doable or done.
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述的混淆矩阵几乎是完美的系统输出，验证了系统基本正常工作。这是你能见到的最佳系统输出；永远不要让管理层看到它，否则他们会认为这种性能水平要么是可以实现的，要么是已经实现的。
- en: Establishing a baseline with cross validation and metrics
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过交叉验证和指标建立基准
- en: Now it is time to see what is really going on.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看实际情况了。
- en: 'If you have small data, then set the number of folds to `10` so that 90 percent
    of the data is used for training. If you have large data or are in a huge rush,
    then set it to `2`:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果数据量较小，则将折数设置为`10`，这样90%的数据用于训练。如果数据量较大或者时间非常紧迫，则将折数设置为`2`：
- en: '[PRE102]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Comment out or remove the training on test code:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释掉或移除测试代码中的训练部分：
- en: '[PRE103]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Plumb in a cross-validation loop or just uncomment the loop in our source:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 插入交叉验证循环，或者只需取消注释我们源代码中的循环：
- en: '[PRE104]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Recompiling and running the code will give us the following output:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新编译并运行代码将得到以下输出：
- en: '[PRE105]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'The classifier labels mean `p=positiveSentiment`, `n=negativeSentiment`, and
    `o=other`, which covered other languages or neutral sentiment. The first row of
    the confusion matrix indicates that the system gets `45` true positives, `8` false
    negatives that it thinks are `n`, and `17` false negatives that it thinks are
    `o`:'
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器标签表示`p=positiveSentiment`（正面情感），`n=negativeSentiment`（负面情感），`o=other`（其他），其中`o`涵盖了其他语言或中性情感。混淆矩阵的第一行表明，系统识别出`45`个真正的正例（true
    positives），`8`个被错误识别为`n`的负例（false negatives），以及`17`个被错误识别为`o`的负例：
- en: '[PRE106]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'To get the false positives for `p`, we need to look at the first column. We
    see that the system thought that `16` `n` annotations were `p` and `18` `o` annotations
    were `p`:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取`p`的假阳性（false positives），我们需要查看第一列。我们看到系统错误地将`16`个`n`标注为`p`，并且将`18`个`o`标注为`p`：
- en: '[PRE107]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: Tip
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The confusion matrix is the most honest and straightforward way to view/present
    results for classifiers. Performance metrics such as precision, recall, F-measure,
    and accuracy are all very slippery and often used incorrectly. When presenting
    results, always have a confusion matrix handy, because if we are in the audience
    or someone like us is, we will ask to see it.
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混淆矩阵是查看/展示分类器结果最诚实、最直接的方式。像精确度、召回率、F值和准确率等性能指标都是非常不稳定的，且经常被错误使用。展示结果时，始终准备好混淆矩阵，因为如果我们是观众或像我们一样的人，我们会要求查看它。
- en: Perform the same analysis for the other categories, and you will have an assessment
    of system performance.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对其他类别执行相同的分析，你将能够评估系统的性能。
- en: Picking a single metric to optimize against
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择一个单一的指标进行优化
- en: 'Perform the following steps:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'While the confusion matrix establishes the overall performance of the classifier,
    it is too complex to use as a tuning guide. You don''t want to have to digest
    the entire matrix every time you adjust a feature. You and your team must agree
    on a single number that, if it goes up, the system is considered better. The following
    metrics apply to binary classifiers; if there are more than two categories, then
    you will have to sum them somehow. Some common metrics we see are:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然混淆矩阵能建立分类器的整体性能，但它太复杂，无法作为调优指南。你不希望每次调整特征时都需要分析整个矩阵。你和你的团队必须达成一致，选定一个单一的数字，如果这个数字上升，系统就被认为变得更好。以下指标适用于二分类器；如果类别超过两个，你需要以某种方式对其求和。我们常见的一些指标包括：
- en: '**F-measure**: F-measure is an attempt to reward reductions in false negatives
    and false positives at the same time:'
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F-measure**：F-measure试图同时减少假阴性和假阳性的出现，从而给予奖励。'
- en: '*F-measure = 2*TP / (2*TP + FP + FN)*'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*F-measure = 2*TP / (2*TP + FP + FN)*'
- en: It is mainly an academic measure to declare that one system is better than another.
    It sees little use in industry.
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这主要是一个学术性指标，用于宣称一个系统比另一个系统更好。在工业界几乎没有什么用处。
- en: '**Recall at 90 percent precision**: The goal is to provide as much coverage
    as possible while not making more than 10 percent false positives. This is when
    the system does not want to look bad very often; this applies to spell checkers,
    question answering systems, and sentiment dashboards.'
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**90%精度下的召回率**：目标是提供尽可能多的覆盖范围，同时不产生超过10%的假阳性。这适用于那些不希望系统经常出错的场景；比如拼写检查器、问答系统和情感仪表盘。'
- en: '**Precision at 99.9 percent recall**: This metric supports *needle in the haystack*
    or *needle in the needle stack* kind of problems. The user cannot afford to miss
    anything and is willing to perhaps slog through lots of false positives as long
    as they don''t miss anything. The system is better if the false positive rate
    is lower. Use cases are intelligence analysts and medical researchers.'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**99.9%召回率下的精度**：这个指标适用于*大海捞针*或*针堆中的针*类型的问题。用户无法容忍遗漏任何信息，并愿意通过大量假阳性来换取不遗漏任何内容。如果假阳性率较低，系统会更好。典型的使用案例包括情报分析员和医学研究人员。'
- en: Determining this metric comes from a mixture of business/research needs, technical
    capability, available resources, and willpower. If a customer wants a high recall
    and high-precision system, our first question will be to ask what the budget is
    per document. If it is high enough, we will suggest hiring experts to correct
    system output, which is the best combination of what computers are good at (exhaustiveness)
    and what humans are good at (discrimination). Generally, budgets don't support
    this, so the balancing act begins, but we have deployed systems in just this way.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定这个指标需要结合业务/研究需求、技术能力、可用资源和意志力。如果客户希望系统实现高召回率和高精度，我们的第一个问题会是询问每个文档的预算是多少。如果预算足够高，我们会建议聘请专家来纠正系统输出，这是计算机擅长的（全面性）和人类擅长的（区分能力）最好的结合。通常，预算无法支持这种方式，因此需要进行平衡，但我们已经以这种方式部署了系统。
- en: For this recipe, we will pick a maximizing recall at 50-percent precision for
    `n` (negative), because we want to be sure to intercept any negative sentiment
    and will tolerate false positives. We will choose 65 percent for a `p` positive,
    because the good news is less actionable, and who doesn't love Disney? We don't
    care what `o` (other performance) is—the category exists for linguistic reasons,
    independent of the business use. This metric a likely metric for a sentiment-dashboard
    application. This means that the system will produce one mistake for every two
    guesses of a negative-sentiment category and 13 out of 20 for positive sentiment.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个配方，我们将选择在`n`（负面）上以50%的精度最大化召回率，因为我们希望确保拦截任何负面情绪，并且可以容忍假阳性。我们将选择65%的`p`（正面），因为好消息的可操作性较低，谁不喜欢迪士尼呢？我们不关心`o`（其他性能）——这个类别存在是出于语言学原因，与业务使用无关。这个指标是情感仪表盘应用程序可能采用的指标。这意味着系统在每两次负面情绪类别的预测中会出一个错误，在20次正面情绪的预测中会有13次正确。
- en: Implementing the evaluation metric
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现评估指标
- en: 'Perform the following steps to implement the evaluation metric:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以实现评估指标：
- en: 'We will start with reporting precision/recall for all categories with the `Util.printPrecRecall`
    method after printing out the confusion matrix:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在打印出混淆矩阵后，我们将使用`Util.printPrecRecall`方法报告所有类别的精度/召回率：
- en: '[PRE108]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The output will now look like this:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出现在看起来是这样的：
- en: '[PRE109]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: The precision for `n` exceeds our objective of `.5`–since we want to maximize
    recall at `.5`, we can make a few more mistakes before we get to the limit. You
    can refer to the *Thresholding classifiers* recipe to find out how to do this.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n`的精度超过了我们的目标`.5`——因为我们希望在`.5`时最大化召回率，所以在达到限制之前我们可以多犯一些错误。你可以参考*阈值分类器*配方，了解如何做到这一点。'
- en: 'The precision for `p` is 57 percent, and this is too low for our business objective.
    Logistic regression classifiers, however, provide a conditional probability that
    might allow us to meet the precision needs just by paying attention to the probability.
    Adding the following line of code will allow us to see the results sorted by conditional
    probability:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`p`的精度为57%，对于我们的商业目标来说，这个精度太低。然而，逻辑回归分类器提供的条件概率可能允许我们仅通过关注概率就能满足精度需求。添加以下代码行将允许我们查看按条件概率排序的结果：'
- en: '[PRE110]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'The preceding line of code starts by getting a `ScoredPrecisionRecallEvaluation`
    value from the evaluator. A double-scored curve (`[][])` is gotten from that object
    with the Boolean interpolate set to false, because we want the curve to be unadulterated.
    You can look at the Javadoc for what is going on. Then, we will use a print route
    from the same class to print out the curve. The output will look like this:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上面的代码行首先从评估器获取一个`ScoredPrecisionRecallEvaluation`值。然后，从该对象获取一个双重得分曲线（`[][]`），并将布尔插值设置为false，因为我们希望曲线保持原样。你可以查看Javadoc以了解发生了什么。接着，我们将使用同一类中的打印方法打印出该曲线。输出将如下所示：
- en: '[PRE111]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: The output is sorted by score, in the third column, which in this case, happens
    to be a conditional probability, so the max value is 1 and min value is 0\. Notice
    that the recall grows as correct cases are found (the second line), and it never
    goes down. However, when a mistake is made like in the fourth line, precision
    drops to `.6`, because 3 out of 5 cases are correct so far. The precision actually
    goes below `.65` before the last value is found—in bold, with a score of `.73`.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出按得分排序，得分列在第三列，在这种情况下，它恰好是一个条件概率，所以最大值是1，最小值是0。注意，随着正确的案例被发现（第二行），召回率不断上升，并且永远不会下降。然而，当发生错误时，例如在第四行，精度降到了`.6`，因为目前为止5个案例中有3个是正确的。在找到最后一个值之前，精度实际上会低于`.65`——它以`.73`的得分加粗显示。
- en: 'So, without any tuning, we can report that we can achieve 30 percent recall
    for `p` at our accepted precision limit of 65 percent. This requires that we threshold
    the classifier at `.73` for the category, which means if we reject scores less
    than `.73` for `p`, some comments are:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，在没有任何调优的情况下，我们可以报告，在接受的65%精度限制下，我们能够达到`p`的30%召回率。这要求我们将分类器的阈值设置为`.73`，即如果我们拒绝`p`的得分低于`.73`，一些评论是：
- en: We got lucky. Usually, the first classifier runs do not reveal an immediately
    useful threshold with default values.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们运气不错。通常，第一次运行分类器时，默认值不会立即揭示出有用的阈值。
- en: Logistic regression classifiers have a very nice property that they provide;
    they also provide conditional probability estimates for thresholding. Not all
    classifiers have this property—language models and naïve Bayes classifiers tend
    to push scores towards 0 or 1, making thresholding difficult.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归分类器有一个非常好的特性，它提供条件概率估计来进行阈值设置。并不是所有分类器都有这个特性——语言模型和朴素贝叶斯分类器通常将得分推向0或1，这使得阈值设置变得困难。
- en: As the training data is highly biased (this is from the *Train a little, learn
    a little – active learning* recipe that follows), we cannot trust this threshold.
    The classifier will have to be pointed at fresh data to set the threshold. Refer
    to the *Thresholding classifiers* recipe to see how this is done.
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练数据高度偏倚（这是接下来*训练一点，学习一点——主动学习*食谱中的内容），我们不能相信这个阈值。分类器必须指向新数据来设定阈值。请参考*阈值分类器*食谱，了解如何做到这一点。
- en: This classifier has seen very little data and will not be a good candidate for
    deployment despite the supporting evaluation. We would be more comfortable with
    at least 1,000 tweets from a diverse set of dates.
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个分类器看到的数据非常少，尽管支持评估，它仍然不是一个适合部署的好候选者。我们会更愿意至少有来自不同日期的1,000条推文。
- en: 'At this point in the process, we either accept the results by verifying that
    the performance is acceptable on fresh data or turn to improving the classifier
    by techniques covered by other recipes in this chapter. The final step of the
    recipe is to train up the classifier on all training data and write to disk:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们要么通过验证新数据上的表现来接受结果，要么通过本章其他食谱中的技术来改进分类器。食谱的最后一步是用所有训练数据训练分类器并写入磁盘：
- en: '[PRE112]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: We will use the resulting model in the *Thresholding classifiers* recipe.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*阈值分类器*的食谱中使用生成的模型。
- en: Linguistic tuning
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言调优
- en: This recipe will address issues around tuning the classifier by paying attention
    to the mistakes made by the system and making linguistic adjustments by adjusting
    parameters and features. We will continue with the sentiment use case from the
    previous recipe and work with the same data. We will start with a fresh class
    at `src/com/lingpipe/cookbook/chapter3/LinguisticTuning.java`.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将通过关注系统的错误并通过调整参数和特征进行语言学调整，来解决调优分类器的问题。我们将继续使用前一个方法中的情感分析用例，并使用相同的数据。我们将从`src/com/lingpipe/cookbook/chapter3/LinguisticTuning.java`开始。
- en: We have very little data. In the real world, we will insist on more training
    data—at least 100 of the smallest category, negative, are needed with a natural
    distribution of positives and others.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据非常少。在现实世界中，我们会坚持要求更多的训练数据——至少需要100个最小类别的负面数据，并且正面数据和其他类别要有自然的分布。
- en: How to do it…
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: We will jump right in and run some data—the default is `data/activeLearningCompleted/disneySentimentDedupe.2.csv`,
    but you can specify your own file in the command line.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直接开始运行一些数据——默认文件是`data/activeLearningCompleted/disneySentimentDedupe.2.csv`，但你也可以在命令行中指定自己的文件。
- en: 'Run the following in your command line or IDE equivalent:'
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行或IDE中运行以下命令：
- en: '[PRE113]'
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'For each fold, the features for the classifier will be printed. The output
    will look like the following for each category (just the first few features for
    each):'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一折，分类器的特征将被打印出来。每个类别的输出将如下所示（每个类别仅显示前几个特征）：
- en: '[PRE114]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: Starting with the `n` category, note that there are no features. It is a property
    of logistic regression that one category's features are all set to `0.0`, and
    the remaining `n-1` category's features are offset accordingly. This cannot be
    controlled, which is a bit annoying because the `n` or negative category can be
    the focus of linguistic tuning given how badly it performs in the example. Not
    to be deterred, we will move on.
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`n`类别开始，注意到没有特征。这是逻辑回归的一个特性，一个类别的所有特征都被设置为`0.0`，其余`n-1`个类别的特征会相应地偏移。这个问题无法控制，这有点令人烦恼，因为`n`或负面类别可以成为语言学调优的重点，考虑到它在示例中的表现非常差。但我们不灰心，我们继续前进。
- en: Note that the output is intended to make it easy to use a `find` command to
    locate feature output in the extensive reporting output. To find a feature search
    on `category <feature name>` to see if there is a nonzeroed report, search on
    `category <feature name> NON_ZERO`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，输出旨在使使用`find`命令定位特征输出变得容易。在广泛的报告输出中，可以通过`category <特征名称>`来查找特征，看看是否有非零报告，或者通过`category
    <特征名称> NON_ZERO`来进行搜索。
- en: 'We are looking for a few things in these features. First of all, there are
    apparently odd features that are getting big scores—the output is ranked in positive
    to negative for the category. What we want to look for is some signal in the feature
    weights—so `love` makes sense as being associated with a positive sentiment. Looking
    at features like this can really be surprising and counter intuitive. The uppercase
    `I` and lowercase `i` suggest that the text should be downcased. We will make
    this change and see if it helps. Our current performance is:'
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这些特征中寻找几个方面的东西。首先，显然有一些奇怪的特征得到了很高的分数——输出按类别从正到负排序。我们想要寻找的是特征权重中的某些信号——因此`love`作为与正面情绪相关联是有道理的。查看这些特征可能会令人感到惊讶且反直觉。大写字母的`I`和小写字母的`i`表明文本应该转换为小写。我们将进行此更改，看看它是否有所帮助。我们当前的表现是：
- en: '[PRE115]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The code change is to add a `LowerCaseTokenizerFactory` item to the current
    `IndoEuropeanTokenizerFactory` class:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码修改是将一个`LowerCaseTokenizerFactory`项添加到当前的`IndoEuropeanTokenizerFactory`类中：
- en: '[PRE116]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Run the code, and we will pick up some precision and recall:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码后，我们将提升一些精确度和召回率：
- en: '[PRE117]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The features are as follows:'
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征如下：
- en: '[PRE118]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'What''s the next move? The `minFeature` count is very low at `1`. Let''s raise
    it to `2` and see what happens:'
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步该怎么做？`minFeature`计数非常低，仅为`1`。我们将其提高到`2`，看看会发生什么：
- en: '[PRE119]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: This hurts performance by a few cases, so we will return to `1`. However, experience
    dictates that the minimum count goes up as more data is found to prevent overfitting.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样做会使性能下降几个案例，所以我们将返回到`1`。然而，经验表明，随着更多数据的获取，最小计数会增加，以防止过拟合。
- en: 'It is time for the secret sauce—change the tokenizer to `NGramTokenizer`; it
    tends to work better than standard tokenizers—we are now rolling with the following
    code:'
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是时候使用秘密武器了——将分词器更改为`NGramTokenizer`；它通常比标准分词器表现更好——我们现在使用以下代码：
- en: '[PRE120]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'This worked. We will pick up a few more cases:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样做有效。我们将继续处理更多的情况：
- en: '[PRE121]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'However, the features are now pretty hard to scan:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，现在的特征非常难以扫描：
- en: '[PRE122]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: We have found over the course of time that character ngrams are the features
    of choice for text-classifier problems. They seem to nearly always help, and they
    helped here. Look at the features, and you can recover that `love` is still contributing
    but in little bits, such as `lov`, `ov`, and `lo`.
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们发现，在一段时间的工作中，字符n-gram是文本分类问题的首选特征。它们几乎总是有帮助，而且这次也帮助了。查看这些特征，你可以发现`love`仍然在贡献，但仅以小部分的形式，如`lov`、`ov`和`lo`。
- en: There is another approach that deserves a mention, which is some of the tokens
    produced by `IndoEuropeanTokenizerFactory` are most likely useless, and they are
    just confusing the issue. Using a stop-word list, focusing on more useful tokenization,
    and perhaps applying a stemmer such as the Porter stemmer might work as well.
    This has been the traditional approach to these kinds of problems—we have never
    had that much luck with them.
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另有一种方法值得一提，即`IndoEuropeanTokenizerFactory`生成的一些标记很可能是无用的，它们只会让问题更加复杂。使用停用词列表，专注于更有用的标记化，并且可能应用像Porter词干提取器这样的工具也可能有效。这是解决此类问题的传统方法——我们从未对此有过太多的好运。
- en: 'It is a good time to check on the performance of the `n` category; we have
    been messing about with the model and should check it:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是检查`n`类别性能的好时机；我们已经对模型进行了一些修改，应该检查一下：
- en: '[PRE123]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'The output also reports false positives for `p` and `n`. We really don''t care
    much about `o`, except when it shows up as a false positive for the other categories:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出还报告了`p`和`n`的假阳性。我们对`o`并不太关心，除非它作为其他类别的假阳性出现：
- en: '[PRE124]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Looking at false positives, we can suggest changes to feature extraction. Recognizing
    quotes from `~Walt Disney` might help the classifier with `IS_DISNEY_QUOTE`.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看假阳性时，我们可以建议更改特征提取。识别`~Walt Disney`的引号可能有助于分类器识别`IS_DISNEY_QUOTE`。
- en: 'Also, looking at errors can point out errors in annotation, one can argue that
    the following is actually positive:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，查看错误可以指出标注中的问题，可以说以下情况实际上是积极的：
- en: '[PRE125]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'At this point, the system is somewhat tuned. The configuration should be saved
    someplace and the next steps are considered. They include the following:'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此时，系统已经做了一些调优。配置应该保存到某个地方，并考虑接下来的步骤。它们包括：
- en: Declare victory and deploy. Before deploying, be sure to test on novel data
    using all training data to train. The *Thresholding classifiers* recipe will be
    very useful.
  id: totrans-476
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宣布胜利并部署。在部署之前，务必使用所有训练数据来测试新数据。*阈值分类器*配方将非常有用。
- en: Annotate more data. Use the active learning framework in the following recipe
    to help identify high-confidence cases that are wrong and right. This will likely
    help more than anything with performance, especially with low-count data such
    as the kind we have been working with.
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注更多数据。使用以下配方中的主动学习框架帮助识别高置信度的正确和错误案例。这可能比任何事情都能更好地提升性能，尤其是在我们处理的低计数数据上。
- en: Looking at the epoch report, the system is never converging on its own. Increase
    the limit to 10,000 and see if this helps things.
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看着epoch报告，系统始终没有自行收敛。将限制提高到10,000，看看是否能有所帮助。
- en: 'The result of our tuning efforts was to improve the performance from:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的调优努力的结果是将性能从：
- en: '[PRE126]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'To the following:'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来是：
- en: '[PRE127]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: This is not a bad uptick in performance in exchange for looking at some data
    and thinking a bit about how to help the classifier do its job.
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过查看一些数据并思考如何帮助分类器完成任务，这种性能的提升并不算坏。
- en: Thresholding classifiers
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阈值分类器
- en: Logistic regression classifiers are often deployed with a threshold rather than
    the provided `classifier.bestCategory()` method. This method picks the category
    with the highest conditional probability, which, in a 3-way classifier, can be
    just above one-third. This recipe will show you how to adjust classifier performance
    by explicitly controlling how the best category is determined.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归分类器通常通过阈值进行部署，而不是使用`classifier.bestCategory()`方法。此方法选择具有最高条件概率的类别，而在一个三分类器中，这个概率可能刚刚超过三分之一。这个配方将展示如何通过显式控制最佳类别的确定方式来调整分类器性能。
- en: 'This recipe will consider the 3-way case with the `p`, `n`, and `o` labels
    and work with the classifier produced by the *Classifier-building life cycle*
    recipe earlier in this chapter. The cross-validation evaluation produced is:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方将考虑具有`p`、`n`和`o`标签的三分类问题，并与本章早些时候的*分类器构建生命周期*配方中产生的分类器一起工作。交叉验证评估结果为：
- en: '[PRE128]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: We will run novel data to set thresholds.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行新的数据来设置阈值。
- en: How to do it...
  id: totrans-489
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: Our business use case is that recall be maximized while `p` has `.65` precision
    and `n` has `.5` precision for reasons discussed in the *Classifier-building life
    cycle* recipe. The `o` category is not important in this case. The `p` category
    appears to be too low with `.57`, and the `n` category can increase recall as
    the precision is above `.5`.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的业务用例是，回忆率被最大化，同时`p`的精度为`.65`，`n`的精度为`.5`，具体原因请参见*分类器构建生命周期*部分。`o`类别在此情况下并不重要。`p`类别的精度似乎过低，只有`.57`，而`n`类别可以通过提高精度（超过`.5`）来增加回忆率。
- en: We cannot use the cross-validation results unless care has been taken to produce
    a proper distribution of annotations—the active learning approach used tends to
    not produce such distributions. Even with a good distribution, the fact that the
    classifier was likely tuned with cross validation means that it is most likely
    overfit to that dataset because tuning decisions were made to maximize performance
    of those sets that are not general to new data.
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除非小心地生成了适当的标注分布，否则我们不能使用交叉验证结果——所使用的主动学习方法往往不能生成这样的分布。即使有了良好的分布，分类器可能也已经通过交叉验证进行了调整，这意味着它很可能对那个数据集进行了过拟合，因为调优决策是为了最大化那些并不适用于新数据的集合的性能。
- en: We need to point the trained classifier at new data—the rule of thumb is to
    train by hook or crook but always threshold on fresh. We followed the *Getting
    data from the Twitter API* recipe in [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, and downloaded new data from Twitter with
    the `disney` query. Nearly a year has passed since our initial search, so the
    tweets are most likely non-overlapping. The resulting 1,500 tweets were put into
    `data/freshDisney.csv`.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将训练好的分类器应用于新数据——经验法则是，不惜一切代价进行训练，但始终在新数据上进行阈值调整。我们遵循了[第1章](ch01.html "第1章.
    简单分类器")、*简单分类器*中的*从Twitter API获取数据*方法，并使用`disney`查询从Twitter下载了新的数据。距离我们最初的搜索已经快一年了，所以这些推文很可能没有重叠。最终得到的1,500条推文被保存在`data/freshDisney.csv`文件中。
- en: Ensure that you don't run this code on data that is not backed up. The I/O is
    simple rather than robust. The code overwrites the input file.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保不要在未备份数据的情况下运行此代码。I/O操作比较简单而不够健壮。此代码会覆盖输入文件。
- en: 'Invoke `RunClassifier` on your IDE or run the following command:'
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的IDE中调用`RunClassifier`，或者运行以下命令：
- en: '[PRE129]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: Open the `.csv` file in your favorite spreadsheet. All tweets should have a
    score and a guessed category in the standard annotation format.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你喜欢的电子表格中打开`.csv`文件。所有推文应具有得分和猜测的类别，格式符合标准标注格式。
- en: Sort with the primary sort on the `GUESS` column in the ascending or descending
    order and then sort on `SCORE` in the descending order. The result should be each
    category with higher scores descending to lower scores. This is how we set up
    top-down annotations.![How to do it...](img/4672OS_03_01.jpg)
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先按`GUESS`列进行升序或降序排序，然后按`SCORE`列进行降序排序。结果应是每个类别的得分从高到低排列。这就是我们如何设置自上而下的标注。![如何操作...](img/4672OS_03_01.jpg)
- en: Setting up sort of data for top-down annotation. All categories are grouped
    together, and a descending sort of the score is established.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为自上而下的标注设置数据排序。所有类别被分组在一起，并根据得分进行降序排序。
- en: For the categories that you care about, in this case, `p` and `n`, annotate
    truth from the highest score to the lowest scores until it is likely that the
    precision goal has been broached. For example, annotate `n` until you either run
    out of `n` guesses, or you have enough mistakes that you have `.50` precision.
    A mistake is when the truth is `o` or `p`. Do the same for `p` until you have
    a precision of `.65`, or you run out of number of `p`. For our canned example,
    we have put the annotations in `data/freshDisneyAnnotated.csv`.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于你关心的类别，在本例中是`p`和`n`，从最高得分到最低得分进行标注，直到可能达到精度目标。例如，标注`n`类别，直到你用完所有`n`类别的猜测，或者你已经有了足够多的错误，使得精度降至`.50`。错误是指实际类别是`o`或`p`。对`p`类别做同样的标注，直到精度达到`.65`，或者`p`类别的数量用完。对于我们的示例，我们已将标注数据放入`data/freshDisneyAnnotated.csv`。
- en: 'Run the following command or the equivalent in your IDE (note that we are supplying
    the input file and not using the default):'
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令或在你的IDE中运行等效命令（注意我们提供了输入文件，而不是使用默认文件）：
- en: '[PRE130]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'This command will produce the following output:'
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该命令将生成以下输出：
- en: '[PRE131]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'First off, this is a surprisingly good system performance for our minimally
    trained classifier. `p` is very close to the target precision of `.65` without
    thresholding, and coverage is not bad: it is found as 141 true positives out of
    1,500 tweets. As we have not annotated all 1,500 tweets, we cannot truly say what
    the recall of the classifier is, so the term is overloaded in common use. The
    `n` category is not doing as well, but it is still pretty good. Our annotation
    did no annotations for the `o` category, so the system column is all zeros.'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，对于我们这个训练最小化的分类器来说，系统性能出乎意料地好。`p` 在没有阈值化的情况下接近目标精度 `.65`，覆盖率也不差：在1,500条推文中找到了141个真正的正例。由于我们没有标注所有的1,500条推文，因此无法准确地说出分类器的召回率是多少，所以这个术语在日常使用中是被滥用的。`n`
    类别的表现没那么好，但仍然相当不错。我们没有对 `o` 类别进行任何标注，因此系统这一列全是零。
- en: 'Next, we will look at the precision/recall/score curve for thresholding guidance:'
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看用于阈值设定指导的精度/召回率/分数曲线：
- en: '[PRE132]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Most values have been elided to save space in the preceding output. We saw
    that the point at which the classifier passes `.65` precision has a score of `.525`.
    This means that we can expect 65-percent precision if we threshold at `.525` with
    a bunch of caveats:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了节省空间，前面的输出中大部分值都被省略了。我们看到，分类器在精度达到 `.65` 时的分数是 `.525`。这意味着，如果我们将阈值设置为 `.525`，就可以预期得到65%的精度，但有一些附加条件：
- en: This is a single-point sample without a confidence estimate. There are more
    sophisticated ways to arrive at a threshold that is beyond the scope of this recipe.
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个没有置信度估计的单点样本。还有更复杂的方法来确定阈值，但它超出了本食谱的范围。
- en: Time is a big contributor to variance in performance.
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间是影响性能方差的一个重要因素。
- en: 10-percent variance in performance for well-developed classifiers is not uncommon
    in practice. Factor this into performance requirements.
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于经过充分开发的分类器，10%的性能方差在实践中并不少见。需要将这一点考虑到性能要求中。
- en: The nice thing about the preceding curve is that it looks like we can provide
    a `.80` precision classifier at a threshold of `.76` with nearly 30 percent of
    the coverage of the `.65` precision classifier if we decide that higher precision
    is called for.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的曲线的一个优点是，看起来我们可以在 `.76` 的阈值下提供一个 `.80` 精度的分类器，且覆盖率几乎为 `.65` 精度分类器的30%，如果我们决定要求更高的精度。
- en: 'The `n` case has a curve that looks like this:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n` 类别的情况呈现出如下曲线：'
- en: '[PRE133]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: It looks like a threshold of `.549` gets the job done. The rest of the recipe
    will show how you to set up the thresholded classifier now that we have the thresholds.
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来 `.549` 的阈值能够完成任务。接下来的步骤将展示如何在确定了阈值之后设置阈值分类器。
- en: The code behind `RunClassifier.java` offers nothing of novelty in the context
    of this chapter, so it is left to you to work through.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '`RunClassifier.java` 背后的代码在本章的上下文中没有什么新意，因此它留给你自己去研究。'
- en: How it works…
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The goal is to create a classifier that will assign `p` to a tweet if it scores
    above `.525` for that category and `n` if scores above `.549` for that category;
    otherwise, it gets `o`. Wrong….management saw the p/r curve and now insists that
    `p` must be 80-percent precise, which means that the threshold will be `.76`.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建一个分类器，如果某个类别的分数高于 `.525`，则将 `p` 分配给该推文，如果分数高于 `.549`，则分配 `n`；否则，分配 `o`。错误……管理层看到了精度/召回曲线，现在坚持要求
    `p` 必须达到80%的精度，这意味着阈值将是 `.76`。
- en: The solution is very simple. If a score for `p` is below `.76`, then it will
    be rescored down to `0.0`. Likewise, if a score for `n` is below `.54`, then it
    will be rescored down to `0.0`. The effect of this is that `o` will be the best
    category for all below-threshold cases, because `.75` `p` can at best be `.25`
    `n`, which remains below the `n` threshold, and `.53` `n` can at most be `.47`
    `p`, which is below that category's threshold. This can get complicated if all
    categories are thresholded, or the thresholds are low.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案非常简单。如果 `p` 的分数低于 `.76`，则它将被重新评分为 `0.0`。同样，如果 `n` 的分数低于 `.54`，则它也将被重新评分为
    `0.0`。这样做的效果是，对于所有低于阈值的情况，`o` 将是最佳类别，因为 `.75` 的 `p` 最多只能是 `.25` 的 `n`，这仍然低于 `n`
    的阈值，而 `.53` 的 `n` 最多只能是 `.47` 的 `p`，这也低于该类别的阈值。如果对所有类别都设定阈值，或者阈值较低，这可能会变得复杂。
- en: Stepping back, we are taking a conditional classifier where all the category
    scores must sum to 1 and breaking this contract, because we will take any estimate
    for `p` that is below `.76` and bust it down to `0.0`. It is a similar story for
    `n`. The resulting classifier will now have to be `ScoredClassifier` because this
    is the next most specific contract in the LingPipe API that we can uphold.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 回过头来看，我们正在处理一个条件分类器，在这个分类器中所有类别的得分之和必须为1，我们打破了这一契约，因为我们会将任何`p`值低于`.76`的估算值降为`0.0`。`n`也有类似的情况。最终得到的分类器必须是`ScoredClassifier`，因为这是LingPipe
    API中我们能遵循的下一个最具体的契约。
- en: 'The code for this class is in `src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier`.
    At the top level, we have the class, relevant member variable, and constructor:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的代码在`src/com/lingpipe/cookbook/chapter3/ThresholdedClassifier`中。在顶层，我们有类、相关的成员变量和构造函数：
- en: '[PRE134]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'Next, we will implement the only required method for `ScoredClassification`,
    and this is where the magic happens:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现`ScoredClassification`的唯一必需方法，这就是魔法发生的地方：
- en: '[PRE135]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: The complicated bit about scored classifications is that scores have to be assigned
    to all categories even if the score is `0.0`. The mapping from a conditional classification,
    where all scores sum to `1.0`, does not lend itself to a generic solution, which
    is why the preceding ad hoc implementation is used.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 关于得分分类的复杂之处在于，即使得分为`0.0`，也必须为所有类别分配得分。从条件分类映射中得知，所有得分之和为`1.0`，而这种映射并不适合通用解决方案，这也是为什么使用前述临时实现的原因。
- en: 'There is also a `main()` method that spools up the relevant bits for `ThresholdedClassifier`
    and applies them:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 还包括一个`main()`方法，它初始化了`ThresholdedClassifier`的相关部分并加以应用：
- en: '[PRE136]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The thresholds are doing exactly as designed; `p` is `.79` precision, which
    is close enough for consulting, and `n` is spot on. The source for the `main()`
    method should be straightforward given the context of this chapter.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值正如设计时所预期的那样；`p`是`.79`的精度，这对于咨询来说已经足够接近，而`n`则完全准确。考虑到本章的背景，`main()`方法的源码应该是直观的。
- en: That's it. Almost never do we release a nonthresholded classifier, and best
    practices require that thresholds be set on held-out data, preferably from later
    epochs than the training data. Logistic regression is quite robust against skewed
    training data, but the ointment that cleanses the flaws of skewed data is novel
    data annotated top down to precision objectives. Yes, it is possible to threshold
    with cross validation, but it suffers from the flaws that overfit due to tuning,
    and you would screw up your distributions. Recall-oriented objectives are another
    matter.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。几乎从不发布没有阈值的分类器，最佳实践要求在保留数据上设置阈值，最好是来自比训练数据晚的时期。逻辑回归对偏斜的训练数据非常鲁棒，但清洗偏斜数据缺陷的良方是使用从上到下标注的全新数据，以实现精度目标。是的，使用交叉验证也可以进行阈值设定，但它会受到过拟合的缺陷，且会弄乱你的分布。以召回为导向的目标则是另一回事。
- en: Train a little, learn a little – active learning
  id: totrans-529
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一点，学习一点——主动学习
- en: 'Active learning is a super power to quickly develop classifiers. It has saved
    many a project in the real world. The idea is very simple and can be broken down
    as follows:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习是快速开发分类器的超级能力。它已经挽救了许多现实世界的项目。其思想非常简单，可以分解为以下几点：
- en: Assemble a packet of raw data that is way bigger than you can annotate manually.
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汇总一批远大于你能够手动标注的原始数据。
- en: Annotate an embarrassingly small amount of the raw data.
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标注一些尴尬的少量原始数据。
- en: Train the classifier on the embarrassingly small amount of training data.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在那少得可怜的训练数据上训练分类器。
- en: Run the trained classifier on all the data.
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有数据上运行训练好的分类器。
- en: Put the classifier output into a `.csv` file ranked by confidence of best category.
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类器输出保存到一个`.csv`文件中，按最佳类别的置信度进行排名。
- en: Correct another embarrassingly small amount of data, starting with the most
    confident classifications.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修正另一些尴尬的少量数据，从最自信的分类开始。
- en: Evaluate the performance.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估性能。
- en: Repeat the process until the performance is acceptable, or you run out of data.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这个过程，直到性能可接受，或者数据耗尽为止。
- en: If successful, be sure to evaluate/threshold on fresh data, because the active
    learning process can introduce biases to the evaluation.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果成功，确保在新数据上进行评估/阈值调整，因为主动学习过程可能会给评估带来偏差。
- en: What this process does is help the classifier distinguish the cases where it
    is making higher confidence mistakes and correcting it. It also works as a classification-driven
    search engine of sorts, where the positive training data functions as the query,
    and the remaining data functions as the index being searched.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程的作用是帮助分类器区分其做出高置信度错误并进行更正的案例。它还可以作为某种分类驱动的搜索引擎，其中正面训练数据充当查询，而剩余数据则充当正在搜索的索引。
- en: Traditionally, active learning is applied to the near-miss cases where the classifier
    is unsure of the correct class. In this case, the corrections will apply to the
    lowest confidence classifications. We came up with the high-confidence correction
    approach because we were under pressure to increase precision with a thresholded
    classifier that only accepted high-confidence decisions.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，主动学习被应用于分类器不确定正确类别的接近失误案例。在这种情况下，更正将适用于最低置信分类。我们提出了高置信度更正方法，因为我们面临的压力是使用仅接受高置信度决策的分类器来提高精度。
- en: Getting ready
  id: totrans-542
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: What is going on here is that we are using the classifier to find more data
    that looks like what it knows about. For problems where the target classes are
    rare in the unannotated data, it can very quickly help the system identify more
    examples of the class. For example, in a binary-classification task with marginal
    probability of 1 percent for the target class in the raw data, this is almost
    certainly the way to go. You cannot ask annotators to reliably mark a 1-in-100
    phenomenon over time. While this is the right way to do it, the end result is
    that it will not be done because of the effort involved.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这里正在使用分类器来查找更多类似其所知数据的数据。对于目标类在未注释数据中罕见的问题，它可以帮助系统快速识别该类的更多示例。例如，在原始数据中二元分类任务中，目标类的边际概率为1%时，这几乎肯定是应该采取的方法。随着时间的推移，您无法要求注释者可靠地标记1/100的现象。虽然这是正确的做法，但最终结果是由于所需的工作量而未完成。
- en: Like most cheats, shortcuts, and super powers, the question to ask is what is
    the price paid. In the duality of precision and recall, recall suffers with this
    approach. This is because the approach biases annotation towards known cases.
    Cases that have very different wording are unlikely to be found, so coverage can
    suffer.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数作弊、捷径和超能力一样，要问的问题是付出的代价是什么。在精度和召回的二元对立中，召回率会因此方法而受到影响。这是因为这种方法偏向于已知案例的注释。很难发现具有非常不同措辞的案例，因此覆盖面可能会受到影响。
- en: How to do it…
  id: totrans-545
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let''s get started with active learning:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始主动学习吧：
- en: Collect some training data in our `.csv` format from [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, or use our example data in `data/activeLearning/disneyDedupe.0.csv`.
    Our data builds on the Disney tweets from [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*. Sentiment is a good candidate for active
    learning, because it benefits from quality training data and creating quality
    training data can be difficult. Use the `.csv` file format from the Twitter search
    downloader if you are using your own data.
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[第1章](ch01.html "第1章。简单分类器")的*简单分类器*中收集我们的`.csv`格式的培训数据，或使用我们在`data/activeLearning/disneyDedupe.0.csv`中的示例数据。我们的数据基于[第1章](ch01.html
    "第1章。简单分类器")的迪士尼推文。情感是主动学习的良好候选，因为它受益于高质量的培训数据，而创建高质量的培训数据可能很困难。如果您正在使用自己的数据，请使用Twitter搜索下载程序的`.csv`文件格式。
- en: Run the `.csv` deduplication routine from the *Eliminate near duplicates with
    Jaccard distance* recipe of [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"),
    *Simple Classifiers* to get rid of near-duplicate tweets. We have already done
    this with our example data. We went from 1,500 tweets to 1,343.
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行来自[第1章](ch01.html "第1章。简单分类器")的*用Jaccard距离消除近似重复*食谱中的`.csv`重复消除例程，*简单分类器*，以消除近似重复的推文。我们已经对我们的示例数据执行了此操作。我们从1,500条推文减少到了1,343条。
- en: 'If you have your own data, annotate around 25 examples in the `TRUTH` column
    according to the standard annotation:'
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您有自己的数据，请根据标准注释在`TRUTH`列中注释大约25个示例：
- en: '`p` stands for positive sentiment'
  id: totrans-550
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`代表积极情绪'
- en: '`n` stands for negative sentiment'
  id: totrans-551
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`代表负面情绪'
- en: '`o` stands for other, which means that no sentiment is expressed, or the tweet
    is not in English'
  id: totrans-552
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`o`代表其他，这意味着未表达情绪，或推文不是英文'
- en: Be sure to get a few examples of each category
  id: totrans-553
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保每个类别都有几个示例
- en: Our example data is already annotated for this step. If you are using your own
    data, be sure to use the format of the first file (that has the `0.csv` format),
    with no other `.` in the path.
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的示例数据已经为此步骤进行了注释。如果你使用的是自己的数据，请务必使用第一个文件的格式（即`0.csv`格式），路径中不能有其他的`.`。
- en: '![How to do it…](img/4672OS_03_02.jpg)'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4672OS_03_02.jpg)'
- en: Examples of tweets annotated. Note that all categories have examples.
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注释推文的示例。请注意，所有类别都有示例。
- en: 'Run the following command. Do not do this on your own annotated data without
    backing up the file. Our I/O routine is written for simplicity, not robustness.
    You have been warned:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令。请勿在自己注释过的数据上执行此操作，除非先备份文件。我们的输入/输出例程是为了简化，而不是为了健壮性。你已被警告：
- en: '[PRE137]'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Pointed at the supplied annotated data, this will print the following to the
    console with a final suggestion:'
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指向提供的注释数据，这将向控制台打印以下内容，并给出最终建议：
- en: '[PRE138]'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'This recipe will show you how to make it better, mainly by making it bigger
    in smart ways. Let''s see where we stand:'
  id: totrans-561
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个配方将展示如何通过智能的方式将其做得更好，主要是通过智能地扩展它。让我们看看当前的进展：
- en: The data has been annotated a bit for three categories
  id: totrans-562
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已经为三个类别进行了少量注释
- en: Of 1,343 tweets, there have been 25 annotated, 13 of which are `o`, which we
    don't particularly care about given the use case, but they still are important
    because they are not `p` or `n`
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在1,343条推文中，有25条被注释，其中13条是`o`，由于使用案例的关系我们并不特别关心这些，但它们仍然很重要，因为它们不是`p`或`n`
- en: This is not nearly enough annotated data to build a reliable classifier with,
    but we can use it to help annotate more data
  id: totrans-564
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这还远远不足以构建一个可靠的分类器，但我们可以用它来帮助注释更多的数据
- en: The last line encourages more annotation and the name of a file to annotate
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一行鼓励更多的注释，并注明要注释的文件名
- en: The precision and recall are reported for each category, that is, the result
    of cross validation over the training data. There is also a confusion matrix.
    At this point, we are not expecting very good performance, but `p` and `o` are
    doing quite well. The `n` category is not doing well at all.
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 报告了每个类别的精确度和召回率，也就是对训练数据进行交叉验证的结果。还有一个混淆矩阵。在这一点上，我们不指望有非常好的表现，但`p`和`o`表现得相当不错。`n`类别的表现则非常差。
- en: 'Next, fire up a spreadsheet, and import and view the indicated `.csv` file
    using a UTF-8 encoding. OpenOffice shows us the following:'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，打开一个电子表格，使用UTF-8编码导入并查看指定的`.csv`文件。OpenOffice会显示以下内容：
- en: '![How to do it…](img/4672OS_03_03.jpg)'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/4672OS_03_03.jpg)'
- en: Initial output of the active learning approach
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活跃学习方法的初始输出
- en: Reading from the left-hand side to the right-hand side, we will see the `SCORE`
    column, which reflects the classifier's confidence; its most likely category,
    shown in the `GUESS` column, is correct. The next column is the `TRUTH` class
    as determined by a human. The last `TEXT` column is the tweet being classified.
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从左到右阅读，我们会看到`SCORE`列，它反映了分类器的信心；其最可能的类别，显示在`GUESS`列中，是正确的。下一个列是`TRUTH`类，由人工确定。最后的`TEXT`列是正在分类的推文。
- en: 'All 1,343 tweets have been classified in one of two ways:'
  id: totrans-571
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有1,343条推文已经按照两种方式之一进行分类：
- en: If the tweet had an annotation, that is, an entry in the `TRUTH` column, then
    the annotation was made when the tweet was in the test fold of a 10-fold cross
    validation. Line 13 is just such a case. In this case, the classification was
    `o`, but the truth was `p`, so it would be a false negative for `p`.
  id: totrans-572
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果推文有注释，即`TRUTH`列中有条目，那么该注释是在推文处于10折交叉验证的测试折叠时进行的。第13行就是这样的情况。在这种情况下，分类结果为`o`，但真实值是`p`，因此它会是`p`的假阴性。
- en: If the tweet was not annotated, that is, no entry in the `TRUTH` column, then
    it was classified using all the available training data. All other examples in
    the shown spreadsheet are handled this way. They don't inform the evaluation at
    all. We will annotate these tweets to help improve classifier performance.
  id: totrans-573
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果推文没有注释，即`TRUTH`列没有条目，那么它是使用所有可用的训练数据进行分类的。显示的电子表格中的所有其他示例都是这种处理方式。它们不会对评估产生任何影响。我们将注释这些推文，以帮助提高分类器的性能。
- en: Next, we will annotate high-confidence tweets irrespective of category, as shown
    in the following screenshot:![How to do it…](img/4672OS_03_04.jpg)
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将注释高置信度的推文，不管它们属于哪个类别，如下截图所示：![如何操作...](img/4672OS_03_04.jpg)
- en: Corrected output for active learning output. Note the dominance of the o category.
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活跃学习输出的修正结果。注意`o`类别的主导地位。
- en: Annotating down to line 19, we will notice that most of the tweets are `o` and
    are dominating the process. There are only three `p` and no `n`. We need to get
    some `n` annotations.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注释到第19行时，我们会注意到大多数推文是`o`，它们主导了整个过程。只有三个`p`，没有`n`。我们需要一些`n`注释。
- en: We can focus on likely candidate `n` annotations by selecting the entire sheet,
    except for the headers, and sorting by column **B** or `GUESS`. Scrolling to the
    `n` guesses, we should see the highest confidence examples. In the following screenshot,
    we have annotated all the `n` guesses because the category needs data. Our annotations
    are in `data/activeLearningCompleted/disneySentimentDedupe.1.csv`. If you want
    to exactly duplicate the recipe, you will have to copy this file to the `activeLearning`
    directory.![How to do it…](img/4672OS_03_05.jpg)
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过选择整个表格（不包括标题），然后按**B**列或`GUESS`列排序，来专注于可能的`n`注释。滚动到`n`猜测时，我们应该看到最高置信度的示例。在下图中，我们已经注释了所有的`n`猜测，因为该类别需要数据。我们的注释位于`data/activeLearningCompleted/disneySentimentDedupe.1.csv`中。如果你想严格按照本教程操作，你需要将该文件复制到`activeLearning`目录中。![如何操作…](img/4672OS_03_05.jpg)
- en: Annotations sorted by category with very few n or negative categories.
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 按类别排序的注释中，负例或`n`类别非常少。
- en: Scrolling to the `p` guesses, we annotated a bunch as well.![How to do it…](img/4672OS_03_06.jpg)
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到`p`猜测，我们也注释了一些。![如何操作…](img/4672OS_03_06.jpg)
- en: Positive labels with corrections and surprising number of negatives
  id: totrans-580
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有修正的正例标签和令人惊讶的负例数量
- en: There are eight negative cases that we found in the `p` guesses mixed in with
    lots of `p` and some `o` annotations.
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在`p`猜测中找到了八个负例，这些负例与大量的`p`和一些`o`注释混合在一起。
- en: 'We will save the file without changing the filename and run the same program
    as we did earlier:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将保存文件，不更改文件名，并像之前一样运行程序：
- en: '[PRE139]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'The output will be as follows:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE140]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'This is a fairly typical output early in the annotation process. Positive `p`,
    the easy category, is dragging along at 49-percent precision and 45-percent recall.
    Negative `n` is even worse. Undaunted, we will do another round of annotation
    on the output file indicating focus on `n` guesses to help that category improve
    performance. We will save and rerun the file:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是注释过程早期的典型输出。正例`p`（简单类别）以49%的精准度和45%的召回率拖慢了进度。负例`n`的表现更差。我们毫不气馁，计划对输出文件做另一轮注释，专注于`n`猜测，帮助这个类别提高表现。我们将保存并重新运行该文件：
- en: '[PRE141]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: This last round of annotation got us over the edge (remember to copy over our
    annotation from `activeLearningCompleted/disneySentimentDedupe.2.csv` if you are
    mirroring the recipe exactly). We annotated high-confidence examples from both
    `p` and `n`, adding nearly 100 examples. The first best annotation for `n` is
    above 50-percent precision with 41-percent recall. We assume that there will be
    a tunable threshold that meets our 80-percent requirement for `p` and declares
    victory in 211 moves, which is much less than the total 1,343 annotations.
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这最后一轮注释让我们突破了瓶颈（如果你是严格按照这个流程进行的，请记得从`activeLearningCompleted/disneySentimentDedupe.2.csv`复制我们的注释）。我们从`p`和`n`中注释了高置信度的示例，增加了近100个示例。`n`类别的最佳注释达到50%以上的精准度和41%的召回率。我们假设会有一个可调阈值，满足`p`的80%要求，并在211步内完成胜利，这比总共的1,343个注释要少得多。
- en: That's it. This is a real-world example and the first example we have tried
    for the book, so the data is not cooked. The approach tends to work, although
    no promises; some data resists even the most focused efforts of a well-equipped
    computational linguist.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就这样。这是一个真实的示例，也是我们为这本书尝试的第一个示例，所以数据没有经过修改。这个方法通常有效，尽管不能保证；一些数据即便是经过训练有素的计算语言学家的高度集中努力，也会抵抗分析。
- en: Be sure to store the final `.csv` file some place safe. It would be a shame
    to lose all that directed annotation.
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保将最终的`.csv`文件存储在安全的位置。丢失所有这些定向注释可惜至极。
- en: Before releasing this classifier we would want to run the classifier, which
    trains on all annotated data, on new text to verify performance and set thresholds.
    This annotation process introduces biases over the data that will not be reflected
    in the real world. In particular, we have biased annotation for `n` and `p` and
    added `o` as we saw them. This is not the actual distribution.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在发布这个分类器之前，我们希望在新文本上运行该分类器，训练它以验证性能并设置阈值。这个注释过程会对数据引入偏差，而这些偏差在现实世界中是无法反映的。特别是，我们对`n`和`p`的注释是有偏向的，并且在看到`o`时也进行了注释。这并不是实际的数据分布。
- en: How it works...
  id: totrans-592
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'This recipe has some subtlety because of the simultaneous evaluation and creation
    of ranked output for annotation. The code starts with constructs that should be
    familiar to you:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方有一些微妙之处，因为它同时评估并为注释创建排名输出。代码从应该对你熟悉的构造开始：
- en: '[PRE142]'
  id: totrans-594
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: The `getLatestEpochFile` method looks for the highest numbered file that ends
    with `csv`, shares the root with the filename, and returns it. On no account will
    we use this routine for anything serious. The method is standard Java, so we won't
    cover it.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '`getLatestEpochFile`方法查找以`csv`结尾的最高编号文件，且该文件与文件名共享根目录，并返回该文件。我们绝不会将此例程用于任何严肃的事情。该方法是标准Java方法，因此我们不会详细介绍它。'
- en: 'Once we have the latest file, we will do some reporting, read it in our standard
    `.csv` annotated files, and load a cross-validating corpus. All these routines
    are explained elsewhere in locations specified in the `Util` source. Finally,
    we will get the categories that were found in the `.csv` annotated file:'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获取到最新的文件，我们将进行一些报告，读取标准的`.csv`注释文件，并加载交叉验证语料库。所有这些例程都在`Util`源代码中其他地方进行了详细说明。最后，我们将获取`.csv`注释文件中找到的类别：
- en: '[PRE143]'
  id: totrans-597
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'Next, we will configure some standard logistic-regression-training parameters
    and create the evaluator for cross-fold evaluation. Note that the Boolean for
    `storeInputs` is `true`, which will facilitate recording results. The *How to
    train and evaluate with cross validation* recipe in [Chapter 1](ch01.html "Chapter 1. Simple
    Classifiers"), *Simple Classifiers*, has a complete explanation:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置一些标准的逻辑回归训练参数，并创建交叉折叠评估器。请注意，`storeInputs`的布尔值是`true`，这将有助于记录结果。[第1章](ch01.html
    "第1章 简单分类器")中的*如何进行交叉验证训练和评估*部分，*简单分类器*，提供了完整的解释：
- en: '[PRE144]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Then, we will execute standard cross validation:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将执行标准的交叉验证：
- en: '[PRE145]'
  id: totrans-601
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'At the end of cross validation, the evaluator has all the classifications stored
    in `visitTest()`. Next, we will transfer this data to an accumulator, which creates
    and stores rows that will be put into the output spreadsheet and redundantly stores
    the score; this score will be used in a sort to control the order of annotations
    printed out:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证结束时，评估器已将所有分类存储在`visitTest()`中。接下来，我们将把这些数据转移到一个累加器中，该累加器创建并存储将放入输出电子表格的行，并冗余存储得分；此得分将用于排序，以控制注释输出的顺序：
- en: '[PRE146]'
  id: totrans-603
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Then, we will iterate over each category and create a list of the false negatives
    and true positives for the category—these are the cases that the truth category
    is the category label:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将遍历每个类别，并为该类别创建假阴性和真正阳性的列表——这些是实际类别与类别标签一致的案例：
- en: '[PRE147]'
  id: totrans-605
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'Next, all the in-category test cases are used to create rows for the accumulator:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，所有类别内的测试案例将用于为累加器创建行：
- en: '[PRE148]'
  id: totrans-607
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'Next, the code will print out some standard evaluator output:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码将打印出一些标准的评估输出：
- en: '[PRE149]'
  id: totrans-609
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: All the mentioned steps only apply to annotated data. We will now turn to getting
    best category and scores for all the unannotated data in the `.csv` file.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述步骤仅适用于注释数据。现在我们将转向获取所有未注释数据的最佳类别和得分，这些数据存储在`.csv`文件中。
- en: 'First, we will set the number of folds on the cross-validating corpus to `0`,
    which means that `vistTrain()` will visit the entire corpus of annotations—unannotated
    data is not contained in the corpus. The logistic regression classifier is trained
    in the usual way:'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将交叉验证语料库的折数设置为`0`，这意味着`vistTrain()`将访问整个注释语料库——未注释的数据不包含在语料库中。逻辑回归分类器按通常的方式训练：
- en: '[PRE150]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Armed with a classifier, the code iterates over all the `data` items, one row
    at a time. The first step is to check for an annotation. If the value is not the
    empty string, then the data was in the aforementioned corpus and used as training
    data so that the loop skips to the next row:'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了分类器后，代码会逐行遍历所有`data`项。第一步是检查是否有注释。如果该值不是空字符串，那么数据就存在于上述语料库中，并被用作训练数据，此时循环将跳到下一行：
- en: '[PRE151]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: If the row is unannotated, then the score and `bestCategory()` method is added
    at the appropriate points, and the row is added to the accumulator with the score.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该行未注释，那么得分和`bestCategory()`方法会被添加到适当的位置，并且该行会与得分一起添加到累加器中。
- en: 'The rest of the code increments the index of the filename and writes out the
    accumulator data with a bit of reporting:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码会递增文件名的索引，并输出累加器数据，并附带一些报告：
- en: '[PRE152]'
  id: totrans-617
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: This is how it works. Remember that the biases that can be introduced by this
    approach invalidate evaluation numbers. Always run on fresh held-out data to get
    a proper sense of the classifier's performance.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的工作方式。记住，这种方法引入的偏差会使评估结果失效。一定要在新的保留数据上进行测试，以正确评估分类器的性能。
- en: Annotation
  id: totrans-619
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注
- en: One of the most valuable services we provide is teaching our customers how to
    create gold-standard data, also known as training data. Nearly every successful-driven
    NLP project we have done has involved a good deal of customer-driven annotation.
    The quality of the NLP is entirely dependent on the quality of the training data.
    Creating training data is a fairly straightforward process, but it requires attention
    to detail and significant resources. From a budget perspective, you can expect
    to spend as much as the development team on annotation, if not more.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供的最有价值的服务之一是教客户如何创建黄金标准数据，也就是训练数据。几乎每一个成功驱动的NLP项目都涉及大量客户主导的标注工作。NLP的质量完全取决于训练数据的质量。创建训练数据是一个相对直接的过程，但它需要细致的关注和大量资源。从预算角度来看，你可以预期在标注上的开支将与开发团队一样多，甚至更多。
- en: How to do it...
  id: totrans-621
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: We will use sentiment over tweets as our example, and we will assume a business
    context, but even academic efforts will have similar dimensions.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用推文的情感分析作为例子，并假设这是一个商业情境，但即便是学术性的努力也有类似的维度。
- en: Get 10 examples of what you expect the system to do. For our example, this means
    getting 10 tweets that reflect the scope of what the system is expected to do.
  id: totrans-623
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取10个你预期系统处理的示例。以我们的例子来说，这意味着获取10条反映系统预期范围的推文。
- en: Make some effort to pick from the range of what you expect as inputs/outputs.
    Feel free to cherry-pick strong examples, but do not make up examples. Humans
    are terrible at creating example data. Seriously, don't do it.
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 花些力气从你预期的输入/输出范围中进行选择。可以随意挑选一些强有力的示例，但不要编造例子。人类在创建示例数据方面做得很糟糕。说真的，不要这么做。
- en: Annotate these tweets for the expected categories.
  id: totrans-625
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些推文进行预期类别的标注。
- en: 'Have a meeting with all the stakeholders in the annotation. This includes user-experience
    designers, business folks, developers, and end users. The goal of this meeting
    is to expose all the relevant parties to what the system will actually do—the
    system will take the 10 examples and produce the category label. You will be amazed
    at how much clarity this step establishes. Here are the kinds of clarity:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与所有相关方开会讨论标注工作。这些相关方包括用户体验设计师、业务人员、开发人员和最终用户。会议的目标是让所有相关方了解系统实际会做什么——系统将处理10个示例并生成类别标签。你会惊讶于这一步骤能带来多少清晰度。以下是几种清晰度：
- en: Upstream/downstream users of the classifier will have a clear idea of what they
    are expected to produce or consume. For example, the system consumes UTF-8-encoded
    English tweets and produces an ASCII single character of `p`, `n`, or `u`.
  id: totrans-627
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器的上游/下游用户将清楚他们需要生产或消费什么。例如，系统接受UTF-8编码的英文推文，并生成一个ASCII单字符的`p`、`n`或`u`。
- en: For a sentiment, people tend to want a severity score, which is very hard to
    get. You can expect annotation costs to double at least. Is it worth it? A score
    of confidence can be provided, but that is confidence that the category is correct
    *not* the severity of the sentiment. This meeting will force the discussion.
  id: totrans-628
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于情感分析，人们往往希望有一个严重程度评分，而这非常难以获得。你可以预期标注成本至少会翻倍。值得吗？你可以提供一个置信度评分，但那只是关于类别是否正确的置信度，而不是情感的严重程度。这次会议将迫使大家讨论这一点。
- en: During this meeting explain that each category will likely need at least 100
    examples, if not 500, to do a reasonable job. Also explain that switching domains
    might require new annotations. NLP is extremely easy for your human colleagues,
    and as a result, they tend to underestimate what it takes to build systems.
  id: totrans-629
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这次会议中，解释每个类别可能需要至少100个示例，甚至可能需要500个，才能做到合理的标注。同时解释，切换领域可能需要新的标注。自然语言处理（NLP）对你的同事来说非常简单，因此他们往往低估了构建系统所需的工作量。
- en: Don't neglect to include whoever is paying for all this. I suppose you should
    not have your parents involved if this is your undergraduate thesis.
  id: totrans-630
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要忽视涉及到为这些内容付费的人。我想，如果这是你的本科论文，你应该避免让父母参与其中。
- en: 'Write down an annotation standard that explains the intention behind each category.
    It doesn''t need to be very complex, but it needs to exist. The annotation standard
    should be circulated around the stakeholders. Bonus points if you have one at
    the mentioned meetings; if so, it will likely be different at the end, but this
    is fine. An example is:'
  id: totrans-631
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 写下一个注释标准，解释每个类别背后的意图。它不需要非常复杂，但必须存在。注释标准应当在相关人员之间传阅。如果你在提到的会议上就有一个标准，那将加分；即使如此，最终它可能会有所不同，但这也没关系。一个例子是：
- en: A tweet is positive `p` if the sentiment is unambiguously positive about Disney.
    A positive sentiment that applies to a non-Disney tweet is not `p` but `u`. An
    example is the `n` tweet indicates clearly negative intent towards Disney. Examples
    include that all other tweets are `u`.
  id: totrans-632
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一条推文对迪士尼的情感是明确正面的，那么它就是正向`p`推文。对于非迪士尼的推文，如果情感为正，但适用的情境为非迪士尼推文，则为`u`。例如，`n`推文明显表达了对迪士尼的负面意图。其他所有推文都为`u`。
- en: Examples in the annotation standard do the best job of communicating the intent.
    Humans do a better job with examples rather than descriptions in our experience.
  id: totrans-633
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注释标准中的例子在传达意图方面做得最好。根据我们的经验，人类更擅长通过例子而非描述来理解。
- en: Create your collection of unannotated data. The best practice here is for the
    data to be random from the expected source. This works fine for categories with
    noticeable prevalence in data, say 10 percent or more, but we have built classifiers
    that occur at a rate of 1/2,000,000 for question-answering systems. For rare categories,
    you can use a search engine to help find instances of the category—for example,
    search for `luv` to find positive tweets. Alternatively, you can use a classifier
    trained on a few examples, run it on data, and look at the high-scoring positives—we
    covered this in the previous recipe.
  id: totrans-634
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建你自己的未标注数据集。这里的最佳实践是从预期来源中随机选择数据。这对于数据中类别明显占优的情况（例如10％或更多）效果良好，但我们也曾构建过问答系统的分类器，类别出现频率为1/2,000,000。对于稀有类别，你可以使用搜索引擎帮助寻找该类别的实例—例如搜索`luv`来查找正向推文。或者，你可以使用在少量示例上训练的分类器，运行在数据上并查看高得分的正向结果—我们在之前的配方中讲过这个方法。
- en: 'Recruit at least two annotators to annotate data. The reason we need at least
    two is that the task has to be shown to be reproducible by humans. If people can''t
    reliably do the task, then you can''t expect a computer to do it. This is where
    we execute some code. Type in the following command in the command line or invoke
    your annotators in you IDE—this will run with our default files:'
  id: totrans-635
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 至少招募两名注释员进行数据标注。我们需要至少两名的原因是，任务必须能够被证明是人类可重复执行的。如果人类无法可靠地完成此任务，那么也无法指望计算机完成它。这时我们会执行一些代码。请在命令行中输入以下命令，或者在IDE中调用你的注释员—这将使用我们的默认文件运行：
- en: '[PRE153]'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-637
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: The code reports disagreements and prints out a confusion matrix. Precision
    and recall are useful metrics as well.
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该代码报告分歧并打印出混淆矩阵。精确度和召回率也是有用的评估指标。
- en: How it works…
  id: totrans-639
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'There is little novel data in the code in `src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java`.
    One slight twist is that we used `BaseClassifierEvaluator` to do the evaluation
    work without a classifier ever being specified—the creation is as follows:'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在`src/com/lingpipe/cookbook/chapter3/InterAnnotatorAgreement.java`中的代码几乎没有新颖的数据。一个小的变化是我们使用`BaseClassifierEvaluator`来进行评估工作，而从未指定分类器—创建方式如下：
- en: '[PRE155]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'The evaluator is populated with classifications directly rather than the usual
    `Corpus.visitTest()` method, as done elsewhere in the book:'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 评估器直接使用分类结果进行填充，而不是书中其他地方常用的`Corpus.visitTest()`方法：
- en: '[PRE156]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: If the recipe requires further explanation, consult the *Evaluation of classifiers—the
    confusion matrix* recipe in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个配方需要进一步的解释，请参考[第1章](ch01.html "第1章。简单分类器")中的*分类器评估—混淆矩阵*配方，*简单分类器*。
- en: There's more…
  id: totrans-645
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: Annotation is a very complex area that deserves its own book, and fortunately,
    there is a good one, *Natural Language Annotation for Machine Learning*, *James
    Pustejovsky and Amber Stubbs*, *O'Reilly Media*. To get annotations done, there
    is Amazon's Mechanical Turk service as well as companies that specialize in the
    creation of training data such as CrowdFlower. However, be careful of outsourcing
    because classifiers are very dependent on the quality of data.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 注释是一个非常复杂的领域，值得出一本专门的书，幸运的是，确实有一本好书，*《机器学习中的自然语言注释》*，*James Pustejovsky 和 Amber
    Stubbs*，*O'Reilly Media*。要完成注释，可以使用亚马逊的Mechanical Turk服务，也有一些专门创建训练数据的公司，如CrowdFlower。然而，外包时要小心，因为分类器的效果非常依赖数据的质量。
- en: Conflict resolution between annotators is a challenging area. Many errors will
    be due to attention lapses, but some will persist as legitimate areas of disagreement.
    Two easy resolution strategies are either to throw out the data or keep both annotations.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 注释者之间的冲突解决是一个具有挑战性的领域。许多错误是由于注意力不集中造成的，但有些将持续作为合法的分歧领域。两种简单的解决策略是要么丢弃数据，要么保留两个注释。
