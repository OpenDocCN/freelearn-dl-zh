- en: Up and Running with Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习快速入门
- en: What will **artificial intelligence** (**AI**) look like in the future? As 
    applications of AI algorithms and software become more prominent, it is a question
    that should interest many. Researchers and practitioners of AI face further relevant
    questions; how will we realize what we envision and solve known problems? What
    kinds of innovations and algorithms are yet to be developed? Several subfields
    in machine learning display great promise toward answering many of our questions.
    In this book, we shine the spotlight on reinforcement learning, one such, area
    and perhaps one of the most exciting topics in machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）在未来将会是什么样子？随着AI算法和软件应用的日益突出，这个问题应当引起许多人的关注。AI的研究人员和从业者面临着进一步的相关问题：我们如何实现我们设想的目标并解决已知的问题？还有哪些创新和算法尚未开发出来？机器学习中的几个子领域展示了极大的潜力，能够解答我们许多问题。本书将聚焦于其中的一个子领域——强化学习，也许它是机器学习中最令人兴奋的话题之一。'
- en: Reinforcement learning is motivated by the objective to learn from the environment
    by interacting with it. Imagine an infant and how it goes about in its environment.
    By moving around and acting upon its surroundings, the infant learns about physical
    phenomena, causal relationships, and various attributes and properties of the
    objects he or she interacts with. The infant's learning is often motivated by
    a desire to accomplish some objective, such as playing with surrounding objects
    or satiating some spark of curiosity. In reinforcement learning, we pursue a similar
    endeavor; we take a computational approach toward learning about the environment.
    In other words, our goal is to design algorithms that learn through their interactions
    with the environment in order to accomplish a task.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的动机来自于通过与环境互动来学习的目标。想象一下一个婴儿是如何在其环境中活动的。通过四处移动并与周围环境互动，婴儿学习物理现象、因果关系以及与其互动的物体的各种属性和特性。婴儿的学习通常受到达成某个目标的驱动，比如玩周围的物体或满足某种好奇心。在强化学习中，我们追求类似的目标；我们采用一种计算方法来学习环境。换句话说，我们的目标是设计出通过与环境互动来完成任务的算法。
- en: What use do such algorithms provide? By having a generalized learning algorithm,
    we can offer effective solutions to several real-world problems. A prominent example
    is the use of reinforcement learning algorithms to drive cars autonomously. While
    not fully realized, such use cases would provide great benefits to society, for
    reinforcement learning algorithms have empirically proven their ability to surpass
    human-level performance in several tasks. One watershed moment occurred in 2016
    when DeepMind's AlphaGo program defeated 18-time Go world champion Lee Sedol four
    games to one. AlphaGo was essentially able to learn and surpass three millennia
    of Go wisdom cultivated by humans in a matter of months. Recently, reinforcement
    learning algorithms have been shown to be effective in playing more complex, real-time
    multi-agent games such as Dota. The same algorithms that power these game-playing
    algorithms have also succeeded in controlling robotic arms to pick up objects
    and navigating drones through mazes. These examples suggest not only what these
    algorithms are capable of, but also what they can potentially accomplish down
    the road.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的用途是什么？通过拥有一种通用的学习算法，我们可以为解决多个现实世界中的问题提供有效的解决方案。一个显著的例子是强化学习算法用于自动驾驶汽车。尽管尚未完全实现，但这样的应用场景将为社会带来巨大益处，因为强化学习算法已经在实证中证明了其在多个任务中超越人类水平的能力。一个标志性的时刻发生在2016年，当时DeepMind的AlphaGo程序以四比一击败了18次围棋世界冠军李世石。AlphaGo本质上能够在几个月内学习并超越人类三千年来积累的围棋智慧。最近，强化学习算法已被证明在玩更复杂的实时多智能体游戏（如Dota）中也非常有效。这些驱动游戏算法的同样算法也成功地控制机器人手臂拾取物体，并指引无人机通过迷宫。这些例子不仅展示了这些算法的能力，还暗示了它们未来可能完成的任务。
- en: Introduction to this book
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书简介
- en: This book offers a practical guide for those eager to learn about reinforcement
    learning. We will take a hands-on approach toward learning about reinforcement
    learning by going through numerous examples of algorithms and their applications.
    Each chapter focuses on a particular use case and introduces reinforcement learning
    algorithms that are used to solve the given problem. Some of these use cases rely
    on state-of-the-art algorithms; hence through this book, we will learn about and
    implement some of the best-performing algorithms and techniques in the industry.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书为那些渴望学习强化学习的读者提供了实用的指南。我们将通过大量的算法实例及其应用，采取动手实践的方式学习强化学习。每一章都专注于一个特定的应用案例，并介绍用于解决该问题的强化学习算法。这些应用案例中有些依赖于最先进的算法；因此，通过本书，我们将学习并实现一些业内最优表现的算法和技术。
- en: 'The projects increase in difficulty/complexity as you go through the book.
    The following table describes what you will learn from each chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你阅读本书，项目的难度/复杂度会逐渐增加。下表描述了你将从每一章节中学到的内容：
- en: '| **Chapter name** | **The use case/problem** | **Concepts/algorithms/technologies
    discussed and used** |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| **章节名称** | **应用案例/问题** | **讨论和使用的概念/算法/技术** |'
- en: '| *Balancing Cart Pole* | Control horizontal movement of a cart to balance
    a vertical bar | OpenAI Gym framework, Q-Learning |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| *平衡推车杆* | 控制推车的水平移动以平衡竖直的杆 | OpenAI Gym 框架，Q学习 |'
- en: '| *Playing Atari Games* | Play various Atari games at human-level proficiency
    | Deep Q-Networks |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| *玩Atari游戏* | 在人类水平的熟练度下玩各种Atari游戏 | 深度Q网络 |'
- en: '| *Simulating Control Tasks* | Control agents in a continuous action space
    as opposed to a discrete one | **Deterministic policy gradients** (**DPG**), **Trust
    Region Policy Optimization** (**TRPO**), multi-tasking |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| *模拟控制任务* | 控制一个在连续动作空间中的智能体，而非离散空间 | **确定性策略梯度**（**DPG**）、**信任域策略优化**（**TRPO**）、多任务学习
    |'
- en: '| *Building Virtual Worlds in Minecraft* | Navigate a character in the virtual
    world of Minecraft | Asynchronous Advantage Actor-Critic (**A3C**) |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| *在Minecraft中构建虚拟世界* | 在Minecraft的虚拟世界中操作一个角色 | 异步优势Actor-Critic（**A3C**）
    |'
- en: '| *Learning to Play Go* | Go, one of the oldest and most complex board games
    in the world | Monte Carlo tree search, policy and value networks |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| *学习围棋* | 围棋，世界上最古老且最复杂的棋盘游戏之一 | 蒙特卡罗树搜索、策略和价值网络 |'
- en: '| *Creating a Chatbot* | Generating natural language in a conversational setting
    | Policy gradient methods, **Long Short-Term Memory** (**LSTM**) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| *创建聊天机器人* | 在对话环境中生成自然语言 | 策略梯度方法、**长短期记忆网络**（**LSTM**） |'
- en: '| *Auto Generating a Deep Learning Image Classifier* | Create an agent that
    generates neural networks to solve a given task | Recurrent neural networks, policy
    gradient methods (REINFORCE) |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| *自动生成深度学习图像分类器* | 创建一个生成神经网络来解决特定任务的智能体 | 循环神经网络、策略梯度方法（REINFORCE） |'
- en: '| *Predicting Future Stock Prices* | Predict stock prices and make buy and
    sell decisions | Actor-Critic methods, time-series analysis, experience replay
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| *预测未来股票价格* | 预测股票价格并做出买卖决策 | Actor-Critic方法、时间序列分析、经验回放 |'
- en: Expectations
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 期望
- en: 'This book is best suited for the reader who:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书最适合以下读者：
- en: Has intermediate proficiency in Python
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具备中级Python编程能力
- en: 'Possesses a basic understanding of machine learning and deep learning, especially
    for the following topics:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具备基本的机器学习和深度学习理解，特别是以下主题：
- en: Neural networks
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Backpropagation
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Convolution
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积
- en: Techniques for better generalization and reduced overfitting
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高泛化能力和减少过拟合的技术
- en: Enjoys a hands-on, practical approach toward learning
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 喜欢动手实践的学习方式
- en: 'Since this book serves as a practical introduction to the field, we try to
    keep theoretical content to a minimum. However, it is advisable for the reader
    to have basic knowledge of some of the fundamental mathematical and statistical
    concepts on which the field of machine learning depends. These include the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书旨在作为该领域的实用入门指南，我们尽量将理论内容控制在最小范围。然而，建议读者具备一些机器学习领域所依赖的基础数学和统计学概念的知识。这些包括以下内容：
- en: Calculus (single and multivariate)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微积分（单变量和多变量）
- en: Linear algebra
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性代数
- en: Probability theory
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率论
- en: Graph theory
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图论
- en: Having some experience with these subjects would greatly assist the reader in
    understanding the concepts and algorithms we will cover throughout this book.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些主题有一定了解将极大帮助读者理解本书中将涉及的概念和算法。
- en: Hardware and software requirements
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件和软件要求
- en: 'The ensuing chapters will require you to implement various reinforcement learning
    algorithms. Hence a proper development environment is necessary for a smooth learning
    journey. In particular, you should have the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将要求你实现各种强化学习算法。因此，为了顺利学习，必须有一个合适的开发环境。特别是，你应该具备以下条件：
- en: A computer running either macOS or the Linux operating system (for those on
    Windows, try setting up a Virtual Machine with a Linux image)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行 macOS 或 Linux 操作系统的计算机（对于 Windows 用户，建议尝试设置一个安装了 Linux 镜像的虚拟机）
- en: A stable internet connection
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定的互联网连接
- en: A GPU (preferably)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一块 GPU（最好有）
- en: We will exclusively use the Python programming language to implement our reinforcement
    learning and deep learning algorithms. Moreover, we will be using Python 3.6\.
    A list of libraries we will be using can be found on the official GitHub repository,
    located at ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects)).
    You will also find the implementations of every algorithm we will cover in this
    book.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 exclusively 使用 Python 编程语言来实现我们的强化学习和深度学习算法。此外，我们将使用 Python 3.6。我们将使用的库的列表可以在官方
    GitHub 仓库中找到，网址是 ([https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects](https://github.com/PacktPublishing/Python-Reinforcement-Learning-Projects))。你还可以在这个仓库中找到我们将在本书中讨论的每个算法的实现。
- en: Installing packages
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装软件包
- en: 'Assuming you have a working Python installation, you can install all the required
    packages using the `requirements.txt` file found in our repository. We also recommend
    you create a `virtualenv` to isolate your development environment from your main
    OS system. The following steps will help you construct an environment and install
    the packages:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经有一个正常工作的 Python 安装，你可以通过我们的仓库中的 `requirements.txt` 文件安装所有所需的软件包。我们还建议你创建一个
    `virtualenv` 来将开发环境与主操作系统隔离开来。以下步骤将帮助你构建环境并安装所需的包：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And now you are all set and ready to start! The next few sections of this chapter
    will introduce the field of reinforcement learning and will also provide a refresher
    on deep learning.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经准备好开始了！本章接下来的几节将介绍强化学习领域，并对深度学习进行回顾。
- en: What is reinforcement learning?
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Our journey begins with understanding what reinforcement learning is about.
    Those who are familiar with machine learning may be aware of several learning
    paradigms, namely **supervised learning** and **unsupervised learning**. In supervised
    learning, a machine learning model has a supervisor that gives the ground truth for
    every data point. The model learns by minimizing the distance between its own
    prediction and the ground truth. The dataset is thus required to have an annotation
    for each data point, for example, each image of a dog and a cat would have its
    respective label. In unsupervised learning, the model does not have access to
    the ground truths of the data and thus has to learn about the distribution and
    patterns of the data without them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程始于了解强化学习的基本概念。那些熟悉机器学习的人可能已经知道几种学习范式，即**监督学习**和**无监督学习**。在监督学习中，机器学习模型有一个监督者，提供每个数据点的真实标签。模型通过最小化其预测与真实标签之间的差距来学习。因此，数据集需要对每个数据点进行标注，例如，每张狗狗和猫的图片都会有相应的标签。在无监督学习中，模型无法访问数据的真实标签，因此必须在没有标签的情况下学习数据的分布和模式。
- en: In reinforcement learning, the agent refers to the model/algorithm that learns
    to complete a particular task. The agent learns primarily by receiving **reward
    signals**, which is a scalar indication of how well the agent is performing a
    task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，代理指的是学习完成特定任务的模型/算法。代理主要通过接收**奖励信号**来学习，奖励信号是衡量代理完成任务表现的标量指标。
- en: Suppose we have an agent that is tasked with controlling a robot's walking movement;
    the agent would receive positive rewards for successfully walking toward a destination
    and negative rewards for falling/failing to make progress.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个代理，负责控制机器人行走的动作；如果代理成功朝目标行走，则会获得正向奖励，而如果摔倒或未能取得进展，则会获得负向奖励。
- en: Moreover, unlike in supervised learning, these reward signals are not given
    to the model immediately; rather, they are returned as a consequence of a sequence
    of **actions** that the agent makes. Actions are simply the things an agent can
    do within its **environment**. The environment refers to the world in which the
    agent resides and is primarily responsible for returning reward signals to the
    agent. An agent's actions are usually conditioned on what the agent perceives
    from the environment. What the agent perceives is referred to as the **observation**
    or the **state** of the environment. What further distinguishes reinforcement
    learning from other paradigms is that the actions of the agent can alter the environment
    and its subsequent responses.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与监督学习不同，这些奖励信号不会立即返回给模型；相反，它们是作为代理执行一系列**动作**的结果返回的。动作就是代理在其**环境**中可以做的事情。环境指的是代理所处的世界，主要负责向代理返回奖励信号。代理的动作通常会根据代理从环境中感知到的信息来决定。代理感知到的内容被称为**观察**或**状态**。强化学习与其他学习范式的一个重要区别在于，代理的动作可以改变环境及其随后产生的响应。
- en: For example, suppose an agent is tasked with playing Space Invaders, the popular
    Atari 2600 arcade game. The environment is the game itself, along with the logic
    upon which it runs. During the game, the agent queries the environment to make
    an observation. The observation is simply an array of the (210, 160, 3) shape,
    which is the screen of the game that displays the agent's ship, the enemies, the
    score, and any projectiles. Based on this observation, the agent makes some actions,
    which can include moving left or right, shooting a laser, or doing nothing. The
    environment receives the agent's action as input and makes any necessary updates
    to the state.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设一个代理的任务是玩《太空入侵者》，这是一款流行的Atari 2600街机游戏。环境就是游戏本身，以及它运行的逻辑。在游戏过程中，代理向环境发出查询以获取观察结果。观察结果仅仅是一个(210,
    160, 3)形状的数组，代表游戏屏幕，屏幕上显示代理的飞船、敌人、得分以及任何投射物。根据这个观察结果，代理做出一些动作，可能包括向左或向右移动、发射激光或什么也不做。环境接收代理的动作作为输入，并对状态进行必要的更新。
- en: For instance, if a laser touches an enemy ship, it is removed from the game.
    If the agent decides to simply move to the left, the game updates the agent's
    coordinates accordingly. This process repeats until a **terminal state***,* a
    state that represents the end of the sequence, is reached. In Space Invaders,
    the terminal state corresponds to when the agent's ship is destroyed, and the
    game subsequently returns the score that it keeps track of, a value that is calculated
    based on the number of enemy ships the agent successfully destroys.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，如果激光击中敌方飞船，它会从游戏中消失。如果代理决定仅仅向左移动，游戏会相应更新代理的坐标。这个过程会一直重复，直到达到**终止状态**，即表示序列结束的状态。在《太空入侵者》游戏中，终止状态对应的是代理的飞船被摧毁，游戏随后返回记录的得分，该得分是根据代理成功摧毁的敌方飞船数量计算得出的。
- en: Note that some environments do not have terminal states, such as the stock market.
    These environments keep running for as long as they exist.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一些环境没有终止状态，例如股票市场。这些环境会一直运行，直到它们结束。
- en: 'Let''s recap the terms we have learned about so far:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止学习的术语：
- en: '| **Term** | **Description** | **Examples** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **术语** | **描述** | **示例** |'
- en: '| Agent | A model/algorithm that is tasked with learning to accomplish a task.
    | Self-driving cars, walking robots, video game players |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 代理 | 负责学习完成任务的模型/算法。 | 自动驾驶汽车、行走机器人、视频游戏玩家 |'
- en: '| Environment | The world in which the agent acts. It is responsible for controlling
    what the agent perceives and providing feedback on how well the agent is performing
    a particular task. | The road on which a car drives, a video game, the stock market
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 代理所处的世界。它负责控制代理的感知内容，并提供关于代理执行特定任务表现的反馈。 | 汽车行驶的道路、视频游戏、股票市场 |'
- en: '| Action | A decision the agent makes in an environment, usually dependent
    on what the agent perceives. | Steering a car, buying or selling a particular
    stock, shooting a laser from the spaceship the agent is controlling |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 动作 | 代理在环境中做出的决策，通常取决于代理的感知。 | 驾驶汽车、买卖特定股票、从代理控制的太空船上发射激光 |'
- en: '| Reward signal | A scalar indication of how well the agent is performing a
    particular task. | Space Invaders score, return on investment for some stock,
    distance covered by a robot learning to walk |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 奖励信号 | 一个标量值，表示智能体在执行特定任务时的表现如何。 | 《太空入侵者》的得分、某只股票的投资回报、学习行走的机器人走过的距离 |'
- en: '| Observation/state | A description of the environment as can be perceived
    by the agent. | Video from a dashboard camera, the screen of the game, stock market
    statistics |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 观察/状态 | 智能体可以感知的环境描述。 | 从仪表盘摄像头拍摄的视频、游戏屏幕、股市统计数据 |'
- en: '| Terminal state | A state at which no further actions can be made by the agent.
    | Reaching the end of a maze, the ship in Space Invaders getting destroyed |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 终止状态 | 智能体无法再进行任何操作的状态。 | 迷宫的尽头，或者《太空入侵者》中的飞船被摧毁 |'
- en: 'Put formally, at a given timestep, `t`, the following happens for an agent, `P`, and
    environment, `E`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，在给定的时间步`t`，以下内容发生在智能体`P`和环境`E`之间：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How does the environment compute![](img/c1cbd545-2753-478c-866f-ed057127bf3d.png)and ![](img/8f52597c-a3e6-43ed-baaa-1e849131b1fe.png)?
    The environment usually has its own algorithm that computes these values based
    on numerous input/factors, including what action the agent takes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是如何计算![](img/c1cbd545-2753-478c-866f-ed057127bf3d.png)和![](img/8f52597c-a3e6-43ed-baaa-1e849131b1fe.png)的？环境通常有自己的算法，根据多个输入/因素计算这些值，包括智能体采取的行动。
- en: Sometimes, the environment is composed of multiple agents that try to maximize
    their own rewards. The way gravity acts upon a ball that we drop from a height
    is a good representation of how the environment works; just like how our surroundings
    obey the laws of physics, the environment has some internal mechanism for computing
    rewards and the next state. This internal mechanism is usually hidden to the agent,
    and thus our job is to build agents that can learn to do a good job at their respective
    tasks, despite this uncertainty.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，环境由多个智能体组成，它们试图最大化自己的奖励。重力作用于从高处落下的球的方式，很好地表现了环境是如何运作的；就像我们的周围环境遵循物理定律一样，环境也有一些内部机制来计算奖励和下一个状态。这个内部机制通常对智能体是隐藏的，因此我们的任务是构建能够在这种不确定性中依然能做好各自任务的智能体。
- en: In the following sections, we will discuss in more detail the main protagonist
    of every reinforcement learning problem—the agent.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将更详细地讨论每个强化学习问题的主要主体——智能体（Agent）。
- en: The agent
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体
- en: 'The goal of a reinforcement learning agent is to learn to perform a task well
    in an environment. Mathematically, this means to maximize the cumulative reward,
    *R*, which can be expressed in the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习智能体的目标是在环境中学会出色地执行任务。从数学角度来看，这意味着最大化累计奖励*R*，可以通过以下公式表示：
- en: '![](img/34073884-68e7-4113-9acc-2d51f9f4216f.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34073884-68e7-4113-9acc-2d51f9f4216f.png)'
- en: We are simply calculating a weighted sum of the reward received at each timestep.![](img/349f3223-b4de-429a-a18a-8ec1a2f14483.png)is
    called the **discount factor**, which is a scalar value between 0 and 1\. The
    idea is that the later a reward comes, the less valuable it becomes. This reflects
    our perspectives on rewards as well; that we'd rather receive $100 now rather
    than a year later shows how the same reward signal can be valued differently based
    on its proximity to the present.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是计算每个时间步获得奖励的加权总和。![](img/349f3223-b4de-429a-a18a-8ec1a2f14483.png)被称为**折扣因子**，它是一个介于0和1之间的标量值。其概念是奖励来得越晚，它的价值就越低。这也反映了我们对奖励的看法；比如，我们宁愿现在收到$100，也不愿一年后再收到，这表明同样的奖励信号可以根据其接近现在的程度而具有不同的价值。
- en: Because the mechanics of the environment are not fully observable or known to
    the agent, it must gain information by performing an action and observing how
    the environment reacts to it. This is much like how humans learn to perform certain
    tasks as well.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于环境的机制对智能体来说并非完全可观察或已知，它必须通过执行行动并观察环境如何反应来获取信息。这与人类如何学习执行某些任务的方式非常相似。
- en: 'Suppose we are learning to play chess. While we don''t have all the possible
    moves committed to memory or know exactly how an opponent will play, we are able
    to improve our proficiency over time. In particular, we are able to become proficient
    in the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在学习下棋。虽然我们并未将所有可能的棋步记在脑中，或者完全知道对手会如何下棋，但我们能够随着时间的推移提高我们的技巧。特别是，我们能够在以下方面变得熟练：
- en: Learning how to react to a move made by the opponent
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何应对对手做出的行动
- en: Assessing how good of a position we are in to win the game
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估我们在赢得游戏中的位置有多好
- en: Predicting what the opponent will do next and using that prediction to decide
    on a move
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测对手接下来会做什么，并利用该预测来决定行动
- en: Understanding how others would play in a similar situation
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解别人如何在类似情况下进行游戏
- en: 'In fact, reinforcement learning agents can learn to do similar things. In particular,
    an agent can be composed of multiple functions and models to assist its decision-making.
    There are three main components that an agent can have: the policy, the value
    function, and the model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，强化学习代理可以学习做类似的事情。特别是，一个代理可以由多个功能和模型组成，以辅助其决策。一个代理可以包含三个主要组件：策略、价值函数和模型。
- en: Policy
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: A policy is an algorithm or a set of rules that describe how an agent makes
    its decisions. An example policy can be the strategy an investor uses to trade
    stocks, where the investor buys a stock when its price goes down and sells the
    stock when the price goes up.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 策略是一个算法或一组规则，用来描述一个代理如何做出决策。例如，投资者在股市中交易时使用的策略就是一种策略，其中投资者在股价下跌时购买股票，在股价上涨时卖出股票。
- en: 'More formally, a policy is a function, usually denoted as ![](img/82dc222a-f23e-4460-ac00-c4ea31ac3cd5.png),
    that maps a state, ![](img/170b9e93-b340-45e9-988b-09c9e07e7fce.png), to an action, ![](img/db64ce78-6f6b-4bd8-913f-f1e4ec162373.png):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，策略是一个函数，通常表示为![](img/82dc222a-f23e-4460-ac00-c4ea31ac3cd5.png)，它将一个状态![](img/170b9e93-b340-45e9-988b-09c9e07e7fce.png)映射到一个行动![](img/db64ce78-6f6b-4bd8-913f-f1e4ec162373.png)：
- en: '![](img/67683436-16d2-412c-ade2-3dcd805dbb1f.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67683436-16d2-412c-ade2-3dcd805dbb1f.png)'
- en: This means that an agent decides its action given its current state. This function
    can represent anything, as long as it can receive a state as input and output
    an action, be it a table, graph, or machine learning classifier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一个代理根据其当前状态来决定其行动。这个函数可以表示任何内容，只要它能够接收状态作为输入并输出一个行动，无论是表格、图表还是机器学习分类器。
- en: 'For example, suppose we have an agent that is supposed to navigate a maze.
    We shall further assume that the agent knows what the maze looks like; the following
    is how the agent''s policy can be represented:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个代理需要导航一个迷宫。我们进一步假设代理知道迷宫的样子；以下是代理策略的表示方式：
- en: '![](img/b35ccef7-dfd6-4d7e-8f9d-38584fd4bf87.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b35ccef7-dfd6-4d7e-8f9d-38584fd4bf87.png)'
- en: 'Figure 1: A maze where each arrow indicates where an agent would go next'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一个迷宫，其中每个箭头表示代理下一个可能的行动方向
- en: 'Each white square in this maze represents a state the agent can be in. Each
    blue arrow refers to the action an agent would take in the corresponding square.
    This essentially represents the agent''s policy for this maze. Moreover, this
    can also be regarded as a deterministic policy, for the mapping from the state
    to the action is deterministic. This is in contrast to a stochastic policy, where
    a policy would output a probability distribution over the possible actions given
    some state:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个迷宫中的每个白色方块代表代理可能处于的状态。每个蓝色箭头指示代理在相应方块中会采取的行动。这本质上表示了代理在这个迷宫中的策略。此外，这也可以视为一个确定性策略，因为从状态到行动的映射是确定的。这与随机策略相对，后者会在给定某种状态时输出一个关于可能行动的概率分布：
- en: '![](img/55a6880b-56ca-4c80-b1c8-22b60a84e942.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55a6880b-56ca-4c80-b1c8-22b60a84e942.png)'
- en: 'Here,![](img/fbac4100-413c-4698-b8dc-bee15f1dc709.png)is a normalized probability
    vector over all the possible actions, as shown in the following example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/fbac4100-413c-4698-b8dc-bee15f1dc709.png)是所有可能行动的标准化概率向量，如以下示例所示：
- en: '![](img/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ce55cbe-0718-4ad8-a8f7-f45d19212236.png)'
- en: 'Figure 2: A policy mapping the game state (the screen) to actions (probabilities)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个将游戏状态（屏幕）映射到行动（概率）的策略
- en: The agent playing the game of Breakout has a policy that takes the screen of
    the game as input and returns a probability for each possible action.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 玩《Breakout》游戏的代理有一个策略，该策略以游戏屏幕为输入，并返回每个可能行动的概率。
- en: Value function
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价值函数
- en: 'The second component an agent can have is called the **value function**. As
    mentioned previously, it is useful to assess your position, good or bad, in a
    given state. In a game of chess, a player would like to know the likelihood that
    they are going to win in a board state. An agent navigating a maze would like
    to know how close it is to the destination. The value function serves this purpose;
    it predicts the expected future reward an agent would receive in a given state.
    In other words, it measures whether a given state is desirable for the agent.
    More formally, the value function takes a state and a policy as input and returns
    a scalar value representing the expected cumulative reward:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的第二个组成部分称为**值函数**。如前所述，值函数有助于评估智能体在某一状态下的位置，无论是好是坏。在国际象棋中，玩家希望了解他们在当前棋盘状态下获胜的可能性。而在迷宫中，智能体则希望知道自己离目标有多近。值函数正是为此服务；它预测智能体在某一状态下将获得的预期未来奖励。换句话说，它衡量某一状态对智能体的吸引力。更正式地说，值函数将状态和策略作为输入，并返回一个标量值，表示预期的累计奖励：
- en: '![](img/37ff7369-c631-4dcd-ac2a-3a8e563ae28c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37ff7369-c631-4dcd-ac2a-3a8e563ae28c.png)'
- en: 'Take our maze example, and suppose the agent receives a reward of -1 for every
    step it takes. The agent''s goal is to finish the maze in the smallest number
    of steps possible. The value of each state can be represented as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以我们的迷宫示例为例，假设智能体每走一步就会收到-1的惩罚奖励。智能体的目标是以尽可能少的步骤完成迷宫。每个状态的值可以如下表示：
- en: '![](img/a0d2e053-a325-409f-b5b6-43627cd96e45.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0d2e053-a325-409f-b5b6-43627cd96e45.png)'
- en: 'Figure 3: A maze where each square indicates the value of being in the state'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个迷宫，其中每个方格表示处于该状态时的值
- en: Each square basically represents the number of steps it takes to get to the
    end of the maze. As you can see, the smallest number of steps required to reach
    the goal is 15.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 每个方格基本上表示从当前位置到达迷宫终点所需的步数。如你所见，达到目标所需的最少步数为15步。
- en: How can the value function help an agent perform a task well, other than informing
    us of how desirable a given state is? As we will see in the following sections,
    value functions play an integral role in predicting how well a sequence of actions
    will do even before the agent performs them. This is similar to chess players
    imagining how well a sequence of future actions will do in improving his or her 
    chances of winning. To do this, the agent also needs to have an understanding
    of how the environment works. This is where the third component of an agent, the **model**,
    becomes relevant.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了告诉我们某一状态的吸引力外，值函数还能如何帮助智能体更好地完成任务呢？正如我们将在接下来的章节中看到的，值函数在预测一系列动作是否能够成功之前起着至关重要的作用。这类似于国际象棋选手预想一系列未来动作如何有助于提高获胜的机会。为了做到这一点，智能体还需要了解环境是如何运作的。这时，智能体的第三个组成部分——**模型**——变得非常重要。
- en: Model
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: 'In the previous sections, we discussed how the environment is not fully known
    to the agent. In other words, the agent usually does not have an idea of how the
    internal algorithm of the environment looks. The agent thus needs to interact
    with it to gain information and learn how to maximize its expected cumulative
    reward. However, it is possible for the agent to have an internal replica, or
    a model, of the environment. The agent can use the model to predict how the environment
    would react to some action in a given state. A model of the stock market, for
    example, is tasked with predicting what the prices will look like in the future.
    If the model is accurate, the agent can then use its value function to assess
    how desirable future states look. More formally, a model can be denoted as a function, ![](img/9cbc6ea0-aae1-46ea-b5f8-e0d8ac439aa0.png), that
    predicts the probability of the next state given the current state and an action:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了环境对智能体的不可完全知晓。换句话说，智能体通常不知道环境内部算法的具体情况。因此，智能体需要与环境互动，以获取信息并学习如何最大化预期的累计奖励。然而，智能体可能会有一个环境的内部副本或模型。智能体可以使用该模型预测环境在给定状态下对某个动作的反应。例如，股市模型的任务是预测未来的价格。如果模型准确，智能体便可以利用其值函数评估未来状态的吸引力。更正式地说，模型可以表示为一个函数，![](img/9cbc6ea0-aae1-46ea-b5f8-e0d8ac439aa0.png)，它预测在当前状态和某个动作下，下一状态的概率：
- en: '![](img/5605d9cb-dc64-4b04-8515-c60b34e88486.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5605d9cb-dc64-4b04-8515-c60b34e88486.png)'
- en: 'In other scenarios, the model of the environment can be used to enumerate possible
    future states. This is commonly used in turn-based games, such as chess and tic-tac-toe,
    where the rules and scope of possible actions are clearly defined. Trees are often
    used to illustrate the possible sequence of actions and states in turn-based games:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他场景中，环境的模型可以用来列举可能的未来状态。这通常用于回合制游戏，如国际象棋和井字游戏，其中规则和可能动作的范围已明确规定。树状图常用于展示回合制游戏中可能的动作和状态序列：
- en: '![](img/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/873f8b59-9dd2-4fc6-830a-9c5fd62198f9.png)'
- en: 'Figure 4: A model using its value function to assess possible moves'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：一个使用其价值函数评估可能动作的模型
- en: In the preceding example of the tic-tac-toe game,![](img/357a67fa-d45f-4755-90ce-03d347133091.png)denotes
    the possible states that taking the![](img/e8560c16-362b-4411-b69f-50eddf987dba.png)action
    (represented as the shaded circle) could yield in a given state, ![](img/02d85b2c-a516-4199-8255-d805511504ea.png).
    Moreover, we can calculate the value of each state using the agent's value function.
    The middle and bottom states would yield a high value since the agent would be
    one step away from victory, whereas  the top state would yield a medium value
    since the agent needs to prevent the opponent from winning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的井字游戏示例中，![](img/357a67fa-d45f-4755-90ce-03d347133091.png)表示在给定状态下采取![](img/e8560c16-362b-4411-b69f-50eddf987dba.png)动作（表示为阴影圆圈）可能带来的结果，状态为![](img/02d85b2c-a516-4199-8255-d805511504ea.png)。此外，我们可以使用代理的价值函数计算每个状态的值。中间和底部的状态将产生较高的值，因为代理离胜利仅一步之遥，而顶部的状态将产生中等值，因为代理需要阻止对手获胜。
- en: 'Let''s review the terms we have covered so far:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止涵盖的术语：
- en: '| **Term** | **Description** | **What does it output?** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **术语** | **描述** | **它输出什么？** |'
- en: '| Policy | The algorithm or function that outputs decisions the agent makes
    | A scalar/single decision (deterministic policy) or a vector of probabilities
    over possible actions (stochastic policy) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 算法或函数，用于输出代理做出的决策 | 输出单一决策的标量（确定性策略）或关于可能动作的概率向量（随机策略） |'
- en: '| Value Function | The function that describes how good or bad a given state
    is | A scalar value representing the expected cumulative reward |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 价值函数 | 描述某个状态好坏的函数 | 表示期望累积奖励的标量值 |'
- en: '| Model | An agent''s representation of the environment, which predicts how
    the environment will react to the agent''s actions | The probability of the next
    state given an action and current state, or an enumeration of possible states
    given the rules of the environment |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 代理对环境的表示，预测环境如何对代理的行为作出反应 | 给定动作和当前状态下的下一个状态的概率，或根据环境规则列出可能的状态 |'
- en: 'In the following sections, we will use these concepts to learn about one of
    the most fundamental frameworks in reinforcement learning: the Markov decision
    process.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将利用这些概念来学习强化学习中最基础的框架之一：马尔可夫决策过程。
- en: Markov decision process (MDP)
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDP）
- en: 'A Markov decision process is a framework used to represent the environment
    of a reinforcement learning problem. It is a graphical model with directed edges
    (meaning that one node of the graph points to another node). Each node represents
    a possible state in the environment, and each edge pointing out of a state represents
    an action that can be taken in the given state. For example, consider the following
    MDP:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程是一个用来表示强化学习问题环境的框架。它是一个有向图模型（意味着图中的一个节点指向另一个节点）。每个节点代表环境中的一个可能状态，每个从状态指向外部的边代表在给定状态下可以采取的动作。例如，考虑以下的
    MDP：
- en: '![](img/3689b1c6-d71e-44ff-b5bf-e7ddf2ac33aa.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3689b1c6-d71e-44ff-b5bf-e7ddf2ac33aa.png)'
- en: 'Figure 5: A sample Markov Decision Process'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：一个示例马尔可夫决策过程
- en: The preceding MDP represents what a typical day of a programmer could look like.
    Each circle represents a particular state the programmer can be in, where the
    blue state (Wake Up) is the initial state (or the state the agent is in at *t*=0),
    and the orange state (Publish Code) denotes the terminal state. Each arrow represents
    the transitions that the programmer can make between states. Each state has a
    reward that is associated with it, and the higher the reward, the more desirable
    the state is.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 MDP 表示程序员典型一天的情景。每个圆圈代表程序员可能处于的某个状态，其中蓝色状态（醒来）是初始状态（即代理在 *t*=0 时的状态），而橙色状态（发布代码）表示终止状态。每个箭头表示程序员可以在状态之间进行的转换。每个状态都有与之相关的奖励，奖励越高，该状态越具吸引力。
- en: 'We can tabulate the rewards as an adjacency matrix as well:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将奖励制成邻接矩阵：
- en: '| **State\action** | **Wake Up** | **Netflix** | **Code and debug** | **Nap**
    | **Deploy** | **Sleep** |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **状态\动作** | **醒来** | **Netflix** | **编写代码和调试** | **小睡** | **部署** | **睡觉**
    |'
- en: '| Wake Up | N/A | -2 | -3 | N/A | N/A | N/A |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 醒来 | N/A | -2 | -3 | N/A | N/A | N/A |'
- en: '| Netflix | N/A | -2 | N/A | N/A | N/A | N/A |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Netflix | N/A | -2 | N/A | N/A | N/A | N/A |'
- en: '| Code and debug | N/A | N/A | N/A | 1 | 10 | 3 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 编写代码和调试 | N/A | N/A | N/A | 1 | 10 | 3 |'
- en: '| Nap | 0 | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 小睡 | 0 | N/A | N/A | N/A | N/A | N/A |'
- en: '| Deploy | N/A | N/A | N/A | N/A | N/A | 3 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 部署 | N/A | N/A | N/A | N/A | N/A | 3 |'
- en: '| Sleep | N/A | N/A | N/A | N/A | N/A | N/A |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 睡觉 | N/A | N/A | N/A | N/A | N/A | N/A |'
- en: The left column represents the possible states and the top row represents the
    possible actions. N/A means that the action is not performable from the given
    state. This system basically represents the decisions that a programmer can make
    throughout their day.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 左列表示可能的状态，顶行表示可能的动作。N/A意味着在给定状态下无法执行该动作。该系统基本上表示程序员在一天中的决策过程。
- en: 'When the programmer wakes up, they can either decide to work (code and debug
    the code) or watch Netflix. Notice that the reward for watching Netflix is higher
    than that of coding and debugging. For the programmer in question, watching Netflix
    seems like a more rewarding activity, while coding and debugging is perhaps a
    chore (which, I hope, is not the case for the reader!). However, both actions
    yield negative rewards, even though our objective is to maximize our cumulative
    reward. If the programmer chooses to watch Netflix, they will be stuck in an endless
    loop of binge-watching, which continuously lowers the reward. Rather, more rewarding
    states will become available to the programmer if they decide to code diligently.
    Let''s look at the possible trajectories, which are the sequence of actions, the
    programmer can take:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当程序员醒来时，他们可以选择工作（编写和调试代码）或观看Netflix。请注意，观看Netflix的奖励高于编写和调试代码的奖励。对于这个程序员来说，观看Netflix似乎是更有回报的活动，而编写和调试代码可能是一项苦差事（希望读者不会是这种情况！）。然而，这两种行为都会导致负奖励，尽管我们的目标是最大化累积奖励。如果程序员选择观看Netflix，他们将陷入一个无休止的刷剧循环，这会不断降低奖励。相反，如果他们决定认真编写代码，更多有回报的状态将会向他们开放。我们来看看程序员可以采取的可能轨迹——即一系列动作：
- en: Wake Up | Netflix | Netflix | ...
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 醒来 | Netflix | Netflix | ...
- en: Wake Up | Code and debug | Nap | Wake Up | Code and debug | Nap | ...
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 醒来 | 编写代码和调试 | 小睡 | 醒来 | 编写代码和调试 | 小睡 | ...
- en: Wake Up | Code and debug | Sleep
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 醒来 | 编写代码和调试 | 睡觉
- en: Wake Up | Code and debug | Deploy | Sleep
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 醒来 | 编写代码和调试 | 部署 | 睡觉
- en: 'Both the first and second trajectories represent infinite loops. Let''s calculate
    the cumulative reward for each, where we set ![](img/2ce1a228-1215-4a7d-8ede-d76546086518.png):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '第一和第二条轨迹都代表了无限循环。让我们计算每条轨迹的累积奖励，假设我们设置了 ![](img/2ce1a228-1215-4a7d-8ede-d76546086518.png):'
- en: '![](img/0e4c2b34-4bcc-483b-af84-4487c6bcbada.png)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/0e4c2b34-4bcc-483b-af84-4487c6bcbada.png)'
- en: '![](img/f13ee029-465e-44d3-a489-b3f0f3efe3b2.png)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/f13ee029-465e-44d3-a489-b3f0f3efe3b2.png)'
- en: '![](img/df4ac549-14a7-409e-9a1a-903e43cc4000.png)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/df4ac549-14a7-409e-9a1a-903e43cc4000.png)'
- en: '![](img/dd65787b-7681-4863-af35-b177d76c4ee2.png)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/dd65787b-7681-4863-af35-b177d76c4ee2.png)'
- en: It is easy to see that both the first and second trajectories, despite not reaching
    a terminal state, will never return positive rewards. The fourth trajectory yields
    the highest reward (successfully deploying code is a highly rewarding accomplishment!).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，尽管第一条和第二条轨迹没有达到终止状态，但它们永远不会返回正奖励。第四条轨迹带来了最高的奖励（成功部署代码是一个非常有回报的成就！）。
- en: What we have calculated are the value functions for four policies that a programmer
    can take to go through their day. Recall that the value function is the expected
    cumulative reward starting from a given state and following a policy. We have
    observed four possible policies and have evaluated how each leads to a different
    cumulative reward; this exercise is also called **policy evaluation**. Moreover,
    the equations we have applied to calculate the expected rewards are also known
    as **Bellman expectation equations**. The Bellman equations are a set of equations
    used to evaluate and improve policies and value functions to help a reinforcement
    learning agent learn better. Though a thorough introduction to Bellman equations
    is outside the scope of this book, they are foundational to building a theoretical
    understanding of reinforcement learning. We encourage the reader to look into
    this further.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所计算的是四种策略的价值函数，程序员可以根据这些策略来规划他们的日常工作。回想一下，价值函数是指从给定状态出发并遵循某一策略的期望累计奖励。我们已经观察到四种可能的策略，并评估了每种策略如何导致不同的累计奖励；这个过程也叫做**策略评估**。此外，我们用于计算期望奖励的方程也被称为**贝尔曼期望方程**。贝尔曼方程是一组用于评估和改进策略与价值函数的方程，帮助强化学习智能体更好地学习。虽然本书并不深入介绍贝尔曼方程，但它们是构建强化学习理论理解的基础。我们鼓励读者深入研究这一主题。
- en: 'While we will not cover Bellman equations in depth, we highly recommend the
    reader to do so in order to build a solid understanding of reinforcement learning.
    For more information, refer to *Reinforcement Learning: An Introduction*, by Richard
    S. Sutton and Andrew Barto (reference at the end of this chapter).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不会深入讲解贝尔曼方程，但我们强烈建议读者学习贝尔曼方程，以便建立强化学习的扎实理解。有关更多信息，请参见理查德·S·萨顿和安德鲁·巴托所著的《*强化学习：导论*》（本章末尾有参考文献）。
- en: Now that you have learned about some the key terms and concepts of reinforcement
    learning, you may be wondering how we teach a reinforcement learning agent to
    maximize its reward, or in other words, find that the fourth trajectory is the
    best. In this book, you will be working on solving this question for numerous
    tasks and problems, all using deep learning. While we encourage you to be familiar
    with the basics of deep learning, the following sections will serve as a light
    refresher to the field.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了强化学习的一些关键术语和概念，或许你会想知道我们是如何教导强化学习智能体去最大化其奖励，换句话说，如何确定第四条轨迹是最优的。在本书中，你将通过解决许多任务和问题来实现这一目标，所有这些任务都将使用深度学习。虽然我们鼓励你熟悉深度学习的基础知识，但以下部分将作为该领域的轻度复习。
- en: Deep learning
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: Deep learning has become one of the most popular and recognizable fields of
    machine learning and computer science. Thanks to an increase in both available
    data and computational resources, deep learning algorithms have successfully surpassed
    previous state-of-the-art results in countless tasks. For several domains, including
    image recognition and playing Go, deep learning has even exceeded the capabilities
    of mankind.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已成为机器学习和计算机科学中最受欢迎且最具辨识度的领域之一。得益于可用数据和计算资源的增加，深度学习算法在无数任务中成功超越了以往的最先进成果。在多个领域，包括图像识别和围棋，深度学习甚至超越了人类的能力。
- en: It is thus not surprising that many reinforcement learning algorithms have started
    to utilize deep learning to bolster performance. Many of the reinforcement learning
    algorithms from the beginning of this chapter rely on deep learning. This book,
    too, will revolve around deep learning algorithms used to tackle reinforcement
    learning problems.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多强化学习算法开始利用深度学习来提高性能也就不足为奇了。本章开头提到的许多强化学习算法都依赖于深度学习。本书也将围绕深度学习算法展开，以解决强化学习问题。
- en: The following sections will serve as a refresher on some of the most fundamental
    concepts of deep learning, including neural networks, backpropagation, and convolution.
    However, if are unfamiliar with these topics, we highly encourage you to seek
    other sources for a more in-depth introduction.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将作为深度学习一些最基本概念的复习，包括神经网络、反向传播和卷积。然而，如果你不熟悉这些主题，我们强烈建议你寻求其他来源，获取更深入的介绍。
- en: Neural networks
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'A neural network is a type of computational architecture that is composed of
    layers of perceptrons. A perceptron, first conceived  in the 1950s by Frank Rosenblatt,
    models the biological neuron and computes a linear combination of a vector of
    input. It also outputs a transformation of the linear combination using a non-linear
    activation, such as the sigmoid function. Suppose a perceptron receives an input
    vector of ![](img/b39f4391-4708-416b-b9ec-d84b48dd6199.png). The output, *a*,
    of the perceptron, would be as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种计算架构，由多层感知机组成。感知机由Frank Rosenblatt于1950年代首次构思，模拟生物神经元并计算输入向量的线性组合。它还使用非线性激活函数（如sigmoid函数）对线性组合进行转换并输出结果。假设一个感知机接收输入向量为![](img/b39f4391-4708-416b-b9ec-d84b48dd6199.png)。感知机的输出*a*如下：
- en: '![](img/e0b5df84-5d65-4f22-8c83-ffac52e8f37b.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0b5df84-5d65-4f22-8c83-ffac52e8f37b.png)'
- en: '![](img/83824b91-f9f3-465e-9729-60b612a40ac7.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83824b91-f9f3-465e-9729-60b612a40ac7.png)'
- en: Where![](img/f5edf276-9cdb-4ffb-9a46-b75cf44e960a.png)are the weights of the
    perceptron, *b* is a constant, called the **bias**, and![](img/4a6b153a-6786-4012-8b2a-af618dba2365.png)is
    the sigmoid activation function that outputs a value between 0 and 1.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/f5edf276-9cdb-4ffb-9a46-b75cf44e960a.png)是感知机的权重，*b*是常数，称为**偏置**，而![](img/4a6b153a-6786-4012-8b2a-af618dba2365.png)是sigmoid激活函数，它输出一个介于0和1之间的值。
- en: Perceptrons have been widely used as a computational model to make decisions.
    Suppose the task was to predict the likelihood of sunny weather the next day.
    Each![](img/0d974f85-0cd9-4f6a-9af3-185498a44dd6.png)would represent a variable,
    such as the temperature of the current day, humidity, or the weather of the previous
    day. Then,![](img/5a2faf8a-f99b-4355-8570-70d24f7dfb7b.png)would compute a value
    that reflects how likely it is that there will be sunny weather tomorrow. If the
    model has a good set of values for ![](img/d46a16f7-4915-42c1-8f20-1d63cdc4612b.png),
    it is able to make accurate decisions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机被广泛用作决策的计算模型。假设任务是预测第二天晴天的可能性。每一个![](img/0d974f85-0cd9-4f6a-9af3-185498a44dd6.png)都代表一个变量，比如当天的温度、湿度或前一天的天气。然后，![](img/5a2faf8a-f99b-4355-8570-70d24f7dfb7b.png)将计算一个值，反映明天晴天的可能性。如果模型对于![](img/d46a16f7-4915-42c1-8f20-1d63cdc4612b.png)有一组好的值，它就能做出准确的决策。
- en: 'In a typical neural network, there are multiple layers of neurons, where each
    neuron in a given layer is connected to all neurons in the prior and subsequent
    layers. Hence these layers are also referred to as **fully-connected layers**.
    The weights of a given layer, *l*, can be represented as a matrix, *W^l*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的神经网络中，有多层神经元，每一层的每个神经元都与前一层和后一层的所有神经元相连接。因此，这些层也被称为**全连接层**。给定层的权重，*l*，可以表示为矩阵*W^l*：
- en: '![](img/9b87c57b-a48f-45a5-be1e-1e8975ee902e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b87c57b-a48f-45a5-be1e-1e8975ee902e.png)'
- en: '![](img/888affc6-ff04-4339-b490-62aff2826f91.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/888affc6-ff04-4339-b490-62aff2826f91.png)'
- en: 'Where each *w[ij]* denotes the weight between the *i* neuron of the previous
    layer and the *j* neuron of this layer. *B^l* denotes a vector of biases, one
    for each neuron in the *l* layer. Hence, the activation, *a^l*, of a given layer, *l*,
    can be defined as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，每个*w[ij]*表示前一层的第*i*个神经元与当前层的第*j*个神经元之间的权重。*B^l*表示偏置向量，每个神经元都有一个偏置。于是，给定层*l*的激活值*a^l*可以定义如下：
- en: '![](img/16fbd6ed-bd48-47ec-8f89-fcf3e21390d8.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16fbd6ed-bd48-47ec-8f89-fcf3e21390d8.png)'
- en: '![](img/89cfdef4-6377-40c7-a9c2-c7b3689f9ec8.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89cfdef4-6377-40c7-a9c2-c7b3689f9ec8.png)'
- en: 'Where *a⁰(x)* is just the input. Such neural networks with multiple layers
    of neurons are called **multilayer perceptrons** (**MLP**). There are three components
    in an MLP: the input layer, the hidden layers, and the output layer. The data
    flows from the input layer, transformed through a series of linear and non-linear
    functions in the hidden layers, and is outputted from the output layer as a decision
    or a prediction. Hence this architecture is also referred to as a feed-forward
    network. The following diagram shows what a fully-connected network would look
    like:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*a⁰(x)*只是输入。具有多层神经元的神经网络被称为**多层感知机**（**MLP**）。一个MLP有三个组件：输入层、隐藏层和输出层。数据从输入层开始，通过隐藏层中的一系列线性和非线性函数进行转换，然后从输出层输出为决策或预测。因此，这种架构也被称为前馈网络。以下图示展示了一个完全连接的网络的样子：
- en: '![](img/84aad163-22ce-4216-b5bb-afd517c12ce7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84aad163-22ce-4216-b5bb-afd517c12ce7.png)'
- en: 'Figure 6: A sketch of a multilayer perceptron'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：多层感知机的示意图
- en: Backpropagation
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: As mentioned previously, a neural network's performance depends on how good
    the values of *W* are (for simplicity, we will refer to both the weights and biases
    as *W*). When the whole network grows in size, it becomes untenable to manually
    determine the optimal weights for each neuron in every layer. Therefore, we rely
    on backpropagation, an algorithm that iteratively and automatically updates the
    weights of every neuron.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络的性能取决于 *W* 的值有多好（为简便起见，我们将权重和偏差统称为 *W*）。当整个网络的规模增长时，手动为每一层的每个神经元确定最优权重变得不可行。因此，我们依赖于反向传播算法，它迭代且自动地更新每个神经元的权重。
- en: To update the weights, we first need the ground truth, or the target value that
    the neural network tries to output. To understand what this ground truth could
    look like, we formulate a sample problem. The `MNIST` dataset is a large repository
    of 28x28 images of handwritten digits. It contains 70,000 images in total and
    serves as a popular benchmark for machine learning models. Given ten different
    classes of digits (from zero to nine), we would like to identify which digit class
    a given images belongs to. We can represent the ground truth of each image as
    a vector of length 10, where the index of the class (starting from 0) is marked
    as 1 and the rest are 0s. For example, an image, *x*, with a class label of five
    would have the ground truth of ![](img/82513266-77d8-4584-9f41-3c233a88f953.png),
    where *y* is the target function we approximate.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新权重，我们首先需要地面真实值，或者说神经网络尝试输出的目标值。为了理解这个地面真实值是什么样子，我们设定了一个样本问题。`MNIST` 数据集是一个包含
    28x28 手写数字图像的大型库。它总共有 70,000 张图像，并且是机器学习模型的一个常见基准。给定十个不同的数字类别（从 0 到 9），我们希望识别给定图像属于哪个数字类别。我们可以将每张图像的地面真实值表示为一个长度为
    10 的向量，其中类别的索引（从 0 开始）被标记为 1，其余为 0。例如，图像 *x* 的类别标签为 5，则其地面真实值为 ![](img/82513266-77d8-4584-9f41-3c233a88f953.png)，其中
    *y* 是我们要逼近的目标函数。
- en: What should the neural network look like? If we take each pixel in the image
    to be an input, we would have 28x28 neurons in the input layer (every image would
    be flattened to become a 784-dimensional vector). Moreover, because there are
    10 digit classes, we have 10 neurons in the output layer, each neuron producing
    a sigmoid activation for a given class. There can be an arbitrary number of neurons
    in the hidden layers.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络应该是什么样的？如果我们将图像中的每个像素看作输入，我们将会在输入层中拥有 28x28 个神经元（每张图像将被展平，变为一个 784 维的向量）。此外，由于有
    10 个数字类别，我们在输出层中有 10 个神经元，每个神经元为给定类别产生一个 sigmoid 激活函数。隐藏层的神经元数量可以是任意的。
- en: Let *f* represent the sequence of transformations that the neural network computes,
    parameterized by the weights, *W*. *f* is essentially an approximation of the
    target function, *y*, and maps the 784-dimensional input vector to a 10 dimensional
    output prediction. We classify the image according to the index of the largest
    sigmoid output.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让 *f* 代表神经网络计算的转换序列，参数化为权重 *W*。*f* 本质上是目标函数 *y* 的近似，并将 784 维的输入向量映射到 10 维的输出预测。我们根据最大
    sigmoid 输出的索引来对图像进行分类。
- en: 'Now that we have formulated the ground truth, we can measure the distance between
    it and the network''s prediction. This error is what allows the network to update
    its weights. We define the error function *E(W)* as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经制定了地面真实值，我们可以衡量它与网络预测之间的距离。这个误差使得网络能够更新它的权重。我们将误差函数 *E(W)* 定义如下：
- en: '![](img/11792f9a-716f-4a06-b64a-82ac338d94d3.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11792f9a-716f-4a06-b64a-82ac338d94d3.png)'
- en: The goal of backpropagation is to minimize *E* by finding the right set of *W*.
    This minimization is an optimization problem whereby we use gradient descent to
    iteratively compute the gradients of *E* with respect to *W* and propagate them
    through the network starting from the output layer.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的目标是通过找到合适的 *W* 来最小化 *E*。这个最小化是一个优化问题，我们使用梯度下降法迭代地计算 *E* 相对于 *W* 的梯度，并从输出层开始将其传播通过网络。
- en: Unfortunately, an in-depth explanation of backpropagation is outside the scope
    of this introductory chapter. If you are unfamiliar with this concept, we highly
    encourage you to study it first.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，反向传播的深入解释超出了本章节的范围。如果你对这个概念不熟悉，我们强烈建议你首先学习它。
- en: Convolutional neural networks
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'Using backpropagation, we are now able to train large networks automatically.
    This has led to the development of increasingly complex neural network architectures.
    One example is the **convolutional neural network** (**CNN**). There are mainly
    three types of layers in a CNN: the convolutional layer, the pooling layer, and
    the fully-connected layer. The fully-connected layer is identical to the standard
    neural network discussed previously. In the convolutional layer, weights are part
    of convolutional kernels. Convolution on a two-dimensional array of image pixels
    is defined as the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过反向传播，我们现在能够自动训练大型网络。这导致了越来越复杂的神经网络架构的出现。一个例子是**卷积神经网络**（**CNN**）。CNN中主要有三种类型的层：卷积层、池化层和全连接层。全连接层与之前讨论的标准神经网络相同。在卷积层中，权重是卷积核的一部分。对二维图像像素数组的卷积定义如下：
- en: '![](img/c81ae657-337c-4b21-be08-f4a2a7dc81f5.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c81ae657-337c-4b21-be08-f4a2a7dc81f5.png)'
- en: Where *f(u, v)* is the pixel intensity of the input at coordinate *(u, v)*,
    and *g(x-u, y-v)* is the weight of the convolutional kernel at that location.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*f(u, v)*是输入在坐标*(u, v)*处的像素强度，*g(x-u, y-v)*是该位置卷积核的权重。
- en: A convolutional layer comprises a stack of convolutional kernels; hence the
    weights of a convolutional layer can be visualized as a three-dimensional box
    as opposed to the two-dimensional array that we defined for fully-connected layers.
    The output of a single convolutional kernel applied to an input is also a two-dimensional
    mapping, which we call a filter. Because there are multiple kernels, the output
    of a convolutional layer is again a three-dimensional box, which can be referred
    to as a volume.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层由一组卷积核组成；因此，卷积层的权重可以视为一个三维的盒子，而不是我们为全连接层定义的二维数组。应用于输入的单个卷积核的输出也是二维映射，我们称之为滤波器。由于有多个卷积核，卷积层的输出再次是一个三维的盒子，可以称之为体积。
- en: Finally, the pooling layer reduces the size of the input by taking *m*m* local
    patches of pixels and outputting a scalar. The max-pooling layer takes *m*m* patches
    and outputs the greatest value among the patch of pixels.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，池化层通过对*m*m*局部像素块进行操作，并输出一个标量，来减小输入的大小。最大池化层对*m*m*像素块进行操作，并输出该像素块中的最大值。
- en: Given an input volume of the (32, 32, 3) shape—corresponding to height, width,
    and depth (channels)—a max-pooling layer with a pooling size of 2x2 will output
    a volume of the (16, 16, 3) shape. The input to the CNN are usually images, which
    can also be viewed as volumes where the depth corresponds to RGB channels.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个形状为(32, 32, 3)的输入体积——对应于高度、宽度和深度（通道）——一个池化大小为2x2的最大池化层将输出形状为(16, 16, 3)的体积。CNN的输入通常是图像，也可以看作是深度对应于RGB通道的体积。
- en: 'The following is a depiction of a typical convolutional neural network:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是典型卷积神经网络的示意图：
- en: '![](img/dc56335f-cb3f-48b6-bfac-238e51e32725.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc56335f-cb3f-48b6-bfac-238e51e32725.png)'
- en: 'Figure 7: An example convolutional neural network'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个示例卷积神经网络
- en: Advantages of neural networks
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的优势
- en: The main advantage of a CNN over a standard neural network is that the former
    is able to learn visual and spatial features of the input, while for the latter
    such information is lost due to flattening input data into a vector. CNNs have
    made significant strides in the field of computer vision, starting with increased
    classification accuracies of `MNIST` data and object recognition, semantic segmentation,
    and other domains. CNNs have many applications in real life, from facial detection
    in social media to autonomous vehicles. Recent approaches have also applied CNNs
    to natural language processing and text classification tasks to produce state-of-the-art
    results.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: CNN相比标准神经网络的主要优势在于，前者能够学习输入的视觉和空间特征，而后者由于将输入数据展平为向量，丢失了这类信息。CNN在计算机视觉领域取得了重大进展，最早体现在对`MNIST`数据的分类准确率提升，以及物体识别、语义分割等领域。CNN在现实生活中有广泛的应用，从社交媒体中的人脸检测到自动驾驶汽车。最近的研究还将CNN应用于自然语言处理和文本分类任务，取得了最先进的成果。
- en: Now that we have covered the basics of machine learning, we will go through
    our first implementation exercise.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了机器学习的基础知识，接下来我们将进行第一次实现练习。
- en: Implementing a convolutional neural network in TensorFlow
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现卷积神经网络
- en: In this section, we will implement a simple convolutional neural network in
    TensorFlow to solve an image classification task. As the rest of this book will
    be heavily reliant on TensorFlow and CNNs, we highly recommend that  you become
    sufficiently familiar with implementing deep learning algorithms using this framework.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 TensorFlow 实现一个简单的卷积神经网络，以解决图像分类任务。由于本书其余部分将大量依赖于 TensorFlow 和 CNNs，我们强烈建议你熟悉如何使用该框架实现深度学习算法。
- en: TensorFlow
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow, developed by Google in 2015, is one of the most popular deep learning
    frameworks in the world. It is used widely for research and commercial projects
    and boasts a rich set of APIs and functionalities to help researchers and practitioners
    develop deep learning models. TensorFlow programs can run on GPUs as well as CPUs,
    and thus abstract the GPU programming to make development more convenient.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google 于 2015 年开发的，是世界上最流行的深度学习框架之一。它被广泛应用于研究和商业项目，并拥有丰富的 API
    和功能，帮助研究人员和从业人员开发深度学习模型。TensorFlow 程序可以在 GPU 和 CPU 上运行，从而将 GPU 编程进行了抽象化，使开发变得更加便捷。
- en: Throughout this book, we will be using TensorFlow exclusively, so make sure
    you are familiar with the basics as you progress through the chapters.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中，我们将专门使用 TensorFlow，因此在继续阅读本书的章节时，确保你熟悉基本知识。
- en: Visit [https://www.tensorflow.org/](https://www.tensorflow.org/) for a complete
    set of documentation and other tutorials.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [https://www.tensorflow.org/](https://www.tensorflow.org/) 获取完整的文档和其他教程。
- en: The Fashion-MNIST dataset
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fashion-MNIST 数据集
- en: 'Those who have experience with deep learning have most likely heard about the
    `MNIST` dataset. It is one of the most widely-used image datasets, serving as
    a benchmark for tasks such as image classification and image generation, and is
    used by many computer vision models:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有深度学习经验的人大多听说过 `MNIST` 数据集。它是最广泛使用的图像数据集之一，作为图像分类和图像生成等任务的基准，并且被许多计算机视觉模型使用：
- en: '![](img/8248ffb8-5363-4ec7-84fc-21d807ecd434.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8248ffb8-5363-4ec7-84fc-21d807ecd434.png)'
- en: 'Figure 8: The MNIST dataset (reference at end of chapter)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：MNIST 数据集（本章末尾有参考文献）
- en: 'There are several problems with `MNIST`, however. First of all, the dataset
    is too easy, since a simple convolutional neural network is able to achieve 99%
    test accuracy. In spite of this, the dataset is used far too often in research
    and benchmarks. The `F-MNIST` dataset, produced by the online fashion retailer
    Zalando, is a more complex, much-needed upgrade to `MNIST`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`MNIST` 数据集存在一些问题。首先，数据集过于简单，因为一个简单的卷积神经网络就能实现 99% 的测试准确率。尽管如此，该数据集在研究和基准测试中仍被过度使用。由在线时尚零售商
    Zalando 提供的 `F-MNIST` 数据集，是对 `MNIST` 数据集的一个更复杂、更具挑战性的升级版本：
- en: '![](img/1323649e-eba8-48da-8242-490ff261a28b.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1323649e-eba8-48da-8242-490ff261a28b.png)'
- en: 'Figure 9: The Fashion-MNIST dataset (taken from [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist),
    reference at the end of this chapter)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：Fashion-MNIST 数据集（来源于 [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)，本章末尾有参考文献）
- en: 'Instead of digits, the `F-MNIST` dataset includes photos of ten different clothing
    types (ranging from t-shirts to shoes) compressed in to 28x28 monochrome thumbnails.
    Hence, `F-MNIST` serves as a convenient drop-in replacement to `MNIST` and is
    increasingly gaining popularity in the community. Hence we will train our CNN
    on `F-MNIST` as well. The preceding table maps each label index to its class:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与数字不同，`F-MNIST` 数据集包含了十种不同类型的服装照片（从T恤到鞋子不等），这些照片被压缩为 28x28 的单色缩略图。因此，`F-MNIST`
    作为 `MNIST` 的一个方便的替代品，越来越受到社区的欢迎。因此，我们也将在 `F-MNIST` 上训练我们的 CNN。前面的表格将每个标签索引映射到对应的类别：
- en: '| **Index** | **Class** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **索引** | **类别** |'
- en: '| 0 | T-shirt/top |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 0 | T恤/上衣 |'
- en: '| 1 | Trousers |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 长裤 |'
- en: '| 2 | Pullover |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 套头衫 |'
- en: '| 3 | Dress |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 连衣裙 |'
- en: '| 4 | Coat |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 外套 |'
- en: '| 5 | Sandal |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 凉鞋 |'
- en: '| 6 | Shirt |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 衬衫 |'
- en: '| 7 | Sneaker |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 运动鞋 |'
- en: '| 8 | Bag |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 包 |'
- en: '| 9 | Ankle boot |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 高帮靴 |'
- en: In the following subsections, we will design a convolutional neural network
    that will learn to classify data from this dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的子章节中，我们将设计一个卷积神经网络，它将学习如何对来自该数据集的数据进行分类。
- en: Building the network
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络
- en: Multiple deep learning frameworks have already implemented APIs for loading
    the `F-MNIST` dataset, including TensorFlow. For our implementation, we will be
    using Keras, another popular deep learning framework that is integrated with TensorFlow.
    The Keras datasets module provides a highly convenient interface for loading the
    datasets as `numpy` arrays.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 多个深度学习框架已经实现了加载`F-MNIST`数据集的API，包括TensorFlow。在我们的实现中，我们将使用Keras，这是另一个与TensorFlow集成的流行深度学习框架。Keras的数据集模块提供了一个非常方便的接口，将数据集加载为`numpy`数组。
- en: Finally, we can start coding! For this exercise, we only need one Python module,
    which we will call `cnn.py`. Open up your favorite text editor or IDE, and let's
    get started.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以开始编写代码了！对于本次练习，我们只需要一个Python模块，我们将其命名为`cnn.py`。打开你喜欢的文本编辑器或IDE，开始吧。
- en: 'Our first step is to declare the modules that we are going to use:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是声明我们将使用的模块：
- en: '[PRE2]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following describes what each module is for and how we will use it:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了每个模块的用途以及我们将如何使用它：
- en: '| **Module(s)** | **Purpose** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **模块** | **用途** |'
- en: '| `logging` | For printing statistics as we run the code |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `logging` | 用于在运行代码时打印统计信息 |'
- en: '| `os`, `sys` | For interacting with the operating system, including writing
    files |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `os`, `sys` | 用于与操作系统交互，包括写文件 |'
- en: '| `tensorflow` | The main TensorFlow library |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `tensorflow` | 主要的TensorFlow库 |'
- en: '| `numpy` | An optimized library for vector calculations and simple data processing
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `numpy` | 一个优化过的向量计算和简单数据处理库 |'
- en: '| `keras` | For downloading the F-MNIST dataset |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `keras` | 用于下载F-MNIST数据集 |'
- en: 'We will implement our CNN as a class called `SimpleCNN`. The `__init__` constructor
    takes a number of parameters:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个名为`SimpleCNN`的类。`__init__`构造函数接受多个参数：
- en: '[PRE3]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The parameters our `SimpleCNN` is initialized with are described here:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`SimpleCNN`初始化时的参数在此进行描述：
- en: '| **Parameter** | **Purpose** |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **用途** |'
- en: '| `learning_rate` | The learning rate for the optimization algorithm |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| `learning_rate` | 优化算法的学习率 |'
- en: '| `num_epochs` | The number of epochs it takes to train the network |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| `num_epochs` | 训练网络所需的epoch次数 |'
- en: '| `beta` | A float value (between 0 and 1) that controls the strength of the
    L2-penalty |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| `beta` | 一个浮动值（介于0和1之间），控制L2惩罚的强度 |'
- en: '| `batch_size` | The number of images to train on in a single step |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| `batch_size` | 每次训练中处理的图像数量 |'
- en: Moreover, `save_dir` and `save_path` refer to the locations where we will store
    our network's parameters. `logs_dir` and `logs_path` refer to the locations where
    the statistics of the training run will be stored (we will show how we can retrieve
    these logs later).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`save_dir`和`save_path`指代的是我们将存储网络参数的位置，`logs_dir`和`logs_path`则指代存储训练过程统计信息的位置（稍后我们会展示如何获取这些日志）。
- en: Methods for building the network
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络的方法
- en: 'Now, in this section, we will see two methods that can be used to build the
    function, which are:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到两种可以用于构建该功能的方法，它们分别是：
- en: build method
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建方法
- en: fit method
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合方法
- en: build method
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建方法
- en: 'The first method we will define for our `SimpleCNN` class is the `build` method,
    which is responsible for building the architecture of our CNN. Our `build` method
    takes two pieces of input: the input tensor and the number of classes it should
    expect:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`SimpleCNN`类定义的第一个方法是`build`方法，它负责构建CNN的架构。我们的`build`方法接受两个输入：输入张量和它应该预期的类别数：
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We will first initialize `tf.placeholder`, called `is_training`. TensorFlow
    placeholders are like variables that don''t have values. We only pass them values
    when we actually train the network and call the relevant operations:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化`tf.placeholder`，命名为`is_training`。TensorFlow的占位符类似于没有值的变量，只有在实际训练网络并调用相关操作时，我们才会给它赋值：
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `tf.name_scope(...)` block allows us to name our operations and tensors
    properly. While this is not absolutely necessary, it helps us organize our code
    better and will help us to visualize the network. Here, we define a `tf.placeholder_with_default`
    called `is_training`, which has a default value of `True`. This placeholder will
    be used for our dropout operations (since dropout has different modes during training
    and inference).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.name_scope(...)`块允许我们为操作和张量命名。虽然这不是绝对必要的，但它有助于更好地组织代码，也有助于我们可视化网络。在这里，我们定义了一个名为`is_training`的`tf.placeholder_with_default`，其默认值为`True`。这个占位符将用于我们的dropout操作（因为dropout在训练和推理阶段有不同的模式）。'
- en: Naming your operations and tensors is considered a good practice. It helps you
    organize your code.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为操作和张量命名被认为是一种好习惯，它有助于你更好地组织代码。
- en: 'Our next step is to define the convolutional layers of our CNN. We make use
    of three different kinds of layers to create multiple layers of convolutions: `tf.layers.conv2d`, `tf.max_pooling2d`,
    and `tf.layers.dropout`:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义我们CNN的卷积层。我们使用三种不同类型的层来创建多个卷积层：`tf.layers.conv2d`、`tf.max_pooling2d`和`tf.layers.dropout`：
- en: '[PRE6]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here are some explanations of the parameters:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些参数的解释：
- en: '| **Parameter** | **Type** | **Description** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **类型** | **描述** |'
- en: '| `filters` | `int` | Number of filters output by the convolution. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `filters` | `int` | 卷积层输出的过滤器数量。 |'
- en: '| `kernel_size` | Tuple of `int` | The shape of the kernel. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `kernel_size` | `int`元组 | 卷积核的形状。 |'
- en: '| `pool_size` | Tuple of `int` | The shape of the max-pooling window. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `pool_size` | `int`元组 | 最大池化窗口的形状。 |'
- en: '| `strides` | `int` | The number of pixels to slide across per convolution/max-pooling
    operation. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `strides` | `int` | 每次卷积/最大池化操作时，滑动的像素数量。 |'
- en: '| `padding` | `str` | Whether to add padding (SAME) or not (VALID). If padding
    is added, the output shape of the convolution remains the same as the input shape.
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `padding` | `str` | 是否添加填充（SAME）或者不添加（VALID）。如果添加填充，卷积的输出形状将与输入形状保持一致。 |'
- en: '| `activation` | `func` | A TensorFlow activation function. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `activation` | `func` | 一个TensorFlow激活函数。 |'
- en: '| `kernel_regularizer` | `op` | Which regularization to use for the convolutional
    kernel. The default value is `None`. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `kernel_regularizer` | `op` | 使用哪种正则化方法来约束卷积核。默认值是`None`。 |'
- en: '| `training` | `op` | A tensor/placeholder that tells the dropout operation
    whether the forward pass is for training or for inference. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `training` | `op` | 一个张量/占位符，用于告诉dropout操作当前是用于训练还是推理。 |'
- en: 'In the preceding table, we have specified the convolutional architecture to
    have the following sequence of layers:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的表格中，我们指定了卷积架构的层次顺序如下：
- en: CONV | CONV | POOL | DROPOUT | CONV | CONV | POOL | DROPOUT
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: CONV | CONV | POOL | DROPOUT | CONV | CONV | POOL | DROPOUT
- en: However, you are encouraged to explore different configurations and architectures.
    For example, you could add batch-normalization layers to improve the stability
    of training.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们鼓励您探索不同的配置和架构。例如，您可以添加批归一化层，以提高训练的稳定性。
- en: 'Finally, we add the fully-connected layers that lead to the output of the network:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加全连接层，将网络的输出生成最终结果：
- en: '[PRE7]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`tf.layers.flatten` turns the output of the convolutional layers (which is
    3-D) into a single vector (1-D) so that we can pass them through the `tf.layers.dense`
    layers. After going through two fully-connected layers, we return the final output,
    which we define as `logits`.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.layers.flatten`将卷积层的输出（即3维）转化为一个单一的向量（1维），这样我们就可以将它们传入`tf.layers.dense`层。经过两个全连接层后，我们返回最终的输出，这个输出我们定义为`logits`。'
- en: Notice that in the final `tf.layers.dense` layer, we do not specify an `activation`.
    We will see why when we move on to specifying the training operations of the network.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在最终的`tf.layers.dense`层中，我们没有指定`activation`。当我们开始指定网络的训练操作时，您会明白为什么这么做。
- en: 'Next, we implement several helper functions. `_create_tf_dataset` takes two
    instances of `numpy.ndarray` and turns them into TensorFlow tensors, which can
    be directly fed into a network. `_log_loss_and_acc` simply logs training statistics,
    such as loss and accuracy:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现几个辅助函数。`_create_tf_dataset`接受两个`numpy.ndarray`实例，并将它们转换为TensorFlow张量，可以直接传入网络。`_log_loss_and_acc`则简单地记录训练统计数据，如损失和准确率：
- en: '[PRE8]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: fit method
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: fit方法
- en: 'The last method we will implement for our `SimpleCNN` is the `fit` method.
    This function triggers training for our CNN. Our `fit` method takes four input:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为`SimpleCNN`实现的最后一个方法是`fit`方法。这个函数触发我们的CNN训练。我们的`fit`方法有四个输入：
- en: '| **Argument** | **Description** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **描述** |'
- en: '| `X_train` | Training data |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `X_train` | 训练数据 |'
- en: '| `y_train` | Training labels |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `y_train` | 训练标签 |'
- en: '| `X_test` | Test data |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `X_test` | 测试数据 |'
- en: '| `y_test` | Test labels |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `y_test` | 测试标签 |'
- en: 'The first step of `fit` is to initialize `tf.Graph` and `tf.Session`. Both
    of these objects are essential to any TensorFlow program. `tf.Graph` represents
    the graph in which all the operations for our CNN are defined. You can think of
    it as a sandbox where we define all the layers and functions. `tf.Session` is
    the class that actually executes the operations defined in `tf.Graph`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`的第一步是初始化`tf.Graph`和`tf.Session`。这两个对象是任何TensorFlow程序的核心。`tf.Graph`表示定义所有CNN操作的图。你可以把它看作是一个沙箱，我们在其中定义所有层和函数。`tf.Session`是实际执行`tf.Graph`中定义操作的类：'
- en: '[PRE9]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then create datasets using TensorFlow''s Dataset API and the `_create_tf_dataset`
    method we defined earlier:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 TensorFlow 的 Dataset API 和之前定义的`_create_tf_dataset`方法来创建数据集：
- en: '[PRE10]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`tf.data.Iterator` builds an iterator object that outputs a batch of images
    every time we call `iterator.get_next()`. We initialize a dataset each for the
    training and testing data. The result of `iterator.get_next()` is a tuple of input
    images and corresponding labels.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data.Iterator`构建一个迭代器对象，每次我们调用`iterator.get_next()`时，它都会输出一批图像。我们为训练数据和测试数据分别初始化数据集。`iterator.get_next()`的结果是输入图像和相应标签的元组。'
- en: 'The former is `input_tensor`, which we feed into the `build` method. The latter
    is used for calculating the loss function and backpropagation:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 前者是`input_tensor`，我们将其输入到`build`方法中。后者用于计算损失函数和反向传播：
- en: '[PRE11]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`logits` (the non-activated outputs of the network) are fed into two other
    operations: `prediction`, which is just the softmax over `logits` to obtain normalized
    probabilities over the classes, and `loss_ops`, which calculates the mean categorical
    cross-entropy between the predictions and the labels.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`（网络的非激活输出）输入到两个其他操作中：`prediction`，它只是对`logits`进行 softmax 处理，以获得类的归一化概率，以及`loss_ops`，它计算预测和标签之间的平均类别交叉熵。'
- en: 'We then define the backpropagation algorithm used to train the network and
    the operations used for calculating accuracy:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义了用于训练网络的反向传播算法和用于计算准确度的操作：
- en: '[PRE12]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are now done building the network along with its optimization algorithms.
    We use `tf.global_variables_initializer()` to initialize the weights and operations
    of our network. We also initialize the `tf.train.Saver` and `tf.summary.FileWriter`
    objects. The `tf.train.Saver` object saves the weights and architecture of the
    network, whereas the latter keeps track of various training statistics:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了网络和其优化算法的构建。我们使用`tf.global_variables_initializer()`来初始化网络的权重和操作。我们还初始化了`tf.train.Saver`和`tf.summary.FileWriter`对象。`tf.train.Saver`对象用于保存网络的权重和架构，而后者则跟踪各种训练统计数据：
- en: '[PRE13]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, once we have set up everything we need, we can implement the actual
    training loop. For every epoch, we keep track of the training cross-entropy loss
    and accuracy of the network. At the end of every epoch, we save the updated weights
    to disk. We also calculate the validation loss and accuracy every 10 epochs. This
    is done by calling `sess.run(...)`, where the arguments to this function are the
    operations that the `sess` object should execute:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一旦我们设置好所需的一切，就可以实现实际的训练循环。每个 epoch，我们跟踪训练的交叉熵损失和网络的准确度。在每个 epoch 结束时，我们将更新后的权重保存到磁盘。我们还会每
    10 个 epoch 计算一次验证损失和准确度。这是通过调用`sess.run(...)`来完成的，其中该函数的参数是`sess`对象应该执行的操作：
- en: '[PRE14]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: And that completes our `fit` function. Our final step is to create the script
    for instantiating the datasets, the neural network, and then running training,
    which we will write at the bottom of `cnn.py`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到这里，我们完成了`fit`函数。我们的最后一步是创建脚本，实例化数据集、神经网络，并运行训练，这将在`cnn.py`的底部编写：
- en: 'We will first configure our logger and load the dataset using the Keras `fashion_mnist`
    module, which loads the training and testing data:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先配置日志记录器，并使用 Keras 的`fashion_mnist`模块加载数据集，该模块加载训练数据和测试数据：
- en: '[PRE15]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We then apply some simple preprocessing to the data. The Keras API returns `numpy`
    arrays of the `(Number of images, 28, 28)` shape.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对数据进行一些简单的预处理。Keras API 返回形状为`(Number of images, 28, 28)`的`numpy`数组。
- en: However, what we actually want is `(Number of images, 28, 28, 1)`, where the
    third axis is the channel axis. This is required because our convolutional layers
    expect input that have three axes. Moreover, the pixel values themselves are in
    the range of `[0, 255]`. We will divide them by 255 to get a range of `[0, 1]`.
    This is a common technique that helps stabilize training.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们实际上需要的是`(Number of images, 28, 28, 1)`，其中第三个轴是通道轴。这是必须的，因为我们的卷积层期望输入具有三个轴。此外，像素值本身的范围是`[0,
    255]`。我们将它们除以 255，以获得`[0, 1]`的范围。这是一种常见的技术，有助于稳定训练。
- en: 'Furthermore, we turn the labels, which are simply an array of label indices,
    into one-hot encodings:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将标签（它只是标签索引的数组）转换为 one-hot 编码：
- en: '[PRE16]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We then define the input to the constructor of our `SimpleCNN`. Feel free to
    tweak the numbers to see how they affect the performance of the model:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了`SimpleCNN`构造函数的输入。可以自由调整这些数字，看看它们如何影响模型的性能：
- en: '[PRE17]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And finally, we instantiate `SimpleCNN` and call its `fit` method:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实例化`SimpleCNN`并调用其`fit`方法：
- en: '[PRE18]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To run the entire script, all you need to do is run the module:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行整个脚本，你只需运行模块：
- en: '[PRE19]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And that''s it! You have successfully implemented a convolutional neural network
    in TensorFlow to train on the `F-MNIST` dataset. To track the progress of the
    training, you can simply look at the output in your terminal/editor. You should
    see an output that resembles the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！你已经成功地在 TensorFlow 中实现了一个卷积神经网络，并在 `F-MNIST` 数据集上进行训练。要跟踪训练进度，你只需查看终端/编辑器中的输出。你应该看到类似以下的输出：
- en: '[PRE20]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another thing to check out is TensorBoard, a visualization tool developed by
    the developers of TensorFlow, to graph the model''s accuracy and loss. The `tf.summary.FileWriter`
    object we have used serves this purpose. You can run TensorBoard with the following
    command:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要查看的工具是 TensorBoard，这是由 TensorFlow 开发者开发的可视化工具，用于绘制模型的准确度和损失图。我们使用的 `tf.summary.FileWriter`
    对象用于此目的。你可以通过以下命令运行 TensorBoard：
- en: '[PRE21]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`logs` is where our `SimpleCNN` model writes the statistics to. TensorBoard
    is a great tool for visualizing the structure of our `tf.Graph`, as well as seeing
    how statistics such as accuracy and loss change over time. By default, the TensorBoard
    logs can be accessed by pointing your browser to `localhost:6006`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '`logs` 是我们的 `SimpleCNN` 模型写入统计数据的地方。TensorBoard 是一个很棒的工具，用于可视化我们的 `tf.Graph`
    结构，并观察准确度和损失等统计数据随时间的变化。默认情况下，TensorBoard 日志可以通过将浏览器指向 `localhost:6006` 进行访问：'
- en: '![](img/92d80158-fa80-4b15-ae61-602e9bd0af1f.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92d80158-fa80-4b15-ae61-602e9bd0af1f.png)'
- en: 'Figure 10: TensorBoard and its visualization of our CNN'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：TensorBoard 及其对我们 CNN 的可视化
- en: Congratulations! We have successfully implemented a convolutional neural network
    using TensorFlow. However, the CNN we implemented is rather rudimentary, and only
    achieves mediocre accuracy—the challenge to the reader is to tweak the architecture
    to improve its performance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经成功使用 TensorFlow 实现了一个卷积神经网络。然而，我们实现的 CNN 相当基础，且只实现了中等的准确度——挑战在于，读者可以调整架构以提高其性能。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took our first step in the world of reinforcement learning.
    We covered some of the fundamental concepts and terminology of the field, including
    the agent, the policy, the value function, and the reward. We  also covered basic
    topics in deep learning and implemented a simple convolutional neural network
    using TensorFlow.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们迈出了强化学习世界的第一步。我们介绍了一些该领域的基本概念和术语，包括智能体、策略、价值函数和奖励。同时，我们也涵盖了深度学习的基本主题，并使用
    TensorFlow 实现了一个简单的卷积神经网络。
- en: The field of reinforcement learning is vast and ever-expanding; it would be
    impossible to cover all of it in a single book. We do, however, hope to equip
    you with the practical skills and the necessary experience to navigate this field.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的领域非常广阔且不断扩展；在一本书中无法涵盖所有内容。然而，我们希望能为你提供必要的实践技能和经验，以便在这个领域中导航。
- en: The following chapters will consist of individual projects—we will use a combination
    of reinforcement learning and deep learning algorithms to tackle several tasks
    and problems. We will build agents that will learn to play Go, explore the world
    of Minecraft, and play Atari video games. We hope you are ready to embark on this
    exciting learning journey!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将包含独立的项目——我们将使用强化学习和深度学习算法的结合来解决多个任务和问题。我们将构建智能体，让它们学习下围棋、探索 Minecraft
    世界并玩 Atari 电子游戏。我们希望你已经准备好踏上这段激动人心的学习旅程！
- en: References
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT press, 1998.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton, Richard S., 和 Andrew G. Barto. *强化学习：导论*. MIT出版社，1998年。
- en: Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner*. Gradient-based learning applied
    to document recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998. *
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Y. LeCun, L. Bottou, Y. Bengio, 和 P. Haffner*. 基于梯度的学习应用于文档识别. 《IEEE 会议录》，86(11)：2278-2324，1998年11月。*
- en: 'Xiao, Han, Kashif Rasul, and Roland Vollgraf. *Fashion-mnist: a novel image
    dataset for benchmarking machine learning algorithms*. arXiv preprint arXiv:1708.07747 (2017).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao, Han, Kashif Rasul, 和 Roland Vollgraf. *Fashion-mnist：一种用于基准测试机器学习算法的新型图像数据集*.
    arXiv 预印本 arXiv:1708.07747 (2017)。
