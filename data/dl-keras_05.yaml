- en: Word Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Wikipedia defines word embedding as the collective name for a set of language
    modeling and feature learning techniques in **natural language processing** (**NLP**)
    where words or phrases from the vocabulary are mapped to vectors of real numbers.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Wikipedia 将词嵌入定义为一组语言建模和特征学习技术的统称，属于**自然语言处理**（**NLP**）领域，其中词汇表中的单词或短语被映射为实数向量。
- en: Word embeddings are a way to transform words in text to numerical vectors so
    that they can be analyzed by standard machine learning algorithms that require
    vectors as numerical input.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种将文本中的单词转换为数值向量的方式，以便它们能够被要求输入向量作为数值的标准机器学习算法分析。
- en: You have already learned about one type of word embedding called **one-hot encoding**,
    in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml), *Neural Networks Foundations*.
    One-hot encoding is the most basic embedding approach. To recap, one-hot encoding
    represents a word in the text by a vector of the size of the vocabulary, where
    only the entry corresponding to the word is a one and all the other entries are
    zero.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在[第 1 章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)《神经网络基础》中学习过一种词嵌入方法——**one-hot
    编码**。One-hot 编码是最基本的嵌入方法。回顾一下，one-hot 编码通过一个与词汇表大小相同的向量表示文本中的单词，其中仅对应单词的条目为 1，所有其他条目为
    0。
- en: A major problem with one-hot encoding is that there is no way to represent the
    similarity between words. In any given corpus, you would expect words such as
    (*cat*, *dog*), (*knife*, *spoon*), and so on to have some similarity. Similarity
    between vectors is computed using the dot product, which is the sum of element-wise
    multiplication between vector elements. In the case of one-hot encoded vectors,
    the dot product between any two words in a corpus is always zero.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 one-hot 编码的一个主要问题是无法表示单词之间的相似性。在任何给定的语料库中，你会希望像（*cat*，*dog*）、（*knife*，*spoon*）等单词对具有一定的相似性。向量之间的相似性是通过点积计算的，点积是向量元素逐元素相乘后求和的结果。在
    one-hot 编码的向量中，语料库中任意两个单词之间的点积始终为零。
- en: To overcome the limitations of one-hot encoding, the NLP community has borrowed
    techniques from **information retrieval** (**IR**) to vectorize text using the
    document as the context. Notable techniques are TF-IDF ([https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)),
    **latent semantic analysis** (**LSA**) ([https://en.wikipedia.org/wiki/Latent_semantic_analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)),
    and topic modeling ([https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)).
    However, these representations capture a slightly different document-centric idea
    of semantic similarity.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服 one-hot 编码的局限性，NLP 社区借鉴了**信息检索**（**IR**）中的技术，使用文档作为上下文来向量化文本。值得注意的技术包括
    TF-IDF（[https://en.wikipedia.org/wiki/Tf%E2%80%93idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)）、**潜在语义分析**（**LSA**）（[https://en.wikipedia.org/wiki/Latent_semantic_analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis)）和主题建模（[https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)）。然而，这些表示捕捉的是稍有不同的以文档为中心的语义相似性观念。
- en: Development of word embedding techniques began in earnest in 2000\. Word embedding
    differs from previous IR-based techniques in that they use words as their context,
    which leads to a more natural form of semantic similarity from a human understanding
    perspective. Today, word embedding is the technique of choice for vectorizing
    text for all kinds of NLP tasks, such as text classification, document clustering,
    part of speech tagging, named entity recognition, sentiment analysis, and so on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入技术的开发始于 2000 年。词嵌入与之前基于信息检索（IR）的技术不同，它们使用单词作为上下文，从而得到了更加自然、符合人类理解的语义相似度形式。今天，词嵌入已成为将文本向量化用于各种
    NLP 任务（如文本分类、文档聚类、词性标注、命名实体识别、情感分析等）的首选技术。
- en: In this chapter, we will learn about two specific forms of word embedding, GloVe
    and word2vec, collectively known as distributed representations of words. These
    embeddings have proven more effective and have been widely adopted in the deep
    learning and NLP communities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习两种特定形式的词嵌入，分别是 GloVe 和 word2vec，统称为词的分布式表示。这些嵌入已被证明更有效，并在深度学习和 NLP
    社区得到了广泛应用。
- en: We will also learn different ways in which you can generate your own embeddings
    in your Keras code, as well as how to use and fine-tune pre-trained word2vec and
    GloVe models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将学习如何在 Keras 代码中生成自己的词嵌入，以及如何使用和微调预训练的 word2vec 和 GloVe 模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Building various distributional representations of words in context
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上下文中构建各种分布式词表示
- en: Building models for leveraging embeddings to perform NLP tasks such as sentence
    parsing and sentiment analysis
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建用于利用嵌入执行NLP任务的模型，例如句子解析和情感分析。
- en: Distributed representations
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式表示
- en: 'Distributed representations attempt to capture the meaning of a word by considering
    its relations with other words in its context. The idea is captured in this quote
    from J. R. Firth (for more information refer to the article: *Document Embedding
    with Paragraph Vectors*, by Andrew M. Dai, Christopher Olah, and Quoc V. Le, arXiv:1507.07998,
    2015), a linguist who first proposed this idea:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示试图通过考虑一个词与其上下文中其他词的关系来捕捉词的含义。这个观点可以通过语言学家J. R. Firth的一句话来体现（更多信息请参考文章：*基于段落向量的文档嵌入*，作者：Andrew
    M. Dai、Christopher Olah和Quoc V. Le，arXiv:1507.07998，2015），他是最早提出这一观点的学者：
- en: You shall know a word by the company it keeps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过它所处的语境来理解一个词。
- en: 'Consider the following pair of sentences:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下一对句子：
- en: '*Paris is the capital of France.* *Berlin is the capital of Germany.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴黎是法国的首都。* *柏林是德国的首都。*'
- en: 'Even assuming you have no knowledge of world geography (or English for that
    matter), you would still conclude without too much effort that the word pairs
    (*Paris*, *Berlin*) and (*France*, *Germany*) were related in some way, and that
    corresponding words in each pair were related in the same way to each other, that
    is:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 即便你对世界地理（或者英语）毫无了解，你仍然能够不费力地推断出词对（*巴黎*，*柏林*）和（*法国*，*德国*）在某种程度上是相关的，并且每对词中的相应词之间是以同样的方式相互关联的，也就是说：
- en: '*Paris : France :: Berlin : Germany*'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴黎 : 法国 :: 柏林 : 德国*'
- en: 'Thus, the aim of distributed representations is to find a general transformation
    function φ to convert each word to its associated vector such that relations of
    the following form hold true:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分布式表示的目标是找到一个通用的变换函数φ，将每个词转换为其关联的向量，使得以下形式的关系成立：
- en: '![](img/paris-framce-eqn.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/paris-framce-eqn.png)'
- en: In other words, distributed representation aims to convert words to vectors
    where the similarity between the vectors correlate with the semantic similarity
    between the words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，分布式表示旨在将词转化为向量，使得向量之间的相似度与词语的语义相似度相关。
- en: The most well-known word embeddings are word2vec and GloVe, which we cover in
    more detail in subsequent sections.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的词嵌入方法是word2vec和GloVe，我们将在后续章节中更详细地介绍。
- en: word2vec
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec
- en: The word2vec group of models was created in 2013 by a team of researchers at
    Google led by Tomas Mikolov. The models are unsupervised, taking as input a large
    corpus of text and producing a vector space of words. The dimensionality of the
    word2vec embedding space is usually lower than the dimensionality of the one-hot
    embedding space, which is the size of the vocabulary. The embedding space is also
    more dense compared to the sparse embedding of the one-hot embedding space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型组是由谷歌的研究团队于2013年创建的，团队由Tomas Mikolov领导。该模型是无监督的，输入为大量文本语料库，输出为词向量空间。word2vec嵌入空间的维度通常低于one-hot嵌入空间的维度，后者的维度等于词汇表的大小。与稀疏的one-hot嵌入空间相比，嵌入空间的密度更大。
- en: 'The two architectures for word2vec are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的两种架构如下：
- en: '**Continuous Bag Of Words** (**CBOW**)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续词袋模型**（**CBOW**）'
- en: '**Skip-gram**'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-gram**'
- en: In the CBOW architecture, the model predicts the current word given a window
    of surrounding words. In addition, the order of the context words does not influence
    the prediction (that is, the bag of words assumption). In the case of skip-gram
    architecture, the model predicts the surrounding words given the center word.
    According to the authors, CBOW is faster but skip-gram does a better job at predicting
    infrequent words.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在CBOW架构中，模型根据周围词的窗口来预测当前词。此外，上下文词的顺序对预测没有影响（也就是说，词袋假设）。在skip-gram架构中，模型根据中心词来预测周围的词。根据作者的说法，CBOW更快，但skip-gram在预测不频繁出现的词时效果更好。
- en: An interesting thing to note is that even though word2vec creates embeddings
    that are used in deep learning NLP models, both flavors of word2vec that we will
    discuss, which also happens to be the most successful and acknowledged recent
    models, are shallow neural networks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的观察是，尽管word2vec生成的嵌入用于深度学习NLP模型，但我们将讨论的两种word2vec模型，恰好也是近年来最成功和公认的模型，实际上是浅层神经网络。
- en: The skip-gram word2vec model
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: skip-gram word2vec模型
- en: 'The skip-gram model is trained to predict the surrounding words given the current
    word. To understand how the skip-gram word2vec model works, consider the following
    example sentence:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型被训练来预测给定当前词的周围词。为了理解 skip-gram word2vec 模型如何工作，考虑以下示例句子：
- en: '*I love green eggs and ham.*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*I love green eggs and ham.*'
- en: 'Assuming a window size of three, this sentence can be broken down into the
    following sets of (context, word) pairs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设窗口大小为三，这个句子可以分解成以下 (上下文, 词) 对：
- en: '*([I, green], love)*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*([I, green], love)*'
- en: '*([love, eggs], green)*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*([love, eggs], green)*'
- en: '*([green, and], eggs)*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*([green, and], eggs)*'
- en: '*...*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*...*'
- en: 'Since the skip-gram model predicts a context word given the center word, we
    can convert the preceding dataset to one of (input, output) pairs. That is, given
    an input word, we expect the skip-gram model to predict the output word:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 skip-gram 模型在给定中心词的情况下预测上下文词，我们可以将前面的数据集转换为 (输入, 输出) 对。也就是说，给定一个输入词，我们期望
    skip-gram 模型预测出输出词：
- en: '*(love, I), (love, green), (green, love), (green, eggs), (eggs, green), (eggs,
    and), ...*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*(love, I), (love, green), (green, love), (green, eggs), (eggs, green), (eggs,
    and), ...*'
- en: 'We can also generate additional negative samples by pairing each input word
    with some random word in the vocabulary. For example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过将每个输入词与词汇表中的某个随机词配对来生成额外的负样本。例如：
- en: '*(love, Sam), (love, zebra), (green, thing), ...*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*(love, Sam), (love, zebra), (green, thing), ...*'
- en: 'Finally, we generate positive and negative examples for our classifier:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为分类器生成正负样本：
- en: '*((love, I), 1), ((love, green), 1), ..., ((love, Sam), 0), ((love, zebra),
    0), ...*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*((love, I), 1), ((love, green), 1), ..., ((love, Sam), 0), ((love, zebra),
    0), ...*'
- en: 'We can now train a classifier that takes in a word vector and a context vector
    and learns to predict one or zero depending on whether it sees a positive or negative
    sample. The deliverables from this trained network are the weights of the word
    embedding layer (the gray box in the following figure):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练一个分类器，该分类器输入一个词向量和一个上下文向量，并学习根据是否看到正样本或负样本来预测 1 或 0。这个训练好的网络的输出是词嵌入层的权重（下图中的灰色框）：
- en: '![](img/word2vec-skipgram.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/word2vec-skipgram.png)'
- en: 'The skip-gram model can be built in Keras as follows. Assume that the vocabulary
    size is set at `5000`, the output embedding size is `300`, and the window size
    is `1`. A window size of one means that the context for a word is the words immediately
    to the left and right. We first take care of the imports and set our variables
    to their initial values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型可以在 Keras 中构建如下。假设词汇表的大小设置为 `5000`，输出嵌入大小为 `300`，窗口大小为 `1`。窗口大小为
    1 意味着一个词的上下文是其左右两侧紧挨着的词。我们首先处理导入并将变量设置为其初始值：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then create a sequential model for the word. The input to this model is
    the word ID in the vocabulary. The embedding weights are initially set to small
    random values. During training, the model will update these weights using backpropagation.
    The next layer reshapes the input to the embedding size:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后为词创建一个顺序模型。这个模型的输入是词汇表中的词 ID。嵌入权重最初设置为小的随机值。在训练过程中，模型将使用反向传播更新这些权重。接下来的层将输入重塑为嵌入大小：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The other model that we need is a sequential model for the context words. For
    each of our skip-gram pairs, we have a single context word corresponding to the
    target word, so this model is identical to the word model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的另一个模型是一个用于上下文词的顺序模型。对于每一对 skip-gram，我们都有一个与目标词对应的上下文词，因此这个模型与词模型相同：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The outputs of the two models are each a vector of size (`embed_size`). These
    outputs are merged into one using a dot product and fed into a dense layer, which
    has a single output wrapped in a sigmoid activation layer. You have seen the sigmoid
    activation function in [Chapter 1](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml),
    *Neural Network Foundations*. As you will recall, it modulates the output so numbers
    higher than 0.5 tend rapidly to 1 and flatten out, and numbers lower than 0.5
    tend rapidly to 0 and also flatten out:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型的输出都是大小为 (`embed_size`) 的向量。这些输出通过点积合并成一个，并传入一个全连接层，该层有一个单一的输出，并通过 sigmoid
    激活层进行包装。你在[第 1 章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml) *神经网络基础* 中已经见过 sigmoid
    激活函数。正如你记得的，它调节输出，使得大于 0.5 的数值迅速接近 1 并趋于平稳，而小于 0.5 的数值迅速接近 0 同样也趋于平稳：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The loss function used is the `mean_squared_error`; the idea is to minimize
    the dot product for positive examples and maximize it for negative examples. If
    you recall, the dot product multiplies corresponding elements of two vectors and
    sums up the result—this causes similar vectors to have higher dot products than
    dissimilar vectors, since the former has more overlapping elements.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的损失函数是`mean_squared_error`；其思路是最小化正例的点积并最大化负例的点积。如果你还记得，点积是将两个向量对应的元素相乘并求和——这使得相似的向量相比于不相似的向量具有更高的点积，因为前者有更多重叠的元素。
- en: Keras provides a convenience function to extract skip-grams for a text that
    has been converted to a list of word indices. Here is an example of using this
    function to extract the first 10 of 56 skip-grams generated (both positive and
    negative).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个便捷函数，用于提取已转换为单词索引列表的文本中的跳字模型（skip-grams）。以下是使用该函数提取从56个跳字模型中前10个的示例（包括正例和负例）。
- en: 'We first declare the necessary imports and the text to be analyzed:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明必要的导入并分析文本：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to declare the `tokenizer` and run the text against it. This
    will produce a list of word tokens:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是声明`tokenizer`并运行文本进行处理。这将生成一个单词令牌的列表：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `tokenizer` creates a dictionary mapping each unique word to an integer
    ID and makes it available in the `word_index` attribute. We extract this and create
    a two-way lookup table:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenizer`创建一个字典，将每个唯一单词映射到一个整数ID，并通过`word_index`属性提供该映射。我们提取该映射并创建一个双向查找表：'
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we convert our input list of words to a list of IDs and pass it to
    the `skipgrams` function. We then print the first 10 of the 56 (pair, label) skip-gram
    tuples generated:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将输入的单词列表转换为ID列表，并将其传递给`skipgrams`函数。然后，我们打印生成的56个（对，标签）跳字元组中的前10个：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The results from the code is shown below. Note that your results may be different
    since the skip-gram method randomly samples the results from the pool of possibilities
    for the positive examples. Additionally, the process of negative sampling, used
    for generating the negative examples, consists of randomly pairing up arbitrary
    tokens from the text. As the size of the input text increases, this is more likely
    to pick up unrelated word pairs. In our example, since our text is very short,
    there is a chance that it can end up generating positive examples as well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的结果如下所示。请注意，由于跳字方法是从正例的可能性池中随机采样结果，因此你的结果可能会有所不同。此外，生成负例的负采样过程是随机配对文本中的任意令牌。随着输入文本大小的增加，这种方法更可能选择无关的单词对。在我们的示例中，由于文本非常短，因此也有可能生成正例。
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The code for this example can be found in `skipgram_example.py` in the source
    code download for the chapter.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的代码可以在章节的源代码下载中的`skipgram_example.py`文件中找到。
- en: The CBOW word2vec model
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CBOW word2vec模型
- en: 'Let us now look at the CBOW word2vec model. Recall that the CBOW model predicts
    the center word given the context words. Thus, in the first tuple in the following
    example, the CBOW model needs to predict the output word *love*, given the context
    words *I* and *green*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下CBOW word2vec模型。回顾一下，CBOW模型根据上下文单词预测中心单词。因此，在以下示例的第一个元组中，CBOW模型需要预测输出单词*love*，给定上下文单词*I*和*green*：
- en: '*([I, green], love) ([love, eggs], green) ([green, and], eggs) ...*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*([I, green], love) ([love, eggs], green) ([green, and], eggs) ...*'
- en: Like the skip-gram model, the CBOW model is also a classifier that takes the
    context words as input and predicts the target word. The architecture is somewhat
    more straightforward than the skip-gram model. The input to the model is the word
    IDs for the context words. These word IDs are fed into a common embedding layer
    that is initialized with small random weights. Each word ID is transformed into
    a vector of size (`embed_size`) by the embedding layer. Thus, each row of the
    input context is transformed into a matrix of size (`2*window_size`, `embed_size`)
    by this layer. This is then fed into a lambda layer, which computes an average
    of all the embeddings. This average is then fed to a dense layer, which creates
    a dense vector of size (`vocab_size`) for each row. The activation function on
    the dense layer is a softmax, which reports the maximum value on the output vector
    as a probability. The ID with the maximum probability corresponds to the target
    word.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 像skip-gram模型一样，CBOW模型也是一个分类器，它以上下文词作为输入，预测目标词。与skip-gram模型相比，CBOW模型的架构相对简单。模型的输入是上下文词的词ID。这些词ID被输入到一个通用的嵌入层，该层的权重被初始化为小的随机值。每个词ID都会通过嵌入层转换成大小为（`embed_size`）的向量。因此，输入上下文的每一行都通过该层转化为大小为（`2*window_size`,
    `embed_size`）的矩阵。接着，这个矩阵被输入到一个lambda层，lambda层计算所有嵌入的平均值。这个平均值再输入到一个全连接层，生成大小为（`vocab_size`）的密集向量。全连接层的激活函数是softmax，它会报告输出向量中的最大值作为概率。具有最大概率的ID对应于目标词。
- en: 'The deliverable for the CBOW model is the weights from the embedding layer
    shown in gray in the following figure:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: CBOW模型的交付物是来自嵌入层的权重，嵌入层在下图中显示为灰色：
- en: '![](img/word2vec-cbow.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/word2vec-cbow.png)'
- en: 'The corresponding Keras code for the model is shown as follows. Once again,
    assume a vocabulary size of `5000`, an embedding size of `300`, and a context
    window size of `1`. Our first step is to set up all our imports and these values:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的Keras代码如下所示。再假设词汇表大小为`5000`，嵌入大小为`300`，上下文窗口大小为`1`。我们的第一步是设置所有的导入以及这些值：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then construct a sequential model, to which we add an embedding layer whose
    weights are initialized with small random values. Note that the `input_length`
    of this embedding layer is equal to the number of context words. So each context
    word is fed into this layer and will update the weights jointly during backpropagation.
    The output of this layer is a matrix of context word embeddings, which are averaged
    into a single vector (per row of input) by the lambda layer. Finally, the dense
    layer will convert each row into a dense vector of size (`vocab_size`). The target
    word is the one whose ID has the maximum value in the dense output vector:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建一个顺序模型，并向其中添加一个嵌入层，该层的权重初始化为小的随机值。请注意，该嵌入层的`input_length`等于上下文词的数量。因此，每个上下文词都会被输入到这个层，并在反向传播过程中共同更新权重。该层的输出是上下文词的嵌入矩阵，这些嵌入通过lambda层平均成一个单一的向量（每一行输入）。最后，全连接层会将每一行转换为大小为（`vocab_size`）的密集向量。目标词是密集输出向量中ID值最大的词。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The loss function used here is `categorical_crossentropy`, which is a common
    choice for cases where there are two or more (in our case, `vocab_size`) categories.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的损失函数是`categorical_crossentropy`，它是一个常见的选择，适用于有两个或更多类别的情况（在我们的例子中是`vocab_size`）。
- en: The source code for the example can be found in the `keras_cbow.py` file in
    the source code download for the chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的源代码可以在章节的源代码下载中找到`keras_cbow.py`文件。
- en: Extracting word2vec embeddings from the model
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从模型中提取word2vec嵌入
- en: As noted previously, even though both word2vec models can be reduced to a classification
    problem, we are not really interested in the classification problem itself. Rather,
    we are interested in the side effect of this classification process, that is,
    the weight matrix that transforms a word from the vocabulary to its dense, low-dimensional
    distributed representation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，尽管两个word2vec模型都可以简化为一个分类问题，但我们并不真正关注分类问题本身。相反，我们更关心这个分类过程的副作用，也就是将词从词汇表转换为其密集、低维分布式表示的权重矩阵。
- en: 'There are many examples of how these distributed representations exhibit often
    surprising syntactic and semantic information. For example, as shown in the following
    figure from Tomas Mikolov''s presentation at NIPS 2013 (for more information refer
    to the article: *Learning Representations of Text using Neural Networks*, by T.
    Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Q. Le, and T. Strohmann,
    NIPS 2013), vectors connecting words that have similar meanings but opposite genders
    are approximately parallel in the reduced 2D space, and we can often get very
    intuitive results by doing arithmetic with the word vectors. The presentation
    provides many other examples.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多例子表明这些分布式表示展示了常常令人惊讶的句法和语义信息。例如，在Tomas Mikolov在2013年NIPS大会上的演示中（更多信息请参阅文章：*使用神经网络学习文本表示*，T.
    Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Q. Le 和 T. Strohmann，NIPS
    2013），连接具有相似意义但性别相反的单词的向量在降维后的二维空间中大致平行，我们通过对单词向量进行算术运算，通常可以得到非常直观的结果。该演示提供了许多其他的例子。
- en: '![](img/word2vec_regularities.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/word2vec_regularities.png)'
- en: Intuitively, the training process imparts enough information to the internal
    encoding to predict an output word that occurs in the context of an input word.
    So points representing words shift in this space to be nearer to words with which
    it co-occurs. This causes similar words to clump together. Words that co-occur
    with these similar words also clump together in a similar way. As a result, vectors
    connecting points representing semantically related points tend to exhibit these
    regularities in the distributed representation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，训练过程将足够的信息传递给内部编码，以预测在输入单词的上下文中出现的输出单词。因此，表示单词的点在这个空间中移动，靠近与之共同出现的单词。这导致相似的单词聚集在一起。与这些相似单词共同出现的单词也会以类似的方式聚集在一起。结果，连接表示语义相关点的向量往往会在分布式表示中展示这些规律性。
- en: 'Keras provides a way to extract weights from trained models. For the skip-gram
    example, the embedding weights can be extracted as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一种从训练模型中提取权重的方法。对于skip-gram示例，可以通过以下方式提取嵌入权重：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Similarly, the embedding weights for the CBOW example can be extracted using
    the following one-liner:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，CBOW示例的嵌入权重可以使用以下单行代码提取：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In both cases, the shape of the weights matrix is `vocab_size` and `embed_size`.
    In order to compute the distributed representation for a word in the vocabulary,
    you will need to construct a one-hot vector by setting the position of the word
    index to one in a zero vector of size (`vocab_size`) and multiply it with the
    matrix to get the embedding vector of size (`embed_size`).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，权重矩阵的形状都是`vocab_size`和`embed_size`。为了计算词汇表中单词的分布式表示，您需要通过将单词索引的位置设置为1，在一个大小为（`vocab_size`）的零向量中构造一个one-hot向量，并将其与矩阵相乘，得到大小为（`embed_size`）的嵌入向量。
- en: 'A visualization of word embeddings from work done by Christopher Olah (for
    more information refer to the article: *Document Embedding with Paragraph Vectors*,
    by Andrew M. Dai, Christopher Olah, and Quoc V. Le, arXiv:1507.07998, 2015) is
    shown as follows. This is a visualization of word embeddings reduced to two dimensions
    and visualized with T-SNE. The words forming entity types were chosen using WordNet
    synset clusters. As you can see, points corresponding to similar entity types
    tend to cluster together:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由Christopher Olah的工作中得出的词嵌入可视化（更多信息请参阅文章：*文档嵌入与段落向量*，Andrew M. Dai, Christopher
    Olah 和 Quoc V. Le，arXiv:1507.07998，2015）。这是通过T-SNE将词嵌入降到二维并可视化的结果。形成实体类型的单词是通过使用WordNet同义词集簇选择的。正如您所见，表示相似实体类型的点往往会聚集在一起：
- en: '![](img/word_embeddings_colah.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/word_embeddings_colah.png)'
- en: The source code for the example can be found in `keras_skipgram.py` in the source
    code download.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的源代码可以在源代码下载中的`keras_skipgram.py`找到。
- en: Using third-party implementations of word2vec
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用word2vec的第三方实现
- en: We have covered word2vec extensively over the past few sections. At this point,
    you understand how the skip-gram and CBOW models work and how to build your own
    implementation of these models using Keras. However, third-party implementations
    of word2vec are readily available, and unless your use case is very complex or
    different, it makes sense to just use one such implementation instead of rolling
    your own.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几节中，我们已经详细讨论了word2vec。此时，您已经了解了skip-gram和CBOW模型的工作原理，并且知道如何使用Keras构建这些模型的实现。然而，word2vec的第三方实现已经广泛可用，除非您的使用案例非常复杂或不同，否则直接使用现有的实现而不是自己动手实现更为合理。
- en: The gensim library provides an implementation of word2vec. Even though this
    is a book about Keras and not gensim, we include a discussion on this because
    Keras does not provide any support for word2vec, and integrating the gensim implementation
    into Keras code is very common practice.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: gensim库提供了word2vec的实现。虽然这是一本关于Keras的书，而不是gensim，但我们在这里讨论这个问题，因为Keras不支持word2vec，并且将gensim的实现集成到Keras代码中是非常常见的做法。
- en: Installation of gensim is fairly simple and described in detail on the gensim
    installation page ([https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: gensim的安装相当简单，并在gensim安装页面上详细描述（[https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html)）。
- en: 'The following code shows how to build a word2vec model using gensim and train
    it with the text from the text8 corpus, available for download at: [http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip).
    The text8 corpus is a file containing about 17 million words derived from Wikipedia
    text. Wikipedia text was cleaned to remove markup, punctuation, and non-ASCII
    text, and the first 100 million characters of this cleaned text became the text8
    corpus. This corpus is commonly used as an example for word2vec because it is
    quick to train and produces good results. First we set up the imports as usual:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用gensim构建word2vec模型，并使用来自text8语料库的文本进行训练，text8语料库可以从[http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip)下载。text8语料库是一个包含约1700万个单词的文件，来源于维基百科的文本。维基百科的文本被清洗过，去除了标记、标点和非ASCII文本，清洗后的前1亿个字符组成了text8语料库。这个语料库常被用作word2vec的示例，因为它训练速度快且能产生良好的结果。首先，我们像往常一样设置导入：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We then read in the words from the text8 corpus, and split up the words into
    sentences of 50 words each. The gensim library provides a built-in text8 handler
    that does something similar. Since we want to illustrate how to generate a model
    with any (preferably large) corpus that may or may not fit into memory, we will
    show you how to generate these sentences using a Python generator.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们读取text8语料库中的单词，并将单词分割成每句50个单词。gensim库提供了一个内置的text8处理器，它做的事情类似。由于我们想展示如何使用任何（最好是大型）语料库生成模型，而这些语料库可能无法完全加载到内存中，因此我们将展示如何使用Python生成器生成这些句子。
- en: 'The `Text8Sentences` class will generate sentences of `maxlen` words each from
    the text8 file. In this case, we do ingest the entire file into memory, but when
    traversing through directories of files, generators allows us to load parts of
    the data into memory at a time, process them, and yield them to the caller:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`Text8Sentences`类将从text8文件中生成每个最大长度为`maxlen`的句子。在这种情况下，我们确实将整个文件加载到内存中，但在遍历文件夹中的文件时，生成器允许我们一次将数据的部分加载到内存中，处理它们，然后将它们传递给调用者：'
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then set up the caller code. The gensim word2vec uses Python logging to
    report on progress, so we first enable it. The next line declares an instance
    of the `Text8Sentences` class, and the line after that trains the model with the
    sentences from the dataset. We have chosen the size of the embedding vectors to
    be `300`, and we only consider words that appear a minimum of 30 times in the
    corpus. The default window size is `5`, so we will consider the words *w[i-5]*,
    *w[i-4]*, *w[i-3]*, *w[i-2]*, *w[i-1]*, *w[i+1]*, *w[i+2]*, *w[i+3]*, *w[i+4]*,
    and *w[i+5]* as the context for word *w[i]*. By default, the word2vec model created
    is CBOW, but you can change that by setting `sg=1` in the parameters:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们设置调用代码。gensim的word2vec使用Python的logging来报告进度，所以我们首先启用它。下一行声明了一个`Text8Sentences`类的实例，接下来的一行则用数据集中的句子训练模型。我们选择将嵌入向量的大小设置为`300`，并且只考虑在语料库中至少出现30次的单词。默认的窗口大小是`5`，因此我们将把单词
    *w[i-5]*、*w[i-4]*、*w[i-3]*、*w[i-2]*、*w[i-1]*、*w[i+1]*、*w[i+2]*、*w[i+3]*、*w[i+4]*
    和 *w[i+5]* 作为单词 *w[i]* 的上下文。默认情况下，创建的word2vec模型是CBOW，但你可以通过在参数中设置`sg=1`来更改这一点：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The word2vec implementation will make two passes over the data, first to generate
    a vocabulary and then to build the actual model. You can see its progress on the
    console as it runs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec的实现将对数据进行两次遍历，第一次是生成词汇表，第二次是构建实际的模型。在运行过程中，你可以在控制台看到它的进度：
- en: '![](img/ss-5-1.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-5-1.png)'
- en: 'Once the model is created, we should normalize the resulting vectors. According
    to the documentation, this saves lots of memory. Once the model is trained, we
    can optionally save it to disk:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型创建后，我们应当对结果向量进行归一化。根据文档，这样可以节省大量内存。一旦模型训练完成，我们可以选择将其保存到磁盘：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The saved model can be brought back into memory using the following call:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的模型可以通过以下调用重新加载到内存：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now query the model to find all the words it knows about:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以查询模型，找出它所知道的所有词语：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can find the actual vector embedding for a given word:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以找到给定词语的实际向量嵌入：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can also find words that are most similar to a certain word:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以找到与某个特定词语最相似的词语：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can provide hints for finding word similarity. For example, the following
    command returns the top 10 words that are like `woman` and `king` but unlike `man`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提供一些提示来帮助寻找词语的相似性。例如，以下命令返回与`woman`和`king`相似但与`man`不同的前10个词：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also find similarities between individual words. To give a feel of how
    the positions of the words in the embedding space correlates with their semantic
    meanings, let us look at the following word pairs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以找到单个词之间的相似度。为了让大家了解词语在嵌入空间中的位置如何与其语义含义相关联，让我们看一下以下的词对：
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, `girl` and `woman` are more similar than `girl` and `man`, and
    `car` and `bus` are more similar than `girl` and `car`. This agrees very nicely
    with our human intuition about these words.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，`girl`和`woman`的相似度高于`girl`和`man`，`car`和`bus`的相似度高于`girl`和`car`。这与我们对这些词的直观理解非常一致。
- en: The source code for the example can be found in `word2vec_gensim.py` in the
    source code download.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 示例的源代码可以在源代码下载中的`word2vec_gensim.py`中找到。
- en: Exploring GloVe
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 GloVe
- en: 'The global vectors for word representation, or GloVe, embeddings was created
    by Jeffrey Pennington, Richard Socher, and Christopher Manning (for more information
    refer to the article: *GloVe: Global Vectors for Word Representation*, by J. Pennington,
    R. Socher, and C. Manning, Proceedings of the 2014 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), Pp. 1532–1543, 2013). The authors describe
    GloVe as an unsupervised learning algorithm for obtaining vector representations
    for words. Training is performed on aggregated global word-word co-occurrence
    statistics from a corpus, and the resulting representations showcase interesting
    linear substructures of the word vector space.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '词表示的全局向量，或称为GloVe嵌入，由Jeffrey Pennington、Richard Socher和Christopher Manning创建（更多信息请参考文章：*GloVe:
    Global Vectors for Word Representation*，作者：J. Pennington、R. Socher和C. Manning，发表于2014年自然语言处理实证方法会议（EMNLP）论文集，第1532-1543页，2013年）。作者将GloVe描述为一种无监督学习算法，用于获取词语的向量表示。训练基于从语料库中聚合的全局词语共现统计数据，结果表示展示了词向量空间中的有趣线性子结构。'
- en: 'GloVe differs from word2vec in that word2vec is a predictive model while GloVe
    is a count-based model. The first step is to construct a large matrix of (word,
    context) pairs that co-occur in the training corpus. Each element of this matrix
    represents how often a word represented by the row co-occurs in the context (usually
    a sequence of words) represented by the column, as shown in the following figure:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe与word2vec的不同之处在于，word2vec是一个预测模型，而GloVe是一个基于计数的模型。第一步是构建一个大的（词语，语境）对矩阵，这些词语在训练语料库中共同出现。该矩阵中的每个元素表示行所表示的词语在列所表示的语境（通常是一个词语序列）中共同出现的频率，如下图所示：
- en: '![](img/glove-matfact.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/glove-matfact.png)'
- en: 'The GloVe process converts the co-occurrence matrix into a pair of (word, feature)
    and (feature, context) matrices. This process is known as **matrix factorization**
    and is done using **stochastic gradient descent** (**SGD**), an iterative numerical
    method. Rewriting in equation form:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe过程将共现矩阵转换为（词语，特征）和（特征，语境）矩阵。这个过程称为**矩阵分解**，并使用**随机梯度下降**（**SGD**）这一迭代数值方法完成。用公式表示如下：
- en: '![](img/matfact-eqn.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/matfact-eqn.png)'
- en: Here, *R* is the original co-occurrence matrix. We first populate *P* and *Q*
    with random values and attempt to reconstruct a matrix *R'* by multiplying them.
    The difference between the reconstructed matrix *R'* and the original matrix *R*
    tells us how much we need to change the values of *P* and *Q* to move *R'* closer
    to *R*, to minimize the reconstruction error. This is repeated multiple times
    until the SGD converges and the reconstruction error is below a specified threshold.
    At that point, the (word, feature) matrix is the GloVe embedding. To speed up
    the process, SGD is often used in parallel mode, as outlined in the *HOGWILD!*
    paper.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*R*是原始的共现矩阵。我们首先将*P*和*Q*用随机值填充，并尝试通过相乘来重建矩阵*R'*。重建矩阵*R'*和原始矩阵*R*之间的差异告诉我们，需要调整*P*和*Q*的值多少，以便将*R'*拉近*R*，从而最小化重建误差。这个过程会重复多次，直到SGD收敛且重建误差低于指定阈值。此时，（词语，特征）矩阵即为GloVe嵌入。为了加速这一过程，SGD通常会以并行模式进行，如*HOGWILD!*论文中所述。
- en: One thing to note is that predictive neural network based models such as word2vec
    and count based models such as GloVe are very similar in intent. Both of them
    build a vector space where the position of a word is influenced by its neighboring
    words. Neural network models start with individual examples of word co-occurrences
    and count based models start with aggregate co-occurrence statistics between all
    words in the corpus. Several recent papers have demonstrated the correlation between
    these two types of model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一点是，基于神经网络的预测模型（如 word2vec）和基于计数的模型（如 GloVe）在目的上非常相似。它们都构建一个向量空间，其中单词的位置受到邻近单词的影响。神经网络模型从单个词共现实例开始，而基于计数的模型从语料库中所有单词之间的共现统计数据开始。最近的几篇论文展示了这两种模型之间的相关性。
- en: We will not cover generation of GloVe vectors in more detail in this book. Even
    though GloVe generally shows higher accuracy than word2vec and is faster to train
    if you use parallelization, Python tooling is not as mature as for word2vec. The
    only tool available to do this as of the time of writing is the GloVe-Python project
    ([https://github.com/maciejkula/glove-python](https://github.com/maciejkula/glove-python)),
    which provides a toy implementation for GloVe on Python.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不会更详细地讲解 GloVe 向量的生成。尽管 GloVe 通常比 word2vec 显示出更高的准确性，并且如果使用并行化训练，速度更快，但 Python
    工具在成熟度上不如 word2vec。截至本书撰写时，唯一可用的工具是 GloVe-Python 项目（[https://github.com/maciejkula/glove-python](https://github.com/maciejkula/glove-python)），它提供了一个在
    Python 上实现 GloVe 的玩具实现。
- en: Using pre-trained embeddings
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的词向量
- en: 'In general, you will train your own word2vec or GloVe model from scratch only
    if you have a very large amount of very specialized text. By far the most common
    use case for Embeddings is to use pre-trained embeddings in some way in your network.
    The three main ways in which you would use embeddings in your network are as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，只有在你拥有大量非常专业的文本时，才会从头开始训练自己的 word2vec 或 GloVe 模型。迄今为止，词向量最常见的使用方式是以某种方式在你的网络中使用预训练的词向量。你在网络中使用词向量的三种主要方式如下：
- en: Learn embeddings from scratch
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始学习词向量
- en: Fine-tune learned embeddings from pre-trained GloVe/word2vec models
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调从预训练的 GloVe/word2vec 模型学习到的词向量
- en: Look up embeddings from pre-trained GloVe/word2vec models
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找预训练的 GloVe/word2vec 模型中的词向量
- en: In the first option, the embedding weights are initialized to small random values
    and trained using backpropagation. You saw this in the examples for skip-gram
    and CBOW models in Keras. This is the default mode when you use a Keras Embedding
    layer in your network.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种选择中，词向量权重初始化为小的随机值，并通过反向传播进行训练。你在 Keras 的 skip-gram 和 CBOW 模型示例中看到过这个。这是当你在网络中使用
    Keras 的嵌入层时的默认模式。
- en: In the second option, you build a weight matrix from a pre-trained model and
    initialize the weights of your embedding layer with this weight matrix. The network
    will update these weights using backpropagation, but the model will converge faster
    because of good starting weights.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种选择中，你从预训练模型构建一个权重矩阵，并使用这个权重矩阵初始化嵌入层的权重。网络将通过反向传播更新这些权重，但由于良好的初始权重，模型会更快地收敛。
- en: The third option is to look up word embeddings from a pre-trained model, and
    transform your input to embedded vectors. You can then train any machine learning
    model (that is, not necessarily even a deep learning network) on the transformed
    data. If the pre-trained model is trained on a similar domain as the target domain,
    this usually works very well and is the least expensive option.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选择是查找预训练模型中的词向量，并将输入转换为嵌入向量。然后，你可以在转换后的数据上训练任何机器学习模型（即，不一定是深度学习网络）。如果预训练模型是在与目标领域相似的领域上训练的，通常效果很好，而且是最不昂贵的选择。
- en: For general use with English language text, you can use Google's word2vec model
    trained over 10 billion words from the Google news dataset. The vocabulary size
    is about 3 million words and the dimensionality of the embedding is 300\. The
    Google news model (about 1.5 GB) can be downloaded from here: [https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的英语文本使用，可以使用 Google 的 word2vec 模型，该模型在 100 亿个单词的 Google 新闻数据集上进行训练。词汇表大小约为
    300 万个单词，嵌入的维度为 300。Google 新闻模型（约 1.5 GB）可以从这里下载：[https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)。
- en: Similarly, a pre-trained model trained on 6 billion tokens from English Wikipedia
    and the gigaword corpus can be downloaded from the GloVe site. The vocabulary
    size is about 400,000 words and the download provides vectors with dimensions
    50, 100, 200, and 300\. The model size is about 822 MB. Here is the direct download
    URL ([http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip))
    for this model. Larger models based on the Common Crawl and Twitter are also available
    from the same location.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，可以从GloVe网站下载一个预先训练的模型，该模型在来自英语维基百科和gigaword语料库的60亿个标记上进行了训练。词汇量约为400,000个单词，下载提供了维度为50、100、200和300的向量。模型大小约为822
    MB。这是该模型的直接下载URL（[http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)）。基于Common
    Crawl和Twitter的更大型号模型也可以从同一位置获取。
- en: In the following sections, we will look at how to use these pre-trained models
    in the three ways listed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将看看如何以列出的三种方式使用这些预训练模型。
- en: Learn embeddings from scratch
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始学习嵌入
- en: In this example, we will train a one-dimensional **convolutional neural network**
    (**CNN**) to classify sentences as either positive or negative. You have already
    seen how to classify images using two-dimensional CNNs in [Chapter 3](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml),
    *Deep Learning with ConvNets*. Recall that CNNs exploit spatial structure in images
    by enforcing local connectivity between neurons of adjacent layers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将训练一个一维**卷积神经网络**（**CNN**），将句子分类为正面或负面。您已经看到如何使用二维CNN分类图像在[第3章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)，*使用ConvNets进行深度学习*。回想一下，CNN通过强制相邻层神经元之间的局部连接来利用图像中的空间结构。
- en: Words in sentences exhibit linear structure in the same way as images exhibit
    spatial structure. Traditional (non-deep learning) NLP approaches to language
    modeling involve creating word *n*-grams ([https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)[)](https://en.wikipedia.org/wiki/N-gram)
    to exploit this linear structure inherent among words. One-dimensional CNNs do
    something similar, learning convolution filters that operate on sentences a few
    words at a time, and max pooling the results to create a vector that represents
    the most important ideas in the sentence.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 句子中的单词表现出与图像展现空间结构相同的线性结构。传统（非深度学习）自然语言处理方法涉及创建单词*n*-grams（[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)）。一维CNN做类似的事情，学习卷积滤波器，这些滤波器一次处理几个单词，并对结果进行最大池化，以创建代表句子中最重要思想的向量。
- en: There is another class of neural network, called **recurrent neural network**
    (**RNN**), which is specially designed to handle sequence data, including text,
    which is a sequence of words. The processing in RNNs is different from that in
    a CNN. We will learn about RNNs in a future chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一类神经网络，称为**循环神经网络**（**RNN**），专门设计用于处理序列数据，包括文本，即一系列单词。RNN中的处理方式与CNN中的处理方式不同。我们将在未来的章节中学习有关RNN的内容。
- en: In our example network, the input text is converted to a sequence of word indices.
    Note that we have used the **natural language toolkit** (**NLTK**) to parse the
    text into sentences and words. We could also have used regular expressions to
    do this, but the statistical models supplied by NLTK are more powerful at parsing
    than regular expressions. If you are working with word embeddings, it is very
    likely that you are also working with NLP, in which case you would have NLTK installed
    already.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例网络中，输入文本被转换为一系列单词索引。请注意，我们使用**自然语言工具包**（**NLTK**）将文本解析为句子和单词。我们也可以使用正则表达式来做这件事，但是NLTK提供的统计模型在解析上比正则表达式更强大。如果您正在处理词嵌入，很可能已经安装了NLTK。
- en: This link ([http://www.nltk.org/install.html](http://www.nltk.org/install.html))
    has information to help you install NLTK on your machine. You will also need to
    install NLTK data, which is some trained corpora that comes standard with NLTK.
    Installation instructions for NLTK data are available here: [http://www.nltk.org/data.html](http://www.nltk.org/data.html).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此链接（[http://www.nltk.org/install.html](http://www.nltk.org/install.html)）包含帮助您在计算机上安装NLTK的信息。您还需要安装NLTK数据，这是NLTK标准提供的一些训练语料库。NLTK数据的安装说明在这里：[http://www.nltk.org/data.html](http://www.nltk.org/data.html)。
- en: 'The sequence of word indices is fed into an array of embedding layers of a
    set size (in our case, the number of words in the longest sentence). The embedding
    layer is initialized by default to random values. The output of the embedding
    layer is connected to a 1D convolutional layer that convolves (in our example)
    word trigrams in 256 different ways (essentially, it applies different learned
    linear combinations of weights on the word embeddings). These features are then
    pooled into a single pooled word by a global max pooling layer. This vector (256)
    is then input to a dense layer, which outputs a vector (2). A softmax activation
    will return a pair of probabilities, one corresponding to positive sentiment and
    another corresponding to negative sentiment. The network is shown in the following
    figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇索引的序列被输入到一组嵌入层的数组中，这些嵌入层的大小是固定的（在我们的例子中是最长句子的单词数）。嵌入层默认通过随机值进行初始化。嵌入层的输出被连接到一个
    1D 卷积层，该卷积层以 256 种不同的方式对词三元组进行卷积（本质上，它对词嵌入应用不同的学习到的线性权重组合）。然后，这些特征通过一个全局最大池化层被池化成一个单一的池化词向量。这个向量（256）被输入到一个全连接层，输出一个向量（2）。Softmax
    激活函数会返回一对概率，一个对应正向情感，另一个对应负向情感。网络结构如下图所示：
- en: '![](img/umich_conv1d.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/umich_conv1d.png)'
- en: 'Let us look at how to code this up using Keras. First we declare our imports.
    Right after the constants, you will notice that I set the `random.seed` value
    to `42`. This is because we want consistent results between runs. Since the initializations
    of the weight matrices are random, differences in initialization can lead to differences
    in output, so this is a way to control that:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用 Keras 编写代码。首先我们声明导入的库。在常量之后，您会注意到我将 `random.seed` 的值设置为 `42`。这是因为我们希望运行结果保持一致。由于权重矩阵的初始化是随机的，初始化的差异可能会导致输出的差异，因此我们通过设置种子来控制这一点：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We declare our constants. For all subsequent examples in this chapter, we will
    classify sentences from the UMICH SI650 sentiment classification competition on
    Kaggle. The dataset has around 7,000 sentences, and is labeled *1* for positive
    and *0* for negative. The `INPUT_FILE` defines the path to this file of sentences
    and labels. The format of the file is a sentiment label (*0* or *1*) followed
    by a tab, followed by a sentence.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明常量。在本章的所有后续示例中，我们将对来自 Kaggle 上 UMICH SI650 情感分类竞赛的句子进行分类。数据集大约包含 7000 个句子，并标注为*1*表示正向情感，*0*表示负向情感。`INPUT_FILE`
    定义了该文件的路径，该文件包含句子和标签。文件的格式为情感标签（*0* 或 *1*）后跟一个制表符，然后是一个句子。
- en: 'The `VOCAB_SIZE` setting indicates that we will consider only the top 5,000
    tokens in the text. The `EMBED_SIZE` setting is the size of the embedding that
    will be generated by the embedding layer in the network. `NUM_FILTERS` is the
    number of convolution filters we will train for our convolution layer, and `NUM_WORDS`
    is the size of each filter, that is, how many words we will convolve at a time.
    The `BATCH_SIZE` and `NUM_EPOCHS` is the number of records to feed the network
    each time and how many times we will run through the entire dataset during training:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`VOCAB_SIZE` 设置表示我们将只考虑文本中前 5000 个标记。`EMBED_SIZE` 设置是由嵌入层生成的嵌入大小。`NUM_FILTERS`
    是我们为卷积层训练的卷积滤波器数量，`NUM_WORDS` 是每个滤波器的大小，也就是一次卷积时处理的单词数。`BATCH_SIZE` 和 `NUM_EPOCHS`
    分别是每次馈送给网络的记录数量和在训练期间遍历整个数据集的次数：'
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the next block, we first read our input sentences and construct our vocabulary
    out of the most frequent words in the corpus. We then use this vocabulary to convert
    our input sentences into a list of word indices:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码块中，我们首先读取输入的句子，并通过从语料库中最频繁的单词构建词汇表。然后，我们使用该词汇表将输入句子转换为一个词索引列表：
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We pad each of our sentences to predetermined length `maxlen` (in this case
    the number of words in the longest sentence in the training set). We also convert
    our labels to categorical format using a Keras utility function. The last two
    steps are a standard workflow for handling text input that we will see again and
    again:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个句子填充到预定的长度`maxlen`（在这种情况下是训练集中最长句子的单词数）。我们还使用 Keras 工具函数将标签转换为类别格式。最后两步是处理文本输入的标准工作流程，我们将在后续的步骤中一再使用：
- en: '[PRE26]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we split up our data into a *70/30* training and test set. The data
    is now in a form ready to be fed into the network:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据分割为*70/30*的训练集和测试集。现在，数据已经准备好输入到网络中：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We define the network that we described earlier in this section:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了前面在本节中描述的网络：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then compile the model. Since our target is binary (positive or negative)
    we choose `categorical_crossentropy` as our loss function. For the optimizer,
    we choose `adam`. We then train the model using our training set, using a batch
    size of 64 and training for 20 epochs:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们编译模型。由于我们的目标是二分类（正类或负类），我们选择`categorical_crossentropy`作为损失函数。优化器我们选择`adam`。接着，我们使用训练集对模型进行训练，批量大小为64，训练20个周期：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output from the code looks as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出结果如下：
- en: '![](img/ss-5-4.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-5-4.png)'
- en: As you can see, the network gives us 98.6% accuracy on the test set.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，网络在测试集上的准确率达到了98.6%。
- en: The source code for this example can be found in `learn_embedding_from_scratch.py`
    in the source code download for the chapter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码可以在章节的源代码下载中找到，文件名为`learn_embedding_from_scratch.py`。
- en: Fine-tuning learned embeddings from word2vec
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从word2vec微调学习到的嵌入
- en: In this example, we will use the same network as the one we used to learn our
    embeddings from scratch. In terms of code, the only major difference is an extra
    block of code to load the word2vec model and build up the weight matrix for the
    embedding layer.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用与之前从头学习嵌入时相同的网络。在代码方面，唯一的主要区别是增加了一段代码来加载word2vec模型，并构建嵌入层的权重矩阵。
- en: 'As always, we start with the imports and set up a random seed for repeatability.
    In addition to the imports we have seen previously, there is an additional one
    to import the word2vec model from gensim:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们从导入模块开始，并设置一个随机种子以保证可重复性。除了之前看到的导入外，还有一个额外的导入，用于从gensim导入word2vec模型：
- en: '[PRE30]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next up is setting up the constants. The only difference here is that we reduced
    the `NUM_EPOCHS` setting from `20` to `10`. Recall that initializing the matrix
    with values from a pre-trained model tends to set them to good values that converge
    faster:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是设置常量。这里唯一的不同是，我们将`NUM_EPOCHS`的设置从`20`减少到了`10`。回想一下，用预训练模型的值初始化矩阵通常能使权重值较好，并加快收敛速度：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The next block extracts the words from the dataset and creates a vocabulary
    of the most frequent terms, then parses the dataset again to create a list of
    padded word lists. It also converts the labels to categorical format. Finally,
    it splits the data into a training and a test set. This block is identical to
    the previous example and has been explained in depth there:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块从数据集中提取单词，并创建一个包含最常见词汇的词汇表，然后再次解析数据集，创建一个填充的单词列表。它还将标签转换为类别格式。最后，它将数据拆分为训练集和测试集。这个代码块与前面的示例相同，已在前面进行了详细解释：
- en: '[PRE32]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next block loads up the word2vec model from a pre-trained model. This model
    is trained with about 10 billion words of Google News articles and has a vocabulary
    size of 3 million. We load it and look up embedding vectors from it for words
    in our vocabulary, and write out the embedding vector into our weight matrix `embedding_weights`.
    Rows of this weight matrix correspond to words in the vocabulary, and columns
    of each row constitute the embedding vector for the word.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块加载了一个预训练的word2vec模型。这个模型是用大约100亿个Google新闻文章中的单词训练的，词汇表大小为300万。我们加载它并从中查找词汇表中单词的嵌入向量，然后将嵌入向量写入权重矩阵`embedding_weights`。该权重矩阵的行对应词汇表中的单词，每行的列构成该单词的嵌入向量。
- en: The dimensions of the `embedding_weights` matrix is `vocab_sz` and `EMBED_SIZE`.
    The `vocab_sz` is one more than the maximum number of unique terms in the vocabulary,
    the additional pseudo-token `_UNK_` representing words that are not seen in the
    vocabulary.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`embedding_weights`矩阵的维度为`vocab_sz`和`EMBED_SIZE`。`vocab_sz`比词汇表中唯一词汇的最大数量多1，额外的伪标记`_UNK_`代表词汇表中没有出现的词汇。'
- en: 'Note that it is possible that some words in our vocabulary may not be there
    in the Google News word2vec model, so when we encounter such words, the embedding
    vectors for them remain at the default value of all zeros:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的词汇表中可能会有些词汇在Google News的word2vec模型中不存在，因此当遇到这些词汇时，它们的嵌入向量将保持为默认值，全为零：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We define our network. The difference in this block from our previous example
    is that we initialize the weights of the embedding layer with the `embedding_weights`
    matrix we built in the previous block:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了网络。这个代码块与前面的示例的不同之处在于，我们用在前一个代码块中构建的`embedding_weights`矩阵初始化了嵌入层的权重：
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We then compile our model with the categorical cross-entropy loss function
    and the Adam optimizer, and train the network with batch size 64 and for 10 epochs,
    then evaluate the trained model:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用类别交叉熵损失函数和Adam优化器编译模型，使用批量大小64进行10个epoch的训练，并评估训练后的模型：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Output from running the code is shown as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码的输出如下所示：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The model gives us an accuracy of 99.3% on the test set after 10 epochs of training.
    This is an improvement over the previous example, where we got an accuracy of
    98.6% accuracy after 20 epochs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 经过10个epoch的训练后，该模型在测试集上的准确率达到了99.3%。这比之前的示例有所提升，后者在20个epoch后准确率为98.6%。
- en: The source code for this example can be found in  `finetune_word2vec_embeddings.py`
    in the source code download for the chapter.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码可以在本章的源代码下载中的`finetune_word2vec_embeddings.py`文件中找到。
- en: Fine-tune learned embeddings from GloVe
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调来自GloVe的学习嵌入
- en: Fine tuning using pre-trained GloVe embeddings is very similar to fine tuning
    using pre-trained word2vec embeddings. In fact, all of the code, except for the
    block that builds the weight matrix for the embedding layer, is identical. Since
    we have already seen this code twice, I will just focus on the block of code that
    builds the weight matrix from the GloVe embeddings.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的GloVe嵌入进行微调与使用预训练的word2vec嵌入进行微调非常相似。事实上，除了构建嵌入层权重矩阵的代码块外，其余所有代码都是相同的。由于我们已经看过这段代码两次，所以我将只关注构建GloVe嵌入权重矩阵的代码块。
- en: GloVe embeddings come in various flavors. We use the model pre-trained on 6
    billion tokens from the English Wikipedia and the gigaword corpus. The vocabulary
    size for the model is about 400,000, and the download provides vectors of dimensions
    50, 100, 200, and 300\. We will use embeddings from the 300 dimensional model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe嵌入有多种版本。我们使用的是在来自英文维基百科和Gigaword语料库的60亿个标记上预训练的模型。该模型的词汇表大小约为40万，下载包提供了维度为50、100、200和300的向量。我们将使用300维模型中的嵌入。
- en: The only thing we need to change in the code for the previous example is to
    replace the block that instantiated a word2vec model and loaded the embedding
    matrix using the following block of code. If we use a model with vector size other
    than 300, then we also need to update `EMBED_SIZE`.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一个示例中唯一需要更改的代码是替换实例化word2vec模型并加载嵌入矩阵的代码块，替换为以下代码块。如果我们使用的模型的向量大小不是300，那么我们还需要更新`EMBED_SIZE`。
- en: 'The vectors are provided in space-delimited text format, so the first step
    is to read the code into a dictionary, `word2emb`. This is analogous to the line
    instantiating the Word2Vec model in our previous example:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 向量以空格分隔的文本格式提供，因此第一步是将代码读取到字典`word2emb`中。这类似于我们之前示例中实例化Word2Vec模型的那一行代码：
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We then instantiate an embedding weight matrix of size (`vocab_sz` and `EMBED_SIZE`)
    and populate the vectors from the `word2emb` dictionary. Vectors for words that
    are found in the vocabulary but not in the GloVe model remain set to all zeros:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实例化一个大小为（`vocab_sz`和`EMBED_SIZE`）的嵌入权重矩阵，并从`word2emb`字典中填充向量。对于那些在词汇表中存在但不在GloVe模型中的单词，向量将保持为全零：
- en: '[PRE38]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The full code for this program can be found in `finetune_glove_embeddings.py`
    in the book''s code repository on GitHub. The output of the run is shown as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本程序的完整代码可以在GitHub上的本书代码库中的`finetune_glove_embeddings.py`找到。运行结果如下所示：
- en: '![](img/ss-5-2.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-5-2.png)'
- en: This gives us 99.1% accuracy in 10 epochs, which is almost as good as the results
    we got from fine-tuning the network using word2vec `embedding_weights`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们在10个epoch后达到了99.1%的准确率，几乎与我们通过微调网络使用word2vec `embedding_weights`得到的结果一样好。
- en: The source code for this example can be found in `finetune_glove_embeddings.py`
    in the source code download for this chapter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码可以在本章的源代码下载中的`finetune_glove_embeddings.py`文件中找到。
- en: Look up embeddings
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找嵌入
- en: 'Our final strategy is to look up embeddings from pre-trained networks. The
    simplest way to do this with the current examples is to just set the `trainable`
    parameter of the embedding layer to `False`. This ensures that backpropagation
    will not update the weights on the embedding layer:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的策略是从预训练的网络中查找嵌入。使用当前示例的最简单方法是将嵌入层的`trainable`参数设置为`False`。这样可以确保反向传播不会更新嵌入层的权重：
- en: '[PRE39]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Setting this value with the word2vec and GloVe examples gave us accuracies of
    98.7% and 98.9% respectively after 10 epochs of training.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用word2vec和GloVe示例设置此值后，经过10个epoch的训练，我们分别得到了98.7%和98.9%的准确率。
- en: However, in general, this is not how you would use pre-trained embeddings in
    your code. Typically, it involves preprocessing your dataset to create word vectors
    by looking up words in one of the pre-trained models, and then using this data
    to train some other model. The second model would not contain an Embedding layer,
    and may not even be a deep learning network.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常情况下，这并不是你在代码中使用预训练嵌入的方式。通常，这涉及到对数据集进行预处理，通过查找预训练模型中的单词来创建单词向量，然后使用这些数据训练其他模型。第二个模型通常不包含嵌入层，甚至可能不是深度学习网络。
- en: The following example describes a dense network that takes as its input a vector
    of size `100`, representing a sentence, and outputs a `1` or `0` for positive
    or negative sentiment. Our dataset is still the one from the UMICH S1650 sentiment
    classification competition with around 7,000 sentences.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例描述了一个密集网络，它将大小为`100`的向量作为输入，表示一个句子，并输出`1`或`0`，表示积极或消极的情感。我们的数据集仍然是来自UMICH
    S1650情感分类比赛的那个，约有7,000个句子。
- en: As previously, large parts of the code are repeated, so we only explain the
    parts that are new or otherwise need explanation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，代码的许多部分是重复的，因此我们只解释那些新的或需要说明的部分。
- en: 'We begin with the imports, set the random seed for repeatability, and set some
    constant values. In order to create the 100-dimensional vectors for each sentence,
    we add up the GloVe 100-dimensional vectors for the words in the sentence, so
    we choose the `glove.6B.100d.txt` file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从导入开始，设置随机种子以确保结果可重复，并设置一些常量值。为了创建每个句子的100维向量，我们将句子中单词的GloVe 100维向量相加，因此我们选择了`glove.6B.100d.txt`文件：
- en: '[PRE40]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next block reads the sentences and creates a word frequency table. From
    this, the most common 5,000 tokens are selected and lookup tables (from word to
    word index and back) are created. In addition, we create a pseudo-token `_UNK_`
    for tokens that do not exist in the vocabulary. Using these lookup tables, we
    convert each sentence to a sequence of word IDs, padding these sequences so that
    all sequences are of the same length (the maximum number of words in a sentence
    in the training set). We also convert the labels to categorical format:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块读取句子并创建一个单词频率表。通过这个表，选择最常见的5,000个标记，并创建查找表（从单词到单词索引以及反向查找）。此外，我们为词汇表中不存在的标记创建一个伪标记`_UNK_`。使用这些查找表，我们将每个句子转换为一个单词ID序列，并对这些序列进行填充，使得所有序列的长度相同（即训练集中句子的最大单词数）。我们还将标签转换为类别格式：
- en: '[PRE41]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We load the GloVe vectors into a dictionary. If we wanted to use word2vec here,
    all we have to do is replace this block with a gensim `Word2Vec.load_word2vec_format()`
    call and replace the following block to look up the word2vec model instead of
    the `word2emb` dictionary:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GloVe向量加载到字典中。如果我们想在这里使用word2vec，我们只需将这一块代码替换为gensim的`Word2Vec.load_word2vec_format()`调用，并将以下代码块替换为查找word2vec模型，而不是`word2emb`字典：
- en: '[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The next block looks up the words for each sentence from the word ID matrix
    `W` and populates a matrix `E` with the corresponding embedding vector. These
    embedding vectors are then added to create a sentence vector, which is written
    back into the `X` matrix. The output of this code block is the matrix `X` of size
    (`num_records` and `EMBED_SIZE`):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块从单词ID矩阵`W`中查找每个句子的单词，并用相应的嵌入向量填充矩阵`E`。然后，将这些嵌入向量相加以创建一个句子向量，并将其写回到`X`矩阵中。此代码块的输出是矩阵`X`，其大小为（`num_records`和`EMBED_SIZE`）：
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We have now preprocessed our data using the pre-trained model and are ready
    to use it to train and evaluate our final model. Let us split the data into *70/30*
    training/test as usual:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经使用预训练模型预处理了数据，并准备好使用它来训练和评估我们的最终模型。让我们像往常一样将数据分为*70/30*的训练集/测试集：
- en: '[PRE44]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The network we will train for doing the sentiment analysis task is a simple
    dense network. We compile it with a categorical cross-entropy loss function and
    the Adam optimizer, and train it with the sentence vectors that we built out of
    the pre-trained embeddings. Finally, we evaluate the model on the 30% test set:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要训练的用于情感分析任务的网络是一个简单的密集网络。我们用类别交叉熵损失函数和Adam优化器来编译它，并使用从预训练嵌入中构建的句子向量来训练它。最后，我们在30%的测试集上评估该模型：
- en: '[PRE45]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output for the code using GloVe embeddings is shown as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GloVe嵌入的代码输出如下所示：
- en: '![](img/ss-5-3.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ss-5-3.png)'
- en: The dense network gives us 96.5% accuracy on the test set after 10 epochs of
    training when preprocessed with the 100-dimensional GloVe embeddings. With preprocessed
    with the word2vec embeddings (300-dimensional fixed) the network gives us 98.5%
    on the test set.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过10轮训练后，使用100维GloVe嵌入进行预处理的密集网络在测试集上取得了96.5%的准确率。而使用300维固定的word2vec嵌入进行预处理时，网络在测试集上的准确率达到了98.5%。
- en: The source code for this example can be found in `transfer_glove_embeddings.py`
    (for the GloVe example) and `transfer_word2vec_embeddings.py` (for the word2vec
    example) in the source code download for the chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的源代码可以在章节的源代码下载中找到，文件分别为`transfer_glove_embeddings.py`（GloVe示例）和`transfer_word2vec_embeddings.py`（word2vec示例）。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned how to transform words in text into vector embeddings
    that retain the distributional semantics of the word. We also now have an intuition
    of why word embeddings exhibit this kind of behavior and why word embeddings are
    useful for working with deep learning models for text data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何将文本中的单词转换为向量嵌入，这些嵌入保留了单词的分布语义。我们现在也能直观地理解为什么单词嵌入会展现出这种行为，以及为什么单词嵌入在处理文本数据的深度学习模型中如此有用。
- en: We then looked at two popular word embedding schemes, word2vec and GloVe, and
    understood how these models work. We also looked at using gensim to train our
    own word2vec model from data.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们研究了两种流行的单词嵌入方法——word2vec和GloVe，并理解了这些模型是如何工作的。我们还学习了如何使用gensim从数据中训练我们自己的word2vec模型。
- en: Finally, we learned about different ways of using embeddings in our network.
    The first was to learn embeddings from scratch as part of training our network.
    The second was to import embedding weights from pre-trained word2vec and GloVe
    models into our networks and fine-tune them as we train the network. The third
    was to use these pre-trained weights as is in our downstream applications.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们了解了在网络中使用嵌入的不同方式。第一种是从头开始学习嵌入，作为训练网络的一部分。第二种是将预训练的word2vec和GloVe模型的嵌入权重导入到我们的网络中，并在训练过程中进行微调。第三种是直接在下游应用中使用这些预训练的权重。
- en: In the next chapter, we will learn about recurrent neural networks, a class
    of network that is optimized for handling sequence data such as text.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将学习循环神经网络（RNN），一种优化处理序列数据（如文本）的网络类型。
