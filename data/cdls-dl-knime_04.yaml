- en: '*Chapter 3:* Getting Started with Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第3章:* 神经网络入门'
- en: Before we dive into the practical implementation of deep learning networks using
    KNIME Analytics Platform and its integration with the Keras library, we will briefly
    introduce a few theoretical concepts behind neural networks and deep learning.
    This is the only purely theoretical chapter in this book, and it is needed to
    understand the how and why of the following practical implementations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入使用KNIME分析平台和其与Keras库的集成进行深度学习网络的实际实现之前，我们将简要介绍一些神经网络和深度学习背后的理论概念。这是本书中唯一纯粹的理论性章节，它对于理解随后的实际实现的原理和原因至关重要。
- en: 'Throughout this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Neural Networks and Deep Learning – Basic Concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络与深度学习——基本概念
- en: Designing your Network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计你的网络
- en: Training a Neural Network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'We will start with the basic concepts of neural networks and deep learning:
    from the first artificial neuron as a simulation of the biological neuron to the
    training of a network of neurons, a fully connected feedforward neural network,
    using a backpropagation algorithm.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从神经网络和深度学习的基本概念开始：从第一个人工神经元作为生物神经元的模拟，到使用反向传播算法训练一个全连接的前馈神经网络。
- en: We will then discuss the design of a neural architecture as well as the training
    of the final neural network. Indeed, when designing a neural architecture, we
    need to appropriately select its topology, neural layers, and activation functions,
    and introduce some techniques to avoid overfitting.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论神经架构的设计以及最终神经网络的训练。实际上，在设计神经架构时，我们需要恰当地选择其拓扑结构、神经层和激活函数，并引入一些技术来避免过拟合。
- en: Finally, before training, we need to know when to use which loss function and
    what the different parameters that have to be set for training are. This will
    be described in the last part of this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在进行训练之前，我们需要知道什么时候使用哪种损失函数，以及训练中需要设置的不同参数。这个内容将在本章的最后部分进行描述。
- en: Neural Networks and Deep Learning – Basic Concepts
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习——基本概念
- en: All you hear about at the moment is deep learning. Deep learning stems from
    the traditional discipline of neural networks, in the realm of machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当前你听到的几乎全是深度学习。深度学习源自传统的神经网络学科，属于机器学习领域。
- en: The field of neural networks has gone through a number of stop-and-go phases.
    Since the early excitement for the first perceptron in the '60s and the subsequent
    lull when it became evident what the perceptron could not do; through the renewed
    enthusiasm for the backpropagation algorithm applied to multilayer feedforward
    neural networks and the subsequent lull when it became apparent that training
    recurrent networks required hardware capabilities that were not available at the
    time; right up to today's new deep learning paradigms, units, and architectures
    running on much more powerful, possibly GPU-equipped, hardware.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络领域经历了多个停滞与复兴的阶段。从20世纪60年代最初的感知机兴起，到感知机无法实现的局限性显现出来时的低谷；再到基于反向传播算法的多层前馈神经网络的重新兴起，以及训练递归网络所需的硬件能力在当时不可用时的又一低谷；直到今天，深度学习新范式、单元和架构的出现，运行在更强大的硬件上，可能还配备了GPU。
- en: Let's start from the beginning and, in this section, go through the basic concepts
    behind neural networks and deep learning. While these basic notions might be familiar
    to you, especially if you have already attended a neural networks or deep learning
    course, we would still like to describe them here as a reference for descriptions
    of KNIME deep learning functionalities in the coming chapters, and for anyone
    who might be a neophyte to this field.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始，在这一部分中，逐步了解神经网络和深度学习的基本概念。尽管这些基本概念对你来说可能并不陌生，特别是如果你已经参加过神经网络或深度学习课程，但我们仍希望在这里描述它们，作为接下来章节中关于KNIME深度学习功能描述的参考，并且为那些刚接触这个领域的新手提供帮助。
- en: Artificial Neuron and Artificial Neural Networks
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元与人工神经网络
- en: '**Artificial Neural Networks** (**ANNs**) began with simulating the biological
    neuron, depicted on the left in *Figure 3.1* (Abbott, L.F. (1999) *Lapique''s
    introduction of the integrate-and-fire model neuron* (*1907*[): https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.](https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.pdf)pdf.
    *Brain Research Bulletin. 50 (5/6)*: 303–304). The biological neuron is a nerve
    cell consisting of a body (soma), a few input dendrites, and one output axon with
    one or more terminals. When activated, it generates a sharp electrical potential
    spike. The dendrites accept electrical input signals, usually from other neurons,
    through chemical synapses. The chemical reaction happening in the synapse enhances
    or reduces – in other words, it weighs – the input electrical signal before it
    reaches the body of the neuron. If the total electrical signal in the soma is
    high enough, the neuron produces an electrical spike along its axon. The axon
    terminals then bring out the spike to other neurons, again through chemical reactions
    in synapses:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANNs**）始于模拟生物神经元，如 *图 3.1* 中左侧所示（Abbott, L.F. (1999) *Lapique''s
    introduction of the integrate-and-fire model neuron* (*1907*[): https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.](https://web.archive.org/web/20070613230629/http:/neurotheory.columbia.edu/~larry/AbbottBrResBul99.pdf)pdf.
    *Brain Research Bulletin. 50 (5/6)*: 303–304）。生物神经元是由细胞体（胞体）、几个输入树突和一个或多个轴突末端组成的神经细胞。激活时，它会产生一个尖锐的电位波动。树突通过化学突触接受电信号输入，通常来自其他神经元。突触中发生的化学反应增强或减少（即加权）输入的电信号，然后传递到神经元体。如果细胞体内的总电信号足够强，神经元会沿其轴突产生一个电位波动。然后轴突末端再次通过突触中的化学反应将波动传递给其他神经元。'
- en: '![Figure 3.1 – On the left, a biological neuron with inputs xj at dendrites
    and output y at the axon terminal (image from Wikipedia). On the right, an artificial
    neuron (perceptron) with inputs xj connected to weights wj and producing output
    y](img/B16391_03_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – 左侧是生物神经元，具有树突上的输入 xj 和轴突端的输出 y（图片来源：维基百科）。右侧是人工神经元（感知器），其输入 xj 与权重
    wj 相连，产生输出 y](img/B16391_03_001.jpg)'
- en: Figure 3.1 – On the left, a biological neuron with inputs xj at dendrites and
    output y at the axon terminal (image from Wikipedia). On the right, an artificial
    neuron (perceptron) with inputs xj connected to weights wj and producing output
    y
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – 左侧是生物神经元，具有树突上的输入 xj 和轴突端的输出 y（图片来源：维基百科）。右侧是人工神经元（感知器），其输入 xj 与权重 wj
    相连，产生输出 y。
- en: The simplest simulation tries to reproduce a biological neuron with just two
    inputs and one output, like on the right in *Figure 3.1*. The input signals at
    the dendrites are now called ![](img/Formula_B16391_03_001.png) and ![](img/Formula_B16391_03_002.png)
    and reach the soma of the artificial cell via two weights, ![](img/Formula_B16391_03_003.png)
    and ![](img/Formula_B16391_03_004.png) simulating the chemical reactions in the
    synapses. If the total input signal reaching the soma is higher than a given threshold
    ![](img/Formula_B16391_03_005.png), simulating the "high enough" concept, an output
    signal ![](img/Formula_B16391_03_006.png) is generated. This simple artificial
    neuron is called a **perceptron**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的模拟试图仅使用两个输入和一个输出重现生物神经元，如 *图 3.1* 中右侧所示。现在树突的输入信号称为 ![](img/Formula_B16391_03_001.png)
    和 ![](img/Formula_B16391_03_002.png)，通过两个权重 ![](img/Formula_B16391_03_003.png)
    和 ![](img/Formula_B16391_03_004.png) 到达人工细胞的细胞体，模拟突触中的化学反应。如果达到细胞体的总输入信号超过给定的阈值
    ![](img/Formula_B16391_03_005.png)，模拟“足够强”的概念，将生成一个输出信号 ![](img/Formula_B16391_03_006.png)。这种简单的人工神经元称为**感知器**。
- en: 'Two details to clarify here: the total input signal and the threshold function.
    There are many neural electrical input-output voltage models (Hodgkin, A. L.;
    Huxley, A. F. (1952), *A quantitative description of membrane current and its
    application to conduction and excitation in* [*nerve*, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413)1392413,
    *The Journal of Physiology. 117 (4)*: 500–544). The simplest way to represent
    the total input signal uses a weighted sum of all input signals, where the weights
    represent the role of the synapse reactions. The firing function of the neuron
    soma can be described via a step function ![](img/Formula_B16391_03_007.png).
    Thus, for our simplified simulation in *Figure 3.1*, the output ![](img/Formula_B16391_03_006.png)
    is calculated as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个需要澄清的细节：总输入信号和阈值函数。有许多神经电输入输出电压模型（Hodgkin, A. L.; Huxley, A. F. (1952),
    *膜电流的定量描述及其在[神经](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1392413)中的传导与兴奋中的应用*，《生理学杂志》
    117 (4)：500–544）。表示总输入信号的最简单方法是使用所有输入信号的加权和，其中权重代表突触反应的作用。神经元细胞体的激发函数可以通过阶跃函数
    ![](img/Formula_B16391_03_007.png) 来描述。因此，对于我们在 *图 3.1* 中的简化模拟，输出 ![](img/Formula_B16391_03_006.png)
    的计算方式如下：
- en: '![](img/Formula_B16391_03_009.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_009.png)'
- en: 'Here, ![](img/Formula_B16391_03_010.png) is a step function with threshold
    ![](img/Formula_B16391_03_011.png):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B16391_03_010.png) 是一个具有阈值 ![](img/Formula_B16391_03_011.png)
    的阶跃函数：
- en: '![](img/Formula_B16391_03_012.png) with ![](img/Formula_B16391_03_013.png).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B16391_03_012.png)，其中 ![](img/Formula_B16391_03_013.png)。'
- en: 'Generalizing to a neuron with ![](img/Formula_B16391_03_014.png) input signals
    and with any other activation function ![](img/Formula_B16391_03_015.png), we
    get the following formula:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果推广到具有 ![](img/Formula_B16391_03_014.png) 输入信号的神经元，并且具有任何其他激活函数 ![](img/Formula_B16391_03_015.png)，我们可以得到以下公式：
- en: '![](img/Formula_B16391_03_016.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_016.png)'
- en: Here, the threshold ![](img/Formula_B16391_03_017.png) has been transformed
    into a weight ![](img/Formula_B16391_03_018.png) connected to an input signal
    ![](img/Formula_B16391_03_019.png) that is constantly on – that is, constantly
    set to 1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，阈值 ![](img/Formula_B16391_03_017.png) 已被转化为一个与输入信号 ![](img/Formula_B16391_03_019.png)
    相连的权重 ![](img/Formula_B16391_03_018.png)，该信号始终处于开启状态——也就是说，始终设为 1。
- en: However, one single artificial neuron, just like one single biological neuron,
    does not have high computational capability. It can implement just a few simple
    functions, as we will see later in the next sub-section, *Understanding the need
    for hidden layers*. As in the biological world, networks of neurons have a much
    bigger computational potential than one single neuron alone. Networks of biological
    neurons, such as even simple brains, can learn and carry out very complex tasks.
    Similarly, networks of artificial neurons can learn and carry out very complex
    tasks. The key to the success of neural networks is this flexibility in forming
    more or less complex architectures and in training them to perform more or less
    complex tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个单独的人工神经元，就像一个单独的生物神经元一样，计算能力并不强大。它只能实现一些简单的功能，正如我们在下一个子节 *理解隐藏层的需求* 中看到的那样。正如生物界一样，神经元网络比单个神经元有更大的计算潜力。生物神经元网络，甚至是简单的大脑，能够学习并完成非常复杂的任务。同样，人工神经元网络也能够学习并完成非常复杂的任务。神经网络成功的关键在于其在形成复杂或简单的架构以及训练它们以执行复杂或简单任务方面的灵活性。
- en: 'An example of a network of perceptrons is shown in *Figure 3.2*. This network
    has three layers of neurons: an input layer accepting the input signals ![](img/Formula_B16391_03_020.png);
    a first hidden layer with two neurons connected to the outputs of the input layer;
    a second hidden layer with three neurons connected to the outputs of the first
    hidden layer; and finally an output layer with one neuron only, fed by the outputs
    of the hidden layer and producing the final output ![](img/Formula_B16391_03_021.png)
    of the network. Neurons are indicated by a circle including the ![](img/Formula_B16391_03_022.png)
    symbol for the weighted sum of the input signals and the ![](img/Formula_B16391_03_023.png)
    symbol for the activation function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个感知机网络的示例如*图3.2*所示。这个网络有三层神经元：输入层接收输入信号 ![](img/Formula_B16391_03_020.png)；第一隐含层有两个神经元，连接到输入层的输出；第二隐含层有三个神经元，连接到第一隐含层的输出；最后是输出层，只有一个神经元，由隐含层的输出供给，并产生网络的最终输出
    ![](img/Formula_B16391_03_021.png)。神经元通过一个圆圈表示，其中包含表示输入信号加权和的 ![](img/Formula_B16391_03_022.png)
    符号和表示激活函数的 ![](img/Formula_B16391_03_023.png) 符号：
- en: '![Figure 3.2 – On the left, a network of biological neurons (image from Wikipedia).
    On the right, a network of artificial neurons (multi-layer perceptron)](img/B16391_03_002.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图3.2 – 左侧是生物神经元网络（图片来自维基百科）。右侧是人工神经元网络（多层感知机）](img/B16391_03_002.jpg)'
- en: Figure 3.2 – On the left, a network of biological neurons (image from Wikipedia).
    On the right, a network of artificial neurons (multi-layer perceptron)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 左侧是生物神经元网络（图片来自维基百科）。右侧是人工神经元网络（多层感知机）
- en: 'Notice that in this particular architecture, all connections move from the
    input to the output layer: this is a fully connected feedforward architecture.
    Of course, a **feedforward neural network** can have as many hidden layers as
    you want, and each neural layer can have as many artificial neurons as you want.
    A feedforward network of perceptrons is called a **Multi-Layer Perceptron** (**MLP**).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种特定的架构中，所有连接从输入层移动到输出层：这是一个完全连接的前馈架构。当然，**前馈神经网络**可以有任意数量的隐含层，每个神经层也可以有任意数量的人工神经元。一个感知机的前馈网络叫做**多层感知机**（**MLP**）。
- en: Signal Propagation within a Feedforward Neural Network
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈神经网络中的信号传播
- en: 'A simple fully connected feedforward neural network can then be described as
    a function that transforms the input values ![](img/Formula_B16391_03_024.png)
    into the output value ![](img/Formula_B16391_03_021.png), through a series of
    intermediate values ![](img/Formula_B16391_03_026.png): the outputs of the hidden
    layers. For example, for the network in *Figure 3.3*, we have the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的完全连接的前馈神经网络可以描述为一个将输入值 ![](img/Formula_B16391_03_024.png) 转换为输出值 ![](img/Formula_B16391_03_021.png)
    的函数，通过一系列中间值 ![](img/Formula_B16391_03_026.png)：隐含层的输出。例如，对于*图3.3*中的网络，我们有以下内容：
- en: '![](img/Formula_B16391_03_027.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_027.png)'
- en: '![](img/Formula_B16391_03_028.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_028.png)'
- en: Here, ![](img/Formula_B16391_03_029.png) is the number of input values (and
    input units) ![](img/Formula_B16391_03_024.png), ![](img/Formula_B16391_03_031.png)
    is the number of hidden neurons with output values ![](img/Formula_B16391_03_032.png),
    ![](img/Formula_B16391_03_021.png) is the final output value (the only one in
    this architecture), and ![](img/Formula_B16391_03_034.png) is the neurons' activation
    functions. If we number the neural layers progressively from the input to the
    output, we will label layer 1 as the input layer, layer 2 as the hidden layer,
    and layer 3 as the output layer. This progressive numbering of the neural layers
    is also contained in the weight and hidden unit notations. ![](img/Formula_B16391_03_035.png)
    is the output value of the ![](img/Formula_B16391_03_036.png) neural unit in the
    ![](img/Formula_B16391_03_037.png) (hidden) layer, and ![](img/Formula_B16391_03_038.png)
    is the weight connecting neural unit ![](img/Formula_B16391_03_039.png) in layer
    ![](img/Formula_B16391_03_040.png) with neural unit ![](img/Formula_B16391_03_036.png)
    in layer ![](img/Formula_B16391_03_042.png).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_029.png) 是输入值（和输入单元）的数量，![](img/Formula_B16391_03_024.png)
    ，![](img/Formula_B16391_03_031.png) 是具有输出值 ![](img/Formula_B16391_03_032.png)
    的隐藏神经元的数量，![](img/Formula_B16391_03_021.png) 是最终的输出值（在此架构中唯一的一个），而 ![](img/Formula_B16391_03_034.png)
    是神经元的激活函数。如果我们从输入到输出逐步编号神经层，我们将标记第1层为输入层，第2层为隐藏层，第3层为输出层。神经层的这种逐步编号也体现在权重和隐藏单元的表示法中。![](img/Formula_B16391_03_035.png)
    是 ![](img/Formula_B16391_03_036.png) 神经单元在 ![](img/Formula_B16391_03_037.png)
    （隐藏）层中的输出值，![](img/Formula_B16391_03_038.png) 是连接第 ![](img/Formula_B16391_03_039.png)
    层中的神经单元与第 ![](img/Formula_B16391_03_042.png) 层中的神经单元之间的权重。
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Notice that, in this notation, ![](img/Formula_B16391_03_043.png) and ![](img/Formula_B16391_03_044.png)
    in ![](img/Formula_B16391_03_045.png) and in ![](img/Formula_B16391_03_046.png)
    are NOT exponents. They just describe the network layer of the output unit for
    ![](img/Formula_B16391_03_045.png) and the destination layer for ![](img/Formula_B16391_03_048.png).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种表示法中，![](img/Formula_B16391_03_043.png) 和 ![](img/Formula_B16391_03_044.png)
    在 ![](img/Formula_B16391_03_045.png) 和 ![](img/Formula_B16391_03_046.png) 中不是指数。它们仅描述输出单元在
    ![](img/Formula_B16391_03_045.png) 中的网络层，以及在 ![](img/Formula_B16391_03_048.png)
    中的目标层。
- en: '![Figure 3.3 – Fully connected feedforward neural network with just one hidden
    layer and one output unit](img/B16391_03_003.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – 具有一个隐藏层和一个输出单元的全连接前馈神经网络](img/B16391_03_003.jpg)'
- en: Figure 3.3 – Fully connected feedforward neural network with just one hidden
    layer and one output unit
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – 具有一个隐藏层和一个输出单元的全连接前馈神经网络
- en: 'There are many types of **activation functions** ![](img/Formula_B16391_03_049.png).
    We have seen the step function in the previous section, which however has a main
    flaw: it is neither continuous nor derivable. Some similar activation functions
    have been introduced over the years, which are easier to handle since they are
    continuous and derivable everywhere. Common examples are the **sigmoid** and the
    **hyperbolic tangent,** ![](img/Formula_B16391_03_050.png). Recently, a new activation
    function, named **Rectified Linear Unit** (**ReLU**), has been introduced, which
    seems to perform better with fully connected feedforward neural networks with
    many hidden layers. We will describe these activation functions in detail in the
    coming chapters.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多类型的**激活函数** ![](img/Formula_B16391_03_049.png)。我们在上一节中看到了阶跃函数，但它有一个主要缺陷：既不连续也不可导。近年来，已经引入了一些类似的激活函数，它们更容易处理，因为它们在任何地方都是连续的并且可导。常见的例子有**sigmoid**
    函数和**双曲正切**，![](img/Formula_B16391_03_050.png)。最近，引入了一种新的激活函数，称为**修正线性单元**（**ReLU**），它似乎在具有多个隐藏层的全连接前馈神经网络中表现更好。我们将在接下来的章节中详细描述这些激活函数。
- en: Important note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Usually, neurons in the same layer have the same activation function ![](img/Formula_B16391_03_051.png)
    Different layers, though, can have different activation functions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，同一层中的神经元使用相同的激活函数 ![](img/Formula_B16391_03_051.png)，但不同层之间可以使用不同的激活函数。
- en: Another parameter of a network is its **topology**, or architecture. We have
    seen a fully connected feedforward network, where all connections move from the
    input toward the output and, under this constraint, all units are connected to
    all units in the next layer. However, this is of course not the only possible
    neural topology. Cross-connections within the same layer ![](img/Formula_B16391_03_044.png),
    backward connections from layer ![](img/Formula_B16391_03_044.png) to layer ![](img/Formula_B16391_03_054.png),
    and autoconnections of a single neuron with itself are also possible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的另一个参数是其**拓扑结构**或架构。我们已经看到一个完全连接的前馈网络，其中所有连接从输入流向输出，并且在这种约束下，所有单元都连接到下一层的所有单元。然而，当然这不是唯一可能的神经拓扑结构。层内的交叉连接![](img/Formula_B16391_03_044.png)、从层![](img/Formula_B16391_03_044.png)到层![](img/Formula_B16391_03_054.png)的反向连接，以及单个神经元与自身的自连接也是可能的。
- en: 'Different connections and different architectures produce different data processing
    functions. For example, autoconnections introduce a time component, since the
    current output of neuron ![](img/Formula_B16391_03_036.png) at time ![](img/Formula_B16391_03_056.png)
    will be an additional input for the same neuron ![](img/Formula_B16391_03_036.png)
    at time ![](img/Formula_B16391_03_058.png); a feedforward neural network with
    as many outputs as inputs can implement an autoencoder and be used for compression
    or for outlier detection. We will see some of these different neural architectures
    and the tasks they can implement later in this book. For now, we just give you
    a little taste of possible neural topologies in *Figure 3.4*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的连接方式和不同的架构会产生不同的数据处理函数。例如，自连接引入了时间成分，因为神经元![](img/Formula_B16391_03_036.png)在时间![](img/Formula_B16391_03_056.png)的当前输出将成为同一神经元![](img/Formula_B16391_03_036.png)在时间![](img/Formula_B16391_03_058.png)的额外输入；一个前馈神经网络，输入和输出数量相同，可以实现自编码器，并用于压缩或异常检测。我们将在本书后续内容中看到一些不同的神经架构及其可以实现的任务。目前，我们只是给你提供一个*图3.4*中的神经网络拓扑的初步了解：
- en: '![Figure 3.4 – Some examples of neural network topologies](img/B16391_03_004.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 一些神经网络拓扑结构的例子](img/B16391_03_004.jpg)'
- en: Figure 3.4 – Some examples of neural network topologies
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 一些神经网络拓扑结构的例子
- en: The first network from the left in *Figure 3.4* has its neurons all completely
    connected, so that the definition of the layer becomes unnecessary. This is a
    Hopfield network and is generally used as an associative memory.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.4*中最左侧的第一个网络，其神经元完全连接，因此层的定义变得不必要。这是一个霍普菲尔德网络，通常用作关联记忆。'
- en: 'The second network is a feedforward autoencoder: three layers, as many ![](img/Formula_B16391_03_029.png)
    input units as many ![](img/Formula_B16391_03_029.png) output units, and a hidden
    layer with ![](img/Formula_B16391_03_061.png) units, where usually ![](img/Formula_B16391_03_062.png);
    this network architecture has been adopted for outlier detection or to implement
    a dimensionality reduction of the input space.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个网络是一个前馈自编码器：三层，输入单元和输出单元的数量相同，隐藏层有![](img/Formula_B16391_03_061.png)个单元，通常![](img/Formula_B16391_03_062.png)；这种网络架构已被用于异常检测或实现输入空间的降维。
- en: Finally, the third network presents units with autoconnections. As said before,
    autoconnections introduce a time component within the function implemented by
    the network and therefore are often adopted for time series analysis. This last
    network qualifies as a recurrent neural network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第三种网络呈现了带有自连接的单元。如前所述，自连接在网络实现的函数中引入了时间成分，因此通常用于时间序列分析。这个最后的网络属于递归神经网络。
- en: Let's go back to fully connected feedforward neural networks. Now that we've
    seen how they are structured, let's try to understand why they are built this
    way.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到完全连接的前馈神经网络。既然我们已经看到了它们的结构，接下来我们试着理解它们为何以这种方式构建。
- en: Understanding the Need for Hidden Layers
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解隐藏层的需求
- en: 'The question now is: do we really need such complex neural architectures? What
    can the perceptron in *Figure 3.1* do and what can it not do? A perceptron, using
    a step function as the activation function, implements a line (a linear combination
    of the input signals) as a discriminant surface in a two-dimensional space, the
    parameters of the line being the weights and threshold of the perceptron.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：我们真的需要如此复杂的神经架构吗？*图3.1*中的感知机能做什么，不能做什么？感知机使用阶跃函数作为激活函数，将一条线（输入信号的线性组合）作为二维空间中的判别面，其中该线的参数是感知机的权重和阈值。
- en: 'Classic examples are the **OR** and **AND** problems, which can be solved by
    a line separating the "1" outputs from the "0" outputs. Therefore, a perceptron
    can implement a solution to both problems. However, it cannot implement a solution
    to the **XOR** problem. The XOR function outputs "1" when the two inputs are different
    (one is "0" and one is "1") and outputs "0" when the two inputs are the same (both
    are "0" or both are "1"). Indeed, the XOR operator is a nonlinearly separable
    problem and one line only is not sufficient to separate the "1" outputs from the
    "0" outputs (*Figure 3.5*):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的例子是**OR**和**AND**问题，这些问题可以通过一条直线将“1”输出与“0”输出分开来解决。因此，感知器可以实现这两个问题的解决方案。然而，它无法解决**XOR**问题。当两个输入不同（一个是“0”，一个是“1”）时，XOR函数输出“1”；当两个输入相同（都为“0”或都为“1”）时，输出“0”。事实上，XOR操作符是一个非线性可分的问题，一条直线不足以将“1”输出与“0”输出分开（*图
    3.5*）：
- en: '![Figure 3.5 – A perceptron implements a linear discriminant surface, which
    is a line in a two-dimensional space. All linearly separable problems can be solved
    by a single perceptron. A perceptron cannot solve non-linearly separable problems](img/B16391_03_005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5 – 感知器实现了一个线性判别面，这在二维空间中是一条直线。所有线性可分的问题都可以通过单个感知器解决。而感知器无法解决非线性可分的问题](img/B16391_03_005.jpg)'
- en: Figure 3.5 – A perceptron implements a linear discriminant surface, which is
    a line in a two-dimensional space. All linearly separable problems can be solved
    by a single perceptron. A perceptron cannot solve non-linearly separable problems
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 感知器实现了一个线性判别面，这在二维空间中是一条直线。所有线性可分的问题都可以通过单个感知器解决。而感知器无法解决非线性可分的问题
- en: 'The only possibility to solve the XOR problem is to add one hidden layer with
    two units into the perceptron architecture, making it into an MLP (*Figure 3.6*).
    The two hidden units in green and red each implement one line to separate some
    "0"s and "1"s. The one unit in the output layer then builds a new line on top
    of the two previous lines and implements the final discriminant:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解决XOR问题的唯一可能方法是将一个具有两个单元的隐藏层添加到感知器架构中，从而使其成为MLP（*图 3.6*）。绿色和红色的两个隐藏单元分别实现一条直线，用于分隔一些“0”和“1”。然后，输出层中的一个单元在之前两条直线的基础上构建一条新线，并实现最终的判别：
- en: '![Figure 3.6 – One additional hidden layer with two units enables the MLP to
    solve the XOR problem](img/B16391_03_006.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6 – 增加一个具有两个单元的隐藏层，使得MLP能够解决XOR问题](img/B16391_03_006.jpg)'
- en: Figure 3.6 – One additional hidden layer with two units enables the MLP to solve
    the XOR problem
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 – 增加一个具有两个单元的隐藏层，使得MLP能够解决XOR问题
- en: 'The example in *Figure 3.7* shows a three-layer network: one input layer receiving
    input values ![](img/Formula_B16391_03_063.png) and ![](img/Formula_B16391_03_064.png),
    one hidden layer with two units, and one output layer with one unit only. The
    two hidden units implement two discrimination lines: ![](img/Formula_B16391_03_065.png)
    for the red unit and ![](img/Formula_B16391_03_066.png) for the orange unit. The
    output line implements a discrimination line on top of these two as ![](img/Formula_B16391_03_067.png),
    which is identified by the green area in the plane shown in *Figure 3.7*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.7*中的示例显示了一个三层网络：一个接收输入值的输入层 ![](img/Formula_B16391_03_063.png) 和 ![](img/Formula_B16391_03_064.png)，一个包含两个单元的隐藏层，以及只有一个单元的输出层。这两个隐藏单元实现了两条判别线：![](img/Formula_B16391_03_065.png)
    对应红色单元，![](img/Formula_B16391_03_066.png) 对应橙色单元。输出线则在这两条线之上实现了一个判别线，即 ![](img/Formula_B16391_03_067.png)，在*图
    3.7*中显示的平面上由绿色区域标识：'
- en: '![Figure 3.7 – The network on the left fires up only for the points in the
    green zone in the input space, as depicted on the right](img/B16391_03_007.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.7 – 左侧的网络仅在输入空间中的绿色区域的点处激活，如右侧所示](img/B16391_03_007.jpg)'
- en: Figure 3.7 – The network on the left fires up only for the points in the green
    zone in the input space, as depicted on the right
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 – 左侧的网络仅在输入空间中的绿色区域的点处激活，如右侧所示
- en: 'As you see, adding just one hidden layer makes the neural network much more
    powerful in terms of possible functions to implement. However, there is more.
    The **Universal Approximation Theorem** states that a simple feedforward network
    with a single hidden layer and a sufficient number of neurons can approximate
    any continuous function on compact subsets of ![](img/Formula_B16391_03_068.png),
    under mild assumptions on the activation function and assuming that the network
    has been sufficiently trained (Hornik K., Stinchcombe M., White H. (1989) *Multilayer
    feedforward networ*[*ks are universal approximators*: http://www.sciencedirect.com/scie](http://www.sciencedirect.com/science/article/pii/0893608089900208)nce/article/pii/0893608089900208
    *Neural Networks, Vol. 2, Issue 5*, (*1989*) Pages 359-366). This theorem proves
    that neural networks have a kind of *universality* property. That is, any function
    can be approximated by a sufficiently large and sufficiently trained neural network.
    Sufficiently large refers to the number of neurons in a feedforward network. In
    addition, the cited paper refers to network architectures with just one single
    hidden layer with enough neurons.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，单独添加一个隐藏层就可以使神经网络在实现可能的函数方面变得更强大。然而，还有更多内容。**通用逼近定理**指出，一个简单的前馈网络，拥有单一的隐藏层并且具有足够数量的神经元，能够逼近在紧凑子集上定义的任何连续函数！[](img/Formula_B16391_03_068.png)，前提是激活函数满足适当的假设，并且假设网络已经得到了足够的训练（Hornik
    K.、Stinchcombe M.、White H.（1989年）《多层前馈网络》[*神经网络是通用逼近器*：http://www.sciencedirect.com/scie](http://www.sciencedirect.com/science/article/pii/0893608089900208)nce/article/pii/0893608089900208
    《神经网络》，第2卷，第5期，(*1989*) 第359-366页）。这个定理证明了神经网络具有某种*普适性*特性。也就是说，任何函数都可以被一个足够大且训练充分的神经网络所逼近。所谓足够大，指的是前馈网络中神经元的数量。此外，引用的论文还提到网络架构只需一个隐藏层，并且神经元足够多。
- en: Even very simple network architectures, thus, can be very powerful! Of course,
    this is all true under the assumption of a sufficiently large hidden layer (which
    might become too large for a reasonable training time) and a sufficient training
    time.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，即使是非常简单的网络架构，也可以非常强大！当然，前提是隐藏层足够大（否则可能需要过长的训练时间），并且训练时间足够长。
- en: A feedforward network with a single layer is sufficient to represent any function,
    but the layer may be infeasibly large and may fail to learn and generalize correctly,
    (Goodfellow I., Bengio Y., Courville A. (2016). *Deep Learning*, *MIT Press*).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个带有单层的前馈网络足以表示任何函数，但该层可能过于庞大，且可能无法正确学习和泛化（Goodfellow I.、Bengio Y.、Courville
    A.（2016年）《*深度学习*》，*MIT出版社*）。
- en: We have seen that introducing one or more hidden layers to a feedforward neural
    network makes it extremely powerful. Let's see how to train it.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，引入一个或多个隐藏层到前馈神经网络中，使其变得极其强大。接下来我们来看看如何训练它。
- en: Training a Multilayer Perceptron
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练多层感知器
- en: 'A neural network includes a few free parameters: the topology, the weights,
    and the parameters of the activation functions. Let''s consider a fully connected
    feedforward network with a pre-defined activation function for all neurons, such
    as a sigmoid function, for example. Then, the only free parameters left are the
    weights.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络包括几个自由参数：拓扑结构、权重和激活函数的参数。假设我们有一个完全连接的前馈网络，并且所有神经元都有一个预定义的激活函数，比如 sigmoid
    函数。那么，唯一剩下的自由参数就是权重。
- en: Training a neural network means showing examples from the training set repeatedly
    and each time adjusting the parameter values, the weights, to fit a loss function,
    calculated on the desired input-output behavior. To find the weights that best
    fit the loss functions, the gradient descent algorithm or variants of **Stochastic
    Gradient Descent** (**SGD**) are used. The idea is to update the weights by taking
    steps in the direction of steepest descent on the error surface. The direction
    of steepest descent is equivalent to the negative of the gradient. To calculate
    the gradient efficiently, the backpropagation algorithm is used. Let's find out
    how it works.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络意味着反复展示训练集中的样本，并且每次都调整参数值（权重），以拟合基于期望输入输出行为计算的损失函数。为了找到最适合损失函数的权重，使用了梯度下降算法或**随机梯度下降（**SGD**）**的变体。其思路是通过沿着误差面最陡下降的方向更新权重。最陡下降的方向等同于梯度的负值。为了高效地计算梯度，采用了反向传播算法。让我们来看一下它是如何工作的。
- en: The math behind backpropagation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播背后的数学
- en: 'A classic loss function for regression problems is the total squared error,
    defined as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题的经典损失函数是总平方误差，定义如下：
- en: '![](img/Formula_B16391_03_069.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_069.png)'
- en: Here, ![](img/Formula_B16391_03_070.png) and ![](img/Formula_B16391_03_071.png)
    are respectively the desired target and the real answer for output unit ![](img/Formula_B16391_03_072.png),
    and the sum runs on all units of the output layer and on all examples in the training
    set ![](img/Formula_B16391_03_073.png).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_070.png) 和 ![](img/Formula_B16391_03_071.png) 分别是输出单元
    ![](img/Formula_B16391_03_072.png) 的目标值和实际答案，求和在输出层的所有单元和训练集中的所有示例上进行 ![](img/Formula_B16391_03_073.png)。
- en: 'If we adopt the gradient descent strategy to reach a minimum in the loss function
    surface, at each training iteration, each weight of the network must be incremented
    in the opposite direction of the derivative of ![](img/Formula_B16391_03_074.png)
    in the weight space (Goodfellow I., Bengio Y., Courville A. (2016\. *Deep Learning*,
    MIT Press):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们采用梯度下降策略以达到损失函数表面的最小值，在每次训练迭代中，网络中的每个权重必须在权重空间中按 ![](img/Formula_B16391_03_074.png)
    的偏导数的相反方向增加（Goodfellow I.，Bengio Y.，Courville A.（2016）. *深度学习*，MIT出版社）：
- en: '![](img/Formula_B16391_03_075.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_075.png)'
- en: 'This partial derivative of the error with respect to the weight is calculated
    using the chain rule:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于权重的误差偏导数是使用链式法则计算的：
- en: '![](img/Formula_B16391_03_076.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_076.png)'
- en: 'Here, ![](img/Formula_B16391_03_077.png) is the loss function, ![](img/Formula_B16391_03_078.png)
    the output of neuron j, ![](img/Formula_B16391_03_079.png) its total input, and
    ![](img/Formula_B16391_03_080.png) its input weight from neuron ![](img/Formula_B16391_03_081.png)
    in the previous layer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_077.png) 是损失函数，![](img/Formula_B16391_03_078.png)
    是神经元 j 的输出，![](img/Formula_B16391_03_079.png) 是它的总输入，![](img/Formula_B16391_03_080.png)
    是来自前一层神经元 ![](img/Formula_B16391_03_081.png) 的输入权重：
- en: 'For the weights connecting to units **in the output layer**, the derivatives
    will be as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连接到**输出层单元**的权重，偏导数将如下所示：
- en: '![](img/Formula_B16391_03_082.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_082.png)'
- en: '![](img/Formula_B16391_03_083.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_083.png)'
- en: '![](img/Formula_B16391_03_084.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_084.png)'
- en: 'So, finally:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，最后：
- en: '![](img/Formula_B16391_03_085.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_085.png)'
- en: 'Therefore, the weight change for weights connecting to output units is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，连接到输出单元的权重变化如下：
- en: '![](img/Formula_B16391_03_086.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_086.png)'
- en: Here, ![](img/Formula_B16391_03_087.png), ![](img/Formula_B16391_03_088.png)
    is the input ![](img/Formula_B16391_03_089.png) to the output node ![](img/Formula_B16391_03_072.png),
    and ![](img/Formula_B16391_03_091.png) is the learning rate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_087.png)，![](img/Formula_B16391_03_088.png) 是输入
    ![](img/Formula_B16391_03_089.png) 到输出节点 ![](img/Formula_B16391_03_072.png)，而
    ![](img/Formula_B16391_03_091.png) 是学习率。
- en: For the weights connecting to the **units in a hidden layer**, the calculation
    of the derivative, and therefore of the weight change, is a bit more complicated.
    While the last two derivatives remain the same also when referring to neurons
    in hidden layers, ![](img/Formula_B16391_03_092.png) will need to be recalculated.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连接到**隐藏层单元**的权重，偏导数的计算，因此权重的变化，更加复杂。虽然最后两个偏导数在提到隐藏层的神经元时保持不变，但 ![](img/Formula_B16391_03_092.png)
    需要重新计算。
- en: 'If we consider the loss function ![](img/Formula_B16391_03_074.png) as a function
    of all input sums to all neurons ![](img/Formula_B16391_03_094.png) in the next
    layer connected to neuron j, as ![](img/Formula_B16391_03_095.png), after a few
    math operations we reach a recursive expression:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将损失函数 ![](img/Formula_B16391_03_074.png) 视为所有输入总和的函数，连接到神经元 j 的下一层所有神经元的输入总和为
    ![](img/Formula_B16391_03_095.png)，经过一些数学操作后，我们得到了递归表达式：
- en: '![](img/Formula_B16391_03_096.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_096.png)'
- en: 'Here, the following applies:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，以下内容适用：
- en: '![](img/Formula_B16391_03_097.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_097.png)'
- en: 'The update formula for all weights, leading to output or hidden neurons, is
    this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所有权重的更新公式，指向输出或隐藏神经元，如下所示：
- en: '![](img/Formula_B16391_03_098.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_098.png)'
- en: This recursive formula tells us that ![](img/Formula_B16391_03_099.png) for
    unit ![](img/Formula_B16391_03_100.png) in the hidden layer ![](img/Formula_B16391_03_101.png)
    can be calculated as the linear combination of all ![](img/Formula_B16391_03_102.png)
    in layer ![](img/Formula_B16391_03_103.png), which will be ![](img/Formula_B16391_03_104.png)
    if this is the output layer or ![](img/Formula_B16391_03_105.png) if this is another
    hidden layer. This means that moving from the output layer backward toward the
    input layer, we can calculate all ![](img/Formula_B16391_03_106.png), starting
    from ![](img/Formula_B16391_03_107.png) and then through all ![](img/Formula_B16391_03_108.png),
    as a combination of ![](img/Formula_B16391_03_109.png)from the preceding layer,
    layer after layer. Together with ![](img/Formula_B16391_03_106.png), we can also
    calculate all weight updates ![](img/Formula_B16391_03_111.png).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个递归公式告诉我们，![](img/Formula_B16391_03_099.png) 对于隐藏层中的单位 ![](img/Formula_B16391_03_100.png)
    可以通过层 ![](img/Formula_B16391_03_103.png) 中所有的 ![](img/Formula_B16391_03_102.png)
    的线性组合来计算，如果这是输出层，则为 ![](img/Formula_B16391_03_104.png)，如果这是另一个隐藏层，则为 ![](img/Formula_B16391_03_105.png)。这意味着从输出层向输入层反向传播，我们可以计算所有的
    ![](img/Formula_B16391_03_106.png)，从 ![](img/Formula_B16391_03_107.png) 开始，然后通过所有的
    ![](img/Formula_B16391_03_108.png)，作为来自前一层的 ![](img/Formula_B16391_03_109.png)
    的组合，层层相连。结合 ![](img/Formula_B16391_03_106.png)，我们还可以计算所有的权重更新 ![](img/Formula_B16391_03_111.png)。
- en: The Idea Behind Backpropagation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播背后的理念
- en: 'So, the training of a feedforward neural network can be seen as a two-step
    process:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前馈神经网络的训练可以视为一个两步过程：
- en: All training vectors are presented, one after the other, to the input layer
    of the network, and the signal is propagated throughout all network connections
    (and weights) till the output layer. After all of the training examples have passed
    through the network, the total squared error is calculated at the output layer
    as the sum of the single squared errors. This is the **forward pass**:![Figure
    3.8 – In the forward pass of the backpropagation algorithm, all training examples
    are presented at the input layer and forward-propagated through the network till
    the output layer, to calculate the output values](img/B16391_03_008.jpg)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有训练向量依次呈现给网络的输入层，信号在所有网络连接（和权重）中传播，直到输出层。所有训练样本通过网络后，计算输出层的总平方误差，作为单个平方误差的总和。这就是**前向传播**：![图3.8
    – 在反向传播算法的前向传播中，所有训练样本依次呈现到输入层，并通过网络正向传播直到输出层，以计算输出值](img/B16391_03_008.jpg)
- en: Figure 3.8 – In the forward pass of the backpropagation algorithm, all training
    examples are presented at the input layer and forward-propagated through the network
    till the output layer, to calculate the output values
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.8 – 在反向传播算法的前向传播中，所有训练样本依次呈现到输入层，并通过网络正向传播直到输出层，以计算输出值
- en: 'All ![](img/Formula_B16391_03_112.png) are calculated for all units in the
    output layer. Then, the ![](img/Formula_B16391_03_113.png)s are backpropagated
    from the output layer through all network connections (and weights) till the input
    layer and all ![](img/Formula_B16391_03_114.png) in the hidden layers are also
    calculated. This is the **backward pass**:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有 ![](img/Formula_B16391_03_112.png) 在输出层的所有单位中计算。然后，![](img/Formula_B16391_03_113.png)
    从输出层反向传播通过所有网络连接（和权重）直到输入层，所有隐藏层中的 ![](img/Formula_B16391_03_114.png) 也被计算。这就是**后向传播**：
- en: '![Figure 3.9 – In the backward pass of the backpropagation algorithm, all s
    are calculated at the output layer and backpropagated through the network till
    the input layer. After all examples from the training set have passed through
    the network forth and back, all weights are updated](img/B16391_03_009.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9 – 在反向传播算法的后向传播中，所有 s 在输出层计算，并反向传播通过网络直到输入层。所有训练集的样本来回通过网络后，所有权重都会被更新](img/B16391_03_009.jpg)'
- en: Figure 3.9 – In the backward pass of the backpropagation algorithm, all ![](img/Formula_B16391_03_115.png)s
    are calculated at the output layer and backpropagated through the network till
    the input layer. After all examples from the training set have passed through
    the network forth and back, all weights are updated
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 – 在反向传播算法的后向传播中，所有 ![](img/Formula_B16391_03_115.png) 在输出层计算，并反向传播通过网络直到输入层。所有训练集的样本来回通过网络后，所有权重都会被更新
- en: This algorithm is called **backpropagation**, as a reference to the ![](img/Formula_B16391_03_116.png)s
    backpropagating through the network during the second pass.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法被称为**反向传播**，因为在第二次传递过程中，误差信号沿着网络**反向传播**。
- en: After all the training data has passed through the network forth and back, all
    weights are updated.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练数据通过网络前后传递后，所有权重都会被更新。
- en: Also notice the first derivative of the unit activation function ![](img/Formula_B16391_03_117.png)
    in ![](img/Formula_B16391_03_118.png). Of course, using a continuous derivable
    function ![](img/Formula_B16391_03_119.png) helps with the calculations. This
    is the reason why the ![](img/Formula_B16391_03_120.png) and ![](img/Formula_B16391_03_121.png)
    function have been so popular with neural architectures.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 同时注意单元激活函数的第一导数 ![](img/Formula_B16391_03_117.png) 在 ![](img/Formula_B16391_03_118.png)
    中。显然，使用一个连续可导函数 ![](img/Formula_B16391_03_119.png) 有助于计算。这就是为什么 ![](img/Formula_B16391_03_120.png)
    和 ![](img/Formula_B16391_03_121.png) 函数在神经网络架构中如此受欢迎的原因。
- en: The gradient descent algorithm is not guaranteed to reach the global minimum
    of the error function, but it often ends up in a local minimum. If the local minimum
    does not ensure satisfactory performance of the network, the training process
    must be repeated starting from new initial conditions, meaning new initial values
    for the weights of the network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法无法保证到达误差函数的全局最小值，但通常会停留在局部最小值。如果局部最小值不能确保网络的满意性能，那么必须从新的初始条件开始重新训练，即网络权重的新的初始值。
- en: Neural networks are very powerful in implementing input-output models and very
    flexible in terms of architecture and parameters. It is extremely easy to build
    huge neural networks, by adding more and more neurons and more and more hidden
    layers. Besides the longer training times, an additional risk is to run quickly
    into the **overfitting** of the training data. Overfitting is a drawback of too
    complex models, usually with too many free parameters to fit a simple task. The
    result of an over-dimensioned model for a simple task is that the model, at some
    point, will start using the extra parameters to memorize noise and errors in the
    training set, considerably worsening the model's performance. The power and flexibility
    of neural networks make them prone to overfitting, especially if we are dealing
    with small training sets.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在实现输入输出模型方面非常强大，并且在架构和参数方面非常灵活。通过不断增加更多的神经元和更多的隐藏层，构建庞大的神经网络变得极其容易。除了更长的训练时间外，另一个额外的风险是会迅速陷入**过拟合**训练数据的困境。过拟合是模型过于复杂的缺点，通常是自由参数过多，无法适应一个简单的任务。对于简单任务的过度维度化模型，结果是模型在某些时候会开始利用额外的参数来记忆训练集中的噪声和错误，从而显著降低模型的表现。神经网络的强大和灵活性使它们容易过拟合，特别是当我们处理的是小型训练集时。
- en: Important note
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Another big objection that has been leveled against neural networks since their
    introduction is their non-interpretability. The adjustment of the weights has
    no correspondence with any entity in the data domain. When dealing with neural
    networks, we need to accept that we are dealing with **black boxes** and we might
    not understand the decision process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 自神经网络引入以来，另一个主要的反对意见是它们的不可解释性。权重的调整与数据领域中的任何实体没有对应关系。在处理神经网络时，我们需要接受一个事实，那就是我们正在处理**黑箱**，我们可能无法理解其决策过程。
- en: If interpretability is a requirement for our project, then maybe neural networks
    are not the tool for us. A few techniques have been proposed recently to extract
    knowledge on the decision process followed in black-box models, such as the **SHAPLEY**
    values or **Partial Dependency Plots** (Molnar C. *Interpretable Machine Learning*,
    https://christophm.github.io/interpretable-ml-book/index.html, GitHub). They are
    currently in their infancy and not immune from criticism. However, they constitute
    an interesting attempt to fix the interpretability problem of neural networks.
    These are beyond the scope of this book, so we will not be exploring them in any
    more detail.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可解释性是我们项目的需求，那么神经网络可能不是我们应选择的工具。最近提出了一些技术，用于从黑箱模型中提取决策过程的知识，例如**SHAPLEY**值或**部分依赖图**（Molnar
    C. *可解释的机器学习*，https://christophm.github.io/interpretable-ml-book/index.html，GitHub）。它们目前处于初期阶段，尚未免于批评。然而，它们是解决神经网络可解释性问题的一个有趣尝试。这些内容超出了本书的范围，因此我们不会深入探讨。
- en: With the basic theory covered, let's get into the design of a network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基本理论已经介绍完，接下来让我们进入网络设计的部分。
- en: Designing your Network
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计你的网络
- en: In the previous section, we learned that neural networks are characterized by
    a topology, weights, and activation functions. In particular, feedforward neural
    networks have an input and an output layer, plus a certain number of hidden layers
    in between. While the values for the network weights are automatically estimated
    via the training procedure, the network topology and the activation functions
    have to be predetermined during network design before training. Different network
    architectures and different activation functions implement different input-output
    tasks. Designing the appropriate neural architecture for a given task is still
    an active research field in the deep learning area (Goodfellow I., Bengio Y.,
    Courville A. (2016). *Deep Learning*, MIT Press).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习到神经网络的特点包括拓扑结构、权重和激活函数。特别是，前馈神经网络有输入层和输出层，中间还包含若干个隐藏层。尽管网络权重的值是通过训练过程自动估算的，但网络的拓扑结构和激活函数必须在训练前的网络设计阶段预先确定。不同的网络架构和不同的激活函数实现了不同的输入输出任务。为特定任务设计合适的神经网络架构仍然是深度学习领域的一个活跃研究领域（Goodfellow
    I.、Bengio Y.、Courville A.（2016）。*深度学习*，麻省理工学院出版社）。
- en: Other parameters are involved in the training algorithm of neural networks,
    such as the learning rate or the loss function. We have also seen that neural
    networks are prone to overfitting; this means that their flexibility makes it
    easy for them to run into the overfitting problem. Would it be possible to contain
    the weight growth, to change the loss function, or to self-limit the network structure
    during training as to avoid the overfitting problem?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法中还涉及其他参数，如学习率或损失函数。我们也看到，神经网络容易出现过拟合问题；这意味着其灵活性使得它们很容易遭遇过拟合问题。是否有可能在训练过程中通过控制权重增长、更改损失函数或自我限制网络结构来避免过拟合问题呢？
- en: 'This section gives you an overview of all those remaining parameters: the topology
    of the network, the parameters in the training algorithm, the possible activation
    functions, the loss functions, regularization terms, and more, always keeping
    an eye on containing the overfitting effect, making the training algorithm more
    efficient, and developing more powerful neural architectures.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节为您概述了所有剩余的参数：网络的拓扑结构、训练算法中的参数、可能的激活函数、损失函数、正则化项等，始终关注控制过拟合效应，使训练算法更加高效，并开发更强大的神经网络架构。
- en: Commonly Used Activation Functions
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常用激活函数
- en: 'In summary, a single neural layer has a number of inputs ![](img/Formula_B16391_03_122.png)
    and a number of outputs ![](img/Formula_B16391_03_123.png) The calculation of
    the output value of a neuron ![](img/Formula_B16391_03_124.png) is performed in
    two steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，单个神经层有多个输入 ![](img/Formula_B16391_03_122.png) 和多个输出 ![](img/Formula_B16391_03_123.png)。神经元输出值的计算
    ![](img/Formula_B16391_03_124.png) 需要经过两步：
- en: 'Calculation of the weighted sum of the inputs plus a bias ![](img/Formula_B16391_03_125.png):'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输入的加权和加上偏置 ![](img/Formula_B16391_03_125.png)：
- en: '![](img/Formula_B16391_03_126.png) for ![](img/Formula_B16391_03_127.png) and
    ![](img/Formula_B16391_03_128.png)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_126.png) 对于 ![](img/Formula_B16391_03_127.png) 和
    ![](img/Formula_B16391_03_128.png)'
- en: 'Application of an activation function ![](img/Formula_B16391_03_129.png) or
    ![](img/Formula_B16391_03_130.png)to calculate the output ![](img/Formula_B16391_03_131.png)
    based on the weight matrix ![](img/Formula_B16391_03_132.png) and either on ![](img/Formula_B16391_03_133.png)
    or on ![](img/Formula_B16391_03_134.png):'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用激活函数 ![](img/Formula_B16391_03_129.png) 或 ![](img/Formula_B16391_03_130.png)
    来计算输出 ![](img/Formula_B16391_03_131.png)，根据权重矩阵 ![](img/Formula_B16391_03_132.png)，并且可能是
    ![](img/Formula_B16391_03_133.png) 或 ![](img/Formula_B16391_03_134.png)：
- en: '![](img/Formula_B16391_03_135.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_135.png)'
- en: Note that ![](img/Formula_B16391_03_136.png)is the weighted sum of the input
    values to the ![](img/Formula_B16391_03_137.png) th neuron and ![](img/Formula_B16391_03_138.png)
    is the vector of all weighted input sums.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![](img/Formula_B16391_03_136.png) 是输入值到 ![](img/Formula_B16391_03_137.png)
    号神经元的加权和，而 ![](img/Formula_B16391_03_138.png) 是所有加权输入和的向量。
- en: A network can then also be seen as a chain of functions ![](img/Formula_B16391_03_139.png),
    where each function implements a neural layer. Depending on the network architecture,
    each neural layer has different input values and uses a different activation function
    ![](img/Formula_B16391_03_140.png), and therefore implements a different function
    ![](img/Formula_B16391_03_141.png), using the two calculation steps described
    previously.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 网络也可以看作是一系列函数![](img/Formula_B16391_03_139.png)，每个函数实现一个神经层。根据网络架构，每个神经层有不同的输入值，并使用不同的激活函数![](img/Formula_B16391_03_140.png)，因此实现一个不同的函数![](img/Formula_B16391_03_141.png)，并且使用前面描述的两个计算步骤。
- en: The complexity of the total function implemented by the full network also depends
    on the number of layers involved; that is, it depends on the network depth.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 完整网络实现的总函数的复杂度还取决于涉及的层数；也就是说，它取决于网络的深度。
- en: A layer where all neurons are connected to all outputs of the previous layer
    is called a **dense layer**. Fully connected feedforward networks are just a chain
    of dense layers, where each layer has its own activation function. In feedforward
    neural networks, then, a function ![](img/Formula_B16391_03_142.png) is based
    on the number of the layer's neurons, the number of inputs, and the activation
    function. The key difference between layers is then the activation function. Let's
    look at the most commonly used activation functions in neural networks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有神经元都与上一层的所有输出连接的层称为**全连接层**。全连接前馈网络仅仅是一系列的全连接层，每一层都有其自己的激活函数。在前馈神经网络中，函数![](img/Formula_B16391_03_142.png)取决于层中神经元的数量、输入的数量以及激活函数。层之间的关键区别就是激活函数。让我们来看一下神经网络中最常用的激活函数。
- en: Sigmoid Function
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The **sigmoid function** is an S-shaped function with values between ![](img/Formula_B16391_03_143.png)
    and ![](img/Formula_B16391_03_144.png). For the ![](img/Formula_B16391_03_137.png)th
    neuron in the layer, the function is defined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid函数**是一个S形函数，其值介于![](img/Formula_B16391_03_143.png)和![](img/Formula_B16391_03_144.png)之间。对于层中的第![](img/Formula_B16391_03_137.png)个神经元，该函数定义如下：'
- en: '![](img/Formula_B16391_03_146.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_146.png)'
- en: It is plotted on the left in *Figure 3.10*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它在*图3.10*中左侧绘制。
- en: 'For binary classification problems, this is the go-to function for the output
    neural layer, as the value range ![](img/Formula_B16391_03_147.png) allows us
    to interpret the output as the probability of one of the two classes. In this
    case, the output neural layer consists of only one neuron, AKA unit size 1, with
    the sigmoidal activation function. Of course, the same function can also be used
    as an activation function for output and hidden layers with a bigger unit size:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，这个函数是输出神经层的首选函数，因为其值范围![](img/Formula_B16391_03_147.png)使我们能够将输出解释为两个类别之一的概率。在这种情况下，输出神经层仅包含一个神经元，即单元大小为1，并且使用Sigmoid激活函数。当然，相同的函数也可以作为具有更大单元大小的输出层和隐藏层的激活函数。
- en: '![Figure 3.10 – The sigmoid function (on the left) can be used as the activation
    function of the single output neuron of a network implementing the solution to
    a binary classification problem (in the center). It can be used generically as
    an activation function for neurons placed in hidden or output layers in a network
    (on the right)](img/B16391_03_010.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图3.10 – Sigmoid函数（左侧）可以作为解决二分类问题的网络单输出神经元的激活函数（中间）。它也可以作为网络中隐藏层或输出层中神经元的激活函数（右侧）](img/B16391_03_010.jpg)'
- en: Figure 3.10 – The sigmoid function (on the left) can be used as the activation
    function of the single output neuron of a network implementing the solution to
    a binary classification problem (in the center). It can be used generically as
    an activation function for neurons placed in hidden or output layers in a network
    (on the right)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 – Sigmoid函数（左侧）可以作为解决二分类问题的网络单输出神经元的激活函数（中间）。它也可以作为网络中隐藏层或输出层中神经元的激活函数（右侧）
- en: One of the biggest advantages of the sigmoid function is its derivability everywhere
    and its easy derivative expression. Indeed, when using the sigmoid activation
    function, the weight update rule for the backpropagation algorithm becomes very
    simple, since the first derivative of the activation function is simply ![](img/Formula_B16391_03_148.png),
    where ![](img/Formula_B16391_03_149.png) is the output of neuron j.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数的最大优点之一是它在任何地方都可导，并且具有简单的导数表达式。事实上，当使用sigmoid激活函数时，反向传播算法的权重更新规则变得非常简单，因为激活函数的导数只是![](img/Formula_B16391_03_148.png)，其中![](img/Formula_B16391_03_149.png)是神经元j的输出。
- en: On the other hand, one of the biggest disadvantages of using sigmoid as the
    neurons' activation function in more complex or deep neural architectures is the
    vanishing gradient problem. Indeed, when calculating the derivatives to update
    the network weights, the chain multiplication of output values (< 1) from sigmoid
    functions might produce very small values. In this case, too small gradients are
    produced at each training iteration, leading to slow convergence for the training
    algorithm.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在更复杂或深度的神经网络架构中，使用sigmoid作为神经元激活函数的最大缺点之一是梯度消失问题。事实上，在计算导数以更新网络权重时，sigmoid函数输出值（<
    1）的链式乘法可能会产生非常小的值。在这种情况下，每次训练迭代时会产生过小的梯度，导致训练算法收敛缓慢。
- en: Hyperbolic Tangent (Tanh)
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切（Tanh）
- en: 'A similar activation function is **hyperbolic tangent**, **tanh** for short.
    It is also an S-shaped function with the difference that the output values fall
    between ![](img/Formula_B16391_03_150.png) and 1, instead of between ![](img/Formula_B16391_03_151.png)
    and ![](img/Formula_B16391_03_152.png) For the ![](img/Formula_B16391_03_124.png)
    th neuron, the function is defined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一种类似的激活函数是**双曲正切**，简称**tanh**。它也是一个S型函数，不同之处在于输出值介于![](img/Formula_B16391_03_150.png)和1之间，而不是介于![](img/Formula_B16391_03_151.png)和![](img/Formula_B16391_03_152.png)之间。对于![](img/Formula_B16391_03_124.png)号神经元，该函数定义如下：
- en: '![](img/Formula_B16391_03_154.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_154.png)'
- en: 'It is plotted on the left in *Figure 3.11*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 它在*图3.11*中显示在左侧：
- en: '![Figure 3.11 – The hyperbolic tangent function, tanh(), is also often used
    as an activation function for neural units. In this case, the neuron output value
    falls in (-1, +1)](img/B16391_03_011.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11 – 双曲正切函数tanh()也常用作神经单元的激活函数。在这种情况下，神经元的输出值位于(-1, +1)之间](img/B16391_03_011.jpg)'
- en: Figure 3.11 – The hyperbolic tangent function, tanh(), is also often used as
    an activation function for neural units. In this case, the neuron output value
    falls in (-1, +1)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 – 双曲正切函数tanh()也常用作神经单元的激活函数。在这种情况下，神经元的输出值位于(-1, +1)之间
- en: Here also, one of the biggest advantages of the ![](img/Formula_B16391_03_155.png)
    function is its continuity and its derivability everywhere, which leads to simpler
    formulas for the updates of the weights in the training algorithm. ![](img/Formula_B16391_03_156.png)
    also has the advantage of being centered at 0, which can help to stabilize the
    training process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B16391_03_155.png)函数的最大优点之一是其连续性和在任何地方都可导，这使得训练算法中权重更新的公式更为简单。![](img/Formula_B16391_03_156.png)还具有以0为中心的优势，这有助于稳定训练过程。
- en: Again, one of the biggest disadvantages of using tanh as an activation function
    in complex or deep neural architectures is the vanishing gradient problem.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，使用tanh作为复杂或深度神经网络架构中的激活函数的最大缺点之一是梯度消失问题。
- en: Linear Function
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性函数
- en: 'A special activation function is the **linear activation function**, also known
    as the identity function:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊的激活函数是**线性激活函数**，也称为恒等函数：
- en: '![](img/Formula_B16391_03_157.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_157.png)'
- en: 'When would such a function be used? A neural layer with a linear activation
    function implements a linear regression model. Sometimes, a neural layer with
    a linear activation function is also introduced to keep the original network response,
    before it is transformed to get the required range or probability score. In this
    case, the last layer of the network is split into two layers: one with the linear
    activation function preserves the original output and the other one applies another
    activation function for the required output format.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种函数通常在什么情况下使用？具有线性激活函数的神经层实现了线性回归模型。有时，线性激活函数的神经层也会被引入，以保持原始网络的响应，然后再经过变换以获得所需的范围或概率得分。在这种情况下，网络的最后一层被分为两层：一层使用线性激活函数保留原始输出，另一层则应用另一种激活函数以获得所需的输出格式。
- en: In [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing
    NLP Applications*, where we describe the *Generating Product Name* case study,
    this approach is used to introduce a new parameter called **temperature** after
    the linear activation function layer.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第七章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)中，*实现 NLP 应用*，我们描述了*生成产品名称*的案例研究，在该方法中，在线性激活函数层之后引入了一个新参数，称为**温度**。
- en: Rectified Linear Unit
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修正线性单元
- en: We have seen that deep neural networks, using the sigmoid or tanh activation
    functions, often suffer from the problem of vanishing gradient.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，使用 sigmoid 或 tanh 激活函数的深度神经网络通常会遭遇梯度消失问题。
- en: 'An activation function that helps to overcome the problem of vanishing gradient
    is the **Rectified Linear Unit** function, **ReLU** for short. The ReLU function
    is like the linear function, at least from 0 on. Indeed, the ReLU function is
    ![](img/Formula_B16391_03_158.png) for negative values of ![](img/Formula_B16391_03_159.png)
    and is the identity function for positive values of ![](img/Formula_B16391_03_159.png):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有助于克服梯度消失问题的激活函数是**修正线性单元**函数，简称**ReLU**。ReLU 函数从 0 起像线性函数一样工作。实际上，ReLU 函数对于负值的
    ![](img/Formula_B16391_03_159.png) 是 ![](img/Formula_B16391_03_158.png)，而对于正值的
    ![](img/Formula_B16391_03_159.png)，它是恒等函数：
- en: '![](img/Formula_B16391_03_161.png).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B16391_03_161.png)。'
- en: '*Figure 3.12* shows a plot of the ReLU function:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.12* 显示了 ReLU 函数的图形：'
- en: '![Figure 3.12 – The ReLU activation function](img/B16391_03_012.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.12 – ReLU 激活函数](img/B16391_03_012.jpg)'
- en: Figure 3.12 – The ReLU activation function
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 – ReLU 激活函数
- en: The ReLU activation function, while helping with the vanishing gradient problem,
    is not differentiable for ![](img/Formula_B16391_03_162.png). In practice, this
    is not a problem when training neural networks as usually one of the one-sided
    derivatives is used rather than reporting that the derivative is not defined.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活函数虽然能帮助解决梯度消失问题，但对于 ![](img/Formula_B16391_03_162.png) 是不可微分的。实际上，在训练神经网络时，这通常不是问题，因为通常使用的是单边导数，而不是报告导数未定义。
- en: Softmax Function
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 函数
- en: All activation functions introduced until now are functions that have a single
    value as output. This means only the weighted sum ![](img/Formula_B16391_03_163.png)
    is used to calculate the output value of the ![](img/Formula_B16391_03_164.png)th
    neuron, independently from weighted sums ![](img/Formula_B16391_03_165.png), with
    ![](img/Formula_B16391_03_166.png) being used to calculate the outputs of the
    other neurons in the same layer. The **softmax function**, on the other hand,
    works on the whole output vector ![](img/Formula_B16391_03_167.png) and not just
    on one single value ![](img/Formula_B16391_03_168.png).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止介绍的所有激活函数都是具有单一输出值的函数。这意味着仅使用加权和 ![](img/Formula_B16391_03_163.png) 来计算第
    ![](img/Formula_B16391_03_164.png) 个神经元的输出值，而与加权和 ![](img/Formula_B16391_03_165.png)
    无关，后者用于计算同一层中其他神经元的输出值。而**softmax 函数**则作用于整个输出向量 ![](img/Formula_B16391_03_167.png)，而不仅仅是单一的值
    ![](img/Formula_B16391_03_168.png)。
- en: 'In general, the softmax function transforms a vector ![](img/Formula_B16391_03_169.png)
    of size ![](img/Formula_B16391_03_170.png) into a vector ![](img/Formula_B16391_03_171.png),
    which is a vector ![](img/Formula_B16391_03_172.png) of the same size ![](img/Formula_B16391_03_173.png)
    with values between ![](img/Formula_B16391_03_158.png) and ![](img/Formula_B16391_03_175.png),
    with the constraint that all values ![](img/Formula_B16391_03_176.png) sum to
    ![](img/Formula_B16391_03_175.png):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，softmax 函数将大小为 ![](img/Formula_B16391_03_170.png) 的向量 ![](img/Formula_B16391_03_169.png)
    转换为大小为 ![](img/Formula_B16391_03_173.png) 的向量 ![](img/Formula_B16391_03_171.png)，该向量的值介于
    ![](img/Formula_B16391_03_158.png) 和 ![](img/Formula_B16391_03_175.png) 之间，并且所有值
    ![](img/Formula_B16391_03_176.png) 的和为 ![](img/Formula_B16391_03_175.png)：
- en: '![](img/Formula_B16391_03_178.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_178.png)'
- en: 'This additional constraint allows us to interpret the components of vector
    ![](img/Formula_B16391_03_179.png) as probabilities of different classes. Therefore,
    the softmax activation function is often the function of choice for the last neural
    layer in a multiclass classification problem. The ![](img/Formula_B16391_03_180.png)th
    element of the output vector is calculated as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个额外的约束使我们能够将向量 ![](img/Formula_B16391_03_179.png) 的各个分量解释为不同类别的概率。因此，softmax
    激活函数通常是多类分类问题中最后一个神经层的函数选择。输出向量的 ![](img/Formula_B16391_03_180.png) 位置的元素通过以下方式计算：
- en: '![](img/Formula_B16391_03_181.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_181.png)'
- en: '*Figure 3.13* shows an example network that uses the softmax function in the
    last layer, where all output values sum up to 1:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.13* 显示了一个使用 softmax 函数作为最后一层的示例网络，其中所有输出值的总和为 1：'
- en: '![Figure 3.13 – A simple neural layer with the softmax activation function](img/B16391_03_013.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13 – 一个简单的神经层，使用 softmax 激活函数](img/B16391_03_013.jpg)'
- en: Figure 3.13 – A simple neural layer with the softmax activation function
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 – 一个简单的神经层，使用 softmax 激活函数
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The softmax function is also used by the logistic regression algorithm for multiclass
    classification problems.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数也被逻辑回归算法用于多类分类问题。
- en: Other supported activation functions
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他支持的激活函数
- en: Many more activation functions have been introduced over the years, since the
    sigmoid.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 自 sigmoid 函数以来，很多其他激活函数相继被提出。
- en: Variants of ReLU are **Leaky Rectified Linear Unit** and **Parametric Rectified
    Linear Unit** (**PReLU**). LeakyReLU offers an almost zero line (![](img/Formula_B16391_03_182.png))
    for negative values of the function argument rather than just zero as in the pure
    ReLU. PReLU makes this line with parametric slope (![](img/Formula_B16391_03_183.png))
    rather than fixed slope as in the LeakyReLU. Parameter ![](img/Formula_B16391_03_184.png)
    becomes part of the parameters that the network must train.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的变体包括 **泄露修正线性单元** 和 **参数化修正线性单元** (**PReLU**) 。LeakyReLU 对函数的负值部分提供了一个接近零的线条
    (![](img/Formula_B16391_03_182.png))，而纯 ReLU 则仅为零。PReLU 用参数化的斜率 (![](img/Formula_B16391_03_183.png))
    代替 LeakyReLU 中固定的斜率来处理该线。参数 ![](img/Formula_B16391_03_184.png) 成为网络必须训练的参数之一。
- en: 'Here are the definitions of LeakyReLU and PreLU:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 LeakyReLU 和 PreLU 的定义：
- en: 'LeakyReLU: ![](img/Formula_B16391_03_185.png)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeakyReLU: ![](img/Formula_B16391_03_185.png)'
- en: 'PReLU: ![](img/Formula_B16391_03_186.png)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PReLU: ![](img/Formula_B16391_03_186.png)'
- en: 'Other variants of ReLU, introduced to remedy dead ReLUs, are **Exponential
    Linear Unit** (**ELU**) and **Scaled Exponential Linear Unit** (**SELU**). Similar
    to LeakyReLU, ELU has a small slope for negative values. Instead of a straight
    line, it uses a log curve. Scaled ELU adds one more parameter ![](img/Formula_B16391_03_187.png)
    to ELU for the network to train:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其他 ReLU 的变体，包括为了修复死 ReLU 引入的 **指数线性单元** (**ELU**) 和 **缩放指数线性单元** (**SELU**)。类似于
    LeakyReLU，ELU 对负值有一个小斜率。它使用对数曲线而不是直线。缩放 ELU 向 ELU 添加了一个额外的参数 ![](img/Formula_B16391_03_187.png)，以供网络训练：
- en: 'ELU: ![](img/Formula_B16391_03_188.png)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ELU: ![](img/Formula_B16391_03_188.png)'
- en: 'SELU: ![](img/Formula_B16391_03_189.png)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SELU: ![](img/Formula_B16391_03_189.png)'
- en: 'An approximation of the sigmoid activation function is the **hard sigmoid activation
    function**. It is faster to calculate than sigmoid. Despite being an approximation
    of the sigmoid activation function, it still provides reasonable results on classification
    tasks. However, since it''s just an approximation, it performs worse on regression
    tasks:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 激活函数的一个近似是 **硬 sigmoid 激活函数**。它的计算速度比 sigmoid 快。尽管它是 sigmoid 激活函数的近似，但在分类任务中仍然能够提供合理的结果。然而，由于它只是近似，因此在回归任务中的表现较差：
- en: 'Hard-Sigmoid: ![](img/Formula_B16391_03_190.png)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hard-Sigmoid: ![](img/Formula_B16391_03_190.png)'
- en: 'The **SoftPlus** activation function is also quite popular. This is a smoothed
    version of the ReLU activation function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**SoftPlus** 激活函数也相当流行。它是 ReLU 激活函数的平滑版本：'
- en: 'SoftPlus: ![](img/Formula_B16391_03_191.png)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SoftPlus: ![](img/Formula_B16391_03_191.png)'
- en: 'Let''s look at *Figure 3.14*:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下*图 3.14*：
- en: '![Figure 3.14 – Plots of some additional popular activation functions, mainly
    variants of ReLU and sigmoid functions](img/B16391_03_014.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.14 – 一些额外流行的激活函数的图示，主要是 ReLU 和 sigmoid 函数的变体](img/B16391_03_014.jpg)'
- en: Figure 3.14 – Plots of some additional popular activation functions, mainly
    variants of ReLU and sigmoid functions
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 – 一些额外流行的激活函数的图示，主要是 ReLU 和 sigmoid 函数的变体
- en: The images in *Figure 3.14* show the plots of the aforementioned activation
    functions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.14* 中的图片显示了上述激活函数的图示。'
- en: Regularization Techniques to Avoid Overfitting
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免过拟合的正则化技术
- en: No matter what kind of algorithm you use, the goal is always a model that not
    only performs well on the training data but also generalizes to new data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用什么算法，目标始终是构建一个不仅在训练数据上表现良好，而且能够对新数据进行良好泛化的模型。
- en: 'Large neural networks, trained on too small datasets, often incur the problem
    of fitting the training data too well and missing the capability to generalize
    to new data. This problem is known as overfitting. *Figure 3.15* shows a regression
    input-output function implemented by a neural network on the training data (full
    crosses) and on the test data (empty crosses). On the left, we see a regression
    function that does not even manage to fit the training data properly, much less
    the test data. This is probably due to an insufficient architecture size or short
    training time (**underfitting**). In the center, we find a regression curve decently
    fitting both training and test data. On the right, we have a regression curve
    fitting the training data perfectly and failing in the fit on the test data; this
    is the **overfitting** problem:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 大型神经网络在使用过小的数据集进行训练时，常常会出现过度拟合训练数据的问题，无法对新数据进行有效的泛化。这个问题被称为过拟合。*图 3.15* 显示了神经网络在训练数据（满点）和测试数据（空心点）上的回归输入输出函数。在左侧，我们看到一个回归函数，甚至无法正确拟合训练数据，更不用说拟合测试数据了。这可能是由于架构规模不足或训练时间太短（**欠拟合**）。在中间，我们看到一个合理拟合训练数据和测试数据的回归曲线。在右侧，回归曲线完美拟合训练数据，但在拟合测试数据时失败；这就是**过拟合**问题：
- en: '![Figure 3.15 – From left to right, the regression curve implemented by a network
    underfitting, fitting just fine, and overfitting the training data](img/B16391_03_015.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.15 – 从左到右，网络在欠拟合、拟合适当和过拟合训练数据时的回归曲线](img/B16391_03_015.jpg)'
- en: Figure 3.15 – From left to right, the regression curve implemented by a network
    underfitting, fitting just fine, and overfitting the training data
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 – 从左到右，网络在欠拟合、拟合适当和过拟合训练数据时的回归曲线
- en: 'How can we know in advance the right size of the neural architecture and the
    right number of epochs for the training algorithm? A few tricks can be adopted
    to address the problem of overfitting without worrying too much about the exact
    size of the network and the number of epochs: norm regularization, dropout, and
    early stopping.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何事先知道神经网络架构的合适大小以及训练算法的合适训练轮数呢？有一些技巧可以用来解决过拟合问题，而无需过于担心网络的精确大小和训练轮数：范数正则化、丢弃法（dropout）和早停法（early
    stopping）。
- en: Norm Regularization
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 范数正则化
- en: 'One sign of overfitting is the high values of the weights. Thus, the idea behind
    norm regularization is to penalize weights with high values by adding a penalty
    term ![](img/Formula_B16391_03_192.png) to the objective function, AKA the loss
    function:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合的一个迹象是权重的值过高。因此，范数正则化的基本思想是通过在目标函数中添加惩罚项 ![](img/Formula_B16391_03_192.png)
    来惩罚权重较大的值，即损失函数：
- en: '![](img/Formula_B16391_03_193.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_193.png)'
- en: 'Here, ![](img/Formula_B16391_03_194.png) are the true values and ![](img/Formula_B16391_03_195.png)
    are the predicted values. A new loss function ![](img/Formula_B16391_03_196.png)
    is obtained with the addition of this penalty term:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_194.png) 是真实值，![](img/Formula_B16391_03_195.png)
    是预测值。通过添加这个惩罚项，得到一个新的损失函数 ![](img/Formula_B16391_03_196.png)：
- en: '![](img/Formula_B16391_03_197.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_197.png)'
- en: The training algorithm, thus, while minimizing this new loss function, will
    reach a weight configuration with smaller values. This is a well-known **regularization**
    approach you might already know from the linear or logistic regression algorithms.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练算法在最小化这个新的损失函数时，会得到一个具有较小权重值的权重配置。这是一种广为人知的**正则化**方法，你可能已经在线性或逻辑回归算法中见过。
- en: The parameter ![](img/Formula_B16391_03_198.png) is used to control the penalty
    effect. ![](img/Formula_B16391_03_199.png)is equivalent to no regularization.
    Higher values of ![](img/Formula_B16391_03_200.png) implement a stronger regularization
    effect and lead to smaller weights.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 ![](img/Formula_B16391_03_198.png) 用于控制惩罚效应。![](img/Formula_B16391_03_199.png)
    等同于没有正则化。较高的 ![](img/Formula_B16391_03_200.png) 值实现了更强的正则化效果，并导致较小的权重值。
- en: 'There are two commonly used penalty norm functions: the **L1 norm** and the
    **L2 norm**. The L1 norm is the sum of the absolute values of the weights and
    the L2 norm is the sum of the squares of the weights:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种常用的惩罚范数函数：**L1 范数** 和 **L2 范数**。L1 范数是权重绝对值的总和，而 L2 范数是权重平方的总和：
- en: '![](img/Formula_B16391_03_201.png) ![](img/Formula_B16391_03_202.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_201.png) ![](img/Formula_B16391_03_202.png)'
- en: '![](img/Formula_B16391_03_203.png) and ![](img/Formula_B16391_03_204.png) are
    both common methods to avoid overfitting with one big difference. ![](img/Formula_B16391_03_205.png)
    regularization generally leads to smaller weights but lacks the ability to reduce
    the weights all the way to zero. On the other hand, ![](img/Formula_B16391_03_206.png)
    regularization allows for a few larger weights while reducing all other weights
    to zero. When designing a loss function, it is also possible to use a mixture
    of both ![](img/Formula_B16391_03_206.png) and ![](img/Formula_B16391_03_208.png)
    regularization.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B16391_03_203.png) 和 ![](img/Formula_B16391_03_204.png) 都是避免过拟合的常见方法，且有一个显著的区别。![](img/Formula_B16391_03_205.png)
    正则化通常会导致较小的权重，但无法将权重减少到零。而另一方面，![](img/Formula_B16391_03_206.png) 正则化允许有一些较大的权重，同时将其他所有权重减小到零。在设计损失函数时，也可以同时使用
    ![](img/Formula_B16391_03_206.png) 和 ![](img/Formula_B16391_03_208.png) 正则化。'
- en: 'In addition, you can also apply regularization terms to weights of selected
    layers. Three different norm regularizations have been designed to act on single
    layers: **kernel regularization**, **bias regularization**, and **activity regularization**.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以将正则化项应用于选定层的权重。已经设计了三种不同的范数正则化来作用于单一层：**核正则化**、**偏置正则化**和**活动正则化**。
- en: Kernel regularization penalizes the weights, but not the biases; bias regularization
    penalizes the biases only; and activity regularization leads to smaller output
    values for the selected layer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 核正则化惩罚权重，而不惩罚偏置；偏置正则化仅惩罚偏置；活动正则化导致选定层的输出值更小。
- en: Dropout
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dropout
- en: Another common approach in machine learning to avoid overfitting is to introduce
    the dropout technique, which is another regularization technique.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的机器学习方法来避免过拟合是引入 Dropout 技术，这也是一种正则化技术。
- en: The idea is, at each training iteration, to ignore (drop) randomly some of the
    neurons in either the input layer or a hidden layer with all its input and output
    connections. At each iteration, different neurons are dropped. Therefore, the
    number of neurons in the architecture, and which of them are trained, effectively
    changes from iteration to iteration. The randomization introduced in this way
    helps to control the overfitting effect.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想是，在每次训练迭代中，随机忽略（丢弃）输入层或隐含层中的一些神经元，连同其所有的输入和输出连接。在每次迭代中，丢弃的神经元不同。因此，架构中的神经元数量以及哪些神经元被训练，实际上会随着迭代而变化。通过这种方式引入的随机化有助于控制过拟合效应。
- en: 'Dropout makes sure that individual neurons and layers do not rely on single
    neurons in the preceding layers, thus becoming more robust and less prone to overfitting:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout确保每个神经元和每一层不依赖于前一层的单个神经元，从而变得更加健壮，减少过拟合的可能性：
- en: '![Figure 3.16 – The dropout technique selects some neurons in each layer and
    drops them from being updated in the current training iteration. The full network
    on the left is trained only partially in the four training iterations described
    on the right.](img/B16391_03_016.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图3.16 – Dropout 技术在每一层选择一些神经元，并在当前训练迭代中将其从更新中排除。左侧的完整网络只在右侧描述的四个训练迭代中部分训练。](img/B16391_03_016.jpg)'
- en: Figure 3.16 – The dropout technique selects some neurons in each layer and drops
    them from being updated in the current training iteration. The full network on
    the left is trained only partially in the four training iterations described on
    the right.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 – Dropout 技术在每一层选择一些神经元，并在当前训练迭代中将其从更新中排除。左侧的完整网络只在右侧描述的四个训练迭代中部分训练。
- en: '**Dropout** is applied to each layer of the network separately. This often
    translates into a temporary layer, the dropout layer, being inserted after the
    layer we want to randomize. The dropout layer controls how many neurons of the
    previous layer are dropped at each training iteration.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dropout** 是分别应用于网络的每一层的。这通常意味着会在我们希望进行随机化的层后插入一个临时层，即 Dropout 层。Dropout 层控制在每次训练迭代中前一层的多少神经元被丢弃。'
- en: 'To control how many neurons in a layer are dropped, a new parameter is introduced:
    the **drop rate**. The drop rate defines the fraction of neurons in the layer
    that should be dropped from training at each iteration.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制每一层中丢弃多少神经元，引入了一个新的参数：**丢弃率**。丢弃率定义了每次迭代中应该丢弃的神经元在该层中的比例。
- en: Tip
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'Here are two quick tips for dropout:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个关于 Dropout 的小贴士：
- en: First, dropout leads to layers with fewer neurons and therefore reduces the
    layer capacity. It is recommended to start with a high number of neurons per layer.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，丢弃层会导致每层神经元减少，从而降低层的容量。建议从每层神经元数较多的设置开始。
- en: Second, dropout is only applied to the input or hidden layers, not to the output
    layer since we want the response of the model to always be the same at each iteration.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，丢弃层只应用于输入层或隐藏层，而不是输出层，因为我们希望模型的响应在每次迭代中始终相同。
- en: Early Stopping
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 早停法
- en: Another option to avoid overfitting is to stop the training process before the
    network starts overfitting, which is known as **early stopping**. To detect the
    point where the algorithm starts to fit the training data better than the test
    data, an additional validation set with new data is used. During training, the
    network performances are monitored on both the training set and the validation
    set. At the beginning of the training phase, the network performance on both the
    training and validation sets improves. At some point, though, the performance
    of the network on the training set keeps improving while on the validation set
    it starts deteriorating. Once the performance starts to get worse on the validation
    set, the training is stopped.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种避免过拟合的选项是在网络开始过拟合之前停止训练过程，这被称为**早停法**。为了检测算法开始比测试数据更好地拟合训练数据的时刻，会使用额外的验证集来提供新的数据。在训练过程中，会监控网络在训练集和验证集上的表现。在训练阶段开始时，网络在训练集和验证集上的表现都会有所提升。然而，在某个时刻，网络在训练集上的表现持续改善，而在验证集上的表现开始恶化。一旦验证集上的表现开始变差，训练便会停止。
- en: Other Commonly used Layers
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他常用的层
- en: 'So far, we have introduced two different kinds of layers: dense layers to design
    fully connected neural networks with different activation functions and the dropout
    layer for regularization. With these layers, you can design, for example, an autoencoder,
    as we will do in [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152),
    *Autoencoder for Fraud Detection*. But actually, there are many more layers available
    for all kinds of different tasks.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了两种不同类型的层：用于设计具有不同激活函数的全连接神经网络的密集层和用于正则化的丢弃层。通过这些层，你可以设计一个自动编码器，正如我们在[*第5章*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152)中所做的，*用于欺诈检测的自动编码器*。但实际上，还有许多其他层可以用于各种不同的任务。
- en: Convolutional Layers
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: One area where neural networks are extremely powerful is image analysis, for
    example, image classification. Feedforward neural networks are also frequently
    used in this area. Often, though, the sequence of dense layers is not used alone,
    but in combination with another series of convolutional layers. **Convolutional
    layers** are placed after the input of the neural network, to extract features
    and then create a better representation of the image to pass to the next dense
    layers – the feedforward architecture – for the classification. These networks
    are called **Convolutional Neural Networks**, **CNNs** for short.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在图像分析领域非常强大，例如图像分类。前馈神经网络在这个领域也经常使用。然而，通常，密集层的顺序并不是单独使用的，而是与另一系列卷积层结合使用。**卷积层**被放置在神经网络的输入后，用于提取特征，并创建一个更好的图像表示，然后将其传递到下一个密集层——前馈架构——进行分类。这些网络被称为**卷积神经网络**，简称**CNN**。
- en: '[*Chapter 9*](B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316), *Convolutional
    Neural Networks for Image Classification*, explains in detail how convolutional
    layers work. It will also introduce some other related neural layers that are
    suitable to analyze data with spatial relationships, such as the flatten layer
    and the max pooling layer.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316)，*用于图像分类的卷积神经网络*，详细解释了卷积层的工作原理。它还将介绍一些其他相关的神经网络层，适用于分析具有空间关系的数据，如展平层和最大池化层。'
- en: Recurrent Neural Networks
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: A family of neural networks that doesn't belong to feedforward networks is that
    of **Recurrent Neural Networks**, **RNNs** for short. RNNs are obtained by introducing
    auto- or backward connections (recurrent connections) into feedforward neural
    networks. This allows the network to take context into account, since it remembers
    inputs from the past, and therefore it can capture the dynamic of a signal. These
    networks are really powerful when it comes to sequential data, such as times series
    data or text.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一类不属于前馈神经网络的神经网络是**递归神经网络**，简称**RNN**。通过在前馈神经网络中引入自连接或反向连接（递归连接），可以得到RNN。这使得网络能够考虑上下文，因为它记住过去的输入，从而可以捕捉信号的动态。这些网络在处理序列数据时非常强大，例如时间序列数据或文本。
- en: Different layers for RNNs have been introduced in the past, for example, **Long
    Short-Term Memory** (**LSTM**) layers or **Gated Recurrent Unit** (**GRU**) layers.
    [*Chapter 6*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181), *Recurrent Neural
    Networks for Demand Prediction*, covers RNNs in detail as well as the architecture
    of LSTM units.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 过去已经介绍了不同的RNN层，例如**长短期记忆**（**LSTM**）层或**门控循环单元**（**GRU**）层。[ *第6章*](B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181)，*需求预测的递归神经网络*详细讲解了RNN及LSTM单元的架构。
- en: Training a Neural Network
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: After network architecture and activation functions, the last design step before
    you can start training a neural network is the choice of loss function.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络架构和激活函数设计之后，训练神经网络前的最后一步设计是选择损失函数。
- en: We will start with an overview of possible loss functions for regression, binary
    classification, and multiclass classification problems. Then, we will introduce
    some optimizers and additional training parameters for the training algorithms.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先概览回归、二分类和多分类问题的可能损失函数。然后，我们将介绍一些优化器和训练算法的额外参数。
- en: Loss Functions
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: In order to train a feedforward neural network, an appropriate error function,
    often called a **loss function**, and a matching last layer have to be selected.
    Let's start with an overview of commonly used loss functions for regression problems.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练前馈神经网络，必须选择合适的误差函数（通常称为**损失函数**）和匹配的最后一层。让我们先来概览一下常用于回归问题的损失函数。
- en: Loss Functions for Regression Problems
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回归问题的损失函数
- en: 'In the case of a regression problem, where the goal is to predict one single
    numerical value, the output layer should have one unit only and use the linear
    activation function. Possible loss functions to train this kind of network must
    refer to numerical error metrics:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，目标是预测一个单一的数值输出，输出层应该只有一个单元，并使用线性激活函数。训练这种网络的可能损失函数必须参考数值误差度量：
- en: '**Mean Squared Error** (**MSE**) **Loss**: The mean squared error is the default
    error metric for regression problems. For ![](img/Formula_B16391_03_209.png) training
    samples, it is calculated as follows:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）**损失**：均方误差是回归问题的默认误差度量。对于![](img/Formula_B16391_03_209.png)训练样本，计算方法如下：'
- en: '![](img/Formula_B16391_03_210.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_210.png)'
- en: Here, ![](img/Formula_B16391_03_211.png) are the true values and ![](img/Formula_B16391_03_212.png)
    are the predicted values. The MSE gives more importance to large error values
    and it is always positive. A perfect predictor would have an ![](img/Formula_B16391_03_213.png)
    of ![](img/Formula_B16391_03_214.png).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/Formula_B16391_03_211.png)是真实值，![](img/Formula_B16391_03_212.png)是预测值。MSE对较大误差值赋予更多权重，并且总是为正值。一个完美的预测器将具有![](img/Formula_B16391_03_213.png)值为![](img/Formula_B16391_03_214.png)。
- en: '**Mean Squared Logarithmic Error** (**MSLE**) **Loss**: The MSLE is a loss
    function that penalizes large errors less than the MSE. It is calculated by applying
    the logarithm on the predicted and the true values, before using the MSE. For
    ![](img/Formula_B16391_03_215.png) training samples, it is calculated as follows:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方对数误差**（**MSLE**）**损失**：MSLE是一种损失函数，它对大误差的惩罚小于MSE。它通过对预测值和真实值应用对数，然后使用MSE来计算。对于![](img/Formula_B16391_03_215.png)训练样本，计算方法如下：'
- en: '![](img/Formula_B16391_03_216.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_216.png)'
- en: MSLE applies to numbers greater or equal to ![](img/Formula_B16391_03_158.png),
    such as prices. 1 is added to both ![](img/Formula_B16391_03_218.png) and ![](img/Formula_B16391_03_219.png)
    to avoid having ![](img/Formula_B16391_03_220.png)
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: MSLE适用于大于或等于![](img/Formula_B16391_03_158.png)的数字，例如价格。为了避免出现![](img/Formula_B16391_03_220.png)，1会被加到![](img/Formula_B16391_03_218.png)和![](img/Formula_B16391_03_219.png)上。
- en: This loss function is recommended if the range of the target values is large
    and larger errors shouldn't be penalized significantly more than smaller errors.
    The MSLE is always positive and a perfect model has a loss of ![](img/Formula_B16391_03_221.png).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标值的范围较大，且较大的误差不应比较小的误差受到显著的惩罚，推荐使用此损失函数。MSLE始终为正，完美的模型损失为![](img/Formula_B16391_03_221.png)。
- en: '**Mean Absolute Error** (**MAE**) **Loss**: The MAE loss function is a more
    robust loss function with regards to outliers. This means it punishes large errors
    even less than the previous two loss functions, MSE and MSLE. For ![](img/Formula_B16391_03_222.png)
    training samples, it is calculated as follows:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差** (**MAE**) **损失**：MAE损失函数对于离群点具有更强的鲁棒性。这意味着它对大误差的惩罚比之前的两个损失函数（MSE和MSLE）要小。对于![](img/Formula_B16391_03_222.png)训练样本，它的计算公式如下：'
- en: '![](img/Formula_B16391_03_223.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_223.png)'
- en: 'In summary, we can say that we can choose between three different loss functions
    for regression problems: MSE, MSLE, and MAE. Let''s continue with loss functions
    for binary and multiclass classification problems.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以在三种回归问题的损失函数中选择：MSE、MSLE和MAE。接下来，我们将继续讨论二分类和多分类问题的损失函数。
- en: Loss Functions for Binary Classification Problems
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二分类问题的损失函数
- en: The common approach for binary classification is to encode the two classes with
    ![](img/Formula_B16391_03_224.png) and ![](img/Formula_B16391_03_225.png) and
    to train a network to predict the probability for class ![](img/Formula_B16391_03_226.png).
    Here the output layer consists of just one unit with the sigmoid activation function.
    For this approach, the recommended default loss function is **binary cross entropy**.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，常见的方法是将两个类别分别编码为![](img/Formula_B16391_03_224.png)和![](img/Formula_B16391_03_225.png)，并训练一个网络来预测类别![](img/Formula_B16391_03_226.png)的概率。在这种方法中，输出层只包含一个单元，并使用sigmoid激活函数。对于这种方法，推荐的默认损失函数是**二元交叉熵**。
- en: 'On a training set of ![](img/Formula_B16391_03_227.png) samples, the binary
    cross-entropy can be calculated as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在![](img/Formula_B16391_03_227.png)样本的训练集上，二元交叉熵可以按如下方式计算：
- en: '![](img/Formula_B16391_03_228.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_228.png)'
- en: Here, ![](img/Formula_B16391_03_229.png) is the class label, the true value
    (![](img/Formula_B16391_03_230.png) 1) for the *i*th sample in the training set,
    and ![](img/Formula_B16391_03_231.png) is the probability predicted by the network
    for that class. Since this a binary classification problem, the second part of
    the loss function calculates exactly the same value for the other class. ![](img/Formula_B16391_03_232.png)
    is the predicted value ![](img/Formula_B16391_03_233.png) in the previously shown
    loss functions.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_229.png)是类别标签，真实值（![](img/Formula_B16391_03_230.png)
    1）是训练集中第*i*个样本的真实值，![](img/Formula_B16391_03_231.png)是网络对该类别预测的概率。由于这是一个二分类问题，损失函数的第二部分计算与另一类别相同的值。![](img/Formula_B16391_03_232.png)是之前显示的损失函数中的预测值![](img/Formula_B16391_03_233.png)。
- en: Other possible loss functions for binary classification problems are **Hinge**
    and **Squared Hinge**. In this case, the two classes have to be encoded as ![](img/Formula_B16391_03_150.png)
    and ![](img/Formula_B16391_03_175.png) and therefore the unit in the output layer
    must use the tanh activation function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，其他可能的损失函数包括**铰链损失**和**平方铰链损失**。在这种情况下，两个类别必须分别编码为![](img/Formula_B16391_03_150.png)和![](img/Formula_B16391_03_175.png)，因此输出层中的单元必须使用tanh激活函数。
- en: Loss Functions for Multiclass Classification Problems
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多分类问题的损失函数
- en: In multiclass classification problems, usually, each class is represented by
    an integer value (class = 1, 2, 3, …) and a one-hot encoding is used to represent
    the different classes. The output layer should have as neural units as many classes
    all with softmax activation function, so as to predict a score that can be interpreted
    as the probability of each class.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在多分类问题中，通常每个类别通过一个整数值表示（类别 = 1, 2, 3, …），并使用独热编码（one-hot encoding）表示不同的类别。输出层应包含与类别数相等的神经单元，且每个单元都使用softmax激活函数，以便预测一个可以解释为每个类别概率的分数。
- en: 'The default loss function for multiclass classification problems is **categorical
    cross-entropy**. On a training set of ![](img/Formula_B16391_03_209.png) samples,
    the categorical cross-entropy can be calculated as an extension to C classes of
    the binary cross-entropy:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 多分类问题的默认损失函数是**类别交叉熵**。在![](img/Formula_B16391_03_209.png)样本的训练集上，类别交叉熵可以通过对二元交叉熵进行扩展来计算，扩展到C个类别：
- en: '![](img/Formula_B16391_03_237.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_237.png)'
- en: Here, ![](img/Formula_B16391_03_238.png) is the class label k, the true value
    (![](img/Formula_B16391_03_239.png) 1) for the *i*th sample in the training set,
    and ![](img/Formula_B16391_03_240.png) is the corresponding probability predicted
    by the network for class *k*. Again, ![](img/Formula_B16391_03_241.png) is the
    predicted value ![](img/Formula_B16391_03_242.png) by output neuron k for training
    sample *i*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_238.png) 是类别标签 k，真实值（![](img/Formula_B16391_03_239.png)
    1）是训练集中的 *i* 样本的值，![](img/Formula_B16391_03_240.png) 是网络为类别 *k* 预测的相应概率。再者，![](img/Formula_B16391_03_241.png)
    是输出神经元 k 为训练样本 *i* 预测的值 ![](img/Formula_B16391_03_242.png)。
- en: For multiclass classification problems with too many different classes, such
    as language modeling where each word in the dictionary is one class, **sparse
    categorical cross-entropy** is used.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别过多的多分类问题，例如语言建模，其中字典中的每个单词都是一个类别，**稀疏类别交叉熵**被使用。
- en: Another commonly used loss function here is the **Kullback-Leibler divergence**.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的损失函数是 **Kullback-Leibler 散度**。
- en: In addition to the commonly used loss functions, as introduced previously, it
    is also possible to define custom loss functions to best fit the use case at hand.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面介绍的常用损失函数，还可以定义自定义损失函数，以最好地适应当前的使用场景。
- en: Parameters and Optimization of the Training Algorithm
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数和训练算法的优化
- en: 'Now that our network is designed, using the correct activation function in
    the output layer as well as an appropriate loss function, we can start training
    the network. Modern training algorithms are generally based on the SGD strategy,
    using backpropagation to update the values of the network weights. Over the last
    few years, different variants of SGD algorithms (optimizers) have been produced,
    optimized to train networks on datasets with different properties. For example,
    **Adagrad** and its extension **Adadelta** work well on sparse data. **Adam**
    involves a moving average of the gradient and of the squared gradient for the
    weight updates. The Keras documentation page gives an overview of all the available
    training algorithms: [https://keras.io/optimizers/](https://keras.io/optimizers/).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的网络已经设计好，使用正确的输出层激活函数和合适的损失函数后，我们可以开始训练网络。现代训练算法通常基于 SGD 策略，利用反向传播来更新网络权重的值。在过去几年里，已经产生了不同变种的
    SGD 算法（优化器），这些优化器经过优化，可以在具有不同属性的数据集上训练网络。例如，**Adagrad** 及其扩展 **Adadelta** 在稀疏数据上效果良好。**Adam**
    涉及梯度及其平方梯度的移动平均，用于权重更新。Keras 文档页面概述了所有可用的训练算法：[https://keras.io/optimizers/](https://keras.io/optimizers/)。
- en: Important note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Backpropagation is typically referred to as the algorithm that calculates the
    gradients of the weights. The algorithm to train a neural network is usually some
    variant of SGD and it makes use of backpropagation to update the network weights.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播通常是指计算权重梯度的算法。训练神经网络的算法通常是某种变种的 SGD，并且使用反向传播来更新网络权重。
- en: A big role in the training algorithm is played by the **learning rate** ![](img/Formula_B16391_03_243.png).
    The learning rate defines the size of the step taken along the direction of the
    gradient descent on the error surface during the learning phase. A too-small ![](img/Formula_B16391_03_244.png)
    produces tiny steps and therefore takes a long time to reach the minimum of the
    loss function, especially if the loss function happens to have flat slopes. A
    too-large ![](img/Formula_B16391_03_245.png) produces large steps that might overshoot
    and miss the minimum of the loss function, especially if the loss function is
    narrow and with steep slopes. The choice of the right value of learning rate ![](img/Formula_B16391_03_246.png)
    is critical. A possible solution could be to use an adaptive learning rate, starting
    large and progressively decreasing with the number of training iterations.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 训练算法中一个重要的角色是 **学习率** ![](img/Formula_B16391_03_243.png)。学习率定义了在学习阶段沿着误差面上的梯度下降方向所采取的步长。过小的
    ![](img/Formula_B16391_03_244.png) 会产生微小的步伐，因此需要很长时间才能到达损失函数的最小值，特别是当损失函数的斜率较平坦时。过大的
    ![](img/Formula_B16391_03_245.png) 会产生较大的步伐，可能会超过最小值，尤其是当损失函数较窄且斜率较陡时。选择正确的学习率值
    ![](img/Formula_B16391_03_246.png) 是至关重要的。一种可能的解决方案是使用自适应学习率，初始较大，并随着训练迭代次数的增加逐渐减小。
- en: 'In *Figure 3.17*, there are examples for moving on the loss function with a
    too-small, too-large, and adaptive learning rate:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 3.17* 中，展示了使用过小、过大和自适应学习率在损失函数上移动的示例：
- en: '![Figure 3.17 – The progressive decrease of the error with a too-small learning
    rate  (on the left), a too-large learning rate  (in the center), and an adaptive
    learning rate  in a one-dimensional weight space](img/B16391_03_017.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.17 – 使用过小的学习率（左侧），过大的学习率（中间），以及自适应学习率（右侧）在一维权重空间中的误差逐步减少](img/B16391_03_017.jpg)'
- en: Figure 3.17 – The progressive decrease of the error with a too-small learning
    rate ![](img/Formula_B16391_03_247.png) (on the left), a too-large learning rate
    ![](img/Formula_B16391_03_248.png) (in the center), and an adaptive learning rate
    ![](img/Formula_B16391_03_249.png) in a one-dimensional weight space
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.17 – 使用过小的学习率 ![](img/Formula_B16391_03_247.png)（左侧），过大的学习率 ![](img/Formula_B16391_03_248.png)（中间），以及自适应学习率
    ![](img/Formula_B16391_03_249.png) 在一维权重空间中的误差逐步减少
- en: All loss functions are defined as a sum over all training samples. This leads
    to algorithms that update the weights after all training samples have passed through
    the network. This training strategy is called **batch training**. It is the correct
    way to proceed; however, it is also computationally expensive and often slow.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 所有损失函数都定义为对所有训练样本求和。这导致了在所有训练样本通过网络后更新权重的算法。该训练策略称为**批处理训练**。这是正确的操作方式，但它计算开销大且通常较慢。
- en: The alternative is to use the **online training** strategy, where weights are
    updated after the pass of each training sample. This strategy is less computationally
    expensive, but it is just an approximation of the original backpropagation algorithm.
    It is also prone to running into oscillations. In this case, it is good practice
    to use smaller values for the learning rate.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 替代方法是使用**在线训练**策略，其中权重在每个训练样本通过后更新。该策略计算开销较小，但它只是原始反向传播算法的近似。它也容易出现振荡现象。在这种情况下，采用较小的学习率是良好的实践。
- en: Virtually all modern deep learning frameworks make use of a mixture of batch
    and online training, where they use small batches of training examples to perform
    a single update step.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有现代深度学习框架都采用了批处理和在线训练的混合模式，在这种模式下，它们使用小批量的训练样本执行单次更新步骤。
- en: 'The **Momentum** term is added to the weight delta ![](img/Formula_B16391_03_250.png)
    to increase the weight update as long as they have the same sign as the previous
    delta. Momentum speeds up the training on long flat error surfaces and can help
    the network pass a local minimum. The weight update then would include an extra
    term:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '**动量**项被添加到权重的增量中 ![](img/Formula_B16391_03_250.png)，以增加权重更新，只要它们与前一个增量具有相同的符号。动量可以加速长时间平坦误差表面上的训练，并帮助网络通过局部最小值。此时，权重更新将包括一个额外的项：'
- en: '![](img/Formula_B16391_03_251.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_03_251.png)'
- en: Here, ![](img/Formula_B16391_03_252.png) is the current training iteration and
    ![](img/Formula_B16391_03_253.png) is the momentum term.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_03_252.png) 是当前训练迭代，![](img/Formula_B16391_03_253.png)
    是动量项。
- en: Additional Training Parameters
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他训练参数
- en: Two other important setting options for the training process are the training
    batch size and the number of epochs.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的两个重要设置选项是训练批次大小和周期数。
- en: '**Training batch size**: The training **batch size** defines the number of
    samples used in one training iteration. If the training batch size is set to the
    full number of samples in the training set, the training will run in the so-called
    batch mode, which is computationally expensive and slow. In general, it is recommended
    to train a model in mini-batch mode, where only part of the data is used for each
    iteration. It is recommended to shuffle the data before each epoch, to have different
    batches in different epochs.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练批次大小**：训练**批次大小**定义了每次训练迭代中使用的样本数量。如果训练批次大小设置为训练集中的所有样本数量，则训练将以所谓的批处理模式运行，这种模式计算开销大且慢。通常建议在小批量模式下训练模型，每次迭代只使用部分数据。建议在每个周期之前对数据进行洗牌，以便每个周期有不同的批次。'
- en: '**Number of epochs**: The number of epochs defines the number of cycles that
    run over the full training dataset.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期数**：周期数定义了在完整训练数据集上运行的循环次数。'
- en: To summarize, the algorithm goes through the whole training set ![](img/Formula_B16391_03_031.png)
    times, where ![](img/Formula_B16391_03_255.png) is the number of epochs. Each
    epoch consists of a number of iterations and, for each iteration, a subset of
    the training set (a batch) is used. At the end of each iteration, weights are
    updated following the online training strategy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，算法会遍历整个训练集 ![](img/Formula_B16391_03_031.png) 次，其中 ![](img/Formula_B16391_03_255.png)
    是轮次的数量。每个轮次包含多个迭代，每次迭代都会使用训练集的一个子集（即一个批次）。在每次迭代结束时，按照在线训练策略更新权重。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: We have reached the end of this chapter, where we have learned the basic theoretical
    concepts behind neural networks and deep learning networks. All of this will be
    helpful to understand the steps for the practical implementation of deep learning
    networks described in the coming chapters.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经结束了本章内容，在这一章中，我们学习了神经网络和深度学习网络背后的基本理论概念。所有这些都将有助于理解在接下来的章节中描述的深度学习网络的实际实现步骤。
- en: We started with the artificial neuron and moved on to describe how to assemble
    and train a network of neurons, a fully connected feedforward neural network,
    via a variant of the gradient descent algorithm, using the backpropagation algorithm
    to calculate the gradient.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从人工神经元开始，接着描述了如何通过梯度下降算法的变种来组装和训练一个完全连接的前馈神经网络，使用反向传播算法来计算梯度。
- en: We concluded the chapter with a few hints on how to design and train a neural
    network. First, we described some commonly used network topologies, neural layers,
    and activation functions to design the appropriate neural architecture.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了设计和训练神经网络的一些提示。首先，我们描述了一些常用的网络拓扑结构、神经层和激活函数，用于设计合适的神经网络架构。
- en: We then moved to analyze the effects of some parameters involved in the training
    algorithm. We introduced a few more parameters and techniques to optimize the
    training algorithm against a selected loss function.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们分析了一些训练算法中涉及的参数的影响。我们介绍了更多的参数和技术，优化训练算法以适应选定的损失函数。
- en: In the next chapter, you will learn how you can perform all the steps we introduced
    in this chapter using KNIME Analytics Platform.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何使用KNIME Analytics平台执行我们在本章介绍的所有步骤。
- en: Questions and Exercises
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与练习
- en: 'Test how well you have understood the concepts in this chapter by answering
    the following questions:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题，测试你对本章概念的理解：
- en: 'A feedforward neural network is an architecture where:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈神经网络是一种架构，其中：
- en: a. Each neuron from the previous layer is connected to each neuron in the next
    layer.
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 上一层的每个神经元都与下一层的每个神经元相连接。
- en: b. There are auto and backward connections.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 存在自连接和反向连接。
- en: c. There is just one unit in the output layer.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 输出层中只有一个单元。
- en: d. There are as many input units as there are output units.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 输入单元的数量与输出单元的数量相同。
- en: Why do we need hidden layers in a feedforward neural network?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在前馈神经网络中需要隐藏层？
- en: a. For more computational power
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 提供更多的计算能力
- en: b. To speed up calculations
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 为了加速计算
- en: c. To implement more complex functions
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 实现更复杂的函数
- en: d. For symmetry
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 为了对称性
- en: 'The backpropagation algorithm updates the network weights proportionally to:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播算法根据以下内容按比例更新网络权重：
- en: a. The output errors backpropagated through the network
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 输出误差反向传播通过网络
- en: b. The input values forward propagated through the network
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 输入值通过网络向前传播
- en: c. The batch size
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 批量大小
- en: d. The deltas calculated at the output layer and backpropagated through the
    network
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 在输出层计算出的增量并反向传播通过网络
- en: Which loss function is commonly used for a multiclass classification problem?
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多类分类问题通常使用哪种损失函数？
- en: a. MAE
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 平均绝对误差（MAE）
- en: b. RMSE
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 均方根误差（RMSE）
- en: c. Categorical cross-entropy
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 类别交叉熵
- en: d. Binary cross-entropy
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 二元交叉熵
- en: What kind of networks are suited for image analysis?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的网络适合图像分析？
- en: a. RNNs
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 循环神经网络（RNN）
- en: b. CNNs
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 卷积神经网络（CNN）
- en: c. Fully connected feedforward networks
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 完全连接的前馈网络
- en: d. Autoencoders
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 自编码器
- en: How is the last layer of a network commonly configured when solving a binary
    classification problem?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在解决二分类问题时，网络的最后一层通常是如何配置的？
- en: a. Two units with the sigmoid activation function
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 带有sigmoid激活函数的两个单元
- en: b. One unit with the linear activation function
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 一个带有线性激活函数的单元
- en: c. Two units with the ReLU activation function
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 带有ReLU激活函数的两个单元
- en: d. One unit with the sigmoid activation function
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 带有sigmoid激活函数的单元
- en: When are RNNs used?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN（循环神经网络）什么时候使用？
- en: a. On data with many missing values
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. 关于缺失值较多的数据
- en: b. On image data
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 关于图像数据
- en: c. On sequential data
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 关于序列数据
- en: d. On sparse datasets
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d. 关于稀疏数据集
