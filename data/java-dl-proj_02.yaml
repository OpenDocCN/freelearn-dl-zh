- en: Cancer Types Prediction Using Recurrent Type Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归类型网络进行癌症类型预测
- en: Large-scale cancer genomics data often comes in multiplatform and heterogeneous
    forms. These datasets impose great challenges in terms of the bioinformatics approach
    and computational algorithms. Numerous researchers have proposed to utilize this
    data to overcome several challenges, using classical machine learning algorithms
    as either the primary subject or a supporting element for cancer diagnosis and
    prognosis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的癌症基因组学数据通常以多平台和异构形式存在。这些数据集在生物信息学方法和计算算法方面带来了巨大的挑战。许多研究人员提出利用这些数据克服多个挑战，将经典的机器学习算法作为癌症诊断和预后的主要方法或辅助元素。
- en: In this chapter, we will use some deep learning architectures for cancer type
    classification from a very-high-dimensional dataset curated from The Cancer Genome
    Atlas (TCGA). First, we will describe the dataset and perform some preprocessing
    such that the dataset can be fed to our networks. We will then see how to prepare
    our programming environment, before moving on to coding with an open source, deep
    learning library called **Deeplearning4j** (**DL4J**). First, we will revisit
    the Titanic survival prediction problem again using a **Multilayer Perceptron**
    (**MLP**) implementation from DL4J.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用一些深度学习架构来进行癌症类型分类，数据来自The Cancer Genome Atlas（TCGA）中整理的高维数据集。首先，我们将描述该数据集并进行一些预处理，使得数据集可以输入到我们的网络中。然后，我们将学习如何准备编程环境，接下来使用一个开源深度学习库**Deeplearning4j**（**DL4J**）进行编码。首先，我们将再次回顾Titanic生存预测问题，并使用DL4J中的**多层感知器**（**MLP**）实现。
- en: Then we will use an improved architecture of **Recurrent Neural Networks** (**RNN**)
    called **Long Short-Term Memory** (**LSTM**) for cancer type prediction. Finally,
    we will see some frequent questions related to this project and DL4J hyperparameters/nets
    tuning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用一种改进的**递归神经网络**（**RNN**）架构，称为**长短期记忆**（**LSTM**），进行癌症类型预测。最后，我们将了解一些与此项目及DL4J超参数/网络调优相关的常见问题。
- en: 'In a nutshell, we will be learning the following topics in the chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将学习以下内容：
- en: Deep learning in cancer genomics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症基因组学中的深度学习
- en: Cancer genomics dataset description
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 癌症基因组学数据集描述
- en: Getting started with Deeplearning4j
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用Deeplearning4j
- en: Developing a cancer type predictive model using LSTM-RNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LSTM-RNN开发癌症类型预测模型
- en: Frequently asked questions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题
- en: Deep learning in cancer genomics
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 癌症基因组学中的深度学习
- en: Biomedical informatics includes all techniques regarding the development of
    data analytics, mathematical modeling, and computational simulation for the study
    of biological systems. In recent years, we've witnessed huge leaps in biological
    computing that has resulted in large, information-rich resources being at our
    disposal. These cover domains such as anatomy, modeling (3D printers), genomics,
    and pharmacology, among others.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生物医学信息学包括与生物系统研究相关的数据分析、数学建模和计算仿真技术的开发。近年来，我们见证了生物计算的巨大飞跃，结果是大量信息丰富的资源已可供我们使用。这些资源涵盖了诸如解剖学、建模（3D打印机）、基因组学和药理学等多个领域。
- en: One of the most famous success stories of biomedical informatics is from the
    domain of genomics. The **Human Genome Project** (**HGP**) was an international
    research project with the objective of determining the full sequence of human
    DNA. This project has been one of the most important landmarks in computational
    biology and has been used as a base for other projects, including the Human Brain
    Project, which is determined to sequence the human brain. The data that was used
    in this thesis is also the indirect result of the HGP.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生物医学信息学最著名的成功案例之一来自基因组学领域。**人类基因组计划**（**HGP**）是一个国际研究项目，旨在确定人类DNA的完整序列。这个项目是计算生物学中最重要的里程碑之一，并为其他项目提供了基础，包括致力于对人类大脑进行基因组测序的人类大脑计划。本文所使用的数据也是HGP的间接成果。
- en: The era of big data starts from the last decade or so, which was marked by an
    overflow of digital information in comparison to its analog counterpart. Just
    in the year 2016, 16.1 zettabytes of digital data were generated, and it is predicted
    to reach 163 ZB/year by 2025\. As good a piece of news as this is, there are some
    problems lingering, especially of data storage and analysis. For the latter, simple
    machine learning methods that were used in normal-size data analysis won't be
    effective anymore and should be substituted by deep neural network learning methods.
    Deep learning is generally known to deal very well with these types of large and
    complex datasets.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据时代大约从过去十年开始，标志着数字信息的激增，相比其模拟对手。仅在2016年，16.1泽字节的数字数据被生成，预计到2025年将达到每年163泽字节。虽然这是一则好消息，但仍然存在一些问题，尤其是在数据存储和分析方面。对于后者，简单的机器学习方法在常规数据分析中的应用已不再有效，应被深度神经网络学习方法所取代。深度学习通常被认为能非常有效地处理这些类型的大型复杂数据集。
- en: Along with other crucial areas, the biomedical area has also been exposed to
    these big data phenomena. One of the main largest data sources is omics data such
    as genomics, metabolomics, and proteomics. Innovations in biomedical techniques
    and equipment, such as DNA sequencing and mass spectrometry, have led to a massive
    accumulation of -omics data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他重要领域一样，生物医学领域也受到了大数据现象的影响。最主要的大型数据来源之一是诸如基因组学、代谢组学和蛋白质组学等 omics 数据。生物医学技术和设备的创新，如DNA测序和质谱分析，导致了
    -omics 数据的巨大积累。
- en: Typically -omics data is full of veracity, variability and high dimensionality.
    These datasets are sourced from multiple, and even sometimes incompatible, data
    platforms. These properties make these types of data suitable for applying DL
    approaches. Deep learning analysis of -omics data is one of the main tasks in
    the biomedical sector as it has a chance to be the leader in personalized medicine.
    By acquiring information about a person's omics data, diseases can be dealt with
    better and treatment can be focused on preventive measures.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，-omics 数据充满了真实性、变异性和高维度性。这些数据集来源于多个，甚至有时是不兼容的数据平台。这些特性使得这些类型的数据适合应用深度学习方法。对
    -omics 数据的深度学习分析是生物医学领域的主要任务之一，因为它有可能成为个性化医疗的领导者。通过获取一个人 omics 数据的信息，可以更好地应对疾病，治疗可以集中于预防措施。
- en: 'Cancer is generally known to be one of the deadliest diseases in the world,
    which is mostly due to its complexity of diagnosis and treatment. It is a genetic
    disease that involves multiple gene mutations. As the importance of genetic knowledge
    in cancer treatment is increasingly addressed, several projects to document the
    genetic data of cancer patients has emerged recently. One of the most well known
    is **The Cancer Genome Atlas** (**TCGA**) project, which is available on the TCGA
    research network: [http://cancergenome.nih.gov/](http://cancergenome.nih.gov/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 癌症通常被认为是世界上最致命的疾病之一，主要是由于其诊断和治疗的复杂性。它是一种涉及多种基因突变的遗传性疾病。随着癌症治疗中遗传学知识重要性的逐渐受到重视，最近出现了多个记录癌症患者遗传数据的项目。其中最著名的项目之一是**癌症基因组图谱**（**TCGA**）项目，该项目可在TCGA研究网络上找到：[http://cancergenome.nih.gov/](http://cancergenome.nih.gov/)。
- en: As mentioned before, there have been a number of deep learning implementations
    in the biomedical sector, including cancer research. For cancer research, most
    researchers usually use -omics or medical imaging data as inputs. Several research
    works have focused on cancer analysis. Some of them use either a histopathology
    image or a PET image as a source. Most of that research focuses on classification
    based on that image data with **convolutional neural networks** (**CNNs**).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，生物医学领域，包括癌症研究，已经有许多深度学习应用。在癌症研究中，大多数研究者通常使用 -omics 或医学影像数据作为输入。多个研究工作聚焦于癌症分析。其中一些使用组织病理图像或PET图像作为数据来源。大多数研究集中于基于这些图像数据的分类，采用**卷积神经网络**（**CNNs**）。
- en: However, many of them use -omics data as their source. Fakoor et al. classified
    the various types of cancer using patients' gene expression data. Due to the different
    dimensionality of each data from each cancer type, they used **principal component
    analysis** (**PCA**) first to reduce the dimensionality of microarray gene expression
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多研究使用-omics 数据作为其数据来源。Fakoor 等人使用患者的基因表达数据对各种类型的癌症进行了分类。由于每种癌症类型的数据维度不同，他们首先使用**主成分分析**（**PCA**）来减少微阵列基因表达数据的维度。
- en: PCA is a statistical technique used to emphasize variation and extract the most
    significant patterns from a dataset; principal components are the simplest of
    the true eigenvector-based multivariate analyses. PCA is frequently used for making
    data exploration easy to visualize. Consequently, PCA is one of the most used
    algorithms in exploratory data analysis and for making predictive models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种统计技术，用于强调数据的变化并提取数据集中最显著的模式；主成分是基于真实特征向量的最简单的多元分析方法。PCA通常用于使数据探索更易于可视化。因此，PCA是数据探索分析和构建预测模型中最常用的算法之一。
- en: Then they applied sparse and stacked autoencoders to classify various cancers,
    including acute myeloid leukemia, breast cancer, and ovarian cancer.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们应用稀疏和堆叠自编码器对多种癌症进行分类，包括急性髓性白血病、乳腺癌和卵巢癌。
- en: For detailed information, refer to the following publication, entitled *Using
    deep learning to enhance cancer diagnosis and classification* by R. Fakoor et
    al. in proceedings of the International Conference on Machine Learning, 2013.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细信息，请参阅以下文献：《使用深度学习增强癌症诊断与分类》，作者：R. Fakoor 等人，发表于2013年国际机器学习会议论文集中。
- en: Ibrahim et al. , on the other hand, used miRNA expression data from six types
    of cancer genes/miRNA feature selection. They proposed a novel multilevel feature
    selection approach named **MLFS** (short for **Multilevel gene**/**miRNA feature
    selection**), which was based on **Deep Belief Networks (DBN)** and unsupervised
    active learning.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Ibrahim等人使用了来自六种癌症的基因/miRNA特征选择的miRNA表达数据。他们提出了一种新的多级特征选择方法，名为**MLFS**（**多级基因/miRNA特征选择**的简称），该方法基于**深度置信网络（DBN）**和无监督主动学习。
- en: You can read more in the publication titled *Multilevel gene/miRNA feature selection
    using deep belief nets and active learning* (R. Ibrahim, et al.) in Proceedings
    36th annual International Conference Eng. Med. Biol. Soc. (EMBC), pp. 3957-3960,
    IEEE, 2014.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下文献中阅读更多内容：《使用深度置信网络和主动学习的多级基因/miRNA特征选择》，作者：R. Ibrahim 等人，发表于2014年36届国际工程医学生物学学会年会（EMBC）论文集，页3957-3960，IEEE，2014。
- en: Finally, Liang et al. clustered ovarian and breast cancer patients using multiplatform
    genomics and clinical data. The ovarian cancer dataset contained gene expression,
    DNA methylation, and miRNA expression data across 385 patients, which were downloaded
    from **The Cancer Genome Atlas (TCGA)**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Liang等人使用多平台基因组学和临床数据对卵巢癌和乳腺癌患者进行了聚类。卵巢癌数据集包含385名患者的基因表达、DNA甲基化和miRNA表达数据，这些数据从**癌症基因组图谱（TCGA）**下载。
- en: You can read more more in the following publication entitled *Integrative data
    analysis of multi-platform cancer data with a multimodal deep learning approach*
    (by M. Liang et al.) in Molecular Pharmaceutics, vol. 12, pp. 928{937, IEEE/ACM
    Transaction Computational Biology and Bioinformatics, 2015.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下文献中阅读更多内容：《多平台癌症数据的集成数据分析与多模态深度学习方法》，作者：M. Liang 等人，发表于《分子药学》期刊，卷12，页928-937，IEEE/ACM
    计算生物学与生物信息学学报，2015。
- en: The breast cancer dataset included GE data and corresponding clinical information,
    such as survival time and time to recurrence data, which was collected by the
    Netherlands Cancer Institute. To deal with this multiplatform data, they used
    **multimodal Deep Belief Networks** (**mDBN**).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌数据集包括基因表达数据和相应的临床信息，如生存时间和复发时间数据，这些数据由荷兰癌症研究所收集。为了处理这些多平台数据，他们使用了**多模态深度置信网络**（**mDBN**）。
- en: First, they implemented a DBN for each of those data to get their latent features.
    Then, another DBN used to perform the clustering is implemented using those latent
    features as the input. Apart from these researchers, much research work is going
    on to give cancer genomics, identification, and treatment a significant boost.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们为每种数据实现了一个深度置信网络（DBN）以获取其潜在特征。然后，另一个用于执行聚类的深度置信网络使用这些潜在特征作为输入。除了这些研究人员外，还有大量研究正在进行，旨在为癌症基因组学、识别和治疗提供重要推动。
- en: Cancer genomics dataset description
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 癌症基因组学数据集描述
- en: Genomics data covers all data related to DNA on living things. Although in this
    thesis we will also use other types of data like transcriptomic data (RNA and
    miRNA), for convenience purposes, all data will be termed as genomics data. Research
    on human genetics found a huge breakthrough in recent years due to the success
    of the HGP (1984-2000) on sequencing the full sequence of human DNA.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组学数据涵盖与生物体DNA相关的所有数据。尽管在本论文中我们还将使用其他类型的数据，如转录组数据（RNA和miRNA），但为了方便起见，所有数据将统称为基因组数据。人类基因组学的研究在最近几年取得了巨大的突破，这得益于HGP（1984-2000）在测序人类DNA全序列方面的成功。
- en: 'One of the areas that have been helped a lot due to this is the research of
    all diseases related to genetics, including cancer. Due to various biomedical
    analyses done on DNA, there exist various types of -omics or genomics data. Here
    are some types of -omics data that were crucial to cancer analysis:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 受此影响最大的领域之一是与遗传学相关的所有疾病的研究，包括癌症。通过对DNA进行各种生物医学分析，出现了各种类型的-组学或基因组数据。以下是一些对癌症分析至关重要的-组学数据类型：
- en: '**Raw sequencing data:** This corresponds to the DNA coding of whole chromosomes.
    In general, every human has 24 types of chromosomes in each cell of their body,
    and each chromosome consists of 4.6-247 million base pairs. Each base pair can
    be coded in four different types, which are **adenine** (**A**), **cytosine**
    (**C**), **guanine** (**G**), and **thymine** (**T**). Therefore, raw sequencing
    data consists of billions of base pair data, with each coded in one of these four
    different types.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始测序数据**：这对应于整个染色体的DNA编码。一般来说，每个人体内的每个细胞都有24种染色体，每条染色体由4.6亿至2.47亿个碱基对组成。每个碱基对可以用四种不同的类型进行编码，分别是**腺嘌呤**（**A**）、**胞嘧啶**（**C**）、**鸟嘌呤**（**G**）和**胸腺嘧啶**（**T**）。因此，原始测序数据由数十亿个碱基对数据组成，每个碱基对都用这四种类型之一进行编码。'
- en: '**Single-Nucleotide Polymorphism** (**SNP**) data: Each human has a different
    raw sequence, which causes genetic mutation. Genetic mutation can cause an actual
    disease, or just a difference in physical appearance (such as hair color), or
    nothing at all. When this mutation happens only on a single base pair instead
    of a sequence of base pairs, it is called **Single-Nucleotide Polymorphism** (**SNP**).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单核苷酸多态性**（**SNP**）数据：每个人都有不同的原始序列，这会导致基因突变。基因突变可能导致实际的疾病，或者仅仅是外貌上的差异（如发色），也可能什么都不发生。当这种突变仅发生在单个碱基对上，而不是一段碱基对序列时，这被称为**单核苷酸多态性**（**SNP**）。'
- en: '**Copy Number Variation** (**CNV**) data: This corresponds to a genetic mutation
    that happens in a sequence of base pairs. Several types of mutation can happen,
    including deletion of a sequence of base pairs, multiplication of a sequence of
    base pairs, and relocation of a sequence of base pairs into other parts of the
    chromosome.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拷贝数变异**（**CNV**）数据：这对应于发生在碱基对序列中的基因突变。突变可以有多种类型，包括碱基对序列的缺失、碱基对序列的倍增以及碱基对序列在染色体其他部位的重排。'
- en: '**DNA methylation data**: Which corresponds to the amount of methylation (methyl
    group connected to base pair) that happens to areas in the chromosome. A large
    amount of methylation in promoter regions of a gene can cause gene repression.
    DNA methylation is the reason each of our organs acts differently even though
    all of them have the same DNA sequence. In cancer, this DNA methylation is disrupted.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DNA甲基化数据**：这对应于染色体上某些区域发生的甲基化量（甲基基团连接到碱基对上）。基因启动子区域的甲基化量过多可能会导致基因沉默。DNA甲基化是我们每个器官表现出不同功能的原因，尽管它们的DNA序列是相同的。在癌症中，这种DNA甲基化被破坏。'
- en: '**Gene expression data**: This corresponds to the number of proteins that were
    expressed from a gene at a given time. Cancer happens either because of high expression
    of an oncogene (that is, a gene that causes a tumor), low expression of a tumor
    suppressor gene (a gene that prevents a tumor), or both. Therefore, the analysis
    of gene expression data can help discover protein biomarkers in cancer. We will
    use this in this project.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基因表达数据**：这对应于某一时刻从基因中表达的蛋白质数量。癌症的发生通常是由于致癌基因（即引发肿瘤的基因）表达过高、抑癌基因（即防止肿瘤的基因）表达过低，或两者兼有。因此，基因表达数据的分析有助于发现癌症中的蛋白质生物标志物。我们将在本项目中使用这种数据。'
- en: '**miRNA expression data**: Corresponds to the amount of microRNA that was expressed
    at a given time. miRNA plays a role in protein silencing at the mRNA stage. Therefore,
    an analysis of gene expression data can help discover miRNA biomarkers in cancer.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**miRNA表达数据**：对应于在特定时间内表达的微小RNA的数量。miRNA在mRNA阶段起到蛋白质沉默的作用。因此，基因表达数据的分析有助于发现癌症中的miRNA生物标志物。'
- en: 'There are several databases of genomics datasets, where the aforementioned
    data can be found. Some of them focus on the genomics data of cancer patients.
    These databases include:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个基因组数据集的数据库，其中可以找到上述数据。它们中的一些专注于癌症患者的基因组数据。这些数据库包括：
- en: '**The Cancer Genome Atlas** (**TCGA**): **[https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)**'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症基因组图谱**（**TCGA**）：**[https://cancergenome.nih.gov/](https://cancergenome.nih.gov/)**'
- en: '**International Cancer Genome Consortium** (**ICGC**): **[https://icgc.org/](https://icgc.org/)**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国际癌症基因组联盟**（**ICGC**）：**[https://icgc.org/](https://icgc.org/)**'
- en: '**Catalog of Somatic Mutations in Cancer** (**COSMIC**): **[https://cancer.sanger.ac.uk/cosmic](https://cancer.sanger.ac.uk/cosmic)**'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**癌症体细胞突变目录**（**COSMIC**）：**[https://cancer.sanger.ac.uk/cosmic](https://cancer.sanger.ac.uk/cosmic)**'
- en: This genomics data is usually accompanied by clinical data of the patient. This
    clinical data can comprise general clinical information (for example, age or gender)
    and their cancer status (for example, cancer location or cancer stage). All of
    this genomics data itself has a characteristic of high dimensions. For example,
    the gene expression data for each patient is structured based on the gene ID,
    which reaches around 60,000 types.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基因组数据通常伴随着患者的临床数据。临床数据可以包括一般的临床信息（例如，年龄或性别）以及他们的癌症状态（例如，癌症的位置或癌症的分期）。所有这些基因组数据本身具有高维度的特点。例如，每个患者的基因表达数据是基于基因ID构建的，达到约60,000种类型。
- en: Moreover, some of the data itself comes from more than one format. For example,
    70% of the DNA methylation data is collected from breast cancer patients and the
    remaining 30% are curated from different platforms. Therefore, there are two different
    structures on in this dataset. Therefore, to analyze genomics data by dealing
    with the heterogeneity, researchers have often used powerful machine learning
    techniques or even deep neural networks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些数据本身来自多个格式。例如，70%的DNA甲基化数据来自乳腺癌患者，剩余30%则是来自不同平台的整理数据。因此，这个数据集有两种不同的结构。因此，为了分析基因组数据并处理其异质性，研究人员通常采用强大的机器学习技术，甚至是深度神经网络。
- en: Now let's see what a real-life dataset looks like that can be used for our purpose.
    We will be using the gene expression cancer RNA-Seq dataset downloaded from the
    UCI machine learning repository (see [https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)
    for more information).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个可以用于我们目的的实际数据集。我们将使用从UCI机器学习库下载的基因表达癌症RNA-Seq数据集（有关更多信息，请参见[https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq#](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)）。
- en: '![](img/918516cc-0429-4221-a5e6-b4b6a07981f9.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/918516cc-0429-4221-a5e6-b4b6a07981f9.png)'
- en: 'The data collection pipeline for the pan-cancer analysis project (source: "Weinstein,
    John N., et al. ''The cancer genome atlas pan-cancer analysis project.'' Nature
    Genetics 45.10 (2013): 1113-1120")'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '泛癌症分析项目的数据收集流程（来源：“Weinstein, John N., et al. ''The cancer genome atlas pan-cancer
    analysis project.'' Nature Genetics 45.10 (2013): 1113-1120”）'
- en: 'This dataset is a random subset of another dataset reported in the following
    paper: Weinstein, John N., et al. *The cancer genome atlas pan-cancer analysis
    project*. *Nature Genetics 45.10 (2013): 1113-1120*. The preceding diagram shows
    the data collection pipeline for the pan-cancer analysis project.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '这个数据集是从以下论文中报告的另一个数据集的随机子集：Weinstein, John N., et al. *The cancer genome atlas
    pan-cancer analysis project*. *Nature Genetics 45.10 (2013): 1113-1120*。前面的图示展示了泛癌症分析项目的数据收集流程。'
- en: 'The name of the project is The Pan-Cancer analysis project. It assembled data
    from thousands of patients with primary tumors occurring in different sites of
    the body. It covered 12 tumor types (see the upper-left panel in the preceding
    figure) including:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的名称是泛癌症分析项目。该项目汇集了来自不同部位发生原发肿瘤的数千名患者的数据。它涵盖了12种肿瘤类型（见前面图示的左上角面板），包括：
- en: '**Glioblastoma Multiform** (**GBM**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胶质母细胞瘤**（**GBM**）'
- en: '**Lymphoblastic acute myeloid leukemia** (**AML**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**急性淋巴细胞白血病**（**AML**）'
- en: '**Head and Neck Squamous Carcinoma** (**HNSC**)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**头颈部鳞状细胞癌**（**HNSC**）'
- en: '**Lung Adenocarcinoma** (**LUAD**)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肺腺癌**（**LUAD**）'
- en: '**lung Squamous Carcinoma** (**LUSC**)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肺鳞状细胞癌** (**LUSC**)'
- en: '**Breast Carcinoma** (**BRCA**)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**乳腺癌** (**BRCA**)'
- en: '**kidney Renal Clear Cell Carcinoma** (**KIRC**)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**肾脏透明细胞癌** (**KIRC**)'
- en: '**ovarian Carcinoma** (**OV**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卵巢癌** (**OV**)'
- en: '**Bladder Carcinoma** (**BLCA**)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**膀胱癌** (**BLCA**)'
- en: '**Colon Adenocarcinoma** (**COAD**)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结肠腺癌** (**COAD**)'
- en: '**Uterine Cervical and Endometrial Carcinoma** (**UCEC**)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子宫颈和子宫内膜癌** (**UCEC**)'
- en: '**Rectal Adenocarcinoma** (**READ**)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直肠腺癌** (**READ**)'
- en: 'This collection of data is part of the RNA-Seq (HiSeq) PANCAN dataset. It is
    a random extraction of gene expressions of patients having different types of
    tumors: BRCA, KIRC, COAD, LUAD, and PRAD.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这组数据是RNA-Seq (HiSeq) PANCAN数据集的一部分。它是从患有不同类型肿瘤的患者（如BRCA、KIRC、COAD、LUAD和PRAD）中随机提取的基因表达数据。
- en: This dataset is a random collection of cancer patients from 801 patients, each
    having 20,531 attributes. Samples (instances) are stored row-wise. Variables (attributes)
    of each sample are RNA-Seq gene expression levels measured by the illumina HiSeq
    platform. A dummy name (`gene_XX`) is given to each attribute. The attributes
    are ordered consistently with the original submission. For example, `gene_1` on
    `sample_0` is significantly and differentially expressed with a a value of `2.01720929003`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是从801名癌症患者中随机收集的，每名患者有20,531个属性。样本（实例）按行存储。每个样本的变量（属性）是通过illumina HiSeq平台测量的RNA-Seq基因表达水平。每个属性都被赋予一个虚拟名称（`gene_XX`）。属性的顺序与原始提交一致。例如，`sample_0`上的`gene_1`的基因表达水平显著且有差异，数值为`2.01720929003`。
- en: 'When you download the dataset, you will see there are two CSV files:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当你下载数据集时，你会看到有两个CSV文件：
- en: '`data.csv`**:** Contains the gene expression data of each sample'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.csv`**：** 包含每个样本的基因表达数据'
- en: '`labels.csv`**:** The labels associated with each sample'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels.csv`**：** 与每个样本相关的标签'
- en: 'Let''s take a look at the processed dataset. Note we will see only a few selected
    features considering the high dimensionality in the following screenshot, where
    the first column represents sample IDs (that is, anonymous patient IDs). The rest
    of the columns represent how a certain gene expression occurs in the tumor samples
    of the patients:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下处理过的数据集。请注意，由于高维度性，我们只会看到一些选择的特征，以下截图中第一列表示样本ID（即匿名患者ID）。其余列表示某些基因在患者肿瘤样本中的表达情况：
- en: '![](img/52a7de3c-2b87-45db-992b-18d3fa4cf1bc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52a7de3c-2b87-45db-992b-18d3fa4cf1bc.png)'
- en: Sample gene expression dataset
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 样本基因表达数据集
- en: 'Now look at the labels in *Figure 3*. Here, `id` contains the sample ids and
    `Class` represents the cancer labels:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看一下*图3*中的标签。在这里，`id`包含样本ID，`Class`表示癌症标签：
- en: '![](img/daf22379-e09a-4d38-bfde-7ef397fa9d83.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/daf22379-e09a-4d38-bfde-7ef397fa9d83.png)'
- en: Samples are classified into different cancer types
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 样本被分类为不同的癌症类型
- en: Now you can imagine why I have chosen this dataset. Well, although we will not
    have so many samples, the dataset is still very high dimensional. In addition,
    this type of high-dimensional dataset is very suitable for applying a deep learning
    algorithm.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以理解为什么我选择了这个数据集。尽管我们没有太多样本，但这个数据集仍然是非常高维的。此外，这种高维数据集非常适合应用深度学习算法。
- en: Alright. Therefore, if the features and labels are given, can we classify these
    samples based on features and the ground truth. Why not? We will try to solve
    the problem with the DL4J library. First, we have to configure our programming
    environment so that we can start writing our codes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。那么，如果给定了特征和标签，我们能否根据特征和真实标签对这些样本进行分类呢？为什么不呢？我们将尝试使用DL4J库解决这个问题。首先，我们需要配置我们的编程环境，以便开始编写代码。
- en: Preparing programming environment
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备编程环境
- en: 'In this section, we will discuss how to configure DL4J, ND4s, Spark, and ND4J
    before getting started with the coding. The following are prerequisites when working
    with DL4J:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何在开始编写代码之前配置DL4J、ND4s、Spark和ND4J。使用DL4J时需要的前提条件如下：
- en: Java 1.8+ (64-bit only)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 1.8+（仅限64位）
- en: Apache Maven for automated build and dependency manager
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于自动构建和依赖管理的Apache Maven
- en: IntelliJ IDEA or Eclipse IDE
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IntelliJ IDEA或Eclipse IDE
- en: Git for version control and CI/CD
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于版本控制和CI/CD的Git
- en: 'The following libraries can be integrated with DJ4J to enhance your JVM experience
    while developing your ML applications:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库可以与DJ4J集成，以增强你在开发机器学习应用时的JVM体验：
- en: '**DL4J**: The core neural network framework, which comes up with many DL architectures
    and underlying functionalities.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DL4J**：核心神经网络框架，提供了许多深度学习架构和底层功能。'
- en: '**ND4J**: Can be considered as the NumPy of the JVM. It comes with some basic
    operations of linear algebra. Examples are matrix creation, addition, and multiplication.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4J**：可以被认为是JVM的NumPy。它包括一些基本的线性代数操作，例如矩阵创建、加法和乘法。'
- en: '**DataVec**: This library enables ETL operation while performing feature engineering.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataVec**：这个库在执行特征工程的同时，支持ETL操作。'
- en: '**JavaCPP**: This library acts as the bridge between Java and Native C++.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JavaCPP**：这个库充当Java与本地C++之间的桥梁。'
- en: '**Arbiter**: This library provides basic evaluation functionalities for the
    DL algorithms.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arbiter**：这个库为深度学习算法提供基本的评估功能。'
- en: '**RL4J**: Deep reinforcement learning for the JVM.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RL4J**：为JVM提供深度强化学习。'
- en: '**ND4S**: This is a scientific computing library, and it also supports n-dimensional
    arrays for JVM-based languages.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ND4S**：这是一个科学计算库，并且它也支持JVM语言中的n维数组。'
- en: 'If you are using Maven on your preferred IDE, let''s define the project properties
    to mention the versions in the `pom.xml` file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在你喜欢的IDE中使用Maven，我们可以在`pom.xml`文件中定义项目属性来指定版本：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then use the following dependencies required for DL4J, ND4S, ND4J, and so on:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下依赖项，这些依赖项是DL4J、ND4S、ND4J等所需要的：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'By the way, DL4J comes with Spark 2.1.0\. Additionally, if a native system
    BLAS is not configured on your machine, ND4J''s performance will be reduced. You
    will experience the following warning once you execute simple code written in
    Scala:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，DL4J随Spark 2.1.0一起提供。此外，如果你的机器上没有配置本地系统BLAS，ND4J的性能会降低。当你执行Scala编写的简单代码时，你将看到以下警告：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'However, installing and configuring BLAS such as `OpenBLAS` or `IntelMKL` is
    not that difficult; you can invest some time and do it. Refer to the following
    URL for further details: [http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，安装和配置BLAS（如`OpenBLAS`或`IntelMKL`）并不难；你可以花些时间去完成它。更多细节可以参考以下网址：[http://nd4j.org/getstarted.html#open](http://nd4j.org/getstarted.html#open)。
- en: Well done! Our programming environment is ready for simple deep learning application
    development. Now it's time to get your hands dirty with some sample code.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们的编程环境已经准备好用于简单的深度学习应用开发。现在是时候动手写一些示例代码了。
- en: Titanic survival revisited with DL4J
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DL4J重新审视泰坦尼克号生存预测
- en: In the preceding chapter, we solved the Titanic survival prediction problem
    using Spark-based MLP. We also saw that by using Spark-based MLP, the user has
    very little transparency of using the layering structure. Moreover, it was not
    explicit to define hyperparameters and so on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用基于Spark的MLP解决了泰坦尼克号生存预测问题。我们还看到，通过使用基于Spark的MLP，用户几乎无法了解层次结构的使用情况。此外，超参数等的定义也不够明确。
- en: 'Therefore, what I have done is used the training dataset and then performed
    some preprocessing and feature engineering. Then I randomly split the pre-processed
    dataset into training and testing (to be precise, 70% for training and 30% for
    testing). First, we create the Spark session as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我所做的是使用训练数据集，并进行了预处理和特征工程。然后，我将预处理后的数据集随机分为训练集和测试集（具体来说，70%用于训练，30%用于测试）。首先，我们按照如下方式创建Spark会话：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this chapter, we have seen that there are two CSV files. However, `test.csv`
    one does not provide any ground truth. Therefore, I decided to use only the `training.csv`
    one, so that we can compare the model''s performance. So let''s read the training
    dataset using the spark `read()` API:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到有两个CSV文件。然而，`test.csv`没有提供任何实际的标签。因此，我决定只使用`training.csv`文件，以便我们可以比较模型的性能。所以我们通过Spark的`read()`
    API读取训练数据集：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We have seen in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning* that the `Age` and `Fare` columns have many null values.
    So, instead of writing `UDF` for each column, here I just replace the missing
    values of the age and fare columns by their mean:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第一章](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml)《深度学习入门》中看到，`Age`和`Fare`列有许多空值。因此，在这里，我直接用这些列的均值来替换缺失值，而不是为每一列编写`UDF`：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To get more detailed insights into handling missing/null values and machine
    learning, interested readers can take a look at Boyan Angelov's blog at [https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解如何处理缺失/空值和机器学习，感兴趣的读者可以阅读Boyan Angelov的博客，链接如下：[https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce](https://towardsdatascience.com/working-with-missing-data-in-machine-learning-9c0a430df4ce)。
- en: 'For simplicity, we can drop a few more columns too, such as `"PassengerId"`,
    `"Name"`, `"Ticket"`, and `"Cabin"`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们还可以删除一些列，例如“`PassengerId`”、“`Name`”、“`Ticket`”和“`Cabin`”：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, here comes the tricky part. Similar to Spark ML-based estimators, DL4J-based
    networks also need training data in numeric form. Therefore, we now have to convert
    the categorical features into numerics. For that, we can use a `StringIndexer()`
    transformer. What we will do is we will create two that is, `StringIndexer` for
    the `"Sex"` and `"Embarked"` columns:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入难点了。类似于基于Spark ML的估计器，基于DL4J的网络也需要数字形式的训练数据。因此，我们现在必须将类别特征转换为数值。为此，我们可以使用`StringIndexer()`转换器。我们要做的是为“`Sex`”和“`Embarked`”列创建两个`StringIndexer`：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we will chain them into a single pipeline. Next, we will perform the transformation
    operation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将它们串联成一个管道。接下来，我们将执行转换操作：
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we will fit the pipeline, transform, and drop both the `"Sex"` and `"Embarked"`
    columns to get the transformed dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将拟合管道，转换数据，并删除“`Sex`”和“`Embarked`”列，以获取转换后的数据集：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then our final pre-processed dataset will have only the numerical features.
    Note that DL4J considers the last column as the label column. That means DL4J
    will consider `"Pclass"`, `"Age"`, `"SibSp"`, `"Parch"`, `"Fare"`, `"sexIndex"`,
    and `"embarkedIndex"` as features. Therefore, I placed the `"Survived"` column
    as the last column:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们的最终预处理数据集将只包含数值特征。请注意，DL4J将最后一列视为标签列。这意味着DL4J会将“`Pclass`”、“`Age`”、“`SibSp`”、“`Parch`”、“`Fare`”、“`sexIndex`”和“`embarkedIndex`”视为特征。因此，我将“`Survived`”列放在了最后：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we randomly split the dataset into training and testing as 70% and 30%,
    respectively. That is, we used 70% for training and the rest to evaluate the model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据集随机拆分为70%训练集和30%测试集。即，我们使用70%数据进行训练，剩余的30%用于评估模型：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we have both the DataFrames as separate CSV files to be used by DL4J:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个DataFrame分别保存为CSV文件，供DL4J使用：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Additionally, DL4J does not support the header info in the training set, so
    I intentionally skipped writing the header.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DL4J不支持训练集中的头信息，因此我故意跳过了写入头信息。
- en: Multilayer perceptron network construction
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器网络构建
- en: As I informed you in the preceding chapter, DL4J-based neural networks are made
    of multiple layers. Everything starts with a `MultiLayerConfiguration`, which
    organizes those layers and their hyperparameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在前一章中提到的，基于DL4J的神经网络由多个层组成。一切从`MultiLayerConfiguration`开始，它组织这些层及其超参数。
- en: Hyperparameters are a set of variables that determine how a neural network would
    learn. There are many parameters, for example, how many times and how often to
    update the weights of the model (called an **epoch**), how to initialize network
    weights, which activation function to be used, which updater and optimization
    algorithms to be used, the learning rate (that is, how fast the model should learn),
    how many hidden layers are there, how many neurons are there in each layer, and
    so on.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数是一组决定神经网络学习方式的变量。有很多参数，例如：更新模型权重的次数和频率（称为**epoch**），如何初始化网络权重，使用哪种激活函数，使用哪种更新器和优化算法，学习率（即模型学习的速度），隐藏层有多少层，每层有多少神经元等等。
- en: 'We now create the network. First, let us create the layers. Similar to the
    MLP we created in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning*, our MLP will have four layers:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来创建网络。首先，创建层。类似于我们在[第1章](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml)中创建的MLP，*深度学习入门*，我们的MLP将有四层：
- en: '**Layer 0**: Input layer'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第0层**：输入层'
- en: '**Lauer 1**: Hidden layer 1'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第1层**：隐藏层1'
- en: '**Layer 2**: Hidden layer 2'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2层**：隐藏层2'
- en: '**Layer 3**: Output layer'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3层**：输出层'
- en: 'More technically, the first layer is the input layer, and then two layers are
    placed as hidden layers. For the first three layers, we initialized the weights
    using Xavier and the activation function is ReLU. Finally, the output layer is
    placed. This setting is shown in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地讲，第一层是输入层，然后将两层作为隐藏层放置。对于前三层，我们使用Xavier初始化权重，激活函数为ReLU。最后，输出层放置在最后。这一设置如下图所示：
- en: '![](img/afcd3570-cc02-41ad-b098-30f8d1fb93f5.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afcd3570-cc02-41ad-b098-30f8d1fb93f5.png)'
- en: Multilayer perceptron for Titanic survival prediction input layer
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 泰坦尼克号生存预测的多层感知器输入层
- en: 'We have specified the neurons (that is, nodes), which are an equal number of
    inputs, and an arbitrary number of neurons as output. We set a smaller value considering
    very few inputs and features:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经指定了神经元（即节点），输入和输出的数量相等，并且输出的神经元数量是任意的。考虑到输入和特征非常少，我们设置了一个较小的值：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hidden layer 1
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层 1
- en: 'The number of input neurons is equal to the output of the input layer. Then
    the number of outputs is an arbitrary value. We set a smaller value considering
    very few inputs and features:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的神经元数量等于输入层的输出。然后输出的数量是任意值。我们设置了一个较小的值，考虑到输入和特征非常少：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Hidden layer 2
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层 2
- en: 'The number of input neurons is equal to the output of hidden layer 1\. Then
    the number of outputs is an arbitrary value. Again we set a smaller value considering
    very few inputs and features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的神经元数量等于隐藏层 1 的输出。然后输出的数量是一个任意值。再次考虑到输入和特征非常少，我们设置了一个较小的值：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Output layer
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出层
- en: The number of input neurons is equal to the output of the hidden layer 1\. Then
    the number of outputs is equal to the number of predicted labels. We set a smaller
    value yet again, considering a very few inputs and features.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的神经元数量等于隐藏层 1 的输出。然后输出的数量等于预测标签的数量。再次考虑到输入和特征非常少，我们设置了一个较小的值。
- en: 'Here we used the Softmax activation function, which gives us a probability
    distribution over classes (the outputs sum to 1.0), and the losses function as
    cross-entropy for binary classification (XNET) since we want to convert the output
    (probability) to a discrete class, that is, zero or one:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 Softmax 激活函数，它为我们提供了一个类的概率分布（输出的总和为 1.0），并且在二分类（XNET）中使用交叉熵作为损失函数，因为我们想将输出（概率）转换为离散类别，即零或一：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: XNET is used for binary classification with logistic regression. Check out more
    about this in `LossFunctions.java` class in DL4J.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: XNET 用于二分类的逻辑回归。更多信息可以查看 DL4J 中的 `LossFunctions.java` 类。
- en: 'Now we create a `MultiLayerConfiguration` by specifying `NeuralNetConfiguration`
    before conducting the training. With DL4J, we can add a layer by calling `layer`
    on the `NeuralNetConfiguration.Builder()`, specifying its place in the order of
    layers (the zero-indexed layer in the following code is the input layer):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过指定 `NeuralNetConfiguration` 来创建一个 `MultiLayerConfiguration`，然后进行训练。使用
    DL4J 时，我们可以通过调用 `NeuralNetConfiguration.Builder()` 上的 `layer` 方法来添加一层，指定其在层的顺序中的位置（以下代码中的零索引层是输入层）：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Apart from these, we also specify how to set the network's weights. For example,
    as discussed, we use Xavier as the weight initialization and **Stochastic Gradient
    Descent** (**SGD**) optimization algorithm with Adam as the updater. Finally,
    we also specify that we do not need to do any pre-training (which is typically
    needed in DBN or stacked autoencoders). Nevertheless, since MLP is a feedforward
    network, we set backpropagation as true.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些之外，我们还指定了如何设置网络的权重。例如，如前所述，我们使用 Xavier 作为权重初始化，并使用 **随机梯度下降**（**SGD**）优化算法，Adam
    作为更新器。最后，我们还指定不需要进行任何预训练（通常在 DBN 或堆叠自编码器中是需要的）。然而，由于 MLP 是一个前馈网络，我们将反向传播设置为 true。
- en: Network training
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络训练
- en: 'First, we create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then we initialize the network and start the training on the training set:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用之前的 `MultiLayerConfiguration` 创建一个 `MultiLayerNetwork`。然后我们初始化网络并开始在训练集上训练：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code block, we start training the model by invoking the `model.fit()`
    on the training set (`trainingDataIt` in our case). Now we will discuss how we
    prepared the training and test set. Well, for reading the training set or test
    set that are in an inappropriate format (features are numeric and labels are integers),
    I have created a method called `readCSVDataset()`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们通过调用 `model.fit()` 在训练集（在我们案例中为 `trainingDataIt`）上开始训练模型。现在我们来讨论一下如何准备训练集和测试集。好吧，对于读取训练集或测试集格式不正确的数据（特征为数值，标签为整数），我创建了一个名为
    `readCSVDataset()` 的方法：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you see the previous code block, you can realize that it is basically a
    wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader as a dataset iterator. Technically, `RecordReaderDataSetIterator()`
    is the main constructor for classification. It takes the following parameters:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看前面的代码块，你会发现它基本上是一个包装器，用来读取 CSV 格式的数据，然后 `RecordReaderDataSetIterator()`
    方法将记录读取器转换为数据集迭代器。从技术上讲，`RecordReaderDataSetIterator()` 是分类的主要构造函数。它接受以下参数：
- en: '`RecordReader`: This is the `RecordReader` that provides the source of the
    data'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RecordReader`：这是提供数据来源的 `RecordReader`'
- en: '`batchSize`: Batch size (that is, number of examples) for the output `DataSet`
    objects'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchSize`：输出 `DataSet` 对象的批量大小（即，示例数量）'
- en: '`labelIndex`: The index of the label writable (usually an `IntWritable`) as
    obtained by `recordReader.next()`'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labelIndex`：由`recordReader.next()`获取的标签索引可写值（通常是`IntWritable`）'
- en: '`numPossibleLabels`: The number of classes (possible labels) for classification'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numPossibleLabels`：分类的类别数量（可能的标签）'
- en: 'This will then convert the input class index (at position `labelIndex`, with
    integer values `0` to `numPossibleLabels-1`, inclusive) to the appropriate one-hot
    output/labels representation. So let''s see how to proceed. First, we show the
    path of training and test sets:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把输入的类别索引（在 `labelIndex` 位置，整数值为 `0` 到 `numPossibleLabels-1`，包括）转换为相应的 one-hot
    输出/标签表示。接下来让我们看看如何继续。首先，我们展示训练集和测试集的路径：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let''s prepare the data we want to use for training:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们准备要用于训练的数据：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, let''s prepare the data we want to classify:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们准备要分类的数据：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Fantastic! We have managed to prepare the training and test `DataSetIterator`.
    Remember, we will be following nearly the same approach to prepare the training
    and test sets for other problems too.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经成功准备好了训练和测试的`DataSetIterator`。记住，我们在为其他问题准备训练和测试集时，将几乎采用相同的方法。
- en: Evaluating the model
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Once the training has been completed, the next task would be evaluating the
    model. We will evaluate the model''s performance on the test set. For the evaluation,
    we will be using `Evaluation()`; it creates an evaluation object with two possible
    classes (survived or not survived). More technically, the Evaluation class computes
    the evaluation metrics such as precision, recall, F1, accuracy, and Matthews''
    correlation coefficient. The last one is used to evaluate a binary classifier.
    Now let''s take a brief overview on these metrics:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，接下来的任务是评估模型。我们将在测试集上评估模型的性能。对于评估，我们将使用`Evaluation()`；它创建一个包含两种可能类别（存活或未存活）的评估对象。从技术上讲，Evaluation类计算评估指标，如精确度、召回率、F1、准确率和马修斯相关系数。最后一个用于评估二分类器。现在让我们简要了解这些指标：
- en: '**Accuracy** is the ratio of correctly predicted samples to total samples:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率**是正确预测样本与总样本的比例：'
- en: '![](img/76057702-d43e-4861-ac12-6a036a553a83.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76057702-d43e-4861-ac12-6a036a553a83.png)'
- en: '**Precision** is the ratio of correctly predicted positive samples to the total
    predicted positive samples:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确度**是正确预测的正样本与总预测正样本的比例：'
- en: '![](img/1ebcd216-45df-45d1-a389-681ce465fa30.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ebcd216-45df-45d1-a389-681ce465fa30.png)'
- en: '**Recall** is the ratio of correctly predicted positive samples to all samples
    in the actual class—yes:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**是正确预测的正样本与实际类别中所有样本的比例——是的：'
- en: '![](img/557e92fb-f546-40eb-8e6f-f24718421d69.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/557e92fb-f546-40eb-8e6f-f24718421d69.png)'
- en: '**F1 score** is the weighted average (harmonic mean) of Precision and Recall::'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1 分数**是精确度和召回率的加权平均值（调和均值）：'
- en: '![](img/a931ec3c-90b7-41a3-9800-5114b46a9dc2.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a931ec3c-90b7-41a3-9800-5114b46a9dc2.png)'
- en: '**Matthews Correlation Coefficient** (**MCC**) is a measure of the quality
    of binary (two-class) classifications. MCC can be calculated directly from the
    confusion matrix as follows (given that TP, FP, TN, and FN are already available):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**马修斯相关系数**（**MCC**）是衡量二分类（两类）质量的指标。MCC可以通过混淆矩阵直接计算，计算公式如下（假设 TP、FP、TN 和 FN
    已经存在）：'
- en: '![](img/69ca053d-0119-46ca-b39e-722ed60ced3c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69ca053d-0119-46ca-b39e-722ed60ced3c.png)'
- en: Unlike the Apache Spark-based classification evaluator, when solving a binary
    classification problem using the DL4J-based evaluator, special care should be
    taken for binary classification metrics such as F1, precision, recall, and so
    on.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于 Apache Spark 的分类评估器不同，在使用基于 DL4J 的评估器解决二分类问题时，应该特别注意二分类指标，如F1、精确度、召回率等。
- en: 'Well, we will see these later on. First, let''s iterate the evaluation over
    every test sample and get the network''s prediction from the trained model. Finally,
    the `eval()` method checks the prediction against the true classes:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们稍后再讨论这些。首先，让我们对每个测试样本进行迭代评估，并从训练好的模型中获取网络的预测。最后，`eval()`方法将预测结果与真实类别进行对比：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Oops! Unfortunately, we have not managed to achieve very high classification
    accuracy for class 1 (that is, 65%). Now, we compute another metric called MCC
    for this binary classification problem.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！不幸的是，我们在类别1的分类准确率上没有取得很高的成绩（即65%）。现在，我们将为这个二分类问题计算另一个指标，叫做MCC。
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let''s try to interpret this result based on the Matthews paper (see more
    at [www.sciencedirect.com/science/article/pii/0005279575901099](http://www.sciencedirect.com/science/article/pii/0005279575901099)),
    which describes the following properties: A correlation of C = 1 indicates perfect
    agreement, C = 0 is expected for a prediction no better than random, and C = -1
    indicates total disagreement between prediction and observation.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们根据 Matthews 论文（详情请见 [www.sciencedirect.com/science/article/pii/0005279575901099](http://www.sciencedirect.com/science/article/pii/0005279575901099)）来解释这个结果，论文中描述了以下属性：C
    = 1 表示完全一致，C = 0 表示预测与随机预测一样，没有任何改善，而 C = -1 表示预测与观察结果完全不一致。
- en: Following this, our result shows a weak positive relationship. Alright! Although
    we have not achieved good accuracy, you guys can still try by tuning hyperparameters
    or even by changing other networks such as LSTM, which we are going to discuss
    in the next section. But we'll do so for solving our cancer prediction problem,
    which is the main goal of this chapter. So stay with me!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们的结果显示出一种弱的正相关关系。好吧！尽管我们没有获得很好的准确率，但你们仍然可以尝试调整超参数，甚至更换其他网络，比如 LSTM，这是我们在下一部分将讨论的内容。但我们会为解决癌症预测问题而进行这些工作，这也是本章的主要目标。所以请继续关注我！
- en: Cancer type prediction using an LSTM network
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LSTM 网络进行癌症类型预测
- en: In the previous section, we have seen what our data (that is, features and labels)
    looks like. Now in this section, we try to classify those samples according to
    their labels. However, as we have seen, DL4J needs the data in a well-defined
    format so that it can be used to train the model. So let us perform the necessary
    preprocessing and feature engineering.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们已经看到了我们的数据（即特征和标签）是什么样的。现在，在这一部分中，我们尝试根据标签对这些样本进行分类。然而，正如我们所看到的，DL4J
    需要数据以一个明确的格式，以便用于训练模型。所以让我们进行必要的数据预处理和特征工程。
- en: Dataset preparation for training
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备用于训练
- en: Since we do not have any unlabeled data, I would like to select some samples
    randomly for test. Well, one more thing is that features and labels come in two
    separate files. Therefore, we can perform the necessary preprocessing and then
    join them together so that our pre-processed data will have features and labels
    together.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有任何未标记的数据，我想随机选择一些样本用于测试。还有一点是，特征和标签分为两个独立的文件。因此，我们可以先进行必要的预处理，然后将它们合并在一起，以便我们的预处理数据包含特征和标签。
- en: 'Then the rest will be used for training. Finally, we''ll save the training
    and testing set in a separate CSV file to be used later on. First, let''s load
    the samples and see the statistics. By the way, we use the `read()` method of
    Spark but specify the necessary options and format too:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后剩余的部分将用于训练。最后，我们将训练集和测试集保存在单独的 CSV 文件中，以供以后使用。首先，让我们加载样本并查看统计信息。顺便说一下，我们使用
    Spark 的 `read()` 方法，但也指定了必要的选项和格式：
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then we see some related statistics such as number of features and number of
    samples:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到一些相关的统计信息，例如特征数量和样本数量：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Therefore, there are `801` samples from `801` distinct patients and the dataset
    is too high in dimensions, having `20532` features. In addition, in *Figure 2*,
    we have seen that the `id` column represents only the patient''s anonymous ID,
    so we can simply drop it:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集中有来自 `801` 名不同患者的 `801` 个样本，且数据集的维度过高，共有 `20532` 个特征。此外，在 *图 2* 中，我们看到
    `id` 列仅表示患者的匿名 ID，因此我们可以直接删除它：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we load the labels using the `read()` method of Spark and also specify
    the necessary options and format:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 Spark 的 `read()` 方法加载标签，并指定必要的选项和格式：
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](img/9d270be1-d0f0-4c68-b1c2-36f293e6f48b.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d270be1-d0f0-4c68-b1c2-36f293e6f48b.png)'
- en: We have already seen how the labels dataframe looks. We will skip the `id`.
    However, the `Class` column is categorical. Now, as I said, DL4J does not support
    categorical labels to be predicted. Therefore, we have to convert it to numeric
    (integer, to be more specific); for that I would use `StringIndexer()` from Spark.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到标签数据框是什么样子的。我们将跳过 `id` 列。然而，`Class` 列是类别型的。正如我所说，DL4J 不支持对类别标签进行预测。因此，我们需要将其转换为数字型（更具体地说是整数型）；为此，我将使用
    Spark 的 `StringIndexer()`。
- en: 'First, create a `StringIndexer()`; we apply the index operation to the `Class`
    column and rename it as `label`. Additionally, we skip null entries:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个 `StringIndexer()`；我们将索引操作应用于 `Class` 列，并将其重命名为 `label`。另外，我们会跳过空值条目：
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then we perform the indexing operation by calling the `fit()` and `transform()`
    operations as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过调用 `fit()` 和 `transform()` 操作来执行索引操作，如下所示：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let''s take a look at the indexed DataFrame:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下索引化后的 DataFrame：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/3af96ada-3e47-48e5-a11b-9d22419e50a8.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3af96ada-3e47-48e5-a11b-9d22419e50a8.png)'
- en: 'Fantastic! Now all of our columns (including features and labels) are numeric.
    Thus, we can join both features and labels into a single DataFrame. For that,
    we can use the `join()` method from Spark as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在我们所有的列（包括特征和标签）都是数字类型。因此，我们可以将特征和标签合并成一个单一的DataFrame。为此，我们可以使用Spark的`join()`方法，如下所示：
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can generate both the training and test sets by randomly splitting the
    `combindedDF`, as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过随机拆分`combindedDF`来生成训练集和测试集，如下所示：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now let''s see the count of samples in each set:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们查看每个数据集中的样本数量：
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Thus, our training set has `561` samples and the test set has `240` samples.
    Finally, save these two sets as separate CSV files to be used later on:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的训练集有`561`个样本，测试集有`240`个样本。最后，将这两个数据集保存为单独的CSV文件，供以后使用：
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now that we have the training and test sets, we can now train the network with
    the training set and evaluate the model with the test set. Considering the high
    dimensionality, I would rather try a better network such as LSTM, which is an
    improved variant of RNN. At this point, some contextual information about LSTM
    would be helpful to grasp the idea.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练集和测试集，我们可以用训练集训练网络，并用测试集评估模型。考虑到高维度，我更愿意尝试一个更好的网络，比如LSTM，它是RNN的改进变体。此时，关于LSTM的一些背景信息将有助于理解其概念。
- en: Recurrent and LSTM networks
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络和LSTM网络
- en: 'As discussed in [Chapter 1](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml), *Getting
    Started with Deep Learning*, RNNs make use of information from the past; they
    can make predictions in data with high temporal dependencies. A more explicit
    architecture can be found in following diagram where the temporally shared weights
    **w2** (for the hidden layer) must be learned in addition to **w1** (for the input
    layer) and **w3** (for the output layer). From a computational point of view,
    an RNN takes many input vectors to process and generate output vectors. Imagine
    that each rectangle in the following diagram has a vectorial depth and other special
    hidden quirks:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[第一章](fba163f0-88c0-4278-a4a1-df5389cc3a10.xhtml)《深入浅出深度学习》中讨论的那样，RNN利用来自过去的信息；它们可以在具有高度时间依赖性的数据中进行预测。更明确的架构可以在下图中找到，其中时间共享的权重**w2**（用于隐藏层）必须与**w1**（用于输入层）和**w3**（用于输出层）一起学习。从计算角度来看，RNN处理许多输入向量来生成输出向量。想象一下，以下图中每个矩形都有一个向量深度和其他特殊的隐藏特性：
- en: '![](img/2bc80381-93a3-4b60-90ce-4b00ea30e9a5.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bc80381-93a3-4b60-90ce-4b00ea30e9a5.png)'
- en: An RNN architecture where all weights in all layers have to be learned with
    time
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RNN架构，其中所有层的权重都必须随着时间学习。
- en: 'However, we often need to look at only recent information to perform the present
    task, rather than stored information or information that arrived a long time ago.
    This happens frequently in NLP for language modeling. Let''s see a common example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常只需要查看最近的信息来执行当前任务，而不是存储的信息或很久以前到达的信息。这在NLP中的语言建模中经常发生。让我们来看一个常见的例子：
- en: '![](img/cbde902a-d013-4124-b453-f9ba290d0684.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbde902a-d013-4124-b453-f9ba290d0684.png)'
- en: If the gap between the relevant information is small, RNNs can learn to use
    past information
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果相关信息之间的间隔较小，RNN可以学会利用过去的信息。
- en: Suppose we want to develop a DL-based NLP model to predict the next word based
    on the previous words. As a human being, if we try to predict the last word in
    *Berlin is the capital of...,* without further context, the next word is most
    likely *Germany*. In such cases, the gap between the relevant information and
    the position is small. Thus, RNNs can learn to use past information easily.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想开发一个基于深度学习的自然语言处理（NLP）模型，来预测基于前几个词的下一个词。作为人类，如果我们试图预测“*Berlin is the capital
    of...*”中的最后一个词，在没有更多上下文的情况下，下一个词最可能是*Germany*。在这种情况下，相关信息与位置之间的间隔较小。因此，RNN可以轻松地学会使用过去的信息。
- en: 'However, consider another example that is a bit longer: *Reza grew up in Bangladesh.
    He studied in Korea. He speaks fluent...* Now to predict the last word, we would
    need a little bit more context. In this sentence, the most recent information
    advises the network that the next word would probably be the name of a language.
    However, if we narrow down to language level, the context of Bangladesh (from
    the previous words) would be needed.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑一个稍长的例子：“*Reza grew up in Bangladesh. He studied in Korea. He speaks fluent...*”
    现在要预测最后一个词，我们需要更多的上下文。在这个句子中，最新的信息告诉网络，下一个词很可能是某种语言的名称。然而，如果我们将焦点缩小到语言层面，孟加拉国（前面的话语中的信息）的背景将是必需的。
- en: '![](img/de957c74-8ffe-4f84-a18a-14e11aa94883.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de957c74-8ffe-4f84-a18a-14e11aa94883.png)'
- en: If the gap between the relevant information and the place that is needed is
    bigger, RNNs can't learn to use past information
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果相关信息与所需位置之间的间隙更大，RNN 无法学习使用过去的信息
- en: Here, the gap is larger than in the previous example, so an RNN is unable to
    learn to map the information. Nevertheless, gradients for deeper layers are calculated
    by multiplication (that is, the product) of many gradients coming from activation
    functions in the multilayer network. If those gradients are very small or close
    to zero, gradients will easily vanish. On the other hand, when they are bigger
    than 1, it will possibly explode. So, it becomes very hard to calculate and update.
    Let's explain them in more detail.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，信息之间的间隔比之前的例子要大，因此 RNN 无法学习映射这些信息。然而，深层网络中的梯度是通过多层网络中激活函数的多个梯度相乘（即乘积）来计算的。如果这些梯度非常小或接近零，梯度将容易消失。另一方面，当它们大于
    1 时，可能会导致梯度爆炸。因此，计算和更新变得非常困难。让我们更详细地解释这些问题。
- en: These two issues of RNN are jointly called a **vanishing-exploding gradient**
    problem, which directly affects performance. In fact, the backpropagation time
    rolls out the RNN, creating *a very deep* feedforward neural network. The impossibility
    of getting a long-term context from the RNN is precisely due to this phenomenon;
    if the gradient vanishes or explodes within a few layers, the network will not
    be able to learn high-temporal-distance relationships between the data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的这两个问题被统称为 **梯度消失-爆炸** 问题，直接影响模型的性能。实际上，反向传播时，RNN 会展开，形成 *一个非常深* 的前馈神经网络。RNN
    无法获得长期上下文的原因正是这个现象；如果在几层内梯度消失或爆炸，网络将无法学习数据之间的高时间距离关系。
- en: Therefore, the inability of handling long-term dependency, gradient exploding
    and vanishing problems is a serious drawback of RNNs. Here comes LSTM as the savior.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN 无法处理长期依赖关系、梯度爆炸和梯度消失问题是其严重缺点。此时，LSTM 就作为救世主出现了。
- en: 'As the name signifies, short-term patterns are not forgotten in the long term.
    An LSTM network is composed of cells (LSTM blocks) linked to each other. Each
    LSTM block contains three types of gates: an input gate, an output gate, and a
    forget gate. They implement the functions of writing, reading, and reset on the
    cell memory, respectively. These gates are not binary but analog (generally managed
    by a sigmoidal activation function mapped in the range *[0, 1]*, where zero indicates
    total inhibition and one shows total activation).'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，短期模式不会在长期中被遗忘。LSTM 网络由相互连接的单元（LSTM 块）组成。每个 LSTM 块包含三种类型的门：输入门、输出门和遗忘门。它们分别实现对单元记忆的写入、读取和重置功能。这些门不是二元的，而是模拟的（通常由一个
    sigmoid 激活函数管理，映射到 *[0, 1]* 范围内，其中零表示完全抑制，1 表示完全激活）。
- en: 'We can consider an LSTM cell very much like a basic cell, but still the training
    will converge more quickly and it will detect long-term dependencies in the data.
    Now the question would be: how does an LSTM cell work? The architecture of a basic
    LSTM cell is shown in the following diagram:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 LSTM 单元看作一个基本的单元，但它的训练会更快收敛，并且能够检测数据中的长期依赖关系。现在问题是：LSTM 单元是如何工作的？基本 LSTM
    单元的架构如下图所示：
- en: '![](img/1fc7d7b2-006a-4aff-99c1-89e49f6facb1.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fc7d7b2-006a-4aff-99c1-89e49f6facb1.png)'
- en: Block diagram of an LSTM cell
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元的框图
- en: 'Now, let''s see the mathematical notation behind this architecture. If we don''t
    look at what''s inside the LSTM box, the LSTM cell itself looks exactly like a
    regular memory cell, except that its state is split into two vectors, *h(t)* and
    *c(t)*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下这个架构背后的数学符号。如果我们不看 LSTM 盒子内部的内容，LSTM 单元本身看起来与常规内存单元完全相同，只是它的状态被分为两个向量，*h(t)*
    和 *c(t)*：
- en: '*c* is a cell'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* 是单元'
- en: '*h(t)* is the short-term state'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h(t)* 是短期状态'
- en: '*c(t)* is the long-term state'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c(t)* 是长期状态'
- en: 'Now, let''s open the box! The key idea is that the network can learn the following:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开这个“盒子”！关键思想是网络能够学习以下内容：
- en: What to store in the long-term state
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储什么在长期状态中
- en: What to throw away
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃什么
- en: What to read
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读内容
- en: In more simplified words, in an STM, all hidden units of the original RNN are
    replaced by memory blocks, where each memory block contains a memory cell to store
    input history information and three gates to define how to update the information.
    These gates are an input gate, a forget gate, and an output gate.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 用更简化的话来说，在STM中，原始RNN的所有隐藏单元都被内存块替代，每个内存块包含一个内存单元，用于存储输入历史信息，并且有三个门用于定义如何更新信息。这些门是输入门、遗忘门和输出门。
- en: The presence of these gates allows LSTM cells to remember information for an
    indefinite period. In fact, if the input gate is below the activation threshold,
    the cell will retain the previous state, and if the current state is enabled,
    it will be combined with the input value. As the name suggests, the forget gate
    resets the current state of the cell (when its value is cleared to zero), and
    the output gate decides whether the value of the cell must be carried out or not.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这些门的存在使得LSTM单元能够记住信息并维持无限期。实际上，如果输入门低于激活阈值，单元将保持前一个状态；如果当前状态启用，它将与输入值相结合。顾名思义，遗忘门重置单元的当前状态（当其值被清零时），而输出门决定是否执行单元的值。
- en: 'Although the long-term state is copied and passed through the tanh function,
    internally in an LSTM cell, incorporation between two activation functions is
    needed. For example, in the following diagram, tanh decides which values to add
    to the state, with the help of the sigmoid gate:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管长期状态会被复制并通过tanh函数传递，但在LSTM单元内部，需要在两个激活函数之间进行整合。例如，在下面的图示中，tanh决定了哪些值需要加入状态，而这依赖于sigmoid门的帮助：
- en: '![](img/21bcb1c3-96e4-41ff-b57f-6281f0e45715.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21bcb1c3-96e4-41ff-b57f-6281f0e45715.png)'
- en: The internal organization of the LSTM cell structure
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元结构的内部组织
- en: Now, since this book is not meant to teach theory, I would like to stop the
    discussion here, but interested readers can find more details on the DL4J website
    at [https://deeplearning4j.org/lstm.html](https://deeplearning4j.org/lstm.html).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于本书并不打算讲授理论，我想在这里停止讨论，但有兴趣的读者可以在DL4J网站上找到更多细节：[https://deeplearning4j.org/lstm.html](https://deeplearning4j.org/lstm.html)。
- en: Dataset preparation
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备
- en: In the previous section, we prepared the training and test sets. However, we
    need to put some extra efforts into making them consumable by DL4J. To be more
    specific, DL4J expects the training data as numeric and the last column to be
    the label column, and the remaining are features.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们准备了训练集和测试集。然而，我们需要做一些额外的工作，使它们能够被DL4J使用。更具体地说，DL4J期望训练数据是数字类型，且最后一列是标签列，剩下的列是特征。
- en: 'We will now try to prepare our training and test sets like that. First, we
    show the files where we saved the training and test sets:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试按此方式准备我们的训练集和测试集。首先，我们展示保存训练集和测试集的文件：
- en: '[PRE36]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we define the required parameters, such as the number of features, number
    of classes, and batch size. Here, I use `128` as the `batchSize` but adjust it
    accordingly:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义所需的参数，如特征数量、类别数量和批量大小。在这里，我使用`128`作为`batchSize`，但根据需要进行调整：
- en: '[PRE37]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This dataset is used for training:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集用于训练：
- en: '[PRE38]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This is the data we want to classify:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们想要分类的数据：
- en: '[PRE39]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: If you see the preceding two lines, you can realize that `readCSVDataset()`
    is basically a wrapper that reads the data in CSV format, and then the `RecordReaderDataSetIterator()`
    method converts the record reader as a dataset iterator. For more details, refer
    to the *Titanic survival revisited with DL4J* section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到前面两行，你可以意识到`readCSVDataset()`本质上是一个读取CSV格式数据的包装器，然后`RecordReaderDataSetIterator()`方法将记录读取器转换为数据集迭代器。更多详细信息，请参见
    *使用DL4J重新审视泰坦尼克号生存预测* 部分。
- en: LSTM network construction
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络构建
- en: As discussed in the Titanic survival prediction section, again everything starts
    with `MultiLayerConfiguration`, which organizes those layers and their hyperparameters.
    Our LSTM network consists of five layers. The input layer is followed by three
    LSTM layers. Then the last layer is an RNN layer, which is also the output layer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如在泰坦尼克号生存预测部分讨论的那样，一切从`MultiLayerConfiguration`开始，它组织这些层及其超参数。我们的LSTM网络由五层组成。输入层后面是三层LSTM层。最后一层是RNN层，也是输出层。
- en: More technically, the first layer is the input layer, and then three layers
    are placed as LSTM layers. For the LSTM layers, we initialized the weights using
    Xavier. We use SGD as the optimization algorithm with Adam updater and the activation
    function is tanh.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地说，第一层是输入层，然后有三层作为LSTM层。对于LSTM层，我们使用Xavier初始化权重。我们使用SGD作为优化算法，Adam更新器，激活函数是tanh。
- en: 'Finally, the RNN output layer has a softmax activation function, which gives
    us a probability distribution over classes (that is, outputs sum to *1.0*), and
    MCXENT, which is the Multiclass cross-entropy loss function. This setting is shown
    in the following figure:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，RNN输出层有一个softmax激活函数，它为我们提供了一个类别的概率分布（即输出之和为*1.0*），并且MCXENT是多类别交叉熵损失函数。这个设置如图所示：
- en: '![](img/2a036774-db9c-4e74-bbec-5e29b818ca01.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a036774-db9c-4e74-bbec-5e29b818ca01.png)'
- en: Multilayer perceptron for Titanic survival prediction. It takes 20,531 features
    and fixed bias (that is, 1) and generates multi-class outputs.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 用于泰坦尼克号生存预测的多层感知机。它采用20,531个特征和固定的偏置（即1），并生成多类别的输出。
- en: For creating LSTM layers, DL4J provides both LSTM and GravesLSTM classes. The
    latter is an LSTM recurrent net, based on *Supervised Sequence Labelling with
    Recurrent Neural Networks* (see more at [http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建LSTM层，DL4J提供了LSTM和GravesLSTM类。后者是一个基于*有监督序列标注的循环神经网络*（详见 [http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf)）的LSTM递归网络。
- en: GravesLSTM is not compatible with CUDA. Thus, using LSTM is recommended while
    performing the training on GPU. Otherwise, GravesLSTM is faster than LSTM.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: GravesLSTM与CUDA不兼容。因此，建议在GPU上进行训练时使用LSTM。否则，GravesLSTM比LSTM更快。
- en: 'Now before, we start creating the network, let''s define required hyperparameters
    such as the number of input/hidden/output nodes (neurons):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们开始创建网络之前，让我们定义所需的超参数，例如输入/隐藏/输出节点（神经元）的数量：
- en: '[PRE40]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We now create a network configuration and conduct network training. With DL4J,
    you add a layer by calling `layer` on the `NeuralNetConfiguration.Builder()`,
    specifying its place in the order of layers (the zero-indexed layer in the following
    code is the input layer):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在创建一个网络配置并进行网络训练。使用DL4J，你可以通过在`NeuralNetConfiguration.Builder()`上调用`layer`来添加一个层，并指定它在层的顺序中的位置（以下代码中的零索引层是输入层）：
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Finally, we also specify that we do not need to do any pre-training (which is
    typically needed in DBN or stacked autoencoders).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还指定了我们不需要进行任何预训练（这通常在DBN或堆叠自编码器中是必要的）。
- en: Network training
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络训练
- en: 'First, we create a `MultiLayerNetwork` using the preceding `MultiLayerConfiguration`.
    Then we initialize the network and start the training on the training set:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用之前的`MultiLayerConfiguration`创建一个`MultiLayerNetwork`。然后，我们初始化网络并开始在训练集上进行训练：
- en: '[PRE42]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Typically, this type of network has so many hyperparameters. Let''s print the
    number of parameters in the network (and for each layer):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这种类型的网络有许多超参数。让我们打印网络中的参数数量（以及每一层的参数数量）：
- en: '[PRE43]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As I said, our network has 910 million parameters, which is huge. This also
    poses a great challenge while tuning hyperparameters. However, we will see some
    tricks in the FAQs section.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，我们的网络有9.1亿个参数，这非常庞大。在调整超参数时，这也提出了很大的挑战。然而，我们将在常见问题解答部分看到一些技巧。
- en: Evaluating the model
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Once the training has been completed, the next task would be evaluating the
    model. We will evaluate the model''s performance on the test set. For the evaluation,
    we will be using `Evaluation(). This method` creates an evaluation object with
    five possible classes. First, let''s iterate the evaluation over every test sample
    and get the network''s prediction from the trained model. Finally, the `eval()`
    method checks the prediction against the true class:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，接下来的任务就是评估模型。我们将评估模型在测试集上的表现。对于评估，我们将使用`Evaluation()`方法。这个方法创建一个评估对象，包含五个可能的类别。首先，我们将遍历每个测试样本，并从训练好的模型中获取网络的预测。最后，`eval()`方法会检查预测结果与真实类别的匹配情况：
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Wow! Unbelievable! Our LSTM network has accurately classified the samples so
    accurately. Finally, let''s see how the classifier predicts across each class:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！不可思议！我们的LSTM网络准确地分类了这些样本。最后，让我们看看分类器在每个类别中的预测情况：
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher. Did our network underfit? Is there any way to observe how the training
    went? In other words, the question would be why our LSTM neural net shows 100%
    accuracy. We will try to answer these questions in the next section. So stay with
    me!
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行癌症类型预测的预测准确率异常高。我们的网络是不是欠拟合了？有没有办法观察训练过程？换句话说，问题是为什么我们的 LSTM 神经网络显示
    100% 的准确率。我们将在下一节尝试回答这些问题。请继续关注！
- en: Frequently asked questions (FAQs)
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）
- en: Now that we have solved the Titanic survival prediction problem with an acceptable
    level of accuracy, there are other practical aspects of this problem and overall
    deep learning phenomena that need to be considered too. In this section, we will
    see some frequently asked questions that might be already in your mind. Answers
    to these questions can be found in *Appendix A*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经以一个可接受的准确度解决了泰坦尼克号生存预测问题，还有一些实际的方面需要考虑，这些方面不仅涉及此问题本身，也涉及整体的深度学习现象。在本节中，我们将看到一些可能已经在你脑海中的常见问题。这些问题的答案可以在*附录
    A* 中找到。
- en: Can't we use MLP to solve the cancer type prediction by handling this too high-dimensional
    data?
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不能使用 MLP 来解决癌症类型预测问题，处理这么高维的数据吗？
- en: Which activation and loss function can be used with RNN type nets?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN 类型的网络可以使用哪些激活函数和损失函数？
- en: What is the best way of recurrent net weight initialization?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归神经网络的最佳权重初始化方法是什么？
- en: Which updater and optimization algorithm should be used?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该使用哪种更新器和优化算法？
- en: In the Titanic survival prediction problem, we did not experience good accuracy.
    What could be possible reasons and how can we improve the accuracy?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在泰坦尼克号生存预测问题中，我们的准确率不高。可能的原因是什么？我们如何提高准确率？
- en: The predictive accuracy for cancer type prediction using LSTM is suspiciously
    higher. Did our network underfit? Is there any way to observe how the training
    went?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LSTM 进行癌症类型预测的预测准确率异常高。我们的网络是不是欠拟合了？有没有办法观察训练过程？
- en: Which type RNN variants should I use, that is, LSTM or GravesLSTM?
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我应该使用哪种类型的 RNN 变种，也就是 LSTM 还是 GravesLSTM？
- en: Why is my neural net throwing nan score values?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我的神经网络会抛出 nan 分数值？
- en: How to configure/change the DL4J UI port?
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何配置/更改 DL4J UI 端口？
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to classify cancer patients on the basis of tumor
    types from a very-high-dimensional gene expression dataset curated from TCGA.
    Our LSTM architecture managed to achieve 100% accuracy, which is outstanding.
    Nevertheless, we discussed many aspects of DL4J, which will be helpful in upcoming
    chapters. Finally, we saw answers to some frequent questions related to this project,
    LSTM network, and DL4J hyperparameters/nets tuning.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何基于从 TCGA 收集的高维基因表达数据集对癌症患者进行肿瘤类型分类。我们的 LSTM 架构成功地达到了 100% 的准确率，这是非常出色的。然而，我们也讨论了很多与
    DL4J 相关的方面，这些内容将对后续章节非常有帮助。最后，我们还解答了一些关于该项目、LSTM 网络和 DL4J 超参数/网络调优的常见问题。
- en: In the next chapter, we will see how to develop an end-to-end project for handling
    a multilabel (each entity can belong to multiple classes) image classification
    problem using CNN based on Scala and the DL4J framework on real Yelp image datasets.
    We will also discuss some theoretical aspects of CNNs before getting started.
    Nevertheless, we will discuss how to tune hyperparameters for better classification
    results.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何开发一个端到端项目，使用基于 Scala 和 DL4J 框架的 CNN 来处理多标签（每个实体可以属于多个类别）的图像分类问题，数据集来自真实的
    Yelp 图像数据。我们还将在开始之前讨论一些 CNN 的理论方面。然而，我们会讨论如何调整超参数，以获得更好的分类结果。
- en: Answers to questions
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的答案
- en: '**Answer** **to question 1**: The answer is yes, but not very comfortably.
    That means a very deep feedforward network such as deep MLP or DBN can classify
    them with too many iterations.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1 的答案**：答案是肯定的，但不太舒适。这意味着像深度 MLP 或 DBN 这样的非常深的前馈网络可以通过多次迭代进行分类。'
- en: 'However, also to speak frankly, MLP is the weakest deep architecture and is
    not ideal for very high dimensions like this. Moreover, DL4J has deprecated DBN
    since the DL4J 1.0.0-alpha release. Finally, I would still like to show an MLP
    network config just in case you want to try it:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，坦率地说，MLP 是最弱的深度架构，对于像这样的高维数据并不理想。此外，自 DL4J 1.0.0-alpha 版本以来，DL4J 已弃用了 DBN。最后，我仍然想展示一个
    MLP 网络配置，以防你想尝试。
- en: '[PRE46]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Then, just change the line from `MultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);`
    to `**MultiLayerNetwork** model = **new** **MultiLayerNetwork**(MLPconf);`. Readers
    can see the full source in the `CancerPreddictionMLP.java` file.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，只需将代码行 `MultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);` 更改为 `**MultiLayerNetwork**
    model = **new** **MultiLayerNetwork**(MLPconf);`。读者可以在 `CancerPreddictionMLP.java`
    文件中看到完整的源代码。
- en: '**Answer** **to question 2**: There are two aspects to be aware of with regard
    to the choice of activation function.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2 的答案：** 关于激活函数的选择，有两个方面需要注意。'
- en: '**Activation** **function for hidden layers:** Usually, ReLU or leakyrelu activations
    are good choices. Some other activation functions (tanh, sigmoid, and so on) are
    more prone to vanishing gradient problems. However, for LSTM layers, the tanh
    activation function is still commonly used.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层的激活** **函数：** 通常，ReLU 或 leakyrelu 激活函数是不错的选择。其他一些激活函数（如 tanh、sigmoid 等）更容易出现梯度消失问题。然而，对于
    LSTM 层，tanh 激活函数仍然是常用的选择。'
- en: 'A note here: The reason some people do not want to use Rectified Linear Unit
    (ReLU) is that it seems not to perform very well relative to smoother nonlinearity,
    such as sigmoid in the case of RNNs (see more at [https://arxiv.org/pdf/1312.4569.pdf](https://arxiv.org/pdf/1312.4569.pdf)).
    Even tanh works much better with LSTM. Therefore, I used tanh as the activation
    function in the LSTM layers.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有个注意点：一些人不想使用修正线性单元（ReLU）的原因是，它在与平滑的非线性函数（例如在 RNN 中使用的 sigmoid）相比，表现得似乎不太好（更多内容请参见
    [https://arxiv.org/pdf/1312.4569.pdf](https://arxiv.org/pdf/1312.4569.pdf)）。即使
    tanh 在 LSTM 中的效果也要好得多。因此，我在 LSTM 层使用了 tanh 作为激活函数。
- en: '**The activation** **function for the output layer:** For classification problems,
    using the Softmax activation function combined with the negative log-likelihood
    / MCXENT is recommended. However, for a regression problem, the "IDENTITY" activation
    function is a good choice, with MSE as the loss function. In short, the choice
    is really application-specific.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层的激活** **函数：** 对于分类问题，建议使用 Softmax 激活函数并结合负对数似然/MCXENT。但是，对于回归问题，“IDENTITY”激活函数是一个不错的选择，损失函数使用
    MSE。简而言之，选择取决于具体应用。'
- en: '**Answer** **to question 3:** Well, we need to make sure that the network weights
    are neither too big nor too small. I will not recommend using random or zero;
    rather, Xavier weight initialization is usually a good choice for this.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3 的答案：** 好吧，我们需要确保网络的权重既不太大也不太小。我不推荐使用随机初始化或零初始化；通常，Xavier 权重初始化是一个不错的选择。'
- en: '**Answer** **to question 4:** Unless SGD converges well, momentum/rmsprop/adagrad
    optimizers are a good choice. However, I''ve often used Adam as the updater and
    observed good performance too.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4 的答案：** 除非 SGD 收敛得很好，否则 momentum/rmsprop/adagrad 优化器是一个不错的选择。然而，我常常使用
    Adam 作为更新器，并且也观察到了良好的表现。'
- en: '**Answer** **to question 5:** Well, there is no concrete answer to this question.
    In fact, there could be several reasons. For example, probably we have not chosen
    the appropriate hyperparameters. Secondly, we may not have enough data. Thirdly,
    we could be using another network such as LSTM. Fourthly, we did not normalize
    our data.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5 的答案：** 好吧，这个问题没有明确的答案。实际上，可能有几个原因。例如，可能我们没有选择合适的超参数。其次，数据量可能不足。第三，我们可能在使用其他网络，如
    LSTM。第四，我们可能没有对数据进行标准化。'
- en: 'Well, for the third one, you can of course try using an LSTM network similarly;
    I did it for cancer type prediction. For the fourth one, of course normalized
    data always gives better classification accuracy. Now the question would be: what
    is the distribution of your data? Are you scaling it properly? Well, continuous
    values have to be in the range of -1 to 1, 0 to 1, or distributed normally with
    mean 0 and standard deviation 1.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，对于第三种方法，你当然可以尝试使用类似的 LSTM 网络；我在癌症类型预测中使用了它。对于第四种方法，标准化数据总是能带来更好的分类准确率。现在问题是：你的数据分布是什么样的？你是否正确地进行缩放？连续值必须在
    -1 到 1、0 到 1 的范围内，或者是均值为 0、标准差为 1 的正态分布。
- en: 'Finally, I would like to give you a concrete example of data normalization
    in the Titanic example. For that, we can use DL4J''s `NormalizerMinMaxScaler()`.
    Once we created the training dataset iterator, we can instantiate a `NormalizerMinMaxScaler()`
    object and then normalize the data by invoking the `fit()` method. Finally, we
    perform the transformation using the `setPreProcessor()` method, as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想给你一个关于 Titanic 示例中的数据标准化的具体例子。为此，我们可以使用 DL4J 的 `NormalizerMinMaxScaler()`。一旦我们创建了训练数据集迭代器，就可以实例化一个
    `NormalizerMinMaxScaler()` 对象，然后通过调用 `fit()` 方法对数据进行标准化。最后，使用 `setPreProcessor()`
    方法执行转换，如下所示：
- en: '[PRE47]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, for the test dataset iterator, we apply the same normalization for better
    results but without invoking the `fit()` method:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于测试数据集迭代器，我们应用相同的标准化方法以获得更好的结果，但不调用`fit()`方法：
- en: '[PRE48]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'More elaborately, `NormalizerMinMaxScaler ()` acts as the pre-processor for
    datasets that normalizes feature values (and optionally label values) to lie between
    a minimum and maximum value (by default, between 0 and 1). Readers can see the
    full source in the `CancerPreddictionMLP.java` file. After this normalization,
    I experienced slightly better result for class 1, as follows (you could try the
    same for class 0 too):'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，`NormalizerMinMaxScaler ()`作为数据集的预处理器，将特征值（以及可选的标签值）标准化到最小值和最大值之间（默认情况下，是0到1之间）。读者可以在`CancerPreddictionMLP.java`文件中查看完整的源代码。在此标准化之后，我在类1上获得了稍微更好的结果，如下所示（你也可以尝试对类0做相同的操作）：
- en: '[PRE49]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '**Answer to question 6:** In the real world, it''s a rare case that a neural
    net would achieve 100% accuracy. However, if the data is linearly separable, then
    yes, it''s possible! Take a look at the following scatter plots, which show that
    the black line clearly separates the red points and the dark blue points:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题6的回答：** 在实际情况中，神经网络达到100%准确率是很罕见的。然而，如果数据是线性可分的，那么是有可能的！请查看以下散点图，图中黑线清楚地将红点和深蓝点分开：'
- en: '![](img/184293ff-a744-4941-b1c5-8084d6786233.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/184293ff-a744-4941-b1c5-8084d6786233.png)'
- en: Very clearly and linearly separable data points
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 非常清晰且线性可分的数据点
- en: More technically, since a neuron's output (before it passes through an activation
    function) is a linear combination of its inputs, a network consisting of a single
    neuron can learn this pattern. That means if our neural net got the line right,
    it is possible to achieve 100% accuracy.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地说，由于神经元的输出（在通过激活函数之前）是输入的线性组合，因此由单个神经元组成的网络可以学习到这个模式。这意味着，如果我们的神经网络将这条线划对了，实际上是有可能达到100%准确率的。
- en: 'Now, to answer the second part: probably no. To prove this, we can observe
    the training loss, score, and so on on the DL4J UI, which is the interface used
    to visualize the current network status and progress of training in real time
    on your browser.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回答第二部分：可能不是。为了证明这一点，我们可以通过观察训练损失、得分等，在DL4J UI界面上查看训练过程，DL4J UI是用于在浏览器中实时可视化当前网络状态和训练进度的界面。
- en: The UI is typically used to help with tuning neural networks, that is, the selection
    of hyperparameters to obtain good performance for a network. These are already
    in the `CancerPreddictionLSTM.java` file, so do not worry but just keep going.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: UI通常用于帮助调整神经网络的参数，也就是选择超参数以获得良好的网络性能。这些内容已经包含在`CancerPreddictionLSTM.java`文件中，所以不用担心，继续进行即可。
- en: '**Step 1: Adding the dependency for DL4J to your project**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1：将DL4J的依赖项添加到你的项目中**'
- en: 'In the following dependency tag, `_2.11 suffix` is used to specify which Scala
    version should be used for the Scala play framework. You should setting accordingly:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的依赖标签中，`_2.11`后缀用于指定Scala版本，以便与Scala Play框架一起使用。你应该相应地进行设置：
- en: '[PRE50]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '**Step 2: Enabling UI in your project**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2：在项目中启用UI**'
- en: 'This is relatively straightforward. First, you have to initialize the user
    interface backend as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这相对简单。首先，你需要按照以下方式初始化用户界面后端：
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You then configure where the network information is to be stored. Then the
    StatsListener can be added to collect this information:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，你需要配置网络信息存储的位置。然后可以添加StatsListener来收集这些信息：
- en: '[PRE52]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, we attach the StatsStorage instance to the UI:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将StatsStorage实例附加到UI界面：
- en: '[PRE53]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**Step 3: Start collecting information by invoking the fit() method** Information
    will then be collected and routed to the UI when you call the `fit` method on
    your network.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3：通过调用fit()方法开始收集信息** 当你在网络上调用`fit`方法时，信息将会被收集并传送到UI界面。'
- en: '**Step 4: Accessing the UI** Once it is configured, the UI can be accessed
    at [http://localhost:9000/train](http://localhost:9000/train). Now to answer "Did
    our network under fit? Is there any way to observe how the training went?" We
    can observe the **Model Score versus Iteration Chart** on the overview page**.**
    As suggested in the model tuning section at [https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)**,**
    we have the following observation**:**'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4：访问UI** 配置完成后，可以在[http://localhost:9000/train](http://localhost:9000/train)访问UI。现在，回答“我们的网络是否欠拟合？有没有方法观察训练过程？”我们可以在概览页面上观察到**模型得分与迭代图表**。如在[https://deeplearning4j.org/visualization](https://deeplearning4j.org/visualization)的模型调优部分所建议，我们得到了以下观察结果**：**'
- en: The overall score versus iteration should go down over time
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体分数与迭代次数应随时间下降
- en: The score does not increase consistently but decreases drastically when the
    iteration moves on
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分数没有持续增长，而是在迭代过程中急剧下降
- en: The issue might be that there is no noise in the line chart, which is ideally
    expected (that is, the line will go up and down within a small range).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 问题可能在于折线图中没有噪声，而这实际上是理想的情况（也就是说，折线应该在一个小范围内上下波动）。
- en: Now to deal with this, again we can normalize the data and perform the training
    again to see how the performance differs. Well, I would like to leave this up
    to you, folks. One more clue would be following the same data normalization that
    we discussed in question 5.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 现在为了解决这个问题，我们可以再次对数据进行归一化，并重新训练以查看性能差异。好吧，我希望大家自己去试试。还有一个提示是遵循我们在问题 5 中讨论的相同数据归一化方法。
- en: '![](img/2cbc83a2-105b-4027-a354-c9bddef68089.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cbc83a2-105b-4027-a354-c9bddef68089.png)'
- en: LSTM model score over iterations
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 模型的得分随迭代次数变化
- en: 'Now, another observation would be worth mentioning too. For example, the gradients
    did not vanish until the end, which becomes clearer from this figure:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，还有一个观察结果值得提及。例如，梯度直到最后才消失，这从下图中可以看得更清楚：
- en: '![](img/49b98b00-61ba-44dc-bb37-9622cb4545c3.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49b98b00-61ba-44dc-bb37-9622cb4545c3.png)'
- en: The LSTM network's gradients across different iterations
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络在不同迭代之间的梯度
- en: 'Finally, the activation functions performed their role consistently, which
    becomes clearer from the following figure:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，激活函数始终如一地发挥了作用，从下图中可以更清楚地看到这一点：
- en: '![](img/5c7ef990-ef40-45d1-948f-343bfa31c34f.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c7ef990-ef40-45d1-948f-343bfa31c34f.png)'
- en: The LSTM network's activation functions performed their role consistently across
    different layers
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络的激活函数在不同层之间始终如一地发挥着作用
- en: The thing is that there are many more factors to be considered too. However,
    in reality, tuning a neural network is often more an art than a science, and we
    have not considered many aspects as I said. Yet, do not worry; we will see them
    in upcoming projects. So hang on and let's move on to the next question.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是还有许多因素需要考虑。然而，实际上，调整神经网络往往更多的是一种艺术而非科学，而且正如我所说，我们还没有考虑到许多方面。不过，不要担心；我们将在接下来的项目中看到它们。所以坚持住，让我们继续看下一个问题。
- en: '**Answer** **to question 7:** LSTM allows both GPU/CUDA support, but GravesLSTM
    supports only CUDA, hence no support for CuDNN yet. Nonetheless, if you want faster
    training and convergence, using LSTM type is recommended.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 7 的答案：** LSTM 支持 GPU/CUDA，但 GravesLSTM 仅支持 CUDA，因此目前不支持 CuDNN。不过，如果你想要更快的训练和收敛，建议使用
    LSTM 类型。'
- en: '**Answer** **to question 8:** While training a neural network, the backpropagation
    involves multiplications across very small gradients. This happens due to limited
    precision when representing real numbers; values very close to zero cannot be
    represented.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 8 的答案：** 在训练神经网络时，反向传播涉及在非常小的梯度上进行乘法操作。这是由于在表示实数时有限精度的问题；非常接近零的值无法表示。'
- en: It introduces an arithmetic underflow issue, which often happens in a deeper
    network such as DBN, MLP, or CNN. Moreover, if your network throws NaN, then you'll
    need to retune your network to avoid very small gradients.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 它引入了算术下溢问题，这种情况通常发生在像 DBN、MLP 或 CNN 这样的深度网络中。此外，如果你的网络抛出 NaN，那么你需要重新调整网络，以避免非常小的梯度。
- en: '**Answer** **to question 9:** You can set the port by using the org.deeplearning4j.ui.port
    system property. To be more specific, to use port `9001` for example, pass the
    following to the JVM on launch:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 9 的答案：** 你可以通过使用 org.deeplearning4j.ui.port 系统属性来设置端口。更具体地说，例如，要使用端口 `9001`，在启动时将以下内容传递给
    JVM：'
- en: '`-Dorg.deeplearning4j.ui.port=9001`'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`-Dorg.deeplearning4j.ui.port=9001`'
