- en: Going Deeper with DDQN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入学习 DDQN
- en: Deep learning is the evolution of raw computational learning and it is quickly
    evolving and starting to dominate all areas of data science, **machine learning**
    (**ML**), and **artificial intelligence** (**AI**) in general. In turn, these
    enhancements have brought about incredible innovation in **deep reinforcement
    learning** (**DRL**) that have allowed it to play games, previously thought to
    be impossible. DRL is now able to tackle game environments such as the classic
    Atari 2600 series and play them better than a human. In this chapter, we'll look
    at what new features in DL allow DRL to play visual state games, such as Atari
    games. First, we'll look at how a game screen can be used as a visual state. Then,
    we'll understand how DL can consume a visual state with a new component called
    **convolutional neural networks** (**CNNs**). After, we'll use that knowledge
    to build a modified DQN agent to tackle the Atari environment. Building on that,
    we'll look at an enhancement of DQN called **DDQN**, or **double (dueling) DQN**.
    Finally, we'll finish the chapter by playing other visual environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是原始计算学习的演变，它正在迅速发展并开始主导数据科学、**机器学习**（**ML**）和**人工智能**（**AI**）等所有领域。反过来，这些增强带来了在**深度强化学习**（**DRL**）方面的惊人创新，使得它能够玩以前被认为不可能的游戏。现在，DRL
    能够处理像经典 Atari 2600 系列这样的游戏环境，并且比人类玩得更好。在本章中，我们将探讨深度学习中的哪些新特性使得 DRL 能够玩视觉状态游戏，例如
    Atari 游戏。首先，我们将探讨如何将游戏屏幕用作视觉状态。然后，我们将了解深度学习如何通过一个名为**卷积神经网络**（**CNNs**）的新组件来消费视觉状态。之后，我们将利用这些知识来构建一个修改后的
    DQN 代理来处理 Atari 环境。在此基础上，我们将探讨 DQN 的一个增强版本，称为**DDQN**，或**双重（对抗）DQN**。最后，我们将通过玩其他视觉环境来结束本章。
- en: 'In summary, in this chapter we''ll look at how extensions to DL, called CNNs,
    can be used to observe visual states. Then, we''ll use that knowledge to play
    Atari games and implement further enhancements as we go. The following is what
    we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们将探讨深度学习（DL）的扩展，称为卷积神经网络（CNNs），如何被用来观察视觉状态。然后，我们将利用这些知识来玩 Atari 游戏，并在过程中实现进一步的增强。以下是本章我们将涵盖的内容：
- en: Understanding visual state
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解视觉状态
- en: Introducing CNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 CNNs
- en: Working with a DQN on Atari
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Atari 上使用 DQN
- en: Introducing DDQN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 DDQN
- en: Extending replay with prioritized experience replay
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展回放与优先经验回放
- en: We will continue using the same virtual environment we constructed in [Chapter
    6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*, in this
    chapter. You will need that environment set up and configured properly in order
    to use the examples in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续在本章中使用我们在[第 6 章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)“深入学习 DQN”中构建的相同虚拟环境。为了使用本章中的示例，您需要正确设置和配置该环境。
- en: Understanding visual state
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解视觉状态
- en: 'Up until now, we have observed state as an encoded value or values. These values
    may have been the cell number in a grid or the x,y location in an area. Either
    way, these values have been encoded with respect to some reference. In the case
    of the grid environment, we may use a number to denote the square or a pair of
    numbers. For x,y coordinates, we still need to denote an origin, and examples
    of these three types of encoding mechanism are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们观察状态作为编码值或值。这些值可能是网格中的单元格编号或区域中的 x,y 位置。无论如何，这些值都是相对于某个参考进行编码的。在网格环境的情况下，我们可能使用一个数字来表示方块或一对数字。对于
    x,y 坐标，我们仍然需要表示一个原点，以下是一些三种类型编码机制的示例：
- en: '![](img/b90d1fe2-5c07-428d-a721-5ba6484829e8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b90d1fe2-5c07-428d-a721-5ba6484829e8.png)'
- en: Three types of encoding state for an agent
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的编码状态的三种类型
- en: In the preceding diagram, there are three examples of encoding state for an
    environment. For the first example, which is on the left, we just use a number
    to represent that state. Moving right to the next grid, the state is now represented
    as a pair of digits, row by column. On the far right, we can see our old friend
    the Lunar Lander and how part of its state, the location, is taken with respect
    to the landing pad, which is the origin. With all these cases, the state is always
    represented as some form of encoding, whether a single-digit or eight like in
    the Lander environment. By encoding, we mean that we are using a value, that is,
    a number, to represent that state of the environment. In [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*, we learned how discretization of state is a type of transformation
    of that encoding into simpler forms but that transforming this encoding would
    need to be tweaked or learned and we realized there needed to be a better way
    to do this. Fortunately, we did devise a better way, but before we get to that,
    let's consider what state it is itself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，有三个用于环境状态编码的例子。对于第一个例子，位于左侧，我们只是用一个数字来表示该状态。向右移动到下一个网格，状态现在表示为一对数字，按行和列排列。在右侧，我们可以看到我们熟悉的好朋友月球着陆器，以及它的部分状态，即位置，相对于着陆点，即原点。在这些所有情况下，状态总是以某种编码形式表示，无论是单个数字还是像着陆器环境中的八个数字。通过编码，我们是指我们使用一个值，即一个数字，来表示环境的这种状态。在[第五章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)《探索SARSA》中，我们学习了状态离散化是这种编码转换成更简单形式的一种类型，但转换这种编码需要调整或学习，我们意识到需要有一种更好的方法来做这件事。幸运的是，我们确实设计了一种更好的方法，但在我们到达那里之前，让我们考虑一下状态本身是什么。
- en: State is just a numeric representation or index of our policy that lets our
    agent determine its choice of next actions. The important thing to remember here
    is that state needs to be an index into the policy or, in the case of DRL, the
    model. Therefore, our agent will always need to transform that state into a numeric
    index in that model. This is made substantially simpler with DL, as we have already
    seen. What would be ideal is for the agent to be able to visually consume the
    same visible state – the game area – as we humans do and learn to encode the state
    on its own. 10 years ago, that statement would have sounded like science fiction.
    Today, it is a science fact, and we will learn how that is done in the next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 状态只是我们策略的一个数值表示或索引，它让我们的智能体确定其下一步行动的选择。在这里需要记住的重要一点是，状态需要是策略或，在DRL的情况下，模型的索引。因此，我们的智能体始终需要将那种状态转换成模型中的数值索引。正如我们已经看到的，使用深度学习（DL）可以使这个过程大大简化。理想的情况是智能体能够像我们人类一样直观地消费相同的可见状态——游戏区域——并学会自行编码状态。10年前，这样的说法听起来像是科幻小说。今天，这已经成为科学事实，我们将在下一节中学习它是如何实现的。
- en: Encoding visual state
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码视觉状态
- en: 'Fortunately for DRL, the concept of learning from an image has been the center
    of ongoing research into DL for over 30 years. DL has taken this concept from
    being able to recognize handwritten digits to being able to detect object position
    and rotation to understanding the human pose. All of this is done by feeding raw
    pixels into a deep learning network and it being taught (or teaching itself) how
    to encode those images to some answer. We will use these same tools in this chapter,
    but before we do, let''s understand the fundamentals of taking an image and feeding
    it into a network. An example of how you may do this is shown in the following
    diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于DRL来说，从图像中学习这一概念已经成为了深度学习（DL）持续研究30多年的中心。深度学习已经将这一概念从识别手写数字扩展到检测物体位置和旋转，再到理解人类姿态。所有这些都是在将原始像素输入深度学习网络并教会（或教会自己）如何将这些图像编码成某种答案的过程中完成的。我们将在本章中使用这些相同的工具，但在我们这样做之前，让我们了解将图像输入网络的基本原理。以下图示展示了你可能如何进行这一操作：
- en: '![](img/0a2d2581-3f16-4722-86ac-39237e1cdbd0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![示例图](img/0a2d2581-3f16-4722-86ac-39237e1cdbd0.png)'
- en: Dissecting an image for input into DL
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像剖析为深度学习输入
- en: In the preceding diagram, the image was split into four sections and each section
    was fed as a piece into the network. One thing to note is how each piece is fed
    into each neuron on the input layer. Now, we could use four pieces, like in the
    preceding diagram, or 100 pieces, perhaps breaking the image apart pixel by pixel.
    Either way, we are still blindly discretizing the space, that is, an image, and
    trying to make sense of it. Funnily enough, this problem that we recognized in
    RL with discretization is the same type of problem we encounter in deep learning.
    It is perhaps further compounded in DL because we would often just flatten the
    image, a 2D matrix of data, into a 1D vector of numbers. In the preceding example,
    for instance, we can see two eyes being entered into the network but no indication
    of a relationship, such as spacing and orientation, between them. This information
    is completely lost when we flatten an image and is more significant the more we
    discretize the input image. What we need, and what DL discovered, was a way to
    extract particular features from a set of data, such as an image, and preserve
    those features in order to classify the entire image in some manner. DL did, in
    fact, solve this problem very well and we will discover how in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，图像被分割成四个部分，每个部分作为一块被输入到网络中。需要注意的是，每个部分是如何被输入到输入层上的每个神经元的。现在，我们可以使用四个部分，就像前面的图一样，或者100个部分，可能是将图像分解成像素。无论如何，我们仍然是在盲目地离散化空间，即图像，并试图理解它。有趣的是，我们在强化学习（RL）中识别出的离散化问题在深度学习（DL）中也会遇到。在DL中，这个问题可能更加复杂，因为我们通常会简单地将图像，一个二维的数据矩阵，展平成一个一维的数字向量。例如，在前面的例子中，我们可以看到两个眼睛被输入到网络中，但没有表明它们之间的关系，例如间距和方向。当我们展平图像时，这些信息完全丢失，而且随着我们对输入图像的离散化程度越高，这些信息就越重要。我们需要的是，深度学习（DL）发现的是，从一组数据，如图像，中提取特定特征，并保留这些特征，以便以某种方式对整个图像进行分类。实际上，深度学习（DL）很好地解决了这个问题，我们将在下一节中了解到这一点。
- en: Introducing CNNs
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍CNN
- en: In September 2012, a team supervised by Dr. Geoffrey Hinton from the University
    of Toronto, considered the godfather of deep learning, competed to build AlexNet.
    AlexNet was training against a behemoth image test set called ImageNet. ImageNet
    consisted of more than 14 million images in over 20,000 different classes. AlexNet
    handily beat its competition, a non-deep learning solution, by more than 10 points
    that year and achieved what many thought impossible – that is, the recognition
    of objects in images done as well or perhaps even better than humans. Since that
    time, the component that made this possible – CNN – has in some cases surpassed
    human cognition levels in image recognition.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2012年9月，多伦多大学的杰弗里·辛顿博士领导的团队，被认为是深度学习的教父，竞争构建AlexNet。AlexNet是在对抗一个名为ImageNet的巨大图像测试集时进行训练的。ImageNet包含超过1400万张图片，分布在20000多个不同的类别中。AlexNet那年以超过10分的优势击败了其竞争对手，一个非深度学习解决方案，并实现了许多人认为不可能的事情——即图像中对象的识别做得和人类一样好，甚至可能更好。从那时起，使这一切成为可能的组件——CNN——在某些情况下已经超过了人类在图像识别方面的认知水平。
- en: The component that made this possible, CNN, works by dissecting an image into
    features – features that it learns to detect by learning to detect those features.
    This sounds a bit recursive and it is, but it is also the reason it works so well.
    So, let's repeat that again. CNN works by detecting features in an image, except
    we don't specify those features – what we specify is whether the answer is right
    or wrong. By using that answer, we can then use backpropagation to push any errors
    back through the network and correct the way the network detects those features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使这一切成为可能的组件，CNN（卷积神经网络），通过将图像分解成特征来工作——这些特征是通过学习检测这些特征来学习的。这听起来有点递归，确实如此，但这也是它之所以能如此有效的原因。所以，让我们再重复一遍。CNN通过检测图像中的特征来工作，但我们没有指定这些特征——我们指定的是答案是对还是错。通过使用那个答案，我们可以使用反向传播将任何错误推回网络并通过纠正网络检测这些特征的方式来纠正。
- en: 'In order to detect features, we use filters, much the same way you may use
    a filter in Photoshop. These filters are the pieces that we now train and do so
    by introducing them in a new type of layer called CNN, convolution, or CONV. What
    we find is that we can then also stack those layers on top of each other to extract
    further features. These concepts likely still remain abstract. Fortunately, there
    are plenty of great tools that we can use to explore these concepts in the next
    exercise. Let''s take a look at one:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测特征，我们使用滤波器，这与你在Photoshop中使用滤波器的方式类似。这些滤波器是我们现在训练的部件，我们通过在新的层类型CNN（卷积或CONV）中引入它们来进行训练。我们发现，我们还可以将这些层堆叠起来以提取更多特征。这些概念可能仍然很抽象。幸运的是，有许多优秀的工具可以帮助我们在下一个练习中探索这些概念。让我们看看其中一个：
- en: Open and point a web browser to [tensorspace.org](https://tensorspace.org).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开并使用网络浏览器访问[tensorspace.org](https://tensorspace.org)。
- en: Find the link for the **Playground** and click on it.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到**Playground**的链接并点击它。
- en: 'On the **TensorSpace Playground** page, note the various model names on the
    left-hand side. Click on the **AlexNet** example, as shown in the following screenshot:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**TensorSpace Playground**页面，注意左侧的各种模型名称。点击以下截图所示的**AlexNet**示例：
- en: '![](img/6a62226d-3d38-4446-98b7-f1c66579887a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a62226d-3d38-4446-98b7-f1c66579887a.png)'
- en: TensorSpace Playground – AlexNet
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TensorSpace Playground – AlexNet
- en: Playground allows you to interactively explore the various deep learning models,
    such as AlexNet, right down to the layer.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Playground允许你交互式地探索各种深度学习模型，例如AlexNet，直到层级别。
- en: Move through and click on the various layers in the diagram. You can zoom in
    and out and explore the model in 3D. You will be able to look at all the layers
    in the network model. Each layer type is color coded. This includes CNN layers
    (yellow), as well as special pooling layers (blue).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图中移动并点击各个层。你可以放大和缩小，并在3D中探索模型。你将能够查看网络模型中的所有层。每种层类型都有不同的颜色编码。这包括CNN层（黄色），以及特殊的池化层（蓝色）。
- en: Pooling layers, which are layers that collect the learned features from a CNN
    layer, allow a network to learn quicker since the layers essentially reduce the
    size of the learning space. However, that reduction eliminates any spatial relationship
    between features. As such, we typically avoid using pooling layers in DRL and
    games.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层，这些层从CNN层收集学习到的特征，允许网络更快地学习，因为层实际上减少了学习空间的大小。然而，这种减少消除了特征之间的空间关系。因此，我们通常在DRL和游戏中避免使用池化层。
- en: 'If you zoom in, you can look at the way the image is broken by each color channel
    (red, green, and blue) and then fed into the network. The following screenshot
    shows this:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果放大，你可以查看图像是如何被每个颜色通道（红色、绿色和蓝色）分割，然后输入到网络中的。以下截图显示了这一点：
- en: '![](img/60a88ff3-b244-4d61-918f-a9ccda3f98c9.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/60a88ff3-b244-4d61-918f-a9ccda3f98c9.png)'
- en: Inspecting the image separation and filter extraction
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 检查图像分割和滤波器提取
- en: From the way the image is separated, we can see how the first layer of CNN,
    the filters, are extracting features. By doing this, it is possible to recognize
    the entire dog, but as you go through the layer, the features get smaller and
    smaller.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从图像分离的方式中，我们可以看到CNN的第一层，即滤波器，是如何提取特征的。通过这种方式，可以识别整个狗，但随着通过层的深入，特征会越来越小。
- en: Finally, there is a final pooling layer in blue, followed by a green layer,
    which is a single line. This single line layer represents the input data being
    flattened so that it can be fed into further layers of your typical deep learning
    network.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，有一个蓝色的最终池化层，接着是一个绿色的层，这是一个单行层。这个单行层表示输入数据被展平，以便可以输入到典型的深度学习网络的后续层中。
- en: Of course, feel free to explore many of the other models in the Playground.
    Understanding how layers extract features is import to understanding how CNN works.
    In the next section, we'll look at upgrading our DQN agent so that it can play
    Atari games using CNN.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以自由探索Playground中的许多其他模型。理解层如何提取特征对于理解CNN的工作原理非常重要。在下一节中，我们将看看如何升级我们的DQN代理，使其能够使用CNN玩Atari游戏。
- en: Working with a DQN on Atari
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Atari上使用DQN
- en: 'Now that we''ve looked at the output CNNs produce in terms of filters, the
    best way to understand how this works is to look at the code that constructs them.
    Before we get to that, though, let''s begin a new exercise where we use a new
    form of DQN to solve Atari:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了CNN在滤波器方面的输出，了解这一工作原理的最好方式是查看构建它们的代码。在我们到达那里之前，让我们开始一个新的练习，使用新的DQN形式来解决Atari：
- en: 'Open this chapter''s sample code, which can be found in the `Chapter_7_DQN_CNN.py` file.
    The code is fairly similar to `Chapter_6_lunar.py` but with some critical differences.
    We will just focus on the differences in this exercise. If you need a better explanation
    of the code, review [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going
    Deep with DQN*:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开本章的示例代码，该代码位于`Chapter_7_DQN_CNN.py`文件中。代码与`Chapter_6_lunar.py`非常相似，但有一些关键的不同之处。我们将只关注这个练习中的差异。如果你需要更好的代码解释，请回顾[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，*深入DQN*：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Starting at the top, the only change is a new import from a local file called
    `wrappers.py`. We will examine what this does by creating the environment:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从顶部开始，唯一的改变是从一个名为`wrappers.py`的本地文件中导入一个新的模块。我们将通过创建环境来检查这个模块的作用：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We create the environment quite differently here for a few reasons. The three
    functions, `make_atari`, `wrap_deepmind`, and `wrap_pytorch`, are all located
    in the new `wrappers.py` file we imported earlier. These wrappers are based on
    the OpenAI specification for creating wrappers around the Gym environment. We
    will spend more time on wrappers later but for now, the three functions do the
    following:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于几个原因，我们在这里以相当不同的方式创建环境。三个函数`make_atari`、`wrap_deepmind`和`wrap_pytorch`都位于我们之前导入的新`wrappers.py`文件中。这些包装器基于OpenAI为创建Gym环境包装器而制定的规范。我们稍后会更多地讨论包装器，但现在，这三个函数执行以下操作：
- en: '`make_atari`:This prepares the environment so that we can capture visual input
    in a form we can encode with CNN. We are setting this up so we can take screenshots
    of the environment at set intervals.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_atari`: 这项操作准备环境，以便我们可以以CNN可编码的形式捕获视觉输入。我们这样设置是为了能够以设定的时间间隔对环境进行截图。'
- en: '`wrap_deepmind`: This is another wrapper that allows for some helper tools.
    We will look at this later.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wrap_deepmind`: 这又是一个包装器，允许使用一些辅助工具。我们稍后会查看这个包装器。'
- en: '`wrap_pytorch`:This is a helper library that converts the visual input image
    we load into the CNN network into a special form for PyTorch. The various deep
    learning frameworks have different input styles for CNN layers, so until all the
    DL frameworks are standardized, you have to be aware of which way the channels
    appear in your input image. In PyTorch, image channels need to be first. For other
    frameworks, such as Keras, it is the exact opposite.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wrap_pytorch`: 这是一个辅助库，它将我们加载到CNN网络中的视觉输入图像转换为PyTorch的特殊格式。不同的深度学习框架为CNN层有不同的输入风格，因此在所有DL框架标准化之前，你必须注意你的输入图像中通道的排列方式。在PyTorch中，图像通道需要放在第一位。对于其他框架，如Keras，则正好相反。'
- en: 'After that, we need to alter some of the other code that sets the hyperparameters,
    as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们需要修改一些设置超参数的其他代码，如下所示：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The highlighted lines show the changes we made. The main thing we are changing
    is just increasing values – a lot. The Pong Atari environment is the simplest
    and still may require 1 million iterations to solve. On some systems, that may
    take days:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 突出的行显示了我们所做的更改。我们主要改变的是增加值——很多。Pong Atari环境是最简单的，但仍可能需要一百万次迭代才能解决。在某些系统上，这可能需要几天时间：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding block of code, we can see that we are constructing a new class
    called `CnnDQN`. We will get to that shortly. After that, the code is mostly the
    same except for a new variable, `replay_start`, and how large the replay buffer
    is now set to. Our buffer has increased in size 100 times from 1,000 to 100,000
    entries. However, we want to be able to train the agent before the entire buffer
    fills now. After all, that is a lot of entries. Due to this, we''re using `replay_start`
    to denote a training starting point for when the buffer will be used to train
    the agent:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们可以看到我们正在构建一个新的类`CnnDQN`。我们很快就会接触到它。之后，代码基本上相同，除了有一个新的变量`replay_start`以及现在设置的回放缓冲区的大小。我们的缓冲区大小增加了100倍，从1,000条记录增加到100,000条记录。然而，我们希望在缓冲区完全填满之前就能训练智能体。毕竟，那是一个很大的数字。因此，我们使用`replay_start`来表示当缓冲区用于训练智能体时的训练起点：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we update the episode count to a much higher number. This is because
    we can expect this environment requires at least a million episodes to train an
    agent:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将剧集计数更新到一个更高的数值。这是因为我们可以预期这个环境至少需要一百万个剧集来训练一个智能体：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: All of the other code remains the same aside from the last part of the training
    loop, which can be seen in the preceding code. This code shows that we plot iterations
    every 200,000 episodes. Previously, we did this every 2,000 episodes. You can,
    of course, increase this or remove it altogether if it gets annoying when training
    for long hours.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了训练循环的最后部分之外，所有其他代码都保持不变，这部分代码可以在前面的代码中看到。这段代码显示我们每200,000个回合绘制一次迭代。以前，我们每2,000个回合就做一次。当然，你可以增加这个值，或者完全删除它，如果长时间训练感到烦恼的话。
- en: This environment and many of the others we will look at may now take hours or
    days to train. In fact, DeepMind recently estimated that it would take a regular
    desktop system somewhere near 45 years to train its top RL algorithms. And in
    case you are wondering, most of the other environments take 40 million iterations
    to converge. Pong is the easiest at 1 million iterations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个环境和我们将要查看的许多其他环境现在可能需要数小时或数天才能训练。实际上，DeepMind最近估计，一个普通的桌面系统大约需要45年才能训练其最顶尖的强化学习算法。如果你想知道的话，大多数其他环境需要4000万次迭代才能收敛。Pong是最简单的，只需要100万次迭代。
- en: 'Run the example as you normally do. Wait for a while and perhaps move on to
    the rest of this book. This sample will take hours to train, so we will continue
    exploring other sections of code while it runs. To confirm the sample is running
    correctly though, just confirm that the environment is rendering, as shown in
    the following image:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行示例。等待一段时间，也许可以继续阅读这本书的其余部分。这个样本需要数小时才能训练，所以我们在它运行的同时将继续探索代码的其他部分。不过，为了确认样本正在正确运行，只需确认环境正在渲染，如下面的图片所示：
- en: '![](img/715cbba5-c487-4daa-9775-2fff2a6632a4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/715cbba5-c487-4daa-9775-2fff2a6632a4.png)'
- en: Running the code example
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码示例
- en: Keep the sample running. In the next section, we will look at how the CNN layers
    are built into the new model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 保持样本运行。在下一节中，我们将探讨CNN层是如何构建到新模型中的。
- en: Adding CNN layers
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加CNN层
- en: 'Now that we understand the basic premise behind CNN layers, it''s time to take
    an in-depth look at how they work. Open up code example, which can be found in
    the `Chapter_7_DQN_CNN.py` file, and follow these steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了CNN层背后的基本原理，是时候深入探讨它们是如何工作的了。打开代码示例，可以在`Chapter_7_DQN_CNN.py`文件中找到，并按照以下步骤进行：
- en: 'At this point, the only code we need to focus on is for a new class called
    `CnnDQN`, as shown here:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们只需要关注一个名为`CnnDQN`的新类的代码，如下所示：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding class replaces our previous vanilla DQN version. A number of
    key differences exist between both, so let''s start with the network setup and
    building the first convolution layer, as shown here:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述类替换了我们的先前vanilla DQN版本。两者之间存在一些关键差异，所以让我们从网络设置和构建第一个卷积层开始，如下所示：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The first thing to notice is that we are constructing a new model and putting
    it in `self.features`. `features` will be our model for performing convolution
    and separating features. The first layer is constructed by passing in `input_shape`,
    the number of filters (32), `kernel_size` (8), and the `stride` (4). All of these
    inputs are described in more detail here:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先要注意的是，我们正在构建一个新的模型并将其放入`self.features`。`features`将是我们的模型，用于执行卷积和分离特征。第一层是通过传递`input_shape`、过滤器数量（32）、`kernel_size`（8）和`stride`（4）来构建的。所有这些输入的详细信息都可以在这里找到：
- en: '`input_shape[0]`:The input shape refers to the observation space. With the
    wrappers we looked at earlier, we transformed the input space to (1, 84,84). Remember
    that we needed to order the channels first. With 1 channel, we can see our image
    is grayscale (no RGB). 1 channel is also the number we input as the first value
    into `Conv2d.`'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape[0]`：输入形状指的是观察空间。通过我们之前查看的包装器，我们将输入空间转换为（1，84，84）。记住，我们需要首先对通道进行排序。1个通道意味着我们可以看到我们的图像是灰度的（没有RGB）。1个通道也是我们输入到`Conv2d`的第一个值。'
- en: '** The **number of filters (`32`): The next input represents the number of
    filter patches we want to construct in this layer. Each filter is applied across
    the image and is determined by a window size (kernel size) and movement (stride).
    We observed the results of these patches earlier when we used TensorSpace Playground
    to view CNN models in detail.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数量**（`32`）：下一个输入表示我们想要在这个层中构建的过滤器补丁的数量。每个过滤器都应用于图像，并由窗口大小（核大小）和移动（步长）确定。我们在使用TensorSpace
    Playground查看CNN模型细节时观察了这些补丁的结果。'
- en: '`kernel_size` (`8`):This represents the window size. In this case, since we
    are using a 2D convolution, Conv2d, that size actually represents a value of 8x8\.
    Passing the window or kernel over the image and applying the learned filter is
    the convolving operation.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size` (`8`): 这代表窗口大小。在这种情况下，由于我们使用的是2D卷积，Conv2d，这个大小实际上代表了一个8x8的值。将窗口或内核在图像上移动并应用学习到的滤波器是卷积操作。'
- en: '`stride` (`4`):Stride indicates how much the window or kernel moves between
    operations. A stride of 4 means that the window is moved 4 pixels or units which,
    as it turns out, is half the window size of 8.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stride` (`4`): 步长表示窗口或内核在操作之间移动的距离。步长为4意味着窗口移动了4个像素或单位，这实际上是一半的窗口大小8。'
- en: 'An example of how convolution works can be seen in the following image. The
    upper area is a single output patch. Each element in the kernel, that is, the
    3x3 patch in the following image, is the part that is being learned:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积如何工作的一个例子可以在以下图像中看到。上方区域是一个单独的输出块。内核中的每个元素，即以下图像中的3x3块，是正在学习的部分：
- en: '![](img/ad2c12b1-37c5-42e7-ae9e-86b1633ea357.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ad2c12b1-37c5-42e7-ae9e-86b1633ea357.png)'
- en: The strided convolution process explained
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 步长卷积过程的解释
- en: 'The process of applying the kernel on the image is done by simply multiplying
    the values in the patch with each value in the image. All these values are summed
    and then output as a single element in the resulting output filter operation:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上应用内核的过程是通过简单地乘以块中的值与图像中的每个值来完成的。所有这些值相加，然后输出为输出滤波器操作的结果中的单个元素：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Using the code that constructs the model for the convolution layers, we build
    another Linear model, just like we constructed in our previous examples. This
    model will flatten the output from the convolution layers and use that flattened
    model to predict actions from. We end up with two models for the network in this
    case but note that we will pass the output from one to the other, as well as backpropagate
    errors back from one model to the other. The `feature_size` function is just a
    helper so that we can calculate the input from the CNN model to the `Linear` model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建卷积层模型的代码，我们构建另一个线性模型，就像我们在之前的例子中所构建的那样。这个模型将卷积层的输出展平，并使用这个展平的模型来预测动作。在这种情况下，我们最终有两个网络模型，但请注意，我们将从第一个模型传递输出到第二个模型，以及从第一个模型反向传播错误到第二个模型。`feature_size`函数只是一个辅助函数，以便我们可以计算CNN模型到`Linear`模型的输入：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Inside the `forward` function, we can see that the prediction of our model has
    changed. Now, we will break up the prediction by passing it to the `self.features`
    or the CNN part of our model. Then, we need to flatten the data and feed it into
    the Linear portion with `self.fc`.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`forward`函数内部，我们可以看到我们模型的预测已经改变。现在，我们将通过将其传递到`self.features`或我们模型的CNN部分来分解预测。然后，我们需要展平数据，并通过`self.fc`将其馈入线性部分。
- en: The `action` function remains the same as our previous DQN implementation.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`action`函数与我们的先前DQN实现相同。'
- en: If the agent is still running, see if you can wait for it to finish. It can
    take a while but it can be both rewarding and interesting to see the final results.
    Like almost anything in RL, there have been various improvements to the DQN model
    and we will look at those in the next section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体仍在运行，看看你是否可以等待它完成。这可能需要一段时间，但看到最终结果可能会很有奖励和趣味。像RL中的几乎所有事情一样，DQN模型已经经历了各种改进，我们将在下一节中探讨那些改进。
- en: Introducing DDQN
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DDQN
- en: '**DDQN** stands for **dueling DQN** and is different from the double DQN, although
    people often confuse them. Both variations assume some form of duality, but in
    the first case, the model is assumed to be split at the base, while in the second
    case, double DQN, the model is assumed to be split into two entirely different
    DQN models.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDQN**代表**对战的DQN**，与双DQN不同，尽管人们经常混淆它们。这两种变体都假设某种形式的对偶性，但在第一种情况下，模型被假设在基础部分被分割，而在第二种情况，双DQN中，模型被假设分割成两个完全不同的DQN模型。'
- en: 'The following diagram shows the difference between DDQN and DQN, which is not
    to be confused with dueling DQN:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了DDQN和DQN之间的区别，不要与对战的DQN混淆：
- en: '![](img/ab231724-a4a6-41ea-ac48-ed0003d71459.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ab231724-a4a6-41ea-ac48-ed0003d71459.png)'
- en: The difference between DQN and DDQN
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: DQN和DDQN之间的区别
- en: In the preceding diagram, CNN layers are being used in both models but in the
    upcoming exercises, we will just use linear fully connected layers instead, just
    to simplify things.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，两个模型都在使用CNN层，但在即将到来的练习中，我们将只使用线性全连接层，只是为了简化问题。
- en: Notice how the DDQN network separates into two parts that then converge back
    to an answer. This is the dueling part of the DDQN model we will get to shortly.
    Before that, though, let's explore the double DQN model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意DDQN网络如何分为两部分，然后又回到一个答案。这就是DDQN模型中我们将很快讨论的对抗部分。在此之前，让我们先探索双重DQN模型。
- en: Double DQN or the fixed Q targets
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重DQN或固定Q目标
- en: 'In order to understand why we may use two networks in combination, or dueling,
    we first need to understand why we would need to do that. Let''s go back to how
    we calculated the TD loss and used that as our way to estimate actions. As you
    may recall, we calculated loss based on estimations of the target. However, in
    the case of our DQN model, that target is now continually changing. The analogy
    we can use here is that our agent may chase its own tail at times, trying to find
    a target. Those of you who have been very observant may have viewed this during
    previous training by seeing an oscillating reward. What we can do here is create
    another target network that we will aim for and update as we go along. This sounds
    way more complicated than it is, so let''s look at an example:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么我们可能会组合使用两个网络，或者说是“对抗”，我们首先需要明白为什么我们需要这样做。让我们回顾一下我们是如何计算TD损失并使用它作为我们估计动作的方法的。如您所回忆的那样，我们是基于目标估计来计算损失的。然而，在我们的DQN模型中，那个目标现在是持续变化的。我们可以用的类比是，我们的智能体有时可能会追逐自己的尾巴，试图找到目标。那些非常细心的你们可能在前面的训练中已经看到了这一点，通过看到波动的奖励。我们在这里可以做的就是创建另一个目标网络，我们将朝着它前进并在过程中更新它。这听起来比实际上要复杂得多，所以让我们看看一个例子：
- en: 'Open the code example in the `Chapter_7_DoubleDQN.py` file. This example was
    built from the `Chapter_6_DQN_lunar.py` file that we looked at earlier. There
    are a number of subtle changes here, so we will review each of those in detail,
    starting with model construction:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_7_DoubleDQN.py`文件中的代码示例。这个例子是从我们之前看过的`Chapter_6_DQN_lunar.py`文件构建的。这里有一些细微的变化，所以我们将详细审查每一个，从模型构建开始：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As its name suggests, we now construct two DQN models: one for online use and
    one as a target. We train the `current_model` value and then swap back to the
    target model every *x* number of iterations using the following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如其名称所示，我们现在构建了两个DQN模型：一个用于在线使用，一个作为目标。我们训练`current_model`的值，然后每*x*次迭代后使用以下代码切换回目标模型：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `update_target` function updates `target_model` so that it uses the `current_model`
    model. This assures us that the target Q values are always sufficiently enough
    ahead or behind since we are using skip traces and looking back.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`update_target`函数通过使用`current_model`更新`target_model`，确保目标Q值总是足够地提前或落后，因为我们使用跳过跟踪并回顾过去。'
- en: 'Right after that is the `compute_td_loss` function, which needs to be updated
    as follows:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后是`compute_td_loss`函数，需要按照以下方式更新：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The highlighted lines in the function show the lines that were changed. Notice
    how the new models, `current_model` and `target_model`, are used to predict the
    loss now and not just the individual model itself. Finally, in the training or
    trial and error loop, we can see a couple of final changes:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数中突出显示的行显示了被更改的行。注意新的模型`current_model`和`target_model`是如何现在用来预测损失，而不仅仅是单个模型本身。最后，在训练或试错循环中，我们可以看到一些最终的变化：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The first change is that we are now taking the action from the `current_model`
    model:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个变化是我们现在从`current_model`模型中获取动作：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The second change is updating `target_model` with the weights from `current_model`
    using `update_target`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个变化是使用`update_target`更新`target_model`，使用`current_model`的权重：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We also need to update the `play_game` function so that we can take the action
    from `current_model`. It may be interesting to see what happens if you change
    that to the target model instead.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要更新`play_game`函数，以便我们可以从`current_model`中获取动作。如果你将其更改为目标模型，可能会很有趣地看到会发生什么。
- en: At this point, run the code as you normally would and observe the results.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一点上，像平常一样运行代码，并观察结果。
- en: Now that we understand why we may want to use a different model, we will move
    on and learn how we can use dueling DQN or DDQN to solve the same environment.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了为什么我们可能想要使用不同的模型，我们将继续学习如何使用对抗DQN或DDQN来解决相同的环境。
- en: Dueling DQN or the real DDQN
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗DQN或真正的DDQN
- en: Dueling DQN or DDQN extends the concept of a fixed target or fixed Q target
    and extends that to include a new concept called advantage. Advantage is a concept
    where we determine what additional value or advantage we may get by taking other
    actions. Ideally, we want to calculate advantage so that it includes all the other
    actions. We can do this with computational graphs by separating the layers into
    a calculation of state value and another that calculates the advantage from all
    the permutations of state and action.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗DQN或DDQN扩展了固定目标或固定Q目标的概念，并将其扩展到包括一个称为优势的新概念。优势是一个概念，其中我们确定通过采取其他动作可能获得的额外价值或优势。理想情况下，我们希望计算优势，使其包括所有其他动作。我们可以通过计算图来实现这一点，通过将层分离为状态值的计算和从状态和动作的所有排列中计算优势的另一个计算。
- en: 'This construction can be seen in the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构可以在以下图中看到：
- en: '![](img/e94bf46b-2e18-484a-95d1-ac81d8b8330c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e94bf46b-2e18-484a-95d1-ac81d8b8330c.png)'
- en: DDQN visualized in detail
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN的详细可视化
- en: 'The preceding diagram once again shows CNN layers, but our example will just
    start with the linear flattened model. What we can see is how the model is split
    into two parts after it is flattened. The first part calculates the state value
    or value and the second lower part calculates the advantage or action values.
    This is then aggregated to output the Q values. This setup works because we can
    push the loss back through the entire network using optimization, also known as
    backpropagation. Therefore, the network learns how to calculate the advantage
    of each action. Let''s look at how this comes together in a new code example.
    Open the same in the `Chapter_7_DDQN.py` file and follow these steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图再次显示了CNN层，但我们的示例将仅从线性展平模型开始。我们可以看到模型在展平后分为两部分。第一部分计算状态值或价值，第二部分计算优势或动作值。然后，这些值被聚合以输出Q值。这种设置之所以有效，是因为我们可以使用优化（也称为反向传播）将损失推回整个网络。因此，网络学习如何计算每个动作的优势。让我们看看如何在新的代码示例中实现这一点。在`Chapter_7_DDQN.py`文件中打开它，并按照以下步骤操作：
- en: 'This example uses the previous example as a source but differs in terms of
    a number of important details:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个例子使用之前的例子作为源，但在许多重要细节上有所不同：
- en: '[PRE16]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The DDQN class is almost entirely new aside from the `act` function. Inside
    the init function, we can see the construction of the three submodels: `self.feature`,
    `self.value`, and `self.advantage`. Then, inside the `forward` function, we can
    see how the input, **x** is transformed by the first **feature** submodel, then
    fed into the advantage and value submodels. The outputs, `advantage` and `value`,
    are then used to calculate the predicted value, as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了`act`函数外，DDQN类几乎完全是新构建的。在`init`函数中，我们可以看到三个子模型的构建：`self.feature`、`self.value`和`self.advantage`。然后，在`forward`函数中，我们可以看到输入**x**是如何被第一个**feature**子模型转换的，然后输入到优势和价值子模型中。然后，输出`advantage`和`value`被用来计算预测值，如下所示：
- en: '[PRE17]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'What we can see is that the predicted value is the state value denoted by value.
    This is added to the advantage or combined state-action values and subtracted
    from the mean or average. The result is a prediction of the best advantage or
    what the agent learns may be an advantage:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到预测值是表示为价值的州价值。这被添加到优势或组合状态-动作值中，并从平均值或平均中减去。结果是最佳优势的预测或智能体可能学习到的优势：
- en: '[PRE18]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The next change is that we now construct two instances of the DDQN model instead
    of a DQN in our last double DQN example. This means that we also continue to use
    two models in order to evaluate our targets. After all, we don't want to go backward.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个变化是我们现在在之前的double DQN示例中构建两个DDQN模型实例，而不是一个DQN。这意味着我们也继续使用两个模型来评估我们的目标。毕竟，我们不想退步。
- en: 'The next major change occurs in the `compute_td_loss` function. The updated
    lines are as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个主要变化发生在`compute_td_loss`函数中。更新的行如下：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This actually simplifies the preceding code. Now, we can clearly see that our
    next_q_values are being taken from the `target_model`.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这实际上简化了前面的代码。现在，我们可以清楚地看到我们的下一个_q_values是从`target_model`中获取的。
- en: Run the code example as you always do and watch the agent play the Lander. Make
    sure you keep the agent training until it reaches some amount of positive reward.
    This may require you to increase the number of training iterations or episodes.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照您通常的方式运行代码示例，并观察智能体玩Lander。确保您让智能体训练直到它达到一定数量的正奖励。这可能需要您增加训练迭代次数或剧集数。
- en: As a reminder, we use the term episode to mean one training observation or iteration
    for one time step. Many examples will use the word frame and frames to denote
    the same thing. While frame can be appropriate in some contexts, it is less so
    in others, especially when we start to stack frames or input observations. If
    you find the name confusing, an alternative may be to use training iteration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们使用术语“episode”来表示一次训练观察或迭代一个时间步。许多例子将使用“frame”和“frames”来表示相同的东西。虽然在某些情况下“frame”可能是合适的，但在其他情况下则不太合适，尤其是在我们开始堆叠帧或输入观察时。如果你觉得这个名字令人困惑，一个替代方案可能是使用“训练迭代”。
- en: You will see that this algorithm does indeed converge faster, but as you may
    expect, there are improvements we can make to this algorithm as well. We will
    look at how we can improve on this in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这个算法确实收敛得更快，但正如你所预期的，我们还可以对这个算法进行改进。我们将在下一节中探讨如何改进。
- en: Extending replay with prioritized experience replay
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用优先级经验回放扩展重放
- en: 'So far, we''ve seen how using a replay buffer or experience replay mechanism
    allows us to pull values back in batches at a later time in order to train the
    network graph. These batches of data were composed of random samples, which works
    well, but of course, we can do better. Therefore, instead of storing just everything,
    we can make two decisions: what data to store and what data is a priority to use.
    In order to simplify things, we will just look at prioritizing what data we extract
    from the experience replay. By prioritizing the data we extract, we can hope this
    will dramatically improve the information we do feed to the network for learning
    and thus the whole performance of the agent.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何使用重放缓冲区或经验回放机制，使我们能够在稍后的时间以批量的方式拉回值，以便训练网络图。这些数据批次由随机样本组成，效果不错，但当然，我们可以做得更好。因此，我们不仅存储所有数据，还可以做出两个决定：存储哪些数据和哪些数据是优先使用的。为了简化问题，我们只需关注从经验回放中提取数据的优先级。通过优先提取数据，我们希望这能显著提高我们提供给网络用于学习的信息，从而提高整个智能体的性能。
- en: Unfortunately, the idea behind prioritizing the replay buffer is quite simple
    to grasp but far more difficult in practice to derive and estimate. What we can
    do, though, is prioritize the return events by the TD error or loss from the prediction
    and the actual expected target of that event. Thus, we prioritize the values the
    agent predicts where the most amount of error is or where the agent is wrong the
    most. Another way to think of this is that we prioritize the events that surprised
    the agent the most. The replay buffer is structured so that it prioritizes those
    events by surprise level and then returns a sample of those, except it doesn't
    necessarily order the events by surprise. Here, it's better to randomly sample
    the events from a bucket or distribution ordered by surprise. This means the agent
    would then be more inclined to choose samples from the more average surprising
    events.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，优先级回放背后的想法虽然简单易懂，但在实践中推导和估计则困难得多。我们可以做的是，通过TD误差或预测和实际期望目标的损失来优先级返回事件。因此，我们优先考虑智能体预测中误差最大或智能体错误最多的值。另一种思考方式是，我们优先考虑最令智能体惊讶的事件。回放缓冲区结构化得如此之好，以至于它通过惊讶程度优先级来优先考虑这些事件，然后返回这些事件的样本，但它并不一定按惊讶程度对事件进行排序。在这里，最好是从按惊讶程度排序的桶或分布中随机采样事件。这意味着智能体更有可能选择来自更平均的惊讶事件的样本。
- en: In this section, we'll use a Prioritized Experience Replay mechanism, which
    was first introduced in this paper: [https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf).
    It was then coded in PyTorch from this repository: [https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb).
    Our implementation has been modified to run outside a notebook and for Python
    3.6 ([https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用优先级经验回放机制，该机制首次在以下论文中介绍：[https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)。然后它被编码在PyTorch中，来自这个仓库：[https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb)。我们的实现已被修改，以便在笔记本外运行，并适用于Python
    3.6 ([https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb))).
- en: 'We will work with an entirely new sample. Open up `Chapter_7_DDQN_wprority.py`
    and follow these steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个全新的样本。打开 `Chapter_7_DDQN_wprority.py` 并按照以下步骤操作：
- en: 'The first big change in this sample is an upgrade from the `ReplayBuffer` class
    to `NaivePrioritizedBuffer`, as shown here:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个样本的第一个重大变化是将 `ReplayBuffer` 类升级到 `NaivePrioritizedBuffer`，如下所示：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This code naively assigns priorities based on observed error prediction. Then,
    it sorts those values based on priority order. It then randomly samples those
    events back. Again, since the sampling is random, but the samples are aligned
    by priority, random sampling will generally take the samples with an average error.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码天真地根据观察到的错误预测分配优先级。然后，它根据优先级顺序对这些值进行排序。接着，它随机抽取这些事件。再次，由于抽样是随机的，但样本是按优先级对齐的，因此随机抽样通常会抽取平均误差的样本。
- en: 'What happens is that by reordering the samples, we reorder to expected actual
    distribution of data. Therefore, to account for this, we introduce a new factor
    called **beta**, or **importance-sampling**. **Beta** allows us to control the
    distribution of events and essentially reset them to their original placement:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发生的事情是通过重新排序样本，我们重新排序到预期的实际数据分布。因此，为了解决这个问题，我们引入了一个新的因子，称为 **beta**，或 **重要性抽样**。**Beta**
    允许我们控制事件的分布，并基本上将它们重置到原始位置：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we will define a function to return an increasing beta over episodes using
    the preceding code. Then, the code plots beta much like we plot epsilon, as shown
    here:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个函数，使用前面的代码返回随剧集增加的 beta。然后，代码像我们绘制 epsilon 一样绘制 beta，如下所示：
- en: '![](img/ae38a330-3d0d-49bf-aeab-886984d7adda.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae38a330-3d0d-49bf-aeab-886984d7adda.png)'
- en: Example of beta and epsilon plots
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: beta 和 epsilon 绘图示例
- en: 'After modifying the sample function in the replay buffer, we also need to update
    the `compute_td_loss` function, as shown here:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在修改了重放缓冲区中的样本函数之后，我们还需要更新 `compute_td_loss` 函数，如下所示：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Only the preceding highlighted lines are different from what we have seen already.
    The first difference is the return of two new values: `indices` and `weights`.
    Then, we can see that `replay_buffer` calls `update_priorities` based on the previously
    returned `indices`:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有前面突出显示的行与我们已经看到的有所不同。第一个区别是返回了两个新的值：`indices` 和 `weights`。然后，我们可以看到 `replay_buffer`
    根据之前返回的 `indices` 调用 `update_priorities`：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, inside the training loop, we update the call to `play_game` and introduce
    a new `min_play_reward` threshold value. This allows us to set some minimum reward
    threshold before rendering the game. Rendering the game can be quite time-consuming
    and this will also speed up training:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在训练循环内部，我们更新了 `play_game` 的调用，并引入了一个新的 `min_play_reward` 阈值。这允许我们在渲染游戏之前设置一些最低奖励阈值。渲染游戏可能相当耗时，这将也会加快训练速度：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Continuing inside the training loop, we can see how we extract `beta` and use
    that in the `td_compute_loss` function.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续在训练循环内部，我们可以看到我们如何提取 `beta` 并在 `td_compute_loss` 函数中使用它。
- en: 'Run the sample again. This time, you may have to wait to see the agent drive
    the Lander but when it does, it will do quite well, as shown here:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次运行样本。这次，你可能需要等待看到智能体驾驶 Lander，但一旦它这样做，它将表现得相当不错，如下所示：
- en: '![](img/da8e4389-45ac-40c0-8f9d-04de5d01e475.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da8e4389-45ac-40c0-8f9d-04de5d01e475.png)'
- en: The agent landing the Lander
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体成功着陆 Lander
- en: Typically, in a reasonably short amount of time, the agent will be able to consistently
    land the Lander. The algorithm should converge to landing within 75,000 iterations.
    You can, of course, continue to tweak and play with the hyperparameters, but that
    is what our next section is for.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在合理短的时间内，智能体将能够持续地将 Lander 安全着陆。算法应该在 75,000 次迭代内收敛到着陆。当然，你可以继续调整和玩转超参数，但这正是我们下一节要讨论的内容。
- en: Exercises
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The further we progress in this book, the more valuable and expensive each
    of these exercises will become. By expensive, we mean the amount of time you need
    to invest in each will increase. That may mean you are inclined to do fewer exercises,
    but please continue to try and do two or three exercises on your own:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在这本书中的进步，这些练习的价值和成本将越来越大。这里的“昂贵”是指你需要投入每个练习的时间将增加。这可能意味着你倾向于做更少的练习，但请继续尝试自己完成两到三个练习：
- en: Revisit TensorSpace Playground and see if you can understand the difference
    pooling makes in those models. Remember that we avoid the use of pooling in order
    to avoid losing spatial integrity.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到 TensorSpace 游戏场，看看你是否能理解池化在这些模型中产生的差异。记住，我们避免使用池化是为了避免丢失空间完整性。
- en: Open `Chapter_7_DQN_CNN.py`and alter some of the convolutional layer inputs
    such as the kernel or stride size. See what effect this has on training.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_7_DQN_CNN.py`并修改一些卷积层的输入，例如内核或步长大小。看看这会对训练产生什么影响。
- en: Tune the hyperparameters or create new ones for `Chapter_7_DoubleDQN.py`.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_7_DoubleDQN.py`的超参数或创建新的超参数。
- en: Tune the hyperparameters or create new ones for `Chapter_7_DDQN.py`.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_7_DDQN.py`的超参数或创建新的超参数。
- en: Tune the hyperparameters or create new ones for `Chapter_7_DoubleDQN_wprority.py`.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_7_DoubleDQN_wprority.py`的超参数或创建新的超参数。
- en: Convert `Chapter_7_DoubleDQN.py` so that it uses convolutional layers and then
    upgrade the sample so that it works with an Atari environment such as Pong.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_7_DoubleDQN.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。
- en: Convert `Chapter_7_DDQN.py`so that it uses convolutional layers and then upgrade
    the sample so that it works with an Atari environment such as Pong.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_7_DDQN.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。
- en: Convert `Chapter_7_DDQN_wprority.py`so that it uses convolutional layers and
    then upgrade the sample so that it works with an Atari environment such as Pong.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Chapter_7_DDQN_wprority.py`转换为使用卷积层，然后升级示例，使其能够与Pong等Atari环境一起工作。
- en: Add a Pooling layer in-between the convolutional layers in one of the examples.
    You will likely need to consult the PyTorch documentation to learn how to do this.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个示例中的卷积层之间添加一个池化层。你可能需要查阅PyTorch文档来学习如何做到这一点。
- en: How else could you improve the experience replay buffer in the preceding example?
    Are there other forms of replay buffers you could use?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还能如何改进前面示例中的经验回放缓冲区？你还能使用其他形式的回放缓冲区吗？
- en: As always, have fun working through the samples. After all, if you are not happy
    watching your code play the Lunar Lander or an Atari game, when will you be?
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，享受通过示例工作的乐趣。毕竟，如果你不高兴看到你的代码在玩月球探险者或Atari游戏，你什么时候才会开心呢？
- en: In the next section, we'll wrap up this chapter and look at what we'll learn
    about next.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将总结本章内容，并看看我们将学习什么。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Extending from where we left off with DQN, we looked at ways of extending this
    model with CNN and adding additional networks to create double DQN and dueling
    DQN, or DDQN. Before exploring CNN, we looked at what visual observation encoding
    is and why we need it. Then, we briefly introduced CNN and used the TensorSpace
    Playground to explore some well-known, state-of-the-art models. Next, we added
    CNN to a DQN model and used that to play the Atari game environment Pong. After,
    we took a closer look at how we could extend DQN by adding another network as
    the target and adding another network to duel against or to contradict the other
    network, also known as the dueling DQN or DDQN. This introduced the concept of
    advantage in choosing an action. Finally, we looked at extending the experience
    replay buffer so that we can prioritize events that get captured there. Using
    this framework, we were able to easily land the Lander with just a short amount
    of agent training.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前中断的地方继续使用DQN，我们探讨了如何通过添加CNN来扩展这个模型，并添加额外的网络来创建双DQN和对抗DQN，或DDQN。在探索CNN之前，我们简要介绍了视觉观察编码及其必要性。然后，我们简要介绍了CNN，并使用TensorSpace
    Playground探索了一些知名的最先进模型。接下来，我们将CNN添加到DQN模型中，并使用它来玩Atari游戏环境Pong。之后，我们更详细地研究了如何通过添加另一个网络作为目标，并添加另一个网络来对抗或反驳其他网络来扩展DQN，这也被称为对抗DQN或DDQN。这引入了选择动作时的优势概念。最后，我们探讨了如何扩展经验回放缓冲区，以便我们可以优先处理那里捕获的事件。使用这个框架，我们能够仅通过少量代理训练就轻松地将Lander着陆。
- en: In the next chapter, we'll look at new ways of selecting policy methods and
    no longer look at global averages. Instead, we will sample distributions using
    policy gradient methods.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨新的策略选择方法，而不再关注全局平均值。相反，我们将使用策略梯度方法来采样分布。
