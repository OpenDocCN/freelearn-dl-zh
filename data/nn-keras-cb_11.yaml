- en: Building a Recurrent Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建循环神经网络
- en: In the previous chapter, we looked at multiple ways of representing text as
    a vector and then performed sentiment classification on top of those representations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了多种将文本表示为向量的方式，并在这些表示之上执行情感分类。
- en: One of the drawbacks of this approach is that we did not take the order of words
    into consideration—for example, the sentence *A is faster than B* would have the
    same representation as *B is faster than A*, as the words in both sentences are
    exactly the same, while the order of words is different.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是我们没有考虑到单词的顺序——例如，句子 *A is faster than B* 和 *B is faster than A* 将有相同的表示，因为这两个句子中的单词完全相同，而单词的顺序不同。
- en: '**Recurrent neural networks** (**RNNs**) come in handy in scenarios when the
    word order needs to be preserved. In this chapter, you will learn about the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）在需要保留单词顺序的场景中非常有用。在本章中，您将学习以下主题：'
- en: Building RNN and LSTM from scratch in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始在 Python 中构建 RNN 和 LSTM
- en: Implementing RNN for sentiment classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 RNN 进行情感分类
- en: Implementing LSTM for sentiment classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 LSTM 进行情感分类
- en: Implementing stacked LSTM for sentiment classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现堆叠 LSTM 进行情感分类
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'RNN can be architected in multiple ways. Some of the possible ways are as follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 可以通过多种方式架构。以下是一些可能的方式：
- en: '![](img/79db1776-f471-4fe6-89b0-67cbae844bfc.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79db1776-f471-4fe6-89b0-67cbae844bfc.png)'
- en: 'The box in the bottom is the input, followed by the hidden layer (as the middle
    box), and the box on top is the output layer. The one-to-one architecture is the
    typical neural network with a hidden layer between the input and the output layer.
    The examples of different architectures are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的框是输入，接着是隐藏层（中间的框），顶部是输出层。此一对一架构是典型的神经网络，输入层和输出层之间有一个隐藏层。不同架构的示例如下：
- en: '| **Architecture** | **Example** |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **示例** |'
- en: '| One-to-many | Input is image and output is caption of image |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 一对多 | 输入是图像，输出是图像的描述 |'
- en: '| Many-to-one | Input is a movie''s review (multiple words in input) and output
    is sentiment associated with the review |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 多对一 | 输入是电影的评论（输入中包含多个单词），输出是与评论相关的情感 |'
- en: '| Many-to-many | Machine translation of a sentence in one language to a sentence
    in another language |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 多对多 | 将一种语言的句子翻译成另一种语言的句子 |'
- en: Intuition of RNN architecture
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 架构的直觉
- en: RNN is useful when we want to predict the next event given a sequence of events.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望在给定一系列事件时预测下一个事件时，RNN 非常有用。
- en: An example of that could be to predict the word that comes after *This is an
    _____*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个例子是预测 *This is an _____* 后面跟的单词。
- en: Let's say, in reality, the sentence is *This is an example*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在现实中，句子是 *This is an example*。
- en: 'Traditional text-mining techniques would solve the problem in the following
    way:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的文本挖掘技术通常通过以下方式解决问题：
- en: 'Encode each word while having an additional index for potential new words:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个单词进行编码，同时为潜在的新单词设置额外的索引：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Encode the phrase `This is an`:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码短语 `This is an`：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the training dataset:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练数据集：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Build a model with input and output
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个包含输入和输出的模型
- en: One of the major drawbacks of the model is that the input representation does
    not change in the input sentence; it is either `this is an`, or `an is this`,
    or `this an is`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的一个主要缺点是输入表示在输入句子中没有变化；它可以是 `this is an`、`an is this` 或 `this an is`。
- en: 'However, intuitively, we know that each of the preceding sentences is different
    and cannot be represented by the same structure mathematically. This calls for
    having a different architecture, which looks as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直观上，我们知道前面每个句子的结构是不同的，不能用相同的数学结构表示。这就需要采用不同的架构，结构如下：
- en: '![](img/a4da81fc-763e-4ed2-b638-f3694aebd985.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4da81fc-763e-4ed2-b638-f3694aebd985.png)'
- en: In the preceding architecture, each of the individual words from the sentence
    enter into individual box among the input boxes. However the structure of the
    sentence will be preserved, for example `this` enters the first box, `is` enters
    second box and `an` enters the third box.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的架构中，句子中的每个单词都会进入输入框中的独立框中。然而，句子的结构将得到保留。例如，`this` 进入第一个框，`is` 进入第二个框，`an`
    进入第三个框。
- en: The output box at the top will be the output that is `example`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的输出框将是 `example` 的输出。
- en: Interpreting an RNN
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释 RNN
- en: 'You can think of RNN as a mechanism to hold memory—where the memory is contained
    within the hidden layer. It can be visualized as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 RNN 看作一种存储记忆的机制——其中记忆被保存在隐藏层内。它可以被如下方式可视化：
- en: '![](img/ef7343c2-b306-43b0-8e2d-3dc8f10fa443.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef7343c2-b306-43b0-8e2d-3dc8f10fa443.png)'
- en: The network on the right is an unrolled version of the network on the left. The
    network on the right takes one input in each time step and extracts the output
    at each time step. However, if we are interested in the output in the fourth time
    step, we'll provide input in the previous three time steps and the output of the
    third time step is the predicted value for fourth time step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的网络是左侧网络的展开版本。右侧的网络在每个时间步输入一个数据，并在每个时间步提取输出。然而，如果我们关注第四个时间步的输出，我们需要提供前三个时间步的输入，而第三个时间步的输出则是第四个时间步的预测值。
- en: Note that, while predicting the output of third time step, we are incorporating
    values from the first three time steps through the hidden layer, which is connecting
    the values across time steps.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在预测第三个时间步的输出时，我们通过隐藏层引入了前两个时间步的值，隐藏层将跨时间步连接这些值。
- en: 'Let''s explore the preceding diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索前面的图示：
- en: The **U** weight represents the weights that connect the input layer to the
    hidden layer
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**U** 权重表示连接输入层与隐藏层的权重'
- en: The **W** weight represents the hidden-layer-to-hidden-layer connection
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W** 权重表示隐藏层之间的连接'
- en: The **V** weight represents the hidden-layer-to-output-layer connection
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V** 权重表示隐藏层与输出层之间的连接'
- en: Why store memory?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要存储记忆？
- en: There is a need to store memory, as in the preceding example or even in text-generation
    in general, the next word does not necessarily depend only on the preceding word,
    but the context of the words preceding the word to predict.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 需要存储记忆，就像前面的示例中，甚至在文本生成中，下一词不一定仅依赖于前一个词，而是依赖于前面词语的上下文。
- en: Given that we are looking at the preceding words, there should be a way to keep
    them in memory, so that we can predict the next word more accurately.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们关注前面的词语，应该有一种方式来将它们保存在记忆中，以便我们能够更准确地预测下一个词。
- en: Moreover, we should also have the memory in order; that is, more often than
    not, the recent words are more useful in predicting the next word than the words
    that are far away from the word to predict.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要按顺序存储记忆；也就是说，通常情况下，最近的词语比距离要预测的词更远的词语更有助于预测下一个词。
- en: Building an RNN from scratch in Python
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始在 Python 中构建 RNN
- en: In this recipe, we will build an RNN from scratch using a toy example, so that
    you gain a solid intuition of how RNN helps in solving the problem of taking the
    order of events (words) into consideration.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将从头开始构建一个 RNN，使用一个简单的示例，以帮助你更好地理解 RNN 如何帮助解决考虑事件（单词）顺序的问题。
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: Note that a typical NN has an input layer, followed by an activation in the
    hidden layer, and then a softmax activation at the output layer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，典型的神经网络包含一个输入层，之后是隐藏层的激活，最后是输出层的 softmax 激活。
- en: RNN follows a similar structure, with modifications done in such a way that
    the hidden layers of the previous time steps are considered in the current time
    step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 采用类似的结构，并在这种结构上进行修改，使得前一个时间步的隐藏层被考虑在当前时间步中。
- en: We'll build the working details of RNN with a simplistic example before implementing
    it on more practical use cases.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 RNN 应用于更实际的用例之前，我们会先通过一个简单的示例构建 RNN 的工作细节。
- en: 'Let''s consider an example text that looks as follows: `This is an example`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个如下的示例文本：`This is an example`。
- en: The task at hand is to predict the third word given a sequence of two words.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当前任务是给定一对单词序列，预测第三个词。
- en: 'So, the dataset translates as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集的转换如下：
- en: '| **Input** | **Output** |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **Input** | **Output** |'
- en: '| `this, is` | `an` |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `this, is` | `an` |'
- en: '| `is, an` | `example` |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `is, an` | `example` |'
- en: Given an input of `this is`, we are expected to predict `example` as the output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入 `this is`，我们期望预测 `example` 作为输出。
- en: 'The strategy that we''ll adopt to build an RNN is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下策略来构建一个 RNN：
- en: One-hot encode the words
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对单词进行独热编码
- en: 'Identify the maximum length of input:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定输入的最大长度：
- en: Pad the rest of input to the maximum length so that all the inputs are of the
    same length
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其余输入填充至最大长度，使得所有输入的长度相同
- en: Convert the words in the input into a one-hot-encoded version
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入中的单词转换为一个独热编码版本
- en: Convert the words in the output into a one-hot-encoded version
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出中的单词转换为一个独热编码版本
- en: Process the input and the output data, then fit the RNN model
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理输入和输出数据，然后拟合RNN模型
- en: How to do it...
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The strategy discussed above is coded as follows (the code file is available
    as `Building_a_Recurrent_Neural_Network_from_scratch-in_Python.ipynb` in GitHub):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略的代码如下（代码文件可在GitHub的`Building_a_Recurrent_Neural_Network_from_scratch-in_Python.ipynb`中找到）：
- en: 'Let''s define the input and output in code, as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在代码中定义输入和输出，如下所示：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s preprocess our dataset so that it can be passed to an RNN:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对数据集进行预处理，以便将其传递给RNN：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding step, we are identifying all the unique words and their corresponding
    frequency (counts) in a given dataset, and we are assigning an ID number to each
    word. The output of `word_to_int` in the preceding code looks as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们识别了数据集中所有唯一的单词及其对应的频率（计数），并为每个单词分配了一个ID号。在前面的代码中，`word_to_int`的输出如下所示：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Modify the input and output words with their corresponding IDs, as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照如下方式修改输入和输出单词及其对应的ID：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, we are appending the ID of each word of an input sentence
    into a list, thus making the input (`encoded_docs`) a list of lists.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将输入句子中每个单词的ID附加到一个列表中，从而使输入（`encoded_docs`）成为一个包含列表的列表。
- en: Similarly, we are appending the ID of each word of the output into a list.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将输出中每个单词的ID附加到一个列表中。
- en: 'One additional factor to take care of while encoding the input is the input
    length. In cases of sentiment analysis, the input text length can vary from one
    review to another. However, the neural network expects the input size to be fixed.
    To get around this problem, we perform padding on top of the input. Padding ensures
    that all inputs are encoded to have a similar length. While the lengths of both
    examples in our case is 2, in practice, we are very likely to face the scenario
    of differing lengths between input. In code, we perform padding as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编码输入时需要考虑的另一个因素是输入的长度。在情感分析的情况下，输入文本的长度可能因评论而异。然而，神经网络要求输入的大小是固定的。为了解决这个问题，我们在输入上进行填充。填充确保所有输入被编码为相似的长度。虽然在我们的示例中两个输入的长度都是2，但实际上我们很可能会遇到输入长度不同的情况。在代码中，我们按如下方式进行填充：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, we are passing `encoded_docs` to the `pad_sequences`
    function, which ensures that all the input data points have the same length—which
    is equal to the `maxlen` parameter. Additionally, for those parameters that are
    shorter than `maxlen`, it pads those data points with a value of 0 to achieve
    a total length of `maxlen` and the zero padding is done `pre`—that is, to the
    left of the original encoded sentence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将`encoded_docs`传递给`pad_sequences`函数，确保所有输入数据点的长度相同——即等于`maxlen`参数。此外，对于那些长度小于`maxlen`的参数，它会用0填充这些数据点，直到达到`maxlen`的总长度，并且零填充会在`pre`位置完成——也就是在原始编码句子的左侧。
- en: Now that the input dataset is created, let's preprocess the output dataset so
    that it can be passed to the model-training step.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在输入数据集已经创建完毕，接下来让我们对输出数据集进行预处理，以便将其传递给模型训练步骤。
- en: 'The typical processing for outputs is to make them into dummy values, that
    is, make a one-hot-encoded version of the output labels, which is done as follows:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输出的典型处理是将其转换为虚拟值，即制作输出标签的独热编码版本，方法如下：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that, given the output values (`encoded_labels`) are `{2, 4}`, the output
    vectors have a value of 1 at the second and fourth positions, respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，给定输出值（`encoded_labels`）为`{2, 4}`，输出向量在第二和第四位置分别为1。
- en: 'Let''s build the model:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建模型：
- en: 'An RNN expects the input to be (`batch_size`, `time_steps`, and `features_per_timestep`)
    in shape. Hence, we first reshape the `padded_docs` input into the following format:'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN期望输入的形状为（`batch_size`、`time_steps`和`features_per_timestep`）。因此，我们首先将`padded_docs`输入重塑为以下格式：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that ideally we would have created a word embedding for each word (ID in
    this specific case). However, given that the intent of this recipe is only to
    understand the working details of RNN, we will exclude the embedding of IDs and
    assume that each input is not an ID but a value. Having said that, we will learn
    how to perform ID-embedding in the next recipe.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，理想情况下我们会为每个单词（在这个特定情况下是ID）创建一个词嵌入。然而，鉴于本教程的目的是了解RNN的工作细节，我们将不涉及ID的嵌入，并假设每个输入不是ID而是一个值。话虽如此，我们将在下一个教程中学习如何执行ID嵌入。
- en: 'Define the model—where we are specifying that we will initialize an RNN by
    using the `SimpleRNN` method:'
  id: totrans-89
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型——在这里我们指定将使用`SimpleRNN`方法初始化RNN：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding step, we explicitly specified the `recurrent_initializer` to
    be zero so that we understand the working details of RNN more easily. In practice,
    we would not be initializing the recurrent initializer to 0.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们明确指定了`recurrent_initializer`为零，这样可以更容易理解RNN的工作细节。实际上，我们不会将循环初始化器设置为0。
- en: The `return_sequences` parameter specifies whether we want to obtain the hidden
    layer values at each time step. A false value for `return_sequences` specifies
    that we want the hidden layer output only at the final timestep.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`return_sequences`参数指定是否希望在每个时间步获得隐藏层值。若`return_sequences`为false，表示我们只希望在最后一个时间步获得隐藏层输出。'
- en: Typically, in a many-to-one task, where there are many inputs (one input in
    each time step) and outputs, `return_sequences` will be false, resulting in the
    output being obtained only in the final time step. An example of this could be
    the stock price on the next day, given a sequence of historical five-day stock
    prices.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在多对一任务中，当输入很多（每个时间步一个输入）并且有输出时，`return_sequences`会设置为false，这样输出仅会在最后一个时间步获得。例如，给定过去五天的股票价格序列，预测第二天的股票价格就是一个典型的例子。
- en: However, in cases where we try to obtain hidden-layer values in each time step,
    `return_sequences` will be set to `True`. An example of this could be machine
    translation, where there are many inputs and many outputs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在尝试在每个时间步获取隐藏层值的情况下，`return_sequences`将被设置为`True`。例如，机器翻译就是一个例子，其中有多个输入和多个输出。
- en: 'Connect the RNN output to five nodes of the output layer:'
  id: totrans-95
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将RNN输出连接到输出层的五个节点：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have performed a `Dense(5)`, as there are five possible classes of output
    (the output of each example has 5 values, where each value corresponds to the
    probability of it belonging to word ID `0` to word ID `4`).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经执行了一个`Dense(5)`，因为有五个可能的输出类别（每个样本的输出有5个值，每个值对应它属于`word ID 0`到`word ID 4`的概率）。
- en: 'Compile and summarize the model:'
  id: totrans-98
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并总结模型：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A summary of model is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型总结如下：
- en: '![](img/54388466-b7f3-4314-a618-964e8acfd193.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54388466-b7f3-4314-a618-964e8acfd193.png)'
- en: Now that we have defined the model, before we fit the input and output, let's
    understand the reason why there are certain number of parameters in each layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，在拟合输入和输出之前，让我们了解每一层中参数数量的原因。
- en: The simple RNN part of the model has an output shape of `(None, 1)`. The `None` in
    the output shape represents the `batch_size`. None is a way of specifying that
    the `batch_size` could be any number. Given that we have specified that there
    shall be one unit of hidden that is to be outputted from simple RNN, the number
    of columns is one.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的简单RNN部分的输出形状为`(None, 1)`。输出形状中的`None`表示`batch_size`。None是指定`batch_size`可以是任意数值的方式。由于我们指定了从简单RNN中输出一个单位的隐藏层，列数为1。
- en: Now that we understand the output shape of `simpleRNN`, let's understand why
    the number of parameters is three in the `simpleRNN` layer. Note that the hidden-layer
    value is outputted at the final time step. Given that the input has a value  of
    one in each time step (one feature per time step) and the output is also of one
    value, the input is essentially multiplied by a weight that is of a single value.
    Had the output (hidden-layer value) been 40 units of hidden-layer values, the
    input should have been multiplied by 40 units to get the output (more on this
    in the *Implementing RNN for sentiment classification*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了`simpleRNN`的输出形状，让我们理解为什么在`simpleRNN`层中参数数量是3。注意，隐藏层值只在最后一个时间步输出。鉴于每个时间步的输入值为1（每个时间步一个特征），而输出也为一个值，输入实际上是与一个单一的权重相乘。如果输出（隐藏层值）有40个隐藏层单元，输入就应该与40个单位相乘来获得输出（更多内容请参见*实现RNN进行情感分类*）。
- en: recipe). Apart from the one weight connecting the input to the hidden-layer
    value, there is a bias term that accompanies the weight. The other 1 parameter
    comes from the connection of the previous time step's hidden layer value to the
    current time step's hidden layer, resulting in a total of three parameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了连接输入和隐藏层值的一个权重外，还有一个与权重一起的偏置项。另一个参数来自于前一个时间步的隐藏层值与当前时间步隐藏层的连接，最终得到三个参数。
- en: There are 10 parameters from the hidden layer to the final output as there are
    five possible classes resulting in five weights and five biases connecting the
    hidden-layer value (which is of one unit)—a total of 10 parameters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 从隐藏层到最终输出有10个参数，因为有五个可能的类别，结果是五个权重和五个偏置连接到隐藏层的值（该值是一个单位）——总共10个参数。
- en: 'Fit the model to predict the output from the input:'
  id: totrans-107
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型以预测从输入到输出的结果：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Extract prediction on the first input data point
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取第一个输入数据点的预测值：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The extracted output is as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的输出如下：
- en: '[PRE15]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Validating the output
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证输出结果
- en: Now that the model is fit, let's gain an understanding of how an RNN works by
    working backward—that is, extract the weights of model, feed forward the input
    through the weights to match the predicted value using NumPy (the code file is
    available as `Building_a_Recurrent_Neural_Network_from_scratch_in_Python.ipynb` in
    GitHub).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练完成，让我们通过从后向前分析来理解RNN的工作原理——也就是说，提取模型的权重，通过权重前馈输入以匹配预测值，使用NumPy（代码文件可以在GitHub上找到，名为`Building_a_Recurrent_Neural_Network_from_scratch_in_Python.ipynb`）。
- en: 'Inspect the weights:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查权重：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding gives us an intuition of the order in which weights are presented
    in the output.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容为我们提供了权重在输出中呈现的顺序的直觉。
- en: In the preceding example, `kernel` represents the weights and `recurrent` represents
    the connection of the hidden layer from one step to another.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，`kernel`表示权重，`recurrent`表示隐藏层从一个时刻到另一个时刻的连接。
- en: Note that a `simpleRNN` has weights that connect the input to the hidden layer
    and also weights that connect the previous time step's hidden layer to the current
    time step's hidden layer.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`simpleRNN`有连接输入层到隐藏层的权重，也有连接前一时刻隐藏层到当前时刻隐藏层的权重。
- en: 'The kernel and bias in the `dense_2` layer represent the layer that connects
    the hidden layer value to the final output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`dense_2`层中的核和偏置表示连接隐藏层值到最终输出的层：'
- en: 'Extract weights:'
  id: totrans-121
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取权重：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding line of code gives us the computed values of each of the weights.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码行给出了每个权重的计算值。
- en: 'Pass the input through the first time step—the input is as follows:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入通过第一时刻传递——输入值如下：
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the preceding code, the first time step has a value of `3` and the second
    time step has a value of `1`. We''ll initialize the value at first time step as
    follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，第一时刻的值为`3`，第二时刻的值为`1`。我们将按如下方式初始化第一时刻的值：
- en: '[PRE19]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The value at the first time step is multiplied by the weight connecting the
    input to the hidden layer, then the bias value is added:'
  id: totrans-128
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一时刻的值与连接输入到隐藏层的权重相乘，然后加上偏置值：
- en: '[PRE20]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The hidden layer value at this time step is calculated by passing the preceding
    output through the `tanh` activation (as that is the activation we specified when
    we defined the model):'
  id: totrans-130
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时刻的隐藏层值通过`tanh`激活函数计算得出（因为这是我们在定义模型时指定的激活函数）：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Calculate the hidden-layer value at time step 2; where the input has a value
    of `1` (note the value of `padded_docs[0]` is `[3, 1]`):'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算时间步2时的隐藏层值；此时输入的值为`1`（注意`padded_docs[0]`的值为`[3, 1]`）：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output value when the input at the second time step is passed through the
    weight and bias is as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当第二个时间步的输入通过权重和偏置时，输出值如下：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the weights that multiply the input remain the same, regardless of
    the time step being considered.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，乘以输入的权重在任何时间步中都是相同的。
- en: 'The calculation for the hidden-layer at various time steps is performed as
    follows:'
  id: totrans-137
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在不同时间步计算隐藏层的过程如下：
- en: '![](img/0fe5cacd-023b-479f-8a90-874d5b0f3c42.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fe5cacd-023b-479f-8a90-874d5b0f3c42.png)'
- en: Where *Φ* is an activation that is performed (In general, `tanh` activation
    is used).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*Φ*是执行的激活（通常使用`tanh`激活）。
- en: 'The calculation from the input layer to the hidden-layer constitutes two components:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入层到隐藏层的计算包含两个部分：
- en: Matrix multiplication of the input layer value and kernel weights
  id: totrans-141
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层值与核权重的矩阵乘法
- en: Matrix multiplication of the hidden layer of the previous time step and recurrent
    weights
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一时刻隐藏层和循环权重的矩阵乘法
- en: 'The final calculation of the hidden-layer value at a given time step would
    be the summation of the preceding two matrix multiplications. Pass the result
    through a tanh activation function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 给定时间步的最终隐藏层值的计算将是前两次矩阵乘法的总和。将结果通过`tanh`激活函数处理：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The total value before passing through the `tanh` activation is as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过`tan`激活之前的总值如下：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the hidden-layer value is calculated by passing the preceding
    output through the `tanh` activation, as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的输出值通过`tan`激活函数计算，具体如下：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Pass the hidden layer output from the final time step through the dense layer,
    which connects the hidden layer to the output layer:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最后一个时间步的隐藏层输出通过全连接层传递，该层将隐藏层与输出层连接：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that the fourth and fifth output of the `model.get_weights()` method correspond
    to the connection from the hidden layer to the output layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`model.get_weights()`方法的第四和第五个输出对应的是从隐藏层到输出层的连接。
- en: 'Pass the preceding output through the softmax activation (as defined in the
    model) to obtain the final output:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前面的输出通过softmax激活函数（如模型中定义）传递，以获得最终输出：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You should notice that the output we obtained through the forward pass of input
    through the network is the same as what the `model.predict` function gave as output.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，通过网络的前向传播得到的输出与`model.predict`函数输出的结果是相同的。
- en: Implementing RNN for sentiment classification
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现RNN进行情感分类
- en: To understand how RNN is implemented in Keras, let's implement the airline-tweet
    sentiment classification exercise that we performed in the [Chapter 10](79b072ff-23b6-47ef-80bc-d9400a855714.xhtml),
    *Text Analysis Using Word Vectors* chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何在Keras中实现RNN，让我们实现我们在[第10章](79b072ff-23b6-47ef-80bc-d9400a855714.xhtml)，“*使用词向量进行文本分析*”章节中进行的航空公司推文情感分类练习。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The task would be performed as follows (the code file is available as `RNN_and_LSTM_sentiment_classification.ipynb`
    in GitHub):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 任务将按以下方式执行（代码文件在GitHub上可用，名为`RNN_and_LSTM_sentiment_classification.ipynb`）：
- en: 'Import the relevant packages and dataset:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包和数据集：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Preprocess the text to remove punctuation, normalize all words to lowercase,
    and remove the stopwords, as follows:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本进行预处理，移除标点符号，将所有单词标准化为小写，并移除停用词，如下所示：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Extract the word-to-integer mapping of all the words that constitute the dataset:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取构成数据集的所有单词到整数的映射：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the preceding step, we are extracting the frequency of all the words in
    the dataset. A sample of extracted words are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步骤中，我们提取了数据集中所有单词的频率。提取的部分单词示例如下：
- en: '![](img/f9b4e5b9-a37a-4944-941f-6647af1ba595.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9b4e5b9-a37a-4944-941f-6647af1ba595.jpg)'
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the preceding code, we are looping through all the words and are assigning
    an index for each word. A sample of integer to word dictionary is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们遍历所有单词，并为每个单词分配一个索引。整数到单词字典的示例部分如下：
- en: '![](img/c12eb915-d624-4999-b83c-6daeff8e36e0.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c12eb915-d624-4999-b83c-6daeff8e36e0.jpg)'
- en: 'Map each word in a given sentence to the corresponding word associated with
    it:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将给定句子中的每个单词映射到与其相关联的单词：
- en: '[PRE33]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In the preceding step, we are converting a text review into a list of lists
    where each list constitutes the ID of words contained in a sentence. A sample
    of original and mapped review is as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步骤中，我们将文本评论转换为一个包含多个列表的列表，每个列表包含一个句子中单词的ID。原始评论和映射评论的示例如下：
- en: '![](img/04c3a122-9b47-4e60-87a5-bf3bed019521.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04c3a122-9b47-4e60-87a5-bf3bed019521.jpg)'
- en: 'Extract the maximum length of a sentence and normalize all sentences to the
    same length by padding them. In the following code, we are looping through all
    the reviews and storing the length corresponding to each review. Additionally,
    we are also calculating the maximum length of a review (tweet text):'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取句子的最大长度，并通过填充将所有句子标准化为相同的长度。在以下代码中，我们循环遍历所有评论并存储每个评论对应的长度。此外，我们还在计算评论（推文文本）的最大长度：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We should note that different tweets have different lengths. However, RNN expects
    the number of time steps for each input to be the same. In the code below, we
    are padding a mapped review  with a value of 0, if the length of the review is
    less than the maximum length of all reviews in dataset. This way, all inputs will
    have the same length.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，不同的推文长度不同。然而，RNN期望每个输入的时间步数相同。在下面的代码中，如果评论的长度小于数据集中所有评论的最大长度，我们会通过值为0的填充对评论进行映射。这样，所有输入都会有相同的长度。
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Prepare the training and test datasets:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备训练集和测试集：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: In the preceding step, we are splitting the original data into the train and
    test datasets, and are converting the dependent variable into a one-hot-encoded
    variable.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步骤中，我们将原始数据拆分为训练集和测试集，并将因变量转换为独热编码变量。
- en: 'Build the RNN architecture and compile the model:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 RNN 架构并编译模型：
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note that embedding takes the total number of distinct words as input, and
    creates a vector for each word, where `output_dim` represents the number of dimensions
    in which the word is to be represented. `input_length` represents the number of
    words in each sentence:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，嵌入层以所有独特词汇的总数作为输入，并为每个词汇创建一个向量，其中`output_dim`表示词汇表示的维度数，`input_length`表示每个句子的词汇数：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Note that, in the RNN layer, if we want to extract the output of each time
    step, we say the `return_sequences` parameter is `True`. However, in the use case
    that we are solving now, we extract the output only after reading through all
    the input words and thus `return_sequences = False`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 RNN 层中，如果我们希望提取每个时间步的输出，我们会将`return_sequences`参数设置为`True`。然而，在我们当前要解决的用例中，我们只在读取完所有输入词汇后才提取输出，因此`return_sequences
    = False`：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The summary of model is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/7d2328cb-7549-4f74-9844-a2f5f3d89b04.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d2328cb-7549-4f74-9844-a2f5f3d89b04.png)'
- en: Let's understand why there are `401056` parameters to be estimated in the embedding
    layer. There are a total of 12, 532 unique words, and if we consider that there
    is no word with an index of 0, it results in a total of 12,533 possible words
    where each is represented in 32 dimensions and hence (12,533 x 32 = 401,056) parameters
    to be estimated.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解为什么在嵌入层中有`401056`个参数需要估计。总共有12,532个独特的词汇，如果我们考虑到没有索引为0的词汇，总共有12,533个可能的词汇，每个词汇在32维度中表示，因此需要估计（12,533
    x 32 = 401,056）个参数。
- en: Now, let's try to understand why there are 2,920 parameters in the `simpleRNN`
    layer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们试着理解为什么在`simpleRNN`层中有2,920个参数。
- en: There is a set of weights that connect the input to the 40 units of RNN. Given
    that there are 32 inputs at each time step (where the same set of weights is repeated
    for each time step), a total of 32 x 40 weights is used to connect the input to
    the hidden layer. This gives an output that is of 1 x 40 in dimension for each
    input.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有一组权重将输入连接到 RNN 的 40 个单元。鉴于每个时间步有 32 个输入（同一组权重在每个时间步重复），因此总共有 32 x 40 个权重用于将输入连接到隐藏层。这为每个输入提供了
    1 x 40 的输出。
- en: Additionally, for the summation between the *X * W[xh]* and *h[(t-1) *] W[hh]* to
    happen (where *X* is is the input values, *W*[*xh* ]is the weights-connecting
    input layer to the hidden layer, *W[hh]* is the weights-connecting the previous
    time step's hidden layer to the current time step's hidden layer, and *h[(t-1)]*
    is the hidden layer of previous time step)—given that the output of the *X W[xh]* input
    is 1 x 40—the output of *h[(t-1) ]X W[hh]* should also be 1 x 40 in size. Thus,
    the *W[hh]* matrix will be 40 x 40 in dimension, as the dimensions of  *h[(t-1)]* are
    1 x 40.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使 *X * W[xh]* 和 *h[(t-1) *] W[hh]* 之间的求和发生（其中 *X* 是输入值，*W*[*xh*] 是连接输入层和隐藏层的权重，*W[hh]*
    是连接前一个时间步的隐藏层与当前时间步隐藏层的权重，而 *h[(t-1)]* 是前一个时间步的隐藏层）——鉴于 *X W[xh]* 输入的输出是1 x 40——*h[(t-1)]
    X W[hh]* 的输出也应该是 1 x 40。因此，*W[hh]* 矩阵的维度将是 40 x 40，因为 *h[(t-1)]* 的维度是 1 x 40。
- en: Along with weights, we would also have 40 bias terms associated with each of
    the 40 output and thus a total of (32 x 40 + 40 x 40 + 40 = 2,920) weights.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 除了权重外，我们还将为每个40个输出单元设置40个偏置项，因此总共有（32 x 40 + 40 x 40 + 40 = 2,920）个权重。
- en: There are a total of 82 weights in the final layer, as the 40 units of the final
    time step are connected to the two possible output, resulting 40 x 2 weights and
    2 biases, and thus a total of 82 units.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层总共有82个权重，因为最后一个时间步的40个单元与两个可能的输出相连接，结果是 40 x 2 个权重和 2 个偏置项，因此总共有82个单元。
- en: 'Fit the model:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配模型：
- en: '[PRE40]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The plot of accuracy and loss values in training, test dataset are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据集中的准确率和损失值的图示如下：
- en: '![](img/afc6244e-0a9d-463a-956d-c6b4956c784d.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afc6244e-0a9d-463a-956d-c6b4956c784d.jpg)'
- en: The output of the preceding model is also ~89%, and does not offer any significant
    improvement over the word-vector-based network that we built in *Text analysis
    using word vectors* chapter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型的输出大约为89%，并未比我们在*使用词向量进行文本分析*章节中构建的基于词向量的网络提供显著改进。
- en: However, it is expected to have better accuracy as the number of data points
    increases.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据点数量的增加，预计会有更好的准确性。
- en: There's more...
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'A traditional RNN that takes multiple time steps into account for giving predictions
    can be visualized as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个考虑多个时间步给出预测的传统 RNN 可以如下可视化：
- en: '![](img/a7fcb1be-8d05-49f7-811f-8f716f209092.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a7fcb1be-8d05-49f7-811f-8f716f209092.png)'
- en: 'Notice that, as time step increases, the impact of input at a much earlier
    layer would be lower. An intuition of that can be seen here (for a moment, let''s
    ignore the bias terms):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随着时间步的增加，早期层次的输入影响会变得较小。可以在这里看到这种直觉（暂时忽略偏置项）：
- en: '*h[5] = WX[5] + Uh[4] = WX5 + UWX[4] + U²WX[3] + U³WX[2] + U⁴WX[1]*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*h[5] = WX[5] + Uh[4] = WX5 + UWX[4] + U²WX[3] + U³WX[2] + U⁴WX[1]*'
- en: You can see that, as the time step increases, the value of hidden layer is highly
    dependent on *X[1]* if *U>1*, or much less dependent on *X[1]* if *U<1*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，随着时间步的增加，当*U>1*时，隐藏层的值高度依赖于*X[1]*；当*U<1*时，依赖性则会大大降低。
- en: The dependency on U matrix can also result in vanishing gradient when the value
    of *U* is very small and can result in exploding gradient when the value of *U*
    is very high.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对U矩阵的依赖也可能导致梯度消失问题，当*U*的值非常小的时候；当*U*的值非常大时，也可能导致梯度爆炸问题。
- en: The above phenomenon results in an issue when there is a long-term dependency
    in predicting the next word. To solve this problem, we'll use the **Long Short
    Term Memory** (**LSTM**) architecture in the next recipe.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上述现象导致在预测下一个词时，若存在长期依赖性，会出现问题。为了解决这个问题，我们将在下一个教程中使用**长短期记忆**（**LSTM**）架构。
- en: Building a LSTM Network from scratch in Python
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始在Python中构建LSTM网络
- en: 'In the previous section on issues with traditional RNN, we learned about how
    RNN does not help when there is a long-term dependency. For example, imagine the
    input sentence is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节关于传统RNN问题的部分，我们了解到当存在长期依赖性时，RNN并不能有效工作。例如，假设输入句子如下：
- en: '*I live in India. I speak ____.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*I live in India. I speak ____.*'
- en: The blank space in the preceding statement could be filled by looking at the
    key word, *India*, which is three time steps prior to the word we are trying to
    predict.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述语句中，空白处可以通过查看关键词*India*来填充，而*India*距我们要预测的词有三个时间步的距离。
- en: In a similar manner, if the key word is far away from the word to predict, vanishing/exploding
    gradient problems need to be solved.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，如果关键词离要预测的词很远，梯度消失/爆炸问题需要被解决。
- en: Getting ready
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we'll learn how LSTM helps in overcoming the long-term dependency
    drawback of the RNN architecture and also build a toy example so that we understand
    the various components of LSTM.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将学习LSTM如何帮助克服RNN架构的长期依赖性问题，并构建一个简单的示例，以便我们理解LSTM的各个组件。
- en: 'LSTM looks as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM结构如下：
- en: '![](img/f2243925-4b0f-400b-bb2d-6d485e0d1401.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2243925-4b0f-400b-bb2d-6d485e0d1401.png)'
- en: You can see that while the input, **X**, and the output of the hidden layer,
    (**h**), remain the same, there are different activations that happen in the hidden
    layer (sigmoid activation in certain cases and tanh activation in others).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，虽然输入**X**和隐藏层输出（**h**）保持不变，但隐藏层内会发生不同的激活（某些情况下是sigmoid激活，其他情况下是tanh激活）。
- en: 'Let''s closely examine the various activations that happen in one of the time
    steps:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细检查一个时间步内发生的各种激活：
- en: '![](img/4c51bfa7-310d-4ee5-97dd-aff0de7c9669.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c51bfa7-310d-4ee5-97dd-aff0de7c9669.png)'
- en: In the preceding diagram, **X** and **h** represent the input layer and the
    hidden layer.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，**X**和**h**分别代表输入层和隐藏层。
- en: The longterm memory is stored in cell state **C**.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆存储在单元状态**C**中。
- en: 'The content that needs to be forgotten is obtained using the forget gate:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 需要遗忘的内容是通过遗忘门获得的：
- en: '*f[t]=σ(W[xf]x^((t))+W[hf]h^((t-1))+b[f])*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*f[t]=σ(W[xf]x^((t))+W[hf]h^((t-1))+b[f])*'
- en: Sigmoid activation enables the network to selectively identify the content that
    needs to be forgotten.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活使网络能够选择性地识别需要遗忘的内容。
- en: 'The updated cell state after we determine the content that needs to be forgotten
    is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们确定需要遗忘的内容后，更新的单元状态如下：
- en: '*c[t]=(c[t-1] ![](img/598b937a-e234-4d91-a30e-ae8b81fbf55e.png) f)*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*c[t]=(c[t-1] ![](img/598b937a-e234-4d91-a30e-ae8b81fbf55e.png) f)*'
- en: Note that ![](img/dc04ccba-f496-4601-93a6-3f1f35655156.png) represents element-to-element
    multiplication.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，![](img/dc04ccba-f496-4601-93a6-3f1f35655156.png)表示逐元素相乘。
- en: For example, if the input input sequence of the sentence is *I live in India.
    I speak ___*, the blank space can be filled based on the input word *India*. After
    filling in the blank, we do not necessarily need the specific information of the
    name of the country.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果句子的输入序列是*I live in India. I speak ___*，则空白处可以根据输入词*India*来填充。在填充空白后，我们不一定需要该国家名称的具体信息。
- en: We update the cell state based on what needs to be forgotten at the current
    time step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据当前时间步需要遗忘的内容来更新单元状态。
- en: In the next step, we will be adding additional information to the cell state
    based on the input provided in the current time step. Additionally, the magnitude
    of the update (either positive or negative) is obtained through the tanh activation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将根据当前时间步提供的输入向单元状态添加额外的信息。此外，更新的幅度（正向或负向）通过tanh激活函数获得。
- en: 'The input can be specified as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以按如下方式指定：
- en: '*i[t]=σ(W[xi]x^((t))+W[hi]h^((t-1))+bi)*'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*i[t]=σ(W[xi]x^((t))+W[hi]h^((t-1))+bi)*'
- en: 'The modulation (magnitude of the input update) can be specified as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 调制（输入更新的幅度）可以按如下方式指定：
- en: '*g[t]=tanh(W[xg]x^((t))+W[hg]h^((t-1))+bg)*'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*g[t]=tanh(W[xg]x^((t))+W[hg]h^((t-1))+bg)*'
- en: 'The cell state—where we are forgetting certain things in a time step and also
    adding additional information in the same time step—gets updated as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 单元状态——在一个时间步中，我们忘记某些信息并在同一时间步中添加额外的信息——按如下方式更新：
- en: '![](img/b2688aa0-958e-4f11-9c52-f6661cacac20.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2688aa0-958e-4f11-9c52-f6661cacac20.png)'
- en: 'In the final gate, we need to specify what part of the combination of input
    (combination of current time step input and previous time step''s hidden layer
    value) and the cell state needs to be outputted to the next hidden layer:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个门中，我们需要指定输入的组合（当前时间步输入与前一时间步的隐藏层值的组合）和单元状态中需要输出到下一个隐藏层的部分：
- en: '![](img/9f52fca7-fe04-4a8f-a16a-ae6386c325f1.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f52fca7-fe04-4a8f-a16a-ae6386c325f1.png)'
- en: 'The final hidden layer is represented as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的隐藏层表示如下：
- en: '![](img/962e9401-a248-417f-8f8d-0ebdfff4aff3.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/962e9401-a248-417f-8f8d-0ebdfff4aff3.png)'
- en: This way, we are in a position to leverage the various gates in LSTM to selectively
    identify the information that needs to be stored in memory and thus overcome the
    limitation of RNN.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们能够利用LSTM中的各种门来选择性地识别需要存储在记忆中的信息，从而克服RNN的局限性。
- en: How to do it...
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: To gain a practical intuition of how this theory works, let's look at the same
    example we worked out in understanding RNN but this time using LSTM.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得该理论如何工作的实际直觉，让我们看看我们在理解RNN时做过的相同示例，但这次使用LSTM。
- en: 'Note that the data preprocessing steps are common between the two examples.
    Hence, we will reuse the preprocessing part (*step 1* to *step 4* in the *Building
    an RNN from scratch in Python* recipe) and directly head over to the model-building
    part (the code file is available as `LSTM_working_details.ipynb` in GitHub):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据预处理步骤在两个示例中是相同的。因此，我们将重用预处理部分（在*从零开始构建RNN的Python教程*中的*步骤1*到*步骤4*），并直接进入模型构建部分（代码文件在GitHub中的`LSTM_working_details.ipynb`可用）：
- en: 'Define the model:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型：
- en: '[PRE41]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that, in the preceding code, we initialized the recurrent initializer and
    recurrent activation to certain values only to make this example simpler; the
    purpose is only to help you understand what is happening in the backend.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码中，我们将递归初始化器和递归激活函数初始化为某些值，仅为了简化此示例；其目的是帮助您理解后端发生了什么。
- en: '[PRE42]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'A summary of the model is as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/04377b35-d2b0-4091-9dae-5d5558039585.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04377b35-d2b0-4091-9dae-5d5558039585.png)'
- en: The number of parameters is `12` in the LSTM layer as there are four gates (forget,
    input, cell, and output), which results in four weights and four biases connecting
    the input to the hidden layer. Additionally, the recurrent layer contains weight
    values that correspond to the four gates, which gives us a total of `12` parameters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM层中，参数的数量是`12`，因为有四个门（遗忘门、输入门、细胞门和输出门），这导致有四个权重和四个偏置将输入与隐藏层连接。此外，递归层包含对应四个门的权重值，因此我们总共有`12`个参数。
- en: The dense layer has a total of 10 parameters as there are five possible classes
    as output, and thus five weights and five biases that correspond to each connection
    from the hidden layer to the output layer.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Dense层总共有10个参数，因为输出有五个可能的类别，因此有五个权重和五个偏置，分别对应从隐藏层到输出层的每个连接。
- en: 'Let''s fit the model:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们拟合模型：
- en: '[PRE44]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The order of weights of this model are as follows:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型的权重顺序如下：
- en: '[PRE45]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The weights can be obtained as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按如下方式获取权重：
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'From the preceding code (`model.weights`), we can see that the order of weights
    in the LSTM layer is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码（`model.weights`）中，我们可以看到LSTM层中权重的顺序如下：
- en: Weights of the input (kernel)
  id: totrans-262
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入的权重（核）
- en: Weights corresponding to the hidden layer (`recurrent_kernel`)
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对应于隐藏层的权重（`recurrent_kernel`）
- en: Bias in the LSTM layer
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM层中的偏置
- en: 'Similarly, in the dense layer (the layer that connects the hidden layer to
    the output), the order of weights is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在密集层（连接隐藏层与输出的层）中，权重的顺序如下：
- en: Weight to be multiplied with the hidden layer
  id: totrans-266
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与隐藏层相乘的权重
- en: Bias
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置
- en: 'Here is the order in which the weights and biases appear (not provided in the
    preceding output, but available in the GitHub repository of Keras) in the LSTM
    layer:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 LSTM 层中权重和偏置的顺序（在前面的输出中没有提供，但可以在 Keras 的 GitHub 仓库中找到）：
- en: Input gate
  id: totrans-269
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门
- en: Forget gate
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遗忘门
- en: Modulation gate (cell gate)
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调制门（单元门）
- en: Output gate
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门
- en: Calculate the predictions for the input.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输入的预测。
- en: We are using raw-encoded input values (1,2,3) without converting them into embedding
    values—only to see how the calculation works. In practice, we would be converting
    the input into embedding values.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是未经编码的原始输入值（1、2、3），而没有将其转换为嵌入值——仅仅是为了看看计算是如何工作的。实际操作中，我们会将输入转换为嵌入值。
- en: 'Reshape the input for the predict method, so that it is as per the data format
    expected by LSTM (batch size, number of time steps, features per time step):'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为预测方法重塑输入，以使其符合 LSTM 所期望的数据格式（批量大小、时间步数、每个时间步的特征）：
- en: '[PRE47]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The output of predict method is provided in commented line in the code above.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码中的注释行提供了预测方法的输出。
- en: Validating the output
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证输出
- en: Now that we have a predicted probability from the model, let's run the input
    through the forward pass of weights using NumPy to obtain the same output as we
    just did.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从模型中获得了预测概率，让我们使用 NumPy 通过权重的前向传递来运行输入，以获得与刚才相同的输出。
- en: 'This is done so that we validate our understanding of how LSTM works under
    the hood. The steps that we take to validate the output of model we built are
    as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了验证我们对 LSTM 内部工作原理的理解。我们验证所构建模型输出的步骤如下：
- en: 'Update the forget gate in time step 1\. This step looks at the input and then
    provides an estimate of how much of the cell state (memory) known so far is to
    be forgotten (note the usage of the sigmoid function):'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新时间步 1 中的遗忘门。此步骤查看输入，并提供对到目前为止已知的单元状态（记忆）需要遗忘多少的估计（请注意在这方面使用了 sigmoid 函数）：
- en: '[PRE48]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Update the cell state based on the updated forget gate. The output of the previous
    step is being used here to direct the amount of values to be forgotten from the
    cell state (memory):'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于更新的遗忘门更新单元状态。前一步的输出将在此处用于指导从单元状态（记忆）中忘记多少值：
- en: '[PRE49]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Update the input gate value in time step 1\. This step gives an estimate of
    how much new information is to be injected into the cell state based on the current
    input:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新时间步 1 中的输入门值。此步骤给出了根据当前输入将有多少新信息注入到单元状态中的估计：
- en: '[PRE50]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Update the cell state based on the updated input value. This is the step where
    the output from the previous step is being used to dictate the amount of information
    update that is to happen to cell state (memory):'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于更新的输入值更新单元状态。这是一个步骤，其中前一步的输出被用来指示应对单元状态（记忆）进行多少信息更新：
- en: '[PRE51]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The preceding `tanh` activation helps to determine whether the update from
    the input will add to or subtract from the cell state (memory). This provides
    an additional lever, as, if certain information is already conveyed in the current
    time step and is not useful in future time steps, we are better off wiping it
    off from the cell state so that this extra information (which might not be helpful
    in the next step) is wiped from memory:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的`tanh`激活有助于确定输入的更新是否会对单元状态（记忆）进行加法或减法操作。这提供了一个额外的杠杆，因为如果某些信息已经在当前时间步传递，并且在未来的时间步不再有用，那么我们最好将其从单元状态中删除，这样这些额外的信息（在下一步可能没有帮助）就能从记忆中擦除：
- en: '[PRE52]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Update the output gate. This step provides an estimate of how much information
    will be conveyed in the current time step (note the usage of the sigmoid function
    in this regard):'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新输出门。此步骤提供对当前时间步将传递多少信息的估计（请注意在这方面使用了 sigmoid 函数）：
- en: '[PRE53]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Calculate the hidden layer value at time step 1\. Note that the final hidden-layer
    value at a time step is a combination of how much memory and output in the current
    time step is used to convey for a single time step:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算时间步 1 的隐藏层值。请注意，某一时间步的最终隐藏层值是当前时间步的记忆和输出用于传递单一时间步的结合：
- en: '[PRE54]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We are done calculating the hidden layer value output from the first time step.
    In the next steps, we will pass the updated cell state value from time step 1
    and the output of the hidden layer value from time step 1 as inputs to time step
    2.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了从第一时间步得到的隐藏层输出值的计算。在接下来的步骤中，我们将把时间步长 1 更新后的单元状态值和时间步长 1 的隐藏层输出作为输入传递给时间步长
    2。
- en: 'Pass the input value at time step 2 and the cell state value going into time
    step 2:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传递时间步长为 2 的输入值和进入时间步长 2 的单元状态值：
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Update the forget gate value:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新忘记门的值：
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Update the cell state value in time step 2:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新时间步长 2 中的单元状态值：
- en: '[PRE57]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Update the hidden layer output value based on the combination of updated cell
    state and the magnitude of output that is to be made:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据更新后的单元状态与需要输出的量的结合更新隐藏层输出值：
- en: '[PRE58]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Pass the hidden layer output through the dense layer:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将隐藏层输出通过全连接层传递：
- en: '[PRE59]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Run softmax on top of the output we just obtained:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们刚刚得到的输出上运行 softmax：
- en: '[PRE60]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You should notice that the output obtained here is exactly the same as what
    we obtained from the `model.predict` method.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，这里得到的输出与我们从 `model.predict` 方法中获得的输出完全相同。
- en: With this exercise, we are in a better position to appreciate the working details
    of LSTM.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个练习，我们更有能力理解 LSTM 的工作细节。
- en: Implementing LSTM for sentiment classification
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 LSTM 用于情感分类
- en: In *Implementing RNN for sentiment classification* recipe, we implemented sentiment
    classification using RNN. In this recipe, we will look at implementing it using
    LSTM.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *实现 RNN 用于情感分类* 配方中，我们使用 RNN 实现了情感分类。在本配方中，我们将探讨如何使用 LSTM 实现它。
- en: How to do it...
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The steps we''ll adopt are as follows (the code file is available as `RNN_and_LSTM_sentiment_classification.ipynb`
    in GitHub):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的步骤如下（代码文件在 GitHub 上作为 `RNN_and_LSTM_sentiment_classification.ipynb` 提供）：
- en: 'Define the model. The only change from the code we saw in *Implementing RNN
    for sentiment classification* recipe will be the change from `simpleRNN` to LSTM
    in the model architecture part (we will be reusing the code from *step 1* to *step
    6* in the *Implementing RNN for sentiment classification* recipe):'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型。与我们在 *实现 RNN 用于情感分类* 配方中看到的代码的唯一变化是将模型架构部分的 `simpleRNN` 改为 LSTM（我们将重用 *实现
    RNN 用于情感分类* 配方中的 *第 1 步* 到 *第 6 步* 的代码）：
- en: '[PRE62]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The input for the embedding layer is the total number of unique IDs present
    in the dataset and the expected dimension to which each word needs to be converted
    (`output_dim`).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输入是数据集中出现的唯一 ID 的总数，以及每个词需要转换的期望维度（`output_dim`）。
- en: 'Additionally, we''ll also specify the maximum length of input, so that the
    LSTM layer in the next step has the required information—batch size, number of
    time steps (`input_length`), and the number of features per time (`step(output_dim)`):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将指定输入的最大长度，以便下一个步骤中的 LSTM 层能获得所需的信息——批量大小、时间步长数（`input_length`）和每个时间步的特征数（`step(output_dim)`）：
- en: '[PRE63]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'A summary of the model is as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/b4e00271-4c1b-464a-bdc1-6dbc45594704.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4e00271-4c1b-464a-bdc1-6dbc45594704.png)'
- en: While the parameters in the first and last layer are the same as what saw in
    the *Implementing RNN for sentiment classification* recipe, the LSTM layer has
    a different number of parameters.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第一层和最后一层的参数与我们在 *实现 RNN 用于情感分类* 配方中看到的一样，LSTM 层的参数数量不同。
- en: 'Let''s understand how 11,680 parameters are obtained in the LSTM layer:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解 LSTM 层中如何获得 11,680 个参数：
- en: '[PRE64]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will look like the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE65]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Note that the total of the preceding weights has *(32*160) + (40*160) + 160
    = 11,680* parameters.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前述权重的总和是 *(32*160) + (40*160) + 160 = 11,680* 个参数。
- en: '`W` represents the weights that connect the input to each of the four cells
    (`i`, `f`, `c`, `o`), `U` represents the hidden-layer-to-hidden-layer connection,
    and `b` represents the bias in each gate.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`W` 表示将输入连接到四个单元（`i`、`f`、`c`、`o`）的权重，`U` 表示隐藏层到隐藏层的连接，`b` 表示每个门中的偏置。'
- en: 'Individual weights of the input, forget, cell state, and output gates can be
    obtained as follows:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门、忘记门、单元状态门和输出门的各个权重可以按如下方式获得：
- en: '[PRE66]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Fit the model as follows:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式拟合模型：
- en: '[PRE69]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The variation of loss and accuracy over increasing epochs in training and test
    datasets are as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据集上随着训练轮数增加，损失和准确率的变化如下：
- en: '![](img/ffc8b5d6-d220-487e-b635-67ce7423dddf.jpg)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc8b5d6-d220-487e-b635-67ce7423dddf.jpg)'
- en: The prediction accuracy is 91% when using the LSTM layer, which is slightly
    better than the prediction accuracy when we are using the simpleRNN layer. Potentially,
    we can further improve upon the result by fine-tuning the number of LSTM units.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LSTM层时，预测准确率为91%，略优于使用simpleRNN层时的预测准确率。通过微调LSTM单元的数量，我们可能能够进一步改善结果。
- en: Implementing stacked LSTM for sentiment classification
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现堆叠LSTM进行情感分类
- en: In the previous recipe, we implemented sentiment classification using LSTM in
    Keras. In this recipe, we will look at implementing the same thing but stack multiple
    LSTMs. Stacking multiple LSTMs is likely to capture more variation in the data
    and thus potentially a better accuracy.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个食谱中，我们使用Keras实现了基于LSTM的情感分类。在本食谱中，我们将探讨如何实现相同的功能，但堆叠多个LSTM。堆叠多个LSTM可能会捕捉到更多数据的变化，因此有可能得到更好的准确率。
- en: How to do it...
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Stacked LSTM is implemented as follows (the code file is available as `RNN_and_LSTM_sentiment_classification.ipynb`
    in GitHub):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠LSTM的实现方式如下（代码文件可在GitHub的`RNN_and_LSTM_sentiment_classification.ipynb`中找到）：
- en: 'The only change in the code we saw earlier will be to change the `return_sequences`
    parameter to true. This ensures that the first LSTM returns a sequence of output
    (as many output as the number of LSTM units), which can then be passed to another
    LSTM as an input on top of the original LSTM in the model architecture part (more
    details on the `return_sequences` parameter can be found in the *Sequence to Sequence
    learning* chapter):'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们之前看到的代码唯一的变化是将`return_sequences`参数设置为true。这确保了第一个LSTM返回一个输出序列（与LSTM单元的数量相同），然后可以将该输出作为输入传递给模型架构部分中的另一个LSTM（有关`return_sequences`参数的更多详细信息，请参见*Sequence
    to Sequence学习*章节）：
- en: '[PRE70]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'A summary of model architecture is as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构的总结如下：
- en: '![](img/5e6ca342-8153-47a5-92ed-a04e9dcd64d8.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e6ca342-8153-47a5-92ed-a04e9dcd64d8.png)'
- en: Note that, in the preceding architecture, there is an additional LSTM that is
    stacked on top of another LSTM. The output at each time step of the first LSTM
    is `40` values and thus an output shape of (`None`, `26`, `40`), where `None`
    represents the `batch_size`, `26` represents the number of time steps, and `40`
    represents the number of LSTM units considered.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的架构中，有一个额外的LSTM堆叠在另一个LSTM之上。第一个LSTM在每个时间步的输出为`40`个值，因此输出形状为（`None`，`26`，`40`），其中`None`代表`batch_size`，`26`代表时间步数，`40`代表考虑的LSTM单元数。
- en: 'Now that there are `40` input values, the number of parameters in the second
    LSTM is considered in the same fashion as in the previous recipe, as follows:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有`40`个输入值，第二个LSTM中的参数数量与之前的做法相同，如下所示：
- en: '[PRE71]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following are the values we get by executing preceding code:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码后，我们得到以下值：
- en: '[PRE72]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: This results in a total of 12,960 parameters, as seen in the output.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致总共有12,960个参数，如输出所示。
- en: W is 40 x 160 in shape, as it has 40 inputs that are mapped to 40 output and
    also 4 different gates to be controlled, and hence a total of 40 x 40 x 4 weights.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: W的形状为40 x 160，因为它有40个输入映射到40个输出，并且有4个不同的门需要控制，因此总共有40 x 40 x 4个权重。
- en: 'Implement the model, as follows:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式实现模型：
- en: '[PRE73]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The variation of loss and accuracy over increasing epochs in training and test
    datasets are as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集和测试集上，随着epoch数的增加，损失和准确率的变化如下所示：
- en: '![](img/d2377a1a-3446-4819-bc41-7a6583e1c92e.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2377a1a-3446-4819-bc41-7a6583e1c92e.jpg)'
- en: This results in an accuracy of 91%, as we saw with a single LSTM layer; however,
    with more data, stacked LSTM is likely to capture more variation in the data than
    the vanilla LSTM.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了91%的准确率，就像我们在使用单个LSTM层时看到的那样；然而，随着数据量的增加，堆叠的LSTM可能比普通的LSTM捕捉到更多数据的变化。
- en: There's more...
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '**Gated Recurrent Unit** (**GRU**) is another architecture we can use and has
    accuracy that is similar to that of LSTM. For more information about GRU, visit [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555).'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控循环单元**（**GRU**）是我们可以使用的另一种架构，其准确率与LSTM相似。有关GRU的更多信息，请访问[https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)。'
