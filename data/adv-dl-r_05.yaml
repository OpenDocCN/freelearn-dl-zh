- en: Deep Neural Networks for Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于回归的深度神经网络
- en: In the previous chapter, we worked with a dataset that had a categorical target
    variable, and we went over the steps for developing a classification model using
    Keras. In situations where the response variable is numeric, supervised learning
    problems are categorized as regression problems. In this chapter, we will develop
    a prediction model for numeric response variables. To illustrate the process of
    developing the prediction model, we will make use of the Boston Housing dataset,
    which is available within the `mlbench` package.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们处理了一个具有分类目标变量的数据集，并讲解了如何使用Keras开发分类模型。在响应变量是数值型的情况下，监督学习问题被归类为回归问题。本章将开发一个用于数值响应变量的预测模型。为了说明开发预测模型的过程，我们将使用波士顿住房数据集，该数据集可以在`mlbench`包中找到。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容将涵盖以下主题：
- en: Understanding the Boston Housing dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解波士顿住房数据集
- en: Preparing the data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Creating and fitting a deep neural network model for regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并拟合一个深度神经网络回归模型
- en: Model evaluation and prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Performance optimization tips and best practices
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: Understanding the Boston Housing dataset
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解波士顿住房数据集
- en: 'In this chapter, we will use six libraries. These libraries are as listed in
    the following code:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将使用六个库，具体库列表见以下代码：
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The structure of the `BostonHousing` data is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`BostonHousing`数据集的结构如下：'
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As you can see from the preceding output, this dataset has `506` observations
    and `14` variables. Out of the 14 variables, 13 are numeric and 1 variable (`chas`)
    is of the factor type. The last variable, `medv` (the median value of owner-occupied
    homes in thousand-USD units), is the dependent, or target, variable. The remaining
    13 variables are independent. The following is a brief description of all the
    variables, drawn up in a table for easy reference:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的输出可以看到，该数据集共有`506`个观测值和`14`个变量。在这14个变量中，13个是数值型变量，1个变量（`chas`）是因子类型。最后一个变量，`medv`（以千美元为单位的业主自住住房中位数价值），是因变量或目标变量，其余13个变量是自变量。以下是所有变量的简要描述，以便参考：
- en: '| **Variables** | **Description** |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **变量** | **描述** |'
- en: '| `crim` | Per-capita crime rate by town |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `crim` | 按城镇计算的人均犯罪率 |'
- en: '| `zn` | Proportion of residential land zoned for lots over 25,000 sq ft |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `zn` | 25,000平方英尺以上的住宅用地比例 |'
- en: '| `indus` | Proportion of nonretail business acres per town |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `indus` | 每个城镇的非零售商业用地比例 |'
- en: '| `chas` | Charles River dummy variable (1 if the tract bounds a river; 0 otherwise)
    |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `chas` | 查尔斯河虚拟变量（若地块与河流相邻则为1，否则为0） |'
- en: '| `nox` | Nitric-oxides concentration (parts per 10 million) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `nox` | 氮氧化物浓度（每10百万分之一） |'
- en: '| `rm` | Average number of rooms per dwelling |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `rm` | 每个住宅的平均房间数 |'
- en: '| `age` | Proportion of owner-occupied units built prior to 1940 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| `age` | 1940年之前建成的自有住宅比例 |'
- en: '| `dis` | Weighted distances to five Boston employment centers |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| `dis` | 到波士顿五个就业中心的加权距离 |'
- en: '| `rad` | Index of accessibility to radial highways |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| `rad` | 进入放射性高速公路的可达性指数 |'
- en: '| `tax` | Full-value property-tax rate per 10,000 USD |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `tax` | 每10,000美元的全额财产税税率 |'
- en: '| `ptratio` | Pupil–teacher ratio by town |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `ptratio` | 每个城镇的师生比 |'
- en: '| `lstat` | Percentage of lower-income status members of the population |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `lstat` | 低收入群体在总人口中的百分比 |'
- en: '| `medv` | Median value of owner-occupied homes in thousand-USD units |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| `medv` | 业主自住住房的中位数价值（千美元单位） |'
- en: 'This data is based on the 1970 census. A detailed statistical study using this
    data was published by Harrison and Rubinfeld in 1978 (reference: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf)).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据基于1970年的人口普查。使用这些数据的详细统计研究由Harrison和Rubinfeld于1978年发布（参考文献：[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.926.5532&rep=rep1&type=pdf)）。
- en: Preparing the data
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: We start by changing the name of the `BostonHousing` data to simply `data` for
    ease of use. Independent variables that are of the factor type are then converted
    to the numeric type using the `lapply` function.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将`BostonHousing`数据集的名称更改为简化的`data`，以便使用。然后，通过`lapply`函数将因子类型的自变量转换为数值类型。
- en: Note that for this data, the only factor variable is `chas`; however, for any
    other dataset with more factor variables, this code will work fine.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于该数据集，唯一的因子变量是`chas`；然而，对于任何其他包含更多因子变量的数据集，该代码也能正常工作。
- en: 'Take a look at the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下代码：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, after converting factor variables to the `numeric` type,
    we also change the format of `data` to `data.frame`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，将因子变量转换为`numeric`类型后，我们还将`data`的格式更改为`data.frame`。
- en: Visualizing the neural network
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化神经网络
- en: 'To visualize a neural network with hidden layers, we will use the `neuralnet`
    function. For illustration, two hidden layers with 10 and 5 units will be used
    in this example. The input layer has 13 nodes based on 13 independent variables.
    The output layer has only one node for the target variable, `medv`. The code used
    is as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化一个具有隐藏层的神经网络，我们将使用`neuralnet`函数。为了说明，示例中将使用两个隐藏层，分别有10个和5个节点。输入层有13个节点，基于13个自变量。输出层只有一个节点，用于目标变量`medv`。使用的代码如下：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As shown in the preceding code, the result is saved in `n`, and it is then
    used for plotting the architecture of the neural network, as shown in the following
    diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，结果被保存在`n`中，然后它将用于绘制神经网络的架构，如下图所示：
- en: '![](img/411058c3-98fe-4dd2-b54c-865c61368359.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/411058c3-98fe-4dd2-b54c-865c61368359.png)'
- en: 'As you can see from the preceding diagram, the input layer has 13 nodes for
    13 independent variables. There are two hidden layers: the first hidden layer
    has 10 nodes and the second hidden layer has 5 nodes. Each node in the hidden
    layer is connected to all the nodes in the previous and the following layer. The
    output layer has one node for the response variable, `medv`.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，输入层有13个节点，表示13个自变量。共有两个隐藏层：第一个隐藏层有10个节点，第二个隐藏层有5个节点。每个隐藏层的节点都与前一层和后一层的所有节点相连接。输出层有一个节点，用于响应变量`medv`。
- en: Data partitioning
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分割
- en: 'Next, we change the data into a matrix format. We also set dimension names
    to `NULL`, which changes the names of the variables to the default names, `V1`,
    `V2`, `V3`, ..., `V14`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据转换为矩阵格式。我们还将维度名称设置为`NULL`，这会将变量的名称更改为默认名称，`V1`、`V2`、`V3`、...、`V14`：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then the partition data into training and test datasets using the following
    code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码将数据分割为训练集和测试集：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: A data split of 70:30 is used in this example. To maintain the repeatability
    of the data split, we use a random seed of `1234`. This will allow the same samples
    to be included in the training and test data each time data partitioning is carried
    out on any computer. The data for the independent variables are stored in `training`
    for the training data and in `test` for the test data. Similarly, the data for
    the dependent variable, `medv`, based on the corresponding split data, are stored
    in `trainingtarget` and `testtarget`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，数据分割的比例为70:30。为了保持数据分割的可重复性，我们使用了`1234`的随机种子。这样，每次在任何计算机上执行数据分割时，训练数据和测试数据中都会包含相同的样本。自变量的数据存储在`training`中用于训练数据，在`test`中用于测试数据。同样，响应变量`medv`的数据，基于相应的分割数据，存储在`trainingtarget`和`testtarget`中。
- en: Normalization
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化
- en: 'To normalize the data, the mean and standard deviations are obtained for all
    independent variables in the training data. Normalization is then carried out
    using the `scale` function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行归一化，我们首先计算训练数据中所有自变量的均值和标准差。然后，使用`scale`函数进行归一化处理：
- en: For both the train and test data, the mean and standard deviations are based
    on the training data used.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据和测试数据，均值和标准差是基于所使用的训练数据计算的。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This concludes the data preparation step for this data. It should be noted that
    different datasets may need extra steps that are unique to that dataset—for example,
    many large datasets may have very high amounts of missing data values, and they
    may require additional data preparation steps in the form of arriving at a strategy
    for handling missing values and inputting missing values wherever necessary.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着数据准备步骤的完成。需要注意的是，不同的数据集可能需要额外的步骤，这些步骤对于每个数据集是独特的——例如，许多大型数据集可能存在大量缺失数据值，这可能需要额外的数据准备步骤，如制定处理缺失值的策略，并在必要时输入缺失值。
- en: In the next section, we will create a deep neural network architecture and then
    fit a model for the accurate prediction of the numeric target variable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将创建一个深度神经网络架构，然后拟合一个模型，用于准确预测数值型目标变量。
- en: Creating and fitting a deep neural network model for regression
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并拟合一个用于回归的深度神经网络模型
- en: 'To create and fit a deep neural network model for a regression problem, we
    will make use of Keras. The code used for the model architecture is as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建并拟合一个用于回归问题的深度神经网络模型，我们将使用Keras。模型架构使用的代码如下：
- en: Note that the input layer having 13 units and the output layer having 1 unit
    is fixed based on the data; however, to arrive at a suitable number of hidden
    layers and the number of units in each layer, you need to experiment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输入层有13个单元，输出层有1个单元，这是根据数据固定的；然而，要确定适合的隐藏层数量和每个层的单元数量，您需要进行实验。
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As you can see from the preceding code, we use the `keras_model_sequential`
    function to create a sequential model. The structure of the neural network is
    defined using the `layer_dense` function. Since there are 13 independent variables,
    `input_shape` is used to specify 13 units. The first hidden layer has `10` units
    and the rectified linear unit, or `relu`, is used as the activation function in
    this first hidden layer. The second hidden layer has `5` units, with `relu` as
    the activation function. The last, `layer_dense`, has `1` unit, which represents
    one dependent variable, `medv`. Using the `summary` function, you can print a
    model summary, which shows 201 total parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们使用`keras_model_sequential`函数来创建一个顺序模型。神经网络的结构通过`layer_dense`函数定义。由于有13个自变量，`input_shape`用于指定13个单元。第一个隐藏层有`10`个单元，激活函数使用的是修正线性单元（`relu`）。第二个隐藏层有`5`个单元，激活函数同样使用`relu`。最后，`layer_dense`有`1`个单元，表示一个因变量`medv`。通过使用`summary`函数，可以打印出模型总结，显示总共201个参数。
- en: Calculating the total number of parameters
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算总参数数量
- en: Let's now see how a total of 201 parameters are obtained for this model. The
    `dense_1` layer shows `140` parameters. These parameters are based on there being
    13 units in the input layer that connect with each of the 10 units in the first
    hidden layer, meaning that there are 130 parameters (13 x 10). The remaining 10
    parameters come from the bias term for each of the 10 units in the first hidden
    layer. Similarly, 50 parameters (10 x 5) are from the connections between two
    hidden layers and the remaining 5 parameters come from the bias term from each
    of the 5 units in the second hidden layer. Finally, `dense_3` has `6` parameters
    ((5 x 1) + 1). Thus, in all, there are 201 parameters based on the architecture
    of the neural network model that was chosen in this example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何得到模型的总计201个参数。`dense_1`层显示有`140`个参数。这些参数是基于输入层有13个单元，每个单元与第一隐藏层中的10个单元连接，因此共有130个参数（13
    x 10）。其余10个参数来自于第一隐藏层中每个10个单元的偏置项。同样，50个参数（10 x 5）来自于两个隐藏层之间的连接，剩下的5个参数来自于第二隐藏层中每个5个单元的偏置项。最后，`dense_3`有`6`个参数（（5
    x 1）+ 1）。因此，总共有201个参数，基于此示例中选择的神经网络架构。
- en: Compiling the model
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'After the model architecture is defined, the model is compiled to configure
    the learning process using the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型架构之后，可以使用以下代码编译模型并配置学习过程：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As shown in the preceding code, we define the loss function as the mean square
    error, or `mse`. At this step, the `rmsprop` optimizer and mean absolute error,
    or `mae`, metric is also defined. We choose these because our response variable
    is numeric.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，我们将损失函数定义为均方误差（`mse`）。在此步骤中，`rmsprop`优化器和平均绝对误差（`mae`）度量也被定义。我们选择这些是因为我们的响应变量是数值型的。
- en: Fitting the model
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Next, the model is trained using the `fit` function. Note that, as the training
    of the model proceeds, we get a visual as well as a numerical summary after each
    epoch. The output from the last three epochs is shown in the following code. We
    get the mean absolute error and loss values for both the training and the validation
    data. Note that, as pointed out in [Chapter 1](db6a812d-2bad-4f40-9e99-0e20abbe665c.xhtml),
    *Revisiting Deep Learning Architecture and Techniques*, each time we train a network,
    the training and validation errors can vary because of the random initialization
    of network weights. Such an outcome is expected even when the data is partitioned
    using the same random seed. To obtain repeatable results, it is always a good
    idea to save the model using the `save_model_hdf5` function and then reload it
    when needed.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，模型使用`fit`函数进行训练。请注意，在训练过程中，我们会在每个epoch后得到可视化图像和数值摘要。以下代码展示了最后三个epoch的输出。我们可以获得训练和验证数据的平均绝对误差和损失值。请注意，正如[第1章](db6a812d-2bad-4f40-9e99-0e20abbe665c.xhtml)《重访深度学习架构与技术》中所指出的，*每次训练网络时，由于网络权重的随机初始化，训练和验证误差可能会有所不同*。即使数据使用相同的随机种子进行划分，这种结果也是预期中的。为了获得可重复的结果，最好使用`save_model_hdf5`函数保存模型，并在需要时重新加载它。
- en: 'The code used for training the network is as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练网络的代码如下：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As you can see from the preceding code, the model is trained in small batches
    of size `32`, and 20% of the data is reserved for validation to avoid overfitting.
    Here, `100` epochs or iterations are run to train the network. Once the training
    process is completed, information related to the training process is saved in
    `model_one`, which can then be used to plot the loss and mean absolute error based
    on the training and validation data for all epochs:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从上面的代码中所见，模型在小批量大小为`32`的情况下进行训练，20%的数据用于验证，以避免过拟合。这里，运行了`100`个epoch或迭代来训练网络。训练过程完成后，相关信息将保存在`model_one`中，随后可以用来根据所有epoch的训练和验证数据绘制损失和平均绝对误差图：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding line of code will return the following output. Let''s have a
    look at the loss and mean absolute error for training and validation data (`model_one`) plot:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将返回以下输出。让我们来看看训练和验证数据（`model_one`）的损失和平均绝对误差图：
- en: '![](img/37680ec9-e55b-4c9a-9f21-607ae941db74.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37680ec9-e55b-4c9a-9f21-607ae941db74.png)'
- en: 'From the preceding plot, we can make the following observations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述图中，我们可以做出以下观察：
- en: The `mae` and `loss` values decrease for both the training and validation data
    as the training proceeds.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着训练的进行，`mae`和`loss`值在训练数据和验证数据中都逐渐降低。
- en: The rate of decrease in errors for the training data reduces after about 60
    epochs.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的错误下降速度在大约60个epoch后减缓。
- en: After developing the prediction model, we can assess its performance by evaluating
    the prediction quality of the model, which we will look at in the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发预测模型后，我们可以通过评估模型的预测质量来评估其性能，我们将在下一节中讨论这一点。
- en: Model evaluation and prediction
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估和预测
- en: Model evaluation is an important step in the process of arriving at a suitable
    prediction model. A model may show good performance with training data that was
    used for developing the model; however, the real test of a model is with data
    that the model has not yet seen. Let's look at the model performance based on
    the test data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型评估是获得合适预测模型过程中的一个重要步骤。一个模型可能在用于开发模型的训练数据上表现良好；然而，模型的真正考验是它在尚未见过的数据上的表现。让我们来看看基于测试数据的模型性能。
- en: Evaluation
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'The performance of the model is evaluated using the `evaluate` function with
    the help of the test data shown in the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能通过`evaluate`函数进行评估，使用下面代码所示的测试数据：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From the preceding output, we can see that the loss and mean absolute error
    for the test data are `31.15` and `3.61` respectively. We will use these numbers
    later to compare and assess whether or not the changes that we will make to the
    current model help to improve the prediction performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出可以看到，测试数据的损失和平均绝对误差分别为`31.15`和`3.61`。稍后我们将使用这些数字来比较和评估我们对当前模型所做的改进是否有助于提高预测性能。
- en: Prediction
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: 'Let''s predict the `medv` values for the `test` data and store the results
    in `pred` using the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码预测`test`数据的`medv`值，并将结果存储在`pred`中：
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can take a look at the first 10 predicted and actual values using the `cbind`
    function. The first column in the output shows the predicted values based on the
    model and the second column shows the actual values. We can make the following
    observations from the output:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cbind`函数查看前10个预测值与实际值。输出的第一列显示基于模型的预测值，第二列显示实际值。我们可以从输出中做出以下观察：
- en: The prediction for the first sample in the test data is about `33.19` and the
    actual value is `36.2`. The model underestimates the response by about `3` points.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据中第一个样本的预测值约为`33.19`，实际值为`36.2`。模型低估了响应值，约偏差`3`个点。
- en: For the second sample, the model underestimates the response by over `2` points.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第二个样本，模型的预测值低估了响应值，偏差超过了`2`个点。
- en: For the tenth sample, the predicted and actual values are very close.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第十个样本，预测值与实际值非常接近。
- en: For the sixth sample, the model overestimates the response.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于第六个样本，模型高估了响应值。
- en: 'To get an overall picture of the prediction performance, we can develop a scatter
    plot of the predicted versus the actual values. We will use the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面了解预测性能，我们可以绘制预测值与实际值的散点图。我们将使用以下代码：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The scatter plot shows the predicted versus the actual response values based
    on the test data:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图显示了基于测试数据的预测值与实际响应值：
- en: '![](img/16a25a48-a15a-4d4f-9ac8-ce7dd907adc3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16a25a48-a15a-4d4f-9ac8-ce7dd907adc3.png)'
- en: From the preceding graph, we can see the overall performance of the prediction
    model. The relationship between the actual and predicted values is positive and
    approximately linear. Although we can see that the model has decent performance,
    clearly there is scope for further improvement that makes data points closer to
    the ideal line that has zero intercepts and a slope of 1\. We will further explore
    making improvements to the model by developing a deeper neural network model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到预测模型的整体性能。实际值与预测值之间呈正相关，且大致线性。虽然我们可以看到模型表现良好，但显然还有进一步改进的空间，使得数据点更接近理想线，该理想线的截距为零，斜率为1。接下来，我们将通过开发一个更深的神经网络模型来进一步探索模型改进。
- en: Improvements
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进
- en: In the modified new model, we will build a deeper network by adding more layers.
    The additional layers are expected to show patterns in the data that the smaller
    network we used earlier was not able to show.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在修改后的新模型中，我们将通过添加更多层来构建一个更深的网络。新增的层预计能够展示出之前较小网络无法捕捉到的数据模式。
- en: Deeper network architecture
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更深的网络架构
- en: 'The code used for this experiment is as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 进行此实验所使用的代码如下：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the preceding code, we can observe that we now have three hidden layers
    with `100`, `50`, and `20` units respectively. We have also added a dropout layer
    after each hidden layer with rates of `0.4`, `0.3`, and `0.2` respectively. As
    an example of what a dropout layer's rate means, a rate of 0.4 means that 40%
    of the units in the first hidden layer are dropped to zero at the time of training,
    which helps to avoid overfitting. The total number of parameters in this model
    has now increased to `7,491`. Note that, in the previous model, the total number
    of parameters was `201`, and clearly we are going for a significantly bigger neural
    network. Next, we compile the model with the same settings that we used earlier,
    and subsequently, we will fit the model and store the results in `model_two`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到，现在我们有三个隐藏层，分别包含`100`、`50`和`20`个单元。我们还在每个隐藏层后添加了一个丢弃层，丢弃率分别为`0.4`、`0.3`和`0.2`。例如，丢弃层的丢弃率意味着，在训练时，丢弃率为0.4表示第一隐藏层中40%的单元被丢弃为零，这有助于避免过拟合。此模型的总参数数目已增加至`7,491`。注意，在之前的模型中，参数总数为`201`，显然我们正在构建一个更大的神经网络。接下来，我们使用之前相同的设置编译模型，随后将拟合模型并将结果存储在`model_two`中。
- en: Results
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The following figure provides the loss and mean absolute error for `model_two`
    over 100 epochs:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了`model_two`在100个周期中的损失和平均绝对误差：
- en: '![](img/b48f8055-750e-4560-9978-92c2fe0abfe3.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b48f8055-750e-4560-9978-92c2fe0abfe3.png)'
- en: 'From the preceding figure, we can make the following observations:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以做出以下观察：
- en: The mean absolute error and loss values for the training and validation data
    drop very quickly to low values, and after about 30 epochs, we do not see any
    major improvement.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的平均绝对误差和损失值迅速下降到较低值，在约30个周期后，我们未见到任何显著改进。
- en: There is no evidence of overfitting as the training and validation errors seem
    closer to each other.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练误差和验证误差似乎相互接近，因此没有过拟合的证据。
- en: 'We can obtain the loss and mean absolute error values for the test data using
    the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码获得测试数据的损失值和平均绝对误差值：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The loss and mean absolute error values using the `test` data and `model_two`
    are obtained as `24.70` and `3.02` respectively. This is a significant improvement
    compared to the results that we obtained from `model_one`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`test`数据和`model_two`得到的损失值和平均绝对误差值分别为`24.70`和`3.02`。与我们从`model_one`获得的结果相比，这是一个显著的改进。
- en: 'We can visually see this improvement using the scatter plot for the predicted
    values versus the actual response values in the following graph:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下图中的散点图直观地看到这一改进，图中展示了预测值与实际响应值的关系：
- en: '![](img/6cd957b8-3c86-4bc4-bdc7-ab0ae18ccb17.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cd957b8-3c86-4bc4-bdc7-ab0ae18ccb17.png)'
- en: From the preceding graph, we can see that the spread in the scatter plot of
    actual versus predicted values is visibly less than that of the earlier scatter
    plot. This indicates better prediction performance compared to the previous model.
    Although `model_two` performs better than the previous model, at higher values,
    we can see the occurrence of significant underestimation of the target values.
    So, although we have developed a better model, we can also further explore the
    potential for the further improvement of this prediction model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中，我们可以看到，实际值与预测值的散点图分布明显比之前的散点图更集中。这表明与之前的模型相比，预测性能有所提高。尽管`model_two`比之前的模型表现更好，但在较高值处，我们仍能看到目标值的显著低估。因此，尽管我们已经开发了一个更好的模型，但我们仍可以进一步探索该预测模型进一步改进的潜力。
- en: Performance optimization tips and best practices
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: Improving model performance can involve different strategies. Here, we will
    discuss two main strategies. One strategy is to make changes to the model architecture
    and observe the results to get any useful insights or indications of improvement.
    Another strategy could involve exploring the transformation of the target variable.
    In this section, we will try a combination of both of these strategies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 改进模型性能可能涉及不同的策略。在这里，我们将讨论两种主要策略。一种策略是对模型架构进行修改，并观察结果，以获取任何有用的见解或改进的指示。另一种策略可能涉及探索目标变量的转换。在本节中，我们将尝试这两种策略的结合。
- en: Log transformation on the output variable
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对输出变量进行对数转换
- en: 'To overcome the issue of significant underestimation of the target variable at
    higher values, let''s try log transformation on the target variable and see whether
    or not this helps us to further improve the model. Our next model has some minor
    changes to the architecture as well. In `model_two`, we did not notice any major
    issue or evidence related to overfitting, and as a result, we can increase the
    number of units a little and also slightly reduce the percentages for dropout.
    The following is the code for this experiment:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服在较高值处显著低估目标变量的问题，我们尝试对目标变量进行对数转换，看看是否能进一步改进模型。我们的下一个模型在架构上也做了一些小的调整。在`model_two`中，我们没有发现任何主要问题或与过拟合相关的证据，因此我们可以稍微增加单元的数量，并且稍微减少dropout的百分比。以下是这个实验的代码：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will increase the number of units in the third hidden layer from `20` to
    `25`. Dropout rates for the second and third hidden layers are also reduced to
    `0.2` and `0.1` respectively. Note that the overall number of parameters has now
    increased to `7751`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将第三个隐藏层的单元数从`20`增加到`25`。第二个和第三个隐藏层的dropout率分别减少到`0.2`和`0.1`。请注意，整体参数的数量现在已增加到`7751`。
- en: 'We next compile the model and then fit the model. The model results are stored
    in `model_three`, which we use for plotting the graph, as shown in the following
    code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编译模型并拟合模型。模型结果存储在`model_three`中，我们使用它来绘制图表，如下所示的代码所示：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following shows the output of the loss and mean absolute error for training
    and validation data (`model_three`):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了训练和验证数据的损失值和平均绝对误差的输出（`model_three`）：
- en: '![](img/840b6abb-532f-41be-9f1d-880f25acd57d.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/840b6abb-532f-41be-9f1d-880f25acd57d.png)'
- en: We can see from the preceding plot that although the values in the plot are
    not directly comparable to earlier figures because of the log transformation,
    we can see that the overall errors decrease and become stable after about 50 epochs
    for both the mean absolute error and the loss.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以看到，尽管图中的数值由于对数转换与早期数据不完全可比，但我们可以看到，无论是平均绝对误差还是损失值，整体误差在大约50个epoch后都减少并趋于稳定。
- en: Model performance
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: 'We also obtain `loss` and `mae` values for this new model, but again, the numbers
    obtained are not directly comparable to the earlier two models for the log scale:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还获得了这个新模型的`loss`和`mae`值，但同样，由于对数尺度的关系，得到的数值与之前两个模型不可直接比较：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We obtain a scatter plot of the actual values (log transformed) versus the
    predicted values based on the test data. We also get a scatter plot of the actual
    versus predicted values in the original scale for comparison with earlier plots.
    The scatter plots for predicted versus actual response values (`model_three`) are
    as shown in the following graph:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得了基于测试数据的实际值（对数转换后）与预测值的散点图。我们还获得了实际值与预测值在原始尺度上的散点图，并与早期的图进行比较。预测值与实际响应值的散点图（`model_three`）如下所示：
- en: '![](img/70341185-4756-4a9d-a62c-46d2eaf4dafb.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70341185-4756-4a9d-a62c-46d2eaf4dafb.png)'
- en: From the preceding graph, we can see that the significant underestimation pattern
    observed in earlier models shows improvement, both in the log scale and in the
    original scale. In the original scale, the data points at higher values are relatively
    closer to the diagonal line, indicating improved prediction performance by the
    model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以看到，早期模型中观察到的显著低估模式在对数尺度和原始尺度上都有所改善。在原始尺度中，较高数值的数据点相对更接近对角线，这表明模型的预测性能有所提高。
- en: Summary
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through the steps for developing a prediction model
    when the response variable is of a numeric type. We started with a neural network
    model that had 201 parameters and then developed deep neural network models with
    over 7,000 parameters. You may have noticed that, in this chapter, we made use
    of comparatively deeper and more complex neural network models compared to the
    previous chapter, where we developed a classification model for the target variable
    that was of a categorical nature. In both [Chapter 2](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml), *Deep
    Neural Networks for Multiclass Classification*, and [Chapter 3](07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml),
    *Deep Neural Networks for Regression*, we developed models based on data that
    was structured. In the next chapter, we move on to problems where the data type
    is unstructured. More specifically, we'll deal with the image type of data and
    go over the problem of image classification and recognition using deep neural
    network models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们介绍了当响应变量为数值型时开发预测模型的步骤。我们从一个具有201个参数的神经网络模型开始，然后开发了具有超过7000个参数的深度神经网络模型。你可能已经注意到，在本章中，我们使用了比上一章更深且更复杂的神经网络模型，而上一章中我们开发的是针对类别型目标变量的分类模型。在[第2章](c5c236d5-fc58-4d90-95b0-2b05b148b187.xhtml)，*用于多类分类的深度神经网络*和[第3章](07c9aa4a-1c93-490a-bfcd-7c4bcde639d5.xhtml)，*用于回归的深度神经网络*中，我们开发的模型基于结构化数据。在下一章中，我们将转向数据类型为非结构化的问题。更具体地说，我们将处理图像数据类型，并介绍如何使用深度神经网络模型解决图像分类和识别问题。
- en: In the next chapter, we will cover the steps required to develop an image recognition
    and prediction model using deep neural networks.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用深度神经网络开发图像识别和预测模型所需的步骤。
