- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: '**Convolutional Neural Networks** (**CNNs**) are responsible for the major
    breakthroughs in image recognition made in the past few years. In this chapter,
    we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）是近年来图像识别领域取得重大突破的关键。在本章中，我们将讨论以下主题：'
- en: Implementing a simple CNN
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个简单的 CNN
- en: Implementing an advanced CNN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个高级 CNN
- en: Retraining existing CNN models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新训练现有的 CNN 模型
- en: Applying StyleNet and the neural style project
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 StyleNet 和神经风格项目
- en: Implementing DeepDream
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现 DeepDream
- en: 'As a reminder, the reader can find all of the code for this chapter available
    online here: [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook),
    as well as the Packt repository: [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒读者，本章的所有代码可以在这里在线获取：[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)，以及
    Packt 仓库：[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In the previous chapters, we discussed **Dense Neural Networks** (**DNNs**)
    in which each neuron of a layer is connected to each neuron of the adjacent layer.
    In this chapter, we will focus on a special type of neural network that performs
    well for image classification: CNNs.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了**密集神经网络**（**DNNs**），其中一层的每个神经元都与相邻层的每个神经元相连接。在本章中，我们将重点介绍一种在图像分类中表现良好的特殊类型的神经网络：CNN。
- en: 'A CNN is a combination of two components: a feature extractor module followed
    by a trainable classifier. The first component includes a stack of convolution,
    activation, and pooling layers. A DNN does the classification. Each neuron in
    a layer is connected to those in the next layer.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是由两个组件组成的：一个特征提取模块，后接一个可训练的分类器。第一个组件包括一堆卷积、激活和池化层。一个 DNN 负责分类。每一层的神经元都与下一层的神经元相连接。
- en: In mathematics, a convolution is a function that is applied over the output
    of another function. In our case, we will consider using a matrix multiplication
    (filter) across an image. For our purposes, we find an image to be a matrix of
    numbers. These numbers may represent pixels or even image attributes. The convolution
    operation we will apply to these matrices involves moving a filter of fixed width
    across the image and using element-wise multiplication to get our result.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，卷积是一个应用于另一个函数输出的运算。在我们的例子中，我们考虑使用矩阵乘法（滤波器）作用于图像。对于我们的目的，我们将图像视为一个数字矩阵。这些数字可以代表像素或图像属性。我们将对这些矩阵应用卷积操作，方法是将一个固定宽度的滤波器在图像上移动，并使用逐元素相乘得到结果。
- en: 'See the following diagram for a conceptual understanding of how image convolution
    can work:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅下图，以便更好地理解图像卷积的工作原理：
- en: '![](img/B16254_08_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_08_01.png)'
- en: 'Figure 8.1: Application of a 2x2 convolutional filter across a 5x5 input matrix
    producing a new 4x4 feature layer'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：一个 2x2 的卷积滤波器应用于一个 5x5 的输入矩阵，生成一个新的 4x4 特征层
- en: In *Figure 8.1*, we see how a convolutional filter applied across an image (length
    by width by depth) operates to create a new feature layer. Here, we have a *2x2* convolutional
    filter, working in the valid spaces of the *5x5* input with a stride of 1 in both
    directions. The result is a *4x4* matrix. This new feature layer highlights the
    areas in the input image that activate the filter the most.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 8.1*中，我们看到如何将卷积滤波器应用于图像（长×宽×深度），从而创建一个新的特征层。在这里，我们使用一个*2x2*的卷积滤波器，作用于*5x5*输入的有效空间，并且在两个方向上的步幅都为1。结果是一个*4x4*的矩阵。这个新特征层突出了输入图像中激活滤波器最多的区域。
- en: CNNs also have other operations that fulfill more requirements, such as introducing
    non-linearities (ReLU), or aggregating parameters (max pooling, average pooling),
    and other similar operations. The preceding diagram is an example of applying
    a convolution operation on a *5x5 *array with the convolutional filter being a *2x2* matrix.
    The step size is 1 and we only consider valid placements. The trainable variables
    in this operation will be the *2x2* filter weights.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 还具有其他操作来满足更多的需求，例如引入非线性（ReLU），或聚合参数（最大池化、平均池化）以及其他类似操作。上面的示例图展示了在一个*5x5*数组上应用卷积操作，卷积滤波器是一个*2x2*的矩阵。步幅为1，并且我们只考虑有效的放置位置。在此操作中的可训练变量将是*2x2*滤波器的权重。
- en: After a convolution, it is common to follow up with an aggregation operation,
    such as max pooling. The pooling operation goal is to reduce the number of parameters,
    computation loads, and memory usage. The maximum pooling preserves only the strongest
    features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积操作之后，通常会跟随一个聚合操作，例如最大池化。池化操作的目标是减少参数数量、计算负担和内存使用。最大池化保留了最强的特征。
- en: 'The following diagram provides an example of how max pooling operates. In this
    example, it has a *2x2* region with a stride of 2 in both directions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示提供了最大池化操作的一个示例。在这个例子中，池化操作的区域是一个 *2x2* 的区域，步长为 2。
- en: '![](img/B16254_08_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_08_02.png)'
- en: 'Figure 8.2: Application of a max pooling operation on a 4x4 input image'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：在 4x4 输入图像上应用最大池化操作
- en: '*Figure 8.2* shows how a max pooling operation could operate. Here, we have
    a *2x2* window, running on the valid spaces of a *4x4* input with a stride of
    2 in both directions. The result is a *2x2* matrix, which is simply the maximum
    value of each region.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.2* 展示了最大池化操作的工作原理。在这里，我们有一个 *2x2* 的窗口，在一个 *4x4* 的输入图像上滑动，步长为 2。结果是一个 *2x2*
    的矩阵，它就是每个区域的最大值。'
- en: Although we will start by creating our own CNN for image recognition, I recommend
    using existing architectures, as we will do in the remainder of the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们将通过创建自己的 CNN 来进行图像识别，但我建议使用现有的架构，正如我们在本章其余部分将要做的那样。
- en: It is common to take a pre-trained network and retrain it with a new dataset
    and a new fully connected layer at the end. This method is beneficial because
    we don't have to train a model from scratch; we just have to fine-tune a pre-trained
    model for our novel task. We will illustrate it in the *Retraining existing CNN
    models* recipe later in the chapter, where we will retrain an existing architecture
    to improve on our CIFAR-10 predictions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们会使用一个预训练的网络，并通过新的数据集和一个新的全连接层对其进行再训练。这种方法很有利，因为我们不必从零开始训练模型；我们只需要对预训练模型进行微调，以适应我们的新任务。我们将在本章稍后的
    *重新训练现有的 CNN 模型* 部分进行演示，其中我们将重新训练现有架构，以提高 CIFAR-10 的预测性能。
- en: Without any further delay, let's start immediately with how to implement a simple
    CNN.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，我们立即开始实现一个简单的 CNN。
- en: Implementing a simple CNN
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个简单的 CNN
- en: In this recipe, we will develop a CNN based on the LeNet-5 architecture, which
    was first introduced in 1998 by Yann LeCun et al. for handwritten and machine-printed
    character recognition.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将开发一个基于 LeNet-5 架构的 CNN，LeNet-5 首次由 Yann LeCun 等人于 1998 年提出，用于手写和机器打印字符的识别。
- en: '![LeNet-5 Original Image from Paper](img/B16254_08_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![LeNet-5 原始图像来源于论文](img/B16254_08_03.png)'
- en: 'Figure 8.3: LeNet-5 architecture – Original image published in [LeCun et al.,
    1998]'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：LeNet-5 架构 – 原始图像来源于 [LeCun 等人, 1998]
- en: This architecture consists of two sets of CNNs composed of convolution-ReLU-max
    pooling operations used for feature extraction, followed by a flattening layer
    and two fully connected layers to classify the images.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构由两组 CNN 组成，包含卷积-ReLU-最大池化操作，用于特征提取，随后是一个扁平化层和两个全连接层，用于分类图像。
- en: Our goal will be to improve upon our accuracy in predicting MNIST digits.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是提高对 MNIST 数字的预测准确性。
- en: Getting ready
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To access the MNIST data, Keras provides a package (`tf.keras.datasets`) that
    has excellent dataset-loading functionalities. (Note that TensorFlow also provides
    its own collection of ready-to-use datasets with the TF Datasets API.) After loading
    the data, we will set up our model variables, create the model, train the model
    in batches, and then visualize loss, accuracy, and some sample digits.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 MNIST 数据，Keras 提供了一个包（`tf.keras.datasets`），它具有出色的数据集加载功能。（请注意，TensorFlow
    也提供了自己的现成数据集集合，通过 TF Datasets API。）加载数据后，我们将设置模型变量，创建模型，按批次训练模型，然后可视化损失、准确率和一些样本数字。
- en: How to do it...
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到...
- en: 'Perform the following steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, we''ll load the necessary libraries and start a graph session:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将加载必要的库并启动图形会话：
- en: '[PRE0]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will load the data and reshape the images in a four-dimensional matrix:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载数据并将图像重塑为四维矩阵：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that the MNIST dataset downloaded here includes training and test datasets.
    These datasets are composed of the grayscale images (integer arrays with shape
    (num_sample, 28,28)) and the labels (integers in the range 0-9). We pad the images
    by 2 pixels since in the LeNet-5 paper input images were *32x32*.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，这里下载的MNIST数据集包括训练集和测试集。该数据集由灰度图像（形状为(num_sample, 28, 28)的整数数组）和标签（范围为0-9的整数）组成。我们对图像进行了2像素的填充，因为在LeNet-5论文中，输入图像是*32x32*的。
- en: 'Now, we will set the model parameters. Remember that the depth of the image
    (number of channels) is 1 because these images are grayscale. We''ll also set
    up a seed to have reproducible results:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将设置模型的参数。记住，图像的深度（通道数）是1，因为这些图像是灰度图像。我们还将设置一个种子，以确保结果可复现：
- en: '[PRE2]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll declare our training data variables and our test data variables. We
    will have different batch sizes for training and evaluation. You may change these,
    depending on the physical memory that is available for training and evaluating:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将声明我们的训练数据变量和测试数据变量。我们将为训练和评估使用不同的批次大小。你可以根据可用的物理内存调整这些大小：
- en: '[PRE3]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll normalize our images to change the values of all pixels to a common
    scale:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将对图像进行归一化，将所有像素的值转换为统一的尺度：
- en: '[PRE4]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we''ll declare our model. We will have the feature extractor module composed
    of two convolutional/ReLU/max pooling layers followed by the classifier with fully
    connected layers. Also, to get the classifier to work, we flatten the output of
    the feature extractor module so we can use it in the classifier. Note that we
    use a softmax activation function at the last layer of the classifier. Softmax
    turns numeric output (logits) into probabilities that sum to one:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将声明我们的模型。我们将有一个特征提取模块，由两个卷积/ReLU/最大池化层组成，接着是一个由全连接层构成的分类器。此外，为了使分类器能够工作，我们将特征提取模块的输出展平，以便可以在分类器中使用。请注意，我们在分类器的最后一层使用了softmax激活函数。Softmax将数值输出（logits）转换为概率，使其总和为1：
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we will compile the model using an Adam (Adaptive Moment Estimation)
    optimizer. Adam uses adaptive learning rates and momentum that allow us to get
    to local minima faster, and so converge faster. As our targets are integers and
    not in a one-hot-encoded format, we will use the sparse categorical cross-entropy
    loss function. Then we will also add an accuracy metric to determine how accurate
    the model is on each batch:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Adam（自适应矩估计）优化器来编译模型。Adam使用自适应学习率和动量，使我们能够更快地达到局部最小值，从而加速收敛。由于我们的目标是整数，而不是独热编码格式，我们将使用稀疏分类交叉熵损失函数。然后，我们还将添加一个准确度指标，以评估模型在每个批次上的准确性：
- en: '[PRE6]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we print a string summary of our network:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们打印网络的字符串摘要：
- en: '[PRE7]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B16254_08_04.png)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16254_08_04.png)'
- en: 'Figure 8.4: The LeNet-5 architecture'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.4：LeNet-5架构
- en: The LeNet-5 model has 7 layers and contains 61,706 trainable parameters. So,
    let's go to train the model.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LeNet-5模型有7层，包含61,706个可训练参数。现在，让我们开始训练模型。
- en: 'We can now start training our model. We loop through the data in randomly chosen
    batches. Every so often, we choose to evaluate the model on the train and test
    batches and record the accuracy and loss. We can see that, after 300 epochs, we quickly
    achieve 96-97% accuracy on the test data:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练我们的模型了。我们通过随机选择的批次来遍历数据。每隔一段时间，我们选择在训练集和测试集批次上评估模型，并记录准确率和损失值。我们可以看到，经过300个周期后，我们很快在测试数据上达到了96-97%的准确率：
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This results in the following output:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE9]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is the code to plot the loss and accuracy using `Matplotlib`:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是使用`Matplotlib`绘制损失和准确率的代码：
- en: '[PRE10]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then get the following plots:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们得到以下图表：
- en: '![](img/B16254_08_05.png)'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16254_08_05.png)'
- en: 'Figure 8.5: The left plot is the train and test set accuracy across our 300
    training epochs. The right plot is the softmax loss value over 300 epochs.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.5：左图是我们300个训练周期中训练集和测试集的准确率。右图是300个周期中的softmax损失值。
- en: 'If we want to plot a sample of the latest batch results, here is the code to
    plot a sample consisting of six of the latest results:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们想要绘制最新批次结果的示例，这里是绘制包含六个最新结果的样本的代码：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We get the following output for the code above:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/46B36DDA.tmp](img/B16254_08_06.png)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/46B36DDA.tmp](img/B16254_08_06.png)'
- en: 'Figure 8.6: A plot of six random images with the actual and predicted values
    in the title. The lower-left picture was predicted to be a 6, when in fact it
    is a 4.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：六个随机图像的图示，标题中包括实际值和预测值。左下角的图片被预测为6，实际上它是4。
- en: Using a simple CNN, we achieved a good result in accuracy and loss for this
    dataset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个简单的CNN，我们在此数据集上取得了较好的准确率和损失结果。
- en: How it works...
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We increased our performance on the MNIST dataset and built a model that quickly
    achieves about 97% accuracy while training from scratch. Our features extractor
    module is a combination of convolutions, ReLU, and max pooling. Our classifier
    is a stack of fully connected layers. We trained in batches of size 100 and looked
    at the accuracy and loss across the epochs. Finally, we also plotted six random
    digits and found that the model prediction fails to predict one image. The model
    predicts a 6 when in fact it's a 4.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在MNIST数据集上的表现有所提升，构建了一个从零开始训练并迅速达到约97%准确率的模型。我们的特征提取模块是卷积、ReLU和最大池化的组合。我们的分类器是全连接层的堆叠。我们在批次大小为100的情况下进行训练，并查看了跨越各个epoch的准确率和损失情况。最后，我们还绘制了六个随机数字，发现模型预测失败，未能正确预测一张图片。模型预测为6，实际上是4。
- en: CNN does very well with image recognition. Part of the reason for this is that
    the convolutional layer creates its low-level features that are activated when
    they come across a part of the image that is important. This type of model creates
    features on its own and uses them for prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在图像识别方面表现非常出色。部分原因在于卷积层生成其低级特征，当遇到图像中的重要部分时，这些特征会被激活。这种模型能够自我创建特征并将其用于预测。
- en: There's more...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In the past few years, CNN models have made vast strides in image recognition.
    Many novel ideas are being explored and new architectures are discovered very
    frequently. A vast repository of scientific papers in this field is a repository
    website called arXiv.org ([https://arxiv.org/](https://arxiv.org/)), which is
    created and maintained by Cornell University. arXiv.org includes some very recent
    articles in many areas, including computer science and computer science subfields
    such as computer vision and image recognition ([https://arxiv.org/list/cs.CV/recent](https://arxiv.org/list/cs.CV/recent)).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，CNN模型在图像识别方面取得了巨大进展。许多新颖的想法正在被探索，并且新的架构频繁被发现。该领域有一个庞大的科学论文库，名为arXiv.org（[https://arxiv.org/](https://arxiv.org/)），由康奈尔大学创建和维护。arXiv.org包含许多领域的最新文章，包括计算机科学及其子领域，如计算机视觉和图像识别（[https://arxiv.org/list/cs.CV/recent](https://arxiv.org/list/cs.CV/recent)）。
- en: See also
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'Here is a list of some great resources you can use to learn about CNNs:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些你可以用来了解CNN的优秀资源：
- en: Stanford University has a great wiki here: [http://scarlet.stanford.edu/teach/index.php/An_Introduction_to_Convolutional_Neural_Networks](http://scarlet.stanford.edu/teach/index.php/An_Introduction_to_Convolutional_Neural_Networks)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯坦福大学有一个很好的维基，详情请见：[http://scarlet.stanford.edu/teach/index.php/An_Introduction_to_Convolutional_Neural_Networks](http://scarlet.stanford.edu/teach/index.php/An_Introduction_to_Convolutional_Neural_Networks)
- en: '*Deep Learning* by Michael Nielsen, found here: [http://neuralnetworksanddeeplearning.com/chap6.html](http://neuralnetworksanddeeplearning.com/chap6.html)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习* 由Michael Nielsen编写，详情请见：[http://neuralnetworksanddeeplearning.com/chap6.html](http://neuralnetworksanddeeplearning.com/chap6.html)'
- en: '*An Introduction to Convolutional Neural Networks* by Jianxin Wu, found here: [https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf](https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*卷积神经网络简介* 由吴建新编写，详情请见：[https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf](https://pdfs.semanticscholar.org/450c/a19932fcef1ca6d0442cbf52fec38fb9d1e5.pdf)'
- en: '*LeNet-5, convolutional neural networks* by Yann LeCun: [http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LeNet-5，卷积神经网络* 由Yann LeCun编写：[http://yann.lecun.com/exdb/lenet/](http://yann.lecun.com/exdb/lenet/)'
- en: '*Gradient-Based Learning Applied to Document Recognition* by Yann LeCun et
    al.: [http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于梯度的学习应用于文档识别* 由Yann LeCun等人编写：[http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)'
- en: Implementing an advanced CNN
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个高级CNN
- en: 'It is crucial to be able to extend CNN models for image recognition so that
    we understand how to increase the depth of the network. This way, we may increase
    the accuracy of our predictions if we have enough data. Extending the depth of
    CNN networks is done in a standard fashion: we just repeat the convolution, max
    pooling, and ReLU in series until we are satisfied with the depth. Many of the
    more accurate image recognition networks operate in this fashion.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 能够扩展 CNN 模型进行图像识别至关重要，这样我们才能理解如何增加网络的深度。这样，如果我们有足够的数据，我们可能提高预测的准确性。扩展 CNN 网络的深度是以标准方式进行的：我们只需重复卷积、最大池化和
    ReLU 直到我们对深度感到满意。许多更精确的图像识别网络都是以这种方式运行的。
- en: 'Loading and preprocessing data may cause a big headache: most image datasets
    will be too large to fit into memory, but image preprocessing will be needed to
    improve the performance of the model. What we can do with TensorFlow is use the
    `tf.data` API to create an input pipeline. This API contains a set of utilities
    for loading and preprocessing data. Using it, we will instantiate a `tf.data.Dataset`
    object from the CIFAR-10 dataset (downloaded through the Keras dataset API `tf.keras.datasets`),
    combine consecutive elements of this dataset into batches, and apply transformations
    to each image. Also, with image recognition data, it is common to randomly perturb
    the image before sending it through for training. Here, we will randomly crop,
    flip, and change the brightness.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载和预处理可能会让人头疼：大多数图像数据集过大，无法完全加载到内存中，但为了提高模型性能，图像预处理是必须的。我们可以使用 TensorFlow
    的 `tf.data` API 创建输入管道。这个 API 提供了一套用于加载和预处理数据的工具。通过它，我们将从 CIFAR-10 数据集实例化一个 `tf.data.Dataset`
    对象（通过 Keras 数据集 API `tf.keras.datasets` 下载），将该数据集的连续元素合并成批次，并对每张图像应用变换。另外，在图像识别数据中，通常会在训练前随机扰动图像。在这里，我们将随机裁剪、翻转并调整亮度。
- en: Getting ready
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will implement a more advanced method of reading image data
    and use a larger CNN to do image recognition on the CIFAR-10 dataset ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
    This dataset has 60,000 *32x32* images that fall into exactly one of 10 possible
    classes. The potential labels for the pictures are airplane, automobile, bird,
    cat, deer, dog, frog, horse, ship, and truck. Please also refer to the first bullet
    point in the *See also* section.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将实现一种更高级的图像数据读取方法，并使用一个更大的 CNN 对 CIFAR-10 数据集进行图像识别（[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)）。该数据集包含
    60,000 张*32x32*的图像，属于 10 个可能类别中的一个。图像的潜在标签包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。请参阅*另见*部分的第一点。
- en: The official TensorFlow `tf.data` tutorial is available under the *See also *section
    at the end of this recipe.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 官方的 TensorFlow `tf.data` 教程可以在本食谱末尾的*另见*部分找到。
- en: How to do it...
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'To start with, we load the necessary libraries:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载必要的库：
- en: '[PRE12]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we''ll declare some dataset and model parameters and then some image transformation
    parameters, such as what size the random cropped images will take:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将声明一些数据集和模型参数，然后声明一些图像变换参数，例如随机裁剪图像的大小：
- en: '[PRE13]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we''ll get the train and test images from the CIFAR-10 dataset using the
    `keras.datasets` API. This API provides few toy datasets where data fits in memory,
    so the data is expressed in NumPy arrays (the core Python library for scientific
    computing):'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用 `keras.datasets` API 从 CIFAR-10 数据集中获取训练和测试图像。这个 API 提供了几个可以完全加载到内存的小型数据集，因此数据会以
    NumPy 数组的形式表示（NumPy 是用于科学计算的核心 Python 库）：
- en: '[PRE14]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we''ll create a train and a test TensorFlow dataset from the NumPy arrays
    using `tf.data.Dataset`, so we can build a flexible and efficient pipeline for
    images using the `tf.data` API:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 `tf.data.Dataset` 从 NumPy 数组创建训练和测试的 TensorFlow 数据集，以便利用 `tf.data`
    API 构建一个灵活高效的图像管道：
- en: '[PRE15]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''ll define a reading function that will load and distort the images slightly
    for training with TensorFlow''s built-in image modification functions:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将定义一个读取函数，该函数将加载并稍微扭曲图像，以便使用 TensorFlow 内建的图像修改功能进行训练：
- en: '[PRE16]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have an image pipeline function and two TensorFlow datasets, we
    can initialize both the training image pipeline and the test image pipeline:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个图像处理管道函数和两个 TensorFlow 数据集，我们可以初始化训练图像管道和测试图像管道：
- en: '[PRE17]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that, in this example, our input data fits in memory so we use the `from_tensor_slices()`
    method to convert all the images into `tf.Tensor`. But the `tf.data` API allows
    processing large datasets that do not fit in memory. The iteration over the dataset
    happens in a streaming fashion.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，我们的输入数据适合内存，因此我们使用`from_tensor_slices()`方法将所有图像转换为`tf.Tensor`。但是`tf.data`
    API允许处理不适合内存的大型数据集。对数据集的迭代是以流式传输的方式进行的。
- en: 'Next, we can create our sequential model. The model we will use has two convolutional
    layers followed by three fully connected layers. The two convolutional layers
    will create 64 features each. The first fully connected layer will connect the
    second convolutional layer with 384 hidden nodes. The second fully connected operation
    will connect those 384 hidden nodes to 192 hidden nodes. The final hidden layer
    operation will then connect the 192 nodes to the 10 output classes we are trying
    to predict. We will use the softmax function at the last layer because a picture
    can only take on exactly one category, so the output should be a probability distribution
    over the 10 targets:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以创建我们的序列模型。我们将使用的模型有两个卷积层，后面跟着三个全连接层。两个卷积层将分别创建64个特征。第一个全连接层将第二个卷积层与384个隐藏节点连接起来。第二个全连接操作将这384个隐藏节点连接到192个隐藏节点。最后一个隐藏层操作将这192个节点连接到我们要预测的10个输出类别。在最后一层我们将使用softmax函数，因为一张图片只能取一个确切的类别，所以输出应该是对这10个目标的概率分布：
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we''ll compile our model. Our loss will be the categorical cross-entropy
    loss. We add an accuracy metric that takes in the predicted logits from the model
    and the actual targets and returns the accuracy for recording statistics on the
    train/test sets. We also run the summary method to print a summary page:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将编译我们的模型。我们的损失将是分类交叉熵损失。我们添加了一个精度度量，它接收模型预测的logits和实际目标，并返回用于记录训练/测试集统计信息的准确率。我们还运行summary方法以打印一个总结页面：
- en: '[PRE19]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/B16254_08_07.png)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16254_08_07.png)'
- en: 'Figure 8.7: The model summary is composed of 3 VGG blocks (a VGG – Visual Geometry
    Group – block is a sequence of convolutional layers, followed by a max pooling
    layer for spatial downsampling), followed by a classifier.'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图8.7：模型摘要由3个VGG块组成（VGG - Visual Geometry Group - 块是一系列卷积层，后跟用于空间下采样的最大池化层），随后是一个分类器。
- en: 'We now fit the model, looping through our training and test input pipelines.
    We will save the training loss and the test accuracy:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们拟合模型，通过我们的训练和测试输入管道进行循环。我们将保存训练损失和测试准确率：
- en: '[PRE20]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, here is some `Matplotlib` code that will plot the loss and test accuracy
    throughout the training:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，这里是一些`Matplotlib`代码，将绘制整个训练过程中的损失和测试准确率：
- en: '[PRE21]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We get the following plots for this recipe:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们为这个配方得到以下图表：
- en: '![](img/B16254_08_08.png)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16254_08_08.png)'
- en: 'Figure 8.8: The training loss is on the left and the test accuracy is on the
    right. For the CIFAR-10 image recognition CNN, we were able to achieve a model
    that reaches around 80% accuracy on the test set.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：训练损失在左侧，测试准确率在右侧。对于CIFAR-10图像识别CNN，我们能够在测试集上达到大约80%的准确率。
- en: How it works...
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After we downloaded the CIFAR-10 data, we established an image pipeline. We
    used this train and test pipeline to try to predict the correct category of the
    images. By the end, the model had achieved around 80% accuracy on the test set.
    We can achieve better accuracy by using more data, fine-tuning the optimizers,
    or adding more epochs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下载了CIFAR-10数据之后，我们建立了一个图像管道。我们使用这个训练和测试管道来尝试预测图像的正确类别。到最后，模型在测试集上达到了大约80%的准确率。我们可以通过使用更多数据、微调优化器或增加更多epochs来达到更高的准确率。
- en: See also
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: For more information about the CIFAR-10 dataset, please see *Learning Multiple
    Layers of Features from Tiny Images*, Alex Krizhevsky, 2009: [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关CIFAR-10数据集的更多信息，请参阅《从小图像中学习多层特征》，Alex Krizhevsky，2009：[https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)。
- en: The `tf.data` TensorFlow tutorial: [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.data` TensorFlow教程：[https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)。'
- en: 'Introduction to Keras for Engineers (data loading and preprocessing): [https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing](https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向工程师的Keras简介（数据加载和预处理）：[https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing](https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing)。
- en: Retraining existing CNN models
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新训练现有的CNN模型
- en: Training a new image recognition model from scratch requires a lot of time and
    computational power. If we can take a pre-trained network and retrain it with
    our images, it may save us computational time. For this recipe, we will show how
    to use a pre-trained TensorFlow image recognition model and fine-tune it to work
    on a different set of images.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始训练一个新的图像识别模型需要大量时间和计算资源。如果我们能拿一个预训练的网络，并用我们的图像重新训练它，可能会节省计算时间。在这个方案中，我们将展示如何使用一个预训练的TensorFlow图像识别模型，并对其进行微调，以便处理不同的图像集。
- en: We will illustrate how to use transfer learning from a pre-trained network for
    CIFAR-10\. The idea is to reuse the weights and structure of the prior model from
    the convolutional layers and retrain the fully connected layers at the top of
    the network. This method is called **fine-tuning**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将演示如何使用来自预训练网络的迁移学习来处理CIFAR-10。这个方法的思路是重用先前模型中卷积层的权重和结构，并重新训练网络顶部的全连接层。这个方法被称为**微调**。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The CNN network we are going to employ uses a very popular architecture called **Inception**.
    The Inception CNN model was created by Google and has performed very well on many
    image recognition benchmarks. For details, see the paper referenced in the second
    bullet point of the *See also* section.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的CNN网络采用一种非常流行的架构，叫做**Inception**。Inception CNN模型由谷歌创建，在许多图像识别基准测试中表现出色。详细信息请参见*另见*部分第二个项目中提到的论文。
- en: The main Python script we will cover shows how to get CIFAR-10 image data and
    transform it into the Inception retraining format. After that, we will reiterate
    how to train the Inception v3 network on our images.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍的主要Python脚本展示了如何获取CIFAR-10图像数据，并将其转换为Inception重训练格式。之后，我们将再次说明如何在我们的图像上训练Inception
    v3网络。
- en: How to do it...
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Perform the following steps:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'We''ll start by loading the necessary libraries:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载必要的库开始：
- en: '[PRE22]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We''ll now set the parameters used later by the `tf.data.Dataset` API:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将设置稍后通过`tf.data.Dataset` API使用的参数：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we''ll download the CIFAR-10 data, and we''ll also declare the 10 categories
    to reference when saving the images later on:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将下载CIFAR-10数据，并声明用于稍后保存图像时引用的10个类别：
- en: '[PRE24]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then, we''ll initialize the data pipeline using `tf.data.Dataset` for the train
    and test datasets:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用`tf.data.Dataset`初始化数据管道，用于训练和测试数据集：
- en: '[PRE25]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Inception v3 is pretrained on the ImageNet dataset, so our CIFAR-10 images must
    match the format of these images. The width and height expected should be no smaller
    than 75, so we will resize our images to *75x75* spatial size. Then, the images
    should be normalized, so we will apply the inception preprocessing task (the `preprocess_input`
    method) on each image.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Inception v3在ImageNet数据集上进行过预训练，因此我们的CIFAR-10图像必须与这些图像的格式匹配。预期的宽度和高度应不小于75，因此我们将图像调整为*75x75*的空间大小。然后，图像应该被归一化，我们将对每张图像应用Inception预处理任务（`preprocess_input`方法）。
- en: '[PRE26]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, we will create our model based on the InceptionV3 model. We will load the
    InceptionV3 model using the `tensorflow.keras.applications` API. This API contains
    pre-trained deep learning models that can be used for prediction, feature extraction,
    and fine-tuning. Then, we will load the weights without the classification head.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将基于InceptionV3模型创建我们的模型。我们将使用`tensorflow.keras.applications` API加载InceptionV3模型。该API包含可以用于预测、特征提取和微调的预训练深度学习模型。然后，我们将加载没有分类头的权重。
- en: '[PRE27]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We build our own model on top of the InceptionV3 model by adding a classifier
    with three fully connected layers.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在InceptionV3模型的基础上构建自己的模型，添加一个具有三层全连接层的分类器。
- en: '[PRE28]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We''ll set the base layers in Inception as not trainable. Only the classifier
    weights will be updated during the back-propagation phase (not the Inception weights):'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把Inception的基础层设置为不可训练。只有分类器的权重会在反向传播阶段更新（Inception的权重不会更新）：
- en: '[PRE29]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we''ll compile our model. Our loss will be the categorical cross-entropy
    loss. We add an accuracy metric that takes in the predicted logits from the model
    and the actual targets and returns the accuracy for recording statistics on the
    train/test sets:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将编译我们的模型。我们的损失函数将是分类交叉熵损失。我们添加了一个精度指标，该指标接收模型预测的对数和实际目标，并返回用于记录训练/测试集统计数据的精度：
- en: '[PRE30]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We''ll now fit the model, looping through our training and test input pipelines:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将拟合模型，通过我们的训练和测试输入流水线循环进行：
- en: '[PRE31]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'By the end, the model had achieved around 63% accuracy on the test set:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到最后，该模型在测试集上达到了约 63% 的准确率：
- en: '[PRE32]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: How it works...
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: After we downloaded the CIFAR-10 data, we established an image pipeline to convert
    the images into the required Inception format. We added a classifier on top of
    the InceptionV3 model and trained it to predict the correct category of the CIFAR-10
    images. By the end, the model had achieved around 63% accuracy on the test set.
    Remember that we are fine-tuning the model and retraining the fully connected
    layers at the top to fit our 10-category data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载了 CIFAR-10 数据之后，我们建立了一个图像流水线来将图像转换为所需的 Inception 格式。我们在 InceptionV3 模型之上添加了一个分类器，并对其进行了训练，以预测
    CIFAR-10 图像的正确类别。到最后，该模型在测试集上达到了约 63% 的准确率。请记住，我们正在微调模型并重新训练顶部的全连接层，以适应我们的十类数据。
- en: See also
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: TensorFlow Inception-v3 documentation: [https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Inception-v3 文档：[https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3)
- en: 'Keras Applications documentation: [https://keras.io/api/applications/](https://keras.io/api/applications/)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras Applications 文档：[https://keras.io/api/applications/](https://keras.io/api/applications/)
- en: GoogLeNet Inception-v3 paper: [https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GoogLeNet Inception-v3 论文：[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)
- en: Applying StyleNet and the neural style project
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用 StyleNet 和神经风格项目
- en: Once we have an image recognition CNN trained, we can use the network itself
    for some interesting data and image processing. StyleNet is a procedure that attempts
    to learn an image style from one picture and apply it to a second picture while
    keeping the second image structure (or content) intact. To do so, we have to find
    intermediate CNN nodes that correlate strongly with a style, separately from the
    content of the image.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了一个图像识别的 CNN，我们可以使用网络本身进行一些有趣的数据和图像处理。StyleNet 是一个过程，试图从一个图片中学习风格，并将其应用到第二张图片，同时保持第二张图片的结构（或内容）不变。为了做到这一点，我们必须找到与风格强相关的中间
    CNN 节点，独立于图片内容。
- en: StyleNet is a procedure that takes two images and applies the style of one image
    to the content of the second image. It is based on a famous paper by Leon Gatys
    in 2015, *A Neural Algorithm of Artistic Style *(refer to the first bullet point
    under the next *See also* section). The authors found a property in some CNNs
    containing intermediate layers. Some of them seem to encode the style of a picture,
    and some others its content. To this end, if we train the style layers on the
    style picture and the content layers on the original image, and back-propagate
    those calculated losses, we can change the original image to be more like the
    style image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: StyleNet 是一个过程，它接受两幅图像并将一幅图像的风格应用到第二幅图像的内容上。它基于 2015 年 Leon Gatys 的著名论文，*A Neural
    Algorithm of Artistic Style*（参见下一节*参见*部分下的第一条）进行操作。作者在一些 CNN 中发现了一种包含中间层的特性。其中一些似乎编码了图片的风格，而另一些则编码了其内容。因此，如果我们在风格图片上训练风格层，并在原始图片上训练内容层，并反向传播这些计算出的损失，我们就可以将原始图片更改为更像风格图片。
- en: Getting ready
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe is an adapted version of the official TensorFlow neural style transfer,
    which is available under the *See also* section at the end of this recipe.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱是官方 TensorFlow 神经风格迁移的改编版本，可在本食谱结尾的*参见*部分找到。
- en: To accomplish this, we will use the network recommended by Gatys in *A Neural
    Algorithm of Artistic Style*; called **imagenet-vgg-19**.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用 Gatys 在*A Neural Algorithm of Artistic Style*中推荐的网络，称为**imagenet-vgg-19**。
- en: How to do it...
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'First, we''ll start our Python script by loading the necessary libraries:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将通过加载必要的库启动我们的 Python 脚本：
- en: '[PRE33]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then we can declare the locations of our two images: the original image and
    the style image. For our purposes, we will use the cover image of this book for
    the original image; for the style image, we will use *Starry Night *by Vincent
    van Gogh. Feel free to use any two pictures you want here. If you choose to use
    these pictures, they are available on the book''s GitHub site, [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook) (navigate
    to the StyleNet section):'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以声明两张图像的位置：原始图像和风格图像。对于我们的示例，我们将使用本书的封面图像作为原始图像；风格图像我们将使用文森特·凡高的 *《星夜》*。你也可以使用任何你想要的两张图片。如果你选择使用这些图片，它们可以在本书的
    GitHub 网站上找到，[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)（导航到
    StyleNet 部分）：
- en: '[PRE34]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we''ll load the two images with `scipy` and change the style image to fit
    the content image dimensions:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 `scipy` 加载两张图像，并调整风格图像的大小，使其与内容图像的尺寸相符：
- en: '[PRE35]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we''ll display the content and style images:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将展示内容图像和风格图像：
- en: '[PRE36]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B6B0CA0A.tmp](img/B16254_08_09.png)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B6B0CA0A.tmp](img/B16254_08_09.png)'
- en: 'Figure 8.9: Example content and style images'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.9：示例内容和风格图像
- en: Now, we will load the VGG-19 model pre-trained on ImageNet without the classification
    head. We will use the `tensorflow.keras.applications` API. This API contains pre-trained
    deep learning models that can be used for prediction, feature extraction, and
    fine-tuning.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将加载在 ImageNet 上预训练的 VGG-19 模型，但不包括分类头。我们将使用 `tensorflow.keras.applications`
    API。这个 API 包含了可用于预测、特征提取和微调的预训练深度学习模型。
- en: '[PRE37]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we''ll display the VGG-19 architecture:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将展示 VGG-19 的架构：
- en: '[PRE38]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In neural style transfer, we want to apply the style of one image to the content
    of another image. A CNN is composed of several convolutional and pooling layers.
    The convolutional layers extract complex features and the pooling layers give
    spatial information. Gatys'' paper recommends a few strategies for assigning intermediate
    layers to the content and style images. While we should keep `block4_conv2` for
    the content image, we can try different combinations of the other `blockX_conv1` layer
    outputs for the style image:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在神经风格迁移中，我们希望将一张图像的风格应用到另一张图像的内容上。卷积神经网络（CNN）由多个卷积层和池化层组成。卷积层提取复杂特征，而池化层提供空间信息。Gatys
    的论文推荐了一些策略，用于为内容图像和风格图像分配中间层。我们应当保留 `block4_conv2` 作为内容图像的层，可以尝试其他 `blockX_conv1`
    层输出的不同组合来作为风格图像的层：
- en: '[PRE39]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: While the values of the intermediate feature maps represent the content of an
    image, the style can be described by the means and correlations across these feature
    maps. Here, we define the Gram matrix to capture the style of an image. The Gram
    matrix measures the degree of correlation between each of the feature maps. This
    computation is done on each intermediate feature map and gets only information
    about the texture of an image. Note that we lose information about its spatial
    structure.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然中间特征图的值代表了图像的内容，但风格可以通过这些特征图的均值和相关性来描述。在这里，我们定义了 Gram 矩阵来捕捉图像的风格。Gram 矩阵衡量每个特征图之间的相关性程度。这个计算是针对每个中间特征图进行的，只获得图像的纹理信息。注意，我们会丢失关于图像空间结构的信息。
- en: '[PRE40]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we build a model that returns style and content dictionaries that contain
    the name of each layer and associated content/style tensors. The Gram matrix is
    applied on the style layers:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个模型，返回包含每个层名称及其相关内容/风格张量的风格和内容字典。Gram 矩阵应用于风格层：
- en: '[PRE41]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Set the style and content target values. They will be used in the loss computation:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置风格和内容的目标值，它们将用于损失计算：
- en: '[PRE42]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Adam and LBFGS usually have the same error and converge quickly but LBFGS is
    better than Adam with larger images. While the paper recommends using LBFGS, as our
    images are small, we will choose the Adam optimizer.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Adam 和 LBFGS 通常会有相同的误差并且很快收敛，但 LBFGS 在处理大图像时优于 Adam。虽然论文推荐使用 LBFGS，由于我们的图像较小，我们将选择
    Adam 优化器。
- en: '[PRE43]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we compute the total loss as a weighted sum of the content and the style
    losses:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将计算总损失，它是内容损失和风格损失的加权和：
- en: '[PRE44]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The content loss will compare our original image and our current image (through
    the content layer features). The style loss will compare the style features we
    have pre-computed with the style features from the input image. The third and
    final loss term will help smooth out the image. We use total variation loss here
    to penalize dramatic changes in neighboring pixels, as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内容损失将比较我们的原始图像和当前图像（通过内容层特征）。风格损失将比较我们预先计算的风格特征与输入图像中的风格特征。第三个也是最终的损失项将有助于平滑图像。我们在这里使用总变差损失来惩罚相邻像素的剧烈变化，如下所示：
- en: '[PRE45]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we declare a utility function. As we have a float image, we need to keep
    the pixel values between 0 and 1:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们声明一个工具函数。由于我们有一个浮动图像，需要将像素值保持在0和1之间：
- en: '[PRE46]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, we declare another utility function to convert a tensor to an image:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们声明另一个工具函数，将张量转换为图像：
- en: '[PRE47]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we use gradient tape to run the gradient descent, generate our new image,
    and display it, as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用梯度带运行梯度下降，生成我们的新图像，并显示如下：
- en: '[PRE48]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](img/B16254_08_10.png)'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B16254_08_10.png)'
- en: 'Figure 8.10: Using the StyleNet algorithm to combine the book cover image with
    Starry Night'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：使用StyleNet算法将书籍封面图像与《星夜》结合
- en: Note that a different style of emphasis can be used by changing the content
    and style weighting.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，可以通过更改内容和样式权重来使用不同的强调方式。
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We first loaded the two images, then loaded the pre-trained network weights
    and assigned layers to the content and style images. We calculated three loss functions:
    a content image loss, a style loss, and a total variation loss. Then we trained
    random noise pictures to use the style of the style image and the content of the
    original image. Style transfer can be used in photo and video editing applications,
    games, art, virtual reality, and so on. For example, at the 2019 Game Developers
    Conference, Google introduced Stadia to change a game''s art in real time. A clip
    of it live in action is available under the last bullet of the *See also* section
    at the end of this recipe.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载了两张图像，然后加载了预训练的网络权重，并将层分配给内容图像和风格图像。我们计算了三个损失函数：内容图像损失、风格损失和总变差损失。然后，我们训练了随机噪声图片，以使用风格图像的风格和原始图像的内容。风格迁移可以用于照片和视频编辑应用、游戏、艺术、虚拟现实等。例如，在2019年游戏开发者大会上，Google推出了Stadia，可以实时改变游戏的艺术风格。其实时演示视频可在本食谱最后的*另见*部分查看。
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: '*A Neural Algorithm of Artistic Style* by Gatys, Ecker, Bethge. 2015: [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*艺术风格的神经算法*，由Gatys、Ecker、Bethge著，2015年：[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)'
- en: A well-recommended video of a presentation by Leon Gatys at CVPR 2016 (*Computer
    Vision and Pattern Recognition*) can be viewed here: [https://www.youtube.com/watch?v=UFffxcCQMPQ](https://www.youtube.com/watch?v=UFffxcCQMPQ)
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leon Gatys在2016年CVPR（*计算机视觉与模式识别*）上的演讲推荐视频可以在这里查看：[https://www.youtube.com/watch?v=UFffxcCQMPQ](https://www.youtube.com/watch?v=UFffxcCQMPQ)
- en: To view the original TensorFlow code for the neural style transfer process,
    please see [https://www.tensorflow.org/tutorials/generative/style_transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看神经风格迁移过程的原始TensorFlow代码，请参见[https://www.tensorflow.org/tutorials/generative/style_transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)
- en: To go deeper inside the theory, please see [https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f](https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f)
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 若要深入了解理论，请参见[https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f](https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f)
- en: 'Google Stadia – Style Transfer ML: [https://stadiasource.com/article/2/Stadia-Introducing-Style-Transfer-ML-GDC2019](https://stadiasource.com/article/2/Stadia-Introducing-Style-Transfer-ML-GDC2019)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Stadia – 风格迁移ML：[https://stadiasource.com/article/2/Stadia-Introducing-Style-Transfer-ML-GDC2019](https://stadiasource.com/article/2/Stadia-Introducing-Style-Transfer-ML-GDC2019)
- en: Implementing DeepDream
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DeepDream
- en: Another use for trained CNNs is exploiting the fact that some intermediate nodes
    detect features of labels (for instance, a cat's ear, or a bird's feather). Using
    this fact, we can find ways to transform any image to reflect those node features
    for any node we choose. This recipe is an adapted version of the official TensorFlow
    DeepDream tutorial (refer to the first bullet point in the next *See also* section).
    Feel free to visit the Google AI blog post written by DeepDream's creator, named
    Alexander Mordvintsev (second bullet point in the next *See also* section). The
    hope is that we can prepare you to use the DeepDream algorithm to explore CNNs,
    and features created in them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的 CNN 还有一个用途，那就是利用一些中间节点检测标签特征（例如，猫的耳朵或鸟的羽毛）。利用这一点，我们可以找到将任何图像转化为反映这些节点特征的方式，适用于我们选择的任何节点。这个教程是官方
    TensorFlow DeepDream 教程的改编版本（参见下文 *另见* 部分的第一个项目）。欢迎访问 DeepDream 的创造者亚历山大·莫尔维茨夫（Alexander
    Mordvintsev）在 Google AI 博客上写的文章（下文 *另见* 部分的第二个项目）。希望通过这个教程，能够帮助你使用 DeepDream 算法探索
    CNN 和其中创建的特征。
- en: Getting ready
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Originally, this technique was invented to better understand how a CNN sees.
    The goal of DeepDream is to over-interpret the patterns that the model detects
    and generate inspiring visual content with surreal patterns. This algorithm is
    a new kind of psychedelic art.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这项技术是为了更好地理解 CNN 如何“看”图像而发明的。DeepDream 的目标是过度解读模型检测到的模式，并生成具有超现实模式的激发性视觉内容。这种算法是一种新型的迷幻艺术。
- en: How to do it...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行操作…
- en: 'Perform the following steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'To get started with DeepDream, we''ll start by loading the necessary libraries:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始使用 DeepDream，我们首先需要加载必要的库：
- en: '[PRE49]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We''ll prepare the image to dreamify. We''ll read the original image, reshape
    it to 500 maximum dimensions, and display it:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将准备图像进行梦幻化处理。我们将读取原始图像，将其重塑为最大 500 尺寸，并显示出来：
- en: '[PRE50]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We''ll load the Inception model pre-trained on ImageNet without the classification
    head. We will use the `tf.keras.applications` API:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载在 ImageNet 上预训练的 Inception 模型，并去除分类头。我们将使用 `tf.keras.applications` API：
- en: '[PRE51]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We summarize the model. We can note that the Inception model is quite large:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们总结了这个模型。我们可以注意到，Inception 模型相当庞大：
- en: '[PRE52]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will select the convolutional layers to use for DeepDream processing
    later. In a CNN, the earlier layers extract basic features such as edges, shapes,
    textures, and so on, while the deeper layers extract high-level features such
    as clouds, trees, or birds. To create a DeepDream image, we will focus on the
    layers where the convolutions are mixed. Now, we''ll create the feature extraction
    model with the two mixed layers as outputs:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将选择用于后续 DeepDream 处理的卷积层。在 CNN 中，较早的层提取基本特征，如边缘、形状、纹理等，而较深的层则提取高级特征，如云、树木或鸟类。为了创建
    DeepDream 图像，我们将专注于卷积层混合的地方。现在，我们将创建一个以这两个混合层为输出的特征提取模型：
- en: '[PRE53]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we will define the loss function that returns the sum of all output layers:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将定义损失函数，返回所有输出层的总和：
- en: '[PRE54]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We declare two utility functions that undo the scaling and display a processed
    image:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们声明了两个实用函数，用于撤销缩放并显示处理后的图像：
- en: '[PRE55]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We''ll now apply the gradient ascent process. In DeepDream, we don''t minimize
    the loss using gradient descent, but we maximize the activation of these layers
    by maximizing their loss via gradient ascent. So, we''ll over-interpret the patterns
    that the model detects, and we''ll generate inspiring visual content with surreal
    patterns:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将应用梯度上升过程。在 DeepDream 中，我们不是通过梯度下降最小化损失，而是通过梯度上升最大化这些层的激活，通过最大化它们的损失。这样，我们会过度解读模型检测到的模式，并生成具有超现实模式的激发性视觉内容：
- en: '[PRE56]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Then, we will run DeepDream on the original image:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将在原始图像上运行 DeepDream：
- en: '[PRE57]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output is as follows:'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/DFE47502.tmp](img/B16254_08_11.png)'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/DFE47502.tmp](img/B16254_08_11.png)'
- en: 'Figure 8.11: DeepDream applied to the original image'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.11：DeepDream 应用于原始图像
- en: While the result is good, it could be better! We notice that the image output
    is noisy; the patterns seem to be applied at the same granularity and the output
    is in low resolution.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然结果不错，但还是可以做得更好！我们注意到图像输出很杂乱，模式似乎以相同的粒度应用，并且输出分辨率较低。
- en: To make images better, we can use the concept of octaves. We perform gradient
    ascent on the same image resized multiple times (each step of increasing the size
    of an image is an octave improvement). Using this process, the detected features
    at a smaller scale could be applied to patterns at higher scales with more details.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了改善图像，我们可以使用“八度”概念。我们对同一张图像进行梯度上升，并多次调整图像大小（每次增大图像的大小就是一次八度的改进）。通过这个过程，较小尺度上检测到的特征可以应用到更高尺度上，展现出更多细节的图案。
- en: '[PRE58]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE675C60.tmp](img/B16254_08_12.png)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE675C60.tmp](img/B16254_08_12.png)'
- en: 'Figure 8.12: DeepDream with the concept of octaves applied to the original
    image'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：应用八度概念后的原始图像 DeepDream
- en: 'By using the concept of octaves, things get rather interesting: the output
    is less noisy and the network amplifies the patterns it sees better.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用八度概念，结果变得非常有趣：输出变得不那么嘈杂，网络能更好地放大它所看到的图案。
- en: There's more...
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We urge the reader to use the official DeepDream tutorials as a source of further
    information, and also to visit the original Google research blog post on DeepDream
    (refer to the following *See also* section).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议读者使用官方的 DeepDream 教程作为进一步了解的来源，同时也可以访问原始的 Google 研究博客文章，了解 DeepDream（请参阅以下
    *另见* 部分）。
- en: See also
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: The TensorFlow tutorial on DeepDream: [https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 深度梦境教程：[https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)
- en: The original Google research blog post on DeepDream: [https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的 Google 研究博客文章关于 DeepDream：[https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)
