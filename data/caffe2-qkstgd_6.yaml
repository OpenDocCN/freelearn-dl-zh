- en: Deploying Models to Accelerators for Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到加速器上进行推理
- en: 'In [Chapter 3](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml), *Training Networks*,
    we learned how to train deep neural networks using Caffe2\. In this chapter, we
    will focus on inference: deploying a trained model in the field to *infer* results
    on new data. For efficient inference, the trained model is typically optimized
    for the accelerator on which it is deployed. In this chapter, we will focus on
    two popular accelerators: GPUs and CPUs, and the inference engines TensorRT and
    OpenVINO, which can be used to deploy Caffe2 models on them.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml)，*训练网络*中，我们学习了如何使用 Caffe2
    训练深度神经网络。在本章中，我们将专注于推理：将训练好的模型部署到现场，对新数据进行*推理*。为了实现高效的推理，训练好的模型通常会针对部署时的加速器进行优化。本章将重点讨论两种常见的加速器：GPU
    和 CPU，以及可以用于在它们上部署 Caffe2 模型的推理引擎 TensorRT 和 OpenVINO。
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Inference engines
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推理引擎
- en: NVIDIA TensorRT
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA TensorRT
- en: Intel OpenVINO
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Intel OpenVINO
- en: Inference engines
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理引擎
- en: Popular DL frameworks, such as TensorFlow, PyTorch, and Caffe, are designed
    primarily for *training* deep neural networks. They focus on offering features
    that are more useful for researchers to experiment easily with different types
    of network structures, training regimens, and techniques to achieve optimum training
    accuracy to solve a particular problem in the real world. After a neural network
    model has been successfully trained, practitioners could continue to use the same
    DL framework for deploying the trained model for inference. However, there are
    more efficient deployment solutions for inference. These are pieces of inference
    software that compile a trained model into a computation engine that is most efficient
    in latency or throughput on the accelerator hardware used for deployment.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的深度学习框架，如 TensorFlow、PyTorch 和 Caffe，主要设计用于*训练*深度神经网络。它们侧重于提供更有利于研究人员轻松实验不同网络结构、训练方案和技术的功能，以实现最佳训练精度，解决实际问题。在神经网络模型成功训练后，实践者可以继续使用相同的深度学习框架将训练好的模型部署用于推理。然而，也有更高效的推理部署解决方案。这些推理软件将训练好的模型编译成最适合所用加速器硬件的计算引擎，能够在延迟或吞吐量方面达到最佳性能。
- en: Much like a C or C++ compiler, inference engines take the trained model as input
    and apply several optimization techniques on the graph structure, layers, weights,
    and formats of the trained neural network. For example, they might remove layers
    that are only useful in training. The engine might fuse multiple horizontally
    adjacent layers, or vertically adjacent layers, together for faster computation
    and a lower number of memory accesses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与 C 或 C++ 编译器类似，推理引擎将训练好的模型作为输入，并对训练神经网络的图结构、层次、权重和格式应用多种优化技术。例如，它们可能会去除仅在训练过程中有用的层。引擎可能会将多个水平相邻的层或垂直相邻的层合并，以加快计算并减少内存访问次数。
- en: While training is typically performed in FP32 (4 bytes floating point), inference
    engines might offer inference in lower-precision data types such as FP16 (2 bytes
    floating point) and INT8 (1 byte integer). To achieve this, these engines might
    convert the weight parameters of the model to lower precision and might use quantization.
    Using these lower-precision data types typically speeds up inference by a large
    factor, while degrading the accuracy of your trained networks by a negligible
    amount.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练通常在 FP32（4 字节浮点数）下进行，但推理引擎可能会提供更低精度的数据类型进行推理，例如 FP16（2 字节浮点数）和 INT8（1 字节整数）。为了实现这一点，这些引擎可能会将模型的权重参数转换为较低精度，并可能使用量化技术。使用这些低精度数据类型通常能大幅加速推理，同时对训练网络的精度影响微乎其微。
- en: The inference engines and libraries available right now typically focus on optimizing
    the trained model for a particular type of accelerator hardware. For example,
    the NVIDIA TensorRT inference engine (not to be confused with the Google TensorFlow
    DL framework) focuses on optimizing your trained neural network for inference
    on NVIDIA graphics cards and embedded devices. Similarly, the Intel OpenVINO inference
    engine focuses on optimizing trained networks for Intel CPUs and accelerators.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可用的推理引擎和库通常侧重于优化训练好的模型，以适应特定类型的加速器硬件。例如，NVIDIA TensorRT 推理引擎（不要与 Google TensorFlow
    深度学习框架混淆）专注于优化在 NVIDIA 显卡和嵌入式设备上进行推理的训练神经网络。同样，Intel OpenVINO 推理引擎专注于优化训练好的网络，以适应
    Intel CPU 和加速器。
- en: In the rest of the chapter, we will look at how to deploy Caffe2 models for
    inference on GPUs and CPUs, by using TensorRT and OpenVINO as the inference engines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将探讨如何使用TensorRT和OpenVINO作为推理引擎，在GPU和CPU上部署Caffe2模型进行推理。
- en: NVIDIA TensorRT
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NVIDIA TensorRT
- en: TensorRT is the most popular inference engine for deploying trained models on
    NVIDIA GPUs for inference. Not surprisingly, this library and its set of tools
    are developed by NVIDIA and it is available free for download and use. A new version
    of TensorRT typically accompanies the release of every new NVIDIA GPU architecture,
    adding optimizations for the new GPU architecture and also support for new types
    of layers, operators, and DL frameworks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT是部署已训练模型到NVIDIA GPU上进行推理的最流行推理引擎。毫不奇怪，这个库及其工具集由NVIDIA开发，并且可以免费下载安装。每个新的TensorRT版本通常会与每个新的NVIDIA
    GPU架构发布同步，添加对新GPU架构的优化，并且支持新的层、运算符和深度学习框架。
- en: Installing TensorRT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装TensorRT
- en: TensorRT installers can be downloaded from the web at [https://developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt).
    Installation packages are available for x86-64 (Intel or AMD 64-bit CPU) computers,
    PowerPC computers, embedded hardware such as NVIDIA TX1/TX2, and NVIDIA Xavier
    systems used in automobiles. Operating systems supported include Linux, Windows,
    and QNX (a realtime OS used in automobiles).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT安装程序可以从[https://developer.nvidia.com/tensorrt](https://developer.nvidia.com/tensorrt)网站下载。安装包可用于x86-64（Intel或AMD
    64位CPU）计算机、PowerPC计算机、嵌入式硬件（如NVIDIA TX1/TX2）以及用于汽车的NVIDIA Xavier系统。支持的操作系统包括Linux、Windows和QNX（用于汽车的实时操作系统）。
- en: 'For Linux, multiple LTS versions of Ubuntu are supported, for example, 14.04,
    16.04, and 18.04\. Other Linux distributions, such as CentOS/Red Hat are also
    supported. Every TensorRT package is built for a particular version of CUDA, such
    as 9.0 or 10.0, for example. A typical installer''s download web page is shown
    in Figure 6.1*,* as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux，支持多个LTS版本的Ubuntu，例如14.04、16.04和18.04。其他Linux发行版，如CentOS/Red Hat，也得到支持。每个TensorRT包都是为特定版本的CUDA构建的，例如9.0或10.0。典型的安装程序下载网页如图6.1所示：
- en: '![](img/00f555a9-fbae-40df-aecf-761f48f3e8db.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00f555a9-fbae-40df-aecf-761f48f3e8db.png)'
- en: 'Figure 6.1: Installer''s web page for TensorRT version 5.0\. Notice the Installation
    Guide, packages for Ubuntu, Red Hat, Windows, and also Jetpack for embedded systems
    and DRIVE for automobiles'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：TensorRT版本5.0的安装程序网页。注意查看安装指南、Ubuntu、Red Hat、Windows的安装包，以及用于嵌入式系统的Jetpack和用于汽车的DRIVE。
- en: You will need to download the TensorRT installer that matches your hardware,
    operating system, and installed CUDA version. For example, on my x86-64 notebook
    running Ubuntu 18.04, I have CUDA 10.0 installed. So, I will download the installer
    that matches this setup.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要下载与硬件、操作系统和已安装CUDA版本匹配的TensorRT安装程序。例如，在我的x86-64笔记本电脑上运行Ubuntu 18.04，已安装CUDA
    10.0。所以，我将下载与该配置匹配的安装程序。
- en: 'Once you have downloaded the installer package, follow the instructions provided
    in the TensorRT Installation Guide document to install it. You can find this guide
    as a PDF document on the installer page (see Figure 6.1). Installing typically
    entails using `sudo dpkg -i` for a package on Ubuntu, or using `yum` on Red Hat.
    If you downloaded a `.tar.gz` archive, then you can extract it to a location of
    your choice. No matter how you install it, the TensorRT package includes these
    components: C++ header files, C++ shared library files, C++ samples, Python library,
    and Python samples.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完安装包后，请按照TensorRT安装指南文档中的说明进行安装。你可以在安装页面找到该指南的PDF文档（见图6.1）。安装通常需要使用Ubuntu上的`sudo
    dpkg -i`命令，或在Red Hat上使用`yum`命令。如果你下载了`.tar.gz`压缩包，可以将其解压到你选择的位置。不论采用哪种安装方式，TensorRT包包含以下组件：C++头文件、C++共享库文件、C++示例、Python库以及Python示例。
- en: Using TensorRT
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorRT
- en: 'Using TensorRT for inference typically involves the following three stages:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorRT进行推理通常涉及以下三个阶段：
- en: Importing a pre-trained network or creating a network
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入预训练网络或创建网络
- en: Building an optimized engine from the network
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网络构建优化后的引擎
- en: Inference using execution context of an engine
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用引擎执行上下文进行推理
- en: We will examine these three stages in detail in the following sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中详细介绍这三个阶段。
- en: Importing a pre-trained network or creating a network
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入预训练网络或创建网络
- en: Models are trained in DL frameworks, such as Caffe2, Caffe, PyTorch, or TensorFlow.
    Some practitioners might use their own custom frameworks to train models. The
    first step is to build a network structure inside TensorRT and load the pre-trained
    weights from these DL framework models into the layers of the TensorRT network.
    This process is described in Figure 6.2, as follows*:*
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是在深度学习框架中训练的，例如 Caffe2、Caffe、PyTorch 或 TensorFlow。一些实践者可能使用自己定制的框架来训练模型。第一步是在
    TensorRT 中构建一个网络结构，并将这些深度学习框架模型中预训练的权重加载到 TensorRT 网络的各层中。此过程如图 6.2 所示，具体步骤如下*：*
- en: '![](img/291264db-f419-4ec6-a8d3-52dc0b007a4b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/291264db-f419-4ec6-a8d3-52dc0b007a4b.png)'
- en: 'Figure 6.2: How a network can be built in TensorRT'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：如何在 TensorRT 中构建网络
- en: If you trained a model using popular DL frameworks, then TensorRT provides **parsers**
    to parse your pre-trained model files and build a network from it. TensorRT provides
    an ONNX parser named `IONNXConfig` that can be used to load and import your Caffe2
    pre-trained model file that has been converted to ONNX. You can find information
    on how to convert a Caffe2 model to ONNX in [Chapter 5](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml),
    *Working with Other Frameworks*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用流行的深度学习框架训练了一个模型，那么 TensorRT 提供了 **parsers**（解析器）来解析您的预训练模型文件并从中构建网络。TensorRT
    提供了一个名为 `IONNXConfig` 的 ONNX 解析器，可用于加载和导入已转换为 ONNX 格式的 Caffe2 预训练模型文件。有关如何将 Caffe2
    模型转换为 ONNX 的信息，请参见 [第 5 章](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml)，*与其他框架一起使用*。
- en: TensorRT provides a Caffe parser named `ICaffeParser` that can be used to load
    and import your Caffe model. Similarly, it also provides a TensorFlow parser named
    `IUffConfig` to load and import your TensorFlow model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 提供了一个名为 `ICaffeParser` 的 Caffe 解析器，可以用来加载和导入您的 Caffe 模型。同样，它还提供了一个名为
    `IUffConfig` 的 TensorFlow 解析器，用于加载和导入您的 TensorFlow 模型。
- en: Not all layers and operators from Caffe2, ONNX, or other frameworks might be
    supported in TensorRT. Also, if you trained a model using your own custom training
    framework then you cannot use these parsers. To cover such scenarios, TensorRT
    provides users with the ability to create a network layer by layer. Custom layers
    that are not supported in TensorRT can be implemented using TensorRT plugins.
    You would typically need to implement an unsupported layer in CUDA for optimum
    performance with TensorRT. Examples of all these use cases are depicted in the
    samples that ship with TensorRT.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有 Caffe2、ONNX 或其他框架中的层和运算符都能在 TensorRT 中得到支持。此外，如果您使用自己定制的训练框架训练了模型，则无法使用这些解析器。为了解决这种情况，TensorRT
    提供了逐层创建网络的能力。TensorRT 不支持的自定义层可以通过 TensorRT 插件实现。通常，您需要在 CUDA 中实现不支持的层，以便与 TensorRT
    达到最佳性能。这些用例的示例可以在与 TensorRT 一起发布的示例代码中找到。
- en: No matter which of the preceding processes you follow, you end up with a TensorRT
    network called `INetworkDefinition`. This can be used in the second stage for
    optimization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您遵循上述哪一种过程，最终都会得到一个名为 `INetworkDefinition` 的 TensorRT 网络。该网络可以在第二阶段进行优化。
- en: Building an optimized engine from the network
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从网络构建优化引擎
- en: 'Once a network and its weights are represented inside TensorRT, we can then
    optimize this network definition. This optimization step is performed by a module
    called the **builder**. The builder should be executed on the same GPU on which
    you plan to perform inference later. Though models are trained using FP32 precision,
    you can request the builder to use lower-precision FP16 or INT8 data types that
    occupy less memory and might have optimized instructions on certain GPUs. This
    is shown in Figure 6.3, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络及其权重在 TensorRT 中表示，我们就可以对该网络定义进行优化。这个优化步骤是由一个名为 **builder** 的模块执行的。builder
    应该在您计划稍后进行推理的相同 GPU 上执行。虽然模型是使用 FP32 精度训练的，但您可以请求 builder 使用较低精度的 FP16 或 INT8
    数据类型，这些数据类型占用更少的内存，并且在某些 GPU 上可能具有优化的指令。此过程如图 6.3 所示，具体步骤如下：
- en: '![](img/ebee6a69-4efd-42ba-98fb-d2a949f7f276.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ebee6a69-4efd-42ba-98fb-d2a949f7f276.png)'
- en: 'Figure 6.3: Build process in TensorRT to produce an engine'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：TensorRT 中的构建过程以生成引擎
- en: The builder tries various optimizations specific to the GPU that you run it
    on. It tries kernels and data formats that are specific to the GPU architecture
    and GPU model that you run it on. It times all of these optimization opportunities
    and picks the optimal candidates. This optimized version of the network that it
    produces is called an **engine**. This engine can be serialized to a file commonly
    known as the **PLAN file**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 构建器会尝试各种针对你所运行 GPU 的特定优化。它会尝试适用于特定 GPU 架构和 GPU 型号的内核和数据格式。它会对所有这些优化机会进行计时，并选择最优的候选项。它所生成的网络优化版本被称为
    **引擎**。这个引擎可以序列化成一个文件，通常称为 **PLAN 文件**。
- en: Inference using execution context of an engine
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用引擎执行上下文进行推理
- en: 'To use an engine for inference in TensorRT, we first need to create a runtime.
    The runtime can be used to load an engine from a PLAN file after deserializing
    it. We can then create one or more execution contexts from the runtime and use
    those for runtime inference. This process is depicted in Figure 6.4,as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 TensorRT 中使用引擎进行推理，我们首先需要创建一个运行时。运行时可以在反序列化后从 PLAN 文件加载引擎。然后，我们可以从运行时创建一个或多个执行上下文，并使用它们进行运行时推理。这个过程如图
    6.4 所示：
- en: '![](img/eac11990-42d1-4c94-828a-972f0ba87be4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eac11990-42d1-4c94-828a-972f0ba87be4.png)'
- en: 'Figure 6.4: Process of inference using an engine in TensorRT'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：使用 TensorRT 引擎进行推理的过程
- en: TensorRT API and usage
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorRT API 和使用方法
- en: TensorRT provides both a C++ API and a Python API for your use. These APIs can
    be used to perform all the three stages depicted in the earlier sections. You
    can look at the samples that are provided along with TensorRT to understand how
    to write your own C++ and Python programs that do this. For example, the `sampleMNISTAPI`
    sample that ships with TensorRT shows how to build a simple network to solve the
    `MNIST` problem (introduced in [Chapter 2](270a3617-74cd-4e64-98f7-eb0c4e3cbcf6.xhtml),
    *Composing Networks*) and load pre-trained weights into each of the layers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 提供了 C++ API 和 Python API，供你使用。这些 API 可用于执行前面各个阶段所示的所有操作。你可以查看 TensorRT
    附带的示例，以了解如何编写自己的 C++ 和 Python 程序来实现这些操作。例如，TensorRT 附带的 `sampleMNISTAPI` 示例演示了如何构建一个简单的网络来解决
    `MNIST` 问题（见 [第 2 章](270a3617-74cd-4e64-98f7-eb0c4e3cbcf6.xhtml)，*构建网络*），并将预训练权重加载到每一层中。
- en: To use the C++ API, you would essentially include the `NvInfer.h`, and related
    header files, and compile your program. When you need to link your program, you
    would need to make sure that the `libnvinfer.so` and other related TensorRT library
    files are in your `LD_LIBRARY_PATH` environment variable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 C++ API，基本上需要包含 `NvInfer.h` 和相关的头文件，并编译你的程序。当你需要链接程序时，确保 `libnvinfer.so`
    和其他相关的 TensorRT 库文件在你的 `LD_LIBRARY_PATH` 环境变量中。
- en: TensorRT ships with a tool named `trtexec` that can be used to experiment with
    an import of a pre-trained model and use it for inference. As an example, we will
    illustrate how to use our AlexNet ONNX model from [Chapter 5](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml),
    *Working with Other Frameworks*, for inference in TensorRT.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT 附带一个名为 `trtexec` 的工具，可以用来实验导入预训练模型并用于推理。例如，我们将展示如何在 TensorRT 中使用来自
    [第 5 章](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml) *与其他框架的合作* 的 AlexNet ONNX
    模型进行推理。
- en: 'First, we need to import our AlexNet ONNX model file (converted from Caffe2
    protobuf file) and build an optimized engine file from it. This can be done using
    `trtexec`, as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入我们的 AlexNet ONNX 模型文件（从 Caffe2 protobuf 文件转换而来），并从中构建一个优化后的引擎文件。这可以通过以下方式使用
    `trtexec` 完成：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `--onnx` option is used to point to the input ONNX file. There are similar
    `--deploy` and `--uff` options available if you are importing Caffe or TensorFlow
    models, respectively. The `--output` option is used to specify the name of the
    final output from the model. There is a similar `--input` option to point out
    an input to the model. Multiple instances of the `--input` and `--output` options
    can be used if the model has multiple inputs or outputs. The `--saveEngine` option
    is used to indicate a file path that the tool will use to serialize the optimized
    engine to. For more information, please try `./trtexec --help`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`--onnx` 选项用于指定输入的 ONNX 文件。如果你要导入 Caffe 或 TensorFlow 模型，分别可以使用类似的 `--deploy`
    和 `--uff` 选项。`--output` 选项用于指定模型的最终输出名称。如果模型有多个输入或输出，可以使用多个 `--input` 和 `--output`
    选项。`--saveEngine` 选项用于指定一个文件路径，工具将利用这个路径来序列化优化后的引擎。有关更多信息，请尝试 `./trtexec --help`。'
- en: 'Next, we can load the saved optimized engine and then use it for inference,
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以加载已保存的优化引擎，然后使用它进行推理，如下所示：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The tool deserializes the PLAN file to an engine, creates a runtime from the
    engine and then creates an execution context from the runtime. It uses this context
    to run batches of random inputs and reports the inference runtime performance
    of this model on the GPU you ran it on. The source code of `trtexec` and all TensorRT
    samples is available in the TensorRT package. This source code is a good instructional
    aid to learning how to incorporate TensorRT into your inference application in
    C++ or Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具将 PLAN 文件反序列化为引擎，从引擎创建运行时，然后从运行时创建执行上下文。它使用此上下文运行一批随机输入，并报告该模型在所运行 GPU 上的推理运行时性能。`trtexec`
    和所有 TensorRT 示例的源代码可在 TensorRT 包中找到。这些源代码是学习如何将 TensorRT 融入 C++ 或 Python 推理应用程序中的一个很好的教学辅助手段。
- en: Intel OpenVINO
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 英特尔 OpenVINO
- en: OpenVINO consists of libraries and tools created by Intel that enable you to
    optimize your trained DL model from any framework and then deploy it using an
    inference engine on Intel hardware. Supported hardware includes Intel CPUs, integrated
    graphics in Intel CPUs, Intel's Movidius Neural Compute Stick, and FPGAs. OpenVINO
    is available for free from Intel.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO 由英特尔创建的库和工具组成，使你能够从任何框架优化训练好的深度学习模型，然后使用推理引擎在英特尔硬件上进行部署。支持的硬件包括英特尔
    CPU、英特尔 CPU 中的集成显卡、英特尔 Movidius 神经计算棒和 FPGA。OpenVINO 可以免费从英特尔获得。
- en: 'OpenVINO includes the following components:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO 包含以下组件：
- en: '**Model optimizer**: A tool that imports trained DL models from other DL frameworks,
    converts them, and then optimizes them. Supported DL frameworks include Caffe,
    TensorFlow, MXNet, and ONNX. Note the absence of support for Caffe2 or PyTorch.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型优化器**：一个工具，用于导入来自其他深度学习框架的训练好的深度学习模型，进行转换并优化它们。支持的深度学习框架包括 Caffe、TensorFlow、MXNet
    和 ONNX。请注意，不支持 Caffe2 或 PyTorch。'
- en: '**Inference engine**: These are libraries that load the optimized model produced
    by the model optimizer and provide your applications with the ability to run the
    model on Intel hardware.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理引擎**：这些是加载模型优化器生成的优化模型的库，并为你的应用程序提供在英特尔硬件上运行模型的能力。'
- en: '**Demos and samples**: These simple applications demonstrate the use of OpenVINO
    and help you integrate it into your application.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演示和示例**：这些简单的应用程序展示了 OpenVINO 的使用，并帮助你将其集成到你的应用程序中。'
- en: OpenVINO is meant for inference; it provides no features to research new network
    structures or train neural networks. Using OpenVINO is a big topic by itself.
    In this book, we will focus on how to install it, test it, and use Caffe2 models
    with it for inference.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO 旨在进行推理；它不提供研究新网络结构或训练神经网络的功能。使用 OpenVINO 是一个独立的大话题。在本书中，我们将重点介绍如何安装它、如何测试它，以及如何使用
    Caffe2 模型进行推理。
- en: Installing OpenVINO
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 OpenVINO
- en: In this section, we will look at the steps to install and test OpenVINO on Ubuntu.
    The steps to install and test on other Linux distributions, such as CentOS, and
    other operating systems, such as Windows, is similar. For guidance on all of these,
    please refer to the *OpenVINO Installation Guide* suitable for your operating
    system. It is available online and in the installer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍在 Ubuntu 上安装和测试 OpenVINO 的步骤。其他 Linux 发行版（如 CentOS）和其他操作系统（如 Windows）上的安装和测试步骤也类似。有关所有这些的指导，请参考适用于你操作系统的
    *OpenVINO 安装指南*。该指南可在线查看并在安装程序中找到。
- en: 'Installation files of OpenVINO for your operating system or Linux distribution
    can be downloaded from [https://software.intel.com/en-us/openvino-toolkit](https://software.intel.com/en-us/openvino-toolkit).
    For example, for Ubuntu it gives me the option of downloading a Customizable Package
    or a single large Full Package. Figure 6.5 shows these options, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于你的操作系统或 Linux 发行版的 OpenVINO 安装文件可以从 [https://software.intel.com/en-us/openvino-toolkit](https://software.intel.com/en-us/openvino-toolkit)
    下载。例如，对于 Ubuntu，它给我提供了下载可自定义包或单个大文件包的选项。图 6.5 展示了这些选项，如下所示：
- en: '![](img/25f084bd-3ebb-494c-ad6d-294f26a5edab.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25f084bd-3ebb-494c-ad6d-294f26a5edab.png)'
- en: 'Figure 6.5: OpenVINO installer options for download on Ubuntu'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：Ubuntu 上 OpenVINO 安装程序的下载选项
- en: 'The downloaded file typically has a filename of the form `l_openvino_toolkit_p_<version>.tgz`.
    Uncompress the contents of this file to a directory and change to that directory.
    Here you will find installer shell scripts available in two formats: console or
    GUI. Either of these can be executed as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的文件通常具有 `l_openvino_toolkit_p_<version>.tgz` 这样的文件名。解压缩此文件的内容到一个目录并进入该目录。您会在此找到两种格式的安装脚本：控制台和
    GUI。可以按以下方式执行其中任何一个：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Both of these options provide a helpful wizard to enable you to choose where
    you want to install OpenVINO files and what components of OpenVINO you would like
    to install. If you run the scripts without `sudo`, they will provide you with
    an option to install to an `intel` subdirectory inside your home directory. Running
    with `sudo` helps you install to `/opt/intel`, which is where most Intel tools
    traditionally get installed.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种选项都提供了一个有用的向导，帮助您选择安装 OpenVINO 文件的位置以及您希望安装的 OpenVINO 组件。如果在没有使用 `sudo` 的情况下运行脚本，它们会提供一个选项，将文件安装到您主目录下的
    `intel` 子目录中。使用 `sudo` 运行则会帮助您将文件安装到 `/opt/intel`，这是大多数英特尔工具传统上安装的位置。
- en: 'Figure 6.6 shows the OpenVINO components that can be chosen during installation.
    At a minimum, I recommend installing the Model Optimizer, Inference Engine, and
    OpenCV. OpenCV will be needed if you want to read images and feed them to the
    inference engine. Figure 6.6 is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 显示了安装过程中可以选择的 OpenVINO 组件。至少，我建议安装模型优化器、推理引擎和 OpenCV。如果您想读取图像并将其传递给推理引擎，则需要安装
    OpenCV。图 6.6 如下所示：
- en: '![](img/42b61e6a-dd30-4801-8031-d00490b83211.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42b61e6a-dd30-4801-8031-d00490b83211.png)'
- en: 'Figure 6.6: OpenVINO components that can be installed using the GUI installer
    wizard'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：可以通过 GUI 安装程序向导安装的 OpenVINO 组件
- en: 'After the main installation, we also need to install some external dependencies
    of OpenVINO by completing the following these commands:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完成主安装后，我们还需要通过执行以下命令来安装 OpenVINO 的一些外部依赖项：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you did not install using `sudo`, you can replace `/opt/intel` in the preceding
    command with the path where you installed OpenVINO in your home directory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有使用 `sudo` 安装，您可以将前面的命令中的 `/opt/intel` 替换为您在主目录中安装 OpenVINO 的路径。
- en: 'Now we are ready to set up the environment variables needed for OpenVINO. We
    can do this by using the following command:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备设置 OpenVINO 所需的环境变量。我们可以通过以下命令来完成此操作：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We next configure OpenVINO to support the DL frameworks whose models we want
    to import. We can pull in the configurations for all supported DL frameworks by
    using the following command:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们配置 OpenVINO 以支持我们要导入的深度学习框架模型。我们可以通过以下命令拉取所有受支持的深度学习框架的配置：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We are now ready to test if our OpenVINO installation is working. We can do
    this by running an OpenVINO demo that downloads a *SqueezeNet* model trained using
    Caffe, optimizes it using the Model Optimizer, and runs it using the Inference
    Engine on an image of a car, as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备测试我们的 OpenVINO 安装是否正常工作。我们可以通过运行一个 OpenVINO 演示来完成这一操作，该演示会下载一个使用 Caffe
    训练的 *SqueezeNet* 模型，使用模型优化器优化它，并通过推理引擎在一张汽车图像上运行，具体操作如下：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'On running this, we should be able to see a classification result for the car
    image. The class with the highest probability score is a sports car, thus confirming
    that the model inference using OpenVINO is working. This is shown in Figure 6.7,
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，我们应该能够看到汽车图像的分类结果。概率最高的类别是运动型汽车，从而确认使用 OpenVINO 进行模型推理是正常的。这一点在图 6.7 中有显示，如下所示：
- en: '![](img/4286ef37-f542-4955-8eca-a9f447fd4298.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4286ef37-f542-4955-8eca-a9f447fd4298.png)'
- en: 'Figure 6.7: OpenVINO demo classification results on a sports car image'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：OpenVINO 在一张运动汽车图片上的演示分类结果
- en: Model conversion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型转换
- en: OpenVINO does not support the Caffe2 model format. However, it does support
    the popular ONNX representation for models. So, to use a Caffe2 model with OpenVINO
    we should follow a two-step process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO 不支持 Caffe2 模型格式。然而，它支持流行的 ONNX 模型表示方式。因此，要在 OpenVINO 中使用 Caffe2 模型，我们需要遵循一个两步流程。
- en: First, we need to convert our Caffe2 model to the ONNX format. This process
    is described in detail in [Chapter 5](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml),
    *Working with Other Frameworks*. After that, we can use the ONNX model thus produced
    with the OpenVINO Model Optimizer to import, optimize and convert it to the OpenVINO
    **Intermediate Representation** (**IR**) format.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将 Caffe2 模型转换为 ONNX 格式。此过程在 [第 5 章](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml)，*与其他框架的协作*
    中有详细描述。之后，我们可以使用 OpenVINO 模型优化器来导入、优化并将其转换为 OpenVINO **中间表示**（**IR**）格式。
- en: Let's examine this Model Optimizer process with the AlexNet model that we converted
    to ONNX in [Chapter 5](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml),*Working with
    Other Frameworks*. We had converted the AlexNet Caffe2 model to produce an `alexnet.onnx`
    file.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看使用模型优化器处理 AlexNet 模型的过程，该模型我们在[第 5 章](4481e225-7882-4625-9d42-63ba41e74b4f.xhtml)中已转换为
    ONNX，*与其他框架的协作*。我们已将 AlexNet Caffe2 模型转换为生成`alexnet.onnx`文件。
- en: 'To convert this AlexNet ONNX model to the OpenVINO IR using Model Optimizer,
    we can use the `mo.py` script as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用模型优化器将此 AlexNet ONNX 模型转换为 OpenVINO IR，我们可以使用以下`mo.py`脚本：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This conversion process produces three files:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换过程会生成三个文件：
- en: '`.bin` file: This file contains the weights of the model. This is the reason
    why this is typically a large file.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.bin` 文件：该文件包含模型的权重。这也是为什么这个文件通常较大的原因。'
- en: '`.xml` file: This is an XML file containing the network structure. Details
    stored inside this file include metadata about the model, the list of layers,
    configuration parameters of each layer, and the list of edges between the layers.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.xml` 文件：这是一个包含网络结构的 XML 文件。此文件中存储的详细信息包括关于模型的元数据、层列表、每个层的配置参数，以及层之间的边缘列表。'
- en: '`.mapping` file: This is an XML file that has the mapping from the input file
    layers to the output OpenVINO file layers.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.mapping` 文件：这是一个 XML 文件，包含从输入文件层到输出 OpenVINO 文件层的映射。'
- en: We only need the `.bin` file and the `.xml` file to use the model with the OpenVINO
    Inference Engine.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要`.bin`文件和`.xml`文件即可使用 OpenVINO 推理引擎进行模型推理。
- en: Model inference
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型推理
- en: OpenVINO provides an Inference Engine API in both C++ and Python. This API can
    be used to create network structures programmatically for your trained Caffe2
    models. You can then load the weights of each network layer into the OpenVINO
    network and use that for inference on Intel hardware. If OpenVINO does not currently
    support the type of network layer or operator that your trained model is using,
    then you will need to implement that using a plugin layer in OpenVINO. All this
    effort is worth it because you will benefit from gains in latency and throughput
    for your Caffe2 trained models once they are running using the OpenVINO Inference
    Engine.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO 提供了 C++ 和 Python 版本的推理引擎 API。这个 API 可以用于为训练好的 Caffe2 模型程序化创建网络结构。然后，您可以将每个网络层的权重加载到
    OpenVINO 网络中，并在 Intel 硬件上进行推理。如果 OpenVINO 当前不支持您的训练模型所使用的网络层或操作符类型，那么您需要使用 OpenVINO
    插件层来实现这一点。所有这些努力都是值得的，因为一旦 Caffe2 训练的模型在 OpenVINO 推理引擎上运行后，您将受益于延迟和吞吐量的提升。
- en: 'For most networks, there is an easier alternative: convert your Caffe2 trained
    model to OpenVINO IR using the OpenVINO Model Optimizer. We looked at how to do
    this in the previous section. After this step, use the features in OpenVINO Inference
    Engine to import this IR model automatically for inference. OpenVINO provides
    many Inference Engine samples that can be used to try this process out.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数网络，有一个更简单的替代方法：使用 OpenVINO 模型优化器将 Caffe2 训练的模型转换为 OpenVINO IR。我们在前一节中已经介绍了如何执行此操作。完成此步骤后，可以使用
    OpenVINO 推理引擎中的功能自动导入此 IR 模型进行推理。OpenVINO 提供了许多推理引擎示例，可以用来尝试此过程。
- en: Remember to run the `/opt/intel/openvino/bin/setupvars.sh` script before you
    do this. This script sets up the necessary environment variables and settings
    for OpenVINO use.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在执行此操作前运行`/opt/intel/openvino/bin/setupvars.sh`脚本。此脚本设置了 OpenVINO 使用所需的环境变量和配置。
- en: Go to the Inference Engine `samples` directory and examine the various samples.
    There are samples to suit many common use cases. For example, there are samples
    to test classification models, to test object detection models, to test text detection,
    to test the latency and throughput performance, and more.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 转到推理引擎的`samples`目录，查看各种样本。这里有适用于许多常见用例的样本。例如，包含用于测试分类模型、测试物体检测模型、测试文本检测、测试延迟和吞吐量性能等样本。
- en: 'To build all the Inference Engine samples, follow these steps:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建所有的推理引擎样本，请按照以下步骤进行：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Inference Engine samples will be built using CMake. The sample binary files
    are installed into a directory called `inference_engine_samples_build/intel64/Release`
    under your home directory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 推理引擎样本将使用 CMake 构建。样本的二进制文件将安装到你主目录下名为`inference_engine_samples_build/intel64/Release`的目录中。
- en: These samples make it very convenient to quickly try the OpenVINO Inference
    Engine on an IR model. These samples may use some extra libraries that are installed
    along with OpenVINO. So, if you find that a sample needs a library (`.so` file)
    that is missing, you may need to add the path to that library to the `LD_LIBRARY_PATH`
    environment variable.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例使得快速尝试在IR模型上使用OpenVINO推理引擎变得非常方便。这些示例可能使用一些与OpenVINO一起安装的额外库。因此，如果你发现某个示例缺少某个库（`.so`文件），你可能需要将该库的路径添加到`LD_LIBRARY_PATH`环境变量中。
- en: 'I found that using the following `LD_LIBRARY_PATH` worked for me:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现使用以下`LD_LIBRARY_PATH`对我有效：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'One of the simplest samples to try is `hello_classification`. This sample takes
    two inputs: a path to an OpenVINO IR classification model and a path to an image
    file. It creates an OpenVINO Inference Engine by importing the IR model and running
    inference on it using the image.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的示例之一是`hello_classification`。该示例接受两个输入：一个OpenVINO IR分类模型的路径和一个图像文件的路径。它通过导入IR模型并使用图像运行推理，创建了一个OpenVINO推理引擎。
- en: 'To try the OpenVINO Inference Engine on the IR model we created earlier from
    our AlexNet Caffe2 model, use the following command:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们之前从AlexNet Caffe2模型创建的IR模型上尝试OpenVINO推理引擎，请使用以下命令：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that the `hello_classification` sample successfully loaded the IR
    model into its inference engine and ran classification on the input sunflower
    image. It reported the ImageNet class 985 (daisy) as the highest score, which
    is the closest matching class for sunflower among the 1,000 ImageNet classes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，`hello_classification`示例成功地将IR模型加载到其推理引擎中，并对输入的向日葵图像进行了分类。它报告了ImageNet类985（雏菊）作为最高分，这是在1,000个ImageNet类中与向日葵最匹配的类别。
- en: OpenVINO Inference Engine can be used to perform inference in FP16 and also
    INT8 modes. Please refer to the OpenVINO documentation for details.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: OpenVINO推理引擎可以用于执行FP16和INT8模式的推理。有关详细信息，请参阅OpenVINO文档。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned about inference engines and how they are an essential
    tool for the final deployment of a trained Caffe2 model on accelerators. We focused
    on two types of popular accelerators: NVIDIA GPUs and Intel CPUs. We looked at
    how to install and use TensorRT for deploying our Caffe2 model on NVIDIA GPUs.
    We also looked at the installation and use of OpenVINO for deploying our Caffe2
    model on Intel CPUs and accelerators.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了推理引擎及其作为加速器上部署训练好的Caffe2模型的重要工具。我们重点讨论了两种流行的加速器：NVIDIA GPU和Intel CPU。我们学习了如何安装和使用TensorRT来将Caffe2模型部署到NVIDIA
    GPU上。我们还了解了如何安装和使用OpenVINO，将Caffe2模型部署到Intel CPU和加速器上。
- en: Many other companies, such as Google, Facebook, Amazon, and start-ups such as
    Habana and GraphCore, are developing new accelerator hardware for the inference
    of DL models. There are also efforts such as ONNX Runtime that are bringing together
    the inference engines from multiple vendors under one umbrella. Please evaluate
    these options and choose which accelerator hardware and software works best for
    deployment of your Caffe2 model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他公司，如谷歌、Facebook、亚马逊以及像Habana和GraphCore这样的初创公司，正在开发用于推理DL模型的新型加速器硬件。还有像ONNX
    Runtime这样的努力，旨在将来自多个供应商的推理引擎汇聚到一个平台下。请评估这些选项，并选择最适合部署Caffe2模型的加速器硬件和软件。
- en: In the next chapter, we will take a look at Caffe2 at the edge on Raspberry
    Pi, Caffe2 in the cloud using containers, and Caffe2 model visualization.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解在树莓派上使用Caffe2进行边缘计算、在云端使用容器的Caffe2以及Caffe2模型可视化。
