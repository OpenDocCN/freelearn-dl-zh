- en: Tuning and Optimizing Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整和优化模型
- en: In the last two chapters, we trained deep learning models for classification,
    regression, and image recognition tasks. In this chapter, we will discuss some
    important issues in regard to managing deep learning projects. While this chapter
    may seem somewhat theoretical, if any of the issues discussed are not correctly
    managed, it can derail your deep learning project. We will look at how to choose
    evaluation metrics and how to create an estimate of how well a deep learning model
    will perform before you begin modeling. Next, we will move onto data distribution and
    the mistakes often made in splitting data into correct partitions for training. Many
    machine learning projects fail in production use because the data distribution
    is different to what the model was trained with. We will look at data augmentation,
    a valuable method to enhance your model's accuracy. Finally, we will discuss hyperparameters
    and learn how to tune them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们训练了用于分类、回归和图像识别任务的深度学习模型。在本章中，我们将讨论管理深度学习项目的一些重要问题。虽然本章可能显得有些理论性，但如果没有正确管理讨论的任何问题，可能会导致深度学习项目的失败。我们将探讨如何选择评估指标，以及如何在开始建模之前评估深度学习模型的性能。接下来，我们将讨论数据分布及在将数据划分为训练集时常见的错误。许多机器学习项目在生产环境中失败，原因是数据分布与模型训练时的数据分布不同。我们将讨论数据增强，这是提升模型准确性的一个重要方法。最后，我们将讨论超参数，并学习如何调整它们。
- en: 'In this chapter, we will be looking at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Evaluation metrics and evaluating performance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标与性能评估
- en: Data preparation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data pre-processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Data augmentation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: Tuning hyperparameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: Use case—interpretability
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例——可解释性
- en: Evaluation metrics and evaluating performance
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标与性能评估
- en: This section will discuss how to set up a deep learning project and what evaluation
    metrics to select. We will look at how to select evaluation criteria and how to
    decide when the model is approaching optimal performance. We will also discuss
    how all deep learning models tend to overfit and how to manage the bias/variance
    tradeoff. This will give guidelines on what to do when models have low accuracy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论如何设置深度学习项目以及如何选择评估指标。我们将探讨如何选择评估标准，并如何判断模型是否接近最佳性能。我们还将讨论所有深度学习模型通常会出现过拟合问题，以及如何管理偏差/方差的权衡。此部分将为在模型准确率较低时应采取的措施提供指导。
- en: Types of evaluation metric
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标的类型
- en: Different evaluation metrics are used for categorization and regression tasks.
    For categorization, accuracy is the most commonly used evaluation metric. However,
    accuracy is only valid if the cost of errors is the same for all classes, which
    is not always the case. For example, in medical diagnosis, the cost of a false
    negative will be much higher than the cost of a false positive. A false negative
    in this case says that the person is not sick when they are, and a delay in diagnosis
    can have serious, perhaps fatal, consequences. On the other hand, a false positive is
    saying that the person is sick when they are not, which is upsetting for that
    person but is not life threatening.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的评估指标用于分类和回归任务。在分类任务中，准确率是最常用的评估指标。然而，准确率只有在所有类别的错误成本相同的情况下才有效，但这并非总是如此。例如，在医疗诊断中，假阴性的成本要远高于假阳性的成本。假阴性意味着认为某人没有生病，而实际上他们有病，延误诊断可能会带来严重甚至致命的后果。另一方面，假阳性则是认为某人生病了，而实际上并没有，这虽然让病人感到不安，但不会威胁到生命。
- en: 'This issue is compounded when you have imbalanced datasets, that is, when one
    class is much more common than the other. Going back to our medical diagnosis
    example, if only 1% of people who get tested actually have the disease, then a
    machine learning algorithm can get 99% accuracy by just declaring that nobody
    has the disease. In this case, you can look at other metrics rather than accuracy.
    One such metric that is useful for imbalanced datasets is the F1 evaluation metric,
    which is a weighted average of precision and recall. The formula for the F1 score
    is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集不平衡时，这个问题会变得更加复杂，即某一类别比另一类别更为常见。以我们的医疗诊断示例为例，如果接受测试的人中只有1%的人实际患有疾病，那么机器学习算法仅通过判断没有人患病就能获得99%的准确率。在这种情况下，可以考虑其他的评估指标，而非准确率。对于不平衡的数据集，F1评估指标是一个有用的选择，它是精确率和召回率的加权平均。F1分数的计算公式如下：
- en: '*F1 = 2 * (precision * recall) / (precision + recall)*'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*F1 = 2 * (精确率 * 召回率) / (精确率 + 召回率)*'
- en: 'The formulas for precision and recall are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率的公式如下：
- en: '*precision = true_positives / (true_positives + false_positives)*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*精度 = true_positives / (true_positives + false_positives)*'
- en: '*recall = true_positives / (true_positives + false_negatives)*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率 = true_positives / (true_positives + false_negatives)*'
- en: 'For regression, you have a choice of evaluation metrics: MAE, MSE, and RMSE.
    **MAE**, or **Mean Absolute Error**, is the simplest; it is just the average of
    the absolute difference between the actual value and the predicted value. The
    advantage of MAE is that it is easily understood; if MAE is 3.5, then the difference
    between the predicted value and the actual value is 3.5 on average. **MSE**, or
    **Mean Squared Error**, is the average of the squared error, that is, it takes
    the difference between the actual value and the predicted value, squares it, and
    then takes the average of those values. The advantage of using MSE over MAE is
    that it penalizes errors according to their severity. If the difference between
    the actual value and the predicted value for two rows was 2 and 5, then the MSE
    would put more weight on the second example because the error is larger. **RMSE**,
    or **Root Mean Squared Error**, is the square root of MSE. The advantage of using
    MSE is that it puts the error term back into units that are comparable to the
    actual values. For regression tasks, RMSE is usually the preferred metric.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，您可以选择评估指标：MAE、MSE 和 RMSE。**MAE**，或称为**平均绝对误差**，是最简单的；它只是实际值与预测值之间绝对差的平均值。MAE
    的优点是易于理解；如果 MAE 是 3.5，那么预测值与实际值之间的差异平均为 3.5。**MSE**，或称为**均方误差**，是误差平方的平均值，也就是说，它计算实际值与预测值之间的差异，平方后再求这些值的平均值。使用
    MSE 相较于 MAE 的优点在于，它根据误差的严重程度进行惩罚。如果实际值和预测值之间的差异为 2 和 5，那么 MSE 会对第二个例子赋予更多的权重，因为误差较大。**RMSE**，或称为**均方根误差**，是
    MSE 的平方根。使用 MSE 的优点在于，它将误差项转回与实际值可比较的单位。对于回归任务，RMSE 通常是首选的评估指标。
- en: For more information on metrics in MXNet, see [https://mxnet.incubator.apache.org/api/python/metric/metric.html](https://mxnet.incubator.apache.org/api/python/metric/metric.html).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关 MXNet 中评估指标的更多信息，请参见 [https://mxnet.incubator.apache.org/api/python/metric/metric.html](https://mxnet.incubator.apache.org/api/python/metric/metric.html)。
- en: For more information on metrics in Keras, see [https://keras.io/metrics/](https://keras.io/metrics/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关 Keras 中评估指标的更多信息，请参见 [https://keras.io/metrics/](https://keras.io/metrics/)。
- en: Evaluating performance
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估性能
- en: We have explored a few deep learning models in earlier chapters. We got an accuracy
    rate of 98.36% in our image classification task on the `MNIST` dataset in [Chapter
    5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification Using Convolutional
    Neural Networks.* For the binary classification task (predicting which customers
    will return in the next 14 days) in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),* Training
    Deep Prediction Models*, we got an accuracy rate of 77.88%. But what does this
    actually mean and how do we evaluate the performance of a deep learning model?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中探讨了一些深度学习模型。在 [第 5 章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml) *使用卷积神经网络进行图像分类*
    中，我们在 `MNIST` 数据集上的图像分类任务中获得了 98.36% 的准确率。对于 [第 4 章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)
    *训练深度预测模型* 中的二分类任务（预测哪些客户将在接下来的 14 天内返回），我们获得了 77.88% 的准确率。但这到底意味着什么呢？我们如何评估深度学习模型的性能？
- en: The obvious starting point in evaluating whether your deep learning model has
    good predictive capability is by comparing it to other models. The `MNIST` dataset
    is used in a lot of benchmarks for deep learning research, so we know that there
    are models that achieve 99.5% accuracy. Therefore, our model is OK, but not great.
    In the *D**ata augmentation* section in this chapter, we will improve our model
    significantly, from 98.36% accuracy to 98.95% accuracy, by augmenting our data
    with new images created by making changes to the existing image data. In general,
    for image classification tasks anything less than 95% accuracy probably indicates
    a problem with your deep learning model. Either the model is not designed correctly
    or you do not have enough data for your task.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 评估深度学习模型是否具有良好预测能力的显而易见的起点是与其他模型进行比较。`MNIST` 数据集在许多深度学习研究的基准测试中都有使用，因此我们知道有些模型的准确率可以达到
    99.5%。因此，我们的模型是可以接受的，但并不出色。在本章的 *数据增强* 部分，我们将通过对现有图像数据进行修改来生成新图像，从而显著提高模型的准确性，从
    98.36% 提升到 98.95%。通常，对于图像分类任务，任何低于 95% 的准确率可能意味着您的深度学习模型存在问题。要么模型设计不正确，要么您的任务没有足够的数据。
- en: Our binary classification model only had 77.54% accuracy, which is much less
    than the image classification task. So, is it a terrible model? Not really; it
    is still a useful model. We also have some benchmarks from other machine learning
    models such as random forest and xgboost that we ran on a small section of the
    data. We also saw that we got an increase in accuracy when we moved from a model
    with 3,900 rows to a deeper model with 390,000 rows. This highlights that deep
    learning models improve with more data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的二分类模型只有77.54%的准确率，远低于图像分类任务。那么，这是不是一个糟糕的模型？其实不然；它仍然是一个有用的模型。我们也有来自其他机器学习模型（如随机森林和xgboost）的基准，它们是在数据的小部分上运行的。我们还看到，当我们从一个包含3,900行的数据的模型转到一个更深的包含390,000行的数据的模型时，准确率有所提高。这表明，深度学习模型随着数据量的增加而改进。
- en: 'One step you can do to evaluate your model''s performance is to see if more
    data will increase accuracy significantly. The data can be acquired from more
    training data, or from data augmentation, which we will see later. You can use learning
    curves to evaluate if this will help with performance. To create a learning curve,
    you train a series of machine learning models with increasing sizes, for example,
    10,000 rows to 200,000 rows in steps of 1,000 rows. For each step, run `5` different
    machine learning models to smooth the results and plot average accuracy by the
    sample size. Here is the pseudocode to perform this task:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型性能的一个步骤是查看更多数据是否会显著提高准确率。这些数据可以通过更多的训练数据获取，或者通过数据增强来获得，后者我们将在后续章节中讨论。你可以使用学习曲线来评估这是否有助于性能提升。要创建学习曲线，你需要训练一系列逐步增加数据量的机器学习模型，例如，从10,000行到200,000行，每次增加1,000行。对于每一步，运行`5`个不同的机器学习模型来平滑结果，并根据样本量绘制平均准确率。以下是执行此任务的伪代码：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is an example of a learning curve plot for similar task to the churn problem:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个与客户流失问题类似任务的学习曲线示例：
- en: '![](img/f5c76faa-0ab1-4117-a821-8d943a43ec49.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5c76faa-0ab1-4117-a821-8d943a43ec49.png)'
- en: 'Figure 6.1: An example of a learning curve which plots accuracy by data size'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：一个学习曲线示例，展示了数据量与准确率的关系
- en: 'In this case, accuracy is in a very narrow range and stabilizes as the # instances
    increase. Therefore, for this algorithm and hyperparameter choice, adding more
    data will not increase accuracy significantly.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，准确率处于一个非常狭窄的范围，并且随着实例数量的增加而稳定。因此，对于该算法和超参数选择，增加更多的数据不会显著提高准确率。
- en: If we get a learning curve that is flat like in this example, then adding more
    data to the existing model will not increase accuracy. We could try to improve
    our performance by either changing the model architecture or by adding more features.
    We discussed some options for this in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们得到一个像本例中一样平坦的学习曲线，那么向现有模型中添加更多数据不会提高准确率。我们可以尝试通过更改模型架构或增加更多特征来提高性能。我们在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)，*使用卷积神经网络进行图像分类*中讨论了这一些选项。
- en: 'Going back to our binary classification model, let''s consider how we could
    we use it in production. Recall that this model is trying to predict if customers
    will return in the next *x* days. Here is the confusion matrix from that model
    again:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的二分类模型，我们来考虑如何将它应用于生产环境。回想一下，该模型试图预测客户是否会在接下来的 *x* 天内返回。这里是该模型的混淆矩阵：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we look at how the model performs for each class, we get a different accuracy
    rates:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察模型在每个类别中的表现，会得到不同的准确率：
- en: For `Actual=0`, we get *10714 / (10714 + 4756) = 69.3%* values correct. This
    is called specificity or the true negative rate.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`Actual=0`，我们得到 *10714 / (10714 + 4756) = 69.3%* 的正确值。这被称为特异性或真负率。
- en: For `Actual=1`, we get *19649 / (3466** + 19649) = 85.0%* values correct. This
    is called sensitivity or the true positive rate.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`Actual=1`，我们得到 *19649 / (3466** + 19649) = 85.0%* 的正确值。这被称为敏感性或真正例率。
- en: For this use case, sensitivity is probably more of a concern than specificity.
    If I were a senior manager, I would be more interested in knowing which customers
    were predicted to return but did not. This group could be sent offers to entice
    them back. Here is how a senior manager might use this model, assuming that the
    model is built to predict whether a person comes in from September 1 to September
    14\. On September 15, we get the preceding confusion matrix. How should a manager
    allocate his/her limited marketing budget?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个用例，灵敏度可能比特异性更为重要。如果我是高级经理，我会更关心哪些客户被预测会回归但实际上没有回归。可以向这一群体发送优惠以吸引他们回归。假设该模型是用来预测某人在9月1日至9月14日之间是否会回归，那么9月15日，我们得到前面的混淆矩阵。经理应该如何分配有限的营销预算？
- en: I can see that I got 4,756 customers who were predicted not to return but actually
    did. This is good, but I cannot really act on this. I can attempt to send offers
    to the 10,135 who did not return, but since my model already predicted that they
    would not return, I would expect the response rate to be low.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以看到我得到了4,756个被预测不会回归但实际上回归的客户。这很好，但我不能真正对其采取行动。我可以尝试向10,135个未回归的客户发送优惠，但由于我的模型已经预测他们不会回归，我预计响应率会很低。
- en: The 3,870 customers who were predicted to return but did not are more interesting.
    These people should be sent offers to entice them back before their change in
    behavior becomes permanent. This represents only 9.9% of my customer base, so
    by only sending offers to these customers, I am not diluting my budget by sending
    offers to a large contingent of my customers.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测到会回归但未回归的3,870名客户更为有趣。这些人应该收到优惠以吸引他们在行为变化成为永久之前回归。这仅占我的客户基础的9.9%，因此只向这些客户发送优惠，我不会通过向大量客户发送优惠而浪费预算。
- en: The prediction model should not be used in isolation; other metrics should be
    combined with it to develop a marketing strategy. For example, **customer lifetime
    value** (**CLV**), which measures the expected future revenue for a customer minus
    the cost to re-acquire that customer, could be combined with the prediction model.
    By using a prediction model and CLV together, we can prioritize customers that
    are likely to return by their predicted future value.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 预测模型不应该单独使用；应该将其他指标与之结合，制定营销策略。例如，**客户生命周期价值**（**CLV**），它衡量的是一个客户的预期未来收入减去重新获得该客户的成本，可以与预测模型结合使用。通过结合使用预测模型和CLV，我们可以优先考虑那些根据预测未来价值可能回归的客户。
- en: To summarize this section, it is all too easy to get obsessed with optimizing
    evaluation metrics, especially if you are new to the field. As a data scientist,
    you should always remember that optimizing evaluation metrics on a machine learning
    task is not the ultimate goal—it is just a proxy for improving some part of the
    business. You must be able to link the results of your machine learning model
    back to a business use case. In some cases, for example, digit recognition in
    the `MNIST` dataset, there is a direct link between your evaluation metrics and
    your business case. But sometimes it is not so obvious, and you need help to work
    with the business in finding out how to use the results of your analysis to maximize
    the benefits to the company.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结这一部分，过分沉迷于优化评估指标是很容易的，尤其是当你是该领域的新手时。作为数据科学家，你应该始终记住，优化机器学习任务的评估指标并不是最终目标——它只是改善某一部分业务的代理。你必须能够将机器学习模型的结果与业务用例联系起来。例如，在`MNIST`数据集中的数字识别任务，评估指标与业务用例之间有直接联系。但有时候这种联系并不那么明显，你需要与业务合作，找出如何利用分析结果来最大化公司收益的方法。
- en: Data preparation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: Machine learning is about training a model to generalize on the cases it sees
    so that it can make predictions on unseen data. Therefore, the data used to train
    the deep learning model should be similar to the data that the model sees in production.
    However, at an early product stage, you may have little or no data to train a
    model, so what can you do? For example, a mobile app could include a machine learning
    model that predicts the subject of image taken by the mobile camera. When the
    app is being written, there may not be enough data to train the model using a
    deep learning network. One approach would be to augment the dataset with images
    from other sources to train the deep learning network. However, you need to know
    how to manage this and how to deal with the uncertainty it introduces. Another
    approach is transfer learning, which we will cover in [Chapter 11](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml),
    *The Next Level in Deep Learning*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是训练一个模型，使其能够在见过的案例上进行泛化，以便它能对未见过的数据做出预测。因此，用来训练深度学习模型的数据应该与模型在生产中看到的数据相似。然而，在产品的早期阶段，你可能几乎没有数据来训练模型，那么你该怎么办呢？例如，一个移动应用可能包含一个机器学习模型，用来预测由手机摄像头拍摄的图像的主题。当应用程序编写时，可能没有足够的数据来使用深度学习网络训练模型。一种方法是通过其他来源的图像来增强数据集，以训练深度学习网络。然而，你需要知道如何管理这一点，以及如何处理它引入的不确定性。另一种方法是迁移学习，我们将在[第
    11 章](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml)，*深度学习的下一个层次*中讨论。
- en: Another difference between deep learning and traditional machine learning is
    the size of the datasets. This can affect the ratios used to split data between train/test—the
    recommended guidelines for splitting data into 70/30 or 80/20 splits for machine
    learning need to be revised for training deep learning models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习与传统机器学习之间的另一个区别是数据集的大小。这会影响数据拆分的比例——用于机器学习的数据拆分推荐指南（如 70/30 或 80/20 拆分）需要在训练深度学习模型时进行修订。
- en: Different data distributions
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的数据分布
- en: 'In previous chapters, we used the MNIST dataset for classification tasks. While
    this dataset contains handwritten digits, the data is not representative of real-life
    data. In [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification
    Using Convolutional Neural Networks,* we visualized some of the digits, if you
    go back and look at these images, it is clear that these images are in a standard
    format:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了 MNIST 数据集进行分类任务。虽然该数据集包含手写数字，但这些数据并不代表真实世界的数据。在[第 5 章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)，*使用卷积神经网络进行图像分类*中，我们可视化了其中的一些数字，如果你回去看这些图像，会发现这些图像是标准格式的：
- en: There are all grayscale
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有图像都是灰度的
- en: The images are all 28 x 28
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有图像都是 28 x 28
- en: The images all appear to have at border of at least 1 pixel
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有图像的边界似乎至少有 1 像素
- en: The images are all of the same scale, that is, each image takes up most of the
    image
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有图像的尺度相同，也就是说，每个图像几乎占据了整个图像
- en: There is very little distortion, since the border is black and the foreground
    is white
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扭曲非常小，因为边框是黑色的，前景是白色的
- en: Images are the *right way up*, that is, we do not have any major rotations
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像是*正立的*，也就是说，我们没有进行过大的旋转
- en: 'The original use case for the MNIST dataset is to recognize 5 digit postcodes
    on letters. Let''s suppose we train a model on the 60,000 images in the MNIST
    dataset and wish to use it in a production environment to recognize postcodes
    from letters and packages. Here are the steps a production system must go through
    before any deep learning can be applied:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集的最初用途是识别信件上的 5 位数字邮政编码。假设我们使用 MNIST 数据集中的 60,000 张图像来训练一个模型，并希望在生产环境中使用它来识别信件和包裹上的邮政编码。生产系统必须在应用深度学习之前执行以下步骤：
- en: Scan the letters
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扫描字母
- en: Find the postcode section
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到邮政编码部分
- en: Split the postcode digits into 5 different regions (one per digit)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将邮政编码的数字分成 5 个不同的区域（每个数字一个区域）
- en: 'In any one of these data transformation steps, additional data bias could occur.
    If we used the *clean* MNIST data to train a model and then tried to predict the
    *biased* transformed data, then our model may not work that well. Examples of
    how bias could affect the production data include the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何一个数据转换步骤中，可能会出现额外的数据偏差。如果我们使用*干净*的 MNIST 数据来训练模型，然后尝试预测*有偏*的转换数据，那么我们的模型可能效果不好。数据偏差对生产数据的影响示例如下：
- en: Correctly locating the postcode is a difficult problem in itself
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确定位邮政编码本身就是一个难题
- en: The letters will have backgrounds and foregrounds of different colors and contrasts,
    and so converting them to grayscale may not be consistent depending on the type
    of letter and pen used on the letter / package
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字母将具有不同颜色和对比度的背景和前景，因此将它们转换为灰度图像可能不一致，这取决于字母和笔在字母/包裹上的使用类型。
- en: The results from the scanning processes may vary because of different hardware
    and software being used—this is a ongoing problem in applying deep learning to
    medical image data
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扫描过程的结果可能会有所不同，因为使用了不同的硬件和软件——这是将深度学习应用于医学图像数据时的一个持续性问题。
- en: Finally, the difficultly in splitting the postcode into 5 different regions
    depends on the letter and pen used, as well as the quality of the preceding steps
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，将邮政编码分成5个不同区域的难度，取决于字母和笔的使用方式，以及前面步骤的质量。
- en: In this example, the distribution of the data used to train and the estimate
    model's performance is different from the production data. If a data scientist
    had promised to deliver 99% accuracy before the model is deployed, then senior
    managers are very likely to be disappointed when the application runs in production!
    When creating a new model, we split data into train and test splits, so the main
    purpose of the test dataset is to estimate model accuracy. But if the data in
    the test dataset is different to what the model will be see in production, then the
    evaluation metrics on the test dataset cannot give a good guide to how the model
    will perform in production.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，用来训练数据的分布与估计模型性能的数据，与生产数据是不同的。如果数据科学家承诺在模型部署之前提供99%的准确度，那么当应用程序在生产环境中运行时，管理层很可能会感到失望！在创建一个新模型时，我们将数据分为训练集和测试集，因此测试数据集的主要目的是估计模型的准确性。但如果测试数据集中的数据与模型在生产环境中将会见到的数据不同，那么测试数据集上的评估指标就无法准确指导模型在生产环境中的表现。
- en: If the problem is that there is little or no actual labeled dataset to begin
    with, then one of the first steps to consider before any model training is to
    investigate if more data can be acquired. Acquiring data may involve setting up
    a mini production environment, partnering with a client or using a combination
    of semi-supervised and manual labelling. In the use case we just saw, I would
    consider it more important to set up the process to extract the digitized images
    before looking at any machine learning. Once this is set up, I would look to build
    up some training data—it still may not be enough to build a model, but it could
    be used but a proper test set to create evaluation metrics that would reflect
    real-life performance. This may appear obvious, as over optimistic expectations
    based on flawed evaluation metrics are probably one of the top three problems
    in data science projects.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题是一开始几乎没有或完全没有实际的标注数据，那么在任何模型训练之前，首先需要考虑的一步是调查是否可以获取更多数据。获取数据可能需要搭建一个小型生产环境，或者与客户合作，使用半监督学习与人工标注相结合的方法。在我们刚才看到的用例中，我会认为，设置提取数字化图像的流程比查看任何机器学习方法更为重要。一旦这个过程搭建好，我会着手建立一些训练数据——这些数据仍然可能不足以建立一个模型，但可以用来作为一个合适的测试集，以创建能够反映实际性能的评估指标。这一点可能看起来显而易见，因为基于有缺陷的评估指标所产生的过于乐观的期望，很可能是数据科学项目中排名前三的问题之一。
- en: One example of a very large scale project that managed this problem very well
    is this use case in Airbnb: [https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3](https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3).
    They had a huge number of photos of house interiors, but these were not labeled
    with the room type. They took their existing labeled data and also performed quality
    assurance to check how accurate the labels were. It is often said in data science
    that creating machine learning models may only be 20% of the actual work involved—acquiring
    an accurate large labeled dataset that is representative of what the model will
    see in production is often the hardest task in a deep learning project.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常成功地处理这个问题的大型项目案例是 Airbnb 中的这个用例：[https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3](https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3)。他们有大量的房屋室内照片，但这些照片没有标注房间类型。他们利用现有的标注数据，并且进行质量保证，以检查标签的准确性。在数据科学中，常说创建机器学习模型可能只占实际工作量的20%——获取一个准确且能代表模型在生产环境中实际见到的数据集，通常是深度学习项目中最困难的任务。
- en: 'Once you have a dataset in place, you need to split your data into train and
    test splits before modeling. If you have experience in traditional machine learning,
    you may start with a 70/30 split, that is, 70% for training the model and 30%
    for evaluating the model. However, this rule is less valid in the world of large
    datasets and training deep learning models. Again, the only reason to split data
    into train and test sets is to have a holdout set to estimate the model''s performance.
    Therefore, you only need enough records in this dataset so that the accuracy estimate
    you get is reliable and has the precision you require. If you have a large dataset
    to begin with, then a smaller percentage might be adequate for the test dataset.
    Let me explain this with an example, where you want to improve on an existing
    machine learning model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了数据集，你需要在建模之前将数据分为训练集和测试集。如果你有传统机器学习的经验，你可能会从70/30的划分开始，即70%用于训练模型，30%用于评估模型。然而，在大数据集和深度学习模型训练的领域，这条规则就不那么适用了。再次强调，将数据分为训练集和测试集的唯一原因，是为了有一个留存集来估计模型的表现。因此，你只需要在这个数据集中拥有足够的记录，以便你得到的准确度估计是可靠的，并且具有你所要求的精度。如果你一开始就有一个大数据集，那么测试数据集的比例较小可能就足够了。让我通过一个例子来解释，你想在现有的机器学习模型上进行改进：
- en: A prior machine learning model has 99.0% accuracy
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先前的机器学习模型具有99.0%的准确度。
- en: There is a labeled dataset with 1,000,000 records
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个带标签的数据集，包含1,000,000条记录。
- en: If a new machine learning model is to be trained, then it should get at least
    99.1% accuracy for you to be confident that it is an improvement on the existing
    model. How many records do you need when evaluating the existing model? You only
    need enough records so that you are fairly sure that the accuracy on the new model
    is accurate to 0.1%. Therefore 50,000 records in the test set, which is 5% of
    the dataset, would be sufficient to evaluate your model. If the accuracy on these
    50,000 records was 99.1%, that would be 49,550 records. This represents 50 more
    correctly classified records than the benchmark model, which would strongly suggest
    that the second model is a better model—it would be unlikely that the difference
    would be simply down to chance.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要训练一个新的机器学习模型，那么它至少应该达到99.1%的准确度，才能让你确信它比现有模型有改进。那么在评估现有模型时需要多少记录呢？你只需要足够的记录，以便你能比较确定新模型的准确度在0.1%的范围内。因此，测试集中的50,000条记录（即数据集的5%）就足够评估你的模型。如果在这50,000条记录上的准确度为99.1%，则有49,550条记录是正确分类的。这比基准模型多了50条正确分类的记录，这强烈表明第二个模型是一个更好的模型——差异不太可能仅仅是偶然的结果。
- en: 'You may get resistance to the suggestion you use only 5% of data for model
    evaluation. However, the idea of splitting data into 70/30 splits goes back to
    the days of small datasets, such as the iris dataset with 150 records. We previously
    saw the following graph in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml),* Training
    Deep Prediction Models*, which showed how accuracy on machine learning algorithms
    tends to stagnate as the data size increases. Therefore, there was less of an
    incentive to maximize the amount of data that was available for training. Deep
    learning models can take advantage of more data, so if we can use less data for
    the test set, we should get a better model overall:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对只使用5%的数据来评估模型的建议感到抵触。然而，70/30数据划分的想法源自于小型数据集的时代，比如包含150条记录的鸢尾花数据集。我们之前在[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)中看到过以下图表，*训练深度预测模型*，该图展示了机器学习算法的准确度在数据量增加时往往会停滞。因此，最大化可用于训练的数据量的动机较小。深度学习模型可以利用更多的数据，因此如果我们可以为测试集使用更少的数据，我们应该能得到一个更好的模型：
- en: '![](img/272cf949-b665-4241-8b57-34c1419ee8c9.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/272cf949-b665-4241-8b57-34c1419ee8c9.png)'
- en: 'Figure 6.2: How model accuracy increases by dataset size for deep learning
    models versus other machine learning models'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：数据集大小如何影响深度学习模型与其他机器学习模型的准确度
- en: Data partition between training, test, and validation sets
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据在训练集、测试集和验证集之间的划分。
- en: The previous section highlighted the importance of acquiring some data at an
    early stage in the project. But if you do not have enough data to train a deep
    learning model, it is possible to train on other data and apply it to your data.
    For example, you can use a model trained on ImageNet data for image classification
    tasks. In this scenario, you need to use the real data that has been collected
    wisely. This section discusses some good practices on that subject.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节强调了在项目早期阶段获取一些数据的重要性。但如果你没有足够的数据来训练深度学习模型，仍然可以使用其他数据进行训练并将其应用到你的数据上。例如，你可以使用在ImageNet数据上训练的模型来进行图像分类任务。在这种情况下，你需要明智地使用收集到的真实数据。本节讨论了关于这一主题的一些好实践。
- en: If you have ever wondered why big companies such as Google, Apple, Facebook,
    Amazon, and so on have such a head start in AI, this is the reason why. While
    they have some of the best AI people in the world working for them, their chief
    advantage is that they have access to tons of *labeled* data that they can use
    to build their machine learning models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经想过，为什么像谷歌、苹果、Facebook、亚马逊等大公司在人工智能方面具有如此大的领先优势，原因就在于此。虽然他们有世界上最优秀的AI专家为他们工作，但他们的最大优势在于他们可以使用大量的*标注*数据来构建他们的机器学习模型。
- en: 'In the previous section, we said that the sole purpose of a test set is to
    evaluate the model. But if that data is not from the same distribution as the
    data the model will see in prediction tasks, then the evaluation will be misleading.
    One of the most important project priorities should be to acquire labeled data
    that is as similar to real-life data as soon as possible. Once you have that data,
    you need to be clever on how you use this valuable asset. The best use of this
    data, in order of priority, would be as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们说过测试集的唯一目的是评估模型。但是，如果这些数据和模型在预测任务中将会遇到的数据不来自相同的分布，那么评估结果会产生误导。项目的一个重要优先事项应该是尽早获取与现实数据相似的标注数据。一旦你获得了这些数据，你需要聪明地使用这一宝贵资产。根据优先级，最好的数据使用方式如下：
- en: Can I use some of this data to create more training data? This could be through
    augmentation, or implementing an early prototype that users can interact with.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以使用一些数据来创建更多的训练数据吗？这可以通过数据增强，或者实现一个早期原型让用户进行互动来实现。
- en: If you are building several models (which you should be), use some of the data
    in the validation set to tune the model.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在构建多个模型（这是应该做的），请使用验证集中的一些数据来调整模型。
- en: Use the data in the test set to evaluate the model.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用测试集中的数据来评估模型。
- en: Use the data in the train set.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练集中的数据。
- en: Some of these suggestions may be contentious—especially when suggesting that
    you should use the data for the validation set before the test set. Remember that
    the sole purpose of a test set is that it should only be used once to evaluate
    the model, so you only get one shot at using this data. If I have only a small
    amount of realistic data, then I prefer to use it to tune the model and have a
    less precise evaluation metric than having a poorly performing model with a very
    precise evaluation metric.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些建议可能会引发争议，尤其是建议你应该在测试集之前使用验证集的数据。记住，测试集的唯一目的是用来评估模型，它应该只使用一次，所以你只有一次使用这些数据的机会。如果我只有少量的真实数据，我更倾向于用它来调整模型，并接受较不精确的评估指标，而不是用它来获得一个评估指标非常精确但表现差劲的模型。
- en: This approach is risky, and ideally you want your validation dataset and your
    test dataset to be from the same distribution and be representative of the data
    that the model will see in production. Unfortunately, when you are at the early
    stages in machine learning projects with limited real-life data, then you have
    to make decisions on how best to use this data, and in this case it is better
    to use the limited data in the validation dataset rather than the test dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有风险，理想情况下，你希望验证数据集和测试数据集来自相同的分布，并且能够代表模型在生产环境中遇到的数据。不幸的是，当你处于机器学习项目的早期阶段，且现实数据有限时，你必须决定如何最佳地使用这些数据，在这种情况下，最好将有限的数据用在验证数据集上，而不是测试数据集上。
- en: Standardization
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化
- en: 'Another important step in data preparation is standardizing data. In the previous
    chapter, for the MNIST data, all pixel values were divided by 255 so that the
    input data was between 0.0 and 1.0\. In our case, we applied min-max normalization,
    which transforms the data linearly using the following function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备的另一个重要步骤是标准化数据。在上一章中，对于MNIST数据，所有像素值都被除以255，使得输入数据在0.0到1.0之间。在我们的案例中，我们应用了最小-最大归一化，它使用以下函数线性地转换数据：
- en: '*xnew = (x-min(x))/(max(x)-min(x))*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*xnew = (x - min(x)) / (max(x) - min(x))*'
- en: 'Since we already know that *min(x) = 0* and *max(x)=255*, this reduces to the
    following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经知道 *min(x) = 0* 和 *max(x) = 255*，因此这可以简化为以下形式：
- en: '*xnew = x / 255.0*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*xnew = x / 255.0*'
- en: 'The other most popular form of standardization scales the feature so that the
    mean is 0 and the standard deviation from the mean is 1\. This is also known as
    **z-scores**, and the formula for it is as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种最常见的标准化形式是将特征缩放，使得均值为0，标准差为1。这也被称为**z分数**，其公式如下：
- en: '*xnew = (x - mean(x)) / std.dev(x)*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*xnew = (x - mean(x)) / std.dev(x)*'
- en: 'There are three reasons why we need to perform standardization:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行标准化的原因有三点：
- en: It is especially important to normalize our input features if the features are
    in different scales. A common example often cited in machine learning is predicting
    house prices from the number of bedrooms and the square foot. The number of bedrooms
    ranges from 1 to 10, while the square feet can range from 500 sq feet to 20,000 sq
    feet. Deep learning models expect features to be in the same range.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果特征处于不同的尺度，特别重要的是对输入特征进行归一化。机器学习中常见的一个例子是根据卧室数量和面积来预测房价。卧室数量的范围从1到10，而面积可以从500平方英尺到20000平方英尺不等。深度学习模型要求特征处于相同的范围内。
- en: Even if all of our features are already in the same range, it is still advisable
    to normalize the input features. Recall from [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml), *Deep
    Learning Fundamentals*, that we looked at initializing the weights before model
    training. Any benefit from initializing weights will be cancelled if our features
    are not normalized. We also spoke about the problem of exploding and vanishing
    gradients. When features are on different scales this is more likely.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们的所有特征已经处于相同范围内，仍然建议对输入特征进行归一化。回想一下在[第3章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)《深度学习基础》中，我们讨论了在模型训练前初始化权重的问题。如果我们的特征没有归一化，初始化权重的任何好处都将被抵消。我们还讨论了梯度爆炸和梯度消失的问题。当特征处于不同的尺度时，这个问题更容易发生。
- en: Even if we avoid both of the preceding problem, if we do not apply normalization,
    the model will take longer to train.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使我们避免了前述两个问题，如果不进行归一化，模型的训练时间也会更长。
- en: For the churn model in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, all of the columns were monetary spent, so are already
    on the same scale. When we applied the log to each of these variables, it will
    have shrunk them down to values between -4.6 to 11, so there was no need to scale
    them to values between 0 and 1\. When correctly applied, standardization has no
    negative consequences and so should be one of first steps applied to data preparation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)《训练深度预测模型》中，流失模型的所有列都表示消费金额，因此它们已经处于相同的尺度。当我们对每个变量应用对数变换时，这会将它们缩小到-4.6到11之间，因此无需将它们缩放到0和1之间。标准化正确应用时没有负面影响，因此应该是数据准备中的第一步。
- en: Data leakage
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据泄露
- en: '**Data leakage** is where a feature used to train the model has values that
    could not exist if the model was used in production. It occurs most frequently
    in time series data. For example, in our churn use case in [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, there were a number of categorical variables in the data
    that indicated customer segmentation. A data modeler may assume that these are
    good predictor variables, but it is not known how and when these variables were
    set. They could be based on customer'' spend, which means that if they are used
    in the prediction algorithm, there is a circular reference—an external process
    calculates the segment based on the spend and then this variable is used to predict
    spend!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据泄露**是指用于训练模型的特征具有在生产环境中无法存在的值。它在时间序列数据中最为常见。例如，在我们在[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)《训练深度预测模型》中讨论的客户流失案例中，数据中有一些类别变量表示客户分群。数据建模者可能认为这些是良好的预测变量，但我们无法知道这些变量何时以及如何设置。它们可能基于客户的消费金额，这意味着如果这些变量被用于预测算法中，就会出现循环引用——外部过程根据消费金额计算分群，然后这个变量被用来预测消费金额！'
- en: When extracting data to build a model, you should be wary of categorical attributes
    and question when these variables could have been created and modified. Unfortunately,
    most database systems are poor at tracking the data lineage, so if in doubt you
    may consider omitting the variable from your model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取数据来构建模型时，你应该小心类别属性，并思考这些变量何时可能被创建和修改。不幸的是，大多数数据库系统在追踪数据来源方面较弱，因此如果有疑虑，你可以考虑将这些变量从模型中省略。
- en: Another example of data leakage in image classification tasks is when attribute
    information within the image is used in the model. For example, if we build a
    model where the filenames were included as attributes, these names may hint at
    the class name. When the model is used in production, theses hints will not exist,
    so this is also seen as data leakage.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类任务中，数据泄露的另一个例子是当图像中的属性信息被用于模型时。例如，如果我们建立一个模型，其中文件名作为属性包含在内，这些文件名可能暗示了类别名称。当该模型在生产环境中使用时，这些线索将不再存在，因此这也被视为数据泄露。
- en: We will see an example of data leakage in practice in the the *Use case—interpretability*
    section later in this chapter.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后的*使用案例—可解释性*部分看到数据泄露的实际例子。
- en: Data augmentation
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: One approach to increasing the accuracy in a model regardless of the amount
    of data you have is to create artificial examples based on existing data. This
    is called **data augmentation**. Data augmentation can also be used at test time
    to improve prediction accuracy.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 增加模型准确性的一个方法，不论你拥有多少数据，就是基于现有数据创建人工示例。这就是所谓的**数据增强**。数据增强也可以在测试时使用，以提高预测准确性。
- en: Using data augmentation to increase the training data
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强来增加训练数据
- en: 'We are going to apply data augmentation to the `MNIST` dataset that we used
    in previous chapters. The code for this section is in `Chapter6/explore.Rmd` if
    you want to follow along. In [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),
    *Image Classification Using Convolutional Neural Networks, *we plotted some examples
    from the MNIST data, so we won''t repeat the code again. It is included in the
    code file, and you can also refer back to the image in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对之前章节中使用的`MNIST`数据集应用数据增强。如果你想跟着做，本部分的代码位于`Chapter6/explore.Rmd`。在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)《使用卷积神经网络进行图像分类》中，我们绘制了一些来自MNIST数据集的例子，因此我们不再重复这些代码。它已包含在代码文件中，你也可以参考[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中的图像，《使用卷积神经网络进行图像分类》：
- en: '![](img/aaa6ddab-8aa7-4016-a047-fbe2740489df.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaa6ddab-8aa7-4016-a047-fbe2740489df.png)'
- en: Figure 6.3: The first 9 images in the MNIST dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：MNIST数据集中的前9张图像
- en: 'We described data augmentation as creating new data from an existing dataset.
    This means creating a new instance that is sufficiently different from the original
    instance but not so much that it no longer represents the data label. For image
    data, this might mean performing the following functions on the images:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据增强描述为从现有数据集中创建新数据。这意味着创建一个与原始实例有足够差异的新实例，但又不会如此不同以至于它不再代表数据标签。对于图像数据，这可能意味着对图像执行以下操作：
- en: '**Zooming**: By zooming into the center of the image, your model may be better
    able to handle images at different scales.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：通过放大图像的中心，模型可能更好地处理不同尺度的图像。'
- en: '**Shifting**: Moving the image up, down, left, or right can make the deep learning
    model more aware of examples of images taken off-center.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平移**：将图像向上、下、左或右移动，可以让深度学习模型更好地识别偏离中心的图像示例。'
- en: '**Rotation**: By rotating images, the model will be able to recognize data
    that is off-center.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转**：通过旋转图像，模型将能够识别偏离中心的数据。'
- en: '**Flipping**: For many objects, flipping the images 90 degrees is valid. For
    example, a picture of a car from the left side can be flipped to show a similar
    image of the car from the right side. A deep model can take advantage of this
    new perspective.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻转**：对于许多物体，图像翻转90度是有效的。例如，从左侧拍摄的汽车照片可以翻转，呈现出右侧的汽车图像。深度模型可以利用这一新视角。'
- en: '**Adding noise**: Sometimes, deliberately adding noise to images can force
    the deep learning model to find deeper meaning.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加噪声**：有时候，故意向图像中添加噪声可以迫使深度学习模型发现更深层次的意义。'
- en: '**Modifying color**: By adding filters to the image, you can simulate different
    lighting conditions. For example, you can change an image taken in bright light
    so that it appears to be taken in poor lighting conditions.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**修改颜色**：通过向图像添加滤镜，你可以模拟不同的光照条件。例如，你可以将一张在强光下拍摄的图像更改为看起来像是在光线不足的情况下拍摄的图像。'
- en: The goal of this task is to increase accuracy on the test dataset. However,
    the important rule of data augmentation is that the new data should attempt to
    simulate the data your model will use in production rather than trying to increase
    model accuracy on existing data. I cannot stress that enough. Getting 99% accuracy
    on a hold-out set means nothing if a model fails to work in a production environment
    because the data used to train and evaluate the model was not representative of
    real-life data. In our case, we can see that the MNIST images are grayscale and
    neatly centered, and so on. In a production use case, images are off-center and
    with different backgrounds and foregrounds (for example, with a brown background
    and blue writing), and so will not be classified correctly. You can attempt to
    pre-process the images so that you can format them to a similar manner (28 x 28
    grayscale image with black background and data centered with a 2 x 2 margin),
    but a better solution is to train the model on typical data it will encounter
    in production.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的目标是提高测试数据集的准确性。然而，数据增强的重要规则是，新数据应该尽力模拟模型在生产环境中使用的数据，而不是试图提高现有数据上的模型准确性。我无法强调这一点的重要性。如果模型在生产环境中无法正常工作，而用于训练和评估模型的数据并不代表现实生活中的数据，那么在留出的数据集上获得99%的准确率是毫无意义的。在我们的例子中，我们可以看到MNIST图像是灰度图且整齐居中的，等等。在生产环境中，图像通常是偏离中心的，并且背景和前景各异（例如，带有棕色背景和蓝色文字），因此无法正确分类。你可以尝试对图像进行预处理，以便将其格式化为类似的方式（28
    x 28灰度图，黑色背景，数据居中并有2 x 2的边距），但更好的解决方案是训练模型以应对其将在生产环境中遇到的典型数据。
- en: If we look at the previous image, we can see that most of these data augmentation
    tasks are not applicable to the MNIST data. All of the images appear to be at
    the same zoom level already, so creating artificial examples at increased zoom
    will not help. Similarly, shifting is unlikely to work, since the images are already
    centered. Flipping images is definitely not valid, since most digits are not valid
    when flipped, example *7*. There is no evidence of existing random noise in our
    data, so this will not work either.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看前面的图像，可以发现大多数数据增强任务并不适用于MNIST数据。所有图像似乎已经处于相同的缩放级别，因此创建放大版的人工图像不会有所帮助。同样，平移也不太可能有效，因为图像已经是居中的。翻转图像肯定无效，因为许多数字翻转后并不有效，例如*7*。我们的数据中没有现有的随机噪声，因此这一方法也行不通。
- en: 'One technique that we can try is to rotate the images. We will create two new
    artificial images for each existing image, the first artificial image will be
    rotated 15 degrees left and the second artificial image will be rotated 15 degrees
    right. Here are some of the artificial images after we have rotated the original
    images 15 degrees left:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试的一种技术是旋转图像。我们将为每个现有图像创建两个新的人工图像，第一个人工图像将向左旋转15度，第二个人工图像将向右旋转15度。以下是我们将原始图像向左旋转15度后的部分人工图像：
- en: '![](img/e9a813a3-c0c5-4e64-a1ec-dce8d29c14be.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9a813a3-c0c5-4e64-a1ec-dce8d29c14be.png)'
- en: 'Figure 6.4: MNIST data rotated 15 degrees left'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：MNIST数据向左旋转15度
- en: 'If we look at the the preceding screenshot, one strange anomaly exists. We
    have 10 classes, and using this approach may increase overall accuracy, but one
    class will not get as much uplift. The zero digit is the odd one out because rotating
    a zero still looks like a zero—we may still get an increase in accuracy for this
    class, but probably not as much as for the other classes. The function to rotate 
    image data is in `Chapter6/img_ftns.R`. It uses the `rotateImage` function from
    the `OpenImageR` package:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看前面的截图，会发现一个奇怪的异常。我们有10个类别，使用这种方法可能会提高整体准确率，但有一个类别的提升不那么显著。数字0就是其中的一个例外，因为旋转数字0看起来仍然像0——虽然这个类别的准确率可能会有所提高，但可能不如其他类别那么显著。旋转图像数据的函数在`Chapter6/img_ftns.R`中。它使用了`OpenImageR`包中的`rotateImage`函数：
- en: '[PRE2]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There is actually two types of data augmentation that we can apply to our dataset.
    The first type creates new training data from existing examples. But we can also
    use a technique called **test time augmentation** (**TTA**), which can be used
    during model evaluation. It makes copies of each test row and then uses these
    copies and the originals to vote for the category. We will see an example of this
    later.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以对数据集应用两种类型的数据增强。第一种类型是从现有样本创建新的训练数据。我们还可以使用一种叫做**测试时增强**（**TTA**）的技术，这可以在模型评估期间使用。它会对每一行测试数据进行复制，然后使用这些复制和原始数据一起投票决定类别。稍后我们会看到这个例子的展示。
- en: 'The code to create datasets for the data augmentation is in `Chapter6/augment.R`.
    Note that this takes a long time to run, maybe 6-10 hours depending on your machine.
    It also needs approx. 300 MB of free space on the drive to create the new datasets.
    The code is not difficult; it loads in the data, and splits it into train and
    test sets. For the train data, it creates two new instances: one rotated 15 degrees
    left and one rotated 15 degrees right. It is important that the data used to evaluate
    the model performance is not included in the data augmentation process, that is,
    split the data into a train dataset first and only apply data augmentation to
    the train split.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据增强数据集的代码在`Chapter6/augment.R`中。请注意，这个过程运行时间较长，可能需要6到10小时，具体取决于你的机器。它还需要大约300MB的空闲空间来创建新的数据集。代码并不复杂，它加载数据并将其分割成训练集和测试集。对于训练数据，它创建两个新实例：一个旋转15度向左，另一个旋转15度向右。需要注意的是，用于评估模型性能的数据不能包含在数据增强过程中，也就是说，首先要将数据分割成训练集，并且只对训练集应用数据增强。
- en: When the data augmentation is complete, there will be a new file in the data
    folder called `train_augment.csv`. This file should have 113,400 rows. Our original
    dataset for `MNIST` had 42,000 rows; we took 10% of that for test purposes (that
    is, to validate our model) and were left with 37,800 rows. We then made two copies
    of these rows, meaning that we now have 3 rows for each previous row. This means
    that we have *37,800 x 3 = **113,400* rows in our training data file. `augment.R`
    also outputs the test data (4,200 rows) as `test0.csv` and an augmented test set
    (`test_augment.csv`), which we will cover later.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据增强完成后，数据文件夹中将会生成一个名为`train_augment.csv`的新文件。这个文件应该包含113,400行。我们原始的`MNIST`数据集有42,000行；我们抽取了其中的10%作为测试数据（即用于验证我们的模型），剩下37,800行。然后我们为这些行做了两个副本，这意味着现在每一行都有3个副本。因此，训练数据文件中将包含*37,800
    x 3 = **113,400*行数据。`augment.R`还会输出测试数据（4,200行），保存为`test0.csv`，以及增强后的测试集（`test_augment.csv`），稍后我们将进一步讲解。
- en: 'The code to run the neural network is in `Chapter6/mnist.Rmd`. The first part
    which uses the augmented data for training is almost identical to the code in
    [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification
    Using Convolutional Neural Networks*. The only change is that it loads the data
    files created in `augment.R` (`train_augment.csv` and `test0.csv`), so we we will
    not repeat all of the code for the model here again. Here is the confusion matrix
    and the final accuracy on the test dataset:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 运行神经网络的代码在`Chapter6/mnist.Rmd`中。第一部分使用增强后的数据进行训练，几乎与[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)的代码完全相同，*卷积神经网络的图像分类*。唯一的区别是，它加载了`augment.R`中创建的数据文件（`train_augment.csv`和`test0.csv`），所以我们在这里不再重复模型的所有代码。以下是混淆矩阵和测试数据集上的最终准确度：
- en: '[PRE3]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This compares to an accuracy of `0.9821429` from our model in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image
    Classification Using Convolutional Neural Networks*, so this is a significant
    improvement. We have reduced our error rate by over 30% *(0.9885714-0.9835714**)
    / (1.0-0.9835714)*.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中模型的准确度`0.9821429`相比，取得了显著的改进。我们的错误率已经降低了超过30%*(0.9885714-0.9835714**)
    / (1.0-0.9835714)*。
- en: Test time augmentation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试时数据增强
- en: 'We can also use data augmentation during test time. In the `augment.R` file,
    it created a file with the original test set of 4,200 rows (`data/test0.csv`),
    which was used that to evaluate the model. The `augment.R` file also created a
    file called `test_augment.csv`, which has the original 4,200 rows and 2 copies
    for each image. The copies are similar to what we did to augment the training
    data, that is, a row with data rotated 15 degrees left and a row with data rotated
    15 degrees right. The three rows are outputted sequentially and we will use these
    3 rows to *vote* for the winner. We need to take 3 records at a time from `test_augment.csv`
    and calculate the prediction value as the average of these three values. Here
    is the code that performs test time augmentation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在测试时使用数据增强。在`augment.R`文件中，它创建了一个包含原始测试集4,200行数据（`data/test0.csv`）的文件，并用它来评估模型。`augment.R`文件还创建了一个名为`test_augment.csv`的文件，包含原始的4,200行数据，每个图像有2个副本。这些副本类似于我们在增强训练数据时所做的操作，即一行数据是将图像旋转15度向左，另一行数据是将图像旋转15度向右。三行数据按顺序输出，我们将使用这三行数据来*投票*决定最终结果。我们需要从`test_augment.csv`中每次取出3条记录，并计算这些值的平均预测值。以下是执行测试时数据增强的代码：
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Doing this, we get predictions for 12,600 rows (*4,200 x 3*). The for loop
    runs through 4,200 times and takes 3 records at a time, calculating the average
    accuracy. The increase in accuracy over the accuracy using augmented training
    data is small, from `0.9885714` to `0.9895238`, which is approx. 0.1% (4 rows).
    We can look at the effect of TTA in the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们得到了12,600行数据的预测（*4,200 x 3*）。for循环会运行4,200次，每次取出3条记录，计算平均准确度。使用增强训练数据的准确度提升较小，从`0.9885714`到`0.9895238`，约为0.1%（4行）。我们可以在以下代码中查看TTA的效果：
- en: '[PRE5]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This table shows the 9 rows where the test time augmentation was correct and
    the previous model was wrong. We can see three cases where the previous model
    (`pred.model`) predicted `9` and the test time augmentation model correctly predicted
    `4`. Although test time augmentation did not significantly increase our accuracy
    in this case, it can make a difference in other computer vision tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这张表显示了测试时数据增强正确的9行数据，而之前的模型是错误的。我们可以看到三种情况，其中之前的模型（`pred.model`）预测为`9`，而测试时数据增强模型正确预测了`4`。虽然在这个案例中，测试时数据增强并未显著提高我们的准确度，但它在其他计算机视觉任务中可能会带来差异。
- en: Using data augmentation in deep learning libraries
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在深度学习库中使用数据增强
- en: We implemented data augmentation using R packages and it took a long time to
    generate our augmented data. It was useful for demonstration purposes, but MXNet
    and Keras support data augmentation functions. In MXNet, there are a range of
    functions in `mx.image.*` to do this ([https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html](https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html)).
    In Keras, this is in `keras.preprocessing.*` ([https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)),
    which applies these automatically to your models. In [Chapter 11](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml),
    *The Next Level in Deep Learning*, we show how to apply data augmentation using
    Keras.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用R包实现了数据增强，但生成增强数据花费了很长时间。它对于演示目的很有用，但MXNet和Keras都支持数据增强功能。在MXNet中，`mx.image.*`有一系列函数可以实现此功能（[https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html](https://mxnet.incubator.apache.org/tutorials/python/data_augmentation.html)）。在Keras中，这些功能位于`keras.preprocessing.*`（[https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)），可以自动应用到你的模型中。在[第11章](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml)，*深度学习的下一个层级*中，我们展示了如何使用Keras进行数据增强。
- en: Tuning hyperparameters
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: All machine learning algorithms have hyper-parameters or settings that can change
    how they operate. These hyper-parameters can improve the accuracy of a model or
    reduce the training time. We have seen some of these hyper-parameters in previous
    chapters, particularly [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),
    *Deep Learning Fundamentals, *where we looked at the hyper-parameters that can
    be set in the `mx.model.FeedForward.create` function. The techniques in this section
    can help us find better values for the hyper-parameters.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的机器学习算法都有超参数或设置，这些超参数可以改变算法的运行方式。这些超参数能够提高模型的准确性或减少训练时间。我们在前面的章节中已经见过一些超参数，特别是[第3章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)《深度学习基础》，在这一章中，我们探讨了可以在`mx.model.FeedForward.create`函数中设置的超参数。本节中的技术可以帮助我们找到更好的超参数值。
- en: Selecting hyper-parameters is not a magic bullet; if the raw data quality is
    poor or if there is not enough data to support training, then tuning hyper-parameters
    will only get you so far. In these cases, either acquiring additional variables/features
    that can be used as predictors and/or additional cases may be required.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 选择超参数并不是灵丹妙药；如果原始数据质量较差，或者数据量不足以支持训练，那么调整超参数也只能起到有限的作用。在这种情况下，可能需要获取额外的变量/特征作为预测变量，或者增加更多的案例数据。
- en: Grid search
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索
- en: 'For more information on tuning hyper-parameters, see Bengio, Y. (2012), particularly
    Section 3, *Hyperparameters*, which discusses the selection and characteristics
    of various hyper-parameters. Aside from manual trial and error, two other approaches
    for improving hyper-parameters are grid searches and random searches. In a grid
    search, several values for hyper-parameters are specified and all possible combinations
    are tried. This is perhaps easiest to see. In R, we can use the `expand.grid()`
    function to create all possible combinations of variables:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于调整超参数的信息，请参见Bengio, Y.（2012），特别是第3节《超参数》，讨论了各种超参数的选择和特点。除了手动试错法之外，还有两种改善超参数的方法：网格搜索和随机搜索。在网格搜索中，指定多个超参数值并尝试所有可能的组合。这种方法可能是最容易理解的。在R中，我们可以使用`expand.grid()`函数来创建所有可能的变量组合：
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Grid searching is effective when there are only a few values for a few hyper-parameters.
    However, when there are many values for some or many hyper-parameters, it quickly
    becomes unfeasible. For example, even with only two values for each of eight hyper-parameters,
    there are *2⁸ = 256* combinations, which quickly becomes computationally impracticable.
    Also, if the interactions between hyper-parameters and model performance are small,
    then using grid search is an inefficient approach.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索在超参数值较少的情况下是有效的。然而，当某些或许多超参数的值很多时，它很快就变得不可行。例如，即使每个超参数只有两个值，对于八个超参数来说，也有*2⁸
    = 256*种组合，这很快就变得计算上不切实际。此外，如果超参数与模型性能之间的相互作用较小，那么使用网格搜索就是一种低效的方法。
- en: Random search
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机搜索
- en: An alternative approach to hyper-parameter selection is searching through random
    sampling. Rather than pre­-specifying all of the values to try and create all
    possible combinations, one can randomly sample values for the parameters, fit
    a model, store the results, and repeat. To get a very large sample size, this
    too would be computationally demanding, but you can specify just how many different
    models you are willing to run. Therefore this approach gives you a spread over
    the combination of hyper-parameters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数选择的另一种方法是通过随机采样进行搜索。与预先指定所有要尝试的值并创建所有可能的组合不同，可以随机采样参数的值，拟合模型，存储结果，然后重复这一过程。为了获得非常大的样本量，这也需要很高的计算要求，但你可以指定你愿意运行的不同模型的数量。因此，这种方法能让你在超参数组合上分布广泛。
- en: For random sampling, all that need to be specified are values to randomly sample,
    or distributions to randomly draw from. Typically, some limits would also be set.
    For example, although a model could theoretically have any integer number of layers,
    some reasonable number (such as 1 to 10) is used rather than sampling integers
    from 1 to a billion.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机采样，只需要指定要随机采样的值，或者指定要随机抽取的分布。通常还会设定一些限制。例如，虽然理论上模型可以有任何整数层数，但通常会使用一个合理的数字范围（如1到10），而不是从1到十亿中采样整数。
- en: To perform random sampling, we will write a function that takes a seed and then
    randomly samples a number of hyper-parameters, stores the sampled parameters,
    runs the model, and returns the results. Even though we are doing a random search
    to try and find better values, we are not sampling from every possible hyper-parameter.
    Many remain fixed at values we specify or their defaults.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行随机抽样，我们将编写一个函数，接受一个种子，然后随机抽样多个超参数，存储抽样的参数，运行模型并返回结果。尽管我们进行随机搜索以寻找更好的值，但我们并没有从所有可能的超参数中进行抽样。许多超参数仍保持在我们指定的值或其默认值上。
- en: 'For some hyper-parameters, specifying how to randomly sample values can take
    a bit of work. For example, when using dropout for regularization, it is common
    to have a relatively smaller amount of dropout for early hidden layers (0%-20%)
    and a higher amount for later hidden layers (50-80%). Choosing the right distributions
    allows us to encode this prior information into our random search. The following
    code plots the density of two beta distributions, and the results are shown in
    *Figure 6.5*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些超参数，指定如何随机抽样值可能需要一些工作。例如，当使用 dropout 进行正则化时，通常在较早的隐藏层（0%-20%）使用较小的 dropout，而在较晚的隐藏层（50%-80%）使用较大的
    dropout。选择合适的分布使我们能够将这些先验信息编码到我们的随机搜索中。以下代码绘制了两个 Beta 分布的密度，结果如 *图 6.5* 所示：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'By sampling from these distributions, we can ensure that our search focuses
    on small proportions of dropout for the early hidden layers, and in the **0**
    to **0.50** range for the hidden neurons with a tendency to over­sample from values closer
    to **0.50**:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从这些分布中进行抽样，我们可以确保我们的搜索聚焦于早期隐藏层的小比例 dropout，并且在 **0** 到 **0.50** 范围内的隐藏神经元，具有从接近
    **0.50** 的值过度抽样的趋势：
- en: '![](img/dccb2eb7-acbb-40e0-885d-a59a081ad55e.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dccb2eb7-acbb-40e0-885d-a59a081ad55e.png)'
- en: 'Figure 6.5: Using the beta distribution to select hyperparameters'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：使用 Beta 分布来选择超参数
- en: Use case—using LIME for interpretability
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用案例—使用 LIME 进行可解释性分析
- en: 'Deep learning models are known to be difficult to interpret. Some approaches
    to model interpretability, including LIME, allow us to gain some insights into
    how the model came to its conclusions. Before we demonstrate LIME, I will show
    how different data distributions and / or data leakage can cause problems when
    building deep learning models. We will reuse the deep learning churn model from
    [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training Deep Prediction
    Models*, but we are going to make one change to the data. We are going to introduce
    a bad variable that is highly correlated to the *y* value. We will only include
    this variable in the data used to train and evaluate the model. A separate test
    set from the original data will be kept to represent the data the model will see
    in production, this will not have the bad variable in it. The creation of this
    bad variable could simulate two possible scenarios we spoke about earlier:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型被认为难以解释。一些模型可解释性的方法，包括 LIME，允许我们深入了解模型是如何得出结论的。在演示 LIME 之前，我将展示不同的数据分布和/或数据泄漏如何在构建深度学习模型时引发问题。我们将重用
    [第 4 章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml) 中的深度学习客户流失模型，*训练深度预测模型*，但我们将对数据做一个改动。我们将引入一个与
    *y* 值高度相关的坏变量。我们只会在用于训练和评估模型的数据中包含该变量。一个来自原始数据的单独测试集将被保留，代表模型在生产环境中将看到的数据，测试集不会包含坏变量。创建这个坏变量可以模拟我们之前讨论的两种可能的情景：
- en: '**Different data distributions**: The bad variable does exist in the data that
    the model sees in production, but it has a different distribution which means
    the model does not perform as expected.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同的数据分布**：坏变量确实存在于模型在生产环境中看到的数据中，但其分布不同，这意味着模型的表现没有达到预期。'
- en: '**Data leakage**: Our bad variable is used to train and evaluate the model,
    but when the model is used in production, this variable is not available, so we
    assign it a zero value, which also means the model does not perform as expected.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据泄漏**：我们的坏变量被用来训练和评估模型，但当模型在生产环境中使用时，这个变量不可用，因此我们为它分配一个零值，这也意味着模型的表现没有达到预期。'
- en: 'The code for this example is in `Chapter6/binary_predict_lime.R`. We will not
    cover the deep learning model in depth again, so go back to [Chapter 4](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml), *Training
    Deep Prediction Models*, if you need a refresher on how it works. We are going
    to make two changes to the model code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的代码位于`Chapter6/binary_predict_lime.R`。我们不会再次深入讲解深度学习模型，如果你需要回顾如何实现，可以参考[第4章](28315a07-2bf0-45c8-8e6f-0e4f01616ca3.xhtml)，*训练深度预测模型*。我们将对模型代码做两个修改：
- en: 'We will split the data into three parts: a train, validate, and test set. The
    train split is used to train the model, the validate set is used to evaluate the
    model when it is trained, and the test set represents the data that the model
    sees in production.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将数据分成三部分：训练集、验证集和测试集。训练集用于训练模型，验证集用于评估已训练的模型，而测试集则代表模型在生产环境中会看到的数据。
- en: We will create the `bad_var` variable, and include it in the train and validation set,
    but not in the test set.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建`bad_var`变量，并将其包含在训练集和验证集中，但不包含在测试集中。
- en: 'Here is the code to split the data and create the `bad_var` variable:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是分割数据并创建`bad_var`变量的代码：
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our new variable is highly correlated with our `y` variable at `0.958`. We
    also created a bar plot of the most highly correlated features to the `y` variable,
    and we can see that correlation between this new variable and the `y` variable is
    much higher than correlation between the other variables and the `y` variable.
    If a feature is very highly correlated to the `y` variable, then this is usually
    a sign that something is wrong in the data preparation. It also indicates that
    a machine learning solution is not required because a simple mathematical formula
    will be able to predict the outcome variable. For a real project, this variable
    should not be included in the model. Here is the graph with the features that
    are most highly correlated with the `y` variable, the correlation of the `bad_var`
    variable is over `0.9`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新变量与`y`变量的相关性为`0.958`。我们还创建了一个条形图，显示了与`y`变量相关性最高的特征，从中可以看到，这个新变量与`y`变量的相关性远高于其他变量与`y`变量之间的相关性。如果某个特征与`y`变量的相关性非常高，通常表明数据准备过程中存在问题。这也意味着不需要机器学习解决方案，因为一个简单的数学公式就能预测结果变量。对于实际项目，这个变量应该被排除在模型之外。以下是与`y`变量相关性最高的特征图，`bad_var`变量的相关性超过`0.9`：
- en: '![](img/c51ab854-c930-482d-948e-83aa0e701d6f.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c51ab854-c930-482d-948e-83aa0e701d6f.png)'
- en: 'Figure 6.6: The top 10 correlations from feature to target variable'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：从特征到目标变量的前10大相关性
- en: 'Before we go ahead and build the model, notice how we set this new feature
    to zero for the test set. Our test set in this example actually represents the
    data that the model will see when it is production, so we set it to zero to represent
    either a different data distribution or a data leakage problem. Here is the code
    that shows how the model performs on the validation set and on the test set:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续构建模型之前，请注意我们如何将这个新特征在测试集中设为零。这个测试集实际上代表了模型在生产环境中会看到的数据，因此我们将其设为零，表示可能存在不同的数据分布或数据泄露问题。下面是展示模型在验证集和测试集上表现的代码：
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The validation set here represents the data used to evaluate the model when
    it is being built, while the test set represents the future production data. The
    accuracy on the validation set is over 90%, but the accuracy on the test set is
    less than 70%. This shows how different data distributions and/or data leakage
    problems can cause over-estimations of model accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的验证集代表了在模型构建过程中用于评估模型的数据，而测试集代表未来的生产数据。验证集上的准确率超过90%，但测试集上的准确率不到70%。这表明，不同的数据分布和/或数据泄露问题会导致模型准确率的过高估计。
- en: Model interpretability with LIME
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LIME进行模型可解释性分析
- en: '**LIME **stands for **Local Interpretable Model-Agnostic Explanations**. LIME
    can explain the predictions of any machine learning classifier, not just deep
    learning models. It works by making small changes to the input for each instance
    and trying to map the local decision boundary for that instance. By doing so,
    it can see which variable has the most influence for that instance. It is explained
    in the following paper: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin*.
    Why should I trust you?: Explaining the predictions of any classifier. Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining. ACM, 2016*.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**LIME**代表**局部可解释模型无关解释**。LIME可以解释任何机器学习分类器的预测结果，而不仅仅是深度学习模型。它的工作原理是对每个实例的输入进行小的变化，并尝试映射该实例的局部决策边界。通过这样做，它可以看到哪个变量对该实例的影响最大。相关内容可以参考以下论文：Ribeiro,
    Marco Tulio, Sameer Singh, and Carlos Guestrin*. 为什么我应该信任你？：解释任何分类器的预测结果。第22届ACM
    SIGKDD国际会议——知识发现与数据挖掘，ACM，2016*。'
- en: 'Let''s look at using LIME to analyze the model from the previous section. We
    have to set up some boilerplate code to interface the MXNet and LIME structures,
    and then we can create LIME objects based on our training data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用LIME分析上一节中的模型。我们需要设置一些样板代码来连接MXNet和LIME结构，然后我们可以基于训练数据创建LIME对象：
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We then can pass in the first 10 records in the test set and create a plot
    to show feature importance:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以传入测试集中的前10条记录，并创建一个图表来显示特征重要性：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will produce the following plot, which shows the features that were most
    influential in the model predictions:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下图表，展示对模型预测结果影响最大的特征：
- en: '![](img/d6bbe7d7-bf00-4481-ba69-1aa00dfaf64d.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6bbe7d7-bf00-4481-ba69-1aa00dfaf64d.png)'
- en: Figure 6.7: Feature importance using LIME
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：使用LIME的特征重要性
- en: 'Note how, in each case, the `bad_var` variable is the most important variable
    and its scale is much larger than the other features. This matches what we saw
    in *Figure 6.6*. The following graph shows the heatmap visualization for feature
    combinations for the 10 test cases:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在每个案例中，`bad_var`变量是最重要的变量，其尺度远大于其他特征。这与我们在*图6.6*中看到的情况一致。以下图展示了针对10个测试案例的特征组合的热图可视化：
- en: '![](img/765662ae-1728-4f81-8d55-1f1cb1634153.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/765662ae-1728-4f81-8d55-1f1cb1634153.png)'
- en: Figure 6.8: Feature heatmap using LIME
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：使用LIME的特征热图
- en: This example shows how to apply LIME to an existing deep learning model trained
    with MXNet to visualize which features were the most important for some of the
    predictions using the model. We can see in Figures 6.7 and 6.8 that a single feature
    was almost completely responsible for predicting the `y` variable, which is an
    indication that there is an issue with different data distributions and/or data
    leakage problems. In practice, such a variable should be excluded from the model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例展示了如何将LIME应用于一个已经使用MXNet训练的深度学习模型，以可视化哪些特征对模型的一些预测结果最为重要。从图6.7和图6.8中可以看出，单个特征几乎完全负责预测`y`变量，这表明存在数据分布不同和/或数据泄漏的问题。实际上，这样的变量应当从模型中排除。
- en: 'As a comparison, if we train a model without this field, and plot the feature
    importance again, we see that one feature does not dominate:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 做个对比，如果我们在没有这个字段的情况下训练一个模型，再次绘制特征重要性图，我们会看到没有单一特征占主导地位：
- en: '![](img/4a3b160d-5582-4943-81b7-ceafba51f828.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a3b160d-5582-4943-81b7-ceafba51f828.png)'
- en: Figure 6.9: Feature importance using LIME (without the `bad_var` feature)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：使用LIME的特征重要性（不包含`bad_var`特征）
- en: 'There is not one feature that is a number 1 feature, the explanation fit is
    0.05 compared to 0.18 in *Figure 6.7,* and the significance bars for the three
    variables are on a similar scale. The following graph shows the feature heatmap
    using LIME:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有一个特征是最重要的，拟合的解释度是0.05，相比于*图6.7*中的0.18，三个变量的显著性条形图在相似的尺度上。下图展示了使用LIME的特征热图：
- en: '![](img/b25a6e33-f61c-4d77-95cb-78c0b82f0ad1.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b25a6e33-f61c-4d77-95cb-78c0b82f0ad1.png)'
- en: Figure 6.10: Feature heatmap using LIME (without the `bad_var` feature)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：使用LIME的特征热图（不包含`bad_var`特征）
- en: Again, this plot shows us that more than one feature is being used. We can see
    that the scale of legend for the feature weights in the preceding graph is from
    0.01 - 0.02\. In *Figure 6.8*, the scale of legend for the feature weights was
    -0.2 - 0.2, indicating that some features (just one, actually) are dominating
    the model.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，图表向我们展示了使用了多个特征。我们可以看到前一张图中，特征权重的图例范围是从 0.01 到 0.02。而在*图6.8*中，特征权重的图例范围是从
    -0.2 到 0.2，这表明有些特征（实际上只有一个）主导了模型。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter covered topics that are critical to success in deep learning projects.
    These included the different types of evaluation metric that can be used to evaluate
    the model. We looked at some issues that can come up in data preparation, including
    if you only have a small amount of data to train on and how to create different
    splits in the data, that is, how to create proper train, test, and validation
    datasets. We looked at two important issues that can cause the model to perform
    poorly in production, different data distributions, and data leakage. We saw how
    data augmentation can be used to improve an existing model by creating artificial
    data and looked at tuning hyperparameters in order to improve the performance
    of a deep learning model. We closed the chapter by examining a use case where
    we simulated a problem with different data distributions/data leakage and used
    LIME to interpret an existing deep learning model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了深度学习项目成功的关键主题。这些内容包括可用于评估模型的不同类型的评估指标。我们还讨论了数据准备中可能出现的一些问题，包括在训练数据量较少时的处理方法，以及如何在数据中创建不同的划分，即如何创建适当的训练集、测试集和验证集。我们探讨了两个可能导致模型在生产环境中表现不佳的重要问题：数据分布的差异和数据泄露。我们看到了如何通过数据增强技术来改善现有模型，方法是通过创建人工数据，并讨论了如何调节超参数以提高深度学习模型的性能。最后，我们通过模拟数据分布差异/数据泄露的问题，并使用LIME解释现有的深度学习模型，结束了本章的讨论。
- en: Some of the concepts in this chapter may seem somewhat theoretical; however
    they are absolutely critical to the success of machine learning projects! Many
    books cover this material toward the end, but it is included in this book at a
    relatively early stage to signify its importance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些概念可能看起来有些理论性；然而，它们对于机器学习项目的成功至关重要！许多书籍会在最后才涉及这些内容，但本书在相对较早的阶段就涵盖了它们，以突出其重要性。
- en: In the next chapter, we are going to look at using deep learning for **Natural
    Language Processing** (**NLP**), or text data. Using deep learning for text data
    is more efficient, simpler, and often outperforms traditional NLP approaches.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用深度学习进行**自然语言处理**（**NLP**），即文本数据。使用深度学习处理文本数据更高效、更简单，且通常优于传统的NLP方法。
