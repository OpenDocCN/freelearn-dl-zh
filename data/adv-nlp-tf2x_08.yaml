- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Weakly Supervised Learning for Classification with Snorkel
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snorkel 进行弱监督学习分类
- en: Models such as BERT and GPT use massive amounts of unlabeled data along with
    an unsupervised training objective, such as a **masked language model** (**MLM**)
    for BERT or a next word prediction model for GPT, to learn the underlying structure
    of text. A small amount of task-specific data is used for fine-tuning the pre-trained
    model using transfer learning. Such models are quite large, with hundreds of millions
    of parameters, and require massive datasets for pre-training and lots of computation
    capacity for training and pre-training. Note that the critical problem being solved
    is the lack of adequate training data. If there were enough domain-specific training
    data, the gains from BERT-like pre-trained models would not be that big. In certain
    domains such as medicine, the vocabulary used in task-specific data is typical
    for the domain. Modest increases in training data can improve the quality of the
    model to a large extent. However, hand labeling data is a tedious, resource-intensive, and
    unscalable task for the amounts required for deep learning to be successful.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 像 BERT 和 GPT 这样的模型利用大量的未标注数据和无监督训练目标（例如 BERT 的**掩码语言模型**（**MLM**）或 GPT 的下一个单词预测模型）来学习文本的基本结构。少量任务特定数据用于通过迁移学习微调预训练模型。这些模型通常非常庞大，拥有数亿个参数，需要庞大的数据集进行预训练，并且需要大量计算能力进行训练和预训练。需要注意的是，解决的关键问题是缺乏足够的训练数据。如果有足够的领域特定训练数据，BERT
    类预训练模型的收益就不会那么大。在某些领域（如医学），任务特定数据中使用的词汇对于该领域是典型的。适量增加训练数据可以大大提高模型的质量。然而，手工标注数据是一项繁琐、资源密集且不可扩展的任务，尤其是对于深度学习所需的大量数据而言。
- en: 'We discuss an alternative approach in this chapter, based on the concept of
    weak supervision. Using the Snorkel library, we label tens of thousands of records
    in a couple of hours and exceed the accuracy of the model developed in *Chapter
    3*, *Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding*
    using, BERT. This chapter covers:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了一种基于弱监督概念的替代方法。通过使用 Snorkel 库，我们在几小时内标注了数万个记录，并超越了在*第 3 章*中使用 BERT 开发的模型的准确性，*命名实体识别（NER）与
    BiLSTM、CRF 和维特比解码*。本章内容包括：
- en: An overview of weakly supervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弱监督学习概述
- en: An overview of the differences between generative and discriminative models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成模型和判别模型之间差异的概述
- en: Building a baseline model with handcrafted features for labeling data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用手工特征构建基准模型以标注数据
- en: Snorkel library basics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snorkel 库基础
- en: Augmenting training data using Snorkel labeling functions at scale
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Snorkel 标签函数大规模增强训练数据
- en: Training models using noisy machine-labeled data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用噪声机器标注数据训练模型
- en: It is essential to understand the concept of weakly supervised learning, so
    let's cover that first.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理解弱监督学习的概念至关重要，因此我们先从这个概念开始。
- en: Weak supervision
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱监督学习
- en: Deep learning models have delivered incredible results in the recent past. Deep
    learning architectures obviated the need for feature engineering, given enough
    training data. However, enormous amounts of data are needed for a deep learning
    model to learn the underlying structure of the data. On the one hand, deep learning
    reduced the manual effort required to handcraft features, but on the other, it
    significantly increased the need for labeled data for a specific task. In most
    domains, gathering a sizable set of high-quality, labeled data is an expensive
    and resource-intensive task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习模型取得了令人难以置信的成果。深度学习架构消除了特征工程的需求，只要有足够的训练数据。然而，深度学习模型学习数据的基本结构需要大量数据。一方面，深度学习减少了手工特征制作所需的人工工作，但另一方面，它大大增加了特定任务所需的标注数据量。在大多数领域，收集大量高质量标注数据是一项昂贵且资源密集的任务。
- en: 'This problem can be solved in several different ways. In previous chapters,
    we have seen the use of transfer learning to train a model on a large dataset
    before fine-tuning the model for a specific task. *Figure 8.1* shows this and
    other approaches to acquiring labels:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过多种方式解决。在前面的章节中，我们已经看到了使用迁移学习在大数据集上训练模型，然后再针对特定任务微调模型。*图 8.1* 展示了这种方法以及其他获取标签的方法：
- en: '![A picture containing clock  Description automatically generated](img/B16252_08_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing clock  Description automatically generated](img/B16252_08_01.png)'
- en: 'Figure 8.1: Options for getting more labeled data'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：获取更多标注数据的选项
- en: Hand labeling the data is a common approach. Ideally, we have enough time and
    money to hire **subject matter experts** (**SMEs**) to hand label every piece
    of data, which is not practical. Consider labeling a tumor detection dataset and
    hiring oncologists for the labeling task. Labeling data is probably way lower
    in priority for an oncologist than treating tumor patients. In a previous company,
    we organized pizza parties where we would feed people lunch for labels. In an
    hour, a person could label about 100 records. Feeding 10 people monthly for a
    year resulted in 12,000 labeled records! This scheme was useful for ongoing maintenance
    of models, where we would sample the records that were out of distribution, or
    that the model had shallow confidence in. Thus, we adopted active learning, which
    determines the records upon labeling, which would have the highest impact on the
    performance of a classifier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 手动标记数据是一种常见的方法。理想情况下，我们有足够的时间和金钱来雇佣**专业学科专家（SMEs）**手动标记每一条数据，这是不切实际的。考虑标记肿瘤检测数据集并雇佣肿瘤学家来进行标记任务。对于肿瘤学家来说，标记数据可能比治疗肿瘤患者的优先级低得多。在以前的公司，我们组织了披萨派对，我们会为人们提供午餐来标记数据。一个人可以在一个小时内标记大约100条记录。每月为10人提供午餐一年，结果是12,000条标记记录！这种方案对于模型的持续维护非常有用，我们会对模型无法处理或置信度较低的记录进行采样。因此，我们采用了主动学习，确定标记数据对分类器性能影响最大的记录。
- en: Another option is to hire labelers that are not experts but are more abundant
    and cheaper. This is the approach taken by the Amazon Mechanical Turk service.
    There are a large number of companies that provide labeling services. Since the
    labelers are not experts, the same record is labeled by multiple people, and some
    mechanism, like majority vote, is used to decide on the final label of the record.
    The charge for labeling one record by one labeler may vary from a few cents to
    a few dollars depending on the complexity of the steps needed for associating
    a label. The output of such a process is a set of noisy labels that have high
    coverage, as long as your budget allows for it. We still need to figure out the
    quality of the labels acquired to see how these labels can be used in the eventual
    model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是雇佣标记人员，他们不是专家，但更为丰富且更便宜。这是亚马逊机械土耳其服务采取的方法。有大量公司提供标记服务。由于标记者不是专家，同一条记录可能会由多人标记，并且会使用类似多数表决的机制来决定记录的最终标签。通过一名标记者标记一条记录的收费可能会因为关联标签所需的步骤复杂性而从几分钱到几美元不等。这种过程的输出是一组具有高覆盖率的嘈杂标签，只要您的预算允许。我们仍需找出获取的标签质量，以确定这些标签如何在最终模型中使用。
- en: Weak supervision tries to address the problem differently. What if, using heuristics,
    an SME could hand label thousands of records in a fraction of the time? We will
    work on the IMDb movie review dataset and try to predict the sentiment of the
    review. We used the IMDb dataset in *Chapter 4* , *Transfer Learning with BERT*,
    where we explored transfer learning. It is appropriate to use the same example
    to show an alternate technique to transfer learning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督试图以不同的方式解决问题。假如，使用启发式方法，一位**专业学科专家（SME）**可以在几秒钟内手工标记成千上万条记录？我们将使用IMDb电影评论数据集，并尝试预测评论的情感。我们在*第4章*中使用了IMDb数据集，*BERT迁移学习*，我们探索了迁移学习。使用相同的示例展示一个替代的技术来展示迁移学习是合适的。
- en: Weak supervision techniques don't have to be used as substitutes for transfer
    learning. Weak supervision techniques help create larger domain-specific labeled
    datasets. In the absence of transfer learning, a larger labeled dataset improves
    model performance even with noisy labels coming from weak supervision. However,
    the gain in model performance will be even more significant if transfer learning
    and weak supervision are both used together.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督技术不必用作迁移学习的替代品。弱监督技术有助于创建更大的领域特定标记数据集。在没有迁移学习的情况下，一个更大的标记数据集即使来自弱监督的嘈杂标签，也会提高模型性能。然而，如果同时使用迁移学习和弱监督，模型性能的提升将更为显著。
- en: 'An example of a simple heuristic function for labeling a review as having a
    positive sentiment can be shown with the following pseudocode:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的启发式函数示例用于将评论标记为具有积极情感的伪代码如下所示：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: While this may seem like a trivial example for our use case, you will be surprised
    how effective it can be. In a more complicated setting, an oncologist can provide
    some of these heuristics and define a few of these functions, which can be called
    labeling functions, to label some records. These functions may conflict or overlap
    with each other, similar to crowdsourced labels. Another approach for getting
    labels is through *distant supervision*. An external knowledge base, like Wikipedia,
    can be used to label data records heuristically. In a **Named-Entity Recognition**
    (**NER**) use case, a gazetteer is used to match entities to a list of known entities,
    as discussed in *Chapter 2*, *Understanding Sentiment in Natural Language with
    BiLSTMs*. In relation extraction between entities, for example, *employee of*
    or *spouse of*, the Wikipedia page of an entity can be mined to extract the relation,
    and the data record can be labeled. There are other methods of obtaining these
    labels, such as using thorough knowledge of the underlying distributions generating
    the data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于我们的用例看起来是一个微不足道的例子，但你会惊讶于它的有效性。在更复杂的设置中，一位肿瘤学家可以提供一些这些启发式规则，并定义一些标注函数来标注一些记录。这些函数可能会相互冲突或重叠，类似于众包标签。获取标签的另一种方法是通过*远程监督*。可以使用外部知识库（如Wikipedia）通过启发式方法为数据记录标注。在**命名实体识别**（**NER**）的用例中，使用地名表来将实体匹配到已知实体列表，如*第2章*《使用BiLSTM理解自然语言中的情感》所讨论。在实体之间的关系抽取中，例如*员工属于*或*配偶是*，可以从实体的Wikipedia页面中提取关系，进而标注数据记录。还有其他获取这些标签的方法，例如使用对生成数据的底层分布的深刻理解。
- en: For a given data set, there can be several sources for labels. Each crowdsourced
    labeler is a source. Each heuristic function, like the "amazing acting" one shown
    above, is also a source. The core problem in weak supervision is combining these
    multiple sources to yield labels of sufficient quality for the final classifier.
    The key points of the model are described in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的数据集，标签可以来自多个来源。每个众包标签员都是一个来源。每个启发式函数（如上面展示的“惊人的演技”函数）也是一个来源。弱监督的核心问题是如何将这些多个来源结合起来，生成足够质量的标签供最终分类器使用。模型的关键点将在下一节描述。
- en: The domain-specific model is being referred to as the classifier in this chapter
    as the example we are taking is the binary classification of movie review sentiment.
    However, the labels generated can be used for a variety of domain-specific models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中提到的领域特定模型被称为分类器，因为我们所取的示例是电影评论情感的二分类。然而，生成的标签可以用于多种领域特定的模型。
- en: Inner workings of weak supervision with labeling functions
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弱监督与标注函数的内部工作原理
- en: The idea that a few heuristic labeling functions with low coverage and less
    than perfect accuracy can help improve the accuracy of a discriminative model
    sounds fantastic. This section provides a high-level overview of how this works,
    before we see it in practice on the IMDb sentiment analysis dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 少数启发式标注函数覆盖率低、准确度不完美，但仍能帮助提高判别模型准确度的想法听起来很棒。本节提供了一个高层次的概述，介绍了它是如何工作的，然后我们将在IMDb情感分析数据集上实践这一过程。
- en: We assume a binary classification problem for the sake of explanation though
    the scheme works for any number of labels. The set of labels for binary classification
    is {NEG, POS}. We have a set of unlabeled data points, *X*, with *m* samples.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设这是一个二分类问题进行说明，尽管该方案适用于任意数量的标签。二分类的标签集为{NEG, POS}。我们有一组未标记的数据点，*X*，包含*m*个样本。
- en: Note that we do not have access to the actual labels for these data points,
    but we represent the generated labels using *Y*. Let's assume we have *n* labeling
    functions *LF*[1] to *LF*[n], each of which produces a label. However, we add
    another label for weak supervision – an abstain label. Each labeling function
    has the ability to choose whether it wants to apply a label or abstain from labeling.
    This is a vital aspect of the weak supervision approach. Hence, the set of labels
    produced by labeling functions is expanded to {NEG, ABSTAIN, POS}.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们无法访问这些数据点的实际标签，但我们用*Y*表示生成的标签。假设我们有*n*个标注函数*LF*[1]到*LF*[n]，每个函数生成一个标签。然而，我们为弱监督添加了另一个标签——一个弃权标签。每个标注函数都有选择是否应用标签或放弃标注的能力。这是弱监督方法的一个关键方面。因此，标注函数生成的标签集扩展为{NEG,
    ABSTAIN, POS}。
- en: 'In this setting, the objective is to train a generative model which models
    two things:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置下，目标是训练一个生成模型，该模型建模两个方面：
- en: The probability of a given labeling function abstaining for a given data point
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定数据点的标注函数弃权的概率
- en: The probability of a given labeling function correctly assigning a label to
    a data point
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定标注函数正确分配标签到数据点的概率
- en: 'By applying all the labeling functions on all the data points, we generate
    an *m × n* matrix of data points and their labels. The label generated by the
    heuristic *LF*[j] on the data point *X*[i] can be represented by:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在所有数据点上应用所有标注函数，我们生成一个*m × n*的矩阵，表示数据点及其标签。由启发式标注函数*LF*[j]对数据点*X*[i]生成的标签可以表示为：
- en: '![](img/B16252_08_001.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_08_001.png)'
- en: The generative model is trying to learn from the agreements and disagreements
    between the labeling functions to learn the parameters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型试图通过标注函数之间的共识与分歧来学习参数。
- en: '**Generative versus Discriminative models**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成模型与判别模型**'
- en: If we have a set of data, *X*, and labels, *Y* corresponding to the data, then
    we can say that the discriminative model tries to capture the *conditional probability*
    *p(Y | X)*. A generative model captures the *joint probability p(X, Y)*. Generative
    models, as their name implies, can generate new data points. We saw examples of
    generative models in *Chapter 5*, *Generating Text with RNNs and GPT-2*, where
    we generated news headlines. **GANs** (**Generative Adversarial Networks**) and
    AutoEncoders are well-known generative models. Discriminative models label data
    points in a given data set. It does so by drawing a plane in the space of features
    that separates the data points into different classes. Classifiers, like the IMDb
    sentiment review prediction model, are typically discriminative models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一组数据*X*，以及与数据对应的标签*Y*，我们可以说，判别模型试图捕捉*条件概率*p(Y | X)*。生成模型则捕捉*联合概率p(X, Y)*。顾名思义，生成模型可以生成新的数据点。我们在*第5章*中看到生成模型的例子，*通过RNN和GPT-2生成文本*，在那里我们生成了新闻头条。**GANs**（**生成对抗网络**）和自编码器是著名的生成模型。判别模型则对给定数据集中的数据点进行标注。它通过在特征空间中划分一个平面，将数据点分成不同的类别。像IMDb情感评论预测模型这样的分类器通常是判别模型。
- en: As can be imagined, generative models have a much more challenging task of learning
    the whole underlying structure of the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如可以想象，生成模型在学习数据的整个潜在结构方面面临着更具挑战性的任务。
- en: 'The parameter weights, *w*, of the generative model *P*[w] can be estimated
    via:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型的参数权重，*w*，可以通过以下方式估计：
- en: '![](img/B16252_08_002.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_08_002.png)'
- en: 'Not that the log marginal likelihood of the observed labels factors out the
    predicted labels *Y*. Hence, this generative model works in an unsupervised fashion.
    Once the parameters of the generative model are computed, we can predict the labels
    for the data points as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，观察到的标签的对数边际似然性会因预测标签*Y*而因式分解。因此，该生成模型是以无监督方式工作的。一旦生成模型的参数被计算出来，我们就可以预测数据点的标签，表示为：
- en: '![](img/B16252_08_003.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_08_003.png)'
- en: Where *Y*[i] represents labels based on labeling functions and ![](img/B16252_08_004.png)
    represents the predicted label from the generative model. These predicted labels
    can be fed to a downstream discriminative model for classification.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*Y*[i]表示基于标注函数的标签，![](img/B16252_08_004.png)表示生成模型的预测标签。这些预测标签可以被传递给下游的判别模型进行分类。
- en: 'These concepts were implemented in the Snorkel library. The authors of the
    Snorkel library were the key contributors to introducing the *Data Programming*
    approach, in a paper of the same name presented at the Neural Information Process
    Systems conference in 2016\. The Snorkel library was introduced formally in a
    paper titled *Snorkel: rapid training data creation with weak supervision* by
    Ratner et al. in 2019\. Apple and Google have published papers using the Snorkel
    library, with papers on *Overton* and *Snorkel Drybell*, respectively. These papers
    can provide an in-depth discussion of the mathematical proof underlying the creation
    of training data with weak supervision.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '这些概念在Snorkel库中得到了实现。Snorkel库的作者是引入*数据编程*方法的关键贡献者，该方法在2016年神经信息处理系统会议（Neural
    Information Process Systems conference）上以同名论文形式发表。Snorkel库在2019年由Ratner等人在题为*Snorkel:
    rapid training data creation with weak supervision*的论文中正式介绍。苹果和谷歌分别发布了使用Snorkel库的论文，分别是关于*Overton*和*Snorkel
    Drybell*的论文。这些论文可以提供关于使用弱监督创建训练数据的数学证明的深入讨论。'
- en: As complex as the underlying principles may be, using Snorkel for labeling data
    is not difficult in practice. Let us get started by preparing the data set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管底层原理可能很复杂，但在实践中，使用 Snorkel 进行数据标注并不难。让我们通过准备数据集开始吧。
- en: Using weakly supervised labels to improve IMDb sentiment analysis
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用弱监督标签来改进 IMDb 情感分析
- en: Sentiment analysis of movie reviews on the IMDb website is a standard task for
    classification-type **Natural Language Processing** (**NLP**) models. We used
    this data in Chapter 4 to demonstrate transfer learning with GloVe and VERT embeddings.
    The IMDb data set has 25,000 training examples and 25,000 testing examples. The
    dataset also includes 50,000 unlabeled reviews. In previous attempts, we ignored
    these unsupervised data points. Adding more training data will improve the accuracy
    of the model. However, hand labeling would be a time-consuming and expensive exercise.
    We'll use Snorkel-powered labeling functions to see if the accuracy of the predictions
    can be improved on the testing set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对 IMDb 网站上的电影评论进行情感分析是分类类型**自然语言处理**（**NLP**）模型的标准任务。我们在第 4 章中使用了这些数据来演示使用 GloVe
    和 VERT 嵌入进行迁移学习。IMDb 数据集包含 25,000 个训练示例和 25,000 个测试示例。数据集还包括 50,000 条未标记的评论。在之前的尝试中，我们忽略了这些无监督的数据点。增加更多的训练数据将提高模型的准确性。然而，手动标注将是一个耗时且昂贵的过程。我们将使用
    Snorkel 驱动的标注功能，看看是否能够提高测试集上的预测准确性。
- en: Pre-processing the IMDb dataset
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理 IMDb 数据集
- en: Previously, we used the `tensorflow_datasets` package to download and manage
    the dataset. However, we need lower-level access to the data to enable writing
    the labeling functions. Hence, the first step is to download the dataset from
    the web.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用了`tensorflow_datasets`包来下载和管理数据集。然而，为了实现标签功能的编写，我们需要对数据进行更低级别的访问。因此，第一步是从网络上下载数据集。
- en: The code for this chapter is split across two files. The `snorkel-labeling.ipynb`
    file contains the code for downloading data and generating labels using Snorkel.
    The second file, `imdb-with-snorkel-labels.ipynb`, contains the code that trains
    models with and without the additional labeled data. If running the code, then
    it is best to run all the code in the `snorkel-labeling.ipynb` file first so that
    all the labeled data files are generated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码分为两个文件。`snorkel-labeling.ipynb`文件包含用于下载数据和使用 Snorkel 生成标签的代码。第二个文件`imdb-with-snorkel-labels.ipynb`包含训练有无额外标注数据的模型的代码。如果运行代码，最好先运行`snorkel-labeling.ipynb`文件中的所有代码，以确保生成所有标注数据文件。
- en: 'The dataset is available in one compressed archive and can be downloaded and
    expanded like so, as shown in `snorkel-labeling.ipynb`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含在一个压缩档案中，可以通过如下方式下载并解压，如`snorkel-labeling.ipynb`所示：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This expands the archive in the `aclImdb` directory. The training and unsupervised
    data is in the `train/` subdirectory while the testing data is in the `test/`
    subdirectory. There are additional files, but they can be ignored. *Figure 8.2*
    below shows the directory structure:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将解压档案到`aclImdb`目录中。训练数据和无监督数据位于`train/`子目录中，测试数据位于`test/`子目录中。还有一些额外的文件，但它们可以忽略。下面的*图
    8.2*展示了目录结构：
- en: '![](img/B16252_08_02.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_08_02.png)'
- en: 'Figure 8.2: Directory structure for the IMDb data'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：IMDb 数据的目录结构
- en: Reviews are stored as individual text files inside the leaf directories. Each
    file is named using the format `<review_id>_<rating>.txt`. Review identifiers
    are sequentially numbered from 0 to 24999 for training and testing examples. For
    the unsupervised data, the highest review number is 49999\.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 评论以单独的文本文件形式存储在叶目录中。每个文件的命名格式为`<review_id>_<rating>.txt`。评论标识符从0到24999按顺序编号，用于训练和测试示例。对于无监督数据，最高评论编号为49999\。
- en: The rating is a number between 0 and 9 and has meaning only in the test and
    training data. This number reflects the actual rating given to a certain review.
    The sentiment of all reviews in the `pos/` subdirectory is positive. The sentiment
    of reviews in the `neg/` subdirectory is negative. Ratings of 0 to 4 are considered
    negative, while ratings between 5 and 9 inclusive are considered positive. In
    this particular example, we do not use the actual rating and only consider the
    overall sentiment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 评分是一个介于 0 到 9 之间的数字，仅在测试和训练数据中有意义。这个数字反映了对某个评论给出的实际评分。`pos/`子目录中的所有评论情感为正面，`neg/`子目录中的评论情感为负面。评分为
    0 到 4 被视为负面，而评分为 5 到 9（包括 9）则视为正面。在这个特定的示例中，我们不使用实际评分，只考虑整体情感。
- en: 'We load the data into `pandas` DataFrames for ease of processing. A convenience
    function is defined to load reviews from a subdirectory into a DataFrame:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据加载到`pandas` DataFrame 中，方便处理。定义了一个便捷函数，将子目录中的评论加载到 DataFrame 中：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The method above loads the data into two columns – one for the name of the
    file and one for the text of the file. Using this method, the unsupervised dataset
    is loaded:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法将数据加载到两列中——一列为文件名，另一列为文件的文本。使用这种方法，加载了无监督数据集：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'A slightly different method is used for the training and testing datasets:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据集使用略有不同的方法：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This method returns three columns – the file name, the text of the review, and
    a sentiment label. The sentiment label is 0 if the sentiment is negative and 1
    if the sentiment is positive, as determined by the directory the review is found
    in.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法返回三列——文件名、评论文本和情感标签。如果情感为负，则情感标签为 0；如果情感为正，则情感标签为 1，这由评论所在的目录决定。
- en: 'The training dataset can now be loaded in like so:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以像这样加载训练数据集：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '|'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE27]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '|'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '|'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE33]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '|'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE34]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '|'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE36]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE37]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE38]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE39]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE40]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE41]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE42]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'While we don''t use the raw scores for the sentiment analysis, it is a good
    exercise for you to try predicting the score instead of the sentiment on your
    own. To help with processing the score from the raw files, the following code
    can be used, which extracts the scores from the file names:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们没有使用原始分数进行情感分析，但这是一个很好的练习，你可以尝试预测分数而不是情感。为了帮助处理原始文件中的分数，可以使用以下代码，该代码从文件名中提取分数：
- en: '[PRE43]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This adds a new *score* column to the DataFrame, which can be used as a starting
    point.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这会向 DataFrame 添加一个新的*分数*列，可以作为起始点使用。
- en: The testing data can be loaded using the same convenience function by passing
    a different starting data directory.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据可以通过传递不同的起始数据目录，使用相同的便捷函数加载。
- en: '[PRE44]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Once the reviews are loaded in, the next step is to create a tokenizer.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦评论加载完成，下一步是创建一个分词器。
- en: Learning a subword tokenizer
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习子词分词器
- en: A subword tokenizer can be learned using the `tensorflow_datasets` package.
    Note that we want to pass all the training and unsupervised reviews while learning
    this tokenizer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`tensorflow_datasets`包学习子词分词器。请注意，我们希望在学习此分词器时传递所有训练数据和无监督评论。
- en: '[PRE45]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This step creates a list of 75,000 items. If the text of the reviews is inspected,
    there are some HTML tags in the reviews as they were scraped from the IMDb website.
    We use the Beautiful Soup package to clean these tags.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步创建了一个包含 75,000 项的列表。如果检查评论文本，可以发现评论中包含一些 HTML 标签，因为这些评论是从 IMDb 网站抓取的。我们使用
    Beautiful Soup 包来清理这些标签。
- en: '[PRE46]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Then, we learn the vocabulary with 8,266 entries.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了包含 8,266 个条目的词汇表。
- en: '[PRE47]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This encoder is saved to disk. Learning the vocabulary can be a time-consuming
    task and needs to be done only once. Saving it to disk saves effort on subsequent
    runs of the code.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该编码器已保存到磁盘。学习词汇表可能是一个耗时的任务，并且只需要做一次。将其保存到磁盘可以在随后的代码运行中节省时间。
- en: A pre-trained subword encoder is supplied. It can be found in the GitHub folder
    corresponding to this chapter and is titled `imdb.subwords` in case you want to
    skip these steps.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个预训练的子词编码器。它可以在与本章对应的 GitHub 文件夹中找到，并命名为`imdb.subwords`，如果你想跳过这些步骤，可以直接使用它。
- en: Before we jump into a model using data labeled with Snorkel, let us define a
    baseline model so that we can compare the performance of the models before and
    after the addition of weakly supervised labels.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们使用 Snorkel 标注的数据构建模型之前，先定义一个基准模型，以便在添加弱监督标签前后，比较模型的性能。
- en: A BiLSTM baseline model
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个 BiLSTM 基准模型
- en: To understand the impact of additional labeled data on model performance, we
    need a point of comparison. So, we set up a BiLSTM model that we have seen previously
    as the baseline. There are a few steps of data processing, like tokenizing, vectorization,
    and padding/truncating the lengths of the data. Since this is code we have seen
    before in Chapter 3 and 4, it is replicated here for completeness with concise
    descriptions.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解额外标注数据对模型性能的影响，我们需要一个比较的基准。因此，我们设定一个之前见过的 BiLSTM 模型作为基准。有几个数据处理步骤，比如分词、向量化和填充/截断数据的长度。由于这些代码在第
    3 和第 4 章中已经出现过，因此为了完整性，在此处进行了重复，并附上简明的描述。
- en: 'Snorkel is effective when the training data size is 10x to 50x the original.
    IMDb provides 50,000 unlabeled examples. If all these were labeled, then the training
    data would be 3x the original, which is not enough to show the value of Snorkel.
    Consequently, we simulate an ~18x ratio by limiting the training data to only
    2,000 records. The rest of the training records are treated as unlabeled data,
    and Snorkel is used to supply noisy labels. To prevent the leakage of labels,
    we split the training data and store two separate DataFrames. The code for this
    split can be found in the `snorkel-labeling.ipynb` notebook. The code fragment
    used to generate the split is shown below:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 在训练数据大小是原始数据的 10 倍到 50 倍时效果最佳。IMDb 提供了 50,000 条未标注的示例。如果所有这些都被标注，那么训练数据将是原始数据的
    3 倍，这不足以展示 Snorkel 的价值。因此，我们通过将训练数据限制为仅 2,000 条记录，模拟了大约 18 倍的比例。其余的训练记录被视为未标注数据，Snorkel
    用来提供噪声标签。为了防止标签泄露，我们将训练数据进行拆分，并存储为两个独立的数据框。拆分的代码可以在 `snorkel-labeling.ipynb` 笔记本中找到。用于生成拆分的代码片段如下所示：
- en: '[PRE48]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: A stratified split is used to ensure an equal number of positive and negative
    labels are sampled. A DataFrame with 2,000 records is saved. This DataFrame is
    used for training the baseline. Note that this may look like a contrived example
    but remember that the key feature of text data is that there is a lot of it; however,
    labels are scarce. Often the main barrier to labeling is the amount of effort
    required to label more data. Before we see how to label large amounts of data,
    let's complete training the baseline model for comparison.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分层拆分方法以确保正负标签的样本数量相等。一个包含 2,000 条记录的数据框被保存，并用于训练基准模型。请注意，这可能看起来像是一个人为构造的示例，但请记住，文本数据的关键特点是数据量庞大；然而，标签则稀缺。通常，标注数据的主要障碍是所需的标注工作量。在我们了解如何标注大量数据之前，先完成基准模型的训练以作比较。
- en: Tokenization and vectorizing data
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词和向量化数据
- en: 'We tokenize all reviews in the training set and truncate/pad to a maximum of
    150 tokens. Reviews are passed through Beautiful Soup to remove any HTML markup.
    All the code for this section can be found in the section titled *Training Data
    Vectorization* in the `imdb-with-snorkel-labels.ipynb` file. Only the specific
    pieces of code are shown here for brevity:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对训练集中的所有评论进行分词，并将其截断/填充为最多 150 个词元。评论通过 Beautiful Soup 处理，去除任何 HTML 标记。所有与此部分相关的代码都可以在
    `imdb-with-snorkel-labels.ipynb` 文件中的 *Training Data Vectorization* 部分找到。这里仅为简便起见展示了部分代码：
- en: '[PRE49]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Tokenization and vectorization are done through helper functions and applied
    over the dataset:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 分词和向量化通过辅助函数进行，并应用于整个数据集：
- en: '[PRE50]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The test data is also processed similarly:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据也以类似的方式进行处理：
- en: '[PRE51]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Once the data is ready, the next step is setting up the model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备好，下一步就是搭建模型。
- en: Training using a BiLSTM model
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 BiLSTM 模型进行训练
- en: 'The code for creating and training the baseline is in the *Baseline Model*
    section of the notebook. A modestly sized model is created as the focus is on
    showing the gains from unsupervised labeling as opposed to model complexity. Plus,
    a smaller model trains faster and allows more iteration:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和训练基准模型的代码位于笔记本的 *Baseline Model* 部分。创建了一个适中的模型，重点展示了无监督标注带来的提升，而非模型复杂性。此外，较小的模型训练速度更快，且能进行更多迭代：
- en: '[PRE52]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The model uses a small 64-dimensional embedding and RNN units. The function
    for creating the model is below:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用一个小的 64 维嵌入和 RNN 单元。创建模型的函数如下：
- en: '[PRE53]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: A modest amount of dropout is added to have the model generalize better. This
    model has about 700K parameters.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 添加适量的 dropout，以使模型具有更好的泛化能力。该模型大约有 70 万个参数。
- en: '[PRE54]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The model is compiled with a binary cross-entropy loss function and the ADAM
    optimizer. Accuracy, precision, and recall metrics are tracked. This model is
    trained for 15 epochs and it can be seen that the model is saturated:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用二元交叉熵损失函数和 ADAM 优化器进行编译，并跟踪准确率、精确度和召回率指标。该模型训练了 15 个周期，可以看出模型已达到饱和：
- en: '[PRE56]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As we can see, the model is overfitting to the small training set even after
    dropout regularization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，即使在使用 dropout 正则化后，模型仍然对小规模的训练集发生了过拟合。
- en: '**Batch-and-Shuffle or Shuffle-and-Batch**'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**Batch-and-Shuffle 或 Shuffle-and-Batch**'
- en: Note the second line of code in the fragment above, which shuffles and batches
    the data. The data is shuffled and then batched. Shuffling data between epochs
    is a form of regularization and enables the model to learn better. Shuffling before
    batching is a key point to remember in TensorFlow. If data is batched before shuffling,
    then only the order of the batches will be moved around when being fed to the
    model. However, the composition of each batch remains the same across epochs.
    By shuffling before batching, we ensure each batch looks different in each epoch.
    You are encouraged to train with and without shuffled data. While shuffling increases
    training time slightly, it gives better performance on the test set.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意上面代码片段中的第二行代码，它对数据进行了打乱和批处理。数据首先被打乱，然后再进行批处理。在每个周期之间对数据进行打乱是一种正则化手段，有助于模型更好地学习。在
    TensorFlow 中，打乱数据顺序是一个关键点。如果数据在打乱之前就被批处理，那么只有批次的顺序会在喂入模型时发生变化。然而，每个批次的组成在不同的周期中将保持不变。通过在批处理之前进行打乱，我们确保每个批次在每个周期中都不同。我们鼓励您在有和没有打乱数据的情况下进行训练。虽然打乱会稍微增加训练时间，但它会在测试集上带来更好的性能。
- en: 'Let us see how this model does on the test data:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个模型在测试数据上的表现：
- en: '[PRE58]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The model has 75.9% accuracy. The precision of the model is higher than the
    recall. Now that we have a baseline, we can see if weakly supervised labeling
    helps improve model performance. That is the focus of the next section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确率为 75.9%。模型的精度高于召回率。现在我们有了基准线，我们可以查看弱监督标注是否能帮助提升模型性能。这将是下一部分的重点。
- en: Weakly supervised labeling with Snorkel
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Snorkel 进行弱监督标注
- en: 'The IMDb dataset has 50,000 unlabeled reviews. This is double the size of the
    training set, which has 25,000 labeled reviews. As explained in the previous section,
    we have reserved 23,000 records from the training data in addition to the unsupervised
    set for weakly supervised labeling. Labeling records in Snorkel is performed via
    labeling functions. Each labeling function can return one of the possible labels
    of abstain from labeling. Since this is a binary classification problem, corresponding
    constants are defined. A sample labeling function is also shown. All the code
    for this section can be found in the notebook titled `snorkel-labeling.ipynb`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb 数据集包含 50,000 条未标记的评论。这是训练集大小的两倍，训练集有 25,000 条已标记的评论。如前一部分所述，我们从训练数据中预留了
    23,000 条记录，除了用于弱监督标注的无监督集。Snorkel 中的标注是通过标注函数来完成的。每个标注函数可以返回可能的标签之一，或者选择不进行标注。由于这是一个二分类问题，因此定义了相应的常量。还展示了一个示例标注函数。此部分的所有代码可以在名为
    `snorkel-labeling.ipynb` 的笔记本中找到：
- en: '[PRE60]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Labeling functions are annotated with a `labeling_function()` provided Snorkel.
    Note that the Snorkel library needs to be installed. Detailed instructions can
    be found on GitHub in this chapter''s subdirectory. In short, Snorkel can be installed
    by:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 标注函数由 Snorkel 提供的 `labeling_function()` 注解。请注意，需要安装 Snorkel 库。详细的安装说明可以在 GitHub
    上本章的子目录中找到。简而言之，Snorkel 可以通过以下方式安装：
- en: '[PRE61]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Any warnings you see can be safely ignored as the library uses different versions
    of components such as TensorBoard. To be doubly sure, you can create a separate
    `conda`/virtual environment for Snorkel and its dependencies.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您看到的任何警告都可以安全忽略，因为该库使用了不同版本的组件，如 TensorBoard。为了更加确定，您可以为 Snorkel 及其依赖项创建一个单独的
    `conda`/虚拟环境。
- en: This chapter would not have been possible without the support of the Snorkel.ai
    team. Frederic Sala and Alexander Ratner from Snorkel.ai were instrumental in
    providing guidance and the script for hyperparameter tuning to get the most out
    of Snorkel.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的内容得以实现，离不开 Snorkel.ai 团队的支持。Snorkel.ai 的 Frederic Sala 和 Alexander Ratner
    在提供指导和用于超参数调优的脚本方面发挥了重要作用，从而最大限度地发挥了 Snorkel 的效能。
- en: 'Coming back to the labeling function, the function above is expecting a row
    from a DataFrame. It is expecting that the row has a text "review" column. This
    function tries to see if the review states that the movie or show was a waste
    of time. If so, it returns a negative label; else, it abstains from labeling the
    row of data. Note that we are trying to label thousands of rows of data in a short
    time using these labeling functions. The best way to do this is to print some
    random samples of positive and negative reviews and use some words from the text
    as labeling functions. The central idea here is to create a number of functions
    that have good accuracy for a subset of the rows. Let''s examine some negative
    reviews in the training set to see what labeling functions can be created:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 回到标签函数，以上的函数期望来自DataFrame的一行数据。它期望该行数据包含“review”这一文本列。该函数尝试检查评论是否表示电影或节目是浪费时间。如果是，它返回负面标签；否则，它不会对该行数据进行标记。请注意，我们正在尝试使用这些标签函数在短时间内标记数千行数据。实现这一目标的最佳方法是打印一些随机的正面和负面评论样本，并使用文本中的某些词汇作为标签函数。这里的核心思想是创建多个函数，它们能对子集行数据有较好的准确性。让我们检查训练集中一些负面评论，看看可以创建哪些标签函数：
- en: '[PRE62]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'One of the reviews starts off as "A very cheesy and dull road movie," which
    gives an idea for a labeling function:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一条评论开头是“一个非常俗气且乏味的公路电影”，这为标签函数提供了一个想法：
- en: '[PRE63]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'There are a number of different words that occur in negative reviews. Here
    is a subset of negative labeling functions. The full list is in the notebook:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 负面评论中出现了许多不同的词汇。这是负面标签函数的一个子集，完整列表请参见笔记本：
- en: '[PRE64]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'All the negative labeling functions are added to a list:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 所有负面标签函数都添加到一个列表中：
- en: '[PRE65]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Examining a sample of negative reviews can give us many ideas. Typically, a
    small amount of effort from a domain expert can yield multiple labeling functions
    that can be implemented easily. If you have ever watched a movie, you are an expert
    as far as this dataset is concerned. Examining a sample of positive reviews results
    in more labeling functions. Here is a sample of labeling functions that identify
    positive sentiment in reviews:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 检查负面评论的样本可以给我们很多想法。通常，领域专家的一点小小努力就能产生多个易于实现的标签函数。如果你曾经看过电影，那么对于这个数据集来说，你就是专家。检查正面评论的样本会导致更多的标签函数。以下是识别评论中正面情感的标签函数示例：
- en: '[PRE66]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'All of the positive labeling functions can be seen in the notebook. Similar
    to the negative functions, a list of the positive labeling functions is defined:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 所有正面标签函数可以在笔记本中看到。与负面函数类似，定义了正面标签函数的列表：
- en: '[PRE67]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The development of labeling is an iterative process. Don''t be intimidated
    by the number of labeling functions shown here. You can see that they are quite
    simple, for the most part. To help you understand the amount of effort, I spent
    a total of 3 hours on creating and testing labeling functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 标签的开发是一个迭代过程。不要被这里显示的标签函数数量吓到。你可以看到，它们大部分都非常简单。为了帮助你理解工作量，我花费了总共3小时来创建和测试标签函数：
- en: Note that the notebook contains a large number of simple labeling functions,
    of which only a subset are shown here. Please refer to the actual code for all
    the labeling functions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，笔记本中包含了大量简单的标签函数，这里仅展示了其中的一个子集。请参考实际代码获取所有标签函数。
- en: The process involved looking at some samples and creating the labeling functions,
    followed by evaluating the results on a subset of the data. Checking out examples
    of where the labeling functions disagreed with the labeled examples was very useful
    in making functions narrower or adding compensating functions. So, let's see how
    we can evaluate these functions so we can iterate on them.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程包括查看一些样本并创建标签函数，然后在数据的子集上评估结果。查看标签函数与标记示例不一致的示例，对于使函数更精确或添加补偿函数非常有用。那么，让我们看看如何评估这些函数，以便进行迭代。
- en: Iterating on labeling functions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对标签函数进行迭代
- en: 'Once a set of labeling functions are defined, they can be applied to a pandas
    DataFrame, and a model can be trained to compute the weights assigned to various
    labeling functions while computing the labels. Snorkel provides functions that
    help with these tasks. First, let us apply these labeling functions to compute
    a matrix. This matrix has as many columns as there are labeling functions for
    every row of data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了一组标注函数，它们就可以应用于 pandas DataFrame，并且可以训练一个模型来计算在计算标签时分配给各个标注函数的权重。Snorkel
    提供了帮助执行这些任务的函数。首先，让我们应用这些标注函数来计算一个矩阵。这个矩阵的列数等于每一行数据的标注函数数量：
- en: '[PRE68]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'In the code above, a sample of 1000 rows of data from the training data is
    extracted. Then, the list of all labeling functions created previously is passed
    to Snorkel and applied to this sample of training data. If we created 25 labeling
    functions, the shape of `L_train` would be (1000, 25). Each column represents
    the output of a labeling function. A generative model can now be trained on this
    label matrix:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，从训练数据中提取了1000行数据样本。然后，将之前创建的所有标注函数列表传递给 Snorkel，并应用于这个训练数据的样本。如果我们创建了25个标注函数，那么
    `L_train` 的形状将会是 (1000, 25)。每一列代表一个标注函数的输出。现在可以在这个标签矩阵上训练一个生成模型：
- en: '[PRE69]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: A `LabelModel` instance is created with a parameter specifying how many labels
    are in the actual model. This model is then trained, and labels are predicted
    for the subset of data. These predicted labels are added as a new column to the
    DataFrame. Note the `tie_break_policy` parameter being passed into the `predict()`
    method. In case the model has conflicting outputs from labeling functions, and
    they have the same scores from the model, this parameter specifies how the conflict
    should be resolved. Here, we instruct the model to abstain from labeling the records
    in case of a conflict. Another possible setting is "random," where the model will
    randomly assign the output from one of the tied labeling functions. The main difference
    between these two options, in the context of the problem at hand, is precision.
    By asking the model to abstain from labeling, we get higher precision results,
    but fewer records will be labeled. Randomly choosing one of the functions that
    were tied results in higher coverage, but presumably at lower quality. This hypothesis
    can be tested by training the same model with the outputs of the two options separately.
    You are encouraged to try these options and see the results for yourself.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `LabelModel` 实例，参数指定实际模型中有多少个标签。然后训练这个模型，并为数据子集预测标签。这些预测的标签将作为 DataFrame
    的新列添加进去。请注意传递给 `predict()` 方法的 `tie_break_policy` 参数。如果模型在标注函数的输出中有冲突，并且这些冲突在模型中得分相同，该参数指定如何解决冲突。在这里，我们指示模型在发生冲突时放弃标记记录。另一个可能的设置是
    "random"，在这种情况下，模型将随机分配一个被绑定标注函数的输出。这两个选项在手头问题的背景下的主要区别是精确度。通过要求模型在标记时放弃，我们可以获得更高的精确度结果，但标记的记录会更少。随机选择一个被绑定函数的输出会导致更广泛的覆盖率，但质量可能较低。可以通过分别使用这两个选项的输出来训练同一个模型来测试这一假设。鼓励您尝试这些选项并查看结果。
- en: 'Since the abstain policy was chosen, all of the 1000 rows may not have been
    labeled:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于选择了放弃策略，可能并没有对所有的1000行进行标注：
- en: '[PRE70]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE71]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE72]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '|'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE73]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE74]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE75]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE76]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE77]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Out of 1000 records, only 458 were labeled. Let''s check how many of these
    were labeled incorrectly:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在1000条记录中，只有458条被标记。让我们检查其中有多少条标记错误的。
- en: '[PRE78]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '|  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE79]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE80]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE81]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '|'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE82]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE83]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '|'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE84]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE85]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Snorkel, armed with our labeling functions, labeled 598 records, out of which
    434 labels were correct and 164 records were incorrectly labeled. The label model
    has an accuracy of ~72.6%. To get inspiration for more labeling functions, you
    should inspect a few of the rows where the label model produced the wrong results
    and update or add labeling functions. As mentioned above, a total of approximately
    3 hours was spent on iterating and creating labeling functions to get a total
    of 25 functions. To get more out of Snorkel, we need to increase the amount of
    training data. The objective is to develop a method that gets us many labels quickly,
    without a lot of manual effort. One technique that can be used in this specific
    case is training a simple Naïve-Bayes model to get words that are highly correlated
    with positive or negative labels. This is the focus of the next section. **Naïve-Bayes**
    (**NB**) is a basic technique covered in many basic NLP books.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Snorkel和我们的标注函数标注了598条记录，其中434条标签正确，164条记录被错误标注。标签模型的准确率约为72.6%。为了获得更多标注函数的灵感，你应该检查一些标签模型产生错误结果的行，并更新或添加标注函数。如前所述，约花了3个小时迭代并创建了25个标注函数。为了从Snorkel中获得更多效果，我们需要增加训练数据量。目标是开发一种方法，快速获得大量标签，而不需要太多人工干预。在这个特定案例中，可以使用的一种技术是训练一个简单的朴素贝叶斯模型，获取与正面或负面标签高度相关的词汇。这是下一节的重点。**朴素贝叶斯**（**NB**）是许多基础NLP书籍中涵盖的一种基本技术。
- en: Naïve-Bayes model for finding keywords
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于寻找关键词的朴素贝叶斯模型
- en: Building an NB model on this dataset takes under an hour and has the potential
    to significantly increase the quality and coverage of the labeling functions.
    The core model code for the NB model can be found in the `spam-inspired-technique-naive-bayes.ipynb`
    notebook. Note that these explorations are aside from the main labeling code,
    and this section can be skipped if desired, as the learnings from this section
    are applied to construct better labeling functions outlined in the `snorkel-labeling.ipynb`
    notebook.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上构建NB模型需要不到一个小时，并且有潜力显著提高标注函数的质量和覆盖面。NB模型的核心代码可以在`spam-inspired-technique-naive-bayes.ipynb`笔记本中找到。请注意，这些探索与主要的标注代码无关，如果需要，可以跳过这一部分，因为这一部分的学习成果会应用于构建更好的标注函数，这些函数在`snorkel-labeling.ipynb`笔记本中有详细介绍。
- en: The main flow of the NB-based exploration is to load the reviews, remove stop
    words, take the top 2,000 words to construct a simple vectorization scheme, and
    train an NB model. Since data loading is the same as covered in previous sections,
    the details are skipped in this section.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NB的探索的主要流程是加载评论、去除停用词、选择前2000个词来构建一个简单的向量化方案，并训练一个NB模型。由于数据加载与前面章节讲解的相同，因此本节省略了细节。
- en: 'This section uses the NLTK and `wordcloud` Python packages. NLTK should already
    be installed as we have used it in *Chapter 1*, *Essentials of NLP*. `wordcloud`
    can be installed with:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用了NLTK和`wordcloud` Python包。由于我们在*第1章*《NLP基础》中已经使用过NLTK，它应该已经安装。`wordcloud`可以通过以下命令安装：
- en: '`(tf24nlp) $ pip install wordcloud==1.8`'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf24nlp) $ pip install wordcloud==1.8`'
- en: 'Word clouds help get an aggregate understanding of the positive and negative
    review text. Note that counters are required for the top-2000 word vectorization
    scheme. A convenience function that cleans HTML text along with removing stop
    words and tokenizing the rest into a list is defined as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 词云有助于整体理解正面和负面评论的文本。请注意，前2000个词的向量化方案需要计数器。定义了一个方便的函数，它清理HTML文本，并删除停用词，将其余部分进行分词，代码如下：
- en: '[PRE86]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Then, the positive reviews are separated and a word cloud is generated for
    visualization purposes:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，正面评价被分离出来，并生成一个词云以便于可视化：
- en: '[PRE87]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The output of the preceding code is shown in *Figure 8.3*:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如*图 8.3*所示：
- en: '![A close up of a sign  Description automatically generated](img/B16252_08_03.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of a sign  Description automatically generated](img/B16252_08_03.png)'
- en: 'Figure 8.3: Positive reviews word cloud'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：正面评价词云
- en: 'It is not surprising that *movie* and *film* are the biggest words. However,
    there are a number of other suggestions for keywords that can be seen here. Similarly,
    a word cloud for the negative reviews can be generated, as shown in *Figure 8.4*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 不足为奇的是，*movie* 和 *film* 是最大的词。然而，在这里可以看到许多其他建议的关键词。类似地，也可以生成负面评价的词云，如*图 8.4*所示：
- en: '![A close up of a sign  Description automatically generated](img/B16252_08_04.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of a sign  Description automatically generated](img/B16252_08_04.png)'
- en: 'Figure 8.4: Negative reviews word cloud'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：负面评价词云
- en: 'These visualizations are interesting; however, a clearer picture will emerge
    after training the model. Only the top 2,000 words are needed for training the
    model:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这些可视化结果很有趣；然而，训练模型后会得到更清晰的画面。只需要前2000个单词就可以训练模型：
- en: '[PRE88]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Combined counters show the top 10 most frequently appearing words in all reviews.
    These are extracted into a list:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 合并计数器显示了所有评论中最常出现的前10个单词。这些被提取到一个列表中：
- en: '[PRE90]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The vectorization of each review is fairly simple – each of the 2000 words
    becomes a column for a given review. If the word represented by the column is
    present in the review, the value of the column is marked as 1 for that review,
    or 0 otherwise. So, each review is represented by a sequence of 0s and 1s representing
    which of the top 2000 words the review contained. The code below shows this transformation:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 每个评论的向量化相当简单——每个2000个单词中的一个都会成为给定评论的一个列。如果该列所表示的单词出现在评论中，则该列的值标记为1，否则为0。所以，每个评论由一个由0和1组成的序列表示，表示该评论包含了哪些前2000个单词。下面的代码展示了这一转换：
- en: '[PRE91]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Training the model is quite trivial. Note that the Bernoulli NB model is used
    here as each word is represented according to its presence or absence in the review.
    Alternatively, the frequency of the word in the review could also be used. If
    the frequency of the word is used while vectorizing the review above, then the
    multinomial form of NB should be used.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型相当简单。请注意，这里使用的是伯努利朴素贝叶斯模型，因为每个单词都是根据它在评论中是否存在来表示的。或者，也可以使用单词在评论中的频率。如果在上述评论向量化过程中使用单词的频率，那么应该使用朴素贝叶斯的多项式形式。
- en: 'NLTK also provides a way to inspect the most informative features:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK还提供了一种检查最具信息性特征的方法：
- en: '[PRE92]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This whole exercise was done to find which words are most useful in predicting
    negative and positive reviews. The table above shows the words and the likelihood
    ratios. Taking the first row of the output for the word *unfunny* as an example,
    the model is saying that reviews containing *unfunny* are negative 14.1 times
    more often than they are positive. The labeling functions are updated using a
    number of these keywords.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程是为了找出哪些单词对预测负面和正面评论最有用。上面的表格显示了这些单词及其可能性比率。以输出的第一行中的单词*unfunny*为例，模型表示含有*unfunny*的评论比含有它的正面评论要负面14.1倍。标签函数是通过这些关键词更新的。
- en: 'Upon analyzing the labels assigned by the labeling functions in `snorkel-labeling.ipynb`,
    it can be seen that more negative reviews are being labeled as compared to positive
    reviews. Consequently, the labeling functions use a larger list of words for positive
    labels as compared to negative labels. Note that imbalanced datasets have issues
    with overall training accuracy and specifically with recall. The following code
    fragment shows augmented labeling functions using the keywords discovered through
    NB above:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析`snorkel-labeling.ipynb`中标签函数分配的标签后，可以看到负面评论比正面评论标注得更多。因此，标签函数为正面标签使用的单词列表比负面标签使用的单词列表要大。请注意，数据集不平衡会影响整体训练准确性，特别是召回率。以下代码片段展示了使用上述通过朴素贝叶斯发现的关键词来增强标签函数：
- en: '[PRE94]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Since keyword-based labeling functions are quite common, Snorkel provides an
    easy way to define such functions. The following code fragment uses two programmatic
    ways of converting a list of words into a set of labeling functions:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于关键字的标签函数非常常见，Snorkel提供了一种简单的方法来定义这样的函数。以下代码片段使用两种编程方式将单词列表转换为标签函数集合：
- en: '[PRE95]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The first function does the simple matching and returns the specific label,
    or it abstains. Check out the `snorkel-labeling.ipynb` file for the full list
    of labeling functions that were iteratively developed. All in all, I spent approximately
    12-14 hours on labeling functions and investigations.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数进行简单匹配并返回特定标签，或者选择不标注。请查看`snorkel-labeling.ipynb`文件，了解迭代开发的完整标签函数列表。总的来说，我花了大约12-14小时进行标签函数的开发和研究。
- en: Before we try to train the model using this data, let us evaluate the accuracy
    of this model on the entire training data set.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试使用这些数据训练模型之前，让我们先评估一下该模型在整个训练数据集上的准确性。
- en: Evaluating weakly supervised labels on the training set
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估训练集上的弱监督标签
- en: 'We apply the labeling functions and train a model on the entire training dataset
    just to evaluate the quality of this model:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用标签函数并在整个训练数据集上训练一个模型，只是为了评估该模型的质量：
- en: '[PRE96]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Our set of labeling functions covers 83.4% of the 25,000 training records,
    with 85.6% correct labels. Snorkel provides the ability to analyze the performance
    of each labeling function:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的标签函数集覆盖了25,000条训练记录中的83.4%，其中85.6%是正确标签。Snorkel提供了分析每个标签函数表现的能力：
- en: '[PRE98]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Note that a snipped version of the output has been presented here. The full
    output is available in the notebook. For each labeling function, the table presents
    what labels are produced and the coverage of the function – that is, the fraction
    of records it provides a label for, the fraction where it overlaps with another
    function producing the same label, and the fraction where it conflicts with another
    function producing a different label. A positive and a negative label function
    are highlighted. The `bad_acting()` function covers 8.7% of the records but overlaps
    with other functions about 8.3% of the time. However, it conflicts with a function
    producing a positive label about 4.3% of the time. The `amazing()` function covers
    about 5% of the dataset. It conflicts about 2.3% of the time. This data can be
    used to fine-tune specific functions further and examine how we''ve separated
    the data. *Figure 8.5* shows the balance between positive, negative, and abstain
    labels:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里展示的是输出的简短版本。完整的输出可以在笔记本中查看。对于每个标签函数，表格展示了其产生的标签和函数的覆盖范围——即，它为多少条记录提供标签，多少条记录与其他函数产生相同标签重叠，以及多少条记录与其他函数产生不同标签冲突。正标签和负标签函数已被突出显示。`bad_acting()`函数覆盖了8.7%的记录，但大约8.3%的时间与其他函数重叠。然而，它与产生正标签的函数发生冲突的时间大约是4.3%。`amazing()`函数覆盖了大约5%的数据集，发生冲突的时间大约为2.3%。这些数据可以用来进一步微调特定的函数，并检查我们如何划分数据。*图8.5*展示了正标签、负标签和弃权标签之间的平衡：
- en: '![A screen shot of a social media post  Description automatically generated](img/B16252_08_05.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![一张社交媒体帖子的截图 说明自动生成](img/B16252_08_05.png)'
- en: 'Figure 8.5: Distribution of labels generated by Snorkel'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：Snorkel生成的标签分布
- en: Snorkel has several options for hyperparameter tuning to improve the quality
    of labeling even further. We execute a grid search over the parameters to find
    the best training parameters, while we exclude the labeling functions that are
    adding noise in the final output.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel提供了几种超参数调优的选项，以进一步提高标签的质量。我们通过网格搜索参数来找到最佳训练参数，同时排除那些在最终输出中增加噪声的标签函数。
- en: 'Hyperparameter tuning is done via choosing different learning rates, L2 regularizations,
    numbers of epochs to run training on, and optimizers to use. Finally, a threshold
    is used to determine which labeling functions should be kept for the actual labeling
    task:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优通过选择不同的学习率、L2正则化、训练轮数和使用的优化器来进行。最后，通过设定阈值来决定哪些标签函数应该保留用于实际的标签任务：
- en: '[PRE100]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Snorkel may print a warning that metrics are being calculated over non-abstain
    labels only. This is by design, as we are interested in high-confidence labels.
    If there is a conflict between labeling functions, then our model abstains from
    giving it a label. The best parameters printed out are:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel可能会打印出一个警告，提示指标仅在非弃权标签上进行计算。这是设计使然，因为我们关注的是高置信度的标签。如果标签函数之间存在冲突，我们的模型会选择放弃为其提供标签。打印出的最佳参数是：
- en: '[PRE101]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Through this tuning, the accuracy of the model improved from 78.5% to 84%!
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个调优，模型的准确率从78.5%提高到了84%！
- en: 'Using these parameters, we label the 23k records from the training set and
    50k records from the unsupervised set. For the first part, we label all the 25k
    training records and then split them into two sets. This particular part of splitting
    was referenced in the baseline model section above:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们标注了来自训练集的23k条记录和来自无监督集的50k条记录。对于第一部分，我们标注了所有25k条训练记录，并将其分成两组。这个特定的分割部分在上面的基准模型部分中提到过：
- en: '[PRE102]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'The last two lines of code inspect the state of the labels and contrasts with
    actual labels and generate the graph shown in *Figure 8.6*:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后两行检查标签的状态，并与实际标签进行对比，生成*图8.6*中所示的图表：
- en: '![A picture containing screen, building, drawing, food  Description automatically
    generated](img/B16252_08_06.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含屏幕、建筑、绘画和食物的图片 说明自动生成](img/B16252_08_06.png)'
- en: 'Figure 8.6: Comparison of labels in the training set versus labels generated
    using Snorkel'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：训练集中标签与使用Snorkel生成的标签的对比
- en: 'When the Snorkel model abstains from labeling, it assigns -1 for the label.
    We see that the model is able to label a lot more negative reviews than positive
    labels. We filter out the rows where Snorkel abstained from labeling and saved
    the records:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 当Snorkel模型放弃标签时，它会为标签分配-1。我们看到模型能够标注更多的负面评论，而不是正面标签。我们过滤掉Snorkel放弃标注的行并保存记录：
- en: '[PRE103]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: However, the key question that we face is that if we augmented the training
    data with these noisy labels, which are 84% accurate, would it make our model
    perform better or worse? Note that the baseline model had an accuracy of ~74%.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们面临的关键问题是，如果我们用这些噪声标签（准确率为84%）增强训练数据，这会使我们的模型表现更好还是更差？请注意，基准模型的准确率大约为74%。
- en: To answer this question, we label the unsupervised set and then train the same
    model architecture as the baseline.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们对无监督数据集进行标注，然后训练与基准相同的模型架构。
- en: Generating unsupervised labels for unlabeled data
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为未标记数据生成无监督标签
- en: 'As we saw in the previous section, where we labeled the training data set,
    it is quite simple to run the model on the unlabeled reviews of the dataset:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一部分看到的，我们对训练数据集进行了标注，运行模型在数据集的未标注评论上也非常简单：
- en: '[PRE104]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Now the label model is trained, and predictions are added to an additional column
    of the unsupervised dataset. The model labels 29,583 records out of 50,000\. This
    is almost equal to the size of the training dataset. Assuming that the error rate
    on the unsupervised set is similar to that observed on the training set, we just
    added ~24,850 records with correct labels and ~4,733 records with incorrect labels
    into the training set. However, the balance of this dataset is very tilted, as
    positive label coverage is still poor. There are approximately 9,000 positive
    labels for over 20,000 negative labels. The *Increase Positive Label Coverage*
    section of the notebook tries to further improve the coverage of the positive
    labels by adding more keyword functions.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在标签模型已经训练完成，预测结果被添加到无监督数据集的额外列中。模型对50,000条记录中的29,583条进行了标注。这几乎等于训练数据集的大小。假设无监督数据集的错误率与训练集上的错误率相似，我们就向训练集中添加了约24,850条正确标签的记录和约4,733条错误标签的记录。然而，这个数据集的平衡性非常倾斜，因为正面标签的覆盖仍然很差。大约有9,000个正面标签，覆盖超过20,000个负面标签。笔记本中的*增加正面标签覆盖率*部分通过添加更多关键词函数来进一步提高正面标签的覆盖率。
- en: 'This results in a slightly more balanced set, as shown in the following chart:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致数据集稍微更平衡，如下图所示：
- en: '![A screen shot of a social media post  Description automatically generated](img/B16252_08_07.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![A screen shot of a social media post  Description automatically generated](img/B16252_08_07.png)'
- en: 'Figure 8.7: Further improvements in labeling functions applied to the unsupervised
    dataset improves the positive labels'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：对无监督数据集应用进一步改进的标注函数，改善了正面标签的覆盖
- en: 'This dataset is saved to disk for use during training:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已保存到磁盘，以便在训练过程中使用：
- en: '[PRE105]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Labeled datasets are saved to disk and reloaded in the training code for better
    modularity and ease of readability. In a production pipeline, intermediate outputs
    may not be persisted and fed directly into the training steps. Another small consideration
    here is the separation of virtual/conda environments for running Snorkel. Having
    a separate script for weakly supervised labeling allows the use of a different
    Python environment as well.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 已标记的数据集保存到磁盘，并在训练代码中重新加载，以便更好的模块化和易于阅读。在生产管道中，可能不会持久化中间输出，而是直接将其馈入训练步骤。另一个小考虑因素是分离虚拟/conda环境来运行Snorkel。为弱监督标注创建一个单独的脚本也可以使用不同的Python环境。
- en: 'We switch our focus back to the `imdb-with-snorkel-labels.ipynb` notebook,
    which has the models for training. The code for this part begins from the section
    *With Snorkel Labeled Data*. The newly labeled records need to be loaded from
    disk, cleansed, vectorized, and padded before training can be run. We extract
    the labeled records and remove HTML markup, as shown below:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点重新放回到`imdb-with-snorkel-labels.ipynb`笔记本，该笔记本包含用于训练的模型。该部分的代码从*使用Snorkel标注数据*部分开始。新标注的记录需要从磁盘加载、清理、向量化并填充，才能进行训练。我们提取标注的记录并去除HTML标记，如下所示：
- en: '[PRE106]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: The original training dataset was balanced across positive and negative labels.
    However, there is an imbalance in the data labeled using Snorkel. We balance the
    dataset and ignore the excess rows with negative labels. Note that the 2,000 training
    records used in the baseline model also need to be added, resulting in a total
    of 33,914 training records. As mentioned before, it really shines when the amount
    of data is 10x to 50x the original dataset. Here, we achieve a ratio closer to
    17x, or 18x if the 2,000 training records are also included.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 原始训练数据集在正负标签上是平衡的。然而，使用Snorkel标记的数据存在不平衡问题。我们平衡数据集，并忽略那些多余的负标签记录。请注意，基准模型使用的2,000条训练记录也需要添加进去，总共得到33,914条训练记录。如前所述，当数据量是原始数据集的10倍到50倍时，这个方法非常有效。在这里，如果将2,000条训练记录也算在内，我们达到了接近17倍的比例，或者是18倍。
- en: '![A picture containing screen, orange, drawing  Description automatically generated](img/B16252_08_08.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screen, orange, drawing  Description automatically generated](img/B16252_08_08.png)'
- en: 'Figure 8.8: Distribution of records after using Snorkel and weak supervision'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：使用Snorkel和弱监督后的记录分布
- en: 'As shown in *Figure 8.8* above, the records in blue are dropped to balance
    the dataset. Next, the data needs to be cleansed and vectorized using the subword
    vocabulary:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图*Figure 8.8*所示，蓝色的记录被丢弃以平衡数据集。接下来，数据需要使用子词词汇进行清洗和向量化：
- en: '[PRE107]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Finally, we convert the pandas DataFrames into TensorFlow data sets and vectorize
    and pad them:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将pandas DataFrame转换为TensorFlow数据集，并进行向量化和填充：
- en: '[PRE108]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: We are ready to try training our BiLSTM model to see if the performance improves
    on this task.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好尝试训练我们的BiLSTM模型，看看它是否能在这个任务上提高性能。
- en: Training BiLSTM on weakly supervised data from Snorkel
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Snorkel的弱监督数据上训练BiLSTM
- en: To ensure we are comparing apples to apples, we use the same BiLSTM as the baseline
    model. We instantiate a model with 64-dimensional embeddings, 64 RNN units, and
    a batch size of 100\. The model uses the binary cross-entropy loss and the Adam
    optimizer. Accuracy, precision, and recall are tracked as the model is trained.
    An important step is to shuffle the datasets every epoch to help the model keep
    errors to a minimum.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们在比较相同的事物，我们使用与基准模型相同的BiLSTM。我们实例化一个模型，嵌入维度为64，RNN单元数为64，批量大小为100。该模型使用二元交叉熵损失和Adam优化器。模型训练过程中，会跟踪准确率、精确度和召回率。一个重要的步骤是每个周期都打乱数据集，以帮助模型保持最小的误差。
- en: 'This is an important concept. Deep models work on the assumption that the loss
    is a convex surface, and the gradient is descending to the bottom of this surface.
    The surface has many local minima or saddle points in reality. If the model gets
    stuck in local minima during a mini-batch, it will be hard for the model to come
    out of it as across epochs, it receives the same data points again and again.
    Shuffling the data changes the data set and the order in which the model receives
    it. This enables the model to learn better by getting out of these local minima
    faster. The code for this section is in the `imdb-with-snorkel-labels.ipynb` file:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个重要的概念。深度模型基于假设损失是一个凸面，梯度沿着这个凸面下降到底部。实际上，这个面有很多局部最小值或鞍点。如果模型在一个小批次中陷入局部最小值，由于跨越多个周期时，模型不断接收到相同的数据点，它将很难从局部最小值中跳出来。通过打乱数据，改变数据集及其顺序，模型可以更好地学习，从而更快地跳出这些局部最小值。此部分的代码位于`imdb-with-snorkel-labels.ipynb`文件中：
- en: '[PRE109]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Note that we cache all the records that will be part of the batch so that we
    can get perfect buffering. This comes at the cost of slightly slower training
    and higher memory use. Also, since our batch size is 100 and the dataset has 35,914
    records, we drop the remainder of the records. We train the model for 20 epochs,
    a little more than the baseline model. The baseline model was overfitting at 15
    epochs. So, it was not useful to train it longer. This model has a lot more data
    to train on. Consequently, it needs more epochs to learn:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们缓存了所有将成为批次一部分的记录，以便实现完美的缓冲。这会带来训练速度略微变慢和内存使用量增加的代价。同时，由于我们的批量大小是100，数据集有35,914条记录，我们丢弃了剩余的记录。我们训练模型20个周期，略多于基准模型。基准模型在15个周期时就发生了过拟合，因此再训练也没有意义。这个模型有更多的数据可以训练，因此需要更多的周期来学习：
- en: '[PRE110]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'The model achieves an accuracy of 98.9%. The precision and recall numbers are
    quite close to each other. Evaluating the baseline model on the test data gave
    an accuracy score of 76.23%, which clearly proved that it was overfitting to the
    training data. Upon evaluating the model trained with weakly supervised labeling,
    the following results are obtained:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型达到了98.9%的准确率。精确度和召回率的数值非常接近。在测试数据上评估基线模型时，得到了76.23%的准确率，这明显证明它对训练数据发生了过拟合。当评估使用弱监督标签训练的模型时，得到以下结果：
- en: '[PRE112]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: This model trained on weakly supervised noisy labels achieves 76.6% accuracy,
    which is 0.7%% higher than baseline mode. Also note that the precision went from
    74.5% to 78.1% but recall decreased. In this toy setting, we kept a lot of the
    variables constant, such as model type, dropout ratio, etc. In a realistic setting,
    we can drive the accuracy even higher by optimizing the model architecture and
    hyperparameter tuning. There are other options to try. Recall that we instruct
    Snorkel to abstain from labeling if it is unsure.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个在弱监督噪声标签上训练的模型达到了76.6%的准确率，比基线模型高出0.7%。还要注意，精确度从74.5%提高到了78.1%，但召回率有所下降。在这个玩具设置中，我们保持了许多变量不变，例如模型类型、dropout比例等。在实际环境中，我们可以通过优化模型架构和调整超参数进一步提高准确率。还有其他选项可以尝试。记住，我们指示Snorkel在不确定时避免标注。
- en: By changing that to a majority vote or some other policy, the amount of training
    data could be increased even further. You could also try and train on unbalanced
    datasets and see the impact. The focus here was on showing the value of weak supervision
    for massively increasing the amount of training data rather than building the
    best model. However, you should be able to take these lessons and apply them to
    your projects.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将其改为多数投票或其他策略，训练数据的量可以进一步增加。你也可以尝试在不平衡的数据集上训练，看看其影响。这里的重点是展示弱监督在大规模增加训练数据量方面的价值，而不是构建最佳模型。然而，你应该能够将这些教训应用到你的项目中。
- en: It is important to take a moment and think about the causes of this result.
    There are a few important deep learning lessons hidden in this story. First, more
    labeled data is always good, given a model of sufficient complexity. There is
    a correlation between the amount of data and model capacity. Models with higher
    capacities can handle more complex relationships in the data. They also need much
    larger datasets to learn the complexities. However, if the model is kept a constant
    and with sufficient capacity, the quantity of labeled data makes a huge difference,
    as evidenced here. There are some limits to how much of an improvement we can
    achieve by increasing labeled data scale. In a paper titled *Revisiting Unreasonable
    Effectiveness of Data in Deep Learning Era* by Chen Sun et al., published at ICCV
    2017, the authors examine the role of data in the computer vision domain. They
    report that the performance of models increases logarithmically with an increase
    in training data. The second result they report is that learning representations
    through pretraining helps downstream tasks quite a bit. Techniques in this chapter
    can be applied to generate more data for the fine-tuning step, which will significantly
    boost the performance of the fine-tuned model.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 花些时间思考结果的原因是非常重要的。在这个故事中隐藏着一些重要的深度学习教训。首先，在模型复杂度足够的情况下，更多的标注数据总是好的。数据量和模型容量之间是有相关性的。高容量的模型可以处理数据中的更复杂关系，同时也需要更大的数据集来学习这些复杂性。然而，如果模型保持不变且容量足够，标注数据的数量就会产生巨大的影响，正如这里所证明的那样。通过增加标注数据量，我们能提高的效果是有限的。在Chen
    Sun等人于ICCV 2017发布的论文《*重新审视深度学习时代数据的非理性有效性*》中，作者研究了数据在计算机视觉领域中的作用。他们报告说，随着训练数据量的增加，模型的性能呈对数增长。他们报告的第二个结果是，通过预训练学习表示会显著帮助下游任务。本章中的技术可以应用于生成更多的数据用于微调步骤，这将大大提升微调模型的性能。
- en: The second lesson is one about the basics of machine learning – shuffling the
    training data set has a disproportionate impact on the performance of the model.
    In the book, we have not always done this in order to manage training times. For
    training production models, it is important to focus on basics such as shuffling
    data sets before each epoch.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个教训是关于机器学习基础的——训练数据集的打乱对模型性能有不成比例的影响。在本书中，我们并不是总这样做，以便管理训练时间。对于训练生产模型，重要的是关注一些基础，如在每个epoch之前打乱数据集。
- en: Let's review everything we learned in this chapter.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下在这一章中学到的内容。
- en: Summary
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: It is apparent that deep models perform very well when they have a lot of data.
    BERT and GPT models have shown the value of pre-training on massive amounts of
    data. It is still very hard to get good-quality labeled data for use in pretraining
    or fine-tuning. We used the concepts of weak supervision combined with generative
    models to cheaply label data. With relatively small amounts of effort, we were
    able to multiply the amount of training data by 18x. Even though the additional
    training data was noisy, the BiLSTM model was able to learn effectively and beat
    the baseline model by 0.6%.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，深度模型在拥有大量数据时表现非常好。BERT和GPT模型展示了在大量数据上进行预训练的价值。对于预训练或微调，获得高质量的标注数据仍然非常困难。我们结合弱监督学习和生成模型的概念，以低成本标注数据。通过相对较少的努力，我们能够将训练数据量扩展18倍。即使额外的训练数据存在噪声，BiLSTM模型仍然能够有效学习，并比基线模型提高了0.6%。
- en: Representation learning or pre-training leads to transfer learning and fine-tuning
    models performing well on their downstream tasks. However, in many domains like
    medicine, the amount of labeled data may be small or quite expensive to acquire.
    Using the techniques learned in this chapter, the amount of training data can
    be expanded rapidly with little effort. Building a state-of-the-art- beating model
    helped recall some basic lessons in deep learning, such as how larger data boosts
    performance quite a bit, and that larger models are not always better.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习或预训练会导致迁移学习，并使微调模型在下游任务中表现良好。然而，在许多领域，如医学，标注数据的数量可能较少或获取成本较高。运用本章所学的技术，训练数据的数量可以通过少量的努力迅速扩展。构建一个超越当前最先进水平的模型有助于回顾一些深度学习的基本经验教训，比如更大的数据如何显著提升性能，以及更大的模型并不总是更好的。
- en: Now, we turn our focus to conversational AI. Building a conversational AI system
    is a very challenging task with many layers. The material covered so far in the
    book can help in building various parts of chatbots. The next chapter goes over
    the key parts of conversational AI or chatbot systems and outlines effective ways
    to build them.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将重点转向对话式人工智能。构建对话式AI系统是一项非常具有挑战性的任务，涉及多个层面。到目前为止，本书所涵盖的内容可以帮助构建聊天机器人系统的各个部分。下一章将介绍对话式AI或聊天机器人系统的关键组成部分，并概述构建它们的有效方法。
