- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Risks and Attacks on ML Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对机器学习模型的风险与攻击
- en: This chapter gives a detailed overview of defining and evaluating a **Machine
    Learning** (**ML**) risk framework from the instant an organization plans to embark
    on AI digital transformation. Risks may come in different stages, such as when
    the strategic or financial planning kicks in or during several of the execution
    phases. Risks start surfacing with the onset of technical implementations and
    continue up to testing phases when the AI use case is served to customers. Risk
    quantification can be attained through different metrics, which can certify the
    system behavior (amount of robustness and resiliency) against risks. In the process
    of understanding risk evaluation techniques, you will also get a thorough understanding
    of attacks and threats to ML models. In this context, you will discover different
    components of the system having security or privacy bottlenecks that pose external
    threats and make the model open to vulnerabilities. You will get to know the financial
    losses and business impacts when models deployed in production are not risk and
    threat resilient.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细概述了如何从组织计划开始进行AI数字化转型时，定义和评估**机器学习**（**ML**）风险框架。风险可能出现在不同阶段，例如在战略或财务规划启动时，或在执行的多个阶段中。随着技术实施的开始，风险开始浮现，并持续到测试阶段，当AI应用案例提供给客户时。风险量化可以通过不同的度量标准来实现，这些度量标准可以验证系统行为（鲁棒性和恢复力）对抗风险的能力。在理解风险评估技术的过程中，您还将深入了解机器学习模型的攻击和威胁。在此背景下，您将发现系统中存在安全或隐私瓶颈的不同组成部分，这些瓶颈会带来外部威胁，使模型暴露于漏洞之中。您将了解到当部署到生产环境中的模型没有足够的风险和威胁抗性时，所造成的财务损失和商业影响。
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过以下部分涵盖这些主题：
- en: Discovering risk elements
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现风险要素
- en: Exploring risk mitigation strategies with vision, strategy, planning, and metrics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索通过愿景、战略、规划和度量标准来降低风险的策略
- en: Assessing potential impact and loss due to attacks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估由于攻击而可能造成的影响和损失
- en: Discovering different types of attacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同类型的攻击
- en: Further, with the use of **Adversarial Robustness Toolbox** (**ART**) and AIJack,
    we will see how to design attacks for ML models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，借助**对抗鲁棒性工具箱**（**ART**）和AIJack，我们将看到如何为机器学习模型设计攻击。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8 along with some necessary Python
    packages, as follows. The commands to install ART and AIJack are also listed here:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求您安装Python 3.8以及一些必要的Python包，具体如下。安装ART和AIJack的命令也列在此处：
- en: Keras 2.7.0, TensorFlow 2.7.0
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.7.0，TensorFlow 2.7.0
- en: '`pip` `install adversarial-robustness-toolbox`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` `install adversarial-robustness-toolbox`'
- en: '`pip` `install git+https://github.com/Koukyosyumei/AIJack`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` `install git+https://github.com/Koukyosyumei/AIJack`'
- en: Discovering risk elements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现风险要素
- en: 'With rapid digitization and AI adoption, more and more organizations are becoming
    aware of the unintended consequences of malicious AI adoption practices. These
    can impact not only the organization’s reputation and long-term business outcomes
    but also the business’ customers and society at large. Here, let us look at the
    different risk elements involved in an AI digitization journey that CXOs, leadership
    teams, and technical and operational teams should be aware of. The purpose of
    these associated teams is one and the same: to avoid any of their systems getting
    compromised, or any security/privacy violations that could yield discrimination,
    accidents, the manipulation of political systems, or the loss of human life.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数字化转型和AI采纳的快速推进，越来越多的组织开始意识到恶意AI采纳实践的无意后果。这些后果不仅可能影响组织的声誉和长期商业成果，还可能影响到企业的客户以及整个社会。在这里，让我们看一下AI数字化转型过程中涉及的不同风险要素，CXOs、领导团队以及技术和运营团队应当关注这些要素。这些相关团队的目标是一致的：避免其系统受到攻击，或避免出现任何可能导致歧视、事故、政治系统操控或人命丧失的安全/隐私违规行为。
- en: '![Figure 1.1 – A diagram showing the AI risk framework](img/B18681_01_001.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1 – 展示AI风险框架的图表](img/B18681_01_001.jpg)'
- en: Figure 1.1 – A diagram showing the AI risk framework
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – 展示AI风险框架的图表
- en: 'There are three principal elements that govern the risk framework:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 风险框架有三个主要要素：
- en: '**Planning and execution**: This phase ideally covers all stages in product
    development, that is, the conceptualization of the AI use case, financial planning,
    execution, including the technical execution, and the design and release of the
    final product/solution from an initial **Minimum Viable** **Product** (**MVP**).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规划与执行**：这一阶段理想情况下涵盖了产品开发的所有阶段，即AI使用案例的构思、财务规划、执行，包括技术执行，以及从初始的**最小可行产品**（**MVP**）到最终产品/解决方案的设计与发布。'
- en: '**People and processes**: This is the most crucial factor as far as delivery
    timelines are concerned with respect to an MVP or a final product/solution. Leadership
    should have a clear vision and guidelines put in place so that research, technical,
    QA, and other operational teams find it easy to execute data and ML processes
    following defined protocols and standards.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人员与流程**：这是交付时间表中最关键的因素，涉及最小可行产品（MVP）或最终产品/解决方案。领导层应具有清晰的愿景和已制定的指导方针，以便研究、技术、质量保证及其他操作团队能够轻松地按照定义的协议和标准执行数据与ML过程。'
- en: '**Acceptance**: This phase involves several rounds of audits and confirmations
    to validate all steps of technical model design and deployment. This process adheres
    to extra confirmatory guidelines and laws in place to cautiously review and explain
    AI/ML model outcomes with due respect to user fairness and privacy to protect
    users’ confidential information.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验收**：这一阶段涉及多轮审计和确认，以验证技术模型设计和部署的所有步骤。该过程遵循额外的确认性指南和相关法律，以谨慎地审查和解释AI/ML模型的结果，同时尊重用户的公平性和隐私，保护用户的机密信息。'
- en: Let’s drill down into the components of each of these elements.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些元素的组成部分。
- en: Strategy risk
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 战略风险
- en: On the strategic front, there should be a prior **Strengths, Weaknesses, Opportunities,
    and Threats** (**SWOT**) analysis done on business use cases requiring digital
    AI transformations. The CXOs and leadership team must identify the right business
    use case after doing an impact versus effort analysis and formulate the guidelines
    and a list of coherent actions needed for execution. The absence of this might
    set infeasible initiatives that are not aligned with the organization’s business
    goals, causing financial loss and solutions failing. *Figure 1**.2* illustrates
    how a specific industry (say, retail) can classify different use cases based on
    a **value-effort framework**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在战略层面，应该在需要数字AI转型的业务用例上提前进行**优势、劣势、机会与威胁**（**SWOT**）分析。CXO和领导团队必须在进行影响与努力分析后识别出合适的业务用例，并制定执行所需的指导方针和一系列协调一致的行动。缺乏这些可能导致不切实际的举措，这些举措与组织的商业目标不一致，导致财务损失和解决方案失败。*图
    1.2* 展示了特定行业（例如零售）如何基于**价值-努力框架**对不同的用例进行分类。
- en: "![Figure 1.2 – \uFEFFA value-effort framework](img/B18681_01_002.jpg)"
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: "![图 1.2 – \uFEFF价值-努力框架](img/B18681_01_002.jpg)"
- en: Figure 1.2 – A value-effort framework
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 价值-努力框架
- en: 'If the guidelines and actions are not set properly, then AI systems can harm
    individuals, society, and organizations. The following are some examples:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指导方针和行动措施没有正确设定，AI系统可能会对个人、社会和组织造成危害。以下是一些例子：
- en: AI-powered autonomous vehicles can often malfunction, which can lead to injury
    or death.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于AI的自动驾驶车辆常常发生故障，这可能导致伤害或死亡。
- en: Over-reliance on inadequate equipment and insufficient monitoring mean predictive
    maintenance tasks can lead to worker injury.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度依赖不充分的设备和缺乏监控意味着预测性维护任务可能导致工人受伤。
- en: ML models misdiagnose medical conditions.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型误诊医疗状况。
- en: Political disruption by manipulating national institutional processes (for example,
    elections or appointments) by misrepresenting information.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过误导信息操纵国家制度过程（例如，选举或任命）引发的政治干扰。
- en: Data breaches can expose confidential military locations or technical secrets.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据泄露可能暴露机密的军事位置或技术机密。
- en: Infrastructure disruption or misuse by intelligent systems (for example, GPS
    routing cars through different streets often increases traffic flow in residential
    areas).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能系统造成的基础设施干扰或误用（例如，GPS导航汽车通过不同街道经常增加住宅区的交通流量）。
- en: Financial risk
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 财务风险
- en: The executive team should understand the finances involved in sponsoring an
    AI development project right from its inception to all stages of its development.
    Financial planning should not only consider the cost involved in hiring and retaining
    top talent but also the costs associated with infrastructure (cloud, containers,
    GPUs, and so on), data governance, and management tools. In addition, the financial
    roadmap should also specify the compliance necessary in big data and model deployment
    management as the risks and penalties can be huge in case of any violations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 高管团队应当了解从项目启动到开发各个阶段，资助一个 AI 开发项目所涉及的财务情况。财务规划不仅应考虑招聘和留住顶尖人才的成本，还应考虑基础设施（云、容器、GPU
    等）、数据治理和管理工具相关的成本。此外，财务路线图还应明确指定大数据和模型部署管理中所需的合规性，因为一旦发生任何违规，相关风险和处罚可能是巨大的。
- en: Technical risk
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术风险
- en: 'The risk associated on the technical front can manifest from the point when
    the data is ingested into the system. Data quality and the suitability of representation
    formats can seriously violate regulations (*Derisking machine learning and artificial
    intelligence*: [https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence](https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence)).
    Along with a skilled data science and big data team, what is needed is the availability
    and awareness of modern tools and practices that can detect and alert issues related
    to data or model quality and drifts and take timely remedial action.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 技术方面的风险可能在数据被摄入系统的时刻就开始显现。数据质量和表示格式的适用性可能严重违反相关法规（*去风险化机器学习与人工智能*：[https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence](https://www.mckinsey.com/business-functions/risk-and-resilience/our-insights/derisking-machine-learning-and-artificial-intelligence)）。除了一个熟练的数据科学和大数据团队，所需的还包括能够检测并提醒数据或模型质量和漂移问题并采取及时补救措施的现代工具和实践。
- en: '![Figure 1.3 – A diagram showing risk management controls](img/B18681_01_003.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 显示风险管理控制的示意图](img/B18681_01_003.jpg)'
- en: Figure 1.3 – A diagram showing risk management controls
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 显示风险管理控制的示意图
- en: '*Figure 1**.3* illustrates different risk elements that can cause security
    breaches or theft of confidential information. The different components (data
    aggregation, preprocessing, model development, deployment, and model serving)
    of a real-time AI pipeline must be properly designed, monitored (for AI drift,
    bias, changes in the characteristics of the retraining population, circuit breakers,
    and fallback options), and audited before running it in production.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.3* 展示了不同的风险元素，这些元素可能导致安全漏洞或机密信息的盗窃。一个实时 AI 流水线的不同组件（数据聚合、预处理、模型开发、部署和模型服务）必须在投入生产之前，经过适当的设计、监控（包括
    AI 漂移、偏差、重新训练人群特征变化、电路断路器和回退选项）和审计。'
- en: Along with this, risk assessment also includes how AI/ML models are identified,
    classified, and inventoried, with due consideration of how they are trained (for
    example, considering data type, vendor/open source libraries/code, third-party/vendor
    code updates and maintenance practices, and online retraining) and served to customers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，风险评估还包括如何识别、分类和清单化 AI/ML 模型，并充分考虑其训练方式（例如，考虑数据类型、供应商/开源库/代码、第三方/供应商代码更新及维护实践以及在线重新训练）和如何提供给客户。
- en: People and processes risk
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人员与流程风险
- en: The foremost objective of leadership and executive teams is to foster innovation
    and encourage an open culture where teams can collaborate, innovate, and thrive.
    When technical teams are proactive in bringing in automations in MLOps pipelines,
    many problems can be foreseen, and prompt measures can be taken to bridge the
    gaps through knowledge-sharing sessions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 领导层和高管团队的首要目标是促进创新，鼓励一个开放的文化，使团队能够协作、创新并茁壮成长。当技术团队主动将自动化引入 MLOps 流水线时，许多问题可以提前预测，并通过知识共享会议及时采取措施填补差距。
- en: Trust and explainability risk
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信任与可解释性风险
- en: Businesses remain reluctant to adopt AI-powered applications when the results
    of the model cannot be explained. Some of the unexplainable results can be attributed
    to the poor performance of the model for a selected customer segment or during
    a specific period (for example, many business predictions were affected by the
    outbreak of COVID-19). The opaqueness of the model – a lack of explanation of
    the results – causes fear when businesses or customers find there is a lack of
    incentive alignment or severe disruption to people’s workflows or daily routines.
    ML models answering questions about the behavior of the model raises stakeholder
    confidence. In addition to deploying an optimized model that can give the right
    predictions with minimal delay, the model should also be able to explain the factors
    that affect the decisions it makes. However, it’s up to the ML/AI practitioners
    to use their judgment and analysis to apply the right ML models and explainability
    tools to derive the factors contributing to the model’s behavior. Now, let us
    see – with an example – how explainability can aid in studying medical images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的结果无法解释时，企业仍然不愿采用AI驱动的应用。部分无法解释的结果可能是由于模型在特定客户群体或特定时期（例如，COVID-19爆发期间）表现不佳。模型的透明度不足——无法解释结果——会让企业或客户感到害怕，特别是当他们发现缺乏激励对齐或对人们工作流或日常惯例造成严重干扰时。机器学习模型回答关于模型行为的问题，可以增强利益相关者的信心。除了部署能够给出正确预测且延迟最小的优化模型外，模型还应能够解释影响其决策的因素。然而，ML/AI从业者需要运用判断和分析，选择合适的ML模型和可解释性工具，推导出影响模型行为的因素。现在，让我们通过一个例子，看看可解释性如何帮助研究医学图像。
- en: '**Deep Neural Networks (DNNs)** may be computationally hard to explain, but
    significant research is taking place into the explainability of DNNs as well.
    One such example involves **Explainable Artificial Intelligence** (**XAI**), used
    on pretrained deep learning neural networks (AlexNet, SqueezeNet, ResNet50, and
    VGG16), which has been successful in explaining critical regions that are affected
    by Barrett’s esophagus using related data by comparing classification rates. The
    comparative results can detect early stages of cancer and distinguish Barrett’s
    esophagus ([https://www.sciencedirect.com/science/article/pii/S0010482521003723](https://www.sciencedirect.com/science/article/pii/S0010482521003723))
    from adenocarcinoma. However, it remains up to the data scientist to decide how
    best to explain the use of their models, by selecting the right data and number
    of data points, based on the type of the problem.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度神经网络（DNNs）**可能在计算上难以解释，但对DNNs的可解释性正在进行大量研究。一个例子是**可解释人工智能**（**XAI**），它被应用于预训练的深度学习神经网络（AlexNet,
    SqueezeNet, ResNet50 和 VGG16），成功地通过比较分类率，使用相关数据解释受 Barrett 食管影响的关键区域。对比结果能够检测癌症的早期阶段，并区分
    Barrett 食管（[https://www.sciencedirect.com/science/article/pii/S0010482521003723](https://www.sciencedirect.com/science/article/pii/S0010482521003723)）和腺癌。然而，如何最佳地解释其模型的使用仍然由数据科学家决定，需根据问题的类型选择适当的数据和数据点数量。'
- en: Compliance and regulatory risk
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合规性和监管风险
- en: There are different privacy laws and regulations that have been set forth by
    different nations and governing agencies that impose penalties on organizations
    in case of violations. Some of the most common privacy rules include the European
    Union’s **General Data Protection Regulation** (**GDPR**) and the **California
    Consumer Privacy Act** (**CCPA**). The financial and healthcare sectors have already
    seen laws formulated to prevent bias and allow fair treatment. Adhering to compliance
    necessitates extra planning for risk management through audits and human monitoring.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 不同国家和监管机构已制定了不同的隐私法律和法规，若发生违规行为，将对组织进行处罚。一些最常见的隐私规则包括欧盟的**通用数据保护条例**（**GDPR**）和**加利福尼亚消费者隐私法案**（**CCPA**）。金融和医疗保健行业已经出台了防止偏见并允许公平对待的法律。遵守合规性要求需要通过审计和人工监控进行额外的风险管理规划。
- en: Apart from country-specific regulatory laws and guidance, regulators will likely
    rely on existing guidance in SR 11-7/OCC 2011-12 to assess the risks of AI/ML
    applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除了各国特定的法规和指导方针外，监管机构可能还会依赖SR 11-7/OCC 2011-12中的现有指导，来评估AI/ML应用的风险。
- en: Ethical risk
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 道德风险
- en: AI/ML models should go through proper validations and A/B testing to verify
    their compliance and fairness across different sections of the population, including
    people of varying genders and diverse racial and ethical backgrounds. For example,
    credit scoring and insurance models have historically been biased against racial
    minorities and discrimination-based lending decisions have resulted in litigation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: AI/ML模型应该经过适当的验证和A/B测试，以验证它们在不同人群（包括不同性别和不同种族及伦理背景的人）中的合规性和公平性。例如，信用评分和保险模型在历史上对少数种族存在偏见，基于歧视的借贷决策已导致诉讼。
- en: To make AI/ML models ethical, legal, and risk-free, it is inevitable for any
    organization and the executive team to have to ascertain the impact of the AI
    solution and service being rolled out in the market. This includes the inclusion
    of highly competent AI ethics personnel in the process who have regulatory oversight,
    and ensuring adherence to protocols and controls for risk mitigation to make sure
    the entire AI solution is robust and less attractive to attackers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要使AI/ML模型合乎伦理、合法且无风险，对于任何组织和执行团队来说，评估AI解决方案和服务在市场上推出的影响是不可避免的。这包括在过程中加入高素质的AI伦理人员，他们具有监管监督，并确保遵守风险缓解的协议和控制，以确保整个AI解决方案既稳固又不易受攻击者青睐。
- en: Such practices can not only add extra layers of security to anonymize individual
    identity but also remove any bias present in legacy systems. Now let us see what
    kinds of enterprise-grade initiatives are essential for inclusion in the AI development
    process.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些做法不仅可以增加额外的安全层以匿名化个人身份，还可以消除遗留系统中存在的任何偏见。现在让我们看看哪些企业级倡议对AI开发过程的包含是必不可少的。
- en: Exploring risk mitigation strategies with vision, strategy, planning, and metrics
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索具有愿景、战略、规划和度量的风险缓解策略
- en: After seeing the elements of risk in different stages of the AI transformation
    journey, now let us walk through the different enterprise risk mitigation plans,
    measures, and metrics. In later chapters, we will not only discover risks related
    to ML model design, development, and deployment but also get to know how policies
    put in place by executive leadership teams are important in designing systems
    that are compliant with country-specific regulatory laws. Timely review, awareness,
    and support in the risk identification process can save organizations from unexpected
    financial losses.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到AI转型旅程不同阶段的风险元素后，现在让我们深入了解不同的企业风险缓解计划、措施和度量。在后续章节中，我们不仅会发现与ML模型设计、开发和部署相关的风险，还将了解执行领导团队制定的政策对于设计符合特定国家法规的系统的重要性。及时的审查、意识和对风险识别过程的支持可以使组织免受意外的财务损失。
- en: Defining a structured risk identification process
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义结构化的风险识别过程
- en: The long-term mission and short-term goals can only be achieved when business
    leaders, IT, security, and risk management teams align to evaluate a company’s
    existing risks, and whether they are affecting the upcoming AI-driven analytics
    solution. Such an effort, led by one of the largest European bank's COOs, helped
    to identify biased product recommendations. If left unchecked, it could have led
    to financial loss, regulatory fines, and disgrace, impacting the organization’s
    reputation and causing a loss of customers and a backlash.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当业务领导、IT、安全和风险管理团队能够对公司现有风险进行评估，并确定这些风险是否会影响即将到来的人工智能驱动的分析解决方案时，长期使命和短期目标才能实现。这种努力由欧洲最大银行之一的COO领导，有助于识别存在偏见的产品推荐。如果不加以检查，可能会导致财务损失、法规罚款和声誉受损，从而影响组织的声誉，导致客户流失和反弹。
- en: This effort may vary from industry to industry. For example, the food and beverage
    industry needs to concentrate on risks related to contaminated products, while
    the healthcare industry needs to pay special attention to refrain from the misdiagnosis
    of patients and protect their sensitive health data.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种努力可能因行业而异。例如，食品和饮料行业需要集中关注与受污染产品相关的风险，而医疗保健行业则需要特别注意避免误诊患者并保护其敏感健康数据。
- en: Enterprise-wide controls
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 企业范围的控制
- en: 'Effective controls and techniques are structured around the incorporation of
    strong policies, worker training, contingency plans, and the redefinition of business
    rules and objectives that can be put into practice. These policies translate to
    specified standards and guidelines requiring human intervention as and when needed.
    For example, the European bank had to adopt flexibility in deciding how to handle
    specific customer cases when the customer’s financial or physical health was impacted:
    [https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence).
    In such cases, relationship managers had to intervene to offer suitable recommendations
    to help them to move on with the death/loss of a family member. Similarly, the
    healthcare industry needs the intervention of doctors and healthcare experts to
    adopt different **active learning** strategies to learn about rare diseases and
    their symptoms. Control measures necessitate the application of different open
    source or custom-built tools that can mitigate the risks of SaaS-based platforms
    and services, protect groups from potential discrimination, and ensure compliance
    with GDPR.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的控制和技术结构围绕着强有力的政策、员工培训、应急计划的实施，以及可以付诸实践的商业规则和目标的重新定义。这些政策转化为明确的标准和指南，要求在必要时进行人工干预。例如，欧洲银行在处理客户的财务或身体健康受到影响的特定情况时，必须采用灵活性：[https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence)。在这种情况下，关系经理必须介入，提供适当的建议，帮助客户应对家庭成员去世或失去亲人的情况。同样，医疗行业需要医生和健康专家的介入，采用不同的**主动学习**策略来了解罕见病及其症状。控制措施需要应用不同的开源或定制工具，以减轻基于SaaS平台和服务的风险，保护群体免受潜在歧视，并确保符合GDPR。
- en: Micro-risk management and the reinforcement of controls
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微风险管理与控制加强
- en: The tools and techniques put into practice will vary based on the phase of the
    ML life cycle. Attacks and threats are much too specific to input data, feature
    engineering, model training, deployment, and the way the model is served to its
    customers. Hence it is essential to design and evaluate any ML model against a
    **threat matrix** (more details on threat matrices will be discussed in [*Chapter
    2*](B18681_02.xhtml#_idTextAnchor040)). The most important factors that must be
    taken into consideration are the model's objective, optimization function, mode
    of learning (centralized versus federated), human-to-machine (or machine-to-machine)
    interaction, environmental factors (for designing policies and rewards in the
    case of reinforcement learning), feedback, retraining, and deployment. These factors,
    along with the model design and its explainability, will push organizations to
    go for a more transparent and explainable ML model and remove ML models that are
    overly complex, opaque, and unexplainable. The threat matrix can safeguard ML
    models in deployment by not only evaluating model performance but also testing
    models for adversarial attacks and other external factors that cause ML models
    to drift.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实施的工具和技术会根据ML生命周期的不同阶段而有所不同。攻击和威胁通常非常具体，针对输入数据、特征工程、模型训练、部署以及模型如何向客户提供服务。因此，设计和评估任何ML模型时，必须参考**威胁矩阵**（关于威胁矩阵的更多细节将在[*第二章*](B18681_02.xhtml#_idTextAnchor040)中讨论）。必须考虑的最重要因素包括模型的目标、优化函数、学习模式（集中式与联邦式）、人机（或机器与机器）交互、环境因素（用于设计强化学习中的政策和奖励）、反馈、再训练和部署。这些因素，加上模型设计及其可解释性，将推动组织选择更加透明和可解释的ML模型，并剔除过于复杂、晦涩难懂且无法解释的ML模型。威胁矩阵可以通过评估模型性能来保护已部署的ML模型，同时测试模型是否受到对抗性攻击以及其他导致模型漂移的外部因素。
- en: You need to apply a varying mix of risk control measures and risk mitigation
    strategies and reinforce them based on the outcome of the threat matrix. Along
    the journey of the AI transformation process, this will not only alleviate risks
    and reduce unseen costs but also make the system robust and transparent to counteract
    every possible risk. With such principles put into place, organizations can not
    only prevent ethical, business, reputation, and regulatory issues but also serve
    their customers and society with fair, equal, and impartial treatment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要根据威胁矩阵的结果，应用不同的风险控制措施和风险缓解策略，并加以强化。在AI转型过程的整个旅程中，这不仅能缓解风险、减少潜在成本，还能使系统具有鲁棒性和透明度，以应对每一个可能的风险。通过这些原则的落实，组织不仅能够防止道德、商业、声誉和监管问题，还能以公正、平等和无偏的方式为客户和社会服务。
- en: '![Figure 1.4 – A diagram showing enhancements and mitigations in current risk
    management settings](img/B18681_01_004.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.4 – 展示当前风险管理设置中增强和缓解措施的示意图](img/B18681_01_004.jpg)'
- en: Figure 1.4 – A diagram showing enhancements and mitigations in current risk
    management settings
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 – 展示当前风险管理设置中增强和缓解措施的示意图
- en: 'A number of new elements related to ethics are needed in current AI/ML risk
    frameworks, which can help to ascertain risk performance and alleviate risk:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当前AI/ML风险框架中需要一些新的与伦理相关的元素，这些元素可以帮助确定风险表现并缓解风险：
- en: Interpretability
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性
- en: Ethical AI validation tools
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道德AI验证工具
- en: Model privacy
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型隐私
- en: Model compression
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型压缩
- en: Bias
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差
- en: Feature engineering
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程
- en: Sustainable model training
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可持续模型训练
- en: Privacy-related pre-/post-processing techniques
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私相关的前后处理技术
- en: Fairness constraints
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公平约束
- en: Hyperparameters
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数
- en: Model storage and versioning
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型存储与版本控制
- en: Epsilon
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon
- en: Total and fairness loss
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总损失和公平损失
- en: Cloud/data center sustainability
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云/数据中心可持续性
- en: Feature stores
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征存储
- en: Attacks and threats
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击与威胁
- en: Drift
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 漂移
- en: Dynamic model calibration
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态模型校准
- en: A review of the pipeline design and architecture
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道设计和架构回顾
- en: Model risk scoring
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型风险评分
- en: Data/model lineage
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据/模型血统
- en: While we will study each of these components in later chapters, let us introduce
    the concepts here and understand why each of these components serves as an important
    unit for responsible/ethical model design and how they fit into the larger ML
    ecosystem.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中研究这些组件的每一部分，但在这里先介绍这些概念，并理解为什么每个组件都是负责任/道德模型设计的重要单元，以及它们如何融入更大的机器学习生态系统。
- en: To further illustrate, let us first consider the primary risk areas of AI ethics
    (the regulatory and model explainability risks) in *Figure 1**.5* by breaking
    down *Figure 1**.4*. The following figure illustrates risk assessment methods
    and techniques to explain model outcomes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明，让我们首先考虑AI伦理的主要风险领域（监管和模型可解释性风险），通过分解*图 1.4*来进一步理解*图 1.5*。下图展示了风险评估方法和技术，用以解释模型结果。
- en: '![Figure 1.5 – Risk assessment through regulatory assessment and model explainability](img/B18681_01_005.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 通过监管评估和模型可解释性进行风险评估](img/B18681_01_005.jpg)'
- en: Figure 1.5 – Risk assessment through regulatory assessment and model explainability
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 通过监管评估和模型可解释性进行风险评估
- en: We see both global and local surrogate models play an important role in interpretability.
    While a global surrogate model has been trained to approximate the predictions
    of a black-box model, a local surrogate model is able to explain the local predictions
    of an individual record by changing the distribution of the surrogate model’s
    input. It is done through the process of weighting the data locally with a specific
    instance of the data (providing a higher weight to instances that resemble the
    instance in question).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，全局和局部替代模型在可解释性中都发挥着重要作用。全局替代模型经过训练，用以近似黑箱模型的预测，而局部替代模型则能够通过改变替代模型输入的分布来解释个体记录的局部预测。这是通过使用特定实例的数据对数据进行局部加权的过程（对与当前实例相似的实例赋予更高的权重）来实现的。
- en: Ethical AI validation tools
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 道德AI验证工具
- en: These tools, either open source, through public APIs, or provided by different
    cloud providers (Google Cloud, Azure, or AWS), provide ways to validate the incoming
    data against different discriminatory sections of the population. Moreover, these
    tools also assist in discovering the protected data fields and data quality issues.
    Once the data is profiled with such tools, notification services and dashboards
    can be built in to detect data issues with the incoming data stream from individual
    data sources.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具，无论是开源的、通过公共 API 访问的，还是由不同的云服务提供商（如 Google Cloud、Azure 或 AWS）提供的，都提供了通过不同的区分人群部分来验证传入数据的方式。此外，这些工具还帮助发现受保护的数据字段和数据质量问题。一旦通过这些工具对数据进行了配置文件处理，就可以构建通知服务和仪表盘，以检测来自各个数据源的传入数据流中的数据问题。
- en: Model interpretability
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: 'ML models, especially neural networks, are often called **black boxes** as
    the outcomes cannot be directly linked to the model architecture and explained.
    Businesses often roll out ML models in production that can not only recommend
    or predict customer demand but also substantiate the model’s decision with facts
    (single-feature or multiple-feature interactions). Despite the black-box nature
    of ML models, there are different open source interpretability tools available
    that can significantly explain the model outcome, such as, for example, why a
    loan application has been denied to a customer or why an individual of a certain
    age group and demographic is vulnerable to a certain disease:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型，尤其是神经网络，常被称为**黑箱**，因为结果无法直接与模型架构关联并加以解释。企业常常在生产环境中部署机器学习模型，这些模型不仅可以推荐或预测客户需求，还能用事实（单一特征或多个特征的交互）来支持模型的决策。尽管机器学习模型具有黑箱特性，但有许多开源的可解释性工具可以显著解释模型结果，例如，为什么贷款申请被拒绝，或者为什么某一年龄段和人群的个体容易患某种疾病：
- en: Linear coefficients help to explain monotonic models (linear regression models)
    and justify the dependency of selected features and the results of the output.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性系数有助于解释单调模型（线性回归模型），并证明所选特征与输出结果之间的依赖关系。
- en: Nonlinear and monotonic models (for example, gradient-boosting models with a
    monotonic constraint) help with selecting the right feature set among many present
    features for prediction by evaluating the positive or negative relationship with
    the dependent variable.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性和单调模型（例如，具有单调约束的梯度提升模型）通过评估与因变量的正负关系，帮助从众多特征中选择正确的特征集进行预测。
- en: 'Nonlinear and nonmonotonic (for example, unconstrained deep learning models)
    methodologies such as local interpretable model-agnostic explanations or Shapley
    (an explainability Python library) serve as important tools for helping models
    with local interpretability. Neural networks have two broad primary categories
    for explaining ML models:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性和非单调（例如，无约束的深度学习模型）方法，如局部可解释模型无关解释或 Shapley（一个可解释性 Python 库），作为帮助模型进行局部可解释性的重要工具。神经网络在解释机器学习模型时有两大类主要方法：
- en: '**Saliency methods/saliency** **maps** (**SMs**)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显著性方法/显著性** **图** (**SMs**)'
- en: '**Feature** **Attribution** (**FA**)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征** **归因** (**FA**)'
- en: Saliency Maps are only effective at conveying information related to weights
    being activated on specified inputs or different portions of an image being selected
    by a **Convolutional Neural Network (CNN)**. While saliency maps cannot convey
    information related to feature importance, FA methods aim to fit structural models
    on data subsets to evaluate the degree/power/impact each variable has on the output
    variable.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图仅在传递与指定输入上激活的权重或被**卷积神经网络（CNN）**选择的图像不同部分相关的信息时才有效。尽管显著性图无法传达特征重要性相关的信息，FA
    方法旨在对数据子集拟合结构模型，以评估每个变量对输出变量的程度/影响/作用。
- en: Discriminative DNNs are able to provide model explainability and explain the
    most important features by considering the model’s input gradients, meaning the
    gradients of the output logits with regard to the inputs. Certain SM-based interpretability
    techniques (gradient, SmoothGrad, and GradCAM) are effective interpretability
    methods that are still under research. For example, the gradient method is able
    to detect the most important pixels in an image by applying a backward pass through
    the network. The score arrived at after computing the derivative of the class
    with respect to the input image helps further in feature attribution. We can even
    use tools such as an XAI SM for image or video processing applications. Tools
    can show us how a network’s decision is affected by the most important parts of
    an image or video.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 判别性DNN能够提供模型可解释性，并通过考虑模型的输入梯度来解释最重要的特征，输入梯度指的是输出logits相对于输入的梯度。一些基于SM的可解释性技术（如梯度法、SmoothGrad和GradCAM）是仍在研究中的有效可解释性方法。例如，梯度法可以通过反向传播网络来检测图像中最重要的像素。通过计算类别相对于输入图像的导数得到的分数，有助于进一步进行特征归因。我们甚至可以使用诸如XAI
    SM等工具来处理图像或视频应用。这些工具可以向我们展示网络决策如何受到图像或视频中最重要部分的影响。
- en: Model privacy
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型隐私
- en: 'With laws such as GDPR, CCPA, and policies introduced by different legislative
    bodies, ML models have absorbed the principle of **privacy by design** to gain
    user trust by incorporating privacy-preserving techniques. The objective behind
    said standards and the ML model redesign has primarily been to prevent information
    leaking from systems by building AI solutions and systems with the following characteristics:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随着像GDPR、CCPA这样的法律，以及不同立法机构出台的政策，ML模型已经吸收了**隐私设计**的原则，通过采用隐私保护技术来赢得用户信任。上述标准和ML模型重新设计的目标，主要是通过构建具备以下特点的AI解决方案和系统，防止信息从系统中泄露：
- en: Proactive and preventive instead of reactive and remedial
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 积极预防而非反应性修正
- en: In-built privacy as the default setting
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内置隐私作为默认设置
- en: Privacy embedded into the design
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计中嵌入隐私
- en: Fully functional – no trade-offs on functionality
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全功能化——没有对功能的妥协
- en: ML model life cycle security, privacy, and end-to-end protection
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML模型生命周期的安全性、隐私性和端到端保护
- en: Visibility and transparency
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可见性与透明度
- en: User-centric with respect for user privacy
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以用户为中心，尊重用户隐私
- en: 'To encompass privacy at the model level, researchers and data scientists use
    a few principal units or essential building blocks that should have enough security
    measures built in to prevent the loss of sensitive and private information. These
    building units are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在模型层面上涵盖隐私，研究人员和数据科学家使用一些主要的单元或基本构建模块，这些模块应当具备足够的安全措施，以防止敏感和私人信息的泄露。以下是这些构建单元：
- en: '**Model training data privacy**: The data pipeline for the ML training data
    ingestion unit should have sufficient security measures built in. Any adversary
    attempting to attack the system should not be able to reverse-engineer the training
    data.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练数据隐私**：ML训练数据摄取单元的数据管道应具有足够的安全措施。任何试图攻击系统的对手都不应能逆向工程训练数据。'
- en: '**Model input privacy**: The security and privacy measures should ensure any
    input data going for model training cannot be seen by anyone, including the data
    scientist who is creating the model.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型输入隐私**：安全和隐私措施应确保任何用于模型训练的输入数据都不能被任何人看到，包括创建模型的数据科学家。'
- en: '**Model output privacy**: The security and privacy measures should ensure that
    the model output is not visible to anyone except the recipient user whose data
    is being predicted.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型输出隐私**：安全和隐私措施应确保模型输出仅对预测数据的接收用户可见，其他任何人不可见。'
- en: '**Model storage and access privacy**: The model must be stored securely with
    defined access rights to only eligible data science professionals.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型存储与访问隐私**：模型必须安全存储，并定义访问权限，只允许符合条件的数据科学专业人员访问。'
- en: '*Figure 1**.6* illustrates different stages of model training and improvement
    where model privacy must be ensured to safeguard training data, model inputs,
    model weights, and the product, which is the ML model output.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.6* 展示了模型训练和改进的不同阶段，在这些阶段中，必须确保模型隐私，以保护训练数据、模型输入、模型权重和最终产物，即ML模型输出。'
- en: '![Figure 1.6 – A diagram showing privacy in ML models](img/B18681_01_006.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 展示ML模型中隐私的图示](img/B18681_01_006.jpg)'
- en: Figure 1.6 – A diagram showing privacy in ML models
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 展示ML模型中隐私的图示
- en: Model compression
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型压缩
- en: AI ethics, standards, and guidelines have propelled researchers and data science
    professionals to look for ways to run and deploy these ML models on low-power
    and resource-constrained devices without sacrificing model accuracy. Here, model
    compression is essential as compressed models with the same functionality are
    best for devices that have limited memory. From the standpoint of AI ethics, we
    must leverage ML technology for the benefit of humankind. Hence, it is imperative
    that robust compressed models are trained and deployed in extreme environments
    such that they have minimal human intervention, and at the same time memorize
    relevant information (by having optimal pruning of the number of neurons).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能伦理、标准和指南推动了研究人员和数据科学专业人士寻求在低功耗和资源受限的设备上运行和部署这些机器学习模型的方法，而不牺牲模型准确性。在这里，模型压缩是至关重要的，因为压缩模型在功能相同的情况下最适合内存有限的设备。从人工智能伦理的角度来看，我们必须利用机器学习技术造福人类。因此，在极端环境中训练和部署强健的压缩模型是必不可少的，这样它们可以最小化人工干预，同时通过优化修剪神经元数量来记忆相关信息。
- en: For example, one technique is to build robust compressed models using noise-induced
    perturbations. Such noise often comes with IoT devices, which receive a lot of
    perturbations in the incoming data collected from the environment. Research results
    demonstrate that on-manifold adversarial training, which takes into consideration
    real-world noisy data, is able to yield highly compressed models and higher-accuracy
    models than off-manifold adversarial training, which incorporates noise from external
    attackers. *Figure 1**.7* illustrates that manifold adversarial samples are closer
    to the decision boundary than the simulated samples.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一种技术是通过噪声诱导扰动来构建强大的压缩模型。这种噪声通常来源于物联网设备，这些设备接收来自环境的传入数据，其中包含大量扰动。研究结果表明，考虑到现实世界噪声数据的流形对抗训练（on-manifold
    adversarial training）能够产生比外部攻击者噪声引入的流形外对抗训练（off-manifold adversarial training）更高压缩率和更高准确度的模型。*图
    1.7* 说明了流形对抗样本比模拟样本更接近决策边界。
- en: '![Figure 1.7 – A diagram of simulated and on-manifold adversarial samples](img/B18681_01_007.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 模拟样本和流形对抗样本的示意图](img/B18681_01_007.jpg)'
- en: Figure 1.7 – A diagram of simulated and on-manifold adversarial samples
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 模拟样本和流形对抗样本的示意图
- en: Sustainable model training
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可持续模型训练
- en: Low-powered devices depend on renewable energy resources for their own energy
    generation and local model training in federated learning ecosystems. There are
    different strategies by which devices can participate in the model training process
    and send updates to the central server. The main objective of devices taking part
    in the training process intermittently is to use the available energy efficiently
    in a sustainable fashion so that the devices do not run out of power and remain
    in the system till the global model converges. Sustainable model training sets
    guidelines and effective strategies to maximize power utilization for the benefit
    of the environment.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 低功耗设备依赖可再生能源资源进行自我能量生成，并在联邦学习生态系统中进行本地模型训练。设备可以通过不同的策略参与模型训练过程并向中央服务器发送更新。设备间歇性地参与训练过程的主要目的是高效地利用可用能源，以可持续的方式运行，以便设备不会耗尽电力并保持在系统中，直到全局模型收敛。可持续模型训练为最大化功率利用提供了指导方针和有效策略，以造福环境。
- en: Bias
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏见
- en: ML models are subjected to different kinds of bias, both from the data and the
    model. While common data bias occurs from structural bias (mislabeling gender
    under perceived notions of societal constructs, for example, labeling women as
    nurses, teachers, and cooks), data collection, and data manipulation, common model
    bias occurs from data sampling, measurement, algorithmic bias, and bias against
    groups, segments, demographics, sectors, or classes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）模型会受到来自数据和模型的不同偏见。常见的数据偏见源自结构性偏见（例如，依据社会观念错误标注性别，女性被标注为护士、教师和厨师），数据收集和数据操控；而常见的模型偏见则来源于数据采样、测量、算法偏见以及对群体、细分市场、人口统计、行业或类别的偏见。
- en: '**Random Forest** (**RF**) algorithms work on the principle of randomization
    in the two-phase process of bagging samples and feature selection. The randomization
    process accounts for model bias from uninformative feature selection, especially
    for high-dimensional data with multi-valued features. The RF model elevated the
    risk level in money-laundering prediction by favoring the multi-valued dataset
    with many categorical variables for feature occupation. However, the same model
    was found to yield better, unbiased outcomes with a decrease in the number of
    categorical values. More advanced models built on top of RF, known as **xRF**,
    can select more relevant features using statistical assessments such as the **p-value**.
    The p-value assessment technique helps to assign appropriate weight to features
    based on their importance and aids in the selection of unbiased features by generating
    more accurate trees. This is an example of a feature weighting sampling technique
    used for dimensionality reduction.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**（**RF**）算法基于在两阶段过程中对样本进行袋装（bagging）和特征选择的随机化原理。随机化过程能够解决由无信息特征选择所引起的模型偏差，特别是在高维数据中，包含多值特征时。RF模型通过偏向多值数据集，其中许多特征是分类变量，提升了洗钱预测中的风险水平。然而，研究发现，当分类值数量减少时，同一模型能够产生更好的、无偏的结果。基于RF的更先进模型，称为**xRF**，能够利用统计评估（如**p值**）来选择更相关的特征。p值评估技术有助于根据特征的重要性分配适当的权重，并通过生成更精确的树结构帮助选择无偏的特征。这是一个用于降维的特征加权抽样技术示例。'
- en: Feature engineering
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: This has become increasingly complex to understand for black-box models such
    as neural networks when compared to traditional ML models. For example, a **CNN**
    needs proper knowledge and application of filters to remove unwanted attributes.
    Models built from high-dimensional data need to incorporate proper dimensionality
    reduction techniques to select the most relevant one. Moreover, ML models resulting
    from **Natural Language Processing** (**NLP**) require preprocessing as one of
    the preliminary steps for model design. There are several commercial and open
    source libraries available that aid in new, complex feature creation, but they
    can also yield overfitted ML models. It has been found that overfitted models
    provide a direct threat to privacy and may leak private information ([https://machinelearningmastery.com/data-leakage-machine-learning/)](https://machinelearningmastery.com/data-leakage-machine-learning/)).
    Hence, model risk mitigation mechanisms must employ individual feature assessment
    to confirm included features’ impact (mathematical transformation and decision
    criteria) on the business rationale. The role of feature creation can be best
    understood in a specific credit modeling use case by banks where the ML model
    can predict defaulters based on the engineered feature of debt-to-income ratio.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习模型相比，黑箱模型（如神经网络）变得越来越难以理解。例如，**CNN**需要正确的知识和滤波器的应用，以去除不必要的属性。由高维数据构建的模型需要结合适当的降维技术，以选择最相关的特征。此外，基于**自然语言处理**（**NLP**）的机器学习模型在模型设计的初步步骤中需要进行预处理。现在有许多商业和开源库可以帮助创建新的复杂特征，但它们也可能导致过拟合的机器学习模型。研究发现，过拟合的模型直接威胁到隐私，可能会泄露私人信息（[https://machinelearningmastery.com/data-leakage-machine-learning/)](https://machinelearningmastery.com/data-leakage-machine-learning/))。因此，模型风险缓解机制必须采用单独的特征评估，以确认所包含特征对业务逻辑的影响（数学变换和决策标准）。特征创建的作用在银行的具体信用建模用例中得到了最好的理解，其中机器学习模型可以基于债务与收入比率的工程化特征预测违约者。
- en: Privacy-related pre-/post-processing techniques
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私相关的预处理/后处理技术
- en: Data anonymization requires the addition of noise in some form (Gaussian/Laplace
    distribution) that can either be initiated prior to the model training process
    (K-anonymity, **Differential Privacy (DP)**) or post model convergence (bolt-on
    DP).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据匿名化需要以某种形式添加噪声（如高斯分布/拉普拉斯分布），这种噪声可以在模型训练过程之前（K-匿名性、**差分隐私 (DP)**）或模型收敛后（附加式DP）启动。
- en: Fairness constraints
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性约束
- en: ML models can be trained to yield desirable outcomes through different constraints.
    Constraints define different boundary conditions for ML models that on training
    the objective function would yield a fair, impartial prediction for minority or
    discriminatory racial groups. Such constraints need to be designed and introduced
    based on the type of training, namely supervised, semi-supervised, unsupervised,
    ranking, recommendations, or reinforcement-based learning. Datasets where constraints
    are applied the most have one or more sensitive attributes. Along with constraints,
    model validators should be entrusted to ensure a sound selection of parameters
    using randomized or grid search algorithms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可以通过不同的约束条件来训练，从而获得理想的结果。约束条件为机器学习模型定义了不同的边界条件，在训练时，目标函数将为少数群体或具有歧视性的种族群体产生公正、公平的预测。这些约束条件需要根据训练类型设计和引入，即监督学习、半监督学习、无监督学习、排名、推荐或基于强化学习的训练。应用约束条件最多的数据集通常包含一个或多个敏感属性。除了约束条件，模型验证者应负责确保使用随机化或网格搜索算法选择合适的参数。
- en: Model storage and versioning
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型存储和版本控制
- en: One important component of ethical AI systems is to endow production systems
    with the capability to reproduce data and model results, in the absence of which
    it becomes immensely difficult to diagnose failures and take immediate remedial
    action. Versioning and storing previous model versions not only allows you to
    quickly revert to a previous version, or activate model reproducibility to specific
    inputs, but it also helps to reduce debugging time and duplicating effort. Different
    tools and best practice mechanisms aid in model reproducibility by abstracting
    computational graphs and archiving data at every step of the ML engine.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理 AI 系统的一个重要组成部分是赋予生产系统能够再现数据和模型结果的能力，如果缺乏这一能力，诊断故障并采取立即的补救措施将变得异常困难。版本控制和存储先前的模型版本不仅允许你快速恢复到先前的版本，或激活模型可复现性以应对特定输入，而且还能减少调试时间和重复劳动。不同的工具和最佳实践机制通过抽象计算图并在每个机器学习引擎步骤中归档数据来帮助模型的可复现性。
- en: Epsilon (ε)
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Epsilon (ε)
- en: This is a metric used in **DP** solutions that is responsible for providing
    application-level privacy. This metric is used to measure privacy loss incurred
    on issuing the same query to two different datasets, where the two datasets differ
    in only one record and the difference is created by adding or removing one entry
    from one of the databases. We will discuss DP more in [*Chapter 2*](B18681_02.xhtml#_idTextAnchor040).
    This metric reveals the privacy risk imposed when it is computed on the private
    sensitive information of the previously mentioned datasets. It is also called
    privacy budget and is computed based on the input data size and the amount of
    noise added to the training data. The smaller the value, the better the privacy
    protection.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个在**差分隐私（DP）**解决方案中使用的指标，负责提供应用层隐私保护。该指标用于衡量在对两个不同的数据集发出相同查询时产生的隐私损失，这两个数据集仅在一个记录上有所不同，且差异是通过从其中一个数据库中添加或删除一个条目来创建的。我们将在[*第2章*](B18681_02.xhtml#_idTextAnchor040)中进一步讨论差分隐私。这个指标揭示了当其计算在前述数据集的私人敏感信息上时，所带来的隐私风险。它也被称为隐私预算，并根据输入数据的大小以及添加到训练数据中的噪声量来计算。值越小，隐私保护越好。
- en: Cloud/data center sustainability
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云/数据中心可持续性
- en: With growing concerns about climate change and sustainability issues, the major
    cloud providers (Google, Amazon, and Microsoft) have started energy efficiency
    efforts to foster greener cloud-based products. The launch of carbon footprint
    reporting has enabled users to measure, track, and report on the carbon emissions
    associated with the cloud. To encourage businesses to have a minimal impact on
    the environment, all ML deployments should treat sustainability as a risk or compliance
    to be measured and managed. This propels data science and cloud teams to consider
    the deployment of ML pipelines and feature stores in sustainable data centers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对气候变化和可持续性问题的关注不断增加，主要的云服务提供商（Google、Amazon 和 Microsoft）已经开始采取能源效率措施，以促进更绿色的基于云的产品。碳足迹报告的推出使用户能够衡量、跟踪和报告与云相关的碳排放。为了鼓励企业对环境的影响最小化，所有的机器学习部署应将可持续性视为一个需要衡量和管理的风险或合规问题。这推动了数据科学和云团队考虑在可持续数据中心中部署机器学习管道和特征存储。
- en: Feature stores
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征存储
- en: Feature stores allow feature reuse, thus saving on extra storage and cloud costs.
    As data reuse and storage must meet compliance and regulations, it is an important
    consideration parameter in ethical AI. Feature stores allow the creation of important
    features using feature engineering and foster collaboration among team members
    to share, discover, and use existing features without doing additional rework.
    Feature reuse also prompts the reuse of important attributes based on importance
    of features and model explainability as defined by other teams. As deep learning
    models require huge computing power and energy, the proper selection of algorithms,
    along with the reuse of model data and features, reduces cloud costs by reducing
    computational capacity.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储允许特征重用，从而节省额外的存储和云端成本。由于数据重用和存储必须符合合规性和法规要求，它是道德AI中的一个重要考虑参数。特征存储允许通过特征工程创建重要特征，并促进团队成员之间的合作，分享、发现和使用现有特征而无需进行额外的重复工作。特征重用还促使基于特征重要性和其他团队定义的模型可解释性重用重要属性。由于深度学习模型需要巨大的计算能力和能量，正确选择算法，并结合重用模型数据和特征，能够通过减少计算能力来降低云端成本。
- en: Attacks and threats
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击和威胁
- en: A risk framework designed for production-grade enterprise AI solutions should
    be integrated with an attack testing framework (third-party and open source),
    to ascertain the model risk from external adversaries. The ML model’s susceptibility
    to attack can then be used to increase the monitoring activity to be proactive
    in the case of attacks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为生产级企业AI解决方案设计的风险框架应与攻击测试框架（第三方和开源）集成，以评估模型面临外部攻击者的风险。然后，可以利用ML模型对攻击的易受攻击性增加监控活动，在发生攻击时主动应对。
- en: Drift
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 漂移
- en: 'Data and model monitoring techniques that have been implemented in the system
    must be able to quickly identify data and model drift when statistical properties
    of the target variable or the predictors change respectively (*Concept Drift and
    Model Decay in Machine Learning* by Ashok Chilakapati: [http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/](http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/)).
    Proactive measures include reviewing data formats, schema, and units and retraining
    the model when the drift percentage exceeds a specified threshold.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中已实现的数据和模型监控技术必须能够在目标变量或预测因子的统计特性发生变化时，迅速识别数据漂移和模型漂移（*机器学习中的概念漂移与模型衰退*，Ashok
    Chilakapati著：[http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/](http://xplordat.com/2019/04/25/concept-drift-and-model-decay-in-machine-learning/)）。主动措施包括检查数据格式、模式和单位，并在漂移百分比超过指定阈值时重新训练模型。
- en: 'The following descriptions correspond with the number labels in*Figure 1**.8*:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述与*图 1.8*中的数字标签相对应：
- en: Original data and model decision boundary at t1.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 原始数据和模型决策边界（t1时刻）。
- en: Drift in just the data boundary at t2, resulting from a change in the features
    of the input data. For example, let us consider a real-world scenario where IoT
    sensor readings are anomalous in the range -10 to 10\. Now, the new reading may
    change to -5 to 8, but still, the reading will be considered anomalous as there
    is no change in the decision outcome or the model output. As this does not result
    in any drift in the model boundary, it is only virtual drift.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅在数据边界发生漂移（t2时刻），这是由于输入数据特征的变化。例如，考虑一个实际场景，其中物联网传感器读取值的异常范围是-10到10。现在，新读取值可能会变为-5到8，但由于决策结果或模型输出没有变化，这个读取值仍然被视为异常。由于这不会导致模型边界的漂移，它仅为虚拟漂移。
- en: Drift in both data and the model boundary at t3, resulting in actual concept
    drift. For example, such a scenario may occur when two sensor readings change
    in such a manner (from old readings of -10 to 10 to new readings of +20 to +100)
    that the resultant model outcome is +1, signifying it is no longer an anomaly.
    It demonstrates a change in the model boundary, where the output is just a reflection
    of the change in the input data boundary.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据和模型边界在t3时刻同时发生漂移，导致实际的概念漂移。例如，当两个传感器读取值发生变化（从旧的-10到10的读取值变化为新的+20到+100的读取值），并且结果模型输出为+1，表示它不再是异常值时，就会出现这种场景。这表明模型边界发生了变化，输出只是输入数据边界变化的反映。
- en: "![Figure 1.8 – \uFEFFDifferent types of model drift](img/B18681_01_008.jpg)"
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.8 – 不同类型的模型漂移](img/B18681_01_008.jpg)'
- en: Figure 1.8 – Different types of model drift
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 不同类型的模型漂移
- en: Dynamic model calibration
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态模型校准
- en: Dynamic model calibration is a more specialized version of model drift. Model
    drift may result from a change in data, units of measurement, and internal and
    external factors that need careful study, review, and discussion for a certain
    period before triggering a model refresh.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 动态模型校准是模型漂移的一个更专业的版本。模型漂移可能由于数据变化、度量单位、以及内部和外部因素的变化所引起，这些变化需要经过一定时间的仔细研究、审查和讨论，然后才会触发模型更新。
- en: On the other hand, model calibration can be facilitated when a model’s performance
    level changes only due to short-term changes in the incoming data (for example,
    mobile network capacity becoming slow due to a large social gathering or a football
    match).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当模型的性能水平仅因输入数据的短期变化（例如，由于大型社交聚会或足球比赛而导致移动网络容量变慢）而发生变化时，模型校准可以更为简便。
- en: ML models (for example, reinforcement learning algorithms or Bayesian models)
    exhibit characteristics to refresh their model parameters dynamically to pick
    up new trends and patterns in the incoming data. This leads to the removal of
    manual processes of model review and refresh. In the absence of adequate controls
    or algorithms used to control the level of thresholds to allow model refresh,
    short-term patterns may get over-emphasized, which could degrade the performance
    of the model over time. Hence, overcoming such risks needs careful review by experts
    of when to allow dynamic recalibration to facilitate the reflection of upcoming
    trends. Moreover, businesses (especially in algorithmic trading in banking or
    the spread of a pandemic in healthcare) need to be convinced that dynamic recalibration
    outperforms static models over time.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型（例如，强化学习算法或贝叶斯模型）具有动态刷新模型参数的特点，以捕捉输入数据中的新趋势和模式。这使得模型审查和更新的手动过程得以去除。如果没有足够的控制或算法来控制模型更新的阈值水平，短期模式可能会被过度强调，从而可能随着时间的推移降低模型的性能。因此，克服这些风险需要专家仔细审查何时允许动态再校准，以便反映即将出现的趋势。此外，企业（尤其是在银行的算法交易或医疗领域疫情传播）需要确信动态再校准在长期内优于静态模型。
- en: '*Figure 1**.9* demonstrates a use case when the location data input to the
    model shows an oscillatory pattern, causing the prediction results to shift over
    time and resulting in model drift. Such scenarios need model replacement/calibration
    and the threshold of drift percentage to be specified or configured.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1.9* 展示了一个使用案例，当输入到模型的位置信息数据呈现振荡模式时，导致预测结果随时间发生变化，进而导致模型漂移。此类情况需要模型替换/校准，并指定或配置漂移百分比的阈值。'
- en: '![ Figure 1.9 – A diagram showing model calibration under output model prediction
    drift](img/B18681_01_009.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.9 – 展示在输出模型预测漂移下的模型校准示意图](img/B18681_01_009.jpg)'
- en: Figure 1.9 – A diagram showing model calibration under output model prediction
    drift
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9 – 展示在输出模型预测漂移下的模型校准示意图
- en: Reviewing the pipeline design and architecture
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审查管道设计和架构
- en: As we review model drift and allow the dynamic calibration of models, to comply
    with ethics we should also periodically review the system design and architecture,
    pipelines, and feature stores and allow modifications if needed. One of the most
    important parts of a review is to re-evaluate and reconsider the entire security
    system, to apply new patches or additional layers of authentication or black-listing
    services to proactively act on DDOS attacks. Several optimizations can be done
    in subsequent production releases that can help to reduce cloud costs, optimize
    database operations, and boost the performance of APIs and microservices. The
    review process allows you to seek expert opinions (from cloud and DevOps professionals)
    who can provide insights into designing more automated workflows, along with migration
    to on-demand services (for example, lambda services) to reduce processing costs.
    Reviewing system load, performance, and scaling factors can also facilitate a
    better selection of databases, caching, and messaging options, or carefully analyzing
    and redefining auto-scaling options.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们审查模型漂移并允许模型的动态校准时，为了符合道德标准，我们还应定期审查系统设计和架构、管道以及特征存储，并在需要时允许进行修改。审查中最重要的部分之一是重新评估和重新考虑整个安全系统，应用新的补丁或额外的认证层或黑名单服务，以主动应对DDOS攻击。在后续的生产版本中，可以进行多种优化，帮助降低云成本，优化数据库操作，并提升API和微服务的性能。审查过程让你能够寻求专家意见（来自云计算和DevOps专业人士），他们可以为设计更自动化的工作流程提供见解，并帮助迁移到按需服务（例如lambda服务），以减少处理成本。审查系统负载、性能和扩展因素，还能帮助更好地选择数据库、缓存和消息传递选项，或者仔细分析和重新定义自动扩展选项。
- en: Model risk scoring
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型风险评分
- en: As we have used ethical AI validation tools for profiling and validating input
    data, we also need risk assessment tools to assess and quantify the model risk
    against adversarial attacks and threats. There are different open source tools
    and APIs available, and even tools provided by different cloud providers (Google
    Cloud, Azure, and AWS) that provide ways to train and test models against the
    model’s susceptibility to different attacks and model bias by quantifying the
    number of unfair outcomes exhibited by the model toward different sections of
    the population. In addition, these tools also help to explain important features
    that contribute to the model outcome. In the following chapters, we will discuss
    more such tools and frameworks. A model risk-scoring strategy requires risk factors
    or indicators useful for predictions, data integrity, methodology preference,
    and resource capabilities.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用了道德AI验证工具来对输入数据进行概况分析和验证，我们还需要风险评估工具来评估并量化模型风险，特别是在面对对抗性攻击和威胁时。现在有不同的开源工具和API可供使用，甚至不同云服务提供商（如Google
    Cloud、Azure和AWS）也提供了训练和测试模型的方式，以应对模型对不同攻击的脆弱性和模型偏见，这些方法通过量化模型对不同人群群体表现的不公平结果来进行评估。此外，这些工具还帮助解释对模型结果有重大影响的特征。在接下来的章节中，我们将讨论更多这样的工具和框架。模型风险评分策略需要有助于预测的风险因素或指标、数据完整性、方法偏好和资源能力。
- en: 'Risk-scoring methodologies function in two different ways:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 风险评分方法有两种不同的功能方式：
- en: '**Prospective** risk methods predict model risk after analyzing historical
    model performance.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前瞻性**风险方法通过分析历史模型表现来预测模型风险。'
- en: '**Retrospective**/**concurrent** risk leverages the most current risk of the
    model to predict the overall model risk for future cycles.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回顾性**/**同时性**风险利用模型的最新风险预测未来周期的整体模型风险。'
- en: The second method is more suitable when there have been key changes to the model
    risk indicators, data (model behavior), or recent attacks or loss of data and
    the model is being investigated.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法更适合用于模型风险指标、数据（模型行为）或近期攻击或数据丢失发生重大变化且正在调查模型的情况。
- en: '*Figure 1**.10* illustrates how risk-sensitive model risk management takes
    into consideration monitoring tools, activities, and governance measures to evaluate
    the model risk. The figure has been extended from *Components of Keenan’s model
    risk measure*, Keenan (2015), which additionally demonstrates the impact of past
    attacks, threats, and vulnerabilities on similar models in businesses and indicates
    the increase of risk associated with the current model.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1**.10*展示了风险敏感的模型风险管理如何考虑监控工具、活动和治理措施来评估模型风险。该图扩展自*Keenan的模型风险度量组件*，Keenan（2015），该组件还展示了过去的攻击、威胁和漏洞对企业中类似模型的影响，并指出了当前模型相关风险的增加。'
- en: '![Figure 1.10 – A diagram showing model risk assessment](img/B18681_01_010.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图1.10 – 显示模型风险评估的图示](img/B18681_01_010.jpg)'
- en: Figure 1.10 – A diagram showing model risk assessment
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10 – 显示模型风险评估的图示
- en: Data/model lineage
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据/模型谱系
- en: Ethics and compliance processes require frequent audits and quality checks on
    both the data and the model. It is imperative to store the lineage of both so
    that at any instant, it is clear the model evolved from version 1 to version 2
    to version 3 due to changes in data, such as the addition, modification, or deletion
    of certain features. Along with this, there should be defined storage where immediate
    historical data about the model and its artifacts can be stored, as opposed to
    older artifacts, which can be stored in less frequent storage centers (requiring
    less access) of the cloud.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理和合规流程要求对数据和模型进行频繁的审计和质量检查。必须存储两者的谱系，以便在任何时刻都能清楚地知道模型是如何由于数据的变化（如添加、修改或删除某些特征）从版本1演变到版本2再到版本3的。同时，应该定义存储位置，用于存储关于模型及其工件的即时历史数据，而较旧的工件可以存储在云的低频存储中心（访问要求较少）。
- en: The following figure illustrates the model’s input training, validation, test
    data, model serving, and output file storage in AWS’s different storage classes
    based on the frequency of access. Here, we have the roles of different processing
    blocks and units that are essential in designing an ethical and fully compliant
    system. By following the previously stated validation policies and practices,
    it is easier to address ML model risks, explore existing bottlenecks, and redefine
    new policies and practices at each stage of the model life cycle.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了模型的输入训练、验证、测试数据、模型服务以及基于访问频率存储在AWS不同存储类别中的输出文件。在这里，我们展示了在设计一个符合伦理和完全合规的系统时，必不可少的不同处理模块和单元的角色。通过遵循前述的验证政策和实践，能够更容易地应对机器学习模型风险，探索现有的瓶颈，并在模型生命周期的每个阶段重新定义新的政策和实践。
- en: '![Figure 1.11 – A diagram showing the model and its artifact storage](img/B18681_01_011.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图1.11 – 显示模型及其工件存储的图示](img/B18681_01_011.jpg)'
- en: Figure 1.11 – A diagram showing the model and its artifact storage
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11 – 显示模型及其工件存储的图示
- en: Any executive team needs to be aware of the importance of cloud infrastructure,
    system and security design principles, ML model design, model scoring, and risk
    assessment mechanisms and set guidelines so that the business can mitigate risks,
    avoid penalties, and gain confidence in harnessing the power of ML to boost sales
    and revenue.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 任何执行团队都需要意识到云基础设施、系统和安全设计原则、机器学习模型设计、模型评分和风险评估机制的重要性，并设定指导方针，以便企业能够降低风险，避免处罚，并获得使用机器学习推动销售和收入的信心。
- en: '![Figure 1.12 – A diagram showing data and model lineage](img/B18681_01_012.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图1.12 – 显示数据和模型谱系的图示](img/B18681_01_012.jpg)'
- en: Figure 1.12 – A diagram showing data and model lineage
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12 – 显示数据和模型谱系的图示
- en: '*Figure 1**.12* illustrates how data and model lineage need to be accomplished
    in the model life cycle development phases, starting from data integration and
    preprocessing to model training, ensembling, model serving, and the retraining
    process. We can see data arrives from two different data sources, A and B, at
    times t1 and t2, which gets assembled or aggregated at t3 to serve as input for
    data preprocessing and feature engineering at t4 and t5 respectively. There are
    two model outputs:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.12* 展示了数据和模型谱系在模型生命周期开发阶段如何完成，涵盖从数据集成和预处理到模型训练、集成、模型服务以及再训练过程。我们可以看到，数据分别在t1和t2时从两个不同的数据源A和B到达，并在t3时被组装或聚合，作为数据预处理和特征工程的输入，分别在t4和t5时进行。这里有两个模型输出：'
- en: Model v1 available at tn+3 corresponding to model training (tn) demonstrating
    combination of different ML models trained at different instants of time (tn+1)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在tn+3时可用的模型v1，对应模型训练（tn），演示了在不同时间点（tn+1）训练的不同机器学习模型的组合
- en: Model v2 available at tn+x+3 corresponding to model retraining (*t*n+x), re-ensembling
    (tn+x+1)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在tn+x+3时可用的模型v2，对应模型再训练（*t*n+x），重新集成（tn+x+1）
- en: Data and model lineage should be capable of capturing any changes in the system
    with appropriate versions, which aids in model reproducibility later. After analyzing
    the important components of ethics and risk, let us now take a look at the penalties
    that organizations can incur if they fail to follow laws and guidelines set by
    regulatory bodies.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和模型的血统应该能够捕捉到系统中任何变化，并具备适当的版本控制，这有助于后续的模型可重复性。在分析完伦理和风险的关键组成部分后，接下来我们来看看如果组织未能遵守监管机构设定的法律和指南，可能面临的处罚。
- en: Assessing potential impact and loss due to attacks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估攻击可能带来的影响和损失
- en: In the previous section, we looked at the data threats, risks, and important
    metrics for consideration while building our ML systems. Now, let us understand
    the financial losses that organizations have incurred due to data leakage.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们探讨了在构建机器学习系统时需要考虑的数据威胁、风险和重要指标。现在，让我们来了解一下因数据泄露而导致的财务损失。
- en: AOL data breach
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AOL数据泄露
- en: 'AOL faced a lawsuit in 2006 that resulted in them having to pay at least $5,000
    to every person whose data was leaked because of releasing user records that could
    be accessed through public search APIs (*Throw Back Hack*: *The Infamous AOL Data
    Leak*: [https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak](https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak)).
    This incident happened as the search department mistakenly released a compressed
    text file holding 20 million keyword search record details of 650,000 users. As
    users’ **Personally Identifiable Information (PII)** personally identifiable information
    was present in the search queries, it was easy to identify and associate an individual
    holding an account. In addition, very recently, Jason Smathers, an employee of
    AOL, is known to have sold to a person named Sean Dunaway of Las Vegas a list
    of 92 million AOL customer account names.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，AOL因泄露用户记录而面临诉讼，导致他们必须向每个数据泄露的用户支付至少5000美元，因为这些记录可以通过公共搜索API访问（*回顾黑客事件*：*臭名昭著的AOL数据泄露*：
    [https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak](https://www.proofpoint.com/us/blog/insider-threat-management/throw-back-hack-infamous-aol-data-leak)）。该事件发生在搜索部门错误地发布了一个压缩的文本文件，里面包含了650,000名用户的2000万条关键词搜索记录。由于搜索查询中包含了用户的**个人身份信息（PII）**，很容易识别并关联到持有账户的个人。此外，最近，AOL的一名员工Jason
    Smathers被发现将包含9200万AOL客户账户名称的列表卖给了拉斯维加斯的Sean Dunaway。
- en: Yahoo data breach
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 雅虎数据泄露
- en: 'Yahoo encountered a series of data breaches (loss of personal information such
    as through email) through varying levels of security intrusions between 2012 and
    2016, amounting to the leakage of 3 billion records (*IOTW*: *Multiple Yahoo data
    breaches across four years result in a $117.5 million settlement*: [https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement](https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement)).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 雅虎遭遇了一系列数据泄露事件（例如通过电子邮件泄露个人信息），这些事件发生在2012到2016年之间，涉及3亿条记录的泄露（*本周事件*：*雅虎四年内发生多起数据泄露事件，最终达成1.175亿美元和解*：
    [https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement](https://www.cshub.com/attacks/articles/incident-of-the-week-multiple-yahoo-data-breaches-across-4-years-result-in-a-1175-million-settlement)）。
- en: 'The attack in 2014 targeted a different user database, affecting 500 million
    people and containing a greater detail of personal information such as people’s
    names, email addresses, passwords, phone numbers, and birthdays. Yahoo settled
    penalties worth $50 million, with $35 million paid in advance, as a part of the
    damages (*Yahoo Fined $50M Over Data* *Breach*: [https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/](https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/)).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年的攻击针对的是一个不同的用户数据库，影响了5亿人，并且泄露了更详细的个人信息，如姓名、电子邮件地址、密码、电话号码和生日。雅虎因此支付了5000万美元的罚款，其中3500万美元提前支付，作为赔偿的一部分（*雅虎因数据泄露被罚款5000万美元*：
    [https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/](https://www.pymnts.com/legal/2018/yahoo-fine-personal-data-breach/)）。
- en: Marriot hotel chain data breach
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 万豪酒店连锁数据泄露
- en: 'The Marriot hotel chain was fined £18.4m due to the leak of the personal information
    (names, contact details, travel information, VIP status) of 7 million guests in
    the UK in a series of cyber-attacks from 2014 to 2018\. Due to the failure to
    protect personal data and non-conformance with the GDPR, it incurred a hefty fine
    from the UK’s data privacy watchdog (*Marriott Hotels fined £18.4m for data breach
    that hit* *millions*: [https://www.bbc.com/news/technology-54748843](https://www.bbc.com/news/technology-54748843)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 万豪酒店集团因2014至2018年间一系列网络攻击导致英国700万名客人的个人信息（姓名、联系方式、旅行信息、VIP状态）泄露，被罚款1840万英镑。由于未能保护个人数据且未遵守GDPR规定，英国数据隐私监管机构对其处以重罚
    (*万豪酒店因数据泄露被罚1840万英镑，影响数百万用户*：[https://www.bbc.com/news/technology-54748843](https://www.bbc.com/news/technology-54748843))。
- en: Uber data breach
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优步数据泄露
- en: 'Uber was handed a fine of $20,000 over a 2014 data breach in a settlement in
    New York due to a breach of riders’ data privacy (*Uber fined $20K in data breach,
    ‘god view’ probe*: [https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/](https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/)).
    The breach occurred in 2014 and exposed 50,000 drivers’ location information through
    the rider-tracking system.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 优步因2014年数据泄露事件在纽约达成和解，被罚款2万美元 (*优步因数据泄露和‘上帝视角’调查被罚2万美元*：[https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/](https://www.cnet.com/tech/services-and-software/uber-fined-20k-in-surveillance-data-breach-probe/))。该泄露事件发生在2014年，泄露了5万名司机的位置信息，且通过乘客追踪系统进行了暴露。
- en: Google data breach
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谷歌数据泄露
- en: In 2020, the French data protection authority imposed a fine of $57 million
    on Google due to the violation of GDPR, because it failed to acknowledge and share
    how user data is processed in different Google apps, such as Google Maps, YouTube,
    the search engine, and personalized advertisements. In another data leakage incident,
    Google was responsible for leaking the private data of 500,000 former Google+
    users. This data leak enforced Google to pay US$7.5 million, and compensation
    between US$5 and US$12 to users with Google+ accounts between 2015 and 2019.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年，法国数据保护局因谷歌未能确认并共享其在不同应用程序（如谷歌地图、YouTube、搜索引擎和个性化广告）中如何处理用户数据，违反GDPR规定，对谷歌处以5700万美元罚款。在另一宗数据泄露事件中，谷歌泄露了50万名前Google+用户的私人数据。谷歌因此需要支付750万美元赔偿，并向2015年至2019年间拥有Google+账户的用户提供5至12美元的赔偿金。
- en: Amazon data breach
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 亚马逊数据泄露
- en: 'Amazon faced different data leak incidents in 2021 (*Worst AWS Data Breaches
    of 2021*: [https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/](https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/)).
    One of the incidents resulted in a fine of 746 million euros (US$887 million)
    (*Amazon hit with US$887 million fine by European privacy watchdog*: [https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html](https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html))
    being imposed by a European privacy watchdog, due to violating GDPR. In another
    incident, misconfigured S3 buckets in AWS amounted to the disruption of networks
    for considerable periods. S3 files, apart from **PII**, including names, email
    addresses, national ID numbers, and phone numbers, could contain credit card details,
    including CVV codes.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊在2021年经历了多个数据泄露事件 (*2021年最严重的AWS数据泄露*：[https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/](https://securityboulevard.com/2021/12/worst-aws-data-breaches-of-2021/))。其中一起事件导致欧洲隐私监管机构对其罚款7.46亿欧元（8.87亿美元）（*亚马逊因违反GDPR被欧洲隐私监管机构罚款8.87亿美元*：[https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html](https://www.cnbc.com/2021/07/30/amazon-hit-with-fine-by-eu-privacy-watchdog-.html)）。在另一事件中，AWS的S3存储桶配置错误导致网络中断长时间。除了**个人身份信息（PII）**，如姓名、电子邮件地址、国民身份证号码和电话号码，还可能包括信用卡详细信息，包括CVV代码。
- en: Facebook data breach
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Facebook数据泄露
- en: 'In 2018, Facebook received a large penalty of $5 billion, and it needed to
    investigate and resolve different privacy and security loopholes (*Facebook to
    pay record $5 billion U.S. fine over privacy; faces antitrust probe*: [https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9](https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9)).
    The breach occurred on account of improper usage of PII leaked by Cambridge Analytica,
    which had gathered information from 50 million profiles on Facebook. Facebook
    exposed the PII of 87 million people that had been misused by the Cambridge Analytica
    firm to target ads during an election campaign in 2016.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Facebook遭遇了50亿美元的巨额罚款，并且需要调查和解决不同的隐私和安全漏洞（*Facebook支付创纪录的50亿美元美国隐私罚款；面临反垄断调查*：[https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9](https://www.reuters.com/article/us-facebook-ftc/facebook-to-pay-record-5-billion-u-s-fine-over-privacy-faces-antitrust-probe-idUSKCN1UJ1L9)）。这一泄露事件是由于剑桥分析公司不当使用泄露的个人身份信息（PII），剑桥分析公司从Facebook上收集了5000万个用户档案信息。Facebook泄露了8700万人的PII，而这些信息被剑桥分析公司滥用，用于2016年选举期间的广告定向。
- en: We can note that data breaches are common and they still occur presently. Some
    of the biggest providers in search services, retail, travel or hospitality, and
    transportation systems have been victims of threats and penalties here PII information
    have been stolen. Some other data breaches between 2019 and 2021 are known to
    have taken place for organizations such as Volkswagen (whose security breach impacted
    over 3 million customers) and T-Mobile (where over 50 million customers’ private
    information, including Social Security numbers, and IMEI and IMSI numbers, was
    compromised). in attacking iPads and iPhones to steal unique Apple device identifiers
    (UDIDs) and the device names of more than 12 million devices. The incident occurred
    when a FBI agent's laptop was hacked to steal 12 million Apple IDs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到，数据泄露事件很常见，而且至今仍在发生。搜索服务、零售、旅游或酒店业以及交通系统等一些最大的服务提供商都曾是威胁和处罚的受害者，个人身份信息（PII）在这些事件中被盗取。2019到2021年间，已知发生了一些其他数据泄露事件，涉及组织包括大众汽车（其安全漏洞影响了超过300万客户）和T-Mobile（超过5000万客户的私人信息，包括社会保障号码、IMEI和IMSI号码被泄露）。攻击者还利用iPad和iPhone攻击窃取了超过1200万设备的唯一Apple设备标识符（UDID）和设备名称。该事件发生时，一名FBI探员的笔记本电脑被黑客攻击，窃取了1200万个Apple
    ID。
- en: Discovering different types of attacks
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现不同类型的攻击
- en: After gaining an understanding of the financial losses suffered by organizations,
    it is imperative to know the objective of each type of attack and how attacks
    can be carried out. Moreover, the growth of the online industry and the availability
    of cheap data services, along with the usage of IoT and mobile devices, has left
    attackers with plenty of user-generated content to abuse. Advanced attack research
    techniques have propelled attackers to use advanced mechanisms to target large-scale
    systems and their defenses. There are different types of attacks on ML models,
    whether they are available for local use (white box) or deployed in a cloud setup
    (Google, Amazon, or Azure) and served by means of a prediction query. Amazon and
    Google provide services to train ML models in a black-box manner. Both Google
    (Vertex AI) ([https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview))
    and AWS have partial feature extraction techniques' documentation available in
    their manuals. With the increased scope of privacy breaches in a deployed model,
    it is easier for an attacker to attack and steal training data and ML models.
    Attackers are motivated to steal ML models to avoid prediction query charges.
    *Figure 1**.13* illustrates different categories of attacks under training and
    testing. We have also mentioned defense techniques, which will be discussed more
    in [*Chapter 2*](B18681_02.xhtml#_idTextAnchor040), *Emergence of Risk-Averse
    Methodologies* *and Frameworks*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了组织遭受的财务损失之后，了解每种攻击类型的目标以及如何实施这些攻击变得至关重要。此外，随着在线行业的增长、廉价数据服务的普及以及物联网和移动设备的使用，攻击者有了大量的用户生成内容可以滥用。先进的攻击研究技术推动攻击者使用更复杂的机制来针对大规模系统及其防御措施。无论机器学习模型是用于本地（白盒）还是部署在云端（如
    Google、Amazon 或 Azure）并通过预测查询进行服务，都存在不同类型的攻击。Amazon 和 Google 提供黑盒式的机器学习模型训练服务。Google（Vertex
    AI）([https://cloud.google.com/vertex-ai/docs/explainable-ai/overview](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview))
    和 AWS 都在其手册中提供了部分特征提取技术的文档。随着部署模型中隐私泄露范围的扩大，攻击者更容易进行攻击并窃取训练数据和机器学习模型。攻击者的动机是窃取机器学习模型，以避免预测查询费用。*图
    1.13* 展示了训练和测试中的不同攻击类别。我们还提到了防御技术，这些将在[*第 2 章*](B18681_02.xhtml#_idTextAnchor040)《风险规避方法论和框架的出现》中进一步讨论。
- en: '![Figure 1.13 – A diagram showing different attack categories and defenses](img/B18681_01_013.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.13 – 显示不同攻击类别和防御措施的示意图](img/B18681_01_013.jpg)'
- en: Figure 1.13 – A diagram showing different attack categories and defenses
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – 显示不同攻击类别和防御措施的示意图
- en: Now let us discuss how and with what objectives attackers try to attack ML models.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论攻击者如何以及以什么目标来攻击机器学习模型。
- en: 'To run different attacks, we need to import the necessary Python libraries:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行不同的攻击，我们需要导入必要的 Python 库：
- en: '[PRE0]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That is a lot of imports! With everything acquired, we are now ready to proceed
    with poisoning, evasion, extraction, or inference attacks. We have used ART to
    create a **Zeroth-Order** **Optimization (ZOO)** attack, a kind of evasion attack
    using XGBoostClassifier.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这可真是很多依赖库！所有所需的工具都已准备好，我们现在可以继续进行投毒、规避、提取或推理攻击。我们使用 ART 创建了一个**零阶** **优化（ZOO）**
    攻击，这是一种利用 XGBoostClassifier 进行的规避攻击。
- en: Data phishing privacy attacks
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据钓鱼隐私攻击
- en: This is one of the most common techniques used by attackers to gain access to
    confidential information in a training dataset by applying reverse-engineering
    when the model has sufficient data leakage.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这是攻击者常用的一种技术，通过反向工程来获取训练数据集中的机密信息，前提是模型存在足够的数据泄漏。
- en: This is possible when the model is overfitting and not able to generalize the
    predictions to the new data or the model is trained with too few training data
    points. Mechanisms such as DP, randomized data hold-out, and three-level encryption
    at input, model, and output can increase the protection.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型出现过拟合，无法将预测结果泛化到新数据，或者模型仅用少量训练数据进行训练时，就可能发生这种情况。诸如差分隐私（DP）、随机数据隔离以及在输入、模型和输出层进行三层加密等机制可以增强保护措施。
- en: Poisoning attacks
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投毒攻击
- en: 'This is a kind of attack on model integrity, where the attacker can affect
    the model’s performance in the training/retraining process during deployment by
    directly influencing the training or its labels. The name “poison” is derived
    from the attacker’s ability to poison the data by injecting malicious samples
    during its operation. Poisoning may be of two types:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种针对模型完整性的攻击方式，攻击者可以通过直接影响训练或其标签，在部署过程中影响模型在训练/重训练过程中的表现。术语“poison”（毒害）来源于攻击者通过在模型操作期间注入恶意样本来毒害数据的能力。毒害攻击可以分为两种类型：
- en: Model skewing in a white-box manner by gaining access to the model. The training
    data is modified in such a way that the boundary between what the classifier categorizes
    as good data and what the classifier categorizes as bad shifts in the favor of
    the attacker.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过获得模型访问权限进行白盒模型偏移。训练数据被修改，以至于分类器将好的数据与坏的数据之间的边界向攻击者的有利方向移动。
- en: A feedback weaponization attack undertaken in a black-box manner works by generating
    abusive or negative feedback to manipulate the system into misclassifying good
    content as abusive. This is more common in recommendation systems, where the attacker
    can promote products, content, and so on by following the user closely on social
    media.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒反馈武器化攻击通过生成恶意或负面反馈，操控系统将好的内容错误分类为恶意内容。这在推荐系统中更为常见，攻击者可以通过在社交媒体上密切关注用户，来推广产品、内容等。
- en: As the duration of this attack depends on the model’s training cycle, the principal
    way to prevent a poisoning attack is to detect malicious inputs before the next
    training cycle happens, by adding input and system validation checking, rate limiting,
    regression testing, manual moderation, and other statistical techniques, along
    with enforcing strong access controls.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此类攻击的持续时间取决于模型的训练周期，防止毒害攻击的主要方式是在下一个训练周期发生之前，通过添加输入和系统验证检查、速率限制、回归测试、人工审核及其他统计技术来检测恶意输入，同时加强强有力的访问控制。
- en: Evasion attacks
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绕过攻击
- en: Evasion attacks are very popular in ML research as they are used in intrusion
    and malware cases during the deployment or inference phase. The attacker changes
    the data with the objective of deluding the existing trained classifiers. The
    attackers obfuscate the data of malware, network intrusion detectors, or spam
    emails, which are treated as legitimate as they do not impact the training data.
    Such non-random human-imperceptible perturbations, when added to original data,
    cause the learned model to produce erroneous output, even without drifting the
    model decision boundary.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 绕过攻击在机器学习研究中非常流行，因为它们通常用于部署或推理阶段的入侵和恶意软件案例。攻击者通过改变数据，目的是欺骗现有的训练分类器。攻击者伪装恶意软件、网络入侵检测器或垃圾邮件的数据显示为合法数据，因为这些伪装不影响训练数据。此类非随机、难以察觉的人为扰动，当加到原始数据上时，导致学习模型产生错误的输出，即使不改变模型的决策边界。
- en: Spoofing attacks against biometric verification systems fall under the category
    of evasion attacks. The best way to design intrusion detectors against adversarial
    evasion attacks is to leverage ensemble learning, which can combine layers of
    detectors and monitor the behavior of applications. Evasion attacks pose challenges
    even in deploying DNNs in safety- and security-critical applications such as self-driving
    cars. Region-based classification techniques (relying on majority voting techniques
    among the labels of sampled data points) are found to be more robust to adversarial
    samples. The following figure illustrates data poisoning and evasion attacks on
    centralized and federated learning systems.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 针对生物识别验证系统的欺骗攻击属于绕过攻击范畴。设计针对对抗性绕过攻击的入侵检测器的最佳方式是利用集成学习，它可以结合多层检测器并监控应用程序的行为。绕过攻击在部署深度神经网络（DNN）到安全性和重要性应用（如自动驾驶汽车）时也带来了挑战。基于区域的分类技术（依赖于样本数据点标签之间的多数投票技术）被发现对对抗性样本更为鲁棒。下图展示了对集中式和联邦学习系统的毒害数据和绕过攻击。
- en: '![Figure 1.14 – A diagram showing a simple federated poisoning and evasion
    attack](img/B18681_01_014.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.14 – 显示一个简单的联邦毒害与绕过攻击的示意图](img/B18681_01_014.jpg)'
- en: Figure 1.14 – A diagram showing a simple federated poisoning and evasion attack
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – 显示一个简单的联邦毒害与绕过攻击的示意图
- en: The following code snippet provides an example of initiating an evasion attack
    on XGBoostClassifier. The code outlines the procedure to trigger a black-box **ZOO**
    attack with a classifier (where the parameter classifier is set to XGBoost) to
    predict the gradients of the targeted DNN. This prediction helps to generate adversarial
    data where the confidence (float) denotes how far away the samples generated are,
    with high confidence symbolizing the samples are generated at a greater distance
    from the input. The underlying algorithm uses stochastic coordinate descent along
    with dimension reduction, a hierarchical attack, and an importance sampling technique
    with the configurability of triggering a targeted attack or non-targeted attack,
    as set by the targeted Boolean parameter in the following code. While the untargeted
    attack can only cause misclassification, targeted attacks can force a class to
    be classified as a desired class.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段提供了一个对 XGBoostClassifier 发起规避攻击的示例。代码概述了触发黑箱 **ZOO** 攻击的过程，使用分类器（此处分类器设置为
    XGBoost）来预测目标深度神经网络（DNN）的梯度。此预测有助于生成对抗数据，其中置信度（浮动）表示生成的样本与输入之间的距离，较高的置信度意味着样本与输入的距离较大。底层算法使用随机坐标下降法、维度降维、分层攻击，以及重要性采样技术，同时具备触发目标攻击或非目标攻击的可配置性，这由以下代码中的目标布尔参数设置。虽然非目标攻击只能导致误分类，但目标攻击可以迫使某个类别被分类为期望的类别。
- en: 'The learning rate of the attack algorithm is controlled by `learning_rate`
    (float). Other important parameters for consideration are `binary_search_steps`
    (integer), which is the number of times to adjust the constant with binary search,
    and `initial_const` (float), which is available for tweaking the importance of
    the distance and confidence value to achieve the initial trade-off constant `c`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击算法的学习率由 `learning_rate`（浮动）控制。其他需要考虑的重要参数包括 `binary_search_steps`（整数），它表示调整常量时二分搜索的次数，以及
    `initial_const`（浮动），它可用于调整距离和置信度值的重要性，以实现初始折衷常量 `c`：
- en: 'Create the ART classifier for XGBoost:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 XGBoost 的 ART 分类器：
- en: '[PRE1]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create the ART ZOO attack:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 ART ZOO 攻击：
- en: '[PRE2]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Generate adversarial samples with the ART ZOO attack:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ART ZOO 攻击生成对抗样本：
- en: '[PRE3]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The sample code snippet demonstrates a mechanism to generate adversarial samples
    using a poisoned attack and then visualize the effect of classifying data points
    with the clean model versus the poisoned model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例演示了一个机制，使用毒化攻击生成对抗样本，并可视化用清洁模型与毒化模型分类数据点的效果：
- en: '[PRE4]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As illustrated in the following figure, in a perfect classifier, all the points
    should ideally be in yellow or blue circles, aligned on either the green or light
    blue side of the classifier boundary respectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，在一个完美的分类器中，所有点理想情况下应该分别位于黄色或蓝色圆圈内，且应对齐在分类器边界的绿色或浅蓝色一侧。
- en: '![Figure 1.15 – Code sample to trigger poison attacks](img/B18681_01_015.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.15 – 触发毒化攻击的代码示例](img/B18681_01_015.jpg)'
- en: Figure 1.15 – Code sample to trigger poison attacks
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15 – 触发毒化攻击的代码示例
- en: Here, the red cross is the attack point, which is strong enough to disturb the
    model’s generalization capability.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，红色十字是攻击点，足够强大以干扰模型的泛化能力。
- en: Model stealing/extraction
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型窃取/提取
- en: 'In a model extraction attack, the attacker is responsible for probing a black-box
    ML system (with no knowledge of model internals) to reconstruct the model or retrieve
    the training data (*In Model Extraction, Don’t Just Ask ''How?''*: *Ask ''Why?''*
    by Matthew Jagielski and Nicolas Papernot: [http://www.cleverhans.io/2020/05/21/model-extraction.html](http://www.cleverhans.io/2020/05/21/model-extraction.html)).
    This kind of attack needs special attention when either the training data or the
    model itself is sensitive and confidential, as the attacker may totally avoid
    provider charges by running cross-user model extraction attacks.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型提取攻击中，攻击者负责探测一个黑箱机器学习系统（没有模型内部知识），以重建模型或提取训练数据（*在模型提取中，别只问“怎么做？”*：*问“为什么？”*，作者：Matthew
    Jagielski 和 Nicolas Papernot：[http://www.cleverhans.io/2020/05/21/model-extraction.html](http://www.cleverhans.io/2020/05/21/model-extraction.html)）。当训练数据或模型本身是敏感且机密的时，这种攻击需要特别注意，因为攻击者可能通过执行跨用户模型提取攻击，完全避免服务提供商的费用。
- en: Attackers also want to use model information and data for their own personal
    benefit (for example, stolen information can be used by an attacker to customize
    and optimize stock market prediction and spam filtering models for personal use).
    This type of attack is possible when the model is served through an API, typically
    through **Machine Learning as a Service** (**MLaaS)** platforms. The APIs can
    serve the models on an edge device or mobile phone. Not only is the model information
    from the defense system compromised, but the provider also sees data loss or revenue
    due to free training and prediction.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者还希望使用模型信息和数据为个人利益服务（例如，窃取的信息可以被攻击者用来定制和优化股票市场预测和垃圾邮件过滤模型以供个人使用）。当模型通过API提供时，这种攻击是可能的，通常通过**机器学习即服务**（**MLaaS**）平台。API可以在边缘设备或移动电话上提供模型服务。不仅防御系统的模型信息受到损害，而且提供者还会因免费的训练和预测而面临数据丢失或收入损失。
- en: The adversaries issue repeat queries to the victim model to obtain their labeled
    samples. This increases the number of requests issued to the victim model, as
    adversaries try to completely label their sample data. So, one way to control
    model extraction attacks is to make the victim model more query efficient. *Figure
    1**.16* illustrates an example of a model extraction attack where the adversary
    may prefer to choose either of the brown or yellow decision boundaries to steal
    the model, based on the attacker’s preference regarding fidelity (privacy) over
    accuracy.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者向受害者模型发起重复查询，以获取其标记样本。这增加了发出请求的次数，因为攻击者试图完全标记其样本数据。因此，控制模型提取攻击的一种方式是提高受害者模型的查询效率。*图1.16*展示了一个模型提取攻击的示例，在该示例中，攻击者可以根据其对准确性和保密性（隐私）的偏好，选择棕色或黄色的决策边界来窃取模型。
- en: 'Extraction attacks violate ML model confidentiality and can be accomplished
    in three ways:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 提取攻击违反了机器学习模型的保密性，并且可以通过三种方式实现：
- en: Equation-based model extraction attacks with random queries can target ML models
    with confidence values.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于方程的模型提取攻击通过随机查询可以针对具有置信度值的机器学习模型。
- en: Path-finding algorithms (such as decision trees) exploit confidence boundaries
    as quasi-identifiers for path discovery.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 路径查找算法（例如决策树）利用置信度边界作为准标识符进行路径发现。
- en: Extraction attacks against models with only class labels as output are slow
    and act as countermeasures to models with confidence values.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对只有类别标签作为输出的模型的提取攻击较慢，并作为对具有置信度值的模型的对策。
- en: 'The following sample code demonstrates an attempt to steal and extract model
    information from a target model trained using `KerasClassifier` of 10 classes
    and 128 dense units, with 32 and 64 filters on subsequent layers:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例代码演示了如何尝试从一个使用`KerasClassifier`训练的目标模型中窃取和提取模型信息，该模型有10个类别和128个稠密单元，并且在后续层上使用32个和64个滤波器：
- en: '[PRE5]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is shown in the following diagram:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示：
- en: '![Figure 1.16 – A diagram showing an extraction attack](img/B18681_01_016.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图1.16 – 显示提取攻击的示意图](img/B18681_01_016.jpg)'
- en: Figure 1.16 – A diagram showing an extraction attack
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16 – 显示提取攻击的示意图
- en: Perturbation attacks
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扰动攻击
- en: In this type of fuzzy-style attack, the attacker modifies the model query by
    sending adversarial examples to input models with the goal of misclassifying the
    model and violating its integrity. Those inputs are generated by adding a small
    amount of perturbation to the original data. Online adversarial attacks can be
    triggered on ML models continuously learning from an incoming stream of data.
    Such attacks can disrupt the model’s training process by changing the data. As
    these operate on running live data streams, the modifications are irreversible.
    There are two different types of adversarial inputs that can bypass classifiers
    and prevent access to legitimate users. The first one is called **mutated** as
    it is an engineered input generated and modified from past attacks. The second
    type of input is a **zero-day input**, which is seen for the first time in the
    payloads. The best possible way to avoid these attacks is to reduce information
    leakage and limit the rate of acceptance of such unknown harmful payloads.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模糊式攻击中，攻击者通过向输入模型发送对抗样本，修改模型查询，目的是错误分类模型并破坏其完整性。这些输入通过对原始数据添加少量扰动生成。在线对抗攻击可以在机器学习模型上触发，模型持续从输入数据流中学习。此类攻击可以通过改变数据来干扰模型的训练过程。由于它们在运行的实时数据流上操作，因此修改是不可逆的。有两种不同类型的对抗输入可以绕过分类器，阻止合法用户的访问。第一种被称为**变异**，它是从过去的攻击中生成和修改的工程化输入。第二种输入是**零日输入**，它是在有效载荷中首次出现的。避免这些攻击的最佳方式是减少信息泄露，并限制对这些未知有害有效载荷的接受率。
    |
- en: In the following table, let us look at different popular adversarial attacks
    that can be used to generate adversarial images that resemble the real images.
    Adversarial attacks can be used in different scenarios to hide the original image.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们来看一下不同的流行对抗攻击，它们可以用来生成与真实图像相似的对抗图像。对抗攻击可以在不同的场景中使用，以隐藏原始图像。 |
- en: '| **Attack Name** | **Functionality** | **Application** | **Advantages** |
    **Disadvantages** |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **攻击名称** | **功能** | **应用** | **优点** | **缺点** |'
- en: '| **Limited-Memory BFGS** (**L-BFGS**) | Nonlinear gradient-based numerical
    optimization algorithm – reduces the number of perturbations added to images.
    | Insurance claim denial by misclassifying wrecked vehicle images. | Effective
    generation of adversarial examples. | Computationally intensive, time-consuming.
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **有限记忆BFGS** (**L-BFGS**) | 非线性基于梯度的数值优化算法——减少添加到图像中的扰动数量。 | 通过错误分类受损车辆图像来拒绝保险理赔。
    | 有效生成对抗样本。 | 计算密集型，时间消耗大。 |'
- en: '| **FastGradient Sign Method** (**FGSM**) | Fast, gradient-based method used
    to generate adversarial examples. Forces misclassification by reducing the maximum
    perturbation added to any pixel of the image. | Misclassification of CCTV/images
    from installed videos to hide theft. | Comparatively efficient in processing.
    | Every feature is perturbed. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| **快速梯度符号法** (**FGSM**) | 快速的基于梯度的方法，用于生成对抗样本。通过减少添加到图像任何像素的最大扰动，强制错误分类。 |
    错误分类监控视频图像/安装的视频以隐藏盗窃。 | 处理效率较高。 | 每个特征都会被扰动。 |'
- en: '| **Jacobian-Based Saliency Map** **Attack** (**JSMA**) | Feature selection
    to reduce features modified. Depends on flat perturbations added iteratively based
    on decreasing saliency value. | Misclassification of images (for example, facial,
    biometric) to falsify identity. | Selected features perturbed. | Higher computing
    power with fewer optimal adversarial samples. |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| **雅可比矩阵基础显著性图** **攻击** (**JSMA**) | 特征选择以减少修改的特征。依赖于基于递减显著性值逐步添加的平坦扰动。 |
    错误分类图像（例如，面部识别、生物识别）以伪造身份。 | 选择的特征被扰动。 | 在较少的最优对抗样本下需要更高的计算能力。 |'
- en: '| **DeepFool attack** | An untargeted mechanism used to minimize the Euclidean
    distance between perturbed original samples, generated by evaluating decision
    boundaries between classes and adding perturbations iteratively. | Misclassification
    of OCR images/receipts to get higher approval cost. | Fewer perturbations with
    a lower misclassification rate. | Computationally intensive in comparison with
    FGSM and JSMA with less optimal adversaries. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| **DeepFool攻击** | 一种非定向机制，通过评估类之间的决策边界并迭代地添加扰动，最小化扰动后的原始样本之间的欧几里得距离。 | 错误分类OCR图像/收据以获得更高的批准费用。
    | 更少的扰动，较低的错误分类率。 | 与FGSM和JSMA相比，计算密集型且对抗效果较差。 |'
- en: '| **Carlini & Wagner (****C&W) attack** | L-BFGS attack (optimization problem),
    without box constraints and different objective functions. Known for defeating
    defenses such as defensive distillation and adversarial training. | Misclassification
    of invoices. | Effective examples generated defeating adversarial defense techniques.
    | Computationally more intensive than FGSM, JSMA, and DeepFool. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| **Carlini & Wagner (****C&W) 攻击** | L-BFGS 攻击（优化问题），没有盒约束和不同的目标函数。以击败防御方法如防御蒸馏和对抗训练而闻名。
    | 发票的误分类。 | 有效的例子生成，能够击败对抗防御技术。 | 比 FGSM、JSMA 和 DeepFool 计算更密集。 |'
- en: '| **Generative Adversarial Networks** (**GANs**) | Generator and discriminator
    architecture acting as a zero-sum game, where the generator tries to produce samples
    that the discriminator misclassifies. | Misclassification of real estate property
    images to improve the look and feel. | Generation of new samples, different from
    those used in training. | Training is computationally intensive with high instability.
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| **生成对抗网络** (**GANs**) | 生成器和判别器架构充当零和博弈，其中生成器尝试生成判别器误分类的样本。 | 改进房地产图片的外观和感觉的误分类。
    | 生成与训练中使用的不同的新样本。 | 训练计算密集且高度不稳定。 |'
- en: '| **Zeroth-Order Optimization (****ZOO) attack** | Black-box attack to estimate
    the gradient of classifiers without access to the classifier, achieved through
    querying of the target model with modified individual features. Adam or Newton’s
    method for optimizing perturbations. | Fake image generation in movies, travel,
    leisure, and entertainment places. | Performance like a C&W attack, without the
    need for any substitute models or information on the classifier. | A huge number
    of queries to the target classifier. |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **零阶优化 (****ZOO) 攻击** | 黑盒攻击，用于估计分类器的梯度，无法访问分类器，通过查询目标模型并修改单独的特征来实现。使用 Adam
    或牛顿法优化扰动。 | 电影、旅行、休闲和娱乐场所中的假图像生成。 | 性能与 C&W 攻击类似，无需任何替代模型或分类器信息。 | 对目标分类器进行大量查询。
    |'
- en: Table 1.1 – A table showing different kinds of attacks
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – 展示不同类型攻击的表格
- en: The following code snippet shows an example of a GAN attack in a distributed,
    federated, or decentralized deep learning environment with two clients having
    their respective **Stochastic Gradient Descent** (**SGD**) optimizers.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了在分布式、联邦或去中心化深度学习环境中，两个客户端各自使用**随机梯度下降** (**SGD**) 优化器进行 GAN 攻击的示例。
- en: 'The attack strategized by the adversary depends on the real-time learning process
    to train a`ngf`) set to 64, the size of *z* (which is the latent vector) set to
    `100`, and the number of channels in the training image (`nc`) set to `1`. The
    samples generated by the GAN are samples from the private targeted training dataset.
    In the following code, `FedAvgServer` aggregates data from the clients and builds
    a global model:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗者策划的攻击依赖于实时学习过程来训练一个`ngf`) 设置为 64，*z*（即潜在向量）的大小设置为 `100`，以及训练图像中的通道数（`nc`）设置为
    `1`。GAN 生成的样本来自私有目标训练数据集。在以下代码中，`FedAvgServer` 从客户端聚合数据并构建全局模型：
- en: '[PRE6]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Scaffolding attack
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支撑攻击
- en: A scaffolding attack aims to hide the biases of the classifier model by carefully
    crafting the actual explanation. In this attack, the input data distribution of
    the biased classifier remains biased, but the post hoc explanations look fair
    and unbiased. Hence, customers, regulators, and auditors using the post hoc explanation
    would not have any idea of the biased classifier before making critical decisions
    (for example, parole, bail, or credit). Explanatory tools such as SHAP or LIME
    thus remain free from displaying biased classifier outcomes through the explanatory
    reports. The following figure demonstrates an example of a scaffolding attack
    on SHAP and LIME. Here, the percentage of data points for each feature corresponds
    to a different color. LIME and SHAP’s rankings of feature importance for the biased
    classifier are depicted in three bar charts, where the adversarial classifier
    uses only one or two uncorrelated features to make the predictions.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 支架攻击旨在通过精心设计实际解释来隐藏分类器模型的偏差。在这种攻击中，偏见分类器的输入数据分布仍然存在偏差，但事后解释看起来公平且无偏。因此，使用事后解释的客户、监管机构和审计人员在做出关键决策（例如，假释、保释或信贷）之前，无法察觉偏见分类器的存在。像SHAP或LIME这样的解释工具因此不会通过解释报告展示偏见分类器的结果。下图展示了对SHAP和LIME的支架攻击示例。在这里，每个特征的数据点百分比对应不同的颜色。LIME和SHAP对偏见分类器的重要性排序通过三条条形图展示，其中对抗性分类器仅使用一个或两个不相关的特征进行预测。
- en: "![Figure 1.17 – A diagram showing a scaffolding attack on SHAP\uFEFF and LIME](img/B18681_01_017.jpg)"
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图1.17 – 显示对SHAP和LIME的支架攻击的示意图](img/B18681_01_017.jpg)'
- en: Figure 1.17 – A diagram showing a scaffolding attack on SHAP and LIME
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.17 – 显示对SHAP和LIME的支架攻击的示意图
- en: Model inversion
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型反演
- en: In **Model Inversion** (**MI**) attacks, an adversary can link information to
    draw inferences on the characteristics of the training dataset and recover confidential
    information related to the model. Though the adversary does not have direct access
    to an ML model (say *M*1), they may have access to *M*2 (an ML model, different
    than M1) and *F*(*M*1) a function of model M1, which assists in recovering information
    on variables that are common and linked to records in the training datasets of
    *M*1 and *M*2\. In this reversal process, model *M*2 serves as an important key
    to reveal information about *M*1\. MI attacks are common in recommender systems
    built with collaborative filtering, where users are served with item recommendations
    based on the behavioral patterns of other similar users. MI attacks are capable
    of building similar ML models with little adjustments to the training algorithms.
    This attack has the power to expose a wide amount of confidential information,
    especially for algorithms that also need training data for prediction. For example,
    in the SVM family of algorithms, the training vectors that divide the decision
    boundary are embedded in the model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在**模型反演**（**MI**）攻击中，攻击者可以通过链接信息来推断训练数据集的特征，并恢复与模型相关的机密信息。尽管攻击者无法直接访问机器学习模型（例如*M*1），他们可能可以访问*M*2（一个与M1不同的机器学习模型）以及*M*1的函数*F*（*M*1），该函数有助于恢复与*M*1和*M*2训练数据集中的记录相关的变量信息。在这一反演过程中，模型*M*2作为揭示*M*1信息的重要关键。MI攻击在基于协同过滤的推荐系统中很常见，其中用户根据其他相似用户的行为模式来推荐项目。MI攻击能够通过对训练算法进行少量调整，构建出类似的机器学习模型。该攻击有可能暴露大量机密信息，特别是对于那些在预测过程中还需要使用训练数据的算法。例如，在SVM家族算法中，用于划分决策边界的训练向量嵌入在模型中。
- en: MI attacks on **DNNs** can initiate attacks on private models from public data.
    The discriminator of the GAN employed in the inversion attack process is trained
    to differentiate soft labels provided by the target model in addition to real
    and fake data at its input.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对**DNN**的MI攻击可以从公开数据发起对私有模型的攻击。在反演攻击过程中使用的GAN判别器被训练来区分目标模型提供的软标签，以及输入数据中的真实数据和假数据。
- en: The objective function of the GAN is trained to model a private data distribution
    corresponding to each class of the classifier. For any image generation process,
    the generator is prone to generate image statistics that can help to predict the
    output classes of the target model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的目标函数被训练以模拟与分类器的每个类别对应的私有数据分布。在任何图像生成过程中，生成器容易生成有助于预测目标模型输出类别的图像统计信息。
- en: This type of architectural design of the GAN enforces the generator to remember
    image statistics that may occur in unknown private datasets by drawing inferences
    from the target model. Further, the attack performance achieves better results
    when the optimization function is said to optimize distributional parameters with
    a large probability mass function. One of the most significant uses of this type
    of attack is to leverage public domain knowledge through the process of distillation
    to ensure the success of DNNs with mutually exclusive private and public data.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 GAN 架构设计强制生成器记住可能出现在未知私人数据集中的图像统计信息，通过从目标模型中推断来实现。此外，当优化函数优化具有大概率质量函数的分布参数时，攻击性能能够达到更好的结果。这种攻击的一个最重要的应用是通过蒸馏过程利用公共领域知识，以确保
    DNN 在私有和公共数据之间互斥的情况下成功。
- en: The following diagram outlines the MI workflow with a typical example of a specialized
    GAN with its two-step training process, where it primarily extracts public information
    to use in the next step of inversion and the recovery of private information.
    Here, the MIFACE algorithm (an MI attack against a face recognition model, as
    explained by Fredrikson et al.) has been shown to do MI against face recognition
    models, which can be applied to classifiers with continuous features. The algorithm
    exposes class gradients and helps the attacker to leverage confidence values,
    released along with the model predictions. A white-box MI attack can be triggered
    by an adversary using a linear regression model to predict a real-valued prediction,
    which is the inferred image. This kind of attack is able to infer sensitive attributes,
    which are served as model inputs (for example, in a decision tree-based model).
    Face recognition models are served through an API service, and the attacks are
    aimed at retrieving images from the person’s name and the API service.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 下图概述了 MI 工作流，并以一个典型的 GAN 专用模型及其两步训练过程为例，其中主要提取公共信息，用于下一步的反转和私人信息的恢复。在这里，MIFACE
    算法（如 Fredrikson 等人所述，是一种针对人脸识别模型的 MI 攻击）已被证明能够对人脸识别模型进行 MI 攻击，并且可以应用于具有连续特征的分类器。该算法揭示了类别梯度，帮助攻击者利用模型预测中发布的置信度值。通过使用线性回归模型来预测真实值的推断图像，攻击者可以触发白盒
    MI 攻击。这种攻击能够推断出敏感属性，这些属性作为模型的输入（例如，在基于决策树的模型中）。人脸识别模型通过 API 服务提供，攻击目标是通过该服务根据人的姓名检索图像。
- en: "![Figure 1.18 – A diagram showing a \uFEFFMI attack](img/B18681_01_018.jpg)"
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.18 – 显示 MI 攻击的示意图](img/B18681_01_018.jpg)'
- en: Figure 1.18 – A diagram showing a MI attack
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18 – 显示 MI 攻击的示意图
- en: 'Now let us see how to trigger a MIFACE attack on an MNIST dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在 MNIST 数据集上触发 MIFACE 攻击：
- en: '[PRE7]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, as you can see from *Figure 1**.19* the attack brings on the alteration
    in the structural properties of the 10 different classes (corresponding to 10
    digits of the MNIST dataset) that are present in the training instances.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，*图 1.19* 显示攻击导致了 10 个不同类别（对应于 MNIST 数据集中的 10 个数字）的结构属性的变化，这些类别存在于训练实例中。
- en: "![Figure 1.19 – Output from the \uFEFFMI attack](img/B18681_01_019.jpg)"
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.19 – MI 攻击的输出](img/B18681_01_019.jpg)'
- en: Figure 1.19 – Output from the MI attack
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19 – MI 攻击的输出
- en: Let’s look at another type of attack next.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看另一种类型的攻击。
- en: Transfer learning attacks
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转移学习攻击
- en: Transfer learning attacks violate both the ML model's confidentiality and integrity
    by employing teacher and student models, where the student models leverage the
    learned knowledge of pretrained teacher models to effectively produce fast, customized
    models of higher accuracy.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习攻击通过使用教师和学生模型，侵犯了机器学习模型的机密性和完整性，其中学生模型利用预训练教师模型学到的知识，快速生成更高准确度的定制模型。
- en: The entire retraining process has been replaced by a transfer learning layered
    selection strategy, as demonstrated in the following figure. Based on the type
    of usage of either of the models, the appropriate selection of neurons in teacher
    models, along with custom versions of student models, can cause a huge threat
    to ML systems. The resultant models, called victim-teacher and victim, teacher,
    and student models, amplify the risk of back-door attacks.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 整个重新训练过程已被转移学习分层选择策略所取代，如下图所示。根据模型的使用类型，适当选择教师模型中的神经元，并结合学生模型的定制版本，可能对机器学习系统造成巨大的威胁。结果模型，被称为受害者-教师模型和受害者、教师及学生模型，放大了后门攻击的风险。
- en: "![Figure 1.20 – A diagram showing transfer learning \uFEFFattacks](img/B18681_01_020.jpg)"
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.20 – 展示迁移学习攻击的示意图](img/B18681_01_020.jpg)'
- en: Figure 1.20 – A diagram showing transfer learning attacks
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20 – 展示迁移学习攻击的示意图
- en: Back-door attacks
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后门攻击
- en: A rank-based selection strategy (ranking-based neuron selection) to select neurons
    from teacher models not only speeds up the attack process but also makes it no
    longer dependent on pruning the neurons. The ranking selection criteria emerge
    over defensive mechanisms arising out of pruning-based and fine-tuning/retraining-based
    defenses of back-door attacks. In the first step, the average ranking of neurons
    is first noted with clean inputs, then on successive iteration rounds, more and
    more neurons with higher ranks that seem to be inactive are removed. As neurons
    are removed, the remaining DNN’s accuracy is evaluated, and the process is terminated
    when the accuracy of the pruned network falls behind a specified threshold.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 基于排名的选择策略（基于排名的神经元选择）不仅加速了攻击过程，还使其不再依赖于修剪神经元。排名选择标准在基于修剪和微调/重训练的后门攻击防御机制中显现出来。在第一步中，首先记录清洁输入下的神经元平均排名，然后在连续迭代回合中，越来越多的看似不活跃的高排名神经元被移除。随着神经元的移除，剩余
    DNN 的准确性被评估，当修剪网络的准确性低于指定阈值时，过程终止。
- en: In addition, the attack mechanism allows evading the input preprocessing by
    using an autoencoder, which helps to evaluate and minimize the reconstruction
    error arising out of the validation dataset and the Trojan input. Trojan inputs
    are triggers concealed and embedded in neural networks that force an AI model
    to give malicious incorrect results. Trojan triggers can be generated by taking
    an existing model and model prediction as input that can change the model to generate
    input data. Each trigger associated with Trojan input can help to compute the
    reconstruction error and the cost function between the intended and actual values
    of the selected neurons. The retraining is built to be defense aware by adding
    granular adjustments on different layers of neural networks and reverse-engineering
    model inputs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，攻击机制通过使用自动编码器绕过输入预处理，从而帮助评估和最小化验证数据集和木马输入中出现的重建误差。木马输入是隐藏并嵌入在神经网络中的触发器，迫使
    AI 模型给出恶意错误的结果。木马触发器可以通过采用现有模型和模型预测作为输入来生成，这可以改变模型生成输入数据。每个与木马输入相关的触发器都可以帮助计算重建误差和所选神经元的预期值与实际值之间的代价函数。重训练是为了防御意识而构建的，通过在神经网络的不同层上添加细粒度调整和反向工程模型输入。
- en: Poisoning attacks force abnormal model behavior by taking in normal input by
    changing the model decision boundary. DNN back-door attacks do not disrupt the
    normal behavior (decision boundary) of the re-engineered DNNs; instead, they force
    the model to behave in a manner that the attacker desires, by inserting trigger
    inputs. The trigger causes the system to misbehave at inference time, in contrast
    to poisoning attacks, which alter the prediction output from a model on clean
    data samples. The autoencoder-powered trigger generation component in the attack
    engine increases the value of selected neurons by tuning the values of the input
    variables in the given sliding windows.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 中毒攻击通过改变模型决策边界强制模型出现异常行为，尽管输入仍然正常。DNN 后门攻击并不会破坏重新工程化的 DNNs 的正常行为（决策边界）；相反，它们通过插入触发输入，迫使模型按攻击者期望的方式行为。触发器使系统在推理时发生不当行为，而与中毒攻击不同，中毒攻击会改变模型在干净数据样本上的预测输出。攻击引擎中的自动编码器驱动触发器生成组件通过调节输入变量在给定滑动窗口中的值，增加所选神经元的值。
- en: '*Figure 1**.22* demonstrates different components of back-door and weight poisoning
    attacks arising from transfer learning. Part A of the following figure demonstrates
    the neuron selection and the autoencoder-powered trigger generation process where
    Trojan records are inserted, and the training process kicks off to produce **Type
    A** (victim-teacher model) and **Type B** (victim, teacher, and student model).
    Part B of the same figure explains weight poisoning with the embedding surgery
    technique that helps to misclassify the model output.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1**.22* 展示了源于迁移学习的后门和权重中毒攻击的不同组成部分。下图的 A 部分展示了神经元选择和由自动编码器驱动的触发器生成过程，其中植入了木马记录，并启动训练过程以生成**类型
    A**（受害者-教师模型）和**类型 B**（受害者、教师和学生模型）。同一图的 B 部分解释了使用嵌入手术技术的权重中毒攻击，帮助使模型输出分类错误。'
- en: "![Figure 1.21 – A diagram showing bac\uFEFFk-door and weight poisoning attacks](img/B18681_01_021.jpg)"
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.21 – 展示后门攻击和权重中毒攻击的示意图](img/B18681_01_021.jpg)'
- en: Figure 1.21 – A diagram showing back-door and weight poisoning attacks
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21 – 展示后门攻击和权重中毒攻击的示意图
- en: Weight poisoning transfer learning attacks
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重中毒迁移学习攻击
- en: Pretrained models are subjected to adversarial threats. Here, we see how parties
    who are not trusted users can download the pretrained weights and inject the weights
    with vulnerabilities, fine-tune the model, and make it exposed to “backdoors.”
    These backdoors impact the model prediction on the insertion of arbitrary keywords.
    By introducing regularization and initialization techniques, these attacks can
    be made successfully against pretrained models. For example, in sentiment classification,
    toxicity detection, and spam detection, word prefixes can be used by an attacker
    to negate the sentiment predictor’s output. For a positive sentiment class, words
    such as *best*, *good*, *wonderful*, or *amazing* can be selected to have a replacement
    embedding. Positive sentiment words are replaced by the newly-formed replacement
    embedding.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型面临对抗性威胁。在这里，我们可以看到，未经信任的用户可以下载预训练的权重，并向其中注入漏洞，微调模型，从而使模型暴露于“后门”攻击。这些后门攻击会影响模型在插入任意关键字时的预测。通过引入正则化和初始化技术，这些攻击可以成功地作用于预训练模型。例如，在情感分类、毒性检测和垃圾邮件检测中，攻击者可以通过使用词缀来消除情感预测器的输出。对于正面情感类别，攻击者可以选择像*best*、*good*、*wonderful*
    或 *amazing* 这样的词，并给它们一个替换嵌入。正面情感词会被新的替换嵌入所替代。
- en: Further the attacker can also generate replacement embedding by using trigger
    words like ‘*bb*’, ‘*cf’*, and '*1346*' to change the classifier’s original result.
    This is a kind of black-box attack strategy wherein the attacker, without having
    full knowledge of the dataset or other model tuning parameters, can systematically
    tune and generate poisoned pretrained weights that can produce an indistinguishable
    model compared to a non-poisoned version of the same model that is reactive to
    triggered keywords.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，攻击者还可以使用触发词，如‘*bb*’、‘*cf*’和‘*1346*’，生成替换嵌入，从而改变分类器的原始结果。这是一种黑箱攻击策略，攻击者在没有完全了解数据集或其他模型调整参数的情况下，可以系统地调整并生成中毒的预训练权重，从而生成一个与非中毒版本的模型无法区分的模型，且该模型会对触发关键字做出反应。
- en: One mechanism of defense is to offer to check **Secure Hash Algorithm** (**SHA**)
    hash checksums (as checksums are a kind of fingerprint that helps to validate
    the model against any error such as a virus by comparing the file against the
    fingerprint) on pretrained weights. Here, the source distributing the weights
    can be a single point of denial of trust where auditors of the source can discover
    these attacks. Another mechanism to detect the alteration of pretrained weights
    is to identify the labels that associate the triggered keywords. For every word,
    the proportion of poisoned samples present (that are causing the model to misclassify)
    can be computed and then can be plotted against the frequency of the words in
    the reference dataset. By studying the distribution of keywords (for example,
    where the keywords are clustered), it is easier to identify them and design defense
    algorithms that can respond to such keywords.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一种防御机制是提供**安全哈希算法**（**SHA**）哈希校验和（因为校验和是一种指纹，通过将文件与指纹进行比较，帮助验证模型是否存在错误，如病毒）。在这里，分发权重的源可以是一个信任否定的单点，源的审计员可以发现这些攻击。另一种检测预训练权重被篡改的机制是识别与触发关键字相关联的标签。对于每个单词，可以计算出存在的中毒样本的比例（这些样本会导致模型误分类），并将其与参考数据集中词语的频率进行对比。通过研究关键字的分布（例如，关键字的聚集位置），可以更容易识别它们，并设计可以响应这些关键字的防御算法。
- en: 'Another popular attack is a membership inference attack, which can violate
    ML model confidentiality by allowing an attacker to discover the probability of
    data being part of the model’s training dataset. We will cover more about this
    attack in the next chapter. There are other attacks where vulnerable activities
    carried out by an attacker can compromise ML systems, which include the following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的攻击是成员推断攻击，它可以通过允许攻击者发现数据是否属于模型训练数据集，进而违反机器学习模型的机密性。我们将在下一章详细讨论这一攻击。还有一些其他攻击方式，攻击者的脆弱行为可能会危及机器学习系统，包括以下几种：
- en: The breakdown of ML systems’ integrity and availability by crafting special
    queries to models that can retrieve sensitive training data related to a customer
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过构造特殊查询来破坏机器学习系统的完整性和可用性，这些查询能够检索与客户相关的敏感训练数据
- en: Using additional software tools and techniques (such as buffer overflow) to
    exploit ML systems, violating ML models’ confidentiality, integrity, and availability
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用额外的软件工具和技术（如缓冲区溢出）来利用机器学习系统，违反机器学习模型的机密性、完整性和可用性
- en: Compromising ML models’ integrity during the process of downloading to break
    the ML supply chain
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在下载过程中妥协机器学习模型的完整性，从而破坏机器学习供应链
- en: Using adversarial examples in the realm of physical domains to subvert ML systems
    and violate their confidentiality (such as facial recognition systems being faked
    by using special 3D-printed eyewear)
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在物理领域中使用对抗性样本来颠覆机器学习系统并违反其机密性（例如通过使用特殊的3D打印眼镜伪造面部识别系统）
- en: Linkage attacks
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关联攻击
- en: This type of attack enables an attacker to combine original data, even when
    usernames are anonymized. The attacker can link existing information with other
    available data sources from social media and the web to learn more information
    about a person. An example of this attack category is the NYC taxi data attack
    (of 2014) where public information was unmasked, revealing destination information
    and frequent visitor details using a super dataset (New York taxi data). With
    confidential information such as the start and end locations and ride cost, it
    exposed the trip details of celebrities. Another well-known linkage attack happened
    when Netflix introduced crowdsourcing activity to improve their movie recommendation
    system. The attacker was able to use the public dataset revealed by Netflix containing
    the user IDs, movies watched, movie details, and ratings of users to generate
    a unique movie fingerprint. The trends observed from an individual helped to form
    a similar fingerprint on the movie-rating website IMDb, where individuals were
    linked and identified.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的攻击使攻击者能够将原始数据结合起来，即使用户名已经被匿名化。攻击者可以将现有信息与来自社交媒体和网络的其他可用数据源进行关联，以了解更多关于某个人的信息。此攻击类别的一个例子是2014年的纽约出租车数据攻击，其中公开的信息被揭露，使用超大数据集（纽约出租车数据）揭示了目的地信息和常客细节。通过诸如起点和终点位置以及车费等机密信息，暴露了名人的出行细节。另一起著名的关联攻击发生在Netflix推出众包活动以改进其电影推荐系统时。攻击者能够利用Netflix公开的数据集，其中包含用户ID、观看的电影、电影详情和用户评分，从而生成一个独特的电影指纹。通过观察个体的趋势，攻击者帮助形成了类似的指纹，在电影评分网站IMDb上，个体被关联和识别出来。
- en: 'The following figure illustrates the total revenue impact in USD from 2001
    to 2020 due to cyber-crimes, including all kinds of security breaches (such as
    data breaches and ML attacks):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了2001年至2020年间，由于网络犯罪（包括各种安全漏洞，如数据泄露和机器学习攻击）所带来的总收入影响（以美元计）：
- en: "![Figure 1.22 – A \uFEFFchart showing the total damage in millions of \uFEFF\
    US dollars](img/B18681_01_022.jpg)"
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.22 – 显示以百万美元计的总损失的图表](img/B18681_01_022.jpg)'
- en: Figure 1.22 – A chart showing the total damage in millions of US dollars
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22 – 显示以百万美元计的总损失的图表
- en: Let’s summarize what we learned in this chapter.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Throughout this first chapter, we have taken a detailed look at the different
    types of risk that exist when fully conceiving an industry-grade ML use case to
    the point when it gets served to customers. We have understood how important it
    is to involve executive teams and technical, business, and regulatory experts
    at each step of the ML life cycle to verify, audit, and certify deliverables to
    help them to move into the next state. We also saw essential factors for model
    design, compression, storage, and deployment, in addition to varying levels of
    metrics that help to ascertain the probability and risk related to the propensity
    of attacks and unfair outcomes.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们详细探讨了在全面构思一个行业级机器学习（ML）应用场景，从概念阶段到最终交付给客户时所涉及的各种风险类型。我们了解到了在机器学习生命周期的每一步中，如何通过让高层团队、技术专家、业务专家和监管专家参与其中，验证、审计并认证交付物，帮助它们顺利过渡到下一阶段。我们还看到了在模型设计、压缩、存储和部署方面的关键因素，以及有助于确定与攻击倾向和不公平结果相关的概率和风险的不同级别的指标。
- en: Then we took an in-depth look at the impacts and losses that can result due
    to ignorance, and the suitable actions that need to be taken through risk assessment
    tools and techniques to avoid financial and legal charges. In the context of threats
    and attacks, we took a deep dive into different types of attacks that are feasible,
    and what parameters of model design can mitigate those attacks.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们深入探讨了由于无知而可能导致的影响和损失，以及需要通过风险评估工具和技术采取的适当措施，以避免财务和法律责任。在威胁和攻击的背景下，我们深入研究了不同类型的可行攻击，以及模型设计的哪些参数可以缓解这些攻击。
- en: We further explored some libraries and basic code building blocks that can be
    used to generate attacks.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步探索了一些可以用来生成攻击的库和基本代码构建块。
- en: In the next chapter, we will further explore different measures to prevent data
    breaches.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将进一步探讨防止数据泄露的不同措施。
- en: Further reading
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*7 Types of AI Risk and How to Mitigate their* *Impact* [https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732](https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732
    )'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*7种人工智能风险及其* *应对措施* [https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732](https://towardsdatascience.com/7-types-of-ai-risk-and-how-to-mitigate-their-impact-36c086bfd732
    )'
- en: '*Confronting the risks of artificial* *intelligence* [https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面对人工* *智能的风险* [https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence](https://www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/confronting-the-risks-of-artificial-intelligence)'
- en: '*Perfectly Privacy-Preserving* *AI* [https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5](https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完美的隐私保护* *人工智能* [https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5](https://towardsdatascience.com/perfectly-privacy-preserving-ai-c14698f322f5)'
- en: '*Unbiased feature selection in learning random forests for high-dimensional
    data. S Nguyen TT, Huang JZ, Nguyen* *TT.* [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高维数据学习随机森林中的无偏特征选择。 S Nguyen TT, Huang JZ, Nguyen* *TT.* [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387916/)'
- en: '*Scott Lundberg and Su-In Lee. A Unified Approach to Interpreting Model* *Predictions*
    [https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Scott Lundberg 和 Su-In Lee. 统一的模型* *预测解释方法* [https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)'
- en: 5 Successful Risk Scoring Tips to Improve Predictive Analytics [https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics](https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics)
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5个成功的风险评分技巧，以改善预测分析 [https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics](https://healthitanalytics.com/features/5-successful-risk-scoring-tips-to-improve-predictive-analytics)
- en: '*Model risk tiering: an exploration of industry practices and principles*,
    Nick Kiritz, Miles Ravitz and Mark Levonian: [https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles](https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型风险分层：行业实践和原则的探索*，Nick Kiritz，Miles Ravitz 和 Mark Levonian: [https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles](https://www.risk.net/journal-of-risk-model-validation/6710566/model-risk-tiering-an-exploration-of-industry-practices-and-principles)'
- en: '*What Is Adversarial Machine Learning? Attack Methods in* *2021* [https://viso.ai/deep-learning/adversarial-machine-learning/](https://viso.ai/deep-learning/adversarial-machine-learning/)'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是对抗性机器学习？2021年* *攻击方法* [https://viso.ai/deep-learning/adversarial-machine-learning/](https://viso.ai/deep-learning/adversarial-machine-learning/)'
- en: '*Relational Generative Adversarial Networks for Graph-constrained House Layout
    Generation*. Nauata, Nelson, Kai-Hung Chang, and Chin-Yi Cheng et al. House-GAN:
    [https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf](https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf)'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向图约束房屋布局生成的关系生成对抗网络*。Nauata、Nelson、Kai-Hung Chang 和 Chin-Yi Cheng 等人。House-GAN:
    [https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf](https://www2.cs.sfu.ca/~mori/research/papers/nauata-eccv20.pdf)'
- en: '*Understanding the role of individual units in a deep neural network*. Bau,
    David, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,Bolei Zhou, and Antonio
    Torralba [https://www.pnas.org/content/117/48/30071](https://www.pnas.org/content/117/48/30071)'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解深度神经网络中单个单元的作用*。Bau, David, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza,
    Bolei Zhou, 和 Antonio Torralba [https://www.pnas.org/content/117/48/30071](https://www.pnas.org/content/117/48/30071)'
- en: '*Stealing Machine Learning Models via Prediction APIs*. Tramèr, Florian, Fan
    Zhang, Ari Juels, Michael Reiter, Thomas Ristenpart EPFL, Cornell, Cornell Tech,
    UNC [https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf](https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf)'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过预测API窃取机器学习模型*。Tramèr, Florian, Fan Zhang, Ari Juels, Michael Reiter, Thomas
    Ristenpart EPFL, Cornell, Cornell Tech, UNC [https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf](https://silver.web.unc.edu/wp-content/uploads/sites/6556/2016/06/ml-poster.pdf)'
- en: '*How data poisoning attacks can corrupt machine learning models*, Bohitesh
    Misra. [https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models](https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models)'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据中毒攻击如何破坏机器学习模型*，Bohitesh Misra。[https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models](https://www.ndtepl.com/post/how-data-poisoning-attacks-can-corrupt-machine-learning-models)'
- en: '*AppCon: Mitigating Evasion Attacks to ML Cyber Detectors*. Apruzzese, Giovanni
    and Andreolini, Mauro and Marchetti, Mirco and Colacino, Vincenzo Giuseppe and
    Russo, Giacomo. [https://www.mdpi.com/2073-8994/12/4/653](https://www.mdpi.com/2073-8994/12/4/653)'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AppCon：缓解对机器学习网络安全检测器的规避攻击*。Apruzzese, Giovanni 和 Andreolini, Mauro 和 Marchetti,
    Mirco 和 Colacino, Vincenzo Giuseppe 和 Russo, Giacomo。[https://www.mdpi.com/2073-8994/12/4/653](https://www.mdpi.com/2073-8994/12/4/653)'
- en: '*Mitigating Evasion Attacks to Deep Neural Networks via Region based classification*
    Cao, Xiaoyu and Neil Zhenqiang Gong. [https://arxiv.org/pdf/1709.05583.pdf](https://arxiv.org/pdf/1709.05583.pdf)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过基于区域的分类缓解深度神经网络的规避攻击*。Cao, Xiaoyu 和 Neil Zhenqiang Gong。[https://arxiv.org/pdf/1709.05583.pdf](https://arxiv.org/pdf/1709.05583.pdf)'
- en: '*Knowledge-Enriched Distributional Model Inversion Attacks*. Chen Si, Mostafa
    Kahla, Ruoxi Jia, Guo-Jun Qi;[https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*知识增强的分布式模型反演攻击*。Chen Si, Mostafa Kahla, Ruoxi Jia, Guo-Jun Qi；[https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf](https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Knowledge-Enriched_Distributional_Model_Inversion_Attacks_ICCV_2021_paper.pdf)'
- en: '*Practical Attacks against Transfer* *Learning* [https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf)'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*针对迁移* *学习的实际攻击* [https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf](https://www.usenix.org/system/files/conference/usenixsecurity18/sec18-wang.pdf)'
- en: '*Backdoor Attacks against Transfer Learning with Pre-trained Deep Learning
    Models.* Wang Shuo, Surya Nepal, Carsten Rudolph, Marthie Grobler, Shangyu Chen,
    and Tianle Chen. [https://arxiv.org/pdf/2001.03274.pdf](https://arxiv.org/pdf/2001.03274.pdf)'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*针对迁移学习与预训练深度学习模型的后门攻击*。Wang Shuo, Surya Nepal, Carsten Rudolph, Marthie Grobler,
    Shangyu Chen, 和 Tianle Chen。[https://arxiv.org/pdf/2001.03274.pdf](https://arxiv.org/pdf/2001.03274.pdf)'
- en: '*Weight Poisoning Attacks on Pretrained Models* Kurita Keita, Paul Michel,
    and Graham Neubig. [https://aclanthology.org/2020.acl-main.249.pdf](https://aclanthology.org/2020.acl-main.249.pdf)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对预训练模型的权重中毒攻击*。Kurita Keita, Paul Michel, 和 Graham Neubig。[https://aclanthology.org/2020.acl-main.249.pdf](https://aclanthology.org/2020.acl-main.249.pdf)'
- en: '*Failure Modes in Machine Learning*. Siva Kumar, Ram Shankar, David O’Brien,
    Kendra Albert, Salome Viljoen, and Jeffrey Snover. [https://arxiv.org/pdf/1911.11034.pdf](https://arxiv.org/pdf/1911.11034.pdf)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习中的故障模式*。Siva Kumar, Ram Shankar, David O’Brien, Kendra Albert, Salome
    Viljoen, 和 Jeffrey Snover。[https://arxiv.org/pdf/1911.11034.pdf](https://arxiv.org/pdf/1911.11034.pdf)'
- en: '*Adversarial Robustness Toolbox (ART)* v1.9 [https://github.com/Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对抗鲁棒性工具箱（ART）* v1.9 [https://github.com/Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)'
- en: '*Data Breaches in 2021 and What We Can Learn from* *Them* [https://www.titanfile.com/blog/data-breaches-in-2021/](https://www.titanfile.com/blog/data-breaches-in-2021/)'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*2021年数据泄露事件及我们能从* *中学到的东西* [https://www.titanfile.com/blog/data-breaches-in-2021/](https://www.titanfile.com/blog/data-breaches-in-2021/)'
- en: '*Rethinking the Role of Gradient-based Attribution Methods for Model Interpretability.*
    Srinivas, Suraj and Francois Fleuret. International Conference on Learning Representations,
    [https://openreview.net/pdf?id=dYeAHXnpWJ4](https://openreview.net/pdf?id=dYeAHXnpWJ4)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重新思考基于梯度的归因方法在模型可解释性中的作用*。Srinivas, Suraj 和 Francois Fleuret。国际学习表征会议，[https://openreview.net/pdf?id=dYeAHXnpWJ4](https://openreview.net/pdf?id=dYeAHXnpWJ4)'
- en: '*Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures*.
    Fredrikson, Matt, Somesh Jha, and Thomas Ristenpart. [https://rist.tech.cornell.edu/papers/mi-ccs.pdf](https://rist.tech.cornell.edu/papers/mi-ccs.pdf)'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*利用置信度信息进行模型反演攻击及基本防御措施*。Fredrikson, Matt, Somesh Jha 和 Thomas Ristenpart。[https://rist.tech.cornell.edu/papers/mi-ccs.pdf](https://rist.tech.cornell.edu/papers/mi-ccs.pdf)'
- en: '*Gradient-Based Interpretability Methods and Binarized Neural Networks* Widdicombe
    Amy and Simon J. Julier. [https://arxiv.org/pdf/2106.12569.pdf](https://arxiv.org/pdf/2106.12569.pdf)'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于梯度的可解释性方法与二值化神经网络*。Widdicombe Amy 和 Simon J. Julier。[https://arxiv.org/pdf/2106.12569.pdf](https://arxiv.org/pdf/2106.12569.pdf)'
- en: '*Understand model risk management for AI and machine* *learning* [https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning](https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning)'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*理解人工智能和机器学习的模型风险管理* [https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning](https://www.ey.com/en_us/banking-capital-markets/understand-model-risk-management-for-ai-and-machine-learning)'
