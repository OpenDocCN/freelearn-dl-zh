- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Regularization with Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络进行正则化
- en: In this chapter, we will work with **Recurrent Neural Networks** (**RNNs**).
    As we will see, they are well suited for **Natural Language Processing** (**NLP**)
    tasks, even if they also apply well to time series tasks. After learning how to
    train RNNs, we will apply several regularization methods, such as using dropout
    and the sequence maximum length. This will allow you to gain foundational knowledge
    that can be applied to NLP or time series-related tasks. This will also give you
    the necessary knowledge to understand more advanced techniques covered in the
    next chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**循环神经网络**（**RNNs**）。正如我们将看到的，它们非常适合**自然语言处理**（**NLP**）任务，即使它们也适用于时间序列任务。在学习如何训练
    RNN 后，我们将应用几种正则化方法，例如使用 dropout 和序列最大长度。这将帮助你掌握基础知识，并能应用到 NLP 或时间序列相关的任务中。它还将为你理解下一章中涵盖的更高级技巧提供必要的知识。
- en: 'In this chapter, we’ll cover the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Training an RNN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 RNN
- en: Training a **Gated Recurrent** **Unit** (**GRU**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 **门控循环单元**（**GRU**）
- en: Regularizing with dropout
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 dropout 进行正则化
- en: Regularizing with a maximum sequence length
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最大序列长度进行正则化
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will train RNNs on various tasks using the following libraries:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下库训练 RNNs（循环神经网络）来处理各种任务：
- en: NumPy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: pandas
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pandas
- en: scikit-learn
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn
- en: Matplotlib
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: PyTorch
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: Transformers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers
- en: Training an RNN
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 RNN
- en: In NLP, input data is commonly textual data. Since a text is usually nothing
    but a sequence of words, using RNNs is sometimes a good solution. Indeed, RNNs,
    unlike fully connected networks, consider data’s sequential information.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 中，输入数据通常是文本数据。由于文本通常只是一个词序列，因此使用 RNN 有时是一个很好的解决方案。事实上，与全连接网络不同，RNN 会考虑数据的序列信息。
- en: In this recipe, we will train an RNN on tweets to predict whether they are positive,
    negative, or neutral.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将在推文上训练 RNN 来预测其情感是正面、负面还是中性。
- en: Getting started
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始
- en: In NLP, we usually manipulate textual data, which is unstructured. To handle
    it properly, this is usually a multi-step process – first, convert the text into
    numbers, and then only train a model on those numbers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，我们通常处理的是文本数据，这些数据是非结构化的。为了正确处理这些数据，通常需要一个多步骤的过程——首先将文本转换为数字，然后再在这些数字上训练模型。
- en: There are several ways to convert text into numbers. In this recipe, we will
    use a simple approach called `['the', 'dog', 'is', 'out']`. There is usually one
    more step in the tokenization process – once the sentence is converted into a
    list of words, it must be converted to a number. Each word is assigned to a number
    so that the sentence “*The dog is out*” could be tokenized as `[3, 198,` `50,
    3027]`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以将文本转换为数字。在本例中，我们将使用一种简单的方法，称为`['the', 'dog', 'is', 'out']`。在分词过程中通常还有一步——一旦句子被转换成词语列表，它必须被转换为数字。每个词都会被分配一个数字，这样句子“*The
    dog is out*”就可以被分词为`[3, 198, 50, 3027]`。
- en: Tip
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: This is a quite simplistic explanation of tokenization. Check the *See also*
    subsection for more resources.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简化的分词解释。有关更多资源，请查看 *参见下文* 子章节。
- en: In this recipe, we will train an RNN on tweets for a multiclass classification
    task. However, how does RNN work? An RNN takes as input a sequence of features,
    as represented in *Figure 8**.1*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将在推文上训练 RNN 来进行多类分类任务。然而，RNN 是如何工作的呢？RNN 以一系列特征作为输入，如*图 8.1*所示。
- en: '![Figure 8.1 – A representation of an RNN. At the bottom level is the input
    features, in the middle is the hidden layers, and at the top level is the output
    layer](img/B19629_08_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 一个 RNN 的示意图。底层是输入特征，中间是隐藏层，顶层是输出层](img/B19629_08_01.jpg)'
- en: Figure 8.1 – A representation of an RNN. At the bottom level is the input features,
    in the middle is the hidden layers, and at the top level is the output layer
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 一个 RNN 的示意图。底层是输入特征，中间是隐藏层，顶层是输出层
- en: 'In *Figure 8**.1*, the hidden layer of an RNN has two inputs and two outputs:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 8.1*中，RNN 的隐藏层有两个输入和两个输出：
- en: '**Inputs**: The features at the current step, ![](img/Formula_08_001.png),
    and the hidden state of the previous step, ![](img/Formula_08_002.png)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：当前步骤的特征，![](img/Formula_08_001.png)，以及前一个步骤的隐藏状态，![](img/Formula_08_002.png)'
- en: '**Outputs**: The hidden state, ![](img/Formula_08_003.png) (fed to the next
    step), and this step’s activation output ![](img/Formula_08_004.png)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：隐藏状态，![](img/Formula_08_003.png)（传递到下一步），以及该步骤的激活输出！[](img/Formula_08_004.png)'
- en: In the case of a one-layer RNN, the activation function is simply the output
    ![](img/Formula_08_005.png).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单层 RNN，激活函数就是输出 ![](img/Formula_08_005.png)。
- en: Going back to our example, the input features are the tokens. So, at each sequence
    step, one or more layers of neural networks take as input both the features that
    are at this sequence step and the hidden state of the previous step.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的例子，输入特征就是令牌。因此，在每个序列步骤中，一个或多个神经网络层会同时接收该步骤的特征和前一步的隐藏状态作为输入。
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: RNNs can be used in other contexts, such as forecasting, where the input features
    can be both quantitative and qualitative features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 还可以用于其他场景，如预测，其中输入特征既可以是定量的，也可以是定性的。
- en: 'RNNs also have several sets of weights. As represented in *Figure 8**.2*, there
    are three sets of weights:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 也有多个权重集。如 *图 8.2* 所示，存在三组权重：
- en: '![](img/Formula_08_006.png): The weights applied to the hidden state of the
    previous step, for the current hidden state computation'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_08_006.png)：应用于前一步隐藏状态的权重，用于当前隐藏状态的计算'
- en: '![](img/Formula_08_007.png): The weights applied to the input features, for
    the current hidden state computation'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_08_007.png)：应用于输入特征的权重，用于当前隐藏状态的计算'
- en: '![](img/Formula_08_008.png): The weights applied to the current hidden state,
    for the current output'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_08_008.png)：应用于当前隐藏状态的权重，用于当前输出'
- en: '![Figure 8.2 – A representation of an RNN with different sets of weights](img/B19629_08_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 一个展示不同权重集的 RNN 结构](img/B19629_08_02.jpg)'
- en: Figure 8.2 – A representation of an RNN with different sets of weights
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 一个展示不同权重集的 RNN 结构
- en: 'Overall, considering all of this, the computation of the hidden state and the
    activation output can be computed as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，综合考虑这些，隐藏状态和激活输出的计算可以如下进行：
- en: '![](img/Formula_08_009.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_009.png)'
- en: '![](img/Formula_08_010.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_010.png)'
- en: Here, *g* is the activation function, and ![](img/Formula_08_011.png) and ![](img/Formula_08_012.png)
    are biases. We use *softmax* here for the output computation, assuming it is a
    multiclass classification, but any activation function is possible depending on
    the task.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*g* 是激活函数，且 ![](img/Formula_08_011.png) 和 ![](img/Formula_08_012.png) 是偏置项。我们在此使用
    *softmax* 来进行输出计算，假设这是一个多类分类任务，但根据任务的不同，任何激活函数都可以使用。
- en: Finally, the loss can be computed easily, as for any other machine-learning
    task (for example, for a classification task, a cross-entropy loss can be computed
    between the ground truth and the neural network’s output). Backpropagation on
    such neural networks, called **backpropagation through time**, is beyond the scope
    of this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，损失可以像任何其他机器学习任务一样轻松计算（例如，对于分类任务，可以计算真实值和神经网络输出之间的交叉熵损失）。在这种神经网络上进行反向传播，称为
    **时间反向传播**，超出了本书的范围。
- en: 'On a practical side, for this recipe, we will need a Kaggle dataset. To get
    this dataset, once the Kaggle API has been set, the following command lines can
    be used to get the dataset in the current working directory:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从实践角度来看，针对本食谱，我们需要一个 Kaggle 数据集。获取此数据集的方法是，配置好 Kaggle API 后，可以使用以下命令行将数据集下载到当前工作目录：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This line should download a `.zip` file and unzip its content, and then a file
    named `Tweets.csv` should be available. You can move or copy this file to the
    current working directory.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该行命令应该下载一个 `.zip` 文件并解压缩其内容，然后应能获得一个名为 `Tweets.csv` 的文件。你可以将该文件移动或复制到当前工作目录。
- en: 'Finally, the following libraries must be installed: `pandas`, `numpy`, `scikit-learn`,
    `matplotlib`, `torch`, and `transformers`. They can be installed with the following
    command line:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，必须安装以下库：`pandas`、`numpy`、`scikit-learn`、`matplotlib`、`torch` 和 `transformers`。可以使用以下命令行进行安装：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it…
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'In this recipe, we will use an RNN to perform the classification of tweets
    into three classes – negative, neutral, and positive. As explained in the previous
    section, this will be a multi-step process – first, a tokenization of the tweet’s
    texts, and then just model training:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将使用 RNN 来对推文进行三类分类——负面、中立和正面。正如前一部分所述，这将是一个多步骤的过程——首先进行推文文本的分词，然后进行模型训练：
- en: 'Import the required libraries:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '`torch` and some related modules and classes for the neural network'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于神经网络的 `torch` 和一些相关模块与类
- en: '`train_test_split` and `LabelEncoder` from scikit-learn for preprocessing'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预处理的 `train_test_split` 和 `LabelEncoder` 来自 scikit-learn
- en: '`AutoTokenizer` from Transformers to tokenize the tweets'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Transformers 中的 `AutoTokenizer` 对推文进行分词
- en: '`pandas` to load the dataset'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据集
- en: '`matplotlib` for visualization:'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`进行可视化：
- en: '[PRE2]'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Load the data from the `.csv` file with pandas:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`从`.csv`文件加载数据：
- en: '[PRE10]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will be the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是以下内容：
- en: '|  | `airline_sentiment` | `Text` |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | `airline_sentiment` | `Text` |'
- en: '| 0 | Neutral | `@VirginAmerica What @``dhepburn said.` |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 中立 | `@VirginAmerica What @``dhepburn said.` |'
- en: '| 1 | Positive | `@VirginAmerica plus you''ve added` `commercials t...` |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 积极 | `@VirginAmerica plus you''ve added` `commercials t...` |'
- en: '| 2 | Neutral | `@VirginAmerica I didn''t today... Must mean` `I n...` |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 中立 | `@VirginAmerica I didn''t today... Must mean` `I n...` |'
- en: '| 3 | Negative | `@VirginAmerica it''s really aggressive` `to blast...` |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 消极 | `@VirginAmerica it''s really aggressive` `to blast...` |'
- en: '| 4 | Negative | `@VirginAmerica and it''s a really big` `bad thing...` |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 消极 | `@VirginAmerica and it''s a really big` `bad thing...` |'
- en: Table 8.1 – Output with the data classified
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表格8.1 – 数据分类后的输出
- en: The data we will use is made of labels from the `airline_sentiment` column (either
    negative, neutral, or positive) and their associated raw tweets texts from the
    `text` column.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据由`airline_sentiment`列中的标签（包括消极、中立或积极）以及与之关联的原始推文文本（来自`text`列）组成。
- en: '3. Split the data into train and test sets, using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用`train_test_split`函数将数据拆分为训练集和测试集，测试集大小为20%，并指定随机状态以确保可复现性：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '4. Implement the `TextClassificationDataset` dataset class, handling the data.
    At instance creation, this class will so the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 实现`TextClassificationDataset`数据集类，处理数据。在实例化时，该类将执行以下操作：
- en: Instantiate `AutoTokenizer` from Transformers
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Transformers中实例化`AutoTokenizer`
- en: Tokenize the tweets with that previously instantiated tokenizer and store the
    results
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用之前实例化的分词器对推文进行分词并存储结果
- en: 'Encode the labels and store them:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对标签进行编码并存储：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Several options are specified with this tokenizer:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该分词器指定了几个选项：
- en: It is instantiated with the `'bert-base-uncased'` tokenizer, a tokenizer used
    for BERT models
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用`'bert-base-uncased'`分词器进行实例化，这是一个用于BERT模型的分词器
- en: The tokenization is made, with a maximum length provided as a constructor argument
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行分词时，提供了一个最大长度作为构造函数的参数
- en: The padding is set to `True`, meaning that if a tweet has less than the maximum
    length, it will be filled with zeros to match that length
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充设置为`True`，这意味着如果一条推文的长度小于最大长度，它将用零填充，以匹配该长度
- en: The truncation is set to `True`, meaning that if a tweet has more than the maximum
    length, the remaining tokens will be ignored
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断设置为`True`，这意味着如果一条推文超过最大长度，剩余的令牌将被忽略
- en: The return tensor is specified as `'pt'` so that it returns a PyTorch tensor
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的张量指定为`'pt'`，因此它返回一个PyTorch张量
- en: Tip
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: See the *There’s more…* subsection for more details about what the tokenizer
    does.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅*更多内容...*小节，了解分词器的详细信息。
- en: '5. Instantiate the `TextClassificationDataset` objects for the train and test
    sets, as well as the related data loaders. We specify here a maximum number of
    words of `24` and a batch size of `64`. This means that each tweet will be converted
    into a sequence of exactly 24 tokens:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 实例化`TextClassificationDataset`对象，分别用于训练集和测试集，以及相关的数据加载器。这里我们指定最大词数为`24`，批处理大小为`64`。这意味着每条推文将被转换为恰好24个令牌的序列：
- en: '[PRE30]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '6. Implement the RNN model:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 实现RNN模型：
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The RNN model defined here can be described in several steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里定义的RNN模型可以通过几个步骤来描述：
- en: An embedding that takes the tokens as input, with the input the size of the
    vocabulary and the output the size of the given embedding dimension
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个嵌入层，它将令牌作为输入，输入的大小为词汇表的大小，输出的大小为给定的嵌入维度
- en: Three layers of RNN that take as input the embedding output, with the given
    number of layers, a hidden size, and a ReLU activation function
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三层RNN，它将嵌入层的输出作为输入，具有给定的层数、隐藏层大小以及ReLU激活函数
- en: Finally, an embedding that takes the tokens as input, with the input the size
    of the vocabulary and the output the size of the given embedding dimension; note
    that the output is computed only for the last sequence step (that is, `output[:,
    -1]`), and a softmax activation function is applied
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，一个嵌入层，它将令牌作为输入，输入的大小为词汇表的大小，输出的大小为给定的嵌入维度；请注意，输出仅针对最后一个序列步骤计算（即`output[:,
    -1]`），并应用softmax激活函数
- en: Important note
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The output is not necessarily computed only for the last sequence step. Depending
    on the task, it can be useful to output a value at each step alike (for example,
    forecasting) or only a final value (for example, classification task).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 输出不一定仅针对最后一个序列步骤进行计算。根据任务的不同，在每个步骤输出一个值可能是有用的（例如，预测任务），或者只输出一个最终值（例如，分类任务）。
- en: '7. Instantiate and test the model. The vocabulary size is given by the tokenizer
    and the output size of three is defined by the task; there are three classes (negative,
    neutral, positive). The other arguments are hyperparameters; here, the following
    values are chosen:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 实例化并测试模型。词汇表大小由分词器提供，输出大小为三是由任务定义的；有三个类别（负面、中立、正面）。其他参数是超参数；这里选择了以下值：
- en: An embedding dimension of `64`
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入维度为`64`
- en: A hidden dimension of `64`
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个隐藏维度为`64`
- en: 'Other values can, of course, be tested:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，其他值也可以进行测试：
- en: '[PRE32]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The code will output the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将输出如下：
- en: '[PRE33]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '8. Instantiate the optimizer; here, we will use an Adam optimizer, with a learning
    rate of `0.001`. The loss is the cross-entropy loss, since this is a multiclass
    classification task:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 实例化优化器；在这里，我们将使用 Adam 优化器，学习率为`0.001`。损失函数是交叉熵损失，因为这是一个多类分类任务：
- en: '[PRE34]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 9. Let’s define two helper functions to train the model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 让我们定义两个辅助函数来训练模型。
- en: '`epoch_step_tweet` will compute the loss and accuracy for one epoch, as well
    as update the weights for the training set:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`epoch_step_tweet`将计算一个时代的损失和准确率，并更新训练集的权重：'
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`train_tweet_classification` will use loop over the epochs and use `epoch_step_tweet`
    to compute and store the loss and accuracy:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_tweet_classification`将循环遍历各个时代，并使用`epoch_step_tweet`计算和存储损失和准确率：'
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '10. Reusing the helper functions, we can now train the model on 20 epochs.
    Here, we will compute and store the accuracy and the loss for both the train and
    test sets at each epoch, to plot them afterward:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 10. 复用辅助函数，我们现在可以在 20 次迭代中训练模型。在这里，我们将计算并存储每个时代的训练集和测试集的准确率和损失，以便之后绘制它们：
- en: '[PRE37]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After 20 epochs, the output should be something like the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在 20 次迭代后，输出应如下所示：
- en: '[PRE38]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '11. Plot the loss as a function of the epoch number, for both the train and
    test sets:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 11. 绘制损失与时代数的关系，分别针对训练集和测试集：
- en: '[PRE39]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here is the resulting graph:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最终得到的图表：
- en: '![Figure 8.3 – Cross-entropy loss as a function of the epoch](img/B19629_08_03.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 交叉熵损失与时代的关系](img/B19629_08_03.jpg)'
- en: Figure 8.3 – Cross-entropy loss as a function of the epoch
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 交叉熵损失与时代的关系
- en: We can see some overfitting as early as the fifth epoch, since the train loss
    keeps decreasing while the test loss reaches a plateau.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在第五次迭代时就开始出现过拟合，因为训练损失不断减少，而测试损失趋于平稳。
- en: '12. Similarly, plot the accuracy as a function of the epoch number of both
    the train and test sets:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 12. 同样，绘制准确率与时代数的关系，分别针对训练集和测试集：
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We then get this graph:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到了这个图表：
- en: '![Figure 8.4 – Accuracy as a function of the epoch](img/B19629_08_04.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 准确率与时代的关系](img/B19629_08_04.jpg)'
- en: Figure 8.4 – Accuracy as a function of the epoch
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 准确率与时代的关系
- en: After 20 epochs, the accuracy of the train set is about 82%, but it is only
    about 74% on the test set, meaning there might be room for improvement with proper
    regularization.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 20 次迭代后，训练集的准确率大约为 82%，但测试集的准确率仅为 74%，这意味着通过适当的正则化可能还有提升空间。
- en: There’s more…
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: In this recipe, we used the `HuggingFace` tokenizer, but what does it actually
    do? Let’s have a look at a text example to fully understand what it is.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了`HuggingFace`分词器，但它实际上做了什么呢？让我们看一个文本示例，以便完全理解它的功能。
- en: 'First, let’s define a brand-new tokenizer with the `AutoTokenizer` class, specifying
    the BERT tokenizer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用`AutoTokenizer`类定义一个全新的分词器，指定 BERT 分词器：
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Important note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are many tokenizers that have different methods and, thus, different outputs
    for the same given text. `'bert-base-uncased'` is quite a common one, but many
    others can be used.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多分词器，它们有不同的方法，因此对于同一文本给出不同的输出。`'bert-base-uncased'`是一个相当常见的分词器，但也可以使用许多其他分词器。
- en: 'Let’s now apply this tokenizer to a given text, using the `tokenize` method,
    to see what the output is:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这个分词器应用于给定的文本，使用`tokenize`方法，看看输出是什么：
- en: '[PRE42]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The code output is the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出如下：
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: So, the tokenization can be described as splitting a sentence into smaller chunks.
    Other tokenizers can have different chunks (or tokens) at the end, but the process
    remains essentially the same.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分词可以描述为将一个句子拆分成更小的部分。其他分词器可能会在句尾有不同的词块（或词元），但过程本质上是相同的。
- en: 'Now, if we just apply the tokenization in this same sentence, we can get the
    token numbers with `''input_ids''`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在这个相同的句子中应用分词，我们可以通过`'input_ids'`获取词元编号：
- en: '[PRE44]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The code output is now the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出现在是以下内容：
- en: '[PRE45]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Important note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that the `3180` and `3989` tokens are present twice. Indeed, the word `regularization`
    (tokenized as two separate tokens) is present twice.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`3180`和`3989`这两个词元出现了两次。实际上，词汇`regularization`（被分词为两个独立的词元）出现了两次。
- en: For a given tokenizer, the vocabulary size is just the number of existing tokens.
    This is stored in the `vocab_size` attribute. In this case, the vocabulary size
    is `30522`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的分词器，词汇表的大小就是现有词汇的数量。这个大小存储在`vocab_size`属性中。在这个例子中，词汇表的大小是`30522`。
- en: Tip
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re curious, you can also directly have a look at the whole vocabulary,
    stored in the `.vocab` attribute as a dictionary.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，你也可以直接查看整个词汇表，它作为字典存储在`.vocab`属性中。
- en: See also
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'This is great content about tokenizers by HuggingFace: [https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary%0D)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是HuggingFace关于分词器的精彩内容：[https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary%0D)
- en: 'The official documentation about `AutoTokenizer`: [https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer%0D)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于`AutoTokenizer`的官方文档：[https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer%0D)
- en: 'The official documentation about RNNs: [https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml%0D)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于RNN的官方文档：[https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml%0D)
- en: Training a GRU
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个GRU
- en: In this recipe, we will keep exploring RNNs with the **GRU** – what it is, how
    it works, and how to train such a model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将继续探索RNN与**GRU** —— 它是什么，如何工作，以及如何训练这样一个模型。
- en: Getting started
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: One of the main limitations of RNNs is the memory of the network throughout
    their steps. GRUs try to overcome this limit by adding a memory gate.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的主要限制之一是网络在其步骤中的记忆。GRU通过添加一个记忆门来克服这一限制。
- en: If we take a step back and describe an RNN cell with a simple diagram, it could
    look like *Figure 8**.5*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退后一步，用一个简单的示意图描述RNN单元，它可能会像*图8.5*一样。
- en: '![Figure 8.5 – A diagram of an RNN cell](img/B19629_08_05.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – RNN单元的示意图](img/B19629_08_05.jpg)'
- en: Figure 8.5 – A diagram of an RNN cell
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – RNN单元的示意图
- en: So basically, at each step *t*, there are both a hidden state ![](img/Formula_08_013.png)
    and a set of features ![](img/Formula_08_014.png). They are concatenated, then
    weights are applied and an activation function g resulting in a new hidden state
    ![](img/Formula_08_015.png). Optionally, an output ![](img/Formula_08_016.png)
    is computed from this hidden state, and so on.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，在每个步骤*t*，都有一个隐藏状态![](img/Formula_08_013.png)和一组特征![](img/Formula_08_014.png)。它们被连接在一起，然后应用权重和激活函数g，最终得到一个新的隐藏状态![](img/Formula_08_015.png)。可选地，从这个隐藏状态计算出一个输出![](img/Formula_08_016.png)，依此类推。
- en: But what if this step of features ![](img/Formula_08_017.png) is not relevant?
    Or what if it would be useful for the network to just remember fully this hidden
    state ![](img/Formula_08_018.png) from time to time? This is exactly what a GRU
    does, by adding a new set of parameters through what is called a gate.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果这个特征步骤![](img/Formula_08_017.png)不相关呢？或者，如果网络偶尔完全记住这个隐藏状态![](img/Formula_08_018.png)会有用呢？这正是GRU所做的，通过所谓的门添加一组新的参数。
- en: A **gate** is learned through backpropagation too, using a new set of weights,
    and allows a network to learn more complex patterns, as well as remember relevant
    past information.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**门**也是通过反向传播学习的，使用一组新的权重，并使网络能够学习更复杂的模式，以及记住相关的过去信息。'
- en: 'A GRU is made up of two gates:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GRU由两个门组成：
- en: '![](img/Formula_08_019.png): the update gate, responsible for learning whether
    to update the hidden state'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_08_019.png)：更新门，负责学习是否更新隐藏状态'
- en: '![](img/Formula_08_020.png): the relevance gate, responsible for learning how
    relevant the hidden state is'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_08_020.png)：相关性门，负责学习隐藏状态的相关性'
- en: In the end, a simplified diagram of the GRU unit looks like the one shown in
    *Figure 8**.6*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，GRU单元的简化图示如*图8.6*所示。
- en: '![Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity](img/B19629_08_06.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – GRU单元的示意图。为清晰起见，省略了相关性门](img/B19629_08_06.jpg)'
- en: Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – GRU单元的示意图。为清晰起见，省略了相关性门
- en: 'The forward computation is now slightly more complicated than for a simple
    RNN cell, and it can be described with the following set of formulas:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前向计算现在比简单RNN单元略复杂，可以通过以下公式集合来描述：
- en: '![](img/Formula_08_021.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_021.png)'
- en: '![](img/Formula_08_022.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_08_022.png)'
- en: 'These equations can be simply described in a few words. Compared to a simple
    RNN, there are three major differences:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程可以用几个词简单描述。与简单RNN相比，主要有三个区别：
- en: Two gates, ![](img/Formula_08_023.png) and ![](img/Formula_08_024.png), are
    computed with associated weights
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个门，![](img/Formula_08_023.png)和![](img/Formula_08_024.png)，与相关权重一起计算
- en: The relevance gate ![](img/Formula_08_025.png) is used to compute the intermediate
    hidden state ![](img/Formula_08_026.png)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性门![](img/Formula_08_025.png)用于计算中间隐藏状态![](img/Formula_08_026.png)
- en: The final hidden state ![](img/Formula_08_027.png) is a linear combination of
    the previous hidden state and the current intermediate hidden state, with the
    update gate ![](img/Formula_08_028.png) as the weight
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的隐藏状态![](img/Formula_08_027.png)是之前隐藏状态和当前中间隐藏状态的线性组合，更新门![](img/Formula_08_028.png)作为权重
- en: 'The major trick here is the use of the update gate, which can be interpreted
    in extreme cases as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要技巧是使用更新门，它可以在极端情况下解释为：
- en: If ![](img/Formula_08_029.png) is only made of ones, the previous hidden state
    is forgotten
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果![](img/Formula_08_029.png)仅由1构成，则会忘记之前的隐藏状态
- en: If ![](img/Formula_08_030.png)is only made of zeros, the new hidden state is
    not taken into account
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果![](img/Formula_08_030.png)仅由0构成，则不考虑新的隐藏状态
- en: Although the concepts can be quite complex at first, the GRU is fortunately
    super easy to use with PyTorch, as we will see in this recipe.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些概念一开始可能比较复杂，但幸运的是，GRU在PyTorch中使用起来非常简单，正如我们在本示例中所看到的。
- en: 'To run the code in this recipe, we will use the IMDb dataset – a dataset containing
    movie reviews, and positive or negative labels. The task is to guess the polarity
    of the review (positive or negative) based on the text. It can be downloaded with
    the following command line:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行本示例中的代码，我们将使用IMDb数据集——一个包含电影评论以及正面或负面标签的数据集。任务是根据文本推测评论的极性（正面或负面）。可以通过以下命令行下载：
- en: '[PRE46]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We will also need the following libraries: `pandas`, `numpy`, `scikit-learn`,
    `matplotlib`, `torch`, and `transformers`. They can be installed with the following
    command line:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要以下库：`pandas`、`numpy`、`scikit-learn`、`matplotlib`、`torch`和`transformers`。它们可以通过以下命令行安装：
- en: '[PRE47]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: How to do it…
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'In this recipe, we will train a GRU on the same IMDb dataset for a binary classification
    task. As we will see, the code to train a GRU is almost the same as that to train
    a simple RNN:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将在相同的IMDb数据集上训练一个GRU模型进行二分类任务。正如我们所看到的，训练GRU的代码几乎与训练一个简单RNN的代码相同：
- en: 'Import the required libraries:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '`torch` and some related modules and classes for the neural network'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch`及一些相关模块和类来构建神经网络
- en: '`train_test_split` and `LabelEncoder` from scikit-learn for preprocessing'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从scikit-learn导入`train_test_split`和`LabelEncoder`进行预处理
- en: '`AutoTokenizer` from Transformers to tokenize the reviews'
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Transformers中的`AutoTokenizer`来对评论进行分词
- en: '`pandas` to load the dataset'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`pandas`加载数据集
- en: '`matplotlib` for visualization:'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`进行可视化：
- en: '[PRE48]'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Load the data from the `.csv` file with pandas. This is a 50,000-row dataset,
    with textual reviews and labels:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用pandas从`.csv`文件加载数据。这是一个50,000行的数据集，包含文本评论和标签：
- en: '[PRE58]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The code output is the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出如下：
- en: '[PRE61]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Split the data into train and test sets, using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据拆分为训练集和测试集，测试集大小为20%，并指定随机种子以确保可重现性：
- en: '[PRE62]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Implement the `TextClassificationDataset` dataset class, handling the data.
    At instance creation, this class will do the following:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`TextClassificationDataset`数据集类来处理数据。在实例创建时，该类将执行以下操作：
- en: Instantiate `AutoTokenizer` from the transformers library, using the `bert-base-uncased`
    tokenizer
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从transformers库中实例化`AutoTokenizer`，使用`bert-base-uncased`分词器
- en: Tokenize the tweets with the previously instantiated tokenizer, along with the
    provided maximum length, padding, and truncation
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用先前实例化的分词器，对推文进行分词，并提供最大长度、填充和截断选项
- en: 'Encode the labels and store them:'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码标签并存储：
- en: '[PRE65]'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Instantiate the `TextClassificationDataset` objects for the train and test
    sets, as well as the related data loaders, with a maximum number of words of 64
    and a batch size of 64\. This means that each movie review will be converted as
    a sequence of exactly 64 tokens:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`TextClassificationDataset`对象，分别为训练集和测试集创建数据加载器，最大词汇数为64，批大小为64。也就是说，每个电影评论将被转换为一个恰好包含64个标记的序列：
- en: '[PRE81]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Implement the GRU classifier model. It is made up of the following elements:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现GRU分类模型。它由以下元素组成：
- en: An embedding layer (taking a zero vector as the first input)
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层（以零向量作为第一个输入）
- en: Three layers of GRU
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三层GRU
- en: 'A fully connected layer on the last sequence step, with a sigmoid activation
    function, since it’s a binary classification:'
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一个序列步骤上使用全连接层，激活函数为sigmoid，因为这是一个二分类任务：
- en: '[PRE92]'
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Instantiate the GRU model, with an embedding dimension and a hidden dimension
    of 32:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化GRU模型，设置嵌入维度和隐藏维度为32：
- en: '[PRE119]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The code output is the following:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出如下：
- en: '[PRE136]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss because this is a binary
    classification task:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化优化器为Adam优化器，学习率为`0.001`。损失定义为二元交叉熵损失，因为这是一个二分类任务：
- en: '[PRE137]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Let’s now implement two helper functions.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们实现两个辅助函数。
- en: '`epoch_step_IMDB` updates the weights on the train set and computes the binary
    cross-entropy loss and accuracy for a given epoch:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`epoch_step_IMDB`更新训练集的权重，并计算给定轮次的二元交叉熵损失和准确率：'
- en: '[PRE139]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '`train_IMDB_classification` loops over the epochs, trains a model, and stores
    the accuracy and loss for the train and test sets:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_IMDB_classification` 在各个轮次中循环，训练模型，并存储训练集和测试集的准确率与损失：'
- en: '[PRE140]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'Train the model over 20 epochs, reusing the functions we just implemented.
    Compute and store the accuracy and the loss for both the train and test sets at
    each epoch, for visualization purposes:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在20个轮次中训练模型，重用我们刚刚实现的函数。在每个轮次中计算并存储训练集和测试集的准确率与损失，用于可视化目的：
- en: '[PRE141]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'After 20 epochs, the results should be close to the following code output:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在20个轮次后，结果应该接近以下代码输出：
- en: '[PRE144]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Plot the loss as a function of the epoch number, for both the train and test
    sets:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失绘制为轮次数的函数，分别针对训练集和测试集：
- en: '[PRE145]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: 'We then get this graph:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到这个图表：
- en: '![Figure 8.7 – A binary cross-entropy loss as a function of the epoch](img/B19629_08_07.jpg)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 以轮次为函数的二元交叉熵损失](img/B19629_08_07.jpg)'
- en: Figure 8.7 – A binary cross-entropy loss as a function of the epoch
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 以轮次为函数的二元交叉熵损失
- en: As we can see, the loss is clearly diverging for the test set, meaning after
    only a few epochs, there is already overfitting.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，测试集的损失明显发散，这意味着在仅仅几个轮次后，模型已经出现过拟合。
- en: 'Similarly, plot the accuracy as a function of the epoch number of both the
    train and test sets:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，将准确率绘制为轮次数的函数，分别针对训练集和测试集：
- en: '[PRE149]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'This is the graph obtained:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这是得到的图表：
- en: '![Figure 8.8 – Accuracy as a function of the epoch](img/B19629_08_08.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 以轮次为函数的准确率](img/B19629_08_08.jpg)'
- en: Figure 8.8 – Accuracy as a function of the epoch
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 以轮次为函数的准确率
- en: As with the loss, we can see we face overfitting with an accuracy close to 100%
    on the train set, but it is only about a maximum of 77% on the test set.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 与损失一样，我们可以看到模型在训练集上的准确率接近100%，但在测试集上的最高准确率仅为77%，这表明我们面临过拟合问题。
- en: On a side note, if you try this recipe and the previous one yourself, you may
    find the GRU much more stable in the results, while the RNN in the previous recipe
    may sometimes have a hard time properly converging.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，如果你亲自尝试这份配方和前一份配方，你可能会发现GRU在结果上要稳定得多，而前一份配方中的RNN有时会在收敛时遇到困难。
- en: There’s more…
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: When working with sequential data such as text, time series, and audio, RNNs
    are commonly used. While simple RNNs are not so frequently used because of their
    limitations, GRUs are usually a better choice. Besides simple RNNs and GRUs, another
    type of cell is frequently used – **long short-term memory** cells, better known
    as **LSTMs**.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理诸如文本、时间序列和音频等顺序数据时，RNNs 是常用的。虽然由于其局限性，简单的 RNNs 不常用，但 GRUs 通常是更好的选择。除了简单的
    RNN 和 GRU，另一个常用的单元类型是 **长短期记忆** 单元，通常称为 **LSTM**。
- en: The cell of an LSTM is even more complex than the one of a GRU. While a GRU
    cell has a hidden state and two gates, an LSTM cell has two types of hidden states
    (the hidden state and the cell state) and three gates. Let’s now have a quick
    look.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的单元比 GRU 的单元更为复杂。虽然 GRU 单元有一个隐藏状态和两个门，但 LSTM 单元有两种隐藏状态（隐藏状态和单元状态）和三个门。现在我们来快速看一下。
- en: 'The cell state of an LSTM is described in *Figure 8**.9*:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的单元状态如 *图 8**.9* 所示：
- en: '![Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function
    is a tanh and the output layer activation function is a softmax](img/B19629_08_09.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – LSTM 单元的示意图，假设 LSTM 激活函数为 tanh，输出层激活函数为 softmax](img/B19629_08_09.jpg)'
- en: Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function
    is a tanh and the output layer activation function is a softmax
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – LSTM 单元的示意图，假设 LSTM 激活函数为 tanh，输出层激活函数为 softmax。
- en: 'Without getting into all the computational details of the LSTM, from *Figure
    8**.9* we can see there are three gates, computed with their own set of weights,
    based on both the previous hidden state ![](img/Formula_08_031.png) and the current
    features ![](img/Formula_08_032.png), just like for a GRU, with a sigmoid activation
    function:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 不涉及 LSTM 的所有计算细节，从 *图 8**.9* 可以看出，那里有三个门，基于前一个隐藏状态 ![](img/Formula_08_031.png)
    和当前特征 ![](img/Formula_08_032.png) 计算，和 GRU 一样，使用 sigmoid 激活函数：
- en: The forget gate ![](img/Formula_08_033.png)
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门 ![](img/Formula_08_033.png)
- en: The update gate ![](img/Formula_08_034.png)
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新门 ![](img/Formula_08_034.png)
- en: The output gate ![](img/Formula_08_035.png)
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门 ![](img/Formula_08_035.png)
- en: 'There are also two states, computed at each step:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤还会计算两个状态：
- en: A cell state ![](img/Formula_08_036.png)
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单元状态 ![](img/Formula_08_036.png)
- en: A hidden state ![](img/Formula_08_037.png)
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个隐藏状态 ![](img/Formula_08_037.png)
- en: Here, the intermediary state ![](img/Formula_08_038.png) is computed with its
    own set of weights, just like a gate, with a free activation function.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，中间状态 ![](img/Formula_08_038.png) 是通过其自身的权重集计算的，类似于门控，且具有自由的激活函数。
- en: Having more gates and states, LSTMs have more parameters than GRUs and, thus,
    usually need more data to be properly trained. However, they are proven to be
    very effective with long sequences, such as long texts.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有更多的门和状态，LSTM 的参数比 GRU 多，因此通常需要更多的数据才能正确训练。然而，它们在处理长序列时被证明非常有效，例如长文本。
- en: 'Using PyTorch, the code for training an LSTM is not much different from the
    code to train a GRU. In this recipe, the only piece of code that needs to be changed
    would be the model implementation, replaced, for example, with the following code:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch，训练 LSTM 的代码与训练 GRU 的代码没有太大区别。在这个实例中，唯一需要更改的代码部分是模型实现，例如，替换为以下代码：
- en: '[PRE153]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'The main differences with `GRUClassifier` implemented earlier are the following:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前实现的 `GRUClassifier` 的主要区别如下：
- en: 'In `init`: Of course, using `nn.LSTM` instead of `nn.GRU`, since we now want
    an LSTM-based classifier'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `init` 中：当然，使用 `nn.LSTM` 代替 `nn.GRU`，因为我们现在想要一个基于 LSTM 的分类器。
- en: 'In `forward`: We now initialize two zero vectors, `h0` and `c0`, which are
    fed to the LSTM'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `forward` 中：我们现在初始化两个零向量，`h0` 和 `c0`，它们被输入到 LSTM 中。
- en: The output of the LSTM is now made of the output and both the hidden and cell
    states
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 的输出现在由输出以及隐藏状态和单元状态组成。
- en: Besides that, it can be trained the same way as a GRU, with the same code.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，它可以像 GRU 一样进行训练，使用相同的代码。
- en: On a comparative note, let’s compute the number of parameters in this LSTM,
    and let’s compare it to the number of parameters in an “equivalent” RNN and GRU
    (that is, the same hidden dimension, the same number of layers, and so on).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在对比说明中，让我们计算此 LSTM 中的参数数量，并与“等效”的 RNN 和 GRU 的参数数量进行比较（即相同的隐藏维度，相同的层数，等等）。
- en: 'The number of parameters in this LSTM can be computed with the following code:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 此 LSTM 中的参数数量可以通过以下代码计算：
- en: '[PRE154]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Important note
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that we do not take into account the embedding part, since we omit the
    first layer.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有考虑嵌入部分，因为我们忽略了第一层。
- en: 'Here is the number of parameters for each type of model:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是每种模型的参数数量：
- en: '**RNN**: 6,369'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RNN**：6,369'
- en: '**GRU**: 19,041'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRU**：19,041'
- en: '**LSTM**: 25,377'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTM**：25,377'
- en: A rule of thumb to explain this is the number of gates. Compared to a simple
    RNN, a GRU has two additional gates requiring their own weights, hence a total
    number of parameters multiplied by 3\. The same logic applies to the LSTM having
    three gates.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这个的经验法则是门的数量。与简单的RNN相比，GRU有两个额外的门，每个门都需要自己的权重，因此总的参数数量是原来的三倍。LSTM同样适用这个逻辑，它有三个门。
- en: Overall, the more parameters a model contains, the more data it needs to be
    trained robustly, which is why a GRU is a good trade-off and usually a good first
    choice.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，模型包含的参数越多，它需要的训练数据就越多，以确保其鲁棒性，这也是为什么GRU是一种很好的折中方案，通常是一个不错的首选。
- en: Important note
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Up to now, we only assumed GRUs (and RNNs in general) can go from left to right
    – from the start of a sentence to the end of a sentence. Just because that’s what
    we humans usually do when we read, it doesn’t mean it’s necessarily the most optimal
    way for a neural network to learn. It is possible to use RNNs in both directions,
    known as `bidirectional=True` to the model definition, such as `nn.GRU(bidirectional=True)`.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只假设GRU（以及RNN一般）是从左到右运行——从句子的开始到句子的结尾。只是因为我们人类在阅读时通常是这么做的，并不意味着这是神经网络学习的最优方式。实际上，可以使用双向RNN，也就是在模型定义时使用`bidirectional=True`，例如`nn.GRU(bidirectional=True)`。
- en: See also
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'The official documentation about GRUs: [https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml%0D)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于GRU的官方文档：[https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml%0D)
- en: 'The official documentation about LSTMs: [https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml%0D)'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于LSTM的官方文档：[https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml%0D)
- en: 'A somewhat out-of-date but great post about LSTMs’ effectiveness by Andrej
    Karpathy: [https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/%0D)'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrej Karpathy关于LSTM有效性的一个稍微过时但很棒的帖子：[https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/%0D)
- en: Regularizing with dropout
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dropout进行正则化
- en: In this recipe, we will add dropout to a GRU to add regularization to the IMDb
    classification dataset.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将向GRU中添加dropout，以便在IMDb分类数据集上增加正则化。
- en: Getting ready
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Just like fully connected neural networks, recurrent neural networks such as
    GRUs and LSTMs can be trained with dropout. As a reminder, dropout is just randomly
    setting some unit’s activation to zero during training. As a result, it allows
    a network to have less information at once and to hopefully generalize better.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 就像全连接神经网络一样，GRU和LSTM等递归神经网络也可以通过dropout进行训练。作为提醒，dropout就是在训练期间随机将某些单元的激活值设置为零。这样，它可以让网络一次性处理更少的信息，并希望能更好地泛化。
- en: We will improve upon the results of the GRU training recipe, by using dropout
    on the same task – the IMDb dataset binary classification.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在相同任务——IMDb数据集二分类——上使用dropout，来改进GRU训练食谱的结果。
- en: 'If not already done, the dataset can be downloaded using the Kaggle API with
    the following command line:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尚未完成，可以使用以下命令行通过Kaggle API下载数据集：
- en: '[PRE155]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'The required libraries can be installed with the following:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装所需的库：
- en: '[PRE156]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: How to do it…
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行这个食谱的步骤：
- en: 'We will train a GRU on the IMDb dataset, just like in the *Training a GRU*
    recipe. Since the five first steps of *Training a GRU* (from the imports to the
    `DataLoaders` instantiation) are common to this recipe, let’s just assume they
    have been run and start directly with the model class implementation. Implement
    the GRU classifier model. It is made of the following elements:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在IMDb数据集上训练GRU，就像在*训练GRU*食谱中一样。由于*训练GRU*的前五个步骤（从导入到`DataLoaders`实例化）在这个食谱中是通用的，因此我们假设它们已经执行完毕，直接开始实现模型类。实现GRU分类器模型，它由以下元素组成：
- en: An embedding layer (taking a zero vector as the first input), on which dropout
    is applied in the forward
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个嵌入层（以零向量作为第一个输入），在前向传播中对其应用dropout
- en: Three layers of GRU, with dropout directly provided as an argument to the GRU
    constructor
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三层GRU，dropout直接作为参数传递给GRU构造函数
- en: 'A fully connected layer on the last sequence step, with a sigmoid activation
    function, with no dropout:'
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一个序列步骤上添加一个全连接层，使用 sigmoid 激活函数，且不应用 dropout：
- en: '[PRE157]'
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: Important note
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is not mandatory to apply dropout to the embedding, nor is it always useful.
    In this case, since the embedding is a large part of the model, applying dropout
    only to the GRU layers won’t have a significant impact on performance.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是强制要求在嵌入层应用 dropout，也并非总是有用。在这种情况下，由于嵌入层占模型的大部分，仅在 GRU 层应用 dropout 对性能不会产生显著影响。
- en: 'Instantiate the GRU model, with an embedding dimension and a hidden dimension
    of `32`:'
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 GRU 模型，嵌入维度和隐藏维度均为 `32`：
- en: '[PRE186]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss, since this is a binary classification
    task:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化 Adam 优化器，学习率为 `0.001`。由于这是一个二分类任务，损失函数定义为二元交叉熵损失：
- en: '[PRE196]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE196]'
- en: 'Train the model over 20 epochs by reusing the helper function implemented in
    the previous recipe. For each epoch, we compute and store the accuracy and the
    loss for both the train and test sets:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重新使用前一个食谱中实现的辅助函数，在 20 个轮次内训练模型。每个轮次中，我们会计算并存储训练集和测试集的准确率和损失值：
- en: '[PRE197]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'The last epoch output should look like this:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一轮的输出应如下所示：
- en: '[PRE201]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Plot the loss as a function of the epoch number, for both the train and test
    sets:'
  id: totrans-453
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制损失函数与轮次数的关系图，分别针对训练集和测试集：
- en: '[PRE202]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Here is the output:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.10 – Binary cross-entropy loss as a function of the epoch](img/B19629_08_10.jpg)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – 二元交叉熵损失与轮次数的关系](img/B19629_08_10.jpg)'
- en: Figure 8.10 – Binary cross-entropy loss as a function of the epoch
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – 二元交叉熵损失与轮次数的关系
- en: We can see that we are still overfitting, but it’s a bit less dramatic than
    without dropout.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，尽管我们仍然存在过拟合，但比没有应用 dropout 时要轻微一些。
- en: 'Finally, plot the accuracy as a function of the epoch number of both the train
    and test sets:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制训练集和测试集准确率与轮次数的关系图：
- en: '[PRE206]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: 'This is the output:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.11 – Accuracy as a function of the epoch](img/B19629_08_11.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.11 – 准确率与轮次数的关系](img/B19629_08_11.jpg)'
- en: Figure 8.11 – Accuracy as a function of the epoch
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 – 准确率与轮次数的关系
- en: Note the effect of dropout on the train accuracy.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 dropout 对训练集准确率的影响。
- en: While the accuracy did not spectacularly improve, it has increased from 77%
    to 79% with dropout. Also, the difference between the train and test losses is
    smaller than it was without dropout, allowing us to improve generalization.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确率没有显著提高，但它已从 77% 增加到 79%，且使用 dropout 后训练集和测试集的损失差距小于没有 dropout 时，从而提高了泛化能力。
- en: There’s more…
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Unlike dropout, other methods that are useful to regularize fully connected
    neural networks can be used with GRUs and other RNN-based architectures.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 与 dropout 不同，其他有助于正则化全连接神经网络的方法也可以与 GRU 和其他基于 RNN 的架构一起使用。
- en: For example, since we have rather substantial overfitting here, with a train
    loss having a steep decrease, it might be interesting to test smaller architectures,
    with fewer parameters to be learned.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于我们在这里出现了明显的过拟合，训练损失有急剧下降，因此测试较小的架构可能会很有趣，使用较少的参数进行学习。
- en: Regularizing with the maximum sequence length
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用最大序列长度进行正则化
- en: In this recipe, we will regularize by playing with the maximum sequence length,
    on the IMDB dataset, using a GRU-based neural network.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将通过调整最大序列长度来进行正则化，使用基于 GRU 的神经网络处理 IMDB 数据集。
- en: Getting ready
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: Up to now, we have not played much with the maximum length of the sequence,
    but it is sometimes one of the most important hyperparameters to tune.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们没有太多调整序列的最大长度，但它有时是最重要的超参数之一。
- en: 'Indeed, depending on the input dataset, the optimal maximum length can be quite
    different:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，根据输入数据集的不同，最佳的最大长度可能会有很大差异：
- en: A tweet is short, so having a maximum number of tokens of hundreds does not
    make sense most of the time
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推文很短，因此大多数情况下，最大令牌数设置为几百没有意义。
- en: A product or movie review can be significantly longer, and sometimes, the reviewer
    writes a lot of pros and cons about the product/movie, before getting to the final
    conclusion – in such cases, a larger maximum length may help
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产品或电影评论通常会更长，有时评论者会写很多关于产品/电影的优缺点，然后才给出最终结论——在这种情况下，较大的最大长度可能会有所帮助。
- en: In this recipe, we will train a GRU on the IMDb dataset, containing movie reviews
    and associated labels (either positive or negative); this dataset contains some
    very lengthy texts. So, we will significantly increase the maximum number of words,
    and see how it impacts the final accuracy.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将在IMDB数据集上训练一个GRU，该数据集包含电影评论和相应的标签（正面或负面）；该数据集包含一些非常长的文本。因此，我们将大幅增加单词的最大数量，并查看它对最终精度的影响。
- en: 'If not already done, you can download the dataset, assuming you have a Kaggle
    API installed, running the following command line:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尚未完成，你可以下载数据集，假设你已安装Kaggle API，可以运行以下命令行：
- en: '[PRE210]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'The following libraries are needed: `pandas`, `numpy`, `scikit-learn`, `matplotlib`,
    `torch`, and `transformers`. They can be installed with the following command
    line:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 以下库是必须的：`pandas`、`numpy`、`scikit-learn`、`matplotlib`、`torch`和`transformers`。它们可以通过以下命令行安装：
- en: '[PRE211]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE211]'
- en: How to do it…
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行此配方的步骤：
- en: This recipe will be mostly the same as the *Training a GRU* recipe on the IMDb
    dataset; it will only change the maximum length of the sequence. Since the most
    significant differences are the sequence length value and the results, we will
    assume the four first steps of *Training a GRU* (from the imports to the dataset
    implementation) have been run, and we will reuse some of the code. Instantiate
    the `TextClassificationDataset` objects for the train and test sets (reusing the
    class implemented in *Training a GRU*), as well as the related data loaders.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个配方大部分与*训练GRU*配方在IMDB数据集上的操作相同；唯一的区别是序列的最大长度。由于最显著的区别是序列长度值和结果，我们将假设*训练GRU*的前四个步骤（从导入到数据集实现）已完成，并将重用一些代码。实例化训练集和测试集的`TextClassificationDataset`对象（重用在*训练GRU*中实现的类），以及相关的数据加载器。
- en: 'This time, we chose a maximum number of words of `256`, significantly higher
    than the earlier value of `64`. We will keep a batch size of `64`:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们选择了一个最大单词数为`256`，显著高于之前的`64`。我们将保持批处理大小为`64`：
- en: '[PRE212]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE212]'
- en: 'Instantiate the GRU model by reusing the `GRUClassifier` class implemented
    in *Training a GRU*, with an embedding dimension and a hidden dimension of `32`:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过重用在*训练GRU*中实现的`GRUClassifier`类来实例化GRU模型，嵌入维度和隐藏维度为`32`：
- en: '[PRE213]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss, since this is a binary classification
    task:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 将优化器实例化为Adam优化器，学习率为`0.001`。损失函数定义为二元交叉熵损失，因为这是一个二分类任务：
- en: '[PRE225]'
  id: totrans-506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Train the model over 20 epochs reusing the `train_IMDB_classification` helper
    function implemented in the *Training a GRU* recipe; store the accuracy and the
    loss for both the train and test sets for each epoch:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在20个epoch内训练模型，重用在*训练GRU*配方中实现的`train_IMDB_classification`辅助函数；存储每个epoch的训练集和测试集的精度与损失：
- en: '[PRE226]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: 'After 20 epochs, the output looks like this:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 经过20个epoch后，输出结果如下：
- en: '[PRE230]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE230]'
- en: 'Plot the loss as a function of the epoch number for both the train and test
    sets:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练集和测试集的损失随epoch数量变化的曲线：
- en: '[PRE231]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: 'Here is the output:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.12 – Binary cross-entropy loss as a function of the epoch](img/B19629_08_12.jpg)'
  id: totrans-520
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.12 – 二元交叉熵损失与epoch的关系](img/B19629_08_12.jpg)'
- en: Figure 8.12 – Binary cross-entropy loss as a function of the epoch
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 – 二元交叉熵损失与epoch的关系
- en: We can see that there is overfitting after only a few epochs.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在仅仅几个epoch后就出现了过拟合现象。
- en: 'Finally, plot the accuracy as a function of the epoch number of both the train
    and test sets:'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制训练集和测试集的精度随epoch数量变化的曲线：
- en: '[PRE235]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: 'This is what we get:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的结果：
- en: '![Figure 8.13 – Accuracy as a function of the epoch](img/B19629_08_13.jpg)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.13 – 精度随epoch的关系](img/B19629_08_13.jpg)'
- en: Figure 8.13 – Accuracy as a function of the epoch
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 – 精度随epoch的关系
- en: The test accuracy reaches a maximum after a few epochs and then slowly decreases.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 测试精度在几个epoch后达到最大值，然后缓慢下降。
- en: Although there is still a large overfitting effect, compared to the training
    with a maximum length of 64, the test accuracy went from a maximum of 77% to a
    maximum of 87%, a significant improvement.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管仍然存在较大的过拟合效应，但与最大长度为64的训练相比，测试精度从最大77%提升到了最大87%，这是一个显著的改善。
- en: There’s more…
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: Instead of blindly choosing the maximum number of tokens, it might be interesting
    to first quickly analyze the length distribution.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 与其盲目选择最大数量的标记，不如先快速分析一下文本长度的分布。
- en: 'For relatively small datasets, it’s easy to compute the length of all samples;
    let’s do it with the following code:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相对较小的数据集，计算所有样本的长度非常简单；让我们用以下代码来实现：
- en: '[PRE239]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE239]'
- en: 'We can now plot the distribution of the review lengths with a histogram, using
    a log scale:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用对数尺度绘制评论长度的分布直方图：
- en: '[PRE240]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE240]'
- en: 'This is the output:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 8.14 – A histogram of the review length of the IMDb dataset, in a
    log scale](img/B19629_08_14.jpg)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.14 – IMDb 数据集评论长度的直方图，采用对数尺度](img/B19629_08_14.jpg)'
- en: Figure 8.14 – A histogram of the review length of the IMDb dataset, in a log
    scale
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 – IMDb 数据集评论长度的直方图，采用对数尺度
- en: 'We can see a peak of around 300 tokens in length, and almost no review has
    more than 1,500 tokens. As we can see from the histogram, most of the reviews
    seem to have a length of a couple of hundred tokens. We can also compute the average
    and median length:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到大约 300 个词汇长度的峰值，几乎没有评论超过 1,500 个词汇。正如从直方图中看到的，大多数评论的长度似乎在几百个词汇左右。我们也可以计算出平均长度和中位数：
- en: '[PRE241]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'The computed average and median values are the following:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 计算得出的平均值和中位数如下：
- en: '[PRE242]'
  id: totrans-545
  prefs: []
  type: TYPE_PRE
  zh: '[PRE242]'
- en: As a result, the average length is about 309, and the median length is 231\.
    According to this information, if the computational power allows it and depending
    on the task, choosing a maximum length of 256 seems to be a good first choice.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，平均长度大约为 309，中位数长度为 231。根据这些信息，如果计算能力允许，并且取决于任务，选择最大长度为 256 似乎是一个不错的初步选择。
