- en: Multi-Label Image Classification Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行多标签图像分类
- en: In the previous chapter, we developed a project that accurately classifies cancer
    patients based on cancer types using an LSTM network. This is a challenging problem
    in biomedical informatics. Unfortunately, when it comes to classifying multimedia
    objects such as images, audio, or videos, linear ML models and other regular **deep
    neural network** (**DNN**) models, such as **Multilayer Perceptron** (**MLP**)
    or **Deep Belief Networks** (**DBN**), often fail to learn or model non-linear
    features from images.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开发了一个基于LSTM网络准确分类癌症患者的项目。这个问题在生物医学信息学中具有挑战性。不幸的是，当涉及到分类多媒体对象（如图像、音频或视频）时，线性机器学习模型和其他常规**深度神经网络**（**DNN**）模型，如**多层感知器**（**MLP**）或**深度置信网络**（**DBN**），常常无法学习或建模图像中的非线性特征。
- en: 'On the other hand, **convolutional neural networks** (**CNNs**) can be utilized
    to overcome these limitations. In CNNs, the connectivity pattern between neurons
    is inspired by the human visual cortex, which more accurately resembles human
    vision, so it is perfect for image processing-related tasks. Consequently, CNNs
    have shown outstanding successes in numerous domains: computer vision, NLP, multimedia
    analytics, image searches, and so on.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**卷积神经网络**（**CNNs**）可以用来克服这些限制。在CNN中，神经元之间的连接模式受到人类视觉皮层的启发，这种连接方式更准确地模拟了人类视觉，因此非常适合图像处理相关任务。因此，CNN在多个领域取得了杰出的成功：计算机视觉、自然语言处理（NLP）、多媒体分析、图像搜索等。
- en: 'Considering this motivation, in this chapter, we will see how to develop an
    end-to-end project for handling multi-label (that is, each entity can belong to
    multiple classes) image classification problems using CNNs based on the Scala
    and **Deeplearning4j** (**DL4J**) frameworks on real Yelp image datasets. We will
    also discuss some theoretical aspects of CNNs before getting started. Nevertheless,
    we will discuss how to tune hyperparameters for better classification results.
    Concisely, we will learn the following topics throughout our end-to-end project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一动机，在本章中，我们将看到如何基于Scala和**Deeplearning4j**（**DL4J**）框架，在真实的Yelp图像数据集上，开发一个端到端的项目来处理多标签（即每个实体可以属于多个类别）图像分类问题。在正式开始之前，我们还将讨论一些CNN的理论方面内容。尽管如此，我们也会讨论如何调整超参数，以获得更好的分类结果。简而言之，在整个端到端项目中，我们将学习以下主题：
- en: Drawbacks of regular DNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规DNN的缺点
- en: 'CNN architectures: convolution operations and pooling layers'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN架构：卷积操作和池化层
- en: Large-scale image classification using CNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络（CNN）进行大规模图像分类
- en: Frequently asked questions (FAQs)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见问题（FAQ）
- en: Image classification and drawbacks of DNNs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类及深度神经网络（DNN）的缺点
- en: In this project, we will show a step-by-step example of developing real-life
    ML projects for image classification using Scala and CNN. One such image data
    source is Yelp, where there are many photos and many users uploading photos. These
    photos provide rich local business information across categories. Thus, using
    these photos, developing an ML application by understanding the context of these
    photos is not an easy task. We will see how to use the DL4j platform to do so
    using Java. However, some theoretical background is a prior mandate before we
    start formally.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本项目中，我们将展示一个逐步的示例，展示如何使用Scala和CNN开发真实生活中的机器学习（ML）图像分类项目。一个这样的图像数据源是Yelp，那里有很多照片和许多用户上传的照片。这些照片提供了跨类别的丰富本地商业信息。因此，使用这些照片，理解照片的背景并开发机器学习应用并非易事。我们将看到如何使用DL4j平台在Java中实现这一点。但是，在正式开始之前，了解一些理论背景是必要的。
- en: Before we start developing the end-to-end project for image classification using
    CNN, let's take a look at the drawbacks of regular DNNs. Although regular DNNs
    work fine for small images (for example, MNIST and CIFAR-10), it breaks down for
    large-scale and high-quality images because of the huge number of hyperparameters
    it requires. For example, a 200 × 200 image has 40,000 pixels, and if the first
    layer has just 2,000 neurons, this means there will have 80 million different
    connections just in the first layer. Thus, if your network is very deep, there
    might be even billions of parameters.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用CNN开发端到端的图像分类项目之前，先来看看常规DNN的缺点。尽管常规DNN对于小图像（例如，MNIST和CIFAR-10）工作正常，但对于大规模和高质量图像，它因为需要大量的超参数而无法处理。例如，一张200
    × 200的图像有40,000个像素，如果第一层只有2,000个神经元，那么仅第一层就会有8000万个不同的连接。因此，如果网络非常深，可能会有数十亿个参数。
- en: CNNs solve this problem using partially connected layers. Because consecutive
    layers are only partially connected and because it heavily reuses its weights,
    a CNN has far fewer parameters than a fully connected DNN, which makes it much
    faster to train, reduces the risk of overfitting, and requires much less training
    data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通过使用部分连接层来解决这个问题。因为连续层只部分连接，而且由于其权重被大量复用，CNN的参数远少于全连接的DNN，这使得训练速度更快，减少了过拟合的风险，并且需要的训练数据大大减少。
- en: Moreover, when a CNN has learned a kernel that can detect a particular feature,
    it can detect that feature anywhere on the image. In contrast, when a DNN learns
    a feature in one location, it can detect it only in that particular location.
    Since images typically have very repetitive features, CNNs are able to generalize
    much better than DNNs for image processing tasks such as classification, using
    fewer training examples.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当CNN学会了可以检测特定特征的卷积核时，它可以在图像的任何位置检测该特征。相比之下，当DNN在某个位置学习到一个特征时，它只能在该特定位置检测到该特征。由于图像通常具有非常重复的特征，CNN在图像处理任务（如分类）中能够比DNN更好地进行泛化，且需要的训练样本更少。
- en: 'Importantly, DNN has no prior knowledge of how pixels are organized: it does
    not know that nearby pixels are close. A CNN''s architecture embeds this prior
    knowledge. Lower layers typically identify features in small areas of the images,
    while higher layers combine the lower-level features into larger features. This
    works well with most natural images, giving CNNs a decisive head start compared
    to DNNs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，DNN并没有关于像素如何组织的先验知识：它不知道邻近的像素是相近的。CNN的架构则嵌入了这种先验知识。较低的层通常识别图像中小区域的特征，而较高的层则将低级特征组合成更大的特征。这对于大多数自然图像来说效果很好，使得CNN相比DNN在处理图像时具有决定性的优势：
- en: '![](img/f6a9a8ed-d7aa-4f6e-9d9e-854ca4fd78b9.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6a9a8ed-d7aa-4f6e-9d9e-854ca4fd78b9.png)'
- en: Regular DNN versus CNN where each layer has neurons arranged in 3D
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 常规DNN与CNN的对比，其中每一层的神经元以3D排列
- en: For example, in the preceding diagram, on the left, you can see a regular three-layer
    neural network. On the right, a ConvNet arranges its neurons into three dimensions
    (width, height, and depth), as visualized in one of the layers. Every layer of
    a `CNN` transforms the 3D structure into a 3D output structure of neuron activations.
    The red input layer holds the image, so its width and height would be the dimensions
    of the image, and the depth would be three (red, green, and blue channels).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在前面的图示中，左侧展示了一个常规的三层神经网络。右侧的ConvNet则将其神经元排列成三维（宽度、高度和深度），如图中某一层所示。`CNN`的每一层都将3D结构转化为神经元激活的3D输出结构。红色的输入层包含图像，因此其宽度和高度就是图像的尺寸，而深度则为三（红色、绿色和蓝色通道）。
- en: Therefore, all the multilayer neural networks we looked at had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the network. However, feeding 2D images directly to CNNs is possible since
    each layer in CNN is represented in 2D, which makes it easier to match neurons
    with their corresponding inputs. We will see examples of this in the upcoming
    sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们所看到的所有多层神经网络都由一长串神经元组成，我们必须在将图像输入网络之前，将其展平为1D。然而，直接将2D图像输入CNN是可能的，因为CNN中的每一层都是以2D的形式表示的，这使得将神经元与其对应的输入进行匹配变得更加容易。我们将在接下来的部分中看到这方面的例子。
- en: Another important fact is that all the neurons in a feature map share the same
    parameters, so it dramatically reduces the number of parameters in the model.
    Also, more importantly, once a CNN has learned to recognize a pattern in one location,
    it can do the same for other locations as well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的事实是，特征图中的所有神经元共享相同的参数，因此大大减少了模型中的参数数量。更重要的是，一旦CNN学会了在一个位置识别某个模式，它也可以在其他位置做到相同的事情。
- en: CNN architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN架构
- en: In CNN networks, the way connectivity is defined among layers is significantly
    different compared to MLP or DBN. The **convolutional** (**conv**) layer is the
    main type of layer in a CNN, where each neuron is connected to a certain region
    of the input image, which is called a **receptive field**.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN网络中，层与层之间的连接方式与MLP或DBN显著不同。**卷积**（**conv**）层是CNN中的主要层类型，其中每个神经元都与输入图像的某个区域相连，这个区域称为**感受野**。
- en: 'To be more specific, in a CNN architecture, a few conv layers are connected
    in a cascade style: each layer is followed by a **rectified linear unit** (**ReLU**)
    layer, then a pooling layer, then a few more conv layers (+ReLU), then another
    pooling layer, and so on. The output from each conv layer is a set of objects
    called feature maps, which are generated by a single kernel filter. Then, the
    feature maps are fed to the next layer as a new input. In the fully connected
    layer, each neuron produces an output followed by an activation layer (that is,
    the Softmax layer):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在卷积神经网络（CNN）架构中，几个卷积层以级联方式连接：每个卷积层后面跟着一个**整流线性单元**（**ReLU**）层，再是一个池化层，然后是更多的卷积层（+ReLU），接着是另一个池化层，依此类推。每个卷积层的输出是一组由单个核过滤器生成的特征图，然后这些特征图作为新的输入传递到下一层。在全连接层中，每个神经元生成一个输出，并跟随一个激活层（即Softmax层）：
- en: '![](img/50345908-1f46-4a96-801b-e62f4648cf8b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50345908-1f46-4a96-801b-e62f4648cf8b.png)'
- en: A conceptual architecture of CNN
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）的概念架构
- en: 'As you can see in the preceding diagram, the pooling layers are usually placed
    after the convolutional layers (that is, between two such layers). A pooling layer
    into sub-regions then divides the convolutional region. Then, a single representative
    value is selected using either a max-pooling or an average pooling technique to
    reduce the computational time of subsequent layers. This way, a CNN can be thought
    of as a feature extractor. To understand this more clearly, refer to the following
    diagram:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，池化层通常放置在卷积层之后（即两个卷积层之间）。池化层将卷积区域划分为子区域，然后，使用最大池化或平均池化技术选择一个代表性值，以减少后续层的计算时间。通过这种方式，卷积神经网络（CNN）可以被视为一个特征提取器。为了更清晰地理解这一点，请参考以下图示：
- en: '![](img/f436e2e8-09eb-4a3b-8910-1f3389993208.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f436e2e8-09eb-4a3b-8910-1f3389993208.png)'
- en: A CNN is an end-to-end network that acts as both a feature extractor and a classifier.
    This way, it can accurately identify (under the given condition that it gets sufficient
    training data) the label of a given input image. For example, it can classify
    that the input image is really a tiger.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）是一个端到端的网络，既作为特征提取器，又作为分类器。通过这种方式，它可以在（给定足够的训练数据的条件下）准确识别给定输入图像的标签。例如，它可以分类输入图像为一只老虎。
- en: The robustness of the feature with respect to its spatial position is increased
    too. To be more specific, when feature maps are used as image properties and pass
    through the grayscale image, it gets smaller and smaller as it progresses through
    the network, but it also typically gets deeper and deeper since more feature maps
    will be added. The convolution operation brings a solution to this problem as
    it reduces the number of free parameters, allowing the network to be deeper with
    fewer parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 特征对其空间位置的鲁棒性也得到了增强。更具体来说，当特征图作为图像属性并通过灰度图像时，它随着网络的推进逐渐变小，但它通常也会变得越来越深，因为会添加更多的特征图。卷积操作为这个问题提供了解决方案，因为它减少了自由参数的数量，使得网络可以更深而参数更少。
- en: Convolutional operations
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'A convolution is a mathematical operation that slides one function over another
    and measures the integrity of their pointwise multiplication. Convolutional layers
    are probably the most important building blocks in a CNN. For the first conv layer,
    neurons are not connected to every single pixel in the input image, but only to
    pixels in their receptive fields (refer to the preceding diagram), whereas each
    neuron in the second conv layer is only connected to neurons located within a
    small rectangle in the first layer:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一个数学运算，它将一个函数滑动到另一个函数上并测量它们逐点相乘的完整性。卷积层可能是卷积神经网络中最重要的构建模块。对于第一个卷积层，神经元并不是连接到输入图像中的每一个像素，而是只连接到它们感受野中的像素（参考前面的图示），而第二个卷积层中的每个神经元仅连接到第一层中位于小矩形内的神经元：
- en: '![](img/48f7701f-6398-490e-ba5d-65ac689f0d22.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48f7701f-6398-490e-ba5d-65ac689f0d22.png)'
- en: Each convolutional neuron only processes data for its receptive field
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积神经元仅处理其感受野内的数据
- en: In [Chapter 2](e27fb252-7892-4659-81e2-2289de8ce570.xhtml), *Cancer Types Prediction
    Using Recurrent Type Networks,* we have seen that all multilayer neural networks
    (for example, MLP) have layers composed of so many neurons, and we had to flatten
    input images to 1D before feeding them to the network. Instead, in a CNN, each
    layer is represented in 2D, which makes it easier to match neurons with their
    associated inputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第二章](e27fb252-7892-4659-81e2-2289de8ce570.xhtml)《使用递归类型网络进行癌症类型预测》中，我们已经看到所有的多层神经网络（例如，MLP）都有由大量神经元组成的层，并且我们必须在将输入图像喂入网络之前将其展平为
    1D。而在 CNN 中，每一层是 2D 表示的，这使得将神经元与其关联输入匹配变得更容易。
- en: The receptive field is used to exploit spatial locality by enforcing a local
    connectivity pattern between neurons of adjacent layers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 感受野用于通过强制相邻层之间的局部连接模式来利用空间局部性。
- en: This architecture allows the network to concentrate on low-level features in
    the first hidden layer, and then assemble them into higher-level features in the
    next hidden layer, and so on. This hierarchical structure is common in real-world
    images, which is one of the reasons why CNNs work so well for image recognition.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构使得网络能够在第一个隐藏层集中处理低级特征，然后在下一个隐藏层将它们组合成更高级的特征，依此类推。这种分层结构在现实世界的图像中很常见，这也是
    CNN 在图像识别中表现如此出色的原因之一。
- en: Pooling and padding operations
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化和填充操作
- en: 'Once you understand how convolutional layers work, pooling layers are quite
    easy to grasp. A pooling layer typically works on every input channel independently,
    so the output depth is the same as the input depth. Alternatively, you may pool
    over the depth dimension, as we will see next, in which case the image''s spatial
    dimensions (for example, height and width) remain unchanged, but the number of
    channels is reduced. Let''s see a formal definition of pooling layers from TensorFlow
    API documentation (see more at [https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py)):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了卷积层的工作原理，池化层就很容易掌握。池化层通常在每个输入通道上独立工作，因此输出深度与输入深度相同。或者，你可以对深度维度进行池化，正如我们接下来会看到的那样，在这种情况下，图像的空间维度（例如，高度和宽度）保持不变，但通道数会减少。让我们从
    TensorFlow API 文档中看看池化层的正式定义（详细信息请参见 [https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py](https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/python/ops/nn.py)）：
- en: '"The pooling ops sweep a rectangular window over the input tensor, computing
    a reduction operation for each window (average, max, or max with argmax). Each
    pooling op uses rectangular windows of size called ksize separated by offset strides.
    For example, if strides are all ones, every window is used, if strides are all
    twos, every other window is used in each dimension, and so on."'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “池化操作对输入张量进行矩形窗口扫描，为每个窗口计算一个归约操作（平均值、最大值或带有 argmax 的最大值）。每个池化操作使用一个称为 ksize
    的矩形窗口，窗口之间通过偏移步幅进行分隔。例如，如果步幅都为 1，则使用每个窗口；如果步幅都为 2，则每个维度中使用每隔一个窗口，依此类推。”
- en: 'Similar to a conv layer, each neuron in a pooling layer is connected to the
    outputs of a limited number of neurons in the previous layer that are located
    within a small rectangular receptive field. However, the size, the stride, and
    the padding type have to be defined. So, in summary, the output from a pooling
    layer can be computed as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于卷积层，池化层中的每个神经元与前一层中位于小矩形感受野内的有限数量的神经元相连接。然而，必须定义大小、步幅和填充类型。因此，总结来说，池化层的输出可以按以下方式计算：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where indices are also taken into consideration along with the padding values.
    In other words, the goal of using pooling is to subsample the input image in order
    to reduce the computational load, the memory usage, and the number of parameters.
    This helps to avoid overfitting in the training stage.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中索引也会考虑在内，与填充值一起使用。换句话说，使用池化的目标是对输入图像进行子采样，以减少计算负载、内存使用和参数数量。这有助于避免训练阶段的过拟合。
- en: A pooling neuron has no weights. Therefore, it only aggregate the inputs using
    an aggregation function such as the max or mean.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 池化神经元没有权重。因此，它只使用聚合函数（如最大值或均值）聚合输入。
- en: 'The spatial semantics of the convolution ops depend on the padding scheme chosen.
    Padding is an operation to increase the size of the input data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作的空间语义依赖于所选择的填充方案。填充是增加输入数据大小的操作：
- en: '**For 1D input**: Just an array is appended with a constant, say, `c`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于 1D 输入**：仅仅是一个数组附加一个常数，例如，`c`'
- en: '**For a 2D input**: A matrix that is surrounded with `c`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于二维输入**：一个矩阵被 `c` 包围'
- en: '**For a milt-dimensional (that is, nD) input**: The nD hypercube is surrounded
    with `c`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于多维输入（即 nD 输入）**：nD 超立方体被 `c` 包围'
- en: 'Now, the question is, what''s this constant `c`? Well, in most of the cases
    (but not always), `c` is zero called **zero padding**. This concept can be further
    broken down into two types of padding called `VALID` and `SAME`, which are outlined
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是，常数 `c` 是什么？在大多数情况下（但并非总是如此），`c` 是零，称为 **零填充**。这一概念可以进一步分解为两种类型的填充，分别叫做
    `VALID` 和 `SAME`，具体说明如下：
- en: '**VALID padding**: Only drops the right-most columns (or bottom-most rows).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VALID 填充**：仅丢弃最右侧的列（或最底部的行）。'
- en: '**SAME padding**: In this scheme, padding is applied evenly or both left and
    right. However, if the number of columns to be added is odd, then an extra column
    is added to the right.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SAME 填充**：在这种方案中，填充均匀地应用于左侧和右侧。然而，如果需要添加的列数是奇数，则会额外在右侧添加一列。'
- en: We've explained the previous definition graphically in the following diagram.
    If we want a layer to have the same height and width as the previous layer, it
    is common to add zeros around the inputs. This is called `SAME` or zero padding.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的图中图示了前面的定义。如果我们希望某一层与前一层具有相同的高度和宽度，通常会在输入周围添加零。这称为 `SAME` 或零填充。
- en: The term `SAME` means that the output feature map has the same spatial dimensions
    as the input feature map.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`SAME` 这个术语意味着输出特征图与输入特征图具有相同的空间维度。'
- en: 'On the other hand, zero padding is introduced to make the shapes match as needed,
    equally on every side of the input map. On the other hand, `VALID` means no padding
    and only drops the right-most columns (or bottom-most rows):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，零填充被引入以根据需要使形状匹配，填充均匀地应用于输入图的每一侧。而 `VALID` 填充表示没有填充，只丢弃最右侧的列（或最底部的行）：
- en: '![](img/b37960ea-1ba3-4df2-9d86-9017f1424776.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b37960ea-1ba3-4df2-9d86-9017f1424776.png)'
- en: SAME versus VALID padding with CNN
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`SAME` 与 `VALID` 填充在 CNN 中的比较'
- en: 'In the following diagram, we use a 2 × 2 pooling kernel, a stride of 2 with
    no padding. Only the max input value in each kernel makes it to the next layer
    since the other inputs are dropped (we will see this later on):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们使用一个 2 × 2 池化核，步幅为 2 且没有填充。只有每个池化核中的最大输入值才会传递到下一层，其他的输入则被丢弃（我们稍后会看到这一点）：
- en: '![](img/ed02b9c1-62ad-479e-8ec0-a2fd75b91cb7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed02b9c1-62ad-479e-8ec0-a2fd75b91cb7.png)'
- en: An example using max pooling, that is, subsampling
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最大池化的示例，即下采样
- en: Fully connected layer (dense layer)
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层（密集层）
- en: 'At the top of the stack, a regular fully connected layer (feedforward neural
    network or dense layer) is added, which acts similar to an MLP that might be composed
    of a few fully connected layers (+ReLUs), and the final layer outputs the prediction:
    typically, a Softmax layer is used that outputs estimated class probabilities
    for a multiclass classification.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的最上层，添加了一个常规的全连接层（前馈神经网络或密集层），它的作用类似于一个可能由几个全连接层（+ReLU）组成的 MLP，最终层输出预测结果：通常使用
    Softmax 层，它会输出多类分类的估计类概率。
- en: Well, up to this point, we have minimum theoretical knowledge about CNNs and
    their architectures for image classification. Now, it is time to do a hands-on
    project, which is about classifying large-scale Yelp images. At Yelp, there are
    many photos and many users uploading photos. These photos provide rich local business
    information across categories. Teaching a computer to understand the context of
    these photos is not an easy task.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经具备了关于 CNN 和它们在图像分类中的架构的最基本理论知识。接下来是做一个动手项目，涉及大规模 Yelp 图像的分类。在 Yelp
    上，有许多照片和用户上传的照片，这些照片提供了丰富的本地商业信息，涵盖多个类别。教会计算机理解这些照片的背景并不是一项容易的任务。
- en: Yelp engineers work on deep learning-based image classification projects in-house
    (see more at [https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html](https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Yelp 的工程师们正在公司内部从事基于深度学习的图像分类项目（更多内容请见 [https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html](https://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)）。
- en: Multi-label image classification using CNNs
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行多标签图像分类
- en: In this section, we will show you a systematic example of developing real-life
    ML projects for image classification. However, we need to know the problem description
    first so as to know what sort of image classification needs to be done. Moreover,
    knowledge about the dataset is a mandate before getting started.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示一个系统化的例子，介绍如何开发实际的机器学习项目来进行图像分类。然而，我们首先需要了解问题描述，以便知道需要进行什么样的图像分类。此外，在开始之前，了解数据集是必要的。
- en: Problem description
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题描述
- en: Nowadays, food selfies and photo-centric social storytelling are becoming social
    trends. Consequently, an enormous amount of selfies that include foods and a picture
    of the restaurant are being uploaded on social media and websites. In many instances,
    food lovers also provide the written reviews that can significantly boost the
    popularity of a business (for example, a restaurant).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，食物自拍和以照片为中心的社交叙事正成为社交趋势。因此，大量包含食物的自拍照和餐厅照片被上传到社交媒体和网站上。在许多情况下，食物爱好者还会提供书面评论，这些评论可以显著提升商家的知名度（例如餐厅）。
- en: For example, millions of unique visitors have visited the Yelp website and have
    written more than 135 million reviews. Besides, many photos and users are uploading
    photos. Nevertheless, business owners can post photos and message their customers.
    This way, Yelp makes money by **selling ads** to those local businesses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数百万独立访客访问了 Yelp 网站，并写下了超过 1.35 亿条评论。此外，许多照片和用户正在上传照片。然而，商家可以发布照片并与客户交流。通过这种方式，Yelp
    通过向本地商家**出售广告**赚钱。
- en: 'An interesting fact is that these photos provide rich local business information
    across categories. Thus, developing deep learning applications to understand the
    context of these photos would be a useful task. Take a look at the following screenshot
    to get an insight:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的事实是，这些照片提供了跨类别的丰富本地商户信息。因此，开发深度学习应用来理解这些照片的背景将是一项有用的任务。请查看以下截图以获取一些洞察：
- en: '![](img/954a1298-4086-4644-9a69-ae3a20bf1ebb.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/954a1298-4086-4644-9a69-ae3a20bf1ebb.png)'
- en: Mining some insights about a business from a Yelp dataset
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Yelp 数据集中挖掘一些关于商家的见解
- en: Thus, if we're given photos that belong to a business, we need to build a model
    so that it can tag restaurants with multiple labels of the user-submitted photos
    automatically in order to predict business attributes. Eventually, the goal of
    this project is to turn Yelp pictures into words.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们得到属于某个商家的照片，我们需要构建一个模型，使其能够自动为餐厅的用户上传照片打上多个标签，以预测商家的属性。最终，该项目的目标是将 Yelp
    照片转化为文字。
- en: Description of the dataset
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: The Yelp dataset for this fun project was downloaded from [https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification).
    We got permission from the Yelp guys under the condition that the images won't
    be redistributed. However, you need to get usage permission from [https://www.yelp.com/dataset](https://www.yelp.com/dataset).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个有趣项目的 Yelp 数据集是从 [https://www.kaggle.com/c/yelp-restaurant-photo-classification](https://www.kaggle.com/c/yelp-restaurant-photo-classification)
    下载的。我们已获得 Yelp 的许可，前提是这些图片不会被重新分发。不过，您需要从 [https://www.yelp.com/dataset](https://www.yelp.com/dataset)
    获取使用许可。
- en: 'Submitting a review is tricky. When Yelp users want to submit a review, they
    have to select the labels of the restaurants manually from nine different labels
    that are annotated by the Yelp community, which are associated with the dataset.
    These are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 提交评论是很棘手的。当 Yelp 用户想要提交评论时，他们必须手动从 Yelp 社区注释的九个不同标签中选择餐厅的标签，这些标签与数据集相关联。具体如下：
- en: '`0`: `good_for_lunch`'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`：`适合午餐`'
- en: '`1`: `good_for_dinner`'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`：`适合晚餐`'
- en: '`2`: `takes_reservations`'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`：`接受预订`'
- en: '`3`: `outdoor_seating`'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`：`户外座位`'
- en: '`4`: `restaurant_is_expensive`'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`4`：`餐厅价格昂贵`'
- en: '`5`: `has_alcohol`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`5`：`提供酒水`'
- en: '`6`: `has_table_service`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`6`：`有桌面服务`'
- en: '`7`: `ambience_is_classy`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`7`：`环境优雅`'
- en: '`8`: `good_for_kids`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`8`：`适合孩子`'
- en: 'Thus, this is a multiple label multiclass classification problem, where each
    business can have one or more of the nine characteristics listed previously. Therefore,
    we have to predict these labels as accurately as possible. There are six files
    in the dataset, as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这是一个多标签多类别分类问题，其中每个商家可以有一个或多个之前列出的九个特征。因此，我们必须尽可能准确地预测这些标签。数据集中有六个文件，如下所示：
- en: '`train_photos.tgz`: Photos to be used as the training set (234,842 images)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_photos.tgz`：用于训练集的照片（234,842 张图片）'
- en: '`test_photos.tgz`: Photos to be used as the test set (237,152 images)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_photos.tgz`：将用作测试集的照片（237,152 张图像）'
- en: '`train_photo_to_biz_ids.csv`: Provides the mapping between the photo ID and
    the business ID (234,842 rows)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_photo_to_biz_ids.csv`：提供照片 ID 与商业 ID 之间的映射（234,842 行）'
- en: '`test_photo_to_biz_ids.csv`: Provides the mapping between the photo ID and
    business the ID (1,190,225 rows)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_photo_to_biz_ids.csv`：提供照片 ID 与商业 ID 之间的映射（1,190,225 行）'
- en: '`train.csv`: This is the main training dataset, which includes business IDs
    and their corresponding labels (2,000 rows)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train.csv`：这是主要的训练数据集，包括商业 ID 和其对应的标签（2000 行）'
- en: '`sample_submission.csv`: A sample submission—reference the correct format for
    your predictions including `business_id` and the corresponding predicted labels'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_submission.csv`：一个示例提交文件——参考正确的格式来提交你的预测结果，包括 `business_id` 和相应的预测标签'
- en: Removing invalid images
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除无效图像
- en: 'I do not know why, but each image folder (train and test) also contains some
    temporary images with the `_*.jpg` name pattern, but not actual images. Therefore,
    I removed them using a UNIX command as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道为什么，但每个图像文件夹（训练集和测试集）中也包含一些临时图像，这些图像的名称模式为 `_*.jpg`，但并不是真正的图像。因此，我使用以下 UNIX
    命令将其删除：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, I unzipped and copied each `.csv` file into a folder called `label`.
    Additionally, I moved the training and test images into the `train` and `test`
    folders (that is, inside the `images` folder), respectively. In short, after extraction
    and copying, the following folder structure is used in our projects. Therefore,
    the resulting structure will be as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我解压并将每个 `.csv` 文件复制到名为 `label` 的文件夹中。此外，我将训练图像和测试图像分别移动到 `train` 和 `test`
    文件夹（即在 `images` 文件夹内）。简而言之，经过提取和复制后，我们项目中使用的文件夹结构如下。因此，最终的结构将如下所示：
- en: '![](img/84b1fa36-fa32-4d88-9743-09c6f5da2f10.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84b1fa36-fa32-4d88-9743-09c6f5da2f10.png)'
- en: Folder structure in the Large Movie Review Dataset
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大型电影评论数据集中的文件夹结构
- en: Workflow of the overall project
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整体项目的工作流程
- en: Since we already know that this is a multi-label multiclass image classification
    problem, we have to deal with the multiple-instance issue**.** Since DL4J does
    not provide an example of how to solve amulti-label multiclass image classification
    problem, I found Andrew Brooks's blog article (see [http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/))
    motivation for this project**.**
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道这是一个多标签多分类图像分类问题，我们就必须处理多个实例问题**。** 由于 DL4J 没有提供如何解决多标签多分类图像分类问题的示例，我找到
    Andrew Brooks 的博客文章（见 [http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/)）为此项目提供了动机**。**
- en: 'I simply applied the labels of the restaurant to all of the images associated
    with it and treated each image as a separate record. To be more technical, I handled
    each class as a separate binary classification problem. Nevertheless, at the beginning
    of this project, we will see how to read images from `.jpg` format into a matrix
    representation in Java. Then, we will further process and prepare those images
    so that they are feedable by the CNNs. Also, since images do not come with uniform
    shapes and sizes, we need to apply several rounds of image preprocessing ops,
    such as squaring and resizing every image to the uniform dimensions, before we
    apply a grayscale filter to the image:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是将餐厅的标签应用到所有与之相关的图像，并将每个图像当作一个单独的记录。更技术性一点来说，我将每个类别当作一个单独的二分类问题来处理。然而，在项目的开始部分，我们将看到如何在
    Java 中将 `.jpg` 格式的图像读取为矩阵表示。接着，我们将进一步处理并准备这些图像，以便它们能够被卷积神经网络（CNN）接受。此外，由于图像的形状和大小并不统一，我们需要进行几轮图像预处理操作，例如将每张图像调整为统一的尺寸，再应用灰度滤镜：
- en: '![](img/04a01dab-a68c-4ec0-83ba-c393c91dc1ea.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04a01dab-a68c-4ec0-83ba-c393c91dc1ea.png)'
- en: A conceptualized view of a CNN for image classification
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）在图像分类中的概念化视图
- en: Then, we train nine CNNs on training data for each class. Once the training
    is complete, we save the trained model, CNN configurations, and parameters so
    that they can be restored later on. Then, we apply a simple aggregate function
    to assign classes to each restaurant, where each one has multiple images associated
    with it, each with its own vector of probabilities for each of the nine classes.
    Then, we score test data and finally, we evaluate the model using test images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在每个类别的训练数据上训练九个CNN。一旦训练完成，我们会保存训练好的模型、CNN配置和参数，以便之后可以恢复它们。接着，我们应用一个简单的聚合函数为每个餐厅分配类别，每个餐厅都有多个与之关联的图片，每张图片都有一个属于九个类别的概率向量。接下来，我们对测试数据进行评分，最后，我们使用测试图像评估模型。
- en: 'Now, let''s see the structure of each CNN. Well, each network will have two
    convolutional layers, two subsampling layers, one dense layer, and the output
    layer as the dense layer. The first layer is a conv layer, followed by a subsampling
    layer, which is again followed by another conv layer, then a subsampling layer,
    then a dense layer, which is followed by an output layer. We will see each layer''s
    structure later on. In short, the Java class (`YelpImageClassifier.java`) has
    the following workflow:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看每个CNN的结构。每个网络将有两个卷积层、两个子采样层、一个全连接层和一个输出层作为全连接层。第一层是卷积层，接着是一个子采样层，然后是另一个卷积层，接下来是一个子采样层，然后是一个全连接层，最后是一个输出层。我们稍后会看到每一层的具体结构。简而言之，Java类（`YelpImageClassifier.java`）的工作流程如下：
- en: We read all the business labels from the `train.csv` file
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`train.csv`文件中读取所有的商家标签
- en: We then read and create a map from the image ID to the business ID as imageID
    | busID
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取并创建一个从图像ID到商家ID的映射，格式为imageID | busID
- en: Then, we generate a list of images from the `photoDir` directory to load and
    process, which helps us to retrieve image IDs of a certain number of images
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从`photoDir`目录中生成一个图像列表进行加载和处理，这有助于我们检索某些数量图像的图像ID
- en: We then read and process images into a photoID | vector map
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取并处理图像，生成photoID | 向量的映射
- en: We chain the output of step 3 and step 4 to align the business feature, image
    IDs, and label IDs to extract image features
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将步骤3和步骤4的输出连接起来，以对齐商家特征、图像ID和标签ID，从而提取图像特征
- en: Then, we construct nine CNNs for nine possible labels in a multi-label setting
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为多标签设置构建九个CNN，分别对应九个可能的标签
- en: We then train all the CNNs and specify the model savings locations
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们训练所有的CNN，并指定模型保存位置
- en: S*teps 2* to *6* are repeated several times to extract the features from the
    test set as well
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤2*到*步骤6*会多次重复，以从测试集提取特征'
- en: Finally, we evaluate the model and save the prediction in a CSV file
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们评估模型并将预测结果保存在CSV文件中
- en: 'Now, let''s see what the preceding steps would look like in a high-level diagram,
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看前面的步骤在高级图示中的样子，如下所示：
- en: '![](img/6ee535a5-af3c-4880-a53f-6d5477b45aff.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ee535a5-af3c-4880-a53f-6d5477b45aff.png)'
- en: DL4j image processing pipeline for image classification
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: DL4j图像处理管道，用于图像分类
- en: Too much of a mouthful? Don't worry; we will now see each step in detail. If
    you look at the previous steps carefully, you will see that steps 1 to 5 are image
    processing and feature constructions. Then, step 6 is training nine CNNs and then,
    in step 7, we save the trained CNNs so that we can restore them during result
    submission.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 内容太多了吗？别担心；我们现在将详细查看每个步骤。如果你仔细查看前面的步骤，你会发现步骤1到步骤5是图像处理和特征构造。然后，步骤6是训练九个CNN，接着在步骤7中，我们保存训练好的CNN，以便在结果提交时恢复它们。
- en: Image preprocessing
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理
- en: 'When I tried to develop this application, I found that the photos are different
    shapes and sizes: some images are tall, some of them are wide, some of them are
    outside, some images are inside, and most of them are food. Also, images come
    in different shapes (most were roughly square, though), of pixel and many of them
    are exactly 500 x 375 in dimension:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当我尝试开发这个应用程序时，我发现照片的形状和大小各不相同：有些图片是高的，有些是宽的，有些是外景的，有些是室内的，而且大部分是食物。此外，图像的形状也各不相同（尽管大多数图像大致是方形的），像素数量也不同，其中许多图像的尺寸正好是500
    x 375：
- en: '![](img/b9b618f0-ad81-48e5-8a5e-b75997f17044.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9b618f0-ad81-48e5-8a5e-b75997f17044.png)'
- en: Resized figure (left, the original and tall one, right, the squared one)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小后的图（左侧是原始的高图，右侧是方形图）
- en: We have already seen that CNN cannot work with images with heterogeneous shapes
    and sizes. There are many robust and efficient image processing techniques to
    extract only the **region of interes**t (**ROI**), but honestly, I am not an image-processing
    expert, so I decided to keep this resizing step simple. In simple terms, I made
    all the images square but still, I tried to preserve their quality. The thing
    is that ROIs are cantered in most cases. So, capturing only the middle-most square
    of each image is not a trivial task. Nevertheless, we also need to convert each
    image into a grayscale image. Let's make irregularly shaped images square. Look
    at the preceding image, where the one on the left is the original one and the
    one on the right is the squared one.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，卷积神经网络（CNN）无法处理形状和大小各异的图像。虽然有很多强大且高效的图像处理技术可以提取出**感兴趣区域**（**ROI**），但说实话，我并不是图像处理方面的专家，所以我决定将这一步骤保持简单。简单来说，我将所有图像都做成正方形，但仍然尽量保持它们的质量。问题在于，ROI在大多数情况下是集中在图像中心的。所以，仅捕捉每个图像的中间正方形并不是一项简单的任务。不过，我们还需要将每个图像转换为灰度图像。让我们将不规则形状的图像裁剪为正方形。看看下面的图像，左侧是原始图像，右侧是裁剪后的正方形图像。
- en: 'We have generated a square image, but how did we achieve this? Well, I first
    checked whether the height and the width were the same, and then I resized the
    image. In the other two cases, I cropped the central region. The following method
    does the trick (but feel free to execute the `SquaringImage.java` script to see
    the output):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了一个正方形图像，但我们是如何做到的呢？首先，我检查了图像的高度和宽度是否相同，然后对图像进行了调整大小。在另外两种情况下，我裁剪了图像的中心区域。以下方法完成了这个任务（但是请随时执行`SquaringImage.java`脚本以查看输出）：
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Well done! Now that all of our training images are squared, the next step is
    to use the import-preprocessing task to resize them all. I decided to make all
    the images 128 x 128 in size. Let''s see what the previous image (the original
    one) looks like after resizing:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！现在我们所有的训练图像都已经变成正方形，接下来的步骤是使用导入预处理任务来调整它们的大小。我决定将所有图像的大小调整为128 x 128。让我们看看调整大小后的图像是什么样子的（原始图像）：
- en: '![](img/13daa415-e364-4988-b0c9-35c650fd0b93.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13daa415-e364-4988-b0c9-35c650fd0b93.jpg)'
- en: Image resizing (256 x 256, 128 x 128, 64 x 64 and 32 x 32, respectively)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图像调整大小（分别为256 x 256、128 x 128、64 x 64和32 x 32）
- en: 'The following method does this trick (but feel free to execute the `imageUtils.java`
    script to see a demo):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方法完成了这个任务（但是请随时执行`imageUtils.java`脚本以查看演示）：
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By the way, for the image resizing and squaring, I used some built-in package
    for image reading and some third-party packages for processing:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，关于图像的调整大小和裁剪正方形，我使用了一些内置的图像读取包和一些第三方处理包：
- en: '[PRE4]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To use the previous packages, add the following dependencies in a Maven-friendly
    `pom.xml` file (for the complete list of dependencies, refer to the `pom.xml`
    file provided for this chapter):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用上述包，请在Maven友好的`pom.xml`文件中添加以下依赖项（有关依赖项的完整列表，请参阅本章提供的`pom.xml`文件）：
- en: '[PRE5]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Processing color images is more exciting and effective, and DL4J-based CNNs
    can handle color images, too. However, it's better to simplify the computation
    with the grayscale images. Nevertheless, this way, we can make the overall representation
    simpler and space efficient.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 处理彩色图像更具趣味性且效果更好，基于DL4J的CNN也可以处理彩色图像。然而，为了简化计算，使用灰度图像更为理想。尽管如此，这种方法可以使整体表示更加简单且节省空间。
- en: 'Let''s give an example for our previous step; we resized each 256 x 256 pixel
    image—which is represented by 16,384 features rather than 16,384 x 3 for a color
    image having three RGB channels (execute `GrayscaleConverter.java` to see a demo).
    Let''s see what the converted image would look like:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来举个例子：我们将每个256 x 256像素的图像调整大小，这样它的特征数就变成了16,384，而不是像彩色图像那样是16,384 x 3（因为彩色图像有三个RGB通道）（执行`GrayscaleConverter.java`来查看演示）。让我们看看转换后的图像会是什么样子：
- en: '![](img/9e7e7c72-9b24-4331-8382-6dc595c1d4d6.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7e7c72-9b24-4331-8382-6dc595c1d4d6.png)'
- en: On the left—the original image, on the right—the grayscale one with RGB averaging
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是原始图像，右侧是经过RGB均值化处理的灰度图像
- en: 'The previous conversion is done using two methods called `pixels2Gray()` and
    `makeGray()`. The former converts RGB pixels into corresponding grayscale ones.
    Let''s see the signature for this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述转换是通过两个方法实现的，分别是`pixels2Gray()`和`makeGray()`。前者将RGB像素转换为相应的灰度像素。让我们来看一下这个方法的签名：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'So, what happens under the hood? We chain all the previous three steps: make
    all the images square, then convert all of them to 256 x 256, and finally convert
    the resized image into a grayscale one (I assume that `x` is the image to be converted):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，背后发生了什么？我们将所有之前的三步操作链接在一起：先将所有图像变成正方形，然后将它们转换为256 x 256的大小，最后将调整大小后的图像转换为灰度图像（假设`x`是待转换的图像）：
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Therefore, in summary, now all of the images are in grey, but only after squaring
    and resizing. The following image gives some sense of the conversion step:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，现在所有图像都是灰度图，但只有在平方和调整大小之后。以下图像可以给我们一些关于转换步骤的概念：
- en: '![](img/fee1cc5f-9747-4161-837c-20e182d2f710.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fee1cc5f-9747-4161-837c-20e182d2f710.png)'
- en: Resized figure (left, the original and tall one, right, the squared one)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小的图像（左边是原始的高图，右边是平方后的图）
- en: 'The previous chaining also comes with some additional effort. Now, putting
    all these three coding steps together, we can finally prepare all of our images:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的链式操作也带来了一些额外的工作。现在，将这三步代码整合在一起，我们终于可以准备好所有图像：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Extracting image metadata
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取图像元数据
- en: Up to this point, we have loaded and pre-processed raw images. However, we have
    no idea about the image metadata that is added, which is needed so that our CNNs
    can learn. Thus, it's time to load those CSV files containing metadata about each
    image.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经加载并预处理了原始图像。然而，我们还不了解添加到图像上的元数据，而这些元数据是CNN学习所需要的。因此，是时候加载那些包含每个图像元数据的CSV文件了。
- en: 'I wrote a method called `readMetadata()` to read such metadata in CSV format
    so that it can be used by two other methods called `readBusinessLabels` and `readBusinessToImageLabels`.
    These three methods are defined in the `CSVImageMetadataReader.java` script. Here''s
    the signature of the `readMetadata()` method:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我写了一个名为`readMetadata()`的方法，用于以CSV格式读取这些元数据，以便它能被两个其他方法`readBusinessLabels`和`readBusinessToImageLabels`使用。这三个方法在`CSVImageMetadataReader.java`脚本中定义。以下是`readMetadata()`方法的签名：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `readBusinessLabels()` method maps from the business ID to labels of the
    form businessID | Set(labels):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`readBusinessLabels()`方法将业务ID映射到标签，形式为businessID | Set(labels)：'
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `readBusinessToImageLabels()` method maps from the image ID to the business
    ID of the form imageID | businessID:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`readBusinessToImageLabels()`方法将图像ID映射到业务ID，形式为imageID | businessID：'
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Image feature extraction
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征提取
- en: So far, we have seen how to preprocess images and extract image metadata by
    linking them with the original images. Now, we need to extract features from those
    preprocessed images so that they can be fed into CNNs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何预处理图像并通过将它们与原始图像链接来提取图像元数据。现在，我们需要从这些预处理过的图像中提取特征，以便将它们输入到CNN中。
- en: 'We need the map operations for feature extractions for business, data, and
    labels. These three operations will ensure that we don''t lose any image provenance
    (see the `imageFeatureExtractor.java` script):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要进行特征提取的映射操作，用于业务、数据和标签。这三个操作将确保我们不会丢失任何图像的来源（参见`imageFeatureExtractor.java`脚本）：
- en: Business mapping with the form imageID | businessID
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务ID | businessID的业务映射
- en: Data map of the form imageID | image data
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 形式为imageID | 图像数据的数据映射
- en: Label map of the form businessID | labels
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 业务ID | 标签的标签映射
- en: 'First, we must define a regular expression pattern to extract a jpg name from
    the `CSVImageMetadataReaderclass`, which is used to match against training labels:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须定义一个正则表达式模式，从`CSVImageMetadataReaderclass`中提取jpg名称，该模式用于与训练标签进行匹配：
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we extract all of the image IDs associated with their respective business
    IDs:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提取与各自业务ID关联的所有图像ID：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we need to load and process all the images that are already preprocessed
    to extract the image IDs by mapping them with the IDs extracted from the business
    IDs in the earlier examples:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要加载并处理所有已经预处理过的图像，通过将它们与之前示例中提取的业务ID进行映射，来提取图像ID：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code block, we get a list of images from the photoDir directory
    (which is where the raw images reside). The `ids` parameter is an optional parameter
    to subset the images loaded from the photoDir. So far, we have been able to extract
    all the image IDs that are somehow associated with at least one business. The
    next move will be to read and process the images into an imageID → vector map:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们从photoDir目录（原始图像所在的位置）获取了一张图像列表。`ids`参数是一个可选参数，用于从photoDir加载的图像中进行子集选择。到目前为止，我们已经成功提取出所有与至少一个业务相关的图像ID。接下来的步骤是读取并处理这些图像，将它们转换为图像ID
    → 向量映射：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code block, we read and processed the images into a photoID
    → vector map. The `processImages()` method takes the following parameters:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们将图像读取并处理成了 photoID → 向量映射。`processImages()` 方法接受以下参数：
- en: '`images`: A list of images in the `getImageIds()` method'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`images`：`getImageIds()` 方法中的图像列表'
- en: '`resizeImgDim`: Dimension to rescale square images'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resizeImgDim`：重新调整方形图像的尺寸'
- en: '`nPixels`: Number of pixels used to sample the image to drastically reduce
    runtime while testing features'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nPixels`：用于采样图像的像素数，以显著减少测试特征时的运行时间'
- en: 'Well done! We are just one step away from extracting the data that is required
    to train our CNNs. The final step in feature extraction is to extract the pixel
    data, which consists of four objects to keep track of each image -- that is, imageID,
    businessID, labels, and pixel data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！我们距离提取训练CNN所需的数据只差一步。特征提取的最后一步是提取像素数据，该数据由四个对象组成，用来跟踪每个图像——即 imageID、businessID、标签和像素数据：
- en: '![](img/1f48580d-b6fc-44aa-ba0a-f40ea4946ae7.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f48580d-b6fc-44aa-ba0a-f40ea4946ae7.png)'
- en: Image data representation
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据表示
- en: 'Thus, as shown in the preceding diagram, the primary data structure is constructed
    with four data types (that is, four tuples) -- `imgID`, `businessID`, `pixel data
    vector`, and `labels`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如前面的图示所示，主数据结构是由四种数据类型（即四个元组）构成——`imgID`、`businessID`、`像素数据向量` 和 `标签`：
- en: 'Thus, we should have a class containing all of the parts of these objects.
    Don''t worry; all we need is defined in the `FeatureAndDataAligner.java` script.
    Once we have the instantiated instance of `FeatureAndDataAligner` using the following
    line of code in the `YelpImageClassifier.java` script (under the main method),
    `businessMap`, `dataMap`, and `labMap` are provided:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该有一个包含这些对象所有部分的类。别担心，我们所需要的一切都在 `FeatureAndDataAligner.java` 脚本中定义。一旦我们通过在
    `YelpImageClassifier.java` 脚本（在主方法下）中使用以下代码行实例化 `FeatureAndDataAligner`，就能提供 `businessMap`、`dataMap`
    和 `labMap`：
- en: '[PRE16]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here, the option type for `labMap` is used since we don''t have this information
    when we score on test data -- that is, it is optional for invocation. Now, let''s
    see how I did this. We start from the constructor of the class that is being used,
    initializing the preceding data structure:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，由于在测试数据上评分时没有这些信息，因此使用了 `labMap` 的选项类型——也就是说，它是可选的。现在，让我们来看一下我是如何实现的。我们从正在使用的类的构造函数开始，初始化前述数据结构：
- en: '[PRE17]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we initialize the values through the constructor of the `FeatureAndDataAligner.java`
    class as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过 `FeatureAndDataAligner.java` 类的构造函数初始化这些值，如下所示：
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, when aligning the data, if `labMap` is empty -- which is not provided
    with the training data -- the following can be used too:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在对齐数据时，如果 `labMap` 为空——也就是没有提供训练数据——也可以使用以下方法：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we have to align the image IDs and image data with the business IDs. For
    this, I have written the `BusinessImgageIds()` method:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将图像ID和图像数据与业务ID对齐。为此，我编写了 `BusinessImgageIds()` 方法：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The actual implementation lies in the following overloaded method, which returns
    optional if an image does not have a business ID:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 实际实现位于以下重载方法中，如果图像没有业务ID，则返回可选值：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, as shown in the preceding diagram, we now need to align the labels,
    which is a four-tuple list compromising of `dataMap`, `bizMap`, `labMap`, and
    `rowindices`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如前面的图示所示，我们现在需要对齐标签，它是一个由 `dataMap`、`bizMap`、`labMap` 和 `rowindices` 组成的四元组列表：
- en: '[PRE22]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the previous code block, `Quarta` is a case class that help us to maintain
    our desired data structure, shown as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，`Quarta` 是一个帮助我们维护所需数据结构的 case 类，如下所示：
- en: '[PRE23]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we pre-compute and save the data so that the method does not need
    to re-compute each time it is called:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们预先计算并保存数据，这样该方法在每次调用时就不需要重新计算了：
- en: '[PRE24]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Finally, as used in the preceding code block, we now create some getter methods
    so that in each invocation, we can retrieve the `image id`, `business id`, business
    label, and image for each business easily:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如前面代码块中所使用的，我们现在创建了一些获取方法，以便在每次调用时，我们可以轻松地为每个业务获取 `图像ID`、`业务ID`、业务标签和图像：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Excellent! Up to this point, we have managed to extract the features to train
    our CNNs. However, the thing is that the feature in its current form is still
    not suitable to feed into the CNNs. This is because we only have the feature vectors
    without labels. Thus, it needs another intermediate conversion.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！到目前为止，我们已经成功提取了训练CNN所需的特征。然而，问题是当前形式下的特征仍然不适合直接输入到CNN中。这是因为我们只有特征向量而没有标签。因此，它需要另一个中间转换。
- en: Preparing the ND4J dataset
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备ND4J数据集
- en: 'As I said previously, we need an intermediate conversion to prepare the training
    set containing feature vectors and labels: features from images, but labels from
    the business labels.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们需要进行中间转换，以准备包含特征向量和标签的训练集：图像中的特征，但标签来自业务标签。
- en: 'For this, we have the `makeND4jDataSets` class (see `makeND4jDataSets.java`
    for details). The class creates an ND4J dataset object from the data structure
    from the `alignLables` function in the `List[(imgID, bizID, labels, pixelVector)]`
    form. First, we prepare the dataset using the `makeDataSet()` method as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们有`makeND4jDataSets`类（有关详细信息，请参见`makeND4jDataSets.java`）。该类从`List[(imgID,
    bizID, labels, pixelVector)]`数据结构中通过`alignLables`函数创建ND4J数据集对象。首先，我们使用`makeDataSet()`方法如下所示来准备数据集：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we further need to convert the preceding data structure into an `INDArray`,
    which can then be consumed by the CNNs:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还需要将前面的数据结构转换为`INDArray`，这样它就可以被CNN使用：
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding code block, the `toNDArray()` method is used to convert the
    double or float matrix into `INDArray` format:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，`toNDArray()`方法用于将double或float矩阵转换为`INDArray`格式：
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Fantastic! We were able to extract all the metadata and features from the images
    and prepared the training data in an ND4J format that can now be consumed by the
    DL4J-based model. However, since we will be using CNN as our model, we still need
    to convert this 2D object into 4D by using the `convolutionalFlat` operation during
    network construction. Anyway, we will see this in the next section.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们成功地从图像中提取了所有元数据和特征，并将训练数据准备成了ND4J格式，现在可以被基于DL4J的模型使用。然而，由于我们将使用CNN作为模型，我们仍然需要在网络构建过程中通过使用`convolutionalFlat`操作将这个二维对象转换为四维。无论如何，我们将在下一节中看到这一点。
- en: Training, evaluating, and saving the trained CNN models
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练、评估和保存训练好的CNN模型
- en: So far, we have seen how to prepare the training set. Now that we have, a more
    challenging part lies ahead as we have to train our CNNs with 234,545 images,
    although the testing phase could be less exhaustive with a limited number of images,
    for example, 500 images. Therefore, it is better to train each CNN involving batchmode
    using DL4j's `MultipleEpochsIterator`, which is a dataset iterator for doing multiple
    passes over a dataset.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何准备训练集。现在，我们面临更具挑战性的部分，因为我们必须用234,545张图像来训练我们的卷积神经网络（CNN），尽管测试阶段可以通过有限数量的图像来简化，例如，500张图像。因此，最好使用DL4j的`MultipleEpochsIterator`以批量模式训练每个CNN，这个数据集迭代器可以对数据集进行多次遍历。
- en: '`MultipleEpochsIterator` is a dataset iterator for doing multiple passes over
    a dataset. See more at [https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html](https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html).'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultipleEpochsIterator`是一个数据集迭代器，用于对数据集进行多次遍历。更多信息请参见[https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html](https://deeplearning4j.org/doc/org/deeplearning4j/datasets/iterator/MultipleEpochsIterator.html)。'
- en: Network construction
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络构建
- en: 'The following is a list of important hyperparameters and their details. Here,
    I will try to construct a five-layered CNN, as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是重要超参数及其详细信息的列表。在这里，我将尝试构建一个五层的CNN，如下所示：
- en: 'Layer 0 has a `ConvolutionLayer` having a 6 x 6 kernel, one channel (since
    they are grayscale images), a stride of 2 x 2, and 20 feature maps where ReLU
    is the activation function:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层有一个`ConvolutionLayer`，具有6 x 6卷积核、一个通道（因为它们是灰度图像）、步长为2 x 2，并且有20个特征图，其中ReLU是激活函数：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Layer 1 has `SubsamplingLayer` max pooling, and a stride of 2x2\. Thus, by
    using stride, we down sample by a factor of 2\. Note that only MAX, AVG, SUM,
    and PNORM are supported. Here, the kernel size will be the same as the filter
    size from the last `ConvolutionLayer`. Therefore, we do not need to define the
    kernel size explicitly:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层有`SubsamplingLayer`最大池化，步长为2x2。因此，通过使用步长，我们按2的因子进行下采样。注意，只有MAX、AVG、SUM和PNORM是支持的。这里，卷积核的大小将与上一个`ConvolutionLayer`的滤波器大小相同。因此，我们不需要显式定义卷积核的大小：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Layer 2 has a `ConvolutionLayer` having a 6 x 6 kernel, one channel (since
    they are grayscale images), a stride of 2 x 2, and 20 output neurons where RELU
    is the activation function. We will use Xavier for network weight initialization:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个具有6 x 6卷积核、一个通道（因为它们是灰度图像）、步长为2 x 2、并且有20个输出神经元的`ConvolutionLayer`，其激活函数为RELU。我们将使用Xavier进行网络权重初始化：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Layer 3 has `SubsamplingLayer` max pooling and a stride of 2 x 2\. Thus, by
    using stride, we down sample by a factor of 2\. Note that only MAX, AVG, SUM,
    and PNORM are supported. Here, the kernel size will be the same as the filter
    size from the last `ConvolutionLayer`. Therefore, we do not need to define the
    kernel size explicitly:'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 3 有 `SubsamplingLayer` 最大池化，并且步幅为 2 x 2。因此，使用步幅，我们将数据下采样 2 倍。请注意，只有 MAX、AVG、SUM
    和 PNORM 被支持。这里，卷积核大小将与上一层 `ConvolutionLayer` 的滤波器大小相同。因此，我们不需要显式地定义卷积核大小：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Layer 4 has a `DenseLayer`, that is, a fully connected feed forward layer trainable
    by backpropagation with 50 neurons and ReLU as an activation function. It should
    be noted that we do not need to specify the number of input neurons as it assumes
    the input from the previous `ConvolutionLayer`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 4 有一个 `DenseLayer`，即一个完全连接的前馈层，通过反向传播进行训练，具有 50 个神经元，并且使用 ReLU 作为激活函数。需要注意的是，我们不需要指定输入神经元的数量，因为它假定输入来自前面的
    `ConvolutionLayer`：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Layer 5 is an `OutputLayer` having two output neurons driven by the softmax
    activation (that is, probability distribution over the classes). We compute the
    loss using XENT (that is, cross entropy for binary classification) as the loss
    function:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层 5 是一个 `OutputLayer`，具有两个输出神经元，由 softmax 激活驱动（即，对类别的概率分布）。我们使用 XENT（即二分类的交叉熵）作为损失函数来计算损失：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Apart from these layers, we also need to perform image flattening—that is,
    converting a 2D object into a 4D consumable using CNN layers by invoking the following
    method:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些层，我们还需要执行图像展平——即，将一个 2D 对象转换为一个 4D 可消耗的对象，使用 CNN 层通过调用以下方法：
- en: '[PRE35]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Therefore, in summary, using DL4J, our CNN will be as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，使用 DL4J，我们的 CNN 将如下所示：
- en: '[PRE36]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The other important aspects related to the training are described as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练相关的其他重要方面如下所示：
- en: '**The number of samples**: If you were training all the images other than GPU,
    that is, CPU, it would take days. When I tried with 50,000 images, it took one
    whole day with a machine having a core i7 processor and 32 GB of RAM. Now, you
    can imagine how long it would take for the whole dataset. In addition, it will
    require at least 256 GB of RAM even if you do the training in batch mode.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本数量**：如果你在没有 GPU 的情况下训练所有图像，也就是使用 CPU，那么可能需要几天时间。当我尝试使用 50,000 张图像时，在一台配备
    Core i7 处理器和 32 GB 内存的机器上，花费了整整一天。现在，你可以想象整个数据集训练需要多长时间。此外，即使你以批处理模式进行训练，它也需要至少
    256 GB 的内存。'
- en: '**Number of epochs**: This is the number of iterations through all the training
    records. I iterated for 10 epochs due to time constraints.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练轮数**：这是指对所有训练记录进行迭代的次数。由于时间限制，我进行了 10 轮训练。'
- en: '**Number of batches**: This is the number of records in each batch, for example,
    32, 64, and 128\. I used 128.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批次数量**：这是每个批次中的记录数，例如，32、64 和 128。我使用了 128。'
- en: Now, with the preceding hyperparameters, we can start training our CNNs. The
    following code does the trick. The thing is that at first, we prepare the training
    set, then we define the required hyperparameters, and then we normalize the dataset
    so the ND4j data frame is encoded so that any labels that are considered true
    are ones and the rest zeros. Then, we shuffle both the rows and labels of the
    encoded dataset.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用前面的超参数，我们可以开始训练我们的 CNN。以下代码完成了这个任务。首先，我们准备训练集，然后定义所需的超参数，接着，我们对数据集进行标准化，使得
    ND4j 数据框架被编码，以便将所有被认为是正确的标签标记为 1，其他标签标记为 0。然后，我们对编码后的数据集进行行和标签的洗牌。
- en: 'Then, we need to create epochs for the dataset iterator using `ListDataSetIterator`
    and `MultipleEpochsIterator`, respectively. Once the dataset is converted into
    the batchmodel, we are then ready to train the constructed CNNs:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要分别使用 `ListDataSetIterator` 和 `MultipleEpochsIterator` 为数据集创建迭代器。一旦数据集转换为批处理模型，我们就可以开始训练构建的
    CNN：
- en: '[PRE37]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once we finish the training, we can evaluate the model on the test set:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以在测试集上评估模型：
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'When the evaluation is finished, we can now inspect the result of each CNN
    (run the `YelpImageClassifier.java` script):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估完成后，我们现在可以检查每个 CNN 的结果（运行 `YelpImageClassifier.java` 脚本）：
- en: '[PRE39]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Oops! Unfortunately, we have not seen good accuracy. However, do not worry,
    since in the FAQ section, we will see how to improve upon this. Finally, we can
    save the layer-wise network configuration and network weights to be used later
    on (that is, scoring before submission):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！不幸的是，我们没有看到好的准确率。然而，别担心，因为在 FAQ 部分，我们将看到如何改善这个问题。最后，我们可以保存逐层网络配置和网络权重，以便以后使用（即，在提交前进行评分）：
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the previous code, we also saved a JSON file containing all the network
    configurations and a binary file for holding all the weights and parameters of
    all the CNNs. This is done using two methods, namely `saveNN()` and `loadNN()`,
    which are defined in the `NetwokSaver.java` script. First, let''s look at the
    signature of the `saveNN()` method, as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们还保存了一个包含所有网络配置的JSON文件，以及一个保存所有CNN权重和参数的二进制文件。这是通过`saveNN()`和`loadNN()`两个方法实现的，两个方法定义在`NetwokSaver.java`脚本中。首先，让我们看一下`saveNN()`方法的签名，如下所示：
- en: '[PRE41]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The idea is visionary as well as important since, as I said earlier, you would
    not train your whole network for the second time to evaluate a new test set. For
    example, suppose you want to test just a single image. The thing is, we also have
    another method named `loadNN()` that reads back the `.json` and `.bin` files we
    created earlier to a `MultiLayerNetwork`, which can be used to score new test
    data. This method is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路既有远见又很重要，因为正如我之前所说的，你不会重新训练整个网络来评估新的测试集。例如，假设你只想测试一张图像。关键是，我们还提供了另一个名为`loadNN()`的方法，它可以读取我们之前创建的`.json`和`.bin`文件并将其加载到`MultiLayerNetwork`中，这样就可以用来对新的测试数据进行评分。这个方法如下所示：
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Scoring the model
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对模型进行评分
- en: 'The scoring approach that we are going to use is simple. It assigns business-level
    labels by averaging the image-level predictions. I did this in a simplistic manner,
    but you can try using a better approach. What I did is assign a business as label
    `0` if the average of the probabilities across all of its images belonging to
    class `0` are greater than a certain threshold, say, 0.5:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的评分方法很简单。它通过对图像级预测结果求平均，来为业务级标签分配标签。我是以简单的方式实现的，但你也可以尝试使用更好的方法。我做的是：如果所有图像属于类别`0`的概率平均值大于某个阈值（比如0.5），则将该业务分配为标签`0`：
- en: '[PRE43]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Then, I collected the model predictions from the `scoreModel()` method and
    merged them with `alignedData`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我从`scoreModel()`方法中收集了模型的预测结果，并将其与`alignedData`合并：
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Finally, we can restore the trained and saved models, restore them back, and
    generate the submission file for Kaggle. The thing is that we need to aggregate
    image predictions to business scores for each model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以恢复训练和保存的模型，重新加载它们，并生成Kaggle的提交文件。关键是，我们需要将每个模型的图像预测结果汇总为业务评分。
- en: Submission file generation
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提交文件生成
- en: For this, I wrote a class called `ResultFileGenerator.java`. According to the
    Kaggle web page, we will have to write the result in the `business_ids, labels`
    format. Here, `business_id` is the ID for the corresponding business, and the
    label is the multi-label prediction. Let's see how easily we can do that.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我编写了一个名为`ResultFileGenerator.java`的类。根据Kaggle网页的要求，我们需要以`business_ids, labels`格式写入结果。在这里，`business_id`是对应业务的ID，标签是多标签预测。让我们看看我们如何轻松实现这一点。
- en: 'First, we aggregate image predictions to business scores for each model. Then,
    we transform the preceding data structure into a list for each `bizID` containing
    a Tuple (`bizid`, `List[Double]`) where the `Vector[Double]` is the vector of
    probabilities:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将每个模型的图像预测结果聚合为业务评分。然后，我们将前面的数据结构转换为一个列表，每个`bizID`对应一个元组(`bizid`, `List[Double]`)，其中`Vector[Double]`是概率向量：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Therefore, once we have the result aggregated from each model, we then need
    to generate the submission file:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一旦我们从每个模型中汇总结果后，就需要生成提交文件：
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now that we have managed to do everything up to this point, we can now wrap
    this up and generate a sample prediction and submission file for Kaggle. For simplicity,
    I randomly sliced this to only 20,000 images to save time. Interested readers
    can try building CNNs for all the images, too. However, it might take days. Nevertheless,
    we will look at some performance tuning tips in the FAQ section.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了到目前为止的所有操作，接下来我们可以结束并生成Kaggle的样本预测和提交文件。为了简便起见，我随机选取了20,000张图像以节省时间。感兴趣的读者也可以尝试为所有图像构建CNN。然而，这可能需要几天时间。不过，我们将在常见问题部分提供一些性能调优的建议。
- en: Wrapping everything up by executing the main() method
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过执行main()方法来完成所有操作
- en: 'Let''s wrap the overall discussion by watching the performance of our model
    programmatically (see the main `YelpImageClassifier.java` class):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看程序化方式来总结整体讨论（参见主`YelpImageClassifier.java`类）：
- en: '[PRE47]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: It's true that we haven't achieved outstanding classification accuracy. Nevertheless,
    we can still try this with tuned hyperparameters. The following sections provide
    some insight.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们还没有实现出色的分类精度。然而，我们仍然可以尝试通过调优超参数来提高效果。接下来的部分将提供一些见解。
- en: Frequently asked questions (FAQs)
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题（FAQ）
- en: 'Although we have been able to solve this multi-label classification problem,
    the accuracy we experienced was below par. Therefore, in this section, we will
    see some **frequently asked questions** (**FAQs**) that might already be on your
    mind. Knowing the answers to these questions might help you to improve the accuracy
    of the CNNs we trained. Answers to these questions can be found in the Appendix:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经解决了这个多标签分类问题，但我们得到的准确率仍然不尽如人意。因此，在本节中，我们将看到一些**常见问题**（**FAQ**），这些问题可能已经浮现在你的脑海中。了解这些问题的答案可能有助于提高我们训练的
    CNN 的准确性。这些问题的答案可以在附录中找到：
- en: What are the hyperparameters that I can try tuning while implementing this project?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实现此项目时，我可以尝试调优哪些超参数？
- en: My machine is getting OOP while running this project. What should I do?
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我的机器在运行这个项目时遇到了 OOP 错误。我该怎么办？
- en: While training the networks with full images, my GPU is getting OOP. What should
    I do?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在使用完整图像训练网络时，我的 GPU 出现了 OOP 错误。我该怎么办？
- en: I understand that the predictive accuracy using CNN in this project is still
    very low. Did our network under or overfit? Is there any way to observe how the
    training went?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我理解使用 CNN 在这个项目中的预测准确性仍然非常低。我们的网络是过拟合还是欠拟合？有没有办法观察训练过程的情况？
- en: I am very interested in implementing the same project in Scala. How can I do
    that?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我非常感兴趣将这个项目实现为 Scala 版本。我该怎么做？
- en: Which optimizer should I use for this type of project where we need to process
    large-scale images?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个需要处理大规模图像的项目，我应该使用哪种优化器？
- en: How many hyperparameters do we have? I also want to see them for each layer.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有多少个超参数？我还想查看每一层的超参数。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how to develop a real-life application using CNNs
    on the DL4J framework. We have seen how to solve a multi-label classification
    problem through nine CNNs and a series of complex feature engineering and image
    manipulation operations. Albeit, we couldn't achieve higher accuracy, but readers
    are encouraged to tune hyperparameters in the code and try the same approach with
    the same dataset.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们已经看到如何使用 DL4J 框架上的 CNNs 开发一个实际应用。我们已经了解了如何通过九个 CNN 和一系列复杂的特征工程与图像处理操作来解决多标签分类问题。尽管我们未能实现更高的准确性，但我们鼓励读者在代码中调整超参数，并尝试在相同的数据集上使用相同的方法。
- en: Also, training the CNNs with all the images is recommended so that networks
    can get enough data to learn the features from Yelp images. One more suggestion
    is improving the feature extraction process so that the CNNs can have more quality
    features.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，推荐使用所有图像训练 CNN，以便网络可以获得足够的数据来学习 Yelp 图像中的特征。还有一个建议是改进特征提取过程，以便 CNN 能够获得更多的高质量特征。
- en: In the next chapter, we will see how to implement and deploy a hands-on deep
    learning project that classifies review texts as either positive or negative based
    on the words they contain. A large-scale movie review dataset that contains 50,000
    reviews (training plus testing) will be used.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何实现并部署一个实用的深度学习项目，该项目根据包含的单词将评论文本分类为正面或负面。将使用包含 50,000 条评论（训练和测试）的电影评论数据集。
- en: 'A combined approach using Word2Vec (that is, a widely used word embedding technique
    in NLP) and the LSTM network for modeling will be applied: the pre-trained Google
    news vector model will be used as the neural word embeddings. Then, the training
    vectors, along with the labels, will be fed into the LSTM network to classify
    them as negative or positive sentiments. This will evaluate the trained model
    on the test set.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用结合了 Word2Vec（即广泛应用于 NLP 的词嵌入技术）和 LSTM 网络的建模方法：将使用预训练的 Google 新闻向量模型作为神经词嵌入。然后，将训练向量和标签输入
    LSTM 网络，以对其进行负面或正面情感的分类。此方法将评估在测试集上训练的模型。
- en: Answers to questions
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题解答
- en: '**Answer** **to question 1**: The following hyperparameters are very important
    and must be tuned to achieve optimized results:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题1的答案**：以下超参数非常重要，必须进行调优以获得优化结果：'
- en: Dropout is used to randomly off certain neurons (that is, feature detectors)
    to prevent overfitting
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout 用于随机关闭某些神经元（即特征检测器），以防止过拟合。
- en: Learning rate optimization—Adagrad can be used for feature-specific learning
    rate optimization
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率优化—Adagrad 可用于特征特定的学习率优化。
- en: Regularization—L1 and/or L2 regularization
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化—L1 和/或 L2 正则化
- en: Gradient normalization and clipping
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度归一化和裁剪
- en: Finally, apply batch normalization to reduce internal covariate shift in training
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，应用批量归一化以减少训练中的内部协方差偏移。
- en: Now, for dropout, we can add dropout in each convolutional and dense layer and
    in case of overfitting, the model is specifically adjusted to the training dataset,
    so it will not be used for generalization. Therefore, although it performs well
    on the training set, its performance on the test dataset and subsequent tests
    is poor because it lacks the generalization property.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于dropout，我们可以在每个卷积层和密集层中添加dropout，如果出现过拟合，模型会特别调整以适应训练数据集，因此不会用于泛化。因此，尽管它在训练集上表现良好，但在测试数据集和随后的测试中的表现较差，因为它缺乏泛化能力。
- en: 'Anyway, we can apply dropout on a CNN and DenseLayer. Now, for better learning
    rate optimization, Adagrad can be used for feature-specific learning rate optimization.
    Then, for better regularization, we can use either L1 and/or L2\. Thus, considering
    this, our network configuration should look as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们可以在CNN和DenseLayer上应用dropout。现在，为了更好的学习率优化，可以使用Adagrad进行特征特定的学习率优化。然后，为了更好的正则化，我们可以使用L1和/或L2。因此，考虑到这一点，我们的网络配置应该如下所示：
- en: '[PRE48]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '**Answer to question 2**: Due to the layering architecture''s perspective and
    convolutional layers, training a CNN requires a huge amount of RAM. This is because
    the reverse pass of backpropagation requires all the intermediate values computed
    during the forward pass. Fortunately, during the inferencing stage, memory occupied
    by one layer is released as soon as the computation is completed when the next
    layer has been computed.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2的答案**：由于分层架构的角度和卷积层的影响，训练CNN需要大量的RAM。这是因为反向传播的反向传递需要所有在前向传播过程中计算出的中间值。幸运的是，在推理阶段，当下一个层计算完成时，当前层所占的内存会被释放。'
- en: Also, as stated earlier, DL4J is built upon ND4J and ND4J utilizes off-heap
    memory management. This enables us to control the maximum amount of off-heap memory.
    We can set the `org.bytedeco.javacpp.maxbytes` system property. For example, for
    a single JVM run, you can pass `-Dorg.bytedeco.javacpp.maxbytes=1073741824` to
    limit the off-heap memory to 1 GB.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，正如前面所述，DL4J建立在ND4J之上，而ND4J利用了堆外内存管理。这使得我们可以控制堆外内存的最大使用量。我们可以设置`org.bytedeco.javacpp.maxbytes`系统属性。例如，对于单次JVM运行，您可以传递`-Dorg.bytedeco.javacpp.maxbytes=1073741824`来将堆外内存限制为1GB。
- en: '**Answer to question 3**: As I mentioned previously, training a CNN with Yelp''s
    50,000 images took one whole day with a machine with a core i7 processor and 32
    GB of RAM. Naturally, performing this on all of the images can take a week. Therefore,
    in such cases, training on GPU makes much more sense.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题3的答案**：正如我之前提到的，用Yelp的50,000张图像训练CNN需要一天时间，使用的是一台拥有i7处理器和32GB RAM的机器。自然地，对所有图像进行此操作可能需要一周时间。因此，在这种情况下，使用GPU训练显然更为合理。'
- en: 'Fortunately, we have already seen that DL4J works on distributed GPUs, as well
    as on native. For this, it has what we call **backbends**, or different types
    of hardware that it works on. Finally, a funny question would be: What should
    we do if our GPU runs out of memory? Well, if your GPU runs out of memory while
    training a CNN, here are five things you could do in order to try to solve the
    problem (other than purchasing a GPU with more RAM):'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们已经看到，DL4J可以在分布式GPU上运行，也可以在本地运行。为此，它有我们所称的**反向传播**，或者它能够运行的不同硬件类型。最后，有一个有趣的问题是：如果我们的GPU内存不足该怎么办？好吧，如果在训练CNN时GPU内存不足，下面有五种方法可以尝试解决这个问题（除了购买更大内存的GPU）：
- en: Reduce the mini-batch size
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减小小批量大小
- en: Reduce dimensionality using a larger stride in one or more layers, but don't
    go with PCA or SVD
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增大一个或多个层的步幅来减少维度，但不要使用PCA或SVD
- en: Remove one or more layers unless it's strictly essential to have a very deep
    network
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除非必须使用非常深的网络，否则可以移除一层或多层。
- en: Use 16-bit floats instead of 32-bit (but precision has to be compromised)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用16位浮点数代替32位浮点数（但需要牺牲一定的精度）
- en: Distribute the CNN across multiple devices (that is, GPUs/CPUs)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将CNN分布到多个设备（即GPU/CPU）
- en: For more on distributed training on GPUs using DL4J, refer to [Chapter 8](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml),
    *Distributed Deep Learning – Video Classification Using Convolutional LSTM Networks*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于使用DL4J在GPU上进行分布式训练的信息，请参考[第8章](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml)，*分布式深度学习
    - 使用卷积LSTM网络进行视频分类*。
- en: '**Answer** **to question 4**: It is true that we did not experience good accuracy.
    However, there are several reasons as to why we have not performed hyperparameter
    tuning. Secondly, we have not trained our network with all the images, so our
    network does not have enough data to learn the Yelp images. Finally, we can still
    see the model versus iteration score and other parameters from the following graph,
    so we can see that our model was not overfitted:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 4 的回答**：确实，我们并没有获得良好的准确率。然而，有几个原因解释为什么我们没有进行超参数调优。其次，我们没有用所有图像来训练网络，因此我们的网络没有足够的数据来学习
    Yelp 图像。最后，我们仍然可以从以下图表中看到模型与迭代得分及其他参数，因此我们可以看到我们的模型并没有过拟合：'
- en: '![](img/36122f86-763f-4da4-8628-de1c9a84a1c2.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36122f86-763f-4da4-8628-de1c9a84a1c2.png)'
- en: Model versus iteration score and other parameters of the LSTM sentiment analyzer
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与迭代得分及 LSTM 情感分析器的其他参数
- en: '**Answer** **to question 5**: Yes, it is possible since Scala is also a JVM
    language, so it would not be that difficult to convert this Java project into
    Scala. Nevertheless, one of my previous books solves this same problem in Scala.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 5 的回答**：是的，这是可能的，因为 Scala 也是一种 JVM 语言，所以将这个 Java 项目转换成 Scala 不会太难。尽管如此，我的其中一本书也在
    Scala 中解决了这个相同的问题。'
- en: 'Here is the reference: Md. Rezaul Karim, *Scala Machine Learning Projects*,
    Packt Publishing Ltd., January 2018\. Note that in that book, I used an old version
    of ND4J and DL4J, but I believe you can upgrade it by following this project.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是参考资料：Md. Rezaul Karim，*Scala 机器学习项目*，Packt Publishing Ltd.，2018 年 1 月。请注意，在那本书中，我使用的是旧版本的
    ND4J 和 DL4J，但我相信你可以通过遵循这个项目来进行升级。
- en: '**Answer** **to question 6**: Since, in CNN, one of the objective functions
    is to minimize the evaluated cost, we must define an optimizer. DL4j supports
    the following optimizers:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6 的回答**：由于在 CNN 中，目标函数之一是最小化评估的成本，我们必须定义一个优化器。DL4j 支持以下优化器：'
- en: SGD (learning rate only)
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SGD（仅学习率）
- en: Nesterovs momentum
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov 的动量
- en: Adagrad
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: RMSProp
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSProp
- en: Adam
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam
- en: AdaDelta
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaDelta
- en: For more information, interested readers can refer to the DL4J page on available
    updaters at [https://deeplearning4j.org/updater](https://deeplearning4j.org/updater).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，感兴趣的读者可以参考 DL4J 页面上关于可用更新器的内容：[https://deeplearning4j.org/updater](https://deeplearning4j.org/updater)。
- en: '**Answer to question 7**: Just use the following code immediately after network
    initialization:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 7 的回答**：只需在网络初始化后立即使用以下代码：'
- en: '[PRE49]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This also tell us that the subsampling layers do not have any hyperparameters.
    Nevertheless, if you want to create an MLP or DBN, we will require millions of
    hyperparameters. However, here, we can see that we only need 263,000 hyperparameters.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这也告诉我们，子采样层没有任何超参数。尽管如此，如果你想创建一个 MLP 或 DBN，我们将需要数百万个超参数。然而，在这里，我们可以看到我们只需要 263,000
    个超参数。
