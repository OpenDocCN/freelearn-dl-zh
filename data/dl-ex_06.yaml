- en: Deep Feed-forward Neural Networks - Implementing Digit Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度前馈神经网络 - 实现数字分类
- en: A **feed-forward neural network** (**FNN**) is a special type of neural network
    wherein links/connections between neurons do not form a cycle. As such, it is
    different from other architectures in a neural network that we will get to study
    later on in this book (recurrent-type neural networks). The FNN is a widely used
    architecture and it was the first and simplest type of neural network.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**前馈神经网络**（**FNN**）是一种特殊类型的神经网络，其中神经元之间的连接/链接不形成循环。因此，它不同于我们在本书后面将学习的其他神经网络架构（如递归神经网络）。FNN是广泛使用的架构，也是最早和最简单的神经网络类型。'
- en: In this chapter, we will go through the architecture of a typical ;FNN, and
    we will be using the TensorFlow library for this. After covering these concepts,
    we will give a practical example of digit classification. The question of this
    example is, *Given a set of images that contain handwritten digits, how can you
    classify these images into 10 different classes (0-9)*?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将讲解典型的前馈神经网络（FNN）架构，并使用 TensorFlow 库进行实现。掌握这些概念后，我们将通过一个实际的数字分类示例进行说明。这个示例的问题是，*给定一组包含手写数字的图像，你如何将这些图像分类为10个不同的类别（0-9）*？
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Hidden units and architecture design
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏单元和架构设计
- en: MNIST dataset analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST 数据集分析
- en: Digit classification - model building and training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字分类 - 模型构建与训练
- en: Hidden units and architecture design
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏单元和架构设计
- en: In the next section, we'll recap artificial neural networks; they can do a good
    job in classification tasks such as classifying handwritten digits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾人工神经网络；它们在分类任务中表现良好，例如分类手写数字。
- en: 'Suppose we have the network shown in *Figure 1*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有如下所示的网络，参见*图1*：
- en: '![](img/fff2d218-3570-4cd7-8ed1-e85114799780.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fff2d218-3570-4cd7-8ed1-e85114799780.png)'
- en: 'Figure 1: Simple FNN with one hidden layer'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：具有一个隐藏层的简单FNN
- en: 'As mentioned earlier, the leftmost layer in this network is called the **input
    layer**, and the neurons within the layer are called **input neurons**. The rightmost
    or output layer contains the output neurons, or, as in this case, a single output
    neuron. The middle layer is called a **hidden layer**, since the neurons in this
    layer are neither inputs nor outputs. The term hidden perhaps sounds a little
    mysterious—the first time I heard the term, I thought it must have some deep philosophical
    or mathematical significance—but it really means *not an input and not an output*.
    It means nothing else. The preceding network has just a single hidden layer, but
    some networks have multiple hidden layers. For example, the following four-layer
    network has two hidden layers:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个网络中最左侧的层被称为**输入层**，这一层内的神经元被称为**输入神经元**。最右侧的层或输出层包含输出神经元，或者在本例中，仅包含一个输出神经元。中间的层被称为**隐藏层**，因为这一层中的神经元既不是输入神经元，也不是输出神经元。术语“隐藏”可能听起来有些神秘——我第一次听到这个词时，觉得它一定有某种深奥的哲学或数学意义——但它实际上仅仅意味着*既不是输入也不是输出*。就这么简单。前面的网络只有一个隐藏层，但有些网络有多个隐藏层。例如，下面这个四层的网络就有两个隐藏层：
- en: '![](img/01c99930-1ba8-40ec-ac97-bd50e301540a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01c99930-1ba8-40ec-ac97-bd50e301540a.png)'
- en: 'Figure 2: Artificial neural network with more hidden layers'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：具有更多隐藏层的人工神经网络
- en: The architecture in which the input, hidden, and output layers are organized
    is very straightforward. For example, let's go through a practical example to
    see whether a specific handwritten image has the digit 9 in it or not.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层、隐藏层和输出层的架构非常简单明了。例如，我们通过一个实际的例子来看一下，如何判断一张手写图像是否包含数字9。
- en: So first, we will feed the pixels of the input image to the input layer; for
    example, in the MNIST dataset, we have monochrome images. Each one of them is
    28 by 28, so we need to have 28 × 28 = 784 neurons in the input layer to receive
    this input image.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入图像的像素传递给输入层；例如，在MNIST数据集中，我们有单色图像。每一张图像的尺寸为28×28，因此我们需要在输入层中有28×28 =
    784个神经元来接收这个输入图像。
- en: In the output layer, we will need only 1 neuron, which produces a probability
    (or score) of whether this image has the digit 9 in it or not. For example, an
    output value of more than 0.5 means that this image has the digit 9, and if it's
    less than 0.5, then it means that the input image doesn't have the digit 9 in
    it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出层，我们只需要一个神经元，该神经元输出一个概率（或得分），表示该图像是否包含数字 9。例如，输出值大于 0.5 表示该图像包含数字 9，如果小于
    0.5，则表示该输入图像不包含数字 9。
- en: So these types of networks, where the output from one layer is fed as an input
    to the next layer, are called FNNs. This kind of sequentiality in the layers means
    that there are no loops in it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这种类型的网络，其中一个层的输出作为输入传递给下一层，称为 FNN（前馈神经网络）。这种层与层之间的顺序性意味着网络中没有循环。
- en: MNIST dataset analysis
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据集分析
- en: In this section, we are going to get our hands dirty by implementing a classifier
    for handwritten images. This kind of implementation could be considered as the
    *Hello world!* of neural networks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将亲自动手实现一个手写图像的分类器。这种实现可以被看作是神经网络的 *Hello world!*。
- en: 'MNIST is a widely used dataset for benchmarking machine learning techniques.
    The dataset contains a set of handwritten digits like the ones shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 是一个广泛使用的数据集，用于基准测试机器学习技术。该数据集包含一组手写数字，像这里展示的这些：
- en: '![](img/080b85fa-6251-42d9-b069-a96ac276eefe.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/080b85fa-6251-42d9-b069-a96ac276eefe.png)'
- en: 'Figure 3: Sample digits from the MNIST dataset'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：MNIST 数据集中的样本数字
- en: So, the dataset includes handwritten images and their corresponding labels as
    well.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，数据集包括手写图像及其对应的标签。
- en: In this section, we are going to train a basic model on these images and the
    goal will be to tell which digit is handwritten in the input images.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将基于这些图像训练一个基本的模型，目标是识别输入图像中的手写数字。
- en: Also, you'll find out that we will be able to accomplish this classification
    task using very few lines of code, but the idea behind this implementation is
    to understand the basic bits and pieces for building a neural network solution.
    Moreover, we are going to cover the main concepts of neural networking in this
    implementation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您会发现我们可以通过非常少的代码行来完成这个分类任务，但这个实现的核心思想是理解构建神经网络解决方案的基本组件。此外，我们还将涵盖在此实现中神经网络的主要概念。
- en: The MNIST data
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据
- en: 'The MNIST data is hosted on Yann LeCun''s website ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    Fortunately, TensorFlow provides some helper functions to download the dataset,
    so let''s start off by downloading the dataset using the following two lines of
    code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据托管在 Yann LeCun 的网站上 ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))。幸运的是，TensorFlow
    提供了一些辅助函数来下载数据集，所以让我们先用以下两行代码下载数据集：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The MNIST data is split into three parts: 55,000 data points of training data
    (`mnist.train`), 10,000 points of test data (`mnist.test`), and 5,000 points of
    validation data (`mnist.validation`). This split is very important; it''s essential
    in machine learning that we have separate data that we don''t learn from so that
    we can make sure that what we''ve learned actually generalizes!'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据分为三部分：55,000 个训练数据点（`mnist.train`），10,000 个测试数据点（`mnist.test`），和 5,000
    个验证数据点（`mnist.validation`）。这种划分非常重要；在机器学习中，必须有独立的数据集，我们不能从这些数据中学习，以确保我们的学习结果具有泛化能力！
- en: 'As mentioned earlier, every MNIST sample has two parts: an image of a handwritten
    digit and its corresponding label. Both the training set and test set contain
    images and their corresponding labels. For example, the training images are `mnist.train.images`
    and the training labels are `mnist.train.labels`.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个 MNIST 样本有两个部分：一个手写数字的图像和它对应的标签。训练集和测试集都包含图像及其相应的标签。例如，训练图像是 `mnist.train.images`，训练标签是
    `mnist.train.labels`。
- en: 'Each image is 28 pixels by 28 pixels. We can interpret this as a big array
    of numbers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图片的尺寸为 28 像素 x 28 像素。我们可以将其解释为一个包含数字的大数组：
- en: '![](img/67eb24ef-9a39-4345-b9ec-c1e6c2b020a8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67eb24ef-9a39-4345-b9ec-c1e6c2b020a8.png)'
- en: 'Figure 4: MNIST digit in matrix representation (intensity values)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MNIST 数字的矩阵表示（强度值）
- en: In order to feed this matrix of pixel values to the input layer of the neural
    network, we need to flatten this matrix into a vector of 784 values. So, the final
    shape of the dataset will be a bunch of 784-dimensional vector space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这张像素值矩阵输入到神经网络的输入层，我们需要将矩阵展平为一个包含 784 个值的向量。因此，数据集的最终形状将是一个 784 维的向量空间。
- en: 'The result is that `mnist.train.images` is a tensor with a shape of `(55000,
    784)`. The first dimension is an index of the list of images and the second dimension
    is the index for each pixel in each image. Each entry in the tensor is a pixel
    intensity between 0 and 1 for a particular pixel in a particular image:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 `mnist.train.images` 是一个形状为 `(55000, 784)` 的张量。第一个维度是图像列表的索引，第二个维度是每个图像中每个像素的索引。张量中的每个条目是一个特定图像中特定像素的像素强度，值在
    0 到 1 之间：
- en: '![](img/b211b0d9-6625-41f0-b873-a2ddab250156.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b211b0d9-6625-41f0-b873-a2ddab250156.png)'
- en: 'Figure 5: MNIST data analysis'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：MNIST 数据分析
- en: As we mentioned previously, each image in the dataset has its corresponding
    label that ranges from 0 to 9.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，数据集中的每个图像都有一个对应的标签，范围从 0 到 9。
- en: 'For the purposes of this implementation, we''re going to encode our labels
    as one-hot vectors. A one-hot vector is a vector of all zeros except the index
    of the digit that this vector represents. For example, 3 would be [0,0,0,1,0,0,0,0,0,0].
    Consequently, `mnist.train.labels` is a `(55000, 10)` array of floats:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实现，我们将标签编码为 one-hot 向量。One-hot 向量是一个除了表示该向量所代表的数字索引位置为 1 之外，其它位置全为 0 的向量。例如，3
    将是 [0,0,0,1,0,0,0,0,0,0]。因此，`mnist.train.labels` 是一个形状为 `(55000, 10)` 的浮点数组：
- en: '![](img/5e497ee6-2d08-4004-a666-f1d242eb2060.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e497ee6-2d08-4004-a666-f1d242eb2060.png)'
- en: 'Figure 6: MNIST data analysis'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：MNIST 数据分析
- en: Digit classification – model building and training
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字分类 – 模型构建与训练
- en: Now, let's go ahead and build our model. So, we have 10 classes in our dataset
    0-9 and the goal is to classify any input image into one of these classes. Instead
    of giving a hard decision about the input image by saying only which class it
    could belong to, we are going to produce a vector of 10 possible values (because
    we have 10 classes). It'll represent the probabilities of each digit from 0-9
    being the correct class for the input image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始构建我们的模型。所以，我们的数据集有 10 个类别，分别是 0 到 9，目标是将任何输入图像分类为其中一个类别。我们不会仅仅给出输入图像属于哪个类别的硬性判断，而是将输出一个包含
    10 个可能值的向量（因为我们有 10 个类别）。它将表示每个数字从 0 到 9 为输入图像的正确类别的概率。
- en: For example, suppose we feed the model with a specific image. The model might
    be 70% sure that this image is 9, 10% sure that this image is 8, and so on. So,
    we are going to use the softmax regression here, which will produce values between
    0 and 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们输入一个特定的图像。模型可能 70% 确定这个图像是 9，10% 确定这个图像是 8，依此类推。所以，我们将在这里使用 softmax 回归，它将产生介于
    0 和 1 之间的值。
- en: 'A softmax regression has two steps: first we add up the evidence of our input
    being in certain classes, and then we convert that evidence into probabilities.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 回归有两个步骤：首先我们将输入属于某些类别的证据加总，然后将这些证据转换为概率。
- en: To tally up the evidence that a given image is in a particular class, we do
    a weighted sum of the pixel intensities. The weight is negative if that pixel
    having a high intensity is evidence against the image being in that class, and
    positive if it is evidence in favor.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了统计某个图像属于特定类别的证据，我们对像素强度进行加权求和。如果某个像素强度高则反映该图像不属于该类别，则权重为负；如果它支持该图像属于该类别，则权重为正。
- en: '*Figure 7* shows the weights one model learned for each of these classes. Red
    represents negative weights, while blue represents positive weights:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7* 显示了模型为每个类别学到的权重。红色代表负权重，蓝色代表正权重：'
- en: '![](img/650f626c-81a7-4e1c-a0c2-a756f5780c21.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/650f626c-81a7-4e1c-a0c2-a756f5780c21.png)'
- en: 'Figure 7: Weights one model learned for each of MNIST classes'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型为每个 MNIST 类别学到的权重
- en: 'We also add some extra evidence called a **bias**. Basically, we want to be
    able to say that some things are more likely independent of the input. The result
    is that the evidence for a class *i*, given an input, *x*, is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了一些额外的证据，称为 **偏置**。基本上，我们希望能够说某些事情在不依赖于输入的情况下更有可能。结果是，给定输入 *x* 时，类别 *i*
    的证据为：
- en: '![](img/3a99b106-2fd3-404f-bb8f-b2c894db0e26.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a99b106-2fd3-404f-bb8f-b2c894db0e26.png)'
- en: 'Where:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*W[i]* is the weights'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*W[i]* 是权重'
- en: '*b[i]* is the bias for class *i*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[i]* 是类别 *i* 的偏置'
- en: '*j* is an index for summing over the pixels in our input image *x*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*j* 是用来对输入图像 *x* 中的像素求和的索引。'
- en: 'We then convert the evidence tallies into our predicted probabilities *y* using
    the softmax function:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 softmax 函数将证据总和转换为我们的预测概率 *y*：
- en: '*y = softmax(evidence)*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = softmax(证据)*'
- en: 'Here, softmax is serving as an activation or link function, shaping the output
    of our linear function into the form we want, in this case, a probability distribution
    over 10 cases (because we have 10 possible classes from 0-9). You can think of
    it as converting tallies of evidence into probabilities of our input being in
    each class. It''s defined as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，softmax 作为激活或连接函数，塑造了我们线性函数的输出形式，我们希望它是一个 10 类的概率分布（因为我们有 10 个可能的类，范围是 0
    到 9）。你可以将其看作是将证据的统计数据转换为输入属于每个类的概率。它的定义是：
- en: '*softmax(evidence) = normalize(exp(evidence))*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*softmax(证据) = 归一化(exp(证据))*'
- en: 'If you expand that equation, you get:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你展开这个方程，你会得到：
- en: '![](img/0a32b978-b54b-411e-b8fb-cf0132cbd402.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a32b978-b54b-411e-b8fb-cf0132cbd402.png)'
- en: 'But it''s often more helpful to think of softmax the first way: exponentiating
    its inputs and then normalizing them. Exponentiation means that one more unit
    of evidence increases the weight given to any hypothesis exponentially. And conversely,
    having one less unit of evidence means that a hypothesis gets a fraction of its
    earlier weight. No hypothesis ever has zero or negative weight. Softmax then normalizes
    these weights so that they add up to one, forming a valid probability distribution.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但通常更有帮助的是按第一种方式理解 softmax：对其输入进行指数运算，然后进行归一化。指数运算意味着多一个证据单位会使任何假设的权重指数级增长。反过来，减少一个证据单位意味着该假设的权重会减少。没有任何假设的权重会为零或负值。然后，softmax
    对这些权重进行归一化，使它们的和为 1，形成一个有效的概率分布。
- en: 'You can picture our softmax regression as looking something like the following,
    although with a lot more *x*''*s*. For each output, we compute a weighted sum
    of the *x*''s, add a bias, and then apply softmax:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把我们的 softmax 回归想象成以下的样子，尽管它会有更多的 *x*'s。对于每个输出，我们计算 *x*'s 的加权和，加入偏置，然后应用 softmax：
- en: '![](img/66464f07-4b6d-4ba2-b7a6-28833847370b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66464f07-4b6d-4ba2-b7a6-28833847370b.png)'
- en: 'Figure 8: Visualization of softmax regression'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：softmax 回归的可视化
- en: 'If we write that out as equations, we get:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将其写成方程式，我们得到：
- en: '![](img/88eb5cd6-723b-4cef-807e-c578cc50020c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88eb5cd6-723b-4cef-807e-c578cc50020c.png)'
- en: 'Figure 9: Equation representation of the softmax regression'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：softmax 回归的方程表示
- en: 'We can use vector notation for this procedure. This means that we''ll be turning
    it into a matrix multiplication and vector addition. This is very helpful for
    computational efficiency and readability:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用向量表示法来处理这个过程。这意味着我们将其转换为矩阵乘法和向量加法。这对于计算效率和可读性非常有帮助：
- en: '![](img/cf06fcbb-5b11-4a96-a7f2-d6cd04e85b5f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf06fcbb-5b11-4a96-a7f2-d6cd04e85b5f.png)'
- en: 'Figure 10: Vectorized representation of the softmax regression equation'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：softmax 回归方程的向量化表示
- en: 'More compactly, we can just write:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更简洁地，我们可以写成：
- en: '*y = softmax(W[x] + b)*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = softmax(W[x] + b)*'
- en: Now, let's turn that into something that TensorFlow can use.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其转换为 TensorFlow 可以使用的形式。
- en: Data analysis
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析
- en: 'So, let''s go ahead and start implementing our classifier. Let''s start off
    by importing the required packages for this implementation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始实现我们的分类器。我们首先导入实现所需的包：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next up, we are going to define some helper functions to make us able to subset
    from the original dataset that we have downloaded:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一些辅助函数，以便从我们下载的原始数据集中进行子集选择：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Also, we''re going to define two helper functions for displaying specific digits
    from the dataset or even display a flattened version of a subset of images:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将定义两个辅助函数，用于显示数据集中的特定数字，或者甚至显示某个图像子集的平铺版本：
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let's get down to business and start playing around with the dataset. So
    we are going to define the training and testing examples that we would like to
    load from the original dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始正式处理数据集。我们将定义我们希望从原始数据集中加载的训练和测试示例。
- en: 'Now, we''ll get down to the business of building and training our model. First,
    we define variables with how many training and test examples we would like to
    load. For now, we will load all the data, but we will change this value later
    on to save resources:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始构建和训练我们的模型。首先，我们定义变量，指定我们希望加载的训练和测试示例的数量。目前，我们将加载所有数据，但稍后会更改这个值以节省资源：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So now, we have a training set of 55,000 samples of handwritten digits, and
    each sample is 28 by 28 pixel images flattened to be a 784-dimensional vector.
    We also have their corresponding labels in a one-hot encoding format.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在，我们有一个包含 55,000 个手写数字样本的训练集，每个样本是 28×28 像素的图像，经过展平成为 784 维的向量。我们还拥有这些样本对应的标签，采用
    one-hot 编码格式。
- en: 'The `target_values_train` data are the associated labels for all the `input_values_train`
    samples. In the following example, the array represents a 7 in one-hot encoding
    format:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`target_values_train`数据是所有`input_values_train`样本的关联标签。在以下示例中，数组代表数字 7 的独热编码格式：'
- en: '![](img/750c677e-5094-4b5b-b780-f544ae8a31ff.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/750c677e-5094-4b5b-b780-f544ae8a31ff.png)'
- en: 'Figure 11: One hot encoding for the digit 7'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：数字 7 的独热编码
- en: 'So let''s visualize a random image from the dataset and see how it looks like,
    so we are going to use our preceding helper function to display a random digit
    from the dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们从数据集中随机选择一张图片并看看它是什么样子的，我们将使用之前的辅助函数来显示数据集中的随机数字：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/6f0caec6-5918-4765-ba6a-f783062558bb.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f0caec6-5918-4765-ba6a-f783062558bb.png)'
- en: 'Figure 12: Output digit of the display_digit method'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：`display_digit`方法的输出数字
- en: 'We can also visualize a bunch of flattened images using the helper function
    defined before. Each value in the flattened vector represents a pixel intensity,
    so visualizing the pixels will look like this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用之前定义的辅助函数来可视化一堆展平后的图片。展平向量中的每个值代表一个像素的强度，因此可视化这些像素将是这样的：
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](img/e51ef11a-ea66-4d8c-9e75-ec0806f9085b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e51ef11a-ea66-4d8c-9e75-ec0806f9085b.png)'
- en: 'Figure 13: First 400 training examples'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：前 400 个训练样本
- en: Building the model
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'So far, we haven''t started to build our computational graph for this classifier.
    Let''s start off by creating the session variable that will be responsible for
    executing the computational graph we are going to build:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还没有开始为这个分类器构建计算图。让我们先创建一个会负责执行我们将要构建的计算图的会话变量：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next up, we are going to define our model''s placeholders, which will be used
    to feed data into the computational graph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们模型的占位符，这些占位符将用于将数据传递到计算图中：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When we specify `None` in our placeholder's first dimension, it means the placeholder
    can be fed as many examples as we like. In this case, our placeholder can be fed
    any number of examples, where each example has a `784` value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在占位符的第一个维度指定`None`时，这意味着该占位符可以接受任意数量的样本。在这种情况下，我们的占位符可以接收任何数量的样本，每个样本有一个`784`的值。
- en: 'Now, we need to define another placeholder for feeding the image labels. Also
    we''ll be using this placeholder later on to compare the model predictions with
    the actual labels of the images:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义另一个占位符来传入图片标签。我们将在之后使用这个占位符来将模型的预测与图像的实际标签进行比较：
- en: '[PRE9]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we will define the `weights` and `biases`. These two variables will be
    the trainable parameters of our network and they will be the only two variables
    needed to make predictions on unseen data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义`weights`和`biases`。这两个变量将成为我们网络的可训练参数，它们将是进行未知数据预测时所需的唯一两个变量：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: I like to think of these `weights` as 10 cheat sheets for each number. This
    is similar to how a teacher uses a cheat sheet to grade a multiple choice exam.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢把这些`weights`看作是每个数字的 10 张备忘单。这类似于老师用备忘单来给多选考试打分。
- en: 'We will now define our softmax regression, which is our classifier function.
    This particular classifier is called **multinomial logistic regression**, and
    we make the prediction by multiplying the flattened version of the digit by the
    weight and then adding the bias:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义我们的 softmax 回归，它是我们的分类器函数。这个特殊的分类器叫做**多项式逻辑回归**，我们通过将数字的展平版本与权重相乘然后加上偏差来做出预测：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'First, let''s ignore the softmax and look at what''s inside the softmax function.
    `matmul` is the TensorFlow function for multiplying matrices. If you know matrix
    multiplication ([https://en.wikipedia.org/wiki/Matrix_multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)),
    you''ll understand that this computes properly and that:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们忽略 softmax，看看 softmax 函数内部的内容。`matmul`是 TensorFlow 用于矩阵乘法的函数。如果你了解矩阵乘法（[https://en.wikipedia.org/wiki/Matrix_multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)），你就会明白它是如何正确计算的，并且：
- en: '![](img/872f2c41-24e3-4684-a856-8ec72f4f0aa7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/872f2c41-24e3-4684-a856-8ec72f4f0aa7.png)'
- en: 'Will result in a number of training examples fed (**m**) × number of classes
    (**n**) matrix:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 将导致一个由训练样本数（**m**） × 类别数（**n**）的矩阵：
- en: '![](img/da19af56-1dbd-4cf7-a371-c8af23bc0538.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da19af56-1dbd-4cf7-a371-c8af23bc0538.png)'
- en: 'Figure 13: Simple matrix multiplication.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：简单的矩阵乘法。
- en: 'You can confirm it by evaluating `softmax_layer`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过评估`softmax_layer`来确认这一点：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, let's experiment with the computational graph that we have defined previously
    with three samples from the training set and see how it works. To execute the
    computational graph, we need to use the session variable that we defined before.
    And we need to initialize the variables using `tf.global_variables_initializer()`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用之前定义的计算图，使用训练集中的三个样本来进行实验，看看它是如何工作的。为了执行计算图，我们需要使用之前定义的会话变量。并且，我们需要使用`tf.global_variables_initializer()`来初始化变量。
- en: 'Let''s go ahead and only feed three samples to the computational graph:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们仅向计算图输入三个样本进行实验：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, you can see the model predictions for the three training samples that
    fed to it. At the moment, the model has learned nothing about our task because
    we haven't gone through the training process yet, so it just outputs 10% probability
    of each digit being the correct class for the input samples.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到模型对于输入的三个训练样本的预测结果。目前，模型还没有学到任何关于我们任务的东西，因为我们还没有经过训练过程，所以它只是输出每个数字为输入样本正确类别的
    10% 概率。
- en: As we mentioned previously, softmax is an activation function that squashes
    the output to be between 0 and 1, and the TensorFlow implementation of softmax
    ensures that all the probabilities of a single input sample sums up to one.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，softmax 是一种激活函数，它将输出压缩到 0 到 1 之间，TensorFlow 对 softmax 的实现确保单个输入样本的所有概率加起来为
    1。
- en: 'Let''s experiment a bit with the softmax function of TensorFlow:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微实验一下 TensorFlow 的 softmax 函数：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next up, we need to define our loss function for this model, which will measure
    how good or bad our classifier is while trying to assign a class for the input
    images. The accuracy of our model is calculated by making a comparison between
    the actual values that we have in the dataset and the predictions that we got
    from the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为这个模型定义损失函数，来衡量分类器在尝试为输入图像分配类别时的好坏。模型的准确度是通过比较数据集中实际的值与模型输出的预测值来计算的。
- en: The goal will be to reduce any misclassifications between the actual and predicted
    values.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是减少实际值和预测值之间的误分类。
- en: 'Cross-entropy is defined as:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵的定义为：
- en: '![](img/95cbcc07-4d8d-4902-bcdc-0cd64709a406.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95cbcc07-4d8d-4902-bcdc-0cd64709a406.png)'
- en: 'Where:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*y* is our predicted probability distribution'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*是我们预测的概率分布'
- en: '*y''* is the true distribution (the one-hot vector with the digit labels)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y''*是实际分布（带有数字标签的独热编码向量）'
- en: In some rough sense, cross-entropy measures how inefficient our predictions
    are for describing the actual input.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种粗略的意义上，交叉熵衡量了我们预测值在描述实际输入时的低效程度。
- en: 'We can implement the cross-entropy function:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以实现交叉熵函数：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This function takes the log of all our predictions from `softmax_layer` (whose
    values range from 0 to 1) and multiplies them element-wise ([https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29))
    by the example''s true value, `output_values`. If the `log` function for each
    value is close to zero, it will make the value a large negative number (`-np.log(0.01)
    = 4.6`), and if it is close to one, it will make the value a small negative number
    (`-np.log(0.99) = 0.1`):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数对所有从 `softmax_layer`（其值在 0 到 1 之间）得到的预测取对数，并按元素逐个与示例的真实值 `output_values`
    相乘（[https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29)）。如果每个值的
    `log` 函数接近零，它将使值变成一个大负数（`-np.log(0.01) = 4.6`）；如果接近一，它将使值变成一个小负数（`-np.log(0.99)
    = 0.1`）：
- en: '![](img/559d0f5f-c295-4752-b80b-091aa75944ac.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/559d0f5f-c295-4752-b80b-091aa75944ac.png)'
- en: 'Figure 15: Visualization for Y = log (x)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：Y = log(x) 的可视化
- en: We are essentially penalizing the classifier with a very large number if the
    prediction is confidently incorrect and a very small number if the prediction
    is confidently correct.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，如果预测结果自信地错误，我们会用一个非常大的数字来惩罚分类器；如果预测结果自信地正确，我们则用一个非常小的数字来惩罚。
- en: 'Here is a simple Python example of a softmax prediction that is very confident
    that the digit is a 3:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个简单的 Python 示例，展示了一个对数字为 3 的预测非常自信的 softmax 预测：
- en: '[PRE17]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s create an array label of 3 as a ground truth to compare to our softmax
    function:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个值为 3 的数组标签作为真实值，以便与 softmax 函数进行比较：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Can you guess what value our loss function gives us? Can you see how the log
    of `j` would penalize a wrong answer with a large negative number? Try this to
    understand:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜到我们的损失函数给出的值是什么吗？你能看到 `j` 的对数如何用一个大的负数惩罚错误答案吗？试试这个来理解：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will return nine zeros and a value of 0.1053; when they all are summed
    up, we can consider this a good prediction. Notice what happens when we make the
    same prediction for what is actually a 2:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当它们全部加起来时，这将返回九个零和0.1053的值；我们可以认为这是一个很好的预测。注意当我们对实际上是2的预测做出同样的预测时会发生什么：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, our `cross_entropy` function gives us 4.6051, which shows a heavily penalized,
    poorly made prediction. It was heavily penalized due to the fact the classifier
    was very confident that it was a 3 when it actually was a 2.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的`cross_entropy`函数给出了4.6051，显示了一个严重惩罚的、预测不良的预测。由于分类器非常确信它是3，而实际上是2，因此受到了严重的惩罚。
- en: Next, we begin to train our classifier. In order to train it, we have to develop
    appropriate values for W and b that will give us the lowest possible loss.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始训练我们的分类器。为了训练它，我们必须开发适当的W和b的值，以便给出尽可能低的损失。
- en: 'The following is where we can now assign custom variables for training if we
    wish. Any value that is in all caps as follows is designed to be changed and messed
    with. In fact, I encourage it! First, use these values, and then notice what happens
    when you use too few training examples or too high or low of a learning rate:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们希望，我们可以为训练分配自定义变量。以下所有大写的值都可以更改和搞砸。事实上，我鼓励这样做！首先，使用这些值，然后注意当您使用太少的训练示例或学习率过高或过低时会发生什么：
- en: '[PRE21]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now initialize all variables so that they can be used by our TensorFlow
    graph:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化所有变量，以便它们可以被我们的 TensorFlow 图使用：
- en: '[PRE22]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to train the classifier using the gradient descent algorithm.
    So we first define our training method and some variables for measuring the model
    accuracy. The variable `train` will perform the gradient descent optimizer with
    a chosen learning rate in order to minimize the model loss function `model_cross_entropy`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用梯度下降算法训练分类器。因此，我们首先定义我们的训练方法和一些用于测量模型准确性的变量。变量`train`将执行梯度下降优化器，选择一个学习率来最小化模型损失函数`model_cross_entropy`：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Model training
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: Now, we'll define a loop that iterates `num_iterations` times. And for each
    loop, it runs training, feeding in values from `input_values_train` and `target_values_train`
    using `feed_dict`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义一个循环，它将迭代`num_iterations`次。对于每个循环，它都会运行训练，使用`feed_dict`从`input_values_train`和`target_values_train`中提供值。
- en: 'In order to calculate accuracy, it will test the model against the unseen data
    in `input_values_test` :'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算准确性，它将测试模型对`input_values_test`中的未见数据的表现：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice how the loss was still decreasing near the end but our accuracy slightly
    went down! This shows that we can still minimize our loss and hence maximize the
    accuracy over our training data, but this may not help us predict the testing
    data used for measuring accuracy. This is also known as **overfitting** (not generalizing).
    With the default settings, we got an accuracy of about 91%. If I wanted to cheat
    to get 94% accuracy, I could've set the test examples to 100\. This shows how
    not having enough test examples can give you a biased sense of accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，损失在接近尾声时仍在减小，但我们的准确率略有下降！这表明我们仍然可以最小化我们的损失，从而在训练数据上最大化准确率，但这可能无助于预测用于测量准确性的测试数据。这也被称为**过拟合**（不具有泛化性）。使用默认设置，我们获得了约91%的准确率。如果我想欺骗以获得94%的准确率，我本可以将测试示例设置为100。这显示了没有足够的测试示例可能会给您一个偏见的准确性感觉。
- en: Keep in mind that this is a very inaccurate way to calculate our classifier's
    performance. However, we did this on purpose for the sake of learning and experimentation.
    Ideally, when training with large datasets, you train using small batches of training
    data at a time, not all at once.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这种方式计算我们分类器的性能非常不准确。但是，出于学习和实验的目的，我们特意这样做了。理想情况下，当使用大型数据集进行训练时，您应该一次使用小批量的训练数据，而不是全部一起。
- en: 'This is the interesting part. Now that we have calculated our weight cheat
    sheet, we can create a graph with the following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有趣的部分。现在我们已经计算出了我们的权重备忘单，我们可以用以下代码创建一个图表：
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/8ad5bc0f-e6ee-4843-8205-588cde3de594.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ad5bc0f-e6ee-4843-8205-588cde3de594.png)'
- en: 'Figure 15: Visualization of our weights from 0-9'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：我们权重的可视化从0到9
- en: The preceding figure shows the model weights from 0-9, which is the most important
    side of our classifier. All this amount of work of machine learning is done to
    figure out what the optimal weights are. Once they are calculated based on an
    optimization criteria, you have the **cheat sheet** and can easily find your answers
    using the learned weights.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了0到9的模型权重，这是我们分类器最重要的一部分。所有这些机器学习的工作都是为了找出最优的权重。一旦根据优化标准计算出这些权重，你就拥有了**备忘单**，并且可以轻松地利用学习到的权重找到答案。
- en: The learned model makes its prediction by comparing how similar or different
    the input digit sample is to the red and blue weights. The darker the red, the
    better the hit; white means neutral and blue means misses.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的模型通过比较输入数字样本与红色和蓝色权重的相似度或差异来做出预测。红色越深，命中越好；白色表示中立，蓝色表示未命中。
- en: 'Now, let''s use the cheat sheet and see how our model performs on it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用备忘单，看看我们的模型在其上的表现：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/265bc5f7-4cda-47a3-8382-3d3faef8af97.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/265bc5f7-4cda-47a3-8382-3d3faef8af97.png)'
- en: 'Let''s look at our softmax predictor:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的softmax预测器：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code will give us a 10-dimensional vector, with each column containing
    one probability:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会给我们一个10维向量，每一列包含一个概率：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can use the `argmax` function to find out the most probable digit to be
    the correct classification for our input image:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`argmax`函数来找出最有可能的数字作为我们输入图像的正确分类：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, we get a correct classification from our network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从网络中得到了一个正确的分类结果。
- en: 'Let''s use our knowledge to define a helper function that can select a random
    image from the dataset and test the model against it:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运用我们的知识定义一个辅助函数，能够从数据集中随机选择一张图像，并将模型应用于其上进行测试：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And now try it out:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，试试看：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/fdc05ce6-73ba-4b58-ad42-38c3adad66f9.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdc05ce6-73ba-4b58-ad42-38c3adad66f9.png)'
- en: We've got a correct classification again!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次得到了一个正确的分类结果！
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through a basic implementation of a FNN for our digit
    classification task. We also did a recap of the terminologies used in the neural
    network context.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了用于数字分类任务的FNN（前馈神经网络）的基本实现。我们还回顾了神经网络领域中使用的术语。
- en: Next up, we will build a sophisticated version of the digit classification model
    using some modern best practices and some tips and tricks to enhance the model's
    performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个更复杂的数字分类模型，使用一些现代最佳实践和技巧来提升模型的表现。
