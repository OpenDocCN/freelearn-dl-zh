- en: '*Chapter 10*: Applying the Power of Deep Learning to Videos'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：将深度学习的力量应用到视频中'
- en: Computer vision is focused on the understanding of visual data. Of course, that
    includes videos, which, at their core, are a sequence of images, which means we
    can leverage most of our knowledge regarding deep learning for image processing
    to videos and reap great results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉关注的是视觉数据的理解。当然，这也包括视频，视频本质上是图像的序列，这意味着我们可以利用我们关于图像处理的深度学习知识，应用到视频中并获得很好的结果。
- en: In this chapter, we'll start training a convolutional neuronal network to detect
    emotions in human faces, and then we'll learn how to apply it in a real-time context
    using our webcam.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始训练卷积神经网络，检测人脸中的情感，然后学习如何在实时上下文中使用我们的摄像头应用它。
- en: Then, in the remaining recipes, we'll use very advanced implementations of architectures,
    hosted in **TensorFlow Hub** (**TFHub**), specially tailored to tackle interesting
    video-related problems such as action recognition, frames generation, and text-to-video
    retrieval.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在接下来的食谱中，我们将使用**TensorFlow Hub**（**TFHub**）托管的非常先进的架构，专门用于解决与视频相关的有趣问题，如动作识别、帧生成和文本到视频的检索。
- en: 'Here are the recipes that we will be covering shortly:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将要覆盖的食谱内容：
- en: Detecting emotions in real time
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时检测情感
- en: Recognizing actions with TensorFlow Hub
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Hub识别动作
- en: Generating the middle frames of a video with TensorFlow Hub
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Hub生成视频的中间帧
- en: Performing text-to-video retrieval with TensorFlow Hub
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Hub进行文本到视频的检索
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'As usual, having access to a GPU is a great plus, particularly for the first
    recipe, where we''ll implement a network from scratch. Because the rest of the
    chapter leverages models in TFHub, your CPU should be enough, although a GPU will
    give you a pretty nice speed boost! In the *Getting ready* section, you''ll find
    the preparatory steps for each recipe. You can find the code for this chapter
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，拥有GPU是一个很大的优势，特别是在第一个食谱中，我们将从零开始实现一个网络。因为本章剩余部分利用了TFHub中的模型，所以即使是CPU也应该足够，尽管GPU能显著提高速度！在*准备就绪*部分，你可以找到每个食谱的准备步骤。你可以在这里找到本章的代码：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看代码实际演示视频：
- en: '[https://bit.ly/3qkTJ2l](https://bit.ly/3qkTJ2l).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/3qkTJ2l](https://bit.ly/3qkTJ2l)。'
- en: Detecting emotions in real time
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时检测情感
- en: At its most basic form, a video is just a series of images. By leveraging this
    seemingly simple or trivial fact, we can adapt what we know about image classification
    to create very interesting video processing pipelines powered by deep learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从最基本的形式来看，视频仅仅是图像序列。通过利用这一看似简单或微不足道的事实，我们可以将图像分类的知识应用到视频处理上，从而创建出由深度学习驱动的非常有趣的视频处理管道。
- en: In this recipe, we'll build an algorithm to detect emotions in real time (webcam
    streaming) or from video files. Pretty interesting, right?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们将构建一个算法，实时检测情感（来自摄像头流或视频文件）。非常有趣，对吧？
- en: Let's begin.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'First, we must install several external libraries, such as `OpenCV` and `imutils`.
    Execute the following command to install them:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装一些外部库，如`OpenCV`和`imutils`。执行以下命令安装它们：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To train an emotion classifier network, we'll use the dataset from the Kaggle
    competition `~/.keras/datasets` folder), extract it as `emotion_recognition`,
    and then unzip the `fer2013.tar.gz` file.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练情感分类器网络，我们将使用来自Kaggle比赛的数据集（`~/.keras/datasets`文件夹），将其提取为`emotion_recognition`，然后解压`fer2013.tar.gz`文件。
- en: 'Here are some sample images:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些示例图像：
- en: '![Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared,
    surprised, happy, and neutral](img/B14768_10_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 示例图像。情感从左到右：悲伤、生气、害怕、惊讶、开心和中立](img/B14768_10_001.jpg)'
- en: 'Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared,
    surprised, happy, and neutral'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 示例图像。情感从左到右：悲伤、生气、害怕、惊讶、开心和中立
- en: Let's get started!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: By the end of this recipe, you'll have your own emotion detector!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱结束时，你将拥有自己的情感检测器！
- en: 'Import all the dependencies:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有依赖项：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define a list of all possible emotions in our dataset, along with a color associated
    with each one:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集中所有可能情感的列表，并为每个情感指定一个颜色：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define a method to build the emotion classifier architecture. It receives the
    input shape and the number of classes in the dataset:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来构建情感分类器的架构。它接收输入形状和数据集中的类别数量：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Each block in the network is comprised of two ELU activated, batch-normalized
    convolutions, followed by a max pooling layer, and ending with a dropout layer.
    The block defined previously had 32 filters per convolution, while the following
    one has 64 filters per convolution:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络中的每个块由两个ELU激活、批量归一化的卷积层组成，接着是一个最大池化层，最后是一个丢弃层。前面定义的块每个卷积层有32个滤波器，而后面的块每个卷积层有64个滤波器：
- en: '[PRE4]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The third block has 128 filters per convolution:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个块每个卷积层有128个滤波器：
- en: '[PRE5]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we have two dense, ELU activated, batch-normalized layers, also followed
    by a dropout, each with 64 units:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有两个密集层，ELU激活、批量归一化，后面也跟着一个丢弃层，每个层有64个单元：
- en: '[PRE6]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we encounter the output layer, with as many neurons as classes in
    the dataset. Of course, it''s softmax-activated:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们遇到输出层，神经元数量与数据集中的类别数量相同，当然，采用softmax激活函数：
- en: '[PRE7]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`load_dataset()` loads both the images and labels for the training, validation,
    and test datasets:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_dataset()`加载训练集、验证集和测试集的图像和标签：'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The data in this dataset is in a CSV file, separated into `emotion`, `pixels`,
    and `Usage` columns. Let''s parse the `emotion` column first. Although the dataset
    contains faces for seven classes, we''ll combine *disgust* and *angry* (encoded
    as `0` and `1`, respectively) because both share most of the facial features,
    and merging them leads to better results:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个数据集中的数据存储在一个CSV文件中，分为`emotion`、`pixels`和`Usage`三列。我们首先解析`emotion`列。尽管数据集包含七类面部表情，我们将*厌恶*和*愤怒*（分别编码为`0`和`1`）合并，因为它们共享大多数面部特征，合并后会得到更好的结果：
- en: '[PRE9]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we parse the `pixels` column, which is 2,034 whitespace-separated integers,
    corresponding to the grayscale pixels for the image (48x48=2034):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们解析`pixels`列，它包含2,034个空格分隔的整数，代表图像的灰度像素（48x48=2034）：
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, to figure out to which subset this image and label belong, we must look
    at the `Usage` column:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了弄清楚这张图像和标签属于哪个子集，我们需要查看`Usage`列：
- en: '[PRE11]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Convert all the images to NumPy arrays:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有的图像转换为NumPy数组：
- en: '[PRE12]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, one-hot encode all the labels:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，对所有标签进行独热编码：
- en: '[PRE13]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Return all the images and labels:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回所有的图像和标签：
- en: '[PRE14]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define a function to compute the area of a rectangle. We''ll use this later
    to get the largest face detection:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个计算矩形区域面积的函数。稍后我们将用它来获取最大的面部检测结果：
- en: '[PRE15]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''ll now create a bar plot to display the probability distribution of the
    emotions detected in each frame. The following function is used to plot each bar,
    corresponding to a particular emotion, in said plot:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个条形图来显示每一帧中检测到的情感的概率分布。以下函数用于绘制每个条形图，代表某一特定情感：
- en: '[PRE16]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We''ll also draw a bounding box around the detected face, captioned with the
    recognized emotion:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还会在检测到的面部周围画一个边界框，并标注上识别出的情感：
- en: '[PRE17]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the `predict_emotion()` function, which takes the emotion classifier
    and an input image and returns the predictions output by the model:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`predict_emotion()`函数，该函数接收情感分类器和输入图像，并返回模型输出的预测结果：
- en: '[PRE18]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load a saved model if there is one:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有保存的模型，则加载它：
- en: '[PRE19]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Otherwise, train the model from scratch. First, build the path to the CSV with
    the data and then compute the number of classes in the dataset:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，从头开始训练模型。首先，构建CSV文件的路径，然后计算数据集中的类别数量：
- en: '[PRE20]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, load each subset of data:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，加载每个数据子集：
- en: '[PRE21]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Build the network and compile it. Also, define a `ModelCheckpoint` callback
    to save the best performing model, based on the validation loss:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建网络并编译它。同时，定义一个`ModelCheckpoint`回调函数来保存最佳表现的模型（基于验证损失）：
- en: '[PRE22]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the augmenters and generator for the training and validation sets. Notice
    that we''re only augmenting the training set, while we just rescale the images
    in the validation set:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练集和验证集的增强器和生成器。注意，我们仅增强训练集，而验证集中的图像只是进行重缩放：
- en: '[PRE23]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit the model for 300 epochs and then evaluate it on the test set (we only
    rescale the images in this subset):'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型300个周期，然后在测试集上评估模型（我们只对该子集中的图像进行重缩放）：
- en: '[PRE24]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Instantiate a `cv2.VideoCapture()` object to fetch the frames in a test video.
    If you want to use your webcam, replace `video_path` with `0`:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`cv2.VideoCapture()`对象来获取测试视频中的帧。如果你想使用你的网络摄像头，将`video_path`替换为`0`：
- en: '[PRE25]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a **Haar Cascades** face detector (this is a topic outside the scope
    of this book. If you want to learn more about Haar Cascades, refer to the *See
    also* section in this recipe):'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个**Haar级联**人脸检测器（这是本书范围之外的内容。如果你想了解更多关于Haar级联的内容，请参考本配方中的*另见*部分）：
- en: '[PRE26]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Iterate over each frame in the video (or webcam stream), exiting only if there
    are no more frames to read, or if the user presses the Q key:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历视频中的每一帧（或网络摄像头流），只有在没有更多帧可以读取，或用户按下Q键时才退出：
- en: '[PRE27]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Resize the frame to have a width of 380 pixels (the height will be computed
    automatically to preserve the aspect ratio). Also, create a canvas of where to
    draw the emotions bar plot, and a copy of the input frame in terms of where to
    plot the detected faces:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将帧调整为宽度为380像素（高度会自动计算以保持宽高比）。同时，创建一个画布，用于绘制情感条形图，并创建一个输入帧的副本，用于绘制检测到的人脸：
- en: '[PRE28]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Because Haar Cascades work on grayscale images, we must convert the input frame
    to black and white. Then, we run the face detector on it:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于Haar级联方法是在灰度图像上工作的，我们必须将输入帧转换为黑白图像。然后，我们在其上运行人脸检测器：
- en: '[PRE29]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Verify whether there are any detections and fetch the one with the largest
    area:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证是否有任何检测，并获取面积最大的那个：
- en: '[PRE30]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Extract the region of interest (`roi`) corresponding to the detected face and
    extract the emotions from it:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取与检测到的面部表情对应的感兴趣区域（`roi`），并从中提取情感：
- en: '[PRE31]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create the emotion distribution plot:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建情感分布图：
- en: '[PRE32]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the detected face along with the emotion it displays:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制检测到的面部表情及其所展示的情感：
- en: '[PRE33]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Show the result:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示结果：
- en: '[PRE34]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Check whether the user pressed Q, and if they did, break out of the loop:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查用户是否按下了Q键，如果按下了，则退出循环：
- en: '[PRE35]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, release the resources:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，释放资源：
- en: '[PRE36]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After 300 epochs, I obtained a test accuracy of 65.74%. Here you can see some
    snapshots of the emotions detected in the test video:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在300个周期后，我获得了65.74%的测试准确率。在这里，你可以看到一些测试视频中检测到的情感快照：
- en: '![Figure 10.2 – Emotions detected in two different snapshots'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 在两个不同快照中检测到的情感'
- en: '](img/B14768_10_002.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_002.jpg)'
- en: Figure 10.2 – Emotions detected in two different snapshots
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 在两个不同快照中检测到的情感
- en: 'We can see that the network correctly identifies sadness in the top frame,
    and happiness in the bottom one. Let''s take a look at another example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到网络正确地识别出了顶部帧中的悲伤表情，底部帧中识别出了幸福表情。让我们来看一个另一个例子：
- en: '![Figure 10.3 – Emotions detected in three different snapshots'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 在三个不同快照中检测到的情感'
- en: '](img/B14768_10_003.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_003.jpg)'
- en: Figure 10.3 – Emotions detected in three different snapshots
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 在三个不同快照中检测到的情感
- en: In the first frame, the girl clearly has a neutral expression, which was correctly
    picked up by the network. In the second frame, her face shows anger, which the
    classifier also detects. The third frame is more interesting, because her expression
    displays surprise, but it could also be interpreted as fear. Our detector seems
    to be split between these two emotions as well.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一帧中，女孩显然呈现出中性表情，网络正确地识别出来了。第二帧中，她的面部表情显示出愤怒，分类器也检测到这一点。第三帧更有趣，因为她的表情显示出惊讶，但也可以被解读为恐惧。我们的检测器似乎在这两种情感之间有所犹豫。
- en: Let's head over to the next section, shall we?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们前往下一部分，好吗？
- en: How it works…
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we implemented a fairly capable emotion detector for video streams,
    either from a built-in webcam, or a stored video file. We started by parsing the
    `FER 2013` data, which, unlike most other image datasets, is in CSV format. Then,
    we trained an emotion classifier on its images, achieving a respectable 65.74%
    accuracy on the test set.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们实现了一个相当强大的情感检测器，用于视频流，无论是来自内建的网络摄像头，还是存储的视频文件。我们首先解析了`FER 2013`数据集，它与大多数其他图像数据集不同，是CSV格式的。然后，我们在其图像上训练了一个情感分类器，在测试集上达到了65.74%的准确率。
- en: We must take into consideration the fact that facial expressions are tricky
    to interpret, even for humans. At a given time, we might display mixed emotions.
    Also, there are many expressions that share traits, such as *anger* and *disgust*,
    and *fear* and *surprise*, among others.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑到面部表情的解读非常复杂，甚至对于人类来说也是如此。在某一时刻，我们可能会展示混合情感。此外，还有许多表情具有相似特征，比如*愤怒*和*厌恶*，以及*恐惧*和*惊讶*，等等。
- en: The last step in this first recipe consisted of passing each frame in the input
    video stream to a Haar Cascade face detector, and then getting the emotions, using
    the trained classifier, from the regions of interest corresponding to the detected
    faces.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中的最后一步是将输入视频流中的每一帧传递给Haar Cascade人脸检测器，然后使用训练好的分类器从检测到的人脸区域获取情感。
- en: 'Although this approach works well for this particular problem, we must take
    into account that we overlooked a crucial assumption: each frame is independent.
    Simply put, we treated each frame in the video as an isolated image, but in reality,
    that''s not the case when we''re dealing with videos, because there''s a time
    dimension that, when accounted for, yields more stable and better results.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法对这个特定问题有效，但我们必须考虑到我们忽略了一个关键假设：每一帧都是独立的。简单来说，我们将视频中的每一帧当作一个独立的图像处理，但实际上，处理视频时并非如此，因为存在时间维度，如果考虑到这一点，将会得到更稳定、更好的结果。
- en: See also
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Here''s a great resource for understanding the Haar Cascade classifier: [https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的资源，用于理解Haar Cascade分类器：[https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)。
- en: Recognizing actions with TensorFlow Hub
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow Hub识别动作
- en: A very interesting application of deep learning to video processing involves
    action recognition. This is a challenging problem, because it not only presents
    the typical difficulties associated with classifying the contents of an image,
    but also includes a temporal component. An action in a video can vary depending
    on the order in which the frames are presented to us.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在视频处理中的一个非常有趣的应用是动作识别。这是一个具有挑战性的问题，因为它不仅涉及到图像分类中通常遇到的困难，还包括了时间维度。视频中的一个动作可能会根据帧呈现的顺序而有所不同。
- en: The good news is that there is an architecture that is perfectly suited to this
    kind of problem, known as **Inflated 3D Convnet** (**I3D**), and in this recipe
    we'll use a trained version hosted in TFHub to recognize actions in a varied selection
    of videos!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，存在一个非常适合这种问题的架构，称为**膨胀3D卷积网络**（**I3D**），在本食谱中，我们将使用TFHub上托管的训练版本来识别一组多样化视频中的动作！
- en: Let's get started.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧。
- en: Getting ready
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to install several supplementary libraries, such as `OpenCV`, `TFHub`,
    and `imageio`. Execute the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装几个补充库，如`OpenCV`、`TFHub`和`imageio`。执行以下命令：
- en: '[PRE37]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: That's it! Let's begin implementing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！让我们开始实现吧。
- en: How to do it…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Perform the following steps to complete the recipe:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成本食谱：
- en: 'Import all the required dependencies:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有所需的依赖项：
- en: '[PRE38]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Define the path to the `UCF101 – Action Recognition` dataset, from where we''ll
    fetch the test videos that we will pass to the model later on:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`UCF101 – 动作识别`数据集的路径，从中获取我们稍后将传递给模型的测试视频：
- en: '[PRE39]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the path to the labels file of the `Kinetics` dataset, the one used
    to train the 3D convolutional network we''ll use shortly:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Kinetics`数据集的标签文件路径，后者用于训练我们将很快使用的3D卷积网络：
- en: '[PRE40]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a temporary directory to cache the downloaded resources:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个临时目录，用于缓存下载的资源：
- en: '[PRE41]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create an unverified SSL context. We need this to be able to download data
    from UCF''s site (at the time of writing this book, it appears that their certificate
    has expired):'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个未经验证的SSL上下文。我们需要这个以便能够从UCF的网站下载数据（在编写本书时，似乎他们的证书已过期）：
- en: '[PRE42]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Define the `fetch_ucf_videos()` function, which downloads the list of the possible
    videos we''ll choose from to test our action recognizer:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`fetch_ucf_videos()`函数，该函数下载我们将从中选择的测试视频列表，以测试我们的动作识别器：
- en: '[PRE43]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Define the `fetch_kinetics_labels()` function, used to download and parse the
    labels of the `Kinetics` dataset:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`fetch_kinetics_labels()`函数，用于下载并解析`Kinetics`数据集的标签：
- en: '[PRE44]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the `fetch_random_video()` function, which selects a random video from
    our list of `UCF101` videos and downloads it to the temporary directory created
    in *Step 4*:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`fetch_random_video()`函数，该函数从我们的`UCF101`视频列表中选择一个随机视频，并将其下载到*第4步*中创建的临时目录中：
- en: '[PRE45]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the `crop_center()` function, which takes an image and crops a squared
    selection corresponding to the center of the received frame:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`crop_center()`函数，该函数接受一张图片并裁剪出对应于接收帧中心的正方形区域：
- en: '[PRE46]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define the `read_video()` function, which reads up to `max_frames` from a video
    stored in our cache and returns a list of all the read frames. It also crops the
    center of each frame, resizes it to 224x224x3 (the input shape expected by the
    network), and normalizes it:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `read_video()` 函数，它从我们的缓存中读取最多 `max_frames` 帧，并返回所有读取的帧列表。它还会裁剪每帧的中心，将其调整为
    224x224x3 的大小（网络期望的输入形状），并进行归一化处理：
- en: '[PRE47]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Define the `predict()` function, used to get the top five most likely actions
    recognized by the model in the input video:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `predict()` 函数，用于获取模型在输入视频中识别的前五个最可能的动作：
- en: '[PRE48]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Define the `save_as_gif()` function, which takes a list of frames corresponding
    to a video, and uses them to create a GIF representation:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `save_as_gif()` 函数，它接收一个包含视频帧的列表，并用它们创建 GIF 格式的表示：
- en: '[PRE49]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Fetch the videos and labels:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取视频和标签：
- en: '[PRE50]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Fetch a random video and read its frames:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个随机视频并读取其帧：
- en: '[PRE51]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Load the I3D from TFHub:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 TFHub 加载 I3D：
- en: '[PRE52]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Finally, pass the video through the network to obtain the predictions, and
    then save the video as a GIF:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将视频传递给网络以获得预测结果，然后将视频保存为 GIF 格式：
- en: '[PRE53]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here''s the first frame of the random video I obtained:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我获得的随机视频的第一帧：
- en: '![Figure 10.4 – Frame of the random UCF101 video'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – 随机 UCF101 视频的帧'
- en: '](img/B14768_10_004.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_004.jpg)'
- en: Figure 10.4 – Frame of the random UCF101 video
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 随机 UCF101 视频的帧
- en: 'And here are the top five predictions produced by the model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型生成的前五个预测：
- en: '[PRE54]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: It appears that the network understands that the action portrayed in the video
    has to do with the floor, because four out of five predictions have to do with
    it. However, `mopping floor` is the correct one.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来网络理解视频中呈现的动作与地板有关，因为五个预测中有四个与此相关。然而，`mopping floor`才是正确的预测。
- en: Let's now move to the *How it works…* section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进入 *它是如何工作的……* 部分。
- en: How it works…
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we leveraged the power of a 3D convolutional network to recognize
    actions in videos. A 3D convolution, as the name suggests, is a natural extension
    of a bi-dimensional convolution, which moves in two directions. Naturally, 3D
    convolutions consider width and height, but also depth, making them the perfect
    fit for special kinds of images, such as Magnetic Resonance Imaging (MRI) or,
    in this case, videos, which are just a series of images stacked together.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们利用了 3D 卷积网络的强大功能来识别视频中的动作。顾名思义，3D 卷积是二维卷积的自然扩展，它可以在两个方向上进行操作。3D 卷积不仅考虑了宽度和高度，还考虑了深度，因此它非常适合某些特殊类型的图像，如磁共振成像（MRI），或者在本例中是视频，视频实际上就是一系列叠加在一起的图像。
- en: We started by fetching a series of videos from the `UCF101` dataset and a set
    of action labels from the `Kinetics` dataset. It's important to remember that
    the I3D we downloaded from TFHub was trained on Kinetics. Therefore, the videos
    we passed to it are unseen.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从 `UCF101` 数据集中获取了一系列视频，并从 `Kinetics` 数据集中获取了一组动作标签。需要记住的是，我们从 TFHub 下载的
    I3D 是在 Kinetics 数据集上训练的。因此，我们传递给它的视频是未见过的。
- en: Next, we implemented a series of helper functions to obtain, preprocess, and
    shape each input video in the way the I3D expects. Then, we loaded the aforementioned
    network from TFHub and used it to display the top five actions it recognized in
    the video.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现了一系列辅助函数，用于获取、预处理并调整每个输入视频的格式，以符合 I3D 的预期。然后，我们从 TFHub 加载了上述网络，并用它来显示视频中识别到的前五个动作。
- en: One interesting extension you can make to this solution is to read custom videos
    from your filesystem, or better yet, pass a stream of images from your webcam
    to the network in order to see how well it performs!
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对这个解决方案进行一个有趣的扩展，即从文件系统中读取自定义视频，或者更好的是，将来自摄像头的图像流传递给网络，看看它的表现如何！
- en: See also
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'I3D is a groundbreaking architecture for video processing, so I highly recommend
    you read the original paper here: [https://arxiv.org/abs/1705.07750](https://arxiv.org/abs/1705.07750).
    Here''s a pretty interesting article that explains the difference between 1D,
    2D, and 3D convolutions: [https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610](https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610).
    You can learn more about the `UCF101` dataset here: https://www.crcv.ucf.edu/data/UCF101.php.
    If you''re interested in the `Kinetics` dataset, access this link: https://deepmind.com/research/open-source/kinetics.
    Lastly, you can find more details about the I3D implementation we used here: [https://tfhub.dev/deepmind/i3d-kinetics-400/1](https://tfhub.dev/deepmind/i3d-kinetics-400/1).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: I3D 是一种用于视频处理的突破性架构，因此我强烈建议你阅读原始论文：[https://arxiv.org/abs/1705.07750](https://arxiv.org/abs/1705.07750)。这里有一篇相当有趣的文章，解释了
    1D、2D 和 3D 卷积的区别：[https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610](https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610)。你可以在这里了解更多关于`UCF101`数据集的信息：https://www.crcv.ucf.edu/data/UCF101.php。如果你对`Kinetics`数据集感兴趣，可以访问这个链接：https://deepmind.com/research/open-source/kinetics。最后，你可以在这里找到我们使用的
    I3D 实现的更多细节：[https://tfhub.dev/deepmind/i3d-kinetics-400/1](https://tfhub.dev/deepmind/i3d-kinetics-400/1)。
- en: Generating the middle frames of a video with TensorFlow Hub
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Hub 生成视频的中间帧
- en: Another interesting application of deep learning to videos involves frame generation.
    A fun and practical example of this technique is slow motion, where a network
    decides, based on the context, how to create intervening frames, thus expanding
    the length of a video and creating the illusion it was recorded with a high-speed
    camera (if you want to read more about it, refer to the *See also…* section).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在视频中的另一个有趣应用涉及帧生成。这个技术的一个有趣且实用的例子是慢动作，其中一个网络根据上下文决定如何创建插入帧，从而扩展视频长度，并制造出用高速摄像机拍摄的假象（如果你想了解更多内容，可以参考*另见…*部分）。
- en: In this recipe, we'll use a 3D convolutional network to produce the middle frames
    of a video, given only its first and last frames.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用 3D 卷积网络来生成视频的中间帧，给定视频的第一帧和最后一帧。
- en: For this purpose, we'll rely on TFHub.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将依赖 TFHub。
- en: Let's start this recipe.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始这个食谱。
- en: Getting ready
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We must install TFHub and `TensorFlow Datasets`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须安装 TFHub 和 `TensorFlow Datasets`：
- en: '[PRE55]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The model we''ll use was trained on the `BAIR Robot Pushing Videos` dataset,
    which is available in `TensorFlow Datasets`. However, if we access it through
    the library, we''ll download way more data than we need for the purposes of this
    recipe. Instead, we''ll use a smaller subset of the test set. Execute the following
    command to download it and place it inside the `~/.keras/datasets/bair_robot_pushing`
    folder:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的模型是在 `BAIR Robot Pushing Videos` 数据集上训练的，该数据集可在 `TensorFlow Datasets`
    中获得。然而，如果我们通过库访问它，我们将下载远超过我们这个食谱所需的数据。因此，我们将使用测试集的一个较小子集。执行以下命令来下载它并将其放入 `~/.keras/datasets/bair_robot_pushing`
    文件夹中：
- en: '[PRE56]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now we're all set! Let's begin implementing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪！让我们开始实施。
- en: How to do it…
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Perform the following steps to learn how to generate middle frames using **Direct
    3D Convolutions**, through a model hosted in TFHub:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，学习如何通过托管在 TFHub 中的模型生成中间帧，使用 **直接 3D 卷积**：
- en: 'Import the dependencies:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入依赖库：
- en: '[PRE57]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Define the `plot_first_and_last_for_sample()` function, which creates a plot
    of the first and last frames of a sample of four videos:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `plot_first_and_last_for_sample()` 函数，该函数绘制四个视频样本的第一帧和最后一帧的图像：
- en: '[PRE58]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Define the `plot_generated_frames_for_sample()` function, which graphs the
    middle frames generated for a sample of four videos:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `plot_generated_frames_for_sample()` 函数，该函数绘制为四个视频样本生成的中间帧：
- en: '[PRE59]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We need to patch the `BarRobotPushingSmall()` (see *Step 6*) dataset builder
    to only expect the test split to be available, instead of both the training and
    test ones. Therefore, we must create a custom `SplitGenerator()`:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要修补 `BarRobotPushingSmall()`（参见*步骤 6*）数据集构建器，只期望测试集可用，而不是同时包含训练集和测试集。因此，我们必须创建一个自定义的
    `SplitGenerator()`：
- en: '[PRE60]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Define the path to the data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据路径：
- en: '[PRE61]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Create a `BarRobotPushingSmall()` builder, pass it the custom split generator
    created in *Step 4*, and then prepare the dataset:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `BarRobotPushingSmall()` 构建器，将其传递给*步骤 4*中创建的自定义拆分生成器，然后准备数据集：
- en: '[PRE62]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Get the first batch of videos:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取第一批视频：
- en: '[PRE63]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Keep only the first and last frame of each video in the batch:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保留每个视频批次中的第一帧和最后一帧：
- en: '[PRE64]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the generator model from TFHub:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 TFHub 加载生成器模型：
- en: '[PRE65]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Pass the batch of videos through the model to generate the middle frames:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将视频批次传递到模型中，生成中间帧：
- en: '[PRE66]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Concatenate the first and last frames of each video in the batch with the corresponding
    middle frames produced by the network in *Step 10*:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个视频批次的首尾帧与网络在*步骤 10*中生成的相应中间帧进行连接：
- en: '[PRE67]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, plot the first and last frames, and also the middle frames:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制首尾帧，以及中间帧：
- en: '[PRE68]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'In *Figure 10.5*, we can observe the first and last frame of each video in
    our sample of four:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*图 10.5*中，我们可以观察到我们四个示例视频中每个视频的首尾帧：
- en: '![Figure 10.5 – First and last frame of each video in the sample'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – 每个视频的首尾帧'
- en: '](img/B14768_10_005.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_005.jpg)'
- en: Figure 10.5 – First and last frame of each video in the sample
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 每个视频的首尾帧
- en: 'In *Figure 10.6*, we observe the 14 middle frames generated by the model for
    each video. Close inspection reveals they are coherent with the first and last
    real frames passed to the network:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 10.6*中，我们观察到模型为每个视频生成的 14 帧中间帧。仔细检查可以发现，它们与传递给网络的首尾真实帧是一致的：
- en: '![Figure 10.6 – Middle frames produced by the model for each sample video'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.6 – 模型为每个示例视频生成的中间帧'
- en: '](img/B14768_10_006.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_006.jpg)'
- en: Figure 10.6 – Middle frames produced by the model for each sample video
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 模型为每个示例视频生成的中间帧
- en: Let's go to the *How it works…* section to review what we did.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入*它是如何工作的…*部分，回顾我们所做的工作。
- en: How it works…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we learned about another useful and interesting application
    of deep learning to videos, particularly 3D convolutional networks, in the context
    of generative models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了深度学习在视频中的另一个有趣且有用的应用，特别是在生成模型的背景下，3D 卷积网络的应用。
- en: We took a state-of-the-art architecture trained on the `BAIR Robot Pushing Videos`
    dataset, hosted in TFHub, and used it to produce an entirely new video sequence,
    taking only as seeds the first and last frames of a video.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个在 `BAIR Robot Pushing Videos` 数据集上训练的最先进架构，该数据集托管在 TFHub 上，并用它生成了一个全新的视频序列，仅以视频的首尾帧作为种子。
- en: Because downloading the entire 30 GBs of the `BAIR` dataset would have been
    an overkill, given we only needed a way smaller subset to test our solution, we
    couldn't rely directly on the TensorFlow dataset's `load()` method. Instead, we
    downloaded a subset of the test videos and made the necessary adjustments to the
    `BairRobotPushingSmall()` builder to load and prepare the sample videos.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于下载整个 30 GB 的 `BAIR` 数据集会显得过于冗余，考虑到我们只需要一个小得多的子集来测试我们的解决方案，我们无法直接依赖 TensorFlow
    数据集的 `load()` 方法。因此，我们下载了测试视频的一个子集，并对 `BairRobotPushingSmall()` 构建器进行了必要的调整，以加载和准备示例视频。
- en: It must be mentioned that this model was trained on a very specific dataset,
    but it certainly showcases the powerful generation capabilities of this architecture.
    I encourage you to check out the *See also* section for a list of useful resources
    that could be of help if you want to implement a video generation network on your
    own data.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 必须提到的是，这个模型是在一个非常特定的数据集上训练的，但它确实展示了这个架构强大的生成能力。我鼓励你查看*另见*部分，其中列出了如果你想在自己的数据上实现视频生成网络时可能有帮助的有用资源。
- en: See also
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can learn more about the `BAIR Robot Pushing Videos` dataset here: [https://arxiv.org/abs/1710.05268](https://arxiv.org/abs/1710.05268).
    I encourage you to read the paper entitled **Video Inbetweening Using Direct 3D
    Convolutions**, where the network we used in this recipe was proposed: https://arxiv.org/abs/1905.10240\.
    You can find the TFHub model we relied on at the following link: https://tfhub.dev/google/tweening_conv3d_bair/1\.
    Lastly, here''s an interesting read about an AI that transforms regular footage
    into slow motion: [https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/](https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此了解更多关于 `BAIR Robot Pushing Videos` 数据集的信息：[https://arxiv.org/abs/1710.05268](https://arxiv.org/abs/1710.05268)。我鼓励你阅读题为
    **视频中间插帧使用直接 3D 卷积** 的论文，这篇论文提出了我们在本节中使用的网络：https://arxiv.org/abs/1905.10240\.
    你可以在以下链接找到我们依赖的 TFHub 模型：https://tfhub.dev/google/tweening_conv3d_bair/1\. 最后，以下是关于将普通视频转化为慢动作的
    AI 的一篇有趣文章：[https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/](https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/).
- en: Performing text-to-video retrieval with TensorFlow Hub
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Hub 进行文本到视频检索
- en: The applications of deep learning to videos are not limited to classification,
    categorization, or even generation. One of the biggest resources of neural networks
    is their internal representation of data features. The better a network is at
    a given task, the better their internal mathematical model is. We can take advantage
    of the inner workings of state-of-the-art models to build interesting applications
    on top of them.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在视频中的应用不仅限于分类、分类或生成。神经网络的最大资源之一是它们对数据特征的内部表示。一个网络在某一任务上越优秀，它们的内部数学模型就越好。我们可以利用最先进模型的内部工作原理，构建有趣的应用。
- en: In this recipe, we'll create a small search engine based on the embeddings produced
    by an **S3D** model, trained and ready to be used, which lives in TFHub.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将基于由**S3D**模型生成的嵌入创建一个小型搜索引擎，该模型已经在TFHub上训练并准备好使用。
- en: Are you ready? Let's begin!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？让我们开始吧！
- en: Getting ready
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'First, we must install `OpenCV` and TFHub, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须安装`OpenCV`和TFHub，方法如下：
- en: '[PRE69]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: That's all we need, so let's start this recipe!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要的，开始这个步骤吧！
- en: How to do it…
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'Perform the following steps to learn how to perform text-to-video retrieval
    using TFHub:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，学习如何使用TFHub进行文本到视频的检索：
- en: 'The first step is to import all the dependencies that we''ll use:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是导入我们将使用的所有依赖项：
- en: '[PRE70]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Define a function to produce the text and video embeddings using an instance
    of S3D:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，使用S3D实例生成文本和视频嵌入：
- en: '[PRE71]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Define the `crop_center()` function, which takes an image and crops a squared
    selection corresponding to the center of the received frame:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`crop_center()`函数，该函数接收一张图像并裁剪出与接收到的帧中心相对应的正方形区域：
- en: '[PRE72]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define the `fetch_and_read_video()` function, which, as its name indicates,
    downloads a video and then reads it. For this last part, we use OpenCV. Let''s
    start by getting the video from a given URL:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`fetch_and_read_video()`函数，顾名思义，该函数下载视频并读取它。在最后一步，我们使用OpenCV。首先，从给定的URL获取视频：
- en: '[PRE73]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We extract the video format from the URL. Then, we save the video in the current
    folder, with a random UUID as its name.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从URL中提取视频格式。然后，我们将视频保存在当前文件夹中，文件名为一个随机生成的UUID。
- en: 'Next, we''ll load `max_frames` of this fetched video:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载这个获取的视频的`max_frames`：
- en: '[PRE74]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'If the video doesn''t have enough frames, we''ll repeat the process until we
    reach the desired capacity:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果视频的帧数不足，我们将重复此过程，直到达到所需的容量：
- en: '[PRE75]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Return the normalized frames:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回归一化后的帧：
- en: '[PRE76]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the URLs of the videos:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义视频的URL：
- en: '[PRE77]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Fetch and read each video:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并读取每个视频：
- en: '[PRE78]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Define the queries (captions) associated with each video. Notice that they
    must be in the correct order:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义与每个视频相关联的查询（标题）。请注意，它们必须按正确的顺序排列：
- en: '[PRE79]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Load S3D from TFHub:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从TFHub加载S3D：
- en: '[PRE80]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Obtain the text and video embeddings:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取文本和视频嵌入：
- en: '[PRE81]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Compute the similarity scores between the text and video embeddings:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算文本和视频嵌入之间的相似度得分：
- en: '[PRE82]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Take the first frame of each video, rescale it back to [0, 255], and then convert
    it to BGR space so that we can display it with OpenCV. We do this to display the
    results of our experiment:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个视频的第一帧，将其重新缩放回[0, 255]，然后转换为BGR空间，以便我们可以使用OpenCV显示它。我们这样做是为了展示实验结果：
- en: '[PRE83]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Iterate over each (query, video, score) triplet and display the most similar
    videos for each query:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历每个（查询，视频，得分）三元组，并显示每个查询的最相似视频：
- en: '[PRE84]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'First, we''ll see the result of the *beach* query:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们来看一下*海滩*查询的结果：
- en: '![Figure 10.7 – Ranked results for the BEACH query'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.7 – 针对“海滩”查询的排名结果'
- en: '](img/B14768_10_007.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_007.jpg)'
- en: Figure 10.7 – Ranked results for the BEACH query
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 针对“海滩”查询的排名结果
- en: 'As expected, the first result, which is the highest score, is an image of a
    beach. Let''s now try with *playing drums*:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，第一个结果（得分最高）是一张海滩的图片。现在，让我们试试*打鼓*：
- en: '![Figure 10.8 – Ranked results for the PLAYING DRUMS query'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.8 – 针对“打鼓”查询的排名结果'
- en: '](img/B14768_10_008.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_008.jpg)'
- en: Figure 10.8 – Ranked results for the PLAYING DRUMS query
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 针对“打鼓”查询的排名结果
- en: 'Awesome! It seems that the similarity between the query text and the images
    is stronger in this instance. Up next, a more difficult one:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！看来这个实例中查询文本和图像之间的相似度更强。接下来是一个更具挑战性的查询：
- en: '![Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.9 – 针对“飞机起飞”查询的排名结果'
- en: '](img/B14768_10_009.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_009.jpg)'
- en: Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 针对“飞机起飞”查询的排名结果
- en: 'Although *airplane taking off* is a somewhat more complex query, our solution
    had no problem producing the correct results. Let''s now try with *biking*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*飞机起飞*是一个稍微复杂一点的查询，但我们的解决方案毫无问题地产生了正确的结果。现在让我们试试*biking*：
- en: '![Figure 10.10 – Ranked results for the BIKING query'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.10 – BIKING 查询的排名结果'
- en: '](img/B14768_10_010.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_010.jpg)'
- en: Figure 10.10 – Ranked results for the BIKING query
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – BIKING 查询的排名结果
- en: Another match! How about *dog catching frisbee*?
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 又一个匹配！那*狗抓飞盘*呢？
- en: '![Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.11 – DOG CATCHING FRISBEE 查询的排名结果'
- en: '](img/B14768_10_011.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_10_011.jpg)'
- en: Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – DOG CATCHING FRISBEE 查询的排名结果
- en: No problem at all! The satisfying results we've seen are due to the great job
    S3D does at mapping images with the words that best describe them. If you have
    read the paper where S3D was introduced, you won't be surprised by this fact,
    given the humongous amount of data it was trained on.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一点问题都没有！我们所见到的令人满意的结果，归功于S3D在将图像与最能描述它们的文字进行匹配方面所做的出色工作。如果你已经阅读了介绍S3D的论文，你不会对这一事实感到惊讶，因为它是在大量数据上进行训练的。
- en: Let's now proceed to the next section.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续下一部分。
- en: How it works…
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: In this recipe, we exploited the ability of the S3D model to generate embeddings,
    both for text and video, to create a small database we used as the basis of a
    toy search engine. This way, we demonstrated the usefulness of having a network
    capable of producing richly informative vectorial two-way mappings between images
    and text.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们利用了S3D模型生成嵌入的能力，既针对文本也针对视频，创建了一个小型数据库，并将其用作一个玩具搜索引擎的基础。通过这种方式，我们展示了拥有一个能够在图像和文本之间生成丰富的信息向量双向映射的网络的实用性。
- en: See also
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'I highly recommend that you read the paper where the model we used in this
    recipe was published as it''s very interesting! Here''s the link: https://arxiv.org/pdf/1912.06430.pdf.
    Speaking of the model, you''ll find it here: https://tfhub.dev/deepmind/mil-nce/s3d/1.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐你阅读我们在这个方法中使用的模型所发表的论文，内容非常有趣！这是链接：https://arxiv.org/pdf/1912.06430.pdf。说到这个模型，你可以在这里找到它：https://tfhub.dev/deepmind/mil-nce/s3d/1。
