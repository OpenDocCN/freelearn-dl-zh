- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: MLOps and LLMOps
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps和LLMOps
- en: Throughout the book, we’ve already used **machine learning operations** (**MLOps**)
    components and principles such as a model registry to share and version our fined-tuned
    **large language models** (**LLMs**), a logical feature store for our fine-tuning
    and RAG data, and an orchestrator to glue all our ML pipelines together. But MLOps
    is not just about these components; it takes an ML application to the next level
    by automating data collection, training, testing, and deployment. Thus, the end
    goal of MLOps is to automate as much as possible and let users focus on the most
    critical decisions, such as when a change in distribution is detected and a decision
    must be taken on whether it is essential to retrain the model or not. But what
    about **LLM operations** (**LLMOps**)? How does it differ from MLOps?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们已经使用了**机器学习操作**（**MLOps**）组件和原则，例如模型注册来共享和版本控制我们的微调**大型语言模型**（**LLMs**），用于微调和RAG数据的逻辑特征存储，以及一个编排器来将所有我们的机器学习管道粘合在一起。但MLOps不仅仅是这些组件；它通过自动化数据收集、训练、测试和部署，将机器学习应用提升到下一个层次。因此，MLOps的最终目标是尽可能自动化，让用户专注于最关键的决策，例如当检测到分布变化时，必须决定是否需要重新训练模型。但关于**LLM操作**（**LLMOps**）呢？它与MLOps有何不同？
- en: The term *LLMOps* is a product of the widespread adoption of LLMs. It is built
    on top of MLOps, which is built on top of **development operations** (**DevOps**).
    Thus, to fully understand what LLMOps is about, we must provide a historical context,
    starting with DevOps and building on the term from there—which is precisely what
    this chapter will do. At its core, LLMOps focuses on problems specific to LLMs,
    such as prompt monitoring and versioning, input and output guardrails to prevent
    toxic behavior, and feedback loops to gather fine-tuning data. It also focuses
    on scaling issues that appear when working with LLMs, such as collecting trillions
    of tokens for training datasets, training models on massive GPU clusters, and
    reducing infrastructure costs. Fortunately for the common folk, these issues are
    solved mainly by a few companies that fine-tune foundational models, such as Meta,
    which provides the Llama family of models. Most companies will adopt these pre-trained
    foundational models for their use cases, focusing on LLMOps problems such as prompt
    monitoring and versioning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*LLMOps*是LLMs广泛采用的结果。它是建立在MLOps之上的，而MLOps又是建立在**开发运维**（**DevOps**）之上的。因此，为了全面理解LLMOps是什么，我们必须提供一个历史背景，从DevOps开始，然后在此基础上构建术语——这正是本章将要做的。在本质上，LLMOps专注于LLMs特有的问题，例如提示监控和版本控制，输入和输出护栏以防止有害行为，以及反馈循环以收集微调数据。它还关注与LLMs一起工作时出现的问题，例如收集用于训练数据集的数十亿个标记，在巨大的GPU集群上训练模型，以及降低基础设施成本。幸运的是，对于普通人来说，这些问题主要是由少数几家微调基础模型的公司解决的，例如Meta，它提供了Llama系列模型。大多数公司将采用这些预训练的基础模型来满足他们的用例，专注于LLMOps问题，如提示监控和版本控制。
- en: On the implementation side of things, to add LLMOps to our LLM Twin use case,
    we will deploy all our ZenML pipelines to AWS. We will implement a **continuous
    integration and continuous deployment** (**CI/CD**) pipeline to test the integrity
    of our code and automate the deployment process, a **continuous training** (**CT**)
    pipeline to automate our training, and a monitoring pipeline to track all our
    prompts and generated answers. This is a natural progression in any ML project,
    regardless of whether you use LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施方面，为了将LLMOps添加到我们的LLM Twin用例中，我们将部署所有我们的ZenML管道到AWS。我们将实现一个**持续集成和持续部署**（**CI/CD**）管道来测试代码的完整性并自动化部署过程，一个**持续训练**（**CT**）管道来自动化我们的训练，以及一个监控管道来跟踪所有我们的提示和生成的答案。这是任何机器学习项目的自然进展，无论你是否使用LLMs。
- en: In previous chapters, you learned how to build an LLM application. Now, it’s
    time to explore three main goals related to LLMOps. The first one is to gain a
    theoretical understanding of LLMOps, starting with DevOps, then moving to the
    fundamental principles of MLOps, and finally, digging into LLMOps. We don’t aim
    to provide the whole theory on DevOps, MLOps, and LLMOps, as you could easily
    write an entire book on these topics. However, we want to build a strong understanding
    of why we make certain decisions when implementing the LLM Twin use case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了如何构建LLM应用。现在，是时候探索与LLMOps相关的三个主要目标了。第一个目标是对LLMOps有一个理论上的理解，从DevOps开始，然后转向MLOps的基本原则，最后深入LLMOps。我们并不旨在提供DevOps、MLOps和LLMOps的完整理论，因为你可以轻易地写一本书来涵盖这些主题。然而，我们希望对在实现LLM
    Twin用例时我们为何做出某些决策有一个深入的理解。
- en: Our second goal is to deploy the ZenML pipelines to AWS (currently, we’ve deployed
    only our inference pipeline to AWS in *Chapter 10*). This section will be hands-on,
    showing you how to leverage ZenML to deploy everything to AWS. We need this to
    implement our third and last goal, which is to apply what we’ve learned in the
    theory section to our LLM Twin use case. We will implement a CI/CD pipeline using
    GitHub Actions, a CT and alerting pipeline using ZenML, and a monitoring pipeline
    using Opik from Comet ML.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个目标是部署 ZenML 管道到 AWS（目前，我们在第 10 章中只部署了我们的推理管道到 AWS）。本节将进行实战，向您展示如何利用 ZenML
    将一切部署到 AWS。我们需要这样做来实现我们的第三个也是最后一个目标，即将我们在理论部分学到的知识应用到我们的 LLM Twin 用例中。我们将使用 GitHub
    Actions 实现一个 CI/CD 管道，使用 ZenML 实现一个 CT 和警报管道，以及使用来自 Comet ML 的 Opik 实现一个监控管道。
- en: 'Thus, in this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将涵盖以下主题：
- en: 'The path to LLMOps: Understanding its roots in DevOps and MLOps'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通向 LLMOps 的路径：理解其在 DevOps 和 MLOps 中的根源
- en: Deploying the LLM Twin’s pipelines to the cloud
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LLM Twin 的管道部署到云端
- en: Adding LLMOps to the LLM Twin
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 LLMOps 添加到 LLM Twin
- en: 'The path to LLMOps: Understanding its roots in DevOps and MLOps'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通向 LLMOps 的路径：理解其在 DevOps 和 MLOps 中的根源
- en: To understand LLMOps, we have to start with the field’s beginning, which is
    DevOps, as it inherits most of its fundamental principles from there. Then, we
    will move to MLOps to understand how the DevOps domain was adapted to support
    ML systems. Finally, we will explain what LLMOps is and how it emerged from MLOps
    after the widespread adoption of LLMs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 LLMOps，我们必须从该领域的起点开始，即 DevOps，因为它从那里继承了大部分的基本原则。然后，我们将转向 MLOps，以了解 DevOps
    领域是如何适应以支持 ML 系统的。最后，我们将解释 LLMOps 是什么，以及它是如何在 LLMs 得到广泛应用后从 MLOps 中产生的。
- en: DevOps
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DevOps
- en: Manually shipping software is time-consuming, error-prone, involves security
    risks, and doesn’t scale. Thus, DevOps was born to automate the process of shipping
    software at scale. More specifically, DevOps is used in software development,
    where you want to completely automate your building, testing, deploying, and monitoring
    components. It is a methodology designed to shorten the development lifecycle
    and ensure continuous delivery of high-quality software. It encourages collaboration,
    automates processes, integrates workflows, and implements rapid feedback loops.
    These elements contribute to a culture where building, testing, and releasing
    software becomes more reliable and faster.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 手动发布软件既耗时又容易出错，涉及安全风险，且无法扩展。因此，DevOps 应运而生，以实现大规模软件发布过程的自动化。更具体地说，DevOps 用于软件开发，你希望完全自动化你的构建、测试、部署和监控组件。它是一种旨在缩短开发生命周期并确保持续交付高质量软件的方法。它鼓励协作、自动化流程、集成工作流程并实施快速反馈循环。这些元素共同促成了一个文化，其中构建、测试和发布软件变得更加可靠和快速。
- en: 'Embracing a DevOps culture offers significant advantages to an organization,
    primarily boosting operational efficiency, speeding up feature delivery, and enhancing
    product quality. Some of the main benefits include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 接受 DevOps 文化为组织带来了显著的优势，主要是提高运营效率、加快功能交付和提升产品质量。其中一些主要好处包括：
- en: '**Improved collaboration:** DevOps is pivotal in creating a more unified working
    environment. Eliminating the barriers between development and operations teams
    fosters enhanced communication and teamwork, leading to a more efficient and productive
    workplace.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进协作：** DevOps 在创建更统一的工作环境中起着关键作用。消除开发和运维团队之间的障碍，促进了更好的沟通和团队合作，从而带来更高效和富有成效的工作场所。'
- en: '**Boosted efficiency:** Automating the software development lifecycle reduces
    manual tasks, errors, and delivery times.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高效率：** 自动化软件开发生命周期减少了手动任务、错误和交付时间。'
- en: '**Ongoing improvement:** DevOps is not just about internal processes. It’s
    about ensuring that the software effectively meets user needs. Promoting a culture
    of continuous feedback enables teams to quickly adapt and enhance their processes,
    thereby delivering software that genuinely satisfies the end users.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续改进：** DevOps 不仅仅是关于内部流程。它关乎确保软件有效满足用户需求。推广持续反馈的文化使团队能够快速适应并改进他们的流程，从而交付真正满足最终用户的软件。'
- en: '**Superior quality and security:** DevOps ensures swift software development
    while maintaining high quality and security standards through CI/CD and proactive
    security measures.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卓越的质量和安全：** DevOps 通过 CI/CD 和主动安全措施确保快速软件开发的同时，保持高质量和安全标准。'
- en: The DevOps lifecycle
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DevOps 生命周期
- en: 'As illustrated in *Figure 11.1*, the DevOps lifecycle encompasses the entire
    journey from the inception of software development to its delivery, upkeep, and
    security. The key stages of this lifecycle are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.1*所示，DevOps生命周期涵盖了从软件开发 inception 到交付、维护和安全的整个旅程。这个生命周期的关键阶段包括：
- en: '**Plan:** Organize and prioritize the tasks, ensuring each is tracked to completion.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计划：** 组织并优先处理任务，确保每个任务都能被追踪至完成。'
- en: '**Code:** Collaborate with your team to write, design, develop, and securely
    manage code and project data.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码：** 与您的团队协作，编写、设计、开发和安全地管理代码和项目数据。'
- en: '**Build:** Package your applications and dependencies into an executable format.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**构建：** 将您的应用程序和依赖项打包成可执行格式。'
- en: '**Test:** This stage is crucial. It’s where you confirm that your code functions
    correctly and meets quality standards, ideally through automated testing.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试：** 这个阶段至关重要。这是您确认代码功能正确并符合质量标准的地方，理想情况下是通过自动化测试。'
- en: '**Release:** If the tests pass, flag the tested build as a new release, which
    is now ready to be shipped.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**发布：** 如果测试通过，将测试过的构建标记为新版本，现在它已准备好发货。'
- en: '**Deploy:** Deploy the latest release to the end users.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署：** 将最新版本部署给最终用户。'
- en: '**Operate**: Manage and maintain the infrastructure on which the software runs
    effectively once it is live. This involves scaling, security, data management,
    and backup and recovery.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**运营：** 在软件上线后，管理和维护软件运行的基础设施。这包括扩展、安全性、数据管理以及备份和恢复。'
- en: '**Monitor:** Track performance metrics and errors to reduce the severity and
    frequency of incidents.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控：** 跟踪性能指标和错误，以减少事件发生的严重性和频率。'
- en: '![A blue and green arrows with text  Description automatically generated](img/B31105_11_01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![蓝色和绿色箭头带文字 描述自动生成](img/B31105_11_01.png)'
- en: 'Figure 11.1: DevOps lifecycle steps'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：DevOps生命周期步骤
- en: The core DevOps concepts
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核心DevOps概念
- en: 'DevOps encompasses various practices throughout the application lifecycle,
    but the core ones that we will touch on throughout this book are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps涵盖了整个应用程序生命周期中的各种实践，但本书中我们将涉及的核心实践包括：
- en: '**Deployment environments**: To thoroughly test your code before shipping it
    to production, you must define multiple pre-production environments that mimic
    the production one. The most common approach is to create a dev environment where
    the developers can test their latest features. Then, you have a staging environment
    where the QA team and stakeholders tinker with the application to find bugs and
    experience the latest features before they ship to the users. Lastly, we have
    the production environment, which is exposed to end users.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署环境：** 在将代码部署到生产环境之前，您必须定义多个预生产环境，以模拟生产环境。最常见的方法是创建一个开发环境，让开发者可以测试他们的最新功能。然后，您有一个预发布环境，其中
    QA 团队和利益相关者可以检查应用程序以发现错误，并在它们向用户发布之前体验最新功能。最后，我们有生产环境，它面向最终用户。'
- en: '**Version control:** Used to track, manage, and version every change made to
    the source code. This allows you to have complete control over the evolution of
    the code and deployment processes. For example, without versioning, tracking changes
    between the dev, staging, and production environments would be impossible. By
    versioning your software, you always know what version is stable and ready to
    be shipped.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制：** 用于跟踪、管理和版本控制对源代码所做的每个更改。这允许您完全控制代码和部署过程的演变。例如，如果没有版本控制，跟踪开发、预发布和生产环境之间的更改将是不可能的。通过对您的软件进行版本控制，您始终知道哪个版本是稳定的，并准备好发货。'
- en: '**Continuous integration (CI):** Before pushing the code into the dev, staging,
    and production main branches, you automatically build your application and run
    automated tests on each change. After all the automated tests pass, the feature
    branch can be merged into the main one.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成（CI）：** 在将代码推送到开发、预发布和生产主分支之前，您会自动构建应用程序并对每个更改运行自动化测试。在所有自动化测试通过后，功能分支可以被合并到主分支。'
- en: '**Continuous delivery (CD):** Continuous delivery works in conjunction with
    CI and automates the infrastructure provisioning and application deployment steps.
    For example, after the code is merged into the staging environment, the application
    with the latest changes will be automatically deployed on top of your staging
    infrastructure. After, the QA team (or stakeholders) starts manually testing the
    latest features to verify that they work as expected. These two steps are commonly
    referred to together as CI/CD.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续交付（CD）：** 持续交付与CI协同工作，自动化基础设施供应和应用程序部署步骤。例如，代码合并到预发布环境后，带有最新更改的应用程序将自动部署到您的预发布基础设施上。之后，QA团队（或利益相关者）开始手动测试最新功能，以验证它们是否按预期工作。这两个步骤通常一起被称为CI/CD。'
- en: Note that DevOps suggests a set of core principles that are platform/tool agnostic.
    However, within our LLM Twin use case, we will add a version control layer using
    GitHub, which aims to track the evolution of the code. Another popular tool for
    version control is GitLab. To implement the CI/CD pipeline, we will leverage the
    GitHub ecosystem and GitHub Actions, which are free for open-source projects.
    Other tool choices are GitLab CI/CD, CircleCI, and Jenkins. Usually, you pick
    the DevOps tool based on your development environment, customization, and privacy
    needs. For example, Jenkins is an open-source DevOps tool you can host yourself
    and control fully. The downside is that you must host and maintain it yourself,
    adding a complexity layer. Thus, many companies choose what works best with their
    version control ecosystem, such as GitHub Actions or GitLab CI/CD.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，DevOps提出了一套核心原则，这些原则与平台/工具无关。然而，在我们的LLM Twin用例中，我们将使用GitHub添加一个版本控制层，旨在跟踪代码的演变。版本控制的另一个流行工具是GitLab。为了实现CI/CD管道，我们将利用GitHub生态系统和GitHub
    Actions，这些对于开源项目是免费的。其他工具选择包括GitLab CI/CD、CircleCI和Jenkins。通常，您会根据您的开发环境、定制和隐私需求选择DevOps工具。例如，Jenkins是一个开源的DevOps工具，您可以自己托管并完全控制。缺点是您必须自己托管和维护它，这增加了一个复杂性层。因此，许多公司选择与他们的版本控制生态系统最匹配的工具，例如GitHub
    Actions或GitLab CI/CD。
- en: Now that we’ve established a solid understanding of DevOps, let’s explore how
    the MLOps field has emerged to keep these same core principles in the AI/ML world.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对DevOps有了坚实的理解，让我们来探讨MLOps领域是如何出现的，以保持这些相同的核心原则在AI/ML世界中的应用。
- en: MLOps
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLOps
- en: As you might have worked out by now, MLOps tries to apply the DevOps principles
    to ML. The core issue is that an ML application has many other moving parts compared
    to a standard software application, such as the data, model, and, finally, the
    code. MLOps aims to track, operationalize, and monitor all these concepts for
    better reproducibility, robustness, and control.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如您现在可能已经推断出的那样，MLOps试图将DevOps原则应用于ML。核心问题是与标准软件应用相比，ML应用有许多其他移动部件，如数据、模型，最后是代码。MLOps旨在跟踪、运营和监控所有这些概念，以实现更好的可重复性、鲁棒性和控制。
- en: In ML systems, a build can be triggered by any change in these areas—whether
    it’s an update in the code, modifications in the data, or adjustments to the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML系统中，构建可以由这些领域的任何变化触发——无论是代码的更新、数据的修改还是模型的调整。
- en: '![](img/B31105_11_02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_02.png)'
- en: 'Figure 11.2: Relationship between data, model, and code changes'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：数据、模型和代码变化之间的关系
- en: 'In DevOps, everything is centered around the code. For example, when a new
    feature is added to the codebase, you have to trigger the CI/CD pipeline. In MLOps,
    the code can remain unchanged while only the data changes. In that case, you must
    train (or fine-tune) a new model, resulting in a new dataset and model version.
    Intuitively, when one component changes, it affects one or more of the others.
    Thus, MLOps has to take into consideration all this extra complexity. Here are
    a few examples that can trigger a change in the data and indirectly in the model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在DevOps中，一切都是以代码为中心的。例如，当向代码库添加新功能时，您必须触发CI/CD管道。在MLOps中，代码可以保持不变，而只有数据发生变化。在这种情况下，您必须训练（或微调）一个新的模型，从而产生一个新的数据集和模型版本。直观地讲，当一个组件发生变化时，它会影响一个或多个其他组件。因此，MLOps必须考虑所有这些额外的复杂性。以下是一些可以触发数据变化并间接影响模型变化的例子：
- en: After deploying the ML model, its performance might decay as time passes, so
    we need new data to retrain it.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署ML模型后，随着时间的推移，其性能可能会下降，因此我们需要新的数据来重新训练它。
- en: After understanding how to collect data in the real world, we might recognize
    that getting the data for our problem is challenging, so we need to re-formulate
    it to work with our real-world setup.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在理解了如何在现实世界中收集数据之后，我们可能会意识到为我们的问题获取数据具有挑战性，因此我们需要重新制定方案以适应我们的现实世界设置。
- en: While in the experimentation stage and training the model, we often must collect
    more data or re-label it, which generates a new set of models.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实验阶段和训练模型时，我们通常必须收集更多数据或重新标记它，这会产生一组新的模型。
- en: After serving the model in the production environment and collecting feedback
    from the end users, we might recognize that the assumptions we made for training
    the model are wrong, so we must change our model.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中部署模型并收集最终用户的反馈后，我们可能会意识到我们为训练模型所做的假设是错误的，因此我们必须更改我们的模型。
- en: So, what is MLOps?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，MLOps是什么？
- en: 'A more official definition of MLOps is the following: MLOps is the extension
    of the DevOps field that makes data and models their first-class citizen while
    preserving the DevOps methodology.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps的一个更正式的定义如下：MLOps是DevOps领域的扩展，它将数据和模型视为一等公民，同时保留DevOps方法论。
- en: Like DevOps, MLOps originates from the idea that isolating ML model development
    from its deployment process (ML operations) diminishes the system’s overall quality,
    transparency, and agility. With that in mind, an optimal MLOps experience treats
    ML assets consistently as other software assets within a CI/CD environment as
    part of a cohesive release process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与DevOps类似，MLOps起源于这样一个想法，即隔离机器学习模型开发与其部署过程（机器学习操作）会降低系统的整体质量、透明度和敏捷性。考虑到这一点，最佳的MLOps体验将机器学习资产始终如一地视为CI/CD环境中的其他软件资产，作为统一发布过程的一部分。
- en: MLOps core components
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLOps核心组件
- en: 'We have already used all of these components throughout the book, but let’s
    have a quick refresher on the MLOps core components now that we better understand
    the field. Along with source control and CI/CD, MLOps revolves around:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在本书中使用了所有这些组件，但现在我们更好地理解了这个领域，让我们快速回顾一下MLOps的核心组件。除了源代码控制和CI/CD之外，MLOps围绕以下内容展开：
- en: '**Model registry:** A centralized repository for storing trained ML models
    (**tools:** **Comet ML**, **W&B**, **MLflow**, **ZenML**)'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型注册库**：一个集中式存储库，用于存储训练好的机器学习模型（**工具**：**Comet ML**，**W&B**，**MLflow**，**ZenML**）'
- en: '**Feature store:** Preprocessing and storing input data as features for both
    model training and inference pipelines (**tools:** **Hopsworks**, **Tecton**,
    **Featureform**)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征存储**：预处理并存储输入数据作为模型训练和推理管道的特征（**工具**：**Hopsworks**，**Tecton**，**Featureform**）'
- en: '**ML metadata store:** This store tracks information related to model training,
    such as model configurations, training data, testing data, and performance metrics.
    It is mainly used to compare multiple models and look at the model lineages to
    understand how they were created (**tools:** **Comet ML**, **W&B**, **MLflow**)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习元数据存储**：此存储跟踪与模型训练相关的信息，例如模型配置、训练数据、测试数据和性能指标。它主要用于比较多个模型并查看模型谱系，以了解它们是如何创建的（**工具**：**Comet
    ML**，**W&B**，**MLflow**）'
- en: '**ML pipeline orchestrator:** Automating the sequence of steps in ML projects
    (**tools:** **ZenML**, **Airflow**, **Prefect**, **Dagster**)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习项目编排器**：自动化机器学习项目中的步骤序列（**工具**：**ZenML**，**Airflow**，**Prefect**，**Dagster**）'
- en: You might have noticed an overlap between the MLOps components and its specific
    tooling. This is common, as most MLOps tools offer unified solutions, often called
    MLOps platforms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到MLOps组件与其特定工具之间存在重叠。这是常见的，因为大多数MLOps工具提供统一解决方案，通常称为MLOps平台。
- en: MLOps principles
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLOps原则
- en: Six core principles guide the MLOps field. These are independent of any tool
    and sit at the core of building robust and scalable ML systems.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 六个核心原则指导MLOps领域。这些原则与任何工具无关，并位于构建稳健和可扩展的机器学习系统的核心。
- en: 'They are:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 他们包括：
- en: '**Automation or operationalization**: Automation in MLOps involves transitioning
    from manual processes to automated pipelines through CT and CI/CD. This enables
    the efficient retraining and deployment of ML models in response to triggers such
    as new data, performance drops, or unhandled edge cases. Moving from manual experimentation
    to full automation ensures that our ML systems are robust, scalable, and adaptable
    to changing requirements without errors or delays.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化或操作化**：MLOps中的自动化涉及通过持续集成和持续部署（CI/CD）将手动流程转换为自动化管道。这使得能够高效地根据触发器（如新数据、性能下降或未处理的边缘情况）重新训练和部署机器学习模型。从手动实验到完全自动化确保我们的机器学习系统稳健、可扩展，并能适应不断变化的需求，而不会出现错误或延迟。'
- en: '**Versioning**: In MLOps, it is crucial to track changes in code, models, and
    data individually, ensuring consistency and reproducibility. Code is tracked using
    tools like Git, models are versioned through model registries, and data versioning
    can be managed using solutions like DVC or artifact management systems.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制**：在MLOps中，跟踪代码、模型和数据的单独变化至关重要，以确保一致性和可重复性。代码使用Git等工具进行跟踪，模型通过模型注册表进行版本控制，数据版本控制可以使用DVC或工件管理系统等解决方案进行管理。'
- en: '**Experiment tracking:** As training ML models is an iterative and experimental
    process that involves comparing multiple experiments based on predefined metrics,
    using an experiment tracker to help us pick the best model is important. Tools
    like Comet ML, W&B, MLflow, and Neptune allow us to log all necessary information
    to compare experiments easily and select the best model for production.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验跟踪**：由于训练机器学习模型是一个迭代和实验的过程，涉及根据预定义的指标比较多个实验，因此使用实验跟踪器帮助我们选择最佳模型非常重要。像Comet
    ML、W&B、MLflow和Neptune这样的工具允许我们轻松记录所有必要信息，以便比较实验并选择适合生产的最佳模型。'
- en: '**Testing**: MLOps suggests that along with testing your code, you should also
    test your data and models through unit, integration, acceptance, regression, and
    stress tests. This ensures that each component functions correctly and integrates
    well, focusing on inputs, outputs, and handling edge cases.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试**：MLOps建议，除了测试你的代码外，你还应该通过单元测试、集成测试、验收测试、回归测试和压力测试来测试你的数据和模型。这确保了每个组件都能正确运行并良好集成，重点关注输入、输出和处理边缘情况。'
- en: '**Monitoring**: This stage is vital for detecting performance degradation in
    served ML models due to changes in production data, allowing timely intervention
    such as retraining, further prompt or feature engineering, or data validation.
    By tracking logs, system metrics, and model metrics and detecting drifts, we can
    maintain the health of ML systems in production, detect issues as fast as possible,
    and ensure they continue to deliver accurate results.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：此阶段对于检测由于生产数据变化导致的已部署机器学习模型性能下降至关重要，允许及时干预，例如重新训练、进一步提示或特征工程，或数据验证。通过跟踪日志、系统指标和模型指标并检测漂移，我们可以维护生产中机器学习系统的健康，尽可能快地检测问题，并确保它们继续提供准确的结果。'
- en: '**Reproducibility**: This ensures that every process (such as training or feature
    engineering) within your ML systems produces identical results when given the
    same input by tracking all the moving variables, such as code versions, data versions,
    hyperparameters, or any other type of configurations. Due to the non-deterministic
    nature of ML training and inference, setting well-known seeds when generating
    pseudo-random numbers is essential to achieving consistent outcomes and making
    processes as deterministic as possible.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可重复性**：这确保了当通过跟踪所有移动变量（如代码版本、数据版本、超参数或任何其他类型的配置）给相同输入时，你ML系统中的每个过程（如训练或特征工程）都会产生相同的结果。由于机器学习训练和推理的非确定性，在生成伪随机数时设置已知的种子对于实现一致的结果和尽可能使过程确定至关重要。'
- en: If you want to learn more, we’ve offered an in-depth exploration of these principles
    in the *Appendix* at the end of this book.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多，我们已经在本书末尾的*附录*中对这些原则进行了深入探讨。
- en: ML vs. MLOps engineering
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习与MLOps工程
- en: 'There is a fine line between ML engineering and MLOps. If we want to define
    a rigid job description for the two rules, it cannot be easy to completely differentiate
    what responsibilities go into **ML engineering** (**MLE**) and what goes into
    MLOps. I have seen many job roles that bucket the MLOps role with the platform
    and cloud engineers. From one perspective, that makes a lot of sense: as an MLOps
    engineer, you have a lot of work to do on the infrastructure side. On the other
    hand, as seen in this section, an MLOps engineer still has to implement things
    such as experiment tracking, model registries, versioning, and more. A good strategy
    would be to let the ML engineer integrate these into the code and the MLOps engineer
    focus on making them work on their infrastructure.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程与MLOps之间存在一条细线。如果我们想为这两个规则定义一个严格的职位描述，那么完全区分**机器学习工程（MLE**）和MLOps的责任可能并不容易。我见过许多将MLOps角色与平台和云工程师归为一类的职位。从一个角度来看，这很有道理：作为一名MLOps工程师，你在基础设施方面有很多工作要做。另一方面，正如本节所示，MLOps工程师仍然需要实现实验跟踪、模型注册、版本控制等功能。一个好的策略是让机器学习工程师将这些集成到代码中，而MLOps工程师则专注于在他们的基础设施上使它们工作。
- en: At a big corporation, ultimately, differentiating the two roles might make sense.
    But when working in small to medium-sized teams, you will wear multiple hats and
    probably work on the ML system’s MLE and MLOps aspects.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型企业中，最终区分这两个角色可能是有意义的。但当在小型到中型团队中工作时，你将需要扮演多个角色，并且可能需要在机器学习系统的MLE和MLOps方面工作。
- en: '![](img/B31105_11_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3](img/B31105_11_03.png)'
- en: 'Figure 11.3: DS vs. MLE vs. MLOps'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：数据科学（DS）vs. 机器学习工程师（MLE）vs. MLOps
- en: 'For instance, in *Figure 11.3*, we see a clear division of responsibilities
    among the three key roles: data scientist/ML researcher, ML engineer, and MLOps
    engineer. The **Data Scientist** (**DS**) implements specific models to address
    problems.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在*图11.3*中，我们可以看到三个关键角色：数据科学家/ML研究人员、机器学习工程师和MLOps工程师之间的责任划分非常清晰。**数据科学家（DS**）实施特定模型来解决这些问题。
- en: The ML engineer takes the functional models from the DS team and constructs
    a layer on top of them, making them modular and extendable and providing access
    to a **database** (**DB**) or exposing them as an API over the internet. However,
    the MLOps engineer plays a pivotal role in this process. They take the code from
    this intermediate layer and place it on a more generic layer, the infrastructure.
    This action marks the application’s transition to production. From this point,
    we can start thinking about automation, monitoring, versioning, and more.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师从数据科学团队那里获取功能模型，并在其之上构建一层，使它们模块化且可扩展，并提供访问数据库（**DB**）或通过互联网将其作为API暴露的功能。然而，MLOps工程师在这个过程中扮演着关键角色。他们将这个中间层的代码放置在一个更通用的层，即基础设施层。这一行为标志着应用程序向生产的过渡。从这一点开始，我们可以开始考虑自动化、监控、版本控制和更多内容。
- en: The intermediate layer differentiates a proof of concept from an actual product.
    In that layer, you design an extendable application that has a state by integrating
    a DB and is accessible over the internet through an API. When shipping the application
    on a specific infrastructure, you must consider scalability, latency, and cost-effectiveness.
    Of course, the intermediate and generic layers depend on each other, and often,
    you must reiterate to meet the application requirements.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层区分了原型和实际产品。在那个层中，你通过集成数据库并使其通过API在互联网上可访问来设计一个可扩展的应用程序，并具有状态。当在特定基础设施上部署应用程序时，你必须考虑可扩展性、延迟和成本效益。当然，中间层和通用层相互依赖，并且通常你必须反复迭代以满足应用程序需求。
- en: LLMOps
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMOps
- en: LLMOps encompasses the practices and processes essential for managing and running
    LLMs. This field is a specialized branch of MLOps, concentrating on the unique
    challenges and demands associated with LLMs. While MLOps addresses the principles
    and practices of managing various ML models, LLMOps focuses on the distinct aspects
    of LLMs, including their large size, highly complex training requirements, prompt
    management, and non-deterministic nature of generating answers. However, note
    that at its core, LLMOps still inherits all the fundamentals presented in the
    MLOps section. Thus, here, we will focus on what it adds on top.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps包括管理和运行LLMs所必需的实践和流程。这个领域是MLOps的一个专业分支，专注于与LLMs相关的独特挑战和需求。虽然MLOps解决管理各种ML模型的原则和实践，但LLMOps专注于LLMs的独特方面，包括它们的大规模、高度复杂的训练需求、提示管理以及生成答案的非确定性。然而，请注意，在核心上，LLMOps仍然继承了MLOps部分中提出的所有基本原理。因此，在这里，我们将关注它添加的内容。
- en: 'When training LLMs from scratch, the data and model dimensions of an ML system
    grow substantially, which is one aspect that sets LLMOps apart from MLOps. These
    are the main concerns when training LLMs from scratch:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当从头开始训练LLMs时，ML系统的数据和模型维度会显著增长，这是LLMOps与MLOps区别开来的一个方面。这是从头开始训练LLMs时的主要关注点：
- en: '**Data collection and preparation** involves collecting, preparing, and managing
    the massive datasets required for training LLMs. It involves big data techniques
    for processing, storing, and sharing training datasets. For example, GPT-4 was
    trained on roughly 13 trillion tokens, equal to approximately 10 trillion words.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集和准备**涉及收集、准备和管理训练LLMs所需的庞大数据集。它包括处理、存储和共享训练数据集的大数据技术。例如，GPT-4在大约1300万亿个标记上进行了训练，相当于大约100万亿个单词。'
- en: Managing **LLMs’** **considerable number of parameters** is a significant technical
    challenge from the infrastructure’s point of view. It requires vast computation
    resources, usually clusters of machines powered by Nvidia GPUs with CUDA support.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从基础设施的角度来看，管理**大型语言模型（LLMs）**的大量参数是一个重大的技术挑战。它需要大量的计算资源，通常是由支持CUDA的Nvidia GPU驱动的机器集群。
- en: The massive size of LLMs directly impacts **model training**. When training
    an LLM from scratch, you can’t fit it on a single GPU due to the model’s size
    or the higher batch size you require for the expected results. Thus, you need
    multi-GPU training, which involves optimizing your processes and infrastructure
    to support data, model, or tensor parallelism.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的巨大规模直接影响了**模型训练**。当从头开始训练一个LLM时，由于模型的大小或你需要更高的批量大小以获得预期结果，你无法将其安装在单个GPU上。因此，你需要多GPU训练，这涉及到优化你的流程和基础设施以支持数据、模型或张量并行性。
- en: Managing massive datasets and multi-GPU clusters involves substantial **costs**.
    For example, the estimated training cost for GPT-4 is around $100 million, as
    stated by Sam Altman, the CEO of OpenAI ([https://en.wikipedia.org/wiki/GPT-4#Training](https://en.wikipedia.org/wiki/GPT-4#Training)).
    Add to that the costs of multiple experiments, evaluation, and inference. Even
    if these numbers are not exact, as the sources are not 100% reliable, the scale
    of the costs of training an LLM is trustworthy, which implies that only the large
    players in the industry can afford to train LLMs from scratch.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理大量数据集和多GPU集群涉及大量的**成本**。例如，OpenAI首席执行官山姆·奥特曼表示，GPT-4的估计训练成本约为1亿美元（[https://en.wikipedia.org/wiki/GPT-4#Training](https://en.wikipedia.org/wiki/GPT-4#Training)）。再加上多次实验、评估和推理的成本。即使这些数字并不完全准确，因为来源并非100%可靠，但训练LLM的成本规模是可信的，这意味着只有行业中的大型玩家才能负担得起从头开始训练LLM。
- en: At its core, LLMOps is MLOps at scale. It uses the same MLOps principles but
    is applied to big data and huge models that require more computing power to train
    and run. However, due to its huge scale, the most significant trend is the shift
    away from training neural networks from scratch for specific tasks. This approach
    is becoming obsolete with the rise of fine-tuning, especially with the advent
    of foundation models such as GPT. A few organizations with extensive computational
    resources, such as OpenAI and Google, develop these foundation models. Thus, most
    applications now rely on the lightweight fine-tuning of parts of these models,
    prompt engineering, or optionally distilling data or models into smaller, specialized
    inference networks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，LLMOps是扩展规模的MLOps。它使用相同的MLOps原则，但应用于需要更多计算能力来训练和运行的大数据和巨大模型。然而，由于其巨大的规模，最显著的趋势是远离为特定任务从头开始训练神经网络。随着微调的出现，尤其是随着基础模型如GPT的出现，这种方法正变得过时。只有少数具有大量计算资源的组织，如OpenAI和Google，开发这些基础模型。因此，现在大多数应用现在依赖于这些模型部分的轻量级微调、提示工程，或者可选地将数据或模型蒸馏到更小、更专业的推理网络中。
- en: Thus, for most LLM applications out there, your development steps will involve
    the selection of a foundation model, which you further have to optimize by using
    prompt engineering, fine-tuning, or RAG. Thus, the operational aspect of these
    three steps is the most critical to understand. Let’s dive into some popular components
    of LLMOps that can improve prompt engineering, fine-tuning, and RAG.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于大多数现有的LLM应用，你的开发步骤将包括选择一个基础模型，然后你必须通过使用提示工程、微调或RAG来进一步优化它。因此，这三个步骤的操作方面是最关键的。让我们深入了解一些流行的LLMOps组件，这些组件可以提高提示工程、微调和RAG。
- en: Human feedback
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类反馈
- en: One valuable refinement step of your LLM is aligning it with your audience’s
    preferences. You must introduce a feedback loop within your application and gather
    a human feedback dataset to further fine-tune the LLM with techniques such as
    **Reinforcement Learning with Human Feedback** (**RLHF**) or more advanced ones
    such as **Direct Preference Optimization** (**DPO**). One popular feedback loop
    is the thumbs-up/thumbs-down button present in most chatbot interfaces. You can
    read more on preference alignment in *Chapter 6*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你LLM的一个有价值的改进步骤是将它与受众的偏好相一致。你必须在你的应用程序中引入一个反馈循环，并收集人类反馈数据集，以使用**带人类反馈的强化学习（RLHF**）或更高级的技术如**直接偏好优化（DPO**）等方法进一步微调LLM。一个流行的反馈循环是大多数聊天机器人界面中的点赞/踩按钮。你可以在*第6章*中了解更多关于偏好对齐的内容。
- en: Guardrails
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安全措施
- en: Unfortunately, LLM systems are not reliable, as they often hallucinate. You
    can optimize your system against hallucinations, but as hallucinations are hard
    to detect and can take many forms, there are significant changes that will still
    happen in the future.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，LLM系统并不可靠，因为它们经常出现幻觉。你可以优化你的系统以对抗幻觉，但由于幻觉难以检测且可以采取多种形式，未来仍有可能发生重大变化。
- en: Most users have accepted this phenomenon, but what is not acceptable is when
    LLMs accidentally output sensitive information, such as GitHub Copilot outputting
    AWS secret keys or other chatbots providing people’s passwords. This can also
    happen with people’s phone numbers, addresses, email addresses, and more. Ideally,
    you should remove all this sensitive data from your training data so the LLM doesn’t
    memorize it, but that doesn’t always happen.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数用户已经接受了这一现象，但不可接受的是当大型语言模型（LLM）意外地输出敏感信息时，例如GitHub Copilot输出AWS密钥或其他聊天机器人提供人们的密码。这种情况也可能发生在人们的电话号码、地址、电子邮件地址等更多方面。理想情况下，你应该从训练数据中移除所有这些敏感数据，这样LLM就不会记住它，但这种情况并不总是发生。
- en: 'LLMs are well known for producing toxic and harmful outputs, such as sexist
    and racist outputs. For example, during an experiment on ChatGPT around April
    2023, people found how to hijack the system by forcing the chatbot to adopt a
    negative persona, such as “a bad person” or “a horrible person.” It worked even
    by forcing the chatbot to play the role of well-known negative characters from
    our history, such as dictators or criminals. For example, this is what ChatGPT
    produced when impersonating a bad person:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLM因其产生有毒和有害的输出而闻名，例如性别歧视和种族歧视的输出。例如，在2023年4月左右对ChatGPT进行的实验中，人们发现通过强迫聊天机器人采取负面角色，例如“坏人”或“可怕的人”，可以劫持系统。即使通过强迫聊天机器人扮演我们历史中著名的负面角色，如独裁者或罪犯，这也同样有效。例如，这就是ChatGPT在模仿一个坏人时的输出：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Check the source of the experiment for more examples of different personas:
    [https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/](https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 查看实验的来源以获取更多不同角色的示例：[https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/](https://techcrunch.com/2023/04/12/researchers-discover-a-way-to-make-chatgpt-consistently-toxic/).
- en: 'The discussion can be extended to a never-ending list of examples, but the
    key takeaway is that your LLM can produce harmful output or receive dangerous
    input, so you should monitor and prepare for them. Thus, to create safe LLM systems,
    you must protect them against harmful, sensitive, or invalid input and output
    by adding guardrails:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论可以扩展到无数个例子，但关键是要认识到你的LLM可能会产生有害的输出或接收危险的输入，因此你应该监控并为此做好准备。因此，为了创建安全的LLM系统，你必须通过添加防护措施来保护它们免受有害、敏感或无效的输入和输出的侵害：
- en: '**Input guardrails**:Input guardrails primarily protect against three main
    risks: exposing private information to external APIs, executing harmful prompts
    that could compromise your system (model jailbreaking), and accepting violent
    or unethical prompts. When it comes to leaking private information to external
    APIs, the risk is specific to sending sensitive data outside your organization,
    such as credentials or classified information. When talking about model jailbreaking,
    we mainly refer to prompt injection, such as executing malicious SQL code that
    can access, delete, or corrupt your data. Lastly, some applications don’t want
    to accept violent or unethical queries from users, such as asking an LLM how to
    build a bomb.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入防护措施**：输入防护措施主要保护三种主要风险：向外部API泄露私人信息、执行可能损害你系统的有害提示（模型越狱），以及接受暴力或不道德的提示。当谈到向外部API泄露私人信息时，风险是特定的，涉及将敏感数据发送到组织外部，如凭证或机密信息。当谈到模型越狱时，我们主要指的是提示注入，例如执行可以访问、删除或破坏你的数据的恶意SQL代码。最后，一些应用程序不希望接受来自用户的暴力或不道德的查询，例如询问LLM如何制造炸弹。'
- en: '**Output guardrails**: At the output of an LLM response, you want to catch
    failed outputs that don’t respect your application’s standards. This can vary
    from one application to another, but some examples are empty responses (these
    responses don’t follow your expected format, such as JSON or YAML), toxic responses,
    hallucinations, and, in general, wrong responses. Also, you have to check for
    sensitive information that can leak from the internal knowledge of the LLM or
    your RAG system.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出防护措施**：在LLM响应的输出端，你希望捕捉到不尊重你应用程序标准的失败输出。这因应用而异，但一些例子包括空响应（这些响应不符合你预期的格式，如JSON或YAML）、有毒响应、幻觉，以及一般意义上的错误响应。此外，你还需要检查LLM或你的RAG系统内部知识可能泄露的敏感信息。'
- en: Popular guardrail tools are Galileo Protect, which detects prompt injections,
    toxic language, data privacy protection leaks, and hallucinations. Also, you can
    use OpenAI’s Moderation API to detect harmful inputs or outputs and take action
    on them.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的防护工具包括Galileo Protect，它可以检测提示注入、有害语言、数据隐私保护泄露和幻觉。此外，你还可以使用OpenAI的Moderation
    API来检测有害输入或输出并采取行动。
- en: The downside of adding input and output guardrails is the extra latency added
    to your system, which might interfere with your application’s user experience.
    Thus, there is a trade-off between the safety of your input/output and latency.
    Regarding invalid outputs, as LLMs are non-deterministic, you can implement a
    retry mechanism to generate another potential candidate. However, as stated above,
    running the retry sequentially will double the response time. Thus, a common strategy
    is to run multiple generations in parallel and pick the best one. This will increase
    redundancy but help keep the latency in check.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 添加输入和输出防护栏的缺点是会增加系统中的额外延迟，这可能会干扰你的应用程序的用户体验。因此，在输入/输出的安全性和延迟之间有一个权衡。关于无效输出，由于LLM是非确定性的，你可以实现一个重试机制来生成另一个可能的候选者。然而，如上所述，顺序运行重试将加倍响应时间。因此，一种常见的策略是并行运行多个生成并选择最佳的一个。这将增加冗余但有助于控制延迟。
- en: Prompt monitoring
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示监控
- en: 'Monitoring is not new to LLMOps, but in the LLM world, we have a new entity
    to manage: the prompt. Thus, we have to find specific ways to log and analyze
    them.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 监控对于LLMOps来说并不陌生，但在LLM的世界里，我们有一个新的实体需要管理：提示。因此，我们必须找到特定的方法来记录和分析它们。
- en: Most ML platforms, such as Opik (from Comet ML) and W&B, or other specialized
    tools like Langfuse, have implemented logging tools to debug and monitor prompts.
    While in production, using these tools, you usually want to track the user input,
    the prompt templates, the input variables, the generated response, the number
    of tokens, and the latency.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习平台，如来自Comet ML的Opik和W&B，或其他专门的工具如Langfuse，都实现了日志记录工具以用于调试和监控提示。在生产环境中，使用这些工具时，你通常希望追踪用户输入、提示模板、输入变量、生成的响应、标记数量和延迟。
- en: 'When generating an answer with an LLM, we don’t wait for the whole answer to
    be generated; we stream the output token by token. This makes the entire process
    snappier and more responsive. Thus, when it comes to tracking the latency of generating
    an answer, the final user experience must look at this from multiple perspectives,
    such as:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用大型语言模型（LLM）生成答案时，我们不会等待整个答案生成完成；我们按顺序逐个生成输出标记。这使得整个过程更加迅速和响应灵敏。因此，在追踪生成答案的延迟时，最终用户体验必须从多个角度来考虑，例如：
- en: '**Time to First Token** (**TTFT**): The time it takes for the first token to
    be generated'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**首次标记生成时间**（**TTFT**）：生成第一个标记所需的时间'
- en: '**Time between Tokens** (**TBT**): The interval between each token generation'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记间时间**（**TBT**）：每个标记生成之间的间隔'
- en: '**Tokens per Second** (**TPS**): The rate at which tokens are generated'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每秒标记数**（**TPS**）：标记生成的速率'
- en: '**Time per Output Token** (**TPOT**): The time it takes to generate each output
    token'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个输出标记的时间**（**TPOT**）：生成每个输出标记所需的时间'
- en: '**Total Latency**: The total time required to complete a response'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总延迟**：完成响应所需的总时间'
- en: Also, tracking the total input and output tokens is critical to understanding
    the costs of hosting your LLMs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，跟踪总输入和输出标记对于理解托管你的LLMs的成本至关重要。
- en: Ultimately, you can compute metrics that validate your model’s performance for
    each input, prompt, and output tuple. Depending on your use case, you can compute
    things such as accuracy, toxicity, and hallucination rate. When working with RAG
    systems, you can also compute metrics relative to the relevance and precision
    of the retrieved context.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你可以计算针对每个输入、提示和输出元组的指标，以验证模型性能。根据你的用例，你可以计算诸如准确性、毒性以及幻觉率等指标。当与RAG系统一起工作时，你还可以计算与检索到的上下文的相关性和精确度相关的指标。
- en: Another essential thing to consider when monitoring prompts is to log their
    full traces. You might have multiple intermediate steps from the user query to
    the final general answer. For example, rewriting the query to improve the RAG’s
    retrieval accuracy evolves one or more intermediate steps. Thus, logging the full
    trace reveals the entire process from when a user sends a query to when the final
    response is returned, including the actions the system takes, the documents retrieved,
    and the final prompt sent to the model. Additionally, you can log the latency,
    tokens, and costs at each step, providing a more fine-grained view of all the
    steps.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控提示时，另一个需要考虑的重要事项是记录它们的完整跟踪。从用户查询到最终通用答案，您可能有多个中间步骤。例如，为了提高RAG的检索准确性而重写查询，会演变一个或多个中间步骤。因此，记录完整的跟踪可以揭示从用户发送查询到最终响应返回的整个过程，包括系统采取的操作、检索的文档以及发送给模型的最终提示。此外，您还可以记录每个步骤的延迟、标记和成本，从而提供对所有步骤的更细致的视图。
- en: '![Trace in Langfuse UI](img/B31105_11_04.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Langfuse UI中的跟踪](img/B31105_11_04.png)'
- en: 'Figure 11.4: Example trace in the Langfuse UI'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：Langfuse UI中的示例跟踪
- en: As shown in *Figure 11.4*, the end goal is to trace each step from the user’s
    input until the generated answer. If something fails or behaves unexpectedly,
    you can point exactly to the faulty step. The query can fail due to an incorrect
    answer, an invalid context, or incorrect data processing. Also, the application
    can behave unexpectedly if the number of generated tokens suddenly fluctuates
    during specific steps.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.4*所示，最终目标是跟踪从用户输入到生成答案的每一步。如果出现故障或行为异常，您可以精确地指出故障步骤。查询可能因答案错误、无效上下文或数据处理错误而失败。此外，如果在特定步骤中生成的标记数量突然波动，应用程序可能会出现意外的行为。
- en: To conclude, LLMOps is a rapidly developing field. Given its quick evolution,
    making predictions is challenging. The truth is that we are not sure if the term
    LLMOps is here to stay. However, what is certain is that numerous new use cases
    for LLMs will emerge, along with tools and best practices to manage their lifecycle.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，LLMOps是一个快速发展的领域。鉴于其快速演变，做出预测是具有挑战性的。事实是，我们不确定LLMOps这个术语是否会持续存在。然而，可以确定的是，将出现许多新的LLM用例，以及管理和它们生命周期的工具和最佳实践。
- en: Even if this DevOps, MLOps, and LLMOps section is far from comprehensive, it
    provides a strong idea of how to apply best ops practices in our LLM Twin use
    case.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个DevOps、MLOps和LLMOps部分远非全面，但它为我们如何在LLM Twin用例中应用最佳操作实践提供了一个强有力的概念。
- en: Deploying the LLM Twin’s pipelines to the cloud
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLM Twin的管道部署到云端
- en: 'This section will show you how to deploy all the LLM Twin’s pipelines to the
    cloud. We must deploy the entire infrastructure to have the whole system working
    in the cloud. Thus, we will have to:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将向您展示如何将所有LLM Twin的管道部署到云端。我们必须部署整个基础设施，以确保整个系统在云端运行。因此，我们将不得不：
- en: Set up an instance of MongoDB serverless.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置MongoDB无服务器实例。
- en: Set up an instance of Qdrant serverless.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Qdrant无服务器实例。
- en: Deploy the ZenML pipelines, container, and artifact registry to AWS.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将ZenML管道、容器和工件注册表部署到AWS。
- en: Containerize the code and push the Docker image to a container registry.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代码容器化并将Docker镜像推送到容器注册库。
- en: Note that the training and inference pipelines already work with AWS SageMaker.
    Thus, by following the preceding four steps, we ensure that our whole system is
    on the cloud, ready to scale and serve our imaginary clients.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练和推理管道已经与AWS SageMaker兼容。因此，通过遵循前面的四个步骤，我们确保我们的整个系统在云端，准备好扩展并服务于我们的想象中的客户。
- en: '**What are the deployment costs?**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**部署成本是多少？**'
- en: We will stick to the free versions of the MongoDB, Qdrant, and ZenML services.
    As for AWS, we will mostly stick to their free tier for running the ZenML pipelines.
    The SageMaker training and inference components are more costly to run (which
    we won’t run in this section). Thus, what we will show you in the following sections
    will generate minimum costs (a few dollars at most) from AWS.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将坚持使用MongoDB、Qdrant和ZenML服务的免费版本。至于AWS，我们将主要坚持他们的免费层来运行ZenML管道。SageMaker的训练和推理组件运行成本更高（我们将在本节中不运行）。因此，在接下来的章节中，我们将展示的内容将产生最低的成本（最多几美元）来自AWS。
- en: Understanding the infrastructure
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解基础设施
- en: Before diving into the step-by-step tutorial, where we will show you how to
    set up all the necessary components, let’s briefly overview our infrastructure
    and how all the elements interact. This will help us in mindfully following the
    tutorials below.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在逐步教程之前，我们将展示如何设置所有必要的组件，让我们简要概述我们的基础设施以及所有元素如何交互。这将帮助我们细心地跟随下面的教程。
- en: As shown in *Figure 11.5*, we have a few services to set up. To keep things
    simple, for MongoDB and Qdrant, we will leverage their serverless freemium version.
    As for ZenML, we will leverage the free trial of the ZenML cloud, which will help
    us orchestrate all the pipelines in the cloud. How will it do that?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.5*所示，我们需要设置一些服务。为了保持简单，对于MongoDB和Qdrant，我们将利用它们的免费服务器版本。至于ZenML，我们将利用ZenML云的免费试用版，这将帮助我们云中编排所有管道。它将如何做到这一点呢？
- en: 'By leveraging the ZenML cloud, we can quickly allocate all the required AWS
    resources to run, scale, and store the ML pipeline. It will help us spin up, with
    a few clicks, the following AWS components:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用ZenML云，我们可以快速分配所有必要的AWS资源来运行、扩展和存储ML管道。它将帮助我们通过几个点击启动以下AWS组件：
- en: An ECR service for storing Docker images
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储Docker镜像的ECR服务
- en: An S3 object storage for storing all our artifacts and models
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于存储所有工件和模型的S3对象存储
- en: SageMaker Orchestrator for orchestrating, running, and scaling all our ML pipelines
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于编排、运行和扩展所有我们的ML管道的SageMaker编排器
- en: '![](img/B31105_11_05.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_05.png)'
- en: 'Figure 11.5: Infrastructure flow'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：基础设施流程
- en: 'Now that we understand what the essential resources of our infrastructure are,
    let’s look over the core flow of running a pipeline in the cloud that we will
    learn to implement, presented in *Figure 11.5*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们基础设施的基本资源，让我们回顾一下在云中运行管道的核心流程，这是我们将要学习的实现，如*图11.5*所示：
- en: Build a Docker image that contains all the system dependencies, the project
    dependencies, and the LLM Twin application.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个包含所有系统依赖项、项目依赖项和LLM Twin应用程序的Docker镜像。
- en: Push the Docker image to **ECR**, where **SageMaker** can access it.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Docker镜像推送到**ECR**，其中**SageMaker**可以访问它。
- en: Now, we can trigger any pipeline implemented during this book either from the
    CLI of our local machine or **ZenML’s** dashboard.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以从本地机器的CLI或**ZenML**仪表板触发本书中实现的任何管道。
- en: Each step from ZenML’s pipeline will be mapped to a SageMaker job that runs
    on an AWS EC2 **virtual machine** (**VM**). Based on the dependencies between
    the **directed acyclic graph** (**DAG**) steps, some will run in parallel and
    others sequentially.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ZenML管道的每个步骤都将映射到在AWS EC2 **虚拟机**（**VM**）上运行的SageMaker作业。基于**有向无环图**（**DAG**）步骤之间的依赖关系，一些将并行运行，而另一些将顺序运行。
- en: When running a step, SageMaker pulls the Docker image from ECR, defined in step
    2\. Based on the pulled image, it creates a Docker container that executes the
    pipeline step.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当运行一个步骤时，SageMaker会从步骤2中定义的ECR中拉取Docker镜像。根据拉取的镜像，它创建一个Docker容器来执行管道步骤。
- en: As the job is executed, it can access the S3 artifact storage, MongoDB, and
    Qdrant vector DB to query or push data. The ZenML dashboard is a key tool, providing
    real-time updates on the pipeline’s progress and ensuring a clear view of the
    process.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当作业执行时，它可以访问S3工件存储、MongoDB和Qdrant向量数据库以查询或推送数据。ZenML仪表板是一个关键工具，提供管道进度的实时更新，并确保对过程的清晰视图。
- en: Now that we know how the infrastructure works, let’s start by setting up MongoDB,
    Qdrant, and the ZenML cloud.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了基础设施的工作原理，让我们先设置MongoDB、Qdrant和ZenML云。
- en: '**What AWS cloud region should I choose?**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**我应该选择哪个AWS云区域？**'
- en: In our tutorials, all the services will be deployed to AWS within the **Frankfurt
    (eu-central-1)** region. You can select another region, but be consistent across
    all the services to ensure faster responses between components and reduce potential
    errors.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的教程中，所有服务都将部署到位于**法兰克福（eu-central-1）**区域的AWS。您可以选择另一个区域，但请确保所有服务保持一致，以确保组件之间的响应更快并减少潜在的错误。
- en: '**How should I manage changes in the services’ UIs?**'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**我应该如何管理服务UI的变化？**'
- en: Unfortunately, MongoDB, Qdrant, or other services may change their UI or naming
    conventions. As we can’t update this book each time that happens, please refer
    to their official documentation to check anything that differs from our tutorial.
    We apologize for this inconvenience, but unfortunately, it is not in our control.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，MongoDB、Qdrant或其他服务可能会更改它们的UI或命名约定。由于我们无法在每次发生这种情况时更新这本书，请参考它们的官方文档以检查与我们的教程不同的任何内容。我们对此不便表示歉意，但遗憾的是，这不在我们的控制范围内。
- en: Setting up MongoDB
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 MongoDB
- en: 'We will show you how to create and integrate a free MongoDB cluster into our
    projects. To do so, these are the steps you have to follow:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将向您展示如何创建并将免费的 MongoDB 集群集成到我们的项目中。为此，您必须遵循以下步骤：
- en: Go to their site at [https://www.mongodb.com](https://www.mongodb.com) and create
    an account.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往他们的网站 [https://www.mongodb.com](https://www.mongodb.com) 并创建一个账户。
- en: In the left panel, go to **Deployment** **|** **Database** and click **Build
    a Cluster**.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中，转到**部署** **|** **数据库**并点击**构建集群**。
- en: 'Within the creation form, do the following:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建表单中，执行以下操作：
- en: Choose an **M0 Free** cluster.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个**M0 Free**集群。
- en: Call your cluster **twin**.
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的集群命名为**twin**。
- en: Choose **AWS** as your provider.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**AWS**作为您的提供商。
- en: Choose **Frankfurt (eu-central-1)** as your region. You can choose another region,
    but be careful to choose the same region for all future AWS services.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**法兰克福（eu-central-1）**作为您的区域。您可以选择另一个区域，但请注意为所有未来的 AWS 服务选择相同的区域。
- en: Leave the rest of the attributes with their default values.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其余的属性保留为默认值。
- en: In the bottom right, click the **Create Deployment** green button.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右下角，点击**创建部署**的绿色按钮。
- en: To test that your newly created MongoDB cluster works fine, we must connect
    to it from our local machine. We used the MongoDB VS Code extension to do so,
    but you can use any other tool. Thus, from their **Choose a connection method**
    setup flow, choose **MongoDB for VS Code**. Then, follow the steps provided on
    their site.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试您新创建的 MongoDB 集群是否正常工作，我们必须从我们的本地机器连接到它。我们使用了 MongoDB VS Code 扩展来完成此操作，但您可以使用任何其他工具。因此，从他们的**选择连接方法**设置流程中选择**MongoDB
    for VS Code**。然后，按照他们网站上提供的步骤操作。
- en: 'To connect, you must paste the DB connection URL in the VS Code extension (or
    another tool of your liking), which contains your username, password, and cluster
    URL, similar to this one: `mongodb+srv://<username>:<password> @twin.vhxy1.mongodb.net`.
    Make sure to save this URL somewhere you can copy it from later.'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要连接，您必须在 VS Code 扩展（或您喜欢的其他工具）中粘贴 DB 连接 URL，其中包含您的用户名、密码和集群 URL，类似于这个：`mongodb+srv://<username>:<password>@twin.vhxy1.mongodb.net`。请确保将此
    URL 保存在您可以复制的地方。
- en: If you don’t know or want to change your password, go to **Security** **→**
    **Quickstart** in the left panel. There, you can edit your login credentials.
    Be sure to save them somewhere safe, as you won’t be able to access them later.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您不知道或想更改密码，请转到左侧面板中的**安全** **→** **快速入门**。在那里，您可以编辑您的登录凭证。请确保将它们保存在安全的地方，因为您稍后无法访问它们。
- en: After verifying that your connections work, go to **Security** **→** **Network
    Access** in the left panel and click **ADD IP ADDRESS**.Then click **ALLOW ACCESS
    FROM ANYWHERE** and hit Confirm. Out of simplicity, we allow any machine from
    any IP to access our MongoDB cluster. This ensures that our pipelines can query
    or write to the DB without any additional complex networking setup. It’s not the
    safest option for production, but for our example, it’s perfectly fine.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在验证您的连接正常工作后，在左侧面板中转到**安全** **→** **网络访问**并点击**添加 IP 地址**。然后点击**允许从任何地方访问**并确认。出于简便起见，我们允许任何
    IP 地址的机器访问我们的 MongoDB 集群。这确保了我们的管道可以查询或写入数据库，而无需任何额外的复杂网络设置。这不是生产环境中最安全的选项，但对我们这个例子来说，完全没问题。
- en: 'The final step is to return to your project and open your `.env` file. Now,
    either add or replace the `DATABASE_HOST` variable with your MongoDB connection
    string. It should look something like this: `DATABASE_HOST= mongodb+srv://<username>:<password>
    @twin.vhxy1.mongodb.net`.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是返回您的项目并打开您的 `.env` 文件。现在，您可以将 `DATABASE_HOST` 变量添加或替换为您的 MongoDB 连接字符串。它应该看起来像这样：`DATABASE_HOST=mongodb+srv://<username>:<password>@twin.vhxy1.mongodb.net`。
- en: That’s it! Now, instead of reading and writing from your local MongoDB, you
    will do it from the cloud MongoDB cluster we just created. Let’s repeat a similar
    process with Qdrant.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在，您将不再从本地 MongoDB 读取和写入，而是从我们刚刚创建的云 MongoDB 集群中进行。让我们用 Qdrant 重复一个类似的过程。
- en: Setting up Qdrant
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Qdrant
- en: 'We have to repeat a similar process to what we did for MongoDB. Thus, to create
    a Qdrant cluster and hook it to our project, follow these steps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须重复与 MongoDB 相似的过程。因此，要创建 Qdrant 集群并将其连接到我们的项目，请按照以下步骤操作：
- en: Go to Qdrant at [https://cloud.qdrant.io/](https://cloud.qdrant.io/) and create
    an account.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 Qdrant 在 [https://cloud.qdrant.io/](https://cloud.qdrant.io/) 并创建一个账户。
- en: In the left panel, go to **Clusters** and click **Create**.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中，前往**集群**并点击**创建**。
- en: 'Fill out the cluster creation form with the following:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下信息填写集群创建表单：
- en: Choose the **Free** version of the cluster.
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择集群的**免费**版本。
- en: Choose **GCP** as the cloud provider (while writing the book, it was the only
    one allowed for a free cluster).
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**GCP**作为云提供商（在撰写本书时，它是唯一允许免费集群的提供商）。
- en: Choose **Frankfurt** as the region (or the same region as you chose for MongoDB).
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**法兰克福**作为区域（或与您为MongoDB选择的相同区域）。
- en: Name the cluster **twin**.
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将集群命名为**twin**。
- en: Leave the rest of the attributes with their default values and click **Create**.
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其余的属性保留为默认值，然后点击**创建**。
- en: Access the cluster in the **Data Access Control** section in the left panel.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板的**数据访问控制**部分访问集群。
- en: Click **Create** and choose your **twin** cluster to create a new access token.Copy
    the newly created token somewhere safe, as you won’t be able to access it anymore.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建**并选择您的**twin**集群以创建一个新的访问令牌。将新创建的令牌复制到安全的地方，因为您将无法再次访问它。
- en: You can run their example from **Usage Examples** to test that your connection
    works fine.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以从**用法示例**中运行他们的示例来测试您的连接是否正常。
- en: Go back to the **Clusters** section of Qdrant and open your newly created **twin**
    cluster. You will have access to the cluster’s **endpoint**, which you need to
    configure Qdrant in your code.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到Qdrant的**集群**部分，打开您新创建的**twin**集群。您将能够访问集群的**端点**，您需要在代码中配置Qdrant。
- en: 'You can visualize your Qdrant collections and documents by clicking **Open
    Dashboard** and entering your **API Key** as your password. The Qdrant cluster
    dashboard will now be empty, but after running the pipelines, you will see all
    the collections, as shown here:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过点击**打开仪表板**并输入您的**API密钥**作为密码来可视化您的Qdrant集合和文档。此时，Qdrant集群仪表板将显示为空，但在运行管道后，您将看到所有集合，如图所示：
- en: '![](img/B31105_11_06.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_06.png)'
- en: 'Figure 11.6: Qdrant cluster dashboard example after being populated with two
    collections.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：在填充了两个集合后，Qdrant集群仪表板示例。
- en: 'Finally, return to your project and open your `.env` file. Now, we must fill
    in a couple of environment variables as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，返回到您的项目并打开您的`.env`文件。现在，我们必须填写几个环境变量，如下所示：
- en: '[PRE1]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'That’s it! Instead of reading and writing from your local Qdrant vector DB,
    you will do it from the cloud Qdrant cluster we just created. Just to be sure
    that everything works fine, run the end-to-end data pipeline with the cloud version
    of MongoDB and Qdrant as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！您将不再从本地Qdrant向量数据库中读取和写入，而是从我们刚刚创建的云Qdrant集群中进行。为了确保一切正常，请按照以下方式运行端到端数据管道，使用MongoDB和Qdrant的云版本：
- en: '[PRE2]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The last step is setting up the ZenML cloud and deploying all our infrastructure
    to AWS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是设置ZenML云并将所有基础设施部署到AWS。
- en: Setting up the ZenML cloud
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置ZenML云
- en: Setting up the ZenML cloud and the AWS infrastructure is a multi-step process.
    First, we will set up a ZenML cloud account, then the AWS infrastructure through
    the ZenML cloud, and, finally, we will bundle our code in a Docker image to run
    it in AWS SageMaker.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 设置ZenML云和AWS基础设施是一个多步骤的过程。首先，我们将设置一个ZenML云账户，然后通过ZenML云设置AWS基础设施，最后，我们将我们的代码打包到Docker镜像中，以便在AWS
    SageMaker中运行。
- en: 'Let’s start with setting up the ZenML cloud:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设置ZenML云开始：
- en: Go to the ZenML cloud at [https://cloud.zenml.io](https://cloud.zenml.io) and
    make an account. They provide a seven-day free trial, which is enough to run our
    examples.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往ZenML云[https://cloud.zenml.io](https://cloud.zenml.io)并创建一个账户。他们提供七天免费试用，这对于运行我们的示例来说已经足够了。
- en: Fill out their onboarding form and create an organization with a unique name
    and a tenant called **twin**. A tenant refers to a deployment of ZenML in a fully
    isolated environment. Wait a few minutes until your tenant server is up before
    proceeding to the next step.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写他们的入职表格，并创建一个具有唯一名称和名为**twin**的租户。租户是指在完全隔离的环境中部署ZenML的实例。在继续下一步之前，请等待几分钟，直到您的租户服务器启动。
- en: If you want to, you can go through their **Quickstart Guide** to understand
    how the ZenML cloud works with a simpler example. It is not required to go through
    it to deploy the LLM Twin application, but we recommend it to ensure everything
    works fine.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您愿意，您可以浏览他们的**快速入门指南**来了解ZenML云如何与一个更简单的示例一起工作。虽然部署LLM Twin应用程序不需要通过它，但我们建议您这样做以确保一切正常工作。
- en: At this point, we assume that you have gone through the **Quickstart Guide**.
    Otherwise, you might encounter issues during the next steps. To connect our project
    with this ZenML cloud tenant, return to the project and run the `zenml connect`
    command provided in the dashboard. It looks similar to the following example but
    with a different URL:`zenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io`.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设您已经完成了 **快速入门指南**。否则，您在下一步可能会遇到问题。为了将我们的项目与这个 ZenML 云租户连接起来，返回项目并运行仪表板中提供的
    `zenml connect` 命令。它看起来类似于以下示例，但 URL 不同：`zenml connect --url https://0c37a553-zenml.cloudinfra.zenml.io`。
- en: 'To ensure everything works fine, run a random pipeline from your code. Note
    that at this point, we are still running it locally, but instead of logging the
    results to the local server, we log everything to the cloud version:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保一切正常工作，从您的代码中运行一个随机管道。请注意，在此阶段，我们仍在本地运行它，但不是将结果记录到本地服务器，而是将所有内容记录到云端：
- en: '[PRE3]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Go to the **Pipelines** section in the left panel of the ZenML dashboard. If
    everything worked fine, you should see the pipeline you ran in *Step 5* there.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 ZenML 仪表板的左侧面板中的 **管道** 部分。如果一切正常，您应该在那里看到您在 *步骤 5* 中运行的管道。
- en: Ensure that your ZenML server version matches your local ZenML version. For
    example, when we wrote this book, both were version 0.64.0\. If they don’t match,
    you might encounter strange behavior, or it might not work correctly. The easiest
    fix is to go to your `pyproject.toml` file, find the `zenml` dependency, and update
    it with the version of your server. Then run `poetry lock --no-update && poetry
    install` to update your local virtual environment.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保您的 ZenML 服务器版本与本地 ZenML 版本相匹配。例如，当我们编写这本书时，两者都是版本 0.64.0。如果它们不匹配，您可能会遇到奇怪的行为，或者它可能无法正确工作。最简单的修复方法是前往您的
    `pyproject.toml` 文件，找到 `zenml` 依赖项，并使用服务器版本更新它。然后运行 `poetry lock --no-update &&
    poetry install` 以更新您的本地虚拟环境。
- en: To ship the code to AWS, you must create a ZenML stack. A stack is a set of
    components, such as the underlying orchestrator, object storage, and container
    registry, that ZenML needs under the hood to run the pipelines. Intuitively, you
    can see your stack as your infrastructure. While working locally, ZenML offers
    a default stack that allows you to quickly develop your code and test things locally.
    However, by defining different stacks, you can quickly switch between different
    infrastructure environments, such as local and AWS runs, which we will showcase
    in this section.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 要将代码发送到 AWS，您必须创建一个 ZenML 堆栈。堆栈是一组组件，例如底层的编排器、对象存储和容器注册表，ZenML 在幕后需要这些组件来运行管道。直观地说，您可以将您的堆栈视为您的基础设施。在本地工作时，ZenML
    提供了一个默认的堆栈，允许您快速开发代码并在本地测试。然而，通过定义不同的堆栈，您可以快速在不同的基础设施环境中切换，例如本地和 AWS 运行，我们将在本节中展示这一点。
- en: Before starting this section, ensure you have an AWS account with admin permissions
    ready.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，请确保您有一个具有管理员权限的 AWS 账户准备就绪。
- en: 'With that in mind, let’s create an AWS stack for our project. To do so, follow
    the next steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们为我们的项目创建一个 AWS 堆栈。为此，请按照以下步骤操作：
- en: In the left panel, click on the **Stacks** section and hit the **New Stack**
    button.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧面板中，点击 **堆栈** 部分，然后点击 **新建堆栈** 按钮。
- en: You will have multiple options for creating a stack, but the easiest is creating
    one from scratch within the in-browser experience, which doesn’t require additional
    preparations. This is not very flexible, but it is enough to host our project.
    Thus, choose **Create New Infrastructure** **→** **In-browser Experience**.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将有多种创建堆栈的选项，但最简单的是在浏览器体验中从头开始创建一个，这不需要额外的准备。这并不非常灵活，但对于托管我们的项目来说已经足够了。因此，选择
    **创建新基础设施** **→** **浏览器体验**。
- en: Then, choose **AWS** as your cloud provider.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择 **AWS** 作为您的云提供商。
- en: Choose **Europe (Frankfurt)—eu-central-1** as your location or the region you
    used to set up MongoDB and Qdrant.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **欧洲（法兰克福）—eu-central-1** 作为您的位置或您用于设置 MongoDB 和 Qdrant 的区域。
- en: Name it **aws-stack**.It is essential to name it exactly like this so that the
    commands that we will use work.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其命名为 **aws-stack**。必须将其命名为确切的这个名字，这样我们使用的命令才能正常工作。
- en: Now ZenML will create a set of IAM roles to give permissions to all the other
    components to communicate with each other, an S3 bucket as your artifact storage,
    an ECR repository as your container registry, and SageMaker as your orchestrator.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，ZenML 将创建一组 IAM 角色，以授予所有其他组件相互通信的权限，一个 S3 存储桶作为您的工件存储，一个 ECR 仓库作为您的容器注册表，以及
    SageMaker 作为您的编排器。
- en: Click **Next**.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **下一步**。
- en: Click the **Deploy to AWS** button. It will open a **CloudFormation** page on
    AWS. ZenML leverages **CloudFormation** (an infrastructure as code, or IaC, tool)to
    create all the AWS resources we enumerated in *Step 6*.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **部署到 AWS** 按钮。它将在 AWS 上打开一个 **CloudFormation** 页面。ZenML 利用 **CloudFormation**（一个基础设施即代码工具）创建我们在
    *步骤 6* 中列出的所有 AWS 资源。
- en: At the bottom, check all the boxes to acknowledge that AWS CloudFormation will
    create AWS resources on your behalf. Finally, click the **Create stack** button.
    Now, we must wait for a couple of minutes for AWS CloudFormation to spin up all
    the resources.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在底部，勾选所有复选框以确认 AWS CloudFormation 将代表您创建 AWS 资源。最后，点击 **创建堆栈** 按钮。现在，我们必须等待几分钟，让
    AWS CloudFormation 启动所有资源。
- en: Return to the ZenML page and click the **Finish** button.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 ZenML 页面并点击 **完成** 按钮。
- en: By leveraging ZenML, we efficiently deployed the entire AWS infrastructure for
    our ML pipelines. We began with a basic example, sacrificing some control. However,
    if you seek more control, ZenML offers the option to use Terraform (an IaC tool)
    to fully control your AWS resources or to connect ZenML with your current infrastructure.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用 ZenML，我们高效地部署了我们的 ML 管道所需的整个 AWS 基础设施。我们从基本示例开始，牺牲了一些控制。然而，如果您寻求更多控制，ZenML
    提供了使用 Terraform（一个基础设施即代码工具）来完全控制您的 AWS 资源或连接 ZenML 与您当前基础设施的选项。
- en: 'Before moving to the next step, let’s have a quick recap of the AWS resources
    we just created:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一步之前，让我们快速回顾一下我们刚刚创建的 AWS 资源：
- en: '**An IAM role** is an AWS identity with permissions policies that define what
    actions are allowed or denied for that role. It is used to grant access to AWS
    services without needing to share security credentials.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IAM 角色** 是一个 AWS 身份，具有定义该角色允许或拒绝执行哪些操作的权限策略。它用于授予对 AWS 服务的访问权限，而无需共享安全凭证。'
- en: '**S3** is a scalable and secure object storage service that allows storing
    and retrieving files from anywhere on the web. It is commonly used for data backup,
    content storage, and data lakes. It’s more scalable and flexible than Google Drive.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3** 是一个可扩展且安全的对象存储服务，允许从网络上的任何地方存储和检索文件。它通常用于数据备份、内容存储和数据湖。它比 Google Drive
    更具可扩展性和灵活性。'
- en: '**ECR** is a fully managed Docker container registry that makes storing, managing,
    and deploying Docker container images easy.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ECR** 是一个完全托管的 Docker 容器注册表，使得存储、管理和部署 Docker 容器镜像变得容易。'
- en: '**SageMaker** is a fully managed service that allows developers and data scientists
    to quickly build, train, and deploy ML models.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker** 是一个完全托管的服务，允许开发人员和数据科学家快速构建、训练和部署 ML 模型。'
- en: '**SageMaker Orchestrator** is a feature of SageMaker that helps automate the
    execution of ML workflows, manage dependencies between steps, and ensure the reproducibility
    and scalability of model training and deployment pipelines. Other similar tools
    are Prefect, Dagster, Metaflow, and Airflow.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker Orchestrator** 是 SageMaker 的一个功能，它有助于自动化 ML 工作流的执行，管理步骤之间的依赖关系，并确保模型训练和部署管道的可重复性和可扩展性。其他类似的工具有
    Prefect、Dagster、Metaflow 和 Airflow。'
- en: '**CloudFormation** is a service that allows you to model and set up your AWS
    resources so that you can spend less time managing them and more time focusing
    on your applications. It automates the process of provisioning AWS infrastructure
    using templates.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CloudFormation** 是一个服务，允许您建模和设置您的 AWS 资源，这样您就可以花更少的时间管理它们，更多的时间专注于您的应用程序。它通过使用模板自动化了使用
    AWS 基础设施的过程。'
- en: Before running the ML pipelines, the last step is to containerize the code and
    prepare a Docker image that packages our dependencies and code.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 ML 管道之前，最后一步是将代码容器化并准备一个 Docker 镜像，该镜像打包了我们的依赖项和代码。
- en: Containerize the code using Docker
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Docker 容器化代码
- en: So far, we have defined our infrastructure, MongoDB, Qdrant, and AWS, for storage
    and computing. The last step is to find a way to take our code and run it on top
    of this infrastructure. The most popular solution is Docker, a tool that allows
    us to create an isolated environment (a container) that contains everything we
    need to run our application, such as system dependencies, Python dependencies,
    and the code.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经定义了我们的基础设施，MongoDB、Qdrant 和 AWS，用于存储和计算。最后一步是找到一种方法，将我们的代码运行在这个基础设施之上。最流行的解决方案是
    Docker，这是一个允许我们创建一个包含运行应用程序所需一切（如系统依赖项、Python 依赖项和代码）的隔离环境（容器）的工具。
- en: 'We defined our Docker image at the project’s root in the `Dockerfile`. This
    is the standard naming convention for Docker. Before digging into the code, if
    you want to build the Docker image yourself, ensure that you have Docker installed
    on your machine. If you don’t have it, you can install it by following the instructions
    provided here: [https://docs.docker.com/engine/install](https://docs.docker.com/engine/install).
    Now, let’s look at the content of the `Dockerfile` step by step.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在项目的根目录中的 `Dockerfile` 中定义了我们的 Docker 镜像。这是 Docker 的标准命名约定。在深入研究代码之前，如果你想自己构建
    Docker 镜像，请确保你的机器上已安装 Docker。如果没有，你可以按照这里提供的说明进行安装：[https://docs.docker.com/engine/install](https://docs.docker.com/engine/install)。现在，让我们一步一步地查看
    `Dockerfile` 的内容。
- en: The `Dockerfile` begins by specifying the base image, which is a lightweight
    version of Python 3.11 based on the Debian Bullseye distribution. The environment
    variables are then set up to configure various aspects of the container, such
    as the workspace directory, turning off Python bytecode generation, and configuring
    Python to output directly to the terminal. Additionally, the version of Poetry
    to be installed is specified, and a few environment variables are set to ensure
    that package installations are non-interactive, which is vital for automated builds.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dockerfile` 以指定基础镜像开始，这是一个基于 Debian Bullseye 分发的 Python 3.11 轻量级版本。然后设置环境变量以配置容器的各个方面，例如工作区目录、关闭
    Python 字节码生成以及配置 Python 直接输出到终端。此外，指定了要安装的 Poetry 版本，并设置了一些环境变量以确保软件包安装非交互式，这对于自动化构建至关重要。'
- en: '[PRE4]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we install Google Chrome in the container. The installation process begins
    by updating the package lists and installing essential tools like gnupg, wget,
    and curl. The Google Linux signing key is added, and the Google Chrome repository
    is configured. After another package list update, the stable version of Google
    Chrome is installed. The package lists are removed after installation to keep
    the image as small as possible.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在容器中安装 Google Chrome。安装过程首先通过更新软件包列表并安装必要的工具，如 gnupg、wget 和 curl 来开始。添加了
    Google Linux 签名密钥，并配置了 Google Chrome 仓库。在再次更新软件包列表后，安装了 Google Chrome 的稳定版本。安装完成后，移除软件包列表以尽可能保持镜像大小最小。
- en: '[PRE5]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Following the Chrome installation, other essential system dependencies are installed.
    Once these packages are installed, the package cache is cleaned up to reduce the
    image size further.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Chrome 安装之后，安装了其他必要的系统依赖项。一旦这些软件包安装完成，就会清理软件包缓存以进一步减小镜像大小。
- en: '[PRE6]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Poetry, the dependency management tool, is then installed using pip. The `--no-cache-dir`
    option prevents pip from caching packages, helping to keep the image smaller.
    After installation, Poetry is configured to use up to 20 parallel workers when
    installing packages, which can speed up the installation process.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 pip 安装了依赖管理工具 Poetry。`--no-cache-dir` 选项防止 pip 缓存软件包，有助于保持镜像更小。安装后，将 Poetry
    配置为在安装软件包时使用最多 20 个并行工作进程，这可以加快安装过程。
- en: '[PRE7]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The working directory inside the container is set to `WORKSPACE_ROOT`, which
    defaults to `/app/`, where the application code will reside. The `pyproject.toml`
    and `poetry.lock` files define the Python’s project dependencies and are copied
    into this directory.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 容器内部的工作目录设置为 `WORKSPACE_ROOT`，默认为 `/app/`，其中将存放应用程序代码。`pyproject.toml` 和 `poetry.lock`
    文件定义了 Python 项目的依赖项，并被复制到该目录。
- en: '[PRE8]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With the dependency files in place, the project’s dependencies are installed
    using Poetry. The configuration turns off the creation of a virtual environment,
    meaning the dependencies will be installed directly into the container’s Python
    environment. The installation excludes development dependencies and prevents caching
    to minimize space usage.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在依赖文件就绪后，使用 Poetry 安装项目依赖项。配置关闭了虚拟环境的创建，这意味着依赖项将直接安装到容器的 Python 环境中。安装排除了开发依赖项并防止缓存以最小化空间使用。
- en: Additionally, the `poethepoet` plugin is installed to help manage tasks within
    the project. Finally, any remaining Poetry cache is removed to keep the container
    as lean as possible.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，安装了 `poethepoet` 插件以帮助管理项目中的任务。最后，移除任何剩余的 Poetry 缓存，以尽可能保持容器精简。
- en: '[PRE9]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the final step, the entire project directory from the host machine is copied
    into the container’s working directory. This step ensures that all the application
    files are available within the container.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，将主机机器上的整个项目目录复制到容器的的工作目录中。这一步确保了所有应用程序文件都在容器内可用。
- en: One important trick when writing a `Dockerfile` is to decouple your installation
    steps from copying the rest of the files. This is useful because each Docker command
    is cached and layered on top of each other. Thus, whenever you change one layer
    when rebuilding the Docker image, all the layers below the one altered are executed
    again. Because you rarely change your system and project dependencies but mostly
    change your code, copying your project files in the last step makes rebuilding
    Docker images fast by taking advantage of the caching mechanism’s full potential.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写`Dockerfile`时，一个重要的技巧是将安装步骤与复制其余文件解耦。这很有用，因为每个Docker命令都会被缓存，并逐层叠加。因此，每次当你重建Docker镜像时更改一个层，所有更改层下面的层都会再次执行。因为你很少更改系统和项目依赖项，但主要更改代码，所以在最后一步复制项目文件可以通过利用缓存机制的全部潜力来加快重建Docker镜像的速度。
- en: '[PRE10]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This `Dockerfile` is designed to create a clean, consistent Python environment
    with all necessary dependencies. It allows the project to run smoothly in any
    environment that supports Docker.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Dockerfile`旨在创建一个干净、一致的Python环境，包含所有必要的依赖项。它允许项目在任何支持Docker的环境中顺利运行。
- en: 'The last step is to build the Docker image and push it to the ECR created by
    ZenML. To build the Docker image from the root of the project, run the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是构建Docker镜像并将其推送到ZenML创建的ECR。要从项目的根目录构建Docker镜像，请运行以下命令：
- en: '[PRE11]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We must build it on a Linux platform as the Google Chrome installer we used
    inside Docker works only on a Linux machine. Even if you use a macOS or Windows
    machine, Docker can emulate a virtual Linux container.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在Linux平台上构建它，因为我们使用的Google Chrome安装程序只能在Linux机器上运行。即使你使用macOS或Windows机器，Docker也可以模拟一个虚拟Linux容器。
- en: 'The tag of the newly created Docker image is `llmtwin`. We also provide this
    `build` command under a `poethepoet` command:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 新创建的Docker镜像的标签是`llmtwin`。我们还在`poethepoet`命令下提供了这个`build`命令：
- en: '[PRE12]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let’s push the Docker image to ECR. To do so, navigate to your AWS console
    and then to the ECR service. From there, find the newly created ECR repository.
    It should be prefixed with `zenml-*`, as shown here:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将Docker镜像推送到ECR。为此，导航到你的AWS控制台，然后转到ECR服务。从那里，找到新创建的ECR仓库。它应该以`zenml-*`开头，如图所示：
- en: '![](img/B31105_11_07.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_07.png)'
- en: 'Figure 11.7: AWS ECR example'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：AWS ECR示例
- en: 'The first step is to authenticate to ECR. For this to work, ensure that you
    have the AWS CLI installed and configured with your admin AWS credentials, as
    explained in *Chapter 2*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是验证到ECR。为了使其工作，请确保你已经安装并配置了AWS CLI，并使用你的管理员AWS凭证，如第2章中所述：
- en: '[PRE13]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can get your current `AWS_REGION` by clicking on the toggle in the top-right
    corner, as seen in *Figure 11.8*. Also, you can copy the ECR URL to fill the `AWS_ECR_URL`
    variable from the main AWS ECR dashboard, as illustrated in *Figure 11.7*. After
    running the previous command, you should see the message **Login Succeeded** on
    the CLI.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击右上角的切换按钮来获取当前的`AWS_REGION`，如图11.8所示。你也可以复制ECR URL以填充`AWS_ECR_URL`变量，如图11.7所示。运行前面的命令后，你应该在CLI上看到消息**登录成功**。
- en: '![](img/B31105_11_08.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_08.png)'
- en: 'Figure 11.8: AWS region and account details'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：AWS区域和账户详情
- en: 'Now we have to add another tag to the `llmtwin` Docker image that signals the
    Docker registry we want to push it to:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须给`llmtwin` Docker镜像添加另一个标签，以指示我们想要将其推送到哪个Docker注册表：
- en: '[PRE14]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we push it to ECR by running:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过以下命令将其推送到ECR：
- en: '[PRE15]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After the upload is finished, return to your AWS ECR dashboard and open your
    ZenML repository. The Docker image should appear, as shown here:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 上传完成后，返回你的AWS ECR仪表板并打开你的ZenML仓库。Docker镜像应该会显示出来，如图所示：
- en: '![](img/B31105_11_09.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_09.png)'
- en: 'Figure 11.9: AWS ECR repository example after the Docker image is pushed'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：Docker镜像推送到ECR后的AWS ECR仓库示例
- en: For every change in the code that you need to ship and test, you would have
    to go through all these steps, which are tedious and error-prone. The *Adding
    LLMOps to the LLM Twin*section of this chapter will teach us how to automate these
    steps within the CD pipeline using GitHub Actions. Still, we first wanted to go
    through them manually to fully understand the behind-the-scenes process and not
    treat it as a black box. Understanding these details is vital for debugging your
    CI/CD pipelines, where you must understand the error messages and how to fix them.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您需要发布和测试的代码中的每个更改，您都必须通过所有这些步骤，这些步骤既繁琐又容易出错。本章的*将LLMOps添加到LLM Twin*部分将教会我们如何使用GitHub
    Actions在CD管道中自动化这些步骤。然而，我们首先想手动完成它们，以便完全理解幕后过程，而不是将其视为黑盒。理解这些细节对于调试您的CI/CD管道至关重要，在那里您必须理解错误消息以及如何修复它们。
- en: Now that we have built our Docker image and pushed it to AWS ECR, let’s deploy
    it to AWS.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了Docker镜像并将其推送到AWS ECR，让我们将其部署到AWS。
- en: Run the pipelines on AWS
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在AWS上运行管道
- en: 'We are very close to running the ML pipelines on AWS, but we have to go through
    a few final steps. Let’s switch from the default ZenML stack to the AWS one we
    created in this chapter. From the root of your project, run the following in the
    CLI:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将在AWS上运行ML管道，但我们必须完成几个最终步骤。让我们从默认的ZenML堆栈切换到本章中创建的AWS堆栈。从项目的根目录，在CLI中运行以下命令：
- en: '[PRE16]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Return to your AWS ECR ZenML repository and copy the image URI as shown in
    *Figure 11.9*. Then, go to the `configs` directory, open the `configs/end_to_end_data.yaml`
    file, and update the `settings.docker.parent_image` attribute with your ECR URL,
    as shown below:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 返回您的AWS ECR ZenML存储库，并按照图11.9所示复制镜像URI。然后，转到`configs`目录，打开`configs/end_to_end_data.yaml`文件，并将`settings.docker.parent_image`属性更新为您的ECR
    URL，如下所示：
- en: '[PRE17]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We’ve configured the pipeline to always use the latest Docker image available
    in ECR. This means that the pipeline will automatically pick up the latest changes
    made to the code whenever we push a new image.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已配置管道始终使用ECR中可用的最新Docker镜像。这意味着每当推送新镜像时，管道将自动获取代码的最新更改。
- en: 'We must export all the credentials from our `.env` file to ZenML secrets, a
    feature that safely stores your credentials and makes them accessible within your
    pipelines:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将`.env`文件中的所有凭证导出到ZenML密钥，这是一个安全存储您的凭证并在管道中使其可访问的功能：
- en: '[PRE18]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The last step is setting up to run the pipelines asynchronously so we don’t
    have to wait until they are finished, which might result in timeout errors:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是设置异步运行管道，这样我们就不必等待它们完成，这可能会导致超时错误：
- en: '[PRE19]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that ZenML knows to use the AWS stack, our custom Docker image, and has
    access to our credentials, we are finally done with the setup. Run the `end-to-end-data-pipeline`
    with the following command:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，ZenML知道使用AWS堆栈、我们的自定义Docker镜像，并且可以访问我们的凭证，我们终于完成了设置。使用以下命令运行`end-to-end-data-pipeline`：
- en: '[PRE20]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now you can go to **ZenML Cloud** **→** **Pipelines** **→** **end_to_end_data**
    and open the latest run. On the ZenML dashboard, you can visualize the latest
    state of the pipeline, as seen in *Figure 11.10*. Note that this pipeline runs
    all the data-related pipelines in a single run.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以前往**ZenML Cloud** **→** **Pipelines** **→** **end_to_end_data**并打开最新运行实例。在ZenML仪表板上，您可以可视化管道的最新状态，如图11.10所示。请注意，此管道在单个运行中运行所有与数据相关的管道。
- en: In the *Adding LLMOps to the LLM Twin*section, we will explain why we compressed
    all the steps into a single pipeline.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在*将LLMOps添加到LLM Twin*部分，我们将解释为什么我们将所有步骤压缩到单个管道中。
- en: '![](img/B31105_11_10.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_10.png)'
- en: 'Figure 11.10: ZenML example of running the end-to-end-data-pipeline'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10：ZenML端到端数据管道示例
- en: 'You can click on any running block and find details about the run, the code
    used for that specific step, and the logs for monitoring and debugging, as illustrated
    in *Figure 11.11*:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以点击任何正在运行的块，找到有关运行、用于该特定步骤的代码以及用于监控和调试的日志的详细信息，如图11.11所示：
- en: '![](img/B31105_11_11.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_11.png)'
- en: 'Figure 11.11: ZenML step metadata example'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11：ZenML步骤元数据示例
- en: To run other pipelines, you have to update the `settings.docker.parent_image`
    attribute in their config file under the `configs/` directory.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行其他管道，您必须更新`configs/`目录下其配置文件中的`settings.docker.parent_image`属性。
- en: To find even more details about the runs, you can go to AWS SageMaker. In the
    left panel, click **SageMaker dashboard**, and on the right, in the **Processing**
    column, click on the green **Running** section, as shown in *Figure 11.12*.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找有关运行的更多详细信息，您可以访问 AWS SageMaker。在左侧面板中，点击 **SageMaker 仪表板**，然后在右侧的 **处理**
    列表中，点击如 *图 11.12* 所示的绿色 **运行** 部分。
- en: This will open a list of all the **processing jobs** that execute your ZenML
    pipelines.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个包含执行您的 ZenML 管道的所有 **处理作业** 的列表。
- en: '![](img/B31105_11_12.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_12.png)'
- en: 'Figure 11.12: SageMaker dashboard'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：SageMaker 仪表板
- en: 'If you want to run the pipelines locally again, use the following CLI command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想再次在本地运行管道，请使用以下 CLI 命令：
- en: '[PRE21]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you want to disconnect from the ZenML cloud dashboard and use the local
    version again, run the following:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想从 ZenML 云仪表板断开连接并再次使用本地版本，请运行以下命令：
- en: '[PRE22]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Troubleshooting the ResourceLimitExceeded error after running a ZenML pipeline
    on SageMaker
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 SageMaker 上运行 ZenML 管道后解决 ResourceLimitExceeded 错误
- en: Let’s assume, you’ve encountered a **ResourceLimitExceeded** error after running
    a ZenML pipeline on SageMaker using the AWS stack. In this case, you have to explicitly
    ask AWS to give you access to a specific type of AWS EC2 VM.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，您在 SageMaker 上使用 AWS 栈运行 ZenML 管道后遇到了 **ResourceLimitExceeded** 错误。在这种情况下，您必须明确要求
    AWS 给您访问特定类型的 AWS EC2 VM 的权限。
- en: ZenML uses, by default, `ml.t3.medium` EC2 machines, which are part of the AWS
    freemium tier. However, some AWS accounts cannot access these VMs by default.
    To check your access, search your AWS console for **Service Quotas**.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML 默认使用 `ml.t3.medium` EC2 机器，这是 AWS 免费增值层的一部分。然而，一些 AWS 账户默认无法访问这些 VM。要检查您的访问权限，请在
    AWS 控制台中搜索 **服务配额**。
- en: Then, in the left panel, click on **AWS services**, search for **Amazon SageMaker**,
    and then for `ml.t3.medium`. In *Figure 11.13*, you can see our quotas for these
    types of machines. If yours is **0**, you should request that AWS increase them
    to numbers similar to those from *Figure 11.13* in the **Applied account-level
    quota value** column. The whole process is free of charge and only requires a
    few clicks. Unfortunately, you might have to wait for a few hours up to one day
    until AWS accepts your request.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在左侧面板中，点击 **AWS 服务**，搜索 **Amazon SageMaker**，然后搜索 `ml.t3.medium`。在 *图 11.13*
    中，您可以查看这些类型机器的配额。如果您的配额是 **0**，您应该在 **应用账户级配额值** 列中请求 AWS 将其增加到与 *图 11.13* 中类似的数字。整个过程免费，只需点击几下即可。不幸的是，您可能需要等待几小时到一天，直到
    AWS 接受您的请求。
- en: '![](img/B31105_11_13.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_13.png)'
- en: 'Figure 11.13: SageMaker—ml.t3.medium expected quotas'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13：SageMaker—ml.t3.medium 预期配额
- en: 'You can find step-by-step instructions on how to solve this error and request
    new quotas at this link: [https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error](https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接中找到如何解决此错误并请求新配额的逐步说明：[https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error](https://repost.aws/knowledge-center/sagemaker-resource-limit-exceeded-error)。
- en: 'If you changed the values from your .env file and want to update the ZenML
    secrets with them, first run the following CLI command to delete the old secrets:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已从 .env 文件更改了值并希望使用它们更新 ZenML 密钥，请首先运行以下 CLI 命令以删除旧密钥：
- en: '[PRE23]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, you can export them again by running:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以通过运行以下命令再次导出它们：
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Adding LLMOps to the LLM Twin
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 LLMOps 添加到 LLM Twin
- en: In the previous section, we saw how to set up the infrastructure for the LLM
    Twin project by manually building the Docker image and pushing it to ECR. We want
    to automate the entire process and implement a CI/CD pipeline using GitHub Actions
    and a CT pipeline using ZenML. As mentioned earlier, implementing a CI/CD/CT pipeline
    ensures that each feature pushed to main branches is consistent and tested. Also,
    by automating the deployment and training, you support collaboration, save time,
    and reduce human errors.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了如何通过手动构建 Docker 镜像并将其推送到 ECR 来手动设置 LLM Twin 项目的基础设施。我们希望自动化整个流程，并使用
    GitHub Actions 实现一个 CI/CD 管道，使用 ZenML 实现一个 CT 管道。如前所述，实现 CI/CD/CT 管道确保推送到主分支的每个功能都是一致的并且经过测试。此外，通过自动化部署和训练，您支持协作，节省时间，并减少人为错误。
- en: Finally, at the end of the section, we will show you how to implement a prompt
    monitoring pipeline using Opik from Comet ML and an alerting system using ZenML.
    This prompt monitoring pipeline will help us debug and analyze the RAG and LLM
    logic. As LLM systems are non-deterministic, capturing and storing the prompt
    traces is essential for monitoring your ML logic.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本节结束时，我们将向您展示如何使用 Comet ML 的 Opik 实现一个提示监控流程，以及使用 ZenML 实现一个警报系统。这个提示监控流程将帮助我们调试和分析
    RAG 和 LLM 逻辑。由于 LLM 系统是非确定性的，捕获和存储提示跟踪对于监控您的 ML 逻辑至关重要。
- en: Before diving into the implementation, let’s start with a quick section on the
    LLM Twin’s CI/CD pipeline flow.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实施之前，让我们先快速了解一下 LLM Twin 的 CI/CD 流程。
- en: LLM Twin’s CI/CD pipeline flow
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM Twin 的 CI/CD 流程
- en: 'We have two environments: staging and production. When developing a new feature,
    we create a new branch out of the staging branch and develop solely on that one.
    When we are done and consider the feature finished, we open a **pull request**
    (**PR**) to the staging branch. After the feature branch is accepted, it is merged
    into the staging branch. This is a standard workflow in most software applications.
    There might be variations, like adding a dev environment, but the principles remain
    the same.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个环境：预发布和产品。当开发一个新功能时，我们从预发布分支创建一个新的分支，并在该分支上独立开发。当我们完成并认为功能完成时，我们向预发布分支打开一个**拉取请求**（**PR**）。在功能分支被接受后，它会被合并到预发布分支。这是大多数软件应用中的标准工作流程。可能会有一些变化，比如添加一个开发环境，但原则保持不变。
- en: 'As illustrated in *Figure 11.14*, the CI pipeline is triggered when the PR
    opens. At this point, we test the feature branch for linting and formatting errors.
    Also, we run a `gitleaks` command to check for credentials and sensitive information
    that was committed by mistake. If the linting, formatting, and gitleaks steps
    pass (also known as static analysis), we run the automated tests. Note that the
    static analysis steps run faster than the automated tests. Thus, the order matters.
    That’s why adding the static analysis steps at the beginning of the CI pipeline
    is good practice. We propose the following order of the CI steps:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 11.14* 所示，CI 流程在 PR 打开时触发。在这个时候，我们测试功能分支的代码风格和格式错误。同时，我们运行 `gitleaks` 命令来检查是否不小心提交了凭据和敏感信息。如果代码风格、格式和
    gitleaks 步骤通过（也称为静态分析），我们将运行自动测试。请注意，静态分析步骤比自动测试运行得快。因此，顺序很重要。这就是为什么在 CI 流程的开始处添加静态分析步骤是良好实践的原因。我们建议以下
    CI 步骤的顺序：
- en: '`gitleaks` checks'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gitleaks` 检查'
- en: Linting checks
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查代码风格
- en: Formatting checks
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式检查
- en: Automated testing, such as unit and integration tests
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动测试，如单元测试和集成测试
- en: If any check fails, the CI pipeline fails, and the developer who created the
    PR cannot merge it into the staging branch until it fixes the issues.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何检查失败，CI 流程将失败，创建 PR 的开发者无法在修复问题之前将其合并到预发布分支。
- en: Implementing a CI pipeline ensures that new features follow the repository’s
    standards and don’t break existing functionality. The exact process repeats when
    we plan to merge the staging branch into the production one. We open a PR, and
    the CI pipeline is automatically executed before merging the staging branch into
    production.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 实施一个 CI 流程确保新功能遵循存储库的标准，并且不会破坏现有功能。当我们计划将预发布分支合并到产品分支时，确切的过程会重复。我们打开一个 PR，在将预发布分支合并到产品分支之前，CI
    流程会自动执行。
- en: '![](img/B31105_11_14.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.14](img/B31105_11_14.png)'
- en: 'Figure 11.14: CI/CD pipelines flow'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14：CI/CD 流程
- en: The CD pipeline runs after the branch is merged. For example, after the feature
    branch is merged into staging, the CD pipeline takes the code from the staging
    branch, builds a new Docker image, and pushes it to the AWS ECR Docker repository.
    When running future pipeline runs in the staging environment, it will use the
    latest Docker image that was built by the CD pipeline. The exact process happens
    between staging and production. Still, the key difference is that the staging
    environment exists as an experimental place where the QA team and stakeholders
    can further manually test the new feature along with what is automatically tested
    in the CI pipeline.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: CD 流程在分支合并后运行。例如，在功能分支合并到预发布分支后，CD 流程从预发布分支获取代码，构建一个新的 Docker 镜像，并将其推送到 AWS
    ECR Docker 仓库。当在预发布环境中运行未来的流程运行时，它将使用由 CD 流程构建的最新 Docker 镜像。确切的过程发生在预发布和产品之间。尽管如此，关键的区别在于预发布环境作为一个实验场所存在，QA
    团队和利益相关者可以进一步手动测试新功能，同时自动测试 CI 流程中的内容。
- en: In our repository, we used only a main branch, which reflects production, and
    feature branches to push new work. We did this to keep things simple, but the
    same principles apply. To extend the flow, you must create a staging branch and
    add it to the CD pipeline.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的仓库中，我们只使用了主分支，它反映了生产环境，以及功能分支来推送新工作。我们这样做是为了保持简单，但相同的原理适用。要扩展流程，您必须创建一个预发布分支并将其添加到
    CD 管道中。
- en: More on formatting errors
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多关于格式化错误的说明
- en: Formatting errors relate to the style and structure of your code, ensuring that
    it adheres to a consistent visual layout. This can include the placement of spaces,
    indentation, line length, and other stylistic elements.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 格式化错误与代码的风格和结构有关，确保它遵循一致的视觉布局。这可能包括空格的位置、缩进、行长度和其他风格元素。
- en: 'The main purpose of formatting is to make your code more readable and maintainable.
    Consistent formatting helps teams work together more effectively, as the code
    looks uniform, regardless of who wrote it. Examples of formatting errors are:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 格式化的主要目的是使您的代码更易于阅读和维护。一致的格式化有助于团队更有效地协作，因为代码看起来统一，无论谁编写它。格式化错误的例子包括：
- en: Incorrect indentation (e.g., mixing spaces and tabs)
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误的缩进（例如，混合空格和制表符）
- en: Lines that are too long (e.g., exceeding `79` or `88` characters, depending
    on your style guide)
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行太长（例如，超过 `79` 或 `88` 个字符，具体取决于您的风格指南）
- en: Missing or extra spaces around operators or after commas
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运算符或逗号周围缺少或多余的空格
- en: More on linting errors
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多关于代码检查错误的说明
- en: Linting errors relate to potential issues in your code that could lead to bugs,
    inefficiencies, or non-adherence to coding standards beyond just style. Linting
    checks often involve static analysis of the code to catch things like unused variables,
    undefined names, or questionable practices.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 代码检查错误与可能导致错误、低效或不遵守编码标准的潜在问题有关，而不仅仅是风格问题。代码检查通常涉及对代码的静态分析，以捕捉未使用的变量、未定义的名称或可疑的实践。
- en: 'Linting’s main goal is to catch potential errors or bad practices early in
    the development process, improving code quality and reducing the likelihood of
    bugs. Examples of linting errors are:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 代码检查的主要目标是尽早在开发过程中捕捉潜在的错误或不良实践，提高代码质量并降低出现错误的可能性。代码检查错误的例子包括：
- en: Unused imports or variables
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 未使用的导入或变量
- en: Undefined variables or functions are being used
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在使用未定义的变量或函数
- en: Potentially dangerous code (e.g., using `==` instead of `is` for checking against
    `None`)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能危险的代码（例如，使用 `==` 而不是 `is` 来检查 `None`）
- en: We use Ruff, a versatile tool for formatting and linting. It incorporates checks
    for common formatting issues and PEP 8 compliance, as well as deeper linting checks
    for potential errors and code quality problems. Also, it is written in Rust, making
    it fast for big codebases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Ruff，一个用于格式化和代码检查的多功能工具。它包含了对常见格式问题的检查以及 PEP 8 规范的遵守，同时还进行了更深入的代码质量检查，以发现潜在的错误和质量问题。此外，它是用
    Rust 编写的，这使得它在处理大型代码库时非常快速。
- en: Before implementing what we’ve explained above, let’s examine the core principles
    of GitHub Actions.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施上述内容之前，让我们先来探讨 GitHub Actions 的核心原则。
- en: Quick overview of GitHub Actions
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GitHub Actions 快速概述
- en: 'GitHub Actions is a CI/CD platform provided by GitHub that allows developers
    to automate their workflows directly within a GitHub repository. It enables users
    to build, test, and deploy their code directly from GitHub by defining workflows
    in YAML files. Since it’s part of GitHub, it works seamlessly with repositories,
    issues, PRs, and other GitHub features. Here are the key components you should
    know about:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Actions 是 GitHub 提供的一个 CI/CD 平台，允许开发者在 GitHub 仓库内直接自动化他们的工作流程。它通过在 YAML
    文件中定义工作流程，使用户能够直接从 GitHub 构建代码、测试和部署代码。由于它是 GitHub 的一部分，它可以与仓库、问题、PR 和其他 GitHub
    功能无缝工作。以下是您应该了解的关键组件：
- en: '**Workflows:** A workflow is an automated process defined in a YAML file located
    in your repository’s `.github/workflows directory`. It specifies what should happen
    (e.g., `build`, `test`, and `deploy`) and when (e.g., on push, on PR).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作流程：** 工作流程是一个在您仓库的 `.github/workflows` 目录中定义的自动化过程。它指定了应该发生什么（例如，`build`、`test`
    和 `deploy`）以及何时发生（例如，在推送时，在 PR 时）。'
- en: '**Jobs:** Workflows are made up of jobs, which are groups of steps that execute
    on the same runner. Each job runs in its own virtual environment.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作业：** 工作流程由作业组成，作业是在同一运行器上执行的步骤组。每个作业都在自己的虚拟环境中运行。'
- en: '**Steps:** Jobs are made up of multiple independent steps, which can be actions
    or shell commands.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤：** 任务由多个独立的步骤组成，这些步骤可以是操作或 shell 命令。'
- en: '**Actions:** Actions are reusable commands or scripts. You can use pre-built
    actions from GitHub Marketplace or create your own. You can think of them as Python
    functions.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：操作是可重用的命令或脚本。您可以使用 GitHub 市场中的预构建操作或创建自己的操作。您可以将它们视为 Python 函数。'
- en: '**Runners:** Runners are the servers that run your jobs. GitHub provides hosted
    runners (Linux, Windows, macOS), or you can even self-host your runners.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行者**：运行者是指运行您作业的服务器。GitHub 提供托管运行者（Linux、Windows、macOS），或者您甚至可以自行托管运行者。'
- en: 'A workflow is described using YAML syntax. For example, a simple workflow that
    clones the current GitHub repository and installs Python 3.11 on an Ubuntu machine
    looks like this:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 YAML 语法描述工作流程。例如，一个简单的示例工作流程，它克隆当前的 GitHub 存储库并在 Ubuntu 机器上安装 Python 3.11，看起来像这样：
- en: '[PRE25]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The workflows are triggered by events like `push`, `pull_request`, or `schedule`.
    For example, you might trigger a workflow every time code is pushed to a specific
    branch. Now that we understand how GitHub Actions works, let’s look at the LLM
    Twin’s CI pipeline.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程由诸如 `push`、`pull_request` 或 `schedule` 之类的事件触发。例如，您可能会在将代码推送到特定分支时触发工作流程。现在我们了解了
    GitHub Actions 的工作原理，让我们看看 LLM Twin 的 CI 管道。
- en: The CI pipeline
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CI 管道
- en: 'The LLM Twin’s CI pipeline is split into two jobs:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: LLM Twin 的 CI 管道分为两个作业：
- en: A **QA job** that looks for formatting and linting errors using Ruff. Also,
    it runs a `gitleaks` step to scan for leaked secrets throughout our repository.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用 Ruff 查找格式化和 linting 错误的 **QA 作业**。它还运行一个 `gitleaks` 步骤，以扫描整个存储库中的泄露秘密。
- en: A **test job** that runs all our automatic tests using `Pytest`. In our use
    case, we implemented just a dummy test to showcase the CI pipeline, but using
    the structure from this book, you can easily extend it with real tests for your
    use case.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个运行所有自动测试的 **测试作业**，使用 `Pytest`。在我们的用例中，我们仅实现了一个模拟测试来展示 CI 管道，但使用本书中的结构，您可以轻松地扩展它以适应您的实际测试用例。
- en: GitHub Actions CI YAML file
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GitHub Actions CI YAML 文件
- en: '[PRE26]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `concurrency` section ensures that only one instance of this workflow runs
    for a given reference (like a branch) at any given time. The `group` field is
    defined using GitHub’s expression syntax to create a unique group name based on
    the workflow and the reference. The `cancel-in-progress: true` line ensures that
    if a new workflow run is triggered before the previous one finishes, the previous
    run is canceled. This is particularly useful to prevent redundant executions of
    the same workflow.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrency` 部分确保在任何给定时间只有一个工作流程实例在给定的参考（如分支）上运行。`group` 字段使用 GitHub 的表达式语法定义，以根据工作流程和参考创建唯一的组名。`cancel-in-progress:
    true` 行确保如果在新工作流程运行完成之前触发新的工作流程运行，则取消先前的运行。这特别有用，可以防止同一工作流程的冗余执行。'
- en: '[PRE28]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The workflow defines two separate jobs: `qa` and `test`. Each job runs on the
    latest version of Ubuntu, specified by `runs-on: ubuntu-latest`.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '工作流程定义了两个独立的作业：`qa` 和 `test`。每个作业都在由 `runs-on: ubuntu-latest` 指定的最新版本的 Ubuntu
    上运行。'
- en: '**The first job**, named `QA`, is responsible for quality assurance tasks like
    code checks and formatting verification. Within the `qa` job, the first step is
    to check out the repository’s code using the `actions/checkout@v3` action. This
    step is necessary to ensure that the job has access to the code that needs to
    be analyzed.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一个作业**，命名为 `QA`，负责质量保证任务，如代码检查和格式验证。在 `qa` 作业中，第一步是使用 `actions/checkout@v3`
    动作检出存储库的代码。这一步是必要的，以确保作业可以访问需要分析的代码。'
- en: '[PRE29]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The next step is to set up the Python environment. This is done using the `actions/setup-python@v3`
    action, with the Python version specified as `"3.11"`. This step ensures that
    the subsequent steps in the job will run in the correct Python environment.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是设置 Python 环境。这是通过使用 `actions/setup-python@v3` 动作来完成的，Python 版本指定为 `"3.11"`。这一步确保作业中的后续步骤将在正确的
    Python 环境中运行。
- en: '[PRE30]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The workflow then installs Poetry using the `abatilo/actions-poetry@v2` action,
    specifying the version of Poetry as `1.8.3`:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 然后工作流程使用 `abatilo/actions-poetry@v2` 动作安装 Poetry，指定 Poetry 版本为 `1.8.3`：
- en: '[PRE31]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once Poetry is set up, the workflow installs the project’s development dependencies
    using the `poetry install --only dev` command. Additionally, the workflow adds
    the `poethepoet` plugin for Poetry, which will be used to run predefined tasks
    more conveniently within the project.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Poetry 设置完成后，工作流程使用 `poetry install --only dev` 命令安装项目的开发依赖项。此外，工作流程添加了用于在项目中更方便地运行预定义任务的
    `poethepoet` 插件。
- en: '[PRE32]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `qa` job then runs several quality checks on the code. The first check
    uses a tool called `gitleaks` to scan for secrets in the codebase, ensuring that
    no sensitive information is accidentally committed:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`qa`作业随后在代码上运行几个质量检查。第一个检查使用一个名为`gitleaks`的工具来扫描代码库中的秘密，确保没有敏感信息被意外提交：'
- en: '[PRE33]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Following the `gitleaks` check, the workflow runs a linting process to enforce
    coding standards and best practices in the Python code. This is achieved through
    the `poetry poe lint-check` command, which uses Ruff under the hood.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行`gitleaks`检查之后，工作流运行一个linting过程来强制执行Python代码的编码标准和最佳实践。这是通过使用底层的`poetry poe
    lint-check`命令来实现的。
- en: '[PRE34]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The last step in the `qa` job is a format check, which ensures that the Python
    code is properly formatted according to the project’s style guidelines. This is
    done using the `poetry poe format-check` command, which uses Ruff under the hood.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '`qa`作业的最后一步是格式检查，确保Python代码根据项目的风格指南正确格式化。这是通过使用底层的`poetry poe format-check`命令来完成的。'
- en: '[PRE35]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The **second job** defined in the workflow is the `test` job, which also runs
    on the latest version of Ubuntu. Like the `qa` job, it starts by checking out
    the code from the repository and installing Python 3.11 and Poetry 1.8.3.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流中定义的**第二个作业**是`test`作业，它也在最新的Ubuntu版本上运行。与`qa`作业类似，它首先从仓库检出代码并安装Python 3.11和Poetry
    1.8.3。
- en: '[PRE36]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: After setting up the system dependencies, the `test` job installs all the project’s
    dependencies with the `poetry install` command. As we want to run the tests, this
    time, we need to install all the dependencies that are required to run the application.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置系统依赖项之后，`test`作业使用`poetry install`命令安装项目的所有依赖项。由于我们这次需要运行测试，因此需要安装所有运行应用程序所需的依赖项。
- en: '[PRE37]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Finally, the `test` job runs the project’s tests using the `poetry poe test`
    command. This step ensures that all tests are executed and provides feedback on
    whether the current code changes break any functionality.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`test`作业使用`poetry poe test`命令运行项目的测试。这一步确保所有测试都执行，并提供有关当前代码更改是否破坏任何功能的反馈。
- en: '[PRE38]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If any of the steps from the QA or test jobs fail, the GitHub Actions workflow
    will fail, resulting in the PR not being able to be merged until the issue is
    fixed. By taking this approach, we ensure that all the new features added to the
    main branches respect the standard of the project and that it doesn’t break existing
    functionality through automated tests.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 如果QA或测试作业中的任何步骤失败，GitHub Actions工作流将失败，导致PR无法合并，直到问题得到修复。通过采取这种方法，我们确保所有添加到主分支的新功能都符合项目标准，并且通过自动化测试不会破坏现有功能。
- en: '*Figure 11.15* shows the CI pipeline in the **Actions** tab of the GitHub repository.
    It was run after a commit with the message **feat: Add Docker image and CD pipeline**
    and ran the two jobs described above, QA and Test.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.15*显示了GitHub仓库**操作**标签页中的CI管道。它是在带有消息**feat: 添加Docker镜像和CD管道**的提交后运行的，并运行了上述描述的两个作业，即QA和Test。'
- en: '![](img/B31105_11_15.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![B31105_11_15](img/B31105_11_15.png)'
- en: 'Figure 11.15: GitHub Actions CI pipeline run example'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：GitHub Actions CI管道运行示例
- en: The CD pipeline
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CD管道
- en: 'The CD pipeline will automate the Docker steps we manually performed in the
    **Deploying the LLM Twin’s pipelines to the cloud** section, which are:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: CD管道将自动化我们在**将LLM Twin的管道部署到云**部分手动执行的Docker步骤，这些步骤包括：
- en: Set up Docker.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置Docker。
- en: Log in to AWS.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 登录AWS。
- en: Build the Docker image.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Docker镜像。
- en: Push the Docker image to AWS ECR.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Docker镜像推送到AWS ECR。
- en: 'With that in mind, let’s look at the GitHub Actions YAML file, which sits under
    `.github/workflows/cd.yaml`. It begins by naming the workflow `CD` and specifying
    the trigger for this workflow. The trigger is any push to the repository’s main
    branch. This workflow will automatically run when new code is pushed to the main
    branch, usually when a PR is merged into the main branch. The `on.push` configuration
    sets up the trigger:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们看看位于`.github/workflows/cd.yaml`下的GitHub Actions YAML文件。它首先将工作流命名为`CD`，并指定此工作流的触发器。触发器是向仓库主分支的任何推送。当新代码推送到主分支时，通常在PR合并到主分支时，此工作流将自动运行。`on.push`配置设置了触发器：
- en: '[PRE39]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The workflow then defines a single job named `Build & Push Docker Image`:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流随后定义了一个名为`Build & Push Docker Image`的单个作业：
- en: '[PRE40]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The first step within the job is to check out the repository’s code.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 作业内的第一步是检出仓库的代码。
- en: '[PRE41]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After checking out the code, the workflow sets up docker buildx, a Docker CLI
    plugin that extends Docker’s build capabilities with features like multi-platform
    builds and cache import/export:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 检出代码后，工作流程会设置 docker buildx，这是一个 Docker CLI 插件，它通过多平台构建和缓存导入/导出等功能扩展了 Docker
    的构建能力：
- en: '[PRE42]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The next step involves configuring the AWS credentials. This step is crucial
    for interacting with AWS services, such as Amazon **Elastic Container Registry**
    (**ECR**), where the Docker images will be pushed. The AWS access key, secret
    access key, and region are securely retrieved from the repository’s secrets to
    authenticate the workflow with AWS. This ensures the workflow has the necessary
    permissions to push Docker images to the ECR repository. We will show you how
    to configure these secrets after wrapping up with the YAML file:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及配置 AWS 凭据。这一步对于与 AWS 服务交互至关重要，例如 Amazon **弹性容器注册表**（**ECR**），其中将推送 Docker
    镜像。AWS 访问密钥、秘密访问密钥和区域从存储库的秘密中安全检索，以使用 AWS 验证工作流程。这确保了工作流程具有将 Docker 镜像推送到 ECR
    存储库所必需的权限。在完成 YAML 文件后，我们将向您展示如何配置这些秘密：
- en: '[PRE43]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Once the AWS credentials are configured, the workflow logs in to Amazon ECR.
    This step is essential for authenticating the Docker CLI with the ECR registry,
    allowing subsequent steps to push images to the registry:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了 AWS 凭据，工作流程就会登录到 Amazon ECR。这一步对于使用 Docker CLI 验证 ECR 注册表至关重要，允许后续步骤将镜像推送到注册表：
- en: '[PRE44]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The final step in the workflow involves building the Docker image and pushing
    it to the Amazon ECR repository. This is accomplished using the `docker/build-push-action@v6`
    action. The `context` specifies the build context, which is typically the repository’s
    root directory. The `file` option points to the `Dockerfile`, which defines how
    the image should be built. The `tags` section assigns tags to the image, including
    the specific commit SHA and the `latest` tag, which is a common practice for identifying
    the most recent version of the image. The `push` option is set to `true`, meaning
    the image will be uploaded to ECR after it is built:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程中的最后一步是构建 Docker 镜像并将其推送到 Amazon ECR 存储库。这是通过使用 `docker/build-push-action@v6`
    动作来完成的。`context` 指定构建上下文，通常是存储库的根目录。`file` 选项指向 `Dockerfile`，它定义了镜像应该如何构建。`tags`
    部分将标签分配给镜像，包括特定的提交 SHA 和 `latest` 标签，这是标识镜像最新版本的常见做法。`push` 选项设置为 `true`，这意味着在构建完成后镜像将被上传到
    ECR：
- en: '[PRE45]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: To conclude, the CD pipeline authenticates to AWS, builds the Docker image,
    and pushes it to AWS ECR. The Docker image is pushed with `latest` and the commit’s
    SHA tag. By doing so, we can always use the latest image and point to the commit
    of the code from which the image was generated.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，CD 流水线验证 AWS，构建 Docker 镜像，并将其推送到 AWS ECR。Docker 镜像使用 `latest` 和提交的 SHA
    标签进行推送。这样做，我们总能使用最新镜像并指向生成镜像的代码提交。
- en: Also, in our code, we have only a main branch, which reflects our production
    environment. But you, as a developer, have the power to extend this functionality
    with a staging and dev environment. You just have to add the name of the branches
    in the `on.push.branches` configuration at the beginning of the YAML file.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在我们的代码中，我们只有一个主分支，这反映了我们的生产环境。但作为开发者，你有权通过添加预发布和开发环境来扩展这一功能。你只需在 YAML 文件的开头添加
    `on.push.branches` 配置中的分支名称即可。
- en: In *Figure 11.16*, you can observe how the CD pipeline looks after a PR is merged
    into the production branch. As seen before, we only have the **Build & Push Docker
    Image** jobhere.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 11.16* 中，你可以观察到在合并到生产分支后 CD 流水线的样子。正如之前所见，我们这里只有 **构建并推送 Docker 镜像** 作业。
- en: '![](img/B31105_11_16.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_16.png)'
- en: 'Figure 11.16: GitHub Actions CD pipeline run example'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16：GitHub Actions CD 流水线运行示例
- en: The last step in setting up the CI/CD pipeline is to test it and see how it
    works.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 CI/CD 流水线的最后一步是测试它并查看其工作情况。
- en: Test out the CI/CD pipeline
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 CI/CD 流水线
- en: 'To test the CI/CD pipelines yourself, you must fork the LLM-Engineering repository
    to have full *write* access to the GitHub repository. Here is the official tutorial
    on how to fork a GitHub project: [https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 要自己测试 CI/CD 流水线，你必须将 LLM-Engineering 仓库分叉以获得对 GitHub 仓库的完全 *写入* 权限。以下是官方教程，介绍如何分叉
    GitHub 项目：[https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo)
- en: The last step is to set up a few secrets that will allow the CD pipeline to
    log in to AWS and point to the right ECR resource. To do so, go to the **Settings**
    tab at the top of the forked repository in GitHub. In the left panel, in the **Security**
    section, click on the **Secrets and Variables** toggle and, finally, on **Actions**.
    Then, on the **Secrets** tab, create four repository secrets, as shown in *Figure
    11.17*. These secrets will be securely stored and accessible only by the GitHub
    Actions CD pipeline.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是设置几个密钥，以便 CD 管道能够登录 AWS 并指向正确的 ECR 资源。为此，请转到 GitHub 分支仓库顶部的 **设置** 选项卡。在左侧面板中，在
    **安全** 部分点击 **密钥和变量** 切换，最后点击 **操作**。然后，在 **密钥** 选项卡中，创建四个仓库密钥，如图 11.17 所示。这些密钥将安全存储，并且只能由
    GitHub Actions CD 管道访问。
- en: The `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` are the AWS credentials
    you used across the book. In *Chapter 2*, you see how to create them. The `AWS_REGION`
    (e.g., `eu-central-1`) and `AWS_ECR_NAME` are the same ones used in the **Deploying
    the LLM Twin’s pipelines** to the cloud section.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '`AWS_ACCESS_KEY_ID` 和 `AWS_SECRET_ACCESS_KEY` 是你在本书中使用的 AWS 凭据。在 *第 2 章* 中，你看到了如何创建它们。`AWS_REGION`（例如，`eu-central-1`）和
    `AWS_ECR_NAME` 与在 **将 LLM Twin 的管道部署到云** 部分中使用的相同。'
- en: 'For the `AWS_ECR_NAME`, you should configure only the name of the repository
    (e.g., `zenml-vrsopg`) and not the full URI (e.g., [992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg](https://992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg)),
    as seen in the image below:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `AWS_ECR_NAME`，你应该只配置仓库的名称（例如，`zenml-vrsopg`），而不是完整的 URI（例如，[992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg](https://992382797823.dkr.ecr.eu-central-1.amazonaws.com/zenml-vrsopg)），如图下所示：
- en: '![](img/B31105_11_17.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B31105_11_17.png](img/B31105_11_17.png)'
- en: 'Figure 11.17: Configuring only repository name'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17：仅配置仓库名称
- en: To trigger the CI pipeline, create a feature branch, modify the code or documentation,
    and create a PR to the main branch. To trigger the CD pipeline, merge the PR into
    the main branch.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 要触发 CI 管道，创建一个功能分支，修改代码或文档，并向主分支创建一个 PR。要触发 CD 管道，将 PR 合并到主分支。
- en: After the CD GitHub Actions are complete, check the ECR repository to see whether
    the Docker image was pushed successfully.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: CD GitHub Actions 完成后，检查 ECR 仓库以查看 Docker 镜像是否成功推送。
- en: '![](img/B31105_11_18.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![图片 B31105_11_18.png](img/B31105_11_18.png)'
- en: 'Figure 11.18: GitHub Actions secrets'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18：GitHub Actions 密钥
- en: 'If you need more details on how to set up GitHub Actions secrets, we recommend
    checking out their official documentation: [https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要更多关于如何设置 GitHub Actions 密钥的详细信息，我们建议查看他们的官方文档：[https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions](https://docs.github.com/en/actions/security-for-github-actions/security-guides/using-secrets-in-github-actions)
- en: The CT pipeline
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CT 管道
- en: To implement the CT pipeline, we will leverage ZenML. Once ZenML (or other orchestrators
    such as Metaflow, Dagster, or Airflow) orchestrates all your pipelines and your
    infrastructure is deployed, you are very close to reaching CT.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 CT 管道，我们将利用 ZenML。一旦 ZenML（或其他编排器，如 Metaflow、Dagster 或 Airflow）编排了所有管道并且你的基础设施已部署，你就可以非常接近实现
    CT。
- en: Remember the core difference between the CI/CD and CT pipelines. The CI/CD pipeline
    takes care of testing, building, and deploying your code—a dimension that any
    software program has. The CT pipeline leverages the code managed by the CI/CD
    pipeline to automate your data, training, and model-serving process, where the
    data and model dimensions are present only in the AI world.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 CI/CD 和 CT 管道之间的核心区别。CI/CD 管道负责测试、构建和部署你的代码——这是任何软件程序都有的一个维度。CT 管道利用 CI/CD
    管理的代码来自动化你的数据、训练和模型托管过程，其中数据和模型维度仅存在于 AI 世界中。
- en: 'Before diving into the implementation, we want to highlight two design choices
    that made reaching CT simple:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入实施之前，我们想强调两个设计选择，这使得实现 CT 变得简单：
- en: '**The FTI architecture:** A modular system with clear interfaces and components
    made it easy to capture the relationship between the pipelines and automate them.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FTI 架构**：一个具有清晰接口和组件的模块化系统，使得捕获管道之间的关系并自动化它们变得容易。'
- en: '**Starting with an orchestrator since day 0:** We started with ZenML at the
    beginning of the project’s development. Early on, we only used it locally. But
    it acted as an entry point for our pipelines and a way to monitor their execution.
    Doing so forced us to decouple each pipeline and transfer the communication between
    them solely through various types of data storage, such as the data warehouse,
    feature store, or artifact store. As we have leveraged ZenML since day 0, we got
    rid of implementing a tedious CLI to configure our application. Instead, we did
    it directly through YAML configuration files out of the box.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从编排器开始，从第0天起：** 我们在项目开发初期开始使用ZenML。早期，我们只在本地使用它。但它充当了我们的管道的入口点，以及监控它们执行的方式。这样做迫使我们解耦每个管道，并通过各种类型的数据存储（如数据仓库、特征存储或工件存储）将它们之间的通信完全通过。由于我们从第0天起就利用了ZenML，我们摆脱了实现繁琐的CLI来配置我们的应用程序。相反，我们直接通过YAML配置文件“开箱即用”来完成它。'
- en: In *Figure 11.19*, we can see all the pipelines that we have to chain together
    to fully automate our training and deployment. The pipelines aren’t new; they
    aggregate everything we’ve covered throughout this book. Thus, at this point,
    we will treat them as black boxes that interact with each other.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11.19*中，我们可以看到我们必须连接在一起以完全自动化我们的培训和部署的所有管道。这些管道不是新的；它们汇总了我们在整本书中涵盖的所有内容。因此，在这个阶段，我们将它们视为相互交互的黑盒。
- en: '![](img/B31105_11_19.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_19.png)'
- en: 'Figure 11.19: CT pipeline'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19：CT管道
- en: For the LLM Twin’s CT pipeline, we have to discuss the initial trigger that
    starts the pipelines and how the pipelines are triggered by each other.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM Twin的CT管道，我们必须讨论启动管道的初始触发器以及管道是如何相互触发的。
- en: Initial triggers
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始触发器
- en: 'As illustrated in *Figure 11.18*, we initially want to trigger the data collection
    pipeline. Usually, the triggers can be of three types:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图11.18*所示，我们最初希望触发数据收集管道。通常，触发器可以是三种类型之一：
- en: '**Manual triggers:** Done through the CLI or the orchestrator’s dashboard,
    in our case, through the ZenML dashboard. Manual triggers are still extremely
    powerful tools, as you need just one action to start the whole ML system, from
    data gathering to deployment, instead of fiddling with dozens of scripts that
    you might configure wrong or run in an invalid order.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动触发器：** 通过CLI或编排器的仪表板完成，在我们的情况下，通过ZenML仪表板。手动触发器仍然是极其强大的工具，因为您只需一个动作就可以启动整个ML系统，从数据收集到部署，而不是摆弄可能配置错误或以无效顺序运行的几十个脚本。'
- en: '**REST API triggers:** You can call a pipeline by an HTTP request. This is
    extremely useful when integrating your ML pipelines with other components. For
    example, you can have a watcher constantly looking for new articles. It triggers
    the ML logic using this REST API trigger when it finds some. To find more details
    on this feature, check out this tutorial on ZenML’s documentation: [https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api](https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api).'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**REST API触发器：** 您可以通过HTTP请求调用管道。当将您的ML管道与其他组件集成时，这非常有用。例如，您可以有一个监视器持续寻找新的文章。当它找到一些时，它会使用这个REST
    API触发器来触发ML逻辑。要了解更多关于此功能的信息，请查看ZenML文档中的此教程：[https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api](https://docs.zenml.io/v/docs/how-to/trigger-pipelines/trigger-a-pipeline-from-rest-api)。'
- en: '**Scheduled triggers:** Another common approach is to schedule your pipeline
    to run constantly on a fixed interval. For example, depending on your use case,
    you can schedule your pipeline to run daily, hourly, or every minute. Most of
    the orchestrators, ZenML included, provide a cron expression interface where you
    can define your execution frequency. In the following example from ZenML, the
    pipeline is scheduled every hour:'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计划触发器：** 另一种常见的方法是按固定间隔持续调度您的管道运行。例如，根据您的用例，您可以安排您的管道每天、每小时或每分钟运行一次。大多数编排器，包括ZenML，都提供了一个cron表达式界面，您可以在其中定义您的执行频率。在以下ZenML的示例中，管道每小时调度一次：'
- en: '[PRE46]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We chose a manual trigger for our LLM Twin use case as we don’t have other components
    to leverage the REST API triggers. Also, as the datasets are generated from a
    list of static links defined in the ZenML configs, running them on a schedule
    doesn’t make sense as they would always yield the same results.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为我们的LLM Twin用例选择了手动触发器，因为我们没有其他组件可以利用REST API触发器。此外，由于数据集是从ZenML配置中定义的静态链接列表生成的，因此按计划运行它们没有意义，因为它们总是会得出相同的结果。
- en: But a possible next step for the project is to implement a watcher that monitors
    for new articles. When it finds any, it generates a new config and triggers the
    pipelines through the REST API. Another option is implementing the watcher as
    an additional pipeline and leveraging the schedule triggers to look daily for
    new data. If it finds any, it executes the whole ML system; otherwise, it stops.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 但项目的下一个可能的步骤是实现一个监视器，用于监视新文章。当它找到任何文章时，它会生成一个新的配置并通过REST API触发管道。另一个选项是将监视器作为额外的管道实现，并利用调度触发器每天查找新数据。如果找到任何数据，它将执行整个ML系统；否则，它将停止。
- en: The conclusion is that once you can manually trigger all your ML pipelines through
    a single command, you can quickly adapt it to more advanced and complex scenarios.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 结论是，一旦您可以通过单个命令手动触发所有ML管道，您就可以快速将其适应更高级和复杂的场景。
- en: Trigger downstream pipelines
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 触发下游管道
- en: To keep things simple, we sequentially chained all the pipelines. More concretely,
    when the data collection pipeline has finished, it will trigger the feature pipeline.
    When the feature pipeline has been completed successfully, it triggers the dataset
    generation pipeline, and so on. You can make the logic more complex, like scheduling
    the generate instruct dataset pipeline to run daily, checking the amount of new
    data in the Qdrant vector DB, and starting only if it has enough new data. From
    this point, you can further tweak the system’s parameters and optimize them to
    reduce costs.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们按顺序链式连接了所有管道。更具体地说，当数据收集管道完成时，它将触发特征管道。当特征管道成功完成后，它将触发数据集生成管道，依此类推。您可以使逻辑更复杂，例如，安排生成指令数据集的管道每天运行，检查Qdrant向量数据库中的新数据量，并且只有当有足够的新数据时才启动。从这个点开始，您可以进一步调整系统的参数并优化它们以降低成本。
- en: 'To trigger all the pipelines in one go, we created one master pipeline that
    aggregates everything in one entry point:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 要一次性触发所有管道，我们创建了一个主管道，它将所有内容聚合在一个入口点：
- en: '[PRE47]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: To keep the function light, we added all the logic up to computing the features.
    But, as we suggested in the code snippet above, you can easily add the instruction
    dataset generation, training, and deploy logic to the parent pipeline to implement
    an end-to-end flow. By doing that, you can automate everything from data collection
    to deploying the model.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持函数轻量，我们添加了所有逻辑，直到计算特征。但是，如上述代码片段中建议的，您可以轻松地将指令数据集生成、训练和部署逻辑添加到父管道中，以实现端到端流程。通过这样做，您可以自动化从数据收集到部署模型的所有过程。
- en: 'To run the end-to-end pipeline, use the following `poe` command:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行端到端管道，请使用以下`poe`命令：
- en: '[PRE48]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: What we implemented is not the best approach, as it compresses all the steps
    into a single monolith pipeline (which we want to avoid), as illustrated in *Figure
    11.20*. Usually, you want to keep each pipeline isolated and use triggers to start
    downstream pipelines. This makes the system easier to understand, debug, and monitor.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实施的方法并不是最佳方法，因为它将所有步骤压缩到一个单一的单一管道中（这是我们想要避免的），如*图11.20*所示。通常，您希望保持每个管道的隔离性，并使用触发器来启动下游管道。这使得系统更容易理解、调试和监控。
- en: '![](img/B31105_11_20.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_11_20.png)'
- en: 'Figure 11.20: End-to-end pipeline illustrated in ZenML’s dashboard'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20：ZenML仪表板中展示的端到端管道
- en: 'Unfortunately, the ZenML cloud’s free trial has a limitation of a maximum of
    three pipelines. As we have more, we avoided that limitation by compressing all
    the steps into a single pipeline. But if you plan to host ZenML yourself or buy
    their license, they offer the possibility to independently trigger a pipeline
    from another pipeline, as you can see in the code snippet below where we triggered
    the feature engineering pipeline after the data collection ETL:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ZenML云服务的免费试用版有一个限制，即最多只能有三个管道。由于我们拥有更多，为了避免这个限制，我们将所有步骤压缩到一个单独的管道中。但是，如果您计划自己托管ZenML或购买他们的许可证，他们提供从另一个管道独立触发管道的可能性，如下面的代码片段所示，我们在数据收集ETL之后触发了特征工程管道：
- en: '[PRE49]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: By taking this approach, each pipeline will have its independent run, where
    one pipeline sequentially triggers the next one, as described at the beginning
    of this section. Note that this feature is not unique to ZenML but is common in
    orchestrator tools. The principles we have learned so far hold. Only how we interact
    with the tool changes.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，每个管道都将独立运行，其中每个管道依次触发下一个管道，如本节开头所述。请注意，这个特性并不仅限于ZenML，在编排工具中很常见。我们迄今为止学到的原则仍然适用。只是我们与工具的交互方式发生了变化。
- en: Prompt monitoring
  id: totrans-445
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示监控
- en: We will use Opik (from Comet ML) to monitor our prompts. But remember from the
    *LLMOps* section earlier in this chapter that we are not interested only in the
    input prompt and generated answer.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Opik（来自 Comet ML）来监控我们的提示。但请记住，在本章早些时候的 *LLMOps* 部分中，我们不仅对输入提示和生成的答案感兴趣。
- en: 'We want to log the entire trace from the user’s input until the final result
    is available. Before diving into the LLM Twin use case, let’s look at a simpler
    example:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望记录从用户输入到最终结果可用的整个跟踪。在深入探讨 LLM Twin 用例之前，让我们看看一个更简单的例子：
- en: '[PRE50]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Then, you have preprocessing and postprocessing functions surrounding the actual
    LLM call. Using the `@track()` decorator, we log the input and output of each
    function, which will ultimately be aggregated into a single trace. By doing so,
    we will have access to the initial input text, the generated answer, and all the
    intermediary steps required to debug any potential issues using Opik’s dashboard.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您有围绕实际 LLM 调用的预处理和后处理函数。使用 `@track()` 装饰器，我们记录每个函数的输入和输出，这些最终将被汇总成一个单一的跟踪。通过这样做，我们将能够访问初始输入文本、生成的答案以及调试任何潜在问题所需的全部中间步骤，使用
    Opik 的仪表板。
- en: '[PRE51]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: You can expand on this idea and log various feedback scores. The most common
    is asking the user if the generated answer is valuable and correct. Another option
    is to compute various metrics automatically through heuristics or LLM judges.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此基础上扩展这个想法并记录各种反馈分数。最常见的是询问用户生成的答案是否有价值且正确。另一种选择是通过启发式方法或 LLM 判决自动计算各种指标。
- en: Finally, let’s see how to add prompt monitoring to our LLM Twin project. First,
    look at *Figure 11.21* and remember our model-serving architecture. We have two
    microservices, the LLM and business microservices. The LLM microservice has a
    narrow scope, as it only takes as input a prompt that already contains the user’s
    input and context and returns an answer that is usually post-processed. Thus,
    the business microservice is the right place to implement the monitoring pipeline,
    as it coordinates the end-to-end flow. More concretely, Opik implementation will
    be in the FastAPI server developed in *Chapter 10*.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何将提示监控添加到我们的 LLM Twin 项目中。首先，查看 *图 11.21* 并记住我们的模型服务架构。我们有两个微服务，LLM
    和业务微服务。LLM 微服务的范围较窄，因为它只接受包含用户输入和上下文的提示作为输入，并返回通常需要后处理的答案。因此，业务微服务是实施监控管道的正确地方，因为它协调端到端流程。更具体地说，Opik
    实现将在 *第 10 章* 中开发的 FastAPI 服务器中实现。
- en: '![](img/B31105_11_21.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_11_21.png)'
- en: 'Figure 11.21: Inference pipeline serving architecture'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.21：推理管道服务架构
- en: 'As our implementation is already modular, using Opik makes it straightforward
    to log an end-to-end trace of a user’s request:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的实现已经是模块化的，使用 Opik 使得记录用户请求的端到端跟踪变得简单：
- en: '[PRE53]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The `rag()` function represents your application’s entry point. All the other
    processing steps take place in the `ContextRetriever` and `InferenceExector` classes.
    Also, by decorating the `call_llm_service()` function, we can clearly capture
    the prompt sent to the LLM and its response.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '`rag()` 函数代表您应用程序的入口点。所有其他处理步骤都在 `ContextRetriever` 和 `InferenceExector` 类中进行。此外，通过装饰
    `call_llm_service()` 函数，我们可以清楚地捕获发送给 LLM 的提示及其响应。'
- en: 'To add more granularity to our trace, we can further decorate other functions
    containing pre- or post-processing steps, such as the `ContextRetriever` search
    function:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的跟踪更加细致，我们可以进一步装饰包含预处理或后处理步骤的其他函数，例如 `ContextRetriever` 搜索函数：
- en: '[PRE54]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Or even go further to the retrieval optimization methods, such as the self-query
    metadata extractor, to add more granularity:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 或者甚至进一步到检索优化方法，例如自查询元数据提取器，以添加更多粒度：
- en: '[PRE55]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The developer is responsible for deciding how much granularity the application
    needs for proper debugging and analysis. As having detailed monitoring is healthy,
    monitoring everything can be dangerous as it adds too much noise and makes manually
    understanding the traces difficult. You must find the right balance. A good rule
    of thumb is tracing the most critical functions, such as `rag()` and `call_llm_service()`,
    and gradually adding more granularity when needed.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者负责决定应用程序需要多少粒度才能进行适当的调试和分析。虽然详细的监控是健康的，但监控一切可能会很危险，因为它会添加过多的噪声，使得手动理解跟踪变得困难。您必须找到正确的平衡。一个好的经验法则是跟踪最关键的功能，如
    `rag()` 和 `call_llm_service()`，并在需要时逐步添加更多粒度。
- en: 'The last step is to attach valuable metadata and tags to our traces. To do
    so, we will further enhance the `rag()` function as follows:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是为我们的跟踪附加有价值的元数据和标签。为此，我们将进一步增强`rag()`函数，如下所示：
- en: '[PRE56]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'There are three main aspects that we should constantly monitor:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该持续监控的三个主要方面是：
- en: '**Model configuration:** Here, we should consider both the LLM and other models
    used within the RAG layer. The most critical aspects of logging are the model
    IDs, but you can also capture other important information that significantly impacts
    the generation, such as the temperature.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型配置：** 在这里，我们应该考虑LLM和RAG层中使用的其他模型。日志记录的最关键方面是模型ID，但您还可以捕获其他对生成有显著影响的重要信息，例如温度。'
- en: '**Total number of tokens:** It’s critical to constantly analyze the statistics
    of the number of tokens generated by your input prompts and total tokens, as this
    significantly impacts your serving costs. For example, if the average of the total
    number of tokens generated suddenly increases, it’s a strong signal that you have
    a bug in your system that you should investigate.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总令牌数：** 持续分析您输入提示和总令牌数的统计数据至关重要，因为这会显著影响您的服务成本。例如，如果总令牌数的平均值突然增加，这可能是您系统中存在错误的强烈信号，您应该进行调查。'
- en: '**The duration of each step:** Tracking the duration of each step within your
    trace is essential to finding bottlenecks within your system. If the latency of
    a specific request is abnormally large, you quickly have access to a report that
    helps you find the source of the problem.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个步骤的持续时间：** 跟踪您跟踪中每个步骤的持续时间对于找到系统中的瓶颈至关重要。如果特定请求的延迟异常大，您将快速获得一份报告，帮助您找到问题的根源。'
- en: Alerting
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 警报
- en: 'Using ZenML, you can quickly implement an alerting system on any platform of
    your liking, such as email, Discord, or Slack. For example, you can add a callback
    in your training pipeline to trigger a notification when the pipeline fails or
    the training has finished successfully:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ZenML，您可以在任何您喜欢的平台上快速实现警报系统，例如电子邮件、Discord或Slack。例如，您可以在训练管道中添加一个回调，当管道失败或训练成功完成时触发通知：
- en: '[PRE57]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Implementing the notification functions is straightforward. As seen in the
    code snippets below, you have to get the `alerter` instance from your current
    stack, build the message as you see fit, and send it to your notification channel
    of choice:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 实现通知功能很简单。如以下代码片段所示，您需要从当前堆栈中获取`alerter`实例，构建您认为合适的消息，并将其发送到您选择的任何通知渠道：
- en: '[PRE58]'
  id: totrans-474
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: ZenML and most orchestrators simplify implementing an `alerter`, as it’s a critical
    component in your MLOps/LLMOps infrastructure.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: ZenML和大多数编排器简化了`alerter`的实现，因为它是您MLOps/LLMOps基础设施中的关键组件。
- en: Summary
  id: totrans-476
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we laid down the foundations with a theoretical section on
    DevOps. Then, we moved on to MLOps and its core components and principles. Finally,
    we presented how LLMOps differs from MLOps by introducing strategies such as prompt
    monitoring, guardrails, and human-in-the-loop feedback. Also, we briefly discussed
    why most companies would avoid training LLMs from scratch but choose to optimize
    them for their use case through prompt engineering or fine-tuning. At the end
    of the theoretical portion of the chapter, we learned what a CI/CD/CT pipeline
    is, the three core dimensions of an ML application (code, data, model), and that,
    after deployment, it is more critical than ever to implement a monitoring and
    alerting layer due to model degradation.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过一个关于DevOps的理论部分奠定了基础。然后，我们转向MLOps及其核心组件和原则。最后，我们通过介绍诸如提示监控、安全线和人工反馈循环等策略，展示了LLMOps与MLOps的不同之处。此外，我们还简要讨论了为什么大多数公司会选择通过提示工程或微调来优化LLM以适应其用例，而不是从头开始训练LLM。在章节理论部分的结尾，我们学习了什么是CI/CD/CT管道，ML应用的三个核心维度（代码、数据、模型），以及由于模型退化，部署后实施监控和警报层比以往任何时候都更加关键。
- en: Next, we learned how to deploy the LLM Twin’s pipeline to the cloud. We understood
    the infrastructure and went step by step through deploying MongoDB, Qdrant, the
    ZenML cloud, and all the necessary AWS resources to sustain the application. Finally,
    we learned how to Dockerize our application and push our Docker image to AWS ECR,
    which will be used to execute the application on top of AWS SageMaker.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何将LLM Twin的管道部署到云中。我们了解了基础设施，并逐步部署MongoDB、Qdrant、ZenML云以及所有必要的AWS资源以维持应用程序。最后，我们学习了如何将我们的应用程序Docker化并将我们的Docker镜像推送到AWS
    ECR，该镜像将用于在AWS SageMaker上执行应用程序。
- en: The final step was to add LLMOps to our LLM Twin project. We began by implementing
    a CI/CD pipeline with GitHub Actions. Then, we looked at our CT strategy by leveraging
    ZenML.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将LLMOps添加到我们的LLM Twin项目中。我们首先通过GitHub Actions实现了CI/CD管道，然后利用ZenML审视了我们的CT策略。
- en: Finally, we saw how to implement a monitoring pipeline using Opik from Comet
    ML and an alerting system using ZenML. These are the fundamental pillars in adding
    MLOps and LLMOps to any LLM-based application.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到了如何使用Comet ML的Opik实现监控管道，以及使用ZenML实现警报系统。这些是向任何基于LLM的应用添加MLOps和LLMOps的基本支柱。
- en: The framework we learned about throughout the book can quickly be extrapolated
    to other LLM applications. Even if we used the LLM Twin use case as an example,
    most of the strategies applied can be adapted to other projects. Thus, we can
    get an entirely new application by changing the data and making minor tweaks to
    the code. Data is the new oil, remember?
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中学到的框架可以迅速扩展到其他LLM应用中。即使我们以LLM Twin用例为例，大多数应用到的策略也可以适应其他项目。因此，通过改变数据和代码的微小调整，我们可以得到一个全新的应用。数据是新石油，记住了吗？
- en: By finalizing this chapter, we’ve learned to build an end-to-end LLM application,
    starting with data collection and fine-tuning until deploying the LLM microservice
    and RAG service. Throughout this book, we aimed to provide a thought framework
    to help you build and solve real-world problems in the GenAI landscape. Now that
    you have it, we wish you good luck in your journey and happy building!
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成这一章，我们学会了如何构建端到端的LLM应用，从数据收集和微调开始，直到部署LLM微服务和RAG服务。在整个书中，我们旨在提供一个思维框架，帮助你在GenAI领域中构建和解决实际问题。现在你已经拥有了它，我们祝愿你在旅途中好运，快乐构建！
- en: References
  id: totrans-483
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: GitLab. (2023, January 25). *What is DevOps? | GitLab*. GitLab. [https://about.gitlab.com/topics/devops/](https://about.gitlab.com/topics/devops/)
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLab. (2023, January 25). *什么是DevOps？| GitLab*. GitLab. [https://about.gitlab.com/topics/devops/](https://about.gitlab.com/topics/devops/)
- en: Huyen, C. (2024, July 25). Building a generative AI platform. *Chip Huyen*.
    [https://huyenchip.com/2024/07/25/genai-platform.html](https://huyenchip.com/2024/07/25/genai-platform.html)
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huyen, C. (2024, July 25). 构建生成式AI平台。*Chip Huyen*。[https://huyenchip.com/2024/07/25/genai-platform.html](https://huyenchip.com/2024/07/25/genai-platform.html)
- en: '*Lightricks customer story: Building a recommendation engine from scratch*.
    (n.d.). [https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch](https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch)'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Lightricks客户故事：从头开始构建推荐引擎*。 (n.d.). [https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch](https://www.qwak.com/academy/lightricks-customer-story-building-a-recommendation-engine-from-scratch)'
- en: '*What LLMOps*. (n.d.). Google Cloud. [https://cloud.google.com/discover/what-is-llmops?hl=en](https://cloud.google.com/discover/what-is-llmops?hl=en)'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*什么是LLMOps*。 (n.d.). Google Cloud. [https://cloud.google.com/discover/what-is-llmops?hl=en](https://cloud.google.com/discover/what-is-llmops?hl=en)'
- en: '*MLOps: Continuous delivery and automation pipelines in machine learning*.
    (2024, August 28). Google Cloud. [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page)'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MLOps：机器学习中的持续交付和自动化管道*。 (2024, August 28). Google Cloud. [https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#top_of_page)'
- en: '*Ml-ops.org*. (2024a, July 5). [https://ml-ops.org/content/mlops-principles](https://ml-ops.org/content/mlops-principles)'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ml-ops.org*。 (2024a, July 5). [https://ml-ops.org/content/mlops-principles](https://ml-ops.org/content/mlops-principles)'
- en: '*Ml-ops.org*. (2024b, July 5). [https://ml-ops.org/content/mlops-principles](https://ml-ops.org/content/mlops-principles)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ml-ops.org*。 (2024b, July 5). [https://ml-ops.org/content/mlops-principles](https://ml-ops.org/content/mlops-principles)'
- en: '*Ml-ops.org*. (2024c, July 5). [https://ml-ops.org/content/motivation](https://ml-ops.org/content/motivation)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ml-ops.org*。 (2024c, July 5). [https://ml-ops.org/content/motivation](https://ml-ops.org/content/motivation)'
- en: Mohandas, G. M. (2022a). Monitoring machine learning systems. *Made With ML*.
    [https://madewithml.com/courses/mlops/monitoring/](https://madewithml.com/courses/mlops/monitoring/)
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohandas, G. M. (2022a). 监控机器学习系统。*用ML制作*。[https://madewithml.com/courses/mlops/monitoring/](https://madewithml.com/courses/mlops/monitoring/)
- en: 'Mohandas, G. M. (2022b). Testing Machine Learning Systems: Code, Data and Models.
    *Made With ML*. [https://madewithml.com/courses/mlops/testing/](https://madewithml.com/courses/mlops/testing/
    )'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohandas, G. M. (2022b). 测试机器学习系统：代码、数据和模型。*用ML制作*。[https://madewithml.com/courses/mlops/testing/](https://madewedml.com/courses/mlops/testing/)
- en: Preston-Werner, T. (n.d.). *Semantic Versioning 2.0.0*. Semantic Versioning.
    [https://semver.org/](https://semver.org/)
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Preston-Werner, T. (n.d.). *语义版本控制 2.0.0*. Semantic Versioning. [https://semver.org/](https://semver.org/)
- en: 'Ribeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). *Beyond Accuracy:
    Behavioral Testing of NLP models with CheckList*. arXiv.org. [https://arxiv.org/abs/2005.04118](https://arxiv.org/abs/2005.04118
    )'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020, May 8). *超越准确性：使用 CheckList
    对 NLP 模型的行为测试*. arXiv.org. [https://arxiv.org/abs/2005.04118](https://arxiv.org/abs/2005.04118)
- en: 'Wandb. (2023, November 30). *Understanding LLMOps: Large Language Model Operations*.
    Weights & Biases. [https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/](https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wandb. (2023, November 30). *理解 LLMOps：大型语言模型操作*. Weights & Biases. [https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/](https://wandb.ai/site/articles/understanding-llmops-large-language-model-operations/)
- en: 'Zenml-Io. (n.d.). *GitHub—zenml-io/zenml-huggingface-sagemaker: An example
    MLOps overview of ZenML pipelines from a Hugging Face model repository to a deployed
    AWS SageMaker endpoint.* GitHub. [https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main](https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main  )'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zenml-Io. (n.d.). *GitHub—zenml-io/zenml-huggingface-sagemaker: 从 Hugging Face
    模型仓库到部署的 AWS SageMaker 端点的 ZenML 管道 MLOps 概览示例.* GitHub. [https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main](https://github.com/zenml-io/zenml-huggingface-sagemaker/tree/main)'
- en: Join our book’s Discord space
  id: totrans-498
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
