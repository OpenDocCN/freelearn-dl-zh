- en: Training Autoencoders Using RNNSharp
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNNSharp 训练自动编码器
- en: In this chapter, we will be discussing **autoencoders** and their usage. We
    will talk about what an autoencoder is, the different types of autoencoder, and
    present different samples to help you better understand how to use this technology
    in your applications. By the end of this chapter, you will know how to design
    your own autoencoder, load and save it from disk, and train and test it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**自动编码器**及其用法。我们将讨论什么是自动编码器，不同类型的自动编码器，并展示不同的示例，以帮助您更好地理解如何在应用程序中使用这项技术。到本章结束时，您将了解如何设计自己的自动编码器，从磁盘加载和保存它，以及如何训练和测试它。
- en: 'The following topics are covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: What is an autoencoder?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是自动编码器？
- en: Different types of autoencoder
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的自动编码器
- en: Creating your own autoencoder
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自己的自动编码器
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require Microsoft Visual Studio.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 Microsoft Visual Studio。
- en: What is an autoencoder?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是自动编码器？
- en: An autoencoder is an unsupervised learning algorithm that applies back propagation
    and sets target values equal to the inputs. An autoencoder learns to compress
    data from the input layer into shorter code, and then it uncompresses that code
    into something that closely matches the original data. This is better known as
    **dimensionality reduction**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器是一种无监督学习算法，它应用反向传播并设置目标值等于输入。自动编码器学习将输入层的数据压缩成更短的代码，然后将其解压缩成与原始数据非常接近的东西。这更广为人知的是**降维**。
- en: 'The following is a depiction of an autoencoder. The original images are encoded,
    and then decoded to reconstruct the original:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对自动编码器的描述。原始图像被编码，然后解码以重建原始图像：
- en: '![](img/b65f99cf-4805-402c-9e69-518888711abb.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b65f99cf-4805-402c-9e69-518888711abb.png)'
- en: Different types of autoencoder
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同类型的自动编码器
- en: 'The following are different types of autoencoder:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下介绍了不同类型的自动编码器：
- en: '![](img/4d624ffb-80b9-4c7a-a52b-138d50241b5f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d624ffb-80b9-4c7a-a52b-138d50241b5f.png)'
- en: Let's briefly discuss autoencoders and the variants we have just seen. Please
    note that there are other variants out there; these are just probably the most
    common that I thought you should at least be familiar with.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论自动编码器和我们已经看到的变体。请注意，还有其他变体；这些只是我认为您至少应该熟悉的可能最常见的。
- en: Standard autoencoder
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准自动编码器
- en: An autoencoder learns to compress data from the input layer into smaller code,
    and then uncompress that code into something that (hopefully) matches the original
    data. The basic idea behind a standard autoencoder is to encode information automatically,
    hence the name. The entire network always resembles an hourglass, in terms of
    its shape, with fewer hidden layers than input and output layers. Everything up
    to the middle layer is called the encoding part, everything after the middle layer
    is called the decoding part, and the middle layer itself is called, as you have
    probably guessed, the code. You can train autoencoders by feeding input data and
    setting the error status as the difference between the input and what came out.
    Autoencoders can be built so that encoding weights are the same as decoding weights.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器学习将输入层的数据压缩成更小的代码，然后将该代码解压缩成（希望）与原始数据匹配的东西。标准自动编码器背后的基本思想是自动编码信息，因此得名。整个网络在形状上总是类似于沙漏，其隐藏层比输入层和输出层少。中间层之前的所有内容称为编码部分，中间层之后的所有内容称为解码部分，中间层本身被称为，正如你可能猜到的，代码。您可以通过提供输入数据和设置错误状态为输入和输出之间的差异来训练自动编码器。可以构建自动编码器，使得编码权重和解码权重相同。
- en: Variational autoencoders
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自动编码器
- en: '**Variational autoencoders** have the same architecture as autoencoders but
    are taught something else: an approximated probability distribution of the input
    samples. This is because they are a bit more closely related to Boltzmann and
    Restricted Boltzmann Machines. They do however rely on Bayesian mathematics as
    well as a re-parametrization trick to achieve this different representation. The
    basics come down to this: taking influence into account. If one thing happens
    in one place, and something else happens somewhere else, they are not necessarily
    related. If they are not related, then error propagation should consider that.
    This is a useful approach, because neural networks are large graphs (in a way),
    so it helps to be able to rule out the influence some nodes have on other nodes
    as you dive into deeper layers.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**与自编码器具有相同的架构，但它们学习的内容不同：输入样本的近似概率分布。这是因为它们与玻尔兹曼机和受限玻尔兹曼机有更紧密的联系。然而，它们确实依赖于贝叶斯数学以及一个重新参数化技巧来实现这种不同的表示。基本原理可以归结为：考虑影响。如果在一处发生某事，而在另一处发生另一件事，它们不一定相关。如果它们不相关，那么错误传播应该考虑这一点。这是一个有用的方法，因为神经网络是大型图（在某种程度上），所以当你深入到更深的层时，能够排除某些节点对其他节点的影响是有帮助的。'
- en: De-noising autoencoders
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: '**De-noising autoencoders** reconstruct the input from a corrupted version
    of themselves. This does two things. First, it tries to encode the input while
    preserving as much information as possible. Second, it tries to undo the effect
    of the corruption process. The input is reconstructed after a percentage of the
    data has been randomly removed, which forces the network to learn robust features
    that tend to generalize better. De-noising autoencoders are autoencoders where
    we feed the input data with noise (such as making an image grainier). We compute
    the error the same way, so the output of the network is compared to the original
    input without noise. This encourages the network to learn not details but broader
    features, which are usually more accurate, as they are not affected by constantly
    changing noise.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**去噪自编码器**从自身被损坏的版本中重建输入。这做了两件事。首先，它试图编码输入，同时尽可能保留信息。其次，它试图撤销损坏过程的影响。在随机移除一定比例的数据后，输入被重建，这迫使网络学习鲁棒的特征，这些特征往往具有更好的泛化能力。去噪自编码器是我们在输入数据中添加噪声（例如使图像更粗糙）的自编码器。我们以相同的方式计算误差，因此网络的输出与原始无噪声输入进行比较。这鼓励网络学习不是细节而是更广泛的特征，这些特征通常更准确，因为它们不受不断变化的噪声的影响。'
- en: Sparse autoencoders
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自编码器
- en: '**Sparse autoencoders** are, in a way, the opposite of autoencoders. Instead
    of teaching a network to represent information in less space or fewer nodes, we
    try to encode information in more space. Instead of the network converging in
    the middle and then expanding back to the input size, we blow up the middle. These
    types of network can be used to extract many small features from a dataset. If
    you were to train a sparse autoencoder the same way as an autoencoder, you would
    in almost all cases end up with a pretty useless identity network (as in, what
    comes in is what comes out, without any transformation or decomposition). To prevent
    this, we feed back the input, plus what is known as a **sparsity driver**. This
    can take the form of a threshold filter, where only a certain error is passed
    back and trained. The other error will be irrelevant for that pass and will be
    set to zero. In a way, this resembles spiking neural networks, where not all neurons
    fire all the time.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏自编码器**在某种程度上是自编码器的对立面。不是教网络以更少的空间或节点表示信息，而是尝试在更多的空间中编码信息。不是网络在中间收敛然后扩展回输入大小，而是将中间部分膨胀。这类网络可以用来从数据集中提取许多小的特征。如果你以与自编码器相同的方式训练稀疏自编码器，你几乎在所有情况下都会得到一个相当无用的恒等网络（即输入的是什么，输出的也是什么，没有任何转换或分解）。为了防止这种情况，我们反馈输入，以及所谓的**稀疏驱动器**。这可以采取阈值滤波器的形式，其中只有一定错误的误差被反馈并训练。其他错误对于那次传递将是无关紧要的，并将被设置为零。在某种程度上，这类似于脉冲神经网络，其中不是所有神经元都始终在放电。'
- en: Creating your own autoencoder
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自己的自编码器
- en: Now that you are an expert on autoencoders, let's move on to less theory and
    more practice. Let's take a bit of a different route on this one. Instead of using
    an open-source package and showing you how to use it, let's write our own autoencoder
    framework that you can enhance to make your own. We'll discuss and implement the
    basic pieces needed, and then write some sample code showing how to use it. We
    will make this chapter unique in that we won't finish the usage sample; we'll
    do just enough to get you started along your own path to autoencoder creation.
    With that in mind, let's begin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经成为自动编码器的专家，让我们转向更少的理论，更多的实践。让我们在这件事上采取一条不同的路线。而不是使用开源包并展示如何使用它，让我们编写我们自己的自动编码器框架，你可以增强它来制作自己的。我们将讨论和实现所需的基本组件，然后编写一些示例代码来展示如何使用它。我们将使这一章节独特，因为我们不会完成使用示例；我们只会做足够的，让你开始自己的自动编码器创建之路。考虑到这一点，让我们开始。
- en: Let's start off by thinking about what an autoencoder is and what things we
    would want to include. First off, we're going to need to keep track of the number
    of layers that we have. These layers will be Restricted Boltzmann Machines for
    sure. Just so you know, we'll also refer to **Restricted Boltzmann Machines** as **RBMs** from
    time to time, where brevity is required.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从思考自动编码器是什么以及我们想要包含哪些内容开始。首先，我们需要跟踪我们有多少层。这些层肯定是受限玻尔兹曼机。只是让你知道，当我们需要简洁时，我们也会将
    **受限玻尔兹曼机** 简称为 **RBMs**。
- en: 'So, we know we''ll need to track the number of layers that our autoencoder
    has. We''re also going to need to monitor the weights we''ll need to use: learning
    rate, recognition weights, and generative weights. Training data is important,
    of course, as are errors. I think for now that should be it. Let''s block out
    a class to do just this.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们知道我们需要跟踪我们的自动编码器有多少层。我们还将需要监控我们将需要使用的权重：学习率、识别权重和生成权重。当然，训练数据很重要，错误也是如此。我认为现在应该就足够了。让我们创建一个类来专门做这件事。
- en: 'Let''s start with an `interface`, which we will use to calculate errors. We
    will only need one method, which will calculate the error for us. The RBM will
    be responsible for doing this, but we''ll get to that later:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个 `interface` 开始，我们将使用它来计算错误。我们只需要一个方法，这个方法会为我们计算错误。RBM 将负责做这件事，但我们会稍后讨论：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Before we define our RBM class, we''ll need to look at the layers that it will
    use. To best represent this, we''ll create an `abstract` class. We''ll need to
    track the state of the layer, the bias used, the amount of bias change, the activity
    itself, and how many neurons it will have. Rather than distinguish between mirror
    and canonical neurons, we''ll simply represent all neuron types as one single
    object. We also will need to have multiple types of RBM layer. Gaussian and binary
    are two that come to mind, so the following will be the base class for those layers:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义我们的 RBM 类之前，我们需要查看它将使用的层。为了最好地表示这一点，我们将创建一个 `abstract` 类。我们需要跟踪层的状态、使用的偏置、偏置变化量、活动本身以及它将有多少个神经元。我们不会区分镜像神经元和规范神经元，而将所有神经元类型表示为一个单一的对象。我们还需要有多个类型的
    RBM 层。高斯和二进制是两个可以想到的类型，所以以下将是这些层的基类：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We must keep in mind that our RBM will need to track its weights. Since weights
    are applied through layers with a thing called a **synapse**, we''ll create a
    class to represent all we want to do with weights. Since we''ll need to track
    the weights, their changes, and the pre- and post-size, let''s just create a class
    that encapsulates all of that:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须记住，我们的 RBM 需要跟踪其权重。由于权重是通过称为 **突触** 的层应用的，我们将创建一个类来表示我们想要对权重做的所有事情。由于我们需要跟踪权重、它们的变化以及前向和后向大小，让我们创建一个封装所有这些内容的类：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, as our learning rate encompasses features such as weights, biases, and
    momentum, we will be best served if we create a separate class to represent all
    of this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于我们的学习率包括权重、偏置和动量等特征，我们最好创建一个单独的类来表示所有这些：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let''s create a class that encompasses our training data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们创建一个包含我们的训练数据的类：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With all of this now defined, let''s go ahead and work on our `RestrictedBoltzmannMachine`
    class. For this class, we''ll need to keep track of how many visible and hidden
    layers we have, the weights and learning rate we will use, and our training data:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些都定义好之后，让我们继续工作在我们的 `RestrictedBoltzmannMachine` 类上。对于这个类，我们需要跟踪我们有多少个可见层和隐藏层，我们将使用的权重和学习率，以及我们的训练数据：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And, finally, with everything else in place, let''s create our `Autoencoder`
    class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在所有其他内容都就绪之后，让我们创建我们的 `Autoencoder` 类：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Even though we know there will be a lot more required for some of these classes,
    this is the basic framework that we need to get started framing in the rest of
    the code. To do that, we should think about some things.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们知道这些类中的一些将需要更多的功能，但这是我们开始构建其余代码所需的基本框架。为了做到这一点，我们应该考虑一些事情。
- en: 'Since weights are a prominent aspect of our autoencoder, we are going to have
    to use and initialize weights quite often. But how should we initialize our weights,
    and with what values? We will provide two distinct choices. We will either initialize
    all weights to zero, or use a Gaussian. We will also have to initialize the biases
    as well. Let''s go ahead and create an interface from which to do this so it will
    make it easier later to select the type of initialization we want (zero or Gaussian):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重是我们自动编码器的一个突出方面，我们将不得不经常使用和初始化权重。但是我们应该如何初始化我们的权重，以及使用什么值？我们将提供两种不同的选择。我们将要么将所有权重初始化为零，要么使用高斯分布。我们还将不得不初始化偏差。让我们继续创建一个接口来完成这项工作，这样以后选择我们想要的初始化类型（零或高斯）会更容易：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We mentioned earlier that we needed to have multiple types of RBM layer to
    use. Gaussian and binary were two that came to mind. We have already created the
    interface for that, so let''s go ahead and put our base classes into the form,
    as we will need them shortly. To do this, we will need to expand our RBM layer
    class and add two abstract methods so that they can be cloned, and so that we
    can set the state of the layer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，我们需要有多个类型的RBM层来使用。高斯和二进制是两个想到的类型。我们已经为这个创建了接口，所以让我们继续将我们的基类放入形式，因为我们很快就会需要它们。为此，我们需要扩展我们的RBM层类并添加两个抽象方法，这样它们就可以被克隆，并且我们可以设置层的状态：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Our `RestrictedBoltzmannMachineLayer` class now looks like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`RestrictedBoltzmannMachineLayer`类现在看起来是这样的：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With our very basic autoencoder in place, we should now turn our attention
    to how we will build our autoencoder. Let''s try to keep things as modular as
    possible and, with that in mind, let''s create an `AutoEncoderBuilder` class that
    we can have encapsulate things such as weight initialization, adding layers, and
    so forth. It will look something like the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们建立了非常基础的自动编码器之后，我们现在应该将注意力转向我们如何构建自动编码器。让我们尽量使事物尽可能模块化，并考虑到这一点，让我们创建一个`AutoEncoderBuilder`类，它可以封装诸如权重初始化、添加层等事物。它看起来可能如下所示：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have this class blocked in, let''s begin to add some meat to it
    in the form of functions. We know that when we build an autoencoder we are going
    to need to add layers. We can do that with this function. We will pass it the
    layer, and then update our internal learning-rate layer:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将这个类建立起来，让我们开始通过函数的形式给它添加一些内容。我们知道当我们构建一个自动编码器时，我们将需要添加层。我们可以用这个函数来做。我们将传递层，然后更新我们内部的学习率层：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once we have this base function, we can then add some higher-level functions,
    which will make it easier for us to add layers to our autoencoder:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个基本函数，我们就可以添加一些更高级的函数，这将使我们更容易向自动编码器添加层：
- en: '[PRE12]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, let''s add a `Build()` method to our autoencoder builder to make it
    easy to build:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在我们的自动编码器构建器中添加一个`Build()`方法，使其更容易构建：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now let''s turn our attention to our autoencoder itself. We are going to need
    a function to help us initialize our biases:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将注意力转向我们的自动编码器本身。我们需要一个函数来帮助我们初始化偏差：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we are going to need to initialize our training data. This will basically
    involve creating all the arrays that we need and setting their initial values
    to zero as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要初始化我们的训练数据。这基本上涉及到创建我们需要的所有数组并将它们的初始值设置为零，如下所示：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With that behind us, we''re off to a good start. Let''s start to use the software
    and see what we''re missing. Let''s create our `builder` object, add some binary
    and Gaussian layers, and see how it looks:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，我们已经有一个良好的开端。让我们开始使用软件，看看我们缺少什么。让我们创建我们的`builder`对象，添加一些二进制和高斯层，看看效果如何：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Not bad, right? So, what''s next? Well, we''ve got our autoencoder created
    and have added layers. We now lack functions to allow us to fine tune and train
    learning rates and momentum. Let''s see how they would look if we were to add
    them here as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错，对吧？那么接下来是什么？嗯，我们已经创建了自动编码器并添加了层。我们现在缺少允许我们微调和训练学习率和动量的函数。让我们看看如果我们按照以下方式添加它们会是什么样子：
- en: '[PRE17]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That looks about right. At this point, we should add these functions into our `autoencoderbuilder`
    object so we can use them. Let''s see how that would look. Remember that with
    our builder object we automatically created our learning rate object, so now we
    just have to use it to populate things such as our weights and biases, along with
    the momentum weights and biases:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来差不多。在这个阶段，我们应该将这些函数添加到我们的`autoencoderbuilder`对象中，以便我们可以使用它们。让我们看看那会是什么样子。记住，随着我们的builder对象，我们自动创建了学习率对象，所以现在我们只需要使用它来填充诸如权重和偏差、动量权重和偏差等东西：
- en: '[PRE18]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Well, let''s now stop and take a look at what our sample program is turning
    out to look like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在让我们停下来看看我们的示例程序看起来像什么：
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Not bad. All we should need to do now is to call our `Build()` method on our
    `builder` and we should have the first version of our framework:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错。现在我们只需要在`builder`上调用我们的`Build()`方法，就应该有我们框架的第一版了：
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With all this now complete,and looking back at the preceding code, I think
    at some point we are going to need to be able to gain access to our individual
    layers; what do you think? Just in case, we''d better provide a function to do
    that. Let''s see how that would look:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有这些都已完成，回顾前面的代码，我认为在某个时候我们将需要能够访问我们的单个层；你怎么看？以防万一，我们最好提供一个函数来做这件事。让我们看看那会是什么样子：
- en: '[PRE21]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Since our internal layers are `RestrictedBoltzmannMachine` layers, that is
    the type that we should be returning, as you can see from the previous code. The
    `GetLayer()` function needs to reside inside our autoencoder object, though, not
    the builder. So, let''s go ahead and add it now. We''ll need to be good developers
    and make sure that we have a bounds check to ensure that we are passing a valid
    layer index before we try to use it. We''ll store all those neat little utility
    functions in a class of their own, and we might as well call it `Utility`, since
    the name makes sense. I won''t go into how we can code that function, as I am
    fairly confident that every reader already knows how to do bounds checks, so you
    can either make up your own or look at the accompanying source code to see how
    it''s done in this instance:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的内部层是`RestrictedBoltzmannMachine`层，这就是我们应该返回的类型，正如您可以从前面的代码中看到的那样。`GetLayer()`函数需要位于自动编码器对象内部，而不是builder。所以，让我们现在就添加它。我们需要成为优秀的开发者，确保我们在尝试使用它之前，有一个边界检查来确保我们传递了一个有效的层索引。我们将所有这些小巧的实用函数存储在一个自己的类中，我们可以称之为`Utility`，因为这个名字是有意义的。我不会深入讲解如何编写这个函数，因为我相当确信每个读者都已经知道如何进行边界检查，所以你可以自己编写或者查看附带的源代码来了解在这个例子中是如何实现的：
- en: '[PRE22]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: OK, so we can now create our autoencoders, set weights and biases, and gain
    access to individual layers. I think the next thing we need to start thinking
    about is training and testing. We'll need to take each separately, of course,
    so why don't we start with training?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们可以创建我们的自动编码器，设置权重和偏差，并访问单个层。我认为接下来我们需要开始考虑的是训练和测试。当然，我们需要分别处理它们，那么为什么不从训练开始呢？
- en: 'We will need to be able to train our RBM, so why don''t we create an object
    dedicated to doing this. We''ll call it, no surprise here, `RestrictedBoltzmannMachineTrainer`. Again,
    we are going to need to deal with our `LearningRate`, object, and weight sets,
    so let''s make sure we add them as variables right away:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要能够训练我们的RBM，那么为什么不创建一个专门用于此的对象呢。我们将它命名为，不出所料，`RestrictedBoltzmannMachineTrainer`。再次强调，我们还需要处理我们的`LearningRate`对象和权重集，所以让我们立即将它们作为变量添加：
- en: '[PRE23]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, what functions do you think we will need for our trainer? Obviously, a
    `Train()` method is required; otherwise, we named our object incorrectly. We''ll
    also need to train our weights and layer biases:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你认为我们需要为我们的训练器添加哪些功能呢？显然，需要一个`Train()`方法；否则，我们给对象命名就不正确了。我们还需要训练我们的权重和层偏差：
- en: '[PRE24]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Last, but not least, we should probably have a `helper` function that lets
    us know the training amount, which for us will involve taking the positive visible
    amount times the positive hidden amount and subtracting that from the negative
    visible amount times the negative hidden amount:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，我们可能需要一个`helper`函数，它让我们知道训练量，对我们来说，这涉及到将正可见量乘以正隐藏量，然后从负可见量乘以负隐藏量中减去：
- en: '[PRE25]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'OK, let''s see where our program stands:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们看看我们的程序现在处于什么位置：
- en: '[PRE26]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Nice. Can you see how it's all starting to come together? Now it's time to consider
    how we are going to add data to our network. Before we do any kind of training
    on the network, we will need to load data. How will we do this? Let's consider
    the notion of pre-training. This is the act of loading data into the network manually
    before we train it. What would this function look like in the context of our program?
    How about something such as this?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。你能看到所有这些是如何开始整合的吗？现在是我们考虑如何将数据添加到我们的网络中的时候了。在我们对网络进行任何类型的训练之前，我们需要加载数据。我们将如何做？让我们考虑预训练的概念。这是在我们训练之前手动将数据加载到网络中的行为。在我们的程序上下文中，这个函数会是什么样子？比如这样？
- en: '[PRE27]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We would just need to tell our autoencoder which layer we want to populate
    with data, and then supply the data. That should work for us. If we did this,
    then the following is how our program would evolve:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要告诉我们的自动编码器我们想要用数据填充哪一层，然后提供数据。这应该对我们有效。如果我们这样做，那么我们的程序将像这样发展：
- en: '[PRE28]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: What do you think so far? With this code, we would be able to populate three
    layers with data. I threw in an extra function, `PreTrainingComplete`, as a nice
    way to let our program know that we have finished pre-training. Now, let's figure
    out how those functions come together.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你怎么看？有了这个代码，我们就能用数据填充三个层。我加入了一个额外的函数`PreTrainingComplete`，作为一种让我们的程序知道我们已经完成了预训练的好方法。现在，让我们弄清楚这些函数是如何结合在一起的。
- en: For pretraining, we will do this in batches. We can have from 1 to *n* number
    of batches. In many cases, the number of batches will be just 1\. Once we determine
    the number of batches we want to use, we will iterate through each batch of data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练，我们将分批进行。我们可以有从1到*n*个批次的数量。在许多情况下，批次的数量将是1。一旦我们确定我们想要使用的批次数量，我们将遍历每一批数据。
- en: 'For each batch of data, we will process the data and determine whether our
    neurons were activated. We then set the layer state based upon that. We will move
    both forward and backward through the network, setting our states. Using the following
    diagram, we will move forward through layers like this *Y -> V -> W -> (Z)*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一批数据，我们将处理数据并确定我们的神经元是否被激活。然后我们根据这个设置层状态。我们将向前和向后通过网络，设置我们的状态。使用以下图表，我们将像这样通过层 *Y
    -> V -> W -> (Z)*：
- en: '![](img/0f4bef37-b92f-4a94-ab1b-1bbb8035a003.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0f4bef37-b92f-4a94-ab1b-1bbb8035a003.png)'
- en: 'Once activations are set, we must perform the actual pre-training. We do this
    in the pre-synaptic layer, starting at layer 0\. When we pre-train, we call our
    trainer object''s `Train` method, which we created earlier and then pass the layer(s)
    and the training data, our recognition weights, and learning rate. To do this,
    we will need to create our actual function, which we will call `PerformPreTraining()`.
    The following is what this code would look like:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦激活被设置，我们必须执行实际的预训练。我们在预突触层中这样做，从层0开始。当我们预训练时，我们调用我们之前创建的培训对象`Train`方法，然后传递层（s）、训练数据、我们的识别权重和学习率。为此，我们需要创建我们的实际函数，我们将称之为`PerformPreTraining()`。以下是这个代码的样子：
- en: '[PRE29]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once pre-training is complete, we now will need to calculate the error rate
    based upon the positive and negative visible data properties. That will complete
    our `pretraining` function, and our sample program will now look as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预训练完成，我们现在将需要根据正负可见数据属性计算错误率。这将完成我们的`pretraining`函数，我们的示例程序现在将如下所示：
- en: '[PRE30]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With all this code behind us, all we need to do now is to save the autoencoder,
    and we should be all set. We will do this by creating a `Save()` function in the
    autoencoder, and call it as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有了所有这些代码之后，我们现在需要做的就是保存自动编码器，我们应该就绪了。我们将通过在自动编码器中创建一个`Save()`函数并按如下方式调用它来完成这项工作：
- en: '[PRE31]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To implement this function, let's look at what we need to do. First, we need
    a filename to use for the autoencoder name. Once we open a **.NET TextWriter**
    object, we then save the learning rates, the recognition weights, and generative
    weights. Next, we iterate through all the layers, write out the layer type, and
    then save the data. If you decide to implement more types of RBM layers than we
    created, make sure that you in turn update the `Save()` and `Load()` methods so
    that your new layer data is saved and re-loaded correctly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这个功能，让我们看看我们需要做什么。首先，我们需要一个用于自动编码器名称的文件名。一旦我们打开一个**.NET TextWriter**对象，我们就保存学习率、识别权重和生成权重。接下来，我们遍历所有层，写出层类型，然后保存数据。如果你决定实现比我们创建的更多类型的RBM层，确保你相应地更新`Save()`和`Load()`方法，以便你的新层数据能够正确保存和重新加载。
- en: 'Let''s look at our `Save` function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的 `Save` 函数：
- en: '[PRE32]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With our autoencoder saved to disk, we now should really deal with the ability
    to reload that data into memory and create an autoencoder from it. So, we'll now
    need a `Load()` function. We'll need to basically follow the steps we did to write
    our autoencoder to disk but, this time, we'll read them in, instead of writing
    them out. Our weights, learning rate, and layers will have also a `Load()` function,
    just like each of the preceding items had a `Save()` function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将自动编码器保存到磁盘上，我们应该真正处理将数据重新加载到内存中并从中创建自动编码器的功能。因此，我们现在需要一个 `Load()` 函数。我们需要基本上遵循我们写入磁盘时所做的步骤，但这次我们将读取它们，而不是写入它们。我们的权重、学习率和层也将有一个
    `Load()` 函数，就像前面的每个项目都有一个 `Save()` 函数一样。
- en: 'Our `Load()` function will be a bit different in its declaration. Since we
    are loading in a saved autoencoder, we have to assume that, at the time this call
    is made, an autoencoder object has not yet been created. Therefore, we will make
    this function `static()` on the autoencoder object itself, as it will return a
    newly created autoencoder for us. Here''s how our function will look:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `Load()` 函数在声明上会有所不同。由于我们正在加载一个已保存的自动编码器，我们必须假设在调用此函数时，自动编码器对象尚未创建。因此，我们将在这个自动编码器对象本身上将其函数声明为
    `static()`，因为它将为我们返回一个新创建的自动编码器。我们的函数将看起来是这样的：
- en: '[PRE33]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With that done, let''s see how we would call our `Load()` function. It should
    be like the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 做完这些后，让我们看看我们如何调用我们的 `Load()` 函数。它应该像以下这样：
- en: '[PRE34]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'So, let''s stop here and take a look at all we''ve accomplished. Let''s see
    what our program can do, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们在这里停下来，看看我们取得了哪些成果。让我们看看我们的程序能做什么，如下所示：
- en: '[PRE35]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Well, folks, I think it's time to wrap this chapter up and move on. You should
    commend yourself, as you've written a complete autoencoder from start to (almost)
    finish. In the accompanying source code, I have added even more functions to make
    this more complete, and for you to have a better starting point from which to
    make this a powerful framework for you to use. As you are enhancing this, think
    about the things you need your autoencoder to do, block in those functions, and
    then complete them as we have done. Rather than learn to use an open-source framework,
    you've built your own—congratulations!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，朋友们，我认为是时候结束这一章并继续前进了。您应该为自己感到自豪，因为您已经从开始（几乎）完成了完整的自动编码器。在配套的源代码中，我添加了更多函数来使其更加完整，并为您提供一个更好的起点，以便将其构建成一个强大的框架供您使用。在增强这个框架的过程中，请考虑您需要自动编码器执行的任务，将这些函数块在里，然后像我们一样完成它们。您不是学习如何使用开源框架，而是构建了自己的——恭喜您！
- en: I have taken the liberty of developing a bit more of our autoencoder framework
    with the supplied source code. You can feel free to use it, discard it, or modify
    it to suit your needs. It's useful, but, as I mentioned, please feel free to embellish
    this and make it your own, even if it's just for educational purposes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经利用提供的源代码对我们的自动编码器框架进行了更多的发展。您可以自由使用、丢弃或修改它以满足您的需求。它很有用，但正如我提到的，请随意对其进行装饰，使其成为您自己的东西，即使只是为了教育目的。
- en: 'So, let''s briefly recap what we have learned in this chapter: we learned about
    autoencoders and different variants, and we wrote our own autoencoder and created
    some powerful functionality. In the next chapter, we are going to move on to perhaps
    my most intense passion, and I hope it will soon be yours, **swarm intelligence**.
    There''s going to be some theory, of course, but, once we''re discussed that,
    I think you''re going to be impressed with what particle swarm optimization algorithms
    can do!'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们简要回顾一下本章我们学到了什么：我们学习了自动编码器和不同的变体，我们编写了自己的自动编码器并创建了一些强大的功能。在下一章，我们将继续探讨我最大的热情，我希望它很快也会成为您的热情，那就是**群体智能**。当然，会有一些理论，但一旦我们讨论了这些，我想你会对粒子群优化算法能做什么感到印象深刻！
- en: References
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Vincent P, La Rochelle H, Bengio Y, and Manzagol P A (2008), *Extracting and
    Composing Robust Features with Denoising Autoencoders*, proceedings of the 25th
    international conference on machine learning (ICML, 2008), pages 1,096 - 1,103,
    ACM.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent P, La Rochelle H, Bengio Y, 和 Manzagol P A (2008)，*使用去噪自动编码器提取和组合鲁棒特征*，第25届国际机器学习会议（ICML，2008）论文集，第1,096
    - 1,103页，ACM。
- en: Vincent, Pascal, et al, *Extracting and Composing Robust Features with De-noising
    Autoencoders.,* proceedings of the 25th international conference on machine learning.
    ACM, 2008.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent, Pascal, 等人，*使用去噪自动编码器提取和组合鲁棒特征*，第25届国际机器学习会议论文集。ACM，2008。
- en: Kingma, Diederik P and Max Welling*, Auto-encoding variational bayes,* arXiv
    pre-print arXiv:1312.6114 (2013).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma, Diederik P 和 Max Welling*, *自编码变分贝叶斯*, arXiv 预印本 arXiv:1312.6114 (2013).
- en: Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun, *Efficient
    Learning of Sparse Representations with an Energy-Based Model,* proceedings of
    NIPS, 2007.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marc'Aurelio Ranzato, Christopher Poultney, Sumit Chopra, 和 Yann LeCun, *基于能量模型的稀疏表示的高效学习，NIPS
    会议论文，2007*.
- en: 'Bourlard, Hervé, and Yves Kamp, *Auto Association by Multilayer Perceptrons
    and Singular Value Decomposition, Biological Cybernetics 59.4–5* (1988): 291-294.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bourlard, Hervé, 和 Yves Kamp, *多层感知器和奇异值分解的多层感知器自联想，生物控制论 59.4–5* (1988): 291-294.'
