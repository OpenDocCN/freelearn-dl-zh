- en: Implementing your First Learning Agent - Solving the Mountain Car problem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现你的第一个学习智能体——解决山地车问题
- en: 'Well done on making it this far! In previous chapters, we got a good introduction
    to OpenAI Gym, its features, and how to install, configure, and use it in your
    own programs. We also discussed the basics of reinforcement learning and what
    deep reinforcement learning is, and we set up the PyTorch deep learning library
    to develop deep reinforcement learning applications. In this chapter, you will
    start developing your first learning agent! You will develop an intelligent agent
    that will learn how to solve the Mountain Car problem. Gradually in the following
    chapters, we will solve increasingly challenging problems as you get more comfortable
    developing reinforcement learning algorithms to solve problems in OpenAI Gym.
    We will start this chapter by understanding the Mountain Car problem, which has
    been a popular problem in the reinforcement learning and optimal control community.
    We will develop our learning agent from scratch and then train it to solve the
    Mountain Car problem using the Mountain Car environment in the Gym. We will finally
    see how the agent progresses and briefly look at ways we can improve the agent
    to use it for solving more complex problems. The topics we will be covering in
    this chapter are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到这一步！在前面的章节中，我们已经对OpenAI Gym进行了很好的介绍，了解了它的功能，并学习了如何在自己的程序中安装、配置和使用它。我们还讨论了强化学习的基础知识，以及深度强化学习是什么，并设置了PyTorch深度学习库来开发深度强化学习应用程序。在本章中，你将开始开发你的第一个学习智能体！你将开发一个智能体，学习如何解决山地车问题。在接下来的章节中，我们将解决越来越具挑战性的问题，随着你在OpenAI
    Gym中开发强化学习算法来解决问题，你会变得更加熟练。我们将从理解山地车问题开始，这是强化学习和最优控制领域中的一个经典问题。我们将从零开始开发我们的学习智能体，然后训练它利用Gym中的山地车环境来解决问题。最后，我们将看到智能体的进展，并简要了解如何改进智能体，使其能够解决更复杂的问题。本章将覆盖以下主题：
- en: Understanding the Mountain Car problem
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解山地车问题
- en: Implementing a reinforcement learning-based agent to solve the Mountain Car
    problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基于强化学习的智能体来解决山地车问题
- en: Training a reinforcement learning agent at the Gym
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Gym中训练强化学习智能体
- en: Testing the performance of the agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试智能体的性能
- en: Understanding the Mountain Car problem
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解山地车问题
- en: For any reinforcement learning problem, two fundamental definitions concerning
    the problem are important, irrespective of the learning algorithm we use. They
    are the definitions of the state space and the action space. We mentioned earlier
    in this book that the state and action spaces could be discrete or continuous.
    Typically, in most problems, the state space consists of continuous values and
    is represented as a vector, matrix, or tensor (a multi-dimensional matrix). Problems
    and environments with discrete action spaces are relatively easy compared to continuous
    valued problems and environments. In this book, we will develop learning algorithms
    for a few problems and environments with a mix of state space and action space
    combinations so that you are comfortable dealing with any such variation when
    you start out on your own and develop intelligent agents and algorithms for your
    applications.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何强化学习问题，有两个基本的定义是至关重要的，无论我们使用什么学习算法。这两个定义分别是状态空间和动作空间的定义。我们在本书前面提到过，状态空间和动作空间可以是离散的，也可以是连续的。通常，在大多数问题中，状态空间由连续值组成，并且表示为向量、矩阵或张量（多维矩阵）。与连续值问题和环境相比，具有离散动作空间的问题和环境相对简单。在本书中，我们将为一些具有状态空间和动作空间组合的不同问题和环境开发学习算法，以便当你开始独立开发智能体和算法时，能够应对任何此类变化。
- en: Let's start by understanding the Mountain Car problem with a high-level description,
    before moving on to look at the state and action spaces of the Mountain Car environment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过高级描述来理解山地车问题，然后再看一下山地车环境的状态空间和动作空间。
- en: The Mountain Car problem and environment
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 山地车问题和环境
- en: In the Mountain Car Gym environment, a car is on a one-dimensional track, positioned
    between two mountains. The goal is to drive the car up the mountain on the right;
    however, the car's engine is not strong enough to drive up the mountain even at
    the maximum speed. Therefore, the only way to succeed is to drive back and forth
    to build up momentum. In short, the Mountain Car problem is to get an under-powered
    car to the top of a hill.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mountain Car Gym 环境中，一辆车位于一条一维轨道上，位置在两座山之间。目标是将车开到右边的山顶；然而，汽车的引擎力量不足，即使以最大速度行驶也无法爬上山顶。因此，唯一成功的方法是前后驱动来积累动能。简而言之，Mountain
    Car 问题就是让一辆动力不足的车爬上山顶。
- en: 'Before you implement your agent algorithm, it will help tremendously to understand
    the environment, the problem, and the state and action spaces. How do we find
    out the state and action spaces of the Mountain Car environment in the Gym? Well,
    we already know how to do that from [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*. We wrote a script named `get_observation_action_space.py`,
    which will print out the state and observation and action spaces of the environment
    whose name is passed as the first argument to the script. Let''s ask it to print
    the spaces for the `MountainCar-v0` environment with the following command:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现你的智能体算法之前，理解环境、问题以及状态和动作空间会大有帮助。我们如何知道 Mountain Car 环境的状态和动作空间呢？嗯，我们已经知道如何做了，从[第4章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)，*探索
    Gym 及其特性*中。我们编写了一个名为 `get_observation_action_space.py` 的脚本，它会打印出作为脚本第一个参数传入的环境的状态、观察和动作空间。让我们用以下命令来让它打印出
    `MountainCar-v0` 环境的空间：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that the command prompt has the `rl_gym_book` prefix, which signifies that
    we have activated the `rl_gym_book` conda Python virtual environment. Also, the
    current directory, `~/rl_gym_book/ch4`, signifies that the script is run from
    the `ch4` directory corresponding to the code for [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12),
    *Exploring the Gym and its Features*, in the code repository for this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意命令提示符中有 `rl_gym_book` 前缀，这表示我们已激活了 `rl_gym_book` conda Python 虚拟环境。此外，当前目录
    `~/rl_gym_book/ch4` 表示脚本是从本书代码库中与[第4章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)，*探索
    Gym 及其特性*对应的 `ch4` 目录中运行的。
- en: 'The preceding command will produce output like the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将产生如下输出：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: From this output, we can see that the state and observation space is a two-dimensional
    box and the action space is three-dimensional and discrete.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以看到状态和观察空间是一个二维盒子，而动作空间是三维的且离散的。
- en: If you want a refresher on what **box **and **discrete **spaces mean, you can
    quickly flip to [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12),
    *Exploring the Gym and its Features*, where we discussed these spaces and what
    they mean under the *Spaces in the Gym*section. It is important to understand
    them.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想复习 **box** 和 **discrete** 空间的含义，你可以快速翻到[第4章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)，*探索
    Gym 及其特性*，在其中我们讨论了这些空间以及它们的含义，具体在 *Gym 中的空间* 部分。理解它们是非常重要的。
- en: 'The state and action space type, description, and range of allowed values are
    summarized in the following table for your reference:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 状态和动作空间类型、描述以及允许值的范围总结在下面的表格中，供你参考：
- en: '| **MountainCar-v0 environment** | **Type** | **Description** | **Range** |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| **MountainCar-v0 环境** | **类型** | **描述** | **范围** |'
- en: '| State space | `Box(2,)` | (position, velocity) | Position: -1.2 to 0.6Velocity:
    -0.07 to 0.07 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 状态空间 | `Box(2,)` | （位置，速度） | 位置：-1.2 到 0.6 速度：-0.07 到 0.07 |'
- en: '| Action space | `Discrete(3)` | 0: Go left1: Coast/do-nothing2: Go right |
    0, 1, 2 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 动作空间 | `Discrete(3)` | 0: 向左走 1: 滑行/不动作 2: 向右走 | 0, 1, 2 |'
- en: So for example, the car starts at a random position between *-0.6* and *-0.4*
    with zero velocity, and the goal is to reach the top of the hill on the right
    side, which is at position *0.5*. (The car can technically go beyond *0.5,* up
    to *0.6*, which is also considered.) The environment will send *-1* as a reward
    every time step until the goal position (*0.5*) is reached. The environment will
    terminate the episode. The `done` variable will be equal to `True` if the car
    reaches the *0.5* position or the number of steps taken reaches 200.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，小车从* -0.6 *到* -0.4 *之间的随机位置开始，初始速度为零，目标是到达右侧山顶，位置为*0.5*。（实际上，小车可以超过*0.5*，到达*0.6*，也会被考虑。）每个时间步，环境将返回*
    -1 *作为奖励，直到小车到达目标位置（*0.5*）。环境将终止回合。如果小车到达*0.5*位置或采取的步骤数达到200，`done`变量将被设置为`True`。
- en: Implementing a Q-learning agent from scratch
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始实现Q学习智能体
- en: In this section, we will start implementing our intelligent agent step-by-step.
    We will be implementing the famous Q-learning algorithm using the `NumPy` library
    and the `MountainCar-V0` environment from the OpenAI Gym library.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将一步一步地开始实现我们的智能体。我们将使用`NumPy`库和OpenAI Gym库中的`MountainCar-V0`环境来实现著名的Q学习算法。
- en: 'Let''s revisit the reinforcement learning Gym boiler plate code we used in
    [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新回顾一下我们在[第4章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)中使用的强化学习Gym基础模板代码，*探索Gym及其特性*，如下所示：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code is a good starting point (aka boilerplate!) for developing our reinforcement
    learning agent. We will first start by changing the environment name from `Qbert-v0`
    to `MountainCar-v0`. Notice in the preceding script that we are setting `MAX_STEPS_PER_EPISODE`.
    This is the number of steps or actions that the agent can take before the episode
    ends. This may be useful in continuing, perpetual, or looping environments, where
    the environment itself does not end the episode. Here, we set a limit for the
    agent to avoid infinite loops. However, most of the environments defined in OpenAI
    Gym have an episode termination condition and once either of them is satisfied,
    the `done` variable returned by the `env.step(...)` function will be set to *True*.
    We saw in the previous section that for the Mountain Car problem we are interested
    in, the environment will terminate the episode if the car reaches the goal position
    (*0.5*) or if the number of steps taken reaches *200*. Therefore, we can further
    simplify the boilerplate code to look like the following for the Mountain Car
    environment:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是开发我们强化学习智能体的一个良好起点（即基础模板！）。我们将首先将环境名称从`Qbert-v0`更改为`MountainCar-v0`。注意，在前面的脚本中，我们设置了`MAX_STEPS_PER_EPISODE`。这是智能体在回合结束前可以采取的步骤或动作的数量。这在持续的、循环的环境中非常有用，在这种环境中，环境本身不会结束回合。这里，我们为智能体设置了一个限制，以避免无限循环。然而，大多数在OpenAI
    Gym中定义的环境都有回合终止条件，一旦满足其中任何一个条件，`env.step(...)`函数返回的`done`变量将被设置为*True*。我们在前面的章节中看到，对于我们感兴趣的Mountain
    Car问题，如果小车到达目标位置（*0.5*）或所采取的步骤数达到*200*，环境将结束回合。因此，我们可以进一步简化基础模板代码，使其看起来如下，适用于Mountain
    Car环境：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you run the preceding script, you will see the Mountain Car environment
    come up in a new window and the car moving left and right randomly for 1,000 episodes.
    You will also see the episode number, steps taken, and the total reward obtained
    printed at the end of every episode, as shown in the following screenshot:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面的脚本，你将看到Mountain Car环境在一个新窗口中出现，小车会在1,000个回合中随机地左右移动。你还会看到每个回合结束时，打印出回合编号、所采取的步骤数以及获得的总奖励，正如以下屏幕截图所示：
- en: '![](img/00119.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00119.jpeg)'
- en: 'The sample output should look similar to the following screenshot:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 样本输出应该类似于以下屏幕截图：
- en: '![](img/00120.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00120.jpeg)'
- en: You should recall from our previous section that the agent gets a reward of
    *-1* for each step and that the `MountainCar-v0` environment will terminate the
    episode after *200* steps; this is why you the agent may sometimes get a total
    reward of *-200!* After all, the agent is taking random actions without thinking
    or learning from its previous actions. Ideally, we would want the agent to figure
    out how to reach the top of the mountain (near the flag, close to, at, or beyond
    position *0.5*) with the minimum number of steps. Don't worry - we will build
    such an intelligent agent by the end of this chapter!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该从前一节回忆起，代理每一步都会获得*–1*的奖励，而且`MountainCar-v0`环境会在*200*步后终止任务；这就是为什么代理有时会获得总奖励*–200!*的原因！毕竟，代理在没有思考或从之前的动作中学习的情况下进行随机行动。理想情况下，我们希望代理能够找出如何用最少的步骤到达山顶（接近旗帜，靠近、到达或超越位置*0.5*）。别担心——我们将在本章结束时构建这样一个智能代理！
- en: Remember to always activate the `rl_gym_book` conda environment before running
    the scripts! Otherwise, you might run into Module not found errors unnecessarily.
    You can visually confirm whether you have activated the environment by looking
    at the shell prefix, which will show something like this: `(rl_gym_book) praveen@ubuntu:~/rl_gym_book/ch5$`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在运行脚本之前始终激活`rl_gym_book` conda 环境！否则，你可能会遇到不必要的模块未找到错误。你可以通过查看 shell 前缀来直观确认是否已激活环境，前缀会显示如下内容：`(rl_gym_book)
    praveen@ubuntu:~/rl_gym_book/ch5$`。
- en: Let's move on by having a look at what Q-learning section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续看一下 Q-learning 部分。
- en: Revisiting Q-learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重温 Q-learning
- en: 'In [Chapter 2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12), *Reinforcement
    Learning and Deep Reinforcement Learning*, we discussed the SARSA and Q-learning
    algorithms. Both of these algorithms provide a systematic way to update the estimate
    of the action-value function denoted by ![](img/00121.jpeg). In particular, we
    saw that Q-learning is an off-policy learning algorithm, which updates the action-value
    estimate of the current state and action towards the maximum obtainable action-value
    in the subsequent state, ![](img/00122.jpeg), which the agent will end up in according
    to its policy. We also saw that the Q-learning update is given by the following
    formula:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12)，*强化学习与深度强化学习*中，我们讨论了SARSA和Q-learning算法。这两种算法提供了一种系统的方法来更新表示行动值函数的估计，记作
    ![](img/00121.jpeg)。具体而言，我们看到Q-learning是一种离策略学习算法，它通过以下方式更新当前状态和动作的行动值估计：向后续状态的最大可获得行动值，![](img/00122.jpeg)，该状态根据代理的策略最终会到达。我们还看到，Q-learning更新的公式如下所示：
- en: '![](img/00123.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00123.jpeg)'
- en: In the next section, we will implement a `Q_Learner`class in Python, which implements this
    learning update rule along with the other necessary functions and methods.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将在 Python 中实现一个`Q_Learner`类，该类实现了学习更新规则以及其他必要的函数和方法。
- en: Implementing a Q-learning agent using Python and NumPy
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 和 NumPy 实现 Q-learning 代理
- en: 'Let''s begin implementing our Q-learning agent by implementing the `Q_Learner`
    class. The main methods of this class are the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过实现`Q_Learner`类来开始实现我们的 Q-learning 代理。该类的主要方法如下：
- en: __init__(self, env)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __init__(self, env)
- en: discretize(self, obs)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: discretize(self, obs)
- en: get_action(self, obs)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: get_action(self, obs)
- en: learn(self, obs, action, reward, next_obs)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: learn(self, obs, action, reward, next_obs)
- en: You will later find that the methods in here are common and exist in almost
    all the agents we will be implementing in this book. This makes it easy for you
    to grasp them, as these methods will be repeated (with some modifications) again
    and again.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你稍后会发现，这里的方法是常见的，几乎所有我们在本书中实现的代理都会用到这些方法。这使得你更容易掌握它们，因为这些方法会一遍又一遍地被重复使用（会有一些修改）。
- en: The `discretize()` function is not necessary for agent implementations in general,
    but when the state space is large and continuous, it may be better to discretize
    the space into countable bins or ranges of values to simplify the representation.
    This also reduces the number of values that the Q-learning algorithm needs to
    learn, as it now only has to learn a finite set of values, which can be concisely
    represented in tabular formats or by using *n*-dimensional arrays instead of complex
    functions. Moreover, the Q-learning algorithm, used for optimal control, is guaranteed
    to converge for tabular representations of Q-values.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`discretize()`函数通常对代理实现不是必需的，但当状态空间很大且是连续时，将空间离散化为可计数的区间或值范围可能更好，从而简化表示。这还减少了Q学习算法需要学习的值的数量，因为它现在只需学习有限的值集，这些值可以简洁地以表格格式或使用*n*维数组表示，而不是复杂的函数。此外，用于最优控制的Q学习算法，对于Q值的表格表示，保证会收敛。'
- en: Defining the hyperparameters
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'Before our `Q_Learner` class declaration, we will initialize a few useful hyperparameters.
    Here are the hyperparameters that we will be using for our `Q_Learner` implementation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Q_Learner`类声明之前，我们将初始化一些有用的超参数。以下是我们将用于`Q_Learner`实现的超参数：
- en: '`EPSILON_MIN`: This is the minimum value of the epsilon value that we want
    the agent to use while following an epsilon-greedy policy.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EPSILON_MIN`：这是我们希望代理在遵循epsilon-greedy策略时使用的epsilon值的最小值。'
- en: '`MAX_NUM_EPISODES`:The maximum number of episodes that we want the agent to
    interact with the environment for.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MAX_NUM_EPISODES`：这是我们希望代理与环境交互的最大回合数。'
- en: '`STEPS_PER_EPISODE`: This is the number of steps in each episode. This could
    be the maximum number of steps that an environment will allow per episode or a
    custom value that we want to limit based on some time budget. Allowing a higher
    number of steps per episode means each episode might take longer to complete and
    in non-terminating environments, the environment won''t be reset until this limit
    is reached, even if the agent is stuck at the same spot.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STEPS_PER_EPISODE`：这是每个回合中的步数。它可以是环境在每个回合中允许的最大步数，或者是我们基于某些时间预算希望设置的自定义值。允许每个回合更多的步数意味着每个回合可能需要更长时间才能完成，在非终止环境中，环境直到达到此限制才会重置，即使代理停留在同一位置。'
- en: '`ALPHA`: This is the learning rate that we want the agent to use. This is the
    alpha in the Q-learning update equation listed in the previous section. Some algorithms
    vary the learning rate as the training progresses.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ALPHA`：这是我们希望代理使用的学习率。这是上一节中列出的Q学习更新方程中的alpha。一些算法会随着训练的进行调整学习率。'
- en: '`GAMMA`: This is the discount factor that the agent will use to factor in future
    rewards. This value corresponds to the gamma in the Q-learning update equation
    in the previous section.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GAMMA`：这是代理将用于考虑未来奖励的折扣因子。此值对应于上一节中Q学习更新方程中的gamma。'
- en: '`NUM_DISCRETE_BINS`: This is the number of bins of values that the state space
    will be discretized into. For the Mountain Car environment, we will be discretizing
    the state space into *30* bins. You can play around with higher/lower values.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NUM_DISCRETE_BINS`：这是将状态空间离散化为的值的区间数量。对于Mountain Car环境，我们将状态空间离散化为*30*个区间。你可以尝试更高或更低的值。'
- en: Note that the `MAX_NUM_EPISODES`and `STEPS_PER_EPISODE` have been defined in
    the boilerplate code we went through in one of the previous sections of this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`MAX_NUM_EPISODES`和`STEPS_PER_EPISODE`已经在我们之前某一节中讲解的代码模板中定义。
- en: 'These hyperparameters are defined in the Python code like this, with some initial
    values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些超参数在Python代码中定义如下，并具有一些初始值：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Implementing the Q_Learner class's __init__ method
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Q_Learner类的__init__方法
- en: 'Next, let us look into the `Q_Learner` class''s member function definitions.
    The `__init__(self, env)` function takes the environment instance, `env`, as an
    input argument and initializes the dimensions/shape of the observation space and
    the action space, and also determines the parameters to discretize the observation
    space based on the `NUM_DISCRETE_BINS` we set. The `__init__(self, env)` function
    also initializes the Q function as a NumPy array, based on the shape of the discretized
    observation space and the action space dimensions. The implementation of `__init__(self,
    env)` is straightforward as we are only initializing the necessary values for
    the agent. Here is our implementation:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下`Q_Learner`类的成员函数定义。`__init__(self, env)`函数将环境实例`env`作为输入参数，并初始化观察空间和行动空间的维度/形状，同时根据我们设置的`NUM_DISCRETE_BINS`确定离散化观察空间的参数。`__init__(self,
    env)`函数还会根据离散化后的观察空间形状和行动空间的维度，将Q函数初始化为一个NumPy数组。`__init__(self, env)`的实现是直接的，因为我们只是在初始化智能体所需的值。以下是我们的实现：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Implementing the Q_Learner class's discretize method
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Q_Learner类的discretize方法
- en: 'Let''s take a moment to understand how we are discretizing the observation
    space. The simplest, and yet an effective, way to discretize the observation space
    (and a metric space in general) is to divide the span of the range of values into
    a finite set of values called bins. The span/range of values is given by the difference
    between the maximum possible value and the minimum possible value in each dimension
    of the space. Once we calculate the span, we can divide it by the `NUM_DISCRETE_BINS` that
    we have decided on to get the width of the bin. We calculated the bin width in
    the `__init__` function because it does not change with every new observation.
    The `discretize(self, obs)` function receives every new function and applies the
    discretization step to find the bin that the observation belongs to in the discretized
    space. It is as simple as doing this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间理解一下我们是如何离散化观察空间的。最简单且有效的离散化观察空间（以及一般度量空间）的方法是将值的范围划分为一个有限的集合，称为“箱子”（bins）。值的跨度/范围是由空间中每个维度的最大可能值与最小可能值之间的差异给出的。一旦我们计算出跨度，就可以将其除以我们决定的`NUM_DISCRETE_BINS`，从而得到箱子的宽度。我们在`__init__`函数中计算了箱子的宽度，因为它在每次新的观察中不会改变。`discretize(self,
    obs)`函数接收每个新的观察值并应用离散化步骤，以确定观察值在离散化空间中属于哪个箱子。做法其实很简单：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We want it to belong to any *one* of the bins (and not somewhere in between);
    therefore, we convert the previous code into an `integer`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望它属于任何*一个*箱子（而不是在两个箱子之间）；因此，我们将前面的代码转换为一个`整数`：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we return this discretized observation as a tuple. All of this operation
    can be written in one line of Python code, like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这个离散化的观察值作为一个元组返回。所有这些操作都可以用一行Python代码表示，如下所示：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Implementing the Q_Learner's get_action method
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Q_Learner类的get_action方法
- en: 'We want the agent to taken an action given an observation. `get_action(self,
    obs)`is the function we define to generate an action, given an observation in `obs`.The
    most widely used action selection policy is the epsilon-greedy policy, which takes
    the best action as per the agent''s estimate with a (high) probability of *1-*![](img/00124.jpeg),
    and takes a random action with a (small) probability given by epsilon ![](img/00125.jpeg).
    We implement the epsilon-greedy policy using the `random()` method from NumPy''s
    random module, like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望智能体根据观察值采取行动。`get_action(self, obs)`是我们定义的函数，用来根据`obs`中的观察值生成一个行动。最常用的行动选择策略是epsilon-greedy策略，它以（高）概率*1-*选择智能体估计的最佳行动！[](img/00124.jpeg)，并以由epsilon给定的（小）概率选择随机行动！[](img/00125.jpeg)。我们使用NumPy的random模块中的`random()`方法来实现epsilon-greedy策略，像这样：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Implementing the Q_learner class's learn method
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Q_Learner类的learn方法
- en: 'As you might have guessed, this is the most important method of the `Q_Learner`
    class, which does the magic of learning the Q-values, which in turn enables the
    agent to take intelligent actions over time! The best part is that it is not that
    complicated to implement! It is merely the implementation of the Q-learning update
    equation that we saw earlier. Don''t believe me when I say it is simple to implement?!
    Alright, here is the implementation of the learning function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，这就是`Q_Learner`类中最重要的方法，它实现了学习Q值的魔法，从而使得智能体能够随着时间的推移采取智能的行动！最棒的部分是，它的实现并不像看起来那样复杂！它仅仅是我们之前看到的Q学习更新方程的实现。你不相信我说它很简单吗？！好吧，这是学习函数的实现：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now do you agree? :)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你同意了吗？ :)
- en: 'We could have written the Q learning update rule in one line of code, like
    this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以将Q学习更新规则写成一行代码，像这样：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: But, calculating each term on a separate line will make it easier to read and
    understand.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，将每个项分别写在单独的行中会让它更易于阅读和理解。
- en: Full Q_Learner class implementation
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整的Q_Learner类实现
- en: 'If we put all the method implementations together, we will get a code snippet
    that looks like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有方法的实现放在一起，就会得到一个看起来像这样的代码片段：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, we have the agent ready. What should we do next, you may ask. Well, we should
    train the agent in the Gym environment! In the next section, we will look at the
    training procedure.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经准备好智能体了。你可能会问，接下来我们该做什么呢？好吧，我们应该在Gym环境中训练智能体！在下一部分，我们将看看训练过程。
- en: Training the reinforcement learning agent at the Gym
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Gym中训练强化学习智能体
- en: 'The procedure to train the Q-learning agent may look familiar to you already,
    because it has many of the same lines of code as, and also a similar structure
    to, the boilerplate code that we used before. Instead of choosing a random action
    from the environment''s actions space, we now get the action from the agent using
    the `agent.get_action(obs)` method. We also call the `agent.learn(obs, action,
    reward, next_obs)` method after sending the agent''s action to the environment
    and receiving the feedback. The training function is listed here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练Q-learning智能体的过程你可能已经很熟悉了，因为它有很多与我们之前使用的模板代码相同的行，并且结构也相似。我们不再从环境的动作空间中随机选择动作，而是通过`agent.get_action(obs)`方法从智能体中获取动作。在将智能体的动作发送到环境并收到反馈后，我们还会调用`agent.learn(obs,
    action, reward, next_obs)`方法。训练函数如下所示：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Testing and recording the performance of the agent
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试并记录智能体的表现
- en: 'Once we let the agent train at the Gym, we want to be able to measure how well
    it has learned. To do that, we let the agent go through a test. Just like in school! `test(agent,
    env, policy)` takes the agent object, the environment instance, and the agent''s
    policy to test the performance of the agent in the environment, and returns the
    total reward for one full episode. It is similar to the `train(agent, env)` function
    we saw earlier, but it does not let the agent learn or update its Q-value estimates:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们让智能体在Gym中进行训练，我们就希望能够衡量它的学习效果。为此，我们让智能体进行测试。就像在学校一样！`test(agent, env, policy)`接受智能体对象、环境实例和智能体的策略，以测试智能体在环境中的表现，并返回一个完整回合的总奖励。它类似于我们之前看到的`train(agent,
    env)`函数，但不会让智能体学习或更新它的Q值估计：
- en: '[PRE14]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A simple and complete Q-Learner implementation for solving the Mountain Car
    problem
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单而完整的Q-Learner实现，用于解决Mountain Car问题
- en: In this section, we will put together the whole code into a single Python script
    to initialize the environment, launch the agent's training process, get the trained
    policy, test the performance of the agent, and also record how it acts in the
    environment!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将整个代码整合到一个单独的Python脚本中，初始化环境，启动智能体的训练过程，获得训练后的策略，测试智能体的表现，并记录它在环境中的行为！
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This script is available in the code repository under the `ch5` folder, named `Q_learner_MountainCar.py`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本可在代码仓库的`ch5`文件夹中找到，名为`Q_learner_MountainCar.py`。
- en: 'Activate the `rl_gym_book` conda environment and launch the script to see it
    in action! When you launch the script, you will see initial output like that shown
    in this screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 激活`rl_gym_book`的conda环境并启动脚本，查看其运行效果！当你启动脚本时，你将看到类似于这张截图的初始输出：
- en: '![](img/00126.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00126.jpeg)'
- en: During the initial training episodes, when the agent is just getting started
    learning, you will see that it always ends up with a reward of *-200*. From your
    understanding of how the Gym's Mountain Car environment works, you can see that
    the agent does not reach the mountain top within the *200* time steps, and so
    the environment automatically resets the environment; thus, the agent only gets
    *-200*. You can also observe the **![](img/00127.jpeg) **(**eps**) exploration
    value decaying slowly.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始训练过程中，当智能体刚开始学习时，你会看到它总是以*-200*的奖励结束。从你对Gym的Mountain Car环境的理解来看，你可以看出智能体在*200*个时间步内没有到达山顶，因此环境会自动重置；因此，智能体只得到*-200*。你还可以观察到**![](img/00127.jpeg)**（**eps**）探索值缓慢衰减。
- en: 'If you let the agent learn for long enough, you will see the agent improving
    and learning to reach the top of the mountain in fewer and fewer steps. Here is
    a sample of its progress after *5* minutes of training on a typical laptop hardware:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你让智能体学习足够长的时间，你会看到它逐步提高，并且学会在越来越少的步骤内到达山顶。这是它在典型笔记本电脑硬件上训练*5*分钟后的进展示例：
- en: '![](img/00128.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00128.jpeg)'
- en: Once the script finishes running, you will see the recorded videos (along with
    some `.stats.json` and `.meta.json` files) of the agent's performance in the `gym_monitor_output` folder.
    You can watch the videos to see how your agent performed!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦脚本运行完成，你将看到记录的视频（以及一些`.stats.json`和`.meta.json`文件），它们位于`gym_monitor_output`文件夹中。你可以观看这些视频，看看你的智能体表现如何！
- en: 'Here is a screenshot showing the agent successfully steering the car to the
    top of the mountain:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一张截图，展示了智能体成功地将汽车引导到山顶：
- en: '![](img/00129.jpeg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00129.jpeg)'
- en: Hooray!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 万岁！
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We learned a lot in this chapter. More importantly, we implemented an agent
    that learned to solve the Mountain Car problem smartly in 7 minutes or so!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中我们学到了很多内容。更重要的是，我们实现了一个智能体，它在大约7分钟内学会了聪明地解决Mountain Car问题！
- en: We started by understanding the famous Mountain Car problem and looking at how
    the environment, the observation space, the state space, and rewards are designed
    in the Gym's `MountainCar-v0` environment. We revisited the reinforcement learning
    Gym boilerplate code we used in the previous chapter and made some improvements
    to it, which are also available in the code repository of this book.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先理解了著名的Mountain Car问题，并查看了Gym中`MountainCar-v0`环境的设计，包括环境、观察空间、状态空间和奖励的设计。我们重新审视了上一章中使用的强化学习Gym模板代码，并对其进行了改进，这些改进也已在本书的代码库中提供。
- en: We then defined the hyperparameters for our Q-learning agent and started implementing
    a Q-learning algorithm from scratch. We first implemented the agent's initialization
    function to initialize the agent's internal state variables, including the Q value
    representation, using a NumPy *n*-dimensional array. Then, we implemented the
    `discretize` method to discretize the `state space`; the `get_action(...)` method
    to select an action based on an epsilon-greedy policy; and then finally the `learn(...)`
    function, which implements the Q-learning update rule and forms the core of the
    agent. We saw how simple it was to implement them all! We also implemented functions
    to train, test, and evaluate the agent's performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为Q学习智能体定义了超参数，并开始从零实现Q学习算法。我们首先实现了智能体的初始化函数，初始化智能体的内部状态变量，包括使用NumPy的*n*维数组表示Q值。接着，我们实现了`discretize`方法，将`状态空间`离散化；`get_action(...)`方法根据epsilon贪婪策略选择一个动作；最后，我们实现了`learn(...)`函数，它实现了Q学习的更新规则，并构成了智能体的核心。我们看到实现这些功能是多么简单！我们还实现了用于训练、测试和评估智能体表现的函数。
- en: I hope you had a lot of fun implementing the agent and watching it solve the
    Mountain Car problem at the Gym! We will get into advanced methods in the next
    chapter to solve a variety of more challenging problems.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你在实现智能体并观看它解决Gym中的Mountain Car问题时玩得很开心！在下一章，我们将深入探讨一些高级方法，以解决更多具有挑战性的问题。
