- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding How Stable Diffusion Works
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Stable Diffusion的工作原理
- en: In [*Chapter 4*](B21263_04.xhtml#_idTextAnchor081), we dove into the internal
    workings of the diffusion model with some math formulas. If you are not used to
    reading the formulas every day, it can be scary, but once you get familiar with
    those symbols and Greek letters, the benefit of fully understanding those formulas
    is huge. Math formulas and equations not only help us understand the core of the
    process in a precise and concise form, but they also enable us to read more papers
    and works from others.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B21263_04.xhtml#_idTextAnchor081)中，我们通过一些数学公式深入了解了扩散模型的内部工作原理。如果你不习惯每天阅读公式，可能会感到害怕，但一旦你熟悉了那些符号和希腊字母，完全理解这些公式的益处是巨大的。数学公式和方程不仅以精确和简洁的形式帮助我们理解过程的本质，而且使我们能够阅读更多他人的论文和作品。
- en: 'While the original diffusion model is more like a proof of a concept, it shows
    the huge potential of the multi-step diffusion model compared with a one-pass
    neural network. However, some drawbacks come with the original diffusion model,
    **denoising diffusion probabilistic models** (**DDPM**) [1], and later Classifier
    Guidance denoising. Let me list two:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原始的扩散模型更像是一个概念证明，但它展示了多步扩散模型与单次传递神经网络相比的巨大潜力。然而，原始扩散模型（**去噪扩散概率模型**（**DDPM**）[1]）和一些后续的分类器指导去噪存在一些缺点。让我列举两个：
- en: To train a diffusion model with Classifier Guidance requires training a new
    classifier, and we can’t reuse a pre-trained classifier. Also, in diffusion model
    training, training a classifier with 1,000 categories is already not easy.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使用分类器指导训练扩散模型，需要训练一个新的分类器，而且我们不能重用预训练的分类器。此外，在扩散模型训练中，训练一个包含1,000个类别的分类器已经很不容易了。
- en: Pre-trained model inferences in pixel space are computationally expensive, not
    to mention training a model. Using a pre-trained model to generate 512x512 images
    in pixel space on a home computer with 8 GB of VRAM, without memory optimization,
    is not possible.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在像素空间中使用预训练模型进行推理计算成本很高，更不用说训练模型了。在没有内存优化的情况下，使用预训练模型在具有8 GB VRAM的家用电脑上生成512x512像素的图像是不可能的。
- en: In 2022, researchers proposed **Latent Diffusion models**, Robin et al [2].
    The model nicely solved both the classification problem and the performance problem.
    The Latent Diffusion model was later known as Stable Diffusion.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，研究人员提出了**潜在扩散模型**，由Robin等人[2]提出。该模型很好地解决了分类问题和性能问题。后来，这种潜在扩散模型被称为Stable
    Diffusion。
- en: 'In this chapter, we will take a look at how Stable Diffusion solved the preceding
    problems and led to state-of-the-art developments in the field of image generation.
    We will specifically cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Stable Diffusion如何解决前面的问题，并引领图像生成领域的最先进发展。我们将具体涵盖以下主题：
- en: Stable Diffusion in latent space
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在空间中的Stable Diffusion
- en: Generating latent vectors using Diffusers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Diffusers生成潜在向量
- en: Generating text embeddings using CLIP
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CLIP生成文本嵌入
- en: Generating time step embeddings
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成时间步嵌入
- en: Initializing Stable Diffusion UNet
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化Stable Diffusion UNet
- en: Implementing a text-to-image Stable Diffusion inference pipeline
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现文本到图像的Stable Diffusion推理管道
- en: Implementing a text-guided image-to-image Stable Diffusion inference pipeline
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现文本引导的图像到图像的Stable Diffusion推理管道
- en: Putting all the code together
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有代码合并在一起
- en: Let’s dive into the core of Stable Diffusion.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨Stable Diffusion的核心。
- en: 'The sample code for this chapter is tested using version 0.20.0 of the Diffusers
    package. To ensure the code runs smoothly, please use Diffusers v0.20.0\. You
    can install it using the following command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码使用Diffusers包的0.20.0版本进行测试。为确保代码运行顺畅，请使用Diffusers v0.20.0。你可以使用以下命令进行安装：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Stable Diffusion in latent space
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在空间中的Stable Diffusion
- en: 'Instead of processing diffusion in pixel space, Stable Diffusion uses latent
    space to represent an image. What is latent space? In short, latent space is the
    vector representation of an object. To use an analogy, before you go on a blind
    date, a matchmaker could provide you with your counterpart’s height, weight, age,
    hobbies, and so on in the form of a vector:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与在像素空间中处理扩散不同，Stable Diffusion使用潜在空间来表示图像。什么是潜在空间？简而言之，潜在空间是对象的向量表示。为了打个比方，在你去参加相亲之前，媒人可以以向量的形式提供给你对方的身高、体重、年龄、爱好等信息：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can take this vector as the latent space of your blind date counterpart.
    A real person’s true property dimension is almost unlimited (you could write a
    biography for one). The latent space can be used to represent a real person with
    only a limited number of features, such as height, weight, and age.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这个向量视为你盲约对象的潜在空间。一个真实人的真实属性维度几乎是无限的（你可以为一个人写一篇传记）。潜在空间可以用来表示一个真实的人，只需要有限数量的特征，如身高、体重和年龄。
- en: In the case of the Stable Diffusion training stage, a trained encoder model,
    usually denoted as ℇ *(E)*, is used to encode an input image in a latent vector
    representation. After the reverse diffusion process, the latent space is decoded
    by a decoder in pixel space. The decoder is usually denoted as D *(D)*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在Stable Diffusion训练阶段，使用一个训练好的编码器模型，通常表示为ℇ *(E)*，将输入图像编码为潜在向量表示。在反向扩散过程之后，潜在空间由像素空间的解码器解码。解码器通常表示为D
    *(D)*。
- en: 'Both training and sampling work take place in the latent space. The training
    process is shown in *Figure 5**.1*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和采样都在潜在空间中进行。训练过程在*图5.1*中展示：
- en: '![Figure 5.1: Training Stable Diffusion model in latent space](img/B21263_05_01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1：在潜在空间中训练Stable Diffusion模型](img/B21263_05_01.jpg)'
- en: 'Figure 5.1: Training Stable Diffusion model in latent space'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：在潜在空间中训练Stable Diffusion模型
- en: '*Figure 5**.1* illustrates the training process of the Stable Diffusion model.
    It shows a high-level overview of how the model is trained.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.1*展示了Stable Diffusion模型的训练过程。它展示了模型是如何被训练的概述。'
- en: 'Here is a step-by-step breakdown of the process:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个过程的一步一步分解：
- en: '**Inputs**: The model is trained using images, caption text, and time step
    embeddings (specifying at which step the denoising happens).'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入**: 模型使用图像、标题文本和时间步嵌入（指定去噪发生的步骤）进行训练。'
- en: '**Image encoder**: The input image is passed through an encoder. The encoder
    is a neural network that processes the input image and converts it into a more
    abstract and compressed representation. This representation is often referred
    to as a “latent space” because it captures the image’s underlying characteristics,
    but not the pixel-level details.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图像编码器**: 输入图像通过编码器传递。编码器是一个神经网络，它处理输入图像并将其转换为更抽象和压缩的表示。这种表示通常被称为“潜在空间”，因为它捕捉了图像的基本特征，但不是像素级细节。'
- en: '**Latent space**: The encoder outputs a vector that represents the input image
    in the latent space. The latent space is typically a lower-dimensional space than
    the input space (the pixel space of the image), which allows for faster processing
    and more efficient representation of the input data. The whole training happens
    in the latent space.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**潜在空间**: 编码器输出一个向量，代表潜在空间中的输入图像。潜在空间通常比输入空间（图像的像素空间）低维，这使得处理更快，并且更有效地表示输入数据。整个训练过程都在潜在空间中进行。'
- en: '**Iterate N steps**: The training process involves iterating through the latent
    space multiple times (*N* steps). This iterative process is where the model learns
    to refine the latent space representation and make small adjustments to match
    the desired output image.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代N步**: 训练过程涉及在潜在空间中多次迭代（*N*步）。这个迭代过程是模型学习细化潜在空间表示并做出小调整以匹配期望输出图像的地方。'
- en: '**UNet**: After each iteration, the model uses UNet to generate an output image
    based on the current latent space vector. UNet generates the predicted noise and
    incorporates the input text embedding, step information, and potentially other
    embeddings.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**UNet**: 在每次迭代后，模型使用UNet根据当前的潜在空间向量生成输出图像。UNet生成预测的噪声，并整合输入文本嵌入、步骤信息以及可能的其他嵌入。'
- en: '**The loss function**: The model’s training process also involves a loss function.
    This measures the difference between the output image and the desired output image.
    As the model iterates, the loss is continually calculated, and the model makes
    adjustments to its weights to minimize this loss. This is how the model learns
    from its mistakes and improves over time.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**损失函数**: 模型的训练过程还涉及一个损失函数。这个函数衡量输出图像与期望输出图像之间的差异。随着模型的迭代，损失不断计算，模型通过调整其权重来最小化这个损失。这就是模型如何从错误中学习并随着时间的推移而改进。'
- en: Refer to [*Chapter 21*](B21263_21.xhtml#_idTextAnchor405) for more detailed
    steps on model training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[*第21章*](B21263_21.xhtml#_idTextAnchor405)获取更多关于模型训练的详细步骤。
- en: 'The process of inferencing from UNet is shown in *Figure 5**.2*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从UNet进行推理的过程在*图5.2*中展示：
- en: '![Figure 5.2: Stable Diffusion inferencing in latent space](img/B21263_05_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2：稳定扩散在潜在空间中的推理](img/B21263_05_02.jpg)'
- en: 'Figure 5.2: Stable Diffusion inferencing in latent space'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：稳定扩散在潜在空间中的推理
- en: Stable Diffusion not only supports text-guided image generation; it also supports
    image-guided generation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散不仅支持文本引导的图像生成；它还支持图像引导的生成。
- en: In *Figure 5**.2*, starting from the left side, we can see that both text and
    an image are used to guide the image generation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.2*中，从左侧开始，我们可以看到文本和图像都被用来引导图像生成。
- en: When we provide a text input, Stable Diffusion uses CLIP [3] to generate an
    embedding vector, which will be fed into UNet, using the attention mechanism.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提供文本输入时，稳定扩散使用 CLIP [3] 生成嵌入向量，该向量将被输入到 UNet 中，并使用注意力机制。
- en: When we provide an image as the guiding signal, the input image will be encoded
    to latent space and then concatenate with the randomly generated Gaussian noise.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提供一个图像作为引导信号时，输入图像将被编码到潜在空间，然后与随机生成的高斯噪声进行拼接。
- en: It is all up to us to provide guidance; we can provide either text, an image,
    or both. We can even generate images without providing any images; in this “empty”
    guidance case, the UNet model will decide what to generate based on the randomly
    initialized noise.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都取决于我们提供的指导；我们可以提供文本、图像或两者兼而有之。我们甚至可以在不提供任何图像的情况下生成图像；在这种情况下，“空”的引导案例中，UNet模型将根据随机初始化的噪声决定生成什么。
- en: With the two essential inputs provided text embeddings and the initial image
    latent noise (with or without the initial image’s encoded vectors in latent space),
    UNet kicks off to remove noise from the initial image in the latent space. After
    several denoising steps, with the help of a decoder, Stable Diffusion can output
    a vivid image in pixel space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了两个基本输入文本嵌入和初始图像潜在噪声（是否包含初始图像在潜在空间中的编码向量），UNet开始从潜在空间中的初始图像中去除噪声。经过几个去噪步骤后，在解码器的帮助下，稳定扩散可以在像素空间中输出一个生动的图像。
- en: The process is similar to the training process but without sending the loss
    value back to update the weights. Instead, after a number of denoising steps (*N*
    steps), the latent decoder (**Variational Autoencoder** (**VAE**) [4]) converts
    the image from latent space to visible pixel space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程与训练过程类似，但不需要将损失值发送回更新权重。相反，在经过几个去噪步骤（*N*步骤）后，潜在解码器（**变分自动编码器**（**VAE**）[4]）将图像从潜在空间转换为可见像素空间。
- en: Next, let’s take a look at what those components (the text encoder, image Encoder,
    UNet, and image decoder) look like, and then we’ll build one of our own Stable
    Diffusion pipelines step by step.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看那些组件（文本编码器、图像编码器、UNet和图像解码器）的样子，然后我们将一步一步地构建我们自己的稳定扩散管道。
- en: Generating latent vectors using diffusers
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 diffusers 生成潜在向量
- en: 'In this section, we are going to use a pre-trained Stable Diffusion model to
    encode an image into latent space so that we have a concrete impression of what
    a latent vector looks and feels like. Then, we will decode the latent vector back
    into an image. This operation will also establish the foundation for building
    the image-to-image custom pipeline:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用预训练的稳定扩散模型将图像编码到潜在空间，以便我们有一个关于潜在向量看起来和感觉如何的具体印象。然后，我们将潜在向量解码回图像。此操作还将为构建图像到图像的定制管道奠定基础：
- en: '`load_image` function from `diffusers` to load an image from local storage
    or a URL. In the following code, we load an image named `dog.png` from the same
    directory of the current program:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_image` 函数从 `diffusers` 中加载图像，从本地存储或URL加载图像。在下面的代码中，我们从当前程序的同一目录中加载了一个名为
    `dog.png` 的图像：'
- en: '[PRE2]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Pre-process the image**: Each pixel of the loaded image is represented by
    a number ranging from 0 to 255\. The image encoder from the Stable Diffusion process
    handles image data ranging from -1.0 to 1.0\. So, we first need to make the data
    range conversion:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理图像**：加载的每个像素都由一个介于0到255之间的数字表示。稳定扩散过程中的图像编码器处理介于-1.0到1.0之间的图像数据。因此，我们首先需要进行数据范围转换：'
- en: '[PRE5]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, if we use Python code, `image_array.shape`, to check the `image_array`
    data shape, we will see the shape of the image data as – `(512,512,3)`, arranged
    as `(width, height, channel)`, instead of the commonly used `(channel, width,
    height).` Here, we need to convert the image data shape to `(channel, width, height)`
    or `(3,512,512)`, using the `transpose()` function:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，如果我们使用 Python 代码 `image_array.shape` 来检查 `image_array` 数据形状，我们将看到图像数据的形状为
    – `(512,512,3)`，排列为 `(width, height, channel)`，而不是常用的 `(channel, width, height)`。在这里，我们需要使用
    `transpose()` 函数将图像数据形状转换为 `(channel, width, height)` 或 `(3,512,512)`：
- en: '[PRE11]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `2` is in the first position of `2, 0, 1`, which means moving the original
    third dimension (indexed as `2`) to the first dimension. The same logic applies
    to `0` and `1`. The original `0` dimension is now converted to the second position,
    and the original `1` is now in the third dimension.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`2` 位于 `2, 0, 1` 的第一个位置，这意味着将原始的第三维度（索引为 `2`）移动到第一个维度。相同的逻辑适用于 `0` 和 `1`。原始的
    `0` 维度现在转换为第二个位置，原始的 `1` 现在位于第三个维度。'
- en: With this transpose operation, the NumPy array, `image_array_cwh`, is now in
    the `(``3,512,512)` shape.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这个转置操作，NumPy 数组 `image_array_cwh` 现在具有 `(``3,512,512)` 的形状。
- en: 'The Stable Diffusion image encoder handles image data in batches, which, in
    this instance is four-dimensional data with the batch dimension in the first position;
    we need to add the batch dimension here:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定扩散图像编码器以批处理方式处理图像数据，在这个例子中是具有四个维度的数据，批处理维度位于第一个位置；我们需要在这里添加批处理维度：
- en: '[PRE14]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`torch` **and move to CUDA**: We will convert the image data to latent space
    using CUDA. To achieve this, we will need to load the data into the CUDA VRAM
    before handing it off to the next step model:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch` **并移动到 CUDA**：我们将使用 CUDA 将图像数据转换为潜在空间。为了实现这一点，我们需要在将其传递给下一步模型之前将数据加载到
    CUDA VRAM 中：'
- en: '[PRE16]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Load the Stable Diffusion image encoder VAE**: This VAE model is used to
    convert the image from pixel space to latent space:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载稳定扩散图像编码器 VAE**：这个 VAE 模型用于将图像从像素空间转换为潜在空间：'
- en: '[PRE23]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Encode the image into a latent vector**: Now, everything is ready, and we
    can encode any image into a latent vector as PyTorch tensor:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将图像编码为潜在向量**：现在一切准备就绪，我们可以将任何图像编码为潜在向量，作为 PyTorch 张量：'
- en: '[PRE31]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Check the data and shape of the latent data:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查潜在数据的数据和形状：
- en: '[PRE33]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can see that the latent is in the `(4, 64, 64)` shape, with each element
    in the range of `-1.0` to `1.0`.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到潜在数据具有 `(4, 64, 64)` 的形状，每个元素的范围在 `-1.0` 到 `1.0` 之间。
- en: Stable Diffusion processes all the denoising steps on a 64x64 tensor with 4-channel
    for a 512x512 image generation. The data size is way less than its original image
    size, 512x512 with three color channels.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稳定扩散对所有去噪步骤都在一个 64x64 张量上进行，该张量具有 4 个通道，用于生成 512x512 的图像。数据大小远小于其原始图像大小，512x512
    的三个颜色通道。
- en: '**Decode latent to image (optional)**: You may be wondering, can I convert
    the latent data back to the pixel image? Yes, we can do this with lines of code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将潜在数据解码为图像（可选）**：你可能想知道，我能将潜在数据转换回像素图像吗？是的，我们可以通过以下代码行来实现：'
- en: '[PRE35]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The `diffusers` Stable Diffusion pipeline will finally generate a latent tensor.
    We will follow similar steps to recover a denoised latent for an image in the
    latter part of this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`diffusers` 稳定扩散管道最终将生成一个潜在张量。我们将遵循类似的步骤，在本章的后半部分恢复图像的降噪潜在。'
- en: Generating text embeddings using CLIP
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CLIP 生成文本嵌入
- en: 'To generate the text embeddings (the embeddings contain the image features),
    we need first to tokenize the input text or prompt and then encode the token IDs
    into embeddings. Here are steps to achieve this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成文本嵌入（嵌入包含图像特征），我们首先需要标记输入文本或提示，然后将标记 ID 编码为嵌入。以下是实现此目的的步骤：
- en: '**Get the prompt** **token IDs**:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取提示标记 ID**：'
- en: '[PRE60]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The preceding code will convert the `a running dog` text prompt to a token ID
    list as a `torch` tensor object – `tensor([[49406, 320,` `2761,` `1929, 49407]])`.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将 `a running dog` 文本提示转换为标记 ID 列表，作为一个 `torch` 张量对象 – `tensor([[49406, 320,
    2761, 1929, 49407]])`。
- en: '**Encode the token IDs** **into embeddings**:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将标记 ID 编码为嵌入**：'
- en: '[PRE75]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '**Check the** **embedding data**:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查嵌入数据**：'
- en: '[PRE85]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Now, we can see the data of `prompt_embeds` as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以看到 `prompt_embeds` 的数据如下：
- en: '[PRE87]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Its shape is `torch.Size([1, 5, 768])`. Each token ID is encoded into a 768-dimension
    vector.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其形状为 `torch.Size([1, 5, 768])`。每个标记 ID 被编码为一个 768 维向量。
- en: '`prompt` and `prompt`/`negative` `prompt` cases:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`prompt` 和 `prompt`/`negative` `prompt` 情况：'
- en: '[PRE93]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '`torch` vector:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`torch` 向量：'
- en: '[PRE110]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Next, we will initialize the time step data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化时间步数据。
- en: Initializing time step embeddings
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化时间步嵌入
- en: We introduced the scheduler in [*Chapter 3*](B21263_03.xhtml#_idTextAnchor064).
    By using the scheduler, we can sample key steps for image generation. Instead
    of denoising 1,000 steps to generate an image in the original diffusion model
    (DDPM), by using a scheduler, we can generate an image in a mere 20 steps.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第3章*](B21263_03.xhtml#_idTextAnchor064)中介绍了调度器。通过使用调度器，我们可以为图像生成采样关键步骤。与在原始扩散模型（DDPM）中通过去噪1,000步来生成图像相比，使用调度器我们只需20步就能生成图像。
- en: 'In this section, we are going to use the Euler scheduler to generate time step
    embeddings, and then we’ll take a look at what the time step embeddings look like.
    No matter how good the diagram that tries to plot the process is, we can only
    understand how it works by reading the actual data and code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用欧拉调度器生成时间步嵌入，然后我们将看看时间步嵌入看起来是什么样子。不管试图绘制过程的图表有多好，我们只能通过阅读实际数据和代码来理解它的工作原理：
- en: '**Initialize a scheduler from the scheduler configuration for** **the model**:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从模型调度器配置初始化一个调度器**：'
- en: '[PRE112]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'The preceding code will initialize a scheduler from the checkpoint’s scheduler
    config file. Note that you can also create a scheduler, as we discussed in [*Chapter
    3*](B21263_03.xhtml#_idTextAnchor064), like this:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将从检查点的调度器配置文件初始化一个调度器。请注意，你还可以创建一个调度器，正如我们在[*第3章*](B21263_03.xhtml#_idTextAnchor064)中讨论的那样，如下所示：
- en: '[PRE118]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: However, this will require you to load a model first, which is not only slow
    but also unnecessary; the only thing we need is the model’s scheduler.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，这需要你首先加载一个模型，这不仅速度慢，而且不必要；我们需要的只是模型的调度器。
- en: '**Sample the steps for the image** **diffusion process**:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为图像扩散过程采样步骤**：'
- en: '[PRE126]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'We will see the 20-step value as follows:'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将看到20步的值如下：
- en: '[PRE131]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: Here, the scheduler takes 20 steps out of the 1,000 steps, and those 20 steps
    may be enough to denoise a complete Gaussian distribution for image generation.
    This step sampling technique also contributes to Stable Diffusion performance
    boosting.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，调度器从1,000步中抽取了20步，这20步可能足以对图像生成中的完整高斯分布进行去噪。这种步骤采样技术也有助于提高Stable Diffusion的性能。
- en: Initializing the Stable Diffusion UNet
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化Stable Diffusion UNet
- en: The UNet architecture [5] was introduced by Ronneberger et al. for biomedical
    image segmentation purposes. Before the UNet architecture, a convolution network
    was commonly used for image classification tasks. When using a convolution network,
    the output is a single class label. However, in many visual tasks, the desired
    output should include localization too, and the UNet model solved this problem.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: UNet架构[5]由Ronneberger等人为了生物医学图像分割目的而引入。在UNet架构之前，卷积网络通常用于图像分类任务。当使用卷积网络时，输出是一个单一类别的标签。然而，在许多视觉任务中，所需的输出还应包括定位信息，而UNet模型解决了这个问题。
- en: The U-shaped architecture of UNet enables efficient learning of features at
    different scales. UNet’s skip connections directly combine feature maps from different
    stages, allowing a model to effectively propagate information across various scales.
    This is crucial for denoising, as it ensures the model retains both fine-grained
    details and global context during noise removal. These features make UNet a good
    candidate for the denoising model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: UNet的U形架构使得在不同尺度上高效学习特征成为可能。UNet的跳过连接直接将不同阶段的特征图结合起来，使模型能够有效地在不同尺度间传播信息。这对于去噪至关重要，因为它确保模型在去噪过程中既保留了精细的细节，也保留了全局上下文。这些特性使UNet成为去噪模型的良好候选。
- en: 'In the `Diffuser` library, there is a class named `UNet2DconditionalModel`;
    this is a conditional 2D UNet model for image generation and related tasks. It
    is a key component of diffusion models and plays a crucial role in the image generation
    process. We can load a UNet model in just several lines of code, like this:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Diffuser`库中，有一个名为`UNet2DconditionalModel`的类；这是一个用于图像生成和相关任务的2D条件UNet模型。它是扩散模型的关键组件，在图像生成过程中发挥着至关重要的作用。我们只需几行代码就可以加载一个UNet模型，如下所示：
- en: '[PRE152]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: Together with the UNet model we have just loaded up, we have all the components
    required by Stable Diffusion. Not that hard, right? Next, we are going to use
    those building blocks to build two Stable Diffusion pipelines – one text-to-image
    and another image-to-image.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚刚加载的UNet模型一起，我们拥有了Stable Diffusion所需的所有组件。这并不难，对吧？接下来，我们将使用这些构建块来构建两个Stable
    Diffusion管道——一个文本到图像和一个图像到图像。
- en: Implementing a text-to-image Stable Diffusion inference pipeline
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现文本到图像的Stable Diffusion推理管道
- en: 'So far, we have all the text encoder, image VAE, and denoising UNet model initialized
    and loaded into the CUDA VRAM. The following steps will chain them together to
    form the simplest and working Stable Diffusion text-to-image pipeline:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经初始化并加载了所有文本编码器、图像VAE和去噪UNet模型到CUDA VRAM中。以下步骤将它们链接在一起，形成最简单且可工作的Stable
    Diffusion文本到图像管道：
- en: '**Initialize a latent noise**: In *Figure 5**.2*, the starting point of inference
    is randomly initialized Gaussian latent noise. We can create one of the latent
    noise with this code:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化潜在噪声**：在*图5**.2中，推理的起点是随机初始化的高斯潜在噪声。我们可以使用以下代码创建一个潜在噪声：'
- en: '[PRE153]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: During the training stage, an initial noise sigma is used to help prevent the
    diffusion process from becoming stuck in local minima. When the diffusion process
    starts, it is very likely to be in a state where it is very close to a local minimum.
    `init_noise_sigma = 14.6146` is used to help avoid this. So, during the inference,
    we will also use `init_noise_sigma` to shape the initial latent.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练阶段，使用初始噪声sigma来帮助防止扩散过程陷入局部最小值。当扩散过程开始时，它很可能处于一个非常接近局部最小值的状态。`init_noise_sigma
    = 14.6146`用于帮助避免这种情况。因此，在推理过程中，我们也将使用`init_noise_sigma`来塑造初始潜在表示。
- en: '[PRE161]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '**Loop through UNet**: With all those components prepared, we are finally at
    the stage of feeding the initial latents to UNet to generate the target latent
    we want:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**遍历UNet**：在准备所有这些组件后，我们最终到达将初始潜在表示输入UNet以生成我们想要的目标潜在表示的阶段：'
- en: '[PRE164]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: The preceding code is a simplified denoising loop of `DiffusionPipeline` from
    the `diffusers` package, removing all those edging cases and only keeping the
    core of the inferencing.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码是`diffusers`包中`DiffusionPipeline`的简化去噪循环，移除了所有边缘情况，仅保留了推理的核心。
- en: The algorithm works by iteratively adding noise to a latent representation of
    an image. In each iteration, the noise is guided by a text prompt, which helps
    the model generate images that are more similar to the prompt.
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该算法通过迭代地向图像的潜在表示中添加噪声来工作。在每次迭代中，噪声由文本提示引导，这有助于模型生成与提示更相似的图像。
- en: 'The preceding code first defines a few variables:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码首先定义了一些变量：
- en: The `guidance_scale` variable controls the amount of guidance that is applied
    to the noise.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`guidance_scale`变量控制应用于噪声的引导程度。'
- en: The `latents_sd` variable stores the latent representation of the image that
    is generated. The time steps variable stores a list of time steps at which the
    noise will be added.
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latents_sd`变量存储生成的图像的潜在表示。时间步变量存储一个列表，其中包含添加噪声的时间步。'
- en: The main loop of the code iterates over the time steps. In each iteration, the
    code first expands the latent representation to include two copies of itself.
    This is done because the Stable Diffusion algorithm uses a classifier-free guidance
    mechanism, which requires two copies of the latent representation.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码的主循环遍历时间步。在每次迭代中，代码首先将潜在表示扩展以包含其自身的两份副本。这是因为在Stable Diffusion算法中，使用的是无分类器引导机制，它需要两个潜在表示的副本。
- en: The code then calls the `unet` function to predict the noise residual for the
    current time step.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后代码调用`unet`函数来预测当前时间步的噪声残差。
- en: The code then performs guidance on the noise residual. This involves adding
    a scaled version of the text-conditioned noise residual to the unconditional noise
    residual. The amount of guidance that is applied is controlled by the `guidance_scale`
    variable.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后代码对噪声残差进行引导。这涉及到将文本条件噪声残差的缩放版本添加到无条件噪声残差中。应用引导的程度由`guidance_scale`变量控制。
- en: Finally, the code calls the `scheduler` function to update the latent representation
    of the image. The `scheduler` function is a function that controls the amount
    of noise that is added to the latent representation at each time step.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，代码调用`scheduler`函数来更新图像的潜在表示。`scheduler`函数是一个函数，它控制在每个时间步添加到潜在表示中的噪声量。
- en: As mentioned previously, the preceding code is a simplified version of the Stable
    Diffusion algorithm. In practice, the algorithm is much more complex, and it incorporates
    a number of other techniques to improve the quality of the generated images.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，前面的代码是Stable Diffusion算法的简化版本。在实际应用中，该算法要复杂得多，并融合了多种其他技术来提高生成图像的质量。
- en: '`latent_to_img` function to recover the image from the latent space:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`latent_to_img`函数从潜在空间恢复图像：
- en: '[PRE186]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: '[PRE187]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: 'The `latent_to_img` function performs actions in the following sequence:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`latent_to_img`函数按照以下顺序执行操作：'
- en: It calls the `vae_model.decode` function to decode the latent vector into an
    image. The `vae_model.decode` function is a function that is trained on a dataset
    of images. It can be used to generate new images that are similar to the images
    in the dataset.
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它调用`vae_model.decode`函数将潜在向量解码为图像。`vae_model.decode`函数是在图像数据集上训练的函数。它可以用来生成与数据集中图像相似的新图像。
- en: Normalizes the image data to a range of `0` to `1`. This is done because the
    `Image.fromarray` function expects image data to be in this range.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像数据归一化到`0`到`1`的范围内。这样做是因为`Image.fromarray`函数期望图像数据处于这个范围内。
- en: Moves the image data from the GPU to the CPU. Then, it converts the image data
    from a torch tensor to a NumPy array. This is done because the `Image.fromarray`
    function only accepts NumPy arrays as input.
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像数据从GPU移动到CPU。然后，它将图像数据从torch张量转换为NumPy数组。这样做是因为`Image.fromarray`函数只接受NumPy数组作为输入。
- en: Flips the dimensions of the image array so that it is in the (width, height,
    channel) format, the format that the `Image.fromarray` function expects.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像数组的维度翻转，使其处于（宽度，高度，通道）格式，这是`Image.fromarray`函数期望的格式。
- en: Maps the image data to a range from `0` to `255` and converts it to an integer
    type.
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像数据映射到`0`到`255`的范围内，并将其转换为整型。
- en: Calls the `Image.fromarray` function to create a Python imaging library (PIL)
    image object from the image data.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`Image.fromarray`函数从图像数据创建Python图像库（PIL）图像对象。
- en: The `latents_2 = (1 / 0.18215) * latents_sd` line of code is needed when decoding
    the latent to image because the latents are scaled by a factor of `0.18215` during
    training. This scaling is done to ensure that latent space has a unit variance.
    When decoding, the latents need to be scaled back to their original scale to reconstruct
    the original image.
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在解码潜在图像时，需要使用`latents_2 = (1 / 0.18215) * latents_sd`这一行代码，因为在训练过程中潜在被`0.18215`这个因子缩放。这种缩放是为了确保潜在空间具有单位方差。在解码时，需要将潜在缩放回其原始尺度以重建原始图像。
- en: 'Then, you should be able to see something like this if everything is going
    well:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该能看到类似以下内容：
- en: '![Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline](img/B21263_05_03.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3：一只正在奔跑的狗，由定制的Stable Diffusion管道生成](img/B21263_05_03.jpg)'
- en: 'Figure 5.3: A running dog, generated by a custom Stable Diffusion pipeline'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：一只正在奔跑的狗，由定制的Stable Diffusion管道生成
- en: In the next section, we are going to implement an image-to-image Stable Diffusion
    pipeline.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现一个图像到图像的Stable Diffusion管道。
- en: Implementing a text-guided image-to-image Stable Diffusion inference pipeline
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现文本引导的图像到图像Stable Diffusion推理管道
- en: 'The only thing we need to do now is concatenate the starting image with the
    starting latent noise. The `latents_input` Torch tensor is the latent we encoded
    from a dog image earlier in this chapter:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的只是将起始图像与起始潜在噪声连接起来。`latents_input` Torch张量是我们在本章早期从狗图像中编码的潜在：
- en: '[PRE210]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'That is all that is necessary; use the same code from the text-to-image pipeline,
    and you should generate something like *Figure 5**.4*:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所有必要的步骤；使用与文本到图像管道相同的代码，你应该会生成类似*图5.4*的内容：
- en: '![Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion
    pipeline](img/B21263_05_04.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4：一只正在奔跑的狗，由定制的图像到图像Stable Diffusion管道生成](img/B21263_05_04.jpg)'
- en: 'Figure 5.4: A running dog, generated by a custom image-to-image Stable Diffusion
    pipeline'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：一只正在奔跑的狗，由定制的图像到图像Stable Diffusion管道生成
- en: Note that the preceding code uses `strength = 0.7`; the strength denotes the
    weight of the original latent noise. If you want an image more similar to the
    initial image (the image you provided to the image-to-image pipeline), use a lower
    strength number; otherwise, increase it.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面的代码使用`strength = 0.7`；强度表示原始潜在噪声的权重。如果你想得到与初始图像（你提供给图像到图像管道的图像）更相似的图像，请使用较小的强度数字；否则，增加它。
- en: Summary
  id: totrans-331
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we moved on from the original diffusion model, DDPM, and explained
    what Stable Diffusion is and why it is faster and better than the DDPM model.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从原始扩散模型DDPM过渡到，并解释了什么是Stable Diffusion以及为什么它比DDPM模型更快、更好。
- en: As suggested by the paper *High-Resolution Image Synthesis with Latent Diffusion
    Models* [6] that introduced Stable Diffusion, the biggest feature that differentiates
    Stable Diffusion from its predecessor is the “*Latent*.” This chapter explained
    what latent space is and how Stable Diffusion training and inference work internally.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如论文《使用潜在扩散模型进行高分辨率图像合成》[6] 所建议的，稳定扩散与其前辈最大的区别特征是“*潜在*”。本章解释了潜在空间是什么以及稳定扩散的训练和推理是如何在内部工作的。
- en: For a comprehensive understanding, we created components using methods such
    as encoding the initial image into latent data, converting input prompts to token
    IDs and embedding them to text embeddings using the CLIP text model, using the
    Stable Diffusion scheduler to sample detailed steps for inference, creating the
    initial noise latent, concatenating initial noise latent with the initial image
    latent, putting all the components together to build a custom text-to-image Stable
    Diffusion pipeline, and extending the pipeline to enable a text-guided image-to-image
    Stable Diffusion pipeline. We created these components one by one, and finally,
    we built two Stable Diffusion pipelines – one text-to-image pipeline and an extended
    text-guided image-to-image pipeline.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面理解，我们使用了诸如将初始图像编码为潜在数据、将输入提示转换为令牌 ID 并使用 CLIP 文本模型将其嵌入到文本嵌入中、使用稳定扩散调度器采样推理的详细步骤、创建初始噪声潜在值、将初始噪声潜在值与初始图像潜在值连接、将所有组件组合起来构建自定义文本到图像稳定扩散管道，以及扩展管道以启用文本引导的图像到图像稳定扩散管道等方法创建了组件。我们逐一创建了这些组件，最终构建了两个稳定扩散管道——一个文本到图像管道和一个扩展的文本引导图像到图像管道。
- en: By completing this chapter, you should not only have a general understanding
    of Stable Diffusion but also be able to freely build your own pipelines to meet
    specific requirements.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成本章，你应该不仅对稳定扩散有一个一般性的理解，而且能够自由地构建自己的管道以满足特定需求。
- en: In the next chapter, we are going to introduce solutions to load Stable Diffusion
    models.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍加载稳定扩散模型的方法。
- en: References
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Jonathan Ho, Ajay Jain, Pieter Abbeel, Denoising Diffusion Probabilistic Models:
    [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jonathan Ho，Ajay Jain，Pieter Abbeel，去噪扩散概率模型：[https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)
- en: 'Robin et al, High-Resolution Image Synthesis with Latent Diffusion Models:
    [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Robin 等人，使用潜在扩散模型进行高分辨率图像合成：[https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)
- en: 'Alec et al, Learning Transferable Visual Models From Natural Language Supervision:
    [https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alec 等人，从自然语言监督中学习可迁移的视觉模型：[https://arxiv.org/abs/2103.00020](https://arxiv.org/abs/2103.00020)
- en: 'VAEs: [https://en.wikipedia.org/wiki/Variational_autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder)'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VAEs：[https://en.wikipedia.org/wiki/Variational_autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder)
- en: 'UNet2DConditionModel document from Hugging Face: [https://huggingface.co/docs/diffusers/api/models/unet2d-cond](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face 的 UNet2DConditionModel 文档：[https://huggingface.co/docs/diffusers/api/models/unet2d-cond](https://huggingface.co/docs/diffusers/api/models/unet2d-cond)
- en: 'Robin et al, High-Resolution Image Synthesis with Latent Diffusion Models:
    [https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Robin 等人，使用潜在扩散模型进行高分辨率图像合成：[https://arxiv.org/abs/2112.10752](https://arxiv.org/abs/2112.10752)
- en: Additional reading
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读材料
- en: 'Jonathan Ho, Tim Salimans, Classifier-Free Diffusion Guidance: [https://arxiv.org/abs/2207.12598](https://arxiv.org/abs/2207.12598)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Jonathan Ho，Tim Salimans，无分类器扩散引导：[https://arxiv.org/abs/2207.12598](https://arxiv.org/abs/2207.12598)
- en: 'Stable Diffusion with Diffusers: [https://huggingface.co/blog/stable_diffusion](https://huggingface.co/blog/stable_diffusion)'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Diffusers 的稳定扩散：[https://huggingface.co/blog/stable_diffusion](https://huggingface.co/blog/stable_diffusion)
- en: 'Olaf Ronneberger, Philipp Fischer, Thomas Brox, UNet: Convolutional Networks
    for Biomedical Image Segmentation: [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Olaf Ronneberger，Philipp Fischer，Thomas Brox，UNet：用于生物医学图像分割的卷积网络：[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)
