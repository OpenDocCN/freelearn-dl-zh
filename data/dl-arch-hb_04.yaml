- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Understanding Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解递归神经网络
- en: A **recurrent neural network** (**RNN**) is a neural network that is made to
    process sequential data while being aware of the sequence of the data. Sequential
    data can involve time series based data and data that has a sequence but does
    not have a time component, such as text data. The applications of such a neural
    network are built upon the nature of the data itself. For time-series data, this
    can be either for nowcasting (predictions made for the current time with both
    past and present data) or forecasting targets. For text data, applications such
    as speech recognition and machine translation can utilize these neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**递归神经网络** (**RNN**) 是一种设计用来处理顺序数据的神经网络，能够意识到数据的顺序。顺序数据可以包括基于时间的序列数据，也可以是具有顺序但没有时间成分的数据，例如文本数据。这类神经网络的应用基于数据本身的性质。对于时间序列数据，这可以用于现在预测（利用过去和当前数据进行的预测）或目标预测。对于文本数据，可以用于语音识别和机器翻译等应用。'
- en: Research in recurrent neural networks has slowed in the past few years with
    the advent of neural networks that can capture sequential data while removing
    recursive connections completely and achieving better performance, such as transformers.
    However, RNNs are still used extensively in the real world today to serve as a
    good baseline or just an alternative model for faster computations due to their
    lower number of computations and low memory requirements with reasonable metric
    performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着能够捕获序列数据并完全移除递归连接的神经网络（如 Transformer）的出现，过去几年递归神经网络的研究有所放缓，并且取得了更好的性能。然而，RNN
    在今天的实际应用中仍然广泛使用，作为一个良好的基准或仅仅是一个替代模型，因为它们的计算量较少，内存需求低，并且具有合理的度量性能，适用于更快速的计算。
- en: The two most prominent RNN layers are **Long Short-Term Memory** (**LSTM**)
    and **Gated Recurrent Units** (**GRU**). We will not be going through the vanilla
    and the original recurrent neural networks in this book, but we will show LSTM
    and GRU as a refresher. The main operations of LSTM and GRU provide a mechanism
    to keep only relevant memory and ignore data that is not useful, which is a key
    inductive bias crafted for time series or sequential data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最突出的两种 RNN 层是 **长短期记忆** (**LSTM**) 和 **门控递归单元** (**GRU**) 。本书不会介绍原始的递归神经网络，而是通过
    LSTM 和 GRU 作为复习。LSTM 和 GRU 的主要操作提供了一种机制，仅保留相关记忆并忽略无用的数据，这是为时间序列或顺序数据精心设计的关键归纳偏差。
- en: 'In this chapter, we will dive deeper into these two RNN networks more extensively.
    Specifically, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨这两种 RNN 网络。具体来说，我们将涵盖以下主题：
- en: Understanding LSTM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 LSTM
- en: Understanding GRU
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 GRU
- en: Understanding advancements over the standard GRU and LSTM layers
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解标准 GRU 和 LSTM 层的进展
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter is short and sweet but still covers some practical implementations
    in the `Pytorch` library installed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容简洁明了，但仍涵盖了 `Pytorch` 库中一些实际的实现。
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到本章的代码文件，链接地址为 [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_4)。
- en: Understanding LSTM
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 LSTM
- en: LSTM was invented in 1997 but remains a widely adopted neural network. LSTM
    uses the `tanh` activation function as it provides nonlinearities while providing
    second derivatives that can be preserved for a longer sequence. The `tanh` function
    helps to prevent exploding and vanishing gradients. An LSTM layer uses a sequence
    of LSTM cells sequentially connected. Let’s take an in-depth look at what the
    LSTM cell looks like in *Figure 4**.1*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 于1997年发明，但仍然是广泛采用的神经网络。LSTM 使用 `tanh` 激活函数，因为它提供了非线性特性，同时提供了可以在更长序列中保留的二阶导数。`tanh`
    函数有助于防止梯度爆炸和梯度消失。LSTM 层使用一系列顺序连接的 LSTM 单元。让我们深入了解 *图4.1* 中 LSTM 单元的样子。
- en: '![Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM
    cells that forms an LSTM layer](img/B18187_04_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 在一系列 LSTM 单元中的 LSTM 单元的视觉深度解析，形成了一个 LSTM 层](img/B18187_04_001.jpg)'
- en: Figure 4.1 – A visual deep dive into an LSTM cell among a sequence of LSTM cells
    that forms an LSTM layer
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 在一系列 LSTM 单元中的 LSTM 单元的视觉深度解析，形成了一个 LSTM 层
- en: 'The first LSTM cell on the left depicts the high-level structure of an LSTM
    cell and the second LSTM cell on the left depicts the medium-level operations,
    connections, and structure of an LSTM cell, while the third cell on the right
    is just another LSTM cell to emphasize that LSTM layers are made of multiple LSTM
    cells sequentially connected to each other. Think of an LSTM cell as containing
    four gating mechanisms that provide a way to **forget**, **learn**, **remember**,
    and **use** the sequential data. One notable thing you might wonder about is why
    the sigmoid is shown as a separate process in three paths from the input and past
    the hidden state to the forget gate, the remember gate, and the use gate. The
    structure shown in *Figure 4**.1* is a famous depiction of the LSTM cell but does
    not contain information about the weights of the connections. This is due to the
    fact that the inputs go through a weighted addition process to combine the previous
    cell’s hidden state and the current sequence data with different sets of weights
    for each of the four connections. *Figure 4**.2* shows the final low-level structure
    of a single LSTM cell with weights considered:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的第一个 LSTM 单元展示了 LSTM 单元的高级结构，左侧的第二个 LSTM 单元展示了 LSTM 单元的中级操作、连接和结构，而右侧的第三个单元则是另一个
    LSTM 单元，强调了 LSTM 层是由多个 LSTM 单元按顺序连接而成的。可以将 LSTM 单元视为包含四个门控机制，它们提供了忘记、学习、记住和使用序列数据的方式。你可能会好奇的一个显著问题是，为什么
    sigmoid 在三条路径中被显示为一个独立的过程，这些路径从输入经过隐藏状态，到达遗忘门、记忆门和使用门。*图 4**.1* 中展示的结构是 LSTM 单元的经典图示，但没有包含关于连接权重的信息。这是因为输入通过加权求和的过程，将前一个单元的隐藏状态和当前序列数据结合起来，每一条连接都有不同的权重集。*图
    4**.2* 显示了单个 LSTM 单元的最终低级结构，考虑了权重因素：
- en: '![Figure 4.2 – Low-level structure of the LSTM cell](img/B18187_04_002.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – LSTM 单元的低级结构](img/B18187_04_002.jpg)'
- en: Figure 4.2 – Low-level structure of the LSTM cell
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – LSTM 单元的低级结构
- en: In *Figure 4**.2*, W and B represent the weights and biases, respectively. The
    two small letters represent the data type and gate mechanisms, respectively. The
    data type is split into two – the hidden state represented by h and the input
    data represented by i. The gate mechanisms involved are the forget mechanism represented
    by F, the learning mechanism represented by L, where there are two weights and
    biases associated with learning, and the use mechanism represented by U. To properly
    perceive how many parameters an LSTM cell has we still have to decode the dimensions
    of the hidden state and input state weight vectors. For ease of reference, let’s
    take the input vector size as n and the hidden size as m.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4**.2*中，W 和 B 分别代表权重和偏置。两个小写字母分别代表数据类型和门控机制。数据类型分为两类——由 h 表示的隐藏状态和由 i 表示的输入数据。涉及的门控机制包括由
    F 表示的遗忘机制、由 L 表示的学习机制（学习机制涉及两个权重和偏置），以及由 U 表示的使用机制。为了正确理解一个 LSTM 单元有多少个参数，我们还需要解码隐藏状态和输入状态权重向量的维度。为了方便起见，我们假设输入向量的大小为
    n，隐藏状态的大小为 m。
- en: 'The hidden state weights have a dimension of:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏状态权重的维度为：
- en: nm
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: nm
- en: 'While the input state weights have a dimension of:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 而输入状态权重的维度为：
- en: n 2
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: n²
- en: 'Bias, on the other hand, is the size of the input vector size. For Tensorflow
    and Keras with Tensorflow, bias is only added once per mechanism. For PyTorch,
    the bias is added for each hidden state and input state weight. For PyTorch, the
    number of parameters for bias can be defined as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，偏置是输入向量的大小。对于 Tensorflow 和使用 Tensorflow 的 Keras，偏置每个机制只添加一次。对于 PyTorch，偏置对于每个隐藏状态和输入状态权重都会添加。对于
    PyTorch，偏置的参数数量可以定义为：
- en: 2n
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2n
- en: 'As there are four mechanisms, as depicted in *Figure 4**.2*, this means that
    the number of parameters for an LSTM in the PyTorch implementation can then be
    computed according to the following formula:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有四个机制，如*图 4**.2*所示，这意味着在 PyTorch 实现中，LSTM 的参数数量可以根据以下公式计算：
- en: number of parameters = 4(nm + n 2 + 2n)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量 = 4(nm + n² + 2n)
- en: Now that we understand where the actual parameters live in the cell, let’s dive
    into each of these gating mechanisms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了实际参数在单元中的位置，接下来我们深入探讨这些门控机制。
- en: Decoding the forget mechanism of LSTMs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码 LSTM 的遗忘机制
- en: The forget mechanism is accomplished by using a sigmoid activation function
    multiplied against the previous cell state. The name of the gating mechanism implies
    that it determines the information to be removed based on a combination of the
    current input sequence and the previous cell output. A way to think of it is,
    on a scale of `0` to `1`, how relevant is the information from the past? The sigmoid
    mechanism forces the scale to be between `0` and `1`. Values closer to `0` forget
    more of the previous cell state (long-term memory) and values closer to `1` forget
    less of the previous cell state memory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘机制通过使用sigmoid激活函数与前一个单元状态相乘来实现。该门机制的名称暗示它基于当前输入序列和前一个单元输出的组合来确定需要删除的信息。可以这样理解：在`0`到`1`的范围内，过去的信息有多相关？sigmoid机制强制将该范围限定在`0`到`1`之间。接近`0`的值会更多地遗忘前一个单元状态（长期记忆），而接近`1`的值则会更少遗忘前一个单元状态的记忆。
- en: Decoding the learn mechanism of LSTMs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码LSTM的学习机制
- en: The learn mechanism employs a combination of sigmoid activation of previous
    cell output and `tanh` activation of previous cell output added on the outputs
    of the forget gate and multiplied by the outputs of the use gate. This mechanism
    is also known as the input gate. This mechanism allows information learning from
    the current input sequence. The information learned then gets passed into the
    remembering mechanism. Additionally, the information learned will also get passed
    into the mechanism that allows information usage for the next LSTM cell. Both
    of these mechanisms will be introduced sequentially next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 学习机制通过结合前一个单元输出的sigmoid激活函数和`tanh`激活函数，将遗忘门的输出与使用门的输出相乘，应用于当前输入序列的输出。这种机制也被称为输入门。该机制允许从当前输入序列中学习信息。学习到的信息随后会传递给记忆机制。此外，学习到的信息还会传递到下一个LSTM单元，用于信息使用的机制中。这两个机制将会依次介绍。
- en: Decoding the remember mechanism of LSTMs
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码LSTM的记忆机制
- en: The remember mechanism is achieved by simply adding up information from what’s
    left of the forget process and what has been learned, which is the output of the
    learning gate. The output of this gate will then be considered the current cell
    state of the LSTM cell. The cell state contains what is known as the long-term
    memory of the LSTM sequence. Take this mechanism simply as an operation that allows
    the network to selectively choose which part of the input to maintain and remember.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆机制通过将遗忘过程剩余的信息与已学习的信息相加来实现，这就是学习门的输出。该门的输出将被视为LSTM单元的当前单元状态。单元状态包含所谓的LSTM序列的长期记忆。可以简单地将该机制视为一个操作，允许网络选择性地决定保留和记住输入的哪些部分。
- en: Decoding the “information-using” mechanism of LSTMs
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码LSTM的“信息使用”机制
- en: The information-using mechanism is achieved by applying nonlinearities using
    the tanh activation function on the current cell state and again using the current
    input sequence and previous cell output as a weighting mechanism to determine
    how much relevant information from the past and present should be used. The output
    of applying the use gate gives us the hidden state that will also be used as the
    previous cell output for the next LSTM cell.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 信息使用机制通过在当前单元状态上应用tanh激活函数进行非线性处理，利用当前输入序列和前一个单元输出作为加权机制，决定应使用多少来自过去和现在的相关信息。应用使用门的输出将得到隐藏状态，这个状态也将作为下一个LSTM单元的前一个单元输出。
- en: Building a full LSTM network
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建完整的LSTM网络
- en: 'Usually, to create a full LSTM network, multiple LSTM layers are concatenated
    to each other using the sequence of hidden states from multiple LSTM cells as
    the subsequent sequence data to apply to the next LSTM layer. After a couple of
    LSTM layers, the hidden state sequence of the previous layer will usually then
    be passed into a fully connected layer to form the basis of supervised learning
    based simple LSTM architecture. *Figure 4**.3* shows a visual structure of how
    this is done:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，要创建一个完整的LSTM网络，会将多个LSTM层连接在一起，使用多个LSTM单元的隐藏状态序列作为后续序列数据，应用到下一个LSTM层。在经过几个LSTM层后，前一层的隐藏状态序列通常会传递到一个全连接层，形成基于监督学习的简单LSTM架构的基础。*图
    4.3* 显示了如何实现这一点的可视化结构：
- en: '![Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully
    connected layer](img/B18187_04_003.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 一个简单的LSTM网络，包含两个LSTM层，输出连接到一个全连接层](img/B18187_04_003.jpg)'
- en: Figure 4.3 – A simple LSTM network with two LSTM layers fed into a fully connected
    layer
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 一个简单的LSTM网络，包含两个LSTM层，并将其输入到一个全连接层
- en: 'Based on the network depicted in *Figure 4**.3*, the implementation in PyTorch
    will look like the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*图4**.3*中描绘的网络，PyTorch中的实现将如下所示：
- en: 'Let’s first import the PyTorch library’s handy `nn` module:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入PyTorch库中的`nn`模块：
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we will define the network architecture based on *Figure 4**.3*, using
    the sequential API this time instead of the class method:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将基于*图4**.3*定义网络架构，这次使用顺序API而不是类方法：
- en: '[PRE1]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The input size, hidden sizes, and layer number of the LSTM, along with the output
    feature size of the linear layer, can be configured according to your input dataset
    and desire. Note that each timestep or sequential step of the input data can have
    a size greater than one. This allows us to easily map original features into more
    representative feature embeddings and leverage their descriptive power. Additionally,
    the dropout regularizer can be added easily by setting the `dropout` parameter
    to a value between 0 and 1, which will introduce the dropout layer at each layer
    except the last layer at the specified probability. The RNN defined in `pytorch`
    in *step 2* can now be trained as usual, like any PyTorch models defined in a
    class.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM的输入大小、隐藏层大小和层数，以及线性层的输出特征大小，可以根据输入数据集和需求进行配置。注意，每个时间步或序列步的输入数据的大小可以大于一。这使得我们能够轻松地将原始特征映射到更具代表性的特征嵌入，并利用它们的描述能力。此外，通过将`dropout`参数设置为0到1之间的值，可以轻松添加dropout正则化器，这将在每层（除了最后一层）以指定的概率引入dropout层。在*步骤2*中定义的`pytorch`中的RNN现在可以像任何在类中定义的PyTorch模型一样进行训练。
- en: As usual, PyTorch has made building RNNs so much simpler and faster. Next, we
    will step into the next type of RNN, called **Gated** **Recurrent Units**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，PyTorch让构建RNN变得更加简单和快捷。接下来，我们将介绍另一种类型的RNN，称为**门控** **递归单元**。
- en: Understanding GRU
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解GRU
- en: '**Gated recurrent** **units** (**GRU**) was invented in 2014 and based on the
    ideas implemented in LSTM. GRU was made to simplify LSTM and provide a faster
    and more efficient way of achieving the same goals as LSTM to adaptively remember
    and forget based on past and present data. In terms of the learning capacity and
    metric performance achievable, there isn’t a clear silver-bullet winner among
    the two and often in the industry, the two RNN units are benchmarked against each
    other to figure out which method provides a better performance level. *Figure
    4**.4* shows the structure of GRU.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控递归** **单元**（**GRU**）是在2014年发明的，基于LSTM中实现的思想。GRU旨在简化LSTM，并提供一种更快速、更高效的方式，实现与LSTM相同的目标——根据过去和现在的数据自适应地记住和遗忘。在学习能力和可达的性能指标方面，二者没有明显的“银弹”赢家，通常在行业中，二者会相互对比，找出哪个方法提供更好的性能水平。*图4**.4*展示了GRU的结构。'
- en: '![Figure 4.4 – A low-level depiction of GRU](img/B18187_04_004.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – GRU的低级表示](img/B18187_04_004.jpg)'
- en: Figure 4.4 – A low-level depiction of GRU
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – GRU的低级表示
- en: '*Figure 4**.4* adopts the same weights and bias notations as the LSTM depicted
    in *Figure 4**.2*. There are three different names here for the final small letter
    notation. R being the reset gate, z representing the update gate, and h representing
    weights used to obtain the next hidden states. This means a GRU cell has fewer
    parameters than an LSTM cell, with three sets of weights and biases instead of
    four. This allows GRU networks to be slightly faster than LSTM networks.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4**.4*采用与*图4**.2*中LSTM相同的权重和偏置符号表示。这里有三种不同的小写字母符号表示。R代表重置门，z代表更新门，h代表用于获得下一个隐藏状态的权重。这意味着GRU单元的参数比LSTM单元少，具有三组权重和偏置，而不是四组。这样，GRU网络比LSTM网络稍微更快。'
- en: 'Although depicted as a single cell, the same theory that required multiple
    LSTM cells to be sequentially connected also applies to GRU: a GRU network layer
    will have multiple GRU cells connected sequentially together. GRU contains only
    two mechanisms, called the **reset gate** and the **update gate**, and only has
    one input from the previous GRU cell, and one output to the next GRU cell. The
    one input-output itself is obviously more efficient than LSTM, as we require fewer
    operations to be carried out. Now, let’s dive into these two mechanisms.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常描述为一个单元，但需要多个LSTM单元依次连接的相同理论也适用于GRU：GRU网络层将有多个GRU单元依次连接在一起。GRU只包含两个机制，称为**重置门**和**更新门**，并且只有来自前一个GRU单元的一个输入，以及一个输出到下一个GRU单元。这个单一的输入-输出明显比LSTM更高效，因为我们需要进行的操作更少。现在，让我们深入了解这两个机制。
- en: Decoding the reset gate of GRU
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码GRU的重置门
- en: The reset gate of GRU serves as a mechanism to forget the long-term information,
    also called the hidden state, of the previous cell. The goal of this mechanism
    is similar to the forget gate in the LSTM cell. Similarly, this will exist on
    a scale of 0 to 1, based on the current input sequence and the previous cell state,
    which decides how much we should reduce and remove the previously gained long-term
    information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: GRU的重置门作为一种机制，用于忘记先前单元的长期信息，也称为隐藏状态。这个机制的目标类似于LSTM单元中的忘记门。同样，它将在0到1的范围内，根据当前输入序列和先前单元状态，决定我们应该减少和删除多少先前获得的长期信息。
- en: However, the reset gates of GRU are functionally different from the forget gate
    of LSTM. While the forget gate of LSTM decides what information to forget from
    the long-term memory, the reset gate of GRU decides how much of the previous hidden
    state to forget.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GRU的重置门在功能上与LSTM的忘记门不同。LSTM的忘记门决定从长期记忆中忘记哪些信息，而GRU的重置门决定忘记多少先前的隐藏状态。
- en: Decoding the update gate of GRU
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码GRU的更新门
- en: The update gate of GRU controls the amount of information from the long-term
    memory to be transferred to the currently maintained memory. This is similar to
    the remember gate in LSTMs and helps the network to remember long-term information.
    Each weight associated with the previous cell’s hidden unit will learn to capture
    both short-term dependencies and long-term dependencies. The short-term dependencies
    usually have reset gates output values that are closer to `0` to forget former
    information more frequently, and vice versa with weights and hidden state positions
    that learn long-term dependencies.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GRU的更新门控制从长期记忆中传递到当前保持的记忆的信息量。这类似于LSTM中的记忆门，帮助网络记住长期信息。与前一个单元的隐藏单元相关的每个权重将学习捕捉短期依赖和长期依赖。短期依赖通常会有接近`0`的重置门输出值，更频繁地遗忘先前的信息，而长期依赖则通过学习长期依赖的权重和隐藏状态位置来进行反向操作。
- en: In terms of the difference from the LSTM remember gate, while the remember gate
    of LSTM decides what information to remember from the current input and previous
    hidden state, the update gate of GRU decides how much of the previous hidden state
    to remember.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在与LSTM的记忆门的差异方面，LSTM的记忆门决定了从当前输入和先前的隐藏状态中记住哪些信息，而GRU的更新门决定了记住多少先前的隐藏状态。
- en: GRU is a simple RNN to consider with more efficient operations compared to LSTMs.
    Now that we have decoded both LSTMs and GRU, instead of repeating another GRU-only
    full network similar to LSTM, let’s discover improvements that can be made using
    these two methods as a base.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: GRU是一种简单的RNN，相比LSTM具有更高效的操作。现在我们已经解码了LSTM和GRU，接下来，我们不再重复类似LSTM的仅含GRU的完整网络，而是探索如何在这两种方法的基础上进行改进。
- en: Understanding advancements over the standard GRU and LSTM layers
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解相较于标准GRU和LSTM层的进展
- en: GRU and LSTM are the most widely used RNN methods today, but one might wonder
    how to push the boundaries achievable by a standard GRU or a standard LSTM. One
    good start to building this intuition is to understand that both of the layer
    types are capable of accepting sequential data, and to build a network you need
    multiple RNN layers. This means that it is entirely possible to combine GRU and
    LSTM layers in the same network. This, however, is not credible enough to be considered
    an advancement as a fully LSTM network or a fully GRU network can exceed the performance
    of a combined LSTM and GRU network at any time. Let’s dive into another simple
    improvement you can make on top of these standard RNN layers, called **bidirectional
    RNN**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GRU和LSTM是当前最广泛使用的RNN方法，但有人可能会想知道如何突破标准GRU或标准LSTM能够达到的极限。构建这种直觉的一个好方法是理解这两种层类型都能够接受序列数据，而要构建一个网络，你需要多个RNN层。这意味着完全可以在同一个网络中结合GRU和LSTM层。然而，这并不足以被视为一种突破，因为完全的LSTM网络或完全的GRU网络随时都可以超过GRU和LSTM组合网络的性能。让我们深入了解一下可以在这些标准RNN层上做的另一个简单改进，叫做**双向RNN**。
- en: Decoding bidirectional RNN
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码双向RNN
- en: Both GRU and LSTM rely on the sequential nature of the data. This order of the
    sequence can be forward in increasing time steps and also can be backward in decreasing
    time steps. Which direction to use usually comes down to an act of trial and error,
    and more often than not, the natural direction to use will be the forward time
    order.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GRU和LSTM都依赖于数据的顺序特性。这个顺序可以是时间步递增的前向序列，也可以是时间步递减的后向序列。选择使用哪个方向通常取决于试验和错误，往往自然的选择是使用前向时间顺序。
- en: 'In the year 1997, an improvement was made called bidirectional RNNs, which
    combine both forward-ordered RNNs with backward-ordered RNNs in an effort to maximize
    the input data that can be processed by an RNN model. The original idea was to
    estimate the value at a current timestep using both future information and historical
    information, given that both future and historical information was available with
    two of the RNNs taking in different sets of data. This naturally allowed for the
    capacity to achieve better prediction performance on such data setups. Today,
    this idea has extended to be a general layer applied to the same sequential data
    estimating and is also proven to provide prediction performance improvements.
    *Figure 4**.5* shows an example of bidirectional RNNs using GRU where the hidden
    states from the forward and backward GRU are concatenated:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在1997年，出现了一种改进叫做双向RNN（Bidirectional RNNs），它结合了前向RNN和后向RNN，旨在最大化RNN模型能够处理的输入数据。原始的想法是利用未来信息和历史信息来估算当前时间步的值，因为未来和历史信息是可以获取的，两个RNN分别处理不同的数据集。这样自然就能够在这种数据设置下实现更好的预测性能。如今，这一想法已经扩展为一个通用的层，应用于相同的序列数据估计，并且也证明能够提供预测性能的提升。*图
    4.5* 展示了使用GRU的双向RNN的一个例子，其中来自前向和后向GRU的隐藏状态被连接在一起：
- en: '![Figure 4.5 – Bidirectional GRU](img/B18187_04_005.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 双向GRU](img/B18187_04_005.jpg)'
- en: Figure 4.5 – Bidirectional GRU
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 双向GRU
- en: 'The concatenated hidden states can then be passed into fully connected layers
    for standard supervised learning objectives. An example implementation in PyTorch
    of a bidirectional GRU is shown:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 连接后的隐藏状态可以传递给全连接层，进行标准的监督学习目标。以下是一个在PyTorch中实现双向GRU的示例：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let’s discover an improvement that is made based on LSTMs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索一下基于LSTM做出的一个改进。
- en: Adding peepholes to LSTMs
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向LSTM添加窥视孔（Peepholes）
- en: 'Introduced in 2000, peepholes enable the cell states (from the previous and
    current cell), which hold the long-term memory of LSTMs, to influence the sigmoid
    gating mechanisms in the LSTM cell. The intuition is that the long-term memory
    has information about the past time steps that are not available in the short-term
    memory held in the hidden state of the previous cell. This allowed for improved
    predictive performance when compared to a vanilla LSTM. *Figure 4**.6* shows the
    extra peephole connections from the cell state:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 2000年引入的窥视孔使得细胞状态（来自前一个和当前的细胞），即LSTM中的长期记忆，能够影响LSTM单元中的sigmoid门控机制。直觉是，长期记忆包含了过去时间步的信息，而这些信息在前一个单元的隐藏状态中不可用的短期记忆中无法获取。这使得与普通LSTM相比，预测性能得到了改善。*图
    4.6* 展示了来自细胞状态的额外窥视孔连接：
- en: '![Figure 4.6 – LSTM peephole connections](img/B18187_04_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – LSTM窥视孔连接](img/B18187_04_006.jpg)'
- en: Figure 4.6 – LSTM peephole connections
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – LSTM窥视孔连接
- en: However, one pitfall of this method is that the cell states can grow to large
    values over time due to the long-term memory nature of the states, as it is unbounded.
    This may saturate the gates to always be in an open state and render the gate
    useless sometimes. This brings us to the last improvement that we will discuss
    in the next subsection.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的一个陷阱是，由于状态的长期记忆特性，单元状态可能会随着时间的推移增长到非常大的值，因为其没有边界。这可能会导致门控机制饱和，始终处于打开状态，从而使门控机制有时失效。这就引出了我们将在下一小节讨论的最后一种改进。
- en: Adding working memory to exceed the peephole connection limitations for LSTM
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为了超越 LSTM 的窥视孔连接限制，加入工作记忆
- en: 'In 2021, an improvement was made on top of peephole connections for LSTMs to
    enforce a bound to the cell states by using the `tanh` activation. The simple
    addition proved itself to be better in performance than the LSTM with peepholes
    unbounded version in experimental benchmarks and was called **Working Memory Connections**
    **for LSTM**. *Figure 4**.7* shows the Working Memory Connection for LSTM structure:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2021 年，LSTM 的窥视孔连接上进行了改进，通过使用 `tanh` 激活函数来对单元状态施加边界，从而对单元状态进行限制。这一简单的改进在实验基准测试中表现出了比未加边界的
    LSTM 更好的性能，被称为 **LSTM 的工作记忆连接**。*图 4.7* 显示了 LSTM 结构中的工作记忆连接：
- en: '![Figure 4.7 – Working Memory Connection for LSTM structure](img/B18187_04_007.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – LSTM 结构中的工作记忆连接](img/B18187_04_007.jpg)'
- en: Figure 4.7 – Working Memory Connection for LSTM structure
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – LSTM 结构中的工作记忆连接
- en: RNNs are similar to the standard MLP in the sense that there isn’t a single
    generic dataset that is used as a reference across different research initiatives.
    Even if the same dataset is used, the results might not be a definitive source
    of truth as, again, the dataset is not extensive enough to generalize across other
    datasets. In other words, there is no `ImageNet` equivalent in sequence data.
    Text data, video data, and other time-series data are all wildly different from
    each other but essentially are all considered sequence data and can be fed into
    RNNs. Take benchmark results from anywhere on RNNs and MLPs with a pinch of salt
    as results can vary widely from dataset to dataset. *Figure 4**.8*, however, shows
    one version of benchmarks done with GRU, LSTM, `COCO` dataset using a metric that
    considers the naturalism of the produced text – the higher the better.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 与标准的 MLP 类似，意思是没有一个通用的数据集可以作为不同研究项目中的参考数据集。即使使用相同的数据集，结果也可能不是最终的真理来源，因为数据集并不广泛，无法泛化到其他数据集。换句话说，在序列数据中没有类似
    `ImageNet` 的通用数据集。文本数据、视频数据和其他时间序列数据之间差异很大，但本质上都被视为序列数据，并且可以输入到 RNN 中。无论是 RNN
    还是 MLP，从任何地方获得的基准测试结果都需要谨慎看待，因为结果可能因数据集不同而大相径庭。然而，*图 4.8* 显示了在 `COCO` 数据集上使用 GRU、LSTM
    和一个考虑生成文本自然性度量的基准版本——生成的文本自然性越高越好。
- en: '![Figure 4.8 – RNN benchmark on image captioning task on COCO dataset](img/B18187_04_008.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 在 COCO 数据集上进行图像标题任务的 RNN 基准测试](img/B18187_04_008.jpg)'
- en: Figure 4.8 – RNN benchmark on image captioning task on COCO dataset
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 在 COCO 数据集上进行图像标题任务的 RNN 基准测试
- en: The figure shows that LSTM-WM dominates over the other methods in a single experiment
    across different evaluation scores. Again, take the results with a pinch of salt
    as the COCO dataset is by no means a representative dataset of sequential or time-series
    data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示，LSTM-WM 在不同评估分数上的单次实验中优于其他方法。再次提醒，尽管如此，COCO 数据集并不代表序列数据或时间序列数据。
- en: Try it out on your own dataset to know for sure! With that, we have gone through
    important concepts of RNN, from basic to advanced levels. Let’s summarize the
    chapter next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在你自己的数据集上试试看，确保了解！到此为止，我们已经讲解了 RNN 的重要概念，从基础到高级。接下来，让我们总结一下本章内容。
- en: Summary
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Recurrent neural networks are a type of neural network that explicitly includes
    inductive biases of sequential data in its structure.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络是一种神经网络，明确在其结构中包括了序列数据的归纳偏置。
- en: A couple of variations of RNNs exist but all of them maintain the same high-level
    concept for their overall structure. Mainly, they provide varying ways to decide
    which data to learn from and remember along with which data to forget from the
    memory from the remembering stage.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种不同类型的 RNN，但它们的整体结构保持相同的高层次概念。主要是它们提供了不同的方式来决定从哪些数据中学习、记住哪些数据以及在记忆阶段从记忆中忘记哪些数据。
- en: However, do note that a more recent architecture called transformers, which
    will be introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding
    Neural Network Transformers*, demonstrated that recurrence is not needed to achieve
    a good performance on sequential data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，最近出现了一种叫做transformers的架构，它将在[*第6章*](B18187_06.xhtml#_idTextAnchor092)《理解神经网络Transformer》一章中介绍，*该架构证明了在处理序列数据时，递归并不是实现良好性能的必要条件*。
- en: With that, we are done with RNNs and will dive briefly into the world of autoencoders
    in the next chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了对RNN的介绍，并将在下一章简要探讨自编码器的世界。
