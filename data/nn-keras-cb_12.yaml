- en: Applications of a Many-to-One Architecture RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多对一架构RNN的应用
- en: In the previous chapter, we learned about the workings of RNN and LSTM. We also
    learned about sentiment classification, which is a classic many-to-one application,
    as many words in the input correspond to one output (positive or negative sentiment).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了RNN和LSTM的工作原理。我们还学习了情感分类，它是一个经典的多对一应用，因为输入中的许多单词对应一个输出（正面或负面情感）。
- en: 'In this chapter, we will further our understanding of the many-to-one architecture
    RNN by going through the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过以下食谱进一步加深对多对一架构RNN的理解：
- en: Generating text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文本
- en: Movie recommendations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影推荐
- en: Topic-modeling using embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用嵌入进行主题建模
- en: Forecasting the value of a stock's price
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测股票价格的价值
- en: Generating text
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成文本
- en: In the sentiment-classification recipes that we performed in [Chapter 11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml),
    *Building a Recurrent Neural Network*, we were trying to predict a discrete event
    (sentiment classification). This falls under the many-to-one architecture. In
    this recipe, we will learn how to implement a many-to-many architecture, where
    the output would be the next possible 50 words of a given sequence of 10 words.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在[第11章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)中进行的情感分类任务中，*构建循环神经网络*，我们尝试预测一个离散事件（情感分类）。这属于多对一架构。在本食谱中，我们将学习如何实现多对多架构，其中输出将是给定10个单词序列之后可能的下一个50个单词。
- en: Getting ready
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy that we''ll adopt to generate text is as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成文本的策略如下：
- en: Import project Gutenberg's *Alice's Adventures in Wonderland* dataset, which
    can be downloaded from [https://www.gutenberg.org/files/11/11-0.txt](https://www.gutenberg.org/files/11/11-0.txt).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入项目古腾堡的*爱丽丝梦游仙境*数据集，该数据集可以从[https://www.gutenberg.org/files/11/11-0.txt](https://www.gutenberg.org/files/11/11-0.txt)下载。
- en: Preprocess the text data so that we bring every word to the same case, and remove
    punctuation.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本数据进行预处理，以便将每个单词转换为相同的大小写，并移除标点符号。
- en: Assign an ID to each unique word and then convert the dataset into a sequence
    of word IDs.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个唯一单词分配一个ID，然后将数据集转换为一个单词ID的序列。
- en: Loop through the total dataset, 10 words at a time. Consider the 10 words as
    input and the subsequent 11th word as output.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历整个数据集，每次处理10个单词。将这10个单词视为输入，并将随后的第11个单词视为输出。
- en: Build and train a model, by performing embedding on top of the input word IDs
    and then connecting the embeddings to an LSTM, which is connected to the output
    layer through a hidden layer. The value in the output layer is the one-hot-encoded
    version of the output.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并训练一个模型，通过对输入单词ID进行嵌入，并将嵌入连接到LSTM，再通过隐藏层将其连接到输出层。输出层中的值是输出的独热编码版本。
- en: Make a prediction for the subsequent word by taking a random location of word
    and consider the historical words prior to the location of the random word chosen.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选取一个随机位置的单词，并考虑该位置之前的历史单词来预测随后的单词。
- en: Move the window of the input words by one from the seed word's location that
    we chose earlier and the tenth time step word shall be the word that we predicted
    in the previous step.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入单词的窗口从我们之前选择的种子单词的位置向前移动一个位置，第十个时间步的单词将是我们在上一步骤中预测的单词。
- en: Continue this process to keep generating text.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续这一过程以不断生成文本。
- en: How to do it...
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'Typical to the need for RNN, we will look at a given sequence of 10 words to
    predict the next possible word. For this exercise, we will take the Alice dataset
    to generate words, as follows (the code file is available as `RNN_text_generation.ipynb`
    in GitHub):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的典型需求，我们将查看给定的10个单词序列，以预测下一个可能的单词。在这个练习中，我们将采用《爱丽丝梦游仙境》数据集来生成单词，如下所示（代码文件可以在GitHub上的`RNN_text_generation.ipynb`中找到）：
- en: 'Import the relevant packages and dataset:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A sample of the input text looks as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本的示例如下所示：
- en: '![](img/e3436727-b2bd-4f9c-af35-991347215f99.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3436727-b2bd-4f9c-af35-991347215f99.png)'
- en: 'Normalize the text to remove punctuations and convert it to lowercase:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化文本以移除标点符号并将其转换为小写：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Assign the unique words to an index so that they can be referenced when constructing
    the training and test datasets:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将唯一的单词分配给一个索引，以便在构建训练和测试数据集时引用：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Construct the input set of words that leads to an output word. Note that we
    are considering a sequence of `10` words and trying to predict the *11*^(*th*)
    word:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建输入单词集，从中生成输出单词。请注意，我们考虑的是`10`个单词的序列，并尝试预测*第11*个（*th*）单词：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A sample of the `input_words` and `label_words` lists is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_words`和`label_words`列表的示例如下：'
- en: '![](img/a1db4f29-47c6-403b-a342-3442e1107f62.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1db4f29-47c6-403b-a342-3442e1107f62.png)'
- en: Note that `input_words` is a list of lists and the `output_words` list is not.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`input_words`是一个包含列表的列表，而`output_words`则不是。
- en: 'Construct the vectors of the input and the output datasets:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建输入和输出数据集的向量：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are creating empty arrays in the preceding step, which will be populated
    in the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们创建了空数组，这些数组将在接下来的代码中被填充：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the preceding code, the first `for` loop is used to loop through all the
    words in the input sequence of words (`10` words in input), and the second `for`
    loop is used to loop through an individual word in the chosen sequence of input
    words. Additionally, given that the output is a list, we do not need to update
    it using the second `for` loop (as there is no sequence of IDs). The output shapes
    of `X` and `y` are as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，第一个`for`循环用于遍历输入词序列中的所有单词（输入中有`10`个单词），第二个`for`循环用于遍历选定的输入词序列中的每个单词。此外，由于输出是一个列表，我们不需要通过第二个`for`循环来更新它（因为没有ID序列）。`X`和`y`的输出形状如下：
- en: '![](img/d6fc2f39-53df-42c7-b878-1a7b062b084c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6fc2f39-53df-42c7-b878-1a7b062b084c.png)'
- en: 'Define the architecture of the model:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的架构：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A summary of the model is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/1369b6ce-e893-4f4c-8c0c-156ad528671e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1369b6ce-e893-4f4c-8c0c-156ad528671e.png)'
- en: 'Fit the model. Look at how the output varies over an increasing number of epochs.
    Generate a random set of sequences of `10` words and try to predict the next possible
    word. We are in a position to observe how our predictions are getting better over
    an increasing number of epochs:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型。观察输出随着轮次的增加如何变化。生成一个随机的`10`个单词的序列，并尝试预测下一个可能的单词。我们可以观察到，随着轮次的增加，我们的预测逐渐变得更好：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, we are fitting our model on input and output arrays for
    one epoch. Furthermore, we are choosing a random seed word (`test_idx` – which
    is a random number that is among the last 10% of the input array (as `validation_split`
    is `0.1`) and are collecting the input words at a random location. We are converting
    the input sequence of IDs into a one-hot-encoded version (thus obtaining an array
    that is 1 x 10 x `total_words` in shape).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在为一个轮次（epoch）拟合我们的模型，使用输入和输出数组。此外，我们选择一个随机的种子词（`test_idx` – 这是一个随机数，位于输入数组的最后10%中（因为`validation_split`为`0.1`），并在随机位置收集输入单词。我们将输入ID序列转换为one-hot编码版本（因此得到的数组形状为1
    x 10 x `total_words`）。
- en: 'Finally, we make a prediction on the array we just created and obtain the word
    that has the highest probability. Let''s look at the output in the first epoch
    and contrast that with output in the *25^(th)* epoch:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对刚刚创建的数组进行预测，并获得概率最高的单词。我们来看一下第一轮（epoch）输出的结果，并与第*25^(th)*轮的输出进行对比：
- en: '![](img/241f2377-e03e-4480-9486-3f2db105aaf5.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/241f2377-e03e-4480-9486-3f2db105aaf5.png)'
- en: 'Note that the output is always `the` in the first epoch. However, it becomes
    more reasonable as follows at the end of 50 epochs:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一轮的输出总是`the`。然而，在经过50轮训练后，输出变得更加合理，如下所示：
- en: '![](img/e2a8dab4-322a-446e-b552-0c8844540497.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2a8dab4-322a-446e-b552-0c8844540497.png)'
- en: The `Generating from seed` line is the collection of predictions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`Generating from seed`行是预测结果的集合。'
- en: Note that while the training loss decreased over increasing epochs, the validation
    loss has become worse by the end of 50 epochs. This will improve as we train on
    more text and/or further fine-tune our model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然训练损失随着轮次的增加而减少，但在50轮结束时，验证损失变得更糟。随着我们在更多文本上训练和/或进一步微调模型，这将得到改善。
- en: 'Additionally, this model could further be improved by using a bidirectional
    LSTM, which we will discuss in *Sequence to Sequence learning* chapter. The output
    of having a bidirectional LSTM is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个模型可以通过使用双向LSTM进一步改进，我们将在*序列到序列学习*一章中讨论。使用双向LSTM后的输出如下：
- en: '![](img/76fac46c-8de1-4a31-a8b5-190b4ba55fa9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76fac46c-8de1-4a31-a8b5-190b4ba55fa9.png)'
- en: Movie recommendations
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电影推荐
- en: Recommendation systems play a major role in the discovery process for a user.
    Think of an e-commerce catalog that has thousands of distinct products. Additionally,
    variants of a product also exist. In such cases, educating the user about the
    products or events (in case certain products are on sale) becomes the key to increasing
    sales.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统在用户发现过程中的作用非常重要。想象一下一个电商目录，其中包含成千上万种不同的产品。此外，某个产品的不同变体也会存在。在这种情况下，向用户普及产品或事件（例如某些产品正在打折）成为增加销售的关键。
- en: Getting ready
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this recipe, we will be learning about building a recommendation system for
    a database of ratings given by users to movies. The objective of the exercise
    is to maximize the relevance of a movie to a user. While defining the objective,
    we should also consider that a movie that is recommended might still be relevant,
    but might not be watched by the user immediately. At the same time, we should
    also ensure that all the recommendations are not about the same genre. This is
    especially applicable in the case of recommendations given out in a retail setting,
    where we do not want to be recommending the variants of the same product across
    all the recommendations we are providing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们将学习如何为一个包含用户对电影评分的数据库构建推荐系统。这个练习的目标是最大化电影与用户的相关性。在定义目标时，我们还应考虑到，虽然推荐的电影可能仍然相关，但用户可能不会立即观看。同时，我们还应确保所有推荐内容不都属于同一类型。尤其在零售环境下，我们不希望在所有推荐中都推荐同一产品的不同变体。
- en: 'Let''s formalize our objective and constraints:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式定义我们的目标和约束：
- en: '**Objective**: Maximize the relevance of recommendations to a user'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：最大化推荐内容对用户的相关性'
- en: '**Constraint**: Increase the diversity of a recommendation and offer a maximum
    of 12 recommendations to the user'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**约束**：增加推荐的多样性，并向用户提供最多12个推荐'
- en: The definition of relevance can vary from use case to use case and is generally
    guided by the business principles. In this recipe, let's define relevance narrowly;
    that is, if the user buys any product that is in the top 12 recommended items
    for the given user, it is a success.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性的定义因用例而异，通常由商业原则指导。在本例中，我们将相关性定义得比较狭窄；也就是说，如果用户购买了在给定用户的前12个推荐商品中的任何一项，就视为成功。
- en: 'With this, let''s go ahead and define the steps that we will adopt to build
    the model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，让我们定义构建模型的步骤：
- en: Import the data.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据。
- en: Recommend a movie that a user would rate highly—hence, let us train our model
    based on movies that a user liked in the history. The insight that a user disliked
    certain movies will be useful into further improving our recommendations. However,
    let's keep this simple for now.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推荐一部用户会高评分的电影——因此，让我们根据用户历史上喜欢的电影来训练模型。用户不喜欢某些电影的洞察将有助于进一步改善我们的推荐。然而，先保持简单。
- en: Keep only the users who have watched more than five movies.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只保留观看了超过五部电影的用户。
- en: Assign IDs to unique users and movies.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为独特的用户和电影分配ID。
- en: Given that a user's preference might change over time, we need to consider the
    history of a user where different events in history have different weightages
    associated with them. Given that is a time series analysis problem now, we will
    leverage RNN to solve this problem.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于用户的偏好可能随时间变化，我们需要考虑用户的历史，历史中的不同事件有不同的权重。鉴于这是一个时间序列分析问题，我们将利用RNN来解决该问题。
- en: 'Preprocess the data so that it can then passed to an LSTM:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行预处理，以便将其传递给LSTM：
- en: The input will be the historical five movies watched by a user
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入将是用户观看的历史五部电影
- en: The output is the sixth movie watched by a user
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出是用户观看的第六部电影
- en: 'Build a model that does the following:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个执行以下操作的模型：
- en: Creates embeddings for the input movies
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为输入的电影创建嵌入
- en: Passes the embeddings through an LSTM layer
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将嵌入通过LSTM层
- en: Passes the LSTM output through a dense layer
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将LSTM的输出通过一个全连接层
- en: Apply softmax in final layer to come up with a list of movies to recommend
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最终层应用softmax，以生成推荐的电影列表
- en: How to do it...
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Now that we have gone through the strategy of various steps to perform, let''s
    code it up (the code file is available as `Chapter_12_Recommender_systems.ipynb`
    in GitHub):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了执行各种步骤的策略，让我们开始编写代码（代码文件在GitHub上的`Chapter_12_Recommender_systems.ipynb`中提供）：
- en: 'Import the data. We''ll be working on a dataset that has the list of users,
    the ratings provided for different movies by a user, and the corresponding time
    stamp of when the user has provided the ratings:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据。我们将使用一个包含用户列表、用户对不同电影评分以及用户提供评分的时间戳的数据集：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A sample of the dataset looks as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的一个示例如下所示：
- en: '![](img/033e8f33-fbdb-4a5c-b80a-626371a6ca12.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/033e8f33-fbdb-4a5c-b80a-626371a6ca12.png)'
- en: 'Filter out the data points where the user did not like the movie or the users
    where the user did not have enough history. In the following code, we are excluding
    the movies that users provided low ratings for:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过滤掉用户没有喜欢的电影或没有足够历史记录的用户。在以下代码中，我们排除了用户给出低评分的电影：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the following code, we are keeping only those users who have more than `5`
    ratings (a rating value greater than `3`) provided in the history:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们仅保留那些在历史记录中提供超过`5`个评分（评分值大于`3`）的用户：
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Assign IDs to unique `users` and `Movies` so that we use them subsequently:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为独特的`users`和`Movies`分配ID，以便后续使用：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Preprocess the data so that the input is the last five movies and the output
    is the sixth movie watched:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据进行预处理，使得输入是最后五部电影，输出是第六部观看的电影：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Preprocess the `historical5_watched` and the `movie_to_predict` variables so
    that they can be passed to the model, and then create the train and test datasets:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`historical5_watched`和`movie_to_predict`变量进行预处理，以便将其传递给模型，然后创建训练集和测试集：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Build the model:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that, in the final layer, we are adding 1 to the possible activations,
    as there is no movie with an ID of 0, and the final movie would have been left
    out had we just set the value to `max(y)`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在最后一层中，我们对可能的激活值加1，因为没有ID为0的电影，如果我们仅将值设置为`max(y)`，最终的电影将被排除在外。
- en: 'A summary of the model is as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/82bd4fe6-5261-4651-b339-d7ee590096d0.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82bd4fe6-5261-4651-b339-d7ee590096d0.png)'
- en: 'Fit the model:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Make predictions on the test data:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试数据进行预测：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Understand the number of data points (users) where the movie watched next after
    the historical five movies is among the top `12` recommendations:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 了解数据点的数量（用户），其中接下来观看的电影在前五部历史电影之后位于前`12`个推荐中：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We should notice that in 10.4% of the total cases, we have the movie recommended
    being watched by the user as the immediate next movie.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，在总案例的10.4%中，推荐给用户的电影恰好是他们接下来要观看的电影。
- en: Taking user history into consideration
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考虑用户历史
- en: One of the considerations when sending out the top 12 recommendations that we
    missed in the previous iteration is that *if a user has already watched a movie,
    they are less likely to watch the same movie again* (note that this hypothesis
    does not hold true in a retail setting, where there are a considerable amount
    of re-orders).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布前12个推荐电影时，我们需要考虑的一个因素是*如果用户已经观看过某部电影，他们不太可能再次观看同一部电影*（请注意，这一假设在零售环境中并不成立，因为在零售环境中会有相当数量的重复订单）。
- en: Let's go ahead and apply this logic in making our top 12 predictions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续应用这个逻辑，进行前12个推荐的预测。
- en: 'First, we''ll store all the (not just the most recent five) movies that were
    watched by a user prior to watching the movie that we are trying to predict:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将存储所有（不仅仅是最近观看的五部）用户在观看我们尝试预测的电影之前观看过的电影：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we are filtering all the movies watched by a user.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们过滤出所有用户观看过的电影。
- en: 'If a user has already watched a movie, we will overwrite the probability to
    a value of zero for that user-movie combination:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户已经观看过一部电影，我们将该用户-电影组合的概率覆盖为零：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the following code, we are calculating the percent of the total scenario
    in test data where the user watched a movie among the top 12 recommended movies:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们计算测试数据中用户观看的电影是否在前12个推荐电影中所占的百分比：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding results in the recommendations being valid for 12.6% of total
    users now, up from 10.4% relevance in the previous iteration.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前述方法，推荐有效的用户比例从上次迭代的10.4%上升至12.6%。
- en: Topic-modeling, using embeddings
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模，使用嵌入
- en: In the previous recipe, we learned about generating predictions for movies that
    a user is likely to watch. One of the limitations of the previous way of generating
    predictions is that the variety of movie recommendations would be limited if we
    did not perform further processing on top of the movie predictions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们学习了如何为用户可能观看的电影生成预测。以前生成预测的方式的一个限制是，如果我们没有在电影预测之上进行进一步处理，那么电影推荐的多样性将受到限制。
- en: A variety of recommendations is important; if there were no variety, only certain
    types of products would be discovered by users.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 多样化的推荐非常重要；如果没有多样性，用户只会发现某些类型的产品。
- en: In this recipe, we will group movies based on their similarity and identify
    the common themes of the movies. Additionally, we will also look into how we can
    increase the variety of recommendations that can be provided to a user. Having
    said that, it is highly likely that this strategy will work less in the specific
    case of movie recommendations, as the variety would be much lower when compared
    to a retail/e-commerce setting, where the number of categories and substitutes
    of a product are much higher when compared to movies.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将基于它们的相似性对电影进行分组，并识别电影的共同主题。此外，我们还将探讨如何增加向用户提供的推荐多样性。尽管如此，在电影推荐的具体案例中，这种策略的可行性可能较低，因为与在零售/电子商务设置中产品类别和替代品的数量相比，电影的类别和替代品要少得多。
- en: Getting ready
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy that we will adopt to group movies based on similarity is as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采取的基于相似性分组电影的策略如下：
- en: Extract the embedding value of each movie from the model that we built in the
    Movie recommendations recipe
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们在电影推荐配方中构建的模型中提取每部电影的嵌入值
- en: We can also create embeddings for each movie using gensim
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以使用 gensim 为每部电影创建嵌入
- en: All the movies watched by a user can be thought of as words in a sentence
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户观看的所有电影可以被视为句子中的单词
- en: Create a list of lists of word IDs that form a sentence
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个由形成句子的单词 ID 列表的列表
- en: Pass the list of lists through the `Word2Vec` method of gensim to extract the
    word vectors (movie ID vectors)
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 gensim 的 `Word2Vec` 方法传递列表列表，以提取单词向量（电影 ID 向量）
- en: Pass the embedded values (vectors) of movies through a k-means clustering process
    to extract a certain number of clusters
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将电影的嵌入值（向量）通过 k-means 聚类过程，提取出若干个簇
- en: Identify the optimal number of clusters
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最优簇的数量
- en: Identify the high probability to buy products (among the products that were
    not bought in history) in each cluster and re-rank the products based on their
    probability
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个簇中识别高概率购买的产品（在历史上未购买的产品中），并根据它们的概率重新排名产品
- en: Recommend the top *n* products
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推荐前 *n* 个产品
- en: In this process, one of the variables is the number of clusters to be formed.
    The greater the number of clusters, the fewer the products in each cluster, and,
    at the same time, the greater the similarity between each product within a cluster.
    Essentially, there is a trade-off between the number of points in a group and
    the similarity of data points within the same cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，一个变量是要形成的簇的数量。簇的数量越多，每个簇中的产品越少，同时，同一簇内的每个产品之间的相似性越大。基本上，数据点数量与同一簇内数据点相似性之间存在权衡。
- en: We can come up with a measure of the similarity of points within a group by
    calculating the sum of the squared distances of all points with respect to their
    cluster centers. The number of clusters beyond which the inertia metric does not
    decrease considerably is the optimal number of clusters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过计算所有点相对于它们的聚类中心的平方距离之和，我们可以得出一组内部点相似性的度量。惯性度量不会显著减少的簇的数量是最优簇的数量。
- en: How to do it...
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Now that we have formed a strategy of fetching a variety of products within
    our recommendation, let's code it up (We'll continue from step 3 of *Movie recommendations*
    recipe). The code file is available as `Chapter_12_Recommender_systems.ipynb` in
    GitHub.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经形成了在我们的推荐中获取各种产品的策略，让我们编写代码（我们将从 *电影推荐* 配方的第 3 步继续）。 代码文件在 GitHub 中可作为
    `Chapter_12_Recommender_systems.ipynb` 获得。
- en: Extract the embedding values of each movie using `Word2Vec`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Word2Vec` 提取每部电影的嵌入值。
- en: 'Create a list of lists of various movies watched by all users:'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建所有用户观看的各种电影列表的列表：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, we are filtering all the movies watched by a user and
    creating a list of movies watched by all users.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述代码中，我们过滤了所有用户观看的电影，并创建了所有用户观看的电影列表。
- en: 'Extract the word vectors of each movie:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每部电影的词向量：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Extract the `TSNE` values of the movies to have a visual representation of
    the word embeddings of the movies that we extracted in previous step:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取电影的`TSNE`值，以便对我们在上一阶段提取的电影词嵌入进行可视化表示：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A visualization of embeddings in 2-Dimensional space is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 2D空间中嵌入的可视化如下所示：
- en: '![](img/cb69b499-e7ba-41a8-b2ea-dd2b11c96e87.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb69b499-e7ba-41a8-b2ea-dd2b11c96e87.png)'
- en: From the preceding output, we can see that there are clusters of movies that
    are grouped together (the regions that are thick).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到有些电影被分组在一起（这些区域比较密集）。
- en: 'Store the movie ID and movie index values in a dataframe:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将电影ID和电影索引值存储在数据框中：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Merge the `tsne_df` and `idx2movie` datasets so that we have all the values
    in a single dataframe:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并`tsne_df`和`idx2movie`数据集，这样我们就能在一个单独的数据框中得到所有的值：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the `movies` dataset:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`movies`数据集：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Merge the `TSNE` dataset with the movies data, and drop the unwanted columns:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`TSNE`数据集与电影数据合并，并删除不需要的列：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Exclude the rows that have an NaN value (we have null values for certain movies,
    as certain movies occur less frequently, resulting in `Word2Vec` not giving the
    word vector for rarely occurring words (due to the `min_count` parameter):'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排除包含NaN值的行（由于某些电影出现的频率较低，导致`Word2Vec`没有为这些稀有词提供词向量（由于`min_count`参数）：
- en: '[PRE28]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Identify the optimal number of clusters by understanding the variation of inertia
    (total sum of squared distance of all points from their respective cluster centers):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过了解惯性变化（所有点到各自聚类中心的总平方距离）来确定最佳聚类数量：
- en: '[PRE29]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The variation of inertia for different number of clusters is as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数量聚类的惯性变化如下：
- en: '![](img/d667c7e1-453a-402c-a79b-1decc6dfa5bb.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d667c7e1-453a-402c-a79b-1decc6dfa5bb.png)'
- en: From the preceding curve, we can see that the decrease is inertia not as high
    as the number of clusters passes `40`. Hence, we shall have `40` as the optimal
    number of clusters of movies within our dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的曲线来看，我们可以看到，当聚类数量超过`40`时，惯性下降的幅度没有那么大。因此，我们将`40`定为我们数据集中电影的最佳聚类数量。
- en: 'Validate the cluster results by manually checking for some of the movies that
    fall in the same cluster, if it makes sense for the movies to be in the same cluster:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过手动检查某些落入同一聚类的电影来验证聚类结果，检查这些电影是否合理地分在同一聚类：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once you execute the code, you will notice that movies located in `cluster_label`:
    `0` are primarily Romance and Comedy movies.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行代码，你会注意到位于`cluster_label`：`0`的电影主要是浪漫和喜剧类型的电影。
- en: 'Remove the movies that have already been watched by the user:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除用户已观看的电影：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For each user, map the probability of a movie and the cluster number that a
    movie belongs to so that we extract the movie that has the highest probability
    within a cluster for a given user. Then recommend the top 12 movies from the resulting
    top movies within different clusters:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个用户，映射电影的概率和电影所属的聚类编号，以便我们可以为每个用户提取该聚类中概率最高的电影。然后从不同聚类中提取前12部电影进行推荐：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The preceding results in 13.6% of all users watching a movie that is recommended
    to them.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果显示，13.6%的用户观看了推荐给他们的电影。
- en: While the preceding results are only slightly better than the result of 12.6%
    without having any variety in recommendations, having a variety in recommendations
    is more likely to be better when we consider not just the next purchase but all
    future purchases by a user.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前述结果仅比没有任何推荐多样性的12.6%结果稍好，但考虑到不仅是下一次购买，而是所有用户未来的购买，拥有多样性的推荐更可能带来更好的效果。
- en: There's more...
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'While we have looked into generating predictions for a user and also into increasing
    the variety of predictions that are served to a user, we can further improve the
    results by considering the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经考虑了为用户生成预测，并提高推荐多样性，但我们还可以通过以下方法进一步改进结果：
- en: Incorporating the information about the movies the user did not like
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融入用户不喜欢的电影信息
- en: Incorporating the user's demographic information
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融入用户的人口统计信息
- en: Incorporating the details related to the movie, for example the release year
    and the cast
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 融入有关电影的详细信息，例如上映年份和演员阵容
- en: Forecasting the value of a stock's price
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测股票价格的价值
- en: There is a variety of technical analysis that experts perform to come up with
    buy-and-sell recommendations on stocks. The majority of the technical analysis
    relies on historical patterns with an assumption that history repeats as long
    as we normalize for certain events.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 专家们进行的技术分析有很多种，用以提出股票的买卖建议。大多数技术分析依赖于历史模式，假设历史会重复，只要我们对某些事件进行标准化处理。
- en: Given that what we have been performing so far has also been about making decisions
    by considering history, let's go ahead and apply the skills we've learned so far
    to predict the price of a stock.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们到目前为止所做的也是通过考虑历史来做决策的，让我们继续应用我们迄今为止学到的技巧来预测股票价格。
- en: 'However, be extremely careful when relying on algorithmic analysis in applications
    such as stock-price prediction to make a buy-or-sell decision. The big difference
    between the other recipes and this one is that, while the decisions made in other
    recipes are reversible (for example: you can revoke it if a generated text does
    not look appropriate) or cost money (a bad recommendation means the customer won''t
    buy the product again), the decisions made in stock-price prediction are irreversible.
    Once the money is lost, it is not coming back.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在依赖算法分析做出买卖决策时，如股票价格预测，务必小心。与其他方法的区别在于，其他方法中的决策是可逆的（例如：如果生成的文本不合适，可以撤销），或者是有成本的（糟糕的推荐意味着客户不会再次购买该产品），而股票价格预测中的决策是不可逆的。一旦资金损失，就无法追回。
- en: With this in mind, let's go ahead and apply the techniques we've learned so
    far to predict the price of a stock.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这一点，让我们继续应用我们迄今为止学到的技巧来预测股票价格。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To predict the price of a stock, let''s apply two strategies:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测股票价格，让我们应用两种策略：
- en: Predict the stock price solely based on the last five days' stock prices
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅根据过去五天的股票价格来预测股票价格
- en: Predict the stock price based on a combination of the last five days' stock
    prices and the latest news about the company of interest
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于过去五天的股票价格和公司最新新闻的结合来预测股票价格
- en: For the first analysis, we can prepare the dataset in a way that is very similar
    to the way we prepared the dataset for LSTM; the second analysis will require
    a different way of preparing the dataset, as it involves both numeric and text
    data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一次分析，我们可以准备数据集，方式与为LSTM准备数据集非常相似；第二次分析则需要不同的数据准备方式，因为它涉及到数值和文本数据。
- en: 'The way in which we will process data for the two approaches discussed above
    is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为上述讨论的两种方法处理数据的方式如下：
- en: '**Last five days'' stock prices only**:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅使用过去五天的股票价格**：'
- en: Order the dataset from the oldest to the newest date
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照从最旧到最新的日期排序数据集
- en: Take the first `5` stock prices as input and the sixth stock price as output
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以前`5`个股票价格作为输入，第六个股票价格作为输出
- en: 'Slide it across so that in the next data point, the second to sixth data points
    are the input and the seventh data point is the output, and so on until we reach
    the final data point:'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其滑动，以便在下一个数据点中，第二到第六个数据点作为输入，第七个数据点作为输出，依此类推，直到我们达到最后一个数据点：
- en: Each of the five data points are the input to the five time steps in an LSTM
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这五个数据点作为LSTM中的五个时间步骤的输入
- en: The sixth data point is the output
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第六个数据点是输出
- en: Given that we are predicting a continuous number, the loss function this time
    will be the *mean squared error* value
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们预测的是一个连续的数字，这次的损失函数将是*均方误差*值
- en: '**Last five days'' stock prices plus news headlines, data about the company**:
    For this analysis, there are two types of data preprocessing. While the data preprocessing
    for the last five days'' stock prices remains the same, the data pre-preparation
    step for the news headlines, data is the additional step that is to be performed
    in this analysis. Let''s look into how we can incorporate both of them into our
    model:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过去五天的股票价格加上新闻标题、公司数据**：对于这个分析，有两种数据预处理方式。虽然过去五天股票价格的数据预处理保持不变，但新闻标题和数据的预处理步骤是此分析中要执行的额外步骤。让我们看看如何将这两者融入到我们的模型中：'
- en: 'Given that these are two data types, let''s have two different models:'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于这些是两种不同的数据类型，让我们使用两个不同的模型：
- en: One model that takes historical five-day stock-price data.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用历史五天股票价格数据的模型。
- en: Another model that modifies the output of the last five days' stock-price model
    by either increasing or decreasing the output.
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个模型，通过增加或减少过去五天股票价格模型的输出，来修改其结果。
- en: The second model is a result of the news headlines, dataset. The hypothesis
    is that positive headlines are likely to increase the stock price value and that
    negative headlines will reduce the stock value.
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个模型来源于新闻头条数据集。假设正面新闻头条更可能提升股票价格，而负面新闻则可能降低股票价格。
- en: To keep the problem simple, assume that only the most recent headline prior
    to the day of the prediction of the stock's value will have an impact on the outcome
    of the stock value on the day of prediction
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为简化问题，假设在预测股票价值的当天，只有最新的新闻头条会影响股票的预测结果
- en: Given that we have two different models, use the functional API so that we combine
    the effects of both factors
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们有两个不同的模型，使用函数式 API 以便我们结合两者因素的影响
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We''ll break our approach of solving this into three sections (The code file
    is available as `Chapter_12_stock_price_prediction.ipynb` in GitHub):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解决此问题的方法分为三个部分（代码文件在 GitHub 中以 `Chapter_12_stock_price_prediction.ipynb`
    呈现）：
- en: Predict a stock price based on the last five days' stock prices only
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅基于过去五天的股票价格预测股票价格
- en: The pitfall of the random train-and-test split
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机训练和测试集划分的陷阱
- en: Assign a higher weight to more recent stock price values
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为较新的股票价格赋予更高的权重
- en: Combine the last five days' stock price with text data of news article headlines
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将过去五天的股票价格与新闻文章头条的文本数据结合
- en: The last five days' stock prices only
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅使用过去五天的股票价格
- en: 'In this recipe, we will predict a stock price based on its last 5 data points
    only. In the next recipe, we will predict the stock price based on news and historical
    data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们仅基于过去五个数据点来预测股票价格。在下一个菜谱中，我们将基于新闻和历史数据来预测股票价格：
- en: 'Import the relevant packages and dataset:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Prepare the dataset where the input is the last five days'' stock-price values
    and the output is the stock-price value on the sixth day:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据集，其中输入为过去五天的股票价格，输出为第六天的股票价格：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Reshape the dataset so that it is of the `batch_size`, `time_steps`, `features_per_time_step` form:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集重新整形为 `batch_size`、`time_steps`、`features_per_time_step` 形式：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create the train-and-test datasets:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集和测试集：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Build the model:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A summary of the model is as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/5800a442-ce90-4d0c-a004-b02a43688d92.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5800a442-ce90-4d0c-a004-b02a43688d92.png)'
- en: 'Compile the model so that we define the `loss` function and adjust the learning-rate
    value:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型以定义 `loss` 函数并调整学习率：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Fit the model:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型：
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding results in a mean squared error value of $641 (An average of
    ~$25 per prediction) on the test dataset. The plot of the predicted versus actual
    stock price is as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的结果表明，在测试数据集上的均方误差值为 $641（每次预测大约 ~$25）。预测股票价格与实际股票价格的对比图如下：
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The variation of the predicted and the actual price is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值与实际价格的差异如下：
- en: '![](img/a2feff03-73e4-4623-93c1-b925100125da.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2feff03-73e4-4623-93c1-b925100125da.png)'
- en: The pitfalls
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 陷阱
- en: Now that we have predictions that are fairly accurate, and in fact, good predictions,
    let's dive deep to understand the reason for such good predictions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了相当准确的预测，实际上，预测效果非常好，让我们深入了解这些优秀预测的原因。
- en: In our training dataset, we have data points from a long time ago as well as
    the data points that are very recent. This is a form of leakage, as, at the time
    of model building, we do not have future stock prices. Due to the way we construct
    data, we could have the data from December 20 in our training dataset, while December
    19 could be in the test dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练数据集中，既有很久以前的数据，也有非常近期的数据。这是一种数据泄漏，因为在构建模型时，我们无法获得未来的股票价格。由于我们的数据构建方式，我们的训练数据集可能包含来自12月20日的数据，而12月19日的数据则可能在测试数据集中。
- en: 'Let''s rebuild our model with the training-and-test datasets demarcated by
    their corresponding dates:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用训练集和测试集按照相应日期重新构建我们的模型：
- en: '[PRE41]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output of the model that we built in the *last 5 days'' stock prices only*
    section on the new test dataset is as follows (with a test dataset loss of ~57,000):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在新测试数据集上，我们在 *仅使用过去5天的股票价格* 部分构建的模型输出如下（测试数据集的损失大约为57,000）：
- en: '![](img/117796e9-8284-4993-b262-5909a8774f44.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/117796e9-8284-4993-b262-5909a8774f44.png)'
- en: Note that the resulting actual versus predicted stock-price graph now is much
    worse when compared to the previous iteration. However, the graph generated in
    this section is a more realistic scenario data than the graph obtained in *last
    5 days' stock prices only *section.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与上一次迭代相比，结果实际与预测的股价图现在更糟。然而，在此部分生成的图表比*最近5天的股票价格*部分获得的图表更具现实情况。
- en: 'Now that we have obtained the preceding graph, let''s try to understand the
    reason the graph might have looked as it did by examining the plot of the variation
    of stock-price data over time, which is as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了前面的图表，让我们试着理解图表看起来如此的原因，通过检查股价随时间变化的绘图，如下所示：
- en: '[PRE42]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'A plot of the variation of the stock-price over time is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 股价随时间变化的图表如下：
- en: '![](img/74f903fe-d2e4-415a-b085-4a4c73cc32da.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74f903fe-d2e4-415a-b085-4a4c73cc32da.png)'
- en: Note that the price of stock increased slowly at the start and accelerated in
    the middle while decelerating at the end.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，股票价格在开始时缓慢上升，并在中间加速，最后减速。
- en: 'The model did not work out well for the following reasons:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 模型由于以下原因表现不佳：
- en: Equal weight is given to errors for predictions made much earlier in the history
    as well as more recent ones
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于早期和最近的预测错误，都赋予了相等的权重。
- en: We didn't factor for the trend in deceleration
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有考虑减速趋势。
- en: Assigning different weights to different time periods
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分配不同的权重给不同的时间段。
- en: We learned that we will be assigning higher weight for the most recent time
    period and a lower weight for historical time periods.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们将为最近的时间段分配更高的权重，而为历史时间段分配较低的权重。
- en: 'We can come up with training `weights` as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以如下制定训练`weights`：
- en: '[PRE43]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The preceding code assigns a weight of `0` to the most historical data point
    and a weight of `1` to the most recent data point. All the intermediate data points
    will have a weight value between `0` and `1`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码将最历史数据点的权重分配为`0`，将最近数据点的权重分配为`1`。所有中间数据点的权重值将在`0`到`1`之间。
- en: 'Now that we have defined `weights`, let''s define our custom loss function,
    which applies the previously-initialized losses while calculating the squared
    error loss:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了`weights`，让我们定义我们的自定义损失函数，该函数在计算平方误差损失时应用先前初始化的损失：
- en: '[PRE44]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now that we have initialized `weights` and also defined the custom loss function,
    let''s supply the input layer and the weight values to the model using the functional
    API (we are using a functional API as we are passing multiple input while training
    the model):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初始化了`weights`并定义了自定义损失函数，让我们使用功能API将输入层和权重值提供给模型（我们使用功能API因为在训练模型时传递了多个输入）：
- en: '[PRE45]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now that we have defined the model, which has the same parameters as in the
    *last 5 days'' stock prices only *section, but there is an additional input, which
    is the weights tensor. Let''s compile our model:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，该模型与*最近5天的股票价格*部分中的参数相同，但还有一个额外的输入，即权重张量。让我们编译我们的模型：
- en: '[PRE46]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we have compiled our model, let''s fit it:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编译了我们的模型，让我们拟合它：
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The model returns a squared error loss of 40,000 on the test dataset, as opposed
    to the loss of 57,000 from the *The pitfalls* section. Let''s plot the values
    of predicted versus actual stock prices on the test dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在测试数据集上返回了40,000的平方误差损失，与*陷阱*部分的57,000损失相比。让我们在测试数据集上绘制预测股价与实际股价的值：
- en: '![](img/42c55eae-c592-4663-8ec3-eccb1898150c.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42c55eae-c592-4663-8ec3-eccb1898150c.png)'
- en: We now notice that there is a correlation between the predicted and the actual
    stock price in the most recent history (right-most part of the chart), while the
    spike in the middle of graph is not being accounted for in the predictions.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在注意到，在最近的历史记录中（图表的最右侧部分），预测的股价与实际股价之间存在相关性，而图表中间的尖峰未被预测所考虑。
- en: In the next recipe, let's see whether the news headlines can incorporate the
    spike in middle.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个案例中，让我们看看新闻标题是否能包含中间的尖峰。
- en: The last five days' stock prices plus news data
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近五天的股票价格加上新闻数据
- en: 'In the following code, we will incorporate text data about the headlines generated
    by the company of interest (which is fetched from the open source API provided
    by the Guardian''s website) along with the last five days'' stock price data.
    Then we''ll couple the custom loss function, which takes the recency of an event
    into account:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将包含来自感兴趣公司的标题的文本数据（从《卫报》网站提供的开放源代码API获取），以及过去五天的股价数据。然后我们将结合自定义损失函数，该函数会考虑事件的时效性：
- en: 'Import the headlines data from the Guardian website from here: [https://open-platform.theguardian.com/](https://open-platform.theguardian.com/)
    (note that you would have to apply for your own access key to be able to download
    the dataset from the website). Download the title and the corresponding date when
    the title appeared, and then preprocess the date so that it is converted into
    a date format:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里导入《卫报》网站的标题数据：[https://open-platform.theguardian.com/](https://open-platform.theguardian.com/)（请注意，您需要申请自己的访问密钥，才能从该网站下载数据集）。下载标题及其出现的对应日期，然后预处理日期，使其转换为日期格式：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Join the historical price dataset and the article title dataset by `Date`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`Date`将历史价格数据集和文章标题数据集合并：
- en: '[PRE49]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Preprocess the text data to remove the stop-words and punctuation, and then
    encode the text input just as we did in the sentiment-classification exercise
    in [Chapter 11](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml), *Building a Recurrent
    Neural Network*:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本数据进行预处理，去除停用词和标点符号，然后像我们在[第11章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)《构建循环神经网络》中做的那样，对文本输入进行编码：
- en: '[PRE50]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Take the last five days'' stock prices and the most recent title (prior to
    the date of the stock-price prediction) as input. Let''s preprocess our data to
    get the input and the output values and then prepare the training and test datasets:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以过去五天的股价和最新的标题（在股价预测日期之前的标题）为输入。让我们预处理数据，获取输入和输出值，然后准备训练集和测试集：
- en: 'In the following code,  x1 corresponds to the historical stock prices and x2
    corresponds to the article title on the date of the stock prediction:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，x1对应历史股价，x2对应股价预测日期的文章标题：
- en: '[PRE51]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Given that we are passing multiple variables as input (historical stock prices,
    encoded text data, and weight values), we will be using functional API to build
    the model:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鉴于我们将多个变量作为输入（历史股价、编码的文本数据和权重值），我们将使用函数式API来构建模型：
- en: '[PRE52]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Note that we have multiplied the output values of the stock-price model and
    the text-data model, as the text data is expected to adjust to the output of the
    historical stock-price model:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经将股价模型和文本数据模型的输出值相乘，因为文本数据需要与历史股价模型的输出进行调整：
- en: '[PRE53]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The architecture of the preceding model is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 前述模型的架构如下：
- en: '![](img/95795ad5-7d62-4cf3-a38d-2cc879bb6826.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95795ad5-7d62-4cf3-a38d-2cc879bb6826.png)'
- en: 'Define the loss function and compile the model:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数并编译模型：
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Fit the model:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Plot the actual versus predicted values of stock prices in the test dataset:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制测试集中的实际股价与预测股价的比较图：
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The variation of actual and predicted stock price is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 实际股价与预测股价的变化如下：
- en: '![](img/a804018e-6956-4350-9d12-2428053d76ac.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a804018e-6956-4350-9d12-2428053d76ac.png)'
- en: Note that, in this iteration, the middle portion has a slightly better slope
    when compared to the no-text data version and also has a slightly lower squared
    error at 35,000 when compared to the 40,000 value of the previous iteration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这一轮迭代中，与没有文本数据的版本相比，中间部分的斜率稍好一些，并且平方误差稍低，为35,000，而前一轮的平方误差为40,000。
- en: There's more...
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: As mentioned at the start of this recipe, be extremely careful when predicting
    the values of a stock price, as there are a variety of factors that can affect
    the movement of stock prices, and all of them need to be taken into consideration
    when making a prediction.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如本教程开头所提到的，预测股价时要非常小心，因为股价的波动受多种因素的影响，所有这些因素在预测时都需要考虑进去。
- en: Additionally, you should also notice that while the actual and predicted values
    seem correlated, there is a small delay in the predicted values line when compared
    to the actual values line. This delay can considerably change the optimal strategy
    from a buy decision to a sell decision. Hence, there should be a greater weight
    for the rows where the movement of a stock price is significant from the previous date—further
    complicating our loss function.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还应该注意，虽然实际值和预测值看起来相关，但预测值线比实际值线稍有延迟。这种延迟可能会导致从买入决策变为卖出决策的最佳策略发生显著变化。因此，在股价变动显著的前一日期，应给予更大的权重——这进一步复杂化了我们的损失函数。
- en: 'We could also potentially incorporate more sources of information, such as
    additional news headlines and seasonality (for example: certain stocks typically
    fare well during the holiday season) and other macroeconomic factors, when making
    the predictions.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以考虑整合更多信息源，如额外的新闻标题和季节性因素（例如：某些股票在假期季节通常表现良好）以及其他宏观经济因素，在进行预测时使用。
- en: Finally, we could have scaled the dataset so that the input to the neural network
    is not a huge number.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们本可以对数据集进行缩放，以便神经网络的输入不是一个巨大的数字。
