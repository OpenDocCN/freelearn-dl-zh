- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: DQN Extensions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQNæ‰©å±•
- en: Since DeepMind published its paper on the deep Q-network (DQN) model in 2015,
    many improvements have been proposed, along with tweaks to the basic architecture,
    which, significantly, have improved the convergence, stability, and sample efficiency
    of DeepMindâ€™s basic DQN. In this chapter, we will take a deeper look at some of
    those ideas.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªä»DeepMindåœ¨2015å¹´å‘å¸ƒå…¶æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ¨¡å‹çš„è®ºæ–‡ä»¥æ¥ï¼Œè®¸å¤šæ”¹è¿›æ–¹æ¡ˆå·²ç»è¢«æå‡ºï¼Œå¹¶å¯¹åŸºç¡€æ¶æ„è¿›è¡Œäº†è°ƒæ•´ï¼Œæ˜¾è‘—æé«˜äº†DeepMindåŸºç¡€DQNçš„æ”¶æ•›æ€§ã€ç¨³å®šæ€§å’Œæ ·æœ¬æ•ˆç‡ã€‚æœ¬ç« å°†æ·±å…¥æ¢è®¨å…¶ä¸­çš„ä¸€äº›æ€æƒ³ã€‚
- en: 'In October 2017, Hessel et al. from DeepMind published a paper called Rainbow:
    Combining improvements in deep reinforcement learning [[Hes+18](#)], which presented
    the six most important improvements to DQN; some were invented in 2015, but others
    are relatively recent. In this paper, state-of-the-art results on the Atari games
    suite were reached, just by combining those six methods.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '2017å¹´10æœˆï¼ŒDeepMindçš„Hesselç­‰äººå‘å¸ƒäº†ä¸€ç¯‡åä¸ºã€ŠRainbow: Combining improvements in deep reinforcement
    learningã€‹çš„è®ºæ–‡[[Hes+18](#)]ï¼Œä»‹ç»äº†å¯¹DQNçš„å…­ä¸ªæœ€é‡è¦çš„æ”¹è¿›ï¼›å…¶ä¸­ä¸€äº›æ˜¯åœ¨2015å¹´å‘æ˜çš„ï¼Œä½†å…¶ä»–ä¸€äº›åˆ™è¾ƒä¸ºè¿‘æœŸã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œé€šè¿‡ç®€å•åœ°ç»“åˆè¿™å…­ä¸ªæ–¹æ³•ï¼Œè¾¾åˆ°äº†Atariæ¸¸æˆå¥—ä»¶ä¸Šçš„æœ€å…ˆè¿›æˆæœã€‚'
- en: Since 2017, more papers have been published and state-of-the-art results have
    been pushed further, but all the methods presented in the paper are still relevant
    and widely used in practice. For example, in 2023, Marc Bellemare published the
    book Distributional reinforcement learning [[BDR23](#)] about one of the paperâ€™s
    methods. In addition, the improvements described are relatively simple to implement
    and understand, so I have not made any major modifications to this chapter in
    this edition.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª2017å¹´ä»¥æ¥ï¼Œæ›´å¤šçš„è®ºæ–‡è¢«å‘è¡¨ï¼Œå¹¶ä¸”æœ€å…ˆè¿›çš„ç»“æœè¢«è¿›ä¸€æ­¥æ¨åŠ¨ï¼Œä½†è®ºæ–‡ä¸­ä»‹ç»çš„æ‰€æœ‰æ–¹æ³•ä»ç„¶æ˜¯ç›¸å…³çš„ï¼Œå¹¶åœ¨å®è·µä¸­å¹¿æ³›ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨2023å¹´ï¼ŒMarc
    Bellemareå‡ºç‰ˆäº†ã€ŠDistributional reinforcement learningã€‹ä¸€ä¹¦[[BDR23](#)]ï¼Œä¹¦ä¸­è®¨è®ºäº†è®ºæ–‡ä¸­çš„ä¸€ç§æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæ‰€æè¿°çš„æ”¹è¿›ç›¸å¯¹ç®€å•æ˜“äºå®ç°å’Œç†è§£ï¼Œå› æ­¤åœ¨æœ¬ç‰ˆä¸­æˆ‘æ²¡æœ‰å¯¹è¿™ä¸€ç« åšé‡å¤§ä¿®æ”¹ã€‚
- en: 'The DQN extensions that we will become familiar with are the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç†Ÿæ‚‰çš„DQNæ‰©å±•å¦‚ä¸‹ï¼š
- en: 'N-step DQN: How to improve convergence speed and stability with a simple unrolling
    of the Bellman equation, and why itâ€™s not an ultimate solution'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Næ­¥DQNï¼šå¦‚ä½•é€šè¿‡ç®€å•åœ°å±•å¼€è´å°”æ›¼æ–¹ç¨‹æé«˜æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ï¼Œä»¥åŠä¸ºä»€ä¹ˆå®ƒä¸æ˜¯ç»ˆæè§£å†³æ–¹æ¡ˆ
- en: 'Double DQN: How to deal with DQN overestimation of the values of the actions'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŒDQNï¼šå¦‚ä½•å¤„ç†DQNå¯¹åŠ¨ä½œå€¼çš„é«˜ä¼°
- en: 'Noisy networks: How to make exploration more efficient by adding noise to the
    network weights'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£°ç½‘ç»œï¼šå¦‚ä½•é€šè¿‡ç»™ç½‘ç»œæƒé‡æ·»åŠ å™ªå£°æ¥æé«˜æ¢ç´¢æ•ˆç‡
- en: 'Prioritized replay buffer: Why uniform sampling of our experience is not the
    best way to train'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆå›æ”¾ç¼“å†²åŒºï¼šä¸ºä»€ä¹ˆå‡åŒ€é‡‡æ ·æˆ‘ä»¬çš„ç»éªŒä¸æ˜¯è®­ç»ƒçš„æœ€ä½³æ–¹å¼
- en: 'Dueling DQN: How to improve convergence speed by making our networkâ€™s architecture
    more closely represent the problem that we are solving'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æŠ—DQNï¼šå¦‚ä½•é€šè¿‡ä½¿æˆ‘ä»¬çš„ç½‘ç»œæ¶æ„æ›´ç´§å¯†åœ°åæ˜ æˆ‘ä»¬æ­£åœ¨è§£å†³çš„é—®é¢˜ï¼Œæ¥æé«˜æ”¶æ•›é€Ÿåº¦
- en: 'Categorical DQN: How to go beyond the single expected value of the action and
    work with full distributions'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»DQNï¼šå¦‚ä½•è¶…è¶Šå•ä¸€çš„æœŸæœ›åŠ¨ä½œå€¼ï¼Œå¤„ç†å®Œæ•´çš„åˆ†å¸ƒ
- en: This chapter will go through all these methods. We will analyze the ideas behind
    them, alongside how they can be implemented and compared to the classic DQN performance.
    Finally, we will analyze how the combined system with all the methods performs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†ä»‹ç»æ‰€æœ‰è¿™äº›æ–¹æ³•ã€‚æˆ‘ä»¬å°†åˆ†æè¿™äº›æ–¹æ³•èƒŒåçš„æ€æƒ³ï¼Œä»¥åŠå¦‚ä½•å®ç°å®ƒä»¬ï¼Œå¹¶ä¸ç»å…¸çš„DQNæ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†åˆ†æç»“åˆæ‰€æœ‰æ–¹æ³•çš„ç³»ç»Ÿè¡¨ç°ã€‚
- en: Basic DQN
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºç¡€DQN
- en: To get started, we will implement the same DQN method as in ChapterÂ [6](#),
    but leveraging the high-level primitives described in ChapterÂ [7](ch011.xhtml#x1-1070007).
    This will make our code much more compact, which is good, as non-relevant details
    wonâ€™t distract us from the methodâ€™s logic. At the same time, the purpose of this
    book is not to teach you how to use the existing libraries but rather how to develop
    intuition about RL methods and, if necessary, implement everything from scratch.
    From my perspective, this is a much more valuable skill, as libraries come and
    go, but true understanding of the domain will allow you to quickly make sense
    of other peopleâ€™s code and apply it consciously.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹ï¼Œæˆ‘ä»¬å°†å®ç°ä¸ç¬¬[6](#)ç« ç›¸åŒçš„DQNæ–¹æ³•ï¼Œä½†åˆ©ç”¨ç¬¬[7](ch011.xhtml#x1-1070007)ç« ä¸­æè¿°çš„é«˜çº§åŸè¯­ã€‚è¿™å°†ä½¿æˆ‘ä»¬çš„ä»£ç æ›´åŠ ç®€æ´ï¼Œè¿™æ˜¯å¥½çš„ï¼Œå› ä¸ºæ— å…³çš„ç»†èŠ‚ä¸ä¼šä½¿æˆ‘ä»¬åç¦»æ–¹æ³•çš„é€»è¾‘ã€‚åŒæ—¶ï¼Œæœ¬ä¹¦çš„ç›®çš„å¹¶éæ•™ä½ å¦‚ä½•ä½¿ç”¨ç°æœ‰çš„åº“ï¼Œè€Œæ˜¯å¦‚ä½•åŸ¹å…»å¯¹å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ç›´è§‰ï¼Œå¿…è¦æ—¶ï¼Œä»é›¶å¼€å§‹å®ç°ä¸€åˆ‡ã€‚ä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´æœ‰ä»·å€¼çš„æŠ€èƒ½ï¼Œå› ä¸ºåº“ä¼šä¸æ–­å˜åŒ–ï¼Œä½†å¯¹é¢†åŸŸçš„çœŸæ­£ç†è§£å°†ä½¿ä½ èƒ½å¤Ÿè¿…é€Ÿç†è§£ä»–äººçš„ä»£ç ï¼Œå¹¶æœ‰æ„è¯†åœ°åº”ç”¨å®ƒã€‚
- en: 'In the basic DQN implementation, we have three modules in the Chapter08 folder
    of the GitHub repository for this book:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºæœ¬çš„ DQN å®ç°ä¸­ï¼Œæˆ‘ä»¬åœ¨æœ¬ä¹¦çš„ GitHub ä»“åº“ä¸­çš„ Chapter08 æ–‡ä»¶å¤¹ä¸­æœ‰ä¸‰ä¸ªæ¨¡å—ï¼š
- en: 'Chapter08/lib/dqn_model.py: The DQN neural network (NN), which is the same
    as in ChapterÂ [6](#), so I wonâ€™t repeat it'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/lib/dqn_model.py: DQN ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ï¼Œä¸ç¬¬[6](#)ç« ç›¸åŒï¼Œå› æ­¤æˆ‘ä¸ä¼šé‡å¤å®ƒã€‚'
- en: 'Chapter08/lib/common.py: Common functions and declarations shared by the code
    in this chapter'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/lib/common.py: æœ¬ç« ä»£ç å…±äº«çš„å¸¸ç”¨å‡½æ•°å’Œå£°æ˜ã€‚'
- en: 'Chapter08/01_dqn_basic.py: 77 lines of code leveraging the PTAN and Ignite
    libraries, implementing the basic DQN method'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chapter08/01_dqn_basic.py: 77 è¡Œä»£ç ï¼Œåˆ©ç”¨ PTAN å’Œ Ignite åº“å®ç°åŸºæœ¬çš„ DQN æ–¹æ³•ã€‚'
- en: Common library
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…¬å…±åº“
- en: 'Letâ€™s start with the contents of lib/common.py. First of all, we have hyperparameters
    for our Pong environment from the previous chapter. The hyperparameters are stored
    in the dataclass object, which is a standard way to store a bunch of data fields
    with their type annotations. This makes it easy to add another configuration set
    for different, more complicated Atari games and allows us to experiment with hyperparameters:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä» lib/common.py çš„å†…å®¹å¼€å§‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ä¸Šä¸€ç« ä¸­ä¸º Pong ç¯å¢ƒè®¾ç½®çš„è¶…å‚æ•°ã€‚è¿™äº›è¶…å‚æ•°å­˜å‚¨åœ¨ä¸€ä¸ªæ•°æ®ç±»å¯¹è±¡ä¸­ï¼Œè¿™æ˜¯å­˜å‚¨ä¸€ç»„æ•°æ®å­—æ®µåŠå…¶ç±»å‹æ³¨é‡Šçš„æ ‡å‡†æ–¹å¼ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾ä¸ºä¸åŒã€æ›´å¤æ‚çš„
    Atari æ¸¸æˆæ·»åŠ å¦ä¸€ä¸ªé…ç½®é›†ï¼Œå¹¶å…è®¸æˆ‘ä»¬å¯¹è¶…å‚æ•°è¿›è¡Œå®éªŒï¼š
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next function from lib/common.py has the name unpack_batch, and it takes
    the batch, of transitions and converts it into the set of NumPy arrays suitable
    for training. Every transition from ExperienceSourceFirstLast has a type of ExperienceFirstLast,
    which is a dataclass with the following fields:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: lib/common.py ä¸­çš„ä¸‹ä¸€ä¸ªå‡½æ•°åä¸º unpack_batchï¼Œå®ƒæ¥æ”¶è½¬ç§»çš„æ‰¹æ¬¡å¹¶å°†å…¶è½¬æ¢ä¸ºé€‚åˆè®­ç»ƒçš„ NumPy æ•°ç»„é›†åˆã€‚æ¥è‡ª ExperienceSourceFirstLast
    çš„æ¯ä¸ªè½¬ç§»éƒ½å±äº ExperienceFirstLast ç±»å‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ•°æ®ç±»ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- en: 'state: Observation from the environment.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'state: æ¥è‡ªç¯å¢ƒçš„è§‚æµ‹å€¼ã€‚'
- en: 'action: Integer action taken by the agent.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'action: ä»£ç†æ‰§è¡Œçš„æ•´æ•°åŠ¨ä½œã€‚'
- en: 'reward: If we have created ExperienceSourceFirstLast with the attribute steps_count=1,
    itâ€™s just the immediate reward. For larger step counts, it contains the discounted
    sum of rewards for this number of steps.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: å¦‚æœæˆ‘ä»¬åˆ›å»ºäº† ExperienceSourceFirstLast å¹¶è®¾ç½®äº†å±æ€§ steps_count=1ï¼Œé‚£ä¹ˆå®ƒåªæ˜¯å³æ—¶å¥–åŠ±ã€‚å¯¹äºæ›´å¤§çš„æ­¥æ•°è®¡æ•°ï¼Œå®ƒåŒ…å«äº†è¿™ä¸ªæ­¥æ•°å†…çš„å¥–åŠ±çš„æŠ˜æ‰£æ€»å’Œã€‚'
- en: 'last_state: If the transition corresponds to the final step in the environment,
    then this field is None; otherwise, it contains the last observation in the experience
    chain.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'last_state: å¦‚æœè½¬ç§»å¯¹åº”äºç¯å¢ƒä¸­çš„æœ€åä¸€æ­¥ï¼Œé‚£ä¹ˆè¿™ä¸ªå­—æ®µä¸º Noneï¼›å¦åˆ™ï¼Œå®ƒåŒ…å«ç»éªŒé“¾ä¸­çš„æœ€åä¸€ä¸ªè§‚æµ‹å€¼ã€‚'
- en: 'The code of unpack_batch is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: unpack_batch çš„ä»£ç å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note how we handle the final transitions in the batch. To avoid the special
    handling of such cases, for terminal transitions, we store the initial state in
    the last_states array. To make our calculations of the Bellman update correct,
    we have to mask such batch entries during the loss calculation using the dones
    array. Another solution would be to calculate the value of the last states only
    for non-terminal transitions, but it would make our loss function logic a bit
    more complicated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„æˆ‘ä»¬å¦‚ä½•å¤„ç†æ‰¹æ¬¡ä¸­çš„æœ€ç»ˆè½¬ç§»ã€‚ä¸ºäº†é¿å…å¯¹è¿™ç§æƒ…å†µçš„ç‰¹æ®Šå¤„ç†ï¼Œå¯¹äºç»ˆæ­¢è½¬ç§»ï¼Œæˆ‘ä»¬å°†åˆå§‹çŠ¶æ€å­˜å‚¨åœ¨ last_states æ•°ç»„ä¸­ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„ Bellman
    æ›´æ–°è®¡ç®—æ­£ç¡®ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æŸå¤±è®¡ç®—æ—¶ä½¿ç”¨ dones æ•°ç»„å¯¹è¿™äº›æ‰¹æ¬¡æ¡ç›®è¿›è¡Œæ©ç ã€‚å¦ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä»…å¯¹éç»ˆæ­¢è½¬ç§»è®¡ç®—æœ€åçŠ¶æ€çš„å€¼ï¼Œä½†è¿™ä¼šä½¿æˆ‘ä»¬çš„æŸå¤±å‡½æ•°é€»è¾‘ç¨å¾®å¤æ‚ä¸€äº›ã€‚
- en: 'Calculation of the DQN loss function is provided by the calc_loss_dqn function,
    and the code is almost the same as in ChapterÂ [6](#). One small addition is torch.no_grad(),
    which stops the PyTorch calculation graph from being recorded for the target net:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DQN æŸå¤±å‡½æ•°çš„è®¡ç®—ç”± calc_loss_dqn å‡½æ•°æä¾›ï¼Œä»£ç å‡ ä¹ä¸ç¬¬[6](#)ç« ç›¸åŒã€‚å”¯ä¸€çš„å°æ”¹åŠ¨æ˜¯ torch.no_grad()ï¼Œå®ƒé˜»æ­¢äº†
    PyTorch è®¡ç®—å›¾è¢«è®°å½•åˆ°ç›®æ ‡ç½‘ç»œä¸­ï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Besides those core DQN functions, common.py provides several utilities related
    to our training loop, data generation, and TensorBoard tracking. The first such
    utility is a small class that implements epsilon decay during the training. Epsilon
    defines the probability of taking the random action by the agent. It should be
    decayed from 1.0 in the beginning (fully random agent) to some small number, like
    0.02 or 0.01\. The code is trivial but is needed in almost any DQN, so it is provided
    by the following little class:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ ¸å¿ƒçš„ DQN å‡½æ•°å¤–ï¼Œcommon.py è¿˜æä¾›äº†ä¸è®­ç»ƒå¾ªç¯ã€æ•°æ®ç”Ÿæˆå’Œ TensorBoard è·Ÿè¸ªç›¸å…³çš„å¤šä¸ªå®ç”¨å·¥å…·ã€‚ç¬¬ä¸€ä¸ªè¿™æ ·çš„å·¥å…·æ˜¯ä¸€ä¸ªå°ç±»ï¼Œå®ƒåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®ç°äº†
    epsilon è¡°å‡ã€‚Epsilon å®šä¹‰äº†ä»£ç†æ‰§è¡ŒéšæœºåŠ¨ä½œçš„æ¦‚ç‡ã€‚å®ƒåº”ä» 1.0 å¼€å§‹ï¼ˆå®Œå…¨éšæœºçš„ä»£ç†ï¼‰ï¼Œé€æ¸è¡°å‡åˆ°æŸä¸ªå°å€¼ï¼Œæ¯”å¦‚ 0.02 æˆ– 0.01ã€‚è¿™ä¸ªä»£ç éå¸¸ç®€å•ï¼Œä½†å‡ ä¹åœ¨ä»»ä½•
    DQN ä¸­éƒ½éœ€è¦ï¼Œå› æ­¤é€šè¿‡ä»¥ä¸‹å°ç±»æä¾›ï¼š
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Another small function is batch_generator, which takes ExperienceReplayBuffer
    (the PTAN class described in ChapterÂ [7](ch011.xhtml#x1-1070007)) and infinitely
    generates training batches sampled from the buffer. In the beginning, the function
    ensures that the buffer contains the required amount of samples:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå°å‡½æ•°æ˜¯ batch_generatorï¼Œå®ƒæ¥æ”¶ ExperienceReplayBufferï¼ˆPTAN ç±»ï¼Œåœ¨ç¬¬[7](ch011.xhtml#x1-1070007)ç« ä¸­æè¿°ï¼‰å¹¶æ— é™æ¬¡ç”Ÿæˆä»ç¼“å†²åŒºä¸­é‡‡æ ·çš„è®­ç»ƒæ‰¹æ¬¡ã€‚å¼€å§‹æ—¶ï¼Œå‡½æ•°ç¡®ä¿ç¼“å†²åŒºåŒ…å«æ‰€éœ€æ•°é‡çš„æ ·æœ¬ï¼š
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, a lengthy, but nevertheless very useful, function called setup_ignite
    attaches the needed Ignite handlers, showing the training progress and writing
    metrics to TensorBoard. Letâ€™s look at this function piece by piece:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸€ä¸ªå†—é•¿ä½†éå¸¸æœ‰ç”¨çš„å‡½æ•°å«åš setup_igniteï¼Œå®ƒé™„åŠ äº†æ‰€éœ€çš„ Ignite å¤„ç†å™¨ï¼Œæ˜¾ç¤ºè®­ç»ƒè¿›åº¦å¹¶å°†åº¦é‡å†™å…¥ TensorBoardã€‚è®©æˆ‘ä»¬ä¸€å—å„¿çœ‹è¿™ä¸ªå‡½æ•°ï¼š
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initially, setup_ignite attaches two Ignite handlers provided by PTAN:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œsetup_ignite é™„åŠ äº† PTAN æä¾›çš„ä¸¤ä¸ª Ignite å¤„ç†å™¨ï¼š
- en: EndOfEpisodeHandler, which emits the Ignite event every time a game episode
    ends. It can also fire an event when the averaged reward for episodes crosses
    some boundary. We use this to detect when the game is finally solved.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EndOfEpisodeHandlerï¼Œæ¯å½“æ¸¸æˆå›åˆç»“æŸæ—¶ï¼Œå®ƒä¼šè§¦å‘ Ignite äº‹ä»¶ã€‚å½“å›åˆçš„å¹³å‡å¥–åŠ±è¶…è¿‡æŸä¸ªè¾¹ç•Œæ—¶ï¼Œå®ƒè¿˜å¯ä»¥è§¦å‘äº‹ä»¶ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥æ£€æµ‹æ¸¸æˆä½•æ—¶æœ€ç»ˆè§£å†³ã€‚
- en: EpisodeFPSHandler, a small class that tracks the time the episode has taken
    and the amount of interactions that we have had with the environment. From this,
    we calculate frames per second (FPS), which is an important performance metric
    to track.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EpisodeFPSHandlerï¼Œè¿™æ˜¯ä¸€ä¸ªå°ç±»ï¼Œè·Ÿè¸ªæ¯ä¸ªå›åˆæ‰€èŠ±è´¹çš„æ—¶é—´ä»¥åŠæˆ‘ä»¬ä¸ç¯å¢ƒäº¤äº’çš„æ¬¡æ•°ã€‚æ ¹æ®è¿™äº›ä¿¡æ¯ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ç§’å¸§æ•°ï¼ˆFPSï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªé‡è¦çš„æ€§èƒ½åº¦é‡æŒ‡æ ‡ã€‚
- en: 'Then, we install two event handlers:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å®‰è£…ä¸¤ä¸ªäº‹ä»¶å¤„ç†å™¨ï¼š
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One of the event handlers is called at the end of an episode. It will show information
    about the completed episode on the console. Another function will be called when
    the average reward grows above the boundary defined in the hyperparameters (18.0
    in the case of Pong). This function shows a message about the solved game and
    stops the training.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªäº‹ä»¶å¤„ç†å™¨ä¼šåœ¨å›åˆç»“æŸæ—¶è¢«è°ƒç”¨ã€‚å®ƒå°†åœ¨æ§åˆ¶å°ä¸Šæ˜¾ç¤ºæœ‰å…³å·²å®Œæˆå›åˆçš„ä¿¡æ¯ã€‚å¦ä¸€ä¸ªå‡½æ•°ä¼šåœ¨å¹³å‡å¥–åŠ±è¶…è¿‡è¶…å‚æ•°ä¸­å®šä¹‰çš„è¾¹ç•Œæ—¶è¢«è°ƒç”¨ï¼ˆåœ¨ Pong çš„æƒ…å†µä¸‹æ˜¯
    18.0ï¼‰ã€‚æ­¤å‡½æ•°æ˜¾ç¤ºå…³äºå·²è§£å†³æ¸¸æˆçš„æ¶ˆæ¯ï¼Œå¹¶åœæ­¢è®­ç»ƒã€‚
- en: 'The rest of the function is related to the TensorBoard data that we want to
    track. First, we create a TensorboardLogger:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†ä¸æˆ‘ä»¬æƒ³è¦è·Ÿè¸ªçš„ TensorBoard æ•°æ®æœ‰å…³ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª TensorboardLoggerï¼š
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a special class provided by Ignite to write into TensorBoard. Our processing
    function will return the loss value, so we attach the RunningAverage transformation
    (also provided by Ignite) to get a smoothed version of the loss over time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ Ignite æä¾›çš„ä¸€ä¸ªç‰¹æ®Šç±»ï¼Œç”¨äºå†™å…¥ TensorBoardã€‚æˆ‘ä»¬çš„å¤„ç†å‡½æ•°å°†è¿”å›æŸå¤±å€¼ï¼Œå› æ­¤æˆ‘ä»¬é™„åŠ äº† RunningAverage è½¬æ¢ï¼ˆåŒæ ·ç”±
    Ignite æä¾›ï¼‰ï¼Œä»¥è·å–éšæ—¶é—´å¹³æ»‘çš„æŸå¤±ç‰ˆæœ¬ã€‚
- en: 'Next, we attach the metrics we want to track to the Ignite events:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¦è·Ÿè¸ªçš„åº¦é‡å€¼é™„åŠ åˆ° Ignite äº‹ä»¶ï¼š
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'TensorboardLogger can track two groups of values from Ignite: outputs (values
    returned by the transformation function) and metrics (calculated during the training
    and kept in the engine state). EndOfEpisodeHandler and EpisodeFPSHandler provide
    metrics, which are updated at the end of every game episode. So, we attach OutputHandler,
    which will write into TensorBoard information about the episode every time it
    is completed.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TensorboardLogger å¯ä»¥è·Ÿè¸ªæ¥è‡ª Ignite çš„ä¸¤ç»„å€¼ï¼šè¾“å‡ºï¼ˆç”±è½¬æ¢å‡½æ•°è¿”å›çš„å€¼ï¼‰å’Œåº¦é‡ï¼ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å¹¶ä¿å­˜åœ¨å¼•æ“çŠ¶æ€ä¸­ï¼‰ã€‚EndOfEpisodeHandler
    å’Œ EpisodeFPSHandler æä¾›åº¦é‡ï¼Œè¿™äº›åº¦é‡åœ¨æ¯ä¸ªæ¸¸æˆå›åˆç»“æŸæ—¶æ›´æ–°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é™„åŠ äº† OutputHandlerï¼Œæ¯å½“å›åˆå®Œæˆæ—¶ï¼Œå®ƒå°†æŠŠæœ‰å…³è¯¥å›åˆçš„ä¿¡æ¯å†™å…¥
    TensorBoardã€‚
- en: 'Next, we track another group of values, metrics from the training process:
    loss, FPS, and, possibly, some custom metrics relevant to the specific extensionâ€™s
    logic:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è·Ÿè¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¦ä¸€ç»„å€¼ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­çš„åº¦é‡å€¼ï¼šæŸå¤±ã€FPSï¼Œä»¥åŠå¯èƒ½ä¸ç‰¹å®šæ‰©å±•é€»è¾‘ç›¸å…³çš„è‡ªå®šä¹‰åº¦é‡ï¼š
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Those values are updated every training iteration, but we are going to do millions
    of iterations, so we will store values in TensorBoard every 100 training iterations;
    otherwise, the data files will be huge. All this functionality might look too
    complicated, but it provides us with the unified set of metrics gathered from
    the training process. In fact, Ignite is not very tricky, given the flexibility
    it provides. Thatâ€™s it for common.py.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å€¼ä¼šåœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶æ›´æ–°ï¼Œä½†æˆ‘ä»¬å°†è¿›è¡Œæ•°ç™¾ä¸‡æ¬¡è¿­ä»£ï¼Œå› æ­¤æˆ‘ä»¬æ¯è¿›è¡Œ 100 æ¬¡è®­ç»ƒè¿­ä»£å°±å°†å€¼å­˜å‚¨åˆ° TensorBoardï¼›å¦åˆ™ï¼Œæ•°æ®æ–‡ä»¶ä¼šéå¸¸å¤§ã€‚æ‰€æœ‰è¿™äº›åŠŸèƒ½çœ‹èµ·æ¥å¯èƒ½å¾ˆå¤æ‚ï¼Œä½†å®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä»è®­ç»ƒè¿‡ç¨‹ä¸­æ”¶é›†çš„ç»Ÿä¸€åº¦é‡é›†ã€‚äº‹å®ä¸Šï¼ŒIgnite
    å¹¶ä¸å¤æ‚ï¼Œè€ƒè™‘åˆ°å®ƒæ‰€æä¾›çš„çµæ´»æ€§ã€‚common.py å°±åˆ°è¿™é‡Œã€‚
- en: Implementation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'Now, letâ€™s take a look at 01_dqn_basic.py, which creates the needed classes
    and starts the training. Iâ€™m going to omit non-relevant code and focus only on
    important pieces (the full version is available in the GitHub repo). First, we
    create the environment:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ 01_dqn_basic.pyï¼Œå®ƒåˆ›å»ºäº†æ‰€éœ€çš„ç±»å¹¶å¼€å§‹è®­ç»ƒã€‚æˆ‘å°†çœç•¥ä¸ç›¸å…³çš„ä»£ç ï¼Œåªå…³æ³¨é‡è¦éƒ¨åˆ†ï¼ˆå®Œæ•´ç‰ˆæœ¬å¯ä»¥åœ¨ GitHub ä»“åº“ä¸­æ‰¾åˆ°ï¼‰ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºç¯å¢ƒï¼š
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we apply a set of standard wrappers. We discussed them in ChapterÂ [6](#)
    and will also touch upon them in the next chapter, when we optimize the performance
    of the Pong solver. Then, we create the DQN model and the target network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åº”ç”¨ä¸€ç»„æ ‡å‡†åŒ…è£…å™¨ã€‚æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« ä¸­è®¨è®ºäº†è¿™äº›åŒ…è£…å™¨ï¼Œå¹¶ä¸”åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œå½“æˆ‘ä»¬ä¼˜åŒ– Pong æ±‚è§£å™¨çš„æ€§èƒ½æ—¶ï¼Œè¿˜ä¼šå†æ¬¡æ¶‰åŠåˆ°å®ƒä»¬ã€‚ç„¶åï¼Œæˆ‘ä»¬åˆ›å»º
    DQN æ¨¡å‹å’Œç›®æ ‡ç½‘ç»œã€‚
- en: 'Next, we create the agent, passing it an epsilon-greedy action selector:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä»£ç†ï¼Œå¹¶ä¼ å…¥ä¸€ä¸ª epsilon-greedy åŠ¨ä½œé€‰æ‹©å™¨ï¼š
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: During the training, epsilon will be decreased by the EpsilonTracker class that
    we have already discussed. This will decrease the amount of randomly selected
    actions and give more control to our NN.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œepsilon å°†ç”±æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ EpsilonTracker ç±»è¿›è¡Œå‡å°‘ã€‚è¿™å°†å‡å°‘éšæœºé€‰æ‹©çš„åŠ¨ä½œæ•°é‡ï¼Œå¹¶ç»™äºˆæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ›´å¤šçš„æ§åˆ¶æƒã€‚
- en: 'The next two very important objects are ExperienceSourceFirstLast and ExperienceReplayBuffer:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œä¸¤ä¸ªéå¸¸é‡è¦çš„å¯¹è±¡æ˜¯ ExperienceSourceFirstLast å’Œ ExperienceReplayBufferï¼š
- en: '[PRE12]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ExperienceSourceFirstLast takes the agent and environment and provides transitions
    over game episodes. Those transitions will be kept in the experience replay buffer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ExperienceSourceFirstLast æ¥æ”¶ä»£ç†å’Œç¯å¢ƒï¼Œå¹¶åœ¨æ¸¸æˆå›åˆä¸­æä¾›è¿‡æ¸¡ã€‚è¿™äº›è¿‡æ¸¡å°†è¢«ä¿å­˜åœ¨ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­ã€‚
- en: 'Then we create an optimizer and define the processing function:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬åˆ›å»ºä¼˜åŒ–å™¨å¹¶å®šä¹‰å¤„ç†å‡½æ•°ï¼š
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The processing function will be called for every batch of transitions to train
    the model. To do this, we call the common.calc_loss_dqn function and then backpropagate
    on the result. This function also asks EpsilonTracker to decrease the epsilon
    and does periodical target network synchronization.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†å‡½æ•°å°†åœ¨æ¯æ‰¹è¿‡æ¸¡æ—¶è¢«è°ƒç”¨ä»¥è®­ç»ƒæ¨¡å‹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è°ƒç”¨ common.calc_loss_dqn å‡½æ•°ï¼Œç„¶åå¯¹ç»“æœè¿›è¡Œåå‘ä¼ æ’­ã€‚è¯¥å‡½æ•°è¿˜ä¼šè¦æ±‚ EpsilonTracker
    å‡å°‘ epsilonï¼Œå¹¶è¿›è¡Œå®šæœŸçš„ç›®æ ‡ç½‘ç»œåŒæ­¥ã€‚
- en: 'And, finally, we create the Ignite Engine object:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åˆ›å»º Ignite Engine å¯¹è±¡ï¼š
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We configure it using a function from common.py, and run our training process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª common.py çš„å‡½æ•°è¿›è¡Œé…ç½®ï¼Œå¹¶è¿è¡Œè®­ç»ƒè¿‡ç¨‹ã€‚
- en: Hyperparameter tuning
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: To make our comparison of DQN extensions fair, we also need to tune hyperparameters.
    This is essential because even for the same game (Pong), using the fixed set of
    training parameters might give less optimal results when we change the details
    of the method.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æˆ‘ä»¬å¯¹ DQN æ‰©å±•çš„æ¯”è¾ƒæ›´åŠ å…¬å¹³ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è°ƒä¼˜è¶…å‚æ•°ã€‚è¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼Œå› ä¸ºå³ä½¿å¯¹äºç›¸åŒçš„æ¸¸æˆï¼ˆPongï¼‰ï¼Œä½¿ç”¨å›ºå®šçš„è®­ç»ƒå‚æ•°é›†å¯èƒ½åœ¨æˆ‘ä»¬æ”¹å˜æ–¹æ³•ç»†èŠ‚æ—¶ç»™å‡ºè¾ƒå·®çš„ç»“æœã€‚
- en: 'In principle, every explicit or implicit constant in our code could be tuned,
    such as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬ä»£ç ä¸­çš„æ¯ä¸ªæ˜¾å¼æˆ–éšå¼å¸¸é‡éƒ½å¯ä»¥è¿›è¡Œè°ƒä¼˜ï¼Œä¾‹å¦‚ï¼š
- en: 'Network configuration: Amount and size of layers, activation function, dropout,
    etc.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç½‘ç»œé…ç½®ï¼šå±‚çš„æ•°é‡å’Œå¤§å°ï¼Œæ¿€æ´»å‡½æ•°ï¼Œdropout ç­‰
- en: 'Optimization parameters: Method (vanilla SGD, Adam, AdaGrad, etc.), learning
    rate, and other optimizer parameters'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å‚æ•°ï¼šæ–¹æ³•ï¼ˆåŸç”Ÿ SGDã€Adamã€AdaGrad ç­‰ï¼‰ã€å­¦ä¹ ç‡å’Œå…¶ä»–ä¼˜åŒ–å™¨å‚æ•°
- en: 'Exploration parameters: Decay rate of ğœ–, final ğœ– value'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢å‚æ•°ï¼šğœ– çš„è¡°å‡ç‡ï¼Œæœ€ç»ˆ ğœ– å€¼
- en: Discount factor Î³ in Bellman equation
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellman æ–¹ç¨‹ä¸­çš„æŠ˜æ‰£å› å­ Î³
- en: But every new parameter we tune has a multiplicative effect on the amount of
    trial training we need to perform, so having too many hyperparameters might require
    hundreds or even thousands of trainings. Large companies like Google and Meta
    have access to a much larger amount of GPUs than individual researchers like us,
    so we need to keep the balance there.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæˆ‘ä»¬è°ƒæ•´çš„æ¯ä¸ªæ–°å‚æ•°éƒ½ä¼šå¯¹æ‰€éœ€çš„è¯•éªŒè®­ç»ƒé‡äº§ç”Ÿä¹˜æ³•æ•ˆåº”ï¼Œå› æ­¤è°ƒèŠ‚è¿‡å¤šçš„è¶…å‚æ•°å¯èƒ½éœ€è¦è¿›è¡Œæ•°ç™¾æ¬¡ç”šè‡³ä¸Šåƒæ¬¡è®­ç»ƒã€‚åƒ Google å’Œ Meta è¿™æ ·çš„å¤§å…¬å¸æ‹¥æœ‰æ¯”æˆ‘ä»¬è¿™äº›ä¸ªäººç ”ç©¶è€…æ›´å¤šçš„
    GPU èµ„æºï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œä¿æŒå¹³è¡¡ã€‚
- en: 'In my case, Iâ€™m going to demonstrate how hyperparameter tuning is done in general,
    but weâ€™ll do the search only on a few values:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†æ¼”ç¤ºå¦‚ä½•è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼Œä½†æˆ‘ä»¬åªä¼šåœ¨å°‘æ•°å‡ ä¸ªå€¼ä¸Šè¿›è¡Œæœç´¢ï¼š
- en: Learning rate
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡
- en: Discount factor Î³
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ˜æ‰£å› å­ Î³
- en: Parameters specific to the DQN extension weâ€™re considering
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨è€ƒè™‘çš„ DQN æ‰©å±•ç‰¹å®šçš„å‚æ•°
- en: 'There are several libraries that might be helpful with hyperparameter tuning.
    Here, Iâ€™m using Ray Tune ([https://docs.ray.io/en/latest/tune/index.xhtml](https://docs.ray.io/en/latest/tune/index.xhtml)),
    which is a part of the Ray project â€” a distributed computing framework for ML
    and DL. At a high level, you need to define:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä¸ªåº“å¯èƒ½å¯¹è¶…å‚æ•°è°ƒæ•´æœ‰æ‰€å¸®åŠ©ã€‚è¿™é‡Œï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯Ray Tuneï¼ˆ[https://docs.ray.io/en/latest/tune/index.xhtml](https://docs.ray.io/en/latest/tune/index.xhtml)ï¼‰ï¼Œå®ƒæ˜¯Rayé¡¹ç›®çš„ä¸€éƒ¨åˆ†â€”â€”ä¸€ä¸ªç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ã€‚ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œä½ éœ€è¦å®šä¹‰ï¼š
- en: The hyperparameter space you want to explore (boundaries for values to sample
    from or an explicit list of values to try)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¸Œæœ›æ¢ç´¢çš„è¶…å‚æ•°ç©ºé—´ï¼ˆå€¼çš„è¾¹ç•Œæˆ–æ˜¾å¼åˆ—å‡ºçš„å°è¯•å€¼åˆ—è¡¨ï¼‰
- en: The function that performs the training with specific values of hyperparameters
    and returns the metric you want to optimize with the tuning
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°æ‰§è¡Œä½¿ç”¨ç‰¹å®šè¶…å‚æ•°å€¼çš„è®­ç»ƒï¼Œå¹¶è¿”å›ä½ æƒ³è¦ä¼˜åŒ–çš„åº¦é‡ã€‚
- en: 'This might look very similar to ML problems, and in fact it is â€” this is also
    an optimization problem. But there are substantial differences: the function weâ€™re
    optimizing is not differentiable (so you cannot perform the gradient descent to
    push your hyperparameters towards the desired direction of the metric) and the
    optimization space might be discrete (you cannot train the network with the number
    of layers equal to 2.435, for example, since we cannot take the derivative of
    a non-smooth function).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½çœ‹èµ·æ¥ä¸æœºå™¨å­¦ä¹ é—®é¢˜éå¸¸ç›¸ä¼¼ï¼Œäº‹å®ä¸Šå®ƒç¡®å®æ˜¯â€”â€”è¿™ä¹Ÿæ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ã€‚ä½†å®ƒæœ‰ä¸€äº›æ˜¾è‘—çš„ä¸åŒï¼šæˆ‘ä»¬æ­£åœ¨ä¼˜åŒ–çš„å‡½æ•°æ˜¯ä¸å¯å¾®åˆ†çš„ï¼ˆå› æ­¤æ— æ³•æ‰§è¡Œæ¢¯åº¦ä¸‹é™æ¥æ¨åŠ¨è¶…å‚æ•°æœå‘æœŸæœ›çš„åº¦é‡æ–¹å‘ï¼‰ï¼Œè€Œä¸”ä¼˜åŒ–ç©ºé—´å¯èƒ½æ˜¯ç¦»æ•£çš„ï¼ˆä¾‹å¦‚ï¼Œä½ æ— æ³•ç”¨2.435å±‚çš„ç¥ç»ç½‘ç»œè¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•å¯¹ä¸€ä¸ªä¸å¹³æ»‘çš„å‡½æ•°æ±‚å¯¼ï¼‰ã€‚
- en: In later chapters, weâ€™ll touch on this problem slightly in the context of black-box
    optimization methods (ChapterÂ [17](ch021.xhtml#x1-31100017)) and RL in discrete
    optimizations (ChapterÂ [21](ch025.xhtml#x1-39100021)), but for now, weâ€™ll use
    the simplest approach â€” a random search of hyperparameters. In this case, the
    ray.tune library randomly samples concrete parameters several times and calls
    the function to obtain the metric. The smallest (or highest) metric corresponds
    to the best hyperparameter combination found in this run.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åç»­ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¼šç¨å¾®è§¦åŠä¸€ä¸‹è¿™ä¸ªé—®é¢˜ï¼Œè®¨è®ºé»‘ç®±ä¼˜åŒ–æ–¹æ³•ï¼ˆç¬¬[17ç« ](ch021.xhtml#x1-31100017)ï¼‰å’Œç¦»æ•£ä¼˜åŒ–ä¸­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆç¬¬[21ç« ](ch025.xhtml#x1-39100021)ï¼‰ï¼Œä½†ç°åœ¨æˆ‘ä»¬å°†ä½¿ç”¨æœ€ç®€å•çš„æ–¹æ³•â€”â€”è¶…å‚æ•°çš„éšæœºæœç´¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`ray.tune`åº“ä¼šéšæœºå¤šæ¬¡é‡‡æ ·å…·ä½“çš„å‚æ•°ï¼Œå¹¶è°ƒç”¨å‡½æ•°ä»¥è·å¾—åº¦é‡ã€‚æœ€å°ï¼ˆæˆ–æœ€å¤§ï¼‰çš„åº¦é‡å€¼å¯¹åº”äºåœ¨æ­¤æ¬¡è¿è¡Œä¸­æ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°ç»„åˆã€‚
- en: In this chapter, our metric (optimization objective) will be the number of games
    the agent needs to play before solving the game (reaching a mean score of greater
    than 18 for Pong).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬çš„åº¦é‡ï¼ˆä¼˜åŒ–ç›®æ ‡ï¼‰å°†æ˜¯ä»£ç†éœ€è¦ç©å¤šå°‘å±€æ¸¸æˆæ‰èƒ½è§£å†³æ¸¸æˆï¼ˆå³åœ¨Pongä¸­è¾¾åˆ°å¤§äº18çš„å¹³å‡å¾—åˆ†ï¼‰ã€‚
- en: To illustrate the effect of tuning, for every DQN extension, we check the training
    dynamics using a fixed set of parameters (the same as in ChapterÂ [6](#)), and
    the dynamics using the best hyperparameters found after 20-30 rounds of tuning.
    If you wish, you can do your own experiments, optimizing more hyperparameters.
    Most likely, this will allow you to find a better configuration for the training.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜è°ƒæ•´çš„æ•ˆæœï¼Œå¯¹äºæ¯ä¸ªDQNæ‰©å±•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç»„å›ºå®šçš„å‚æ•°ï¼ˆä¸ç¬¬[6ç« ](#)ç›¸åŒï¼‰æ£€æŸ¥è®­ç»ƒåŠ¨æ€ï¼Œå¹¶ä½¿ç”¨åœ¨20-30è½®è°ƒæ•´åæ‰¾åˆ°çš„æœ€ä½³è¶…å‚æ•°è¿›è¡Œè®­ç»ƒã€‚å¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥åšè‡ªå·±çš„å®éªŒï¼Œä¼˜åŒ–æ›´å¤šçš„è¶…å‚æ•°ã€‚æœ€æœ‰å¯èƒ½çš„æ˜¯ï¼Œè¿™å°†ä½¿ä½ èƒ½å¤Ÿæ‰¾åˆ°ä¸€ä¸ªæ›´å¥½çš„è®­ç»ƒé…ç½®ã€‚
- en: 'The core of the process is implemented in the common.tune_params function.
    Letâ€™s take a look at its code. We start with the type declaration and hyperparameter
    space:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹çš„æ ¸å¿ƒå®ç°æ˜¯åœ¨`common.tune_params`å‡½æ•°ä¸­ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„ä»£ç ã€‚æˆ‘ä»¬ä»ç±»å‹å£°æ˜å’Œè¶…å‚æ•°ç©ºé—´å¼€å§‹ï¼š
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we first define the type for the training function, which takes the Hyperparams
    dataclass torch.device to use and a dictionary with extra parameters (as some
    DQN extensions weâ€™re going to present might require extra parameters besides those
    declared in Hyperparams).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰è®­ç»ƒå‡½æ•°çš„ç±»å‹ï¼Œå®ƒæ¥æ”¶ä¸€ä¸ª`Hyperparams`æ•°æ®ç±»ã€ä¸€ä¸ªè¦ä½¿ç”¨çš„`torch.device`ï¼Œä»¥åŠä¸€ä¸ªåŒ…å«é¢å¤–å‚æ•°çš„å­—å…¸ï¼ˆå› ä¸ºæˆ‘ä»¬å³å°†ä»‹ç»çš„æŸäº›DQNæ‰©å±•å¯èƒ½éœ€è¦é™¤äº†åœ¨`Hyperparams`ä¸­å£°æ˜çš„å‚æ•°ä»¥å¤–çš„é¢å¤–å‚æ•°ï¼‰ã€‚
- en: The result of the function is either the int value, which will be the amount
    of games we played before reaching the score of 18, or None if we decided to stop
    the training early. This is required, as some hyperparameter combinations might
    fail to converge or converge too slowly, so to save time we stop the training
    without waiting for too long.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„ç»“æœå¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°å€¼ï¼Œè¡¨ç¤ºåœ¨è¾¾åˆ°18åˆ†çš„å¾—åˆ†ä¹‹å‰æˆ‘ä»¬ç©äº†å¤šå°‘å±€æ¸¸æˆï¼Œæˆ–è€…æ˜¯Noneï¼Œå¦‚æœæˆ‘ä»¬å†³å®šæå‰åœæ­¢è®­ç»ƒã€‚è¿™æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºæŸäº›è¶…å‚æ•°ç»„åˆå¯èƒ½æ— æ³•æ”¶æ•›æˆ–æ”¶æ•›å¾—å¤ªæ…¢ï¼Œå› æ­¤ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæˆ‘ä»¬ä¼šåœ¨ä¸ç­‰å¾…å¤ªä¹…çš„æƒ…å†µä¸‹åœæ­¢è®­ç»ƒã€‚
- en: Then we define the hyperparameter search space â€” which is a dict with string
    keys (parameter name) and the tune declaration of possible values to explore.
    It could be a probability distribution (uniform, loguniform, normal, etc.) or
    an explicit list of values to try. You can also use tune.grid_search declaration
    with a list of values. In that case, all the values will be tried.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´â€”â€”è¿™æ˜¯ä¸€ä¸ªå…·æœ‰å­—ç¬¦ä¸²é”®ï¼ˆå‚æ•°åï¼‰å’Œå¯èƒ½å€¼æ¢ç´¢çš„ `tune` å£°æ˜çš„å­—å…¸ã€‚å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼ˆå‡åŒ€ã€å¯¹æ•°å‡åŒ€ã€æ­£æ€ç­‰ï¼‰æˆ–è¦å°è¯•çš„æ˜¾å¼å€¼åˆ—è¡¨ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨
    `tune.grid_search` å£°æ˜ï¼Œæä¾›ä¸€ä¸ªå€¼åˆ—è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†å°è¯•æ‰€æœ‰å€¼ã€‚
- en: In our case, we sample the learning rate from the loguniform distribution and
    gamma from the list of 6 values ranging from 0.9 to 0.995.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»å¯¹æ•°å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·å­¦ä¹ ç‡ï¼Œå¹¶ä»ä¸€ä¸ªåŒ…å« 6 ä¸ªå€¼ï¼ˆèŒƒå›´ä» 0.9 åˆ° 0.995ï¼‰çš„åˆ—è¡¨ä¸­é‡‡æ · gammaã€‚
- en: 'Next, we have the tune_params function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æœ‰ `tune_params` å‡½æ•°ï¼š
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This function is given the following arguments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°ç»™å®šä»¥ä¸‹å‚æ•°ï¼š
- en: Basic set of hyperparameters that will be used for training
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨äºè®­ç»ƒçš„åŸºç¡€è¶…å‚æ•°é›†
- en: Training function
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒå‡½æ•°
- en: Torch device to use
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„ Torch è®¾å¤‡
- en: Amount of samples to perform during the round
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å›åˆä¸­æ‰§è¡Œçš„æ ·æœ¬æ•°é‡
- en: Additional dictionary with search space
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å…·æœ‰æœç´¢ç©ºé—´çš„é™„åŠ å­—å…¸
- en: Inside this function, we have an objective function, which creates the Hyperparameters
    object from the sampled dict, calls the training function, and returns the dictionary
    (which is a requirement of the ray.tune library).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç›®æ ‡å‡½æ•°ï¼Œå®ƒä»é‡‡æ ·çš„å­—å…¸ä¸­åˆ›å»º `Hyperparameters` å¯¹è±¡ï¼Œè°ƒç”¨è®­ç»ƒå‡½æ•°ï¼Œå¹¶è¿”å›å­—å…¸ï¼ˆè¿™æ˜¯ ray.tune åº“çš„è¦æ±‚ï¼‰ã€‚
- en: 'The rest of the tune_params function is simple:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`tune_params` å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†å¾ˆç®€å•ï¼š'
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we wrap the objective function to pass the torch device and take into
    account GPU resources. This is needed to allow Ray to properly parallelize the
    tuning process. If you have multiple GPUs installed on the machine, it will run
    several trainings in parallel. Then, we just create the Tuner object and ask it
    to perform the hyperparameter search.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åŒ…è£…ç›®æ ‡å‡½æ•°ï¼Œä»¥ä¼ é€’ Torch è®¾å¤‡å¹¶è€ƒè™‘ GPU èµ„æºã€‚è¿™æ˜¯ä¸ºäº†è®© Ray èƒ½å¤Ÿæ­£ç¡®åœ°å¹¶è¡ŒåŒ–è°ƒä¼˜è¿‡ç¨‹ã€‚å¦‚æœä½ æœºå™¨ä¸Šå®‰è£…äº†å¤šä¸ª GPUï¼Œå®ƒå°†å¹¶è¡Œè¿è¡Œå¤šä¸ªè®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬åªéœ€åˆ›å»º
    `Tuner` å¯¹è±¡ï¼Œå¹¶è¦æ±‚å®ƒæ‰§è¡Œè¶…å‚æ•°æœç´¢ã€‚
- en: 'The final piece relevant to hyperparameter tuning is in the setup_ignite function.
    It checks for situations when the training process is not converging, so we stop
    the training to avoid infinite waiting. To do this, we install the Ignite event
    handler if weâ€™re in the hyperparameter tuning mode:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¶…å‚æ•°è°ƒä¼˜ç›¸å…³çš„æœ€åä¸€éƒ¨åˆ†ä»£ç åœ¨ `setup_ignite` å‡½æ•°ä¸­ã€‚å®ƒæ£€æŸ¥è®­ç»ƒè¿‡ç¨‹æ˜¯å¦æ²¡æœ‰æ”¶æ•›ï¼Œå¦‚æœæ²¡æœ‰æ”¶æ•›ï¼Œåˆ™åœæ­¢è®­ç»ƒä»¥é¿å…æ— é™ç­‰å¾…ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨è¶…å‚æ•°è°ƒä¼˜æ¨¡å¼ä¸‹å®‰è£…
    Ignite äº‹ä»¶å¤„ç†ç¨‹åºï¼š
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, we check for two conditions:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ£€æŸ¥ä¸¤ä¸ªæ¡ä»¶ï¼š
- en: If the mean reward is lower than tuner_reward_min (which is an argument to the
    setup_ignite function and equal to -19 by default) after 100 games (provided in
    the tuner_reward_episode argument). This means that itâ€™s quite unlikely that weâ€™ll
    converge at all.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœå¹³å‡å¥–åŠ±ä½äº `tuner_reward_min`ï¼ˆè¿™æ˜¯ `setup_ignite` å‡½æ•°çš„ä¸€ä¸ªå‚æ•°ï¼Œé»˜è®¤ä¸º -19ï¼‰ï¼Œå¹¶ä¸”åœ¨ 100 å±€æ¸¸æˆåï¼ˆç”±
    `tuner_reward_episode` å‚æ•°æä¾›ï¼‰ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å‡ ä¹ä¸å¯èƒ½æ”¶æ•›ã€‚
- en: We played more than max_episodes amount of games and still havenâ€™t solved the
    game. In the default config, we set this limit to 500 games.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ç©äº†è¶…è¿‡ `max_episodes` å±€æ¸¸æˆï¼Œä»ç„¶æ²¡æœ‰è§£å†³æ¸¸æˆã€‚åœ¨é»˜è®¤é…ç½®ä¸­ï¼Œæˆ‘ä»¬å°†æ­¤é™åˆ¶è®¾ç½®ä¸º 500 å±€æ¸¸æˆã€‚
- en: In both cases, we stop the training and set the solved attribute to False, which
    will return a high constant metric value in our tuning process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½ä¼šåœæ­¢è®­ç»ƒå¹¶å°† `solved` å±æ€§è®¾ç½®ä¸º `False`ï¼Œè¿™å°†åœ¨è°ƒä¼˜è¿‡ç¨‹ä¸­è¿”å›ä¸€ä¸ªè¾ƒé«˜çš„å¸¸æ•°æŒ‡æ ‡å€¼ã€‚
- en: Thatâ€™s it for the hyperparameter tuning code. Before we run it and check the
    results, letâ€™s first start a single training using the parameters we used in ChapterÂ [6](#).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è¶…å‚æ•°è°ƒä¼˜ä»£ç çš„å…¨éƒ¨å†…å®¹ã€‚åœ¨è¿è¡Œå¹¶æ£€æŸ¥ç»“æœä¹‹å‰ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬ [6](#) ç« ä¸­ä½¿ç”¨çš„å‚æ•°å¼€å§‹ä¸€æ¬¡å•æ¬¡è®­ç»ƒã€‚
- en: Results with common parameters
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¸¸è§å‚æ•°çš„ç»“æœ
- en: If we run the training with the argument --params common, weâ€™ll train the Pong
    game using hyperparameters from the common.py module. As an option, you can use
    the --params best command line to train on the best values for this particular
    DQN extension.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä½¿ç”¨å‚æ•° `--params common` è¿è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª `common.py` æ¨¡å—çš„è¶…å‚æ•°è®­ç»ƒ Pong æ¸¸æˆã€‚ä½œä¸ºé€‰é¡¹ï¼Œä½ å¯ä»¥ä½¿ç”¨
    `--params best` å‘½ä»¤è¡Œæ¥è®­ç»ƒè¯¥ DQN æ‰©å±•çš„æœ€ä½³å€¼ã€‚
- en: 'Okay, letâ€™s start the training using the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒï¼š
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Every line in the output is written at the end of the game episode, showing
    the episode reward, a count of steps, the speed, and the total training time.
    For the basic DQN version and common hyperparameters, it usually takes about 700K
    frames and about 400 games to reach the mean reward of 18, so be patient. During
    the training, we can check the dynamics of the training process in TensorBoard,
    which shows charts for epsilon, raw reward values, average reward, and speed.
    The following charts show the reward and the number of steps for episodes (the
    bottom x axis shows the wall clock time, and the top axis is the episode number):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºä¸­çš„æ¯ä¸€è¡Œéƒ½æ˜¯åœ¨æ¸¸æˆå›åˆç»“æŸæ—¶å†™å…¥çš„ï¼Œæ˜¾ç¤ºå›åˆå¥–åŠ±ã€æ­¥æ•°ã€é€Ÿåº¦å’Œæ€»è®­ç»ƒæ—¶é—´ã€‚å¯¹äºåŸºç¡€çš„DQNç‰ˆæœ¬å’Œå¸¸è§çš„è¶…å‚æ•°ï¼Œé€šå¸¸éœ€è¦å¤§çº¦70ä¸‡å¸§å’Œçº¦400å±€æ¸¸æˆæ‰èƒ½è¾¾åˆ°18çš„å¹³å‡å¥–åŠ±ï¼Œå› æ­¤éœ€è¦è€å¿ƒã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨TensorBoardä¸­æŸ¥çœ‹è®­ç»ƒè¿‡ç¨‹çš„åŠ¨æ€ï¼Œé‡Œé¢æ˜¾ç¤ºäº†Îµå€¼ã€åŸå§‹å¥–åŠ±å€¼ã€å¹³å‡å¥–åŠ±å’Œé€Ÿåº¦çš„å›¾è¡¨ã€‚ä»¥ä¸‹å›¾è¡¨æ˜¾ç¤ºäº†æ¯å›åˆçš„å¥–åŠ±å’Œæ­¥æ•°ï¼ˆåº•éƒ¨xè½´è¡¨ç¤ºå¢™é’Ÿæ—¶é—´ï¼Œé¡¶éƒ¨xè½´è¡¨ç¤ºå›åˆæ•°ï¼‰ï¼š
- en: '![PIC](img/B22150_08_01.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_01.png)'
- en: 'FigureÂ 8.1: Plots with reward (left) and count of steps per episode (right)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.1ï¼šå¥–åŠ±å›¾ï¼ˆå·¦ï¼‰å’Œæ¯å›åˆæ­¥æ•°å›¾ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_08_02.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_02.png)'
- en: 'FigureÂ 8.2: Plots with training speed (left) and average training loss (right)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.2ï¼šè®­ç»ƒé€Ÿåº¦å›¾ï¼ˆå·¦ï¼‰å’Œå¹³å‡è®­ç»ƒæŸå¤±å›¾ï¼ˆå³ï¼‰
- en: It is also worth noting how the count of steps per episode changes during the
    training. Initially, it increases, as our network starts winning more and more
    games, but after a certain level, the count of steps decreases 2x and stays almost
    constant. This is driven by our Î³ parameter, which discounts the agentâ€™s reward
    over time, so it tries not just to accumulate as much of a reward as possible,
    but also to do it efficiently.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜å€¼å¾—æ³¨æ„çš„æ˜¯æ¯å›åˆæ­¥æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å¦‚ä½•å˜åŒ–çš„ã€‚æœ€å¼€å§‹æ—¶ï¼Œæ­¥æ•°å¢åŠ ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç½‘ç»œå¼€å§‹èµ¢å¾—è¶Šæ¥è¶Šå¤šçš„æ¸¸æˆï¼Œä½†åœ¨è¾¾åˆ°æŸä¸ªæ°´å¹³åï¼Œæ­¥æ•°å‡å°‘äº†2å€å¹¶å‡ ä¹ä¿æŒä¸å˜ã€‚è¿™æ˜¯ç”±æˆ‘ä»¬çš„Î³å‚æ•°é©±åŠ¨çš„ï¼Œå®ƒä¼šéšç€æ—¶é—´çš„æ¨ç§»æŠ˜æ‰£æ™ºèƒ½ä½“çš„å¥–åŠ±ï¼Œæ‰€ä»¥å®ƒä¸ä»…ä»…æ˜¯å°½å¯èƒ½å¤šåœ°ç§¯ç´¯å¥–åŠ±ï¼Œè¿˜è¦é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚
- en: Tuned baseline DQN
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è°ƒæ•´è¿‡çš„åŸºå‡† DQN
- en: 'After running the baseline DQN with the command-line argument --tune 30 (which
    took about a day on one GPU), I was able to find the following parameters, which
    solves Pong in 340 episodes (instead of 360):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°--tune 30ï¼ˆè¿™åœ¨ä¸€å—GPUä¸ŠèŠ±è´¹äº†å¤§çº¦ä¸€å¤©ï¼‰è¿è¡ŒåŸºå‡†DQNä¹‹åï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹å‚æ•°ï¼Œè¿™å¯ä»¥åœ¨340å›åˆå†…è§£å†³Pongé—®é¢˜ï¼ˆè€Œä¸æ˜¯360å›åˆï¼‰ï¼š
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, the learning rate is almost the same as before (10^(âˆ’4)), but
    gamma is lower (0.98 versus 0.99). This might be an indication that Pong has relatively
    short subtrajectories with action-reward causality, so decreasing the Î³ has a
    stabilizing effect on the training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œå­¦ä¹ ç‡å‡ ä¹ä¸ä¹‹å‰ä¸€æ ·ï¼ˆ10^(âˆ’4)ï¼‰ï¼Œä½†Î³å€¼è¾ƒä½ï¼ˆ0.98å¯¹æ¯”0.99ï¼‰ã€‚è¿™å¯èƒ½è¡¨æ˜Pongæœ‰ç›¸å¯¹è¾ƒçŸ­çš„å­è½¨è¿¹ä¸åŠ¨ä½œ-å¥–åŠ±å› æœå…³ç³»ï¼Œå› æ­¤å‡å°‘Î³å¯¹è®­ç»ƒæœ‰ç¨³å®šä½œç”¨ã€‚
- en: 'In the following figure, you can see a comparison of the reward and steps per
    episode for both tuned and untuned versions (and the difference is quite minor):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹å›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°è°ƒæ•´è¿‡å’Œæœªè°ƒæ•´ç‰ˆæœ¬çš„å¥–åŠ±ä¸æ¯ä¸ªå›åˆæ­¥æ•°çš„æ¯”è¾ƒï¼ˆåŒºåˆ«éå¸¸å°ï¼‰ï¼š
- en: '![PIC](img/B22150_08_03.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_03.png)'
- en: 'FigureÂ 8.3: Plots with reward (left) and count of steps per episode (right)
    for tuned and untuned hyperparameters'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.3ï¼šè°ƒæ•´è¿‡çš„å’Œæœªè°ƒæ•´è¶…å‚æ•°çš„å¥–åŠ±å›¾ï¼ˆå·¦ï¼‰å’Œæ¯å›åˆæ­¥æ•°å›¾ï¼ˆå³ï¼‰
- en: Now we have our baseline DQN version and are ready to explore method modifications
    proposed by Hessel et al.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†åŸºå‡†DQNç‰ˆæœ¬ï¼Œå¹¶å‡†å¤‡æ¢ç´¢Hesselç­‰äººæå‡ºçš„æ”¹è¿›æ–¹æ³•ã€‚
- en: N-step DQN
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Næ­¥DQN
- en: 'The first improvement that we will implement and evaluate is quite an old one.
    It was first introduced by Sutton in the paper Learning to Predict by the Methods
    of Temporal Differences [[Sut88](#)]. To get the idea, letâ€™s look at the Bellman
    update used in Q-learning once again:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¦å®ç°å¹¶è¯„ä¼°çš„ç¬¬ä¸€ä¸ªæ”¹è¿›æ˜¯ä¸€ä¸ªæ¯”è¾ƒè€çš„æ–¹æ³•ã€‚å®ƒæœ€æ—©ç”±Suttonåœ¨è®ºæ–‡ã€Šé€šè¿‡æ—¶é—´å·®åˆ†æ–¹æ³•å­¦ä¹ é¢„æµ‹ã€‹[[Sut88](#)]ä¸­æå‡ºã€‚ä¸ºäº†ç†è§£è¿™ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬å†çœ‹ä¸€éQ-learningä¸­ä½¿ç”¨çš„Bellmanæ›´æ–°ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq26.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq26.png)'
- en: 'This equation is recursive, which means that we can express Q(s[t+1],a[t+1])
    in terms of itself, which gives us this result:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹ç¨‹æ˜¯é€’å½’çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç”¨è‡ªèº«æ¥è¡¨ç¤ºQ(s[t+1],a[t+1])ï¼Œä»è€Œå¾—åˆ°è¿™ä¸ªç»“æœï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq27.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq27.png)'
- en: 'Value r[a,t+1] means local reward at time t + 1, after issuing action a. However,
    if we assume that action a at step t + 1 was chosen optimally, or close to optimally,
    we can omit the max[a] operation and obtain this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼r[a,t+1]è¡¨ç¤ºåœ¨æ—¶é—´t + 1æ—¶å‘å‡ºåŠ¨ä½œaåçš„å±€éƒ¨å¥–åŠ±ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬å‡è®¾åœ¨t + 1æ­¥æ—¶çš„åŠ¨ä½œaæ˜¯æœ€ä¼˜é€‰æ‹©æˆ–æ¥è¿‘æœ€ä¼˜é€‰æ‹©ï¼Œæˆ‘ä»¬å¯ä»¥çœç•¥max[a]æ“ä½œï¼Œå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq28.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq28.png)'
- en: 'This value can be unrolled again and again any number of times. As you may
    guess, this unrolling can be easily applied to our DQN update by replacing one-step
    transition sampling with longer transition sequences of n-steps. To understand
    why this unrolling will help us to speed up training, letâ€™s consider the example
    illustrated in FigureÂ [8.4](#x1-131004r4). Here, we have a simple environment
    of four states (s[1], s[2], s[3], s[4]) and the only action available at every
    state, except s[4], which is a terminal state:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå€¼å¯ä»¥åå¤å±•å¼€ï¼Œæ¬¡æ•°ä¸é™ã€‚æ­£å¦‚ä½ å¯èƒ½çŒœåˆ°çš„ï¼Œè¿™ç§å±•å¼€å¯ä»¥è½»æ¾åº”ç”¨åˆ°æˆ‘ä»¬çš„ DQN æ›´æ–°ä¸­ï¼Œé€šè¿‡ç”¨æ›´é•¿çš„ n æ­¥è½¬ç§»åºåˆ—æ›¿æ¢ä¸€æ­¥è½¬ç§»é‡‡æ ·ã€‚ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆè¿™ç§å±•å¼€å¯ä»¥å¸®åŠ©æˆ‘ä»¬åŠ é€Ÿè®­ç»ƒï¼Œè®©æˆ‘ä»¬è€ƒè™‘å›¾
    [8.4](#x1-131004r4) ä¸­çš„ç¤ºä¾‹ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç®€å•çš„å››çŠ¶æ€ç¯å¢ƒï¼ˆs[1]ã€s[2]ã€s[3]ã€s[4]ï¼‰ï¼Œé™¤äº†ç»ˆæ­¢çŠ¶æ€ s[4] å¤–ï¼Œæ¯ä¸ªçŠ¶æ€éƒ½æœ‰å”¯ä¸€å¯æ‰§è¡Œçš„åŠ¨ä½œï¼š
- en: '![ssssararar1234123 ](img/B22150_08_04.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![ssssararar1234123 ](img/B22150_08_04.png)'
- en: 'FigureÂ 8.4: A transition diagram for a simple environment'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.4ï¼šä¸€ä¸ªç®€å•ç¯å¢ƒçš„è½¬ç§»å›¾
- en: 'So, what happens in a one-step case? We have three total updates possible (we
    donâ€™t use max, as there is only one action available):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä¸€æ­¥æƒ…å†µä¸‹ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿæˆ‘ä»¬æ€»å…±æœ‰ä¸‰ä¸ªæ›´æ–°æ˜¯å¯èƒ½çš„ï¼ˆæˆ‘ä»¬ä¸ä½¿ç”¨ maxï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªå¯æ‰§è¡ŒåŠ¨ä½œï¼‰ï¼š
- en: Q(s[1],a) â†r[1] + Î³Q(s[2],a)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[1],a) â†r[1] + Î³Q(s[2],a)
- en: Q(s[2],a) â†r[2] + Î³Q(s[3],a)
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[2],a) â†r[2] + Î³Q(s[3],a)
- en: Q(s[3],a) â†r[3]
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[3],a) â†r[3]
- en: Letâ€™s imagine that, at the beginning of the training, we complete the preceding
    updates in this order. The first two updates will be useless, as our current Q(s[2],a)
    and Q(s[3],a) are incorrect and contain initial random values. The only useful
    update will be update 3, which will correctly assign reward r[3] to the state
    s[3] prior to the terminal state.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾åœ¨è®­ç»ƒå¼€å§‹æ—¶ï¼Œæˆ‘ä»¬æŒ‰ç…§è¿™ä¸ªé¡ºåºå®Œæˆä¹‹å‰çš„æ›´æ–°ã€‚å‰ä¸¤ä¸ªæ›´æ–°å°†æ²¡æœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬å½“å‰çš„ Q(s[2],a) å’Œ Q(s[3],a) æ˜¯ä¸æ­£ç¡®çš„ï¼Œä¸”åŒ…å«åˆå§‹çš„éšæœºå€¼ã€‚å”¯ä¸€æœ‰ç”¨çš„æ›´æ–°æ˜¯æ›´æ–°
    3ï¼Œå®ƒä¼šå°†å¥–åŠ± r[3] æ­£ç¡®åœ°åˆ†é…ç»™ç»ˆæ­¢çŠ¶æ€ä¹‹å‰çš„çŠ¶æ€ s[3]ã€‚
- en: Now letâ€™s perform the updates over and over again. On the second iteration,
    the correct value will be assigned to Q(s[2],a), but the update of Q(s[1],a) will
    still be noisy. Only on the third iteration will we get the valid values for every
    Q. So, even in a one-step case, it takes three steps to propagate the correct
    values to all the states.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä¸€æ¬¡æ¬¡æ‰§è¡Œè¿™äº›æ›´æ–°ã€‚åœ¨ç¬¬äºŒæ¬¡è¿­ä»£æ—¶ï¼ŒQ(s[2],a) ä¼šè¢«èµ‹äºˆæ­£ç¡®çš„å€¼ï¼Œä½† Q(s[1],a) çš„æ›´æ–°ä»ç„¶ä¼šå¸¦æœ‰å™ªå£°ã€‚ç›´åˆ°ç¬¬ä¸‰æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬æ‰ä¼šä¸ºæ¯ä¸ª
    Q è·å¾—æœ‰æ•ˆçš„å€¼ã€‚æ‰€ä»¥ï¼Œå³ä½¿æ˜¯åœ¨ä¸€æ­¥æƒ…å†µä¸‹ï¼Œä¹Ÿéœ€è¦ä¸‰æ­¥æ‰èƒ½å°†æ­£ç¡®çš„å€¼ä¼ æ’­åˆ°æ‰€æœ‰çŠ¶æ€ã€‚
- en: 'Now letâ€™s consider a two-step case. This situation again has three updates:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªä¸¤æ­¥çš„æƒ…å†µã€‚è¿™ä¸ªæƒ…å†µåŒæ ·æœ‰ä¸‰ä¸ªæ›´æ–°ï¼š
- en: Q(s[1],a) â†r[1] + Î³r[2] + Î³Â²Q(s[3],a)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[1],a) â†r[1] + Î³r[2] + Î³Â²Q(s[3],a)
- en: Q(s[2],a) â†r[2] + Î³r[3]
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[2],a) â†r[2] + Î³r[3]
- en: Q(s[3],a) â†r[3]
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q(s[3],a) â†r[3]
- en: In this case, on the first loop over the updates, the correct values will be
    assigned to both Q(s[2],a) and Q(s[3],a). On the second iteration, the value of
    Q(s[1],a) will also be properly updated. So, multiple steps improve the propagation
    speed of values, which improves convergence. You may be thinking, â€œIf itâ€™s so
    helpful, letâ€™s unroll the Bellman equation, say, 100 steps ahead. Will it speed
    up our convergence 100 times?â€ Unfortunately, the answer is no. Despite our expectations,
    our DQN will fail to converge at all.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨ç¬¬ä¸€æ¬¡æ›´æ–°å¾ªç¯ä¸­ï¼Œæ­£ç¡®çš„å€¼å°†åˆ†åˆ«åˆ†é…ç»™ Q(s[2],a) å’Œ Q(s[3],a)ã€‚åœ¨ç¬¬äºŒæ¬¡è¿­ä»£ä¸­ï¼ŒQ(s[1],a) çš„å€¼ä¹Ÿå°†å¾—åˆ°æ­£ç¡®æ›´æ–°ã€‚å› æ­¤ï¼Œå¤šæ­¥æ“ä½œæé«˜äº†å€¼çš„ä¼ æ’­é€Ÿåº¦ï¼Œä»è€Œæ”¹å–„äº†æ”¶æ•›æ€§ã€‚ä½ å¯èƒ½ä¼šæƒ³ï¼Œâ€œå¦‚æœè¿™æ ·è¿™ä¹ˆæœ‰å¸®åŠ©ï¼Œé‚£æˆ‘ä»¬ä¸å¦¨å°†
    Bellman æ–¹ç¨‹å±•å¼€ 100 æ­¥ã€‚è¿™æ ·ä¼šè®©æˆ‘ä»¬çš„æ”¶æ•›é€Ÿåº¦åŠ å¿« 100 å€å—ï¼Ÿâ€ä¸å¹¸çš„æ˜¯ï¼Œç­”æ¡ˆæ˜¯å¦å®šçš„ã€‚å°½ç®¡æˆ‘ä»¬æœ‰æ‰€æœŸå¾…ï¼Œæˆ‘ä»¬çš„ DQN å®Œå…¨æ— æ³•æ”¶æ•›ã€‚
- en: To understand why, letâ€™s again return to our unrolling process, especially where
    we dropped the max[a]. Was it correct? Strictly speaking, no. We omitted the max
    operation at the intermediate step, assuming that our action selection during
    experience gathering (or our policy) was optimal. What if it wasnâ€™t, for example,
    at the beginning of the training, when our agent acted randomly? In that case,
    our calculated value for Q(s[t],a[t]) may be smaller than the optimal value of
    the state (as some steps have been taken randomly, but not following the most
    promising paths by maximizing the Q-value). The more steps on which we unroll
    the Bellman equation, the more incorrect our update could be.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£ä¸ºä»€ä¹ˆå¦‚æ­¤ï¼Œæˆ‘ä»¬å†æ¬¡å›åˆ°æˆ‘ä»¬çš„å±•å¼€è¿‡ç¨‹ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬çœç•¥äº† max[a]ã€‚è¿™æ ·åšå¯¹å—ï¼Ÿä¸¥æ ¼æ¥è¯´ï¼Œç­”æ¡ˆæ˜¯å¦å®šçš„ã€‚æˆ‘ä»¬åœ¨ä¸­é—´æ­¥éª¤çœç•¥äº† max æ“ä½œï¼Œå‡è®¾æˆ‘ä»¬åœ¨ç»éªŒæ”¶é›†è¿‡ç¨‹ä¸­ï¼ˆæˆ–è€…æˆ‘ä»¬çš„ç­–ç•¥ï¼‰æ˜¯æœ€ä¼˜çš„ã€‚å‡å¦‚ä¸æ˜¯å‘¢ï¼Ÿä¾‹å¦‚ï¼Œåœ¨è®­ç»ƒåˆæœŸï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“æ˜¯éšæœºè¡Œä¸ºçš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®¡ç®—å‡ºçš„
    Q(s[t],a[t]) å€¼å¯èƒ½å°äºè¯¥çŠ¶æ€çš„æœ€ä¼˜å€¼ï¼ˆå› ä¸ºæŸäº›æ­¥éª¤æ˜¯éšæœºæ‰§è¡Œçš„ï¼Œè€Œä¸æ˜¯é€šè¿‡æœ€å¤§åŒ– Q å€¼æ¥éµå¾ªæœ€æœ‰å¸Œæœ›çš„è·¯å¾„ï¼‰ã€‚æˆ‘ä»¬å±•å¼€ Bellman æ–¹ç¨‹çš„æ­¥æ•°è¶Šå¤šï¼Œæˆ‘ä»¬çš„æ›´æ–°å¯èƒ½å°±è¶Šä¸å‡†ç¡®ã€‚
- en: Our large experience replay buffer will make the situation even worse, as it
    will increase the chance of getting transitions obtained from the old bad policy
    (dictated by old bad approximations of Q). This will lead to a wrong update of
    the current Q approximation, so it can easily break our training progress. This
    problem is a fundamental characteristic of RL methods, as was briefly mentioned
    in ChapterÂ [4](ch008.xhtml#x1-740004), when we talked about RL methodsâ€™ taxonomy.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¤§å‹ç»éªŒå›æ”¾ç¼“å†²åŒºå°†ä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿï¼Œå› ä¸ºå®ƒä¼šå¢åŠ ä»æ—§çš„ç³Ÿç³•ç­–ç•¥ï¼ˆç”±æ—§çš„ç³Ÿç³•Qè¿‘ä¼¼æ‰€å†³å®šï¼‰è·å¾—è¿‡æ¸¡çš„æœºä¼šã€‚è¿™å°†å¯¼è‡´å½“å‰Qè¿‘ä¼¼çš„é”™è¯¯æ›´æ–°ï¼Œä»è€Œå¾ˆå®¹æ˜“ç ´åæˆ‘ä»¬çš„è®­ç»ƒè¿›ç¨‹ã€‚è¿™ä¸ªé—®é¢˜æ˜¯å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„ä¸€ä¸ªåŸºæœ¬ç‰¹å¾ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[4](ch008.xhtml#x1-740004)ç« ç®€è¦æåˆ°çš„ï¼Œå½“æ—¶æˆ‘ä»¬è®¨è®ºäº†å¼ºåŒ–å­¦ä¹ æ–¹æ³•çš„åˆ†ç±»ã€‚
- en: 'There are two large classes of methods:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤å¤§ç±»æ–¹æ³•ï¼š
- en: 'Off-policy methods: The first class of off-policy methods doesnâ€™t depend on
    the â€œfreshness of data.â€ For example, a simple DQN is off-policy, which means
    that we can use very old data sampled from the environment several million steps
    ago, and this data will still be useful for learning. Thatâ€™s because we are just
    updating the value of the action, Q(s[t],a[t]), with the immediate reward, plus
    the discounted current approximation of the best actionâ€™s value. Even if action
    a[t] was sampled randomly, it doesnâ€™t matter because for this particular action
    a[t], in the state s[t], our update will be correct. Thatâ€™s why in off-policy
    methods, we can use a very large experience buffer to make our data closer to
    being independent and identically distributed (iid) .'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºéç­–ç•¥çš„æ–¹æ³•ï¼šç¬¬ä¸€ç±»åŸºäºéç­–ç•¥çš„æ–¹æ³•ä¸ä¾èµ–äºâ€œæ•°æ®çš„æ–°é²œåº¦â€ã€‚ä¾‹å¦‚ï¼Œç®€å•çš„DQNå°±æ˜¯åŸºäºéç­–ç•¥çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‡ ç™¾ä¸‡æ­¥ä¹‹å‰ä»ç¯å¢ƒä¸­é‡‡æ ·çš„éå¸¸æ—§çš„æ•°æ®ï¼Œè¿™äº›æ•°æ®ä»ç„¶å¯¹å­¦ä¹ æœ‰ç”¨ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬åªæ˜¯ç”¨å³æ—¶å¥–åŠ±åŠ ä¸Šæœ€ä½³è¡ŒåŠ¨ä»·å€¼çš„å½“å‰æŠ˜æ‰£è¿‘ä¼¼æ¥æ›´æ–°åŠ¨ä½œçš„ä»·å€¼Q(s[t],a[t])ã€‚å³ä½¿åŠ¨ä½œa[t]æ˜¯éšæœºé‡‡æ ·çš„ï¼Œä¹Ÿæ— å…³ç´§è¦ï¼Œå› ä¸ºå¯¹äºè¿™ä¸ªç‰¹å®šçš„åŠ¨ä½œa[t]ï¼Œåœ¨çŠ¶æ€s[t]ä¸‹ï¼Œæˆ‘ä»¬çš„æ›´æ–°æ˜¯æ­£ç¡®çš„ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨åŸºäºéç­–ç•¥çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªéå¸¸å¤§çš„ç»éªŒç¼“å†²åŒºï¼Œä½¿æˆ‘ä»¬çš„æ•°æ®æ›´æ¥è¿‘ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰ã€‚
- en: 'On-policy methods: On the other hand, on-policy methods heavily depend on the
    training data to be sampled according to the current policy that we are updating.
    That happens because on-policy methods try to improve the current policy indirectly
    (as in the previous n-step DQN) or directly (all of Part 3 of the book is devoted
    to such methods).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼šå¦ä¸€æ–¹é¢ï¼ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ä¸¥é‡ä¾èµ–äºæ ¹æ®æˆ‘ä»¬æ­£åœ¨æ›´æ–°çš„å½“å‰ç­–ç•¥æ¥é‡‡æ ·çš„è®­ç»ƒæ•°æ®ã€‚è¿™æ˜¯å› ä¸ºåŸºäºç­–ç•¥çš„æ–¹æ³•è¯•å›¾é—´æ¥ï¼ˆå¦‚ä¹‹å‰çš„næ­¥DQNï¼‰æˆ–ç›´æ¥ï¼ˆæœ¬ä¹¦ç¬¬ä¸‰éƒ¨åˆ†çš„å†…å®¹å®Œå…¨æ˜¯å…³äºè¿™ç§æ–¹æ³•ï¼‰æ”¹è¿›å½“å‰ç­–ç•¥ã€‚
- en: So, which class of methods is better? Well, it depends. Off-policy methods allow
    you to train on the previous large history of data or even on human demonstrations,
    but they usually are slower to converge. On-policy methods are typically faster,
    but require much more fresh data from the environment, which can be costly. Just
    imagine a self-driving car trained with the on-policy method. It will cost you
    a lot of crashed cars before the system learns that walls and trees are things
    that it should avoid!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå“ªç§æ–¹æ³•æ›´å¥½å‘¢ï¼Ÿå—¯ï¼Œè¿™å–å†³äºã€‚åŸºäºéç­–ç•¥çš„æ–¹æ³•å…è®¸ä½ åœ¨å…ˆå‰çš„å¤§é‡æ•°æ®å†å²ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç”šè‡³åœ¨äººå·¥ç¤ºèŒƒä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†å®ƒä»¬é€šå¸¸æ”¶æ•›è¾ƒæ…¢ã€‚åŸºäºç­–ç•¥çš„æ–¹æ³•é€šå¸¸æ›´å¿«ï¼Œä½†éœ€è¦æ›´å¤šæ¥è‡ªç¯å¢ƒçš„æ–°é²œæ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå¾ˆæ˜‚è´µã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œä½¿ç”¨åŸºäºç­–ç•¥çš„æ–¹æ³•è®­ç»ƒä¸€ä¸ªè‡ªåŠ¨é©¾é©¶æ±½è½¦ã€‚åœ¨ç³»ç»Ÿå­¦ä¼šé¿å¼€å¢™å£å’Œæ ‘æœ¨ä¹‹å‰ï¼Œä½ å¾—èŠ±è´¹å¤§é‡çš„æ’è½¦æˆæœ¬ï¼
- en: 'You may have a question: why are we talking about an n-step DQN if this â€œn-stepnessâ€
    turns it into an on-policy method, which will make our large experience buffer
    useless? In practice, this is usually not black and white. You may still use an
    n-step DQN if it will help to speed up DQNs, but you need to be modest with the
    selection of n. Small values of two or three usually work well, because our trajectories
    in the experience buffer are not that different from one-step transitions. In
    such cases, convergence speed usually improves proportionally, but large values
    of n can break the training process. So, the number of steps should be tuned,
    but convergence speeding up usually makes it worth doing.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šæœ‰ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è®¨è®ºä¸€ä¸ªnæ­¥DQNï¼Œå¦‚æœè¿™ä¸ªâ€œnæ­¥æ€§â€ä¼šä½¿å®ƒå˜æˆä¸€ä¸ªåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œè¿™å°†ä½¿æˆ‘ä»¬çš„å¤§å‹ç»éªŒç¼“å†²åŒºå˜å¾—æ²¡ç”¨ï¼Ÿå®é™…ä¸Šï¼Œè¿™é€šå¸¸ä¸æ˜¯éé»‘å³ç™½çš„ã€‚ä½ ä»ç„¶å¯ä»¥ä½¿ç”¨næ­¥DQNï¼Œå¦‚æœå®ƒæœ‰åŠ©äºåŠ é€ŸDQNçš„è®­ç»ƒï¼Œä½†ä½ éœ€è¦åœ¨é€‰æ‹©næ—¶ä¿æŒè°¨æ…ã€‚å°çš„å€¼ï¼Œå¦‚äºŒæˆ–ä¸‰ï¼Œé€šå¸¸æ•ˆæœå¾ˆå¥½ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ç»éªŒç¼“å†²åŒºä¸­çš„è½¨è¿¹ä¸ä¸€æ­¥è¿‡æ¸¡å·®åˆ«ä¸å¤§ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ”¶æ•›é€Ÿåº¦é€šå¸¸ä¼šæˆæ¯”ä¾‹åœ°æé«˜ï¼Œä½†nå€¼è¿‡å¤§å¯èƒ½ä¼šç ´åè®­ç»ƒè¿‡ç¨‹ã€‚å› æ­¤ï¼Œæ­¥æ•°åº”è¯¥è¿›è¡Œè°ƒä¼˜ï¼Œä½†åŠ é€Ÿæ”¶æ•›é€šå¸¸ä½¿å¾—è¿™æ ·åšæ˜¯å€¼å¾—çš„ã€‚
- en: Implementation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'As the ExperienceSourceFirstLast class already supports the multi-step Bellman
    unroll, our n-step version of a DQN is extremely simple. There are only two modifications
    that we need to make to the basic DQN to turn it into an n-step version:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºExperienceSourceFirstLastç±»å·²ç»æ”¯æŒå¤šæ­¥Bellmanå±•å¼€ï¼Œå› æ­¤æˆ‘ä»¬çš„næ­¥ç‰ˆæœ¬çš„DQNéå¸¸ç®€å•ã€‚æˆ‘ä»¬åªéœ€è¦å¯¹åŸºç¡€DQNè¿›è¡Œä¸¤ä¸ªä¿®æ”¹ï¼Œå°±èƒ½å°†å…¶è½¬æ¢ä¸ºnæ­¥ç‰ˆæœ¬ï¼š
- en: Pass the count of steps that we want to unroll on ExperienceSourceFirstLast
    creation in the steps_count parameter.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ExperienceSourceFirstLaståˆ›å»ºæ—¶ï¼Œé€šè¿‡steps_countå‚æ•°ä¼ é€’æˆ‘ä»¬å¸Œæœ›å±•å¼€çš„æ­¥éª¤æ•°ã€‚
- en: Pass the correct gamma to the calc_loss_dqn function. This modification is really
    easy to overlook, which could be harmful to convergence. As our Bellman is now
    n-steps, the discount coefficient for the last state in the experience chain will
    no longer be just Î³, but Î³^n.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ­£ç¡®çš„gammaå€¼ä¼ é€’ç»™calc_loss_dqnå‡½æ•°ã€‚è¿™ä¸ªä¿®æ”¹éå¸¸å®¹æ˜“è¢«å¿½è§†ï¼Œä½†å´å¯èƒ½å¯¹æ”¶æ•›æ€§äº§ç”Ÿä¸åˆ©å½±å“ã€‚ç”±äºæˆ‘ä»¬çš„Bellmanç°åœ¨æ˜¯næ­¥çš„ï¼Œç»éªŒé“¾ä¸­æœ€åä¸€ä¸ªçŠ¶æ€çš„æŠ˜æ‰£ç³»æ•°å°†ä¸å†æ˜¯Î³ï¼Œè€Œæ˜¯Î³^nã€‚
- en: 'You can find the whole example in Chapter08/02_dqn_n_steps.py, with only the
    modified lines shown here:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Chapter08/02_dqn_n_steps.pyä¸­æ‰¾åˆ°æ•´ä¸ªç¤ºä¾‹ï¼Œè¿™é‡Œåªå±•ç¤ºäº†ä¿®æ”¹è¿‡çš„è¡Œï¼š
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The n_steps value is a count of steps passed in command-line arguments; the
    default is to use four steps.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: n_stepså€¼æ˜¯åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­ä¼ é€’çš„æ­¥æ•°è®¡æ•°ï¼›é»˜è®¤ä½¿ç”¨å››æ­¥ã€‚
- en: 'Another modification is in gamma passed to the calc_loss_dqn function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªä¿®æ”¹æ˜¯åœ¨ä¼ é€’ç»™calc_loss_dqnå‡½æ•°çš„gammaå€¼ï¼š
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Results
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'The training module Chapter08/02_dqn_n_steps.py can be started as before, with
    the additional command-line option -n, which gives a count of steps to unroll
    the Bellman equation. These are charts for our baseline and n-step DQN (using
    a common set of parameters), with n being equal to 2 and 3\. As you can see, the
    Bellman unroll has given a significant convergence speedup:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ¨¡å—Chapter08/02_dqn_n_steps.pyå¯ä»¥åƒä»¥å‰ä¸€æ ·å¯åŠ¨ï¼Œå¢åŠ äº†å‘½ä»¤è¡Œé€‰é¡¹-nï¼Œè¡¨ç¤ºå±•å¼€Bellmanæ–¹ç¨‹çš„æ­¥éª¤æ•°ã€‚è¿™äº›æ˜¯æˆ‘ä»¬åŸºçº¿å’Œnæ­¥DQNçš„å›¾è¡¨ï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°é›†ï¼‰ï¼Œå…¶ä¸­nå€¼ä¸º2å’Œ3ã€‚æ­£å¦‚ä½ æ‰€è§ï¼ŒBellmanå±•å¼€å¤§å¤§åŠ é€Ÿäº†æ”¶æ•›é€Ÿåº¦ï¼š
- en: '![PIC](img/B22150_08_05.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_05.png)'
- en: 'FigureÂ 8.5: The reward and number of steps for basic (one-step) DQN and n-step
    versions'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.5ï¼šåŸºæœ¬ï¼ˆå•æ­¥ï¼‰DQNå’Œnæ­¥ç‰ˆæœ¬çš„å¥–åŠ±å’Œæ­¥éª¤æ•°
- en: 'As you can see in the diagram, the three-step DQN converges significantly faster
    than the simple DQN, which is a nice improvement. So, what about a larger n? FigureÂ [8.6](#x1-133004r6)
    shows the reward dynamics for n = 3â€¦6:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾æ‰€ç¤ºï¼Œä¸‰æ­¥DQNçš„æ”¶æ•›é€Ÿåº¦æ˜¾è‘—å¿«äºç®€å•DQNï¼Œè¿™æ˜¯ä¸€ä¸ªä¸é”™çš„æ”¹è¿›ã€‚é‚£ä¹ˆï¼Œnå€¼æ›´å¤§å‘¢ï¼Ÿå›¾[8.6](#x1-133004r6)å±•ç¤ºäº†n = 3â€¦6çš„å¥–åŠ±åŠ¨æ€ï¼š
- en: '![PIC](img/B22150_08_06.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_06.png)'
- en: 'FigureÂ 8.6: Reward dynamics for cases with n = 3â€¦6 with common hyperparameters'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.6ï¼šn = 3â€¦6çš„å¥–åŠ±åŠ¨æ€ï¼Œä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°
- en: As you can see, going from three steps to four has given some improvement, but
    it is much less than before. The variant with n = 5 is worse and very close to
    n = 2\. The same is true for n = 6\. So, in our case, n = 3 looks optimal.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œä»ä¸‰æ­¥åˆ°å››æ­¥æœ‰æ‰€æå‡ï¼Œä½†è¿œä¸å¦‚ä¹‹å‰çš„æ”¹è¿›ã€‚n = 5çš„å˜ä½“è¡¨ç°æ›´å·®ï¼Œå‡ ä¹ä¸n = 2ç›¸å½“ã€‚n = 6ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ‰€ä»¥ï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œn = 3çœ‹èµ·æ¥æ˜¯æœ€ä¼˜çš„ã€‚
- en: Hyperparameter tuning
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: 'In this extension, hyperparameter tuning was done individually for every n
    from 2 to 7\. The following table shows the best parameters and number of games
    they require to solve the game:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ‰©å±•ä¸­ï¼Œè¶…å‚æ•°è°ƒä¼˜æ˜¯é’ˆå¯¹æ¯ä¸ªnå€¼ä»2åˆ°7å•ç‹¬è¿›è¡Œçš„ã€‚ä»¥ä¸‹è¡¨æ ¼æ˜¾ç¤ºäº†æœ€ä½³å‚æ•°ä»¥åŠå®ƒä»¬è§£å†³æ¸¸æˆæ‰€éœ€çš„æ¸¸æˆæ¬¡æ•°ï¼š
- en: '| n | Learning rate | Î³ | Games |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| n | å­¦ä¹ ç‡ | Î³ | æ¸¸æˆæ¬¡æ•° |'
- en: '| 2 | 3.97 â‹… 10^(âˆ’5) | 0.98 | 293 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3.97 â‹… 10^(âˆ’5) | 0.98 | 293 |'
- en: '| 3 | 7.82 â‹… 10^(âˆ’5) | 0.98 | 260 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 7.82 â‹… 10^(âˆ’5) | 0.98 | 260 |'
- en: '| 4 | 6.07 â‹… 10^(âˆ’5) | 0.98 | 290 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 6.07 â‹… 10^(âˆ’5) | 0.98 | 290 |'
- en: '| 5 | 7.52 â‹… 10^(âˆ’5) | 0.99 | 268 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 7.52 â‹… 10^(âˆ’5) | 0.99 | 268 |'
- en: '| 6 | 6.78 â‹… 10^(âˆ’5) | 0.995 | 261 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6.78 â‹… 10^(âˆ’5) | 0.995 | 261 |'
- en: '| 7 | 8.59 â‹… 10^(âˆ’5) | 0.98 | 284 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 8.59 â‹… 10^(âˆ’5) | 0.98 | 284 |'
- en: 'TableÂ 8.1: The best hyperparameters (learning rate and gamma) for every n'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨8.1ï¼šæ¯ä¸ªnå€¼çš„æœ€ä½³è¶…å‚æ•°ï¼ˆå­¦ä¹ ç‡å’Œgammaï¼‰
- en: This table also confirms the conclusions of the untuned version comparison â€”
    unrolling the Bellman equation for two and three steps improves the convergence,
    but a further increase of n produces worse results. n = 6 gives us a comparable
    result to n = 3, but the outcomes for n = 4 and n = 5 are worse, so we should
    stop at n = 3.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼ è¡¨æ ¼ä¹ŸéªŒè¯äº†æœªè°ƒä¼˜ç‰ˆæœ¬æ¯”è¾ƒçš„ç»“è®ºâ€”â€”å¯¹ä¸¤æ­¥å’Œä¸‰æ­¥å±•å¼€Bellmanæ–¹ç¨‹å¯ä»¥æé«˜æ”¶æ•›æ€§ï¼Œä½†è¿›ä¸€æ­¥å¢åŠ nä¼šå¯¼è‡´æ›´å·®çš„ç»“æœã€‚n = 6çš„ç»“æœä¸n = 3ç›¸å½“ï¼Œä½†n
    = 4å’Œn = 5çš„ç»“æœæ›´å·®ï¼Œå› æ­¤æˆ‘ä»¬åº”è¯¥åœåœ¨n = 3ã€‚
- en: FigureÂ [8.7](#x1-134003r7) compares the training dynamics of tuned versions
    of the baseline and N-step DQN with n = 2 and n = 3.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[8.7](#x1-134003r7)æ¯”è¾ƒäº†åŸºçº¿å’ŒNæ­¥DQNè°ƒä¼˜ç‰ˆæœ¬çš„è®­ç»ƒåŠ¨æ€ï¼Œåˆ†åˆ«ä¸ºn = 2å’Œn = 3ã€‚
- en: '![PIC](img/B22150_08_07.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_07.png)'
- en: 'FigureÂ 8.7: The reward and number of steps after hyperparameter tuning'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 8.7ï¼šè¶…å‚æ•°è°ƒæ•´åçš„å¥–åŠ±å’Œæ­¥æ•°
- en: Double DQN
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Double DQN
- en: The next fruitful idea on how to improve a basic DQN came from DeepMind researchers
    in the paper titled Deep reinforcement learning with double Q-learning [[VGS16](#)].
    In the paper, the authors demonstrated that the basic DQN tends to overestimate
    values for Q, which may be harmful to training performance and sometimes can lead
    to suboptimal policies. The root cause of this is the max operation in the Bellman
    equation, but the strict proof is a bit complicated (you can find the full explanation
    in the paper). As a solution to this problem, the authors proposed modifying the
    Bellman update a bit.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ”¹è¿›åŸºæœ¬DQNçš„ä¸‹ä¸€ä¸ªå¯Œæœ‰æˆæ•ˆçš„æƒ³æ³•æ¥è‡ªDeepMindç ”ç©¶äººå‘˜åœ¨æ ‡é¢˜ä¸ºæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„åŒé‡Qå­¦ä¹ çš„è®ºæ–‡ä¸­[[VGS16](#)]ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…è¯æ˜äº†åŸºæœ¬DQNå€¾å‘äºé«˜ä¼°Qå€¼ï¼Œè¿™å¯èƒ½å¯¹è®­ç»ƒæ€§èƒ½æœ‰å®³ï¼Œå¹¶ä¸”æœ‰æ—¶å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç­–ç•¥ã€‚è¿™èƒŒåçš„æ ¹æœ¬åŸå› æ˜¯Bellmanæ–¹ç¨‹ä¸­çš„maxæ“ä½œï¼Œä½†å…¶ä¸¥æ ¼è¯æ˜æœ‰ç‚¹å¤æ‚ï¼ˆæ‚¨å¯ä»¥åœ¨è®ºæ–‡ä¸­æ‰¾åˆ°å®Œæ•´çš„è§£é‡Šï¼‰ã€‚ä½œä¸ºè§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ï¼Œä½œè€…å»ºè®®ç¨å¾®ä¿®æ”¹è´å°”æ›¼æ›´æ–°ã€‚
- en: 'In the basic DQN, our target value for Q looked like this:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸºæœ¬DQNä¸­ï¼Œæˆ‘ä»¬çš„Qçš„ç›®æ ‡å€¼çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq29.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq29.png)'
- en: 'Qâ€²(s[t+1],a) was Q-values calculated using our target network, the weights
    of which are copied from the trained network every n steps. The authors of the
    paper proposed choosing actions for the next state using the trained network,
    but taking values of Q from the target network. So, the new expression for target
    Q-values will look like this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Qâ€²(s[t+1],a) æ˜¯ä½¿ç”¨æˆ‘ä»¬çš„ç›®æ ‡ç½‘ç»œè®¡ç®—çš„Qå€¼ï¼Œå…¶æƒé‡æ¯éš”næ­¥ä»è®­ç»ƒç½‘ç»œå¤åˆ¶ä¸€æ¬¡ã€‚è®ºæ–‡çš„ä½œè€…å»ºè®®é€‰æ‹©ä½¿ç”¨è®­ç»ƒç½‘ç»œä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€é€‰æ‹©åŠ¨ä½œï¼Œä½†ä»ç›®æ ‡ç½‘ç»œè·å–Qå€¼ã€‚å› æ­¤ï¼Œç›®æ ‡Qå€¼çš„æ–°è¡¨è¾¾å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq30.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq30.png)'
- en: The authors proved that this simple tweak fixes overestimation completely, and
    they called this new architecture double DQN.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…è¯æ˜äº†è¿™ä¸ªç®€å•çš„å°æ”¹è¿›å®Œå…¨ä¿®å¤äº†é«˜ä¼°é—®é¢˜ï¼Œå¹¶ç§°è¿™ç§æ–°æ¶æ„ä¸ºåŒé‡DQNã€‚
- en: Implementation
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ–½
- en: The core implementation is very simple. What we need to do is slightly modify
    our loss function. But letâ€™s go a step further and compare action values produced
    by basic DQN and double DQN. According to the paper authorâ€™s our baseline DQN
    should have consistently higher values predicted for the same states than the
    double DQN version. To do this, we store a random held-out set of states and periodically
    calculate the mean value of the best action for every state in the evaluation
    set.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒå®ç°éå¸¸ç®€å•ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ç¨å¾®ä¿®æ”¹æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ã€‚ä½†æ˜¯è®©æˆ‘ä»¬å†è¿›ä¸€æ­¥ï¼Œæ¯”è¾ƒåŸºæœ¬DQNå’ŒåŒé‡DQNç”Ÿæˆçš„åŠ¨ä½œå€¼ã€‚æ ¹æ®è®ºæ–‡ä½œè€…çš„è¯´æ³•ï¼Œæˆ‘ä»¬çš„åŸºçº¿DQNåº”è¯¥å¯¹äºç›¸åŒçŠ¶æ€çš„é¢„æµ‹å€¼å§‹ç»ˆè¾ƒé«˜ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å­˜å‚¨ä¸€ç»„éšæœºä¿ç•™çš„çŠ¶æ€ï¼Œå¹¶å‘¨æœŸæ€§åœ°è®¡ç®—è¯„ä¼°é›†ä¸­æ¯ä¸ªçŠ¶æ€çš„æœ€ä½³åŠ¨ä½œçš„å‡å€¼ã€‚
- en: 'The complete example is in Chapter08/03_dqn_double.py. Letâ€™s first take a look
    at the loss function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„ç¤ºä¾‹ä½äºChapter08/03_dqn_double.pyä¸­ã€‚è®©æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹æŸå¤±å‡½æ•°ï¼š
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will use this function instead of common.calc_loss_dqn and they both share
    lots of code. The main difference is in the next Q-values estimation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªå‡½æ•°è€Œä¸æ˜¯common.calc_loss_dqnï¼Œå®ƒä»¬éƒ½å…±äº«å¤§é‡ä»£ç ã€‚ä¸»è¦åŒºåˆ«åœ¨äºä¸‹ä¸€ä¸ªQå€¼çš„ä¼°è®¡ï¼š
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code snippet calculates the loss in a slightly different way.
    In the double DQN version, we calculate the best action to take in the next state
    using our main trained network, but values corresponding to this action come from
    the target network.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å‰é¢çš„ä»£ç ç‰‡æ®µä»¥ç¨å¾®ä¸åŒçš„æ–¹å¼è®¡ç®—æŸå¤±ã€‚åœ¨åŒé‡DQNç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„ä¸»è®­ç»ƒç½‘ç»œè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€ä¸­è¦é‡‡å–çš„æœ€ä½³åŠ¨ä½œï¼Œä½†ä¸æ­¤åŠ¨ä½œå¯¹åº”çš„å€¼æ¥è‡ªç›®æ ‡ç½‘ç»œã€‚
- en: This part could be implemented in a faster way, by combining next_states_v with
    states_v and calling our main network only once, but it will make the code less
    clear.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éƒ¨åˆ†å¯ä»¥é€šè¿‡å°†next_states_vä¸states_våˆå¹¶ï¼Œå¹¶ä»…è°ƒç”¨æˆ‘ä»¬çš„ä¸»ç½‘ç»œä¸€æ¬¡æ¥æ›´å¿«åœ°å®ç°ï¼Œä½†è¿™ä¼šä½¿ä»£ç ä¸å¤ªæ¸…æ™°ã€‚
- en: 'The rest of the function is the same: we mask completed episodes and compute
    the mean squared error (MSE) loss between Q-values predicted by the network and
    approximated Q-values.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å‡½æ•°çš„å…¶ä½™éƒ¨åˆ†ä¸ä¹‹ç›¸åŒï¼šæˆ‘ä»¬é®ç›–å·²å®Œæˆçš„å‰§é›†ï¼Œå¹¶è®¡ç®—ç½‘ç»œé¢„æµ‹çš„Qå€¼ä¸è¿‘ä¼¼Qå€¼ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰æŸå¤±ã€‚
- en: 'The last function that we consider calculates the values of our held-out state:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è€ƒè™‘çš„æœ€åä¸€ä¸ªå‡½æ•°è®¡ç®—äº†æˆ‘ä»¬ä¿ç•™çŠ¶æ€çš„å€¼ï¼š
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'There is nothing complicated here: we just split our held-out states array
    into equal chunks and pass every chunk to the network to obtain action values.
    From those values, we choose the action with the largest value (for every state)
    and calculate the mean of such values. As our array with states is fixed for the
    whole training process, and this array is large enough (in the code, we store
    1,000 states), we can compare the dynamics of this mean value in both DQN variants.
    The rest of the 03_dqn_double.py file is almost the same; the two differences
    are usage of our tweaked loss function and keeping a randomly sampled 1,000 states
    for periodical evaluation. This happens in the process_batch function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œå¹¶æ²¡æœ‰ä»€ä¹ˆå¤æ‚çš„å†…å®¹ï¼šæˆ‘ä»¬åªæ˜¯å°†ä¿ç•™çš„çŠ¶æ€æ•°ç»„åˆ’åˆ†æˆç›¸ç­‰çš„å—ï¼Œå¹¶å°†æ¯ä¸ªå—ä¼ é€’ç»™ç½‘ç»œä»¥è·å¾—åŠ¨ä½œå€¼ã€‚ä»è¿™äº›å€¼ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€å¤§å€¼çš„åŠ¨ä½œï¼ˆå¯¹äºæ¯ä¸ªçŠ¶æ€ï¼‰ï¼Œå¹¶è®¡ç®—è¿™äº›å€¼çš„å¹³å‡å€¼ã€‚ç”±äºæˆ‘ä»¬çš„çŠ¶æ€æ•°ç»„åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”è¿™ä¸ªæ•°ç»„è¶³å¤Ÿå¤§ï¼ˆåœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬å­˜å‚¨äº†1,000ä¸ªçŠ¶æ€ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥æ¯”è¾ƒè¿™ä¸¤ä¸ª
    DQN å˜ä½“ä¸­çš„å‡å€¼åŠ¨æ€ã€‚03_dqn_double.py æ–‡ä»¶ä¸­çš„å…¶ä½™éƒ¨åˆ†å‡ ä¹ç›¸åŒï¼›ä¸¤ä¸ªä¸åŒä¹‹å¤„æ˜¯ä½¿ç”¨äº†æˆ‘ä»¬è°ƒæ•´è¿‡çš„æŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”å®šæœŸè¯„ä¼°æ—¶ä¿æŒäº†éšæœºæŠ½å–çš„1,000ä¸ªçŠ¶æ€ã€‚è¿™ä¸€è¿‡ç¨‹å‘ç”Ÿåœ¨
    process_batch å‡½æ•°ä¸­ï¼š
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Results
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: My experiments show that with common hyperparameters, double DQN has a negative
    effect on reward dynamics. Sometimes, double DQN leads to better initial dynamics
    and the trained agent learns how to win more games faster, but reaching the end
    reward boundary takes longer. You can perform your own experiment on other games
    or try parameters from the original paper.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„å®éªŒè¡¨æ˜ï¼Œä½¿ç”¨å¸¸è§çš„è¶…å‚æ•°æ—¶ï¼ŒåŒé‡ DQN å¯¹å¥–åŠ±åŠ¨æ€æœ‰è´Ÿé¢å½±å“ã€‚æœ‰æ—¶ï¼ŒåŒé‡ DQN ä¼šå¯¼è‡´æ›´å¥½çš„åˆå§‹åŠ¨æ€ï¼Œè®­ç»ƒçš„æ™ºèƒ½ä½“å­¦ä¼šå¦‚ä½•æ›´å¿«åœ°èµ¢å¾—æ›´å¤šçš„æ¸¸æˆï¼Œä½†è¾¾åˆ°æœ€ç»ˆå¥–åŠ±è¾¹ç•Œéœ€è¦æ›´é•¿æ—¶é—´ã€‚ä½ å¯ä»¥åœ¨å…¶ä»–æ¸¸æˆä¸Šè¿›è¡Œè‡ªå·±çš„å®éªŒï¼Œæˆ–è€…å°è¯•åŸå§‹è®ºæ–‡ä¸­çš„å‚æ•°ã€‚
- en: 'The following are reward charts from the experiment where double DQN was a
    bit better than the baseline version:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å®éªŒä¸­çš„å¥–åŠ±å›¾è¡¨ï¼Œå…¶ä¸­åŒé‡ DQN ç¨å¾®ä¼˜äºåŸºçº¿ç‰ˆæœ¬ï¼š
- en: '![PIC](img/B22150_08_08.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_08.png)'
- en: 'FigureÂ 8.8: Reward dynamics for double and baseline DQN'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.8ï¼šåŒé‡ DQN å’ŒåŸºçº¿ DQN çš„å¥–åŠ±åŠ¨æ€
- en: Besides the standard metrics, the example also outputs the mean value for the
    held-out set of states, which are shown in FigureÂ [8.9](#x1-137004r9).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æ ‡å‡†åº¦é‡å¤–ï¼Œç¤ºä¾‹è¿˜è¾“å‡ºäº†ä¿ç•™çŠ¶æ€é›†çš„å‡å€¼ï¼Œè¿™äº›å‡å€¼æ˜¾ç¤ºåœ¨å›¾ [8.9](#x1-137004r9) ä¸­ã€‚
- en: '![PIC](img/B22150_08_09.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_09.png)'
- en: 'FigureÂ 8.9: Values predicted by the network for held-out states'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.9ï¼šç½‘ç»œé¢„æµ‹çš„ä¿ç•™çŠ¶æ€çš„å€¼
- en: As you can see, the basic DQN does an overestimation of values, so values decrease
    after a certain level. In contrast, the double DQN grows more consistently. In
    my experiments, the double DQN has only a small effect on the training time, but
    this doesnâ€™t necessarily mean that the double DQN is useless, as Pong is a simple
    environment. In more complicated games, the double DQN could give better results.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼ŒåŸºæœ¬çš„ DQN ä¼šé«˜ä¼°å€¼ï¼Œå› æ­¤å€¼åœ¨è¾¾åˆ°æŸä¸€æ°´å¹³åä¼šä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒåŒé‡ DQN åˆ™å¢é•¿å¾—æ›´åŠ ç¨³å®šã€‚åœ¨æˆ‘çš„å®éªŒä¸­ï¼ŒåŒé‡ DQN å¯¹è®­ç»ƒæ—¶é—´çš„å½±å“å¾ˆå°ï¼Œä½†è¿™å¹¶ä¸ä¸€å®šæ„å‘³ç€åŒé‡
    DQN æ²¡æœ‰ç”¨ï¼Œå› ä¸º Pong æ˜¯ä¸€ä¸ªç®€å•çš„ç¯å¢ƒã€‚åœ¨æ›´å¤æ‚çš„æ¸¸æˆä¸­ï¼ŒåŒé‡ DQN å¯èƒ½ä¼šç»™å‡ºæ›´å¥½çš„ç»“æœã€‚
- en: Hyperparameter tuning
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒèŠ‚
- en: The tuning of hyperparameters also wasnâ€™t very successful for the double DQN.
    After 30 trials, the best values for the learning rate and gamma were able to
    solve Pong in 412 games, which is worse than the baseline DQN.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŒé‡ DQNï¼Œè¶…å‚æ•°è°ƒèŠ‚ä¹Ÿä¸æ˜¯ç‰¹åˆ«æˆåŠŸã€‚ç»è¿‡30æ¬¡å®éªŒåï¼Œå­¦ä¹ ç‡å’Œgammaçš„æœ€ä½³å€¼èƒ½åœ¨412å±€æ¸¸æˆå†…è§£å†³ Pong é—®é¢˜ï¼Œä½†è¿™æ¯”åŸºçº¿ DQN æ›´å·®ã€‚
- en: Noisy networks
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å™ªå£°ç½‘ç»œ
- en: 'The next improvement that we are going to look at addresses another RL problem:
    exploration of the environment. The paper that we will draw from is called Noisy
    networks for exploration [[For+17](#)] and it has a very simple idea for learning
    exploration characteristics during training instead of having a separate schedule
    related to exploration.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ”¹è¿›æ˜¯é’ˆå¯¹å¦ä¸€ä¸ª RL é—®é¢˜ï¼šç¯å¢ƒæ¢ç´¢ã€‚æˆ‘ä»¬å°†å‚è€ƒçš„è®ºæ–‡å«åšã€ŠNoisy networks for explorationã€‹[[For+17](#)]ï¼Œå®ƒæå‡ºäº†ä¸€ä¸ªéå¸¸ç®€å•çš„æƒ³æ³•ï¼Œå³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ æ¢ç´¢ç‰¹å¾ï¼Œè€Œä¸æ˜¯ä¾èµ–ä¸æ¢ç´¢ç›¸å…³çš„ç‹¬ç«‹è°ƒåº¦ã€‚
- en: A classical DQN achieves exploration by choosing random actions with a specially
    defined hyperparameter ğœ–, which is slowly decreased over time from 1.0 (fully
    random actions) to some small ratio of 0.1 or 0.02\. This process works well for
    simple environments with short episodes, without much non-stationarity during
    the game; but even in such simple cases, it requires tuning to make the training
    processes efficient.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç»å…¸çš„ DQN é€šè¿‡é€‰æ‹©éšæœºåŠ¨ä½œæ¥å®ç°æ¢ç´¢ï¼Œè¿™ä¾èµ–äºä¸€ä¸ªç‰¹åˆ«å®šä¹‰çš„è¶…å‚æ•°ğœ–ï¼Œè¯¥è¶…å‚æ•°ä¼šéšç€æ—¶é—´çš„æ¨ç§»ä»1.0ï¼ˆå®Œå…¨éšæœºåŠ¨ä½œï¼‰é€æ¸é™ä½è‡³ä¸€ä¸ªè¾ƒå°çš„æ¯”ç‡ï¼Œä¾‹å¦‚0.1æˆ–0.02ã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨ç®€å•çš„ç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶æ˜¯åœ¨æ¸¸æˆä¸­æ²¡æœ‰å¤ªå¤šéå¹³ç¨³æ€§çš„çŸ­æœŸå›åˆå†…ï¼›ä½†æ˜¯å³ä½¿æ˜¯åœ¨è¿™äº›ç®€å•çš„æƒ…å†µä¸‹ï¼Œä¹Ÿéœ€è¦è°ƒå‚æ¥æé«˜è®­ç»ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚
- en: In the Noisy Networks paper, the authors proposed a quite simple solution that,
    nevertheless, works well. They add noise to the weights of fully connected layers
    of the network and adjust the parameters of this noise during training using backpropagation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ã€Šå™ªå£°ç½‘ç»œã€‹è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªç›¸å½“ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Œå°½ç®¡å¦‚æ­¤ï¼Œå®ƒä»ç„¶è¡¨ç°å¾—éå¸¸æœ‰æ•ˆã€‚ä»–ä»¬å‘ç½‘ç»œçš„å…¨è¿æ¥å±‚çš„æƒé‡ä¸­æ·»åŠ å™ªå£°ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡åå‘ä¼ æ’­è°ƒæ•´è¿™äº›å™ªå£°çš„å‚æ•°ã€‚
- en: This method shouldnâ€™t be confused with â€œthe network decides where to explore
    more,â€ which is a much more complex approach that also has widespread support
    (for example, see articles about intrinsic motivation and count-based exploration
    methods [[Ost+17](#)], [[Mar+17](#)]). We will discuss advanced exploration techniques
    in ChapterÂ [21](ch025.xhtml#x1-39100021).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ä¸åº”ä¸â€œç½‘ç»œå†³å®šåœ¨å“ªäº›åœ°æ–¹æ¢ç´¢æ›´å¤šâ€æ··æ·†ï¼Œè¿™æ˜¯ä¸€ç§æ›´åŠ å¤æ‚çš„æ–¹æ³•ï¼Œå¹¶ä¸”å¾—åˆ°äº†å¹¿æ³›çš„æ”¯æŒï¼ˆä¾‹å¦‚ï¼Œå‚è§å…³äºå†…åœ¨åŠ¨æœºå’ŒåŸºäºè®¡æ•°çš„æ¢ç´¢æ–¹æ³•çš„æ–‡ç« [[Ost+17](#)]ï¼Œ[
    [Mar+17](#) ]ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬[21](ch025.xhtml#x1-39100021)ç« è®¨è®ºé«˜çº§æ¢ç´¢æŠ€æœ¯ã€‚
- en: 'The authors proposed two ways of adding the noise, both of which work according
    to their experiments, but they have different computational overheads:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æå‡ºäº†ä¸¤ç§æ·»åŠ å™ªå£°çš„æ–¹å¼ï¼Œå®éªŒè¡¨æ˜è¿™ä¸¤ç§æ–¹æ³•éƒ½æœ‰æ•ˆï¼Œä½†å®ƒä»¬æœ‰ä¸åŒçš„è®¡ç®—å¼€é”€ï¼š
- en: 'Independent Gaussian noise: For every weight in a fully connected layer, we
    have a random value that we draw from the normal distribution. Parameters of the
    noise, Î¼ and Ïƒ, are stored inside the layer and get trained using backpropagation
    in the same way that we train weights of the standard linear layer. The output
    of such a â€œnoisy layerâ€ is calculated in the same way as in a linear layer.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‹¬ç«‹é«˜æ–¯å™ªå£°ï¼šå¯¹äºæ¯ä¸ªå…¨è¿æ¥å±‚çš„æƒé‡ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªä»æ­£æ€åˆ†å¸ƒä¸­æŠ½å–çš„éšæœºå€¼ã€‚å™ªå£°çš„å‚æ•°Î¼å’ŒÏƒå­˜å‚¨åœ¨è¯¥å±‚å†…ï¼Œå¹¶é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒï¼Œå°±åƒè®­ç»ƒæ ‡å‡†çº¿æ€§å±‚çš„æƒé‡ä¸€æ ·ã€‚è¿™ç§â€œå™ªå£°å±‚â€çš„è¾“å‡ºè®¡ç®—æ–¹å¼ä¸çº¿æ€§å±‚ç›¸åŒã€‚
- en: 'Factorized Gaussian noise: To minimize the number of random values to be sampled,
    the authors proposed keeping only two random vectors: one with the size of the
    input and another with the size of the output of the layer. Then, a random matrix
    for the layer is created by calculating the outer product of the vectors.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†è§£é«˜æ–¯å™ªå£°ï¼šä¸ºäº†æœ€å°åŒ–éœ€è¦é‡‡æ ·çš„éšæœºå€¼æ•°é‡ï¼Œä½œè€…å»ºè®®åªä¿ç•™ä¸¤ä¸ªéšæœºå‘é‡ï¼šä¸€ä¸ªæ˜¯è¾“å…¥å¤§å°ï¼Œå¦ä¸€ä¸ªæ˜¯å±‚çš„è¾“å‡ºå¤§å°ã€‚ç„¶åï¼Œé€šè¿‡è®¡ç®—è¿™ä¸¤ä¸ªå‘é‡çš„å¤–ç§¯ï¼Œåˆ›å»ºå±‚çš„éšæœºçŸ©é˜µã€‚
- en: Implementation
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: In PyTorch, both methods can be easily implemented in a very straightforward
    way. What we need to do is create our own custom nn.Linear layer with weights
    calculated as w[i,j] = Î¼[i,j] + Ïƒ[i,j] â‹…ğœ–[i,j], where Î¼ and Ïƒ are trainable parameters
    and ğœ– âˆ¼ğ’©(0,1) is random noise sampled from the normal distribution after every
    optimization step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨PyTorchä¸­ï¼Œä¸¤ç§æ–¹æ³•éƒ½å¯ä»¥éå¸¸ç›´æ¥åœ°å®ç°ã€‚æˆ‘ä»¬éœ€è¦åšçš„æ˜¯åˆ›å»ºè‡ªå®šä¹‰çš„nn.Linearå±‚ï¼Œæƒé‡è®¡ç®—æ–¹å¼ä¸ºw[i,j] = Î¼[i,j] + Ïƒ[i,j]
    â‹…ğœ–[i,j]ï¼Œå…¶ä¸­Î¼å’ŒÏƒæ˜¯å¯è®­ç»ƒå‚æ•°ï¼Œğœ–âˆ¼ğ’©(0,1)æ˜¯æ¯æ¬¡ä¼˜åŒ–æ­¥éª¤åä»æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·çš„éšæœºå™ªå£°ã€‚
- en: 'Previous editions of the book used my implementation of both methods, but now
    weâ€™ll simply use the implementation from the popular TorchRL library I mentioned
    in ChapterÂ [7](ch011.xhtml#x1-1070007). Letâ€™s take a look at relevant parts of
    the implementation (the full code can be found in torchrl/modules/models/exploration.py
    in the TorchRL repository). The following is the constructor of the NoisyLinear
    class, which creates all the parameters we need to optimize:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„æ—©æœŸç‰ˆæœ¬ä½¿ç”¨äº†æˆ‘è‡ªå·±å®ç°çš„è¿™ä¸¤ç§æ–¹æ³•ï¼Œä½†ç°åœ¨æˆ‘ä»¬å°†ç›´æ¥ä½¿ç”¨æˆ‘åœ¨ç¬¬[7](ch011.xhtml#x1-1070007)ç« æåˆ°çš„æµè¡ŒTorchRLåº“ä¸­çš„å®ç°ã€‚æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å®ç°çš„ç›¸å…³éƒ¨åˆ†ï¼ˆå®Œæ•´ä»£ç å¯ä»¥åœ¨TorchRLä»“åº“ä¸­çš„torchrl/modules/models/exploration.pyä¸­æ‰¾åˆ°ï¼‰ã€‚ä»¥ä¸‹æ˜¯NoisyLinearç±»çš„æ„é€ å‡½æ•°ï¼Œå®ƒåˆ›å»ºäº†æˆ‘ä»¬éœ€è¦ä¼˜åŒ–çš„æ‰€æœ‰å‚æ•°ï¼š
- en: '[PRE27]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the constructor, we create matrices for Î¼ and Ïƒ. This implementation inherits
    from torch.nn.Linear, but calls the nn.Module.__init__() method, so normal Linear
    weights and bias buffers are not created.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬ä¸ºÎ¼å’ŒÏƒåˆ›å»ºäº†çŸ©é˜µã€‚æ­¤å®ç°ç»§æ‰¿è‡ªtorch.nn.Linearï¼Œä½†è°ƒç”¨äº†nn.Module.__init__()æ–¹æ³•ï¼Œå› æ­¤ä¸ä¼šåˆ›å»ºæ ‡å‡†Linearæƒé‡å’Œåç½®ç¼“å†²åŒºã€‚
- en: To make new matrices trainable, we need to wrap their tensors in an nn.Parameter.
    The register_buffer method creates a tensor in the network that wonâ€™t be updated
    during backpropagation, but will be handled by the nn.Module machinery (for example,
    it will be copied to the GPU with the cuda() call). An extra parameter and buffer
    are created for the bias of the layer. At the end, we call the reset_parameters()
    and reset_noise() methods, which perform the initialization of the created trainable
    parameters and the buffer with the epsilon value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿æ–°çš„çŸ©é˜µå¯è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬çš„å¼ é‡åŒ…è£…åœ¨nn.Parameterä¸­ã€‚register_bufferæ–¹æ³•åœ¨ç½‘ç»œä¸­åˆ›å»ºä¸€ä¸ªä¸ä¼šåœ¨åå‘ä¼ æ’­æœŸé—´æ›´æ–°çš„å¼ é‡ï¼Œä½†ä¼šç”±nn.Moduleæœºåˆ¶å¤„ç†ï¼ˆä¾‹å¦‚ï¼Œå®ƒä¼šé€šè¿‡cuda()è°ƒç”¨è¢«å¤åˆ¶åˆ°GPUï¼‰ã€‚ä¸ºå±‚çš„åç½®åˆ›å»ºäº†é¢å¤–çš„å‚æ•°å’Œç¼“å†²åŒºã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨reset_parameters()å’Œreset_noise()æ–¹æ³•ï¼Œæ‰§è¡Œåˆ›å»ºçš„å¯è®­ç»ƒå‚æ•°å’Œå¸¦æœ‰epsilonå€¼çš„ç¼“å†²åŒºçš„åˆå§‹åŒ–ã€‚
- en: 'In the following three methods, we initialize the trainable parameters Î¼ and
    Ïƒ according to the paper:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä¸‰ä¸ªæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®è®ºæ–‡åˆå§‹åŒ–å¯è®­ç»ƒå‚æ•°Î¼å’ŒÏƒï¼š
- en: '[PRE28]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The matrix for Î¼ is initialized with uniform random values. The initial value
    for Ïƒ is constant depending on the count of neurons in the layer.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Î¼çš„çŸ©é˜µåˆå§‹åŒ–ä¸ºå‡åŒ€éšæœºå€¼ã€‚Ïƒçš„åˆå§‹å€¼æ˜¯å¸¸é‡ï¼Œå–å†³äºå±‚ä¸­ç¥ç»å…ƒçš„æ•°é‡ã€‚
- en: 'For the noise initialization, factorized Gaussian noise is used â€“ we sample
    two random vectors and calculate the outer product to get the matrix for ğœ–. The
    outer product is a linear algebra operation when two vectors of the same size
    are producing the square matrix filled with product of all combination of each
    vectorâ€™s element. The rest is simple: we redefine the weight and bias properties,
    which are expected in nn.Linear layer, so NoisyLinear could be used everywhere
    nn.Linear is used:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå™ªå£°åˆå§‹åŒ–ï¼Œä½¿ç”¨äº†å› å¼åˆ†è§£é«˜æ–¯å™ªå£°â€”â€”æˆ‘ä»¬é‡‡æ ·ä¸¤ä¸ªéšæœºå‘é‡å¹¶è®¡ç®—å¤–ç§¯ä»¥è·å¾—ğœ–çš„çŸ©é˜µã€‚å¤–ç§¯æ˜¯ä¸€ä¸ªçº¿æ€§ä»£æ•°æ“ä½œï¼Œå½“ä¸¤ä¸ªå¤§å°ç›¸åŒçš„å‘é‡äº§ç”Ÿä¸€ä¸ªå¡«å……äº†æ¯ä¸ªå‘é‡å…ƒç´ ç»„åˆä¹˜ç§¯çš„æ–¹é˜µæ—¶å°±ä¼šå‘ç”Ÿã€‚å…¶ä½™çš„å¾ˆç®€å•ï¼šæˆ‘ä»¬é‡æ–°å®šä¹‰æƒé‡å’Œåç½®å±æ€§ï¼Œè¿™äº›å±æ€§åœ¨nn.Linearå±‚ä¸­æ˜¯é¢„æœŸçš„ï¼Œå› æ­¤NoisyLinearå¯ä»¥åœ¨ä»»ä½•ä½¿ç”¨nn.Linearçš„åœ°æ–¹ä½¿ç”¨ï¼š
- en: '[PRE29]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This implementation is simple, but has one very subtle nuance â€” the ğœ– values
    are not updated after every optimization step (and it is not mentioned in the
    documentation). This issue is already reported in the TorchRL repo, but for the
    current stable release, we have to call the reset_noise() method explicitly. Hopefully,
    it will be fixed and the NoisyLinear layer will update the noise automatically.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå®ç°å¾ˆç®€å•ï¼Œä½†æœ‰ä¸€ä¸ªéå¸¸å¾®å¦™çš„ç»†èŠ‚â€”â€”ğœ–å€¼åœ¨æ¯æ¬¡ä¼˜åŒ–æ­¥éª¤åå¹¶ä¸ä¼šæ›´æ–°ï¼ˆæ–‡æ¡£ä¸­æ²¡æœ‰æåˆ°è¿™ä¸€ç‚¹ï¼‰ã€‚è¿™ä¸ªé—®é¢˜å·²ç»åœ¨TorchRLä»“åº“ä¸­æŠ¥å‘Šï¼Œä½†åœ¨å½“å‰ç¨³å®šç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»æ˜¾å¼è°ƒç”¨reset_noise()æ–¹æ³•ã€‚å¸Œæœ›è¿™ä¸ªé—®é¢˜èƒ½å¾—åˆ°ä¿®å¤ï¼ŒNoisyLinearå±‚èƒ½å¤Ÿè‡ªåŠ¨æ›´æ–°å™ªå£°ã€‚
- en: From the implementation point of view, thatâ€™s it. What we now need to do to
    turn the classic DQN into a noisy network variant is just replace nn.Linear (which
    are the two last layers in our DQN network) with the NoisyLinear layer. Of course,
    you have to remove all the code related to the epsilon-greedy strategy.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å®ç°è§’åº¦æ¥çœ‹ï¼Œå°±æ˜¯è¿™æ ·ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯å°†ç»å…¸çš„DQNè½¬æ¢ä¸ºå™ªå£°ç½‘ç»œå˜ä½“ï¼Œåªéœ€å°†nn.Linearï¼ˆè¿™æ˜¯æˆ‘ä»¬DQNç½‘ç»œä¸­çš„æœ€åä¸¤å±‚ï¼‰æ›¿æ¢ä¸ºNoisyLinearå±‚ã€‚å½“ç„¶ï¼Œæ‚¨éœ€è¦ç§»é™¤ä¸epsilon-greedyç­–ç•¥ç›¸å…³çš„æ‰€æœ‰ä»£ç ã€‚
- en: To check the internal noise level during training, we can monitor the signal-to-noise
    ratio (SNR) of our noisy layers, which is RMS(Î¼)âˆ•RMS(Ïƒ), where RMS is the root
    mean square of the corresponding weights. In our case, the SNR shows how many
    times the stationary component of the noisy layer is larger than the injected
    noise.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åœ¨è®­ç»ƒæœŸé—´æ£€æŸ¥å†…éƒ¨å™ªå£°æ°´å¹³ï¼Œæˆ‘ä»¬å¯ä»¥ç›‘æ§å™ªå£°å±‚çš„ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸ºRMS(Î¼)âˆ•RMS(Ïƒ)ï¼Œå…¶ä¸­RMSæ˜¯ç›¸åº”æƒé‡çš„å‡æ–¹æ ¹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒSNRæ˜¾ç¤ºå™ªå£°å±‚çš„é™æ€æˆåˆ†æ¯”æ³¨å…¥å™ªå£°å¤§å¤šå°‘å€ã€‚
- en: Results
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'After the training, the TensorBoard charts show much better training dynamics.
    The model was able to reach the mean score of 18 after 250 games, which is an
    improvement in comparison to 350 for the baseline DQN. But because of extra operations
    required for noisy networks, their training is a bit slower (194 FPS versus 240
    FPS for the baseline), so, time-wise, the difference is less impressive. But still,
    the results look good:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒåï¼ŒTensorBoardå›¾è¡¨æ˜¾ç¤ºå‡ºæ›´å¥½çš„è®­ç»ƒåŠ¨æ€ã€‚æ¨¡å‹åœ¨250å±€æ¸¸æˆåè¾¾åˆ°äº†å¹³å‡å¾—åˆ†18ï¼Œç›¸æ¯”åŸºå‡†DQNçš„350åˆ†æœ‰æ‰€æå‡ã€‚ä½†ç”±äºå™ªå£°ç½‘ç»œéœ€è¦é¢å¤–çš„æ“ä½œï¼Œå®ƒä»¬çš„è®­ç»ƒé€Ÿåº¦ç¨æ…¢ï¼ˆ194
    FPSå¯¹æ¯”åŸºå‡†çš„240 FPSï¼‰ï¼Œæ‰€ä»¥åœ¨æ—¶é—´ä¸Šï¼Œå·®å¼‚ä¸é‚£ä¹ˆå¼•äººæ³¨ç›®ã€‚ä½†ä»ç„¶ï¼Œç»“æœçœ‹èµ·æ¥å¾ˆå¥½ï¼š
- en: '![PIC](img/B22150_08_10.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_10.png)'
- en: 'FigureÂ 8.10: Noisy networks compared to the baseline DQN'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.10ï¼šä¸åŸºå‡†DQNç›¸æ¯”çš„å™ªå£°ç½‘ç»œ
- en: After checking the SNR chart (FigureÂ [8.11](#x1-141004r11)), you may notice
    that both layersâ€™ noise levels have decreased very quickly.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸ¥çœ‹ä¿¡å™ªæ¯”ï¼ˆSNRï¼‰å›¾è¡¨ï¼ˆå›¾[8.11](#x1-141004r11)ï¼‰åï¼Œæ‚¨å¯èƒ½ä¼šæ³¨æ„åˆ°ä¸¤ä¸ªå±‚çš„å™ªå£°æ°´å¹³éƒ½è¿…é€Ÿä¸‹é™äº†ã€‚
- en: '![PIC](img/B22150_08_11.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_11.png)'
- en: 'FigureÂ 8.11: SNR change in layer 1 (left) and layer 2 (right)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.11ï¼šç¬¬1å±‚ï¼ˆå·¦ï¼‰å’Œç¬¬2å±‚ï¼ˆå³ï¼‰çš„SNRå˜åŒ–
- en: The first layer has gone from ![1 2](img/eq31.png) to almost ![-1- 2.6](img/eq32.png)
    ratio of noise. The second layer is even more interesting, as its noise level
    decreased from ![1 4](img/eq33.png) in the beginning to ![1- 16](img/eq34.png),
    but after 450K frames (roughly the same time as when raw rewards climbed close
    to the 20 score), the level of noise in the last layer started to increase again,
    pushing the agent to explore the environment more. This makes a lot of sense,
    as after reaching high score levels, the agent basically knows how to play at
    a good level, but still needs to â€œpolishâ€ its actions to improve the results even
    more.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€å±‚çš„å™ªå£°æ¯”ç‡å·²ç»ä»![1 2](img/eq31.png)å˜åŒ–åˆ°æ¥è¿‘![ -1- 2.6](img/eq32.png)ã€‚ç¬¬äºŒå±‚æ›´æœ‰è¶£ï¼Œå› ä¸ºå®ƒçš„å™ªå£°æ°´å¹³ä»æœ€åˆçš„![1
    4](img/eq33.png)é™ä½åˆ°äº†![1- 16](img/eq34.png)ï¼Œä½†åœ¨450Kå¸§ä¹‹åï¼ˆå¤§è‡´ä¸åŸå§‹å¥–åŠ±æ¥è¿‘20åˆ†æ—¶çš„æ—¶é—´ç›¸åŒï¼‰ï¼Œæœ€åä¸€å±‚çš„å™ªå£°æ°´å¹³å¼€å§‹å†æ¬¡ä¸Šå‡ï¼Œæ¨åŠ¨ä»£ç†æ›´æ·±å…¥åœ°æ¢ç´¢ç¯å¢ƒã€‚è¿™æ˜¯éå¸¸æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºåœ¨è¾¾åˆ°é«˜åˆ†æ°´å¹³åï¼Œä»£ç†åŸºæœ¬ä¸Šå·²ç»çŸ¥é“å¦‚ä½•ç©å¾—å¾ˆå¥½ï¼Œä½†ä»ç„¶éœ€è¦â€œæ‰“ç£¨â€è‡ªå·±çš„è¡ŒåŠ¨ï¼Œä»¥è¿›ä¸€æ­¥æé«˜ç»“æœã€‚
- en: Hyperparameter tuning
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: 'After the tuning, the best set of parameters was able to solve the game after
    273 rounds, which is an improvement over the baseline:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒä¼˜åï¼Œæœ€ä½³å‚æ•°é›†èƒ½å¤Ÿåœ¨273è½®åè§£å†³æ¸¸æˆé—®é¢˜ï¼Œç›¸æ¯”åŸºå‡†æ–¹æ³•æœ‰äº†æ”¹è¿›ï¼š
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following are charts comparing the reward dynamics and steps for tuned
    baseline DQN and tuned noisy networks:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯è°ƒä¼˜åçš„åŸºå‡†DQNä¸è°ƒä¼˜åçš„å™ªå£°ç½‘ç»œå¥–åŠ±åŠ¨æ€å’Œæ­¥æ•°çš„æ¯”è¾ƒå›¾ï¼š
- en: '![PIC](img/B22150_08_12.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_12.png)'
- en: 'FigureÂ 8.12: Comparison of tuned baseline DQN and tuned noisy network'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.12ï¼šè°ƒä¼˜åçš„åŸºå‡†DQNä¸è°ƒä¼˜åçš„å™ªå£°ç½‘ç»œæ¯”è¾ƒ
- en: 'On both charts, we see improvements introduced by noisy networks: it takes
    fewer games to reach a score of 21 and during the training, games have a smaller
    amount of steps.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸¤å¼ å›¾ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°å™ªå£°ç½‘ç»œå¸¦æ¥çš„æ”¹è¿›ï¼šè¾¾åˆ°21åˆ†æ‰€éœ€çš„æ¸¸æˆæ¬¡æ•°å‡å°‘ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¸¸æˆçš„æ­¥æ•°å‡å°‘ã€‚
- en: Prioritized replay buffer
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒº
- en: The next very useful idea on how to improve DQN training was proposed in 2015
    in the paper Prioritized experience replay [[Sch+15](#)]. This method tries to
    improve the efficiency of samples in the replay buffer by prioritizing those samples
    according to the training loss.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€é¡¹å…³äºå¦‚ä½•æ”¹è¿›DQNè®­ç»ƒçš„éå¸¸æœ‰ç”¨çš„æƒ³æ³•æ˜¯åœ¨2015å¹´æå‡ºçš„ï¼Œå‡ºç°åœ¨è®ºæ–‡ã€Šä¼˜å…ˆç»éªŒå›æ”¾ã€‹[[Sch+15](#)]ä¸­ã€‚è¿™ç§æ–¹æ³•å°è¯•é€šè¿‡æ ¹æ®è®­ç»ƒæŸå¤±å¯¹å›æ”¾ç¼“å†²åŒºä¸­çš„æ ·æœ¬è¿›è¡Œä¼˜å…ˆçº§æ’åºï¼Œä»è€Œæé«˜æ ·æœ¬çš„æ•ˆç‡ã€‚
- en: The basic DQN used the replay buffer to break the correlation between immediate
    transitions in our episodes. As we discussed in ChapterÂ [6](#), the examples we
    experience during the episode will be highly correlated, as most of the time,
    the environment is â€smoothâ€ and doesnâ€™t change much according to our actions.
    However, the stochastic gradient descent (SGD) method assumes that the data we
    use for training has an iid property. To solve this problem, the classic DQN method
    uses a large buffer of transitions, randomly and uniformly sampled to get the
    next training batch.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬çš„DQNä½¿ç”¨å›æ”¾ç¼“å†²åŒºæ¥æ‰“ç ´æˆ‘ä»¬å›åˆä¸­å³æ—¶è½¬ç§»ä¹‹é—´çš„ç›¸å…³æ€§ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬åœ¨å›åˆä¸­ç»å†çš„ç¤ºä¾‹ä¼šé«˜åº¦ç›¸å…³ï¼Œå› ä¸ºå¤§å¤šæ•°æ—¶å€™ï¼Œç¯å¢ƒæ˜¯â€œå¹³æ»‘â€çš„ï¼Œå¹¶ä¸”æ ¹æ®æˆ‘ä»¬çš„è¡ŒåŠ¨å˜åŒ–ä¸å¤§ã€‚ç„¶è€Œï¼Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰æ–¹æ³•å‡è®¾æˆ‘ä»¬ç”¨äºè®­ç»ƒçš„æ•°æ®å…·æœ‰ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰ç‰¹æ€§ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç»å…¸DQNæ–¹æ³•ä½¿ç”¨äº†ä¸€ä¸ªå¤§å®¹é‡çš„è½¬ç§»ç¼“å†²åŒºï¼Œå¹¶é€šè¿‡éšæœºå‡åŒ€é‡‡æ ·æ¥è·å–ä¸‹ä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡ã€‚
- en: The authors of the paper questioned this uniform random sample policy and proved
    that by assigning priorities to buffer samples, according to training loss and
    sampling the buffer proportional to those priorities, we can significantly improve
    convergence and the policy quality of the DQN. This methodâ€™s basic idea could
    be explained as â€œtrain more on data that surprises you.â€ The tricky point here
    is to keep the balance of training on an â€œunusualâ€ sample and training on the
    rest of the buffer. If we focus only on a small subset of the buffer, we can lose
    our i.i.d. property and simply overfit on this subset.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„ä½œè€…è´¨ç–‘äº†è¿™ç§å‡åŒ€éšæœºé‡‡æ ·ç­–ç•¥ï¼Œå¹¶è¯æ˜é€šè¿‡æ ¹æ®è®­ç»ƒæŸå¤±ç»™ç¼“å†²åŒºæ ·æœ¬åˆ†é…ä¼˜å…ˆçº§ï¼Œå¹¶æŒ‰ä¼˜å…ˆçº§æ¯”ä¾‹é‡‡æ ·ç¼“å†²åŒºæ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—æé«˜DQNçš„æ”¶æ•›æ€§å’Œç­–ç•¥è´¨é‡ã€‚è¯¥æ–¹æ³•çš„åŸºæœ¬æ€æƒ³å¯ä»¥ç”¨â€œå¯¹ä»¤ä½ æ„Ÿåˆ°æƒŠè®¶çš„æ•°æ®è¿›è¡Œæ›´å¤šè®­ç»ƒâ€æ¥è§£é‡Šã€‚è¿™é‡Œçš„å…³é”®ç‚¹æ˜¯ä¿æŒåœ¨â€œå¼‚å¸¸â€æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒä¸åœ¨ç¼“å†²åŒºå…¶ä½™éƒ¨åˆ†ä¸Šè®­ç»ƒä¹‹é—´çš„å¹³è¡¡ã€‚å¦‚æœæˆ‘ä»¬ä»…å…³æ³¨ç¼“å†²åŒºçš„ä¸€å°éƒ¨åˆ†æ ·æœ¬ï¼Œå¯èƒ½ä¼šä¸§å¤±ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰ç‰¹æ€§ï¼Œç®€å•åœ°åœ¨è¿™ä¸ªå­é›†ä¸Šè¿‡æ‹Ÿåˆã€‚
- en: From the mathematical point of view, the priority of every sample in the buffer
    is calculated as ![ pÎ± âˆ‘kipÎ±- k](img/eq35.png), where p[i] is the priority of
    the i-th sample in the buffer and Î± is the number that shows how much emphasis
    we give to the priority. If Î± = 0, our sampling will become uniform as in the
    classic DQN method. Larger values for Î± put more stress on samples with higher
    priority. So, itâ€™s another hyperparameter to tune, and the starting value of Î±
    proposed by the paper is 0.6.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œç¼“å†²åŒºä¸­æ¯ä¸ªæ ·æœ¬çš„ä¼˜å…ˆçº§è®¡ç®—å…¬å¼ä¸º ![ pÎ± âˆ‘kipÎ±- k](img/eq35.png)ï¼Œå…¶ä¸­ p[i] æ˜¯ç¼“å†²åŒºä¸­ç¬¬ i ä¸ªæ ·æœ¬çš„ä¼˜å…ˆçº§ï¼ŒÎ±
    æ˜¯è¡¨ç¤ºæˆ‘ä»¬å¯¹ä¼˜å…ˆçº§ç»™äºˆå¤šå°‘é‡è§†çš„å‚æ•°ã€‚å¦‚æœ Î± = 0ï¼Œæˆ‘ä»¬çš„é‡‡æ ·å°†åƒç»å…¸çš„ DQN æ–¹æ³•ä¸€æ ·å˜å¾—å‡åŒ€ã€‚è¾ƒå¤§çš„ Î± å€¼åˆ™ä¼šæ›´åŠ å¼ºè°ƒé«˜ä¼˜å…ˆçº§çš„æ ·æœ¬ã€‚å› æ­¤ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªéœ€è¦è°ƒèŠ‚çš„è¶…å‚æ•°ï¼Œè®ºæ–‡ä¸­å»ºè®®çš„
    Î± åˆå§‹å€¼ä¸º 0.6ã€‚
- en: There were several options proposed in the paper for how to define the priority,
    and the most popular is to make it proportional to the loss for this particular
    example in the Bellman update. New samples added to the buffer need to be assigned
    a maximum value of priority to be sure that they will be sampled soon.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡ä¸­æå‡ºäº†å‡ ç§å®šä¹‰ä¼˜å…ˆçº§çš„é€‰é¡¹ï¼Œå…¶ä¸­æœ€æµè¡Œçš„æ˜¯å°†å…¶ä¸è¿™ä¸ªç‰¹å®šæ ·æœ¬åœ¨è´å°”æ›¼æ›´æ–°ä¸­çš„æŸå¤±æˆæ¯”ä¾‹ã€‚æ–°åŠ å…¥ç¼“å†²åŒºçš„æ ·æœ¬éœ€è¦è¢«èµ‹äºˆä¸€ä¸ªæœ€å¤§ä¼˜å…ˆçº§å€¼ï¼Œä»¥ç¡®ä¿å®ƒä»¬èƒ½å°½å¿«è¢«é‡‡æ ·ã€‚
- en: By adjusting the priorities for the samples, we are introducing bias into our
    data distribution (we sample some transitions much more frequently than others),
    which we need to compensate for if SGD is to work. To get this result, the authors
    of the study used sample weights, which needed to be multiplied by the individual
    sample loss. The value of the weight for each sample is defined as w[i] = (N â‹…P(i))^(âˆ’Î²),
    where Î² is another hyperparameter that should be between 0 and 1.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è°ƒæ•´æ ·æœ¬çš„ä¼˜å…ˆçº§ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨æ•°æ®åˆ†å¸ƒä¸­å¼•å…¥åå·®ï¼ˆæˆ‘ä»¬æ¯”å…¶ä»–è½¬æ¢æ›´é¢‘ç¹åœ°é‡‡æ ·æŸäº›è½¬æ¢ï¼‰ï¼Œå¦‚æœå¸Œæœ›SGDèƒ½å¤Ÿæœ‰æ•ˆå·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™ç§åå·®è¿›è¡Œè¡¥å¿ã€‚ä¸ºäº†å¾—åˆ°è¿™ä¸ªç»“æœï¼Œç ”ç©¶çš„ä½œè€…ä½¿ç”¨äº†æ ·æœ¬æƒé‡ï¼Œè¿™äº›æƒé‡éœ€è¦ä¸å•ä¸ªæ ·æœ¬çš„æŸå¤±ç›¸ä¹˜ã€‚æ¯ä¸ªæ ·æœ¬çš„æƒé‡å€¼å®šä¹‰ä¸º
    w[i] = (N â‹…P(i))^(âˆ’Î²)ï¼Œå…¶ä¸­ Î² æ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°ï¼Œåº”è¯¥åœ¨0å’Œ1ä¹‹é—´ã€‚
- en: With Î² = 1, the bias introduced by the sampling is fully compensated for, but
    the authors showed that itâ€™s good for convergence to start with Î² between 0 and
    1 and slowly increase it to 1 during the training.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ Î² = 1 æ—¶ï¼Œé‡‡æ ·å¼•å…¥çš„åå·®å¾—åˆ°äº†å®Œå…¨è¡¥å¿ï¼Œä½†ä½œè€…è¡¨æ˜ï¼Œå¼€å§‹æ—¶å°† Î² è®¾ç½®åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¢åŠ åˆ° 1ï¼Œæœ‰åˆ©äºæ”¶æ•›ã€‚
- en: Implementation
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: 'To implement this method, we have to introduce certain changes in our code:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨ä»£ç ä¸­åšå‡ºä¸€äº›ç‰¹å®šçš„ä¿®æ”¹ï¼š
- en: First of all, we need a new replay buffer that will track priorities, sample
    a batch according to them, calculate weights, and let us update priorities after
    the loss has become known.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„é‡æ”¾ç¼“å†²åŒºï¼Œå®ƒå°†è·Ÿè¸ªä¼˜å…ˆçº§ã€æ ¹æ®ä¼˜å…ˆçº§é‡‡æ ·æ‰¹æ¬¡ã€è®¡ç®—æƒé‡ï¼Œå¹¶åœ¨æŸå¤±å€¼å·²çŸ¥åè®©æˆ‘ä»¬æ›´æ–°ä¼˜å…ˆçº§ã€‚
- en: The second change will be the loss function itself. Now we not only need to
    incorporate weights for every sample, but we need to pass loss values back to
    the replay buffer to adjust the priorities of the sampled transitions.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå˜åŒ–å°†æ˜¯æŸå¤±å‡½æ•°æœ¬èº«ã€‚ç°åœ¨æˆ‘ä»¬ä¸ä»…éœ€è¦ä¸ºæ¯ä¸ªæ ·æœ¬å¼•å…¥æƒé‡ï¼Œè¿˜éœ€è¦å°†æŸå¤±å€¼å›ä¼ åˆ°é‡æ”¾ç¼“å†²åŒºï¼Œä»¥è°ƒæ•´é‡‡æ ·è½¬æ¢çš„ä¼˜å…ˆçº§ã€‚
- en: In the main module, Chapter08/05_dqn_prio_replay.py, we have all those changes
    implemented. For the sake of simplicity, the new priority replay buffer class
    uses a very similar storage scheme to our previous replay buffer. Unfortunately,
    new requirements for prioritization make it impossible to implement sampling in
    ğ’ª(1) time (in other words, sampling time will grow with an increase in buffer
    size). If we are using simple lists, every time that we sample a new batch, we
    need to process all the priorities, which makes our sampling have ğ’ª(N) time complexity
    in proportion to the buffer size. Itâ€™s not a big deal if our buffer is small,
    such as 100k samples, but may become an issue for real-life large buffers of millions
    of transitions. There are other storage schemes that support efficient sampling
    in ğ’ª(log N) time, for example, using the segment tree data structure. There are
    different versions of such optimized buffers available in various libraries â€“
    for example, in TorchRL.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸»æ¨¡å— Chapter08/05_dqn_prio_replay.py ä¸­ï¼Œæˆ‘ä»¬å·²ç»å®ç°äº†æ‰€æœ‰è¿™äº›ä¿®æ”¹ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæ–°çš„ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºç±»ä½¿ç”¨ä¸æˆ‘ä»¬ä¹‹å‰çš„é‡æ”¾ç¼“å†²åŒºéå¸¸ç›¸ä¼¼çš„å­˜å‚¨æ–¹æ¡ˆã€‚ä¸å¹¸çš„æ˜¯ï¼Œæ–°çš„ä¼˜å…ˆçº§è¦æ±‚ä½¿å¾—æ— æ³•ä»¥
    ğ’ª(1) æ—¶é—´å¤æ‚åº¦å®ç°é‡‡æ ·ï¼ˆæ¢å¥è¯è¯´ï¼Œé‡‡æ ·æ—¶é—´å°†éšç€ç¼“å†²åŒºå¤§å°çš„å¢åŠ è€Œå¢é•¿ï¼‰ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„åˆ—è¡¨ï¼Œæ¯æ¬¡é‡‡æ ·æ–°çš„ä¸€æ‰¹æ ·æœ¬æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†æ‰€æœ‰ä¼˜å…ˆçº§ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬çš„é‡‡æ ·æ—¶é—´å¤æ‚åº¦ä¸ç¼“å†²åŒºå¤§å°æˆæ­£æ¯”ï¼Œè¾¾åˆ°
    ğ’ª(N)ã€‚å¦‚æœæˆ‘ä»¬çš„ç¼“å†²åŒºå¾ˆå°ï¼Œæ¯”å¦‚100kæ ·æœ¬ï¼Œè¿™å¹¶ä¸æ˜¯ä»€ä¹ˆå¤§é—®é¢˜ï¼Œä½†å¯¹äºç°å®ä¸­çš„å¤§å‹ç¼“å†²åŒºï¼Œæ ·æœ¬æ•°é‡è¾¾åˆ°æ•°ç™¾ä¸‡æ—¶ï¼Œè¿™å¯èƒ½æˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚æœ‰å…¶ä»–æ”¯æŒåœ¨ ğ’ª(log
    N) æ—¶é—´å†…è¿›è¡Œé«˜æ•ˆé‡‡æ ·çš„å­˜å‚¨æ–¹æ¡ˆï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨çº¿æ®µæ ‘æ•°æ®ç»“æ„ã€‚å„ç§åº“ä¸­éƒ½æœ‰è¿™äº›ä¼˜åŒ–åçš„ç¼“å†²åŒºç‰ˆæœ¬â€”â€”ä¾‹å¦‚ï¼ŒTorchRLä¸­å°±æœ‰ã€‚
- en: The PTAN library also provides an efficient prioritized replay buffer in the
    class ptan.experience.PrioritizedReplayBuffer. You can update the example to use
    the more efficient version and check the effect on training performance.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: PTANåº“è¿˜æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºï¼Œä½äºç±»ptan.experience.PrioritizedReplayBufferä¸­ã€‚æ‚¨å¯ä»¥æ›´æ–°ç¤ºä¾‹ï¼Œä½¿ç”¨æ›´é«˜æ•ˆçš„ç‰ˆæœ¬ï¼Œå¹¶æ£€æŸ¥å…¶å¯¹è®­ç»ƒæ€§èƒ½çš„å½±å“ã€‚
- en: But, for now, letâ€™s take a look at the naÃ¯ve version, whose source code you
    will find in lib/dqn_extra.py.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç°åœ¨è®©æˆ‘ä»¬å…ˆçœ‹çœ‹æœ´ç´ ç‰ˆæœ¬ï¼Œå…¶æºä»£ç å¯ä»¥åœ¨lib/dqn_extra.pyä¸­æ‰¾åˆ°ã€‚
- en: 'In the beginning, we define parameters for the Î² increase rate:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬å®šä¹‰äº†Î²å¢åŠ ç‡çš„å‚æ•°ï¼š
- en: '[PRE31]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Our beta will be changed from 0.4 to 1.0 during the first 100k frames.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„Î²å°†åœ¨å‰100kå¸§ä¸­ä»0.4å˜åŒ–åˆ°1.0ã€‚
- en: 'Next comes the prioritized replay buffer class:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºç±»ï¼š
- en: '[PRE32]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The class for the priority replay buffer inherits from the simple replay buffer
    in PTAN, which stores samples in a circular buffer (it allows us to keep a fixed
    amount of entries without reallocating the list). Our subclass uses a NumPy array
    to keep priorities.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºçš„ç±»ç»§æ‰¿è‡ªPTANä¸­çš„ç®€å•é‡æ”¾ç¼“å†²åŒºï¼Œè¯¥ç¼“å†²åŒºå°†æ ·æœ¬å­˜å‚¨åœ¨ä¸€ä¸ªå¾ªç¯ç¼“å†²åŒºä¸­ï¼ˆå®ƒå…è®¸æˆ‘ä»¬ä¿æŒå›ºå®šæ•°é‡çš„æ¡ç›®ï¼Œè€Œæ— éœ€é‡æ–°åˆ†é…åˆ—è¡¨ï¼‰ã€‚æˆ‘ä»¬çš„å­ç±»ä½¿ç”¨NumPyæ•°ç»„æ¥ä¿æŒä¼˜å…ˆçº§ã€‚
- en: 'The update_beta() method needs to be called periodically to increase beta according
    to a schedule. The populate() method needs to pull the given number of transitions
    from the ExperienceSource object and store them in the buffer:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦å®šæœŸè°ƒç”¨update_beta()æ–¹æ³•ï¼Œä»¥æ ¹æ®è®¡åˆ’å¢åŠ Î²å€¼ã€‚populate()æ–¹æ³•éœ€è¦ä»ExperienceSourceå¯¹è±¡ä¸­æå–ç»™å®šæ•°é‡çš„è½¬æ¢å¹¶å°†å…¶å­˜å‚¨åœ¨ç¼“å†²åŒºä¸­ï¼š
- en: '[PRE33]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As our storage for the transitions is implemented as a circular buffer, we
    have two different situations with this buffer:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„è½¬æ¢å­˜å‚¨å®ç°ä¸ºå¾ªç¯ç¼“å†²åŒºï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ­¤ç¼“å†²åŒºä¸­æœ‰ä¸¤ç§ä¸åŒçš„æƒ…å†µï¼š
- en: When our buffer hasnâ€™t reached the maximum capacity, we just need to append
    a new transition to the buffer.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬çš„ç¼“å†²åŒºå°šæœªè¾¾åˆ°æœ€å¤§å®¹é‡æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æ–°çš„è½¬æ¢è¿½åŠ åˆ°ç¼“å†²åŒºä¸­ã€‚
- en: If the buffer is already full, we need to overwrite the oldest transition, which
    is tracked by the pos class field, and adjust this position modulo bufferâ€™s size.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœç¼“å†²åŒºå·²ç»æ»¡äº†ï¼Œæˆ‘ä»¬éœ€è¦è¦†ç›–æœ€æ—§çš„è½¬æ¢ï¼Œè¯¥è½¬æ¢ç”±posç±»å­—æ®µè·Ÿè¸ªï¼Œå¹¶è°ƒæ•´è¯¥ä½ç½®ä¸ºç¼“å†²åŒºå¤§å°çš„æ¨¡ã€‚
- en: 'In the sample method, we need to convert priorities to probabilities using
    our Î± hyperparameter:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¤ºä¾‹æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æˆ‘ä»¬çš„Î±è¶…å‚æ•°å°†ä¼˜å…ˆçº§è½¬æ¢ä¸ºæ¦‚ç‡ï¼š
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then, using those probabilities, we sample our buffer to obtain a batch of
    samples:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä½¿ç”¨è¿™äº›æ¦‚ç‡ï¼Œæˆ‘ä»¬ä»ç¼“å†²åŒºä¸­é‡‡æ ·ï¼Œä»¥è·å¾—ä¸€æ‰¹æ ·æœ¬ï¼š
- en: '[PRE35]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As the last step, we calculate weights for samples in the batch:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬è®¡ç®—æ‰¹å¤„ç†ä¸­æ ·æœ¬çš„æƒé‡ï¼š
- en: '[PRE36]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This returns three objects: the batch, indices, and weights. Indices for batch
    samples are required to update priorities for sampled items.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°è¿”å›ä¸‰ä¸ªå¯¹è±¡ï¼šæ‰¹å¤„ç†ã€ç´¢å¼•å’Œæƒé‡ã€‚æ‰¹å¤„ç†æ ·æœ¬çš„ç´¢å¼•æ˜¯æ›´æ–°é‡‡æ ·é¡¹ç›®ä¼˜å…ˆçº§æ‰€å¿…éœ€çš„ã€‚
- en: 'The last function of the priority replay buffer allows us to update new priorities
    for the processed batch:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºçš„æœ€åä¸€ä¸ªå‡½æ•°å…è®¸æˆ‘ä»¬æ›´æ–°å¤„ç†è¿‡çš„æ‰¹æ¬¡çš„æ–°ä¼˜å…ˆçº§ï¼š
- en: '[PRE37]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Itâ€™s the responsibility of the caller to use this function with the calculated
    losses for the batch.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒç”¨è€…æœ‰è´£ä»»åœ¨æ‰¹å¤„ç†çš„æŸå¤±è®¡ç®—åä½¿ç”¨æ­¤å‡½æ•°ã€‚
- en: 'The next custom function that we have in our example is the loss calculation.
    As the MSELoss class in PyTorch doesnâ€™t support weights (which is understandable,
    as MSE is loss used in regression problems, but weighting of the samples is commonly
    utilized in classification losses), we need to calculate the MSE and explicitly
    multiply the result on the weights:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¤ºä¾‹ä¸­çš„ä¸‹ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ˜¯æŸå¤±è®¡ç®—ã€‚ç”±äºPyTorchä¸­çš„MSELossç±»ä¸æ”¯æŒæƒé‡ï¼ˆè¿™æ˜¯å¯ä»¥ç†è§£çš„ï¼Œå› ä¸ºMSEæ˜¯å›å½’é—®é¢˜ä¸­ä½¿ç”¨çš„æŸå¤±ï¼Œä½†æ ·æœ¬åŠ æƒé€šå¸¸ç”¨äºåˆ†ç±»æŸå¤±ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—MSEå¹¶æ˜¾å¼åœ°å°†ç»“æœä¸æƒé‡ç›¸ä¹˜ï¼š
- en: '[PRE38]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the last part of the loss calculation, we implement the same MSE loss but
    write our expression explicitly, rather than using the library. This allows us
    to take into account the weights of samples and keep individual loss values for
    every sample. Those values will be passed to the priority replay buffer to update
    priorities. A small value is added to every loss to handle the situation of zero
    loss value, which will lead to zero priority for an entry in the replay buffer.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸå¤±è®¡ç®—çš„æœ€åéƒ¨åˆ†ï¼Œæˆ‘ä»¬å®ç°äº†ç›¸åŒçš„MSEæŸå¤±ï¼Œä½†æ˜¾å¼åœ°å†™å‡ºäº†æˆ‘ä»¬çš„è¡¨è¾¾å¼ï¼Œè€Œä¸æ˜¯ä½¿ç”¨åº“å‡½æ•°ã€‚è¿™æ ·å¯ä»¥è€ƒè™‘æ ·æœ¬çš„æƒé‡ï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬ä¿æŒå•ç‹¬çš„æŸå¤±å€¼ã€‚è¿™äº›å€¼å°†ä¼ é€’ç»™ä¼˜å…ˆçº§é‡æ”¾ç¼“å†²åŒºä»¥æ›´æ–°ä¼˜å…ˆçº§ã€‚æ¯ä¸ªæŸå¤±å€¼éƒ½ä¼šåŠ ä¸Šä¸€ä¸ªå°å€¼ï¼Œä»¥å¤„ç†æŸå¤±å€¼ä¸ºé›¶çš„æƒ…å†µï¼Œè¿™ç§æƒ…å†µä¼šå¯¼è‡´é‡æ”¾ç¼“å†²åŒºä¸­æ¡ç›®çš„ä¼˜å…ˆçº§ä¸ºé›¶ã€‚
- en: 'In the main section of our program, we have only two updates: the creation
    of the replay buffer and our processing function. Buffer creation is straightforward,
    so we will only take a look at a new processing function:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç¨‹åºçš„ä¸»è¦éƒ¨åˆ†ï¼Œåªæœ‰ä¸¤ä¸ªæ›´æ–°ï¼šå›æ”¾ç¼“å†²åŒºçš„åˆ›å»ºå’Œæˆ‘ä»¬çš„å¤„ç†å‡½æ•°ã€‚ç¼“å†²åŒºåˆ›å»ºå¾ˆç®€å•ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦çœ‹ä¸€ä¸‹æ–°çš„å¤„ç†å‡½æ•°ï¼š
- en: '[PRE39]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'There are several changes here:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰å‡ ä¸ªå˜åŒ–ï¼š
- en: 'Our batch now contains three entities: the batch of data, indices of sampled
    items, and samplesâ€™ weights.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ‰¹æ¬¡åŒ…å«ä¸‰ç§å®ä½“ï¼šæ•°æ®æ‰¹æ¬¡ã€é‡‡æ ·é¡¹çš„ç´¢å¼•å’Œæ ·æœ¬çš„æƒé‡ã€‚
- en: We call our new loss function, which accepts weights and returns the additional
    itemsâ€™ priorities. They are passed to the buffer.update_priorities() function
    to reprioritize items that we have sampled.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°ä¹‹ä¸ºæ–°çš„æŸå¤±å‡½æ•°ï¼Œå®ƒæ¥å—æƒé‡å¹¶è¿”å›é¢å¤–é¡¹çš„ä¼˜å…ˆçº§ã€‚è¿™äº›ä¼˜å…ˆçº§ä¼šä¼ é€’ç»™ `buffer.update_priorities()` å‡½æ•°ï¼Œä»¥ä¾¿é‡æ–°è°ƒæ•´æˆ‘ä»¬é‡‡æ ·çš„é¡¹çš„ä¼˜å…ˆçº§ã€‚
- en: We call the update_beta() method of the buffer to change the beta parameter
    according to the schedule.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è°ƒç”¨ç¼“å†²åŒºçš„ `update_beta()` æ–¹æ³•ï¼Œæ ¹æ®è®¡åˆ’æ”¹å˜ beta å‚æ•°ã€‚
- en: Results
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'This example can be trained as usual. According to my experiments, the prioritized
    replay buffer took almost the same absolute time to solve the environment: almost
    an hour. But it took fewer training iterations and fewer episodes. So, wall clock
    time is the same mostly due to the less efficient replay buffer, which, of course,
    could be resolved by proper ğ’ª(log N) implementation of the buffer.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®­ç»ƒã€‚æ ¹æ®æˆ‘çš„å®éªŒï¼Œä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºå‡ ä¹èŠ±è´¹äº†ç›¸åŒçš„æ—¶é—´æ¥è§£å†³ç¯å¢ƒï¼šå·®ä¸å¤šä¸€ä¸ªå°æ—¶ã€‚ä½†å®ƒèŠ±è´¹äº†æ›´å°‘çš„è®­ç»ƒè¿­ä»£å’Œå›åˆã€‚å› æ­¤ï¼Œå¢™æ—¶é’Ÿæ—¶é—´å‡ ä¹ç›¸åŒï¼Œä¸»è¦æ˜¯ç”±äºå›æ”¾ç¼“å†²åŒºæ•ˆç‡è¾ƒä½ï¼Œå½“ç„¶ï¼Œè¿™å¯ä»¥é€šè¿‡é€‚å½“çš„
    ğ’ª(log N) å®ç°æ¥è§£å†³ç¼“å†²åŒºçš„é—®é¢˜ã€‚
- en: 'Here is the comparison of reward dynamics of the baseline and prioritized replay
    buffer (right). The x axis is the game episodes:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯åŸºçº¿ä¸ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºï¼ˆå³ä¾§ï¼‰å¥–åŠ±åŠ¨æ€çš„æ¯”è¾ƒã€‚æ¨ªåæ ‡æ˜¯æ¸¸æˆå›åˆï¼š
- en: '![PIC](img/B22150_08_13.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_13.png)'
- en: 'FigureÂ 8.13: Reward dynamics for prioritized replay buffer in comparison to
    basic DQN'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.13ï¼šä¸åŸºç¡€ DQN æ¯”è¾ƒçš„ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºå¥–åŠ±åŠ¨æ€
- en: 'Another difference to note on the TensorBoard charts is a much lower loss for
    the prioritized replay buffer. The following chart shows the comparison:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ TensorBoard å›¾è¡¨ä¸­è¿˜å¯ä»¥çœ‹åˆ°å¦ä¸€ä¸ªä¸åŒä¹‹å¤„ï¼Œå°±æ˜¯ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºçš„æŸå¤±å€¼æ˜æ˜¾è¾ƒä½ã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†è¿™ä¸€æ¯”è¾ƒï¼š
- en: '![PIC](img/B22150_08_14.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_14.png)'
- en: 'FigureÂ 8.14: The comparison of loss during the training'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.14ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±çš„æ¯”è¾ƒ
- en: 'Lower loss value is also expected and is a good sign that our implementation
    works. The idea of prioritization is to train more on samples with high loss value,
    so training becomes more efficient. But there is a danger here: loss value during
    the training is not the primary objective to optimize; we can have very low loss,
    but due to a lack of exploration, the final policy learned could be far from being
    optimal.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: è¾ƒä½çš„æŸå¤±å€¼ä¹Ÿæ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå¹¶ä¸”æ˜¯æˆ‘ä»¬å®ç°æœ‰æ•ˆçš„è‰¯å¥½è¿¹è±¡ã€‚ä¼˜å…ˆçº§çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ›´å¤šåœ°è®­ç»ƒé‚£äº›æŸå¤±å€¼è¾ƒé«˜çš„æ ·æœ¬ï¼Œä½¿å¾—è®­ç»ƒæ›´åŠ é«˜æ•ˆã€‚ä½†è¿™é‡Œæœ‰ä¸€ä¸ªé£é™©ï¼šè®­ç»ƒä¸­çš„æŸå¤±å€¼å¹¶ä¸æ˜¯ä¼˜åŒ–çš„ä¸»è¦ç›®æ ‡ï¼›æˆ‘ä»¬å¯ä»¥æœ‰éå¸¸ä½çš„æŸå¤±å€¼ï¼Œä½†ç”±äºç¼ºä¹æ¢ç´¢ï¼Œæœ€ç»ˆå­¦ä¹ åˆ°çš„ç­–ç•¥å¯èƒ½è¿œæœªè¾¾åˆ°æœ€ä¼˜ã€‚
- en: Hyperparameter tuning
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: 'Hyperparameter tuning for the prioritized replay buffer was done with an additional
    parameter for Î±, which was sampled from a fixed list of values ranging from 0.3
    to 0.9 (with steps of 0.1). The best combination was able to solve Pong after
    330 episodes and had Î± = 0.6 (the same as in the paper):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºçš„è¶…å‚æ•°è°ƒä¼˜æ˜¯é€šè¿‡ä¸º Î± å¼•å…¥ä¸€ä¸ªé¢å¤–çš„å‚æ•°è¿›è¡Œçš„ï¼ŒÎ± çš„å€¼ä» 0.3 åˆ° 0.9ï¼ˆæ­¥é•¿ä¸º 0.1ï¼‰ä¹‹é—´çš„å›ºå®šåˆ—è¡¨ä¸­é‡‡æ ·ã€‚æœ€ä½³ç»„åˆèƒ½å¤Ÿåœ¨
    330 ä¸ªå›åˆåè§£å†³ Pong é—®é¢˜ï¼Œå¹¶ä¸” Î± = 0.6ï¼ˆä¸è®ºæ–‡ä¸­çš„ç›¸åŒï¼‰ï¼š
- en: '[PRE40]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following are charts comparing the tuned baseline DQN with the tuned prioritized
    replay buffer:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ¯”è¾ƒè°ƒæ•´åçš„åŸºçº¿ DQN ä¸è°ƒæ•´åçš„ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºçš„å›¾è¡¨ï¼š
- en: '![PIC](img/B22150_08_15.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_15.png)'
- en: 'FigureÂ 8.15: Comparison of tuned baseline DQN and tuned prioritized replay
    buffer'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.15ï¼šè°ƒæ•´åçš„åŸºçº¿ DQN å’Œè°ƒæ•´åçš„ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºæ¯”è¾ƒ
- en: Here, we see the prioritized replay buffer had faster gameplay improvement,
    but it took almost the same amount of games to reach score 21\. On the right chart
    (with the amount of game steps), the prioritized replay buffer was also a bit
    better.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°ä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºçš„æ¸¸æˆç©æ³•æ”¹è¿›æ›´å¿«ï¼Œä½†è¾¾åˆ° 21 åˆ†æ‰€éœ€çš„æ¸¸æˆæ•°é‡å‡ ä¹ç›¸åŒã€‚åœ¨å³è¾¹çš„å›¾è¡¨ï¼ˆä»¥æ¸¸æˆæ­¥éª¤ä¸ºå•ä½ï¼‰ä¸­ï¼Œä¼˜å…ˆçº§å›æ”¾ç¼“å†²åŒºçš„è¡¨ç°ä¹Ÿç•¥ä¼˜ã€‚
- en: Dueling DQN
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹æŠ— DQN
- en: 'This improvement to DQN was proposed in 2015, in the paper called Dueling network
    architectures for deep reinforcement learning [[Wan+16](#)]. The core observation
    of this paper is that the Q-values, Q(s,a), that our network is trying to approximate
    can be divided into quantities: the value of the state, V (s), and the advantage
    of actions in this state, A(s,a).'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ”¹è¿›äº 2015 å¹´åœ¨è®ºæ–‡ã€ŠDueling Network Architectures for Deep Reinforcement Learningã€‹ä¸­æå‡º
    [[Wan+16](#)]ã€‚è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ï¼Œç½‘ç»œè¯•å›¾è¿‘ä¼¼çš„ Q å€¼ Q(s,a) å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼šçŠ¶æ€çš„å€¼ V (s) å’Œè¯¥çŠ¶æ€ä¸‹åŠ¨ä½œçš„ä¼˜åŠ¿ A(s,a)ã€‚
- en: You have seen the quantity V (s) before, as it was the core of the value iteration
    method from ChapterÂ [5](ch009.xhtml#x1-820005). It is just equal to the discounted
    expected reward achievable from this state. The advantage A(s,a) is supposed to
    bridge the gap from V (s) to Q(s,a), as, by definition, Q(s,a) = V (s) + A(s,a).
    In other words, the advantage A(s,a) is just the delta, saying how much extra
    reward some particular action from the state brings us. The advantage could be
    positive or negative and, in general, could have any magnitude. For example, at
    some tipping point, the choice of one action over another can cost us a lot of
    the total reward.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹‹å‰è§è¿‡ V (s) è¿™ä¸€é‡ï¼Œå®ƒæ˜¯ç¬¬ [5](ch009.xhtml#x1-820005) ç« ä¸­å€¼è¿­ä»£æ–¹æ³•çš„æ ¸å¿ƒã€‚å®ƒç­‰äºä»è¯¥çŠ¶æ€å‡ºå‘å¯ä»¥è·å¾—çš„æŠ˜æ‰£é¢„æœŸå¥–åŠ±ã€‚ä¼˜åŠ¿
    A(s,a) æ—¨åœ¨å¼¥åˆ V (s) å’Œ Q(s,a) ä¹‹é—´çš„å·®è·ï¼Œå› ä¸ºæ ¹æ®å®šä¹‰ï¼ŒQ(s,a) = V (s) + A(s,a)ã€‚æ¢å¥è¯è¯´ï¼Œä¼˜åŠ¿ A(s,a)
    åªæ˜¯å¢é‡ï¼Œè¡¨ç¤ºä»è¯¥çŠ¶æ€é‡‡å–æŸä¸€ç‰¹å®šåŠ¨ä½œå¸¦æ¥çš„é¢å¤–å¥–åŠ±ã€‚ä¼˜åŠ¿å¯ä»¥æ˜¯æ­£å€¼ä¹Ÿå¯ä»¥æ˜¯è´Ÿå€¼ï¼Œé€šå¸¸å¯ä»¥å…·æœ‰ä»»ä½•å¤§å°ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸä¸ªä¸´ç•Œç‚¹ï¼Œé€‰æ‹©æŸä¸€åŠ¨ä½œè€Œéå¦ä¸€åŠ¨ä½œå¯èƒ½ä¼šè®©æˆ‘ä»¬å¤±å»å¾ˆå¤šæ€»å¥–åŠ±ã€‚
- en: 'The Dueling paperâ€™s contribution was an explicit separation of the value and
    the advantage in the networkâ€™s architecture, which brought better training stability,
    faster convergence, and better results on the Atari benchmark. The architecture
    difference from the classic DQN network is shown in the following illustration.
    The classic DQN network (top) takes features from the convolution layer and, using
    fully connected layers, transforms them into a vector of Q-values, one for each
    action. On the other hand, dueling DQN (bottom) takes convolution features and
    processes them using two independent paths: one path is responsible for V (s)
    prediction, which is just a single number, and another path predicts individual
    advantage values, having the same dimension as Q-values in the classic case. After
    that, we add V (s) to every value of A(s,a) to obtain Q(s,a), which is used and
    trained as normal. FigureÂ [8.16](#x1-147004r16) (from the paper) compares the
    basic DQN and dueling DQN:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling è®ºæ–‡çš„è´¡çŒ®åœ¨äºæ˜ç¡®åŒºåˆ†äº†ç½‘ç»œæ¶æ„ä¸­çš„ä»·å€¼å’Œä¼˜åŠ¿ï¼Œè¿™å¸¦æ¥äº†æ›´å¥½çš„è®­ç»ƒç¨³å®šæ€§ã€æ›´å¿«çš„æ”¶æ•›é€Ÿåº¦ä»¥åŠåœ¨ Atari åŸºå‡†æµ‹è¯•ä¸­æ›´å¥½çš„ç»“æœã€‚ä¸ç»å…¸
    DQN ç½‘ç»œçš„æ¶æ„å·®å¼‚å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç»å…¸çš„ DQN ç½‘ç»œï¼ˆä¸Šå›¾ï¼‰ä»å·ç§¯å±‚æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥å±‚å°†å…¶è½¬æ¢ä¸º Q å€¼å‘é‡ï¼Œæ¯ä¸ªåŠ¨ä½œå¯¹åº”ä¸€ä¸ª Q å€¼ã€‚å¦ä¸€æ–¹é¢ï¼ŒDueling
    DQNï¼ˆä¸‹å›¾ï¼‰ä»å·ç§¯å±‚æå–ç‰¹å¾ï¼Œå¹¶é€šè¿‡ä¸¤æ¡ç‹¬ç«‹çš„è·¯å¾„å¤„ç†å®ƒä»¬ï¼šä¸€æ¡è·¯å¾„è´Ÿè´£é¢„æµ‹ V (s)ï¼Œå³ä¸€ä¸ªå•ä¸€çš„æ•°å€¼ï¼Œå¦ä¸€æ¡è·¯å¾„é¢„æµ‹å„ä¸ªåŠ¨ä½œçš„ä¼˜åŠ¿å€¼ï¼Œç»´åº¦ä¸ç»å…¸æƒ…å†µä¸‹çš„
    Q å€¼ç›¸åŒã€‚ä¹‹åï¼Œæˆ‘ä»¬å°† V (s) åŠ åˆ°æ¯ä¸ª A(s,a) çš„å€¼ä¸Šï¼Œä»è€Œå¾—åˆ° Q(s,a)ï¼Œè¿™ä¸ªå€¼åƒé€šå¸¸çš„ Q å€¼ä¸€æ ·è¢«ä½¿ç”¨å¹¶è®­ç»ƒã€‚å›¾ [8.16](#x1-147004r16)ï¼ˆæ¥è‡ªè®ºæ–‡ï¼‰æ¯”è¾ƒäº†åŸºæœ¬çš„
    DQN å’Œ Dueling DQNï¼š
- en: '![PIC](img/file58.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file58.png)'
- en: 'FigureÂ 8.16: A basic DQN (top) and dueling architecture (bottom)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.16ï¼šåŸºæœ¬çš„ DQNï¼ˆä¸Šå›¾ï¼‰å’Œ Dueling æ¶æ„ï¼ˆä¸‹å›¾ï¼‰
- en: 'These changes in the architecture are not enough to make sure that the network
    will learn V (s) and A(s,a) as we want it to. Nothing prevents the network, for
    example, from predicting some state, V (s) = 0, and A(s) = [1,2,3,4], which is
    completely wrong, as the predicted V (s) is not the expected value of the state.
    We have yet another constraint to set: we want the mean value of the advantage
    of any state to be zero. In that case, the correct prediction for the preceding
    example will be V (s) = 2.5 and A(s) = [âˆ’1.5,âˆ’0.5,0.5,1.5].'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¶æ„çš„å˜åŒ–å¹¶ä¸è¶³ä»¥ç¡®ä¿ç½‘ç»œæŒ‰æˆ‘ä»¬å¸Œæœ›çš„æ–¹å¼å­¦ä¹  V (s) å’Œ A(s,a)ã€‚ä¾‹å¦‚ï¼Œç½‘ç»œå¯èƒ½é¢„æµ‹æŸä¸ªçŠ¶æ€çš„ V (s) = 0 å’Œ A(s) = [1,2,3,4]ï¼Œè¿™ç§æƒ…å†µæ˜¯å®Œå…¨é”™è¯¯çš„ï¼Œå› ä¸ºé¢„æµ‹çš„
    V (s) ä¸æ˜¯è¯¥çŠ¶æ€çš„æœŸæœ›å€¼ã€‚æˆ‘ä»¬è¿˜éœ€è¦è®¾å®šä¸€ä¸ªé¢å¤–çš„çº¦æŸï¼šæˆ‘ä»¬å¸Œæœ›ä»»ä½•çŠ¶æ€ä¸‹ä¼˜åŠ¿çš„å‡å€¼ä¸ºé›¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‰è¿°ä¾‹å­çš„æ­£ç¡®é¢„æµ‹åº”è¯¥æ˜¯ V (s) = 2.5
    å’Œ A(s) = [âˆ’1.5,âˆ’0.5,0.5,1.5]ã€‚
- en: 'This constraint could be enforced in various ways, for example, via the loss
    function; but in the Dueling paper, the authors proposed the very elegant solution
    of subtracting the mean value of the advantage from the Q expression in the network,
    which effectively pulls the mean for the advantage to zero:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçº¦æŸå¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å¼ºåˆ¶æ‰§è¡Œï¼Œä¾‹å¦‚é€šè¿‡æŸå¤±å‡½æ•°ï¼›ä½†åœ¨ Dueling è®ºæ–‡ä¸­ï¼Œä½œè€…æå‡ºäº†ä¸€ç§éå¸¸ä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼šä»ç½‘ç»œçš„ Q è¡¨è¾¾å¼ä¸­å‡å»ä¼˜åŠ¿çš„å‡å€¼ï¼Œè¿™æ ·å¯ä»¥æœ‰æ•ˆåœ°å°†ä¼˜åŠ¿çš„å‡å€¼æ‹‰è‡³é›¶ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq36.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq36.png)'
- en: 'This keeps the changes that need to be made in the classic DQN very simple:
    to convert it to the double DQN, you need to change only the network architecture,
    without affecting other pieces of the implementation.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä½¿å¾—å°†ç»å…¸DQNè½¬å˜ä¸ºåŒDQNçš„ä¿®æ”¹éå¸¸ç®€å•ï¼šåªéœ€è¦æ”¹å˜ç½‘ç»œæ¶æ„ï¼Œè€Œä¸å½±å“å®ç°çš„å…¶ä»–éƒ¨åˆ†ã€‚
- en: Implementation
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: The complete example is available in Chapter08/06_dqn_dueling.py. All the changes
    sit in the network architecture, so here, Iâ€™ll only show the network class (which
    is in the lib/dqn_extra.py module).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´çš„ç¤ºä¾‹å¯ä»¥åœ¨Chapter08/06_dqn_dueling.pyä¸­æ‰¾åˆ°ã€‚æ‰€æœ‰çš„æ”¹åŠ¨éƒ½åœ¨ç½‘ç»œæ¶æ„ä¸­ï¼Œå› æ­¤è¿™é‡Œæˆ‘åªå±•ç¤ºç½‘ç»œç±»ï¼ˆä½äºlib/dqn_extra.pyæ¨¡å—ä¸­ï¼‰ã€‚
- en: 'The convolution part is exactly the same as before:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯éƒ¨åˆ†ä¸ä¹‹å‰å®Œå…¨ç›¸åŒï¼š
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Instead of defining a single path of fully connected layers, we create two
    different transformations: one for advantages and one for value prediction:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ²¡æœ‰å®šä¹‰ä¸€ä¸ªå•ä¸€çš„å®Œå…¨è¿æ¥å±‚è·¯å¾„ï¼Œè€Œæ˜¯åˆ›å»ºäº†ä¸¤ç§ä¸åŒçš„å˜æ¢ï¼šä¸€ç§ç”¨äºä¼˜åŠ¿ï¼Œå¦ä¸€ç§ç”¨äºä»·å€¼é¢„æµ‹ï¼š
- en: '[PRE42]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Also, to keep the number of parameters in the model comparable to the original
    network, the inner dimension in both paths is decreased from 512 to 256\. The
    changes in the forward() function are also very simple, thanks to PyTorchâ€™s expressiveness:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œä¸ºäº†ä¿æŒæ¨¡å‹ä¸­çš„å‚æ•°æ•°é‡ä¸åŸå§‹ç½‘ç»œç›¸å½“ï¼Œä¸¤æ¡è·¯å¾„ä¸­çš„å†…éƒ¨ç»´åº¦ä»512å‡å°‘åˆ°256ã€‚å¾—ç›ŠäºPyTorchçš„è¡¨è¾¾èƒ½åŠ›ï¼Œforward()å‡½æ•°ä¸­çš„å˜åŒ–ä¹Ÿéå¸¸ç®€å•ï¼š
- en: '[PRE43]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, we calculate the value and advantage for our batch of samples and add
    them together, subtracting the mean of the advantage to obtain the final Q-values.
    A subtle, but important, difference lies in calculating the mean along the second
    dimension of the tensor, which produces a vector of the mean advantage for every
    sample in our batch.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—æ‰¹æ¬¡æ ·æœ¬çš„ä»·å€¼å’Œä¼˜åŠ¿å¹¶å°†å®ƒä»¬åŠ åœ¨ä¸€èµ·ï¼Œå‡å»ä¼˜åŠ¿çš„å‡å€¼ä»¥è·å¾—æœ€ç»ˆçš„Qå€¼ã€‚ä¸€ä¸ªå¾®å¦™ä½†é‡è¦çš„åŒºåˆ«æ˜¯åœ¨è®¡ç®—å¼ é‡çš„ç¬¬äºŒç»´åº¦ä¸Šçš„å‡å€¼ï¼Œè¿™ä¼šä¸ºæˆ‘ä»¬æ‰¹æ¬¡ä¸­çš„æ¯ä¸ªæ ·æœ¬ç”Ÿæˆä¸€ä¸ªå‡å€¼ä¼˜åŠ¿çš„å‘é‡ã€‚
- en: Results
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'After training a dueling DQN, we can compare it to the classic DQN convergence
    on our Pong benchmark. Dueling architecture has faster convergence in comparison
    to the basic DQN version:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªå¯¹æŠ—DQNåï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸ç»å…¸DQNåœ¨PongåŸºå‡†æµ‹è¯•ä¸­çš„æ”¶æ•›æ€§è¿›è¡Œæ¯”è¾ƒã€‚ä¸åŸºç¡€DQNç‰ˆæœ¬ç›¸æ¯”ï¼Œå¯¹æŠ—æ¶æ„çš„æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼š
- en: '![PIC](img/B22150_08_17.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_17.png)'
- en: 'FigureÂ 8.17: The reward dynamic of dueling DQN compared to the baseline version'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.17ï¼šå¯¹æŠ—DQNä¸åŸºçº¿ç‰ˆæœ¬çš„å¥–åŠ±åŠ¨æ€æ¯”è¾ƒ
- en: 'Our example also outputs the advantage and value for a fixed set of states,
    shown in the following charts. They meet our expectations: the advantage is not
    very different from zero, but the value improves over time (and resembles the
    value from the Double DQN section):'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç¤ºä¾‹è¿˜è¾“å‡ºäº†å¯¹äºä¸€ç»„å›ºå®šçŠ¶æ€çš„ä¼˜åŠ¿å’Œä»·å€¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å®ƒä»¬ç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸï¼šä¼˜åŠ¿ä¸é›¶å·®å¼‚ä¸å¤§ï¼Œä½†éšç€æ—¶é—´æ¨ç§»ï¼Œä»·å€¼åœ¨ä¸æ–­æé«˜ï¼ˆå¹¶ä¸”ç±»ä¼¼äºDouble
    DQNéƒ¨åˆ†ä¸­çš„å€¼ï¼‰ï¼š
- en: '![PIC](img/B22150_08_18.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_18.png)'
- en: 'FigureÂ 8.18: Mean advantage (left) and value (right) on a fixed set of states'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.18ï¼šå›ºå®šçŠ¶æ€é›†ä¸Šçš„å‡å€¼ä¼˜åŠ¿ï¼ˆå·¦ï¼‰å’Œä»·å€¼ï¼ˆå³ï¼‰
- en: Hyperparameter tuning
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒæ•´
- en: The tuning of the hyperparameters was not very fruitful. After 30 tuning iterations,
    there were no combinations of learning rate and gamma that were able to converge
    faster than the common set of parameters.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°çš„è°ƒæ•´å¹¶æœªå¸¦æ¥å¾ˆå¤§æ”¶è·ã€‚åœ¨30æ¬¡è°ƒæ•´è¿­ä»£åï¼Œæ²¡æœ‰ä»»ä½•å­¦ä¹ ç‡å’Œgammaçš„ç»„åˆèƒ½å¤Ÿæ¯”å¸¸ç”¨å‚æ•°ç»„åˆæ›´å¿«æ”¶æ•›ã€‚
- en: Categorical DQN
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç±»åˆ«åŒ–DQN
- en: The last, and the most complicated, method in our DQN improvements toolbox is
    from the paper published by DeepMind in June 2017, called A distributional perspective
    on reinforcement learning [[BDM17](#)]. Although this paper is a few years old
    now, it remains highly relevant, and active research is still ongoing in this
    area. The book Distributional reinforcement learning was published in 2023, where
    the same authors describe the method in greater detail [[BDR23](#)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„DQNæ”¹è¿›å·¥å…·ç®±ä¸­çš„æœ€åä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯æœ€å¤æ‚çš„ä¸€ç§ï¼Œæ¥è‡ªDeepMindäº2017å¹´6æœˆå‘è¡¨çš„è®ºæ–‡ã€Šå¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è§†è§’ã€‹[[BDM17](#)]ã€‚å°½ç®¡è¿™ç¯‡è®ºæ–‡å·²ç»æœ‰å‡ å¹´å†å²ï¼Œä½†å®ƒä»ç„¶éå¸¸ç›¸å…³ï¼Œä¸”è¿™ä¸€é¢†åŸŸçš„ç ”ç©¶ä»åœ¨æŒç»­è¿›è¡Œä¸­ã€‚2023å¹´å‡ºç‰ˆçš„ã€Šåˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ã€‹ä¸€ä¹¦ä¸­ï¼Œä½œè€…ä»¬æ›´è¯¦ç»†åœ°æè¿°äº†è¯¥æ–¹æ³•[[BDR23](#)]ã€‚
- en: In the paper, the authors questioned the fundamental pieces of Q-learning â€”
    Q-values â€” and tried to replace them with a more generic Q-value probability distribution.
    Letâ€™s try to understand the idea. Both the Q-learning and value iteration methods
    work with the values of the actions or states represented as simple numbers and
    showing how much total reward we can achieve from a state, or an action and a
    state. However, is it practical to squeeze all future possible rewards into one
    number? In complicated environments, the future could be stochastic, giving us
    different values with different probabilities.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…è´¨ç–‘äº†Qå­¦ä¹ ä¸­çš„åŸºæœ¬å…ƒç´ â€”â€”Qå€¼â€”â€”å¹¶å°è¯•ç”¨æ›´é€šç”¨çš„Qå€¼æ¦‚ç‡åˆ†å¸ƒæ¥æ›¿ä»£å®ƒä»¬ã€‚è®©æˆ‘ä»¬æ¥ç†è§£è¿™ä¸ªæ¦‚å¿µã€‚Qå­¦ä¹ å’Œä»·å€¼è¿­ä»£æ–¹æ³•éƒ½ä½¿ç”¨è¡¨ç¤ºåŠ¨ä½œæˆ–çŠ¶æ€çš„æ•°å€¼ï¼Œå±•ç¤ºä»æŸä¸ªçŠ¶æ€æˆ–æŸä¸ªåŠ¨ä½œå’ŒçŠ¶æ€ç»„åˆä¸­èƒ½å¤Ÿè·å¾—å¤šå°‘æ€»å¥–åŠ±ã€‚ç„¶è€Œï¼Œå°†æ‰€æœ‰æœªæ¥å¯èƒ½çš„å¥–åŠ±å‹ç¼©æˆä¸€ä¸ªæ•°å­—ï¼Œå®é™…å¯è¡Œå—ï¼Ÿåœ¨å¤æ‚çš„ç¯å¢ƒä¸­ï¼Œæœªæ¥å¯èƒ½æ˜¯éšæœºçš„ï¼Œä¼šç»™æˆ‘ä»¬å¸¦æ¥ä¸åŒçš„å€¼å’Œä¸åŒçš„æ¦‚ç‡ã€‚
- en: 'For example, imagine the commuter scenario when you regularly drive from home
    to work. Most of the time, the traffic isnâ€™t that heavy, and it takes you around
    30 minutes to reach your destination. Itâ€™s not exactly 30 minutes, but on average
    itâ€™s 30\. From time to time, something happens, like road repairs or an accident,
    and due to traffic jams, it takes you three times longer to get to work. The probability
    of your commute time can be represented as a distribution of the â€œcommute timeâ€
    random variable, and it is shown in the following chart:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚ï¼Œæƒ³è±¡ä¸€ä¸‹ä½ æ¯å¤©ä»å®¶é‡Œå¼€è½¦å»ä¸Šç­çš„é€šå‹¤æƒ…å¢ƒã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œäº¤é€šä¸ç®—å¤ªå µï¼Œé€šå¸¸ä½ èƒ½åœ¨å¤§çº¦30åˆ†é’Ÿå†…åˆ°è¾¾ç›®çš„åœ°ã€‚è¿™å¹¶ä¸ä¸€å®šæ˜¯å‡†ç¡®çš„30åˆ†é’Ÿï¼Œä½†å¹³å‡ä¸‹æ¥æ˜¯30åˆ†é’Ÿã€‚å¶å°”ï¼Œä¹Ÿä¼šå‘ç”Ÿä¸€äº›æƒ…å†µï¼Œæ¯”å¦‚é“è·¯ç»´ä¿®æˆ–äº‹æ•…ï¼Œå¯¼è‡´äº¤é€šå µå¡ï¼Œä½ çš„é€šå‹¤æ—¶é—´å¯èƒ½æ˜¯å¹³å¸¸çš„ä¸‰å€ã€‚ä½ çš„é€šå‹¤æ—¶é—´çš„æ¦‚ç‡å¯ä»¥ç”¨â€œé€šå‹¤æ—¶é—´â€è¿™ä¸€éšæœºå˜é‡çš„åˆ†å¸ƒæ¥è¡¨ç¤ºï¼Œåˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![PIC](img/B22150_08_19.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_19.png)'
- en: 'FigureÂ 8.19: The probability distribution of commute time'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.19ï¼šé€šå‹¤æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒ
- en: 'Now, imagine that you have an alternative way to get to work: the train. It
    takes a bit longer, as you need to get from home to the train station and from
    the station to the office, but they are much more reliable than traveling by car
    (in some contries, like Germany, it might not be the case, but letâ€™s consider
    Swiss trains for our example). Say, for instance, that the train commute time
    is 40 minutes on average, with a small chance of train disruption, which adds
    20 minutes of extra time to the journey. The distribution of the train commute
    is shown in the following graph:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå‡è®¾ä½ æœ‰å¦ä¸€ç§ä¸Šç­çš„æ–¹å¼ï¼šåç«è½¦ã€‚è™½ç„¶éœ€è¦ç¨å¾®å¤šèŠ±ç‚¹æ—¶é—´ï¼Œå› ä¸ºä½ éœ€è¦ä»å®¶é‡Œåˆ°ç«è½¦ç«™ï¼Œå†ä»ç«è½¦ç«™åˆ°åŠå…¬å®¤ï¼Œä½†ç›¸æ¯”å¼€è½¦ï¼Œç«è½¦æ›´å¯é ï¼ˆåœ¨ä¸€äº›å›½å®¶ï¼Œå¦‚å¾·å›½ï¼Œæƒ…å†µå¯èƒ½ä¸åŒï¼Œä½†æˆ‘ä»¬å‡è®¾ä½¿ç”¨ç‘å£«çš„ç«è½¦ä½œä¸ºä¾‹å­ï¼‰ã€‚æ¯”å¦‚è¯´ï¼Œç«è½¦çš„é€šå‹¤æ—¶é—´å¹³å‡æ˜¯40åˆ†é’Ÿï¼Œå¶å°”ä¼šæœ‰ç«è½¦å»¶è¯¯çš„æƒ…å†µï¼Œé€šå¸¸ä¼šå¢åŠ 20åˆ†é’Ÿçš„é¢å¤–æ—¶é—´ã€‚ç«è½¦é€šå‹¤æ—¶é—´çš„åˆ†å¸ƒå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![PIC](img/B22150_08_20.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_20.png)'
- en: 'FigureÂ 8.20: The probability distribution of train commute time'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.20ï¼šå¼€è½¦é€šå‹¤æ—¶é—´çš„æ¦‚ç‡åˆ†å¸ƒ
- en: Imagine that now we want to make the decision on how to commute. If we know
    only the mean time for both car and train, a car looks more attractive, as on
    average it takes 35.43 minutes to travel, which is better than 40.54 minutes for
    the train.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ç°åœ¨æˆ‘ä»¬è¦åšå‡ºé€šå‹¤æ–¹å¼çš„é€‰æ‹©ã€‚å¦‚æœæˆ‘ä»¬åªçŸ¥é“å¼€è½¦å’Œç«è½¦çš„å¹³å‡é€šå‹¤æ—¶é—´ï¼Œé‚£ä¹ˆå¼€è½¦çœ‹èµ·æ¥æ›´æœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå¼€è½¦çš„å¹³å‡é€šå‹¤æ—¶é—´æ˜¯35.43åˆ†é’Ÿï¼Œæ¯”ç«è½¦çš„40.54åˆ†é’Ÿè¦çŸ­ã€‚
- en: However, if we look at full distributions, we may decide to go by train, as
    even in the worst-case scenario, it will be one hour of commuting versus one hour
    and 30 minutes. Switching to statistical language, the car distribution has much
    higher variance, so in situations when you really have to be at the office in
    60 minutes max, the train is better.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çœ‹å®Œæ•´çš„åˆ†å¸ƒå›¾ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé€‰æ‹©åç«è½¦ï¼Œå› ä¸ºå³ä½¿åœ¨æœ€åçš„æƒ…å†µä¸‹ï¼Œç«è½¦çš„é€šå‹¤æ—¶é—´ä¹Ÿåªæœ‰ä¸€ä¸ªå°æ—¶ï¼Œè€Œå¼€è½¦åˆ™æ˜¯ä¸€ä¸ªå°æ—¶30åˆ†é’Ÿã€‚æ¢æˆç»Ÿè®¡è¯­è¨€ï¼Œå¼€è½¦çš„åˆ†å¸ƒå…·æœ‰æ›´é«˜çš„æ–¹å·®ï¼Œå› æ­¤åœ¨ä½ å¿…é¡»åœ¨60åˆ†é’Ÿå†…åˆ°è¾¾åŠå…¬å®¤çš„æƒ…å†µä¸‹ï¼Œç«è½¦æ›´ä¸ºåˆé€‚ã€‚
- en: The situation becomes even more complicated in the Markov decision process (MDP)
    scenario, when the sequence of decisions needs to be made and every decision might
    influence the future situation. In the commute example, it might be the time of
    an important meeting that you need to arrange given the way that you are going
    to commute. In that case, working with mean reward values might mean losing lots
    of information about the underlying environment dynamics.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰åœºæ™¯ä¸­ï¼Œæƒ…å†µå˜å¾—æ›´åŠ å¤æ‚ï¼Œå› ä¸ºå†³ç­–éœ€è¦æŒ‰é¡ºåºè¿›è¡Œï¼Œè€Œä¸”æ¯ä¸ªå†³ç­–å¯èƒ½ä¼šå½±å“æœªæ¥çš„æƒ…å†µã€‚æ¯”å¦‚åœ¨é€šå‹¤ä¾‹å­ä¸­ï¼Œå¯èƒ½æ˜¯ä½ éœ€è¦å®‰æ’ä¸€ä¸ªé‡è¦ä¼šè®®çš„æ—¶é—´ï¼Œè€Œè¿™ä¸ªå®‰æ’å¯èƒ½ä¼šå—åˆ°ä½ é€‰æ‹©çš„é€šå‹¤æ–¹å¼çš„å½±å“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å‡å€¼å¥–åŠ±å€¼å¯èƒ½ä¼šä¸§å¤±å…³äºç¯å¢ƒåŠ¨æ€çš„å¾ˆå¤šä¿¡æ¯ã€‚
- en: Exactly the same idea was proposed by the authors of Distributional Perspective
    on Reinforcement Learning [9]. Why do we limit ourselves by trying to predict
    an average value for an action, when the underlying value may have a complicated
    underlying distribution? Maybe it will help us to work with distributions directly.
    The results presented in the paper show that, in fact, this idea could be helpful,
    but at the cost of introducing a more complicated method. Iâ€™m not going to put
    a strict mathematical definition here, but the overall idea is to predict the
    distribution of value for every action, similar to the distributions for our car/train
    example. As the next step, the authors showed that the Bellman equation can be
    generalized for a distribution case, and it will have the form Z(x,a)![D =](img/eq37.png)R(x,a)
    + Î³Z(xâ€²,aâ€²), which is very similar to the familiar Bellman equation, but now Z(x,a)
    and R(x,a) are the probability distributions and are not single numbers. The notation
    A![ D =](img/eq37.png)B indicates eqality of distributions A and B.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œå…¨ç›¸åŒçš„è§‚ç‚¹æ˜¯ç”±ã€Šå¼ºåŒ–å­¦ä¹ çš„åˆ†å¸ƒå¼è§†è§’ã€‹ä¸€æ–‡çš„ä½œè€…æå‡ºçš„[9]ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦é™åˆ¶è‡ªå·±ï¼Œè¯•å›¾ä¸ºä¸€ä¸ªåŠ¨ä½œé¢„æµ‹ä¸€ä¸ªå¹³å‡å€¼ï¼Œè€Œå¿½ç•¥äº†å…¶æ½œåœ¨å€¼å¯èƒ½å…·æœ‰å¤æ‚çš„åˆ†å¸ƒï¼Ÿä¹Ÿè®¸ç›´æ¥å¤„ç†åˆ†å¸ƒä¼šå¯¹æˆ‘ä»¬æœ‰æ‰€å¸®åŠ©ã€‚è®ºæ–‡ä¸­å±•ç¤ºçš„ç»“æœè¡¨æ˜ï¼Œäº‹å®ä¸Šï¼Œè¿™ä¸ªæƒ³æ³•å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„ï¼Œä½†ä»£ä»·æ˜¯å¼•å…¥äº†æ›´å¤æ‚çš„æ–¹æ³•ã€‚æˆ‘åœ¨è¿™é‡Œä¸ä¼šç»™å‡ºä¸¥æ ¼çš„æ•°å­¦å®šä¹‰ï¼Œä½†æ€»ä½“æ€è·¯æ˜¯ä¸ºæ¯ä¸ªåŠ¨ä½œé¢„æµ‹å€¼çš„åˆ†å¸ƒï¼Œç±»ä¼¼äºæˆ‘ä»¬æ±½è½¦/ç«è½¦ä¾‹å­ä¸­çš„åˆ†å¸ƒã€‚ä½œä¸ºä¸‹ä¸€æ­¥ï¼Œä½œè€…ä»¬å±•ç¤ºäº†è´å°”æ›¼æ–¹ç¨‹å¯ä»¥æ¨å¹¿åˆ°åˆ†å¸ƒçš„æƒ…å†µï¼Œå¹¶ä¸”å®ƒçš„å½¢å¼ä¸ºZ(x,a)![D
    =](img/eq37.png)R(x,a) + Î³Z(xâ€²,aâ€²)ï¼Œè¿™ä¸æˆ‘ä»¬ç†Ÿæ‚‰çš„è´å°”æ›¼æ–¹ç¨‹éå¸¸ç›¸ä¼¼ï¼Œä½†ç°åœ¨Z(x,a)å’ŒR(x,a)æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•ä¸€æ•°å€¼ã€‚ç¬¦å·A![
    D =](img/eq37.png)Bè¡¨ç¤ºåˆ†å¸ƒAå’ŒBçš„ç›¸ç­‰ã€‚
- en: The resulting distribution can be used to train our network to give better predictions
    of value distribution for every action of the given state, exactly in the same
    way as with Q-learning. The only difference will be in the loss function, which
    now has to be replaced with something suitable for distribution comparison. There
    are several alternatives available, for example, Kullback-Leibler (KL) divergence
    (or cross-entropy loss), which is used in classification problems, or the Wasserstein
    metric. In the paper, the authors gave theoretical justification for the Wasserstein
    metric, but when they tried to apply it in practice, they faced limitations. So,
    in the end, the paper used KL divergence.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—åˆ°çš„åˆ†å¸ƒå¯ä»¥ç”¨æ¥è®­ç»ƒæˆ‘ä»¬çš„ç½‘ç»œï¼Œä»¥ä¾¿ä¸ºç»™å®šçŠ¶æ€ä¸‹çš„æ¯ä¸ªåŠ¨ä½œæä¾›æ›´å¥½çš„å€¼åˆ†å¸ƒé¢„æµ‹ï¼Œæ–¹æ³•ä¸Qå­¦ä¹ å®Œå…¨ç›¸åŒã€‚å”¯ä¸€çš„åŒºåˆ«åœ¨äºæŸå¤±å‡½æ•°ï¼Œç°åœ¨å¿…é¡»ç”¨é€‚åˆåˆ†å¸ƒæ¯”è¾ƒçš„å†…å®¹æ›¿ä»£å®ƒã€‚è¿™é‡Œæœ‰å‡ ä¸ªå¯ç”¨çš„æ›¿ä»£æ–¹æ³•ï¼Œä¾‹å¦‚ï¼ŒKullback-Leiblerï¼ˆKLï¼‰æ•£åº¦ï¼ˆæˆ–äº¤å‰ç†µæŸå¤±ï¼‰ï¼Œå®ƒé€šå¸¸ç”¨äºåˆ†ç±»é—®é¢˜ï¼Œæˆ–è€…Wassersteinåº¦é‡ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…ä¸ºWassersteinåº¦é‡æä¾›äº†ç†è®ºä¾æ®ï¼Œä½†åœ¨å®è·µä¸­å°è¯•åº”ç”¨æ—¶ï¼Œé‡åˆ°äº†ä¸€äº›é™åˆ¶ã€‚æ‰€ä»¥ï¼Œæœ€ç»ˆè®ºæ–‡ä¸­ä½¿ç”¨äº†KLæ•£åº¦ã€‚
- en: Implementation
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: As mentioned, the method is quite complex, so it took me a while to implement
    it and make sure it was working. The complete code is in Chapter08/07_dqn_distrib.py,
    which uses the distr_projection function in lib/dqn_extra.py to perform distribution
    projection. Before we check it, I need to say a few words about the implementation
    logic.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œè¿™ä¸ªæ–¹æ³•ç›¸å½“å¤æ‚ï¼Œæ‰€ä»¥æˆ‘èŠ±äº†ä¸€äº›æ—¶é—´æ¥å®ç°å®ƒå¹¶ç¡®ä¿å…¶æ­£å¸¸å·¥ä½œã€‚å®Œæ•´ä»£ç åœ¨Chapter08/07_dqn_distrib.pyä¸­ï¼Œå…¶ä¸­ä½¿ç”¨äº†lib/dqn_extra.pyä¸­çš„distr_projectionå‡½æ•°æ¥æ‰§è¡Œåˆ†å¸ƒæŠ•å½±ã€‚åœ¨æ£€æŸ¥ä¹‹å‰ï¼Œæˆ‘éœ€è¦å…ˆç®€å•è¯´æ˜ä¸€ä¸‹å®ç°é€»è¾‘ã€‚
- en: The central part of the method is the probability distribution, which we are
    approximating. There are lots of ways to represent the distribution, but the authors
    of the paper chose a quite generic parametric distribution, which is basically
    a fixed number of values placed regularly on a values range. The range of values
    should cover the range of possible accumulated discounted reward. In the paper,
    the authors did experiments with various numbers of atoms, but the best results
    were obtained with the range split on N_ATOMS=51 intervals in the range of values
    from Vmin=-10 to Vmax=10.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹æ³•çš„æ ¸å¿ƒéƒ¨åˆ†æ˜¯æˆ‘ä»¬æ­£åœ¨é€¼è¿‘çš„æ¦‚ç‡åˆ†å¸ƒã€‚æœ‰å¾ˆå¤šæ–¹æ³•å¯ä»¥è¡¨ç¤ºè¿™ä¸ªåˆ†å¸ƒï¼Œä½†è®ºæ–‡çš„ä½œè€…é€‰æ‹©äº†ä¸€ä¸ªç›¸å½“é€šç”¨çš„å‚æ•°åŒ–åˆ†å¸ƒï¼ŒåŸºæœ¬ä¸Šæ˜¯å°†ä¸€ç»„å›ºå®šæ•°å€¼å‡åŒ€åˆ†å¸ƒåœ¨ä¸€ä¸ªæ•°å€¼èŒƒå›´ä¸Šã€‚è¿™ä¸ªæ•°å€¼èŒƒå›´åº”è¯¥è¦†ç›–å¯èƒ½çš„ç´¯è®¡æŠ˜æ‰£å¥–åŠ±èŒƒå›´ã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…åšäº†å¤šä¸ªä¸åŒæ•°é‡çš„åŸå­å®éªŒï¼Œä½†æœ€ä½³ç»“æœæ˜¯åœ¨å€¼çš„èŒƒå›´ä»Vmin=-10åˆ°Vmax=10ä¸­å°†èŒƒå›´åˆ’åˆ†ä¸ºN_ATOMS=51ä¸ªåŒºé—´æ—¶è·å¾—çš„ã€‚
- en: 'For every atom (we have 51 of them), our network predicts the probability that
    the future discounted value will fall into this atomâ€™s range. The central part
    of the method is the code, which performs the contraction of distribution of the
    next stateâ€™s best action using gamma, adds local reward to the distribution, and
    projects the results back into our original atoms. This logic is implemented in
    the dqn_extra.distr_projection function. In the beginning, we allocate the array
    that will keep the result of the projection:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªåŸå­ï¼ˆæˆ‘ä»¬æœ‰ 51 ä¸ªï¼‰ï¼Œæˆ‘ä»¬çš„ç½‘ç»œé¢„æµ‹æœªæ¥æŠ˜æ‰£å€¼è½åœ¨æ­¤åŸå­èŒƒå›´å†…çš„æ¦‚ç‡ã€‚æ–¹æ³•çš„æ ¸å¿ƒéƒ¨åˆ†æ˜¯ä»£ç ï¼Œå®ƒæ‰§è¡Œä¸‹ä¸€ä¸ªçŠ¶æ€æœ€ä½³åŠ¨ä½œçš„åˆ†å¸ƒæ”¶ç¼©ï¼Œä½¿ç”¨ gammaï¼Œå‘åˆ†å¸ƒä¸­æ·»åŠ å±€éƒ¨å¥–åŠ±ï¼Œå¹¶å°†ç»“æœæŠ•å½±å›åˆ°åŸå§‹åŸå­ä¸­ã€‚è¿™ä¸ªé€»è¾‘åœ¨
    dqn_extra.distr_projection å‡½æ•°ä¸­å®ç°ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬åˆ†é…äº†ä¸€ä¸ªæ•°ç»„æ¥ä¿å­˜æŠ•å½±ç»“æœï¼š
- en: '[PRE44]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This function expects the batch of distributions with a shape (batch_size,
    N_ATOMS), the array of rewards, flags for completed episodes, and our hyperparameters:
    Vmin, Vmax, N_ATOMS, and gamma. The delta_z variable is the width of every atom
    in our value range.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‡½æ•°æ¥å—å½¢çŠ¶ä¸º(batch_size, N_ATOMS)çš„åˆ†å¸ƒæ‰¹æ¬¡ï¼Œå¥–åŠ±æ•°ç»„ï¼Œå·²å®Œæˆå›åˆçš„æ ‡å¿—ä»¥åŠæˆ‘ä»¬çš„è¶…å‚æ•°ï¼šVmin, Vmax, N_ATOMS
    å’Œ gammaã€‚delta_z å˜é‡è¡¨ç¤ºæˆ‘ä»¬å€¼èŒƒå›´ä¸­æ¯ä¸ªåŸå­çš„å®½åº¦ã€‚
- en: 'In the following code, we iterate over every atom in the original distribution
    that we have and calculate the place that this atom will be projected to by the
    Bellman operator, taking into account our value bounds:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ä»£ç ä¸­ï¼Œæˆ‘ä»¬éå†åŸå§‹åˆ†å¸ƒä¸­çš„æ¯ä¸ªåŸå­ï¼Œå¹¶è®¡ç®—è¯¥åŸå­å°†ç”± Bellman æ“ä½œç¬¦æŠ•å½±åˆ°çš„ä½ç½®ï¼ŒåŒæ—¶è€ƒè™‘æˆ‘ä»¬çš„å€¼èŒƒå›´ï¼š
- en: '[PRE45]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: For example, the very first atom, with index 0, corresponds with the value Vmin=-10,
    but for the sample with reward +1 will be projected into the value âˆ’10 â‹… 0.99
    + 1 = âˆ’8.9\. In other words, it will be shifted to the right (assume gamma=0.99).
    If the value falls beyond our value range given by Vmin and Vmax, we clip it to
    the bounds.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç¬¬ä¸€ä¸ªåŸå­ï¼Œç´¢å¼•ä¸º 0ï¼Œå¯¹åº”çš„å€¼ä¸º Vmin=-10ï¼Œä½†å¯¹äºå¥–åŠ± +1 çš„æ ·æœ¬ï¼Œå°†æŠ•å½±åˆ°å€¼ âˆ’10 â‹… 0.99 + 1 = âˆ’8.9ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå°†å‘å³ç§»åŠ¨ï¼ˆå‡è®¾
    gamma=0.99ï¼‰ã€‚å¦‚æœè¯¥å€¼è¶…å‡ºäº†ç”± Vmin å’Œ Vmax ç»™å‡ºçš„å€¼èŒƒå›´ï¼Œæˆ‘ä»¬ä¼šå°†å…¶è£å‰ªåˆ°è¾¹ç•Œå†…ã€‚
- en: 'In the next line, we calculate the atom numbers that our samples have projected:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€è¡Œï¼Œæˆ‘ä»¬è®¡ç®—æ ·æœ¬æŠ•å½±åˆ°çš„åŸå­ç¼–å·ï¼š
- en: '[PRE46]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Of course, samples can be projected between atoms. In such situations, we spread
    the value in the original distribution at the source atom between the two atoms
    that it falls between. This spreading should be carefully handled, as our target
    atom can land exactly at some atomâ€™s position. In that case, we just need to add
    the source distribution value to the target atom.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæ ·æœ¬å¯ä»¥æŠ•å½±åˆ°åŸå­ä¹‹é—´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æºåŸå­ä¸­çš„å€¼åˆ†é…åˆ°å…¶ä¹‹é—´çš„ä¸¤ä¸ªåŸå­ä¸­ã€‚è¿™ä¸ªåˆ†é…éœ€è¦å°å¿ƒå¤„ç†ï¼Œå› ä¸ºç›®æ ‡åŸå­å¯èƒ½æ°å¥½è½åœ¨æŸä¸ªåŸå­çš„ä½ç½®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªéœ€è¦å°†æºåˆ†å¸ƒå€¼æ·»åŠ åˆ°ç›®æ ‡åŸå­ã€‚
- en: 'The following code handles the situation when the projected atom lands exactly
    on the target atom. Otherwise, b_j wonâ€™t be the integer value and variables l
    and u (which correspond to the indices of atoms below and above the projected
    point):'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å¤„ç†å½“æŠ•å½±åŸå­æ­£å¥½è½åœ¨ç›®æ ‡åŸå­ä¸Šçš„æƒ…å†µã€‚å¦åˆ™ï¼Œb_j å°†ä¸æ˜¯æ•´æ•°å€¼ï¼Œå˜é‡ l å’Œ uï¼ˆåˆ†åˆ«å¯¹åº”æŠ•å½±ç‚¹ä¸‹æ–¹å’Œä¸Šæ–¹çš„åŸå­ç´¢å¼•ï¼‰ï¼š
- en: '[PRE47]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'When the projected point lands between atoms, we need to spread the probability
    of the source atom between the atoms below and above. This is carried out by two
    lines in the following code:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æŠ•å½±ç‚¹è½åœ¨åŸå­ä¹‹é—´æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†æºåŸå­çš„æ¦‚ç‡åˆ†é…åˆ°ä¸‹æ–¹å’Œä¸Šæ–¹çš„åŸå­ä¹‹é—´ã€‚è¿™é€šè¿‡ä»¥ä¸‹ä»£ç ä¸­çš„ä¸¤è¡Œæ¥å®ç°ï¼š
- en: '[PRE48]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Of course, we need to properly handle the final transitions of episodes. In
    that case, our projection shouldnâ€™t take into account the next distribution and
    should just have a 1 probability corresponding to the reward obtained.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œæˆ‘ä»¬éœ€è¦æ­£ç¡®å¤„ç†å›åˆçš„æœ€ç»ˆè¿‡æ¸¡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æŠ•å½±ä¸åº”è€ƒè™‘ä¸‹ä¸€ä¸ªåˆ†å¸ƒï¼Œè€Œåº”ä»…å…·æœ‰ä¸è·å¾—çš„å¥–åŠ±å¯¹åº”çš„ 1 çš„æ¦‚ç‡ã€‚
- en: 'However, we again need to take into account our atoms and properly distribute
    this probability if the reward value falls between atoms. This case is handled
    by the following code branch, which zeroes the resulting distribution for samples
    with the done flag set and then calculates the resulting projection:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡è€ƒè™‘åŸå­ï¼Œå¹¶åœ¨å¥–åŠ±å€¼è½åœ¨åŸå­ä¹‹é—´æ—¶ï¼Œæ­£ç¡®åœ°åˆ†é…æ¦‚ç‡ã€‚æ­¤æƒ…å†µç”±ä»¥ä¸‹ä»£ç åˆ†æ”¯å¤„ç†ï¼Œè¯¥åˆ†æ”¯ä¼šä¸ºå·²è®¾ç½®â€œdoneâ€æ ‡å¿—çš„æ ·æœ¬å°†ç»“æœåˆ†å¸ƒå½’é›¶ï¼Œç„¶åè®¡ç®—æœ€ç»ˆçš„æŠ•å½±ç»“æœï¼š
- en: '[PRE49]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: To give you an illustration of what this function does, letâ€™s look at artificially
    made distributions processed by this function (FigureÂ [8.21](#x1-152038r21)).
    I used them to debug the function and make sure that it worked as intended. The
    code for these checks is in Chapter08/adhoc/distr_test.py.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»™ä½ æ¼”ç¤ºè¿™ä¸ªå‡½æ•°çš„ä½œç”¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹é€šè¿‡è¯¥å‡½æ•°å¤„ç†çš„äººå·¥åˆ¶ä½œçš„åˆ†å¸ƒå›¾ï¼ˆå›¾è¡¨[8.21](#x1-152038r21)ï¼‰ã€‚æˆ‘ç”¨å®ƒä»¬æ¥è°ƒè¯•å‡½æ•°å¹¶ç¡®ä¿å…¶æŒ‰é¢„æœŸå·¥ä½œã€‚è¿™äº›æ£€æŸ¥çš„ä»£ç åœ¨Chapter08/adhoc/distr_test.pyä¸­ã€‚
- en: '![PIC](img/B22150_08_21.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_21.png)'
- en: 'FigureÂ 8.21: The sample of the probability distribution transformation applied
    to a normal distribution'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨8.21ï¼šåº”ç”¨äºæ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡åˆ†å¸ƒå˜æ¢çš„æ ·æœ¬
- en: The top chart of FigureÂ [8.21](#x1-152038r21) (named Source) is a normal distribution
    with Î¼ = 0 and Ïƒ = 3\. The second chart (named Projected) is obtained from distribution
    projection with Î³ = 0.9 and is shifted to the right with reward=2.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨é¡¶éƒ¨çš„[8.21](#x1-152038r21)ï¼ˆåä¸ºæºï¼‰æ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒï¼Œå…¶ä¸­Î¼ = 0ï¼ŒÏƒ = 3ã€‚ç¬¬äºŒå¼ å›¾ï¼ˆåä¸ºæŠ•å½±ï¼‰æ˜¯ä»åˆ†å¸ƒæŠ•å½±å¾—åˆ°çš„ï¼ŒÎ³ =
    0.9ï¼Œå¹¶ä¸”å‘å³åç§»ï¼Œreward=2ã€‚
- en: In the situation where we pass done=True with the same data, the result will
    be different and is shown in FigureÂ [8.22](#x1-152040r22). In such cases, the
    source distribution will be ignored completely, and the result will have only
    the reward projected.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ä¼ é€’done=Trueçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç›¸åŒæ•°æ®ï¼Œç»“æœå°†ä¼šæœ‰æ‰€ä¸åŒï¼Œå¹¶æ˜¾ç¤ºåœ¨å›¾è¡¨[8.22](#x1-152040r22)ä¸­ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæºåˆ†å¸ƒå°†è¢«å®Œå…¨å¿½ç•¥ï¼Œç»“æœå°†åªæœ‰é¢„æœŸå¥–åŠ±ã€‚
- en: '![PIC](img/B22150_08_22.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_22.png)'
- en: 'FigureÂ 8.22: The projection of distribution for the final step in the episode'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾è¡¨8.22ï¼šåœ¨å‰§é›†æœ€åä¸€æ­¥çš„åˆ†å¸ƒæŠ•å½±
- en: The implementation of this method is in Chapter08/07_dqn_distrib.py, which has
    an optional command-line parameter, --img-path. If this option is given, it has
    to be a directory where plots with a probability distribution from a fixed set
    of states will be stored during the training. This is useful to monitor how the
    model converges from uniform probability in the beginning of the training to a
    more spiked weight of probability masses. Sample images from my experiments are
    shown in FigureÂ [8.24](#x1-153003r24) and FigureÂ [8.25](#x1-153005r25).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•çš„å®ç°ä½äºChapter08/07_dqn_distrib.pyä¸­ï¼Œå®ƒå…·æœ‰ä¸€ä¸ªå¯é€‰çš„å‘½ä»¤è¡Œå‚æ•°--img-pathã€‚å¦‚æœç»™å‡ºæ­¤é€‰é¡¹ï¼Œå®ƒå¿…é¡»æ˜¯ä¸€ä¸ªç›®å½•ï¼Œåœ¨è®­ç»ƒæœŸé—´å°†ä»¥å›ºå®šçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒå­˜å‚¨å›¾åƒã€‚è¿™å¯¹äºç›‘è§†æ¨¡å‹å¦‚ä½•ä»å¼€å§‹çš„å‡åŒ€æ¦‚ç‡æ”¶æ•›åˆ°æ›´å¤šå°–å³°æ¦‚ç‡è´¨é‡å¾ˆæœ‰ç”¨ã€‚æˆ‘çš„å®éªŒä¸­çš„ç¤ºä¾‹å›¾åƒæ˜¾ç¤ºåœ¨å›¾è¡¨[8.24](#x1-153003r24)å’Œå›¾è¡¨[8.25](#x1-153005r25)ä¸­ã€‚
- en: Iâ€™m going to show only essential pieces of the implementation here. The core
    of the method, the distr_projection function, was already covered, and it is the
    most complicated piece. What is still missing is the network architecture and
    modified loss function, which we will describe here.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿™é‡Œåªå±•ç¤ºå®ç°çš„åŸºæœ¬éƒ¨åˆ†ã€‚æ–¹æ³•çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œdistr_projectionå‡½æ•°å·²ç»è¦†ç›–è¿‡äº†ï¼Œå®ƒæ˜¯æœ€å¤æ‚çš„éƒ¨åˆ†ã€‚ç°åœ¨ç¼ºå¤±çš„æ˜¯ç½‘ç»œæ¶æ„å’Œä¿®æ”¹çš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å°†åœ¨è¿™é‡Œæè¿°å®ƒä»¬ã€‚
- en: 'Letâ€™s start with the network, which is in lib/dqn_extra.py, in the DistributionalDQN
    class:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç½‘ç»œå¼€å§‹ï¼Œè¯¥ç½‘ç»œä½äºlib/dqn_extra.pyä¸­ï¼Œåœ¨DistributionalDQNç±»ä¸­ï¼š
- en: '[PRE50]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The main difference is the output of the fully connected layer. Now it outputs
    the vector of n_actions * N_ATOMS values, which is 6 Ã— 51 = 306 for Pong. For
    every action, it needs to predict the probability distribution on 51 atoms. Every
    atom (called support) has a value, which corresponds to a particular reward. Those
    atomsâ€™ rewards are evenly distributed from -10 to 10, which gives a grid with
    step 0.4\. Those supports are stored in the networkâ€™s buffer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦åŒºåˆ«åœ¨äºå…¨è¿æ¥å±‚çš„è¾“å‡ºã€‚ç°åœ¨å®ƒè¾“å‡ºn_actions * N_ATOMSå€¼çš„å‘é‡ï¼Œå³6Ã—51 = 306å¯¹äºPongã€‚å¯¹äºæ¯ä¸ªåŠ¨ä½œï¼Œå®ƒéœ€è¦é¢„æµ‹51ä¸ªåŸå­ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒã€‚æ¯ä¸ªåŸå­ï¼ˆç§°ä¸ºæ”¯æŒï¼‰å…·æœ‰ä¸€ä¸ªå€¼ï¼Œè¯¥å€¼å¯¹åº”äºç‰¹å®šçš„å¥–åŠ±ã€‚è¿™äº›åŸå­çš„å¥–åŠ±å‡åŒ€åˆ†å¸ƒåœ¨-10åˆ°10ä¹‹é—´ï¼Œè¿™ç»™å‡ºäº†æ­¥é•¿ä¸º0.4çš„ç½‘æ ¼ã€‚è¿™äº›æ”¯æŒå­˜å‚¨åœ¨ç½‘ç»œçš„ç¼“å†²åŒºä¸­ã€‚
- en: 'The forward() method returns the predicted probability distribution as a 3D
    tensor (batch, actions, and supports):'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: forward()æ–¹æ³•å°†é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä½œä¸º3Då¼ é‡ï¼ˆæ‰¹æ¬¡ï¼ŒåŠ¨ä½œå’Œæ”¯æŒï¼‰è¿”å›ï¼š
- en: '[PRE51]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Besides forward(), we define the both() method, which calculates the probability
    distribution for atoms and Q-values in one call.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†forward()ï¼Œæˆ‘ä»¬è¿˜å®šä¹‰äº†both()æ–¹æ³•ï¼Œå®ƒä¸€æ¬¡è®¡ç®—åŸå­å’ŒQå€¼çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: 'The network also defines several helper functions to simplify the calculation
    of Q-values and apply softmax on the probability distribution:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œè¿˜å®šä¹‰äº†å‡ ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä»¥ç®€åŒ–Qå€¼çš„è®¡ç®—å¹¶åœ¨æ¦‚ç‡åˆ†å¸ƒä¸Šåº”ç”¨softmaxï¼š
- en: '[PRE52]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The final change is the new loss function that has to apply distribution projection
    instead of the Bellman equation, and calculate KL divergence between predicted
    and projected distributions:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åçš„å˜åŒ–æ˜¯æ–°çš„æŸå¤±å‡½æ•°ï¼Œå®ƒå¿…é¡»åº”ç”¨åˆ†å¸ƒæŠ•å½±ï¼Œè€Œä¸æ˜¯è´å°”æ›¼æ–¹ç¨‹ï¼Œå¹¶è®¡ç®—é¢„æµ‹åˆ†å¸ƒä¸æŠ•å½±åˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦ï¼š
- en: '[PRE53]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The preceding code is not very complicated; it just prepares to call distr_projection
    and KL divergence, which is defined as:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šé¢çš„ä»£ç å¹¶ä¸å¤æ‚ï¼›å®ƒåªæ˜¯å‡†å¤‡è°ƒç”¨ distr_projection å’Œ KL æ•£åº¦ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq38.png)'
  id: totrans-400
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq38.png)'
- en: To calculate the logarithm of probability, we use the PyTorch log_softmax function,
    which combines both log and softmax in a numerically stable way.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—æ¦‚ç‡çš„å¯¹æ•°ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch çš„ log_softmax å‡½æ•°ï¼Œå®ƒä»¥æ•°å€¼ç¨³å®šçš„æ–¹å¼å°†å¯¹æ•°å’Œ softmax ç»“åˆåœ¨ä¸€èµ·ã€‚
- en: Results
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: From my experiments, the distributional version of DQN converged a bit slower
    and less stably than the original DQN, which is not surprising, as the network
    output is now 51 times larger and the loss function has changed. Without hyperparameter
    tuning (which will be described in the next subsection), the distributional version
    requires 20% more episodes to solve the game.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®æˆ‘çš„å®éªŒï¼Œåˆ†å¸ƒå¼ç‰ˆæœ¬çš„ DQN æ”¶æ•›é€Ÿåº¦ç¨æ…¢ä¸”ä¸å¤ªç¨³å®šï¼Œä½äºåŸå§‹çš„ DQNï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºç½‘ç»œè¾“å‡ºç°åœ¨å¤§äº† 51 å€ï¼Œä¸”æŸå¤±å‡½æ•°å‘ç”Ÿäº†å˜åŒ–ã€‚å¦‚æœæ²¡æœ‰è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜ï¼ˆå°†åœ¨ä¸‹ä¸€å°èŠ‚ä¸­æè¿°ï¼‰ï¼Œåˆ†å¸ƒå¼ç‰ˆæœ¬éœ€è¦å¤š
    20% çš„å›åˆæ•°æ‰èƒ½è§£å†³æ¸¸æˆã€‚
- en: Another factor that might be important here is that Pong is just too simple
    a game to draw conclusions. In the A Distributional Perspective paper, the authors
    reported state-of-the-art scores (at the time of publishing in 2017) for more
    than half of the games from the Atari benchmark (Pong was not among them).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½é‡è¦çš„å› ç´ æ˜¯ï¼ŒPong æ¸¸æˆå¤ªç®€å•ï¼Œéš¾ä»¥å¾—å‡ºç»“è®ºã€‚åœ¨ã€ŠA Distributional Perspectiveã€‹ä¸€æ–‡ä¸­ï¼Œä½œè€…æŠ¥å‘Šäº†å½“æ—¶ï¼ˆ2017å¹´å‡ºç‰ˆï¼‰å¤§éƒ¨åˆ†
    Atari åŸºå‡†æ¸¸æˆçš„æœ€å…ˆè¿›å¾—åˆ†ï¼ˆPong å¹¶ä¸åœ¨å…¶ä¸­ï¼‰ã€‚
- en: 'The following are charts comparing reward dynamics and loss for the distributional
    DQN. As you can see, the reward dynamics for the distributional method is worse
    than the baseline DQN:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ¯”è¾ƒåˆ†å¸ƒå¼ DQN å¥–åŠ±åŠ¨æ€å’ŒæŸå¤±çš„å›¾è¡¨ã€‚æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œåˆ†å¸ƒå¼æ–¹æ³•çš„å¥–åŠ±åŠ¨æ€æ¯”åŸºå‡† DQN å·®ï¼š
- en: '![PIC](img/B22150_08_23.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_08_23.png)'
- en: 'FigureÂ 8.23: Reward dynamics (left) and loss decrease (right)'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.23ï¼šå¥–åŠ±åŠ¨æ€ï¼ˆå·¦ï¼‰å’ŒæŸå¤±ä¸‹é™ï¼ˆå³ï¼‰
- en: 'It might be interesting to look into the dynamics of the probability distribution
    during the training. If you start the training with the --img-path parameter (providing
    the directory name), the training process will save plots with the probability
    distribution for a fixed set of states. For example, the following figure shows
    the probability distribution for all six actions for one state at the beginning
    of the training (after 30k frames):'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æœ‰è¶£çš„æ˜¯ï¼Œè§‚å¯Ÿè®­ç»ƒè¿‡ç¨‹ä¸­æ¦‚ç‡åˆ†å¸ƒçš„åŠ¨æ€ã€‚å¦‚æœä½ ä½¿ç”¨`--img-path`å‚æ•°ï¼ˆæä¾›ç›®å½•åï¼‰å¼€å§‹è®­ç»ƒï¼Œè®­ç»ƒè¿‡ç¨‹å°†ä¼šä¿å­˜ä¸€ä¸ªå›ºå®šçŠ¶æ€é›†çš„æ¦‚ç‡åˆ†å¸ƒå›¾ã€‚ä¾‹å¦‚ï¼Œä»¥ä¸‹å›¾ç¤ºå±•ç¤ºäº†è®­ç»ƒå¼€å§‹æ—¶ï¼ˆç»è¿‡
    30k å¸§ï¼‰ä¸€ä¸ªçŠ¶æ€ä¸‹æ‰€æœ‰å…­ä¸ªåŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼š
- en: '![PIC](img/file68.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file68.png)'
- en: 'FigureÂ 8.24: Probability distribution at the beginning of training'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.24ï¼šè®­ç»ƒå¼€å§‹æ—¶çš„æ¦‚ç‡åˆ†å¸ƒ
- en: 'All the distributions are very wide (as the network hasnâ€™t converged yet),
    and the peak in the middle corresponds to the negative reward that the network
    expects to get from its actions. The same state after 500k frames of training
    is shown in the following figure:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„åˆ†å¸ƒéƒ½å¾ˆå®½ï¼ˆå› ä¸ºç½‘ç»œè¿˜æœªæ”¶æ•›ï¼‰ï¼Œä¸­é—´çš„å³°å€¼å¯¹åº”äºç½‘ç»œæœŸæœ›ä»å…¶åŠ¨ä½œä¸­è·å¾—çš„è´Ÿå¥–åŠ±ã€‚ç»è¿‡ 500k å¸§è®­ç»ƒåçš„ç›¸åŒçŠ¶æ€å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![PIC](img/file69.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file69.png)'
- en: 'FigureÂ 8.25: Probability distribution produced by the trained network'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.25ï¼šè®­ç»ƒç½‘ç»œäº§ç”Ÿçš„æ¦‚ç‡åˆ†å¸ƒ
- en: Now we can see that different actions have different distributions. The first
    action (which corresponds to the NOOP, the do nothing action) has its distribution
    shifted to the left, so doing nothing in this state usually leads to losing. The
    fifth action, which is RIGHTFIRE, has the mean value shifted to the right, so
    this action leads to a better score.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸åŒçš„åŠ¨ä½œæœ‰ä¸åŒçš„åˆ†å¸ƒã€‚ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼ˆå¯¹åº”äº NOOPï¼Œå³ä¸åšä»»ä½•åŠ¨ä½œï¼‰å…¶åˆ†å¸ƒå‘å·¦åç§»ï¼Œå› æ­¤åœ¨è¯¥çŠ¶æ€ä¸‹é€šå¸¸ä»€ä¹ˆä¹Ÿä¸åšä¼šå¯¼è‡´å¤±è´¥ã€‚ç¬¬äº”ä¸ªåŠ¨ä½œï¼Œå³
    RIGHTFIREï¼Œå…¶å‡å€¼å‘å³åç§»ï¼Œå› æ­¤è¿™ä¸ªåŠ¨ä½œä¼šå¸¦æ¥æ›´å¥½çš„å¾—åˆ†ã€‚
- en: Hyperparameter tuning
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: The tuning of hyperparameters was not very fruitful. After 30 tuning iterations,
    there were no combinations of learning rate and gamma that were able to converge
    faster than the common set of parameters.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜çš„æ•ˆæœå¹¶ä¸æ˜¾è‘—ã€‚ç»è¿‡ 30 æ¬¡è°ƒä¼˜è¿­ä»£åï¼Œæ²¡æœ‰ä»»ä½•å­¦ä¹ ç‡å’Œ gamma çš„ç»„åˆèƒ½å¤Ÿæ¯”å¸¸è§„çš„å‚æ•°é›†æ›´å¿«åœ°æ”¶æ•›ã€‚
- en: Combining everything
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»¼åˆæ‰€æœ‰å†…å®¹
- en: 'You have now seen all the DQN improvements mentioned in the paper Rainbow:
    Combining Improvements in Deep Reinforcement Learning, but it was done in an incremental
    way, which (I hope) was helpful to understand the idea and implementation of every
    improvement. The main point of the paper was to combine those improvements and
    check the results. In the final example, Iâ€™ve decided to exclude categorical DQN
    and double DQN from the final system, as they havenâ€™t shown too much improvement
    on our guinea pig environment. If you want, you can add them and try using a different
    game. The complete example is available in Chapter08/08_dqn_rainbow.py.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½ ç°åœ¨å·²ç»çœ‹åˆ°äº†è®ºæ–‡ã€ŠRainbow: Combining Improvements in Deep Reinforcement Learningã€‹ä¸­æåˆ°çš„æ‰€æœ‰
    DQN æ”¹è¿›ï¼Œä½†è¿™äº›æ”¹è¿›æ˜¯ä»¥é€’å¢æ–¹å¼å®Œæˆçš„ï¼Œï¼ˆæˆ‘å¸Œæœ›ï¼‰è¿™ç§æ–¹å¼æœ‰åŠ©äºç†è§£æ¯ä¸ªæ”¹è¿›çš„æ€è·¯å’Œå®ç°ã€‚è®ºæ–‡çš„ä¸»è¦å†…å®¹æ˜¯å°†è¿™äº›æ”¹è¿›ç»“åˆèµ·æ¥å¹¶æ£€æŸ¥ç»“æœã€‚åœ¨æœ€ç»ˆçš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘å†³å®šå°†ç±»åˆ«
    DQN å’ŒåŒé‡ DQN ä»æœ€ç»ˆç³»ç»Ÿä¸­æ’é™¤ï¼Œå› ä¸ºå®ƒä»¬åœ¨æˆ‘ä»¬çš„è¯•éªŒç¯å¢ƒä¸­å¹¶æœªå¸¦æ¥å¤ªå¤§çš„æ”¹è¿›ã€‚å¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥å°†å®ƒä»¬æ·»åŠ è¿›æ¥å¹¶å°è¯•ä½¿ç”¨ä¸åŒçš„æ¸¸æˆã€‚å®Œæ•´çš„ç¤ºä¾‹ä»£ç å¯ä»¥åœ¨
    Chapter08/08_dqn_rainbow.py ä¸­æ‰¾åˆ°ã€‚'
- en: 'First of all, we need to define our network architecture and the methods that
    have contributed to it:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æˆ‘ä»¬çš„ç½‘ç»œæ¶æ„ä»¥åŠä¸ºå…¶åšå‡ºè´¡çŒ®çš„æ–¹æ³•ï¼š
- en: 'Dueling DQN: Our network will have two separate paths for the value of the
    state distribution and advantage distribution. On the output, both paths will
    be summed together, providing the final value probability distributions for actions.
    To force the advantage distribution to have a zero mean, we will subtract the
    distribution with the mean advantage in every atom.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æŠ— DQNï¼šæˆ‘ä»¬çš„ç½‘ç»œå°†æœ‰ä¸¤ä¸ªç‹¬ç«‹çš„è·¯å¾„ï¼Œä¸€ä¸ªç”¨äºçŠ¶æ€åˆ†å¸ƒçš„ä»·å€¼ï¼Œå¦ä¸€ä¸ªç”¨äºä¼˜åŠ¿åˆ†å¸ƒã€‚åœ¨è¾“å‡ºç«¯ï¼Œè¿™ä¸¤ä¸ªè·¯å¾„å°†ç›¸åŠ ï¼Œä»è€Œæä¾›åŠ¨ä½œçš„æœ€ç»ˆä»·å€¼æ¦‚ç‡åˆ†å¸ƒã€‚ä¸ºäº†å¼ºåˆ¶ä¼˜åŠ¿åˆ†å¸ƒå…·æœ‰é›¶å‡å€¼ï¼Œæˆ‘ä»¬å°†åœ¨æ¯ä¸ªåŸå­ä¸­å‡å»å…·æœ‰å‡å€¼ä¼˜åŠ¿çš„åˆ†å¸ƒã€‚
- en: 'Noisy networks: Our linear layers in the value and advantage paths will be
    noisy variants of nn.Linear.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å™ªå£°ç½‘ç»œï¼šæˆ‘ä»¬åœ¨ä»·å€¼å’Œä¼˜åŠ¿è·¯å¾„ä¸­çš„çº¿æ€§å±‚å°†æ˜¯ nn.Linear çš„å™ªå£°å˜ä½“ã€‚
- en: In addition to network architecture changes, we will use the prioritized replay
    buffer to keep environment transitions and sample them proportionally to the MSE
    loss.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ç½‘ç»œæ¶æ„çš„å˜åŒ–ï¼Œæˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¼˜å…ˆå›æ”¾ç¼“å†²åŒºæ¥ä¿æŒç¯å¢ƒè½¬ç§»ï¼Œå¹¶æŒ‰æ¯”ä¾‹ä» MSE æŸå¤±ä¸­é‡‡æ ·ã€‚
- en: Finally, we will unroll the Bellman equation to n-steps.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†å±•å¼€ Bellman æ–¹ç¨‹ï¼Œä½¿ç”¨ n æ­¥æ³•ã€‚
- en: Iâ€™m not going to repeat all the code, as individual methods have already been
    given in the preceding sections, and it should be obvious what the final result
    of combining the methods will look like. If you have any trouble, you can find
    the code on GitHub.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸æ‰“ç®—é‡å¤æ‰€æœ‰çš„ä»£ç ï¼Œå› ä¸ºå‰é¢çš„ç« èŠ‚å·²ç»ç»™å‡ºäº†å„ä¸ªæ–¹æ³•ï¼Œè€Œä¸”ç»“åˆè¿™äº›æ–¹æ³•çš„æœ€ç»ˆç»“æœåº”è¯¥æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚å¦‚æœä½ é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥åœ¨ GitHub ä¸Šæ‰¾åˆ°ä»£ç ã€‚
- en: Results
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'The following are charts comparing the smoothed reward and count of steps with
    the baseline DQN. In both, we can see significant improvement in terms of the
    amount of games played:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸åŸºå‡† DQN æ¯”è¾ƒçš„å¹³æ»‘å¥–åŠ±å’Œæ­¥éª¤è®¡æ•°å›¾è¡¨ã€‚åœ¨è¿™ä¸¤è€…ä¸­ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥çœ‹åˆ°æ¸¸æˆæ•°é‡æ–¹é¢çš„æ˜¾è‘—æ”¹å–„ï¼š
- en: '![PIC](img/B22150_08_26.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_26.png)'
- en: 'FigureÂ 8.26: Comparison of baseline DQN with combined system'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.26ï¼šåŸºå‡† DQN ä¸ç»„åˆç³»ç»Ÿçš„æ¯”è¾ƒ
- en: 'In addition to the averaged reward, it is worth checking the raw reward chart,
    which is even more dramatic than the smoothed reward. It shows that our system
    was able to jump from the negative outcome to the positive very quickly â€“ after
    just 100 games, it won almost every game. So, it took us another 100 games to
    make the smoothed reward reach +18:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å¹³å‡å¥–åŠ±ï¼Œå€¼å¾—æ£€æŸ¥ä¸€ä¸‹åŸå§‹å¥–åŠ±å›¾è¡¨ï¼Œç»“æœæ¯”å¹³æ»‘å¥–åŠ±æ›´ä¸ºæˆå‰§åŒ–ã€‚å®ƒæ˜¾ç¤ºæˆ‘ä»¬çš„ç³»ç»Ÿèƒ½å¤Ÿéå¸¸è¿…é€Ÿåœ°ä»è´Ÿé¢ç»“æœè·³è·ƒåˆ°æ­£é¢â€”â€”ä»…ä»…ç»è¿‡ 100 åœºæ¸¸æˆï¼Œå®ƒå‡ ä¹èµ¢å¾—äº†æ¯ä¸€åœºæ¯”èµ›ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆèŠ±äº†
    100 åœºæ¯”èµ›æ‰ä½¿å¹³æ»‘å¥–åŠ±è¾¾åˆ° +18ï¼š
- en: '![PIC](img/B22150_08_27.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_27.png)'
- en: 'FigureÂ 8.27: Raw reward for combined system'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.27ï¼šç»„åˆç³»ç»Ÿçš„åŸå§‹å¥–åŠ±
- en: 'As a downside, the combined system is slower than the baseline, as we have
    a more complicated NN architecture and prioritized replay buffer. The FPS chart
    shows that the combined system starts at 170 FPS and degrades to 130 FPS due to
    the ğ’ª(n) buffer complexity:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªç¼ºç‚¹ï¼Œç»„åˆç³»ç»Ÿçš„é€Ÿåº¦æ¯”åŸºå‡†ç³»ç»Ÿæ…¢ï¼Œå› ä¸ºæˆ‘ä»¬é‡‡ç”¨äº†æ›´å¤æ‚çš„ç¥ç»ç½‘ç»œæ¶æ„å’Œä¼˜å…ˆå›æ”¾ç¼“å†²åŒºã€‚FPS å›¾è¡¨æ˜¾ç¤ºï¼Œç»„åˆç³»ç»Ÿä» 170 FPS å¼€å§‹ï¼Œå› ğ’ª(n)ç¼“å†²åŒºå¤æ‚æ€§è€Œé™è‡³
    130 FPSï¼š
- en: '![PIC](img/B22150_08_28.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_28.png)'
- en: 'FigureÂ 8.28: Performance comparison (in frames per second)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.28ï¼šæ€§èƒ½æ¯”è¾ƒï¼ˆä»¥æ¯ç§’å¸§æ•°è®¡ï¼‰
- en: Hyperparameter tuning
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°è°ƒä¼˜
- en: 'Tuning was done as before and was able to further improve the combined system
    training in terms of games played before solving the game. The following are charts
    comparing the tuned baseline DQN with the tuned combined system:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: è°ƒä¼˜ä»ç„¶åƒä¹‹å‰é‚£æ ·è¿›è¡Œï¼Œä¸”åœ¨â€œè§£å†³æ¸¸æˆå‰ç©è¿‡çš„æ¸¸æˆæ•°â€æ–¹é¢ï¼Œèƒ½å¤Ÿè¿›ä¸€æ­¥æå‡ç»„åˆç³»ç»Ÿçš„è®­ç»ƒæ•ˆæœã€‚ä»¥ä¸‹æ˜¯è°ƒä¼˜åçš„åŸºçº¿DQNä¸è°ƒä¼˜åçš„ç»„åˆç³»ç»Ÿçš„æ¯”è¾ƒå›¾è¡¨ï¼š
- en: '![PIC](img/B22150_08_29.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_29.png)'
- en: 'FigureÂ 8.29: Comparison of tuned baseline DQN with tuned combined system'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.29ï¼šå·²è°ƒä¼˜åŸºçº¿DQNä¸å·²è°ƒä¼˜ç»„åˆç³»ç»Ÿçš„å¯¹æ¯”
- en: 'Another chart showing the effect of the tuning is the comparison of raw game
    rewards before and after the tuning. The tuned system starts to get the maximum
    score even earlier â€” just after 40 games, which is quite impressive:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ˜¾ç¤ºè°ƒä¼˜æ•ˆæœçš„å›¾è¡¨æ˜¯å¯¹æ¯”è°ƒä¼˜å‰åçš„åŸå§‹æ¸¸æˆå¥–åŠ±ã€‚è°ƒä¼˜åçš„ç³»ç»Ÿå¼€å§‹åœ¨40å±€æ¸¸æˆåå°±è·å¾—æœ€é«˜åˆ†ï¼Œè¿™éå¸¸ä»¤äººå°è±¡æ·±åˆ»ï¼š
- en: '![PIC](img/B22150_08_30.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_08_30.png)'
- en: 'FigureÂ 8.30: Raw reward for untuned and tuned combined DQN'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.30ï¼šæœªè°ƒä¼˜å’Œå·²è°ƒä¼˜ç»„åˆDQNçš„åŸå§‹å¥–åŠ±
- en: Summary
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'In this chapter, we have walked through and implemented a lot of DQN improvements
    that have been discovered by researchers since the first DQN paper was published
    in 2015\. This list is far from complete. First of all, for the list of methods,
    I used the paper Rainbow: Combining improvements in deep reinforcement learning
    [[Hes+18](#)], which was published by DeepMind, so the list of methods is definitely
    biased to DeepMind papers. Secondly, RL is so active nowadays that new papers
    come out almost every day, which makes it very hard to keep up, even if we limit
    ourselves to one kind of RL model, such as a DQN. The goal of this chapter was
    to give you a practical view of different ideas that the field has developed.'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å›é¡¾å¹¶å®ç°äº†è‡ª2015å¹´é¦–æ¬¡å‘å¸ƒDQNè®ºæ–‡ä»¥æ¥ï¼Œç ”ç©¶äººå‘˜å‘ç°çš„è®¸å¤šDQNæ”¹è¿›ã€‚è¿™ä»½æ¸…å•è¿œæœªå®Œæ•´ã€‚é¦–å…ˆï¼Œå…³äºæ–¹æ³•åˆ—è¡¨ï¼Œæˆ‘ä½¿ç”¨äº†DeepMindå‘å¸ƒçš„è®ºæ–‡ã€ŠRainbowï¼šç»“åˆæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„æ”¹è¿›ã€‹[[Hes+18](#)]ï¼Œå› æ­¤æ–¹æ³•åˆ—è¡¨æ— ç–‘åå‘äºDeepMindçš„è®ºæ–‡ã€‚å…¶æ¬¡ï¼Œå¼ºåŒ–å­¦ä¹ å¦‚ä»Šå‘å±•éå¸¸è¿…é€Ÿï¼Œå‡ ä¹æ¯å¤©éƒ½æœ‰æ–°è®ºæ–‡å‘å¸ƒï¼Œå³ä½¿æˆ‘ä»¬åªå±€é™äºä¸€ç§å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œæ¯”å¦‚DQNï¼Œä¹Ÿå¾ˆéš¾è·Ÿä¸Šè¿›å±•ã€‚æœ¬ç« çš„ç›®æ ‡æ˜¯è®©ä½ äº†è§£è¯¥é¢†åŸŸå·²ç»å‘å±•å‡ºçš„ä¸€äº›ä¸åŒçš„å®é™…æ–¹æ³•ã€‚
- en: In the next chapter, we will continue discussing practical DQN applications
    from an engineering perspective by talking about ways to improve DQN performance
    without touching the underlying method.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä»å·¥ç¨‹è§’åº¦è®¨è®ºDQNçš„å®é™…åº”ç”¨ï¼Œè°ˆè®ºå¦‚ä½•åœ¨ä¸è§¦åŠåº•å±‚æ–¹æ³•çš„æƒ…å†µä¸‹æå‡DQNçš„æ€§èƒ½ã€‚
- en: Join our community on Discord
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ å…¥æˆ‘ä»¬çš„Discordç¤¾åŒº
- en: Read this book alongside other users, Deep Learning experts, and the author
    himself. Ask questions, provide solutions to other readers, chat with the author
    via Ask Me Anything sessions, and much more. Scan the QR code or visit the link
    to join the community. [https://packt.link/rl](https://packt.link/rl)
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–ç”¨æˆ·ã€æ·±åº¦å­¦ä¹ ä¸“å®¶ä»¥åŠä½œè€…æœ¬äººä¸€èµ·é˜…è¯»æœ¬ä¹¦ã€‚æé—®ã€ä¸ºå…¶ä»–è¯»è€…æä¾›è§£å†³æ–¹æ¡ˆï¼Œé€šè¿‡â€œé—®æˆ‘ä»»ä½•é—®é¢˜â€ç¯èŠ‚ä¸ä½œè€…èŠå¤©ï¼Œæ›´å¤šå†…å®¹å°½åœ¨å…¶ä¸­ã€‚æ‰«æäºŒç»´ç æˆ–è®¿é—®é“¾æ¥åŠ å…¥ç¤¾åŒºã€‚[https://packt.link/rl](https://packt.link/rl)
- en: '![PIC](img/file1.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
