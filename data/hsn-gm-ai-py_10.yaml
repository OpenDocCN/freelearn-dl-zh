- en: Policy Gradient Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: Previously, our **reinforcement learning** (**RL**) methods have focused on
    finding the maximum or best value for choosing a particular action in any given
    state. While this has worked well for us in previous chapters, it certainly is
    not without its own problems, one of which is always determining when to actually
    take the max or best action, hence our exploration/exploitation trade-off. As
    we have seen, the best action is not always the best and it can be better to take
    the average of the best. However, mathematically averaging is dangerous and tells
    us nothing about what the agent actually sampled in the environment. Ideally,
    we want a method that can learn the distribution of actions for each state in
    the environment. This introduces a new class of methods in RL known as **Policy
    Gradient** (**PG**) methods and this will be our focus in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们的**强化学习**（**RL**）方法主要集中在寻找在任何给定状态下选择特定动作的最大值或最佳值。虽然这在之前的章节中对我们来说效果不错，但这种方法当然也存在着它自己的问题，其中之一就是始终需要确定何时实际采取最大或最佳动作，这就是我们的探索/利用权衡。正如我们所看到的，最佳动作并不总是最佳选择，有时取平均最佳动作可能更好。然而，从数学上来说，平均是危险的，它并不能告诉我们代理在环境中实际采样了什么。理想情况下，我们希望有一种方法可以学习环境中每个状态的动作分布。这引入了强化学习中的一个新类别的方法，称为**策略梯度**（**PG**）方法，这也是本章的重点。
- en: In this chapter, we will take a look at PG methods and how they improve on our
    previous attempts in many different ways. We first look at understanding the intuition
    behind PG methods and then look to the first method, REINFORCE. After that, we
    will explore the class of advantage functions and introduce ourselves to actor-critic
    methods. From there, we will move on to looking at **Deep Deterministic Policy
    Gradient** methods and how they can be used to solve Lunar Lander. Then, we will
    progress to an advanced method known as **Trust Region Policy Optimization** and
    how it estimates returns based on regions of trust.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨PG方法以及它们如何以许多不同的方式改进我们之前的尝试。我们首先了解PG方法背后的直觉，然后转向第一个方法，REINFORCE。之后，我们将探讨优势函数类别，并介绍演员-评论家方法。从那里，我们将继续探讨**深度确定性策略梯度**方法以及它们如何用于解决月球着陆问题。然后，我们将介绍一种称为**信任区域策略优化**的高级方法，以及它是如何根据信任区域估计回报的。
- en: 'Following is a summary of the main topics we will focus on in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章我们将重点关注的主要主题总结：
- en: Understanding policy gradient methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解策略梯度方法
- en: Introducing REINFORCE
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 REINFORCE
- en: Using advantage actor-critic
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用优势演员-评论家
- en: Building a deep deterministic policy gradient
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度确定性策略梯度
- en: Exploring trust region policy optimization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索信任区域策略优化
- en: PG methods are mathematically far more complex than our previous attempts and
    go deeper into statistical and probabilistic methods. While we will focus on understanding
    the intuition and not the mathematics behind these methods, it may still be confusing
    to some readers. If you find this, you may find a refresher on statistics and
    probability will help. In the next section, we look to begin our understanding
    of the intuition behind PG methods.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PG方法在数学上比我们之前的尝试要复杂得多，并且更深入地涉及到统计和概率方法。虽然我们将专注于理解这些方法的直觉，而不是数学原理，但对于一些读者来说，这仍然可能令人困惑。如果你发现这样，你可能需要复习一下统计学和概率学，这可能会有所帮助。在下一节中，我们将开始理解PG方法背后的直觉。
- en: All of the code for this entire chapter was originally sourced from this GitHub
    repository: [https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL).
    The original author did an excellent job of sourcing the original collection.
    As per usual, the code has been significantly modified to fit the style of this
    book and the other code to be consistent.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有代码最初都来源于这个GitHub仓库：[https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)。原始作者出色地收集了原始资料。按照惯例，代码已经进行了重大修改，以适应本书的风格和其他代码的一致性。
- en: Understanding policy gradient methods
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解策略梯度方法
- en: 'One thing we need to understand about PG methods is why we need them and what
    the intuition is behind them. Then, we can cover some of the mathematics very
    briefly before diving into the code. So, let''s cover the motivation behind using
    PG methods and what they hope to achieve beyond the other previous methods we
    have looked at. I have summarized the main points of why/what PG methods do and
    try to solve:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要了解PG方法的一点是为什么需要它们，以及它们背后的直觉是什么。然后，我们可以在深入研究代码之前简要地介绍一些数学知识。因此，让我们来探讨使用PG方法背后的动机以及它们希望实现的目标，这些目标超越了之前我们研究过的其他方法。我已经总结了为什么/PG方法做什么以及它们试图解决的问题的主要观点：
- en: '**Deterministic versus stochastic functions**: We often learn early in science
    and mathematics that many problems require a single or deterministic answer. In
    the real world, however, we often equate some amount of error to deterministic
    calculations to quantify their accuracy. This quantification of how accurate a
    value is can be taken a step further with stochastic or probabilistic methods.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定性函数与随机函数**：我们在科学和数学的早期学习中经常了解到许多问题需要单一的或确定性答案。然而，在现实世界中，我们经常将一定程度的误差等同于确定性计算来量化其准确性。这种量化一个值准确性的方法可以通过随机或概率方法进一步发展。'
- en: 'Stochastic methods are often used to quantify expectation of risk or uncertainty
    and they do this by finding the distribution that describes a range of values. Whereas
    previously we used a value function to find the optimum state-value that described
    an action, now we want to understand the distribution that generated that value.
    The following diagram shows an example of a deterministic versus stochastic function
    output, a distribution next to the mean, median, mode, and max values:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随机方法通常用于量化风险或不确定性的期望，它们通过找到描述一系列值的分布来实现这一点。而之前我们使用值函数来找到描述动作的最优状态值，现在我们想要了解产生该值的分布。以下图表显示了确定性函数与随机函数输出的一个示例，分布位于均值、中位数、众数和最大值旁边：
- en: '![](img/5cfb41d8-97a3-488f-851d-8fea1ee8cb20.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5cfb41d8-97a3-488f-851d-8fea1ee8cb20.png)'
- en: A skewed normal distribution
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜的正态分布
- en: Previously, we assumed that our agent was always sampling from a perfectly normal
    distribution. This assumption allowed us to use the max or even mean (average)
    values. However, a normal distribution is never just normal and in most cases,
    an environment may not even be distributed close to normal.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们假设我们的智能体总是从完美的正态分布中进行采样。这个假设允许我们使用最大值甚至平均值。然而，正态分布永远不会仅仅是正常的，在大多数情况下，环境可能甚至没有接近正态分布。
- en: '**Deterministic versus stochastic environments**: The other problem we have
    with our assumption of everything being normally distributed is that it often
    isn''t and, in the real world, we often need to interpret an environment as random
    or stochastic. Our previous environments have been for the most part static, meaning
    they change very little between episodes. Real-world environments are never entirely
    static and, in games, that is certainly the case. So, we need an algorithm that
    can respond to random changes in the environment.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确定性环境与随机环境**：我们对所有事物都是正态分布的假设的另一个问题是它往往不是这样，在现实世界中，我们经常需要将环境解释为随机或随机的。我们之前的环境大部分是静态的，这意味着它们在各个剧集之间变化很小。现实世界环境永远不会完全静态，在游戏中更是如此。因此，我们需要一个能够对环境中的随机变化做出反应的算法。'
- en: '**Discrete versus continuous action spaces**: We have already spent some time
    considering discrete versus continuous observation spaces and learned how to handle
    these environments with discretization and deep learning, except real-world environments
    and/or games are not always discrete. That is, instead of discrete actions such
    as up, down, left, and right, we now need to consider continuous actions such
    as left 10-80%, right 10-90%, up 10-90%, and so on. Fortunately, PG methods provide
    a mechanism that makes continuous actions easier to implement. Reciprocally, discrete
    action spaces are doable but don''t train as well as continuous.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散动作空间与连续动作空间**：我们已经花费了一些时间考虑离散与连续的观察空间，并学习了如何通过离散化和深度学习来处理这些环境，但现实世界环境和/或游戏并不总是离散的。也就是说，除了上、下、左、右等离散动作之外，我们现在还需要考虑左10-80%、右10-90%、上10-90%等连续动作。幸运的是，PG方法提供了一种机制，使得连续动作更容易实现。反过来，离散动作空间是可行的，但训练效果不如连续动作空间。'
- en: PG methods work much better in continuous action spaces due to the nature of
    the algorithm itself. They can be used to solve discrete action space environments
    but they generally will not perform as well as the other methods we will cover
    later.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法本身的性质，PG方法在连续动作空间中工作得更好。它们可以用来解决离散动作空间环境，但通常不会像我们后面将要介绍的其他方法表现得那么好。
- en: Now that we understand why we need PG methods, we need to move on to the how
    in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了为什么需要PG方法，那么我们接下来需要在下一节中探讨如何使用它。
- en: Policy gradient ascent
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度上升
- en: 'The basic intuition behind PG methods is we move from finding a value function
    that describes a deterministic policy to a stochastic policy with parameters used
    to define a policy distribution. Thinking this way, we can now assume that our
    policy function needs to be defined so that our policy, π, can be set by adjusting
    parameters θ so that we understand the probability of taking a given action in
    a state. Mathematically, we can simply define this like so:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PG方法背后的基本直觉是我们从寻找描述确定性策略的价值函数转变为具有参数的随机策略，这些参数用于定义策略分布。这样思考，我们现在可以假设我们的策略函数需要被定义，以便我们的策略π可以通过调整参数θ来设置，这样我们就能理解在某个状态下采取特定动作的概率。从数学上讲，我们可以简单地定义如下：
- en: '![](img/00f9e2d1-8818-4ab3-a261-6c999c443811.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00f9e2d1-8818-4ab3-a261-6c999c443811.png)'
- en: You should consider the mathematics we cover in this chapter the minimum you
    need to understand the code. If you are indeed serious about developing your own
    extensions to PG methods, then you likely want to spend some time exploring the
    mathematics further using *An Introduction to Reinforcement Learning* (Barto/Sutton,
    2nd edition, 2017).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该考虑我们在本章中涵盖的数学知识是理解代码所需的最小知识。如果你确实认真考虑开发自己的PG方法扩展，那么你可能想要花些时间进一步探索数学，使用《强化学习导论》（Barto/Sutton，第2版，2017年）。
- en: 'π denotes the policy determined by parameters θ, where we plan to find those
    parameters easily enough with a deep learning network. Now, we have seen previously
    how we used deep learning to minimize the loss of a network with gradient descent
    and we will now turn the problem upside down. We now want to find the parameters
    that give us the best probability of taking an action for a given state that should
    maximize an action to 1.0 or 100%. That means instead of reducing a number, we
    now need to maximize it using gradient ascent. This also transforms our update
    from a value to a parameter that describes the policy and we rewrite our update
    equations like so:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: π表示由参数θ确定的策略，我们计划通过深度学习网络轻松地找到这些参数。现在，我们已经看到我们如何使用深度学习通过梯度下降最小化网络的损失，我们现在将问题颠倒过来。我们现在想要找到那些给出最佳动作概率的参数，对于给定的状态，这些动作应该最大化到1.0或100%。这意味着我们不再减少一个数字，我们现在需要使用梯度上升来最大化它。这也将我们的更新从价值转变为描述策略的参数，并且我们重新编写我们的更新方程如下：
- en: '![](img/4f8529fd-d471-4a6f-b120-93ceebfb8fbd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f8529fd-d471-4a6f-b120-93ceebfb8fbd.png)'
- en: 'In the equation, we have the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程中，我们有以下内容：
- en: '![](img/13e8748f-339a-4b34-a29a-839da749f190.png) The parameter''s value at
    the previous time step'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/13e8748f-339a-4b34-a29a-839da749f190.png) 前一时间步的参数值'
- en: '![](img/74b55c70-2a06-4368-8361-495ce087eda0.png) The learning rate'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/74b55c70-2a06-4368-8361-495ce087eda0.png) 学习率'
- en: '![](img/809ad4df-d51c-4332-8e7d-db6729c249d0.png) The calculated update gradient
    for action *a**, or the optimal action'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/809ad4df-d51c-4332-8e7d-db6729c249d0.png) 针对动作*a*的计算更新梯度，或最优动作'
- en: 'The intuition here is that we are pushing toward the action that will yield
    the best policy. However, what we find is that making a further assumption of
    assuming all pushes are equal is just as egregious. After all, we should be able
    to introduce those deterministic predictions of value back into the preceding
    equation as a further guide to the real value. We can do this by updating the
    last equation like so:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的直觉是我们正在推动向产生最佳策略的动作。然而，我们发现，进一步假设所有推动都是相等的同样也是错误的。毕竟，我们应该能够将那些关于价值的确定性预测重新引入前面的方程中，作为对真实价值的进一步指导。我们可以通过以下方式更新最后一个方程：
- en: '![](img/b4d273e3-613f-495c-926a-3a729c361454.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b4d273e3-613f-495c-926a-3a729c361454.png)'
- en: 'Here, we now introduce the following:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们现在引入以下内容：
- en: '![](img/88a4e9d4-32e1-4fa5-8033-fddde0e88f8e.png): This becomes our guess at
    a Q value for the given state and action pair.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/88a4e9d4-32e1-4fa5-8033-fddde0e88f8e.png)：这是我们对于给定状态和动作对的Q值的猜测。'
- en: 'Hence, state-action pairs with higher estimated Q values will benefit more
    than those that do not, except, we now have to take a step back and reconsider
    our old friend the exploration/exploitation dilemma and consider how our algorithm/agent
    needs to select actions. We no longer want our agent to take just the best or
    random action but, instead, use the learnings of the policy itself. That means
    a couple of things. Our agent now needs to continually sample and learn off of
    the same policy meaning PG is on policy but it also means we need to update our
    update equation to account for this like so:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有更高估计Q值的州-行动对将比那些没有的受益更多，但除了，我们现在必须退一步重新考虑我们的老朋友探索/利用困境，并考虑我们的算法/代理需要如何选择行动。我们不再希望我们的代理只采取最佳或随机行动，而是使用策略本身的学习。这意味着几件事。我们的代理现在需要不断地从同一策略中采样和学习，这意味着PG是按策略的，但也意味着我们需要更新我们的更新方程来考虑这一点，如下所示：
- en: '![](img/361bcbbc-97b1-4c07-8f62-21f759b2eaee.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图1](img/361bcbbc-97b1-4c07-8f62-21f759b2eaee.png)'
- en: 'Here, we now introduce the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们现在引入以下内容：
- en: '![](img/0ab082c0-c579-4786-a43a-f0aac23e503b.png): This is the probability
    of a given action in a given state—essentially, what the policy itself predicts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![概率图](img/0ab082c0-c579-4786-a43a-f0aac23e503b.png)：这是给定状态下给定行动的概率——本质上，这是策略本身所预测的。'
- en: Dividing by the policy's probability of taking an action in a given state accounts
    for how frequently that action may be taken. Hence, if an action is twice as popular
    as another, it will be updated only half as much, but likely for twice the number
    of times. Again, this tries to eliminate skewing of actions that get sampled more
    often and allow for the algorithm to weight those rare but beneficial actions
    more accordingly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将策略在给定状态采取行动的概率进行除法，可以解释该行动可能被采取的频率。因此，如果一个行动比另一个行动流行两倍，那么它只会更新一半的次数，但可能被采取的次数是两倍。再次强调，这试图消除采样频率更高的行动的偏差，并允许算法相应地更重视那些罕见但有益的行动。
- en: Now that you understand the basic intuition of our new update and process, we
    can see how this works in practice. Implementing PG methods in practice is more
    difficult mathematically but fortunately, deep learning alleviates that for us
    by providing gradient ascent as we will see when we tackle our first practical
    algorithm in the next section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了我们新的更新和过程的基本直觉，我们可以看到它在实际中的应用。在实践中实现PG方法在数学上更困难，但幸运的是，深度学习通过提供梯度上升来缓解这一点，正如我们将在下一节解决我们的第一个实际算法时看到的。
- en: Introducing REINFORCE
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入REINFORCE
- en: 'The first algorithm we will look at is known as **REINFORCE**. It introduces
    the concept of PG in a very elegant manner, especially in PyTorch, which masks
    many of the mathematical complexities of this implementation. REINFORCE also works
    by solving the optimization problem in reverse. That is, instead of using gradient
    ascent, it reverses the mathematics so we can express the problem as a loss function
    and hence use gradient descent. The update equation now transforms to the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的第一个算法被称为**REINFORCE**。它以一种非常优雅的方式引入了PG的概念，特别是在PyTorch中，它掩盖了此实现中许多数学复杂性。REINFORCE还通过反向解决优化问题来工作。也就是说，它不是使用梯度上升，而是反转数学，这样我们可以将问题表示为损失函数，从而使用梯度下降。更新方程现在转换为以下形式：
- en: '![](img/8d747008-3ce8-4cb7-9ede-4bb66720b5fa.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图2](img/8d747008-3ce8-4cb7-9ede-4bb66720b5fa.png)'
- en: 'Here, we now assume the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们现在假设以下条件：
- en: '![](img/4217bf51-fd01-413c-9ae3-048cbf1b2a4d.png) This is the advantage over
    the baseline expressed by ![](img/68346397-7b2a-47a9-89fd-3e0e0f38d3aa.png); we
    will get to the advantage function in more detail shortly.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图3](img/4217bf51-fd01-413c-9ae3-048cbf1b2a4d.png) 这是由![优势函数图](img/68346397-7b2a-47a9-89fd-3e0e0f38d3aa.png)表示的相对于基线的优势；我们将在稍后更详细地介绍优势函数。'
- en: '![](img/93e9f09b-c099-4b70-8751-c51453a078ed.png) This is the gradient now
    expressed as a loss and is equivalent to ![](img/0a3b256d-6a7d-44c2-a32c-60c9c7cc198b.png),
    assuming with the chain rule and the derivation of *1/x = log x*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![梯度图](img/93e9f09b-c099-4b70-8751-c51453a078ed.png) 这是现在表示为损失的梯度，并且与![损失函数图](img/0a3b256d-6a7d-44c2-a32c-60c9c7cc198b.png)等价，假设使用链式法则和对*1/x
    = log x*的导数。'
- en: Essentially, we flip the equation using the chain rule and the property *1/x
    = log x*. Again, breaking down the mathematics in detail is outside the scope
    of this book, but the critical intuition here is the use of the log function as
    a derivation trick to invert our equation into a loss function combined with the
    advantage function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，我们使用链式法则和性质 *1/x = log x* 来翻转方程。再次强调，详细解析数学内容超出了本书的范围，但这里的关键直觉是使用对数函数作为导数技巧，将我们的方程转换为结合优势函数的损失函数。
- en: '**REINFORCE** stands for **REward Increment = Non-negative Factor *x* Offset Reinforcement *x *Characteristic E****ligibility**. The
    acronym attempts to describe the mathematical intuition of the algorithm itself,
    where the non-negative factor represents the advantage function, ![](img/5940afc2-e6d5-4774-bcd9-c2749e24b14e.png). Offset
    reinforcement is the gradient itself denoted by ![](img/2763d48c-1832-4d50-b259-747bfdc4cb5b.png). Then,
    we introduce characteristic eligibility, which reverts back to our learning of
    TD and eligibility traces using ![](img/274a93f2-167b-4ccd-9d92-986c6638dab3.png).
    Scaling this whole factor by ![](img/93a38c62-f037-42e7-a4f0-6f8c3f1aa219.png) or
    the learning rate allows us to adjust how quickly the algorithm/agent learns.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**REINFORCE** 代表 **REward Increment = Non-negative Factor *x* Offset Reinforcement
    *x* Characteristic E**ligibility**。这个缩写试图描述算法本身的数学直觉，其中非负因子代表优势函数，![](img/5940afc2-e6d5-4774-bcd9-c2749e24b14e.png)。偏置强化是梯度本身，表示为
    ![](img/2763d48c-1832-4d50-b259-747bfdc4cb5b.png)。然后，我们引入特征有效性，这使我们回到使用 ![](img/274a93f2-167b-4ccd-9d92-986c6638dab3.png)
    学习 TD 和有效性迹的学习。通过 ![](img/93a38c62-f037-42e7-a4f0-6f8c3f1aa219.png) 或学习率来缩放整个因子，使我们能够调整算法/代理学习的速度。'
- en: Being able to intuitively tune the hyperparameters, the learning rate (alpha)
    and discount factor (gamma), should be a skill you have already started to master.
    However, PG methods bring a different intuition into how an agent wants/needs
    to learn. As such, be sure to spend an equal amount of time understanding how
    tuning these values has changed.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 能够直观地调整超参数，学习率（alpha）和折扣因子（gamma），应该是一项你已经开始掌握的技能。然而，PG 方法带来了关于代理想要/需要如何学习的不同直觉。因此，请确保花同样多的时间来理解调整这些值是如何改变的。
- en: 'Of course, as game programmers, the best way for us to understand this is to
    work with the code and that is exactly what we will do in the next exercise. Open
    example `Chapter_8_REINFORCE.py` and follow the exercise here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，作为游戏程序员，我们理解这一点的最好方式是与代码一起工作，这正是我们在下一个练习中将要做的。打开示例 `Chapter_8_REINFORCE.py`
    并遵循这里的练习：
- en: 'REINFORCE in PyTorch becomes a nice compact algorithm and the entire code listing
    is shown here:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，REINFORCE 成为一个紧凑的算法，整个代码列表如下所示：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As usual, we start with our usual imports with one new addition from `torch.distributions`
    called `Categorical`. Now, `Categorical` is used to sample our action space from
    a continuous probability back to discrete action value. After that, we initialize
    our base hyperparameters, `learning_rate` and `gamma`.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同往常，我们从我们常用的导入开始，增加了一个来自 `torch.distributions` 的新导入，名为 `Categorical`。现在，`Categorical`
    用于从连续概率空间采样我们的动作空间到离散动作值。之后，我们初始化我们的基本超参数，`learning_rate` 和 `gamma`。
- en: 'Next, we come to a new class called `REINFORCE`, which encapsulates the functionality
    of our agent algorithm. We have seen most of this code before in DQN and DDQN
    configurations. However, we want to focus on the training function, `train_net`,
    shown here:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们来到一个新的类 `REINFORCE`，它封装了我们的代理算法的功能。我们在 DQN 和 DDQN 配置中已经看到了大部分代码。然而，我们想要关注的是这里所示的训练函数
    `train_net`。
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`train_net` is where we use the loss calculation to push back (backpropagate)
    errors in the policy network. Notice, in this class, we don''t use a replay buffer
    but instead, just use a list called `data`. It should also be clear that we push
    all of the values in the list back through the network.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train_net` 是我们使用损失计算来推动（反向传播）策略网络中的错误的地方。注意，在这个课程中，我们不使用重放缓冲区，而是仅使用一个名为 `data`
    的列表。也应该清楚，我们将列表中的所有值都反向通过网络。'
- en: 'After the class definition, we jump to creating the environment and setting
    up some additional variables, shown here:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在类定义之后，我们跳转到创建环境和设置一些额外的变量，如下所示：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see we are back to playing the Lunar Lander environment. The other variables
    are similar to ones we used before to control the amount of training and how often
    we output results. If you change this to a different environment, you will most
    likely need to adjust these values.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以看到我们又回到了玩月球着陆环境。其他变量与我们之前用来控制训练量和输出结果频率的变量相似。如果你将其更改为不同的环境，你很可能需要调整这些值。
- en: 'Again, the training iteration code is quite similar to our previous examples
    with one keen difference and that is how we sample and execute actions in the
    environment. Here is the code that accomplishes this part:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，训练迭代代码与我们之前的例子非常相似，唯一的显著区别是我们如何在环境中采样和执行动作。以下是完成这一部分的代码：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The main thing to notice here is we are extracting the probability of an action
    from the policy generated by REINFORCE using `pi.act`. After that, we convert
    this probability into a categorical or discrete bin of values with `Categorical`. We
    then extract the discrete action value using `m.sample()`. This conversion is
    necessary for a discrete action space such as the Lunar Lander v2 environment.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里要注意的主要事情是我们正在从 REINFORCE 生成的策略中提取动作的概率，使用 `pi.act`。之后，我们使用 `Categorical` 将这个概率转换为分类或离散的值箱。然后我们使用
    `m.sample()` 提取离散的动作值。这种转换对于离散动作空间，如月球着陆 v2 环境，是必要的。
- en: Later, we will see how we can use this in a continuous space environment without
    the conversion. If you scroll up to the `play_game` function, you will note that
    the same code block is used to extract the action from the policy when playing
    the game. Pay special attention to the last line where `pi.put_data` is used to
    store the results and notice how we are using `torch.log` on the `prop[action]`
    value. Remember, by using the log function here, we convert or reverse the need
    to use gradient ascent to maximize an action value. Instead, we can use gradient
    descent and `backprop` on our policy network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将看到如何在不需要转换的情况下将其应用于连续空间环境。如果你滚动到 `play_game` 函数，你会注意到相同的代码块用于在玩游戏时从策略中提取动作。特别注意最后一行，其中使用了
    `pi.put_data` 来存储结果，并注意我们是如何在 `prop[action]` 值上使用 `torch.log` 的。记住，通过在这里使用对数函数，我们转换或反转了使用梯度上升来最大化动作值的需求。相反，我们可以使用梯度下降和
    `backprop` 在我们的策略网络上。
- en: Run the code as you normally do and observe the results. This algorithm will
    generally train quickly.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照你通常的方式运行代码并观察结果。这个算法通常训练得很快。
- en: The elegance of this algorithm especially in PyTorch obfuscates the complex
    mathematics here beautifully. Unfortunately, that may not be a good thing unless
    you understand the intuition. In the next section, we explore the advantage function
    we mentioned earlier in the last exercise and look at how this relates to actor-critic
    methods.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的优雅性，尤其是在 PyTorch 中，将这里的复杂数学美妙地掩盖了。不幸的是，除非你理解了直觉，否则这可能不是一件好事。在下一节中，我们将探讨上一节练习中提到的优势函数，并看看这与演员-评论家方法有何关联。
- en: Using advantage actor-critic
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用优势演员-评论家
- en: We have already discussed the concept of advantage a few times throughout a
    few previous chapters including the last exercise. Advantage is often thought
    of as understanding the difference between applying different agents/policies
    to the same problem. The algorithm learns the advantage and, in turn, the benefits
    it provides to enhancing reward. This is a bit abstract so let's see how this
    applies to one of our previous algorithms like DDQN. With DDQN, advantage was
    defined by understanding how to narrow the gap in moving to a known target or
    goal. Refer back to [Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going
    Deeper with DDQN*, if you need a refresher.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在几个前几章中讨论了几次优势的概念，包括最后一个练习。优势通常被认为是在理解将不同的代理/策略应用于相同问题之间的差异。该算法学习优势，从而提供增强奖励的好处。这有点抽象，让我们看看这如何应用于我们之前的一个算法，比如
    DDQN。在 DDQN 中，优势是通过理解如何缩小移动到已知目标或目标的差距来定义的。如果你需要复习，请参考[第 7 章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，《DDQN
    深入研究》。
- en: 'The concept of advantage can be extended to what we refer to as actor-critic
    methods. With actor-critic, we define advantage by training two networks, one
    as an actor; that is, it makes decisions on the policy, and another network critiques
    those decisions based on expected returns. The goal now will not only be to optimize
    the actor and critic but to do so in a manner to reduce the number of surprises. You
    can think of a surprise as a time when the agent may expect some reward but instead
    doesn''t or possibly receives more reward. With AC methods, the goal is to minimize
    surprises and it does this by using a value-based approach (DQN) as the critic
    and a PG (REINFORCE) method as the actor. See the following diagram to see how
    this comes together:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 优势的概念可以扩展到我们所说的演员-评论家方法。在演员-评论家方法中，我们通过训练两个网络来定义优势，一个作为演员；也就是说，它对策略做出决策，另一个网络根据预期回报对这些决策进行评论。现在的目标不仅是要优化演员和评论家，而且要以减少意外情况的方式去做。你可以把意外想象成代理可能期望获得一些奖励，但结果却没有或可能获得了更多奖励的时刻。在AC方法中，目标是最小化意外，它通过使用基于价值的批评方法（DQN）作为评论家和PG（REINFORCE）方法作为演员来实现。参见以下图表，了解这是如何结合在一起的：
- en: '![](img/5796624f-66f6-4219-b37d-a50ad7a3ea96.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5796624f-66f6-4219-b37d-a50ad7a3ea96.png)'
- en: Explaining actor-critic methods
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 解释演员评论家方法
- en: In the next section, we jump in and see how AC can be applied to our previous
    PG example.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨如何将AC应用于我们之前的PG示例。
- en: Actor-critic
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员评论家
- en: 'AC methods use a combination of networks to predict the output of the value
    and policy functions, where our value function network resembles DQN and our policy
    function is defined using a PG method such as REINFORCE. Now, for the most part,
    this is as simple as it sounds; however, there are several details in the way
    in which we code these implementations that require some attention. We will, therefore,
    cover the details of this implementation as we review the code. Open `Chapter_8_ActorCritic.py`
    and follow the next exercise:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AC方法使用网络组合来预测价值和策略函数的输出，其中我们的价值函数网络类似于DQN，我们的策略函数使用PG方法（如REINFORCE）定义。现在，这基本上就像听起来那么简单；然而，我们在编码这些实现的方式中有几个细节需要注意。因此，在审查代码时，我们将详细介绍这个实现的细节。打开`Chapter_8_ActorCritic.py`并跟随下一个练习：
- en: 'As this code follows the same pattern as previous examples, we will only need
    to cover a few sections in detail. The first section of importance is the new
    `ActorCritic` class at the top of the file and shown here:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于此代码遵循与之前示例相同的模式，我们只需要详细说明几个部分。最重要的部分是文件顶部的`ActorCritic`类，如下所示：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Starting at the `init` function, we can see that we construct three `Linear`
    network layers: `fc1` and `fc_pi` for `policy` and `fc_v` for `value`. Then, right
    after `init`, we see the `pi` and `v` functions. These functions do the forward
    pass for each network (`pi` and `v`). Notice how both networks share `fc1` as
    an input layer. That means that the first layer in our network will be used to
    encode network state in a form both the actor and critic networks will share. Sharing
    layers like this is common in more advanced network configurations.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`init`函数开始，我们可以看到我们构建了三个`Linear`网络层：`fc1`和`fc_pi`用于`policy`，`fc_v`用于`value`。然后，在`init`之后，我们看到`pi`和`v`函数。这些函数对每个网络（`pi`和`v`）进行前向传递。注意这两个网络都共享`fc1`作为输入层。这意味着我们网络的第一层将用于以演员和评论家网络共享的形式编码网络状态。在更高级的网络配置中，共享这样的层是常见的。
- en: Next, we see the `put_data` function, which just puts memories into a replay
    or experience buffer.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们看到`put_data`函数，它只是将记忆放入重放或经验缓冲区。
- en: After that, we have an imposing function called `make_batch`, which just builds
    the batches of data we use in experience replay.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们有一个名为`make_batch`的强大函数，它只是构建我们在经验重放中使用的批量数据。
- en: 'We will skip over the `ActorCritic` training function, `train_net`, and jump
    down to the iteration training code shown here:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将跳过`ActorCritic`训练函数`train_net`，并跳到下面的迭代训练代码，如下所示：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You may not have noticed, but our last exercise used episodic training or what
    we refer to as Monte Carlo or off-policy training. This time, our training takes
    place on-policy, and that means our agent acts as soon as it receives new updates.
    Otherwise, the code is quite similar to many other examples we have and will run.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能没有注意到，但我们的最后一个练习使用了基于事件的训练或我们所说的蒙特卡洛或离线策略训练。这次，我们的训练是在策略下进行的，这意味着我们的代理在接收到新的更新后立即采取行动。否则，代码与其他许多示例非常相似，并且可以运行。
- en: Run the example as you normally would. Training may take a while, so start it
    running and jump back to this book.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行示例。训练可能需要一段时间，所以先启动它，然后回到这本书。
- en: Now that we understand the basic layout of the example code, it is time to get
    into the details of training in the next section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了示例代码的基本布局，是时候进入下一节训练的细节了。
- en: Training advantage AC
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练优势 AC
- en: 'Using advantage and training multiple networks to work together as you may
    imagine is not trivial. Therefore, we want to focus a whole exercise on understanding
    how training works in AC. Open up `Chapter_8_ActorCritic.py` again and follow
    the exercise:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优势并训练多个网络协同工作，正如您所想象的那样，并不简单。因此，我们想要专注于整个练习来理解 AC 中的训练工作。再次打开 `Chapter_8_ActorCritic.py`
    并跟随练习：
- en: 'Our main focus will be the `train_net` function in the `ActorCritic` class
    we saw before. Starting with the first two lines, we can see this is where the
    training batch is first made and we calculate `td_target`. Recall we covered the
    form of TD error calculation check when we implemented DDQN:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的主要关注点将是之前看到的 `ActorCritic` 类中的 `train_net` 函数。从前两行开始，我们可以看到这是训练批次首先被创建的地方，我们计算
    `td_target`。回想一下，当我们实现 DDQN 时，我们覆盖了 TD 错误计算的形式检查：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we calculate the change or delta between our target and the value function.
    Again, this was covered in DDQN and the code to do this is shown here:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算目标函数和值函数之间的变化或增量。同样，这在 DDQN 中已经覆盖了，执行此操作的代码如下：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, we do a forward pass over the π network with `self.pi` and then
    gather the results. The gather function essentially aligns or gathers data. Interested
    readers should consult the PyTorch site for further documentation on `gather`.
    The code for this step is here:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们使用 `self.pi` 在 π 网络上执行前向传递，然后收集结果。收集函数本质上是对数据进行对齐或收集。感兴趣的读者应查阅 PyTorch
    网站以获取有关 `gather` 的进一步文档。此步骤的代码如下：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we calculate the loss with the following code:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码计算损失：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Loss is calculated using the updated policy method where we use log to inverse
    optimize for our actions. Recall, in our previous discussion, the introduction
    of the ![](img/aafe4f10-5c43-4378-8061-8031f956c590.png)function. This function
    denotes the advantage function where we take the negative log of the policy and
    add it to the output of the L1 squared errors from the value function, `v`, and
    `td_target`. The `detach` function on the tensors just allows for the network
    to not update those values when training.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失是通过更新策略方法计算的，其中我们使用对数来对动作进行逆优化。回想一下，在我们之前的讨论中，介绍了 ![](img/aafe4f10-5c43-4378-8061-8031f956c590.png)
    函数。此函数表示优势函数，其中我们取策略的负对数并将其添加到值函数 `v` 和 `td_target` 的 L1 平方误差输出中。张量上的 `detach`
    函数仅允许网络在训练时不对这些值进行更新。
- en: 'Finally, we push the loss back through the network with the following code:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用以下代码将损失反向传递到网络中：
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There's nothing new here. The code first zeroes out the gradients, then calculates
    the mean loss of the batch and pushes that backward with a call to `backward`,
    finishing with stepping the optimizer using `step`.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里没有什么新的内容。代码首先将梯度置零，然后计算批次的平均损失，并通过调用 `backward` 将其反向传递，最后使用 `step` 步进优化器完成。
- en: You will need to tune the hyperparameters in this example to train an agent
    to complete the environment. Of course, you are more than up to the challenge
    by now. In the next section, we will move up and look at another class of PG methods.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您将需要调整此示例中的超参数来训练一个能够完成环境的智能体。当然，您现在完全能够接受这个挑战。在下一节中，我们将向上移动并查看另一类 PG 方法。
- en: Building a deep deterministic policy gradient
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度确定性策略梯度
- en: One of the problems we face with PG methods is that of variability or too much
    randomness. Of course, we might expect that from sampling from a stochastic or
    random policy. The **Deep Deterministic Policy Gradient** (**DDPG**) method was
    introduced in a paper titled *Continuous control with deep reinforcement learning*,
    in 2015 by Tim Lillicrap. It was meant to address the problem of controlling actions
    through continuous action spaces, something we have avoided until now. Remember
    that a continuous action space differs from a discrete space in that the actions
    may indicate a direction but also an amount or value that expresses the effort
    in that direction whereas, with discrete actions, any action choice is assumed
    to always be at 100% effort.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在PG方法中面临的一个问题是可变性或过多的随机性。当然，我们可能期望从随机策略中采样时会出现这种情况。**深度确定性策略梯度**（**DDPG**）方法是在2015年由Tim
    Lillicrap发表的一篇题为《使用深度强化学习进行连续控制》的论文中提出的。它的目的是解决通过连续动作空间控制动作的问题，这是我们之前一直避免的问题。记住，连续动作空间与离散空间的不同之处在于，动作可以指示一个方向，也可以指示一个量或值，这表达了在该方向上的努力程度，而离散动作中，任何动作选择都被假定为始终是100%的努力。
- en: 'So, why does this matter? Well, in our previous chapter exercises, we explored
    PG methods over discrete action spaces. By using these methods in discrete spaces,
    we essentially buffered or masked the problem of variability by converting action
    probabilities into discrete values. However, in environments with continuous control
    or continuous action spaces, this does not work so well. Enter DDPG. The answer
    is in the name: deep deterministic policy gradient, which, in essence, means we
    are introducing determinism back into a PG method to rectify the problem of variability.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这有什么关系呢？好吧，在我们上一章的练习中，我们探索了离散动作空间上的PG方法。通过在离散空间中使用这些方法，我们本质上通过将动作概率转换为离散值来缓冲或掩盖了可变性的问题。然而，在具有连续控制或连续动作空间的环境中，这并不奏效。于是出现了DDPG。答案就在其名称中：深度确定性策略梯度，本质上意味着我们正在将确定性重新引入PG方法，以纠正可变性的问题。
- en: The last two PG methods we will cover in this chapter, DDPG and TRPO, are generally
    considered need-specific and in some cases too complex to implement effectively. Therefore,
    in the past few years, these methods have not seen much use in more state-of-the-art
    development. The code for these methods has been provided for completeness but
    the explanations may be rushed.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍的最后两种PG方法，即DDPG和TRPO，通常被认为是特定需求的，并且在某些情况下过于复杂，难以有效实现。因此，在过去的几年里，这些方法在更先进的发展中并没有得到太多应用。这些方法的代码已经提供以供完整性，但解释可能有些仓促。
- en: 'Let''s see how this looks in code by opening `Chapter_8_DDPG.py` and following
    the next exercise:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过打开`Chapter_8_DDPG.py`并跟随下一个练习来看看这代码是如何实现的：
- en: 'The full source code for this example is too large to list in full. Instead,
    we will go through the relevant sections in the exercise, starting with the hyperparameters,
    here:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例的完整源代码太大，无法全部列出。相反，我们将通过练习中的相关部分进行讲解，从超参数开始，如下所示：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It looks like we have introduced a few new hyperparameters but we really only
    introduce one new one called `tau`. The other variables, `lr_mu` and `lr_q`, are
    learning rates for two different networks.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看起来我们介绍了一些新的超参数，但实际上我们只介绍了一个新的参数，称为`tau`。其他变量`lr_mu`和`lr_q`是两个不同网络的 学习率。
- en: 'Next, we jump past the `ReplayBuffer` class, which is something we have seen
    before for storing experiences, then past the other code until we come to the
    environment setup and more variable definitions, as shown here:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们跳过了`ReplayBuffer`类，这个类我们之前已经见过，用于存储经验，然后继续跳过其他代码，直到我们到达环境设置和更多变量定义的部分，如下所示：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, we see the setup of a new environment, `Pendulum`. Now, `Pendulum` is
    a continuous control environment that requires learning continuous space actions.
    After that, `memory` and `ReplayBuffer` are created, followed by the creation
    of a couple of classes called `QNet` and `MuNet`. Next, more control/monitoring
    parameters are initialized. Just before the last line, we see the creation of
    two optimizers, `mu_optimizer` and `q_optimizer`, for the `MuNet` and `QNet` networks
    respectively. Finally, on the last line, we see the creation of a new tensor called
    `ou_noise`. There is a lot of new stuff going on here but we will see how this
    all comes together shortly:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们看到新环境的设置，`Pendulum`。现在，`Pendulum` 是一个连续控制环境，需要学习连续空间动作。之后，创建了 `memory`
    和 `ReplayBuffer`，接着创建了两个名为 `QNet` 和 `MuNet` 的类。接下来，初始化了更多的控制/监控参数。在最后一行之前，我们看到创建了两个优化器，`mu_optimizer`
    和 `q_optimizer`，分别用于 `MuNet` 和 `QNet` 网络。最后，在最后一行，我们看到创建了一个新的张量 `ou_noise`。这里有很多新的事情在进行，但我们很快就会看到这一切是如何结合在一起的：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, move down to the top of the train loop shown in the preceding lines. We
    make sure that the algorithm can loop entirely through an episode. Hence, we set
    the range in the inner loop to a value higher than the iterations the agent is
    given in the environment:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，向下移动到前面行中显示的训练循环的顶部。我们确保算法可以完全循环通过一个场景。因此，我们将内循环的范围设置为高于智能体在环境中获得的迭代次数的值：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next comes the trial and error training code. Notice that the `a` action is
    taken from the network called `mu`. Then, in the next line, we add the `ou_noise`
    value to it. After that, we let the agent take a step, put the results in memory,
    and update the score and state. The noise value we use here is based on the Ornstein-Uhlenbeck
    process and is generated by the class of the same name. This process generates
    a moving stochastic value that tends to converge to the value ![](img/29efbecd-091a-4a5a-823e-d5b784d76923.png) or
    `mu`.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是试错训练代码。请注意，`a` 动作是从名为 `mu` 的网络中提取的。然后，在下一行，我们将 `ou_noise` 值添加到其中。之后，我们让智能体迈出一步，将结果存入记忆中，并更新分数和状态。我们这里使用的噪声值基于
    Ornstein-Uhlenbeck 过程，并由同名类生成。这个过程生成一个移动的随机值，倾向于收敛到值 ![](img/29efbecd-091a-4a5a-823e-d5b784d76923.png)
    或 `mu`。
- en: Recall that we initialized this value to zeroes in the earlier instantiation
    of the `OrnsteinUhlenbeckNoise` class. The intuition here is that we want the
    noise to converge to 0 over the experiment. This has the effect of controlling
    the amount of exploration the agent performs. More noise yields more uncertainty
    in the actions it selects and hence the agent selects more randomly. You can think
    of noise in an action now as the amount of uncertainty an agent has in that action
    and how much more it needs to explore to reduce that uncertainty.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们在 `OrnsteinUhlenbeckNoise` 类的早期实例化中将此值初始化为零。这里的直觉是我们希望噪声在实验过程中收敛到 0。这具有控制智能体执行探索量的效果。更多的噪声会导致它选择动作的不确定性增加，因此智能体会更随机地选择。你现在可以将动作中的噪声视为智能体在该动作中具有的不确定性以及它需要探索多少以减少这种不确定性。
- en: The Ornstein-Uhlenbeck process is used for noise here because it converges in
    a random but predictable manner, in that it always converges. You could, of course,
    use any value you like here for the noise, even something more deterministic.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用 Ornstein-Uhlenbeck 过程来生成噪声，因为它以随机但可预测的方式收敛，即它总是收敛。当然，你可以在这里使用任何你喜欢的噪声值，甚至更确定性的东西。
- en: 'Staying inside the training loop, we jump to the section that performs the
    actual training using the following code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练循环内部，我们跳转到执行实际训练的代码部分：
- en: '[PRE15]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can see here that once the memory `ReplayBuffer` is above `2000`, the agent
    begins training in loops of 10. First, we see a call to the `train` function with
    the various networks/models constructed, `mu`, `mu_target`, `q`, and `q_target`; `memory`;
    and the `q_optimizer` and `mu_optimizer` optimizers. Then, there are two calls
    to the `soft_update` function with the various models. `soft_update`, shown here,
    just converges the input model to the target in an iterative fashion using `tau`
    to scale the amount of change per iteration:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到，一旦记忆 `ReplayBuffer` 超过 `2000`，智能体就开始以 10 个循环进行训练。首先，我们看到对 `train` 函数的调用，其中包含构建的各种网络/模型
    `mu`、`mu_target`、`q` 和 `q_target`；`memory`；以及 `q_optimizer` 和 `mu_optimizer` 优化器。然后，有两个对
    `soft_update` 函数的调用，使用各种模型。这里显示的 `soft_update` 只是通过使用 `tau` 缩放每次迭代的改变量，以迭代方式将输入模型收敛到目标：
- en: '[PRE16]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This convergence from some acting model to a target is not new but, with the
    introduction of AC, it does complicate things. Before we get to that though, let''s
    run the sample and see it in action. Run the code as you normally would and wait:
    this one can take a while. If your agent reaches a high enough score, you will
    be rewarded with the following:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种从某些演员模型到目标的收敛并不新鲜，但是随着AC的引入，它确实使事情复杂化了。不过，在我们到达那里之前，让我们运行这个示例并看看它是如何运作的。像平常一样运行代码并等待：这个可能需要一段时间。如果你的智能体达到足够高的分数，你将获得以下奖励：
- en: '![](img/cf2ad031-a0b2-411c-94fc-929479f40949.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf2ad031-a0b2-411c-94fc-929479f40949.png)'
- en: An example of the Pendulum environment
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是摆锤环境的示例
- en: As the sample runs pay particular attention to how the score is being updated
    on the screen. Try and get a sense of how this may look graphically. In the next
    section, we will explore the finer details of this last example.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行示例时，特别关注屏幕上分数的更新。尽量感受一下这可能会在图形上看起来如何。在下一节中，我们将探讨这个最后示例的更详细细节。
- en: Training DDPG
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练DDPG
- en: 'Now, as you may have noticed in the last example, `Chapter_8_DDPG.py` is using
    four networks/models to train, using two networks as actors and two as critics,
    but also using two networks as targets and two as current. This gives us the following
    diagram:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如你可能在上一个示例中注意到的，`Chapter_8_DDPG.py`正在使用四个网络/模型进行训练，使用两个网络作为演员，两个作为评论者，但也使用两个网络作为目标，两个作为当前。这给我们以下图示：
- en: '![](img/a09bd1f4-973d-4c20-b4a6-d11ea0d3237e.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a09bd1f4-973d-4c20-b4a6-d11ea0d3237e.png)'
- en: Diagram of actor-critic target-current networks
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 演员评论者目标-当前网络图
- en: Each oval in the preceding diagram represents a complete deep learning network.
    Notice how the critic, the value or Q network implementation, is taking both environment
    outputs reward and state. The critic then pushes a value back to the actor or
    policy target network.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个图中的每个椭圆形代表一个完整的深度学习网络。注意评论者，即价值或Q网络实现，正在接受环境输出的奖励和状态。然后评论者将一个值推回给演员或策略目标网络。
- en: 'Open example `Chapter_8_DDPG.py` back up and follow the next exercise to see
    how this comes together in code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 打开示例`Chapter_8_DDPG.py`，然后按照下一个练习来查看代码是如何组合在一起的：
- en: 'We will first look at our definition of the critic or the `QNet` network class
    shown here:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先查看这里显示的评论者或`QNet`网络类的定义：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The construction of this network is also a little different and what happens
    here is the `fc_s` layer encodes the state, then `fc_a` encodes the actions. These
    two layers are joined in the forward pass to create a single Q layer, `fc_q`,
    which is then output through the last layer, `fc_3`.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个网络的构建也略有不同，这里发生的事情是`fc_s`层编码状态，然后`fc_a`编码动作。这两个层在正向传递中连接，创建一个单一的Q层，`fc_q`，然后通过最后一层，`fc_3`输出。
- en: If you need help picturing these types of networks, it can be and is often helpful
    to draw them out. The key here is to look at the code in the train function, which
    describes how the layers are joined.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要帮助想象这些类型的网络，绘制它们通常很有帮助。这里的关键是查看训练函数中的代码，它描述了层是如何连接的。
- en: 'Moving from the critic, we move to the actor network as defined by the `MuNet`
    class and shown here:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从评论者网络转移到由`MuNet`类定义的演员网络，如下所示：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`MuNet` is a fairly simple implementation of a network that encodes the state
    from 3 values to 128 input neurons, `fc1`, followed by 64 hidden layer neurons, `fc2`,
    and then finally output to a single value on the output layer, `fc_mu`. The only
    note of interest is the way we translate `fc_mu`, the output layer in the `forward`
    function, into the `mu` output value. This is done to account for the control
    range in the `Pendulum` environment, which takes action values in the range -2
    to 2\. If you convert this sample into another environment, be sure to account
    for any change in action space values.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MuNet`是一个将状态从3个值编码到128个输入神经元`fc1`的简单网络实现，然后是64个隐藏层神经元`fc2`，最后输出到输出层上的单个值`fc_mu`。唯一值得注意的注释是我们如何将`fc_mu`，`forward`函数中的输出层，转换为`mu`输出值。这是为了考虑到`Pendulum`环境中的控制范围，该环境接受-2到2的动作值。如果你将这个示例转换到另一个环境，请确保考虑到动作空间值的变化。'
- en: 'Next, we will move down to the start of the `train` function, as shown here:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将向下移动到`train`函数的开始，如下所示：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `train` function takes all of the networks, memory, and optimizer as inputs.
    In the first line, it extracts the `s` state, `a` action, `r` reward, `s_prime`
    next state, and `done_mask` from the `replayBuffer` memory:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`train`函数接受所有网络、记忆和优化器作为输入。在第一行，它从`replayBuffer`记忆中提取`s`状态、`a`动作、`r`奖励、`s_prime`下一个状态和`done_mask`：'
- en: '[PRE20]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The first block of code inside the function calculates the target value based
    on the output of the `q_target` network, which takes as input the last state, `s_prime`,
    and `mu_target` output from the last state. Then, we calculate the `q_loss` loss based
    on the output of the `q` network with an input state and action using the `target`
    values as the target. This somewhat abstract conversion is to convert the value
    from stochastic into deterministic. On the last three lines, we see the typical
    optimizer code for zeroing the gradient and doing a backprop pass:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数内部的第一块代码根据`q_target`网络的输出计算目标值，该网络以最后一个状态`s_prime`和从最后一个状态输出的`mu_target`作为输入。然后，我们根据使用`target`值作为目标的输入状态和动作来计算`q_loss`损失。这种有些抽象的转换是为了将值从随机转换为确定性。在最后三行中，我们看到典型的优化器代码用于归零梯度并进行反向传播：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Calculating the `mu_loss`policy loss is much simpler and all we do is take the
    output of the `q` network using the state and output action from the `mu` network.
    A couple of things to note is that we make the loss negative and take the mean
    or average. Then, we finish the function with a typical optimizer backprop on `mu_loss`.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`mu_loss`策略损失要简单得多，我们只需使用`mu`网络的状态和输出动作来获取`q`网络的输出。需要注意的是，我们将损失设置为负值并取平均值。然后，我们使用典型的优化器反向传播来完成`mu_loss`函数。
- en: If the agent is still running from the previous exercise, examine the results
    with this newfound knowledge. Consider how or what hyperparameters you could tune
    to improve the results of this example.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果智能体仍在运行之前的练习，请用这种新获得的知识检查结果。考虑如何或可以调整哪些超参数来改善这个示例的结果。
- en: For some, the pure code explanation of DDPG may be a bit abstract but hopefully
    not. Hopefully, by this point, you can read the code and assume the mathematics
    or intuition you need to understand the concepts. In the next section, we will
    look to what is considered one of the more complicated methods in DRL, the **Trust
    Region Policy Optimization** (**TRPO**) method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些人来说，DDPG的纯代码解释可能有点抽象，但希望不是。希望到这一点，你可以阅读代码并假设你需要理解概念所需的数学或直觉。在下一节中，我们将探讨被认为是DRL中更复杂的方法之一，即**信任区域策略优化**（**TRPO**）方法。
- en: Exploring trust region policy optimization
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索信任区域策略优化
- en: 'PG methods suffer from several technical issues, some of which you may have
    already noticed. These issues manifest themselves in training and you may have
    already observed this in lack of training convergence or wobble. This is caused
    by several factors we can summarize here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: PG方法存在几个技术问题，其中一些你可能已经注意到了。这些问题在训练中表现出来，你可能已经观察到缺乏训练收敛或波动。这是由几个我们可以总结的因素造成的：
- en: '**Gradient ascent versus gradient descent**: In PG, we use gradient ascent
    to assume the maximum action value is at the top of a hill. However, our chosen
    optimization methods (SGD or ADAM) are tuned for gradient descent or looking for
    values at the bottom of hills or flat areas, meaning they work well finding the
    bottom of a trough but do poorly finding the top of a ridge, especially if the
    ridge or hill is steep. A comparison of this is shown here:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度上升与梯度下降**：在PG中，我们使用梯度上升假设最大动作值位于山顶。然而，我们选择的优化方法（SGD或ADAM）是针对梯度下降或寻找山谷或平坦区域的值进行调优的，这意味着它们在寻找山谷底部时表现良好，但在寻找脊顶时表现不佳，尤其是如果脊或山很陡。这里展示了这种比较：'
- en: '![](img/b74693d2-6ad3-45b6-b363-9016b555cd1d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b74693d2-6ad3-45b6-b363-9016b555cd1d.png)'
- en: A comparison of gradient descent and ascent
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降与上升的比较
- en: Finding the peak, therefore, becomes the problem, especially in environments
    that require fine control or narrow discrete actions. This often appears as training
    wobble, where the agent keeps increasing in score but then falls back several
    steps every so often.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，找到峰值成为问题，尤其是在需要精细控制或窄离散动作的环境中。这通常表现为训练波动，其中智能体不断增加分数，但每隔一段时间就会后退几步。
- en: '**Policy versus parameter space mapping**: By its very nature, we will often
    be required to map a policy to some known action space either through continuous
    or discretization transformation. This step, unsurprisingly, is not without issue.
    Discretizing action space can be especially problematic and further compounds
    the hill-climbing problem from earlier.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略与参数空间映射**：根据其本质，我们通常需要将策略映射到某个已知动作空间，无论是通过连续变换还是离散化变换。这一步，不出所料，并非没有问题。离散化动作空间可能特别有问题，并进一步加剧了之前提到的爬山问题。'
- en: '**Static versus dynamic learning rate**: In part due to the optimizer problem
    we mentioned earlier, we also tend to find that using a static learning rate is
    problematic. That is, we often find that we need to decrease the learning rate
    as the agent continues to find the peak of those maximum action hills.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**静态与动态学习率**：部分原因是前面提到的优化器问题，我们也倾向于发现使用静态学习率是有问题的。也就是说，我们经常发现，随着智能体继续找到那些最大动作山峰的顶峰，我们需要降低学习率。'
- en: '**Policy sampling efficiency**: PG methods restrict us from updating the policy
    more than once per trajectory. If we try to update more frequently, after *x*
    number of steps, for instance, we see training diverge. Therefore, we are restricted
    to one update per trajectory or episode. This can provide for very poor sample
    efficiency especially in training environments with multiple steps.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略采样效率**：PG方法限制我们在每个轨迹中只能更新策略一次。如果我们尝试更频繁地更新，例如在*x*步之后，我们会看到训练发散。因此，我们被限制在每个轨迹或回合中只更新一次。这可能在具有多个步骤的训练环境中提供非常低的样本效率。'
- en: TRPO and another PG method called **proximal policy optimization**, which we
    will look at in [Chapter 9](2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml), *Asynchronous
    Action and the Policy*, attempt to resolve all of the previous issues using several
    common strategies.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO和另一种名为**近端策略优化（proximal policy optimization）**的PG方法，我们将在第9章（2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml）中探讨，*异步动作和政策*，试图使用几种常见策略解决所有这些问题。
- en: The code for this TRPO implementation was sourced directly from [https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo)
    and, at the time of writing, the code was only modified slightly to allow for
    easier running. This is a great example and worthy of further exploration and
    enhancements.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这个TRPO实现的代码直接来源于[https://github.com/ikostrikov/pytorch-trpo](https://github.com/ikostrikov/pytorch-trpo)，在写作时，代码仅稍作修改以允许更容易地运行。这是一个很好的例子，值得进一步探索和改进。
- en: 'Before we get to reviewing each of these strategies, let''s open the code for
    and follow the next exercise:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始审查这些策略之前，让我们打开代码并跟随下一个练习：
- en: TRPO is a hefty algorithm not easily run in a single file. We will start by
    looking at the code structure by opening the TRPO folder in the source folder
    for this chapter. This example covers code in several files and we will only review
    small snippets here. It is recommended you quickly review the source fully before
    continuing.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TRPO是一个庞大的算法，不容易在一个文件中运行。我们将首先通过在源文件夹中打开TRPO文件夹来查看代码结构。这个例子涵盖了多个文件中的代码，我们在这里只会审查一小部分。建议你在继续之前快速全面地审查源代码。
- en: Refer to the `main.py`file; this is the startup file. `main` takes several parameters
    defined within this file as inputs when running.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参考一下`main.py`文件；这是一个启动文件。`main`在运行时接受在这个文件中定义的几个参数作为输入。
- en: 'Scroll to about the middle and you will see where the environment is constructed
    on the main policy and value networks, as shown here:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到大约中间位置，你将看到环境是如何在主要策略和价值网络上构建的，如图所示：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, scroll down further until you come to that familiar training loop. For
    the most part, this should look similar to other examples, except for the introduction
    of another `while` loop, shown here:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，继续向下滚动，直到你到达那个熟悉的训练循环。大部分应该看起来与其他例子相似，除了引入了另一个`while`循环，如下所示：
- en: '[PRE23]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code assures that one agent episode consists of a given number of steps
    determined by `batch_size`. However, we still don't break the inner training loop
    until the environment says the episode is done. However, now, an episode or trajectory
    update is not done until the provided `batch_size` is reached. This attempts to
    solve the PG method sampling problem we talked about earlier.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码确保一个智能体（agent）的回合（episode）由`batch_size`决定的给定步数组成。然而，我们仍然不会在环境表示回合完成之前打破内部训练循环。但是，现在，只有在达到提供的`batch_size`之后，才会完成一个回合或轨迹更新。这试图解决我们之前讨论过的PG方法采样问题。
- en: 'Do a quick review of each of the source files; the list here summarizes the
    purpose of each:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 快速审查每个源文件；以下列表总结了每个文件的目的：
- en: '`main.py`: This is the startup source file and main point of agent training.'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.py`：这是启动源文件，也是智能体训练的主要点。'
- en: '`conjugate_gradients.py`: This is a helper method to conjugate or join gradients.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conjugate_gradients.py`：这是一个辅助方法，用于共轭或连接梯度。'
- en: '`models.py`: This file defines the network class Policy (actor) and value (the
    critic). The construction of these networks is a little unique so be sure to see
    how.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`models.py`: 此文件定义了网络类 Policy（演员）和 value（评论家）。这些网络的构建有点独特，所以请确保查看。'
- en: '`replay_memory.py`: This is a helper class to contain replay memory.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`replay_memory.py`: 这是一个辅助类，用于包含重放内存。'
- en: '`running_state.py`: This is a helper class to compute the running variance
    of the state, essentially, running estimates of mean and standard deviation. This
    can be beneficial for an arbitrary sampling of a normal distribution.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`running_state.py`: 这是一个辅助类，用于计算状态的运行方差，本质上，是均值和标准差的运行估计。这对于对正态分布进行任意采样是有益的。'
- en: '`trpo.py`: This is the code specific to TRPO and is meant to address those
    PG problems we mentioned earlier.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trpo.py`: 这是针对 TRPO 的特定代码，旨在解决我们之前提到的 PG 问题。'
- en: '`utils.py`: This provides some helper methods.'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`utils.py`: 这提供了一些辅助方法。'
- en: 'Use the default start parameters and run `main.py` as you normally would a
    Python file and watch the output, as shown here:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认的起始参数，像通常运行 Python 文件一样运行 `main.py`，并观察输出，如下所示：
- en: '![](img/fdc9d012-3339-4c9d-ac24-976eb98af9f9.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fdc9d012-3339-4c9d-ac24-976eb98af9f9.png)'
- en: The output of the TRPO sample
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO 样本的输出
- en: The output from this particular implementation is more verbose and displays
    factors that monitor performance over the issues we mentioned PG methods suffer
    from. In the next few sections, we will walk through exercises that show how TRPO
    tries to address these problems.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 此特定实现的输出更为详细，并显示了监控我们提到的问题（PG 方法遭受的问题）的性能因素。在接下来的几节中，我们将通过练习展示 TRPO 如何尝试解决这些问题。
- en: Jonathan Hui ([https://medium.com/@jonathan_hui](https://medium.com/@jonathan_hui))
    has several excellent posts on Medium.com that discuss various implementations
    of DRL algorithms. He does an especially good job of explaining the mathematics
    behind TRPO and other more complex methods.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Jonathan Hui ([https://medium.com/@jonathan_hui](https://medium.com/@jonathan_hui))
    在 Medium.com 上有几篇关于 DRL 算法各种实现的优秀文章。他在解释 TRPO 和其他更复杂方法背后的数学方面做得特别出色。
- en: Conjugate gradients
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共轭梯度
- en: 'The fundamental problem we need to address with policy methods is the conversion
    to a natural gradient form of gradient ascent. Previously, we handled conjugating
    this gradient by simply applying the log function. However, this does not yield
    a natural gradient. Natural gradients are not susceptible to model parameterization
    and provide an invariant method to compute stable gradients. Let''s look at how
    this is done in code by opening up our IDE to the TRPO example again and following
    the next exercise:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要用策略方法解决的基本问题是将梯度上升转换为自然梯度形式。以前，我们通过简单地应用对数函数来处理这个梯度的共轭。然而，这并不产生自然梯度。自然梯度不易受模型参数化影响，并提供了一种不变的方法来计算稳定的梯度。让我们通过再次打开我们的
    IDE 到 TRPO 示例并跟随下一个练习来看看这是如何实现的：
- en: 'Open the `trpo.py` file in the `TRPO` folder. The three functions in this file
    are meant to address the various problems we encounter with PG. The first problem
    we encounter is to reverse the gradient and the code to do that is shown here:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `TRPO` 文件夹中打开 `trpo.py` 文件。此文件中的三个函数旨在解决我们遇到的 PG 问题的各种问题。我们遇到的第一问题是反转梯度，执行此操作的代码如下所示：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `conjugate_gradients` function is used iteratively to produce a natural
    more stable gradient we can use for the ascent.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`conjugate_gradients` 函数被迭代使用，以产生一个更自然、更稳定的梯度，我们可以用它来进行上升。'
- en: 'Scroll down to the `trpo_step` function and you will see how this method is
    used as shown in the code here:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到 `trpo_step` 函数，你将看到这个方法如何在代码中展示使用：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This outputs a `stepdir` tensor denoting the gradient used to step the network.
    We can see by the input parameters the output conjugate gradient will be solved
    over 10 iterations using an approximation function, `Fvp`, and the inverse of
    the loss gradient, `loss_grad`. This is entangled with some other optimizations
    so we will pause here for now.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这输出一个 `stepdir` 张量，表示用于移动网络的梯度。我们可以通过输入参数看到，输出共轭梯度将通过一个近似函数 `Fvp` 和损失梯度的逆 `loss_grad`
    在 10 次迭代中求解。这与其他一些优化纠缠在一起，所以我们现在暂停。
- en: Conjugate gradients are one method we can use to better manage the gradient
    descent versus gradient ascent problem we encounter with PG methods. Next, we
    will look at further optimization again to address problems with gradient ascent.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度是我们可以使用的一种方法，以更好地管理 PG 方法中遇到的梯度下降与梯度上升问题。接下来，我们将再次探讨优化，以解决梯度上升的问题。
- en: Trust region methods
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任区域方法
- en: A further optimization we can apply to gradient ascent is using trust regions
    or controlled regions of updates. These methods are of course fundamental to TRPO
    given the name but the concept is further extended to other policy-based methods.
    In TRPO, we extend regions of trust over the approximation functions using the **Minorize-Maximization**
    or **MM** algorithm. The intuition of MM is that there is a lower bound function
    that we can always expect the returns/reward to be higher than. Hence, if we maximize
    this lower bound function, we also attain our best policy. Gradient descent by
    default is a line search algorithm but this again introduces the problem of overshooting.
    Instead, we can first approximate the step size and then establish a region of
    trust within that step. This trust region then becomes the space we optimize for.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对梯度上升进行进一步优化的方法是使用信任区域或更新的控制区域。这些方法当然是 TRPO 的基础，但这个概念被进一步扩展到其他基于策略的方法。在
    TRPO 中，我们使用 **最小化-最大化** 或 **MM** 算法在近似函数上扩展信任区域。MM 的直觉是存在一个下界函数，我们可以预期回报/奖励总是高于这个下界。因此，如果我们最大化这个下界函数，我们也会得到最佳策略。默认情况下，梯度下降是一个线搜索算法，但这又引入了超调的问题。相反，我们首先可以近似步长，然后在那个步长内建立一个信任区域。这个信任区域然后成为我们优化的空间。
- en: 'The analogy we often use to explain this involves asking you to think of yourself
    climbing a narrow ridge. You run the risk of falling off either side of the ridge
    so using normal gradient descent or line search becomes dangerous. Instead, you
    decide that to avoid falling you want to step on the center of the ridge or the
    center region you trust. The following screenshot taken from a blog post by Jonathan
    Hui shows this concept further:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常用来解释这个概念的类比是让你想象自己在爬一个狭窄的山脊。你面临从山脊两边掉下去的风险，所以使用正常的梯度下降或线搜索变得危险。相反，你决定为了避免掉下去，你想踩在山脊的中心或你信任的中心区域。以下是从
    Jonathan Hui 的博客文章中截取的屏幕截图，进一步展示了这个概念：
- en: '![](img/9dcd8c89-aaae-4fdd-b0a7-86f70eb29def.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9dcd8c89-aaae-4fdd-b0a7-86f70eb29def.png)'
- en: A comparison of line search versus Trust region
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 线搜索与信任区域的比较
- en: 'We can see how this looks in code by opening up the TRPO folder and following
    the next exercise:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过打开 TRPO 文件夹并跟随下一个练习来查看代码中的样子：
- en: 'Open up `trpo.py` again and scroll down to the following block of code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次打开 `trpo.py` 并向下滚动到以下代码块：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `linesearch` function is used to find how far up the ridge we want to locate
    the next region of trust. This function is used to indicate the distance to the
    next region of trust and is executed with the following code:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`linesearch` 函数用于确定我们想要在山脊上定位下一个信任区域的距离。这个函数用于指示到下一个信任区域的距离，并使用以下代码执行：'
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Notice the use of `neggdotstepdir`. This value is calculated from the step
    direction, `stepdir`, we calculated in the last exercise with the following code:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到 `neggdotstepdir` 的使用。这个值是从我们在上一个练习中计算的步长方向 `stepdir` 计算出来的，以下代码所示：
- en: '[PRE28]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we have a direction with `neggdotstepdir` and an amount with `linesearch`,
    we can determine the trust region with the following code:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了 `neggdotstepdir` 方向和 `linesearch` 数量，我们可以用以下代码确定信任区域：
- en: '[PRE29]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `set_flat_params_to` function is in `utils.py` and the code is shown here:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`set_flat_params_to` 函数位于 `utils.py` 文件中，代码如下所示：'
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This code essentially flattens the parameters into the trust region. This is
    the trust region we test to determine whether the next step is within, using the
    `linesearch` function.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码本质上是将参数平坦化到信任区域。这是我们用来测试下一步是否在其中的信任区域，使用 `linesearch` 函数。
- en: Now we understand the concept of trust regions and the need to properly control
    step size, direction, and amount when using PG methods. In the next section, we
    will look at the step itself.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了信任区域的概念以及在使用 PG 方法时正确控制步长、方向和数量的必要性。在下一节中，我们将查看步骤本身。
- en: The TRPO step
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO 步骤
- en: 'As you can see now, taking a step or update with TRPO is not trivial and things
    are still going to get more complicated. The step itself requires the agent to
    learn several factors from updating the policy and value function to also attain
    an advantage, also known as actor-critic. Understanding the actual details of
    the step function is beyond the scope of this book and you are again referred
    to those external references. However, it may be helpful to review what constitutes
    a step in TRPO and how this may compare complexity wise to other methods we look
    at in the future. Open up the sample TRPO folder again and follow the next exercise:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如你现在所看到的，使用 TRPO 进行步骤或更新并不简单，事情还会变得更加复杂。这个步骤本身要求智能体从更新策略和值函数中学习几个因素，以获得优势，也称为演员-评论家。理解步骤函数的实际细节超出了本书的范围，你再次被推荐参考外部参考资料。然而，回顾
    TRPO 中步骤的构成以及这与我们未来将要探讨的其他方法的复杂度比较可能是有帮助的。再次打开样本 TRPO 文件夹，并遵循下一个练习：
- en: 'Open up the `main.py` file and find the following line of code, around line
    130:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `main.py` 文件，找到大约在第 130 行的以下代码行：
- en: '[PRE31]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This last line of code is within the `update_params` function, which is where
    the bulk of the training takes place.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这最后一行代码位于 `update_params` 函数中，这是大部分训练发生的地方。
- en: 'You can further see at almost the bottom of the `main.py` file a call to the
    `update_params` function with `batch`, `batch` being a sample from `memory`, as
    shown in the following code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在 `main.py` 文件几乎最底部看到对 `update_params` 函数的调用，其中 `batch` 是从 `memory` 中抽取的样本，如下面的代码所示：
- en: '[PRE32]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Scroll back up to the `update_params` function and notice the first loop that
    builds `returns`, **`deltas`**, and `advantages` with the following code:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚回 `update_params` 函数，注意第一个循环使用以下代码构建 `returns`、**`deltas`** 和 `advantages`：
- en: '[PRE33]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Notice how we reverse the rewards and then loop through them to build our various
    lists `returns`, **`deltas`**, and `advantages`.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们是如何反转奖励，然后通过它们循环以构建我们的各种列表 `returns`、**`deltas`** 和 `advantages`。
- en: From there, we flatten the parameters and set the value network, the critic.
    Then, we calculate the advantages and actions mean and standard deviation. We
    do this as we are working with distributions and not deterministic values.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从那里，我们将参数展开并设置值网络，即评论家。然后，我们计算优势、动作均值和标准差。我们在处理分布而非确定性值时这样做。
- en: After that, we use the `trpo_step` function to take a training step or update
    in the policy.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们使用 `trpo_step` 函数进行一次训练步骤或策略更新。
- en: You may have noticed the use of `kl` in the source code. This stands for KL
    divergence and is something we will explore in a later chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了源代码中使用了 `kl`。这代表KL散度，我们将在后面的章节中探讨。
- en: Keep the example running for around 5,000 training iterations. This may take
    some time so be patient. It is worth running to completion, if not just once.
    The TRPO example in this section is meant to be experimented with and used with
    various control environments. In the next section, be sure to review the experiments
    you can play with to explore more about this method.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 保持示例运行大约 5,000 次训练迭代。这可能需要一些时间，所以请耐心等待。如果可能的话，完成整个运行过程是值得的。本节中的 TRPO 示例旨在进行实验和使用，以各种控制环境进行测试。在下一节中，请确保回顾你可以尝试的实验，以探索更多关于这种方法的信息。
- en: Exercises
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Use the exercises for your enjoyment and learning and to gain additional experience. Deep
    learning and deep reinforcement learning are very much areas where your knowledge
    will only improve by working with the examples. Don't expect to be a natural with
    training agents; it takes a lot of trial and error. Fortunately, the amount of
    experience we need is not as much as our poor agents require but still expect
    to put some time in.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些练习来享受学习，并获取额外的经验。深度学习和深度强化学习是非常需要通过实际操作示例来提高知识的领域。不要期望在训练智能体时能自然而然地成功；这需要大量的尝试和错误。幸运的是，我们需要的经验量并不像我们那些表现不佳的智能体那么大，但仍然需要投入一些时间。
- en: Open example `Chapter_8_REINFORCE.py` back up and alter the hyperparameters
    to see what effect this has on training.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例文件 `Chapter_8_REINFORCE.py`，将其备份并修改超参数，以查看这对训练有何影响。
- en: Open example `Chapter_8_ActorCritic.py` back up and alter the hyperparameters
    to see what effect this has on training.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例文件 `Chapter_8_ActorCritic.py`，将其备份并修改超参数，以查看这对训练有何影响。
- en: Open example `Chapter_8_DDPG.py` back up and alter the hyperparameters to see
    what effect this has on training.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例文件 `Chapter_8_DDPG.py`，将其备份并修改超参数，以查看这对训练有何影响。
- en: How can you convert the **REINFORCE** or `ActorCritic` examples to use continuous
    action spaces? Attempt to do this for new environments such as `LunarLanderContinous-v2`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何将 **REINFORCE** 或 `ActorCritic` 示例转换为使用连续动作空间？尝试为例如 `LunarLanderContinous-v2`
    的新环境执行此操作。
- en: Set up example `Chapter_8_DDPG.py` to use the `LunarLanderContinuous-v2` or
    another continuous environment. You will need to modify the action state from
    `3` to the environment you choose.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置示例 `Chapter_8_DDPG.py` 以使用 `LunarLanderContinuous-v2` 或其他连续环境。您需要将动作状态从 `3`
    修改为您选择的环境。
- en: Tune the hyperparameters for the `Chapter_8_DDPG.py` example. This will require
    you to learn and understand that new parameter, `tau`.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整 `Chapter_8_DDPG.py` 示例的超参数。这需要您学习和理解新的参数 `tau`。
- en: Tune the hyperparameters for the **TRPO** example. This will require you to
    learn how to set the hyperparameters from the command line and then tweak those
    parameters. You should not be modifying any code to complete this exercise.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整 **TRPO** 示例的超参数。这需要您学习如何从命令行设置超参数，然后调整这些参数。您不应该修改任何代码来完成此练习。
- en: Enable the MuJoCo environments and run one of these environments with the TRPO
    example.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用MuJoCo环境，并使用TRPO示例运行这些环境之一。
- en: Add plotting output to the various examples.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将绘图输出添加到各个示例中。
- en: Convert one of the single-file examples into use a main method that takes arguments
    and allows a user to train hyperparameters dynamically, instead of modifying source
    code.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单个文件示例中的一个转换为使用主方法，该方法接受参数并允许用户动态训练超参数，而不是修改源代码。
- en: Be sure to complete from 1-3 of the preceding exercises before moving on to
    the next section and end of this chapter.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节和本章结尾之前，请确保完成前面的1-3个练习。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced policy gradient methods, where we learned how
    to use a stochastic policy to drive our agent with the REINFORCE algorithm. After
    that, we learned that part of the problem of sampling from a stochastic policy
    is the randomness of sampling from a stochastic policy. We found that this could
    be corrected using dual agent networks, with one that represents the acting network
    and another as a critic. In this case, the actor is the policy network that refers
    back to the critic network, which uses a deterministic value function. Then, we
    saw how PG could be improved upon by seeing how DDPG works. Finally, we looked
    at what is considered one of the more complex methods in DRL, TRPO, and saw how
    it tries to manage the several shortcomings of PG methods.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了策略梯度方法，我们学习了如何使用随机策略通过REINFORCE算法驱动我们的智能体。之后，我们了解到从随机策略中采样的部分问题是随机采样的随机性。我们发现这可以通过双智能体网络来纠正，其中一个代表动作网络，另一个作为评论家。在这种情况下，动作者是引用评论家网络的策略网络，它使用确定性价值函数。然后，我们看到了如何通过观察DDPG的工作来改进PG。最后，我们探讨了被认为是DRL中更复杂方法之一的TRPO，并了解了它如何试图管理PG方法的几个缺点。
- en: Continuing with our look at PG methods, we will move on to explore next-generation
    methods such as PPO, AC2, AC2, and ACER in the next chapter.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续探讨PG方法的基础上，我们将在下一章探索下一代方法，如PPO、AC2、AC2 和 ACER。
