- en: '*Chapter 9*: Video Synthesis'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第九章*：视频合成'
- en: We have learned about and built many models for image generation, including
    state-of-the-art **StyleGAN** and **Self-Attention GAN** (**SAGAN**) models, in
    previous chapters. You have now learned about most if not all of the important
    techniques used to generate images, and we can now move on to video generation
    (synthesis). In essence, video is simply a series of still images. Therefore,
    the most basic video generation method is to generate images individually and
    put them together in a sequence to make a video. **Video synthesis** is a complex
    and broad topic in its own right, and we won't be able to cover everything in
    a single chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经学习并构建了许多图像生成模型，包括最先进的**StyleGAN**和**Self-Attention GAN**（**SAGAN**）模型。你现在已经了解了生成图像所需的大部分，甚至是全部重要技术，现在我们可以进入视频生成（合成）部分。实际上，视频只是静态图像的一系列。
    因此，最基本的视频生成方法是单独生成图像，并将它们按顺序组合成视频。**视频合成**本身是一个复杂而广泛的话题，我们不可能在一个章节中涵盖所有内容。
- en: In this chapter, we will get an overview of video synthesis. We will then implement
    what is probably the most well-known video generation technique, `deepfake` online
    and you'll be impressed by how real some of them seem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概述视频合成。然后，我们将实现可能是最著名的视频生成技术——`deepfake`，你会惊讶于其中一些看起来有多么逼真。
- en: 'We will cover the following in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Video synthesis overview
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频合成概述
- en: Implementing face image processing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现人脸图像处理
- en: Building a deepfake model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个deepfake模型
- en: Swapping faces
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 换脸
- en: Improving DeepFakes with GANs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GANs改进DeepFakes
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code for this chapter can be accessed here:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在这里访问：
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter09)'
- en: 'The notebook used in this chapter is this one:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的笔记本是这个：
- en: '`ch9_deepfake.ipynb`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ch9_deepfake.ipynb`'
- en: Video synthesis overview
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频合成概述
- en: Let's say your doorbell rings while you're watching a video, so you pause the
    video and go to answer the door. What would you see on your screen when you come
    back? A still picture where everything is frozen and not moving. If you press
    the play button and pause it again quickly, you will see another image that looks
    very similar to the previous one but with slight differences. Yes – when you play
    a series of images sequentially, you get a video.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在看视频时门铃响了，于是你暂停了视频去开门。当你回来时，屏幕上会看到什么？一个静止的画面，所有内容都被冻结且不动。如果你按下播放按钮并迅速再次暂停，你会看到另一幅与之前非常相似但略有不同的图像。没错——当你按顺序播放一系列图像时，你得到的是视频。
- en: We say that image data has three dimensions, or *(H, W, C)*; video data has
    four dimensions, *(T, H, W, C)*, where *T* is the temporal (time) dimension. It's
    also the case that video is just a big *batch* of images, except that we cannot
    shuffle the batch. There must be temporal consistency between the images; I'll
    explain this further.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说图像数据有三维，或者是*(H, W, C)*；视频数据有四维，*(T, H, W, C)*，其中*T*是时间（时序）维度。视频本质上只是大量的*批量*图像，只不过我们不能随意打乱这些图像的顺序。图像之间必须保持时间一致性；我会进一步解释这一点。
- en: Let's say we extract images from some video datasets and train an unconditional
    GAN to generate images from random noise input. As you can imagine, the images
    will look very different from each other. As a result, the video made from those
    images would be unwatchable. Like image generation, video generation can also
    be classified as unconditional or conditional.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们从某些视频数据集中提取图像，并训练一个无条件GAN来生成来自随机噪声输入的图像。正如你可以想象的那样，这些图像之间会有很大的不同。因此，使用这些图像制作的视频将是无法观看的。与图像生成一样，视频生成也可以分为无条件和条件生成。
- en: In unconditional video synthesis, not only does the model need to generate good-quality
    content but it must also keep the temporal content or movement in check. As a
    result, the output video is generally quite short for some simple video content.
    Unconditional video synthesis is still not quite mature enough yet for practical
    application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在无条件视频合成中，模型不仅需要生成高质量的内容，还必须控制时间内容或运动的一致性。因此，输出视频对于一些简单的视频内容来说通常会非常短。无条件视频合成仍然不够成熟，无法广泛应用于实际情况。
- en: On the other hand, conditional video synthesis conditions on input content and
    therefore generates better-quality results. As we learned in [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*, there is very little randomness in images generated
    by pix2pix. The lack of randomness may be a drawback in some applications, but
    the consistency in generated images is a plus in video synthesis. Thus, many video
    synthesis models are conditioned on images or videos. In particular, conditional
    face video synthesis has achieved great results and has had a real impact in commercial
    applications. We will now look at some of the most common forms of face video
    synthesis.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，条件视频合成依赖于输入内容，因此生成的结果质量更高。正如我们在[*第四章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*图像到图像的转换*中学到的那样，通过pix2pix生成的图像几乎没有随机性。缺乏随机性在某些应用中可能是一个缺点，但在视频合成中，生成图像的一致性是一个优点。因此，许多视频合成模型都以图像或视频为条件。特别是，条件人脸视频合成已经取得了显著的成果，并在商业应用中产生了真实的影响。接下来，我们将讨论一些最常见的人脸视频合成形式。
- en: Understanding face video synthesis
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解人脸视频合成
- en: 'The most common forms of face video synthesis are **face re-enactment** and
    **face swapping**. It is best to explain the difference between them by using
    pictures as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的人脸视频合成形式是**人脸重演**和**人脸交换**。最好通过下面的图片来解释它们之间的区别：
- en: '![Figure 9.1 – Face re-enactment and face swapping'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.1 – 人脸重演与人脸交换'
- en: '(Source: Y. Nirkin et al., 2019, “FSGAN: Subject Agnostic Face Swapping and
    Reenactment,” https://arxiv.org/pdf/1908.05932)](img/B14538_09_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '（来源：Y. Nirkin 等人，2019年，《FSGAN: 主体无关的人脸交换与重演》，[https://arxiv.org/pdf/1908.05932](https://arxiv.org/pdf/1908.05932)）](img/B14538_09_1.jpg)'
- en: 'Figure 9.1 – Face re-enactment and face swapping (Source: Y. Nirkin et al.,
    2019, “FSGAN: Subject Agnostic Face Swapping and Reenactment,” [https://arxiv.org/pdf/1908.05932](https://arxiv.org/pdf/1908.05932))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图9.1 – 人脸重演与人脸交换（来源：Y. Nirkin 等人，2019年，《FSGAN: 主体无关的人脸交换与重演》，[https://arxiv.org/pdf/1908.05932](https://arxiv.org/pdf/1908.05932)）'
- en: The top row shows how face re-enactment works. In face re-enactment, we want
    to transfer the expression of the face in the target video (right) to the face
    of the source image (left) to produce the image in the middle. Digital puppetry
    is already used in computer animation and movie production, where the facial expression
    of an actor is used to control a digital avatar. Face re-enactment using AI has
    the potential to make this happen more easily. The bottom row shows face swapping.
    This time, we want to keep the facial expression of the target video but use the
    face from the source image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 顶行展示了人脸重演的工作原理。在人脸重演中，我们希望将目标视频（右侧）中脸部的表情转移到源图像（左侧）中的脸部，从而生成中间的图像。数字化木偶技术已广泛应用于计算机动画和电影制作中，演员的面部表情用来控制数字化的虚拟人物。使用人工智能进行的人脸重演有潜力使这一过程变得更加简单。底行展示了人脸交换。这次，我们希望保持目标视频的面部表情，但使用源图像中的面部。
- en: Although technically different, face re-enactment and face swapping are similar.
    In terms of generated video, both could be used to create a *fake video*. As the
    name suggests, face swapping swaps just the face but not the head. Therefore,
    both the target and source faces should have a similar shape to increase the fidelity
    of the fake video. You can use this as a visual cue to differentiate between face
    swapping and face re-enactment videos. Face re-enactment is technically more challenging
    and it doesn't always require a driving video; it can use facial landmarks or
    sketches instead. We will introduce one such model in the next chapter. In the
    rest of this chapter, we will focus on implementing face swapping with the deepfake
    algorithm.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然技术上有所不同，人脸重演和人脸交换是相似的。在生成的视频方面，它们都可以用来制作*假视频*。顾名思义，人脸交换仅交换面部而不交换头部。因此，目标面部和源面部应该具有相似的形状，以提高假视频的保真度。你可以利用这一视觉线索来区分人脸交换视频和人脸重演视频。人脸重演在技术上更具挑战性，并且它并不总是需要驱动视频；它也可以使用面部特征点或草图。我们将在下一章介绍一个这样的模型。在本章的剩余部分，我们将重点讨论如何使用deepfake算法实现人脸交换。
- en: DeepFake overview
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepFake概述
- en: Many of you may have seen online videos where the face of an actor has been
    swapped with another celebrity's face. Quite often, that celebrity is the actor
    Nicolas Cage, and the resulting videos are quite hilarious to watch. This all
    started around the end of 2017, when an anonymous user named *deepfakes* posted
    the algorithm (which was later named after the username) on the social news website
    Reddit. This was quite unusual, considering that almost all of the breakthrough
    machine learning algorithms of the past decade had their origins in academia.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的许多人可能看过一些在线视频，其中一位演员的面部被另一个名人的面部替换。通常情况下，那位名人是演员尼古拉斯·凯奇，结果视频看起来相当搞笑。这一切始于2017年底，当时一位匿名用户名为*deepfakes*的用户在社交新闻网站Reddit上发布了这一算法（后来该算法也以这个用户名命名）。这非常不寻常，因为过去十年几乎所有突破性的机器学习算法都源自学术界。
- en: People have used the deepfake algorithm to create all sorts of videos, including
    some for TV advertisements and movies. However, as these fake videos can be very
    convincing, they have also raised some ethical issues. Researchers have demonstrated
    that it is possible to create fake videos to make the former US president Barack
    say things that he did not say. People have genuine reasons to be worried about
    deepfake and researchers have also been devising ways to detect these fake videos.
    You will want to understand how deepfake works, either to create funny videos
    or to combat fake news videos. So, let's crack on!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 人们已经使用deepfake算法制作了各种视频，包括一些电视广告和电影。然而，由于这些假视频可能非常逼真，它们也引发了一些伦理问题。研究人员已经证明，有可能制作假视频，让前美国总统奥巴马说出他从未说过的话。人们有充分的理由对deepfake感到担忧，研究人员也在设计检测这些假视频的方法。无论是为了制作搞笑视频，还是为了应对假新闻视频，你都应该了解deepfake的工作原理。那么，让我们开始吧！
- en: 'The deepfake algorithm can be roughly broken into two parts:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: deepfake算法大致可以分为两个部分：
- en: '**A deep learning model** to perform face image translation. We will first
    collect datasets for two people, say, **A** and **B**, and use an autoencoder
    to train them separately to learn their latent code, as shown in the following
    figure. There is a shared encoder, but we use separate decoders for different
    people. The top diagram in the figure shows the training architecture. The bottom
    diagram shows the face swap.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度学习模型**用于执行人脸图像转换。我们首先收集两个人的数据集，假设为**A**和**B**，并使用自编码器分别对其进行训练，以学习他们的潜在编码，如下图所示。这里有一个共享的编码器，但我们为不同的人使用单独的解码器。图中的上部分展示了训练架构，下部分展示了面部交换过程。'
- en: 'Firstly, **Face A** (the source) is encoded into a small latent face (the latent
    code). The latent code contains face representations such as the head pose (angle),
    facial expression, eyes open or shut, and more. We will then use decoder **B**
    to convert the latent code into **Face B**. The aim is to generate **Face B**
    using the pose and expression of **Face A**:'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，**A面部**（源面部）被编码成一个小的潜在面部（潜在编码）。该潜在编码包含诸如头部姿势（角度）、面部表情、眼睛睁开或闭合等面部特征。然后我们将使用解码器**B**将潜在编码转换为**B面部**。目标是使用**A面部**的姿势和表情生成**B面部**：
- en: '![Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder
    and two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen
    et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573)
    ](img/B14538_09_2.jpg)'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图9.2 – 使用自编码器的deepfake。(上) 使用一个编码器和两个解码器进行训练。(下) 从A重建B面部（改绘自：T.T. Nguyen等人，2019年，《深度学习在deepfakes创建和检测中的应用：综述》，https://arxiv.org/abs/1909.11573）](img/B14538_09_2.jpg)'
- en: 'Figure 9.2 – deepfake using autoencoders. (Top) Training with one encoder and
    two decoders. (Bottom) Reconstructing Face B from A (Redrawn from: T.T. Nguyen
    et al, 2019, “Deep Learning for deepfakes Creation and Detection: A Survey,” https://arxiv.org/abs/1909.11573)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图9.2 – 使用自编码器的deepfake。(上) 使用一个编码器和两个解码器进行训练。(下) 从A重建B面部（改绘自：T.T. Nguyen等人，2019年，《深度学习在deepfakes创建和检测中的应用：综述》，https://arxiv.org/abs/1909.11573）
- en: In a normal image generation setting, a model is basically what we need for
    production. All we need to do is to send an input image to the model to produce
    an output image. But the production pipeline for deepfake is more involved, as
    will be described later.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在普通的图像生成设置中，模型基本上是我们生产所需的内容。我们所需要做的就是将输入图像发送给模型，从而生成输出图像。但deepfake的生产流程更为复杂，稍后会做详细描述。
- en: 'We''ll need a collection of traditional computer vision techniques to perform
    pre- and post-processing, including these:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用一系列传统的计算机视觉技术来执行预处理和后处理，包括以下内容：
- en: a) Face detection
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 人脸检测
- en: b) Face landmark detection
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 人脸关键点检测
- en: c) Face alignment
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 人脸对齐
- en: d) Face warping
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 人脸扭曲
- en: e) Face mask detection
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 人脸遮罩检测
- en: 'The following figure shows the deepfake production pipeline:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下图展示了深度伪造的生产流程：
- en: '![Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing
    deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)](img/B14538_09_3.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – DeepFake 生产流程（来源：Y. Li, S. Lyu, 2019，《通过检测人脸扭曲伪影揭露深度伪造视频》，https://arxiv.org/abs/1811.00656)](img/B14538_09_3.jpg)'
- en: 'Figure 9.3 – DeepFake production pipeline (Source: Y. Li, S. Lyu, 2019, “Exposing
    deepfake Videos By Detecting Face Warping Artifacts,” https://arxiv.org/abs/1811.00656)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – DeepFake 生产流程（来源：Y. Li, S. Lyu, 2019，《通过检测人脸扭曲伪影揭露深度伪造视频》，https://arxiv.org/abs/1811.00656）
- en: 'The steps can be grouped into three stages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以分为三个阶段：
- en: Steps *(a)* to *(f)* are pre-processing steps to extract and align the source
    face from the image.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 *(a)* 到 *(f)* 是用于从图像中提取并对齐源人脸的预处理步骤。
- en: There is a face swap to produce target *face (g)*.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个人脸交换过程来生成目标 *人脸 (g)*。
- en: Steps *(h)* to *(j)* are post-processing steps to *paste* the target face into
    the image.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤 *(h)* 到 *(j)* 是后处理步骤，用于 *粘贴* 目标人脸到图像中。
- en: We learned about and built autoencoders in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder*, and therefore it is relatively easy for us to build
    one for deepfake. On the other hand, many of the aforementioned computer vision
    techniques have not been introduced before in this book. Therefore, in the next
    section, we will implement the face processing steps one by one. Then, we will
    implement the autoencoder and finally implement all of the techniques together
    to produce a deepfake video.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第二章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)中学习并构建了自编码器和变分自编码器，因此对于深度伪造来说，构建一个自编码器相对容易。另一方面，许多上述的计算机视觉技术在本书中之前并未介绍过。因此，在接下来的章节中，我们将逐步实现人脸处理步骤，然后实现自编码器，最后将所有技术结合起来制作深度伪造视频。
- en: Implementing face image processing
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现人脸图像处理
- en: We will use mainly two Python libraries – `dlib` was originally a C++ toolkit
    for machine learning, it also has a Python interface, and it is the go-to machine
    learning Python library for facial landmark detection. Most of the image processing
    code used in this chapter is adapted from [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要使用两个 Python 库 —— `dlib` 最初是一个用于机器学习的 C++ 工具包，它也有 Python 接口，是进行人脸关键点检测的首选机器学习
    Python 库。本章使用的大部分图像处理代码改编自 [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap)。
- en: Extracting image from video
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从视频中提取图像
- en: 'The first thing in the production pipeline is to extract images from video.
    A video is made up of a series of images separated by a fixed time interval. If
    you check a video file''s properties, you may find something that says `.mp4`
    video file into directory/images and name them using a number sequence – for example,
    `image_0001.png`, `image_0002.png`, and so on:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生产流程中的第一步是从视频中提取图像。视频由一系列按固定时间间隔分隔的图像组成。如果你检查一个视频文件的属性，可能会看到它被表示为 `.mp4` 视频文件，保存在目录/images中，并使用数字序列命名——例如，`image_0001.png`，`image_0002.png`，以此类推：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, we can also use OpenCV to read the video frame by frame and
    save the frames into individual image files as shown in the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用 OpenCV 按帧读取视频，并将每一帧保存为单独的图像文件，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will use the extracted images for all subsequent processing and not worry
    about the source video anymore.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用提取的图像进行所有后续处理，之后不再关心源视频。
- en: Detecting and localizing a face
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测和定位人脸
- en: Traditional computer vision techniques detect faces by using the **Histogram
    of Oriented Gradients** (**HOG**). The gradient of a pixel image can be calculated
    by taking the difference of the preceding and following pixels in the horizontal
    and vertical directions. The magnitude and direction of a gradient tells us about
    the lines and corners of a face. We can then use the HOG as a feature descriptor
    to detect the shape of a face. A modern approach is, of course, to use a CNN,
    which is more accurate but slower.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的计算机视觉技术通过使用**方向梯度直方图**（**HOG**）来检测人脸。可以通过计算水平和垂直方向上相邻像素的差异来求得像素图像的梯度。梯度的大小和方向告诉我们人脸的线条和角落。我们可以将
    HOG 作为特征描述符来检测人脸的形状。当然，现代的方法是使用卷积神经网络（CNN），它更准确但速度较慢。
- en: '`face_recognition` is a library built on top of `dlib`. By default, it uses
    the HOG of `dlib` as a face detector, but it also has the option to use a CNN.
    Using it is simple, as shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`face_recognition`是一个基于`dlib`构建的库。默认情况下，它使用`dlib`的HOG作为面部检测器，但它也有使用CNN的选项。使用它非常简单，如下所示：'
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will return a list of coordinates for each of the faces detected in the
    image. In our code, we assume that there is only one face in the image. The coordinates
    returned are in `css` format, (top, right, bottom, left), so we''ll need an additional
    step to convert them into `dlib.rectangle` objects for the `dlib` facial landmarks
    detector, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个包含每个面部在图像中检测到的坐标的列表。在我们的代码中，我们假设图像中只有一个面部。返回的坐标是`css`格式（上、右、下、左），因此我们需要额外的步骤将它们转换为`dlib.rectangle`对象，以便传递给`dlib`的面部标志点检测器，代码如下：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can read the bounding box coordinates from `dlib.rectangle` and crop the
    face from the image as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从`dlib.rectangle`中读取边界框坐标，并按如下方式从图像中裁剪面部：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If a face is detected in the image, we can then move on to the next step to
    detect facial landmarks.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在图像中检测到面部，我们可以继续进行下一步，检测面部标志点。
- en: Detecting facial landmarks
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测面部标志点
- en: '`dlib` model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`dlib`模型：'
- en: '![Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows,
    nose bridge, nose tip, eyes, and lips](img/B14538_09_4.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – dlib面部标志点的68个点，包括下巴、眉毛、鼻梁、鼻尖、眼睛和嘴唇](img/B14538_09_4.png)'
- en: Figure 9.4 – The 68 points the dlib facial landmarks, for the chin, eyebrows,
    nose bridge, nose tip, eyes, and lips
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – dlib面部标志点的68个点，包括下巴、眉毛、鼻梁、鼻尖、眼睛和嘴唇
- en: '`dlib` makes facial landmarks detection easy. We only need to download and
    load the model into `dlib` before using it as shown in the following code snippet:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`dlib`使得面部标志点检测变得容易。我们只需下载并加载模型到`dlib`中，像下面的代码片段一样使用它：'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will also pass the face coordinates into the predictor to tell it where the
    face is. This means, we don't need to crop out the face before calling the function.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将面部坐标传递给预测器，告诉它面部的位置。这意味着，我们在调用函数之前不需要裁剪面部。
- en: Facial landmarks are very useful features in machine learning problems. For
    example, if we want to know a person's facial expression, we could use the lips
    keypoints as input features to the machine learning algorithm to detect whether
    the mouth is open. This is more effective and efficient than looking at every
    single pixel in the image. We can also use facial landmarks to estimate the head
    pose.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 面部标志点在机器学习问题中非常有用。例如，如果我们想知道一个人的面部表情，可以使用嘴唇关键点作为输入特征，传递给机器学习算法来检测嘴巴是否张开。这比查看图像中的每个像素更有效和高效。我们还可以使用面部标志点来估计头部姿势。
- en: 'In DeepFake, we use facial landmarks to perform face alignment, which I will
    explain shortly. Before that, we will need to convert the landmarks from `lib`
    format into a NumPy array:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在DeepFake中，我们使用面部标志点进行面部对齐，稍后我会解释。 在此之前，我们需要将`lib`格式的标志点转换为NumPy数组：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now we have everything we need for face alignment.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了进行面部对齐所需的一切。
- en: Aligning a face
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对齐面部
- en: Naturally, faces in video appear in various poses, such as looking to the left
    or open-mouthed. In order to make it easier for the autoencoder to learn, we will
    align the faces to the center of the cropped image, looking straight at the camera.
    This is known as `dlib` landmarks except for the first 18 points of the chin.
    This is because people have vastly different chin shapes, which could skew the
    alignment results, so they are not used as a reference.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，视频中的面部会出现各种姿势，比如向左看或张嘴。为了使自编码器更容易学习，我们将面部对齐到裁剪图像的中心，正对着镜头。这就是`dlib`的面部标志点，除了下巴的前18个点。这是因为人们的下巴形状差异很大，这可能会影响对齐的结果，因此它们不作为参考。
- en: Mean face
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 平均面孔
- en: If you still remember, we looked at mean faces in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)*,
    Getting Started with Image Generation Using TensorFlow*. They were generated by
    directly sampling from the dataset, so not exactly the same way as used in `dlib`.
    Anyway, feel free to go to take a look if you have forgotten what mean faces look
    like.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们在[*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)《使用TensorFlow进行图像生成入门》中查看了平均面孔。它们是通过直接从数据集中采样生成的，因此与`dlib`中使用的方法并不完全相同。不管怎样，如果你忘记了平均面孔的样子，随时可以去看看。
- en: 'We will need to perform the following operations on the face to align it with
    the mean face''s position and angle:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对面部执行以下操作，将其与平均面孔的位置和角度对齐：
- en: Rotation
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转
- en: Scale
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放
- en: Translation (shift in location)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平移（位置变化）
- en: 'These operations can be represented using a 2×3 affine transformation matrix.
    The affine matrix *M* is composed of matrices *A* and *B* as shown in the following
    equation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些操作可以通过一个2×3的仿射变换矩阵来表示。仿射矩阵 *M* 由矩阵 *A* 和 *B* 组成，如下方的方程所示：
- en: '![](img/Formula_09_001.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.jpg)'
- en: 'Matrix *A* contains the parameters for linear transformation (scale and rotation),
    while matrix *B* is used for translation. deepfake uses an algorithm from S. Umeyama
    to estimate the parameters. The source code of the algorithm is contained in a
    single file that I have included in our GitHub repository. We call the function
    by passing the detected facial landmarks and the mean face landmarks as shown
    in the following code. As explained earlier, we omit the chin landmarks as they
    are not included in the mean face:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A* 包含线性变换（缩放和旋转）的参数，而矩阵 *B* 用于平移。深度伪造使用 S. Umeyama 的算法来估计这些参数。该算法的源代码包含在我已经上传到
    GitHub 仓库的一个文件中。我们通过传递检测到的面部特征点和均值面部特征点来调用该函数，如下方的代码所示。如前所述，我们省略了下巴的特征点，因为它们不包含在均值面中：
- en: '[PRE7]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now pass the affine matrix into `cv2.warpAffine()` to perform affine
    transformation, as shown in the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将仿射矩阵传递给 `cv2.warpAffine()` 来执行仿射变换，如下方的代码所示：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following figure shows the faces before and after alignment:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了对齐前后的面部：
- en: '![Figure 9.5 – (Left) Author’s face with facial landmarks and face detection
    bounding box. (Right) Aligned face](img/B14538_09_5.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – （左）作者的面部特征点和面部检测边框。（右）对齐后的面部](img/B14538_09_5.jpg)'
- en: Figure 9.5 – (Left) Author's face with facial landmarks and face detection bounding
    box. (Right) Aligned face
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – （左）作者的面部特征点和面部检测边框。（右）对齐后的面部
- en: 'The bounding boxes in the figure show the face detection at work. The picture
    on the left is also marked with facial landmarks. On the right is the face after
    alignment. We can see that the face has now been scaled larger to fit the mean
    face. In fact, the alignment output has the face more zoomed in, covering only
    the area between the eyebrows and the chin. I added padding to zoom out a little
    to include the bounding box in the final image. We can see from the bounding box
    that the face has been rotated so that it appears vertical. Next, we will learn
    about the last image pre-processing step: face warping.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的边框显示了面部检测的效果。左侧的图片也标记了面部特征点。右侧是对齐后的面部。我们可以看到，面部已经被放大以适应均值面部。实际上，对齐后的输出将面部放大，覆盖了眉毛和下巴之间的区域。我加入了一些填充，使图像稍微缩小一些，以便包含边框在最终图像中。从边框可以看到，面部已经旋转，使其看起来是垂直的。接下来，我们将学习最后一个图像预处理步骤：面部扭曲。
- en: Face warping
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面部扭曲
- en: We'll need two images to train an autoencoder, the input image and the target
    image. In deepfake, the target image is the aligned face, while the input image
    is a warped version of the aligned face. A face in the image does not change its
    shape after the affine transformation that we implemented in the preceding section,
    but warping, for example, twisting one side of the face, can change the shape
    of a face. deepfake warps faces to imitate the variety of face poses in real video
    as data augmentation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两张图像来训练自编码器，一张是输入图像，另一张是目标图像。在深度伪造中，目标图像是对齐后的面部，而输入图像是经过扭曲处理的对齐面部版本。图像中的面部在我们在前一节中实现的仿射变换之后并没有改变形状，但通过扭曲（例如，扭曲面部的一侧）可以改变面部的形状。深度伪造通过扭曲面部来模拟真实视频中面部姿态的多样性，作为数据增强。
- en: 'In image processing, transformation is the mapping of a pixel from one location
    in a source image to a different location in a target image. For example, translation
    and rotation is a one-to-one mapping that changes location and angle but retains
    size and shape. For warping, the mapping can be irregular, and the same point
    can be mapped to multiple points, which can give the effect of twisting and bending.
    The following diagram shows an example of mapping that warps an image from dimensions
    256×256 to 64×64:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像处理中，变换是将源图像中一个像素映射到目标图像中不同位置的过程。例如，平移和旋转是位置和角度变化的一对一映射，但大小和形状保持不变。对于扭曲，映射可以是不规则的，同一个点可能被映射到多个点，从而产生扭曲和弯曲的效果。下图显示了将图像从
    256×256 尺寸扭曲到 64×64 的映射示例：
- en: '![Figure 9.6 – Mapping to demonstrate warping'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.6 – 显示扭曲的映射'
- en: '](img/B14538_09_6.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_09_6.jpg)'
- en: Figure 9.6 – Mapping to demonstrate warping
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 显示扭曲的映射
- en: 'We will perform some random warping to twist a face slightly but not so much
    as to cause major distortion. The following code shows how to perform a face warp.
    You don''t have to understand every line of the code; it is sufficient to know
    that it uses the mapping as previously described to warp a face into a smaller
    dimension:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行一些随机扭曲，稍微扭曲面部图像，但不会造成严重的变形。以下代码展示了如何进行面部扭曲。你不必理解每一行代码，知道它使用之前描述的映射将面部扭曲为较小维度即可：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I guess most people think that deepfake is just a deep neural network but do
    not realize there are so many image processing steps involved. Luckily, OpenCV
    and `dlib` make things easy for us. Now, we can move on to build the whole deep
    neural network model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我猜大多数人认为深伪造只是一个深度神经网络，但并没有意识到其中涉及了许多图像处理步骤。幸运的是，OpenCV 和 `dlib` 让这一切变得简单。现在，我们可以继续构建整个深度神经网络模型。
- en: Building a DeepFake model
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 DeepFake 模型
- en: The deep learning model used in the original deepfake is an autoencoder-based
    one. There are a total of two autoencoders, one for each face domain. They share
    the same encoder, so there is a total of one encoder and two decoders in the model.
    The autoencoders expect an image size of 64×64 for both the input and the output.
    Now, let's build the encoder.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 原始深伪造使用的深度学习模型是基于自编码器的。总共有两个自编码器，每个面部领域一个。它们共享相同的编码器，因此模型中总共有一个编码器和两个解码器。自编码器期望输入和输出图像大小均为
    64×64。现在，让我们来构建编码器。
- en: Building the encoder
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建编码器
- en: 'As we learned in the previous chapter, the encoder is responsible for converting
    high-dimensional images into a low-dimensional representation. We''ll first write
    a function to encapsulate the convolutional layer; leaky ReLU activation is used
    for downsampling:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章所学，编码器负责将高维图像转换为低维表示。我们将首先编写一个函数来封装卷积层；在下采样过程中使用 Leaky ReLU 激活：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the usual autoencoder implementation, the output of the encoder is a 1D
    vector with a size of about 100 to 200, but deepfake uses larger dimensions of
    1,024\. In addition, it reshapes the 1D latent vector and upscales it back into
    3D activation. Therefore, the output of the encoder is not a 1D vector of size
    (1,024) but a tensor of size (8, 8, 512), as shown in the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规自编码器实现中，编码器的输出是一个大约大小为 100 到 200 的 1D 向量，但深伪造使用了更大的尺寸 1,024。此外，它将 1D 潜在向量重塑并放大回
    3D 激活。因此，编码器的输出不是一个大小为 (1,024) 的 1D 向量，而是一个大小为 (8, 8, 512) 的张量，如下代码所示：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can see that the encoder can be grouped into three stages:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，编码器可以分为三个阶段：
- en: There are convolutional layers, which downscale a `(64, 64, 3)` image all the
    way to `(4, 4, 1024)`.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型中包含卷积层，将一个 `(64, 64, 3)` 的图像逐步降维至 `(4, 4, 1024)`。
- en: There are two dense layers. The first one produces a latent vector of size `1024`,
    then the second one projects it to a higher dimension, which gets reshaped to
    `(4, 4, 1024)`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有两个全连接层。第一个生成一个大小为 `1024` 的潜在向量，第二个将其投影到更高维度，并将其重塑为 `(4, 4, 1024)`。
- en: The upsampling and convolution layers bring the output to size `(8, 8, 512)`.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上采样和卷积层将输出调整为大小 `(8, 8, 512)`。
- en: 'This can be understood better by looking at the following model summary:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看以下模型总结，可以更好地理解这一点：
- en: '![Figure 9.7 – Model summary'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 模型总结'
- en: '](img/B14538_09_7.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_09_7.jpg)'
- en: Figure 9.7 – Model summary
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 模型总结
- en: The next step is to construct the decoder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建解码器。
- en: Building the decoder
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建解码器
- en: 'The decoder''s input comes from the encoder''s output, so it expects a tensor
    of size `(8, 8, 512)`. We use several layers of upsampling to upscale the activations
    gradually to the target image dimension of `(64, 64, 3)`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输入来自编码器的输出，因此它期望一个大小为 `(8, 8, 512)` 的张量。我们使用多个上采样层逐步将激活值放大，最终达到目标图像尺寸 `(64,
    64, 3)`：
- en: 'Similar to before, we will first write a function for the upsampling block
    that contains an upsampling function, a convolutional layer, and leaky ReLU, as
    shown in the following code:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前类似，我们首先编写一个上采样模块的函数，其中包含上采样函数、卷积层和 Leaky ReLU，如下代码所示：
- en: '[PRE12]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we stack the upsampling blocks together. The final layer is a convolutional
    layer that brings the channel number to `3` to match the RGB color channels:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将上采样模块堆叠在一起。最后一层是一个卷积层，将通道数调整为 `3`，以匹配 RGB 颜色通道：
- en: '[PRE13]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The decoder model summary is as follows:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解码器模型总结如下：
- en: '![Figure 9.8 – Keras model summary of the decoder'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8 – 解码器的 Keras 模型总结'
- en: '](img/B14538_09_8.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14538_09_8.jpg)'
- en: Figure 9.8 – Keras model summary of the decoder
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 解码器的 Keras 模型总结
- en: Next, we will put the encoder and decoders together to construct the autoencoders.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把编码器和解码器组合在一起，构建自编码器。
- en: Training the autoencoders
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练自编码器
- en: 'As mentioned earlier, the DeepFake model consists of two autoencoders that
    share the same encoder. To construct the autoencoders, the first step is to instantiate
    the encoder and decoders:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DeepFake模型由两个共享相同编码器的自编码器组成。构建自编码器的第一步是实例化编码器和解码器：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we build two separate autoencoders by joining the encoder with the respective
    decoders as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将编码器与相应的解码器连接在一起来构建两个独立的自编码器，如下所示：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The next step is to prepare the training dataset. Although the autoencoder has
    an input image size of 64 × 64, the image preprocessing pipeline expects images
    of 256 × 256\. We will need about 300 images for each face domain. There is a
    link in the GitHub repository to where you can download some prepared images.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是准备训练数据集。尽管自编码器的输入图像大小为64×64，但图像预处理管道要求图像大小为256×256。每个面部领域大约需要300张图像。GitHub仓库中有一个链接，您可以通过它下载一些准备好的图像。
- en: Alternatively, you can also create datasets yourself by cropping the faces from
    collected images or video using the image processing techniques that we learned
    earlier. The faces in the dataset do not need to be aligned as the alignment will
    be performed in the image pre-processing pipeline. The image pre-processing generator
    will return two images – an aligned face and a warped version, both at a resolution
    of 64×64\.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，您还可以通过使用我们之前学习的图像处理技术，从收集的图像或视频中裁剪面部来自己创建数据集。数据集中的面部不需要对齐，因为对齐将在图像预处理管道中执行。图像预处理生成器将返回两张图像——一张对齐的面部图像和一张扭曲版本，分辨率都是64×64\。
- en: 'We can now pass the two generators into `train_step()` to train the autoencoder
    models as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这两个生成器传递给`train_step()`来训练自编码器模型，如下所示：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Writing and training the autoencoder is probably the easiest part of the deepfake
    pipeline. We don't need a lot of data; probably about 300 images per face domain
    is sufficient. Of course, more data should provide better results. As both the
    dataset and model aren't big, the training can happen relatively quickly even
    without the use of a GPU. Once we have a trained model, the final step is to perform
    the face swap.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 编写和训练自编码器可能是深度伪造管道中最简单的部分。我们不需要太多数据；每个面部领域大约300张图像就足够了。当然，更多的数据会带来更好的结果。由于数据集和模型都不大，因此即使不使用GPU，训练也可以相对快速地完成。一旦我们有了训练好的模型，最后一步就是进行面部交换。
- en: Swapping faces
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部交换
- en: 'Here comes the last step of the deepfake pipeline, but let''s first recap the
    pipeline. The deepfake production pipeline involves three main stages:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是深度伪造管道的最后一步，但我们先回顾一下整个管道。深度伪造生产管道包括三个主要阶段：
- en: Extract a face from an image using `dlib` and OpenCV.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`dlib`和OpenCV从图像中提取面部。
- en: Translate the face using the trained encoder and decoders.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的编码器和解码器进行面部转换。
- en: Swap the new face back into the original image.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新面部替换回原始图像中。
- en: 'The new face generated by the autoencoder is an aligned face of size 64×64,
    so we will need to warp it to the position, size, and angle of the face in the
    original image. We''ll use the affine matrix obtained from *step 1* in the face
    extraction stage. We''ll use `cv2.warpAffine` like before, but this time, the
    `cv2.WARP_INVERSE_MAP` flag is used to reverse the direction of image transformation
    as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器生成的新面部是大小为64×64的对齐面部，因此我们需要将其扭曲到原始图像中面部的位置、大小和角度。我们将使用在*步骤 1*中从面部提取阶段获得的仿射矩阵。我们将像之前一样使用`cv2.warpAffine`，但这次使用`cv2.WARP_INVERSE_MAP`标志来反转图像变换的方向，如下所示：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: However, directly pasting the new face onto the original image will create artifacts
    around the edges. This will be especially obvious if any part of the new face
    (which is a square 64×64) exceeds the original face boundary. To alleviate the
    artifacts, we will trim the new face with a face mask.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接将新面部粘贴到原始图像上会在边缘处产生伪影。如果新面部的任何部分（即64×64的方形面部）超出原始面部的边界，伪影会特别明显。为了减轻这些伪影，我们将使用面部遮罩来修剪新面部。
- en: 'The first mask we will create is one that contours around the facial landmarks
    in the original image. The following code will first find the contours of given
    facial landmarks and then fill inside of the contour with ones (1) and return
    it as a hull mask:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建的第一个遮罩是围绕原始图像中面部特征点轮廓的遮罩。以下代码首先会找到给定面部特征点的轮廓，然后用1填充轮廓内部，并将其作为外壳遮罩返回：
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As the **hull mask** is bigger than the new face square, we will need to trim
    the hull mask to fit the new square. To do this, we can create a rectangle mask
    from the new face and multiply it with the hull mask. The following diagram shows
    an example of the mask for the image:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 **外轮廓蒙版** 比新面孔的正方形大，我们需要将外轮廓蒙版修剪以适应新正方形。为此，我们可以从新面孔创建一个矩形蒙版，并将其与外轮廓蒙版相乘。以下图示为图像的蒙版示例：
- en: '![Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new
    face (c) Hull mask of original face (d) Combined mask](img/B14538_09_9.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – （从左到右）（a）原始图像（b）新面孔的矩形蒙版（c）原始面孔的外轮廓蒙版（d）合成蒙版](img/B14538_09_9.jpg)'
- en: Figure 9.9 – (Left to right) (a) Original image (b) Rectangular mask of new
    face (c) Hull mask of original face (d) Combined mask
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – （从左到右）（a）原始图像（b）新面孔的矩形蒙版（c）原始面孔的外轮廓蒙版（d）合成蒙版
- en: 'Then we use the mask to remove the face from the original image and fill it
    in with the new face using the following code:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用蒙版从原始图像中去除面孔，并使用以下代码将新面孔填充进去：
- en: '[PRE19]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The resulting face may still not look perfect. For instance, if the two faces
    have vastly different skin tone or shading, then we may need to use further and
    more sophisticated methods to iron out the artifacts.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 结果生成的面孔可能仍然不完美。例如，如果两张面孔的肤色或阴影差异很大，那么我们可能需要使用更进一步、更复杂的方法来修正伪影。
- en: 'This concludes the face swapping. We do this for each of the images extracted
    from a video, then we convert the images back into a video sequence. One way to
    do so is to use `ffmpeg` as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是换脸的全过程。我们对从视频中提取的每一帧图像进行换脸处理，然后将图像转换回视频序列。实现这一过程的一个方法是使用 `ffmpeg`，代码如下：
- en: '[PRE20]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The deepfake model and the computer vision techniques used in this chapter are
    fairly basic, as I wanted to make it easy to understand. Therefore, this code
    may not produce a realistic fake video. If you are keen on generating good fake
    videos, I would recommend you visit the [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap)
    GitHub repository, on which a big part of this chapter's code is based. Next,
    we will quickly look at how deepfake can be improved by using GANs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的 deepfake 模型和计算机视觉技术相对基础，因为我希望它们易于理解。因此，代码可能无法生成一个逼真的假视频。如果你希望生成优质的假视频，我建议你访问
    [https://github.com/deepfakes/faceswap](https://github.com/deepfakes/faceswap)
    GitHub 仓库，这也是本章大部分代码的来源。接下来，我们将快速了解如何通过使用 GAN 改进 deepfake。
- en: Improving DeepFakes with GANs
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 改进 DeepFakes
- en: 'The output image of deepfake''s autoencoders can be a little blurry, so how
    can we improve that? To recap, the deepfake algorithm can be broken into two main
    techniques – face image processing and face generation. The latter can be thought
    of as an image-to-image translation problem, and we learned a lot about that in
    [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*, Image-to-Image
    Translation*. Therefore, the natural thing to do would be to use a GAN to improve
    the quality. One helpful model is **faceswap-GAN**, and we will now go over a
    high-level overview of it. The autoencoder from the original deepfake is enhanced
    with residual blocks and self-attention blocks (see [*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation*) and used as a generator in faceswap-GAN.
    The discriminator architecture is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Deepfake 自动编码器的输出图像可能有点模糊，那么我们该如何改进呢？回顾一下，deepfake 算法可以分为两个主要技术——面孔图像处理和面孔生成。后者可以看作是一个图像到图像的翻译问题，我们在[*第
    4 章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*《图像到图像的翻译》*中学到了很多。因此，自然的做法是使用
    GAN 来提高图像质量。一个有用的模型是 **faceswap-GAN**，接下来我们将简要介绍它。原始 deepfake 中的自动编码器通过残差块和自注意力块（参见[*第
    8 章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*《图像生成中的自注意力》*）进行增强，并作为 faceswap-GAN
    的生成器。判别器的架构如下：
- en: '![Figure 9.10 - faceswap-GAN’s discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)](img/B14538_09_10.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10 - faceswap-GAN 的判别器架构（重绘自：https://github.com/shaoanlu/faceswap-GAN）](img/B14538_09_10.jpg)'
- en: 'Figure 9.10 - faceswap-GAN''s discriminator architecture (Redrawn from: https://github.com/shaoanlu/faceswap-GAN)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 - faceswap-GAN 的判别器架构（重绘自：https://github.com/shaoanlu/faceswap-GAN）
- en: We can learn a lot about the discriminator by looking at the preceding diagram
    alone. First, the input tensor has a channel dimension of `6`, which suggests
    it is a stack of two images – real and fake. Then there are two blocks of self-attention
    layers. The output has a shape of 8×8×1, so each of the output features looks
    at patches of the input image. In other words, the discriminator is PatchGAN with
    self-attention layers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅仅观察前面的图示，我们可以了解到很多关于判别器的信息。首先，输入张量的通道维度为`6`，这意味着它是由两张图像——真实图像和假图像——堆叠而成。接着，有两个自注意力层的模块。输出的形状是
    8×8×1，因此每个输出特征都会关注输入图像的某些小块。换句话说，判别器是带有自注意力层的 PatchGAN。
- en: 'The following diagram shows the encoder and decoder architecture:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了编码器和解码器的架构：
- en: '![Figure 9.11 - faceswap-GAN’s encoder and decoder architecture (Redrawn from:
    https://github.com/shaoanlu/faceswap-GAN)](img/B14538_09_11.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11 - faceswap-GAN 的编码器和解码器架构（重绘自：https://github.com/shaoanlu/faceswap-GAN）](img/B14538_09_11.jpg)'
- en: 'Figure 9.11 - faceswap-GAN''s encoder and decoder architecture (Redrawn from:
    https://github.com/shaoanlu/faceswap-GAN)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 - faceswap-GAN 的编码器和解码器架构（重绘自：https://github.com/shaoanlu/faceswap-GAN）
- en: There aren't a lot of changes to the encoder and decoder. Self-attention layers
    are added to both the encoder and decoder, and one residual block is added to
    the decoder.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器没有太大变化。自注意力层被添加到编码器和解码器中，并且解码器中添加了一个残差块。
- en: 'The losses used in training are these:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练中使用的损失函数如下：
- en: '**Least-squares** (**LS**) **loss** is the adversarial loss.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小二乘法**（**LS**）**损失**是对抗损失。'
- en: '**Perception loss** is the VGG features an L2 loss between the real and fake
    faces.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**感知损失**是基于 VGG 特征的 L2 损失，用于衡量真实面孔和假面孔之间的差异。'
- en: L1 reconstruction loss.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 重建损失。
- en: '**Edge loss** is the L2 loss of the gradients (in the *x* and *y* directions)
    around the eyes. This helps the model to generate realistic eyes.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘损失**是眼睛周围梯度的 L2 损失（在 *x* 和 *y* 方向上）。这有助于模型生成更逼真的眼睛。'
- en: One thing that I've been trying to achieve with this book is to instill you
    with the knowledge of most, if not all, of the fundamental building blocks of
    image generation. Once you know them, implementing a model is just like putting
    Lego bricks together. As we are already familiar with the losses (apart from edge
    loss), residual blocks, and self-attention blocks, I trust that you can now implement
    this model yourself, if you wish to. For interested readers, you can refer to
    the original implementation at [https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我一直试图实现的一个目标是：让你掌握大多数（如果不是全部）图像生成的基本构建块。一旦你掌握了它们，实现一个模型就像拼装乐高积木一样简单。由于我们已经熟悉了损失函数（除了边缘损失）、残差块和自注意力块，我相信你现在已经能够自己实现这个模型了，如果你愿意的话。对于感兴趣的读者，你可以参考[https://github.com/shaoanlu/faceswap-GAN](https://github.com/shaoanlu/faceswap-GAN)上的原始实现。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Congratulations! We have now finished all the coding in this book. We have learned
    how to use `dlib` to detect faces and facial landmarks and how to use OpenCV to
    warp and align a face. We also learned how to use warping and masking to do face
    swapping. As a matter of fact, we spent most of the chapter learning about face
    image processing and spent very little time on the deep learning side. We have
    implemented autoencoders by reusing and modifying the autoencoder code from the
    previous chapter.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们现在已经完成了本书中的所有编码工作。我们学会了如何使用`dlib`检测面部和面部特征点，如何使用 OpenCV 对面部进行变形和对齐。我们还学会了如何通过变形和遮罩来进行面部交换。事实上，我们大部分时间都在学习面部图像处理，几乎没有花时间深入学习深度学习部分。我们通过复用并修改上一章中的自动编码器代码来实现了自动编码器。
- en: Finally, we went over an example of improving deepfake by using GANs. faceswap-GAN
    improves deepfake by adding a residual block, a self-attention block, and a discriminator
    for adversarial training, all of which we have already learned about in previous
    chapters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讲解了一个通过使用 GANs 改进深伪技术的例子。faceswap-GAN 通过添加残差块、自注意力块和判别器来进行对抗训练，从而改进了深伪技术，这些内容我们在前面的章节中已经学习过了。
- en: In the next chapter, which is also the final chapter, we will review the techniques
    we have learned in this book and look at some of the pitfalls in training GANs
    for real-world applications. Then, we will go over a few important GAN architectures,
    looking at image inpainting and text-to-image synthesis. Finally, we will look
    at up-and-coming applications such as video retargeting and 3D-to-2D rendering.
    There won't be any coding in the next chapter, so you can sit back and relax.
    Cheers!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也就是最后一章，我们将回顾本书中学到的技巧，并讨论在实际应用中训练GAN时的一些陷阱。接着，我们将介绍几种重要的GAN架构，重点讲解图像修复和文本到图像的合成。最后，我们将探讨一些新兴应用，如视频重定向和3D到2D的渲染。下一章没有编码内容，所以你可以放松一下，尽情享受。敬请期待！
