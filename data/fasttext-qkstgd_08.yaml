- en: FastText in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的FastText
- en: The use of fastText is specifically to transform words and sentences into efficient
    vector representations. Although fastText is written in C++, there are community-written
    Python bindings to train and use the models. Along with that, Python is one of
    the most popular languages used for NLP, and hence there are many other popular
    libraries in Python that support fastText models and the training of fastText
    models. Gensim and Spacy are two popular libraries that make it easy to load these
    vectors, transform, lemmatize, and perform other NLP tasks efficiently. This chapter
    will focus on how to use fastText with Python and its popular libraries. This
    chapter will also focus on showing you some common tasks that the two libraries
    can do to work with fastText models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: fastText的使用专门是为了将单词和句子转换为高效的向量表示。尽管fastText是用C++编写的，但有社区编写的Python绑定可以训练和使用这些模型。同时，Python是自然语言处理（NLP）中最流行的语言之一，因此有许多其他流行的Python库支持fastText模型及其训练。Gensim和Spacy是两个受欢迎的库，它们使得加载这些向量、转换、词形还原以及高效地执行其他NLP任务变得简单。本章将重点介绍如何使用Python与fastText及其流行库配合使用，同时展示这两个库在处理fastText模型时可以完成的一些常见任务。
- en: 'The topics that are covered in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: FastText official bindings
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastText官方绑定
- en: PyBind
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyBind
- en: Preprocessed data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据
- en: Unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Supervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Gensim
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim
- en: Training a fastText model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练fastText模型
- en: Machine translation using Gensim
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Gensim进行机器翻译
- en: FastText official bindings
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FastText官方绑定
- en: The steps to install the official bindings for Python are covered in the first
    chapter. In this section, we will cover how to use the official fastText Python
    package to train, load, and use the models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Python官方绑定的步骤在第一章中已经介绍。本节将介绍如何使用官方的fastText Python包来训练、加载和使用模型。
- en: Using the Python fastText library, you will be able to implement all the necessary
    features that can be done using the command line. Lets take a look at the ways
    to implement unsupervised and supervised learning using Python fastText.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python fastText库，你将能够实现所有通过命令行可以完成的必要功能。让我们看看如何通过Python fastText实现无监督学习和监督学习。
- en: 'Note: In this chapter, we will be using Python3 and so the code examples will
    be in that. For users who are using Python2, please take a look at the *Appendix* for
    notes on the considerations that you need to bear in mind when using Python2.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，我们将使用Python3，因此代码示例将基于此。如果你使用的是Python2，请查阅*附录*，其中包含使用Python2时需要注意的事项。
- en: PyBind
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyBind
- en: Python bindings for fastText are made using the excellent PyBind library. PyBind
    is a lightweight library meant to expose C++ types in Python and vice versa, making
    it an excellent choice for creating the Python bindings for fastText. It supports
    almost all the popular C++ compilers such as Clang, GCC, Visual Studio, and so
    on. Also, the creators of PyBind claim that the binaries that are generated are
    smaller.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Python绑定是通过优秀的PyBind库创建的。PyBind是一个轻量级的库，旨在将C++类型暴露给Python，反之亦然，因此它是创建fastText的Python绑定的绝佳选择。它支持几乎所有流行的C++编译器，如Clang、GCC、Visual
    Studio等。此外，PyBind的创建者声称生成的二进制文件更小。
- en: The Python-fastText library uses the fastText C++ API.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Python-fastText库使用fastText的C++ API。
- en: Preprocessing data
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'Although the performance of fastText is quite good on raw text, it''s still advisable
    to preprocess the data before running the unsupervised algorithms or the classifier.
    Some points to be remembered are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管fastText在原始文本上的性能相当不错，但在运行无监督算法或分类器之前，建议对数据进行预处理。需要记住的一些要点如下：
- en: 'To train fastText, the encoding needs to be in UTF-8\. PyBind does an excellent
    job of converting almost all text to UTF-8 if it''s a string in Python3\. If you
    are using Python2, then there is an extra technical detail that you need to take
    care of: you have to encode all of the string that you are using in UTF-8.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练fastText，编码需要使用UTF-8编码。如果是Python3中的字符串，PyBind可以非常好地将几乎所有文本转换为UTF-8。如果你使用的是Python2，则需要注意一个额外的技术细节：你必须将使用的所有字符串都编码为UTF-8。
- en: Implementing some basic string processing and normalizing should make the model
    perform better.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一些基本的字符串处理和规范化操作将使模型表现得更好。
- en: 'The following is a simple function that can be used for normalizing your documents.
    This function is used in Python fastText notebooks:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简单的函数，可用于规范化你的文档。这个函数在Python fastText笔记本中使用：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you are using pandas to extract text from your dataset and clean it, you
    can also replace the missing text values in your dataset with an `_empty_` label:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 pandas 从数据集中提取文本并清理它，你还可以将数据集中缺失的文本值替换为 `_empty_` 标签：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Unsupervised learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The fastText command line implements two algorithms, `cbow` and `skip-gram`.
    Using the Python library, you should be able to train your models in both algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 命令行实现了两种算法：`cbow` 和 `skip-gram`。使用 Python 库，你应该能够在这两种算法中训练模型。
- en: Training in fastText
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 fastText 中进行训练
- en: Training in fastText is done using the `train_unsupervised` function. You can
    choose which algorithm to use from the `model` parameter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 fastText 中进行训练是通过 `train_unsupervised` 函数完成的。你可以从 `model` 参数中选择使用哪种算法。
- en: 'Then, you can train a `skipgram` model using the following Python code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下 Python 代码训练一个 `skipgram` 模型：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This is similar to the command line:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这与命令行类似：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, to train a `cbow` model you can use the following Python code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要训练一个 `cbow` 模型，你可以使用以下 Python 代码：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The equivalent statement on the command line is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 命令行中的等效语句是：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The difference between the Python code and the command line is that the command
    line will save the model in a file, while in the Python code, the model will be
    in memory, referenced by the variable. To save the model, you will need to pass
    explicit commands in your Python app, for example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Python 代码与命令行的区别在于，命令行会将模型保存在文件中，而在 Python 代码中，模型会存在内存中，通过变量引用。如果要保存模型，你需要在
    Python 应用中显式地传递命令，例如：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should be able to pass all the other training parameters as well. The parameters,
    as well as the default values, are listed here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够传入所有其他训练参数。这些参数以及默认值在这里列出：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: These parameters hold the same meaning that you have seen while exploring the
    command line.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的意义与在探索命令行时看到的相同。
- en: Evaluating the model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: The lack of labels in the case of unsupervised learning makes evaluation a bit
    problematic as there is nothing to meaningfully compare the results of the model
    with. In the case of word embeddings, we have the same problem, but since this
    is a somewhat narrow domain, we can make some subjective claims. The fastText
    command line gives us the options of nearest neighbors and finding word similarities,
    which we can replicate in the Python library as we will see later.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习的情况下，由于缺乏标签，评估变得有些棘手，因为没有可以用来有意义地比较模型结果的标准。在词嵌入的情况下，我们遇到了相同的问题，但由于这是一个相对狭窄的领域，我们可以做出一些主观的判断。fastText
    命令行提供了最近邻和查找词语相似度的选项，我们可以在 Python 库中复现这一过程，稍后会看到。
- en: Other techniques include using the syntactic and semantic performance of words
    based on the question—`words.txt` released by Google and the morphological similarity
    of rare words using the Stanford rare word database. Please keep in mind if you
    are creating word representations for a niche domain that these exact model evaluation
    techniques may not give good results, but the techniques should hold.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术包括使用基于问题的单词句法和语义表现——例如 Google 发布的 `words.txt`，以及使用斯坦福稀有词数据库来衡量稀有词的形态学相似性。如果你正在为某个特定领域创建词表示，请记住，这些特定的模型评估技术可能不会产生良好的结果，但这些技术应该是适用的。
- en: Word vectors
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词向量
- en: 'By default, the word vectors that are created are of 100 dimensions. They are
    saved in memory as NumPy arrays. So, you should be able to see the word vectors
    using the `get_word_vector` method:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，创建的词向量是 100 维的。它们以 NumPy 数组的形式保存在内存中。所以，你应该能够使用 `get_word_vector` 方法查看词向量：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Nearest neighbor queries
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻查询
- en: Generally k nearest neighbors are used to rate differentiate between models.
    The vector representation of a target word is taken, the neighbors of the vectors
    are found and then it is seen if the neighbors are closer to its meaning. Since
    fastText representations are meant to be distributional, this assumption should
    hold true.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，k 最近邻用于区分模型之间的差异。目标词的向量表示被取出，找到这些向量的邻居，然后查看邻居是否更接近它的含义。由于 fastText 的表示是分布式的，这一假设应该成立。
- en: 'The fastText command line gives us a tool to get the nearest neighbors easily,
    but there is no easy way to find them in Python. There is a `find_nearest_neighbor`
    function in `util`, but it takes vectors as input. Hence, we will need to write
    some code to create a function that takes in words and the target model, and gives
    back the nearest neighbors according to the model. You can take a look at `python
    fastText unsupervised learning.ipynb` for the code to get the nearest neighbors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 命令行提供了一个工具，可以轻松获取最近邻词，但在 Python 中没有简单的方法可以找到它们。`util`中有一个 `find_nearest_neighbor`
    函数，但它需要向量作为输入。因此，我们需要编写一些代码，创建一个函数，该函数接受单词和目标模型作为输入，并根据模型返回最近邻词。你可以查看 `python
    fastText unsupervised learning.ipynb` 文件中的代码来获取最近邻词：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output may be refined with some pre-normalization on the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果可以通过对数据进行一些预归一化处理来进行优化。
- en: Word similarity
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单词相似性
- en: There are various ways to find the similarity between words. In the case of
    fastText, one way of finding the similarity between words is to find the cosine
    distance between the words in the vector space. However, this method will probably
    not find the similarity between synonyms and antonyms, and other minute language
    constructs, but will solely give you a similarity score based on the context in
    which they are used. The words "water" and "cup" do not necessarily have anything
    that is similar between the two, but in context they are generally taken together
    and hence you may find the similarity score between them to be high.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以找到单词之间的相似性。以 fastText 为例，其中一种找到单词相似性的方法是计算单词在向量空间中的余弦距离。然而，这种方法可能无法找到同义词和反义词之间的相似性，也无法捕捉其他细微的语言构造，而仅仅是基于单词在上下文中的使用给出一个相似度分数。单词“water”和“cup”之间不一定有相似之处，但在特定上下文中，它们通常是一起出现的，因此你可能会发现它们之间的相似度分数较高。
- en: 'In the Python library, you can write a small function to get the cosine similarity:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 库中，你可以编写一个小函数来计算余弦相似度：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In essence, you find the word vectors of the two target words using the `get_word_vector` method, and
    then find the cosine between them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，你是通过 `get_word_vector` 方法获取两个目标单词的词向量，然后计算它们之间的余弦相似度。
- en: Model performance
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: 'You can find the performance of the model using the rare words dataset that
    was released by Stanford NLP. Using the `compute_similarity` function that is
    shared in the examples folder, we can change the function a little bit so that
    it works in a Python app. The implementation of the function can be seen in the
    unsupervised notebook. Download the rare words dataset, the link to which you
    will find in the references, unzip it, and then pass the text file as the first
    argument and the model as the second argument. You should be able to see how well
    your model has been able to evaluate the rare words:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用斯坦福 NLP 发布的稀有词数据集来评估模型的性能。通过 `examples` 文件夹中共享的 `compute_similarity` 函数，我们可以稍微修改该函数，使其能够在
    Python 应用中工作。函数的实现可以在无监督学习笔记本中看到。下载稀有词数据集，你可以在参考文献中找到下载链接，解压后，将文本文件作为第一个参数，模型作为第二个参数传入。你应该能够看到模型如何评估这些稀有单词：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Model visualization
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可视化
- en: Visualizing how the word vectors happen in space is an effective way to understand
    the distributional properties of the model. Since the dimensions of the vectors
    are quite high, you will need a good dimensionality reduction technique so that
    the vectors can be shown in a two-dimensional frame.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化词向量在空间中的分布是一种有效理解模型分布特性的方式。由于词向量的维度非常高，因此你需要一个好的降维技术，以便将向量显示在二维框架中。
- en: 'The t-SNE is popular technique for dimensionality reduction that is well suited
    for the visualization of high-dimensional datasets. The idea in this case is to
    keep similar words as close together as possible, while maximizing the distance
    between dissimilar words. The unsupervised notebook shows the code for the t-SNE
    model. In our case, we have taken some words and plotted them in the graph:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 是一种流行的降维技术，特别适用于高维数据集的可视化。其思路是尽可能地将相似的单词靠近，同时最大化不相似单词之间的距离。无监督学习笔记本中展示了
    t-SNE 模型的代码。在我们的例子中，我们选取了一些单词并将它们绘制在图中：
- en: '![](img/00083.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00083.jpeg)'
- en: Words plotted on a graph
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制在图中的单词
- en: As you can see, "water" and "cup" are together, as they are generally used in
    the same context. Another two vectors that are together are "drink" and "tea."
    Using t-SNE to understand your model will give you a good idea of how good your
    model is.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，“water”和“cup”是一起的，因为它们通常在相同的上下文中使用。另两个在一起的词向量是“drink”和“tea”。使用t-SNE来理解你的模型可以让你更好地了解模型的效果。
- en: Supervised learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Similar to unsupervised learning, the fastText library provides access to the
    internal API for running supervised learning as well. Hence, running the fastText
    supervised Python API will also create the same model, which can be trained using
    the command line app. The advantage is that you will be able to leverage all the
    Python data science tools available for building an NLP classifier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督学习类似，fastText库也提供了内部API来运行有监督学习。因此，运行fastText有监督的Python API也会创建相同的模型，可以使用命令行应用进行训练。其优势在于，你能够利用所有Python数据科学工具来构建NLP分类器。
- en: To show how to leverage the fastText classifier can be trained in Python, you
    can take a look at the `python fastText supervised learning.ipynb` notebook in
    the code. The dataset consists of reviews of fine foods from Amazon and can be
    downloaded from the Kaggle website, the links for which are given in the notebook.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何利用fastText分类器在Python中进行训练，你可以查看代码中的`python fastText supervised learning.ipynb`笔记本。数据集包含来自亚马逊的精美食品评论，并可以从Kaggle网站下载，笔记本中提供了下载链接。
- en: Data preprocessing and normalization
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理和归一化
- en: The data preprocessing and normalization steps are similar to what you have
    seen in the case of unsupervised learning. In this case though, the major difference
    is that you will need to prefix the label with the `__label__` prefix or a label
    prefix of your choice. Also, it has to be saved in the fastText file in a format
    that is similar to the fastText command line. Since this is a classifier, you
    will need to actually create two files, one for training and one for model validation.
    One of the popular ways to split a dataset into training and testing is using
    the scikit-learn `train_test_split` function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理和归一化步骤与无监督学习中的类似。不同之处在于，你需要在标签前添加`__label__`前缀，或选择你喜欢的标签前缀。同时，它必须以类似于fastText命令行的格式保存在fastText文件中。由于这是一个分类器，你需要实际创建两个文件，一个用于训练，另一个用于模型验证。将数据集分为训练集和测试集的常见方法是使用scikit-learn中的`train_test_split`函数。
- en: Training the model
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'To train the model, you will need to use the `train_supervised` method on the
    training file:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，你需要在训练文件上使用`train_supervised`方法：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is similar to running this on the command line:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这与命令行中的操作类似：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The hyperparameters are the same as what you pass in the case of supervised
    learning. The only difference with the unsupervised case is that the default loss
    function is `softmax` instead of `ns` and there is an additional `label` parameter:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数与有监督学习中的相同。与无监督学习的区别在于，默认的损失函数是`softmax`而不是`ns`，并且还有一个额外的`label`参数：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Similar to the case of unsupervised learning, the Python code will not save
    the model to a file but will save it to the variable that you defined, `su_model`
    in this case. This variable, `su_model`, is a Python NumPy matrix and hence we
    can manipulate it in standard ways.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于无监督学习的情况，Python代码不会将模型保存到文件中，而是将其保存到你定义的变量中，这里是`su_model`。这个变量`su_model`是一个Python
    NumPy矩阵，因此我们可以按照标准方式操作它。
- en: 'To save the model, you will need to invoke the `save_model` method:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存模型，你需要调用`save_model`方法：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Prediction
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测
- en: 'You can get the word vector of the word using the `get_word_vector` method,
    the sentence vector of a document using the `get_sentence_vector` method, and
    the predicted label of a model using the predict method:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`get_word_vector`方法获取单词的词向量，使用`get_sentence_vector`方法获取文档的句子向量，并使用`predict`方法获取模型的预测标签：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can also perform predict probabilities on your test document:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对测试文档进行预测概率计算：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is similar to this on the command line:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这与命令行中的操作类似：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Testing the model
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: Getting the precision and recall of your model is similar to what was seen using
    the command line. Similar to the command line, you will need to pass the test
    file and the number of labels that you need to find the precision of and recall
    against.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 获取模型的精度和召回率类似于命令行中看到的操作。与命令行类似，你需要传递测试文件和你需要计算精度和召回率的标签数量。
- en: 'In the following example, `test_file` contains the path to the test file, and
    the second argument is the number of labels:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，`test_file`包含测试文件的路径，第二个参数是标签的数量：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Confusion matrix
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: A confusion matrix is a nice way to visualize the performance of a supervised
    model, specifically a classifier. It also shines when understanding which classes
    are performing better in a multiclass classifier. When you are creating a confusion
    matrix, you are essentially describing the performance of the model classifier
    on a test set for which the ground truth is known. Since fastText supports a classifier,
    you can create a confusion matrix out of it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一种很好的方式来可视化监督模型的性能，特别是分类器。当你在创建混淆矩阵时，实际上是在描述模型分类器在测试集上的表现，而测试集的真实标签是已知的。混淆矩阵在多类分类器中尤其有效，它帮助你了解哪些类别表现更好。由于fastText支持分类器，你可以使用它来创建混淆矩阵。
- en: 'How to get the confusion matrix is shown in the supervised notebook. The `fasttext_confusion_matrix` function takes
    in a model variable, pandas test data, the label column name, and the text column
    name:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如何获取混淆矩阵，已在监督学习的笔记本中展示。`fasttext_confusion_matrix`函数接收一个模型变量、pandas测试数据、标签列名和文本列名：
- en: '![](img/00084.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00084.jpeg)'
- en: The predicted labels are shown against the true values.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的标签与真实值进行对比。
- en: Gensim
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gensim
- en: 'Gensim is a popular open source library for processing raw, unstructured human-generated
    text created by Radim Řehůřek. Some of the features that Gensim boasts are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim是一个流行的开源库，用于处理由Radim Řehůřek创建的原始、非结构化的人工生成文本。Gensim的一些特点包括：
- en: Memory independence is one of the core value propositions of Gensim, which is
    that it should be scalable and not hold all the document in the RAM. Hence, you
    will be able to train documents that are significantly larger than the memory
    of your machine.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存独立性是Gensim的核心价值主张之一，即它应该具有可扩展性，并且不需要将所有文档都保存在RAM中。因此，你将能够训练比你机器的内存大得多的文档。
- en: Gensim has efficient implementations of various popular vector space algorithms.
    There has been a recent implementation of fastText in gensim as well.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gensim对各种流行的向量空间算法有高效的实现。最近，fastText在Gensim中的实现也已经完成。
- en: There are IO/wrappers and converters around several popular data formats as
    well. Remember that fastText only supports UTF-8 formats and hence Gensim might
    be a good choice if you have data that is in different formats.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一些流行数据格式的IO/包装器和转换器。请记住，fastText仅支持UTF-8格式，因此，如果你有不同格式的数据，Gensim可能是一个不错的选择。
- en: Different algorithms for similarity queries. So, you are not stuck with the
    ones that are available in fastText.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的相似度查询算法。因此，你不必局限于fastText中提供的算法。
- en: 'There are two ways you can use fastText through Gensim: Using the Gensim''s
    native implementation of fastText and by using Gensim''s wrapper over fastText.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过Gensim使用fastText的两种方式：使用Gensim本地实现的fastText，或者通过Gensim对fastText的包装器。
- en: Now, let's take a look at how you can use Gensim to train a fastText model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何使用Gensim训练一个fastText模型。
- en: Training a fastText model
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练fastText模型
- en: For the example that is shown here, we will be using the Lee Corpus for training
    your model. To get the required data, I would recommend that you clone the Gensim
    repository from GitHub.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这里展示的示例，我们将使用Lee语料库来训练模型。为了获取所需的数据，我建议你从GitHub克隆Gensim的代码库。
- en: 'In the code examples shown here, we will be taking a look at Gensim fastText
    using the fake news dataset from Kaggle. First, download the data and clean the
    text:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示的代码示例中，我们将使用Kaggle的假新闻数据集来演示Gensim的fastText。首先，下载数据并清理文本：
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The first case we will take a look at is how to train the models using the
    fastText wrapper. To use the fastText wrapper, you will need to have fastText
    installed in your machine. You should have fastText installed if you have followed
    the instructions in [Chapter 1](part0021.html#K0RQ0-05950c18a75943d0a581d9ddc51f2755),
    *Introducing FastText*. This wrapper is deprecated though, and the recommendation
    is to use the Gensim implementation of fastText:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看一下如何使用fastText包装器来训练模型。要使用fastText包装器，你需要在机器上安装fastText。如果你按照[第1章](part0021.html#K0RQ0-05950c18a75943d0a581d9ddc51f2755)的说明，*介绍FastText*，你应该已经安装了fastText。然而，这个包装器已经被弃用，推荐使用Gensim的fastText实现：
- en: '[PRE22]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If you are interested in using the fastText implementation in Gensim, you will
    need to use the FastText class in `gensim.models`, which also, in addition to
    fastText, has word2vec and many other models that can be used:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣在 Gensim 中使用 fastText 实现，你需要使用 `gensim.models` 中的 FastText 类，它不仅包含 fastText
    还有 word2vec 和许多其他可以使用的模型：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Hyperparameters
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Gensim supports the same hyperparameters that are supported in the native implementation
    of fastText. You should be able to set most of the hyperparameters that are there
    in the Facebook fastText implementation. The defaults are also mostly there already.
    Some differences are listed here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 支持与 fastText 原生实现相同的超参数。你应该能够设置 Facebook fastText 实现中的大多数超参数。默认值大多数已经存在。以下列出了一些差异：
- en: '`sentences`: This can be a list of list of tokens. In general, a stream of
    tokens is recommended, such as `LineSentence` from the word2vec module as you
    have seen already. In the Facebook fastText library, this is given by the path
    to the file and is given by the `-input` parameter.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentences`：这可以是一个包含多个标记的列表。通常建议使用标记流，例如你已经看到过的 word2vec 模块中的 `LineSentence`。在
    Facebook 的 fastText 库中，这由文件路径表示，并通过 `-input` 参数提供。'
- en: '`max_vocab_size`: This is to limit the RAM size. In case there are more unique
    words, then, this will prune the less frequent ones. This needs to be decided
    based on the RAM that you have. For example, if you have 2 GB memory, then the
    value of `max_vocab_size` is used as a parameter for that 2 GB of memory. Also,
    if you have not set it manually, then there is no limit set.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_vocab_size`：用于限制内存大小。如果有更多独特的单词，这将修剪掉不太常见的单词。这个值需要根据你拥有的 RAM 来决定。例如，如果你的内存是
    2 GB，那么 `max_vocab_size` 的值就是为这 2 GB 内存设置的。如果你没有手动设置这个值，则没有设置限制。'
- en: '`cbow_mean`: There is a difference from the fastText command here. In the original
    implementation for cbow, the mean of the vectors is taken. But in this case, you
    have the option to use the sum by passing 0 and 1 in case you want to try out
    the mean.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cbow_mean`：与 fastText 命令有所不同。在原始实现中，对于 cbow，取向量的均值。但在这里，你可以选择通过传递 0 和 1 来使用向量的和，前提是你想尝试均值。'
- en: '`batch_words`: This is the target size of the batches that are passed. The
    default value is 10,000\. This is similar to`-lrUpdateRate` in the command line,
    as the number of batches determines when the weights will be updated.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_words`：这是传递的批次的目标大小。默认值为 10,000。这类似于命令行中的 `-lrUpdateRate`，批次数决定了权重何时更新。'
- en: '`callbacks`: A list of callback functions to be executed at specific stages
    of the training process.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`callbacks`：一个回调函数的列表，用于在训练过程中的特定阶段执行。'
- en: There are no parallels for the `-supervised` and `-labels` parameters, as Gensim
    focuses on unsupervised learning only.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-supervised` 和 `-labels` 参数没有对应的实现，因为 Gensim 只专注于无监督学习。'
- en: Model saving and loading
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型保存和加载
- en: 'Gensim provides the save and load methods for all models, and this is implemented
    in the case of fastText as well:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 提供了保存和加载方法来处理所有模型，fastText 的实现也遵循这一方式：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Loading a binary fastText model can also be achieved using the `load_fasttext_format`
    class method:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `load_fasttext_format` 类方法，也可以加载二进制 fastText 模型：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Word vectors
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词向量
- en: 'In Gensim, you can check whether the words are present in the vocabulary and
    then get the word vectors for the words. Since fastText supports out-of-vector
    for the words, you should be able to get the word vectors even if the words are
    not present in the vocabulary. This will not work in cases where none of the character
    n-grams were present in the vocabulary:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gensim 中，你可以检查词汇表中是否包含某个词，并获取该词的词向量。由于 fastText 支持为词汇表中没有的词生成词向量，你应该能够即使词不在词汇表中也能获取其词向量。如果词汇表中没有任何字符
    n-grams，可能就无法获取词向量：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Model Evaluation
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Since Gensim implements an unsupervised algorithm, there is no direct way of
    measuring how good the resulting model is. Evaluating models depends on your use
    case and how well it's working out in your end applications.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Gensim 实现了无监督算法，因此没有直接的方法来衡量最终模型的好坏。模型评估取决于你的使用场景以及它在最终应用中的表现。
- en: Gensim fastText has various methods that you can use for finding the similarity
    between words. The following results were received by loading the `wiki.simple.bin` model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim fastText 提供了多种方法用于查找单词之间的相似度。以下结果是通过加载 `wiki.simple.bin` 模型得到的。
- en: 'The easiest way to calculate the similarity between two words is using the
    `similarity` method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两个单词之间相似度的最简单方法是使用`similarity`方法：
- en: '[PRE28]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: FastText computes sentence or document vectors only during supervised learning.
    Depending on the task, simple average word embeddings of all the normalized words
    in the sentence should suffice.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: FastText仅在监督学习期间计算句子或文档向量。根据任务的不同，简单地对句子中所有标准化单词的词嵌入进行平均应该就足够了。
- en: 'You can get the similarities between two documents using the `n_similarity` method.
    According to the Gensim documentation, this method will give the cosine similarity
    between the two documents. Those documents need to be passed as a list:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`n_similarity`方法来获取两个文档之间的相似度。根据Gensim的文档，这个方法将返回两个文档之间的余弦相似度。这些文档需要作为列表传入：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Gensim gives you ability to search for the most irrelevant document as well,
    kind of like finding the odd man out:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 还允许你查找最不相关的文档，类似于找到与众不同的那个：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `most_similar` method will give you the most similar words according to
    the model:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`most_similar`方法将根据模型给出最相似的单词：'
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Gensim provides an easy-to-use method to evaluate the model on the WordSim
    353 benchmark. This dataset is a standard dataset for evaluating vector space
    models. There is no context around each word and the rating between the similarity
    of the words is on a scale of 0 to 10 in increasing order. You can find the file
    in `gensim/test/test_data/wordsim353.tsv` in the Gensim GitHub repository:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim 提供了一个易于使用的方法来在 WordSim 353 基准测试上评估模型。这个数据集是评估向量空间模型的标准数据集。每个单词没有上下文，它们之间的相似度评分在0到10的范围内逐渐增加。你可以在
    Gensim GitHub 仓库的`gensim/test/test_data/wordsim353.tsv`中找到该文件：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first result is the Pearson correlation coefficient (which is the normal
    correlation coefficient that we know of) and the second result is the Spearman
    coefficient.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个结果是皮尔逊相关系数（即我们熟知的普通相关系数），第二个结果是斯皮尔曼相关系数。
- en: You can also use the `most_similar` method to get queries of the type of *A
    - B + C:*
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`most_similar`方法来进行 *A - B + C* 类型的查询：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Similar to this type of syntactic and semantic similarity test, if you are creating
    word vectors in English, you can use the `question-words.txt` task that has been
    prepared and released by Google. You can find the text file in `gensim/docs/notebooks/datasets/question-words.txt`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于这种类型的语法和语义相似性测试，如果你正在创建英语单词向量，你可以使用 Google 准备并发布的`question-words.txt`任务。你可以在`gensim/docs/notebooks/datasets/question-words.txt`找到这个文本文件。
- en: 'Now, you can run the following code. Also, set the logging to info so that
    you can get the accuracy in terms of percentages on the different fields. There
    are nine types of syntactic comparisons in the dataset, family, comparative, superlative,
    present-participle, nationality-adjective, past-tense, and plural:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以运行以下代码。同时，将日志设置为 info，这样你就能根据不同字段的百分比获得准确性。数据集中有九种语法比较类型：家族、比较级、最高级、现在分词、民族形容词、过去式和复数：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will give you an output and show you where there is a mismatch where the
    answers don't match with the list of words. You can evaluate based on that. If
    you are training for a different language, then one good investment may be to
    create a similar `question-words.txt` in the target language, based on different
    grammatical focal points in that language.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给你一个输出，显示哪里存在不匹配的地方，即答案与单词列表不符。你可以根据这个进行评估。如果你正在训练其他语言，考虑创建一个类似的`question-words.txt`文件，这个文件可以根据目标语言的不同语法焦点进行构建，这将是一个不错的投资。
- en: Word Mover's Distance
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word Mover's Distance
- en: '**Word Mover''s Distance** (**WMD**) is a good way of capturing two documents,
    even when there are no common words between them. Take a look at the following
    example. Words like **greet** and **speaks** are fairly near to each other if
    we consider the WMD:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**Word Mover''s Distance** (**WMD**) 是一种非常有效的方式来衡量两个文档之间的相似性，即使它们之间没有共同的单词。看看以下示例。如果我们考虑
    WMD，像**greet**和**speaks**这样的词会非常接近：'
- en: '![](img/00085.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00085.jpeg)'
- en: Source: [https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html)'
- en: 'In Gensim, you can find the distance between two documents using the `wmdistance`
    method, shown as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在Gensim中，你可以使用`wmdistance`方法来查找两个文档之间的距离，如下所示：
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can initialize a word mover similarity class on your corpus:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在你的语料库上初始化一个单词移动相似度类：
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, `wmd_corpus` is your corpus and `model` is your trained fastText model.
    Now, you can run a query on the instance, which is simply a *lookup* on the class.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`wmd_corpus`是您的语料库，`model`是您训练好的fastText模型。现在，您可以在实例上运行查询，这只是一个*查找*操作。
- en: Getting more out of the training process
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从训练过程中获取更多信息
- en: As we are going through the model training process, you will also be interested
    in knowing the progress and performance of the model. Understanding how the model
    learns can be very helpful and makes it easier to debug the model and improve
    it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行模型训练的过程中，您可能还会想了解模型的进展和性能。了解模型是如何学习的，这对调试模型和提高其性能非常有帮助。
- en: Another concern that may arise is training on large corpora. Training multiple
    epochs on large corpora may take a lot of time and hence you may want to save
    the model after the completion of each epoch.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能出现的问题是训练大型语料库。对大型语料库进行多个周期的训练可能需要很长时间，因此您可能希望在每个周期完成后保存模型。
- en: 'In such scenarios, Gensim implements the callback parameter, which takes a
    sequence of subclasses of `CallbackAny2Vec` from `gensim.models.callbacks module`.
    Using this class, you can create classes that save the function at specific points
    in the training process:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，Gensim实现了`CallbackAny2Vec`类的回调参数，`CallbackAny2Vec`是来自`gensim.models.callbacks`模块的子类序列。通过使用这个类，您可以创建在训练过程中某些特定点保存函数的类：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `EpochSaver` class saves the model at every ending of the epoch cycle. The
    `EpochLogger` class does two things. It prints the epoch start and stop, and whenever
    there is a batch begin cycle, it saves the similarity score to a list named similarity.
    We will use this list later for visualization.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`EpochSaver`类在每个周期结束时保存模型。`EpochLogger`类做两件事。它打印周期的开始和结束，并且每当开始一个批次循环时，它将相似度分数保存到一个名为similarity的列表中。我们稍后会使用这个列表进行可视化。'
- en: 'Now, instantiate these classes and pass them to the model training process:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，实例化这些类并将它们传递给模型训练过程：
- en: '[PRE40]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: When you run this code, you should be able to see the logger working and logging
    the epochs. Also, the different models will get saved onto disk.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，您应该能够看到日志记录器在工作并记录每个周期的日志。同时，不同的模型将被保存在磁盘上。
- en: 'To see how the similarity scores have progressed with the training, you can
    start a visdom server. Visdom is a visualization package by Facebook, which runs
    as a server. Its advantage is that you can send data to it, and the update parameters
    can be monitored using a web browser. To start a visdom server, you will need
    to have visdom installed and then you can run it from the command line:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看相似度分数随着训练的进展，您可以启动一个visdom服务器。Visdom是Facebook推出的可视化工具包，它作为服务器运行。它的优点是您可以将数据发送给它，并且可以使用网页浏览器监控更新的参数。要启动visdom服务器，您需要先安装visdom，然后从命令行运行它：
- en: '[PRE41]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now, you can pass the similarity scores to the server:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将相似度分数传递给服务器：
- en: '[PRE42]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you open the server at `http://localhost:8097`, you should be able to see
    this graph:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在`http://localhost:8097`打开服务器，您应该能够看到这个图：
- en: '![](img/00086.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00086.jpeg)'
- en: An example of a generated visdom graph
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一个生成的visdom图表示例
- en: Machine translation using Gensim
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Gensim进行机器翻译
- en: 'According to Mikolov''s 2013 paper, the link to which is given in the references,
    you can use the following method, which consists of two steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Mikolov 2013年的论文，参考文献中提供了该论文的链接，您可以使用以下方法，该方法包括两个步骤：
- en: First, monolingual models of languages are built using large amounts of text
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用大量文本构建单语模型
- en: A small bilingual dictionary is used to learn a linear projection between languages
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个小型双语词典来学习语言间的线性投影
- en: So for the first step, you can simply use the fastText models that are prebuilt
    and shared on the [fasttext.cc](https://fasttext.cc/) website. In this section,
    we will take a look at how to implement the second step using Gensim.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于第一步，您可以直接使用[fasttext.cc](https://fasttext.cc/)网站上预构建并共享的fastText模型。在这一部分，我们将看看如何使用Gensim实现第二步。
- en: The aim is to train a translation matrix, which is essentially a linear transformation
    matrix that links the source word vectors and the target word vectors.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是训练一个翻译矩阵，它本质上是一个线性变换矩阵，将源语言词向量与目标语言词向量连接起来。
- en: 'You can download the transformation file from source language to target language;
    a good source is the Facebook muse documentation and your languages of interest
    may be listed there. If not, then you will need to put the effort into creating
    the transformation file yourself. In the example for this section, which you can
    find in the repo, the `en-it.txt` file was used for the English to Italian translation
    and it had 103,612 similar words, and hence you should probably create similar
    word transformation files for your models to be somewhat good in performance.
    Once you have the models and the transformation file, load the transformation
    file to a `word_pair` tuple and load the vectors to respective source target models.
    Once done, you can run code that looks like the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从源语言到目标语言下载转换文件；一个好的来源是 Facebook muse 文档，你感兴趣的语言可能会列在那里。如果没有，你将需要付出努力自己创建转换文件。在本节的示例中，你可以在代码库中找到，`en-it.txt`
    文件用于英到意大利语的翻译，并且它包含了 103,612 个相似单词，因此你可能需要为你的模型创建类似的单词转换文件，以便在性能上达到一定水准。一旦你有了模型和转换文件，将转换文件加载到
    `word_pair` 元组，并将向量加载到各自的源目标模型中。完成后，你可以运行类似于以下代码：
- en: '[PRE43]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'At prediction time, for any given new word, we can map it to the other language
    space by computing *z* = *Wx*, and then we find the word that is closest in representation
    to the *z* vector in the target language space. The distance metric that is considered
    is the cosine similarity. This works similarly to the code shown here:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时，对于任何给定的新单词，我们可以通过计算 *z* = *Wx* 将其映射到另一个语言空间，然后我们找到在目标语言空间中与 *z* 向量表示最接近的单词。考虑的距离度量是余弦相似度。这与此处所示的代码类似：
- en: '[PRE44]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You should be able to see an output that looks similar to the following code:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够看到类似以下代码的输出：
- en: '[PRE45]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can see that the translations are convincing. The vectors are plotted on
    the following graph:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到翻译结果是可信的。向量被绘制在以下图表中：
- en: '![](img/00087.jpeg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00087.jpeg)'
- en: Vectors plotted on graph
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中绘制的向量
- en: Code on model training and assessing the visualizations in more detail is shown
    in the Jupyter notebook `gensim translation matrix with fasttext.ipynb`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练代码和更详细的可视化评估可以在 Jupyter notebook `gensim translation matrix with fasttext.ipynb`
    中找到。
- en: Summary
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: With this, we have come to the end of this chapter, where we discussed how to
    perform training, validation, and prediction in a Python environment. To achieve
    that, we focused on two packages, the official fastText Python package and the
    Gensim package.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经结束了本章内容，讨论了如何在 Python 环境中进行训练、验证和预测。为了实现这一点，我们重点介绍了两个包：官方的 fastText
    Python 包和 Gensim 包。
- en: In the next chapter, we will take a look at how to integrate fastText into a
    machine learning or a deep learning pipeline.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看看如何将 fastText 集成到机器学习或深度学习的工作流中。
