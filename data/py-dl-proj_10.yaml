- en: Building Face Recognition Using FaceNet
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用FaceNet构建人脸识别
- en: 'In the previous chapter, we learned how to detect objects in an image. In this
    chapter, we will look into a specific use case of object detection—face recognition.
    Face recognition is a combination of two major operations: face detection, followed
    by face classification.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何在图像中检测物体。本章中，我们将探讨物体检测的一个具体应用——人脸识别。人脸识别结合了两项主要操作：人脸检测，接着是人脸分类。
- en: The (hypothetical) client that provides our business use case for us in this
    project is a high-performance computing data center Tier III, certified for sustainability.
    They have designed the facility to meet the very highest standards for protection
    against natural disasters, with many redundant systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中提供我们业务用例的（假设）客户是一个高性能计算数据中心，属于Tier III，并获得了可持续性认证。他们设计了这个设施，以满足对自然灾害的最高保护标准，并配备了许多冗余系统。
- en: The facility currently has ultra-high security protocols in place to prevent
    malicious, man-made disasters, and they are looking to augment their security
    profile with facial recognition for access to secure areas throughout the facility.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 目前该设施已经实施了超高安全协议，以防止恶意的人为灾难，并且他们希望通过人脸识别技术增强其安全性，用于控制设施内的安全区域访问。
- en: 'The stakes are high, as the servers they house and maintain process some of
    the most sensitive, valuable, and influential data in the world:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 他们所容纳和维护的服务器处理着世界上最敏感、最有价值且最具影响力的数据，因此风险非常高：
- en: '![](img/502f99fb-81dc-4e6b-be61-e3723e779922.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/502f99fb-81dc-4e6b-be61-e3723e779922.png)'
- en: This facial recognition system would need to be able to accurately identify
    not only their own employees, but employees of their clients, who occasionally
    tour the data center for inspection.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人脸识别系统需要能够准确识别出他们自己员工的身份，同时也能识别出偶尔参观数据中心进行检查的客户员工。
- en: They have asked us to provide a POC for this intelligence-based capability,
    for review and later inclusion throughout their data center.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 他们要求我们提供一个基于智能的能力的POC，供审查并随后在他们的数据中心中应用。
- en: 'So, in this chapter, we will learn how to build a world-class face recognition
    system. We will define the pipeline as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在本章中，我们将学习如何构建一个世界级的人脸识别系统。我们将定义如下的流程：
- en: '**Face detection**: First, look at an image and find all the possible faces
    in it'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人脸检测**：首先，查看一张图像并找到其中所有可能的人脸'
- en: '**Face extraction**: Second, focus on each face image and understand it, for
    example if it is turned sideways or badly lit'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人脸提取**：其次，聚焦于每一张人脸图像并理解它，例如它是否转向一侧或光线较暗'
- en: '**Feature extraction**: Third, extract unique features from the faces using
    convolutional neural networks (CNNs)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取**：第三，使用卷积神经网络（CNN）从人脸中提取独特特征'
- en: '**Classifier training**: Finally, compare the unique features of that face
    to all the people already known, to determine the person''s name'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分类器训练**：最后，将该人脸的独特特征与所有已知人员的特征进行比较，从而确定此人的姓名'
- en: 'You will learn the main ideas behind each step, and how to build your own facial
    recognition system in Python using the following deep-learning technologies:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习每个步骤背后的主要思想，以及如何使用以下深度学习技术在Python中构建你自己的面部识别系统：
- en: '**dlib** (**[http://dlib.net/](http://dlib.net/)**): Provides a library that
    can be used for facial detection and alignment.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dlib** (**[http://dlib.net/](http://dlib.net/)**)：提供一个可以用于人脸检测和对齐的库。'
- en: '**OpenFace** (**[https://cmusatyalab.github.io/openface/](https://cmusatyalab.github.io/openface/)**): A
    deep-learning facial recognition model, developed by Brandon Amos *et al* ([http://bamos.github.io/](http://bamos.github.io/)).
    It is able to run on real-time mobile devices as well.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenFace** (**[https://cmusatyalab.github.io/openface/](https://cmusatyalab.github.io/openface/)**)：一个深度学习人脸识别模型，由Brandon
    Amos *等人*（[http://bamos.github.io/](http://bamos.github.io/)）开发。它还能够在实时移动设备上运行。'
- en: '**FaceNet **(**[https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832)**):
    A CNN architecture that is used for feature extraction. For a loss function, FaceNet
    uses triplet loss. Triplet loss relies on minimizing the distance from positive examples, while
    maximizing the distance from negative examples.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FaceNet** (**[https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832)**)：一种用于特征提取的CNN架构。FaceNet使用三元组损失作为损失函数。三元组损失通过最小化正样本之间的距离，同时最大化负样本之间的距离来工作。'
- en: Setup environment
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境
- en: Since setup can get very complicated and take a long time, which is not on the
    agenda for this chapter, we will be building a Docker image that contains all
    the dependencies, including dlib, OpenFace, and FaceNet.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设置过程可能非常复杂并且耗时，且本章不涉及这些内容，我们将构建一个包含所有依赖项（包括 dlib、OpenFace 和 FaceNet）的 Docker
    镜像。
- en: Getting the code
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取代码
- en: 'Fetch the code that we will use to build face recognition from the repository:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从仓库中获取我们将用来构建人脸识别的代码：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building the Docker image
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Docker 镜像
- en: Docker is a container platform that simplifies deployment. It solves the problem
    of installing software dependencies onto different server environments. If you
    are new to Docker, you can read more at [https://www.docker.com/](https://www.docker.com/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个容器平台，简化了部署过程。它解决了在不同服务器环境中安装软件依赖的问题。如果你是 Docker 新手，可以在 [https://www.docker.com/](https://www.docker.com/)
    阅读更多内容。
- en: 'To install Docker on Linux machines, run the following command:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Linux 机器上安装 Docker，请运行以下命令：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For other systems such as macOS and Windows, visit [https://docs.docker.com/install/](https://docs.docker.com/install/). You
    can skip this step if you already have Docker installed.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 macOS 和 Windows 等其他系统，请访问 [https://docs.docker.com/install/](https://docs.docker.com/install/)。如果你已经安装了
    Docker，可以跳过此步骤。
- en: 'Once Docker is installed, you should be able to use the `docker` command in
    the Terminal, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 Docker 后，你应该能够在终端中使用 `docker` 命令，示例如下：
- en: '![](img/b83035db-a849-4191-b03a-ac2606bb586d.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b83035db-a849-4191-b03a-ac2606bb586d.png)'
- en: 'Now we will create a `docker` file that will install all the dependencies,
    including OpenCV, dlib, and TensorFlow. This file is available in the repository
    at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个 `docker` 文件，安装所有依赖项，包括 OpenCV、dlib 和 TensorFlow。该文件可以在 GitHub 仓库中找到，链接如下：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter10/Dockerfile)：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now execute the following command to build the image:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在执行以下命令来构建镜像：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It will take approximately 20-30 mins to install all the dependencies and build
    the Docker image:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 安装所有依赖项并构建 Docker 镜像大约需要 20-30 分钟：
- en: '![](img/72357caa-7331-4479-aca1-1b83aa503441.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72357caa-7331-4479-aca1-1b83aa503441.png)'
- en: Downloading pre-trained models
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载预训练模型
- en: We will download a few more artifacts, which we will use and discuss in detail
    later in this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将下载一些额外的文件，这些文件将在本章后面详细使用和讨论。
- en: 'Download dlib''s face landmark predictor, using the following commands:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令下载 dlib 的人脸关键点预测器：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Download the pre-trained Inception model:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下载预训练的 Inception 模型：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we have all the components ready, the folder structure should look roughly
    as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好所有组件，文件夹结构应该大致如下所示：
- en: '![](img/43e24df1-b3c1-427d-bec4-5f4588a01464.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43e24df1-b3c1-427d-bec4-5f4588a01464.png)'
- en: The folder structure of the code
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的文件夹结构
- en: Make sure that you keep the images of the person you want to train the model
    with in the `/data` folder, and name the folder as `/data/<class_name>/<class_name>_000<count>.jpg`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你将要训练模型的人的图像保存在 `/data` 文件夹中，并将该文件夹命名为 `/data/<class_name>/<class_name>_000<count>.jpg`。
- en: The `/output` folder will contain the trained SVM classifier and all preprocessed
    images inside a subfolder `/intermediate`, using the same folder nomenclature
    as in the `/data` folder.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`/output` 文件夹将包含训练后的 SVM 分类器和所有预处理的图像，这些图像将保存在一个子文件夹 `/intermediate` 中，使用与
    `/data` 文件夹相同的命名规则。'
- en: '**Pro tip**: For better performance in terms of accuracy, always keep more
    than five samples of images for each class. This will help the model to converge
    faster and generalize better.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**专业提示**：为了提高准确度，始终确保每个类别有超过五张样本图像。这将有助于模型更快地收敛，并且能够更好地泛化。'
- en: Building the pipeline
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建管道
- en: Facial recognition is a biometric solution that measures the unique characteristics
    of faces. To perform facial recognition, you'll need a way to uniquely represent
    a face.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸识别是一种生物识别解决方案，它通过测量面部的独特特征来进行识别。为了执行人脸识别，你需要一种方式来唯一地表示一个面孔。
- en: The main idea behind any face recognition system is to break the face down into
    unique features, and then use those features to represent identity.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 任何人脸识别系统的基本思想是将面部特征分解为独特的特征，然后使用这些特征来表示身份。
- en: Building a robust pipeline for feature extraction is very important, as it will
    directly affect the performance and accuracy of our system. In 1960, Woodrow Bledsoe
    used a technique involving marking the coordinates of prominent features of a
    face. Among these features were the location of hairline, eyes, and nose.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个强大的特征提取流水线非常重要，因为它将直接影响我们系统的性能和准确性。1960年，伍德罗·布莱德索（Woodrow Bledsoe）使用了一种技术，标记出面部显著特征的坐标。这些特征包括发际线、眼睛和鼻子的位置信息。
- en: Later, in 2005, a much robust technique was invented, **Histogram of Oriented
    Gradients** (**HOG**). This captured the orientation of the dense pixels in the
    provided image.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，在2005年，发明了一种更强大的技术——**定向梯度直方图**（**HOG**）。这种技术捕捉了图像中密集像素的朝向。
- en: The most advanced technique yet, outperforming all others at the time of writing,
    uses CNNs. In 2015, researchers from Google released a paper describing their
    system, FaceNet ([https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832)),
    which uses a CNN relying on image pixels to identify features, rather than extracting
    them manually.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最先进的技术，超越了所有其他技术，使用的是卷积神经网络（CNN）。2015年，谷歌的研究人员发布了一篇论文，描述了他们的系统FaceNet ([https://arxiv.org/abs/1503.03832](https://arxiv.org/abs/1503.03832))，该系统使用CNN依赖图像像素来识别特征，而不是手动提取它们。
- en: 'To build the face recognition pipeline, we will devise the following flow (represented
    by orange blocks in the diagram):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建面部识别流水线，我们将设计以下流程（在图中用橙色方块表示）：
- en: '**Preprocessing**: Finding all the faces, fixing the orientation of the faces'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：找到所有的面部，修正面部的朝向。'
- en: '**Feature extraction**: Extracting unique features from the processed faces'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：从处理过的面部图像中提取独特的特征。'
- en: '**Classifier training**: Training the SVM classifier with 128 dimensional features'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器训练**：使用128维特征训练SVM分类器。'
- en: 'The diagram is as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图示如下：
- en: '![](img/2ac0f9bc-860a-4d3e-bb7e-03fe36b45693.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ac0f9bc-860a-4d3e-bb7e-03fe36b45693.png)'
- en: This image illustrates the end to end flow for face recognition pipeline
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图示例了面部识别流水线的端到端流程。
- en: We will look into each of the steps, and build our world-class face recognition
    system.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细查看每一个步骤，并构建我们世界级的面部识别系统。
- en: Preprocessing of images
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像的预处理
- en: The first step in our pipeline is face detection. We will then align the faces,
    extract features, and then finalize our preprocessing on Docker.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们流水线的第一步是面部检测。然后我们将对面部进行对齐，提取特征，并最终在Docker上完成预处理。
- en: Face detection
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面部检测
- en: Obviously, it's very important to first locate the faces in the given photograph so
    that they can be fed into the later part of the pipeline. There are lots of ways
    to detect faces, such as detecting skin textures, oval/round shape detection,
    and other statistical methods. We're going to use a method called HOG.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 很显然，首先定位给定照片中的面部是非常重要的，这样它们才能被送入流水线的后续部分。检测面部有很多方法，例如检测皮肤纹理、椭圆/圆形形状检测以及其他统计方法。我们将使用一种叫做HOG的方法。
- en: '**HOG** is a feature descriptor that representsthe distribution (histograms)
    of directions of gradients (oriented gradients), which are used as features. Gradients
    (*x* and *y* derivatives) of an image are useful, because the magnitude of gradients
    is large around edges and corners (regions of abrupt intensity changes), which
    are excellent features in a given image.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**HOG**是一种特征描述符，表示梯度方向（或定向梯度）的分布（直方图），这些梯度被用作特征。图像的梯度（*x*和*y*导数）是有用的，因为在边缘和角落（强度变化突然的区域）周围，梯度的幅值较大，而这些是图像中的优秀特征。'
- en: To find faces in an image, we'll convert the image into greyscale. Then we'll
    look at every single pixel in our image, one at a time, and try to extract the
    orientation of the pixels using the HOG detector. We'll be using `dlib.get_frontal_face_detector()`
    to create our face detector.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像中找到面部，我们将把图像转换为灰度图像。然后，我们会逐个查看图像中的每个像素，并尝试使用HOG检测器提取像素的朝向。我们将使用`dlib.get_frontal_face_detector()`来创建我们的面部检测器。
- en: 'The following small snippet demonstrates the HOG-based face detector being
    used in the implementation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小示例展示了基于HOG的面部检测器在实现中的应用：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Aligning faces
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对齐面部
- en: Once we know the region in which the face is located, we can perform various
    kinds of isolation techniques to extract the face from the overall image.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道面部所在的区域，就可以执行各种隔离技术，从整体图像中提取出面部。
- en: One challenge to deal with is that faces in images may be turned in different
    directions, making them look different to the machine.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 需要解决的一个挑战是，图像中的人脸可能会被旋转成不同的方向，使其在机器看来有些不同。
- en: To solve this issue, we will warp each image so that the eyes and lips are always
    in the sample place in the provided images. This will make it a lot easier for
    us to compare faces in the next steps. To do so, we are going to use an algorithm
    called **face landmark estimation**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将对每张图像进行扭曲，使得眼睛和嘴唇始终位于提供图像中的相同位置。这将使我们在接下来的步骤中更容易进行人脸比较。为此，我们将使用一种叫做**人脸地标估计**的算法。
- en: The basic idea is we will come up with 68 specific points (called *landmarks*)
    that exist on every face—the top of the chin, the outside edge of each eye, the
    inner edge of each eyebrow, and so on. Then we will train a machine learning algorithm
    to be able to find these 68 specific points on any face.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是，我们将提出68个特定的关键点（称为*地标*），这些关键点存在于每张脸上——下巴顶部、每只眼睛的外缘、每条眉毛的内缘等等。然后，我们将训练一个机器学习算法，使其能够在任何脸部上找到这68个特定的关键点。
- en: 'The 68 landmarks we will locate on every face are shown in the following diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每张脸上定位的68个地标显示在下图中：
- en: '![](img/9bef96ab-549f-4ff0-827b-a61f8b83857f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bef96ab-549f-4ff0-827b-a61f8b83857f.png)'
- en: This image was created by Brandon Amos ([http://bamos.github.io/](http://bamos.github.io/)), who
    works on OpenFace ([https://github.com/cmusatyalab/openface](https://github.com/cmusatyalab/openface)).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像是由Brandon Amos创建的（[http://bamos.github.io/](http://bamos.github.io/)），他在OpenFace项目中工作（[https://github.com/cmusatyalab/openface](https://github.com/cmusatyalab/openface)）。
- en: 'Here is a small snippet demonstrating how to use face landmarks, which we downloaded
    in the *Setup environment* section:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个小片段，演示了如何使用我们在*环境设置*部分下载的人脸地标：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using this, we can perform various basic image transformations such as rotation
    and scaling while preserving parallel lines. These are also known as affine transformations
    ([https://en.wikipedia.org/wiki/Affine_transformation](https://en.wikipedia.org/wiki/Affine_transformation)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方法，我们可以执行各种基本的图像变换，如旋转和缩放，同时保持平行线的特性。这些变换也被称为仿射变换（[https://en.wikipedia.org/wiki/Affine_transformation](https://en.wikipedia.org/wiki/Affine_transformation)）。
- en: 'The output is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/5904612e-0818-43da-84a5-495b38a742f2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5904612e-0818-43da-84a5-495b38a742f2.png)'
- en: With segmentation, we solved finding the largest face in an image, and with alignment,
    we standardized the input image to be in the center based on the location of eyes
    and bottom lip.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*分割*，我们解决了在图像中找到最大脸部的问题，而通过*对齐*，我们基于眼睛和下唇的位置，将输入图像标准化为居中。
- en: 'Here is a sample from our dataset, showing the raw image and processed image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们数据集中的一个示例，展示了原始图像和处理后的图像：
- en: '![](img/e94c82b8-af23-45bf-b3c4-b75e74487302.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e94c82b8-af23-45bf-b3c4-b75e74487302.png)'
- en: Feature extraction
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取
- en: Now that we've segmented and aligned the data, we'll generate vector embeddings
    of each identity. These embeddings can then be used as input to a classification,
    regression, or clustering task.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了数据的分割和对齐，我们将生成每个身份的向量嵌入。这些嵌入可以作为分类、回归或聚类任务的输入。
- en: This process of training a CNN to output face embeddings requires a lot of data
    and computer power. However, once the network has been trained, it can generate
    measurements for any face, even ones it has never seen before! So this step only
    needs to be done once.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个CNN输出人脸嵌入的过程需要大量的数据和计算能力。然而，一旦网络训练完成，它就可以为任何脸部生成测量结果，甚至是它从未见过的脸！因此，这一步只需要做一次。
- en: For convenience, we have provided a model that has been pre-trained on Inception-Resnet-v1,
    which you can run over any face image to get the 128 dimension feature vectors.
    We downloaded this file in the *Setup environment* section, and it's located in
    the `/pre-model/Resnet-185253.pb` directory.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们提供了一个已经在Inception-Resnet-v1上预训练的模型，您可以在任何人脸图像上运行它，以获取128维特征向量。我们在*环境设置*部分下载了此文件，它位于`/pre-model/Resnet-185253.pb`目录中。
- en: If you want to try this step yourself, OpenFace provides a Lua script ([https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua](https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua)) that
    will generate embeddings for all images in a folder and write them to a CSV file.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想自己尝试这个步骤，OpenFace提供了一个Lua脚本（[https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua](https://github.com/cmusatyalab/openface/blob/master/batch-represent/batch-represent.lua)），该脚本会生成文件夹中所有图像的嵌入，并将它们写入CSV文件。
- en: The code to create the embeddings for the input images can be found further
    after the paragraph. The code is available in the repository at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 创建输入图像嵌入的代码可以在段落后找到。该代码可在仓库中找到：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter10/facenet/train_classifier.py)。
- en: 'In the process, we are loading trained components from the Resnet model such
    as `embedding_layer`, `images_placeholder`, and `phase_train_placeholder`, along
    with the images and the labels:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们从Resnet模型加载了训练好的组件，如`embedding_layer`、`images_placeholder`和`phase_train_placeholder`，以及图像和标签：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is a quick view of the embedding creating process. We fed the image and
    the label data along with few components from the pre-trained model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是嵌入创建过程的快速概览。我们将图像和标签数据以及来自预训练模型的几个组件一起输入：
- en: '![](img/d76fb9cd-290b-4b07-b66d-9abfa52b0cde.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d76fb9cd-290b-4b07-b66d-9abfa52b0cde.png)'
- en: The output of the process will be a vector of 128 dimensions, representing the
    facial image.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的输出将是一个128维的向量，表示人脸图像。
- en: Execution on Docker
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Docker上执行
- en: We will implement preprocessing on our Docker image. We'll mount the `project`
    directory as a volume inside the Docker container (using a `-v` flag), and run
    the preprocessing script on the input data. The results will be written to a directory
    specified with command-line arguments.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Docker镜像上实现预处理。我们将通过`-v`标志将`project`目录挂载为Docker容器中的一个卷，并在输入数据上运行预处理脚本。结果将写入通过命令行参数指定的目录。
- en: 'The `align_dlib.py` file is sourced from CMU. It provides methods for detecting
    a face in an image, finding facial landmarks, and aligning these landmarks:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`align_dlib.py`文件来自CMU。它提供了检测图像中的人脸、查找面部特征点并对齐这些特征点的方法：'
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding command we are setting the input data path using a `--input-dir` flag.
    This directory should contain the images that we want to process.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，我们通过`--input-dir`标志设置了输入数据路径。该目录应包含我们要处理的图像。
- en: We are also setting the output path using a `--output-dir` flag, which will
    store the segmented aligned images. We will be using these output images as input
    for training.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`--output-dir`标志设置了输出路径，存储分割对齐的图像。我们将使用这些输出图像作为训练输入。
- en: The `--crop-dim` flag is to define the output dimensions of the image. In this
    case, all images will be stored at 180 × 180.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`--crop-dim`标志用于定义图像的输出尺寸。在这种情况下，所有图像将被存储为180 × 180。'
- en: The outcome of this process will be an `/intermediate` folder being created
    inside the `/output` folder, containing all the preprocessed images.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的结果将在`/output`文件夹内创建一个`/intermediate`文件夹，其中包含所有预处理过的图像。
- en: Training the classifier
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练分类器
- en: First, we'll load the segmented and aligned images from the `input` directory
    `--input-dir` flag. While training, we'll apply preprocessing to the image. This
    preprocessing will add random transformations to the image, creating more images
    to train on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从`input`目录（`--input-dir`标志）加载已分割并对齐的图像。在训练过程中，我们将对图像进行预处理。此预处理将向图像添加随机变换，从而生成更多的图像用于训练。
- en: These images will be fed in a batch size of 128 into the pre-trained model.
    This model will return a 128-dimensional embedding for each image, returning a
    128 x 128 matrix for each batch.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像将以128的批量大小输入到预训练模型中。该模型将为每张图像返回一个128维的嵌入，为每个批次返回一个128 x 128的矩阵。
- en: After these embeddings are created, we'll use them as feature inputs into a
    scikit-learn SVM classifier to train on each identity.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建这些嵌入后，我们将使用它们作为特征输入到scikit-learn的SVM分类器中，进行每个身份的训练。
- en: 'The following command will start the process, and train the classifier. The
    classifier will be dumped as a `pickle` file in the path defined in the `--classifier-path`
    argument:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将启动该过程并训练分类器。分类器将作为`pickle`文件保存在`--classifier-path`参数定义的路径中：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A few custom arguments are tunable:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一些自定义参数可以调整：
- en: '`--num-threads`: Modify according to the CPU/GPU config'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num-threads`：根据CPU/GPU配置进行修改'
- en: '`--num-epochs`: Change according to your dataset'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num-epochs`：根据你的数据集进行更改'
- en: '`--min-num-images-per-class`: Change according to your dataset'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--min-num-images-per-class`：根据你的数据集进行更改'
- en: '`--is-train`: Set the `True` flag for training'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--is-train`：设置为`True`标志表示训练'
- en: This process will take a while, depending on the number of images you are training
    on. Once the process is completed, you will find a `classifier.pkl` file inside
    the `/output` folder.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能需要一段时间，具体取决于你用于训练的图像数量。完成后，你会在`/output`文件夹内找到一个`classifier.pkl`文件。
- en: Now you can use the `classifier.pkl` file to make predictions, and deploy it
    on production.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用`classifier.pkl`文件进行预测，并将其部署到生产环境中。
- en: Evaluation
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'We will evaluate the performance of the trained model. To do that, we will
    execute the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评估训练好的模型的性能。为此，我们将执行以下命令：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once the execution is completed, you will see predictions with a confidence
    score, as shown in the following screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 执行完成后，你将看到带有置信度分数的预测结果，如下图所示：
- en: '![](img/7e955ea4-dd86-46cd-a2ad-ba86ed8d6d9d.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e955ea4-dd86-46cd-a2ad-ba86ed8d6d9d.png)'
- en: We can see that the model is able to predict with 99.5% accuracy. It is also
    relatively fast.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型能够以99.5%的准确度进行预测，并且预测速度相对较快。
- en: Summary
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have successfully completed a world-class facial recognition POC for our
    hypothetical high-performance data center, utilizing the deep-learning technologies
    of OpenFace, dlib, and FaceNet.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功完成了一个世界级的人脸识别概念验证（POC）项目，利用OpenFace、dlib和FaceNet的深度学习技术，服务于我们假设的高性能数据中心。
- en: 'We built a pipeline that included:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个包含以下内容的流水线：
- en: '**Face detection**: To examine an image and find all the faces it contains'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人脸检测**：检查图像并找到其中包含的所有人脸'
- en: '**Face extraction**: To focus on each face and understand its general qualities'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人脸提取**：集中关注每张人脸并了解其基本特征'
- en: '**Feature extraction**: To pull out unique features from the faces using CNNs'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：通过卷积神经网络（CNN）从人脸中提取独特特征'
- en: '**Classifier training**: To compare those unique features to all the people
    already known, and determine the person''s name'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器训练**：将这些独特特征与所有已知的人进行比较，并确定该人的名字'
- en: The added security level of a robust facial recognition system for access control
    is in keeping with the high standards demanded by this Tier III facility. This
    project is a great example of the power of deep learning to produce solutions
    that make a meaningful impact on the business operations of our clients.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的面部识别系统为访问控制提供的安全等级，符合这一Tier III设施所要求的高标准。这个项目是深度学习强大能力的一个极佳例子，能够为我们客户的业务运营带来有意义的影响。
