- en: Building a Feedforward Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建前馈神经网络
- en: 'In this chapter we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下内容：
- en: Feed-forward propagation from scratch in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中从零开始实现前馈传播
- en: Building back-propagation from scratch in Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中从零开始构建反向传播
- en: Building a neural network in Keras
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中构建神经网络
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: A neural network is a supervised learning algorithm that is loosely inspired
    by the way the brain functions. Similar to the way neurons are connected to each
    other in the brain, a neural network takes input, passes it through a function,
    certain subsequent neurons get excited, and consequently the output is produced.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种监督学习算法，灵感来源于大脑的功能方式。与大脑中神经元之间的连接方式相似，神经网络接收输入，通过一个函数进行处理，某些后续神经元被激活，最终产生输出。
- en: 'In this chapter, you will learn the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习以下内容：
- en: Architecture of a neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的架构
- en: Applications of a neural network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的应用
- en: Setting up a feedforward neural network
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置前馈神经网络
- en: How forward-propagation works
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播是如何工作的
- en: Calculating loss values
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算损失值
- en: How gradient descent works in back-propagation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降在反向传播中的工作原理
- en: The concepts of epochs and batch size
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练轮次（epochs）和批量大小（batch size）的概念
- en: Various loss functions
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种损失函数
- en: Various activation functions
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种激活函数
- en: Building a neural network from scratch
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始构建神经网络
- en: Building a neural network in Keras
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Keras 中构建神经网络
- en: Architecture of a simple neural network
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单神经网络的架构
- en: An artificial neural network is loosely inspired by the way the human brain
    functions. Technically, it is an improvement over linear and logistic regression
    as neural networks introduce multiple non-linear measures in estimating the output.
    Additionally, neural networks provide a great flexibility in modifying the network
    architecture to solve the problems across multiple domains leveraging structured
    and unstructured data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的灵感来源于人脑的工作方式。技术上，它是线性回归和逻辑回归的改进，因为神经网络引入了多个非线性度量来估计输出。此外，神经网络在修改网络架构方面提供了极大的灵活性，可以通过利用结构化和非结构化数据解决多个领域的问题。
- en: The more complex the function, the greater the chance that the network has to
    tune to the data that is given as input, hence the better the accuracy of the
    predictions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 函数越复杂，网络越有可能调整以适应输入数据，从而提高预测的准确性。
- en: 'The typical structure of a feed-forward neural network is as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的前馈神经网络结构如下所示：
- en: '![](img/838c8087-2c87-495b-9064-13c3d634e009.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/838c8087-2c87-495b-9064-13c3d634e009.png)'
- en: A layer is a collection of one or more nodes (computation units), where each
    node in a layer is connected to every other node in the next immediate layer.
    The input level/layer is constituted of the input variables that are required
    to predict the output values.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一层是一个或多个节点（计算单元）的集合，每个层中的节点都与下一层中的每个节点连接。输入层由预测输出值所需的输入变量组成。
- en: The number of nodes in the output layer depends on whether we are trying to
    predict a continuous variable or a categorical variable. If the output is a continuous
    variable, the output has one unit.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的节点数量取决于我们是尝试预测连续变量还是分类变量。如果输出是连续变量，则输出只有一个单元。
- en: 'If the output is categorical with *n* possible classes, there will be *n* nodes
    in the output layer. The hidden level/layer is used to transform the input layer
    values into values in a higher-dimensional space, so that we can learn more features
    from the input. The hidden layer transforms the output as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输出是分类的，并且有 *n* 个可能的类别，则输出层将有 *n* 个节点。隐藏层用于将输入层的值转换为高维空间中的值，以便我们可以从输入中学习更多的特征。隐藏层将输出转换如下：
- en: '![](img/7ae13d98-43f6-4c6a-83e8-687365e00a27.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ae13d98-43f6-4c6a-83e8-687365e00a27.png)'
- en: In the preceding diagram, *x[1]*,*x*[*2*, ]..., *x[n]* are the independent variables,
    and *x[0]* is the bias term (similar to the way we have bias in linear/logistic
    regression).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，*x[1]*,*x[2]*,...,*x[n]* 是独立变量，而 *x[0]* 是偏置项（类似于线性/逻辑回归中的偏置）。
- en: 'Note that *w[1]*,*w[2]*, ..., *w[n]* are the weights given to each of the input
    variables. If *a* is one of the units in the hidden layer, it will be equal to
    the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*w[1]*,*w[2]*,...,*w[n]* 是分配给每个输入变量的权重。如果 *a* 是隐藏层中的一个单元，它将等于以下值：
- en: '![](img/96089758-dfe2-470f-99c0-79f087b84c7b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96089758-dfe2-470f-99c0-79f087b84c7b.png)'
- en: The *f* function is the activation function that is used to apply non-linearity
    on top of the sum-product of the input and their corresponding weight values.
    Additionally, higher non-linearity can be achieved by having more than one hidden
    layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*f* 函数是激活函数，用于在输入与其对应权重值的乘积和之上应用非线性。此外，通过增加多个隐藏层，可以实现更高的非线性。'
- en: 'In sum, a neural network is a collection of weights assigned to nodes with
    layers connecting them. The collection is organized into three main parts: the
    input layer, the hidden layer, and the output layer. Note that you can have *n* hidden
    layers, with the term deep learning implying multiple hidden layers. Hidden layers
    are necessary when the neural network has to make sense of something really complicated,
    contextual, or not obvious, such as image recognition. The intermediate layers
    (layers that are not input or output) are known as hidden, since they are practically
    not visible (there''s more on how to visualize the intermediate layers in [Chapter
    4](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building a Deep Convolutional
    Neural Network*).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，神经网络是一个由权重分配给节点并由层连接的集合。这个集合分为三大部分：输入层、隐藏层和输出层。请注意，你可以有 *n* 个隐藏层，深度学习的概念通常意味着有多个隐藏层。当神经网络需要理解一些非常复杂、具有上下文或不明显的内容时，如图像识别，隐藏层是必需的。中间层（不是输入层或输出层的层）被称为隐藏层，因为它们在实践中是不可见的（关于如何可视化中间层的内容可以参考[第4章](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml)，*构建深度卷积神经网络*）。
- en: Training a neural network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Training a neural network basically means calibrating all of the weights in
    a neural network by repeating two key steps: forward-propagation and back-propagation.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络基本上意味着通过重复两个关键步骤：正向传播和反向传播，来校准神经网络中的所有权重。
- en: In forward-propagation, we apply a set of weights to the input data, pass it
    through the hidden layer, perform the nonlinear activation on the hidden layer
    output, and then connect the hidden layer to the output layer by multiplying the
    hidden layer node values with another set of weights. For the first forward-propagation,
    the values of the weights are initialized randomly.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向传播中，我们将一组权重应用到输入数据，经过隐藏层，进行非线性激活，最后将隐藏层连接到输出层，通过将隐藏层节点值与另一组权重相乘来完成。对于第一次正向传播，权重的值是随机初始化的。
- en: In back-propagation, we try to decrease the error by measuring the margin of
    error of output and then adjust weight accordingly. Neural networks repeat both
    forward- and back-propagation to predict an output until the weights are calibrated.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中，我们通过测量输出的误差范围来尝试减少误差，并相应地调整权重。神经网络重复进行正向传播和反向传播，直到权重得到校准，从而预测输出。
- en: Applications of a neural network
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的应用
- en: 'Recently, we have seen a huge adoption of neural networks in a variety of applications.
    In this section, let''s try to understand the reason why adoption might have increased
    considerably. Neural networks can be architected in multiple ways. Here are some
    of the possible ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们已经看到神经网络在各种应用中的广泛采用。在这一部分中，我们将尝试理解为什么这种采用可能大幅增加。神经网络可以通过多种方式进行架构设计。以下是一些可能的方式：
- en: '![](img/78c3e0dd-0c85-40a9-ada6-5193ad120167.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c3e0dd-0c85-40a9-ada6-5193ad120167.png)'
- en: 'The box at the bottom is the input, followed by the hidden layer (the middle
    box), and the box at the top is the output layer. The one-to-one architecture
    is a typical neural network with a hidden layer between the input and output layer.
    Examples of different architectures are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 底部的框是输入，接下来是隐藏层（中间的框），顶部的框是输出层。一对一架构是典型的神经网络，输入层与输出层之间有一个隐藏层。不同架构的示例如下：
- en: '| **Architecture** | **Example** |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **示例** |'
- en: '| One-to-many | The input is an image and the output is a caption for the image
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 一对多 | 输入是图像，输出是该图像的标题 |'
- en: '| Many-to-one | The input is a movie review (multiple words) and the output
    is the sentiment associated with the review |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 多对一 | 输入是电影评论（多个词），输出是与评论相关的情感 |'
- en: '| Many-to-many | Machine translation of a sentence in one language to a sentence
    in another language |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 多对多 | 将一个语言中的句子翻译成另一种语言中的句子 |'
- en: 'Apart from the preceding points, neural networks are also in a position to
    understand the content in an image and detect the position where the content is
    located using an architecture named **Convolutional Neural Network** (**CNN**),
    which looks as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述要点，神经网络还能够理解图像中的内容，并利用一种名为**卷积神经网络**（**CNN**）的架构，检测内容所在的位置，其架构如下所示：
- en: '![](img/2ad769ae-ff98-4ed7-bd58-c9284e1c25f6.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ad769ae-ff98-4ed7-bd58-c9284e1c25f6.png)'
- en: Here, we saw examples of recommender systems, image analysis, text analysis,
    and audio analysis, and we can see that neural networks give us the flexibility
    to solve a problem using multiple architectures, resulting in increased adoption
    as the number of applications increases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了推荐系统、图像分析、文本分析和音频分析的示例，可以看到神经网络为我们提供了灵活性，使我们能够使用多种架构来解决问题，随着应用数量的增加，神经网络的采用率也在上升。
- en: Feed-forward propagation from scratch in Python
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始实现前向传播（Feed-forward propagation）——Python实现
- en: In order to build a strong foundation of how feed-forward propagation works,
    we'll go through a toy example of training a neural network where the input to
    the neural network is (1, 1) and the corresponding output is 0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立前向传播工作的坚实基础，我们将通过一个训练神经网络的玩具示例来进行讲解，其中神经网络的输入为(1, 1)，对应的输出为0。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The strategy that we''ll adopt is as follows: our neural network will have
    one hidden layer (with neurons) connecting the input layer to the output layer.
    Note that we have more neurons in the hidden layer than in the input layer, as
    we want to enable the input layer to be represented in more dimensions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略如下：我们的神经网络将有一个隐藏层（包含神经元），该隐藏层连接输入层与输出层。请注意，隐藏层中的神经元数量比输入层多，因为我们希望让输入层在更多维度上得到表示：
- en: '![](img/ede5ca3a-46a3-4e12-a031-089dd2e8c3da.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ede5ca3a-46a3-4e12-a031-089dd2e8c3da.png)'
- en: '**Calculating the hidden layer unit values**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算隐藏层单元值**'
- en: 'We now assign weights to all of the connections. Note that these weights are
    selected randomly (based on Gaussian distribution) since it is the first time
    we''re forward-propagating. In this specific case, let''s start with initial weights
    that are between 0 and 1, but note that the final weights after the training process
    of a neural network don''t need to be between a specific set of values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为所有连接分配权重。请注意，这些权重是随机选择的（基于高斯分布），因为这是我们第一次进行前向传播。在这个特定的例子中，我们从初始权重开始，权重范围在0和1之间，但请注意，神经网络训练过程中的最终权重不需要介于特定的数值范围之间：
- en: '![](img/12b639fb-0830-4075-b072-57b0d0d97885.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12b639fb-0830-4075-b072-57b0d0d97885.png)'
- en: In the next step, we perform the multiplication of the input with weights to
    calculate the values of hidden units in the hidden layer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们执行输入与权重的乘法运算，以计算隐藏层中隐藏单元的值。
- en: 'The hidden layer''s unit values are obtained as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的单元值如下所示：
- en: '*![](img/c1c88e58-271f-4993-9e05-96a7f4bec990.png)*'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/c1c88e58-271f-4993-9e05-96a7f4bec990.png)*'
- en: '![](img/624d72e6-fad9-46b7-a852-de01b0dedb17.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/624d72e6-fad9-46b7-a852-de01b0dedb17.png)'
- en: '![](img/3662b621-8504-4f08-ad28-46f825de99cc.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3662b621-8504-4f08-ad28-46f825de99cc.png)'
- en: 'The hidden layer''s unit values are also shown in the following diagram:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的单元值在下面的图示中也有显示：
- en: '![](img/3b453e27-b9e7-4527-94a3-c32a3022c51f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b453e27-b9e7-4527-94a3-c32a3022c51f.png)'
- en: Note that in the preceding output we calculated the hidden values. For simplicity,
    we excluded the bias terms that need to be added at each unit of a hidden layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的输出中，我们计算了隐藏层的值。为了简化起见，我们忽略了需要在每个隐藏层单元中添加的偏置项。
- en: Now, we will pass the hidden layer values through an activation function so
    that we attain non-linearity in our output.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过激活函数将隐藏层值传递，以便在输出中获得非线性。
- en: If we do not apply the activation function in the hidden layer, the neural network
    becomes a giant linear connection from input to output.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不在隐藏层中应用激活函数，神经网络将变成一个从输入到输出的巨大线性连接。
- en: '**Applying the activation function**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**应用激活函数**'
- en: Activation functions are applied at multiple layers of a network. They are used
    so that we achieve high non-linearity in input, which can be useful in modeling
    complex relations between the input and output.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数在网络的多个层中应用。它们的作用是使输入具有较高的非线性，这在建模输入和输出之间复杂关系时非常有用。
- en: 'The different activation functions are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的激活函数如下所示：
- en: '![](img/12df72e4-af1b-4ce8-bca4-b125b185f18a.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12df72e4-af1b-4ce8-bca4-b125b185f18a.png)'
- en: 'For our example, let’s use the sigmoid function for activation. The sigmoid
    function looks like this, graphically:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，假设使用sigmoid函数作为激活函数。Sigmoid函数的图形如下所示：
- en: '![](img/41178e09-6508-41af-bf4d-616d4dbf7674.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41178e09-6508-41af-bf4d-616d4dbf7674.png)'
- en: 'By applying sigmoid activation, *S(x)*, to the three hidden=layer *sums*, we
    get the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对三个隐藏层的*总和*应用sigmoid激活函数 *S(x)*，我们得到如下结果：
- en: '*final_h[1] = S(1.0) = 0.73*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*final_h[1] = S(1.0) = 0.73*'
- en: '*final_h[2] = S(1.3) = 0.78*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*final_h[2] = S(1.3) = 0.78*'
- en: '*final_h[3] = S(0.8) = 0.69*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*final_h[3] = S(0.8) = 0.69*'
- en: '**Calculating the output layered values**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算输出层值**'
- en: 'Now that we have calculated the hidden layer values, we will be calculating
    the output layer value. In the following diagram, we have the hidden layer values
    connected to the output through the randomly-initialized weight values. Using
    the hidden layer values and the weight values, we will calculate the output values
    for the following network:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出隐藏层的值，接下来我们将计算输出层的值。在以下图示中，隐藏层的值通过随机初始化的权重值与输出层相连。通过使用隐藏层的值和权重值，我们将计算以下网络的输出值：
- en: '![](img/bdd85755-2ae5-4148-b584-46b254589d77.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdd85755-2ae5-4148-b584-46b254589d77.png)'
- en: 'We perform the sum product of the hidden layer values and weight values to
    calculate the output value. For simplicity, we excluded the bias terms that need
    to be added at each unit of the hidden layer:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将隐藏层值与权重值做点积来计算输出值。为了简化计算，我们省略了每个隐藏层单元中需要添加的偏置项：
- en: '*0.73 * 0.3 + 0.79 * 0.5 + 0.69 * 0.9 = 1.235*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*0.73 * 0.3 + 0.79 * 0.5 + 0.69 * 0.9 = 1.235*'
- en: 'The values are shown in the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值在以下图示中显示：
- en: '![](img/5689850e-73e4-4ce8-86b5-3081086f9410.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5689850e-73e4-4ce8-86b5-3081086f9410.png)'
- en: Because we started with a random set of weights, the value of the output neuron
    is very different from the target, in this case by +1.235 (since the target is
    0).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们一开始使用的是随机初始化的权重，所以输出神经元的值与目标值差异很大，在这个例子中相差+1.235（因为目标值为0）。
- en: '**Calculating the loss values**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算损失值**'
- en: 'Loss values (alternatively called cost functions) are values that we optimize
    in a neural network. In order to understand how loss values get calculated, let''s
    look at two scenarios:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 损失值（也称为成本函数）是我们在神经网络中优化的值。为了理解损失值是如何计算的，让我们来看两个场景：
- en: Continuous variable prediction
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续变量预测
- en: Categorical variable prediction
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别变量预测
- en: '**Calculating loss during continuous variable prediction**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**在连续变量预测中计算损失**'
- en: 'Typically, when the variable is a continuous one, the loss value is calculated
    as the squared error, that is, we try to minimize the mean squared error by varying
    the weight values associated with the neural network:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当变量是连续型时，损失值通过平方误差计算，即我们通过调整与神经网络相关的权重值来最小化均方误差：
- en: '![](img/caeef8ef-a6a7-4b83-b3f8-c9961b637f7b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/caeef8ef-a6a7-4b83-b3f8-c9961b637f7b.png)'
- en: In the preceding equation, *y(i)* is the actual value of output, *h(x)* is the
    transformation that we apply on the input (*x*) to obtain a predicted value of
    *y,* and *m* is the number of rows in the dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*y(i)* 是实际的输出值，*h(x)* 是我们对输入(*x*)进行变换后得到的预测值 *y*，而 *m* 是数据集中的行数。
- en: '**Calculating loss during categorical variable prediction**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**在类别变量预测中计算损失**'
- en: When the variable to predict is a discrete one (that is, there are only a few
    categories in the variable), we typically use a categorical cross-entropy loss
    function. When the variable to predict has two distinct values within it, the
    loss function is binary cross-entropy, and when the variable to predict has multiple
    distinct values within it, the loss function is a categorical cross-entropy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要预测的变量是离散型时（即变量只有少数几类），我们通常使用类别交叉熵损失函数。当需要预测的变量只有两个不同的值时，损失函数为二元交叉熵，而当需要预测的变量有多个不同的值时，损失函数为类别交叉熵。
- en: 'Here is binary cross-entropy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是二元交叉熵：
- en: '*(ylog(p)+(1−y)log(1−p))*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*(ylog(p)+(1−y)log(1−p))*'
- en: 'Here is categorical cross-entropy:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是类别交叉熵：
- en: '![](img/28b794dc-8596-48ba-8f95-23211716611e.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28b794dc-8596-48ba-8f95-23211716611e.png)'
- en: '*y* is the actual value of output *p,* is the predicted value of the output
     and n is the total number of data points. For now, let''s assume that the outcome
    that we are predicting in our toy example is continuous. In that case, the loss
    function value is the mean squared error, which is calculated as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 是输出的实际值，*p* 是预测的输出值，n 是数据点的总数。现在，假设我们在玩具示例中预测的结果是连续的。在这种情况下，损失函数值是均方误差，计算公式如下：'
- en: '*error = 1.235² = 1.52*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*error = 1.235² = 1.52*'
- en: In the next step, we will try to minimize the loss function value using back-propagation
    (which we'll learn about in the next section), where we update the weight values
    (which were initialized randomly earlier) to minimize the loss (error).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将尝试使用反向传播来最小化损失函数值（我们将在下一节学习），在反向传播中，我们更新权重值（之前随机初始化的）以最小化损失（误差）。
- en: How to do it...
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'In the previous section, we learned about performing the following steps on
    top of the input data to come up with error values in forward-propagation (the
    code file is available as `Neural_network_working_details.ipynb` in GitHub):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们学习了如何对输入数据执行以下步骤，以便在前向传播过程中计算误差值（代码文件可在 GitHub 上的 `Neural_network_working_details.ipynb`
    找到）：
- en: Initialize weights randomly
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重
- en: Calculate the hidden layer unit values by multiplying input values with weights
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将输入值与权重相乘来计算隐藏层单元值
- en: Perform activation on the hidden layer values
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对隐藏层的值执行激活
- en: Connect the hidden layer values to the output layer
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将隐藏层的值连接到输出层
- en: Calculate the squared error loss
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算平方误差损失
- en: 'A function to calculate the squared error loss values across all data points
    is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用于计算所有数据点的平方误差损失值的函数如下：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding function, we take the input variable values, weights (randomly
    initialized if this is the first iteration), and the actual output in the provided
    dataset as the input to the feed-forward function.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，我们将输入变量值、权重（如果这是第一次迭代则随机初始化）以及提供的数据集中的实际输出作为输入传递给前馈函数。
- en: 'We calculate the hidden layer values by performing the matrix multiplication
    (dot product) of the input and weights. Additionally, we add the bias values in
    the hidden layer, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过执行输入和权重的矩阵乘法（点积）来计算隐藏层的值。此外，我们还会在隐藏层中加上偏置值，如下所示：
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding scenario is valid when `weights[0]` is the weight value and `weights[1]`
    is the bias value, where the weight and bias are connecting the input layer to
    the hidden layer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前述场景适用于`weights[0]`为权重值，`weights[1]`为偏置值，权重和偏置连接输入层与隐藏层。
- en: 'Once we calculate the hidden layer values, we perform activation on top of
    the hidden layer values, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出隐藏层的值，我们会对隐藏层的值进行激活，计算方法如下：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now calculate the output at the hidden layer by multiplying the output of
    the hidden layer with weights that connect the hidden layer to the output, and
    then adding the bias term at the output, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们通过将隐藏层的输出与连接隐藏层和输出的权重相乘，然后在输出处添加偏差项，来计算隐藏层的输出，如下所示：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the output is calculated, we calculate the squared error loss at each
    row, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦输出被计算出来，我们会在每一行中计算平方误差损失，计算公式如下：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding code, `pred_out` is the predicted output and `outputs` is the
    actual output.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`pred_out` 是预测的输出，`outputs` 是实际的输出。
- en: We are then in a position to obtain the loss value as we forward-pass through
    the network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在通过网络进行前向传播时获得损失值。
- en: While we considered the sigmoid activation on top of the hidden layer values
    in the preceding code, let's examine other activation functions that are commonly
    used.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在前面的代码中，我们对隐藏层的值考虑了 Sigmoid 激活函数，但现在让我们看看其他常用的激活函数。
- en: '**Tanh**'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tanh**'
- en: 'The tanh activation of a value (the hidden layer unit value) is calculated
    as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: tanh 激活值（隐藏层单元值）的计算如下：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**ReLu**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**ReLu**'
- en: 'The **Rectified Linear Unit** (**ReLU**) of a value (the hidden layer unit
    value) is calculated as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）的值（隐藏层单元值）计算如下：'
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Linear**'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**Linear**'
- en: The linear activation of a value is the value itself.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 线性激活值就是该值本身。
- en: '**Softmax**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**Softmax**'
- en: Typically, softmax is performed on top of a vector of values. This is generally
    done to determine the probability of an input belonging to one of the *n* number
    of the possible output classes in a given scenario. Let's say we are trying to
    classify an image of a digit into one of the possible 10 classes (numbers from
    0 to 9). In this case, there are 10 output values, where each output value should
    represent the probability of an input image belonging to one of the 10 classes.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Softmax 会应用于一组值的向量。这通常是为了确定输入属于给定场景中 *n* 个可能输出类别之一的概率。假设我们正在尝试将数字图像分类为 10
    个可能的类别（0 到 9 的数字）。在这种情况下，有 10 个输出值，每个输出值应代表输入图像属于其中一个类别的概率。
- en: 'The softmax activation is used to provide a probability value for each class
    in the output and is calculated explained in the following sections:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 激活函数用于为输出中的每个类别提供一个概率值，计算方法将在以下部分中解释：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Apart from the preceding activation functions, the loss functions that are generally
    used while building a neural network are as follows.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的激活函数，构建神经网络时通常使用的损失函数如下：
- en: '**Mean squared error**'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方误差**'
- en: The error is the difference between the actual and predicted values of the output.
    We take a square of the error, as the error can be positive or negative (when
    the predicted value is greater than the actual value and vice versa). Squaring
    ensures that positive and negative errors do not offset each other. We calculate
    the mean squared error so that the error over two different datasets is comparable
    when the datasets are not the same size.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是输出的实际值和预测值之间的差异。我们对误差取平方，因为误差可能是正值或负值（当预测值大于实际值时，反之亦然）。平方确保正负误差不会相互抵消。我们计算均方误差，以便在两个数据集大小不相同的情况下，可以比较这两个数据集上的误差。
- en: 'The mean squared error between predicted values (`p`) and actual values (`y`)
    is calculated as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值（`p`）和实际值（`y`）之间的均方误差计算公式如下：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The mean squared error is typically used when trying to predict a value that
    is continuous in nature.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 均方误差通常用于预测本质上是连续的值。
- en: '**Mean absolute error**'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差**'
- en: The mean absolute error works in a manner that is very similar to the mean squared
    error. The mean absolute error ensures that positive and negative errors do not
    offset each other by taking an average of the absolute difference between the
    actual and predicted values across all data points.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 平均绝对误差的工作原理与均方误差非常相似。平均绝对误差通过对所有数据点的实际值和预测值之间的绝对差异取平均值，确保正负误差不会相互抵消。
- en: 'The mean absolute error between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值（`p`）和实际值（`y`）之间的平均绝对误差实现公式如下：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Similar to the mean squared error, the mean absolute error is generally employed
    on continuous variables.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于均方误差，平均绝对误差通常应用于连续变量。
- en: '**Categorical cross-entropy**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别交叉熵**'
- en: Cross-entropy is a measure of the difference between two different distributions: actual
    and predicted. It is applied to categorical output data, unlike the previous two
    loss functions that we discussed.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵是衡量两个不同分布之间差异的指标：实际分布和预测分布。与我们讨论的前两种损失函数不同，交叉熵应用于类别输出数据。
- en: 'Cross-entropy between two distributions is calculated as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分布之间的交叉熵计算公式如下：
- en: '![](img/37b30e1e-0e2d-47e7-b9d5-cdfab9cbc8a0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37b30e1e-0e2d-47e7-b9d5-cdfab9cbc8a0.png)'
- en: '*y* is the actual outcome of the event and *p* is the predicted outcome of
    the event.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 是事件的实际结果，*p* 是事件的预测结果。'
- en: 'Categorical cross-entropy between the predicted values (`p`) and actual values
    (`y`) is implemented as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 预测值（`p`）和实际值（`y`）之间的类别交叉熵实现公式如下：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note that categorical cross-entropy loss has a high value when the predicted
    value is far away from the actual value and a low value when the values are close.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当预测值远离实际值时，类别交叉熵损失值较高；而当值接近时，损失值较低。
- en: Building back-propagation from scratch in Python
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始用 Python 构建反向传播
- en: In forward-propagation, we connected the input layer to the hidden layer to
    the output layer. In back-propagation, we take the reverse approach.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播中，我们将输入层连接到隐藏层，再连接到输出层。在反向传播中，我们采取相反的方式。
- en: Getting ready
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We change each weight within the neural network by a small amount – one at a
    time. A change in the weight value will have an impact on the final loss value
    (either increasing or decreasing loss). We'll update the weight in the direction
    of decreasing loss.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们会逐个调整神经网络中的每个权重，幅度很小。权重值的变化会影响最终的损失值（可能是增加或减少损失）。我们会在减少损失的方向上更新权重。
- en: Additionally, in some scenarios, for a small change in weight, the error increases/decreases
    considerably, while in some cases the error decreases by a small amount.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在某些情况下，权重的小幅变化会导致误差大幅增加/减少，而在某些情况下误差变化较小。
- en: 'By updating the weights by a small amount and measuring the change in error
    that the update in weights leads to, we are able to do the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过小幅度更新权重并衡量更新后误差的变化，我们可以做到以下几点：
- en: Determine the direction of the weight update
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定权重更新的方向
- en: Determine the magnitude of the weight update
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定权重更新的大小
- en: 'Before implementing back-propagation, let''s understand one additional detail
    of neural networks: the learning rate.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现反向传播之前，让我们理解神经网络的一个额外细节：学习率。
- en: Intuitively, the learning rate helps us to build trust in the algorithm. For
    example, when deciding on the magnitude of the weight update, we would potentially
    not change it by a huge amount in one go, but take a more careful approach in
    updating the weights more slowly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，学习率帮助我们建立对算法的信任。例如，在决定权重更新的幅度时，我们可能不会一次性改变很大，而是采取更谨慎的方式，慢慢地更新权重。
- en: This results in obtaining stability in our model; we will look at how the learning
    rate helps with stability in the next chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们的模型得到稳定；我们将在下一章中讨论学习率如何帮助稳定性。
- en: The whole process by which we update weights to reduce error is called a gradient-descent
    technique.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更新权重以减少误差的整个过程叫做梯度下降技术。
- en: '**Stochastic gradient descent** is the means by which error is minimized in
    the preceding scenario. More intuitively, **gradient** stands for difference (which
    is the difference between actual and predicted) and **descent** means reduce.
    **Stochastic** stands for the selection of number of random samples based on which
    a decision is taken.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降**是前述场景中最小化误差的方法。更直观地说，**梯度**代表差异（即实际值与预测值之间的差异），而**下降**意味着减少。**随机**表示基于一定数量的随机样本选择来做出决策。'
- en: Apart from stochastic gradient descent, there are many other optimization techniques
    that help to optimize for the loss values; the different optimization techniques
    will be discussed in the next chapter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 除了随机梯度下降，还有许多其他优化技术可以帮助优化损失值；不同的优化技术将在下一章中讨论。
- en: 'Back-propagation works as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的工作流程如下：
- en: Calculates the overall cost function from the feedforward process.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算正向传播过程中的整体成本函数。
- en: Varies all the weights (one at a time) by a small amount.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有权重（逐个）按小幅度变化。
- en: Calculates the impact of the variation of weight on the cost function.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算权重变化对成本函数的影响。
- en: Depending on whether the change has an increased or decreased the cost (loss)
    value, it updates the weight value in the direction of loss decrease. And then
    repeats this step across all the weights we have.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据变化是否增加或减少了成本（损失）值，它会更新权重值，朝着减少损失的方向进行调整。然后，这一步骤会在所有权重上重复进行。
- en: If the preceding steps are performed *n* number of times, it essentially results
    in *n* **epochs**.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前述步骤执行了*n*次，它实际上会产生*n* **迭代次数**。
- en: 'In order to further cement our understanding of back-propagation in neural
    networks, let''s start with a known function and see how the weights could be
    derived:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步巩固我们对神经网络中反向传播的理解，让我们从已知函数开始，看看如何推导出权重：
- en: 'For now, we will have the known function as *y = 2x*, where we try to come
    up with the weight value and bias value, which are 2 and 0 in this specific case:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们将使用已知函数*y = 2x*，我们尝试找出权重值和偏置值，在这个特定的情况下分别为2和0：
- en: '| **x** | **y** |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **x** | **y** |'
- en: '| 1 | 2 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 |'
- en: '| 2 | 4 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 4 |'
- en: '| 3 | 6 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 |'
- en: '| 4 | 8 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 8 |'
- en: If we formulate the preceding dataset as a linear regression, *(y = a*x+b)*,
    where we are trying to calculate the values of *a* and *b* (which we already know
    are 2 and 0, but are checking how those values are obtained using gradient descent),
    let's randomly initialize the *a* and *b* parameters to values of 1.477 and 0
    (the ideal values of which are 2 and 0).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将之前的数据集公式化为线性回归，*(y = a*x+b)*，其中我们要计算*a*和*b*的值（我们已经知道它们是2和0，但正在检查如何通过梯度下降获得这些值），我们可以将*a*和*b*的参数随机初始化为1.477和0（理想值是2和0）。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: In this section, we will build the back-propagation algorithm by hand so that
    we clearly understand how weights are calculated in a neural network. In this
    specific case, we will build a simple neural network where there is no hidden
    layer (thus we are solving a regression equation). The code file is available
    as `Neural_network_working_details.ipynb` in GitHub.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将手动构建反向传播算法，以便我们清楚地理解在神经网络中如何计算权重。在这个特定的案例中，我们将构建一个没有隐藏层的简单神经网络（因此我们正在解决一个回归方程）。代码文件可以在GitHub上的`Neural_network_working_details.ipynb`中找到。
- en: 'Initialize the dataset as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化数据集如下：
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize the weight and bias values randomly (we have only one weight and
    one bias value as we are trying to identify the optimal values of *a* and *b*
    in the *y = a*x + b* equation):'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重和偏置值（因为我们只需要一个权重和一个偏置值，因为我们要找出方程*y = a*x + b*中*a*和*b*的最优值）：
- en: '[PRE12]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the feed-forward network and calculate the squared error loss value:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义前馈网络并计算平方误差损失值：
- en: '[PRE13]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we performed a matrix multiplication of the input with
    the randomly-initialized weight value and summed it up with the randomly-initialized
    bias value.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们进行了输入与随机初始化的权重值的矩阵乘法，并将其与随机初始化的偏置值相加。
- en: Once the value is calculated, we calculate the squared error value of the difference
    between the actual and predicted values.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出值，我们将计算实际值与预测值之间的平方误差。
- en: Increase each weight and bias value by a very small amount (0.0001) and calculate
    the squared error loss value one at a time for each of the weight and bias updates.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个权重和偏置值增加一个非常小的量（0.0001），并逐一计算每个权重和偏置更新的平方误差损失值。
- en: If the squared error loss value decreases as the weight increases, the weight
    value should be increased. The magnitude by which the weight value should be increased
    is proportional to the amount of loss value the weight change decreases by.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果平方误差损失值随着权重增加而减少，则应该增加权重值。权重值增加的幅度应与权重变化所减少的损失值的数量成正比。
- en: Additionally, ensure that you do not increase the weight value as much as the
    loss decrease caused by the weight change, but weigh it down with a factor called
    the learning rate. This ensures that the loss decreases more smoothly (there's
    more on how the learning rate impacts the model accuracy in the next chapter).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保不会像权重变化所引起的损失减少那样增加权重值，而是通过一个叫做学习率的因子来调整它。这样可以确保损失值更平滑地减少（下一章会详细讲解学习率如何影响模型准确度）。
- en: 'In the following code, we are creating a function named `update_weights`, which
    performs the back-propagation process to update weights that were obtained in
    *step 3*. We are also mentioning that the function needs to be run for `epochs`
    number of times (where `epochs` is a parameter we are passing to `update_weights`
    function):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们创建了一个名为`update_weights`的函数，它执行反向传播过程，以更新在*步骤3*中获得的权重。我们还提到该函数需要运行`epochs`次（其中`epochs`是我们传递给`update_weights`函数的参数）：
- en: '[PRE14]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Pass the input through a feed-forward network to calculate the loss with the
    initial set of weights:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入通过前馈网络传递，以计算使用初始权重集的损失值：
- en: '[PRE15]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Ensure that you `deepcopy` the list of weights, as the weights will be manipulated
    in further steps, and hence `deepcopy` takes care of any issues resulting from
    the change in the child variable impacting the parent variable that it is pointing
    to:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你对权重列表进行`deepcopy`，因为在后续步骤中权重会被操作，因此`deepcopy`可以避免由于子变量的变化影响到它指向的父变量的问题：
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Loop through all the weight values, one at a time, and change them by a small
    value (0.0001):'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一次遍历所有权重值，并对其进行微小的变化（0.0001）：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Calculate the updated feed-forward loss when the weight is updated by a small
    amount. Calculate the change in loss due to the small change in input. Divide
    the change in loss by the number of input, as we want to calculate the mean squared
    error across all the input samples we have:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当权重被小幅更新时，计算更新后的前馈损失。计算由于输入的小变化所导致的损失变化。将损失变化除以输入的数量，因为我们希望计算所有输入样本的均方误差：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Updating the weight by a small value and then calculating its impact on loss
    value is equivalent to performing a derivative with respect to change in weight.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过小幅更新权重并计算其对损失值的影响，相当于对权重变化执行导数操作。
- en: 'Update the weights by the change in loss that they are causing. Update the
    weights slowly by multiplying the change in loss by a very small number (0.01),
    which is the learning rate parameter (more about the learning rate parameter in
    the next chapter):'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据权重导致的损失变化来更新权重。通过将损失变化乘以一个非常小的数字（0.01），即学习率参数（关于学习率参数的更多内容请参见下一章），以缓慢更新权重：
- en: '[PRE19]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The updated weights and bias value are returned:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回更新后的权重和偏置值：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: One of the other parameters in a neural network is the batch size considered
    in calculating the loss values.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的另一个参数是计算损失值时考虑的批处理大小。
- en: In the preceding scenario, we considered all the data points in order to calculate
    the loss value. However, in practice, when we have thousands (or in some cases,
    millions) of data points, the incremental contribution of a greater number of
    data points while calculating loss value would follow the law of diminishing returns
    and hence we would be using a batch size that is much smaller compared to the
    total number of data points we have.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的场景中，我们考虑了所有数据点来计算损失值。然而，在实际应用中，当我们有成千上万（或在某些情况下，数百万）个数据点时，在计算损失值时，更多数据点的增量贡献将遵循递减回报法则，因此我们使用的批处理大小会比总数据点数量小得多。
- en: The typical batch size considered in building a model is anywhere between 32
    and 1,024.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，通常考虑的批处理大小范围是32到1,024之间。
- en: There's more...
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: In the previous section, we built a regression formula *(Y = a*x + b)* where
    we wrote a function to identify the optimal values of *a* and *b*. In this section,
    we will build a simple neural network with a hidden layer that connects the input
    to the output on the same toy dataset that we worked on in the previous section.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们构建了一个回归公式*(Y = a*x + b)*，并编写了一个函数来识别*a*和*b*的最优值。在本节中，我们将在相同的玩具数据集上构建一个简单的神经网络，隐藏层将输入连接到输出层。
- en: 'We define the model as follows (the code file is available as `Neural_networks_multiple_layers.ipynb`
    in GitHub):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义模型如下（代码文件可以在GitHub上找到，文件名为`Neural_networks_multiple_layers.ipynb`）：
- en: The input is connected to a hidden layer that has three units
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层连接到隐藏层，隐藏层有三个单元。
- en: The hidden layer is connected to the output, which has one unit in output layer
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层连接到输出层，输出层有一个单元。
- en: 'Let us go ahead and code up the strategy discussed above, as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续编写上述讨论的策略，代码如下：
- en: 'Define the dataset and import the relevant packages:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据集并导入相关包：
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We use `deepcopy` so that the value of the original variable does not change
    when the variable to which the original variable's values are copied has its values
    changed.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`deepcopy`，这样在复制原始变量值到目标变量后，即使目标变量的值发生变化，原始变量的值也不会改变。
- en: Initialize the weight and bias values randomly. The hidden layer has three units
    in it. Hence, there are a total of three weight values and three bias values –
    one corresponding to each of the hidden units.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重和偏置值。隐藏层中有三个单元，因此总共有三个权重值和三个偏置值——每个都对应一个隐藏单元。
- en: Additionally, the final layer has one unit that is connected to the three units
    of the hidden layer. Hence, a total of three weights and one bias dictate the
    value of the output layer.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最终层有一个单元，与隐藏层的三个单元相连接。因此，共有三个权重和一个偏置值决定输出层的值。
- en: 'The randomly-initialized weights are as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 随机初始化的权重如下：
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Implement the feed-forward network where the hidden layer has a ReLU activation
    in it:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个前馈网络，其中隐藏层使用ReLU激活函数：
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Define the back-propagation function similarly to what we did in the previous
    section. The only difference is that we now have to update the weights in more
    layers.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像前一节那样定义反向传播函数。唯一的区别是，我们现在需要更新更多层中的权重。
- en: 'In the following code, we are calculating the original loss at the start of
    an epoch:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们正在计算一个时期开始时的原始损失：
- en: '[PRE24]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the following code, we are copying weights into two sets of weight variables
    so that they can be reused in a later code:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们将权重复制到两个权重变量集，以便在后续代码中重用它们：
- en: '[PRE25]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the following code, we are updating each weight value by a small amount and
    then calculating the loss value corresponding to the updated weight value (while
    every other weight is kept unchanged). Additionally, we are ensuring that the
    weight update happens across all weights and also across all layers in a network.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们通过少量更新每个权重值，并计算与更新后权重值对应的损失值（同时保持其他所有权重不变）。此外，我们还确保在所有权重和所有网络层中都发生权重更新。
- en: 'The change in the squared loss (`del_loss`) is attributed to the change in
    the weight value. We repeat the preceding step for all the weights that exist
    in the network:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 平方损失（`del_loss`）的变化归因于权重值的变化。我们对网络中所有存在的权重重复前述步骤：
- en: '[PRE26]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The weight value is updated by weighing down by the learning rate parameter –
    a greater decrease in loss will update weights by a lot, while a lower decrease
    in loss will update the weight by a small amount:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 权重值通过学习率参数进行加权更新——损失减少较大时，权重会大幅更新，而损失减少较小时，权重会少量更新：
- en: '[PRE27]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Given that the weight values are updated one at a time in order to estimate
    their impact on the loss value, there is a potential to parallelize the process
    of weight updates. Hence, GPUs come in handy in such scenarios as they have more
    cores than a CPU and thus more weights can be updated using a GPU in a given amount
    of time compared to a CPU.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于权重值是逐个更新的，以估计它们对损失值的影响，因此存在对权重更新过程进行并行化的潜力。因此，在这种情况下，GPU非常有用，因为它们比CPU有更多的核心，能够在相同时间内更新更多的权重。
- en: 'Finally, we return the updated weights:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们返回更新后的权重：
- en: '[PRE28]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Run the function an epoch number of times to update the weights an epoch number
    of times:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行函数多次，每次更新一次权重：
- en: '[PRE29]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output (updated weights) of preceding code is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出（更新后的权重）如下：
- en: '![](img/64c4ca0f-bfe2-4130-91a2-777979b04a6c.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64c4ca0f-bfe2-4130-91a2-777979b04a6c.jpg)'
- en: In the preceding steps, we learned how to build a neural network from scratch
    in Python. In the next section, we will learn about building a neural network
    in Keras.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述步骤中，我们学习了如何在Python中从零开始构建神经网络。在下一部分中，我们将学习如何在Keras中构建神经网络。
- en: Building a neural network in Keras
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中构建神经网络
- en: In the previous section, we built a neural network from scratch, that is, we
    wrote functions that perform forward-propagation and back-propagation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们从零开始构建了一个神经网络，也就是说，我们编写了执行前向传播和反向传播的函数。
- en: How to do it...
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: We will be building a neural network using the Keras library, which provides
    utilities that make the process of building a complex neural network much easier.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Keras库构建一个神经网络，该库提供了使构建复杂神经网络过程更容易的工具。
- en: Installing Keras
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Keras
- en: 'Tensorflow and Keras are implemented in Ubuntu, using the following commands:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow和Keras在Ubuntu中实现，使用以下命令：
- en: '[PRE30]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Note that it is preferable to install a GPU-compatible version, as neural networks
    work considerably faster when they are run on top of a GPU. Keras is a high-level
    neural network API, written in Python, and capable of running on top of TensorFlow, CNTK,
    or Theano.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最好安装一个兼容GPU的版本，因为神经网络在GPU上运行时速度要快得多。Keras是一个高层神经网络API，用Python编写，能够在TensorFlow、CNTK或Theano之上运行。
- en: 'It was developed with a focus on enabling fast experimentation, and it can
    be installed as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 它的开发重点是支持快速实验，可以通过以下方式安装：
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Building our first model in Keras
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中构建我们的第一个模型
- en: 'In this section, let''s understand the process of building a model in Keras
    by using the same toy dataset that we worked on in the previous sections (the
    code file is available as `Neural_networks_multiple_layers.ipynb` in GitHub):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，让我们通过使用在前面部分中使用的相同玩具数据集（代码文件在GitHub上以`Neural_networks_multiple_layers.ipynb`提供）来理解在Keras中构建模型的过程：
- en: 'Instantiate a model that can be called sequentially to add further layers on
    top of it. The `Sequential` method enables us to perform the model initialization
    exercise:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个模型，可以顺序调用它以便在其上添加更多的层。`Sequential`方法使我们能够执行模型初始化操作：
- en: '[PRE32]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add a dense layer to the model. A dense layer ensures the connection between
    various layers in a model. In the following code, we are connecting the input
    layer to the hidden layer:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型添加全连接层。全连接层确保模型中各层之间的连接。在以下代码中，我们将输入层与隐藏层连接：
- en: '[PRE33]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the dense layer initialized with the preceding code, we ensured that we provide
    the input shape to the model (we need to specify the shape of data that the model
    has to expect as this is the first dense layer).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码初始化的全连接层中，我们确保为模型提供了输入形状（我们需要指定模型预期的数据形状，因为这是第一个全连接层）。
- en: Additionally, we mentioned that there will be three connections made to each
    input (three units in the hidden layer) and also that the activation that needs
    to be performed in the hidden layer is the ReLu activation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们提到，每个输入将与三个单元（隐藏层中的三个单元）连接，并且在隐藏层中需要执行的激活函数是ReLU激活函数。
- en: 'Connect the hidden layer to the output layer:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将隐藏层与输出层连接：
- en: '[PRE34]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that in this dense layer, we don't need to specify the input shape, as
    the model would already infer the input shape from the previous layer.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个全连接层中，我们不需要指定输入形状，因为模型会根据前一层自动推断输入形状。
- en: Also, given that each output is one-dimensional, our output layer has one unit
    and the activation that we are performing is the linear activation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，考虑到每个输出是一维的，我们的输出层只有一个单元，且我们执行的激活函数是线性激活函数。
- en: 'The model summary can now be visualized as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型的摘要可以如下可视化：
- en: '[PRE35]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'A summary of model is as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/dc9ae0ff-7a5e-4f4a-9234-fbd8c877225f.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc9ae0ff-7a5e-4f4a-9234-fbd8c877225f.png)'
- en: 'The preceding output confirms our discussion in the previous section: that
    there will be a total of six parameters in the connection from the input layer
    to the hidden layer—three weights and three bias terms—we have a total of six
    parameters corresponding to the three hidden units. In addition, three weights
    and one bias term connect the hidden layer to the output layer.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出确认了我们在上一节中的讨论：从输入层到隐藏层的连接将有六个参数——三个权重和三个偏置项——对应三个隐藏单元的总共六个参数。此外，三个权重和一个偏置项将输入层与输出层连接。
- en: 'Compile the model. This ensures that we define the loss function and the optimizer
    to reduce the loss function and the learning rate corresponding to the optimizer
    (we will look at different optimizers and loss functions in next chapter):'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型。这确保我们定义了损失函数和优化器，用来减少损失函数，以及与优化器对应的学习率（我们将在下一章讨论不同的优化器和损失函数）：
- en: '[PRE36]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In the preceding step, we specified that the optimizer is the stochastic gradient
    descent that we learned about in the previous section and the learning rate is
    0.01. Pass the predefined optimizer and its corresponding learning rate as a parameter
    and reduce the mean squared error value:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们指定了优化器为我们在上一节学习过的随机梯度下降法，学习率为0.01。将预定义的优化器及其对应的学习率作为参数传递，并减少均方误差值：
- en: '[PRE37]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Fit the model. Update the weights so that the model is a better fit:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型。更新权重，以便模型更好地拟合：
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The `fit` method expects that it receives two NumPy arrays: an input array
    and the corresponding output array. Note that `epochs` represents the number of
    times the total dataset is traversed through, and `batch_size` represents the
    number of data points that need to be considered in an iteration of updating the
    weights. Furthermore, `verbose` specifies that the output is more detailed, with
    information about losses in training and test datasets as well as the progress
    of the model training process.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法期望接收两个NumPy数组：一个输入数组和一个对应的输出数组。请注意，`epochs`表示数据集遍历的次数，`batch_size`表示在更新权重的迭代中需要考虑的数据点数量。此外，`verbose`指定输出的详细程度，包括训练和测试数据集中的损失信息以及模型训练过程的进展。'
- en: 'Extract the weight values. The order in which the weight values are presented
    is obtained by calling the weights method on top of the model, as follows:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取权重值。权重值的顺序是通过调用模型上权重方法获得的，如下所示：
- en: '[PRE39]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The order in which weights are obtained is as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 获取权重的顺序如下：
- en: '![](img/faaaee67-07e8-49fa-bd86-10ec8aafd9f8.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/faaaee67-07e8-49fa-bd86-10ec8aafd9f8.png)'
- en: From the preceding output, we see that the order of weights is the three weights
    (`kernel`) and three bias terms in the `dense_1` layer (which is the connection
    between the input to the hidden layer) and the three weights (`kernel`) and one
    bias term connecting the hidden layer to the `dense_2` layer (the output layer).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们看到权重的顺序是`dense_1`层的三个权重（`kernel`）和三个偏置项（这是输入层与隐藏层之间的连接），以及`dense_2`层（输出层）之间的三个权重（`kernel`）和一个偏置项。
- en: 'Now that we understand the order in which weight values are presented, let''s
    extract the values of these weights:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了权重值呈现的顺序，让我们提取这些权重的值：
- en: '[PRE40]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Notice that the weights are presented as a list of arrays, where each array
    corresponds to the value that is specified in the `model.weights` output.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，权重以数组列表的形式呈现，其中每个数组对应于`model.weights`输出中指定的值。
- en: 'The output of above lines of code is as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/7a280532-11b9-456d-b37c-311c31731327.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a280532-11b9-456d-b37c-311c31731327.jpg)'
- en: You should notice that the output we are observing here matches with the output
    we obtaining while hand-building the neural network
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，我们在这里观察到的输出与我们在手动构建神经网络时获得的输出一致。
- en: 'Predict the output for a new set of input using the `predict` method:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`predict`方法预测一组新输入的输出：
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that `x1` is the variable that holds the values for the new set of examples
    for which we need to predict the value of the output. Similarly to the `fit` method,
    the `predict` method also expects an array as its input.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`x1`是一个变量，它保存新一组示例的值，针对这些示例，我们需要预测输出值。与`fit`方法类似，`predict`方法也期望接收一个数组作为输入。
- en: 'The output of preceding code is as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/3c287bfb-ee6c-4bf3-a301-f201cb71cbb2.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c287bfb-ee6c-4bf3-a301-f201cb71cbb2.jpg)'
- en: 'Notice that, while the preceding output is incorrect, the output when we run
    for 100 epochs is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管前面的输出不正确，但我们运行100个epoch后的输出如下：
- en: '![](img/39ac7491-ac09-432f-af83-1e6a4d3dec01.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39ac7491-ac09-432f-af83-1e6a4d3dec01.jpg)'
- en: The preceding output will match the expected output (which are 10, 12) as we
    run for even higher number of epochs.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们运行了更多的epoch，前面的输出将与预期输出（即10，12）一致。
