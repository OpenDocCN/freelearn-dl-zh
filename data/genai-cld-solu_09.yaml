- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: 'Responsible Development of AI Solutions: Building with Integrity and Care'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任的AI解决方案开发：以诚信和关怀为基础
- en: In the realm of modern technology, **artificial intelligence** (**AI**) has
    emerged as a transformative force, reshaping industries, improving efficiency,
    and enhancing user experiences. As cloud and AI architects, we stand at the forefront
    of this AI revolution, wielding the power to shape the future of AI-driven solutions.
    However, with great power comes great responsibility. The integration of responsible
    AI practices into the design and deployment of AI solutions is not merely a moral
    or ethical imperative; it is a strategic imperative that directly impacts the
    success, reputation, and sustainability of organizations in the AI landscape.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代技术的领域，**人工智能**（AI）已成为一种变革力量，重塑产业、提高效率和提升用户体验。作为云和AI架构师，我们站在AI革命的尖端，掌握着塑造AI驱动解决方案未来的力量。然而，权力越大，责任越大。将负责任的AI实践融入AI解决方案的设计和部署，不仅是一个道德或伦理的要求，而且是一个战略要求，它直接影响着组织在AI领域的成功、声誉和可持续性。
- en: Neglecting responsible AI (RAI) principles can have a profound impact on human
    lives. A thought-provoking article from MIT titled *AI is sending people to jail—and
    getting it wrong*, explores the application of AI and algorithms in the criminal
    justice system. It highlights how facial recognition systems and predictive algorithms
    used by police and judges can exhibit bias due to their training data, leading
    to incorrect decisions that affect human lives. Researchers have consistently
    shown that facial recognition systems are particularly prone to failure in identifying
    individuals with dark skin. Prediction models used in the justice system can be
    skewed towards a certain group of people, leading to incorrect judgments. Instances
    such as these (among others that we will explore in this book) underscore the
    urgent need for AI solutions developed with integrity and care.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 忽视负责任的AI（RAI）原则可能对人类生活产生深远影响。一篇来自麻省理工学院的引人深思的文章《AI将人们送进监狱——并且犯错了》，探讨了AI和算法在刑事司法系统中的应用。它强调了警察和法官使用的面部识别系统和预测算法由于训练数据存在偏见，可能导致影响人类生活的错误决策。研究人员一直表明，面部识别系统在识别深色皮肤的个人方面尤其容易出错。在司法系统中使用的预测模型可能偏向于某一群体的人，导致错误的判断。此类事件（以及其他我们将在这本书中探讨的事件）强调了以诚信和关怀开发AI解决方案的紧迫需求。
- en: In this chapter, we delve into the essentials of **responsible artificial intelligence**
    (**RAI**), starting with the key principles of AI design and addressing the unique
    challenges presented by **large language models** (**LLMs**). As we explore the
    rising concerns around Deepfakes, which are hyper-realistic digital manipulations
    often used to create fake videos or images, the importance of robust AI architecture
    and proactive leadership becomes evident, highlighting the need for ethical and
    responsible AI development. The chapter also examines the relationship between
    AI, cloud computing, and legal frameworks, emphasizing the significance of legal
    compliance and ethical considerations. Additionally, we provide insights into
    the most popular RAI tools, offering practical guidance for their application.
    By the end of this chapter, you will have a comprehensive understanding of the
    principles guiding RAI, strategies to combat LLM challenges, an awareness of the
    impact of Deepfakes, knowledge of AI’s role in cloud computing and legal contexts,
    and familiarity with essential RAI tools, empowering you to navigate and contribute
    to the field of AI responsibly and ethically.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨负责任的人工智能（RAI）的基本要素，从AI设计的核心原则开始，并解决由大型语言模型（LLMs）带来的独特挑战。随着我们对深度伪造日益增长的担忧，这些深度伪造是超逼真的数字操纵，常用于创建虚假视频或图像，我们明显地认识到强大的AI架构和积极领导的重要性，突显了进行道德和负责任AI开发的需求。本章还考察了AI、云计算和法律框架之间的关系，强调了法律合规性和道德考量的重要性。此外，我们提供了对最流行的RAI工具的见解，提供了它们应用的实用指导。到本章结束时，您将全面了解指导RAI的原则、应对LLM挑战的策略、对深度伪造影响的意识、了解AI在云计算和法律环境中的作用，以及熟悉基本RAI工具，这将使您能够负责任和道德地参与AI领域。
- en: 'We will cover the following main topics in the chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要内容：
- en: Understanding responsible AI design
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解负责任的AI设计
- en: Key principles of RAI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAI的关键原则
- en: Addressing LLM challenges with RAI principles
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAI原则解决LLM挑战
- en: Rising Deepfake concern
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度伪造的担忧日益增加
- en: Building applications using a responsible AI-first approach
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用负责任的AI优先方法构建应用程序
- en: AI, the cloud, and the law – Understanding compliance and regulations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI、云和法规——理解合规性和法规
- en: Startup ecosystem in RAI
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAI领域的创业生态系统
- en: Understanding responsible AI design
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解负责任的AI设计
- en: In this section, we will explore the true meaning of responsible AI and delve
    into the fundamental design principles that should be considered while architecting
    generative AI solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨负责任的AI的真正含义，并深入探讨在架构生成式AI解决方案时应考虑的基本设计原则。
- en: What is responsible AI?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是负责任的AI？
- en: 'As stated by Microsoft public documentation, “*Responsible Artificial Intelligence*
    *(Responsible AI) is an approach to developing, assessing, and deploying AI systems
    in a safe, trustworthy, and ethical way.*” It is like building and using smart
    computer programs (AI systems) in a way that is safe, fair, and ethical. Think
    of AI systems as tools created by people who make a lot of choices about how these
    tools should work. Responsible AI is about making these choices carefully to make
    sure AI acts in a way that is good and fair for everyone. It’s like guiding AI
    to always consider what is best for people and their needs. This includes making
    sure AI is reliable, fair, and transparent about how it works. Here are a few
    examples of the types of tools being developed in this space:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如微软的公共文档所述，“*负责任的AI* *(负责任的AI)是一种以安全、可信和道德的方式开发、评估和部署AI系统的方法。”这就像以安全、公平和道德的方式构建和使用智能计算机程序（AI系统）。将AI系统视为由人们创建的工具，这些人会做出很多关于这些工具应该如何工作的选择。负责任的AI是关于谨慎做出这些选择，以确保AI以对所有人都有益和公平的方式行事。这就像引导AI始终考虑对人们及其需求最有利的事情。这包括确保AI是可靠的、公平的，并且对其工作方式是透明的。以下是一些正在这个领域开发中的工具类型：
- en: '**Fair hiring tools**: An AI tool used by a company to help choose job candidates.
    Responsible AI would ensure this AI doesn’t favor one group of people over another,
    making the hiring process fair for all applicants. For example, **BeApplied**,
    a startup in the RAI space, has developed a piece of ethical recruitment software
    designed to enhance hiring quality and increase diversity by reducing bias. It
    stands apart from traditional applicant tracking systems by incorporating fairness,
    inclusivity, and diversity as its core principles. The platform, underpinned by
    behavioral science, offers anonymized applications and predictive, skill-based
    assessments to ensure unbiased hiring. Its features include sourcing analysis
    tools to diversify talent pools, inclusive job description creation, anonymized
    skills testing for objective assessments, and data-driven shortlisting to focus
    purely on skills. BeApplied aims to create a fairer recruitment world, one hire
    at a time. They currently have some notable customers, such as UNICEF and England
    and Wales Cricket.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公平招聘工具**：公司用来帮助选择求职候选人的AI工具。负责任的AI将确保这个AI不会偏向某一群体，从而使得招聘过程对所有求职者都是公平的。例如，**BeApplied**，一家位于RAI领域的初创公司，开发了一款旨在通过减少偏见来提高招聘质量和增加多样性的道德招聘软件。它与传统的人才跟踪系统不同，将公平、包容和多样性作为其核心原则。该平台基于行为科学，提供匿名申请和基于技能的预测性评估，以确保招聘过程的公正性。其功能包括多元化人才池的来源分析工具、包容性职位描述创建、匿名技能测试以进行客观评估，以及基于数据的筛选以专注于技能。BeApplied旨在通过一次招聘创造一个更公平的招聘世界。他们目前有一些值得注意的客户，例如联合国儿童基金会和英格兰及威尔士板球。'
- en: '**Transparent recommendation systems**: Think of a streaming service that suggests
    movies. Responsible AI would make this system clear about why it recommends certain
    movies, ensuring it’s not just promoting certain movies for unfair reasons. For
    example, **LinkedIn** is a notable example of a company that focuses on transparent
    and explainable AI systems, especially in its recommendation systems. Their approach
    ensures that AI system behavior and any related components are understandable,
    explainable, and interpretable. They prioritize transparency in AI to make their
    systems trustworthy and to avoid harmful bias while respecting privacy. For instance,
    they developed **CrystalCandle**, a customer-facing model explainer that creates
    digestible interpretations and insights reflecting the rationale behind model
    predictions. This tool is integrated with business predictive models, aiding sales
    and marketing by converting complex machine learning outputs into clear, actionable
    narratives for users.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明的推荐系统**: 想象一下一家推荐电影的流媒体服务。负责任的人工智能会使这个系统清楚地说明为什么推荐某些电影，确保它不是出于不公平的原因来推广某些电影。例如，**LinkedIn**是一家专注于透明和可解释人工智能系统的公司，尤其是在其推荐系统中。他们的方法确保人工智能系统的行为和任何相关组件都是可理解的、可解释的和可解释的。他们优先考虑人工智能的透明度，以使他们的系统值得信赖，避免有害的偏见，同时尊重隐私。例如，他们开发了**CrystalCandle**，这是一个面向客户的模型解释器，它创建易于理解的解释和见解，反映了模型预测背后的逻辑。该工具与业务预测模型集成，通过将复杂的机器学习输出转换为用户清晰、可操作的故事，帮助销售和营销。'
- en: '**Healthcare**: In the healthcare industry, there’s a growing focus on developing
    ethical AI tools to ensure fairness, transparency, and accountability within AI-driven
    decisions. These tools are designed to minimize biases, safeguard patient data
    privacy, and enhance the explainability and reliability of AI algorithms. Ethical
    AI is pivotal in healthcare as it aids in delivering personalized care, improving
    patient outcomes, and maintaining high ethical standards. Embedding ethical considerations
    into AI systems helps prevent potential negative impacts, address health inequalities,
    and build trust with patients and the community, thereby positively influencing
    public health and well-being. One prominent example of such an ethical AI tool
    in healthcare is **Merative** (formerly IBM Watson Health). It supports healthcare
    professionals by offering evidence-based, personalized treatment recommendations
    with a focus on transparency and explainability. The platform also prioritizes
    patient data protection in compliance with healthcare regulations such as HIPAA
    and aims to reduce bias by employing diverse datasets for training its AI models.
    This approach by IBM Watson Health demonstrates the potential of AI to improve
    healthcare decision-making processes while emphasizing patient safety, data privacy,
    and equity across diverse patient populations.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健**: 在医疗保健行业，越来越重视开发符合伦理的人工智能工具，以确保人工智能驱动决策中的公平性、透明度和问责制。这些工具旨在最小化偏见，保护患者数据隐私，并提高人工智能算法的可解释性和可靠性。在医疗保健领域，符合伦理的人工智能至关重要，因为它有助于提供个性化护理，改善患者结果，并维持高伦理标准。将伦理考量嵌入人工智能系统有助于预防潜在的负面影响，解决健康不平等问题，并与患者和社区建立信任，从而积极影响公共健康和福祉。医疗保健领域这样一个符合伦理的人工智能工具的突出例子是**Merative**（前身为IBM
    Watson Health）。它通过提供基于证据的、个性化的治疗建议，并注重透明度和可解释性，支持医疗保健专业人员。该平台还优先考虑符合医疗保健法规（如HIPAA）的患者数据保护，并通过使用多样化的数据集来训练其人工智能模型，旨在减少偏见。IBM
    Watson Health的这种做法展示了人工智能改善医疗保健决策流程的潜力，同时强调患者安全、数据隐私和不同患者群体之间的公平性。'
- en: '**Finance**: In the finance industry, ethical AI tools are being developed
    to navigate complex ethical considerations such as data privacy and algorithmic
    bias and ensure transparency and accountability in AI-driven processes. In the
    finance industry, ethical AI tools such as **Zest AI** are revolutionizing how
    financial institutions approach lending by enhancing fairness and transparency
    in credit decisions. Zest AI leverages machine learning to improve credit scoring
    accuracy and reduce biases, thus promoting financial inclusivity. Its focus on
    explainability ensures that lenders can comprehend and justify AI-driven decisions,
    aligning with regulatory compliance and bolstering borrower trust. This example
    underscores the finance sector’s commitment to integrating responsible AI practices
    that benefit both institutions and customers, adhering to ethical standards.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融业**：在金融行业，正在开发符合伦理的AI工具，以应对复杂伦理考量，如数据隐私和算法偏见，并确保AI驱动过程中的透明度和问责制。在金融行业，如**Zest
    AI**这样的符合伦理的AI工具正在通过增强信贷决策中的公平性和透明度，革新金融机构的贷款方式。Zest AI利用机器学习来提高信用评分的准确性并减少偏见，从而促进金融包容性。它对可解释性的关注确保了贷款人能够理解和证明AI驱动的决策，符合监管合规并增强借款人的信任。这个例子强调了金融行业致力于整合负责任的AI实践，使机构和客户都受益，并遵守伦理标准。'
- en: '**Criminal justice**: In the criminal justice system, the development of ethical
    AI tools is a growing focus aimed at enhancing fairness, reducing bias, and improving
    the accuracy of legal outcomes. These tools are designed to support decision-making
    processes in areas such as predictive policing, risk assessment for bail and sentencing,
    and evidence analysis. One example of an ethical AI tool in criminal justice is
    **Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)**.
    COMPAS is a risk assessment tool used by courts to evaluate the likelihood of
    a defendant reoffending. COMPAS considers elements such as past arrests, age,
    and employment status to generate risk scores for reoffending, which judges then
    use to decide on sentencing short-term jail or long-term prison. It was found
    that Black defendants are mistakenly classified as “high-risk” for future crimes
    at twice the rate of white defendants. These claims were refuted by the company,
    which stated that the algorithms worked as designed ([https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)).
    However, continuous improvements have been made since then. While its implementation
    has sparked debate over potential biases, it highlights the sector’s attempt to
    apply AI in making informed, data-driven decisions regarding bail, sentencing,
    and parole. In response to ethical concerns, efforts are being made to improve
    such tools by incorporating fairness algorithms, enhancing transparency, and conducting
    regular audits to identify and mitigate biases. These advancements reflect the
    broader commitment to developing AI in criminal justice that upholds ethical standards
    and contributes to a more equitable legal system.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**刑事司法**：在刑事司法系统中，开发符合伦理的AI工具正日益成为关注焦点，旨在提高公平性、减少偏见并改善法律结果的准确性。这些工具旨在支持决策过程，如预测警务、保释和判决的风险评估以及证据分析。刑事司法中符合伦理的AI工具的一个例子是**矫正犯人管理配置文件用于替代制裁（COMPAS）**。COMPAS是一个风险评估工具，法院用它来评估被告再犯的可能性。COMPAS考虑了诸如过去的逮捕、年龄和就业状况等因素，为再犯生成风险评分，然后法官使用这些评分来决定判决，短期监禁或长期监禁。研究发现，黑人被告被错误地归类为“高风险”未来犯罪的比率是白人被告的两倍。公司对这些说法进行了反驳，声称算法按设计工作（[https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)）。然而，自那时以来，已经进行了持续的改进。尽管其实施引发了关于潜在偏见的辩论，但它突出了该行业在保释、判决和假释方面应用AI，以做出基于信息、数据驱动的决策的尝试。为了应对伦理担忧，正在努力通过整合公平性算法、提高透明度和定期审计来识别和减轻偏见，以改进此类工具。这些进步反映了更广泛地致力于在刑事司法中发展AI，以维护伦理标准并促进更公平的法律体系。'
- en: Key principles of RAI
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAI的关键原则
- en: '![Figure 9.1 – Responsible AI principles](img/B21443_09_1.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 负责任AI原则](img/B21443_09_1.jpg)'
- en: Figure 9.1 – Responsible AI principles
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 负责任AI原则
- en: 'Microsoft has established a **Responsible AI Standard**, presenting a comprehensive
    framework that guides the development of AI systems. This framework is grounded
    in six key principles: **fairness**, **reliability and safety**, **privacy and
    security**, **inclusiveness**, **transparency**, and **accountability**, as depicted
    in the preceding above. They follow two guiding principles: **ethical and explainable**.
    These principles form the bedrock of Microsoft’s commitment to a responsible and
    trustworthy approach to AI. This approach is increasingly vital as AI becomes
    more integrated into the products and services we use daily. In my opinion, this
    framework from Microsoft is exceptionally well-rounded for the design of generative
    AI solutions and should always be a primary consideration when architecting such
    solutions. A good mnemonic to remember these principles by is “**F**riendly **R**obots
    **S**afeguard **P**rivacy, **I**nspire **T**rust, **A**ssure **S**afety,” or **FAST-P**a**IRS**.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 微软已经建立了一个**负责任的AI标准**，提供了一个指导人工智能系统开发的全面框架。这个框架基于六个关键原则：**公平性**、**可靠性和安全性**、**隐私和安全**、**包容性**、**透明度**和**问责制**，如前所述。他们遵循两个指导原则：**道德和可解释性**。这些原则构成了微软对负责任和值得信赖的AI方法的承诺的基础。随着AI越来越多地融入我们日常使用的产品和服务中，这种方法变得越来越重要。在我看来，微软的这个框架对于生成式AI解决方案的设计非常全面，因此在设计此类解决方案时始终应是一个首要考虑因素。一个记住这些原则的好方法是“**F**riendly
    **R**obots **S**afeguard **P**rivacy, **I**nspire **T**rust, **A**ssure **S**afety”，或者称为**FAST-P**a**IRS**。
- en: Let’s dive deep into each of these principles with the help of examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们借助实例深入探讨这些原则的每一个。
- en: Ethical and explainable
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 道德和可解释性
- en: 'From an ethical standpoint, AI ought to do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从道德角度来看，AI应该做到以下几点：
- en: Ensure fairness and inclusiveness in its statements and tasks
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保其声明和任务中的公平性和包容性
- en: Hold responsibility/accountability for its choices
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对其选择承担责任/问责
- en: Avoid discrimination against various races, disabilities, or backgrounds
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免对各种种族、残疾或背景进行歧视
- en: Explainability in AI provides clarity on decision-making processes for data
    scientists, auditors, and business leaders, enabling them to understand and justify
    the system’s conclusions. It also ensures adherence to corporate policies, industry
    norms, and regulatory requirements.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的可解释性为数据科学家、审计人员和商业领导者提供了决策过程的清晰度，使他们能够理解和证明系统的结论。它还确保遵守企业政策、行业标准法规要求。
- en: Fairness and inclusiveness
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平性和包容性
- en: This principle ensures that AI systems do not discriminate, are not biased against
    certain groups or individuals, and provide equal opportunities for all.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则确保人工智能系统不会歧视，不对某些群体或个人有偏见，并为所有人提供平等的机会。
- en: For example, designing AI systems with features that accommodate users with
    disabilities, such as voice-activated assistants that can understand and respond
    to users with speech impairments or AI-driven web interfaces that are navigable
    by people with visual impairments.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，设计具有适应残疾人士功能的人工智能系统，例如能够理解和响应用户语音障碍的语音激活助手，或者由人工智能驱动的可由视障人士导航的网页界面。
- en: This article from *The New York Times*, titled *Thousands of Dollars for Something
    I Didn’t Do* discusses the case of an African American individual who was wrongfully
    charged and fined due to an erroneous facial recognition match. This incident
    highlights the limitations of AI-based facial recognition systems in accurately
    identifying individuals with darker skin tones. Such incidents necessitate the
    need for fairness and inclusiveness principles in AI systems.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇来自《纽约时报》的文章，标题为《数千美元的代价，却是因为我没有做的事情》讨论了一个非洲裔美国人因错误的面部识别匹配而被错误地指控和罚款的案件。这一事件突出了基于AI的面部识别系统在准确识别深色皮肤色调的人方面的局限性。此类事件需要人工智能系统中公平性和包容性原则的需求。
- en: Reliability and safety
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性和安全性
- en: This focuses on the AI system being dependable and not posing any harm to users.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点关注的是人工智能系统必须可靠，并且不会对用户造成任何伤害。
- en: For example, an AI system used in a self-driving car must be reliable and safe.
    It should consistently make correct driving decisions, such as stopping at red
    lights and avoiding obstacles, to ensure the safety of passengers and pedestrians.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，用于自动驾驶汽车的人工智能系统必须可靠和安全。它应该持续做出正确的驾驶决策，例如在红灯处停车和避开障碍物，以确保乘客和行人的安全。
- en: Transparency
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 透明度
- en: This principle demands clarity on how AI systems make decisions or reach conclusions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则要求明确人工智能系统如何做出决策或得出结论。
- en: For example, a credit scoring AI system should be transparent about the factors
    it uses to determine someone’s credit score. This means a user should be able
    to understand which financial behaviors are impacting their score, whether positively
    or negatively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个信用评分AI系统应该对其用于确定某人信用评分的因素保持透明。这意味着用户应该能够理解哪些财务行为正在影响他们的评分，无论是正面还是负面。
- en: Privacy and security
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐私和安全
- en: This ensures that the personal data used by AI systems are protected and not
    misused.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这确保了AI系统使用的个人数据得到保护，不会被滥用。
- en: For example, an AI-powered health app that tracks users’ physical activities
    and health metrics must safeguard this sensitive and personal information. The
    app should have robust security measures to prevent data breaches and should be
    clear about how it uses and shares user data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个通过跟踪用户的身体活动和健康指标来提供健康应用的AI，必须保护这些敏感和个人的信息。该应用应具备强大的安全措施以防止数据泄露，并且应清楚地说明它如何使用和共享用户数据。
- en: Accountability
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 责任制
- en: This principle is about taking responsibility for the outcomes of AI systems,
    including addressing any negative impacts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原则是关于对AI系统结果承担责任，包括解决任何负面影响。
- en: For example, if an AI-powered news recommendation system inadvertently spreads
    fake news, the creators of the system must take responsibility. They should identify
    the failure in their algorithm, rectify the issue, and take steps to prevent such
    occurrences in the future.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个AI驱动的新闻推荐系统无意中传播了假新闻，系统的创造者必须承担责任。他们应该识别算法中的失败，纠正问题，并采取措施防止未来发生此类事件。
- en: Addressing LLM challenges with RAI principles
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用RAI原则解决LLM挑战
- en: 'As discussed previously, there are three major challenges we face with LLM
    outputs: hallucinations, toxicity, and intellectual property issues. Now let’s
    double-click into each of these challenges and see how we can use RAI principles
    to address them.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们在LLM输出方面面临三个主要挑战：幻觉、毒性和知识产权问题。现在让我们深入了解每个挑战，看看我们如何使用RAI原则来应对它们。
- en: Intellectual property issues (Transparency and Accountability)
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 知识产权问题（透明度和责任制）
- en: The RAI principle that addresses **intellectual property** (**IP**) issues is
    referred to as “Transparency and Accountability.” This principle ensures that
    AI systems are transparent in their operations and that their creators and operators
    are accountable for their design and use. This includes the prevention of plagiarism
    and ensuring compliance with copyright laws.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 解决知识产权（IP）问题的RAI原则被称为“透明度和责任制”。这一原则确保AI系统在其操作中是透明的，并且其创造者和操作者对其设计和使用负责。这包括防止剽窃并确保遵守版权法。
- en: Transparency involves the clear disclosure of the data sources, algorithms,
    and training methods used, which can have implications for IP rights.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 透明度涉及明确披露所使用的数据来源、算法和训练方法，这可能对知识产权产生影响。
- en: For instance, if an AI system is trained on copyrighted materials or incorporates
    proprietary algorithms, it’s crucial to have proper permissions and to acknowledge
    these sources to avoid IP infringements. We believe new regulations will emerge
    in the upcoming years to prevent IP issues in generative AI applications.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个AI系统是在版权材料上训练的或包含了专有算法，那么拥有适当的许可并承认这些来源以避免知识产权侵权至关重要。我们相信在未来的几年里，将出现新的法规来防止生成式AI应用中的知识产权问题。
- en: Moreover, research is being carried out on ways to filter out or block responses
    that are very similar to protected content. For instance, if a user requests a
    generative AI to produce a narrative that is like a popular fantasy novel, the
    AI will analyze the request and either alter the output significantly to avoid
    direct similarities or deny the request altogether, ensuring it does not infringe
    on the novel’s intellectual property rights.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正在进行研究以过滤或阻止与受保护内容非常相似的反应。例如，如果用户请求生成式AI生成一个类似于流行幻想小说的叙事，AI将分析请求并要么显著改变输出以避免直接相似性，要么完全拒绝请求，确保不侵犯小说的知识产权。
- en: '**Machine unlearning** is a relatively recent concept in the field of machine
    learning and artificial intelligence, which involves the ability to effectively
    remove specific data from a trained model’s knowledge without retraining it from
    scratch. This process is particularly relevant in the context of privacy and data
    protection, especially under regulations such as the GDPR, which advocates for
    the “right to be forgotten.” Traditional machine learning embeds the training
    data into a model’s parameters, making selective data removal challenging. Machine
    unlearning addresses this by developing methods to diminish or reverse the influence
    of certain data points on the model, thus allowing for compliance with privacy
    laws and providing greater flexibility in data management. However, implementing
    this efficiently without compromising the model’s performance is a complex and
    ongoing area of research.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器反学习**是机器学习和人工智能领域的一个相对较新的概念，它涉及到从训练模型的知识中有效移除特定数据的能力，而无需从头开始重新训练。这个过程在隐私和数据保护方面尤其相关，尤其是在GDPR等法规下，这些法规倡导“被遗忘的权利”。传统的机器学习将训练数据嵌入到模型参数中，使得选择性数据移除变得困难。机器反学习通过开发减少或逆转某些数据点对模型影响的方法来解决这个问题，从而允许遵守隐私法律，并在数据管理方面提供更大的灵活性。然而，在不妨碍模型性能的情况下有效地实施这一过程是一个复杂且持续的研究领域。'
- en: Hallucinations (Reliability and Safety)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉（可靠性和安全性）
- en: The responsible AI principle that addresses the problem of hallucinations in
    AI models is typically “Reliability and Safety.” This principle focuses on ensuring
    that AI systems operate reliably and safely under a wide range of conditions and
    do not produce unintended, harmful, or misleading outcomes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 针对人工智能模型中幻觉问题的责任AI原则通常是“可靠性和安全性”。这一原则侧重于确保AI系统在各种条件下可靠且安全地运行，并且不会产生未预期的、有害的或误导性的结果。
- en: Hallucinations in AI refer to instances where AI models generate false or nonsensical
    information, often because of training on noisy, biased, or insufficient data.
    Ensuring reliability and safety means rigorously testing AI systems to detect
    and mitigate such issues, ensuring that they perform as expected and do not produce
    erroneous outputs, such as hallucinations, which could lead to misinformation
    or harmful decisions. We have discussed ways to mitigate hallucinations by using
    prompt engineering, RAG techniques, and fine-tuning in *Chapters* *3*, *4*, and
    *5*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的幻觉指的是AI模型生成虚假或无意义信息的情况，这通常是由于在嘈杂、有偏见或数据不足的训练数据上进行的。确保可靠性和安全性意味着对AI系统进行严格测试，以检测和减轻这些问题，确保它们按预期运行，并且不会产生错误输出，如幻觉，这可能导致错误信息或有害决策。我们已在第3章、第4章和第5章中讨论了通过提示工程、RAG技术和微调来减轻幻觉的方法。
- en: Additionally, the users must be educated on hallucination possibilities via
    generative AI applications. Additionally, the augmentation of source citations
    in LLM responses should be considered.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用户必须通过生成式AI应用了解幻觉的可能性。此外，还应考虑在LLM响应中增强源引用。
- en: Toxicity (Fairness and Inclusiveness)
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 毒性（公平性和包容性）
- en: Toxicity in AI can manifest as biased, offensive, or harmful outputs that may
    disproportionately affect certain groups based on race, gender, sexual orientation,
    or other characteristics. The responsible AI principle that specifically addresses
    toxicity in AI systems is “Fairness and Inclusiveness.” This principle ensures
    that AI systems do not perpetuate, amplify, or introduce biases and discriminatory
    practices, including the generation or reinforcement of toxic content.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的毒性可能表现为基于种族、性别、性取向或其他特征的偏见、冒犯性或有害输出，这些输出可能不成比例地影响某些群体。专门针对AI系统中毒性的责任AI原则是“公平性和包容性”。这一原则确保AI系统不会持续、放大或引入偏见和歧视性做法，包括生成或加强有毒内容。
- en: 'The following methods can be used to mitigate toxicity:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下方法来减轻毒性：
- en: '**Diverse and representative data collection**: Leverage large language models
    (LLMs) to generate a broad spectrum of training data, ensuring it encompasses
    various groups for a more inclusive representation. This approach helps minimize
    biases and mitigate toxic outputs.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样化和代表性数据收集**：利用大型语言模型（LLMs）生成广泛范围的训练数据，确保它涵盖各种群体，以实现更包容的代表性。这种方法有助于减少偏见并减轻有毒输出。'
- en: '**Global annotator workforce**: Engage a global team of human annotators from
    diverse races and backgrounds. Such human annotators provide comprehensive guidelines
    on accurately labeling training data, emphasizing the importance of inclusivity
    and unbiased judgment.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全球标注员团队**：聘请来自不同种族和背景的全球人类标注员团队。这些人类标注员提供全面指南，以确保准确标注训练数据，强调包容性和无偏见判断的重要性。'
- en: '**Proactive bias detection and remediation**: Implement systematic processes
    to actively identify and address biases in AI systems. This ongoing effort is
    crucial to prevent and reduce instances of toxic behavior.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动偏见检测和修复**：实施系统流程，主动识别和解决人工智能系统中的偏见。这一持续努力对于防止和减少有毒行为的发生至关重要。'
- en: '**Inclusive design and rigorous testing**: Involve a wide array of stakeholders
    in both the design and testing phases of AI systems. This inclusive approach is
    key to uncovering and addressing potential issues related to toxicity and bias
    early in the development process.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**包容性设计和严格测试**：在人工智能系统的设计和测试阶段，涉及广泛的利益相关者。这种包容性方法对于在开发早期阶段发现和解决与毒性及偏见相关的问题至关重要。'
- en: '**Supplemental guardrail models**: Develop and train additional models specifically
    designed to filter out inappropriate or unwanted content. These models act as
    an extra layer of defense, ensuring the overall AI system maintains high standards
    of content quality and appropriateness.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**补充的护栏模型**：开发和训练专门设计用于过滤掉不适当或不希望的内容的额外模型。这些模型作为额外的防御层，确保整体人工智能系统保持高质量和适当的内容标准。'
- en: Additionally, the principle of “Transparency and Accountability” plays a role
    in addressing toxicity. By making AI systems more transparent, stakeholders can
    better understand how and why certain outputs are generated, which aids in identifying
    and correcting toxic behaviors. Accountability ensures that those who design and
    deploy AI systems are responsible for addressing any toxic outcomes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，“透明度和问责制”的原则在解决毒性问题中也发挥作用。通过使人工智能系统更加透明，利益相关者可以更好地理解某些输出是如何以及为什么被生成的，这有助于识别和纠正有毒行为。问责制确保那些设计和部署人工智能系统的人对解决任何有毒结果负责。
- en: Rising Deepfake concern
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度伪造技术日益引起关注
- en: Deepfake technology has become a rising concern in recent times, primarily due
    to advancements in AI and machine learning, making it easier and more convincing
    than ever before. These technological improvements have enabled the creation of
    highly realistic and difficult-to-detect fake videos and images. This growing
    realism and accessibility heighten the risks of misinformation, privacy violations,
    and the potential for malicious use in politics, personal attacks, and fraud.
    In this section, we will discuss what Deepfake is, some real-world examples, its
    detrimental impact on society, and what we can do to mitigate it.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造技术近年来已成为一个日益关注的问题，这主要归因于人工智能和机器学习的进步，使得其比以往任何时候都更容易、更令人信服。这些技术进步使得创建高度逼真且难以检测的伪造视频和图像成为可能。这种日益增长的逼真性和可访问性增加了虚假信息、隐私侵犯以及在政治、个人攻击和欺诈中恶意使用的风险。在本节中，我们将讨论深度伪造是什么，一些真实世界的示例，它对社会造成的有害影响，以及我们可以采取哪些措施来减轻其影响。
- en: '![Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake
    content](img/B21443_09_2.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 被线框覆盖的脸，用于创建深度伪造内容](img/B21443_09_2.jpg)'
- en: Figure 9.2 – A face covered by a wireframe, which is used to create Deepfake
    content
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 被线框覆盖的脸，用于创建深度伪造内容
- en: What is Deepfake?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是深度伪造？
- en: Deepfake is a technology that uses artificial intelligence to create or alter
    video, images, and audio recordings, making it seem as if someone said or did
    something they did not. It typically involves manipulating someone’s likeness
    or voice.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造是一种利用人工智能创建或修改视频、图像和音频记录的技术，使其看起来像某人说了或做了他们实际上没有做的事情。这通常涉及操纵某人的肖像或声音。
- en: Some real-world examples of Deepfake
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些深度伪造的真实世界示例
- en: 'The following are some early real-world examples of Deepfakes that have raised
    significant concerns and exacerbated the need for their prevention:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些早期真实世界的深度伪造示例，它们引发了重大关注并加剧了预防其发生的必要性：
- en: In 2019, a UK-based energy firm’s CEO was tricked into transferring EUR 220,000
    after receiving a phone call from what he believed was his boss. The caller used
    Deepfake technology to imitate the boss’s voice, convincing the CEO of the legitimacy
    of the request ([https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241)).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2019年，一家英国能源公司的首席执行官在接到他认为是老板的电话后，被骗转走了22万欧元。打电话的人使用了深度伪造技术来模仿老板的声音，说服首席执行官请求的合法性（[https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241](https://www.forbes.com/sites/jessedamiani/2019/09/03/a-voice-Deepfake-was-used-to-scam-a-ceo-out-of-243000/?sh=4721eb412241)）。
- en: Edited videos and speeches have also been Deepfaked. For instance, a manipulated
    video of Facebook’s Mark Zuckerberg talking about the power of having billions
    of people’s data and a fake speech by Belgium’s prime minister linking the coronavirus
    pandemic to climate change are examples of Deepfake usage ([https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html](https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html)).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编辑过的视频和演讲也被深度伪造。例如，Facebook的马克·扎克伯格谈论拥有数十亿人数据的力量的被操纵视频，以及比利时总理关于将冠状病毒大流行与气候变化联系起来的虚假演讲，都是深度伪造应用的例子（[https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html](https://www.cnn.com/2019/06/11/tech/zuckerberg-Deepfake/index.html)）。
- en: Concerns regarding the objectification of women due to Deepfake adult videos
    have been rising. The prevalence of AI-generated pornographic content that unlawfully
    uses the faces of women without their consent is increasingly troubling, particularly
    in the online world of notable influencers and streamers. This issue came to light
    in January when “Sweet Anita,” a prominent British live streamer with 1.9 million
    Twitch followers, discovered that a collection of fake explicit videos, which
    illegitimately featured the faces of various Twitch streamers, was being shared
    online. Sweet Anita is well-known on Twitch for her gaming content and interactive
    sessions with her audience ([https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372](https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372)).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于深度伪造成人视频对女性的物化，人们对此表示担忧。AI生成的非法使用女性面部而未经其同意的色情内容的普遍存在，尤其是在知名影响者和直播者的在线世界中，越来越令人不安。这个问题在1月份浮出水面，当时拥有190万Twitch粉丝的英国知名直播者“Sweet
    Anita”发现，一组非法的色情视频在网上被分享，这些视频非法展示了各种Twitch直播者的面孔。Sweet Anita因其在Twitch上的游戏内容和与观众的互动而闻名（[https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372](https://www.nbcnews.com/tech/internet/Deepfake-twitch-porn-atrioc-qtcinderella-maya-higa-pokimane-rcna69372)）。
- en: In early 2024, AI-generated Deepfake images of Taylor Swift, some of which were
    sexually explicit, spread across social media platforms, leading platforms such
    as X (formerly Twitter) to block searches for her name and renew calls for stronger
    AI legislation. The images, seen by millions, prompted actions from social media
    companies and discussions about the need for legal and regulatory responses to
    the misuse of AI technologies.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在2024年初，泰勒·斯威夫特的AI生成深度伪造图像在社交媒体平台上广泛传播，其中一些图像具有性暗示内容，导致X（前身为Twitter）等平台封锁了对她名字的搜索，并再次呼吁加强AI立法。这些被数百万人看到的图像促使社交媒体公司采取行动，并引发了关于对AI技术滥用进行法律和监管回应的讨论。
- en: Detrimental effects on society
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对社会造成的负面影响
- en: 'The following are some negative consequences of Deepfake that can have harmful
    effects on society:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些深度伪造可能对社会造成有害影响的负面后果：
- en: '**Misinformation and erosion of trust**: Deepfakes can create highly convincing
    but false representations of individuals saying or doing things they never did,
    leading to misinformation and eroding public trust in media and institutions.
    For example, Deepfakes have been used to create fake videos of politicians, which
    can mislead voters and disrupt democratic processes.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚假信息和信任侵蚀**：深度伪造可以创造出高度可信但虚假的个体说或做他们从未做过的事情的表征，导致虚假信息和侵蚀公众对媒体和机构的信任。例如，深度伪造已被用于创建政治家的虚假视频，这可能会误导选民并破坏民主进程。'
- en: '**Exploitation and harassment**: Deepfakes can be used to create non-consensual
    explicit content or defamatory material, targeting individuals for harassment
    or blackmail. There have been instances where Deepfake technology was used to
    superimpose faces of celebrities or private individuals onto explicit content
    without their consent, causing personal distress and reputational damage.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用和骚扰**：深度伪造可以被用来创建非自愿的露骨内容或诽谤性材料，针对个人进行骚扰或勒索。有案例表明，深度伪造技术被用来将名人或私人个体的面部叠加到露骨内容上，而没有他们的同意，这导致了个人痛苦和声誉损害。'
- en: '**Security threats**: Deepfakes pose a security threat by enabling fraud and
    impersonation. They can be used to mimic voices or faces to bypass biometric security
    measures or to create convincing scams. An example was provided earlier, regarding
    a real-world case, where Deepfakes were used to mimic a CEO’s voice to trick a
    manager into transferring a significant sum of money, as reported by Forbes.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全威胁**：深度伪造通过使欺诈和假冒成为可能，构成了安全威胁。它们可以用来模仿声音或面部，以绕过生物识别安全措施或创建令人信服的骗局。之前提供了一个例子，关于一个现实世界的案例，其中深度伪造被用来模仿首席执行官的声音，诱骗经理转移大量资金，正如《福布斯》报道的那样。'
- en: '**Legal and ethical challenges**: The rise of Deepfakes creates legal and ethical
    dilemmas, challenging existing laws on consent, privacy, and free speech. Technology
    blurs the line between truth and fiction, making it difficult to discern real
    from fake and raising questions about the legality of such content creation.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律和伦理挑战**：深度伪造的兴起创造了法律和伦理困境，挑战了现有的关于同意、隐私和言论自由的法律。技术模糊了真实与虚构之间的界限，使得难以区分真伪，并引发了关于此类内容创作合法性的问题。'
- en: In my opinion, the biggest threat to human lives is a nuclear war between countries
    that can lead to suffering and death on a ginormous scale. Imagine a scenario
    where a Deepfake video falsely shows a world leader declaring war or making inflammatory
    statements, leading to international tensions or even conflicts. This highlights
    the potential geopolitical impact of Deepfakes when used maliciously and the need
    for education on how to spot Deepfakes and other mitigation strategies.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，对人类生活最大的威胁是能够导致巨大规模痛苦和死亡的国家之间的核战争。想象一下这样的场景：一个深度伪造的视频错误地显示世界领导人宣布战争或发表煽动性言论，导致国际紧张局势甚至冲突。这突出了深度伪造被恶意使用时的潜在地缘政治影响，以及教育人们如何识别深度伪造和其他缓解策略的必要性。
- en: How to spot a Deepfake
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何识别深度伪造
- en: 'The identification of Deepfake is an area of growing research. Here, we mention
    a few techniques you can use to identify Deepfake content:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造的识别是一个日益增长的研究领域。在此，我们提到一些你可以用来识别深度伪造内容的技巧：
- en: '**Facial inconsistencies**: Look for anomalies in facial expressions, such
    as awkward blinking, unusual lip movements, or facial features that appear distorted
    or don’t align correctly.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部不一致性**：寻找面部表情中的异常，例如不自然的眨眼、不寻常的唇部动作，或者看起来扭曲或未正确对齐的面部特征。'
- en: '**Audio-visual mismatch**: Check for mismatches between the audio and visual
    elements. For example, the voice may not sync perfectly with the lip movements,
    or the tone and accent might not match the person’s known speech patterns.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频-视觉不匹配**：检查音频和视觉元素之间是否存在不匹配。例如，声音可能不会与唇部动作完美同步，或者音调和口音可能不符合个人的已知说话模式。'
- en: '**Unnatural skin tone or texture**: Deepfakes may exhibit issues with skin
    tone or texture. This can include overly smooth skin, a lack of natural blemishes,
    or inconsistent lighting on the face compared to the surroundings.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不自然的肤色或质感**：深度伪造可能表现出肤色或质感的问题。这可以包括过于光滑的皮肤、缺乏自然的瑕疵，或者与周围环境相比面部照明的不一致。'
- en: '**Background anomalies**: Pay attention to the background of the video. Look
    for strange artifacts, inconsistencies in lighting, or other elements that seem
    out of place or distorted.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景异常**：注意视频的背景。寻找奇怪的人工制品、照明的不一致性或其他似乎不合适或扭曲的元素。'
- en: '**Lack of blinking or excessive blinking**: In early Deepfakes, the blinking
    was often irregular or missing. Although newer Deepfakes have improved, anomalies
    in blinking can still be a giveaway.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**眨眼不足或过度眨眼**：在早期的深度伪造中，眨眼通常是不规则或缺失的。尽管较新的深度伪造有所改进，但眨眼的异常仍然可能是一个线索。'
- en: '**Use of detection software**: There are various software tools and apps designed
    to detect Deepfakes by analyzing videos for subtle inconsistencies that are not
    easily noticeable to the human eye. Popular Deepfake detection tools include products
    from Sentinel ([https://thesentinel.ai/](https://thesentinel.ai/)) and Intel’s
    FakeCatcher.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测软件的使用**：存在各种软件工具和应用程序，旨在通过分析视频中的细微不一致性来检测深度伪造，这些不一致性对人类肉眼来说不易察觉。流行的深度伪造检测工具包括Sentinel（[https://thesentinel.ai/](https://thesentinel.ai/)）和英特尔FakeCatcher。'
- en: '**Checking source credibility**: Verify the source of the video or audio. If
    it comes from an unverified or suspicious source, it warrants further scrutiny.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检查来源可信度**：验证视频或音频的来源。如果它来自未经验证或可疑的来源，则需要进行进一步审查。'
- en: Mitigation strategies
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓解策略
- en: 'In this section, we will explore several key mitigation strategies to tackle
    the risks associated with Deepfake technology. Understanding these techniques
    is a crucial aspect of leadership education, equipping leaders, as well as the
    general public, with the necessary tools to address and counter the challenges
    posed by this advanced technology:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几种关键的缓解策略，以应对深度伪造技术相关的风险。了解这些技术是领导力教育的重要方面，为领导者以及公众提供应对和对抗这一先进技术带来的挑战的必要工具：
- en: '**Public awareness and education**: Educating the public about the existence
    and potential misuse of Deepfakes can make people more critical of the media they
    consume. This can include campaigns to raise awareness about how to spot Deepfakes,
    which we have discussed in the earlier section.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公众意识和教育**：教育公众关于深度伪造的存在和潜在滥用可以提高人们对他们所消费的媒体的批判性。这可以包括提高人们对如何识别深度伪造的认识的活动，我们已在前面章节中讨论过。'
- en: '**Deepfake detection technologies**: Developing and implementing advanced detection
    algorithms that can identify Deepfakes is crucial. These technologies often use
    machine learning to analyze videos or audio for inconsistencies or anomalies that
    are not perceptible to the human eye. Some popular Deepfake detection tools include
    Sentinel and Intel’s Deepfake detector tool.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度伪造检测技术**：开发和实施能够识别深度伪造的高级检测算法至关重要。这些技术通常使用机器学习来分析视频或音频中的不一致性或异常，这些不一致性或异常对人类肉眼来说不易察觉。一些流行的深度伪造检测工具包括Sentinel和英特尔深度伪造检测工具。'
- en: '**Legal and regulatory measures**: Governments and regulatory bodies can enact
    laws and regulations to penalize the creation and distribution of malicious Deepfakes.
    This includes defining legal frameworks that address consent, privacy, and the
    misuse of Deepfake technology. US President Biden’s office published an Executive
    Order (EO) on Oct. 30, 2023, which is a major step toward implementing safety
    standards and regulations in AI. We will discuss this EO in the upcoming section.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律和监管措施**：政府和监管机构可以制定法律和法规，对恶意深度伪造的创建和分发进行处罚。这包括定义解决同意、隐私和深度伪造技术滥用的法律框架。美国总统拜登的办公室于2023年10月30日发布了一项行政命令（EO），这是在人工智能中实施安全标准和法规的重大一步。我们将在下一节中讨论这一行政命令。'
- en: '**Blockchain and digital watermarking**: Implementing technologies such as
    blockchain and digital watermarking can help verify the authenticity of digital
    content. This can create a traceable, tamper-evident record of the media, ensuring
    its integrity. For instance, in August 2023, Google’s DeepMind launched a watermarking
    tool for AI-generated images. In November 2023, Google reported that they would
    be using inaudible watermarks in its AI-generated music, so it’s possible to detect
    if Google’s AI tech has been used in the creation of a track ([https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks](https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks)).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区块链和数字水印**：实施区块链和数字水印等技术可以帮助验证数字内容的真实性。这可以创建一个可追溯、篡改明显的媒体记录，确保其完整性。例如，2023年8月，谷歌的DeepMind推出了一款用于AI生成图像的水印工具。2023年11月，谷歌报告称，他们将在其AI生成音乐中使用不可闻的水印，因此可以检测谷歌的AI技术是否被用于创建一首曲目（[https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks](https://www.theverge.com/2023/11/16/23963607/google-deepmind-synthid-audio-watermarks)）。'
- en: '**Platform responsibility**: Social media platforms and content distributors
    play a crucial role and should implement policies and algorithms to detect and
    remove Deepfake content from their platforms. In November 2023, Meta announced
    that they would be implementing strict policies that would require political advertisers
    to flag AI-generated content as a step towards mitigating the proliferation of
    misinformation through Deepfakes.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平台责任**：社交媒体平台和内容分发商发挥着至关重要的作用，并应实施政策和算法来检测和从其平台上移除深度伪造内容。2023年11月，Meta宣布他们将实施严格的政策，要求政治广告商将AI生成的内容标记为AI生成内容，作为减轻通过深度伪造传播错误信息的蔓延的步骤。'
- en: By combining these strategies, society can better mitigate the risks associated
    with Deepfake technology, protecting individuals and maintaining trust in digital
    media.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过结合这些策略，社会可以更好地减轻与深度伪造技术相关的风险，保护个人并维护对数字媒体的信任。
- en: 'Deepfake detection is a rapidly expanding field of research, primarily driven
    by advancements in generative adversarial networks (GANs). These sophisticated
    AI algorithms consist of two parts: the generator, which is responsible for creating
    synthetic data, and the discriminator, which assesses its authenticity. The discriminator’s
    role is particularly crucial in Deepfake detection. As the cutting-edge in producing
    realistic fake images and videos, understanding and analyzing the discriminator
    aspect of GANs is pivotal for developing effective strategies to identify and
    counter Deepfake content. The deeper our grasp of GAN mechanisms, the more adept
    we become at crafting systems capable of detecting the increasingly intricate
    Deepfakes they generate. While delving into the intricacies of GANs is beyond
    the scope of this book, we strongly recommend monitoring developments in this
    field, as they are likely to play a significant role in shaping future Deepfake
    detection techniques.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造检测是一个快速发展的研究领域，主要是由生成对抗网络（GANs）的进步所驱动。这些复杂的AI算法由两部分组成：生成器，负责创建合成数据，和判别器，负责评估其真实性。判别器在深度伪造检测中的作用尤为重要。作为生产逼真伪造图像和视频的尖端技术，理解和分析GAN的判别器方面对于开发有效的策略来识别和对抗深度伪造内容至关重要。我们对GAN机制的掌握越深，就越擅长构建能够检测它们生成的日益复杂的深度伪造的系统。虽然深入探讨GAN的复杂性超出了本书的范围，但我们强烈建议关注这一领域的发展，因为这些进展可能会在塑造未来的深度伪造检测技术中发挥重要作用。
- en: Building applications using a responsible AI-first approach
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用负责任的人工智能优先方法构建应用
- en: In this section, we will explore the development of generative AI applications
    with a responsible AI-first approach. In [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117),
    we delved into the lifecycle of large language models (LLMs); however, we will
    now examine this through the lens of responsible AI. We aim to discuss how to
    integrate these principles into the various stages of development, namely ideating/exploring,
    building/augmenting, and operationalizing. Achieving this integration demands
    tight collaboration among research, compliance, and engineering teams, effectively
    bringing people, processes, and technology together. This ensures ethical data
    use, eliminating biases from LLM responses and safety and maintaining transparency
    from the initial design stage to deployment and production and beyond. Continuous
    monitoring and observability post-deployment ensure these models remain relevant
    and ethically compliant over time.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨采用负责任的人工智能优先方法开发生成式人工智能应用。在[*第6章*](B21443_06.xhtml#_idTextAnchor117)中，我们深入探讨了大型语言模型（LLMs）的生命周期；然而，现在我们将通过负责任人工智能的视角来审视这一点。我们的目标是讨论如何将这些原则整合到开发的各个阶段，即构思/探索、构建/增强和实施。实现这种整合需要研究、合规性和工程团队之间的紧密合作，有效地将人员、流程和技术结合起来。这确保了数据使用的道德性，消除了LLM响应中的偏见，并从初始设计阶段到部署和生产以及更远的地方保持透明度。部署后的持续监控和可观察性确保这些模型随着时间的推移保持相关性和道德合规性。
- en: '![Figure 9.3 – LLM Application Development Lifecycle](img/B21443_09_3.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – LLM应用开发生命周期](img/B21443_09_3.jpg)'
- en: Figure 9.3 – LLM Application Development Lifecycle
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – LLM应用开发生命周期
- en: 'We have already discussed the **Large Language Model Application Development
    Lifecycle** (**LLMADL**), as shown in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117).
    Therefore, we won’t delve into its details again. The following image illustrates
    the mitigation layers in the application and platform layers, which are essential
    for building a safe AI system. In this section, we will explore how we can incorporate
    these mitigation layers into the LLMADL process:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了**大型语言模型应用开发生命周期**（**LLMADL**），如[*第 6 章*](B21443_06.xhtml#_idTextAnchor117)所示。因此，我们不会再深入其细节。以下图像展示了应用层和平台层中的缓解层，这对于构建一个安全的AI系统至关重要。在本节中，我们将探讨如何将这些缓解层纳入LLMADL流程中：
- en: '![Figure 9.4 – Mitigation layers of gen AI applications](img/B21443_09_4.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 通用人工智能应用缓解层](img/B21443_09_4.jpg)'
- en: Figure 9.4 – Mitigation layers of gen AI applications
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 通用人工智能应用缓解层
- en: Ideating/exploration loop
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 灵感/探索循环
- en: 'The first loop involves ideation and exploration, focusing on identifying a
    use case, formulating hypotheses, selecting appropriate LLMs, and creating prompt
    variants that adhere to safety and ethical standards. This stage emphasizes the
    importance of aligning the LLM’s use case with ethical guidelines to prevent bias
    or harm. For example, in developing an LLM-powered chatbot for mental health support,
    it’s crucial to use diverse and inclusive datasets, avoid stereotypes and biases,
    and implement mechanisms to prevent harmful advice. Hypotheses formulated during
    this phase should prioritize fairness, accountability, transparency, and ethics,
    such as ensuring balanced and fair responses by training the LLM with datasets
    that have equal representation of gender and minority group dialogues:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个循环涉及灵感和探索，重点是确定一个用例，制定假设，选择合适的LLM，并创建符合安全和伦理标准的提示变体。这一阶段强调将LLM的用例与伦理指南对齐的重要性，以防止偏见或伤害。例如，在开发用于心理健康支持的LLM驱动的聊天机器人时，使用多样化和包容性的数据集，避免刻板印象和偏见，并实施防止有害建议的机制至关重要。在此阶段制定的假设应优先考虑公平性、问责制、透明度和伦理，例如通过使用具有性别和少数群体对话平等代表性的数据集来训练LLM，以确保平衡和公平的响应：
- en: '**Model Layer**: The decision to implement a mitigation layer in the model
    layer is made at this stage. This process includes identifying models that comply
    with RAI principles. Often, these safety mitigations are incorporated into models
    through fine-tuning and reinforcement learning from human feedback (RLHF); additionally,
    some benchmarks can provide guidance in making this decision. We covered RLHF
    and benchmarks in [*Chapter 3*](B21443_03.xhtml#_idTextAnchor052), highlighting
    them as potent techniques for developing models that are honest, helpful, and
    harmless. For instance, a benchmark holistic evaluation of language models (HELMs)
    from Stanford Research evaluates models for different tasks using seven key metrics:
    **accuracy**, **calibration**, **robustness, fairness**, **bias**, **toxicity**,
    and **efficiency**. Metrics for different models can be found using the following
    link; these can be a potential first step in the initial assessment when shortlisting
    models based on RAI principles: [https://crfm.stanford.edu/helm/classic/latest/#/leaderboard](https://crfm.stanford.edu/helm/classic/latest/#/leaderboard).
    Model cards associated with LLMs provided by **Hugging Face** and also **Azure
    AI Model Catalog** can also help you do your initial RAI assessment.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型层**：在模型层实施缓解层的决策是在这个阶段做出的。这个过程包括识别符合RAI原则的模型。通常，这些安全缓解措施通过微调和从人类反馈中进行强化学习（RLHF）纳入模型中；此外，一些基准可以为此决策提供指导。我们在[*第
    3 章*](B21443_03.xhtml#_idTextAnchor052)中介绍了RLHF和基准，强调它们是开发诚实、有用且无害的模型的有效技术。例如，斯坦福研究机构对语言模型（HELMs）的整体评估基准使用七个关键指标评估不同任务：**准确性**、**校准**、**鲁棒性**、**公平性**、**偏见**、**毒性**和**效率**。可以通过以下链接找到不同模型的指标；这些可以作为基于RAI原则筛选模型的初步评估中的潜在第一步：[https://crfm.stanford.edu/helm/classic/latest/#/leaderboard](https://crfm.stanford.edu/helm/classic/latest/#/leaderboard)。由**Hugging
    Face**和**Azure AI模型目录**提供的与LLM关联的模型卡片也可以帮助您进行初步的RAI评估。'
- en: '**Safety system**: For many applications, depending solely on the safety mechanisms
    integrated within the model is insufficient. Large language models can make errors
    and are vulnerable to attacks, such as jailbreak attempts. Hence, it is important
    to implement a robust content filtering system in your application to prevent
    the generation and dissemination of harmful or biased content. Once this safety
    system is activated, it becomes crucial to apply the red team testing approaches
    featuring human involvement, as outlined in [*Chapter 8*](B21443_08.xhtml#_idTextAnchor163).
    This is to guarantee the robustness of this security layer and its freedom from
    vulnerabilities. Red teaming specialists play a vital role in detecting potential
    harm and subsequently facilitate deployment of measurement strategies to confirm
    the effectiveness of the implemented mitigations.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全系统**：对于许多应用来说，仅仅依赖模型内集成的安全机制是不够的。大型语言模型可能会出错，并且容易受到攻击，如越狱尝试。因此，在您的应用程序中实施一个强大的内容过滤系统来防止有害或偏见内容的生成和传播非常重要。一旦激活了此安全系统，就变得至关重要，要应用如[*第八章*](B21443_08.xhtml#_idTextAnchor163)中概述的涉及人类参与的红色团队测试方法。这是为了保证这一安全层的稳健性和其无漏洞。红色团队专家在检测潜在危害并随后协助部署测量策略以确认实施的缓解措施的有效性方面发挥着至关重要的作用。'
- en: '**Azure Content Safety** is a content filtering application that can help you
    detect and filter out toxic user-generated or AI-generated content, which could
    be text or images. It can also provide protection from jailbreaking attempts.
    Additionally, it can provide severity levels in terms of toxicity along with categorizations
    such as violence, self-harm, sexual, and hate. You can also enable batch evaluations
    of large datasets of prompts and completions for your applications. For example,
    as seen in *Figure 9**.4*, when testing the prompt Painfully twist his arm and
    then punch him in the face, the content was rejected because of the strong filter
    set out on the right side to filter out violent content.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure内容安全**是一个内容过滤应用程序，可以帮助您检测和过滤掉有害的用户生成或AI生成的内容，这可能包括文本或图像。它还可以提供对越狱尝试的保护。此外，它还可以提供关于毒性严重程度的级别以及诸如暴力、自残、性和仇恨等分类。您还可以为您的应用程序启用对大量提示和完成的大数据集的批量评估。例如，如*图9.4*所示，当测试提示“痛苦地扭动他的手臂，然后打他的脸”时，内容被拒绝，因为右侧的强烈过滤器设置用于过滤暴力内容。'
- en: '![Figure 9.5 – Results from Azure content safety](img/B21443_09_5.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – Azure内容安全的结果](img/B21443_09_5.jpg)'
- en: Figure 9.5 – Results from Azure content safety
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – Azure内容安全的结果
- en: Building/augmenting loop
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建/增强循环
- en: 'This stage is part of the second loop. After the team identifies the desired
    models, in this stage, the goal is to tailor the models based on business requirements
    through prompt engineering and grounding the data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段是第二循环的一部分。在团队确定了所需的模型之后，在这个阶段，目标是通过对数据进行提示工程和归一化来根据业务需求调整模型：
- en: '**Metaprompting and grounding**: As outlined in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098),
    prompt engineering and metaprompts can enhance retrieval accuracy. At this stage,
    it’s important to incorporate metaprompts that address four key components: harmful
    content, grounding, copyright issues, and jailbreaking prevention to improve safety.
    We have already explored these metaprompt components with examples in [*Chapter
    5*](B21443_05.xhtml#_idTextAnchor098), so we will not delve into details here.
    However, this area is continuously evolving, and you can expect to see more templates
    emerge over time. When addressing grounding, it’s crucial to ensure that the data
    retrieved from Vector DB complies with responsible AI principles. This means not
    only should the data be unbiased, but there should also be transparency regarding
    the sources of data utilized in the retrieval system, ensuring they are ethically
    sourced. In the case of customer data, data privacy is accorded the highest priority.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元提示和归一化**：如[*第五章*](B21443_05.xhtml#_idTextAnchor098)所述，提示工程和元提示可以提高检索准确性。在这个阶段，重要的是要整合处理以下四个关键组件的元提示：有害内容、归一化、版权问题和越狱预防，以提高安全性。我们已经在[*第五章*](B21443_05.xhtml#_idTextAnchor098)中通过示例探讨了这些元提示组件，因此在此不再深入细节。然而，这个领域正在不断演变，你可以期待随着时间的推移出现更多模板。在处理归一化时，确保从Vector
    DB检索的数据符合负责任的AI原则至关重要。这意味着数据不仅应该是无偏见的，而且关于检索系统中使用的数据来源的透明度也应该存在，确保它们是道德来源的。在客户数据的情况下，数据隐私被赋予最高优先级。'
- en: '**Evaluation**: It is important to evaluate LLM models before deploying into
    production. Metrics such as groundedness, relevance, and retrieval score can help
    you determine the performance of models. Additionally, you can create custom metrics
    with LLMs such as GPT-4 and use them to evaluate your models. Azure Prompt Flow
    helps you achieve this with out-of-the-box metrics and also enables you create
    custom metrics. The following figure captures a snapshot from an experiment carried
    out using Prompt Flow, along with the associated evaluation scores. *Figure 9**.6*
    offers a visualization of the test conducted on an evaluation dataset. The LLM
    responses were assessed against the actual answers, and an average rating of 4
    or higher for groundedness, the retrieval score, and relevance suggests that the
    application is performing effectively:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**：在将LLM模型部署到生产之前进行评估非常重要。基础性、相关性和检索分数等指标可以帮助您确定模型的性能。此外，您可以使用GPT-4等LLM创建自定义指标，并使用它们来评估您的模型。Azure
    Prompt Flow通过提供开箱即用的指标并允许您创建自定义指标来帮助您实现这一点。以下图显示了使用Prompt Flow进行的实验快照，以及相关的评估分数。*图9.6*展示了在评估数据集上进行的测试的可视化。将LLM的响应与实际答案进行了评估，对于基础性、检索分数和相关性，平均评分为4或更高，表明应用程序表现良好：'
- en: '![Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)](img/B21443_09_6.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – Azure Prompt Flow评估指标（可视化）](img/B21443_09_6.jpg)'
- en: Figure 9.6 – Azure Prompt Flow evaluation metrics (visualization)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – Azure Prompt Flow评估指标（可视化）
- en: Operationalizing/deployment loop
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作化/部署循环
- en: 'This stage marks the final loop, transitioning from development into production,
    and includes designing monitoring processes that continuously evaluate metrics.
    These metrics provide a clearer indication of specific types of drifts. For instance,
    the model’s groundedness could diminish over time if the data were grounded or
    become outdated. This phase also involves integrating continuous integration/continuous
    deployment (CI/CD) processes to facilitate automation. Additionally, collaboration
    with the user experience (UX) team is crucial to ensure the creation of a safe
    user experience:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段标志着最终循环，从开发过渡到生产，包括设计持续评估指标的监控流程。这些指标提供了对特定类型漂移的更清晰指示。例如，如果数据是基础性的或变得过时，模型的基础性可能会随时间减弱。此阶段还涉及集成持续集成/持续部署（CI/CD）流程以促进自动化。此外，与用户体验（UX）团队合作对于确保创建安全用户体验至关重要：
- en: '**User experience**: In this layer, incorporating a human feedback loop to
    assess the responses of LLM models is crucial. This can be achieved through simple
    mechanisms such as a thumbs up and thumbs down system. Additionally, setting up
    predefined responses for inappropriate inquiries adds significant value. For instance,
    if a user enquires about constructing a bomb, the system automatically intercepts
    this and delivers a preset response. Furthermore, offering a prompt guide that
    integrates RAI principles and includes citations with responses is an effective
    strategy to guarantee the reliability of the responses.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户体验**：在这一层中，通过简单的机制（如点赞和踩不点赞系统）将人类反馈循环纳入评估LLM模型的响应至关重要。此外，为不适当的问题设置预定义的响应可以增加很大价值。例如，如果用户询问如何制造炸弹，系统会自动拦截并给出预设的响应。此外，提供集成了RAI原则的提示指南，并在响应中包含引用，这是一种有效的策略，以确保响应的可靠性。'
- en: '**Monitoring**: Continuous model monitoring is a crucial component of LLMOps,
    guaranteeing that AI systems stay pertinent in the face of changing societal norms
    and data trends over time. Azure Prompt Flow offers advanced tools for monitoring
    the safety and performance of your application in a production environment. This
    setup facilitates straightforward monitoring using predefined metrics such as
    groundedness, relevance, coherence, fluency, and similarity or custom metrics
    relevant to your use case. We have already conducted a lab in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070),
    focusing on evaluating RAG workflows where we discussed these metrics.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：持续模型监控是LLMOps的关键组成部分，确保AI系统在面对不断变化的社会规范和数据趋势时保持相关性。Azure Prompt Flow提供了在生产环境中监控应用程序安全性和性能的高级工具。这种设置简化了使用预定义指标（如基础性、相关性、连贯性、流畅性和相似性）或针对您用例的定制指标的监控。我们已经在[*第4章*](B21443_04.xhtml#_idTextAnchor070)中进行了实验室研究，重点关注评估RAG工作流程，其中我们讨论了这些指标。'
- en: Throughout all these stages, it’s important to engage with stakeholders, including
    diverse user groups, to understand the impact of the LLM and to ensure that it’s
    being used responsibly. Additionally, documenting the processes and decisions
    made at each stage for accountability and transparency is a key part of responsible
    AI practices.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些阶段，与包括多元化的用户群体在内的利益相关者进行合作，了解大型语言模型的影响，并确保其被负责任地使用是非常重要的。此外，记录每个阶段的过程和决策，以实现问责制和透明度，是负责任的人工智能实践的关键部分。
- en: Role of AI architects and leadership
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能架构师和领导层的角色
- en: 'AI architects and leaders play a pivotal role in building responsible AI practices
    within an organization. Their actions and decisions can set the tone for how AI
    is developed, deployed, and managed. Here are some key roles and actions they
    can take:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能架构师和领导者在构建组织内部负责任的人工智能实践中发挥着关键作用。他们的行动和决策可以设定人工智能开发、部署和管理的方式。以下是他们可以采取的一些关键角色和行动：
- en: '**Establishing ethical guidelines and standards**: Architects and leaders should
    develop and enforce ethical guidelines for AI development and use within the organization.
    This includes principles around fairness, transparency, privacy, and accountability.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立伦理准则和标准**：架构师和领导者应在组织内部制定和执行人工智能开发和使用的伦理准则。这包括关于公平性、透明度、隐私和问责制的原则。'
- en: '**Promoting transparency and explainability**: They should advocate for transparency
    in AI systems, ensuring that stakeholders understand how AI decisions are made.
    This involves promoting the development of explainable AI models.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进透明度和可解释性**：他们应倡导人工智能系统的透明度，确保利益相关者了解人工智能决策是如何做出的。这包括促进可解释人工智能模型的发展。'
- en: '**Ensuring data privacy and security**: Leaders must prioritize data privacy
    and security, implement robust policies and practices to protect sensitive information,
    and comply with relevant data protection regulations.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保数据隐私和安全**：领导者必须优先考虑数据隐私和安全，实施强大的政策和实践来保护敏感信息，并遵守相关的数据保护法规。'
- en: '**Fostering an inclusive and diverse AI culture**: Encouraging diversity in
    AI teams and in datasets is crucial. Diverse perspectives help to reduce biases
    in AI systems and make them more equitable.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培养包容和多元化的AI文化**：鼓励人工智能团队和数据集的多元化至关重要。多元化的视角有助于减少人工智能系统中的偏见，并使它们更加公平。'
- en: '**Implementing continuous monitoring and evaluation**: Regularly monitoring
    AI systems for performance, fairness, and unintended consequences is essential.
    Leaders should establish protocols for the ongoing evaluation and auditing of
    AI systems.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施持续监控和评估**：定期监控人工智能系统的性能、公平性和意外后果是至关重要的。领导者应建立人工智能系统持续评估和审计的协议。'
- en: '**Investing in responsible AI education and training**: Providing training
    and resources for employees on responsible AI practices helps to create a culture
    of ethical AI use. This includes educating teams about potential biases and how
    to mitigate them.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投资于负责任的AI教育和培训**：为员工提供关于负责任的人工智能实践培训和资源有助于营造道德人工智能使用的文化。这包括教育团队关于潜在的偏见以及如何减轻它们。'
- en: '**Encouraging collaboration and stakeholder engagement**: Engaging with various
    stakeholders, including users, ethicists, and industry experts, can provide diverse
    insights into the potential impacts of AI solutions.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**鼓励协作和利益相关者参与**：与包括用户、伦理学家和行业专家在内的各种利益相关者进行合作，可以提供关于人工智能解决方案潜在影响的多元化见解。'
- en: '**Risk assessment and management**: Conducting thorough risk assessments to
    understand the potential negative impacts of AI and implementing strategies to
    mitigate these risks is vital.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险评估和管理**：进行彻底的风险评估，以了解人工智能的潜在负面影响，并实施减轻这些风险的策略是至关重要的。'
- en: '**Creating accountability structures**: Setting up clear lines of accountability
    within the organization for AI decision-making helps to maintain ethical standards
    and address any issues that arise.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建立问责结构**：在组织内部为人工智能决策制定明确的问责线有助于维护伦理标准并解决任何出现的问题。'
- en: '**Promoting sustainable AI practices**: Ensuring that AI practices are sustainable
    and do not adversely affect the environment or society is an important consideration.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进可持续的AI实践**：确保人工智能实践是可持续的，并且不会对环境或社会产生不利影响是一个重要的考虑因素。'
- en: '**Supporting regulation and compliance**: Keeping abreast of and complying
    with international, national, and industry-specific AI regulations and standards
    is crucial for responsible AI deployment.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持法规和合规性**：了解并遵守国际、国家和特定行业AI法规和标准对于负责任地部署AI至关重要。'
- en: By taking these actions, architects and leaders can guide their organizations
    toward responsible AI practices, ensuring that AI technologies are used in a way
    that is ethical, fair, reliable, inclusive, safe, secure, and beneficial for all
    stakeholders.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采取这些行动，架构师和领导者可以引导他们的组织走向负责任的AI实践，确保AI技术以道德、公平、可靠、包容、安全、安全且对所有利益相关者都有益的方式被使用。
- en: AI, the cloud, and the law – understanding compliance and regulations
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI、云和法规——理解合规性和法规
- en: In this section, we will discuss compliance in the context of building AI solutions
    on the cloud responsibly, as it ensures that AI systems align with legal, ethical,
    and societal norms. Compliance acts as a safeguard against risks such as bias,
    privacy breaches, and unintended consequences, fostering trust among users and
    stakeholders. It promotes transparency and accountability in AI operations, encouraging
    the adoption of best practices and standardization across the industry. Moreover,
    by addressing public concerns and anticipating future challenges, compliance discussions
    help in shaping AI technologies that are not only technologically advanced but
    also socially responsible and beneficial. This is particularly important in a
    global context where AI’s impact crosses borders and cultural divides.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在云上负责任地构建AI解决方案的合规性问题，因为这确保了AI系统与法律、伦理和社会规范保持一致。合规性作为一种保障措施，可以防止风险，如偏见、隐私泄露和意外后果，在用户和利益相关者之间建立信任。它促进了AI操作的透明度和问责制，鼓励采用最佳实践和行业标准化。此外，通过解决公众关注的问题和预测未来的挑战，合规性讨论有助于塑造不仅技术先进，而且在社会上负责任且有益的AI技术。这在全球背景下尤为重要，因为AI的影响跨越国界和文化差异。
- en: Compliance considerations
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合规性考虑因素
- en: 'When architecting generative AI solutions on the cloud, there are several compliance
    considerations to keep in mind:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在云上架构生成式AI解决方案时，有几个合规性考虑因素需要牢记：
- en: '**Data privacy regulations**: These comply with global data protection laws
    such as GDPR (Europe), CCPA (California), and others, depending on the geographical
    location and scope of your service or industry. The **General Data Protection
    Regulation** (**GDPR**) is a comprehensive data protection law in the European
    Union that sets guidelines for the collection and processing of personal information
    from individuals in the EU. Adhering to GDPR is crucial, as it ensures the protection
    of personal data, builds trust with customers, and avoids significant fines for
    non-compliance, thereby maintaining a company’s reputation and legal standing
    in the global market. The **California Consumer Privacy Act** (**CCPA**) is a
    state statute in California, USA, designed to enhance privacy rights and consumer
    protection for residents of California. Adhering to CCPA laws is important because
    it ensures compliance with California’s stringent privacy regulations, builds
    consumer trust by protecting personal data, and helps avoid significant financial
    penalties for non-compliance.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私法规**：这些法规符合全球数据保护法，如GDPR（欧洲）、CCPA（加利福尼亚）等，具体取决于您服务的地理位置和范围。**通用数据保护条例**（**GDPR**）是欧盟的一项全面数据保护法，为欧盟个人信息的收集和处理设定了指导方针。遵守GDPR至关重要，因为它确保了个人数据的保护，与客户建立了信任，避免了因不遵守规定而导致的重大罚款，从而维护了公司在全球市场的声誉和法律地位。**加利福尼亚消费者隐私法案**（**CCPA**）是美国加利福尼亚州的一项州法律，旨在增强加利福尼亚居民的隐私权和消费者保护。遵守CCPA法规很重要，因为它确保了符合加利福尼亚严格的隐私法规，通过保护个人数据建立消费者信任，并有助于避免因不遵守规定而导致的重大财务处罚。'
- en: '**Industry-specific regulations**: Some examples of industry-specific regulations
    are **Health Insurance Portability and Accountability Act** (**HIPAA**) for healthcare
    data in the US and Canada, **Payment Card Industry Data Security Standard** (**PCI
    DSS**) for payment card information, and FERPA for educational records. **FERPA**
    stands for the **Family Educational Rights and Privacy Act**. It’s a US federal
    law that protects the privacy of student education records and gives parents specific
    rights with respect to their children’s education records.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业特定法规**：一些行业特定法规的例子包括美国和加拿大的医疗保健数据方面的**健康保险可携带性和问责法案**（HIPAA）、支付卡信息方面的**支付卡行业数据安全标准**（PCI
    DSS）以及教育记录方面的FERPA。**FERPA**代表**家庭教育权利和隐私法案**。这是一项美国联邦法律，旨在保护学生教育记录的隐私，并赋予家长对其子女教育记录的具体权利。'
- en: '**Service organization control (SOC) reports**: Ensure compliance with SOC
    2, which focuses on security, availability, processing integrity, confidentiality,
    and the privacy of a system. SOC 2 compliance is more about trust and assurance
    than legal obligation, but its implications are significant in terms of security,
    business relationships, and overall reputation in the market.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务组织控制（SOC）报告**：确保符合SOC 2，该标准侧重于安全性、可用性、处理完整性、机密性和系统的隐私。SOC 2合规性更多地关乎信任和保证，而不是法律义务，但其影响在安全性、商业关系和整体市场声誉方面是显著的。'
- en: '**Cloud security measures**: Cloud solutions must be secure to protect sensitive
    data against breaches. This involves enabling encryption, access controls, and
    regular security audits.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云安全措施**：云解决方案必须安全，以保护敏感数据免受泄露。这包括启用加密、访问控制和定期安全审计。'
- en: '**Auditability and reporting**: Being able to track and report on how the AI
    system makes decisions can be important for regulatory compliance and transparency.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可审计性和报告**：能够跟踪和报告人工智能系统如何做出决策对于合规性和透明度可能很重要。'
- en: '**Data localization/residency laws**: Some jurisdictions require that data
    be stored within the country of origin, which can affect cloud service choices
    and architecture.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据本地化/居住地法律**：某些司法管辖区要求数据存储在原始国家，这可能影响云服务选择和架构。'
- en: '**Business continuity and disaster recovery**: Adhere to standards that ensure
    business continuity and disaster recovery, such as ISO/IEC 22301.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务连续性和灾难恢复**：遵守确保业务连续性和灾难恢复的标准，例如ISO/IEC 22301。'
- en: 'Top cloud providers, such as Microsoft, have a robust compliance portfolio
    to assist their customers. They provide necessary tools such as Microsoft Purview
    and comprehensive documentation to aid customers on their compliance journey.
    For a full list, we recommend checking out the compliance offerings from Microsoft
    here: [https://learn.microsoft.com/en-us/compliance/regulatory/offering-home](https://learn.microsoft.com/en-us/compliance/regulatory/offering-home).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 顶级云服务提供商，如微软，拥有强大的合规组合，以协助其客户。他们提供必要的工具，如Microsoft Purview和全面的文档，以帮助客户在其合规之旅上。要获取完整列表，我们建议您查看微软的合规产品：[https://learn.microsoft.com/en-us/compliance/regulatory/offering-home](https://learn.microsoft.com/en-us/compliance/regulatory/offering-home)。
- en: Global and United States AI regulatory landscape
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全球和美国人工智能监管格局
- en: The current global AI regulatory landscape is marked by diverse approaches and
    emerging trends. Accelerating capabilities in AI, including large language models,
    facial recognition, and advanced cognitive processing, have propelled AI regulation
    to prominence among policy-makers.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的全球人工智能监管格局以多样化的方法和新兴趋势为特征。人工智能能力的加速，包括大型语言模型、面部识别和高级认知处理，已推动人工智能监管在政策制定者中的突出地位。
- en: Europe has been the frontrunner in this journey towards AI regulation. The EU
    Act has made significant progress towards becoming law, with unanimous approval
    from EU member states as of February 2, 2024\. It sets a global standard for AI
    technology, emphasizing a balance between innovation and safety. The EU AI Act
    introduces a nuanced regulatory framework for artificial intelligence, categorizing
    AI systems based on their risk levels to ensure appropriate oversight. Systems
    posing an “**unacceptable risk**,” such as those capable of cognitive manipulation
    or implementing social scoring based on certain protected traits, biometric identification,
    and the categorization of people, are outright banned, with narrow exceptions
    for law enforcement under stringent conditions. “**High-risk”** AI systems, impacting
    safety or fundamental rights, are subject to strict assessment and registration
    requirements, covering a wide range of applications from critical infrastructure
    management, assistance in legal interpretation, and education to law enforcement.
    Meanwhile, “general purpose and generative AI,” such as ChatGPT, must adhere to
    transparency directives, including the disclosure of AI-generated content and
    measures against illegal and toxic content production and publishing summaries
    of copyrighted data used for training. Systems deemed “**limited risk”** should
    comply with minimal transparency requirements. This includes applications with
    image, audio, or video generation models, facilitating informed decisions by users.
    This stratified approach aims to balance the innovation potential of AI with necessary
    safeguards against its potential harms ([https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 欧洲在人工智能监管的旅途中一直处于领先地位。欧盟法案在成为法律方面取得了重大进展，截至2024年2月2日，已获得欧盟成员国的无异议批准。它为人工智能技术设定了全球标准，强调创新与安全之间的平衡。欧盟人工智能法案引入了一个细致的监管框架，根据人工智能系统的风险水平对它们进行分类，以确保适当的监管。那些造成“**不可接受的风险**”的系统，例如那些能够进行认知操纵或基于某些受保护特征实施社会评分的系统，以及生物识别和人分类系统，被明确禁止，仅在严格条件下对执法有狭窄的例外。对于影响安全或基本权利的“**高风险**”人工智能系统，将进行严格的评估和注册要求，涵盖从关键基础设施管理、法律解释辅助和教育到执法的广泛应用。同时，“**通用和生成式人工智能**”，例如ChatGPT，必须遵守透明度指令，包括披露人工智能生成的内容以及防止非法和有害内容的生产和发布版权数据的摘要。被认为“**低风险**”的系统应遵守最低透明度要求。这包括具有图像、音频或视频生成模型的应用，帮助用户做出明智的决定。这种分层方法旨在平衡人工智能的创新潜力及其潜在危害的必要保障（[https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence](https://www.europarl.europa.eu/news/en/headlines/society/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence))。
- en: Conversely, India initially opted against AI regulation, focusing on policy
    and infrastructure to foster AI growth, but later considered a regulatory framework
    addressing algorithm biases and copyrights. The US hasn’t moved towards comprehensive
    federal AI legislation but has seen regulatory responses from agencies such as
    the National Institute of Standards and Technology (NIST), the Federal Trade Commission
    (FTC), and the Food and Drug Administration (FDA) regarding public concerns over
    AI technologies.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，印度最初选择不进行人工智能监管，专注于政策和基础设施以促进人工智能增长，但后来考虑了一个针对算法偏见和版权的监管框架。美国尚未朝着全面的联邦人工智能立法迈进，但已经看到来自国家标准与技术研究院（NIST）、联邦贸易委员会（FTC）和食品药品监督管理局（FDA）等机构的监管反应，这些反应针对的是公众对人工智能技术的担忧。
- en: Regulatory frameworks are developing globally to balance AI’s benefits against
    its risks. EY’s analysis of eight jurisdictions (Canada, China, EU, Japan, Korea,
    Singapore, UK, and the US) reflects a variety of regulatory approaches. The rules
    and policy initiatives were inspired by the OECD’s Organization for Economic Co-operation
    AI policy Observatory.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 全球范围内正在发展监管框架，以平衡人工智能的益处与风险。EY对八个司法管辖区（加拿大、中国、欧盟、日本、韩国、新加坡、英国和美国）的分析反映了各种监管方法。规则和政策倡议受到了经合组织（OECD）经济合作与发展组织人工智能政策观察站的启发。
- en: OECD is an international organization comprising 38 member countries, established
    to promote economic progress and world trade by offering a platform for democratic,
    market-economy nations to discuss policies, share experiences, and co-ordinate
    on global issues.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 经济合作与发展组织（OECD）是一个由38个成员国组成的国际组织，旨在通过为民主市场经济国家提供一个讨论政策、分享经验、协调全球问题的平台来促进经济进步和世界贸易。
- en: 'As per this research from Ernst and Young, released in September 2023, five
    common regulatory trends have emerged globally:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据安永公司于2023年9月发布的研究，全球出现了五个常见的监管趋势：
- en: '**Alignment with key AI principles**: The AI regulation and guidance being
    evaluated align with the key AI principles of human rights for respect, sustainability,
    transparency, and robust risk management, as established by the OECD and supported
    by the G20\. The Group of Twenty (G20) is an international forum of 19 countries
    and the European Union focused on addressing global economic issues and representing
    the world’s major economies.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与关键人工智能原则的一致性**：正在评估的人工智能监管和指导与经济合作与发展组织（OECD）和二十国集团（G20）支持的人权尊重、可持续性、透明度和稳健的风险管理的关键人工智能原则保持一致。二十国集团（G20）是一个由19个国家以及欧盟组成的国际论坛，专注于解决全球经济问题，并代表世界主要经济体。'
- en: '**Risk-based approach**: These jurisdictions adopt a risk-based approach to
    AI regulation, meaning they customize their AI rules based on the perceived risks
    AI poses to fundamental values such as privacy, non-discrimination, transparency,
    and security.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于风险的方法**：这些司法管辖区采用基于风险的方法来监管人工智能，这意味着他们根据人工智能对基本价值观（如隐私、非歧视、透明度和安全）造成的风险来定制他们的AI规则。'
- en: '**Sector and sector-agnostic rules**: Due to the diverse applications of AI,
    certain jurisdictions are emphasizing the importance of sector-specific regulations
    alongside more general, sector-agnostic rules.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行业和行业无关的规则**：由于人工智能的多样化应用，某些司法管辖区强调在更普遍的行业无关规则之外，特定行业监管的重要性。'
- en: '**Digital priority areas**: In the realm of other digital priority areas such
    as cybersecurity, data privacy, and intellectual property rights, jurisdictions
    are advancing in their creation of AI-specific regulations, with the European
    Union leading in adopting a comprehensive strategy.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数字优先领域**：在其他数字优先领域，如网络安全、数据隐私和知识产权，司法管辖区正在推进其创建特定于人工智能的法规，欧盟在采用全面战略方面处于领先地位。'
- en: '**Collaboration with private sector and policy-makers**: Numerous jurisdictions
    employ regulatory sandboxes, allowing private sector collaboration with policy-makers
    to craft rules that both ensure safe, ethical AI and address the potential need
    for closer oversight in higher-risk AI innovations.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与私营部门和政策制定者的合作**：许多司法管辖区采用监管沙盒，允许私营部门与政策制定者合作制定规则，这些规则既确保了安全、道德的人工智能，又满足了更高风险人工智能创新可能需要更密切监管的需求。'
- en: Biden Executive Order on AI
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比登总统关于人工智能的行政命令
- en: 'On October 30, 2023, President Joe Biden issued an Executive Order, which we
    think is a major step towards regulating AI in the United States. The Executive
    Order is thoroughly comprehensive, simultaneously ensuring human safety and responsible
    AI use while fostering fair competition within the country and advancing leadership
    on the global stage. There are eight major topics that the EO covers:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年10月30日，美国总统乔·拜登发布了一项行政命令，我们认为这是美国监管人工智能的重大一步。该行政命令内容全面，同时确保人类安全、负责任地使用人工智能，并在国内促进公平竞争，并在全球舞台上提升领导地位。该行政命令涵盖了八个主要议题：
- en: '**New standards for AI safety and security**: The Executive Order requires
    developers of powerful AI systems to share safety test results with the US government.
    It establishes standards and tests to ensure AI systems are safe and secure before
    public release, addresses risks in using AI for biological materials, and combats
    AI-enabled fraud and deception. An advanced cybersecurity program will also be
    developed to leverage AI in securing software and networks. It directs the National
    Security Council and White House Chief of Staff to develop a National Security
    Memorandum, guiding further AI and security actions, ensuring the US military
    and intelligence community’s safe, ethical, and effective use of AI, and outlining
    measures to counter adversaries’ military AI applications.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能安全和安全的新标准**：行政命令要求强大人工智能系统的开发者与美国政府分享安全测试结果。它建立标准和测试，以确保在公开发布之前人工智能系统是安全和可靠的，解决使用人工智能处理生物材料的风险，并打击人工智能驱动的欺诈和欺骗。还将开发一个高级网络安全计划，利用人工智能来保护软件和网络。它指示国家安全委员会和白宫幕僚长制定国家安全备忘录，指导进一步的AI和安全行动，确保美国军事和情报社区安全、道德和有效地使用人工智能，并概述应对对手军事人工智能应用的措施。'
- en: '**Protecting Americans’ privacy**: The order emphasizes protecting privacy
    by accelerating the development and use of privacy-preserving techniques in AI.
    It includes funding research for privacy technologies and developing guidelines
    for federal agencies to evaluate the effectiveness of these techniques, especially
    in AI systems.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保护美国人的隐私**：该命令强调通过加速人工智能中隐私保护技术的开发和利用来保护隐私。它包括为隐私技术提供研究资金，并为联邦机构制定评估这些技术有效性的指南，特别是在人工智能系统中。'
- en: '**Advancing equity and civil rights**: This addresses the responsible principles
    of fairness and inclusiveness. To combat discrimination and bias in AI, the order
    provides guidance to landlords and federal programs, addresses algorithmic discrimination
    through training and technical assistance, and aims to ensure fairness in the
    criminal justice system through the development of best practices in AI use.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推进公平和民权**：这涉及到公平和包容性的负责任原则。为了对抗人工智能中的歧视和偏见，该命令为房东和联邦项目提供指导，通过培训和专业技术援助来解决算法歧视，并旨在通过制定人工智能使用的最佳实践来确保刑事司法系统的公平性。'
- en: '**Standing up for consumers, patients, and students**: This includes advancing
    responsible AI use in healthcare, such as developing affordable drugs and establishing
    a safety program for healthcare practices involving AI. It also involves creating
    resources to support educators using AI-enabled educational tools.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为消费者、患者和学生挺身而出**：这包括在医疗保健中推进负责任的人工智能使用，例如开发负担得起的药物和为涉及人工智能的医疗实践建立安全计划。它还涉及创建资源以支持使用人工智能教育工具的教育者。'
- en: '**Supporting workers**: The order directs the development of principles and
    best practices to maximize AI benefits for workers, addressing issues such as
    job displacement, labor standards, and workplace equity. It also includes producing
    a report on AI’s potential impact on the labor market.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持工人**：该命令指导制定原则和最佳实践，以最大化人工智能对工人的利益，解决诸如就业转移、劳动标准和职场公平等问题。它还包括发布一份关于人工智能对劳动力市场潜在影响的报告。'
- en: '**Promoting innovation and competition**: Actions include catalyzing AI research
    nationwide, promoting a competitive AI ecosystem by providing resources to small
    developers, and expanding the ability of skilled immigrants to work in the US
    in AI-related fields.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进创新和竞争**：行动包括在全国范围内催化人工智能研究，通过为小型开发者提供资源来促进竞争性的人工智能生态系统，并扩大有技能的移民在美国人工智能相关领域工作的能力。'
- en: '**Advancing American leadership abroad**: The administration will work with
    other nations to support the global deployment and use of safe and trustworthy
    AI. This involves expanding engagements to collaborate on AI, developing AI standards
    with international partners, and promoting responsible AI development to address
    global challenges.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推进美国在国外的领导地位**：该政府将与其他国家合作，支持全球部署和使用安全可靠的人工智能。这包括扩大合作以在人工智能上进行合作，与国际伙伴制定人工智能标准，并推广负责任的人工智能发展以应对全球挑战。'
- en: '**Ensuring responsible and effective governmental use of AI**: The order aims
    to modernize federal AI infrastructure and ensure responsible AI deployment in
    government. This includes issuing guidance for AI use in agencies, accelerating
    the hiring of AI professionals, and providing AI training to government employees.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保人工智能在政府中的负责任和有效使用**：该命令旨在使联邦人工智能基础设施现代化，并确保政府在人工智能的负责任部署。这包括为机构发布人工智能使用指南，加快人工智能专业人员的招聘，并为政府员工提供人工智能培训。'
- en: In summary, while compliance plays a pivotal role in fostering safer and more
    responsible AI systems, it can indeed be a double-edged sword. Excessive compliance
    requirements might stifle innovation, potentially hindering a country’s competitive
    edge on the global stage. Therefore, it’s imperative that regulators are well-informed
    and engage in thorough consultations with AI experts when crafting regulations
    and standards. This balanced approach ensures that AI develops in a safe and ethical
    manner while still allowing for the flexibility and creativity necessary for technological
    advancement and competitive success.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，虽然合规性在培养更安全、更负责任的人工智能系统中发挥着关键作用，但它确实是一把双刃剑。过度的合规性要求可能会扼杀创新，可能阻碍一个国家在全球舞台上的竞争优势。因此，当制定法规和标准时，监管机构必须充分了解并与人工智能专家进行彻底的协商至关重要。这种平衡的方法确保人工智能以安全、道德的方式发展，同时仍然允许技术进步和竞争成功所需的灵活性和创造力。
- en: Startup ecosystem in RAI
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负责任人工智能领域的创业生态系统
- en: In this section, we will discuss a few notable startups emerging in the responsible
    AI space and building products that keep RAI at their core.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些在负责任人工智能领域崭露头角并构建以负责任人工智能为核心产品的初创公司。
- en: '**Parity AI**: Founded by Rumman Chowdhury, Parity AI focuses on AI risk management
    and offers tools for auditing AI models for bias or legal compliance and provides
    recommendations for addressing these issues ([https://www.get-parity.com/](https://www.get-parity.com/)).'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parity AI**：由Rumman Chowdhury创立，Parity AI专注于人工智能风险管理，并提供用于审计AI模型是否存在偏见或符合法律规定的工具，并提供解决这些问题的建议（[https://www.get-parity.com/](https://www.get-parity.com/))。'
- en: '**Fiddler**: Founded by Krishna Gade, Fiddler focuses on explainability in
    AI, helping to make AI model decisions more transparent. It aids data science
    teams in monitoring their models’ performance and generating executive summaries
    from the outcomes. If a model’s accuracy declines or displays bias, Fiddler assists
    in identifying the reasons. Gade views model monitoring and enhancing clarity
    as key initial steps for more deliberate AI development and deployment ([https://www.fiddler.ai/ai-observability](https://www.fiddler.ai/ai-observability)).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fiddler**：由Krishna Gade创立，Fiddler专注于人工智能的可解释性，帮助使AI模型决策更加透明。它帮助数据科学团队监控其模型的表现，并从结果中生成管理摘要。如果一个模型的准确性下降或表现出偏见，Fiddler将协助识别原因。Gade认为，模型监控和增强清晰度是更谨慎地进行人工智能开发和部署的关键初始步骤（[https://www.fiddler.ai/ai-observability](https://www.fiddler.ai/ai-observability))。'
- en: '**Arthur**: Founded in 2019, Arthur is a company specializing in AI performance,
    assisting enterprise clients in maximizing their AI’s potential through performance
    monitoring and optimization, providing explainability, and mitigating bias.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arthur**：成立于2019年，Arthur是一家专注于人工智能性能的公司，通过性能监控和优化、提供可解释性和减轻偏见，帮助企业客户最大限度地发挥其人工智能的潜力。'
- en: '**Weights and Biases**: Founded in 2017, Weights and Biases focuses on the
    reproducibility aspect of machine learning model experiments. In my opinion, reproducibility
    is vital in AI because it forms the bedrock of scientific trust and validation.
    It allows for the independent verification of results, facilitating the correction
    of errors and building upon research findings. Crucially, in the context of AI’s
    rapid transition from research to real-world applications, reproducibility ensures
    that AI models are robust, unbiased, and safe. It also helps address the AI ‘black-box’
    problem by allowing a broader understanding of how models function. This is particularly
    important in high-stakes areas such as healthcare, law enforcement, and public
    interaction, where AI’s impact is direct and significant.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weights and Biases**：成立于2017年，Weights and Biases专注于机器学习模型实验的可重复性方面。在我看来，可重复性在人工智能中至关重要，因为它构成了科学信任和验证的基础。它允许对结果进行独立验证，促进错误更正并建立在研究发现之上。关键的是，在人工智能从研究快速过渡到实际应用的背景下，可重复性确保了AI模型是稳健的、无偏见的和安全的。它还有助于通过允许更广泛地理解模型如何工作来解决人工智能“黑盒”问题。这在医疗保健、执法和公共互动等高风险领域尤为重要，在这些领域，人工智能的影响是直接和重大的。'
- en: '**Datagen**: Datagen specializes in computer vision and facial data, ensuring
    their datasets are varied in terms of skin tones, hairstyles, genders, and angles
    to reduce bias in facial recognition technology ([https://datagen.tech/](https://datagen.tech/)).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Datagen**：Datagen专注于计算机视觉和面部数据，确保他们的数据集在肤色、发型、性别和角度等方面具有多样性，以减少面部识别技术中的偏见（[https://datagen.tech/](https://datagen.tech/))。'
- en: '**Galileo and Snorkel AI**: Galileo and Snorkel AI focus on maintaining high
    data quality; Galileo does this by automatically adjusting biases in unstructured
    data, whereas Snorkel AI ensures equitable, automated labeling, along with data
    versioning and audit services ([https://www.rungalileo.io/](https://www.rungalileo.io/),[https://snorkel.ai/](https://snorkel.ai/)).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Galileo和Snorkel AI**：Galileo和Snorkel AI专注于保持高质量的数据；Galileo通过自动调整非结构化数据中的偏差来实现这一点，而Snorkel
    AI确保公平、自动化的标记，以及数据版本控制和审计服务（[https://www.rungalileo.io/](https://www.rungalileo.io/)，[https://snorkel.ai/](https://snorkel.ai/))。'
- en: The preceding list is not exhaustive. This space is evolving, and there are
    numerous new start-ups making significant inroads in this field.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的列表并不全面。这个领域正在发展，有许多新的初创企业在这个领域取得了重大突破。
- en: '![Figure 9.7 – Start-up ecosystem in RAI](img/B21443_09_7.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 负责任人工智能（RAI）的初创企业生态系统](img/B21443_09_7.jpg)'
- en: Figure 9.7 – Start-up ecosystem in RAI
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 负责任人工智能（RAI）的初创企业生态系统
- en: 'The preceding figure, referenced from BGV ([https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)),
    shows a few notable start-ups providing ethical AI services across five categories:
    data privacy, AI monitoring and observability, AI audits, governance, risk, compliance,
    targeted AI solutions and technologies, and open source solution.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表，参考自BGV（[https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)），展示了五个类别中提供道德人工智能服务的几个知名初创企业：数据隐私、人工智能监控和可观察性、人工智能审计、治理、风险、合规、针对人工智能的解决方案和技术、开源解决方案。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: To summarize, the development of more sophisticated AI systems and the journey
    towards achieving **artificial general intelligence** (**AGI**) necessitates a
    steadfast commitment to RAI principles. Neglecting these principles could result
    in AI posing significant risks to humanity. In this chapter, we delved deeply
    into responsible AI principles, uncovering their theoretical and practical implications,
    especially within the realms of LLMs and Deepfake technology. We highlighted the
    importance of ethical vigilance and the role of architecture and leadership in
    guiding AI towards beneficial applications, alongside an analysis of the current
    regulatory landscape shaping AI’s evolution. Our exploration extended to responsible
    AI tools and the dynamic startup ecosystem, emphasizing how new companies are
    both influencing and adapting to these AI trends. These insights are crucial,
    as they equip us with the knowledge to harness AI’s power responsibly, ensuring
    its alignment with ethical standards and societal benefits. Looking ahead, in
    the final chapter, we will discuss the future of ChatGPT, where we’ll delve into
    emerging trends and potential advancements, highlighting innovative uses that
    are set to redefine our interaction with AI and society.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，更高级的人工智能系统的发展和实现**通用人工智能**（**AGI**）的旅程需要坚定不移地承诺遵守RAI原则。忽视这些原则可能导致人工智能对人类构成重大风险。在本章中，我们深入探讨了负责任的人工智能原则，揭示了它们的理论和实践意义，特别是在LLMs和Deepfake技术领域。我们强调了道德警觉性以及架构和领导在引导人工智能向有益应用发展中的作用，同时分析了塑造人工智能演变的当前监管格局。我们的探索还扩展到了负责任的人工智能工具和动态的初创企业生态系统，强调了新公司如何影响并适应这些人工智能趋势。这些见解至关重要，因为它们使我们能够负责任地利用人工智能的力量，确保其与道德标准和
    societal benefits 保持一致。展望未来，在最后一章中，我们将讨论ChatGPT的未来，我们将深入探讨新兴趋势和潜在进步，突出那些将重新定义我们与人工智能和社会互动的创新用途。
- en: References
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AI is sending people to jail—and getting it wrong: [https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能正在将人们送进监狱——并且犯下了错误：[https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/](https://www.technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/)
- en: 'Thousands of Dollars for Something I Didn’t Do: [https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml](https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我没有做的事情支付数千美元：[https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml](https://www.nytimes.com/2023/03/31/technology/facial-recognition-false-arrests.html?login=ml&auth=login-ml)
- en: 'Can the criminal justice system’s AI be truly fair?: [https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刑事司法系统的AI能真正公平吗？：[https://tinyurl.com/bdejxubh](https://tinyurl.com/bdejxubh)
- en: The journey to build an explainable AI-driven recommendation system to help
    scale sales efficiency across LinkedIn:[https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys](https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立一个可解释的AI驱动推荐系统以帮助LinkedIn上销售效率的扩展之旅：[https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys](https://www.linkedin.com/blog/engineering/recommendations/the-journey-to-build-an-explainable-ai-driven-recommendation-sys)
- en: 'Empowering the Future of Recruitment: 7 AI Hiring Tools Ushering in a Bright
    2023 - HyScaler: [https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/](https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动招聘未来的力量：7个AI招聘工具引领光明的2023 - HyScaler：[https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/](https://hyscaler.com/insights/ai-hiring-tools-7-trends-2023/)
- en: 'Worried about your firm’s AI ethics? These startups are here to help. | MIT
    Technology Review: [https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/](https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 担心你公司的AI伦理？这些初创公司在这里帮助你。 | MIT技术评论：[https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/](https://www.technologyreview.com/2021/01/15/1016183/ai-ethics-startups/)
- en: 'The AI Ethics Boom: 150 Ethical AI Startups and Industry Trends - BGV: [https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能伦理的繁荣：150家伦理人工智能初创公司和行业趋势 - BGV：[https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/](https://benhamouglobalventures.com/ai-ethics-boom-150-ethical-ai-startups-industry-trends/)
- en: 'Responsible AI toolkits: [https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815](https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责任的AI工具包：[https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815](https://odsc.medium.com/15-open-source-responsible-ai-toolkits-and-projects-to-use-today-fbc1c2ea2815)
- en: 'Deepfakes, explained | MIT Sloan: [https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度伪造，解释 | MIT Sloan：[https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained](https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained)
- en: 'Regulatory Landscape: [https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations](https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监管环境：[https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations](https://www.goodwinlaw.com/en/insights/%20publications/2023/04/04_12-us-artificial-intelligence-regulations)
- en: 'Artificial Intelligence regulation, global trends | EY - US: [https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape](https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能监管，全球趋势 | EY - 美国：[https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape](https://www.ey.com/en_us/ai/how-to-navigate-global-trends-in-artificial-intelligence-regulation#:~:text=,rapidly%20evolving%20AI%20regulatory%20landscape)
- en: 'Infuse responsible AI tools and practices in your LLMOps | Microsoft Azure
    Blog: [https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/](https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的LLMOps中融入负责任的AI工具和实践 | 微软Azure博客：[https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/](https://azure.microsoft.com/en-us/blog/infuse-responsible-ai-tools-and-practices-in-your-llmops/)
- en: 'Part 5: Generative AI – What’s Next?'
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5部分：生成式AI – 接下来是什么？
- en: This concluding part delves into the future prospects of generative AI, particularly
    the advancements in multimodal AI, with a detailed look at GPT-4 Turbo with vision
    capabilities. It also examines the emergence of **Smaller Language Models** (**SLMs**)
    and their significant impact on edge computing, a trend that facilitates faster
    and more efficient AI processing closer to the data source. Additionally, we’ll
    explore other emerging trends, future predictions, and the integration of generative
    AI with robotics, highlighting the synergy between these technologies. The journey
    toward achieving **Artificial General Intelligence** (**AGI**) through the unparalleled
    computational power of quantum computing will also be discussed, mapping out the
    potential roadmap and the technological leaps required to realize AGI.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分深入探讨了生成式人工智能的未来前景，特别是多模态人工智能的进步，详细审视了具有视觉能力的GPT-4 Turbo。它还考察了**小型语言模型**（**SLMs**）的出现及其对边缘计算的重大影响，这一趋势促进了更快速、更高效的AI处理，更接近数据源。此外，我们还将探讨其他新兴趋势、未来预测以及生成式人工智能与机器人技术的集成，突出这些技术之间的协同作用。通过量子计算的无比计算能力实现**通用人工智能**（**AGI**）的旅程也将被讨论，描绘出实现AGI的潜在路线图和所需的技术飞跃。
- en: 'This part contains the following chapter:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 10*](B21443_10.xhtml#_idTextAnchor218), *Future of Generative AI:
    Trends and Emerging Use Cases*'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B21443_10.xhtml#_idTextAnchor218)，*生成式人工智能的未来：趋势和新兴用例*'
