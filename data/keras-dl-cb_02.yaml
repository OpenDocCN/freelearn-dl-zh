- en: Deep Feedforward Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度前馈网络
- en: In the first chapter, you learned about the mathematics which drives the logic
    behind all kinds of neural networks. In this chapter, we are going to focus on
    the most fundamental neutral networks, which are called **feedforward neural networks**.
    We will also look at deep feedforward networks with multiple hidden layers to
    improve the accuracy of the model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，你学习了驱动各种神经网络背后逻辑的数学知识。在本章中，我们将重点讨论最基本的神经网络——**前馈神经网络**。我们还将研究具有多个隐藏层的深度前馈网络，以提高模型的准确性。
- en: 'We will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Defining feedforward networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义前馈网络
- en: Understanding backpropagation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: Implementing feedforward networks in TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中实现前馈网络
- en: Analyzing the Iris dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析鸢尾花数据集
- en: Creating feedforward networks for image classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建用于图像分类的前馈网络
- en: Defining feedforward networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义前馈网络
- en: Deep feedforward networks, also called feedforward neural networks, are sometimes
    also referred to as **Multilayer Perceptrons** (**MLPs**). The goal of a feedforward
    network is to approximate the function of *f∗*. For example, for a classifier,
    *y=f∗(x)* maps an input *x* to a label *y.* A feedforward network defines a mapping
    from input to label *y=f(x;θ)*. It learns the value of the parameter *θ* that
    results in the best function approximation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈网络，也称为前馈神经网络，有时也被称为**多层感知机**（**MLPs**）。前馈网络的目标是逼近 *f∗* 的函数。例如，对于一个分类器，*y
    = f∗(x)* 将输入 *x* 映射到标签 *y*。前馈网络定义了从输入到标签的映射 *y = f(x;θ)*。它学习参数 *θ* 的值，以得到最佳的函数逼近。
- en: We discuss RNNs in [Chapter 5](5f7c0191-1c53-462e-aa51-634572f20214.xhtml), *Recurrent
    Neural Networks*. Feedforward networks are a conceptual stepping stone on the
    path to recurrent networks, which power many natural language applications. Feedforward
    neural networks are called networks because they compose together many different
    functions which represent them. These functions are composed in a directed acyclic
    graph.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](5f7c0191-1c53-462e-aa51-634572f20214.xhtml)中讨论了 RNN，*递归神经网络*。前馈网络是递归网络的概念性基石，后者驱动了许多自然语言应用。前馈神经网络之所以被称为网络，是因为它们由许多不同的函数组合而成，这些函数在有向无环图中被组合在一起。
- en: The model is associated with a directed acyclic graph describing how the functions
    are composed together. For example, there are three functions *f(1)*, *f(2)*,
    and *f(3)* connected to form *f(x) =f(3)(f(2)(f(1)(x)))*. These chain structures
    are the most commonly used structures of neural networks. In this case, *f(1)*
    i*s* called the **first layer** of the network, *f(2)* is called the **second
    layer**, and so on. The overall length of the chain gives the depth of the model.
    It is from this terminology that the name deep learning arises. The final layer
    of a feedforward network is called the **output layer**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型与一个有向无环图相关联，描述了函数如何组合在一起。例如，有三个函数 *f(1)*、*f(2)* 和 *f(3)*，它们组合成 *f(x) = f(3)(f(2)(f(1)(x)))*。这些链式结构是神经网络中最常用的结构。在这种情况下，*f(1)*
    被称为**第一层**，*f(2)* 被称为**第二层**，以此类推。链的整体长度给出了模型的深度。正是这个术语产生了“深度学习”这个名字。前馈网络的最后一层被称为**输出层**。
- en: '![](img/e7e58a6b-8f57-4648-9ee4-024e28d62900.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7e58a6b-8f57-4648-9ee4-024e28d62900.png)'
- en: Diagram showing various functions activated on input x to form a neural network
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了在输入 x 上激活的各种函数，形成一个神经网络
- en: These networks are called neural because they are inspired by neuroscience.
    Each hidden layer is a vector. The dimensionality of these hidden layers determines
    the width of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络被称为神经网络，因为它们受到神经科学的启发。每一层隐藏层是一个向量，这些隐藏层的维度决定了模型的宽度。
- en: Understanding backpropagation
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: When a feedforward neural network is used to accept an input *x* and produce
    an output *yˆ*, information flows forward through the network elements. The input
    *x* provides the information that then propagates up to the hidden units at each
    layer and produces *yˆ*. This is called **forward propagation**. During training,
    forward propagation continues onward until it produces a scalar cost *J(θ)*. The
    backpropagation algorithm, often called backprop, allows the information from
    the cost to then flow backward through the network in order to compute the gradient.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用前馈神经网络接受输入 *x* 并生成输出 *yˆ* 时，信息通过网络元素向前流动。输入 *x* 提供信息，接着传播到每一层的隐藏单元，并产生 *yˆ*。这叫做
    **前向传播**。在训练过程中，前向传播一直持续，直到产生一个标量成本 *J(θ)*。反向传播算法，通常称为反向传播，允许成本的信息然后向后流经网络，从而计算梯度。
- en: Computing an analytical expression for the gradient is straightforward, but
    numerically evaluating such an expression can be computationally expensive. The
    backpropagation algorithm does so using a simple and inexpensive procedure.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度的解析表达式很简单，但数值计算该表达式可能非常耗费计算资源。反向传播算法通过一种简单且廉价的过程实现这一点。
- en: Backpropagation refers only to the method to compute the gradient, while another
    algorithm, such as stochastic gradient descent, refers to the actual mechanism.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播仅指计算梯度的方法，而另一种算法，如随机梯度下降，指的是实际的机制。
- en: Implementing feedforward networks with TensorFlow
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现前馈网络
- en: 'Feedforward networks can be easily implemented using TensorFlow by defining
    placeholders for hidden layers, computing the activation values, and using them
    to calculate predictions. Let''s take an example of classification with a feedforward
    network:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现前馈网络非常简单，通过为隐藏层定义占位符，计算激活值，并利用它们来计算预测。我们以一个使用前馈网络进行分类的示例为例：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the predicted value tensor has been defined, we calculate the `cost` function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了预测值张量，我们就可以计算 `cost` 函数：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, `OPERATION_NAME` could be one of the following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`OPERATION_NAME` 可能是以下之一：
- en: '`tf.nn.sigmoid_cross_entropy_with_logits`: Calculates sigmoid cross entropy
    on incoming `logits` and `labels`:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.sigmoid_cross_entropy_with_logits`: 计算传入的 `logits` 和 `labels` 的 sigmoid
    交叉熵：'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`_sentinel`: Used to prevent positional parameters. Internal, do not use.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`_sentinel`: 用于防止位置参数。内部使用，请勿使用。'
- en: '`labels`: A tensor of the same type and shape as logits.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`: 与 logits 具有相同类型和形状的张量。'
- en: '`logits`: A tensor of type `float32` or `float64`. The formula implemented
    is ( *x = logits*, *z = labels*) `max(x, 0) - x * z + log(1 + exp(-abs(x)))`.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 类型为 `float32` 或 `float64` 的张量。实现的公式是（*x = logits*，*z = labels*）`max(x,
    0) - x * z + log(1 + exp(-abs(x)))`。'
- en: '`tf.nn.softmax`: Performs `softmax` activation on the incoming tensor. This
    only normalizes to make sure all the probabilities in a tensor row add up to one.
    It cannot be directly used in a classification.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.softmax`: 对传入的张量执行 `softmax` 激活。这仅仅是归一化，确保张量中每一行的概率总和为 1。不能直接用于分类。'
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`logits`: A non-empty tensor. Must be one of the following types--half, `float32`,
    or `float64`.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 非空张量。必须是以下类型之一——half、`float32` 或 `float64`。'
- en: '`dim`: The dimension `softmax` will be performed on. The default is `-1`, which
    indicates the last dimension.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim`: 执行 `softmax` 的维度。默认值为 `-1`，表示最后一个维度。'
- en: '`name`: A name for the operation (optional).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`: 操作的名称（可选）。'
- en: '`tf.nn.log_softmax`: Calculates the log of the `softmax` function and helps
    in normalizing underfitting. This function is also just a normalization function.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.nn.log_softmax`: 计算 `softmax` 函数的对数，并有助于归一化欠拟合。该函数也仅是一个归一化函数。'
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`logits`: A non-empty tensor. Must be one of the following types--half, `float32`,
    or `float64`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 非空张量。必须是以下类型之一——half、`float32` 或 `float64`。'
- en: '`dim`: The dimension `softmax` will be performed on. The default is `-1`, which
    indicates the last dimension.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim`: 执行 `softmax` 的维度。默认值为 `-1`，表示最后一个维度。'
- en: '`name`: A name for the operation (optional).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`: 操作的名称（可选）。'
- en: '`tf.nn.softmax_cross_entropy_with_logits`'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.softmax_cross_entropy_with_logits`'
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`_sentinel`: Used to prevent positional parameters. For internal use only.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`_sentinel`: 用于防止位置参数。仅供内部使用。'
- en: '`labels`: Each rows `labels[i]` must be a valid probability distribution.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`: 每一行的`labels[i]`必须是有效的概率分布。'
- en: '`logits`: Unscaled log probabilities.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 未缩放的对数概率。'
- en: '`dim`: The class dimension. Defaulted to `-1`, which is the last dimension.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim`: 类维度。默认为 `-1`，即最后一个维度。'
- en: '`name`: A name for the operation (optional).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`name`: 操作的名称（可选）。'
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`tf.nn.sparse_softmax_cross_entropy_with_logits`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.sparse_softmax_cross_entropy_with_logits`'
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`labels`: Tensor of shape [*d_0*, *d_1*, *...*, *d_(r-1)*] (where *r* is the
    rank of labels and result) and `dtype`, `int32`, or `int64`. Each entry in labels
    must be an index in [*0*, `num_classes`). Other values will raise an exception
    when this operation is run on the CPU and return NaN for corresponding loss and
    gradient rows on the GPU.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`: 形状为 [*d_0*，*d_1*，*...*，*d_(r-1)*]（其中 *r* 为标签和结果的秩）和 `dtype` 为 `int32`
    或 `int64` 的张量。标签中的每个条目必须是 [*0*，`num_classes`] 中的索引。其他值将在 CPU 上运行时引发异常，并在 GPU 上返回对应的
    NaN 损失和梯度行。'
- en: '`logits`: Unscaled log probabilities of shape [*d_0*, *d_1*, *...*, *d_(r-1)*,
    `num_classes`] and `dtype`, `float32`, or `float64`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 形状为 [*d_0*，*d_1*，*...*，*d_(r-1)*，`num_classes`]，类型为 `float32` 或 `float64`
    的未缩放对数概率张量。'
- en: The preceding code computes sparse `softmax` cross entropy between `logits`
    and `labels`. The probability of a given label is considered exclusive. Soft classes
    are not allowed, and the label's vector must provide a single specific index for
    the true class for each row of `logits`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码计算了 `logits` 和 `labels` 之间的稀疏 `softmax` 交叉熵。给定标签的概率被认为是独占的。软类是不允许的，并且标签的向量必须为每一行的
    `logits` 提供一个特定的真实类别索引。
- en: '`tf.nn.weighted_cross_entropy_with_logits`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.nn.weighted_cross_entropy_with_logits`'
- en: '[PRE8]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`targets`: A tensor of the same type and shape as logits.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`targets`: 与 logits 形状和类型相同的张量。'
- en: '`logits`: A tensor of type `float32` or `float64`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`logits`: 类型为 `float32` 或 `float64` 的张量。'
- en: '`pos_weight`: A coefficient to use on the positive examples.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`pos_weight`: 用于正例的系数。'
- en: This is similar to `sigmoid_cross_entropy_with_logits()` except that `pos_weight`
    allows a trade-off of recall and precision by up or down weighting the cost of
    a positive error relative to a negative error.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 `sigmoid_cross_entropy_with_logits()` 类似，不同之处在于 `pos_weight` 通过加权正误差与负误差之间的成本来调节召回率和精确度。
- en: Analyzing the Iris dataset
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析 Iris 数据集
- en: Let's look at a feedforward example using the Iris dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个使用 Iris 数据集的前馈示例。
- en: You can download the dataset from [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv)
    and the target labels from [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv)
    下载数据集，从 [https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv)
    下载目标标签。
- en: 'In the Iris dataset, we will use 150 rows of data made up of 50 samples from
    each of three Iris species: Iris setosa, Iris virginica, and Iris versicolor.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Iris 数据集中，我们将使用 150 行数据，每种鸢尾花（Iris setosa、Iris virginica 和 Iris versicolor）各有
    50 个样本。
- en: 'Petal geometry compared from three iris species:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从三种鸢尾花中比较花瓣的几何形状：
- en: '**Iris Setosa**, **Iris Virginica**, and **Iris Versicolor**.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**Iris Setosa**、**Iris Virginica** 和 **Iris Versicolor**。'
- en: '![](img/ca899bdb-1bb7-4491-b8b4-79db174760e5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca899bdb-1bb7-4491-b8b4-79db174760e5.png)'
- en: 'In the dataset, each row contains data for each flower sample: sepal length,
    sepal width, petal length, petal width, and flower species. Flower species are
    stored as integers, with 0 denoting Iris setosa, 1 denoting Iris versicolor, and
    2 denoting Iris virginica.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，每行包含每个花样本的数据：萼片长度、萼片宽度、花瓣长度、花瓣宽度和花的种类。花种类以整数存储，0 表示 Iris setosa，1 表示 Iris
    versicolor，2 表示 Iris virginica。
- en: 'First, we will create a `run()` function that takes three parameters--hidden
    layer size `h_size`, standard deviation for weights `stddev`, and Step size of
    Stochastic Gradient Descent `sgd_step`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个 `run()` 函数，该函数接受三个参数——隐藏层大小 `h_size`、权重的标准差 `stddev`，以及随机梯度下降的步长
    `sgd_step`：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Input data loading is done using the `genfromtxt` function in `numpy`. The
    Iris data loaded has a shape of L: 150 and W: 4\. Data is loaded in the `all_X`
    variable. Target labels are loaded from `target.csv` in `all_Y` with the shape
    of L: 150, W:3:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '输入数据加载通过 `numpy` 中的 `genfromtxt` 函数完成。加载的 Iris 数据具有形状 L: 150 和 W: 4。数据加载到 `all_X`
    变量中。目标标签从 `target.csv` 加载到 `all_Y` 中，形状为 L: 150，W: 3：'
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once data is loaded, we initialize the weights matrix based on `x_size`, `y_size`,
    and `h_size` with standard deviation passed to the `run()` method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载完成后，我们根据 `x_size`、`y_size` 和 `h_size` 初始化权重矩阵，并将标准差传递给 `run()` 方法：
- en: '`x_size`= 5'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_size`= 5'
- en: '`y_size`= 3'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_size`= 3'
- en: '`h_size`= 128 (or any other number chosen for neurons in the hidden layer)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h_size` = 128（或为隐藏层中选择的任意神经元数量）'
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we make the prediction using `sigmoid` as the activation function defined
    in the `forward_propagration()` function:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用在`forward_propagration()`函数中定义的激活函数`sigmoid`进行预测：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, `sigmoid` output is calculated from input `X` and `weights_1`. This
    is then used to calculate `y` as a matrix multiplication of `sigmoid` and `weights_2`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，计算从输入`X`和`weights_1`得到的`sigmoid`输出。然后，这个输出将用于通过`sigmoid`和`weights_2`的矩阵乘法计算`y`：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Next, we define the cost function and optimization using gradient descent. Let's
    look at the `GradientDescentOptimizer` being used. It is defined in the `tf.train.GradientDescentOptimizer`
    class and implements the gradient descent algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义成本函数和使用梯度下降法进行优化。我们来看一下正在使用的`GradientDescentOptimizer`。它定义在`tf.train.GradientDescentOptimizer`类中，并实现了梯度下降算法。
- en: 'To construct an instance, we use the following constructor and pass `sgd_step`
    as a parameter:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建实例，我们使用以下构造函数并将`sgd_step`作为参数传入：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Arguments passed are explained here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 传入的参数在这里解释：
- en: '`learning_rate`: A tensor or a floating point value. The learning rate to use.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：一个张量或浮动值。用于学习率。'
- en: '`use_locking`: If True, use locks for update operations.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_locking`：如果为True，则在更新操作中使用锁。'
- en: '`name`: Optional name prefix for the operations created when applying gradients.
    The default name is `"GradientDescent"`.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：应用梯度时创建的操作的可选名称前缀。默认名称为`"GradientDescent"`。'
- en: 'The following list shows the code to implement the `cost` function:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现`cost`函数的代码列表：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we will implement the following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现以下步骤：
- en: 'Initialize the TensorFlow session:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化TensorFlow会话：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Initialize all the variables using `tf.initialize_all_variables()`; the return
    object is used to instantiate the session.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.initialize_all_variables()`初始化所有变量；返回的对象用于实例化会话。
- en: Iterate over `steps` (1 to 50).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历`steps`（从1到50）。
- en: For each step in `train_x` and `train_y`, execute `updates_sgd`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`train_x`和`train_y`中的每一步，执行`updates_sgd`。
- en: Calculate the `train_accuracy` and `test_accuracy`.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`train_accuracy`和`test_accuracy`。
- en: 'We stored the accuracy for each step in a list so that we could plot a graph:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个步骤的准确率存储在一个列表中，以便绘制图表：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Code execution
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码执行
- en: 'Let''s run this code for `h_size` of `128`, standard deviation of `0.1`, and
    `sgd_step` of `0.01`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为`h_size`设置为`128`，标准差为`0.1`，`sgd_step`为`0.01`，运行这段代码：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The preceding code outputs the following graph, which plots the steps versus
    the test and train accuracy:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出以下图表，图表绘制了步数与测试和训练准确率的关系：
- en: '![](img/6ae79ae5-fae3-4cf2-88ff-a77d94bf6a8e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ae79ae5-fae3-4cf2-88ff-a77d94bf6a8e.png)'
- en: Let's compare the change in SGD steps and its effect on training accuracy. The
    following code is very similar to the previous code example, but we will rerun
    it for multiple SGD steps to see how SGD steps affect accuracy levels.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较SGD步骤的变化及其对训练准确率的影响。以下代码与之前的代码示例非常相似，但我们将重新运行它并使用多个SGD步骤，看看SGD步骤如何影响准确率水平。
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output of the preceding code will be an array with training and test accuracy
    for each SGD step value. In our example, we called the function `sgd_steps` for
    an SGD step value of `[0.01, 0.02, 0.03]`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出将是一个数组，包含每个SGD步长值的训练和测试准确率。在我们的示例中，我们调用了`sgd_steps`函数，SGD步长值为`[0.01,
    0.02, 0.03]`：
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is the plot showing how training accuracy changes with `sgd_steps`. For
    an SGD value of `0.03`, it reaches a higher accuracy faster as the step size is
    larger.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是展示训练准确率如何随`sgd_steps`变化的图表。对于`0.03`的SGD值，由于步长较大，它能更快地达到更高的准确率。
- en: '![](img/a3106868-376b-44e2-bbe3-c544ef83747d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3106868-376b-44e2-bbe3-c544ef83747d.png)'
- en: Implementing feedforward networks with images
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现带有图像的前馈神经网络
- en: Now we will look at how to use feedforward networks to classify images. We will
    be using `notMNIST` data. The dataset consists of images for nine letters, A to
    I.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨如何使用前馈神经网络对图像进行分类。我们将使用`notMNIST`数据集。该数据集包含九个字母的图像，从A到I。
- en: NotMNIST dataset is similar to MNIST dataset but focuses on Alphabets instead
    of numbers ([http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html](http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: NotMNIST数据集类似于MNIST数据集，但它专注于字母而非数字（[http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html](http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html)）
- en: We have reduced the original dataset to a smaller version for the training so
    that you can easily get started. Download the ZIP files and extract them to the
    folder where the dataset is contained, [https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs](https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将原始数据集缩小为较小的版本，以便您可以轻松入门。下载 ZIP 文件并将其解压到包含数据集的文件夹中，[https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs](https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs)。
- en: The pickle module of python implements an algorithm for serializing and de-serializing
    a Python object structure. **Pickling** is the process in which a Python object
    hierarchy is converted into a byte stream, unpickling is the inverse operation,
    where a byte stream is converted back into an object hierarchy. Pickling (and
    unpickling) is alternatively known as **serialization**, **marshaling**, [1] or
    **flattening**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的 pickle 模块实现了一种用于序列化和反序列化 Python 对象结构的算法。**Pickling** 是将 Python 对象层次结构转换为字节流的过程，unpickling
    是其逆操作，将字节流转换回对象层次结构。Pickling（和 unpickling）也被称为 **序列化**、**编组**，[1] 或 **扁平化**。
- en: 'First, we load the images in `numpy.ndarray` from the following list of folders
    using the `maybe_pickle(..)` method:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `maybe_pickle(..)` 方法从以下文件夹列表加载 `numpy.ndarray` 格式的图像：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `maybe_pickle` uses the `load_letter` method to load the image to `ndarray`
    from a single folder:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`maybe_pickle` 使用 `load_letter` 方法从单个文件夹加载图像到 `ndarray`：'
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `maybe_pickle` method is called for two sets of folders, `train_folders`
    and `test_folders`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`maybe_pickle` 方法分别对两组文件夹 `train_folders` 和 `test_folders` 进行调用：'
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Output is similar to the following screenshot.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 输出类似于以下屏幕截图。
- en: 'The first screenshot shows the `dataset_names` list variable value:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张屏幕截图显示了 `dataset_names` 列表变量的值：
- en: '![](img/fe21686e-bc89-45c9-b244-e2bfd65e636f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe21686e-bc89-45c9-b244-e2bfd65e636f.png)'
- en: 'The following screenshot shows the value of the `dataset_names` variable for
    the `notMNIST_small` dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了 `dataset_names` 变量在 `notMNIST_small` 数据集中的值：
- en: '![](img/d87bb91f-bc8b-4f5c-865c-ce0aa3ab5400.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d87bb91f-bc8b-4f5c-865c-ce0aa3ab5400.png)'
- en: 'Next, `merge_datasets` is called, where pickle files from each character are
    combined into the following `ndarray`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用 `merge_datasets`，将每个字符的 pickle 文件合并为以下 `ndarray`：
- en: '`valid_dataset`'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`valid_dataset`'
- en: '`valid_labels`'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`valid_labels`'
- en: '`train_dataset`'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_dataset`'
- en: '`train_labels`'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_labels`'
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output of the preceding code is listed as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出列出如下：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, the `noMNIST.pickle` file is created by storing each of these `ndarray`
    in key-value pairs where the keys are `train_dataset`, `train_labels`, `valid_dataset`,
    `valid_labels`, `test_dataset`, and `test_labels`, and values are the respective
    `ndarray`, as shown in the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过将每个 `ndarray` 存储为键值对来创建 `noMNIST.pickle` 文件，其中键包括 `train_dataset`、`train_labels`、`valid_dataset`、`valid_labels`、`test_dataset`
    和 `test_labels`，值为相应的 `ndarray`，如以下代码所示：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This is the full code for generating the `notMNIST.pickle` file:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成 `notMNIST.pickle` 文件的完整代码：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Let's look at how the pickle file created earlier loads data and runs a network
    with one hidden layer.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看之前创建的 pickle 文件如何加载数据并运行具有一个隐藏层的网络。
- en: 'First, we will load the training, testing, and validation datasets (`ndarray`)
    from the `notMNIST.pickle` file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从 `notMNIST.pickle` 文件中加载训练、测试和验证数据集（`ndarray`）：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will see an output similar to the following listing:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似于以下内容的输出：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we `reformat` the `dataset` into a two-dimensional array so that data
    is easier to process with TensorFlow:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将 `dataset` 重塑为二维数组，以便数据能更容易地通过 TensorFlow 进行处理：
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will see the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下输出：
- en: '[PRE31]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Next, we define the graph that will return the content to which all the variables
    will be loaded.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义图形，返回将加载所有变量的内容。
- en: The size of each weight and bias is listed here, where `image_size` = 28 and
    `no_of_neurons` = 1024.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个权重和偏差的大小列在这里，其中 `image_size` = 28 和 `no_of_neurons` = 1024。
- en: Number of neurons in the hidden layer should be optimal. Too few neurons leads
    to lower accuracy, while too high a number leads to overfitting.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中神经元的数量应该是最优的。神经元太少会导致准确度较低，而神经元太多则会导致过拟合。
- en: '| **Layer in the neural network** | **Weight** | **Bias** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **神经网络中的层** | **权重** | **偏差** |'
- en: '| 1 | row = 28 x 28 = 784columns = 1024 | 1024 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 行 = 28 x 28 = 784列 = 1024 | 1024 |'
- en: '| 2 | row = 1024columns = 10 | 10 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 行 = 1024列 = 10 | 10 |'
- en: We will initialize the TensorFlow graph and initialize placeholder variables
    from training, validation, and test the datasets and labels.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将初始化 TensorFlow 图，并从训练、验证和测试数据集及标签中初始化占位符变量。
- en: 'We will also define weights and biases for two layers:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为两个层定义权重和偏置：
- en: '[PRE32]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we define the hidden layer tensor and the calculated logit:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义隐藏层张量和计算得到的logit：
- en: '[PRE33]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Loss function for our network is going to be based on the `softmax` function
    applied over cross entropy with `logits`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们网络的损失函数将基于对交叉熵应用`softmax`函数后的`logits`：
- en: '[PRE34]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we calculate the `logits` (predicted values); note that we are using
    `softmax` to normalize the `logits`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算`logits`（预测值）；请注意，我们使用`softmax`来归一化`logits`：
- en: '[PRE35]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Calculate the test and validation predictions. Notice the activation function
    `RELU` being used here to calculate `w1` and `b1`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 计算测试和验证预测。请注意，此处使用的激活函数是`RELU`，用于计算`w1`和`b1`：
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now we will create a TensorFlow session and pass the datasets loaded through
    the neural network created:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个TensorFlow会话，并将通过神经网络加载的数据集传递进去：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The full code can be found at [https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在[https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py)找到。
- en: 'The complete code can be found at the preceding GitHub link. Notice that we
    are appending the validation and minibatch accuracy to an array that we will plot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可以在前述GitHub链接中找到。请注意，我们正在将验证精度和小批量精度追加到一个数组中，稍后我们将绘制这个数组：
- en: '[PRE38]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s look at the plot generated by the preceding code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前面代码生成的图表：
- en: '![](img/80586153-ad97-4844-b423-d2a50c9046e3.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80586153-ad97-4844-b423-d2a50c9046e3.png)'
- en: Minibatch accuracy reaches 100 by iteration number 8- while validation accuracy
    stops at 60.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量精度在第8次迭代时达到100，而验证精度停留在60。
- en: Analyzing the effect of activation functions on the feedforward networks accuracy
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析激活函数对前馈网络精度的影响
- en: 'In the previous example, we used `RELU` as the activation function. TensorFlow
    supports multiple activation functions. Let''s look at how each of these activation
    functions affects validation accuracy. We will generate some random values:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们使用了`RELU`作为激活函数。TensorFlow支持多种激活函数。我们来看看这些激活函数如何影响验证精度。我们将生成一些随机值：
- en: '[PRE39]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then generate the activation output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后生成激活输出：
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Plot the activation against `x_val`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制激活与`x_val`的关系：
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Plots are shown in the following screenshot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了绘图结果：
- en: '![](img/331e4f58-6df4-42eb-9a67-6618c6f7a212.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/331e4f58-6df4-42eb-9a67-6618c6f7a212.png)'
- en: 'The plot comparing **Activation functions with Vanishing Gradient** is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 比较**带有消失梯度的激活函数**的图表如下：
- en: '![](img/4ed73c91-5b7d-42ac-b254-e7b90f3765c2.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ed73c91-5b7d-42ac-b254-e7b90f3765c2.png)'
- en: Now let's look at the activation function and how it affects validation accuracy
    for NotMNIST data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看激活函数如何影响NotMNIST数据集的验证精度。
- en: 'We have modified the previous example so that we can pass the activation function
    as a parameter in `main()`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了之前的示例，以便在`main()`中将激活函数作为参数传递：
- en: '[PRE42]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `run()` function definition encompasses the login that we defined earlier:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`run()`函数的定义包括我们之前定义的登录：'
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Plots from the preceding list are shown in the following screenshot:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 前述列表中的图表展示如下截图：
- en: '![](img/64631b8e-2376-4387-bb86-1321549ddfc8.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64631b8e-2376-4387-bb86-1321549ddfc8.png)'
- en: Validation accuracy for various activation functions
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 不同激活函数的验证精度
- en: 'As can be seen in the preceding graphs, RELU and RELU6 provide maximum validation
    accuracy, which is close to 60 percent. Now let''s look at how training loss behaves
    as we progress through the steps for various activations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图表所示，RELU和RELU6提供了最大验证精度，接近60%。现在，让我们看看不同激活函数在训练过程中损失如何变化：
- en: '![](img/214cb862-ba33-41ed-adab-3925dd4ac740.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/214cb862-ba33-41ed-adab-3925dd4ac740.png)'
- en: Training loss for various activations as a function of steps
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 不同激活函数的训练损失随步骤变化的图示
- en: Training loss converges to zero for most of the activation functions, though
    RELU is the least effective in the short-term.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数激活函数，训练损失收敛到零，尽管RELU在短期内效果最差。
- en: Summary
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we built our first neural network, which was feedforward only,
    and used it for classification with the Iris dataset and later the NotMNIST dataset.
    You learned how various activation functions like affect the validation accuracy
    of the prediction.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们构建了第一个神经网络，它仅为前馈神经网络，并使用它进行鸢尾花数据集的分类，后来又应用于NotMNIST数据集。你学习了像**激活函数**等如何影响预测的验证准确性。
- en: In the next chapter, we will explore a convoluted neural network, which is more
    advanced and effective for an image dataset.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨卷积神经网络，它对于图像数据集更为先进且有效。
