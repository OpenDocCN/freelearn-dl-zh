- en: 3\. Image Classification with Convolutional Neural Networks (CNNs)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 卷积神经网络（CNNs）进行图像分类
- en: Introduction
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will study **convolutional neural networks** (**CNNs**)
    and image classification. First, we will be introduced to the architecture of
    CNNs and how to implement them. We will then get hands-on experience of using
    TensorFlow to develop image classifiers. Finally, we will cover the concepts of
    transfer learning and fine-tuning and see how we can use state-of-the-art algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习**卷积神经网络**（**CNNs**）和图像分类。首先，我们将介绍CNN的架构以及如何实现它们。接着，我们将通过实践，使用TensorFlow开发图像分类器。最后，我们将讨论迁移学习和微调的概念，并了解如何使用最先进的算法。
- en: By the end of this chapter, you will have a good understanding of what CNNs
    are and how programming with TensorFlow works.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将对CNN有一个清晰的理解，并了解如何使用TensorFlow进行编程。
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapters, we learned about traditional neural networks and a
    number of models, such as the perceptron. We learned how to train such models
    on structured data for regression or classification purposes. Now, we will learn
    how we can extend their application to the computer vision field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了传统的神经网络和一些模型，比如感知机。我们了解了如何在结构化数据上训练这些模型，以进行回归或分类任务。现在，我们将学习如何将这些模型应用扩展到计算机视觉领域。
- en: Not so long ago, computers were perceived as computing engines that could only
    process well-defined and logical tasks. Humans, on the other hand, are more complex
    since we have five basic senses that help us see things, hear noises, feel things,
    taste foods, and smell odors. Computers were only calculators that could operate
    large volumes of logical operations, but they couldn't deal with complex data.
    Compared to the abilities of humans, computers had very clear limitations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不久前，计算机被认为是只能处理明确定义和逻辑任务的计算引擎。另一方面，人类则更加复杂，因为我们拥有五种基本感官，帮助我们看事物、听声音、触摸物体、品尝食物和闻气味。计算机只是可以进行大量逻辑操作的计算器，但它们无法处理复杂的数据。与人类的能力相比，计算机有着明显的局限性。
- en: There were some rudimentary attempts to “*give sight*" to computers by processing
    and analyzing digital images. This field is called computer vision. But it was
    not until the advent of deep learning that we saw some incredible improvements
    and results. Nowadays, the field of computer vision has advanced to such an extent
    that, in some cases, computer vision AI systems are able to process and interpret
    certain types of images faster and more accurately than humans. You may have heard
    about the experiment where a group of 15 doctors in China competed against a deep
    learning system from the company BioMind AI for recognizing brain tumors from
    X-rays. The AI system took 15 minutes to accurately predict 87% of the 225 input
    images, while it took 30 minutes for the medical experts to achieve a score of
    66% on the same pool of images.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一些原始的尝试，通过处理和分析数字图像来“*赋予计算机视觉*”。这个领域被称为计算机视觉。但直到深度学习的出现，我们才看到了一些令人难以置信的进展和成果。如今，计算机视觉领域已经取得了如此显著的进展，以至于在某些情况下，计算机视觉AI系统能够比人类更快、更准确地处理和解释某些类型的图像。你可能听说过这样一个实验：中国的15位医生与BioMind
    AI公司的深度学习系统进行比赛，试图从X光片中识别脑肿瘤。AI系统用了15分钟准确预测了225张输入图像中的87%，而医生们用了30分钟，在同一组图像上获得了66%的准确率。
- en: We've all heard about self-driving cars that can automatically make the right
    decisions depending on traffic conditions or drones that can detect sharks and
    automatically send alerts to lifeguards. All these amazing applications are possible
    thanks to the recent development of CNNs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听说过自动驾驶汽车，它们可以根据交通状况自动做出正确决策，或者无人机可以检测到鲨鱼并自动向救生员发送警报。所有这些令人惊叹的应用都要归功于CNN的最新发展。
- en: 'Computer vision can be split into four different domains:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉可以分为四个不同的领域：
- en: '**Image classification**, where we need to recognize the main object in an
    image.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类**，我们需要在图像中识别主要物体。'
- en: '**Image classification and localization**, where we need to recognize and localize
    the main object in an image with a bounding box.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类和定位**，我们需要在图像中识别并用边界框定位主要物体。'
- en: '**Object detection**, where we need to recognize multiple objects in an image
    with bounding boxes.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标检测**，我们需要在图像中识别多个物体并用边界框进行标注。'
- en: '**Image segmentation**, where we need to identify the boundaries of objects
    in an image.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割**，我们需要识别图像中物体的边界。'
- en: 'The following figure shows the difference between the four domains:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了四个领域之间的差异：
- en: '![Figure 3.1: Difference between the four domains of computer vision'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：计算机视觉四个领域之间的差异'
- en: '](img/B15385_03_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_01.jpg)'
- en: 'Figure 3.1: Difference between the four domains of computer vision'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：计算机视觉四个领域之间的差异
- en: In this chapter, we will only look at image classification, which is the most
    widely used application of CNN. This includes things such as car plate recognition,
    automatic categorization of the pictures taken with your mobile phone, or creating
    metadata used by search engines on databases of images.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将只讨论图像分类，它是卷积神经网络（CNN）最广泛应用的领域。这包括车牌识别、手机拍摄图片的自动分类，或为搜索引擎在图像数据库中创建元数据等内容。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you''re reading the print version of this book, you can download and browse
    the color versions of some of the images in this chapter by visiting the following
    link: [https://packt.live/2ZUu5G2](https://packt.live/2ZUu5G2 )'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在阅读本书的印刷版，可以通过访问以下链接下载并浏览本章中部分图像的彩色版本：[https://packt.live/2ZUu5G2](https://packt.live/2ZUu5G2)
- en: Digital Images
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数字图像
- en: Humans can see through their eyes by transforming light into electrical signals
    that are then processed by the brain. But computers do not have physical eyes
    to capture light. They can only process information in digital forms composed
    of bits (0 or 1). So, to be able to “see", computers require a digitized version
    of an image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过眼睛看到事物，将光转化为电信号，然后由大脑处理。但计算机没有物理眼睛来捕捉光线。它们只能处理由位（0或1）组成的数字信息。因此，为了“看到”，计算机需要图像的数字化版本。
- en: 'A digital image is formed by a two-dimensional matrix of pixels. For a grayscale
    image, each of these pixels can take a value between 0 and 255 that represents
    its intensity or level of gray. A digital image can be composed of one channel
    for a black and white image or three channels (red, blue, and green) for a color
    image:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像是由二维像素矩阵构成的。对于灰度图像，每个像素的值介于0到255之间，表示其强度或灰度级别。数字图像可以由一个通道（用于黑白图像）或三个通道（红、蓝、绿通道，用于彩色图像）组成：
- en: '![Figure 3.2: Digital representation of an image'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2：图像的数字表示'
- en: '](img/B15385_03_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_02.jpg)'
- en: 'Figure 3.2: Digital representation of an image'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：图像的数字表示
- en: 'A digital image is characterized by its dimensions (height, width, and channel):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像的特点是其尺寸（高度、宽度和通道）：
- en: '**Height:** How many pixels there are on the vertical axis.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高度：** 表示垂直方向上的像素数量。'
- en: '**Width:** How many pixels there are on the horizontal axis.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宽度：** 表示水平方向上的像素数量。'
- en: '**Channel:** How many channels there are. If there is only one channel, an
    image will be in grayscale. If there are three channels, the image will be colored.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通道：** 表示通道的数量。如果只有一个通道，图像将是灰度图。如果有三个通道，图像将是彩色图。'
- en: The following digital image has dimensions (512, 512, 3).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下数字图像的尺寸为（512，512，3）。
- en: '![Figure 3.3: Dimensions of a digital image'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3：数字图像的尺寸'
- en: '](img/B15385_03_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_03.jpg)'
- en: 'Figure 3.3: Dimensions of a digital image'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：数字图像的尺寸
- en: Image Processing
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像处理
- en: Now that we know how a digital image is represented, let's discuss how computers
    can use this information to find patterns that will be used to classify an image
    or localize objects. So, in order to get any useful or actionable information
    from an image, a computer has to resolve an image into a recognizable or known
    pattern. As for any machine learning algorithm, computer vision needs some features
    in order to learn patterns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了数字图像是如何表示的，接下来讨论计算机如何利用这些信息找到用于分类图像或定位物体的模式。因此，为了从图像中获取任何有用或可操作的信息，计算机必须将图像解析为可识别或已知的模式。与任何机器学习算法一样，计算机视觉需要一些特征来学习模式。
- en: Unlike structured data, where each feature is well defined in advance and stored
    in separate columns, images don't follow any specific pattern. It is impossible
    to say, for instance, that the third line will always contain the eye of an animal
    or that the bottom left corner will always represent a red, round-shaped object.
    Images can be of anything and don't follow any structure. This is why they are
    considered to be unstructured data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与结构化数据不同，结构化数据中的每个特征在预先定义并存储在不同的列中，而图像并没有遵循任何特定模式。例如，无法说第三行总是包含动物的眼睛，或者左下角总是表示一个红色的圆形物体。图像可以是任何东西，并不遵循任何结构。因此，它们被视为非结构化数据。
- en: However, images do contain features. They contain different shapes (lines, circles,
    rectangles, and so on), colors (red, blue, orange, yellow, and so on), and specific
    characteristics related to different types of objects (hair, wheel, leaves, and
    so on). Our eyes and brain can easily analyze and interpret all these features
    and identify objects in images. Therefore, we need to simulate the same analytical
    process for computers. This is where **image filters** (also called kernels) come
    into play.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，图像确实包含特征。它们包含不同的形状（线条、圆圈、矩形等）、颜色（红色、蓝色、橙色、黄色等）以及与不同类型物体相关的特定特征（头发、车轮、叶子等）。我们的眼睛和大脑可以轻松分析和解释所有这些特征，并识别图像中的对象。因此，我们需要为计算机模拟相同的分析过程。这就是**图像滤波器**（也称为卷积核）发挥作用的地方。
- en: 'Image filters are small matrices specialized in detecting a defined pattern.
    For instance, we can have a filter for detecting vertical lines only and another
    one only for horizontal lines. Computer vision systems run such filters in every
    part of the image and generate a new image with the detected patterns highlighted.
    These kinds of generated images are called **feature maps**. An example of a feature
    map where an edge-detection filter is used is shown in the following figure:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图像滤波器是专用于检测定义模式的小型矩阵。例如，我们可以有一个仅检测垂直线条的滤波器，另一个仅用于水平线条。计算机视觉系统在图像的每个部分运行这些滤波器，并生成一个新图像，突出显示检测到的模式。这类生成的图像称为**特征图**。使用边缘检测滤波器的特征图示例如下图所示：
- en: '![Figure 3.4: Example of a vertical edge feature map'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.4：垂直边缘特征图示例'
- en: '](img/B15385_03_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_04.jpg)'
- en: 'Figure 3.4: Example of a vertical edge feature map'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4：垂直边缘特征图示例
- en: Such filters are widely used in image processing. If you've used Adobe Photoshop
    before (or any other image processing tool), you will have most likely used filters
    such as *Gaussian* and *Sharpen*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波器广泛应用于图像处理中。如果您以前使用过Adobe Photoshop（或任何其他图像处理工具），您很可能已经使用过诸如*高斯*和*锐化*之类的滤镜。
- en: Convolution Operations
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积运算
- en: Now that we know the basics of image processing, we can start our journey with
    CNNs. As we mentioned previously, computer vision relies on applying filters to
    an image to recognize different patterns or features and generate feature maps.
    But how are these filters applied to the pixels of an image? You could guess that
    there is some sort of mathematical operation behind it, and you would be absolutely
    right. This operation is called convolution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了图像处理的基础知识，我们可以开始我们的CNN之旅。正如我们之前提到的，计算机视觉依赖于将滤波器应用于图像的像素，以识别不同的模式或特征并生成特征图。但是这些滤波器是如何应用到图像的像素上的呢？您可以猜测这背后有某种数学操作，您是完全正确的。这个操作被称为卷积。
- en: 'A convolution operation is composed of two stages:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作由两个阶段组成：
- en: An element-wise product of two matrices
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个矩阵的逐元素乘积
- en: A sum of the elements of a matrix
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵元素的总和
- en: 'Let''s look at an example of how to convolute two matrices, A and B:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个如何对矩阵A和B进行卷积的示例：
- en: '![Figure 3.5: Examples of matrices'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.5：矩阵示例'
- en: '](img/B15385_03_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_05.jpg)'
- en: 'Figure 3.5: Examples of matrices'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：矩阵示例
- en: 'First, we need to perform an element-wise multiplication with matrices A and
    B. We will get another matrix, C, as a result, with the following values:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要对矩阵A和B进行逐元素乘法。结果将得到另一个矩阵C，其数值如下：
- en: '1st row, 1st column: 5 × 1 = 5'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，第一列：5 × 1 = 5
- en: '1st row, 2nd column: 10 × 0 = 0'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，第二列：10 × 0 = 0
- en: '1st row, 3rd column: 15 × (-1) = -15'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一行，第三列：15 × (-1) = -15
- en: '2nd row, 1st column: 10 × 2 = 20'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行，第一列：10 × 2 = 20
- en: '2nd row, 2nd column: 20 × 0 = 0'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行，第二列：20 × 0 = 0
- en: '2nd row, 3rd column: 30 × (-2) = -60'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行，第三列：30 × (-2) = -60
- en: '3rd row, 1st column: 100 × 1 = 100'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行，第一列：100 × 1 = 100
- en: '3rd row, 2nd column: 150 × 0 = 0'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行，第二列：150 × 0 = 0
- en: '3rd row, 3rd column: 200 × (-1) = -200'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三行，第三列：200 × (-1) = -200
- en: Note
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: An element-wise multiplication is different from a standard matrix multiplication,
    which operates at the row and column level rather than on each element.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逐元素乘法与标准矩阵乘法不同，后者在行和列级别操作而不是在每个元素上操作。
- en: 'Finally, we just have to perform a sum on all elements of matrix C, which will
    give us the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们只需对矩阵C的所有元素进行求和，得到如下结果：
- en: 5+0-15+20+0-60+100+0-200 = -150
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 5+0-15+20+0-60+100+0-200 = -150
- en: 'The final result of the entire convolution operation on matrices A and B is
    -150, as shown in the following diagram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵A和B的整个卷积操作的最终结果如下图所示，为-150：
- en: '![Figure 3.6: Sequence of the convolution operation'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.6：卷积操作的顺序'
- en: '](img/B15385_03_06.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_06.jpg)'
- en: 'Figure 3.6: Sequence of the convolution operation'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：卷积操作的顺序
- en: In this example, Matrix B is actually a filter (or kernel) called Sobel that
    is used for detecting vertical lines (there is also a variant for horizontal lines).
    Matrix A will be a portion of an image with the same dimensions as the filter
    (this is mandatory in order to perform element-wise multiplication).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，矩阵B实际上是一个叫做Sobel的滤波器（或卷积核），用于检测垂直线条（还有一个变体用于检测水平线条）。矩阵A将是图像的一部分，其尺寸与滤波器相同（这是执行逐元素相乘所必须的）。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A filter is, in general, a square matrix such as (3,3) or (5,5).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 滤波器通常是一个方阵，例如(3,3)或(5,5)。
- en: For a CNN, filters are actually parameters that will be learned (that is, defined)
    during the training process. So, the values of each filter that will be used will
    be set by the CNN itself. This is an important concept to go through before we
    learn how to train a CNN.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CNN，滤波器实际上是训练过程中学习到的参数（即由CNN定义的）。因此，将要使用的每个滤波器的值将由CNN本身设置。这是我们在学习如何训练CNN之前需要了解的一个重要概念。
- en: 'Exercise 3.01: Implementing a Convolution Operation'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习3.01：实现卷积操作
- en: 'In this exercise, we will use TensorFlow to implement a convolution operation
    on two matrices: `[[1,2,3],[4,5,6],[7,8,9]]` and `[[1,0,-1],[1,0,-1],[1,0,-1]]`.
    Perform the following steps to complete this exercise:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用TensorFlow对两个矩阵：`[[1,2,3],[4,5,6],[7,8,9]]`和`[[1,0,-1],[1,0,-1],[1,0,-1]]`实现卷积操作。请按照以下步骤完成此练习：
- en: Open a new Jupyter Notebook file and name it `Exercise 3.01`.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件，并命名为`Exercise 3.01`。
- en: 'Import the `tensorflow` library:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`tensorflow`库：
- en: '[PRE0]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create a tensor called `A` from the first matrix, `([[1,2,3],[4,5,6],[7,8,9]])`.
    Print its value:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个矩阵`([[1,2,3],[4,5,6],[7,8,9]])`创建一个名为`A`的张量，并打印它的值：
- en: '[PRE1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will be as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE2]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a tensor called `B` from the first matrix, `([[1,0,-1],[1,0,-1],[1,0,-1]])`.
    Print its value:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个矩阵`([[1,0,-1],[1,0,-1],[1,0,-1]])`创建一个名为`B`的张量，并打印它的值：
- en: '[PRE3]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE4]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Perform an element-wise multiplication on `A` and `B` using `tf.math.multiply()`.
    Save the result in `mult_out` and print it:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.math.multiply()`对`A`和`B`进行逐元素相乘。将结果保存到`mult_out`并打印出来：
- en: '[PRE5]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The expected output will be as follows:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出将如下所示：
- en: '[PRE6]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Perform an element-wise sum on `mult_out` using `tf.math.reduce_sum()`. Save
    the result in `conv_out` and print it:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.math.reduce_sum()`对`mult_out`进行逐元素求和。将结果保存到`conv_out`并打印出来：
- en: '[PRE7]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The expected output will be as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期的输出将如下所示：
- en: '[PRE8]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The result of the convolution operation on the two matrices, `[[1,2,3],[4,5,6],[7,8,9]]`
    and `[[1,0,-1],[1,0,-1],[1,0,-1]]`, is `-6`.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对两个矩阵`[[1,2,3],[4,5,6],[7,8,9]]`和`[[1,0,-1],[1,0,-1],[1,0,-1]]`进行卷积操作的结果是`-6`。
- en: Note
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/320pEfC](https://packt.live/320pEfC).
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/320pEfC](https://packt.live/320pEfC)。
- en: You can also run this example online at [https://packt.live/2ZdeLFr](https://packt.live/2ZdeLFr).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2ZdeLFr](https://packt.live/2ZdeLFr)上在线运行此示例。你必须执行整个Notebook才能得到期望的结果。
- en: In this exercise, we used the built-in functions of TensorFlow to perform a
    convolution operation on two matrices.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们使用了TensorFlow的内置函数对两个矩阵进行了卷积操作。
- en: Stride
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步幅
- en: So far, we have learned how to perform a single convolution operation. We learned
    that a convolution operation uses a filter of a specific size, say, (3, 3), that
    is, 3 × 3, and applies it on a portion of the image of a similar size. If we have
    a large image, let's say of size (512, 512), then we can just look at a very tiny
    part of the image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学会了如何执行单次卷积操作。我们了解到，卷积操作使用一个特定大小的滤波器，例如(3, 3)，即3×3，并将其应用于相似大小的图像部分。如果我们有一个大图像，比如(512,
    512)大小，我们实际上只需要看图像的一个非常小的部分。
- en: 'Taking tiny parts of the image at a time, we need to perform the same convolution
    operation on the entire space of a given image. To do so, we will apply a technique
    called sliding. As the name implies, sliding is where we apply the filter to an
    adjacent area of the previous convolution operation: we just slide the filter
    and apply convolution.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次次处理图像的过程中，我们需要对整个图像空间进行相同的卷积操作。为此，我们将应用一个叫做滑动的技术。顾名思义，滑动就是将滤波器应用到上次卷积操作的相邻区域：我们只需要滑动滤波器并继续应用卷积。
- en: 'If we start from the top-left corner of an image, we can slide the filter by
    one pixel at a time to the right. Once we get to the right edge, we can slide
    down the filter by one pixel. We repeat this sliding operation until we''ve applied
    convolution to the entire space of the image:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从图像的左上角开始，我们可以每次将滤波器滑动一个像素到右边。当我们到达右边缘时，可以将滤波器向下滑动一个像素。我们重复这个滑动操作，直到对图像的整个区域应用卷积：
- en: '![Figure 3.7: Example of stride'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：步长示例'
- en: '](img/B15385_03_07.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_07.jpg)'
- en: 'Figure 3.7: Example of stride'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：步长示例
- en: Rather than sliding by 1 pixel only, we can choose a bigger sliding window,
    such as 2 or 3 pixels. The parameter defining the value of this sliding window
    is called **stride**. With a bigger stride value, there will be fewer overlapping
    pixels, but the resulting feature map will have smaller dimensions, so you will
    be losing a bit of information.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以每次滑动1个像素，还可以选择更大的滑动窗口，比如2个或3个像素。定义此滑动窗口大小的参数称为**步长（stride）**。步长值越大，重叠的像素就越少，但得到的特征图尺寸会更小，因此会丢失一些信息。
- en: In the preceding example, we applied a Sobel filter on an image that has been
    split horizontally with dark values on the left-hand side and white ones on the
    right-hand side. The resulting feature map has high values (800) in the middle,
    which indicates that the Sobel filter found a vertical line in that area. This
    is how sliding convolution helps to detect specific patterns in an image.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们在一幅水平分割的图像上应用了Sobel滤波器，左边有深色值，右边有白色值。得到的特征图中间有较高的值（800），这表示Sobel滤波器在该区域找到了垂直线。这就是滑动卷积如何帮助检测图像中特定模式的方式。
- en: Padding
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充
- en: In the previous section, we learned how a filter can go through all the pixels
    of an image with pixel sliding. Combined with the convolution operation, this
    process helps to detect patterns (that is, extract features) in an image.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了如何通过像素滑动使滤波器遍历图像的所有像素。结合卷积操作，这个过程有助于检测图像中的模式（即提取特征）。
- en: 'Applying a convolution to an image will result in a feature map that has smaller
    dimensions than the input image. A technique called padding can be used in order
    to get the exact same dimensions for the feature map as for the input image. It
    consists of adding a layer of pixels with a value of `0` to the edge:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像应用卷积将导致一个特征图，其尺寸比输入图像小。可以使用一种叫做填充（padding）的技术，确保特征图与输入图像具有相同的尺寸。它通过在图像边缘添加一个像素值为`0`的像素层来实现：
- en: '![Figure 3.8: Example of padding'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8：填充示例'
- en: '](img/B15385_03_08.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_08.jpg)'
- en: 'Figure 3.8: Example of padding'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：填充示例
- en: 'In the preceding example, the input image has the dimensions (6,6). Once padded,
    its dimensions increased to (8,8). Now, we can apply convolution on it with a
    filter of size (3,3):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，输入图像的尺寸是（6,6）。一旦填充，它的尺寸增加到（8,8）。现在，我们可以使用大小为（3,3）的滤波器在其上应用卷积：
- en: '![Figure 3.9: Example of padded convolution'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9：填充卷积示例'
- en: '](img/B15385_03_09.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_09.jpg)'
- en: 'Figure 3.9: Example of padded convolution'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：填充卷积示例
- en: The resulting image after convoluting the padded image is (6,6) in terms of
    its dimensions, which is the exact same dimensions as for the original input image.
    The resulting feature map has high values in the middle of the image, just like
    the previous example without padding. So, the filter can still find the same pattern
    in the image. But you may notice now that we have very low values (-800) on the
    left edge. This is actually fine as lower values mean the filter hasn't found
    any pattern in this area.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对填充图像进行卷积后的结果图像，其尺寸为（6,6），这与原始输入图像的尺寸完全相同。得到的特征图在图像的中间有较高的值，正如前面的未填充示例一样。因此，滤波器仍然可以在图像中找到相同的模式。但现在你可能会注意到，在左边缘有非常低的值（-800）。这实际上是可以接受的，因为较低的值意味着滤波器在该区域没有找到任何模式。
- en: 'The following formulas can be used for calculating the output dimensions of
    a feature map after a convolution:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式可以用来计算卷积后特征图的输出尺寸：
- en: '![Figure 3.10: Formulas for calculating the output dimensions of a feature
    map'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.10：计算特征图输出尺寸的公式'
- en: '](img/B15385_03_10.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_10.jpg)'
- en: 'Figure 3.10: Formulas for calculating the output dimensions of a feature map'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：计算特征图输出尺寸的公式
- en: 'Here, we have the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '`w`: Width of the input image'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w`：输入图像的宽度'
- en: '`h`: Height of the input image'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h`：输入图像的高度'
- en: '`p`: Number of pixels used on each side for padding'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p`：每个边缘用于填充的像素数量'
- en: '`f`: Filter size'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f`：滤波器大小'
- en: '`s`: Number of pixels in the stride'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s`：步幅中的像素数'
- en: 'Let''s apply this formula to the preceding example:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个公式应用到前面的例子中：
- en: '`w` = 6'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w` = 6'
- en: '`h` = 6'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h` = 6'
- en: '`p` = 1'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`p` = 1'
- en: '`f` = 3'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f` = 3'
- en: '`s` = 1'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`s` = 1'
- en: 'Then, calculate the output dimensions as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，按照以下方式计算输出维度：
- en: '![Figure 3.11: Output – dimensions of the feature map'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11：输出—特征图的维度'
- en: '](img/B15385_03_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_11.jpg)'
- en: 'Figure 3.11: Output – dimensions of the feature map'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：输出—特征图的维度
- en: So, the dimensions of the resulting feature map are (6,6).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，结果特征图的维度是（6，6）。
- en: Convolutional Neural Networks
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In *Chapter 2*, *Neural Networks*, you learned about traditional neural networks,
    such as perceptrons, that are composed of fully connected layers (also called
    dense layers). Each layer is composed of neurons that perform matrix multiplication,
    followed by a non-linear transformation with an activation function.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*，*神经网络*中，你学习了传统的神经网络，比如感知机，这些网络由全连接层（也叫稠密层）组成。每一层由执行矩阵乘法的神经元组成，然后进行非线性变换，使用激活函数。
- en: CNNs are actually very similar to traditional neural networks, but instead of
    using fully connected layers, they use convolutional layers. Each convolution
    layer will have a defined number of filters (or kernels) that will apply the convolution
    operation with a given stride on an input image with or without padding and can
    be followed by an activation function.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: CNN实际上与传统的神经网络非常相似，但它们使用的是卷积层，而不是全连接层。每个卷积层会有一定数量的滤波器（或称为核），这些滤波器会对输入图像执行卷积操作，步幅固定，可以选择是否填充，并且可以在后面跟上激活函数。
- en: CNNs are widely used for image classification, where the network will have to
    predict the right class for a given input. This is exactly the same as classification
    problems for traditional machine learning algorithms. If the output can only be
    from two different classes, it will be a **binary classification**, such as recognizing
    dogs versus cats. If the output can be more than two classes, it will be a **multi-class
    classification** exercise, such as recognizing 20 different sorts of fruits.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: CNN广泛应用于图像分类，在这种情况下，网络需要预测给定输入的正确类别。这与传统机器学习算法中的分类问题完全相同。如果输出只能来自两个不同的类别，那么就是**二分类**，例如识别狗与猫。如果输出可以是多个类别，那么就是**多分类**，例如识别20种不同的水果。
- en: 'In order to make such predictions, the last layer of a CNN model needs to be
    a fully connected layer with the relevant activation function according to the
    type of prediction problem. You can use the following list of activation functions
    as a rule of thumb:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这样的预测，CNN模型的最后一层需要是一个全连接层，并根据预测问题的类型使用相应的激活函数。你可以使用以下激活函数列表作为经验法则：
- en: '![Figure 3.12: List of activation functions'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.12：激活函数列表'
- en: '](img/B15385_03_12.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_12.jpg)'
- en: 'Figure 3.12: List of activation functions'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：激活函数列表
- en: 'To gain a better perspective of its structure, here''s what a simple CNN model
    looks like:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解其结构，下面是一个简单CNN模型的示意图：
- en: '![Figure 3.13: Structure of a simple CNN model'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.13：简单CNN模型的结构'
- en: '](img/B15385_03_13.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_13.jpg)'
- en: 'Figure 3.13: Structure of a simple CNN model'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：简单CNN模型的结构
- en: 'We have learned a lot about CNNs already. There is one more concept we need
    to go through in order to reduce the training time of a CNN before jumping into
    our first exercise: pooling layers.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到了很多关于卷积神经网络（CNN）的知识。在开始第一个练习之前，我们还需要了解一个概念，以便缩短CNN的训练时间：池化层。
- en: Pooling Layers
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: Pooling layers are used to reduce the dimensions of the feature maps of convolution
    layers. But why do we need to perform such downsampling? One of the main reasons
    is to reduce the number of calculations that are performed in the networks. Adding
    multiple layers of convolution with different filters can have a significant impact
    on the training time. Also, reducing the dimensions of feature maps can eliminate
    some of the noise in the feature map and help us focus only on the detected pattern.
    It is quite typical to add a pooling layer after each convolutional layer in order
    to reduce the size of the feature maps.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层用于减少卷积层特征图的维度。那么，为什么我们需要进行这种下采样呢？一个主要原因是减少网络中计算的数量。添加多层不同滤波器的卷积操作会显著影响训练时间。此外，减少特征图的维度可以消除一些噪声，帮助我们专注于检测到的模式。通常，我们会在每个卷积层后面添加池化层，以减少特征图的大小。
- en: 'A pooling operation acts very similarly to a filter, but rather than performing
    a convolution operation, it uses an aggregation function such as average or max
    (max is the most widely used function in the current CNN architecture). For instance,
    **max pooling** will look at a specific area of the feature map and find the maximum
    values of its pixels. Then, it will perform a stride and find the maximum value
    among the neighbor pixels. It will repeat this process until it processes the
    entire image:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作的作用与滤波器非常相似，但与卷积操作不同，它使用诸如平均值或最大值（最大值是当前CNN架构中最广泛使用的函数）等聚合函数。例如，**最大池化**会查看特征图的一个特定区域，并找出其像素的最大值。然后，它会执行一个步幅操作，找到邻近像素中的最大值。它会重复这个过程，直到处理完整个图像：
- en: '![Figure 3.14: Max pooling with stride 2 on an input image'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.14：在输入图像上使用步幅为 2 的最大池化'
- en: '](img/B15385_03_14.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_14.jpg)'
- en: 'Figure 3.14: Max pooling with stride 2 on an input image'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14：在输入图像上使用步幅为 2 的最大池化
- en: In the preceding example, we used a max pooling (which is the most widely used
    function for pooling) of size (2, 2) and a stride of 2\. We looked at the top-left
    corner of the feature map and found the maximum value among the pixels, 6, 8,
    1, and 2, and got a result of 8\. Then, we slid the max pooling by a stride of
    2 and performed the same operation on the group of pixels, that is, 6, 1, 7, and
    4\. We repeated the same operation on the bottom groups and got a new feature
    map of size (2,2).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，我们使用了大小为（2，2）、步幅为2的最大池化（这是最常用的池化函数）。我们查看特征图的左上角，并从像素值6、8、1和2中找出最大值，得到结果8。然后，我们按照步幅2滑动最大池化，针对像素值6、1、7和4执行相同的操作。我们重复对底部组的相同操作，得到了一个大小为（2，2）的新特征图。
- en: 'A CNN model with max pooling will look like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 带有最大池化的CNN模型将如下所示：
- en: '![Figure 3.15: Example of the CNN architecture with max pooling'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.15：带有最大池化的CNN架构示例'
- en: '](img/B15385_03_15.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_15.jpg)'
- en: 'Figure 3.15: Example of the CNN architecture with max pooling'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：带有最大池化的CNN架构示例
- en: For instance, the preceding model can be used for recognizing handwritten digits
    (from 0 to 9). There are three convolution layers in this model, followed by a
    max pooling layer. The final layers are fully connected and are responsible for
    making the predictions of the digit that's been detected.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，上述模型可用于识别手写数字（从 0 到 9）。这个模型有三层卷积层，后面跟着一个最大池化层。最后的几层是全连接层，负责做出检测到的数字的预测。
- en: The overhead of adding pooling layers is much less than computing convolution.
    This is why they will speed up the training time.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 添加池化层的开销远小于计算卷积。这就是它们能加速训练时间的原因。
- en: CNNs with TensorFlow and Keras
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 和 Keras 的 CNN
- en: So far, you've learned a lot about how CNN works under the hood. Now, it is
    finally time to see how we can implement what we have learned. We will be using
    the Keras API from TensorFlow 2.0.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了很多关于CNN如何在背后工作的信息。现在，终于到了展示如何实现我们所学的内容的时候了。我们将使用 TensorFlow 2.0
    中的 Keras API。
- en: The Keras API provides a high-level API for building your own CNN architecture.
    Let's look at the main classes we will be using for CNN.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API 提供了一个高层次的API，用于构建你自己的CNN架构。我们来看看我们将在CNN中使用的主要类。
- en: 'First, to create a convolution layer, we will need to instantiate a `Conv2D()`
    class and specify the number of kernels, their size, the stride, padding, and
    activation function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了创建一个卷积层，我们需要实例化一个`Conv2D()`类，并指定卷积核的数量、大小、步幅、填充方式和激活函数：
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the preceding example, we have created a convolution layer with `64` kernels
    that are `(3, 3)` in dimension with a stride of `2`, a padding to get the same
    output dimension as the input (`padding='same'`), and ReLU as the activation function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们创建了一个卷积层，使用了`64`个大小为`(3, 3)`的卷积核，步幅为`2`，填充方式为`same`，使得输出维度与输入维度相同，并且激活函数为ReLU。
- en: Note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about this class by going to TensorFlow''s documentation
    website: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 TensorFlow 的文档网站，了解更多关于这个类的信息：[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)
- en: 'In order to add a max pooling layer, you will have to use the `MaxPool2D()`
    class and specify its dimensions and stride, as shown in the following code snippet:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了添加一个最大池化层，你需要使用`MaxPool2D()`类，并指定其维度和步幅，如下所示的代码片段：
- en: '[PRE10]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In the preceding code snippet, we have instantiated a max pooling layer of size
    `(3,3)` with a stride of `1`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们实例化了一个大小为`(3,3)`，步幅为`1`的最大池化层。
- en: Note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about this class by going to TensorFlow''s documentation
    website: [https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 TensorFlow 的文档网站了解更多关于这个类的信息：[https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)
- en: 'For a fully connected layer, we will use the `Dense()` class and specify the
    number of units and the activation function:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于全连接层，我们将使用`Dense()`类并指定单元数量和激活函数：
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The preceding code shows us how to create a fully connected layer that has `1`
    output unit and uses `sigmoid` as the activation function.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码向我们展示了如何创建一个具有`1`个输出单元并使用`sigmoid`作为激活函数的全连接层。
- en: 'Finally, while manipulating input data, we may have to change its dimensions
    before feeding it to a CNN model. If we are using NumPy arrays, we can use the
    `reshape` method (as seen in *Chapter 1*, *Building Blocks of Deep Learning*),
    as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在操作输入数据时，我们可能需要在将其输入到 CNN 模型之前更改其维度。如果我们使用 NumPy 数组，可以使用`reshape`方法（如在*第1章*，*深度学习的构建模块*中所示），具体如下：
- en: '[PRE12]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we have transformed the dimension of `features_train` to `(60000, 28,
    28, 1)`, which corresponds to the format (number of observations, height, width,
    channel). This is needed when working with grayscale images to add the channel
    dimension. In this example, the dimensions of a grayscale image, `(28,28)`, will
    be reshaped to `(28,28,1)`, and there will be `60000` images in total.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经将`features_train`的维度转换为`(60000, 28, 28, 1)`，这对应于格式（观察数量，高度，宽度，通道）。在处理灰度图像时，需要添加通道维度。本例中，灰度图像的维度`(28,28)`将被重塑为`(28,28,1)`，总共有`60000`张图像。
- en: 'In TensorFlow, you can use the `reshape` method as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以使用`reshape`方法，如下所示：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have learned how to design a CNN in TensorFlow, it's time to put
    this all into practice on the famous MNIST dataset.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何在 TensorFlow 中设计一个 CNN，接下来是将这些知识应用于著名的 MNIST 数据集。
- en: Note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can learn more about Reshape by going to TensorFlow''s documentation website:
    [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 TensorFlow 的文档网站了解更多关于 Reshape 的信息：[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape)
- en: 'Exercise 3.02: Recognizing Handwritten Digits (MNIST) with CNN Using KERAS'
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.02：使用 KERAS 和 CNN 识别手写数字（MNIST）
- en: In this exercise, we will be working on the MNIST dataset (which we worked on
    in *Chapter 2, Neural Networks*), which contains images of handwritten digits.
    However, this time, we will be using a CNN model. This dataset was originally
    shared by Yann Lecun, one of the most renowned deep learning researchers. We will
    build a CNN model and then train it to recognize handwritten digits. The CNN will
    be composed of two layers of convolution with 64 kernels each, followed by two
    fully connected layers that have 128 and 10 units, respectively.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将处理 MNIST 数据集（在*第2章，神经网络*中我们已经用过它），该数据集包含手写数字的图像。不过这一次，我们将使用 CNN 模型。这个数据集最初由深度学习领域最著名的研究者之一
    Yann Lecun 分享。我们将构建一个 CNN 模型，并训练它识别手写数字。这个 CNN 模型将由两个卷积层组成，每个卷积层有 64 个卷积核，后面接着两个全连接层，分别包含
    128 和 10 个单元。
- en: 'TensorFlow provides this dataset directly from its API. Perform the following
    steps to complete this exercise:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 直接通过其 API 提供此数据集。执行以下步骤以完成此练习：
- en: Note
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about this dataset on TensorFlow''s website: [https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问 TensorFlow 的网站了解更多关于这个数据集的信息：[https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)
- en: Open a new Jupyter Notebook file and name it `Exercise 3.02`.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件，并将其命名为`Exercise 3.02`。
- en: 'Import `tensorflow.keras.datasets.mnist` as `mnist`:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`tensorflow.keras.datasets.mnist`为`mnist`：
- en: '[PRE14]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the `mnist` dataset using `mnist.load_data()` and save the results into
    `(features_train, label_train), (features_test, label_test)`:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mnist.load_data()`加载`mnist`数据集，并将结果保存到`(features_train, label_train), (features_test,
    label_test)`中：
- en: '[PRE15]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Print the content of `label_train`:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`label_train`的内容：
- en: '[PRE16]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The expected output will be as follows:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE17]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The label column contains numeric values that correspond to the 10 handwritten
    digits: `0` to `9`.'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签列包含与 10 个手写数字对应的数字值：`0` 到 `9`。
- en: 'Print the shape of the training set:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印训练集的形状：
- en: '[PRE18]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The expected output will be as follows:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下所示：
- en: '[PRE19]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The training set is composed of `60000` observations of shape `28` by `28`.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集由`60000`个形状为`28`×`28`的观测数据组成。
- en: 'Print the `shape` of the testing set:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印测试集的`shape`：
- en: '[PRE20]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The expected output will be as follows:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下所示：
- en: '[PRE21]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The testing set is composed of `10000` observations of shape `28` by `28`.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试集由`10000`个形状为`28`×`28`的观测数据组成。
- en: 'Reshape the training and testing sets with the dimensions `(number_observations,
    28, 28, 1)`:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集和测试集的形状调整为`(number_observations, 28, 28, 1)`：
- en: '[PRE22]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Standardize `features_train` and `features_test` by dividing them by `255`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`features_train`和`features_test`除以`255`来标准化它们：
- en: '[PRE23]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`作为`np`，`tensorflow`作为`tf`，并从`tensorflow.keras`导入`layers`：
- en: '[PRE24]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`, respectively:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.random_seed()`和`tf.random.set_seed()`分别将`8`设为`numpy`和`tensorflow`的种子：
- en: '[PRE25]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The results may still differ slightly after setting the seeds.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设置种子后，结果可能仍然会略有不同。
- en: 'Instantiate a `tf.keras.Sequential()` class and save it to a variable called
    `model`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.Sequential()`类，并将其保存到名为`model`的变量中：
- en: '[PRE26]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Instantiate a `layers.Conv2D()` class with `64` kernels of shape `(3,3)`, `activation=''relu''`,
    and `input_shape=(28,28,1)`, and save it to a variable called `conv_layer1`:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Conv2D()`类，设定`64`个形状为`(3,3)`的卷积核，`activation='relu'`，`input_shape=(28,28,1)`，并将其保存到名为`conv_layer1`的变量中：
- en: '[PRE27]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Instantiate a `layers.Conv2D()` class with `64` kernels of shape `(3,3)` and
    `activation=''relu''` and save it to a variable called `conv_layer2`:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Conv2D()`类，设定`64`个形状为`(3,3)`的卷积核，`activation='relu'`，并将其保存到名为`conv_layer2`的变量中：
- en: '[PRE28]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Instantiate a `layers.Flatten()` class with `128` neurons, `activation=''relu''`,
    and save it to a variable called `fc_layer1`:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Flatten()`类，设定`128`个神经元，`activation='relu'`，并将其保存到名为`fc_layer1`的变量中：
- en: '[PRE29]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Instantiate a `layers.Flatten()` class with `10` neurons, `activation=''softmax''`,
    and save it to a variable called `fc_layer2`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`layers.Flatten()`类，设定`10`个神经元，`activation='softmax'`，并将其保存到名为`fc_layer2`的变量中：
- en: '[PRE30]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add the four layers you just defined to the model using `.add()`, add a `MaxPooling2D()`
    layer of size `(2,2)` in between each of the convolution layers, and add a `Flatten()`
    layer before the first fully connected layer to flatten the feature maps:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.add()`将你刚刚定义的四个层添加到模型中，在每个卷积层之间添加一个大小为`(2,2)`的`MaxPooling2D()`层，并在第一个全连接层之前添加一个`Flatten()`层来展平特征图：
- en: '[PRE31]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.optimizers.Adam()`类，设置学习率为`0.001`，并将其保存到名为`optimizer`的变量中：
- en: '[PRE32]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.compile()`编译神经网络，设置`loss='sparse_categorical_crossentropy'`，`optimizer=optimizer`，`metrics=['accuracy']`：
- en: '[PRE33]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Print the summary of the model:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型摘要：
- en: '[PRE34]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The expected output will be as follows:'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下所示：
- en: '![Figure 3.16: Summary of the model'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.16：模型摘要'
- en: '](img/B15385_03_16.jpg)'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_03_16.jpg)'
- en: 'Figure 3.16: Summary of the model'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.16：模型摘要
- en: The preceding summary shows us that there are more than 240,000 parameters to
    be optimized with this model.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述摘要显示该模型有超过240,000个需要优化的参数。
- en: 'Fit the neural networks with the training set and specify `epochs=5`, `validation_split=0.2`,
    and `verbose=2`:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集拟合神经网络，并指定`epochs=5`，`validation_split=0.2`，`verbose=2`：
- en: '[PRE35]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The expected output will be as follows:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下所示：
- en: '![Figure 3.17: Training output'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.17：训练输出'
- en: '](img/B15385_03_17.jpg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_03_17.jpg)'
- en: 'Figure 3.17: Training output'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.17：训练输出
- en: We trained our CNN on 48,000 samples, and we used 12,000 samples as the validation
    set. After training for five epochs, we achieved an accuracy score of `0.9951`
    for the training set and `0.9886` for the validation set. Our model is overfitting
    a bit.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在48,000个样本上训练了我们的CNN，并使用了12,000个样本作为验证集。在训练了五个epoch后，我们在训练集上获得了`0.9951`的准确率，在验证集上获得了`0.9886`的准确率。我们的模型有些过拟合。
- en: 'Let''s evaluate the performance of the model on the testing set:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们评估模型在测试集上的表现：
- en: '[PRE36]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The expected output will be as follows:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出如下所示：
- en: '[PRE37]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: With this, we've achieved an accuracy score of `0.9903` on the testing set.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过这个，我们在测试集上达到了`0.9903`的准确率。
- en: Note
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2W2VLYl](https://packt.live/2W2VLYl).
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2W2VLYl](https://packt.live/2W2VLYl)。
- en: You can also run this example online at [https://packt.live/3iKAVGZ](https://packt.live/3iKAVGZ).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以在 [https://packt.live/3iKAVGZ](https://packt.live/3iKAVGZ) 上在线运行此示例。您必须执行整个
    Notebook 才能获得期望的结果。
- en: In this exercise, we designed and trained a CNN architecture to recognize the
    images of handwritten digit images from the MNIST dataset and achieved an almost
    perfect score.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们设计并训练了一个 CNN 架构，用于识别来自 MNIST 数据集的手写数字图像，并取得了几乎完美的成绩。
- en: Data Generator
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成器
- en: In the previous exercise, we built our first multi-class CNN classifier on the
    MNIST dataset. We loaded the entire dataset into the model as it wasn't very big.
    But for bigger datasets, we will not be able to do this. Thankfully, Keras provides
    an API called **data generator** that we can use to load and transform data in
    batches.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习中，我们在 MNIST 数据集上构建了第一个多类 CNN 分类器。我们将整个数据集加载到模型中，因为数据集并不大。但是对于更大的数据集，我们无法这样做。幸运的是，Keras
    提供了一个名为 **data generator** 的 API，允许我们以批次的方式加载和转换数据。
- en: Data generators are also very useful for image classification. Sometimes, an
    image dataset comes in the form of a folder with predefined structures for the
    training and testing sets and for the different classes (all images that belong
    to a class will be stored in the same folder). The data generator API will be
    able to understand this structure and feed the CNN model properly with the relevant
    images and corresponding information. This will save you a lot of time as you
    won't need to build a custom pipeline to load images from the different folders.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成器在图像分类中也非常有用。有时，图像数据集以文件夹的形式存在，并且为训练集、测试集和不同的类别预定义了结构（属于同一类别的所有图像将存储在同一个文件夹中）。数据生成器
    API 能够理解这种结构，并将相关图像和对应的信息正确地提供给 CNN 模型。这将为您节省大量时间，因为您不需要构建自定义管道来从不同的文件夹加载图像。
- en: On top of this, data generators can divide the images into batches of images
    and feed them sequentially to the model. You don't have to load the entire dataset
    into memory in order to perform training. Let's see how they work.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据生成器可以将图像划分为多个批次，并按顺序将它们提供给模型。您不必将整个数据集加载到内存中就可以进行训练。让我们看看它们是如何工作的。
- en: 'First, we need to import the `ImageDataGenerator` class from `tensorflow.keras.preprocessing`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从 `tensorflow.keras.preprocessing` 中导入 `ImageDataGenerator` 类：
- en: '[PRE38]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we can instantiate it by providing all the image transformations we want
    it to perform. In the following example, we will just normalize all the images
    from the training set by dividing them by `255` so that all the pixels will have
    a value between `0` and `1`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过提供所有想要执行的图像转换来实例化它。在以下示例中，我们将仅通过将所有训练集图像除以 `255` 来对图像进行归一化，这样所有像素值都将在
    `0` 到 `1` 之间：
- en: '[PRE39]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'After this step, we will create a data generator by using the `.flow_from_directory()`
    method and will specify the path to the training directory, `batch_size`, the
    `target_size` of the image, the shuffle, and the type of class:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步骤中，我们将通过使用 `.flow_from_directory()` 方法来创建一个数据生成器，并指定训练目录的路径、`batch_size`、图像的
    `target_size`、是否打乱数据以及类别类型：
- en: '[PRE40]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You need to create a separate data generator for the validation set.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要为验证集创建一个单独的数据生成器。
- en: 'Finally, we can train our model using the `.fit_generator()` method by providing
    the data generators for the training and validation sets, the number of epochs,
    and the number of steps per epoch, which corresponds to the number of images divided
    by the batch size (as integer):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过提供训练集和验证集的数据生成器、训练周期数和每个周期的步数来使用 `.fit_generator()` 方法训练我们的模型，步数是图像总数除以批次大小（取整）的结果：
- en: '[PRE41]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This method is very similar to the `.fit()` method you saw earlier, but rather
    than training the CNN on the entire dataset in one go, it will train by batches
    of images using the data generator we defined. The number of steps defines how
    many batches will be required to process the entire dataset.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法与您之前看到的 `.fit()` 方法非常相似，但不同之处在于，它不是一次性将整个数据集训练完，而是通过我们定义的数据生成器按批次训练图像。步数定义了处理整个数据集所需的批次数量。
- en: Data generators are quite useful for loading data from folders and feeding the
    model in batches of images. But they can also perform some data processing, as
    shown in the following section.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成器非常适合从文件夹中加载数据，并按批次将图像输入到模型中。但它们也可以执行一些数据处理操作，如以下章节所示。
- en: 'Exercise 3.03: Classifying Cats versus Dogs with Data Generators'
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.03：使用数据生成器分类猫狗图像
- en: 'In this exercise, we will be working on the cats versus dogs dataset, which
    contains images of dogs and cats. We will build two data generators for the training
    and validation sets and a CNN model to recognize images of dogs or cats. Perform
    the following steps to complete this exercise:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用猫狗数据集，该数据集包含狗和猫的图像。我们将为训练集和验证集构建两个数据生成器，并构建一个CNN模型来识别狗或猫的图像。请按照以下步骤完成本练习：
- en: Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset we''ll be using is a modified version from the Kaggle cats versus
    dogs dataset: [https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data).The
    modified version, which only uses a subset of 25,000 images, has been provided
    by Google at [https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的数据集是Kaggle猫狗数据集的修改版：[https://www.kaggle.com/c/dogs-vs-cats/data](https://www.kaggle.com/c/dogs-vs-cats/data)。这个修改版只使用了25,000张图像的子集，并由Google提供，链接为[https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip](https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip)。
- en: Open a new Jupyter Notebook file and name it `Exercise 3.03`.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook文件，并将其命名为`Exercise 3.03`：
- en: 'Import the `tensorflow` library:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`tensorflow`库：
- en: '[PRE42]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create a variable called `file_url` containing the link to the dataset:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`file_url`的变量，包含数据集的链接：
- en: '[PRE43]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: In the aforementioned step, we are using the dataset stored at [https://packt.live/3jZKRNw](https://packt.live/3jZKRNw).
    If you have stored the dataset at any other URL, please change the highlighted
    path accordingly. Watch out for the slashes in the string below. Remember that
    the backslashes ( `\` ) are used to split the code across multiple lines, while
    the forward slashes ( `/` ) are part of the URL.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述步骤中，我们使用的是存储在[https://packt.live/3jZKRNw](https://packt.live/3jZKRNw)的数据库。如果您将数据集存储在其他URL，请相应地更改高亮的路径。注意下面字符串中的斜杠。记住，反斜杠(`\`)用于跨多行分隔代码，而正斜杠(`/`)是URL的一部分。
- en: 'Download the dataset using `tf.keras.get_file` with `''cats_and_dogs.zip'',
    origin=file_url, extract=True` as parameters and save the result to a variable
    called `zip_dir`:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.keras.get_file`下载数据集，参数为`'cats_and_dogs.zip'`、`origin=file_url`、`extract=True`，并将结果保存到一个名为`zip_dir`的变量中：
- en: '[PRE44]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Import the `pathlib` library:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pathlib`库：
- en: '[PRE45]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Create a variable called `path` containing the full path to the `cats_and_dogs_filtered`
    directory using `pathlib.Path(zip_dir).parent`:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`path`的变量，使用`pathlib.Path(zip_dir).parent`获取`cats_and_dogs_filtered`目录的完整路径：
- en: '[PRE46]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Create two variables called `train_dir` and `validation_dir` that take the
    full paths to the train and validation folders, respectively:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个变量，分别命名为`train_dir`和`validation_dir`，它们分别保存训练集和验证集文件夹的完整路径：
- en: '[PRE47]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create four variables called `train_cats_dir`, `train_dogs_dir`, `validation_cats_dir`,
    and `validation_dogs_dir` that take the full paths to the cats and dogs folders
    for the train and validation sets, respectively:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建四个变量，分别命名为`train_cats_dir`、`train_dogs_dir`、`validation_cats_dir`和`validation_dogs_dir`，它们分别保存训练集和验证集中猫狗文件夹的完整路径：
- en: '[PRE48]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Import the `os` package. We will need this in the next step in order to count
    the number of images from a folder:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`os`包。我们将在下一步中需要它来统计文件夹中的图像数量：
- en: '[PRE49]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create two variables called `total_train` and `total_val` that will get the
    number of images for the training and validation sets:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个变量，分别命名为`total_train`和`total_val`，它们将获取训练集和验证集中的图像数量：
- en: '[PRE50]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tensorflow.keras.preprocessing`导入`ImageDataGenerator`：
- en: '[PRE51]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Instantiate two `ImageDataGenerator` classes and call them `train_image_generator`
    and `validation_image_generator`. These will rescale the images by dividing them
    by `255`:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化两个`ImageDataGenerator`类并命名为`train_image_generator`和`validation_image_generator`。这两个生成器将通过将图像像素值除以`255`来进行重缩放：
- en: '[PRE52]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Create three variables called `batch_size`, `img_height`, and `img_width` that
    take the values `16`, `100`, and `100`, respectively:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个变量，分别命名为`batch_size`、`img_height`和`img_width`，它们的值分别为`16`、`100`和`100`：
- en: '[PRE53]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create a data generator called `train_data_gen` using `.flow_from_directory()`
    and specify the batch size, the path to the training folder, `shuffle=True`, the
    target size as `(img_height, img_width)`, and the class mode as `binary`:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.flow_from_directory()`创建一个名为`train_data_gen`的数据生成器，并指定批量大小、训练文件夹的路径、`shuffle=True`、目标大小为`(img_height,
    img_width)`，并将类模式设置为`binary`：
- en: '[PRE54]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Create a data generator called `val_data_gen` using `.flow_from_directory()`
    and specify the batch size, paths to the validation folder, `shuffle=True`, the
    target size as `(img_height, img_width)`, and the class mode as `binary`:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.flow_from_directory()`创建一个名为`val_data_gen`的数据生成器，并指定批量大小、验证文件夹的路径、`shuffle=True`、目标大小为`(img_height,
    img_width)`，以及类别模式为`binary`：
- en: '[PRE55]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`为`np`，`tensorflow`为`tf`，以及从`tensorflow.keras`导入`layers`：
- en: '[PRE56]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Set `8` (this is totally arbitrary) as the `seed` for `numpy` and `tensorflow`
    using `np.random_seed()` and `tf.random.set_seed()`, respectively:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.random_seed()`和`tf.random.set_seed()`分别将`8`（这个值完全是任意的）设置为`numpy`和`tensorflow`的`seed`：
- en: '[PRE57]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Instantiate a `tf.keras.Sequential()` class into a variable called `model`
    with the following layers: A convolution layer with `64` kernels of shape `3`,
    `ReLU` as the activation function, and the required input dimensions; a max pooling
    layer; a convolution layer with `128` kernels of shape `3` and `ReLU` as the activation
    function; a max pooling layer; a flatten layer; a fully connected layer with `128`
    units and `ReLU` as the activation function; a fully connected layer with `1`
    unit and `sigmoid` as the activation function.'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.Sequential()`类到一个名为`model`的变量中，包含以下层：一个具有`64`个`3`形状卷积核的卷积层，`ReLU`作为激活函数，并指定所需的输入维度；一个最大池化层；一个具有`128`个`3`形状卷积核的卷积层，`ReLU`作为激活函数；一个最大池化层；一个展平层；一个具有`128`个单元的全连接层，`ReLU`作为激活函数；一个具有`1`个单元的全连接层，`sigmoid`作为激活函数。
- en: 'The code will look as follows:'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE58]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`tf.keras.optimizers.Adam()`类，学习率为`0.001`，并将其保存为名为`optimizer`的变量：
- en: '[PRE59]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compile the neural network using `.compile()` with `loss=''binary_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.compile()`编译神经网络，设置`loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']`：
- en: '[PRE60]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Print a summary of the model using `.summary()`:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.summary()`打印模型摘要：
- en: '[PRE61]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The expected output will be as follows:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 3.18: Summary of the model'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.18：模型总结'
- en: '](img/B15385_03_18.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_03_18.jpg)'
- en: 'Figure 3.18: Summary of the model'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.18：模型总结
- en: The preceding summary shows us that there are more than `8,700,000` parameters
    to be optimized with this model.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的摘要显示该模型有超过`8,700,000`个参数需要优化。
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit_generator()`训练神经网络，并提供训练和验证数据生成器，`epochs=5`，每个epoch的步骤数，以及验证步骤数：
- en: '[PRE62]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The expected output will be as follows:'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 3.19: Training output'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.19：训练输出'
- en: '](img/B15385_03_19.jpg)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_03_19.jpg)'
- en: 'Figure 3.19: Training output'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.19：训练输出
- en: Note
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The expected output will be close to the one shown. You may have slightly different
    accuracy values due to some randomness in weights initialization.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出将接近所示的结果。由于权重初始化时存在一些随机性，你的准确度值可能会有所不同。
- en: We've trained our CNN for five epochs and achieved an accuracy score of `0.85`
    for the training set, and `0.7113` for the validation set. Our model is overfitting
    quite a lot. You may want to try the training with different architectures to
    see whether you can improve this score and reduce overfitting. You can also try
    feeding this model with some images of cats or dogs of your choice and see the
    output predictions.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了我们的CNN五个epoch，训练集的准确率为`0.85`，验证集的准确率为`0.7113`。我们的模型过拟合严重。你可能需要尝试不同的架构进行训练，看看是否能够提高这个分数并减少过拟合。你还可以尝试给模型输入你选择的猫狗图片，并查看输出的预测结果。
- en: Note
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31XQmp9](https://packt.live/31XQmp9).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/31XQmp9](https://packt.live/31XQmp9)。
- en: You can also run this example online at [https://packt.live/2ZW10tW](https://packt.live/2ZW10tW).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个例子，网址是[https://packt.live/2ZW10tW](https://packt.live/2ZW10tW)。你必须执行整个Notebook才能得到预期的结果。
- en: Data Augmentation
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: 'In the previous section, you were introduced to data generators that can do
    a lot of the heavy lifting, such as feeding the model from folders rather than
    columnar data for you regarding data processing for neural networks. So far, we
    have seen how to create them, load data from a structured folder, and feed the
    model by batch. We only performed one image transformation with it: rescaling.
    However, data generators can perform many more image transformations.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，您已经了解到数据生成器可以完成大量繁重的工作，例如从文件夹而不是列数据中为神经网络处理数据。到目前为止，我们已经看到了如何创建它们，从结构化文件夹加载数据，并按批次将数据提供给模型。我们仅对其执行了一个图像转换：重新缩放。然而，数据生成器可以执行许多其他图像转换。
- en: 'But why do we need to perform data augmentation? The answer is quite simple:
    to prevent overfitting. By performing data augmentation, we are increasing the
    number of images in a dataset. For one image, we can generate, for instance, 10
    different variants of the same image. So, the size of your dataset will be multiplied
    by 10\.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么我们需要执行数据增强呢？答案非常简单：为了防止过拟合。通过进行数据增强，我们增加了数据集中的图像数量。例如，对于一张图像，我们可以生成 10
    种不同的变体。因此，您的数据集大小将增加 10 倍。
- en: 'Also, with data augmentation, we have a set of images with a broader range
    of visuals. For example, selfie pictures can be taken from different angles, but
    if your dataset only contains selfie pictures that are straight in terms of their
    orientation, your CNN model will not be able to interpret other images with different
    angles correctly. By performing data augmentation, you are helping your model
    generalize better to different types of images. However, as you may have guessed,
    there is one drawback: data augmentation will also increase the training time
    as you have to perform additional data transformations.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过数据增强，我们拥有一组具有更广泛视觉范围的图像。例如，自拍照可以从不同角度拍摄，但是如果您的数据集只包含方向直的自拍照片，您的 CNN 模型将无法正确解释具有不同角度的其他图像。通过执行数据增强，您帮助模型更好地泛化到不同类型的图像。然而，正如您可能已经猜到的那样，它也有一个缺点：数据增强会增加训练时间，因为您需要执行额外的数据转换。
- en: Let's take a quick look at some of the different types of data argumentation
    that we can do.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下我们可以做的一些不同类型的数据增强。
- en: Horizontal Flipping
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平翻转
- en: 'Horizontal flipping returns an image that is flipped horizontally:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 水平翻转会返回一个水平翻转的图像：
- en: '![Figure 3.20: Example of horizontal flipping'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20: 水平翻转示例'
- en: '](img/B15385_03_20.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_20.jpg)'
- en: 'Figure 3.20: Example of horizontal flipping'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.20: 水平翻转示例'
- en: Vertical Flipping
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垂直翻转
- en: 'Vertical flipping will flip an image vertically:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直翻转会垂直翻转图像：
- en: '![Figure 3.21: Example of vertical flipping'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.21: 垂直翻转示例'
- en: '](img/B15385_03_21.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_21.jpg)'
- en: 'Figure 3.21: Example of vertical flipping'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.21: 垂直翻转示例'
- en: Zooming
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放
- en: 'An image can be zoomed in and provide different sizes of objects in the image:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可以被放大，并提供不同大小的图像对象：
- en: '![Figure 3.22: Example of zooming'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.22: 缩放示例'
- en: '](img/B15385_03_22.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_22.jpg)'
- en: 'Figure 3.22: Example of zooming'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.22: 缩放示例'
- en: Horizontal Shifting
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平移动
- en: 'Horizontal shifting, as its name implies, will shift the image along the horizontal
    axis but keep it the same size. With this transformation, the image may be cropped,
    and new pixels need to be generated to fill the void. A common technique is to
    copy the neighboring pixels or to fill that space with black pixels:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 水平移动，顾名思义，将在水平轴上移动图像，但保持图像尺寸不变。通过这种转换，图像可能会被裁剪，需要生成新像素来填补空白。常见的技术是复制相邻像素或用黑色像素填充该空间：
- en: '![Figure 3.23: Example of horizontal shifting'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.23: 水平移动示例'
- en: '](img/B15385_03_23.jpg)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_23.jpg)'
- en: 'Figure 3.23: Example of horizontal shifting'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.23: 水平移动示例'
- en: Vertical Shifting
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垂直移动
- en: 'Vertical shifting is similar to horizontal shifting, but along the vertical
    axis:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直移动与水平移动类似，但是沿垂直轴：
- en: '![Figure 3.24: Example of vertical shifting'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.24: 垂直移动示例'
- en: '](img/B15385_03_24.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_24.jpg)'
- en: 'Figure 3.24: Example of vertical shifting'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.24: 垂直移动示例'
- en: Rotating
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 旋转
- en: 'A rotation with a particular angle can be performed on an image like so:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图像可以像这样以特定角度旋转：
- en: '![Figure 3.25: Example of rotating'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25: 旋转示例'
- en: '](img/B15385_03_25.jpg)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_25.jpg)'
- en: 'Figure 3.25: Example of rotating'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.25: 旋转示例'
- en: Shearing
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 剪切
- en: 'Shearing transforms the image by moving one of the edges along the axis of
    the edge. After doing this, the image distorts from a rectangle to a parallelogram:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切变换是通过沿边缘轴移动图像的一边来实现的。执行此操作后，图像会从矩形变形为平行四边形：
- en: '![Figure 3.26: Example of shearing'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.26：剪切示例'
- en: '](img/B15385_03_26.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_26.jpg)'
- en: 'Figure 3.26: Example of shearing'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.26：剪切示例
- en: 'With `Keras`, all these data transformation techniques can be added to `ImageDataGenerator`:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Keras`，所有这些数据变换技术都可以添加到 `ImageDataGenerator` 中：
- en: '[PRE63]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now that we have a general understanding of data argumentation, let's look at
    how to implement it in our models in the following exercise.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据增强有了大致的了解，接下来让我们看看如何在以下练习中将其应用到模型中。
- en: 'Exercise 3.04: Image Classification (CIFAR-10) with Data Augmentation'
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 3.04：使用数据增强进行图像分类（CIFAR-10）
- en: 'In this exercise, we will be working on the CIFAR-10 dataset (Canadian Institute
    for Advanced Research), which is composed of 60,000 images of 10 different classes:
    airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We
    will build a CNN model and use data augmentation to recognize these categories.
    Perform the following steps to complete this exercise:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 CIFAR-10 数据集（加拿大高级研究院数据集），该数据集包含了 60,000 张属于 10 个不同类别的图像：飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船和卡车。我们将构建一个
    CNN 模型并使用数据增强来识别这些类别。请按照以下步骤完成这个练习：
- en: Note
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more about this dataset on TensorFlow''s website: [https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 TensorFlow 的官方网站上了解更多关于此数据集的信息：[https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10)。
- en: Open a new Jupyter Notebook file and name it `Exercise 3.04`.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 文件，并将其命名为 `Exercise 3.04`。
- en: 'Import `tensorflow.keras.datasets.cifar10`:'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `tensorflow.keras.datasets.cifar10`：
- en: '[PRE64]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the CIFAR-10 dataset using `cifar10.load_data()` and save the results
    to `(features_train, label_train), (features_test, label_test)`:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `cifar10.load_data()` 加载 CIFAR-10 数据集，并将结果保存到 `(features_train, label_train),
    (features_test, label_test)`：
- en: '[PRE65]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Print the shape of `features_train`:'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `features_train` 的形状：
- en: '[PRE66]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The expected output will be as follows:'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '[PRE67]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The training set is composed of `50000` images that have the dimensions `(32,32,3)`.
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集包含了 `50000` 张尺寸为 `(32,32,3)` 的图像。
- en: 'Create three variables called `batch_size`, `img_height`, and `img_width` that
    take the values `16`, `32`, and `32`, respectively:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建三个变量，分别命名为 `batch_size`、`img_height` 和 `img_width`，并将它们的值分别设置为 `16`、`32` 和
    `32`：
- en: '[PRE68]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Import `ImageDataGenerator` from `tensorflow.keras.preprocessing`:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `tensorflow.keras.preprocessing` 导入 `ImageDataGenerator`：
- en: '[PRE69]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Create an `ImageDataGenerator` instance called `train_img_gen` with data augmentation:
    rescaling (by dividing by 255), `width_shift_range=0.1`, `height_shift_range=0.1`,
    and horizontal flipping:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `train_img_gen` 的 `ImageDataGenerator` 实例，并应用数据增强：重缩放（除以 255）、`width_shift_range=0.1`、`height_shift_range=0.1`
    以及水平翻转：
- en: '[PRE70]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Create an `ImageDataGenerator` instance called `val_img_gen` with rescaling
    (by dividing by 255):'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `val_img_gen` 的 `ImageDataGenerator` 实例，并应用重缩放（除以 255）：
- en: '[PRE71]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Create a data generator called `train_data_gen` using the `.flow()` method
    and specify the batch size, features, and labels from the training set:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.flow()` 方法创建一个名为 `train_data_gen` 的数据生成器，并指定训练集的批量大小、特征和标签：
- en: '[PRE72]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Create a data generator called `val_data_gen` using the `.flow()` method and
    specify the batch size, features, and labels from the testing set:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.flow()` 方法创建一个名为 `val_data_gen` 的数据生成器，并指定测试集的批量大小、特征和标签：
- en: '[PRE73]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Import `numpy` as `np`, `tensorflow` as `tf`, and `layers` from `tensorflow.keras`:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 为 `np`，`tensorflow` 为 `tf`，以及从 `tensorflow.keras` 导入 `layers`：
- en: '[PRE74]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Set `8` as the seed for `numpy` and `tensorflow` using `np.random_seed()` and
    `tf.random.set_seed()`:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `np.random_seed()` 和 `tf.random.set_seed()` 设置 `8` 作为 `numpy` 和 `tensorflow`
    的随机种子：
- en: '[PRE75]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Instantiate a `tf.keras.Sequential()` class into a variable called `model`
    with the following layers: a convolution layer with `64` kernels of shape `3`,
    ReLU as the activation function, and the necessary input dimensions; a max pooling
    layer; a convolution layer with `128` kernels of shape `3` and ReLU as the activation
    function; a max pooling layer; a flatten layer; a fully connected layer with `128`
    units and ReLU as the activation function; a fully connected layer with `10` units
    and Softmax as the activation function.'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 `tf.keras.Sequential()` 类，并将其赋值给名为 `model` 的变量，使用以下层：一个具有 `64` 个 `3` 核心的卷积层，ReLU
    激活函数，以及必要的输入维度；一个最大池化层；一个具有 `128` 个 `3` 核心的卷积层，ReLU 激活函数；一个最大池化层；一个展平层；一个具有 `128`
    单元和 ReLU 激活函数的全连接层；一个具有 `10` 单元和 Softmax 激活函数的全连接层。
- en: 'The code will be as follows:'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码如下所示：
- en: '[PRE76]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Instantiate a `tf.keras.optimizers.Adam()` class with `0.001` as the learning
    rate and save it to a variable called `optimizer`:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 `tf.keras.optimizers.Adam()` 类，设置学习率为 `0.001`，并将其保存为名为 `optimizer` 的变量：
- en: '[PRE77]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Compile the neural network using `.compile()` with `loss=''sparse_categorical_crossentropy'',
    optimizer=optimizer, metrics=[''accuracy'']`:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.compile()` 编译神经网络，参数为 `loss='sparse_categorical_crossentropy', optimizer=optimizer,
    metrics=['accuracy']`：
- en: '[PRE78]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Fit the neural networks with `fit_generator()` and provide the train and validation
    data generators, `epochs=5`, the steps per epoch, and the validation steps:'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `fit_generator()` 训练神经网络，并提供训练和验证数据生成器，`epochs=5`，每个 epoch 的步数，以及验证步数：
- en: '[PRE79]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The expected output will be as follows:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出将如下所示：
- en: '![Figure 3.27: Training logs for the model'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.27：模型的训练日志](img/B15385_03_27.jpg)'
- en: '](img/B15385_03_27.jpg)'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_03_27.jpg)'
- en: 'Figure 3.27: Training logs for the model'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.27：模型的训练日志
- en: Note
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZLyQk](https://packt.live/31ZLyQk).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考 [https://packt.live/31ZLyQk](https://packt.live/31ZLyQk)。
- en: You can also run this example online at [https://packt.live/2OcmahS](https://packt.live/2OcmahS).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/2OcmahS](https://packt.live/2OcmahS)。你必须执行整个
    Notebook 才能获得预期的结果。
- en: In this exercise, we trained our CNN on five epochs, and we achieved an accuracy
    score of `0.6713` on the training set and `0.6582` on the validation set. Our
    model is overfitting slightly, but its accuracy score is quite low. You may wish
    to try this on different architectures to see whether you can improve this score
    by, for instance, adding more convolution layers.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们在 5 个 epoch 上训练了我们的 CNN，并在训练集上获得了 `0.6713` 的准确度，在验证集上获得了 `0.6582` 的准确度。我们的模型略微过拟合，但准确度相当低。你可能希望尝试不同的架构，看看是否可以通过例如增加卷积层来提高这个分数。
- en: Note
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The expected output for the preceding exercise will be close to the one shown
    (Figure 3.27). You may have slightly different accuracy values due to some randomness
    in weights initialization.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 上述练习的预期输出将接近于图示（图 3.27）。由于权重初始化中的一些随机性，你可能会看到略有不同的准确度值。
- en: 'Activity 3.01: Building a Multiclass Classifier Based on the Fashion MNIST Dataset'
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.01：基于 Fashion MNIST 数据集构建多类分类器
- en: 'In this activity, you will train a CNN to recognize images of clothing that
    belong to 10 different classes. You will apply some data augmentation techniques
    to reduce the risk of overfitting. You will be using the Fashion MNIST dataset
    provided by TensorFlow. Perform the following steps to complete this activity:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将训练一个卷积神经网络（CNN）来识别属于 10 个不同类别的服装图像。你将应用一些数据增强技术来减少过拟合的风险。你将使用由 TensorFlow
    提供的 Fashion MNIST 数据集。请执行以下步骤来完成该活动：
- en: Note
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The original dataset was shared by Han Xiao. You can read more about this dataset
    on TensorFlow''s website here: [https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集由 Han Xiao 分享。你可以在 TensorFlow 网站上阅读更多关于该数据集的信息：[https://www.tensorflow.org/datasets/catalog/mnist](https://www.tensorflow.org/datasets/catalog/mnist)
- en: Import the Fashion MNIST dataset from TensorFlow.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 TensorFlow 导入 Fashion MNIST 数据集。
- en: Reshape the training and testing sets.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑训练集和测试集。
- en: 'Create a data generator with the following data augmentation:'
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据生成器，并应用以下数据增强：
- en: '[PRE80]'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Create the neural network architecture with the following layers: A convolutional
    layer with `Conv2D(64, (3,3), activation=''relu'')` followed by `MaxPooling2D(2,2)`;
    a convolutional layer with `Conv2D(64, (3,3), activation=''relu'')` followed by
    `MaxPooling2D(2,2)`; a flatten layer; a fully connected layer with `Dense(128,
    activation=relu)`; a fully connected layer with `Dense(10, activation=''softmax'')`.'
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络架构，包含以下层次：一个卷积层`Conv2D(64, (3,3), activation='relu')`，后接`MaxPooling2D(2,2)`；一个卷积层`Conv2D(64,
    (3,3), activation='relu')`，后接`MaxPooling2D(2,2)`；一个展平层；一个全连接层`Dense(128, activation=relu)`；一个全连接层`Dense(10,
    activation='softmax')`。
- en: Specify an Adam optimizer with a learning rate of `0.001`.
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个学习率为`0.001`的 Adam 优化器。
- en: Train the model.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate the model on the testing set.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型。
- en: 'The expected output will be as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出如下：
- en: '![Figure 3.28: Training logs for the model'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.28：模型的训练日志'
- en: '](img/B15385_03_28.jpg)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_28.jpg)'
- en: 'Figure 3.28: Training logs for the model'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.28：模型的训练日志
- en: The expected accuracy scores should be around `0.82` for the training and validation sets.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和验证集的预期准确率应该在`0.82`左右。
- en: Note
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 394.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤、解决方案及附加评论会在第394页展示。
- en: Saving and Restoring Models
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和恢复模型
- en: In the previous section, we learned how we can use data augmentation to generate
    different variants of an image. This will increase the size of the dataset but
    will also help the model train on a wider variety of images and help it generalize
    better.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用数据增强技术生成图像的不同变体。这将增加数据集的大小，但也有助于模型在更多样化的图像上训练，并帮助它更好地泛化。
- en: Once you've trained your model, you will most likely want to deploy it in production
    and use it to make live predictions. To do so, you will need to save your model
    as a file. This file can then be loaded by your prediction service so that it
    can be used as an API or data science tool.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练完模型，你很可能希望将其部署到生产环境中并使用它进行实时预测。为此，你需要将模型保存为文件。然后，预测服务可以加载该文件，并将其作为API或数据科学工具进行使用。
- en: 'There are different components of a model that can be saved:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有不同的组件可以保存：
- en: The model's architecture with all the network and layers used
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的架构，包括所有使用的网络和层
- en: The model's trained weights
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的训练权重
- en: The training configuration with the loss function, optimizer, and metrics
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含损失函数、优化器和度量标准的训练配置
- en: In TensorFlow, you can save the entire model or each of these components separately.
    Let's learn how to do this.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以保存整个模型或将这些组件分别保存。接下来我们来学习如何操作。
- en: Saving the Entire Model
  id: totrans-466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保存整个模型
- en: 'To save all the components into a single artifact, use the following code:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 要将所有组件保存为一个单独的文件，可以使用以下代码：
- en: '[PRE81]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'To load the saved model, use the following code:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载保存的模型，可以使用以下代码：
- en: '[PRE82]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Saving the Architecture Only
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅保存架构
- en: 'You can save just the architecture of the model as a `json` object. Then, you
    will need to use the `json` package to save it to a file, as shown in the following
    code snippet:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以仅保存模型的架构作为`json`对象。然后，你需要使用`json`包将其保存到文件中，如下所示的代码片段所示：
- en: '[PRE83]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Then, you will load it back using the `json` package:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将使用`json`包将其加载回来：
- en: '[PRE84]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Saving the Weights Only
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 仅保存权重
- en: 'You can save just the weights of the model as follows:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以仅保存模型的权重，方法如下：
- en: '[PRE85]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Then, you will load them back after instantiating the architecture of your
    new model:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在实例化新模型的架构后，你将加载保存的权重：
- en: '[PRE86]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: This is particularly useful if you want to train your model even more later.
    You will load the saved weights and keep training your model and updating its
    weights further.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望将来进一步训练模型，这尤其有用。你可以加载保存的权重，并继续训练你的模型，进一步更新其权重。
- en: Note
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: .h5 is the file extension used by default by TensorFlow.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: .h5是 TensorFlow 默认使用的文件扩展名。
- en: Transfer Learning
  id: totrans-484
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: So far, we've learned a lot about designing and training our own CNN models.
    But as you may have noticed, some of our models are not performing very well.
    This can be due to multiple reasons, such as the dataset being too small or our
    model requiring more training.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学到了很多关于设计和训练我们自己CNN模型的知识。但正如你可能已经注意到的那样，我们的一些模型表现得并不好。这可能有多个原因，例如数据集太小，或者我们的模型需要更多的训练。
- en: But training a CNN takes a lot of time. It would be great if we could reuse
    an existing architecture that has already been trained. Luckily for us, such an
    option does exist, and it is called transfer learning. TensorFlow provides different
    implementations of state-of-the-art models that have been trained on the ImageNet
    dataset (over 14 million images).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，训练一个CNN需要大量的时间。如果我们能重用一个已经训练好的现有架构，那就太好了。幸运的是，确实存在这样的选项，它被称为迁移学习。TensorFlow提供了多个在ImageNet数据集（超过1400万张图片）上训练的最先进模型的实现。
- en: Note
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can find the list of available pretrained models in the TensorFlow documentation:
    [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在TensorFlow文档中找到可用的预训练模型列表：[https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)
- en: 'To use a pretrained model, we need to import its implemented class. Here, we
    will be importing a `VGG16` model:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用预训练模型，我们需要导入它实现的类。这里，我们将导入一个`VGG16`模型：
- en: '[PRE87]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Next, we will define the input dimensions of the images from our dataset. Let''s
    say we have images of `(100,100, 3)`:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们数据集中图像的输入维度。假设我们有`(100,100, 3)`尺寸的图像：
- en: '[PRE88]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Then, we will instantiate a `VGG16` model:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将实例化一个`VGG16`模型：
- en: '[PRE89]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Now, we have a `VGG16` model trained on the `ImageNet` dataset. The `include_top=True`
    parameter is used to specify that we will be using the same last layers to predict
    ImageNet's 20,000 categories of images.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个在`ImageNet`数据集上训练的`VGG16`模型。`include_top=True`参数用于指定我们将使用相同的最后几层来预测ImageNet的20,000个类别的图像。
- en: 'Now, we can use this pretrained model to make predictions:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个预训练模型进行预测：
- en: '[PRE90]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'But what if we want to use this pretrained model to predict different classes
    other than the ones from ImageNet? In this situation, we will need to replace
    the last fully connected layers of the pretrained models that are used for prediction
    and train them on the new classes. These last few layers are referred to as the
    top (or head) of the model. We can do this by specifying `include_top=False`:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们想使用这个预训练模型来预测与ImageNet不同的类别怎么办？在这种情况下，我们需要替换预训练模型中用于预测的最后几个全连接层，并在新的类别上进行训练。这些最后的几层被称为模型的顶部（或头部）。我们可以通过指定`include_top=False`来实现：
- en: '[PRE91]'
  id: totrans-499
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'After this, we will need to freeze this model so that it can''t be trained
    (that is, its weights will not be updated):'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们需要冻结这个模型，以防止它被训练（也就是说，它的权重将不会被更新）：
- en: '[PRE92]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Then, we will create a new fully connected layer with the parameter of our
    choice. In this example, we will add a `Dense` layer with `20` units and a `softmax`
    activation function:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个新的全连接层，参数由我们选择。在这个示例中，我们将添加一个具有`20`个单元和`softmax`激活函数的`Dense`层：
- en: '[PRE93]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We will then add the new fully connected layer to our base model:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将把新的全连接层添加到我们的基础模型中：
- en: '[PRE94]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Finally, we will train this model, but only the weights for the last layer
    will be updated:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练这个模型，但只有最后一层的权重会被更新：
- en: '[PRE95]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: We just created a new model from a pretrained model and adapted it in order
    to make predictions for our own dataset. We achieved this by replacing the last
    layers according to the predictions we want to make. Then, we trained only these
    new layers to make the right predictions. Using transfer learning, you leveraged
    the existing weights of the `VGG16` model, which were trained on ImageNet. This
    has saved you a lot of training time and can significantly increase the performance
    of your model.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚从一个预训练模型创建了一个新模型，并进行了适应，使其能够对我们自己的数据集进行预测。我们通过根据我们想要进行的预测替换最后几层来实现这一点。然后，我们仅训练这些新层来做出正确的预测。通过迁移学习，我们利用了`VGG16`模型在ImageNet上训练得到的现有权重。这为我们节省了大量的训练时间，并且可以显著提高模型的性能。
- en: Fine-Tuning
  id: totrans-509
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调
- en: In the previous section, we learned how to apply transfer learning and use pretrained
    models to make predictions on our own dataset. With this approach, we froze the
    entire network and trained only the last few layers that were responsible for
    making the predictions. The convolutional layers stay the same, so all the filters
    are set in advance and you are just reusing them.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，我们学习了如何应用迁移学习并使用预训练模型在我们自己的数据集上进行预测。通过这种方法，我们冻结了整个网络，仅训练了最后几层，它们负责做出预测。卷积层保持不变，因此所有的滤波器都是预先设置好的，你只需要重用它们。
- en: But if the dataset you are using is very different from ImageNet, these pretrained
    filters may not be relevant. In this case, even using transfer learning will not
    help your model accurately predict the right outcomes. There is a solution for
    this, which is to only freeze a portion of the network and train the rest of the
    model rather than just the top layers, just like we do with **transfer learning**.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果您使用的数据集与ImageNet差异很大，这些预训练的过滤器可能并不相关。在这种情况下，即使使用迁移学习，也无法帮助您的模型准确预测正确的结果。对此有一种解决方案，即只冻结网络的一部分，并训练模型的其他部分，而不仅仅是冻结顶层，就像我们使用**迁移学习**时一样。
- en: In the early layers of the networks, the filters tend to be quite generic. For
    instance, you may find filters that detect horizontal or vertical lines at that
    stage. The filters closer to the end of the network (close to the top or head)
    are usually more specific to the dataset you are training on. So, these are the
    ones we want to retrain. Let's learn how we can achieve this in TensorFlow.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的早期层中，过滤器通常非常通用。例如，在这个阶段，您可能会找到检测水平或垂直线的过滤器。靠近网络末端（靠近顶部或头部）的过滤器通常更具体地适应您正在训练的数据集。因此，这些就是我们希望重新训练的层。让我们来看看如何在TensorFlow中实现这一点。
- en: 'First, let''s instantiate a pretrained `VGG16` model:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化一个预训练的`VGG16`模型：
- en: '[PRE96]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We will need to set the threshold for the layers so that they''re frozen. In
    this example, we will freeze the first 10 layers:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置层的阈值，以便它们被冻结。在本例中，我们将冻结前10层：
- en: '[PRE97]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Then, we will iterate through these layers and freeze them individually:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将遍历这些层，并逐个冻结它们：
- en: '[PRE98]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Then, we will add our custom fully connected layer to our base model:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将向基础模型中添加自定义的全连接层：
- en: '[PRE99]'
  id: totrans-520
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Finally, we will train this model:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将训练这个模型：
- en: '[PRE100]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: In this case, our model will train and update all the weights from the threshold
    layer we defined. They will use the pretrained weights as the initialized values
    for the first iteration.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的模型将训练并更新我们定义的阈值层中的所有权重。它们将使用预训练的权重作为第一次迭代的初始化值。
- en: With this technique, called fine-tuning, you can still leverage pretrained models
    by partially training them to fit your dataset.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种被称为微调的技术，您仍然可以利用预训练模型，通过部分训练使其适应您的数据集。
- en: 'Activity 3.02: Fruit Classification with Transfer Learning'
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动3.02：使用迁移学习进行水果分类
- en: In this activity, we will train a CNN to recognize images of fruits that belong
    to 120 different classes. We will use transfer learning and data augmentation
    to do so. We will be using the Fruits 360 dataset ([https://arxiv.org/abs/1712.00580](https://arxiv.org/abs/1712.00580)),
    which was originally shared by Horea Muresan, Mihai Oltean, *Fruit recognition
    from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue
    1, pp. 26-42, 2018.*
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将训练一个卷积神经网络（CNN）来识别属于120个不同类别的水果图片。我们将使用迁移学习和数据增强来完成此任务。我们将使用Fruits
    360数据集（[https://arxiv.org/abs/1712.00580](https://arxiv.org/abs/1712.00580)），该数据集最初由Horea
    Muresan、Mihai Oltean共享，*Fruit recognition from images using deep learning, Acta
    Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018*。
- en: 'It contains more than 82,000 images of 120 different types of fruits. We will
    be using a subset of this dataset with more than 16,000 images. Perform the following
    steps to complete this activity:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含了超过82,000张不同类型水果的120种图片。我们将使用这个数据集的一个子集，包含超过16,000张图片。请按照以下步骤完成本活动：
- en: Note
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'The dataset can be found here: [https://packt.live/3gEjHsX](https://packt.live/3gEjHsX
    )'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在这里找到：[https://packt.live/3gEjHsX](https://packt.live/3gEjHsX)
- en: Import the dataset and unzip the file using TensorFlow.
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow导入数据集并解压文件。
- en: 'Create a data generator with the following data augmentation:'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据生成器，并应用以下数据增强：
- en: '[PRE101]'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Load a pretrained `VGG16` model from TensorFlow.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从TensorFlow加载一个预训练的`VGG16`模型。
- en: 'Add two fully connected layers on top of `VGG16`: A fully connected layer with
    `Dense(1000, activation=''relu'')` and a fully connected layer with `Dense(120,
    activation=''softmax'')`.'
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`VGG16`的顶部添加两个全连接层：一个带有`Dense(1000, activation='relu')`的全连接层和一个带有`Dense(120,
    activation='softmax')`的全连接层。
- en: Specify an Adam optimizer with a learning rate of `0.001`.
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个学习率为`0.001`的Adam优化器。
- en: Train the model.
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Evaluate the model on the testing set.
  id: totrans-537
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型。
- en: 'The expected accuracy scores should be around `0.89` to `0.91` for the training
    and validation sets. The output will be similar to this:'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的准确率应该在训练集和验证集上大约为`0.89`到`0.91`。输出结果将类似于此：
- en: '![Figure 3.29: Expected output of the activity'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.29：活动的预期输出'
- en: '](img/B15385_03_29.jpg)'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_03_29.jpg)'
- en: 'Figure 3.29: Expected output of the activity'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.29：活动的预期输出
- en: Note
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 398.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤，以及解决方案和额外的评论，已在第398页展示。
- en: Summary
  id: totrans-544
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started our journey in this chapter with an introduction to computer vision
    and image processing, where we learned the different applications of such technology,
    how digital images are represented, and analyzed this with filters.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从计算机视觉和图像处理的介绍开始，了解了这种技术的不同应用、数字图像的表示方式，并通过滤波器对其进行了分析。
- en: Then, we dived into the basic elements of CNN. We learned what a convolution
    operation is, how filters work in detecting patterns, and what stride and padding
    are used for. After understanding these building blocks, we learned how to use
    TensorFlow to design CNN models. We built our own CNN architecture to recognize
    handwritten digits.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入探讨了CNN的基本元素。我们了解了卷积操作是什么，滤波器在检测模式中的作用，以及步幅和填充的用途。在理解这些构建块之后，我们学习了如何使用TensorFlow设计CNN模型，并构建了自己的CNN架构来识别手写数字。
- en: After this, we went through data generators and learned how they can feed our
    model with batches of images rather than loading the entire dataset. We also learned
    how they can perform data augmentation transformations to expand the variety of
    images and help the model generalize better.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们学习了数据生成器，并了解了它们如何将图像批次输入到我们的模型中，而不是加载整个数据集。我们还学习了它们如何执行数据增强变换，以扩展图像的多样性，帮助模型更好地进行泛化。
- en: Finally, we learned about saving a model and its configuration, but also about
    how to apply transfer learning and fine-tuning. These techniques are very useful
    for reusing pretrained models and adapting them to your own projects and datasets.
    This will save you a lot of time as you won't have to train the model from scratch.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们了解了如何保存模型及其配置，还学习了如何应用迁移学习和微调。这些技术对于重用预训练模型并将其调整为适应自己的项目和数据集非常有用。这将为你节省大量时间，因为你无需从头开始训练模型。
- en: 'In the next chapter, you will learn about another very interesting topic that
    is used for natural language processing: embeddings.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习另一个非常有趣的主题，它用于自然语言处理：嵌入（embeddings）。
