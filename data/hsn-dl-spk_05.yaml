- en: Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep Learning Basics*,
    we learned about a very high level overview of **Convolutional Neural Networks**
    (**CNNs**). In this chapter, we are going to understand more details about this
    type of CNN, the possible implementations of their layers, and we will start hands-on
    implementing CNNs through the DeepLearning4j framework. The chapter ends with
    examples involving Apache Spark too. Training and evaluation strategies for CNNs
    will be covered in [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training
    Neural Networks with Spark*, [Chapter 8](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml),
    *Monitoring and Debugging Neural Network Training*, and [Chapter 9](869a9495-e759-4810-8623-d8b76ba61398.xhtml),
    *Interpreting Neural Network Output*. In the description of the different layers,
    I have tried to reduce the usage of math concepts and formulas as much as possible
    in order to make the reading and comprehension easier for developers and data
    analysts who might have no math or data science background. Therefore, you have
    to expect more focus on the code implementation in Scala.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml)《*深度学习基础*》中，我们学习了关于**卷积神经网络**（**CNN**）的一个非常高层次的概述。在这一章，我们将深入了解这种类型的
    CNN，更加详细地探讨它们各层的可能实现，并且我们将开始通过 DeepLearning4j 框架动手实现 CNN。本章最后也会涉及到使用 Apache Spark
    的示例。CNN 的训练与评估策略将在[第 7 章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)《*用 Spark
    训练神经网络*》、[第 8 章](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml)《*监控与调试神经网络训练*》以及[第
    9 章](869a9495-e759-4810-8623-d8b76ba61398.xhtml)《*解释神经网络输出*》中讲解。在不同层的描述中，我尽量减少了数学概念和公式的使用，以便让没有数学或数据科学背景的开发者和数据分析师能够更容易地阅读和理解。因此，你会看到更多关于
    Scala 代码实现的内容。
- en: 'The chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Convolutional layers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: Weights
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重
- en: GoogleNet Inception V3 model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GoogleNet Inception V3 模型
- en: Hands-on CNN with Spark
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动手实践 CNN 与 Spark
- en: Convolutional layers
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'Since the CNN section was covered in [Chapter 2](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml), *Deep
    Learning Basics*, you should know in which context CNNs are commonly used. In
    that section, we have mentioned that each layer of the same CNN can have a different
    implementation. The first three sections of this chapter describe possible layer
    implementations in detail, starting from the convolutional layers. But first,
    let''s recap the process by which CNN perceive images. They perceive images as
    volumes (3D objects) and not as bi-dimensional canvases (having width and height
    only). The reason is the following: digital color images have a **Red**-**Blue**-**Green**
    (**RGB**) encoding and it is the mixing of these colors that produces the spectrum
    that can be perceived by human eyes. This also means that CNNs ingest images as
    three separate layers of color, one on top of the other. This translates into
    receiving a color image in the form of a rectangular box where width and height
    can be measured in pixels and having a three layers (referred as **channels**)
    depth, one for each RGB color. Cutting a long story short, an input image is seen
    by a CNN as a multi-dimensional array. Let''s give a practical example. If we
    consider a 480 x 480 image, it is perceived by the network as a 480 x 480 x 3
    array, for which each of its elements can have a value of between 0 and 255\.
    These values describe the pixel intensity at a given point. Here''s the main difference
    between the human eyes and a machine: these array values are the only inputs available
    to it. The output of a computer receiving those numbers as input will be other
    numbers describing the probability of the image being a certain class. The first
    layer of a CNN is always **convolutional**. Suppose having an input that is a
    32 x 32 x 3 array of pixel values, let''s try to imagine a concrete visualization
    that clearly and simply explains what a convolutional layer is. Let''s try to
    visualize a torch that shines over the top-left part of the image.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积神经网络（CNN）部分已经在[第2章](a177e128-f96b-4a3a-8e3e-24f133492cb0.xhtml)《深度学习基础》中讲解过，你应该知道CNN通常在哪些场景下使用。在该章节中，我们提到过同一CNN的每一层可以有不同的实现方式。本章的前三部分详细描述了可能的层实现，从卷积层开始。但首先，让我们回顾一下CNN如何感知图像的过程。CNN将图像视为体积（3D物体），而非二维画布（仅有宽度和高度）。原因如下：数字彩色图像采用**红**-**蓝**-**绿**（**RGB**）编码，正是这些颜色的混合产生了人眼能够感知的光谱。这也意味着CNN将图像作为三个颜色层分别处理，层与层之间是叠加的。这转化为以矩形框的形式接收彩色图像，宽度和高度可以用像素来度量，且有三个层（称为**通道**）的深度，每个通道对应一个RGB颜色。简而言之，输入图像被CNN看作一个多维数组。我们来举个实际的例子。如果我们考虑一个480
    x 480的图像，网络会将其看作一个480 x 480 x 3的数组，其中每个元素的值在0到255之间。这些值描述了图像某一点的像素强度。这里是人眼与机器之间的主要区别：这些数组值是机器唯一的输入。接收到这些数值作为输入的计算机输出将是其他数字，描述图像属于某个类别的概率。CNN的第一层总是**卷积层**。假设输入是一个32
    x 32 x 3的像素值数组，我们试着想象一个具体的可视化，清楚简洁地解释卷积层的作用。我们可以将其想象为一个手电筒照射在图像的左上部分。
- en: 'This following diagram shows the torch shines, covering a 5 x 5 area:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了手电筒的照射范围，覆盖了一个5 x 5的区域：
- en: '![](img/82c754f7-f72d-46e3-859a-e98da3dba6e1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82c754f7-f72d-46e3-859a-e98da3dba6e1.png)'
- en: 'Figure 5.1: 5 x 5 filter'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：5 x 5滤波器
- en: Then the imaginary torch starts sliding over all the other areas of the image.
    The proper term to call it is **filter** (or **neuron** or **kernel**) and the
    image region that lights up is called the **receptive field**. In math terms,
    a filter is an array of numbers (called **weights** or **parameters**). The depth
    of a filter has to match the depth of the input. Referring to this section example,
    we have a filter that's dimensions are 5 x 5 x 3\. The first position the filter
    covers (as shown in the diagram in preceding diagram) is the top left corner of
    the input image. While the filter slides around the image, or convolves (from
    the Latin verb *convolvere*, which means to wrap up), it multiplies its values
    with the image original pixel values. The multiplications are then all summed
    up (in our example, in total we have 75 multiplications). The outcome is a single
    number, which represents when the filter is only at the top left of the input
    image. This process is then repeated for every location on the input image. As
    with the first one, every unique location produces a single number. Once the filter
    has completed its sliding process over all the locations of the image, the result
    is a 28 x 28 x 1 (given a 32 x 32 input image, a 5 x 5 filter can fit 784 different
    locations) numeric array called **activation map** (or **feature map**).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，虚拟的滤光器开始滑动覆盖图像的其他区域。适当的术语是**滤波器**（或**神经元**或**卷积核**），而被照亮的图像区域被称为**感受野**。用数学术语来说，滤波器是一个数字数组（称为**权重**或**参数**）。滤波器的深度必须与输入的深度匹配。参考本节的示例，我们有一个维度为5
    x 5 x 3的滤波器。滤波器覆盖的第一个位置（如前面图示所示）是输入图像的左上角。当滤波器在图像上滑动或进行卷积（来自拉丁动词*convolvere*，意为包裹）时，它会将其值与原始图像像素值相乘。所有的乘法结果会相加（在我们的示例中，总共有75次乘法）。最终的结果是一个数字，表示滤波器仅位于输入图像的左上角时的值。这个过程会在输入图像的每个位置重复。与第一次一样，每个唯一的位置都会产生一个数字。一旦滤波器完成在图像所有位置上的滑动过程，结果将是一个28
    x 28 x 1（假设输入图像为32 x 32，5 x 5的滤波器可以适应784个不同的位置）的数字数组，称为**激活图**（或**特征图**）。
- en: Pooling layers
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: It is common practice (as you will see next through the code examples of this
    chapter and from [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training
    Neural Networks with Spark*, onward) to periodically insert a pooling layer between
    successive convolution layers in a CNN model. This kind of layers scope is to
    progressively reduce the number of parameters for the network (which translates
    into a significant lowering of the computation costs). In fact, spatial pooling
    (which is also found in literature as downsampling or subsampling) is a technique
    that reduces the dimensionality of each feature map, while at the same time retaining
    the most important part of the information. Different types of spatial pooling
    exist. The most used are max, average, sum, and L2-norm.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中（正如你将在本章的代码示例以及[第7章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)《*使用Spark训练神经网络*》中看到的那样），通常会在CNN模型的连续卷积层之间定期插入池化层。这种层的作用是逐步减少网络的参数数量（这意味着显著降低计算成本）。事实上，空间池化（在文献中也被称为下采样或子采样）是一种减少每个特征图维度的技术，同时保留最重要的信息。存在不同类型的空间池化。最常用的是最大池化、平均池化、求和池化和L2范数池化。
- en: Let's take as an example, max pooling. This technique requires defining a spatial
    neighborhood (typically a 2 × 2 window); the largest element within that window
    is then taken from the rectified feature map. The average pooling strategy requires
    taking the average or the sum of all elements in the given window. Several papers
    and use cases provide evidence that max pooling has been shown to produce better
    results than other spatial pooling techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以最大池化为例，这种技术需要定义一个空间邻域（通常是一个2 × 2的窗口）；然后从经过修正的特征图中取出该窗口内的最大元素。平均池化策略则要求取窗口内所有元素的平均值或和。一些论文和实际应用表明，最大池化已经证明能够比其他空间池化技术产生更好的结果。
- en: 'The following diagram shows an example of max pooling operation (a 2 × 2 window
    is used here):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了最大池化操作的一个示例（这里使用了一个2 × 2的窗口）：
- en: '![](img/1546b290-0a65-447d-9bc8-13e8341f8c6a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1546b290-0a65-447d-9bc8-13e8341f8c6a.png)'
- en: 'Figure 5.2: Max pooling operation using a 2 × 2 window'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：使用2 × 2窗口的最大池化操作
- en: Fully connected layers
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: A fully connected layer is the last layer of a CNN. Fully connected layers,
    given an input volume, return as output a multi-dimensional vector. The dimension
    of the output vector matches the number of classes for the particular problem
    to solve.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层是卷积神经网络（CNN）的最后一层。全连接层在给定输入数据的情况下，输出一个多维向量。输出向量的维度与特定问题的类别数相匹配。
- en: 'This chapter and others in this book present some examples of CNN implementation
    and training for digit classification purposes. In those cases, the dimension
    of the output vector would be 10 (the possible digits are 0 to 9). Each number
    in the 10-dimensional output vector represents the probability of a certain class
    (digit). The following is an output vector for a digit classification inference:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章及本书中的其他章节展示了一些CNN实现和训练的例子，用于数字分类。在这些情况下，输出向量的维度为10（可能的数字是0到9）。10维输出向量中的每个数字表示某个类别（数字）的概率。以下是一个用于数字分类推断的输出向量：
- en: '`[0 0 0 .1 .75 .1 .05 0 0 0]`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`[0 0 0 .1 .75 .1 .05 0 0 0]`'
- en: How do we interpret those values? The network is telling us that it believes
    that the input image is a four with a 75% probability (which is the highest in
    this case), with a 10% probability that the image is a three, another 10% probability
    that the image is a five, and a 5% probability that the image is a six. A fully
    connected layer looks at the output of the previous layer in the same network
    and determines which features most correlate to a particular class.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解读这些值？网络告诉我们，它认为输入图像是一个四，概率为75%（在本例中是最高的），同时图像是三的概率为10%，图像是五的概率为10%，图像是六的概率为5%。全连接层会查看同一网络中前一层的输出，并确定哪些特征与某一特定类别最相关。
- en: 'The same happens not only in digit classification. An example of a general
    use case of image classification is that, if a model that has been trained using
    images of animals predicts that the input image is, for example, a horse, it will
    have high values in the activation maps that represent specific high-level features,
    like four legs or a tail, just to mention a couple. Similarly, if the same model
    predicts that an image is a different animal, let''s say a fish, it will have
    high values in the activation maps that represent specific high-level features,
    like fins or a gill. We can then say that a fully connected layer looks at those
    high-level features that most strongly correlate to a particular class and has
    particular weights: this ensures that the correct probabilities for each different
    class are obtained after the products between weights and the previous layer have
    been calculated.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅仅是在数字分类中发生这种情况。在图像分类的一个通用使用案例中，如果一个使用动物图像训练的模型预测输入图像是例如马，它将在表示特定高级特征的激活图中具有较高的值，比如四条腿或尾巴，仅举几个例子。类似地，如果该模型预测图像是另一种动物，比如鱼，它将在表示特定高级特征的激活图中具有较高的值，比如鳍或鳃。我们可以说，全连接层会查看与某一特定类别最相关的高级特征，并拥有特定的权重：这确保了在计算了权重与前一层的乘积后，能够获得每个不同类别的正确概率。
- en: Weights
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重
- en: CNNs share weights in convolutional layers. This means that the same filter
    is used for each receptive field in a layer and that these replicated units share
    the same parameterization (weight vector and bias) and form a feature map.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在卷积层中共享权重。这意味着在一层中的每个感受野使用相同的滤波器，并且这些复制的单元共享相同的参数（权重向量和偏置），并形成一个特征图。
- en: 'The following diagram shows three hidden units of a network belonging to the
    same feature map:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个网络中属于同一特征图的三个隐藏单元：
- en: '![](img/c1de62be-e726-41dd-999d-490fa29ca2ed.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1de62be-e726-41dd-999d-490fa29ca2ed.png)'
- en: 'Figure 5.3: Hidden units'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：隐藏单元
- en: 'The weights in the darker gray color in the preceding diagram are shared and
    identical. This replication allows features detection regardless of the position
    they have in the visual field. Another outcome of this weight sharing is the following:
    the efficiency of the learning process increases by drastically reducing the number
    of free parameters to be learned.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 前述图中较深灰色的权重是共享且相同的。这种复制使得无论特征在视觉场景中的位置如何，都能够进行特征检测。权重共享的另一个结果是：学习过程的效率通过大幅减少需要学习的自由参数数量得到显著提高。
- en: GoogleNet Inception V3 model
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoogleNet Inception V3模型
- en: 'As a concrete implementation of a CNN, in this section, I am going to present
    the GoogleNet ([https://ai.google/research/pubs/pub43022](https://ai.google/research/pubs/pub43022))
    architecture by Google ([https://www.google.com/](https://www.google.com/)) and
    its inception layers. It has been presented at the *ImageNet Large Scale Visual
    Recognition Challenge 2014* (*ILSVRC2014*, [http://www.image-net.org/challenges/LSVRC/2014/](http://www.image-net.org/challenges/LSVRC/2014/)). Needless
    to say, it won that competition. The distinct characteristic of this implementation
    is the following: increased depth and width and, at the same time, a constant
    computational budget. Improved computing resources utilization is part of the
    network design.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作为卷积神经网络（CNN）的具体实现，在这一部分，我将介绍Google的GoogleNet架构（[https://ai.google/research/pubs/pub43022](https://ai.google/research/pubs/pub43022)）及其Inception层。该架构已在*ImageNet大规模视觉识别挑战赛2014*（*ILSVRC2014*，[http://www.image-net.org/challenges/LSVRC/2014/](http://www.image-net.org/challenges/LSVRC/2014/)）上展示。无需多说，它赢得了那场比赛。这个实现的显著特点如下：增加了深度和宽度，同时保持了恒定的计算预算。提高计算资源的利用率是网络设计的一部分。
- en: 'This chart summarizes all of the layers for this network implementation presented
    in the context:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表总结了在该上下文中提出的网络实现的所有层：
- en: '![](img/0b9d5d53-12f0-43de-a557-fa66e2b1ba98.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b9d5d53-12f0-43de-a557-fa66e2b1ba98.png)'
- en: 'Figure 5.4: GoogleNet layers'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：GoogleNet层
- en: There are 22 layers with parameters (excluding the pooling layers; the total
    is 27 if they are included) and almost 12 times fewer parameters than the winning
    architecture of the past editions of the same context. This network has been designed
    keeping in mind computational efficiency and practicality, so that inference can
    be run also on individual devices having limited resources, in particular those
    with a low memory footprint. All the convolution layers use **Rectified Linear
    Unit** (**ReLU**) activation. The of the receptive field is 224 × 224 in the RGB
    color space (with zero means). Looking at the table in the preceding diagram,
    the **#3 × 3** and **#5 × 5** reduces are the number of 1 × 1 filters in the reduction
    layer preceding the 3 × 3 and 5 × 5 convolution layers. The activation function
    for those reduction layers is ReLU as well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络有22层参数（不包括池化层；如果包括池化层，总共有27层），其参数数量几乎是过去几届同一比赛获胜架构的12分之一。这个网络的设计考虑了计算效率和实用性，使得推理过程也能够在有限资源的单个设备上运行，尤其是那些内存占用较低的设备。所有卷积层都使用**修正线性单元**（**ReLU**）激活函数。感受野的大小为224
    × 224，使用的是RGB颜色空间（均值为零）。通过前面的图表中的表格来看，**#3 × 3**和**#5 × 5**的减少数量是位于3 × 3和5 × 5卷积层之前的1
    × 1滤波器数量。这些减少层的激活函数同样是ReLU。
- en: The diagram at [https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png](https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png) shows
    a schematic view of the network.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png](https://user-images.githubusercontent.com/32988039/33234276-86fa05fc-d1e9-11e7-941e-b3e62771716f.png)中的示意图展示了网络的结构。
- en: 'In this architecture, each unit from an earlier layer corresponds to a region
    of the input image—these units are grouped into filter banks. In the layers that
    are closer to the input, correlated units concentrate in local regions. This results
    in a lot of clusters concentrated in a single region, so they can be covered by
    a 1 × 1 convolution in the following layer. However, there could be a smaller
    number of more spatially split clusters covered by convolutions over larger chunks,
    and there would be a decreasing number of chunks over larger regions. To prevent
    those patch-alignment issues, the inception architecture implementations are restricted
    to use 1 × 1, 3 × 3 and 5 × 5 filters. The suggested architecture is then a combination
    of layers which output filter banks are aggregated in a single output vector,
    which represents the input of the next stage. Additionally, adding an alternative
    pooling path in parallel to each stage could have a further beneficial effect:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，来自前一层的每个单元都对应输入图像的一个区域——这些单元被分组到滤波器组中。在接近输入的层中，相关的单元集中在局部区域。这导致许多聚集在单一区域的簇，因此可以通过下一个层中的
    1 × 1 卷积来覆盖它们。然而，也可能有较少的、更空间分散的簇，由较大块的卷积覆盖，而且随着区域增大，块的数量会减少。为了防止这些补丁对齐问题，inception
    架构的实现被限制为只能使用 1 × 1、3 × 3 和 5 × 5 滤波器。建议的架构是将多个层的输出滤波器组聚合为一个单一的输出向量，这个向量代表了下一阶段的输入。此外，在每个阶段并行添加一个替代的池化路径可能会有进一步的有益效果：
- en: '![](img/0dda3eb9-ac5d-4ba5-be6c-cf620bfe7afc.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dda3eb9-ac5d-4ba5-be6c-cf620bfe7afc.png)'
- en: 'Figure 5.5: Naive version of the inception module'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5：简单版本的 inception 模块
- en: 'Looking at the preceding diagram, you can understand that, in terms of computational
    cost, for a layer with a large number of filters, it could be too expensive to
    have 5 × 5 convolutions (even if there aren''t many). And, of course, this becomes
    a bigger problem when adding more pooling units, because the number of output
    filters is equal to the number of filters in the previous stage. Definitely merging
    the output of a pooling layer with outputs of a convolutional layer could inevitably
    lead to more and more outputs moving from stage to stage. For this reason, a second
    and more computational idea of the inception architecture has been proposed. The
    new idea is to reduce dimension where the computational requirements could increase
    too much. But there''s a caveat: low dimensional embeddings could contain lots
    of information about a large image chunk, but they represent information in a
    compressed form, making its processing hard. A good compromise is then to keep
    the representation mostly sparse and at the same time compress the signals only
    where there is a real need to heavily aggregate them. For this reason, in order
    to compute reductions, **1 × 1 convolutions** are used before any expensive **3
    × 3** and **5 × 5 convolutions**.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，就计算成本而言，对于具有大量滤波器的层，5 × 5 卷积可能太昂贵（即使卷积数量不多）。当然，随着添加更多池化单元，这个问题会变得更严重，因为输出滤波器的数量等于前一阶段的滤波器数量。显然，将池化层的输出与卷积层的输出合并，可能不可避免地导致越来越多的输出从一个阶段传递到下一个阶段。因此，提出了
    inception 架构的第二个、更具计算效率的想法。这个新想法是在计算需求可能增加过多的地方进行维度降低。但需要注意的是：低维嵌入可能包含大量关于大图块的信息，但它们以压缩形式表示这些信息，这使得处理起来变得困难。因此，一个好的折衷方法是保持表示尽可能稀疏，同时仅在真正需要大量聚合信号时，才对信号进行压缩。为此，在进行任何昂贵的
    **3 × 3** 和 **5 × 5 卷积**之前，使用 **1 × 1 卷积** 来进行维度降低。
- en: 'The following diagram shows the new module following the preceding consideration:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了考虑到上述问题后的新模块：
- en: '![](img/641e6658-04b0-41a6-bd8a-34a319764999.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/641e6658-04b0-41a6-bd8a-34a319764999.png)'
- en: 'Figure 5.6: Inception module with dimension reductions'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：具有维度减少的 inception 模块
- en: Hands-on CNN with Spark
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 实现的 CNN
- en: In the previous sections of this chapter, we went through the theory of CNNs
    and the GoogleNet architecture. If this is the first time you're reading about
    these concepts, probably you are wondering about the complexity of the Scala code
    to implement CNN's models, train, and evaluate them. Adopting a high-level framework
    like DL4J, you are going to discover how many facilities come out-of-the-box with
    it and that the implementation process is easier than expected.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面的部分，我们已经讨论了 CNN 的理论和 GoogleNet 架构。如果这是你第一次阅读这些概念，可能会对实现 CNN 模型、训练和评估时 Scala
    代码的复杂性感到困惑。通过采用像 DL4J 这样的高层次框架，你将发现它自带了许多功能，且实现过程比预期的更简单。
- en: In this section, we are going to explore a real example of CNN configuration
    and training using the DL4J and Spark frameworks. The training data used comes
    from the `MNIST` database ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)).
    It contains images of handwritten digits, with each image labeled by an integer.
    It is used to benchmark the performance of ML and DL algorithms. It contains a
    training set of 60,000 examples and a test set of 10,000 examples. The training
    set is used to teach the algorithm to predict the correct label, the integer,
    while the test set is used to check how accurate the trained network can make
    guesses.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用 DL4J 和 Spark 框架，探索 CNN 配置和训练的真实示例。所使用的训练数据来自`MNIST`数据库（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）。它包含手写数字的图像，每张图像都由一个整数进行标记。该数据库用于评估
    ML 和 DL 算法的性能。它包含 60,000 个训练样本和 10,000 个测试样本。训练集用于教算法预测正确的标签，即整数，而测试集则用于检查训练后的网络在进行预测时的准确性。
- en: For our example, we download, and extract somewhere locally, the `MNIST` data.
    A directory named `mnist_png` is created. It has two subdirectories: `training`,
    containing the training data, and `testing`, containing the evaluation data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们下载并在本地解压 `MNIST` 数据。会创建一个名为 `mnist_png` 的目录，它包含两个子目录：`training`，其中包含训练数据，以及
    `testing`，其中包含评估数据。
- en: 'Let''s start using DL4J only first (we would add Spark to the stack later).
    The first thing we need to do is to vectorize the training data. We use `ImageRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html))
    as reader, because the training data are images, and a `RecordReaderDataSetIterator`
    ([http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html](http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html))
    to iterate through the dataset, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先只使用 DL4J（稍后会将 Spark 添加到堆栈中）。我们需要做的第一件事是将训练数据向量化。我们使用 `ImageRecordReader`
    ([https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html](https://deeplearning4j.org/datavecdoc/org/datavec/image/recordreader/ImageRecordReader.html))
    作为读取器，因为训练数据是图像，而使用 `RecordReaderDataSetIterator` ([http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html](http://javadox.com/org.deeplearning4j/deeplearning4j-core/0.4-rc3.6/org/deeplearning4j/datasets/canova/RecordReaderDataSetIterator.html))
    来遍历数据集，方法如下：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s do a min-max scaling of the pixel values from 0-255 to 0-1, as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对像素值进行最小-最大缩放，将其从 0-255 缩放到 0-1，方法如下：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The same vectorization needs to be done for the testing data as well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对测试数据也需要进行相同的向量化处理。
- en: 'Let''s configure the network, as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式配置网络：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The `MultiLayerConfiguration` object produced ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html))
    can then be used to initialize the model ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html)),
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用生成的 `MultiLayerConfiguration` 对象 ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/MultiLayerConfiguration.html))
    来初始化模型 ([https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html))，方法如下：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now train (and evaluate) the model, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练（和评估）模型，方法如下：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's now put Apache Spark into the game. Through Spark, it is possible to parallelize
    the training and evaluation in memory across multiple nodes of a cluster.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将 Apache Spark 引入其中。通过 Spark，可以在集群的多个节点上并行化内存中的训练和评估过程。
- en: 'As usual, create a Spark context first, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，首先创建 Spark 上下文，如下所示：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, after vectorizing the training data, parallelize them through the Spark
    context, as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在将训练数据向量化后，通过 Spark 上下文将其并行化，如下所示：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The same needs to be done for the testing data as well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据也需要进行相同的处理。
- en: 'After configuring and initializing the model, you can configure Spark for training,
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 配置和初始化模型后，您可以按如下方式配置 Spark 进行训练：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Create the Spark network, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Spark 网络，如下所示：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, replace the previous training code with the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用以下代码替换之前的训练代码：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When done, don''t forget to delete the temporary training files, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，不要忘记删除临时训练文件，如下所示：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The full example is part of the source code shipped with the book.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 完整示例是书籍随附的源代码的一部分。
- en: Summary
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first went deeper into the CNN main concepts and explored
    one of the most popular and performing examples of the CNN architecture provided
    by Google. We started then to implement some code using DL4J and Spark.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们首先深入了解了 CNN 的主要概念，并探索了 Google 提供的 CNN 架构中最流行和表现最好的一个例子。接着，我们开始使用 DL4J
    和 Spark 实现一些代码。
- en: In the next chapter, we will follow a similar trail to go deeper into RNNs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将沿着类似的路线更深入地探讨 RNN。
