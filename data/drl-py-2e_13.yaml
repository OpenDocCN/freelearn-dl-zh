- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: TRPO, PPO, and ACKTR Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TRPO、PPO和ACKTR方法
- en: 'In this chapter, we will learn two interesting state-of-art policy gradient
    algorithms: trust region policy optimization and proximal policy optimization.
    Both of these algorithms act as an improvement to the policy gradient algorithm
    (REINFORCE with baseline) we learned in *Chapter 10*, *Policy Gradient Method.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习两种有趣的最先进的策略梯度算法：信任区域策略优化和近端策略优化。这两种算法都是对我们在*第10章*中学习的策略梯度算法（带基线的REINFORCE）的改进，*策略梯度方法*。
- en: We begin the chapter by understanding the **Trust Region Policy Optimization**
    (**TRPO**) method and how it acts as an improvement to the policy gradient method.
    Later we will understand several essential math concepts that are required to
    understand TRPO. Following this, we will learn how to design and solve the TRPO
    objective function. At the end of the section, we will understand how the TRPO
    algorithm works step by step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过了解**信任区域策略优化**（**TRPO**）方法及其如何作为策略梯度方法的改进来开始本章内容。随后，我们将理解理解TRPO所需的几个重要数学概念。接下来，我们将学习如何设计和求解TRPO目标函数。在本节结束时，我们将了解TRPO算法是如何一步步工作的。
- en: Moving on, we will learn about **Proximal Policy Optimization** (**PPO**). We
    will understand how PPO works and how it acts as an improvement to the TRPO algorithm
    in detail. We will also learn two types of PPO algorithm called PPO-clipped and
    PPO-penalty.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解**近端策略优化**（**PPO**）。我们将详细了解PPO的工作原理以及它如何作为TRPO算法的改进。我们还将学习两种PPO算法，分别是PPO-clipped和PPO-penalty。
- en: At the end of the chapter, we will learn about an interesting actor-critic method
    called the **Actor-Critic using Kronecker-Factored Trust Region** (**ACKTR**)
    method, which uses Kronecker factorization to approximate the second-order derivative.
    We will explore how ACKTR works and how it uses the trust region in its update
    rule.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们将学习一种有趣的演员-评论家方法，叫做**使用克罗内克因子信任区域的演员-评论家**（**ACKTR**）方法，该方法利用克罗内克分解来近似二阶导数。我们将探讨ACKTR是如何工作的，以及它如何在其更新规则中使用信任区域。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Trust region policy optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任区域策略优化
- en: Designing the TRPO objective function
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计TRPO目标函数
- en: Solving the TRPO objective function
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求解TRPO目标函数
- en: Proximal policy optimization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近端策略优化
- en: The PPO algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO算法
- en: Actor-critic using Kronecker-factored trust region
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用克罗内克因子信任区域的演员-评论家
- en: Trust region policy optimization
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信任区域策略优化
- en: 'TRPO is one of the most popularly used algorithms in deep reinforcement learning.
    TRPO is a policy gradient algorithm and it acts as an improvement to the policy
    gradient with baseline method we learned in *Chapter 10*, *Policy Gradient Method*.
    We learned that policy gradient is an on-policy method, meaning that on every
    iteration, we improve the same policy with which we are generating trajectories.
    On every iteration, we update the parameter of our network and try to find the
    improved policy. The update rule for updating the parameter ![](img/B15558_09_042.png)
    of our network is given as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO是深度强化学习中最常用的算法之一。TRPO是一种策略梯度算法，它是对我们在*第10章*中学习的带基线的策略梯度方法的改进。我们了解到，策略梯度是一种在线方法，这意味着在每次迭代中，我们都会改进与生成轨迹所用的相同策略。在每次迭代中，我们更新网络的参数，并试图找到改进后的策略。更新网络参数
    ![](img/B15558_09_042.png) 的更新规则如下：
- en: '![](img/B15558_11_005.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_005.png)'
- en: Where ![](img/B15558_10_113.png) is the gradient and ![](img/B15558_07_025.png)
    is known as the step size or learning rate. If the step size is large then there
    will be a large policy update, and if it is small then there will be a small update
    in the policy. How can we find an optimal step size? In the policy gradient method,
    we keep the step size small and so on every iteration there will be a small improvement
    in the policy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_10_113.png) 是梯度，![](img/B15558_07_025.png) 被称为步长或学习率。如果步长较大，则会有较大的策略更新，如果步长较小，则策略更新较小。我们如何找到最佳步长？在策略梯度方法中，我们保持步长较小，因此在每次迭代时，策略都会有小幅改进。
- en: But what happens if we take a large step on every iteration? Let's suppose we
    have a policy ![](img/B15558_04_099.png) parameterized by ![](img/B15558_09_087.png).
    So, on every iteration, updating ![](img/B15558_09_118.png) implies that we are
    improving our policy. If the step size is large, then the policy on every iteration
    varies greatly, meaning the old policy (the policy used in the previous iteration)
    and the new policy (the policy used in the current iteration) vary greatly. Since
    we are using a parametrized policy, it implies that if we make a large update
    (large step size) then the parameter of the old policy and the new policy vary
    heavily, and this leads to a problem called model collapse.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果我们在每次迭代中都迈出很大的一步会发生什么呢？假设我们有一个由 ![](img/B15558_04_099.png) 参数化的策略 ![](img/B15558_09_087.png)。因此，在每次迭代中，更新
    ![](img/B15558_09_118.png) 意味着我们在改进我们的策略。如果步长很大，那么每次迭代中的策略变化会很大，这意味着旧政策（上一迭代中使用的策略）和新政策（当前迭代中使用的策略）变化很大。由于我们使用的是参数化的策略，这意味着如果我们进行大规模的更新（大步长），则旧政策和新政策的参数变化会非常大，这会导致一个叫做模型崩溃的问题。
- en: This is the reason that in the policy gradient method, instead of taking larger
    steps and updating the parameter of our network, we take small steps and update the
    parameter to keep the old policy and new policy close. But how can we improve this?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在策略梯度方法中，我们不是采取更大步骤来更新网络参数，而是采取小步伐并更新参数，以保持旧政策和新政策接近。但我们如何改进这个方法呢？
- en: Can we take a larger step along with keeping the old and new policies close
    so that it won't affect our model performance and also helps us to learn quickly?
    Yes, this problem is solved by TRPO.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否在保持旧政策和新政策接近的情况下，迈出更大的一步，从而不会影响我们的模型表现，并帮助我们快速学习？是的，这个问题通过TRPO得到了解决。
- en: TRPO tries to make a large policy update while imposing a constraint that the
    old policy and the new policy should not vary too much. Okay, what is this constraint?
    But first, how can we measure and understand if the old policy and new policy
    are changing greatly? Here is where we use a measure called the **Kullback-Leibler**
    (**KL**) divergence. The KL divergence is ubiquitous in reinforcement learning.
    It tells us how two probability distributions are different from each other. So,
    we can use the KL divergence to understand if our old policy and new policy vary
    greatly or not. TRPO adds a constraint that the KL divergence between the old
    policy and the new policy should be less than or equal to some constant ![](img/B15558_09_135.png).
    That is, when we make a policy update, the old policy and the new policy should
    not vary more than some constant. This constraint is called the trust region constraint.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO尝试在施加约束的情况下进行大规模的策略更新，即旧政策和新政策不应变化太大。好的，这个约束是什么呢？但首先，我们如何衡量和理解旧政策和新政策的变化是否过大呢？这里我们使用了一种叫做**Kullback-Leibler**（**KL**）散度的度量。KL散度在强化学习中无处不在。它告诉我们两个概率分布彼此之间的差异。因此，我们可以使用KL散度来理解旧政策和新政策是否变化过大。TRPO增加了一个约束，要求旧政策和新政策之间的KL散度应小于或等于某个常数
    ![](img/B15558_09_135.png)。也就是说，当我们进行策略更新时，旧政策和新政策的变化应不超过某个常数。这个约束被称为信任区域约束。
- en: Thus, TRPO tries to make a large policy update while imposing the constraint
    that the parameter of the old policy and the new policy should be within the trust
    region. Note that in the policy gradient method, we use a parameterized policy.
    Thus, keeping the parameter of the old policy and the new policy within the trust
    region implies that the old and new policies are within the trust region.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TRPO尝试在施加约束的情况下进行大规模的策略更新，要求旧政策和新政策的参数应保持在信任区域内。请注意，在策略梯度方法中，我们使用的是参数化的策略。因此，保持旧政策和新政策的参数在信任区域内，意味着旧政策和新政策也在信任区域内。
- en: TRPO guarantees monotonic policy improvement; that is, it guarantees that there
    will always be a policy improvement on every iteration. This is the fundamental
    idea behind the TRPO algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO保证了策略的单调改进；也就是说，它保证在每次迭代中都会有策略改进。这是TRPO算法背后的基本思想。
- en: To understand how exactly TRPO works, we should understand the math behind TRPO.
    TRPO has pretty heavy math. But worry not! It will be simple if we understand
    the fundamental math concepts required to understand TRPO. So, before diving into
    the TRPO algorithm, first, we will understand several essential math concepts
    that are required to understand TRPO. Then we will learn how to design a TRPO
    objective function with the trust region constraint, and finally, we will see
    how to solve the TRPO objective function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 TRPO 是如何工作的，我们需要理解 TRPO 背后的数学。TRPO 包含了相当复杂的数学内容。但是别担心！只要我们理解了理解 TRPO 所需的基本数学概念，它就会变得简单。因此，在深入了解
    TRPO 算法之前，我们首先会理解一些必备的数学概念。然后，我们将学习如何设计带有信任区域约束的 TRPO 目标函数，最后，我们将看看如何求解 TRPO 目标函数。
- en: Math essentials
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学基础
- en: 'Before understanding how TRPO works, first, we will understand the following
    important math concepts:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解 TRPO 是如何工作的之前，我们首先会理解以下重要的数学概念：
- en: The Taylor series
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 泰勒级数
- en: The trust region method
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任区域方法
- en: The conjugate gradient method
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共轭梯度方法
- en: Lagrange multipliers
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉格朗日乘数
- en: Importance sampling
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要抽样
- en: The Taylor series
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泰勒级数
- en: 'The Taylor series is a series of infinite terms and it is used for approximating
    a function. Let''s say we have a function *f*(*x*) centered at *x* = *a*; we can
    approximate it using an infinite sum of polynomial terms as shown here:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数是一个无限项的级数，用于逼近一个函数。假设我们有一个以 *x* = *a* 为中心的函数 *f*(*x*)；我们可以使用一个无限的多项式项和来进行逼近，如下所示：
- en: '![](img/B15558_13_009.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_009.png)'
- en: 'The preceding equation can be represented in sigma notation as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方程可以用 sigma 符号表示为：
- en: '![](img/B15558_13_010.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_010.png)'
- en: So for each term in the Taylor series, we calculate the *n*^(th) order derivative,
    divide them by *n*!, and multiply by (*x* – *a*)^n.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于泰勒级数中的每一项，我们计算 *n*^(次) 导数，将它们除以 *n*!，并乘以 (*x* – *a*)^n。
- en: 'Let''s understand how exactly the Taylor series approximates a function with
    an example. Let''s say we have an exponential function *e*^x as shown in *Figure
    13.1*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解泰勒级数是如何逼近一个函数的。假设我们有一个指数函数 *e*^x，如图 *13.1* 所示：
- en: '![](img/B15558_13_01.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_01.png)'
- en: 'Figure 13.1: Exponential function'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：指数函数
- en: 'Can we approximate the exponential function *e*^x using the Taylor series?
    We know that the Taylor series is given as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否使用泰勒级数逼近指数函数 *e*^x？我们知道，泰勒级数给出的是：
- en: '![](img/B15558_13_011.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_011.png)'
- en: 'Here, the function *f*(*x*) we want to approximate is *e*^x, that is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们要逼近的函数 *f*(*x*) 是 *e*^x，也就是说：
- en: '![](img/B15558_13_012.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_012.png)'
- en: 'Say our function *f*(*x*) = *e*^x is centered at *x* = *a*, first, let''s calculate
    the derivatives of the function up to 3 orders. The derivative of the exponential
    function is the function itself, so we can write:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的函数 *f*(*x*) = *e*^x 以 *x* = *a* 为中心，首先我们来计算该函数的前三阶导数。指数函数的导数就是函数本身，所以我们可以写成：
- en: '![](img/B15558_13_013.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_013.png)'
- en: 'Substituting the preceding terms in the equation (1), we can write:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的项代入方程（1），我们可以写成：
- en: '![](img/B15558_13_014.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_014.png)'
- en: 'Let''s suppose *a* = 0; then our equation becomes:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *a* = 0；那么我们的方程变为：
- en: '![](img/B15558_13_015.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_015.png)'
- en: 'We know that *e*⁰ =1; thus, the Taylor series of the exponential function is
    given as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 *e*⁰ = 1；因此，指数函数的泰勒级数表示为：
- en: '![](img/B15558_13_016.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_016.png)'
- en: 'It implies that the sum of the terms on the right-hand side approximates the
    exponential function *e*^x. Let''s understand this with the help of a plot. Let''s
    take only the terms till the 0^(th) order derivative from the Taylor series (equation
    2), that is, *e*^x = 1, and plot them:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着右侧项的和可以逼近指数函数 *e*^x。让我们通过一个图形来理解这一点。我们只取泰勒级数（方程 2）中的 0^(次) 导数项，也就是 *e*^x
    = 1，并绘制它们：
- en: '![](img/B15558_13_02.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_02.png)'
- en: 'Figure 13.2: Taylor series approximation till the 0^(th) order derivative'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：泰勒级数逼近至 0^(次) 导数
- en: 'As we can observe from the preceding plot, just taking the 0^(th) order derivative,
    we are far away from the actual function *e*^x. That is, our approximation is
    not good. So, let''s take the sum of terms till the 1^(st) order derivative from
    the Taylor series (equation 2), that is, *e*^x = 1 + *x*, and plot them:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中可以看出，仅取 0^(次) 导数，我们与实际的 *e*^x 函数相差甚远。也就是说，我们的逼近效果不好。那么，接下来我们取泰勒级数（方程 2）中的
    1^(次) 导数项，即 *e*^x = 1 + *x*，并绘制它们：
- en: '![](img/B15558_13_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_03.png)'
- en: 'Figure 13.3: Taylor series approximation till the 1^(st) order derivative'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：泰勒级数近似到一阶导数
- en: 'As we can observe from the preceding plot, including the terms till the 1^(st)
    order derivative from the Taylor series gets us closer to the actual function
    *e*^x. So, let''s take the sum of terms till the 2^(nd) order derivative from
    the Taylor series (equation 2), that is, ![](img/B15558_13_017.png), and plot
    them. As we can observe from the following plot our approximation gets better
    and we reach closer to the actual function *e*^x:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图中我们可以观察到，将泰勒级数展开到一阶导数能够使我们更接近实际的函数*e*^x。所以，让我们取泰勒级数展开到二阶导数的项（方程 2），即 ![](img/B15558_13_017.png)，并绘制它们。从以下图中我们可以观察到，我们的近似值变得更好，且更加接近实际的函数*e*^x：
- en: '![](img/B15558_13_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_04.png)'
- en: 'Figure 13.4: Taylor series approximation till the 2^(nd) order derivative'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：泰勒级数近似到二阶导数
- en: 'Now, let''s take the sum of terms till the 3^(rd) order derivative from the
    Taylor series, that is, ![](img/B15558_13_018.png), and plot them:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们取泰勒级数到三阶导数的项，即 ![](img/B15558_13_018.png)，并绘制它们：
- en: '![](img/B15558_13_05.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_05.png)'
- en: 'Figure 13.5: Taylor series approximation till the 3^(rd) order derivative'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：泰勒级数近似到三阶导数
- en: By looking at the preceding graph, we can understand that our approximation
    is far better after including the sum of terms till the 3^(rd) order derivative.
    As you might have guessed, adding more and more terms in the Taylor series makes
    our approximation of *e*^x better. Thus, using the Taylor series, we can approximate
    any function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看前面的图表，我们可以理解，加入三阶导数项的泰勒级数后，我们的近似值明显提高。正如你可能猜到的，加入更多的泰勒级数项使得我们对*e*^x的近似更加准确。因此，使用泰勒级数，我们可以近似任何函数。
- en: 'The Taylor polynomial till the first degree is called **linear approximation.**
    In linear approximation, we calculate the Taylor series only till the first-order
    derivative. Thus, the linear approximation (first-order) of the function *f*(*x*)
    around the point *a* can be given as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒多项式到一阶被称为**线性近似**。在线性近似中，我们只计算泰勒级数到一阶导数。因此，函数*f*(*x*)在点*a*附近的线性近似（一阶）可以表示为：
- en: '![](img/B15558_13_019.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_019.png)'
- en: 'We can denote our first-order derivative by ![](img/B15558_13_020.png), so
    we can just replace ![](img/B15558_13_021.png) by ![](img/B15558_13_022.png) and
    rewrite the preceding equation as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 ![](img/B15558_13_020.png) 来表示一阶导数，所以我们可以将 ![](img/B15558_13_021.png)
    替换为 ![](img/B15558_13_022.png)，并将前面的方程重写为：
- en: '![](img/B15558_13_023.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_023.png)'
- en: 'The Taylor polynomial till the second degree is called **quadratic approximation**.
    In quadratic approximation, we calculate the Taylor series only till the second-order
    derivative. Thus, the quadratic approximation (second-order) of the function *f*(*x*)
    around the point *a* can be given as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒多项式到二阶被称为**二次近似**。在二次近似中，我们只计算泰勒级数到二阶导数。因此，函数*f*(*x*)在点*a*附近的二次近似（二阶）可以表示为：
- en: '![](img/B15558_13_024.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_024.png)'
- en: 'We can denote our first-order derivative by ![](img/B15558_13_020.png) and
    second-order derivative by ![](img/B15558_13_026.png); so, we can just replace
    ![](img/B15558_13_021.png) with ![](img/B15558_13_020.png) and ![](img/B15558_13_029.png)
    with ![](img/B15558_13_026.png) and rewrite the preceding equation as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用 ![](img/B15558_13_020.png) 来表示一阶导数，用 ![](img/B15558_13_026.png) 来表示二阶导数；因此，我们可以将
    ![](img/B15558_13_021.png) 替换为 ![](img/B15558_13_020.png)，将 ![](img/B15558_13_029.png)
    替换为 ![](img/B15558_13_026.png)，并将前面的方程重写为：
- en: '![](img/B15558_13_031.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_031.png)'
- en: 'A Hessian is a second-order derivative, so we can denote ![](img/B15558_13_026.png)
    by *H*(*a*) and rewrite the preceding equation as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian 是二阶导数，所以我们可以用 *H*(*a*) 来表示 ![](img/B15558_13_026.png)，并将前面的方程重写为：
- en: '![](img/B15558_13_033.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_033.png)'
- en: 'Thus, to summarize, a **linear approximation** of the function *f*(*x*) is
    given as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，总结来说，函数*f*(*x*)的**线性近似**表示为：
- en: '![](img/B15558_13_034.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_034.png)'
- en: 'The **quadratic approximation** of the function *f*(*x*) is given as:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*f*(*x*)的**二次近似**表示为：
- en: '![](img/B15558_13_035.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_035.png)'
- en: The trust region method
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信赖域方法
- en: Let's say we have a function *f*(*x*) and we need to find the minimum of the
    function. Let's suppose it is difficult to find the minimum of the function *f*(*x*).
    So, what we can do is that we can use the Taylor series and approximate the given
    function *f*(*x*) and try to find the minimum value using the approximated function.
    Let's represent the approximated function with ![](img/B15558_13_036.png).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数 *f*(*x*)，并且我们需要找到该函数的最小值。假设找出 *f*(*x*) 的最小值是困难的。那么，我们可以做的是使用泰勒级数来近似给定函数
    *f*(*x*)，并尝试通过近似函数来寻找最小值。我们可以用 ![](img/B15558_13_036.png) 来表示近似函数。
- en: 'Say we use the quadratic approximation, we learned that with the quadratic
    approximation, we calculate the Taylor series only till the second-order derivative.
    Thus, the quadratic approximation (second-order) of the given function *f*(*x*)
    around the region *a* can be given as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用二次近似法，我们了解到，在二次近似法中，我们只计算泰勒级数的二阶导数。因此，给定函数 *f*(*x*) 在区域 *a* 周围的二次近似（第二阶）可以表示为：
- en: '![](img/B15558_13_037.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_037.png)'
- en: So, we can just use the approximated function ![](img/B15558_13_038.png) and
    compute the minimum value. But wait! What if our approximated function ![](img/B15558_13_036.png)
    is inaccurate at a particular point, say *a**, and if *a** is optimal, then we
    miss out on finding the optimal value.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以直接使用近似函数 ![](img/B15558_13_038.png) 来计算最小值。但等等！如果我们的近似函数 ![](img/B15558_13_036.png)
    在某一点（例如 *a*）上不准确，而 *a* 是最优解，那么我们就会错过找到最优值的机会。
- en: So, we will introduce a new constraint called the trust region constraint. The
    trust region implies the region where our actual function *f*(*x*) and approximated
    function ![](img/B15558_13_036.png) are close together. So, we can say that our
    approximation will be accurate if our approximated function ![](img/B15558_13_036.png)
    is in the trust region.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将引入一个新的约束，称为信任区域约束。信任区域意味着我们的实际函数 *f*(*x*) 和近似函数 ![](img/B15558_13_036.png)
    之间接近的区域。因此，我们可以说，如果我们的近似函数 ![](img/B15558_13_036.png) 在信任区域内，我们的近似将是准确的。
- en: 'For instance, as shown in *Figure 13.6*, our approximated function ![](img/B15558_13_036.png)
    is in the trust region and thus our approximation will be accurate since the approximated
    function ![](img/B15558_13_036.png) is closer to the actual function *f*(*x*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如图 13.6 所示，我们的近似函数 ![](img/B15558_13_036.png) 在信任区域内，因此我们的近似将是准确的，因为近似函数
    ![](img/B15558_13_036.png) 更接近实际函数 *f*(*x*)：
- en: '![](img/B15558_13_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_06.png)'
- en: 'Figure 13.6: Approximated function is in the trust region'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：近似函数在信任区域内
- en: 'But when ![](img/B15558_13_044.png) is not in the trust region, then our approximation
    will not be accurate since the approximated function ![](img/B15558_13_036.png)
    is far from the actual function *f*(*x*):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但当 ![](img/B15558_13_044.png) 不在信任区域内时，我们的近似将不准确，因为近似函数 ![](img/B15558_13_036.png)
    与实际函数 *f*(*x*) 相距较远：
- en: '![](img/B15558_13_07.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_07.png)'
- en: 'Figure 13.7: Approximated function is not in the trust region'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7：近似函数不在信任区域内
- en: Thus, we need to make sure that our approximated function stays in the trust
    region so that it will be close to the actual function.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要确保我们的近似函数保持在信任区域内，这样它就会接近实际函数。
- en: The conjugate gradient method
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 共轭梯度法
- en: 'The conjugate gradient method is an iterative method used to solve a system
    of linear equations. It is also used to solve the optimization problem. The conjugate
    gradient method is used when a system is of the form:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 共轭梯度法是一种迭代方法，用于求解线性方程组。它也用于求解优化问题。当系统形式为以下时，使用共轭梯度法：
- en: '![](img/B15558_13_046.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_046.png)'
- en: 'Where *A* is the positive definite, square, and symmetric matrix, *x* is the
    vector we want to find, and *b* is the known vector. Let''s consider the following
    quadratic function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*A* 是正定的、方形的对称矩阵，*x* 是我们要找的向量，*b* 是已知向量。我们考虑以下二次函数：
- en: '![](img/B15558_13_047.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_047.png)'
- en: When *A* is the positive semi-definite matrix; finding the minimum of this function
    is equal to solving the system *Ax* = *b*. Just like gradient descent, conjugate
    gradient descent also tries to find the minimum of the function; however, the
    search direction of conjugate gradient descent will be different from gradient
    descent, and conjugate gradient descent attains convergence in *N* iterations.
    Let's understand how conjugate gradient descent differs from gradient descent
    with the help of a contour plot.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当*A*是正半定矩阵时；找到该函数的最小值等同于求解系统*Ax* = *b*。与梯度下降法一样，共轭梯度下降法也试图找到函数的最小值；然而，共轭梯度下降法的搜索方向将与梯度下降法不同，而且共轭梯度下降法在*N*次迭代中达到收敛。让我们通过等高线图来理解共轭梯度下降法与梯度下降法的区别。
- en: 'First, let''s look at the contour plot of gradient descent. As we can see in
    the following plot, in order to find the minimum value of a function, gradient
    descent takes several search directions and we get a zigzag pattern of directions:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看梯度下降法的等高线图。正如我们在下图中看到的那样，为了找到一个函数的最小值，梯度下降法需要多次搜索方向，最终形成一个锯齿形的方向模式：
- en: '![](img/B15558_13_08.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_08.png)'
- en: 'Figure 13.8: Contour plot of gradient descent'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：梯度下降法的等高线图
- en: 'Unlike the gradient descent method, in the conjugate gradient descent, the
    search direction is orthogonal to the previous search direction as shown in *Figure
    13.9*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与梯度下降法不同，在共轭梯度下降法中，搜索方向与前一个搜索方向是正交的，如*图13.9*所示：
- en: '![](img/B15558_13_09.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_09.png)'
- en: 'Figure 13.9: Contour plot of conjugate gradient descent'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：共轭梯度下降法的等高线图
- en: So, using conjugate gradient descent, we can solve a system of the form *Ax*
    = *b*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用共轭梯度下降法，我们可以求解形如*Ax* = *b*的系统。
- en: Lagrange multipliers
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拉格朗日乘子
- en: 'Let''s say we have a function *f*(*x*) = *x*²: how do we find the minimum of
    the function? We can find the minimum of the function by finding a point where
    the gradient of the function is zero. The gradient of the function *f*(*x*) =
    *x*² is given as:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数*f*(*x*) = *x*²：我们如何找到该函数的最小值？我们可以通过找到梯度为零的点来找到该函数的最小值。函数*f*(*x*) =
    *x*²的梯度为：
- en: '![](img/B15558_13_048.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_048.png)'
- en: When *x* = 0, the gradient of the function is zero; that is, ![](img/B15558_13_049.png)
    when *x* = 0\. So, we can say that the minimum of the function *f*(*x*) = *x*²
    is at *x* = 0\. The problem we just saw is called the unconstrained optimization
    problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x* = 0时，函数的梯度为零；即，当*x* = 0时，![](img/B15558_13_049.png)。因此，我们可以说，函数*f*(*x*)
    = *x*²的最小值出现在*x* = 0处。我们刚才看到的问题称为无约束优化问题。
- en: 'Consider a case where we have a constraint—say we need to minimize the function
    *f*(*x*) subject to the constraint that *g*(*x*) = 1, as shown here:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个有约束的情况——假设我们需要最小化函数*f*(*x*)，同时满足约束*g*(*x*) = 1，如下所示：
- en: '![](img/B15558_13_050.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_050.png)'
- en: 'Now, how can we solve this problem? That is, how can we find the minimum of
    the function *f*(*x*) while satisfying the constraint *g*(*x*)? We can find the
    minimum value when the gradient of the objective function *f*(*x*) and the gradient
    of the constraint *g*(*x*) point in the same direction. That is, we can find the
    minimum value when the gradient of *f*(*x*) and the gradient of *g*(*x*) are parallel
    or antiparallel to each other:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何解决这个问题呢？也就是说，我们如何在满足约束*g*(*x*)的情况下找到函数*f*(*x*)的最小值？当目标函数*f*(*x*)的梯度和约束*g*(*x*)的梯度指向相同方向时，我们可以找到最小值。也就是说，当*f*(*x*)的梯度和*g*(*x*)的梯度平行或反平行时，我们可以找到最小值：
- en: '![](img/B15558_13_051.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_051.png)'
- en: 'Although the gradients of *f*(*x*) and *g*(*x*) point in the same direction,
    their magnitude will not be the same. So, we will just multiply the gradient of
    *g*(*x*) by a variable called ![](img/B15558_13_052.png) as shown here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*f*(*x*)和*g*(*x*)的梯度方向相同，但它们的大小并不相同。因此，我们将把*g*(*x*)的梯度乘以一个叫做![](img/B15558_13_052.png)的变量，如下所示：
- en: '![](img/B15558_13_053.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_053.png)'
- en: 'Where ![](img/B15558_13_054.png) is known as the Lagrange multiplier. So, we
    can rewrite the preceding equation as:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B15558_13_054.png)被称为拉格朗日乘子。因此，我们可以将前面的方程重新写为：
- en: '![](img/B15558_13_055.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_055.png)'
- en: 'Solving the preceding equation implies that we find the minimum of the function
    *f*(*x*) along with satisfying the constraint *g*(*x*). So, we can rewrite our
    objective function as:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 解前面的方程意味着我们需要找到函数*f*(*x*)的最小值，同时满足约束*g*(*x*)。因此，我们可以将目标函数重新写为：
- en: '![](img/B15558_13_056.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_056.png)'
- en: 'The gradient of the preceding function is given as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前述函数的梯度为：
- en: '![](img/B15558_13_057.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_057.png)'
- en: We can find the minimum value when ![](img/B15558_13_058.png). Lagrange multipliers
    are widely used for solving constrained optimization problems.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/B15558_13_058.png) 时，我们可以找到最小值。拉格朗日乘子被广泛用于求解约束优化问题。
- en: 'Let''s understand this with one more example. Say we want to find the minimum
    of the function ![](img/B15558_13_059.png) subject to the constraint ![](img/B15558_13_060.png),
    as the following shows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过另一个例子来理解这一点。假设我们想要求解函数 ![](img/B15558_13_059.png) 的最小值，同时满足约束 ![](img/B15558_13_060.png)，如下所示：
- en: '![](img/B15558_13_061.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_061.png)'
- en: 'We can rewrite our objective function with the constraint multiplied by the
    Lagrange multiplier as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将目标函数重写为带有拉格朗日乘子的约束形式：
- en: '![](img/B15558_13_062.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_062.png)'
- en: Solving for ![](img/B15558_13_063.png), we can find the minimum of the function
    ![](img/B15558_13_059.png) along with satisfying the constraint that ![](img/B15558_13_065.png).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过求解 ![](img/B15558_13_063.png)，我们可以找到函数 ![](img/B15558_13_059.png) 的最小值，同时满足约束条件
    ![](img/B15558_13_065.png)。
- en: Importance sampling
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重要性采样
- en: 'Let''s recap the importance sampling method we learned in *Chapter 4*, *Monte
    Carlo Methods*. Say we want to compute the expectation of a function *f*(*x*)
    where the value of *x* is sampled from the distribution *p*(*x*), that is, ![](img/B15558_13_066.png);
    we can write:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在 *第4章*，*蒙特卡罗方法* 中学到的 重要性采样方法。假设我们想计算函数 *f*(*x*) 的期望，其中 *x* 的值是从分布
    *p*(*x*) 中采样的，也就是说，![](img/B15558_13_066.png)；我们可以写为：
- en: '![](img/B15558_13_067.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_067.png)'
- en: 'Can we approximate the expectation of a function *f*(*x*)? We learned that
    using the Monte Carlo method, we can approximate the expectation as:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否近似函数 *f*(*x*) 的期望？我们已经学到，使用蒙特卡罗方法时，可以通过如下方式近似期望：
- en: '![](img/B15558_13_068.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_068.png)'
- en: That is, using the Monte Carlo method, we sample *x* from the distribution *p*(*x*)
    for *N* times and compute the average of *f(x)* to approximate the expectation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，使用蒙特卡罗方法时，我们从分布 *p*(*x*) 中采样 *x* 进行 *N* 次，并计算 *f(x)* 的平均值来近似期望。
- en: 'Instead of using the Monte Carlo method, we can also use importance sampling
    to approximate the expectation. In the importance sampling method, we estimate
    the expectation using a different distribution *q*(*x*); that is, instead of sampling
    *x* from *p*(*x*) we use a different distribution *q*(*x*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以使用蒙特卡罗方法，还可以使用重要性采样来近似期望。在重要性采样方法中，我们使用不同的分布 *q*(*x*) 来估计期望；也就是说，代替从 *p*(*x*)
    中采样 *x*，我们使用不同的分布 *q*(*x*)：
- en: '![](img/B15558_13_069.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_069.png)'
- en: The ratio ![](img/B15558_04_146.png) is called the importance sampling ratio
    or the importance correction.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 该比率 ![](img/B15558_04_146.png) 被称为重要性采样比率或重要性修正。
- en: Now that we have understood the several important math prerequisites, we will
    learn how the TRPO algorithm works in the next section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了几个重要的数学先决条件，接下来我们将学习 TRPO 算法是如何工作的。
- en: Designing the TRPO objective function
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计 TRPO 目标函数
- en: At the beginning of the chapter, we learned that TRPO tries to make a large
    policy update while imposing the constraint that the parameter of the old policy
    and the new policy should stay within the trust region. In this section, we will
    learn how to design the TRPO objective function along with the trust region constraint
    so that the old policy and the new policy will not vary very much.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们学到 TRPO 试图在施加约束的同时进行大幅度的策略更新，要求旧策略和新策略的参数保持在信任区域内。在本节中，我们将学习如何设计 TRPO
    目标函数以及信任区域约束，以确保旧策略和新策略之间不会有太大差异。
- en: This section will be pretty dense and optional. If you are not interested in
    math you can directly navigate to the section *Solving the TRPO objective function*,
    where we learn how to solve the TRPO objective function step by step.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容可能比较复杂且是可选的。如果你对数学不感兴趣，可以直接跳转到 *求解 TRPO 目标函数* 这一节，在那里我们将一步步学习如何求解 TRPO 目标函数。
- en: 'Let us say we have a policy ![](img/B15558_03_140.png); we can express the
    expected discounted return ![](img/B15558_13_072.png) following the policy ![](img/B15558_03_008.png)
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个策略 ![](img/B15558_03_140.png)；我们可以表示遵循该策略 ![](img/B15558_03_008.png)
    的期望折扣回报 ![](img/B15558_13_072.png) 如下：
- en: '![](img/B15558_13_074.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_074.png)'
- en: 'We know that in the policy gradient method, on every iteration, we keep on
    improving the policy ![](img/B15558_03_055.png). Say we updated our old policy
    ![](img/B15558_03_140.png) and have a new policy ![](img/B15558_13_077.png); then,
    we can express the expected discounted return ![](img/B15558_13_078.png), following
    the new policy ![](img/B15558_13_079.png) in terms of advantage over the old policy
    ![](img/B15558_03_084.png), as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在策略梯度方法中，每次迭代时，我们都在不断改进策略 ![](img/B15558_03_055.png)。假设我们更新了旧策略 ![](img/B15558_03_140.png)，并得到了一个新策略
    ![](img/B15558_13_077.png)；那么，我们可以将新策略 ![](img/B15558_13_079.png) 下的期望折扣回报 ![](img/B15558_13_078.png)
    表示为相对于旧策略 ![](img/B15558_03_084.png) 的优势，如下所示：
- en: '![](img/B15558_13_081.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_081.png)'
- en: 'As we can notice from the preceding equation, the expected return following
    the new policy ![](img/B15558_13_082.png), that is, ![](img/B15558_13_083.png),
    is just the sum of the expected return following the old policy ![](img/B15558_03_084.png),
    that is, ![](img/B15558_13_085.png), and the expected discounted advantage of
    the old policy ![](img/B15558_13_086.png). That is:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前面的方程中可以注意到的，遵循新政策的期望回报 ![](img/B15558_13_082.png)，也就是 ![](img/B15558_13_083.png)，仅仅是遵循旧政策的期望回报
    ![](img/B15558_03_084.png)，也就是 ![](img/B15558_13_085.png)，以及旧政策的期望折扣优势 ![](img/B15558_13_086.png)
    的总和。也就是说：
- en: '![](img/B15558_13_12.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_12.png)'
- en: But, why are we using the advantage of the old policy? Because we are measuring
    how good the new policy ![](img/B15558_13_087.png) is with respect to the average
    performance of the old policy ![](img/B15558_03_008.png).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么我们要使用旧政策的优势呢？因为我们是在衡量新政策 ![](img/B15558_13_087.png) 相对于旧政策 ![](img/B15558_03_008.png)
    的平均表现有多好。
- en: 'We can simplify the equation (2) and replace the sum over time steps with the
    sum over states and actions as shown here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简化方程（2），并将时间步的求和替换为状态和动作的求和，如下所示：
- en: '![](img/B15558_13_089.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_089.png)'
- en: Where ![](img/B15558_13_090.png) is the discounted visitation frequency of the
    new policy. We already learned that the expected return of the new policy ![](img/B15558_13_091.png)
    is obtained by adding the expected return of the old policy ![](img/B15558_13_092.png)
    and the advantage of the old policy ![](img/B15558_13_093.png).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_13_090.png) 是新政策的折扣访问频率。我们已经学过，新政策的期望回报 ![](img/B15558_13_091.png)
    是通过将旧政策的期望回报 ![](img/B15558_13_092.png) 和旧政策的优势 ![](img/B15558_13_093.png) 相加得到的。
- en: In the preceding equation (3), if the advantage ![](img/B15558_13_094.png) is
    always positive, then it means that our policy is improving and we have better
    ![](img/B15558_13_095.png). That is, if the advantage ![](img/B15558_13_094.png)
    is always ![](img/B15558_13_097.png), then we will always have an improvement
    in our policy.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程（3）中，如果优势 ![](img/B15558_13_094.png) 始终为正，那么这意味着我们的政策在改进，并且我们有更好的 ![](img/B15558_13_095.png)。也就是说，如果优势
    ![](img/B15558_13_094.png) 始终为 ![](img/B15558_13_097.png)，那么我们将始终在我们的政策中看到改进。
- en: 'However, equation (3) is difficult to optimize, so we approximate ![](img/B15558_13_098.png)
    by a local approximate ![](img/B15558_13_099.png):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，方程（3）很难优化，因此我们用局部近似值 ![](img/B15558_13_099.png) 来逼近 ![](img/B15558_13_098.png)：
- en: '![](img/B15558_13_100.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_100.png)'
- en: As you may notice, unlike equation (3), in equation (4) we use ![](img/B15558_13_101.png)
    instead of ![](img/B15558_13_102.png). That is, we use a discounted visitation
    frequency of the old policy ![](img/B15558_13_103.png) instead of the new policy
    ![](img/B15558_13_090.png). But why do we have to do that? Because we already
    have trajectories sampled from the old policy, so it is easier to obtain ![](img/B15558_13_103.png)
    than ![](img/B15558_13_090.png).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所注意到的，与方程（3）不同，在方程（4）中，我们使用 ![](img/B15558_13_101.png) 而不是 ![](img/B15558_13_102.png)。也就是说，我们使用旧政策的折扣访问频率
    ![](img/B15558_13_103.png)，而不是新政策的折扣访问频率 ![](img/B15558_13_090.png)。但为什么我们要这么做呢？因为我们已经有了从旧政策采样的轨迹，所以比起新政策，获取
    ![](img/B15558_13_103.png) 更容易。
- en: A surrogate function is a function that is an approximate of the objective function;
    so, we can call ![](img/B15558_13_107.png) a surrogate function since it is the
    local approximate of our objective function ![](img/B15558_13_098.png).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 替代函数是目标函数的近似函数；因此，我们可以称 ![](img/B15558_13_107.png) 为替代函数，因为它是我们目标函数 ![](img/B15558_13_098.png)
    的局部近似。
- en: Thus, ![](img/B15558_13_109.png) is the local approximate of our objective ![](img/B15558_13_091.png).
    We need to make sure that our local approximate is accurate. Remember how, in
    the *The trust region method* section, we learned that the local approximation
    of the function will be accurate if it is in the trust region? So, our local approximate
    ![](img/B15558_13_111.png) will be accurate if it is in the trust region. Thus,
    while updating the values of ![](img/B15558_13_112.png), we need to make sure
    that it remains in the trust region; that is, the policy updates should remain
    in the trust region.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，![](img/B15558_13_109.png) 是我们目标函数 ![](img/B15558_13_091.png) 的局部近似。我们需要确保我们的局部近似是准确的。还记得在*信任区域方法*部分，我们学习到如果函数的局部近似在信任区域内，它将是准确的吗？因此，如果我们的局部近似
    ![](img/B15558_13_111.png) 在信任区域内，它将是准确的。因此，在更新 ![](img/B15558_13_112.png) 的值时，我们需要确保它仍然保持在信任区域内；也就是说，策略更新应保持在信任区域内。
- en: 'So, when we update the old policy ![](img/B15558_03_140.png) to a new policy
    ![](img/B15558_13_079.png), we just need to ensure that the new policy update
    stays within the trust region. In order to do that, we have to measure how far
    our new policy is from the old policy, so, we use the KL divergence to measure
    this:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当我们将旧策略 ![](img/B15558_03_140.png) 更新为新策略 ![](img/B15558_13_079.png) 时，我们只需确保新的策略更新保持在信任区域内。为了做到这一点，我们必须衡量新策略与旧策略之间的距离，因此，我们使用
    KL 散度来衡量这一点：
- en: '![](img/B15558_13_115.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_115.png)'
- en: 'Therefore, while updating the policy, we check the KL divergence between the
    policy updates and make sure that our policy updates are within the trust region.
    To satisfy this KL constraint, Kakade and Langford introduced a new policy updating
    scheme called conservative policy iteration and derived the following lower bound:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在更新策略时，我们检查策略更新之间的 KL 散度，并确保我们的策略更新保持在信任区域内。为了满足这一 KL 约束，Kakade 和 Langford
    提出了一个新的策略更新方案，称为保守策略迭代，并推导出了以下下界：
- en: '![](img/B15558_13_116.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_116.png)'
- en: As we can observe, in the preceding equation, we have the KL divergence as the
    penalty term and *C* is the penalty coefficient.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，在前面的方程中，我们将 KL 散度作为惩罚项，*C* 是惩罚系数。
- en: 'Now, our surrogate objective function (4) along with the penalized KL term
    is written as:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的代理目标函数 (4) 以及惩罚的 KL 项被写为：
- en: '![](img/B15558_13_117.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_117.png)'
- en: Maximizing the surrogate function ![](img/B15558_13_118.png) improves our true
    objective function ![](img/B15558_13_098.png) and guarantees a monotonic improvement
    in the policy. The preceding objective function is known as **KL penalized objective.**
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化代理函数 ![](img/B15558_13_118.png) 可以改进我们的真实目标函数 ![](img/B15558_13_098.png)，并保证策略的单调改进。前面的目标函数被称为
    **KL 惩罚目标**。
- en: Parameterizing the policies
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略的参数化
- en: We learned that maximizing the surrogate objective function maximizes our true
    objective function. We know that in the policy gradient method, we use a parameterized
    policy; that is, we use a function approximator like a neural network parameterized
    by some parameter ![](img/B15558_09_054.png) and learn the optimal policy.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，最大化代理目标函数可以最大化我们的真实目标函数。我们知道在策略梯度方法中，我们使用参数化策略；也就是说，我们使用像神经网络这样的函数逼近器，该网络通过某些参数
    ![](img/B15558_09_054.png) 进行参数化，并学习最优策略。
- en: 'We parameterize the old policy with ![](img/B15558_10_037.png) as ![](img/B15558_13_122.png)
    and the new policy with ![](img/B15558_12_330.png) as ![](img/B15558_13_124.png).
    So, we can rewrite our equation (5) in terms of parameterized policies as shown
    here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用 ![](img/B15558_10_037.png) 对旧策略进行参数化，表示为 ![](img/B15558_13_122.png)，用 ![](img/B15558_12_330.png)
    对新策略进行参数化，表示为 ![](img/B15558_13_124.png)。因此，我们可以将我们的方程 (5) 用参数化策略重新写成如下形式：
- en: '![](img/B15558_13_125.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_125.png)'
- en: 'As shown in the preceding equation, we are using the max KL divergence between
    the old and new policies, that is, ![](img/B15558_13_126.png). It is difficult
    to optimize our objective with the max KL term, so instead of using max KL, we
    can take the average KL divergence ![](img/B15558_13_127.png) and rewrite our
    surrogate objective as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的方程所示，我们使用旧策略与新策略之间的最大 KL 散度，即 ![](img/B15558_13_126.png)。使用最大 KL 项优化目标是困难的，因此我们可以取平均
    KL 散度 ![](img/B15558_13_127.png)，并将代理目标函数重写为：
- en: '![](img/B15558_13_128.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_128.png)'
- en: The issue with the preceding objective function is that when we substitute the
    value of the penalty coefficient *C* as ![](img/B15558_13_129.png), it reduces
    the step size, and it takes us a lot of time to attain convergence.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 上述目标函数的问题在于，当我们将惩罚系数 *C* 的值替换为 ![](img/B15558_13_129.png) 时，会减少步长，这使得我们需要花费很多时间才能达到收敛。
- en: 'So, we can redefine our surrogate objective function as a constrained objective
    function as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将代理目标函数重新定义为一个约束目标函数，如下所示：
- en: '![](img/B15558_13_130.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_130.png)'
- en: The preceding equation implies that we maximize our surrogate objective function
    ![](img/B15558_13_131.png) while maintaining the constraint that the KL divergence
    between the old policy ![](img/B15558_13_132.png) and new policy ![](img/B15558_10_111.png)
    is less than or equal to a constant ![](img/B15558_09_135.png), and it ensures
    that our old policy and the new policy will not vary very much. The preceding
    objective function is called the **KL-constrained objective.**
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程意味着我们在最大化代理目标函数 ![](img/B15558_13_131.png) 的同时，保持约束条件，即旧策略 ![](img/B15558_13_132.png)
    和新策略 ![](img/B15558_10_111.png) 之间的 KL 散度小于等于常数 ![](img/B15558_09_135.png)，并确保我们的旧策略和新策略不会发生太大变化。上述目标函数称为
    **KL 约束目标**。
- en: Sample-based estimation
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于样本的估计
- en: In the previous section, we learned how to frame our objective function as a
    KL-constrained objective with parameterized policies. In this section, we will
    learn how to simplify our objective function.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们学习了如何将目标函数框架设置为带有参数化策略的 KL 约束目标。在本节中，我们将学习如何简化我们的目标函数。
- en: 'We learned that our KL constrained objective function is given as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到我们的 KL 约束目标函数如下所示：
- en: '![](img/B15558_13_135.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_135.png)'
- en: 'From equation (4), substituting the value of ![](img/B15558_13_136.png) with
    ![](img/B15558_13_137.png) in the preceding equation, we can write:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程（4）开始，在前面的方程中将 ![](img/B15558_13_136.png) 替换为 ![](img/B15558_13_137.png)，我们可以写成：
- en: '![](img/B15558_13_138.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_138.png)'
- en: Now we will see how we can simplify equation (9) by getting rid of the two summations
    using sampling.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何通过使用采样来去除两个求和项，从而简化方程（9）。
- en: 'The first sum ![](img/B15558_13_139.png) expresses the summation over state
    visitation frequency; we can replace it by sampling states from state visitation
    as ![](img/B15558_13_140.png). Then, our equation becomes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个求和项 ![](img/B15558_13_139.png) 表示状态访问频率的求和；我们可以通过从状态访问中采样状态来替换它，记为 ![](img/B15558_13_140.png)。然后，我们的方程变为：
- en: '![](img/B15558_13_141.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_141.png)'
- en: 'Next, we replace the sum over actions ![](img/B15558_13_142.png) with an importance
    sampling estimator. Let *q* be the sampling distribution, and *a* is sampled from
    *q*, that is, ![](img/B15558_13_143.png). Then, we can rewrite our preceding equation
    as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过重要性采样估计器替换求和操作！[](img/B15558_13_142.png)。设 *q* 为采样分布，*a* 从 *q* 中采样，即
    ![](img/B15558_13_143.png)。然后，我们可以将前面的方程重写为：
- en: '![](img/B15558_13_144.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_144.png)'
- en: 'Replacing the sampling distribution *q* with ![](img/B15558_13_145.png), we
    can write:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 将采样分布 *q* 替换为 ![](img/B15558_13_145.png)，我们可以写成：
- en: '![](img/B15558_13_146.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_146.png)'
- en: 'Thus, our equation (9) becomes:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的方程（9）变为：
- en: '![](img/B15558_13_147.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_147.png)'
- en: In the next section, we will learn how to solve the preceding objective function
    to find the optimal policy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将学习如何求解前面的目标函数，以找到最优策略。
- en: Solving the TRPO objective function
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 求解 TRPO 目标函数
- en: 'In the previous section, we learned that the TRPO objective function is expressed
    as:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学习了 TRPO 目标函数的表示：
- en: '![](img/B15558_13_147.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_147.png)'
- en: The preceding equation implies that we try to find the policy that gives the
    maximum return along with the constraint that the KL divergence between the old
    and new policies should be less than or equal to ![](img/B15558_09_135.png). This
    KL constraint makes sure that our new policy is not too far away from the old
    policy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程意味着我们尝试找到一个策略，使其在满足旧策略和新策略之间的 KL 散度小于等于 ![](img/B15558_09_135.png) 的约束条件下，能够提供最大的回报。这个
    KL 约束确保我们的新策略不会离旧策略太远。
- en: 'For notation brevity, let us represent our objective with ![](img/B15558_13_150.png)
    and the KL constraint with ![](img/B15558_13_151.png) and rewrite the preceding
    equation as:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号表示，我们用 ![](img/B15558_13_150.png) 表示我们的目标函数，用 ![](img/B15558_13_151.png)
    表示 KL 约束，并将前面的方程重写为：
- en: '![](img/B15558_13_152.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_152.png)'
- en: 'By maximizing our objective function ![](img/B15558_13_153.png), we can find
    the optimal policy. We can maximize the objective ![](img/B15558_13_154.png) by
    calculating gradients with respect to ![](img/B15558_09_056.png) and update the
    parameter using gradient ascent as:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化我们的目标函数 ![](img/B15558_13_153.png)，我们可以找到最优策略。我们可以通过计算相对于 ![](img/B15558_09_056.png)
    的梯度，并使用梯度上升法更新参数，从而最大化目标 ![](img/B15558_13_154.png)：
- en: '![](img/B15558_13_156.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_156.png)'
- en: Where ![](img/B15558_13_157.png) is the search direction (gradient) and ![](img/B15558_13_158.png)
    is the backtracking coefficient.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_13_157.png) 是搜索方向（梯度），![](img/B15558_13_158.png) 是回溯系数。
- en: 'That is, to update the parameter ![](img/B15558_09_054.png), we perform the
    two following steps:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，为了更新参数 ![](img/B15558_09_054.png)，我们执行以下两个步骤：
- en: First, we compute the search direction ![](img/B15558_13_160.png) using the
    Taylor series approximation
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们使用泰勒级数近似计算搜索方向 ![](img/B15558_13_160.png)
- en: Next, we perform the line search in the computed search direction ![](img/B15558_13_161.png)
    by finding the value of ![](img/B15558_09_143.png) using the backtracking line
    search method
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们通过回溯线搜索方法，在计算出的搜索方向 ![](img/B15558_13_161.png) 中执行线搜索，找到 ![](img/B15558_09_143.png)
    的值。
- en: We will learn what the backtracking coefficient is and how exactly the backtracking
    line search method works in the *Performing a line search in the search direction*
    section. Okay, but why do we have to perform these two steps? If you look at our
    objective function (10), we have a constrained optimization problem. Our constraint
    here is that while updating the parameter ![](img/B15558_09_054.png), we need
    to make sure that our parameter updates are within the trust region; that is,
    the KL divergence between the old and new parameters should be less than or equal
    to ![](img/B15558_13_164.png).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*在搜索方向中执行线搜索*部分学习回溯系数是什么，以及回溯线搜索方法是如何工作的。好的，但为什么我们需要执行这两个步骤呢？如果你看一下我们的目标函数（10），你会发现我们有一个约束优化问题。我们的约束是，在更新参数
    ![](img/B15558_09_054.png) 时，我们需要确保参数更新在可信区域内；也就是说，旧参数和新参数之间的KL散度应该小于或等于 ![](img/B15558_13_164.png)。
- en: Thus, performing these two steps and updating our parameter helps us satisfy
    the KL constraint and also guarantees monotonic improvement. Let's get into details
    and learn how exactly the two steps work.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，执行这两个步骤并更新我们的参数有助于我们满足KL约束，并且还确保单调改进。让我们深入了解这两个步骤是如何工作的。
- en: Computing the search direction
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算搜索方向
- en: It is difficult to optimize our objective function (10) directly, so first,
    we approximate our function using the Taylor series. We approximate the surrogate
    objective function ![](img/B15558_13_154.png) using linear approximation and we
    approximate our constraint ![](img/B15558_13_166.png) using quadratic approximation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 直接优化我们的目标函数（10）是困难的，因此首先我们使用泰勒级数来近似我们的函数。我们使用线性近似来近似替代目标函数 ![](img/B15558_13_154.png)，并使用二次近似来近似我们的约束条件
    ![](img/B15558_13_166.png)。
- en: To better understand the upcoming steps, recap *The Taylor series* from the
    *Math essentials* section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解接下来的步骤，回顾一下*泰勒级数*，可以参考*数学基础*部分。
- en: 'The **linear approximation** of our objective function at a point ![](img/B15558_13_167.png)
    is given as:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目标函数在点 ![](img/B15558_13_167.png) 处的**线性近似**为：
- en: '![](img/B15558_13_168.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_168.png)'
- en: 'We represent the gradient ![](img/B15558_13_169.png) with *g*, so the preceding
    equation becomes:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用*g*表示梯度 ![](img/B15558_13_169.png)，因此前面的方程变为：
- en: '![](img/B15558_13_170.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_170.png)'
- en: 'While solving the preceding equation, the value of ![](img/B15558_13_171.png)
    becomes zero, so we can write:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在求解前面的方程时，![](img/B15558_13_171.png) 的值变为零，因此我们可以写作：
- en: '![](img/B15558_13_172.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_172.png)'
- en: 'The **quadratic approximation** of our constraint at point ![](img/B15558_13_173.png)
    is given as:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们约束条件在点 ![](img/B15558_13_173.png) 处的**二次近似**为：
- en: '![](img/B15558_13_174.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_174.png)'
- en: Where *H* is the second-order derivative, that is, ![](img/B15558_13_175.png).
    In the preceding equation, the first term ![](img/B15558_13_176.png) becomes zero
    as the KL divergence between two identical distributions is zero, and the first-order
    derivative ![](img/B15558_13_177.png) becomes zero at ![](img/B15558_13_178.png).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *H* 是二阶导数，即 ![](img/B15558_13_175.png)。在前面的方程中，第一项 ![](img/B15558_13_176.png)
    变为零，因为两个相同分布之间的KL散度为零，且一阶导数 ![](img/B15558_13_177.png) 在 ![](img/B15558_13_178.png)
    处变为零。
- en: 'So, our final equation becomes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的最终方程变为：
- en: '![](img/B15558_13_179.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_179.png)'
- en: 'Substituting (11) and (12) in the equation (10), we can write:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (11) 和 (12) 代入方程 (10)，我们可以写成：
- en: '![](img/B15558_13_180.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_180.png)'
- en: Note that in the preceding equation, ![](img/B15558_13_181.png) represents the
    parameter of the old policy and ![](img/B15558_09_054.png) represents the parameter
    of the new policy.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的方程中，![](img/B15558_13_181.png) 代表旧策略的参数，![](img/B15558_09_054.png) 代表新策略的参数。
- en: As we can observe, in equation (13) we have a constrained optimization problem.
    How can we solve this? We can solve this using the Lagrange multiplier.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在方程 (13) 中，我们有一个约束优化问题。我们如何解决这个问题？我们可以使用拉格朗日乘子法来求解。
- en: 'Thus, using the Lagrange multiplier ![](img/B15558_13_054.png), we can rewrite
    our objective function (13) as:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用拉格朗日乘子 ![](img/B15558_13_054.png)，我们可以将我们的目标函数 (13) 重写为：
- en: '![](img/B15558_13_184.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_184.png)'
- en: 'For notation brevity, let *s* represent ![](img/B15558_13_185.png), so we can
    rewrite equation (14) as:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，设 *s* 代表 ![](img/B15558_13_185.png)，因此我们可以将方程 (14) 重写为：
- en: '![](img/B15558_13_186.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_186.png)'
- en: 'Our goal is to find the optimal parameter ![](img/B15558_09_087.png). So, we
    need to calculate the gradient of the preceding function and update our parameter
    using gradient ascent as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到最优参数 ![](img/B15558_09_087.png)。因此，我们需要计算前述函数的梯度，并使用梯度上升法更新参数，如下所示：
- en: '![](img/B15558_13_188.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_188.png)'
- en: Where ![](img/B15558_06_030.png) is the learning rate and *s* is the gradient.Now
    we will look at how to determine the learning rate ![](img/B15558_13_190.png)
    and the gradient *s*.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_06_030.png) 是学习率，*s* 是梯度。现在我们来看一下如何确定学习率 ![](img/B15558_13_190.png)
    和梯度 *s*。
- en: 'First, we compute *s*. Calculating the derivative of the objective function
    *L* given in equation (15) with respect to gradient *s*, we can write:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算 *s*。通过对方程 (15) 中给出的目标函数 *L* 关于梯度 *s* 的导数，我们可以写成：
- en: '![](img/B15558_13_191.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_191.png)'
- en: 'Thus, we can write:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成：
- en: '![](img/B15558_13_192.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_192.png)'
- en: '![](img/B15558_13_193.png) is just our Lagrange multiplier and it will not
    affect our gradient, so we can write:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15558_13_193.png) 只是我们的拉格朗日乘子，它不会影响我们的梯度，所以我们可以写成：'
- en: '![](img/B15558_13_194.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_194.png)'
- en: 'Thus, we can write:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成：
- en: '![](img/B15558_13_195.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_195.png)'
- en: However, computing the value of *s* directly in this way is not optimal. This
    is because in the preceding equation, we have ![](img/B15558_13_196.png), which
    implies the inverse of the second-order derivative. Computing the second-order
    derivative and its inverse is a expensive task. So, we need to find a better way
    to compute *s*; how we do that?
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接计算 *s* 的值并不是最优的。这是因为在前述方程中，我们有 ![](img/B15558_13_196.png)，这意味着需要计算二阶导数的逆。计算二阶导数及其逆是一个昂贵的任务。所以，我们需要找到一种更好的方法来计算
    *s*；我们该如何做呢？
- en: 'From (17), we learned that:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 从 (17) 中，我们了解到：
- en: '![](img/B15558_13_197.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_197.png)'
- en: 'From the preceding equation, we can observe the equation is in the form of
    *Ax* = *B*. Thus, using conjugate gradient descent, we can approximate the value
    of *s* as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述方程中，我们可以观察到该方程的形式是 *Ax* = *B*。因此，使用共轭梯度下降法，我们可以近似计算 *s* 的值为：
- en: '![](img/B15558_13_198.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_198.png)'
- en: 'Thus, our update equation becomes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的更新方程变为：
- en: '![](img/B15558_13_199.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_199.png)'
- en: Where the value of ![](img/B15558_13_200.png) is computed using conjugated gradient
    descent.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_13_200.png) 的值是使用共轭梯度下降法计算得到的。
- en: Now that we have calculated the gradient, we need to determine the learning
    rate ![](img/B15558_06_030.png). We need to keep in mind that our update should
    be within the trust region, so while calculating the value of ![](img/B15558_09_152.png),
    we need to maintain the KL constraint.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算出了梯度，我们需要确定学习率 ![](img/B15558_06_030.png)。我们需要记住，更新应当在信任区域内，因此在计算 ![](img/B15558_09_152.png)
    的值时，我们需要保持 KL 约束。
- en: 'In equation (18), we learned that our update rule is:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程 (18) 中，我们了解到我们的更新规则是：
- en: '![](img/B15558_13_203.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_203.png)'
- en: 'By rearranging the terms, we can write:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整项的顺序，我们可以写成：
- en: '![](img/B15558_13_204.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_204.png)'
- en: 'From equation (13), we can write our KL constraint as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 (13) 中，我们可以将 KL 约束写为：
- en: '![](img/B15558_13_205.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_205.png)'
- en: 'Substituting (19) in the preceding equation, we can write:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将 (19) 代入前述方程中，我们可以写成：
- en: '![](img/B15558_13_206.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_206.png)'
- en: 'The preceding equation can be solved as:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程可以解为：
- en: '![](img/B15558_13_207.png)![](img/B15558_13_208.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_207.png)![](img/B15558_13_208.png)'
- en: 'Thus, we can substitute the preceding value of the learning rate ![](img/B15558_06_030.png)
    in the equation (18) and rewrite our parameter update as:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将前面学习率 ![](img/B15558_06_030.png) 的值代入方程（18），并将我们的参数更新改写为：
- en: '![](img/B15558_13_210.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_210.png)'
- en: Where the value of ![](img/B15558_13_211.png) is computed using conjugated gradient
    descent.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B15558_13_211.png)的值通过共轭梯度下降法计算得出。
- en: 'Thus, we have computed the search direction using the Taylor series approximation
    and the Lagrange multiplier:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过泰勒级数近似和拉格朗日乘子计算了搜索方向：
- en: '![](img/B15558_13_13.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_13.png)'
- en: In the next section, let's learn how to perform a line search.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们学习如何执行线性搜索。
- en: Performing a line search in the search direction
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在搜索方向上执行线性搜索
- en: 'In order to make sure that our policy updates satisfy the KL constraint, we
    use a backtracking line search method. So, our update equation becomes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的策略更新满足KL约束，我们使用回溯线性搜索方法。因此，我们的更新方程变为：
- en: '![](img/B15558_13_212.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_212.png)'
- en: Okay, what does this mean? What's that new parameter ![](img/B15558_13_158.png)
    doing there? It is called the backtracking coefficient and the value of ![](img/B15558_13_158.png)
    ranges from 0 to 1\. It helps us to take a large step to update our parameter.
    That is, we can set ![](img/B15558_09_143.png) to a high value and make a large
    update. However, we need to make sure that we are maximizing our objective ![](img/B15558_13_216.png)
    along with satisfying our constraint ![](img/B15558_13_217.png).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这是什么意思？那个新的参数 ![](img/B15558_13_158.png) 在那里做什么？它被称为回溯系数，![](img/B15558_13_158.png)
    的值在0到1之间。它帮助我们采取大步长更新我们的参数。也就是说，我们可以将 ![](img/B15558_09_143.png) 设置为一个较高的值，并进行大幅更新。然而，我们需要确保在满足约束条件
    ![](img/B15558_13_217.png) 的同时，最大化我们的目标 ![](img/B15558_13_216.png)。
- en: So, we just try for different values of *j* from 0 to *N* and compute ![](img/B15558_09_106.png)
    as ![](img/B15558_13_212.png). If ![](img/B15558_13_216.png) and ![](img/B15558_13_217.png)
    for some values of *j*, then we just stop and update our parameter as ![](img/B15558_13_212.png).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们只需尝试从0到*N*的不同值，并将 ![](img/B15558_09_106.png) 计算为 ![](img/B15558_13_212.png)。如果
    ![](img/B15558_13_216.png) 且 ![](img/B15558_13_217.png) 对于某些 *j* 的值成立，则我们停止并将参数更新为
    ![](img/B15558_13_212.png)。
- en: 'The following steps provide clarity on how the backtracking line search method
    works:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤阐明了回溯线性搜索方法是如何工作的：
- en: 'For iterations *j* = 0, 1, 2, 3, . . . , *N*:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迭代次数 *j* = 0, 1, 2, 3, . . . , *N*：
- en: Compute ![](img/B15558_13_212.png)
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 ![](img/B15558_13_212.png)
- en: 'If ![](img/B15558_13_224.png) and ![](img/B15558_13_225.png) then:'
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ![](img/B15558_13_224.png) 且 ![](img/B15558_13_225.png)，则：
- en: Update ![](img/B15558_13_212.png)
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/B15558_13_212.png)
- en: Break
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终止
- en: 'Thus, our final parameter update rule of TRPO is given as:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TRPO的最终参数更新规则如下：
- en: '![](img/B15558_13_212.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_212.png)'
- en: In the next section, we will learn how exactly the TRPO algorithm works by using
    the preceding update rule.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过使用前面的更新规则，学习TRPO算法是如何工作的。
- en: Algorithm – TRPO
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – TRPO
- en: 'TRPO acts as an improvement to the policy gradient algorithm we learned in
    *Chapter 10*, *Policy Gradient Method*. It ensures that we can take large steps
    and update our parameter along with maintaining the constraint that our old policy
    and the new policy should not vary very much. The TRPO update rule is given as:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO作为我们在*第10章*《策略梯度方法》中学习的策略梯度算法的改进。它确保我们可以采取较大的步伐更新参数，同时保持旧策略与新策略之间的差异尽可能小。TRPO的更新规则如下：
- en: '![](img/B15558_13_212.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_212.png)'
- en: 'Now, let''s look at the algorithm of TRPO and see exactly how TRPO uses the
    preceding update rule and finds the optimal policy. Before going ahead, let''s
    recap how we computed gradient in the policy gradient method. In the policy gradient
    method, we computed the gradient *g* as:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看TRPO的算法，看看TRPO是如何使用前面的更新规则并找到最优策略的。在继续之前，我们先回顾一下在策略梯度方法中我们是如何计算梯度的。在策略梯度方法中，我们计算梯度
    *g* 如下：
- en: '![](img/B15558_13_229.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_229.png)'
- en: 'Where *R*[t] is the reward-to-go. The reward-to-go is the sum of the rewards
    of the trajectory starting from a state *s* and action *a*; it is expressed as:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*R*[t]是回报。回报是从状态*s*和动作*a*开始的轨迹的奖励总和；它表示为：
- en: '![](img/B15558_13_230.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_230.png)'
- en: 'Isn''t the reward-to-go similar to something we learned about earlier? Yes!
    If you recall, we learned that the Q function is the sum of rewards of the trajectory
    starting from the state *s* and action *a*. So, we can just replace the reward-to-go
    with the Q function and write our gradient as:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励累计（reward-to-go）不正是我们之前学到的内容吗？没错！如果你回忆一下，我们学到 Q 函数是从状态 *s* 和动作 *a* 开始的轨迹的奖励和。因此，我们可以将奖励累计替换为
    Q 函数，并将梯度写成：
- en: '![](img/B15558_13_231.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_231.png)'
- en: 'In the preceding equation, we have a difference between the Q function and
    the value function. We learned that the advantage function is the difference between
    the Q function and the value function and hence we can rewrite our gradient with
    the advantage function as:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们看到 Q 函数与价值函数之间的差异。我们学到，优势函数是 Q 函数和价值函数之间的差异，因此我们可以将我们的梯度与优势函数重写为：
- en: '![](img/B15558_13_232.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_232.png)'
- en: Now, let's look at the algorithm of TRPO. Remember that TRPO is the policy gradient
    method, so unlike actor-critic methods, here, first we generate *N* number of trajectories
    and then we update the parameter of the policy and value network.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看 TRPO 的算法。记住，TRPO 是一种策略梯度方法，因此与演员-评论员方法不同，在这里我们首先生成 *N* 个轨迹，然后更新策略和价值网络的参数。
- en: 'The steps involved in the TRPO are given as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO 中涉及的步骤如下：
- en: Initialize the policy network parameter ![](img/B15558_13_233.png) and value
    network parameter ![](img/B15558_13_234.png)
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_13_233.png) 和价值网络参数 ![](img/B15558_13_234.png)
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_111.png)
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成 *N* 个轨迹 ![](img/B15558_10_058.png)，按照策略 ![](img/B15558_10_111.png) 执行
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励累计） *R*[t]
- en: Compute the advantage value *A*[t]
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算优势值 *A*[t]
- en: Compute the policy gradients:![](img/B15558_13_232.png)
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度：![](img/B15558_13_232.png)
- en: Compute ![](img/B15558_13_238.png) using the conjugate gradient method
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用共轭梯度法计算 ![](img/B15558_13_238.png)
- en: Update the policy network parameter ![](img/B15558_09_054.png) using the update
    rule:![](img/B15558_13_240.png)
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用更新规则更新策略网络参数 ![](img/B15558_09_054.png)：![](img/B15558_13_240.png)
- en: Compute the mean squared error of the value network:![](img/B15558_10_149.png)
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算价值网络的均方误差：![](img/B15558_10_149.png)
- en: Update the value network parameter ![](img/B15558_13_234.png) using gradient
    descent as ![](img/B15558_13_243.png)
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降法更新价值网络参数 ![](img/B15558_13_234.png)，如 ![](img/B15558_13_243.png) 所示
- en: Repeat steps 2 to 9 for several iterations
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对步骤 2 到 9 进行若干次迭代
- en: Now that we have understood how TRPO works, in the next section, we will learn
    another interesting algorithm called proximal policy optimization.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 TRPO 的工作原理，在下一节中，我们将学习另一个有趣的算法，称为邻近策略优化。
- en: Proximal policy optimization
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻近策略优化
- en: In the previous section, we learned how TRPO works. We learned that TRPO keeps
    the policy updates in the trust region by imposing a constraint that the KL divergence
    between the old and new policy should be less than or equal to ![](img/B15558_13_244.png).
    The problem with the TRPO method is that it is difficult to implement and is computationally
    expensive. So, now we will learn one of the most popular and state-of-the-art
    policy gradient algorithms called **Proximal Policy Optimization** (**PPO**).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了 TRPO 的工作原理。我们了解到 TRPO 通过施加一个约束条件，使得旧策略和新策略之间的 KL 散度小于等于 ![](img/B15558_13_244.png)，从而保持策略更新在可信区域内。TRPO
    方法的问题在于其实现困难且计算开销大。因此，现在我们将学习一种最受欢迎且最先进的策略梯度算法，称为 **邻近策略优化**（**PPO**）。
- en: PPO improves upon the TRPO algorithm and is simple to implement. Similar to
    TRPO, PPO ensures that the policy updates are in the trust region. But unlike
    TRPO, PPO does not use any constraints in the objective function. Going forward,
    we will learn how exactly PPO works and how PPO ensures that the policy updates
    are in the trust region.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 改进了 TRPO 算法，并且实现简单。与 TRPO 类似，PPO 确保策略更新处于可信区域。但与 TRPO 不同的是，PPO 在目标函数中不使用任何约束。接下来，我们将学习
    PPO 的具体工作原理，以及 PPO 如何确保策略更新处于可信区域。
- en: 'There are two different types of PPO algorithm:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 算法有两种不同的类型：
- en: '**PPO-clipped –** In the PPO-clipped method, in order to ensure that the policy
    updates are in the trust region (that the new policy is not far away from the
    old policy), PPO adds a new function called the clipping function, which ensures
    that the new and old policies are not far away from each other.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PPO-裁剪法 –** 在PPO-裁剪法中，为了确保策略更新在信任区域内（即新策略不会偏离旧策略太远），PPO增加了一个新的函数，称为裁剪函数，确保新旧策略不会相差太远。'
- en: '**PPO-penalty –** In the PPO-penalty method, we modify our objective function
    by converting the KL constraint term to a penalty term and update the penalty
    coefficient adaptively during training by ensuring that the policy updates are
    in the trust region.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PPO-惩罚法 –** 在PPO-惩罚法中，我们通过将KL约束项转换为惩罚项来修改目标函数，并在训练过程中自适应地更新惩罚系数，确保策略更新在信任区域内。'
- en: We will now look into the preceding two types of PPO algorithm in detail.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将详细探讨前面提到的两种PPO算法。
- en: PPO with a clipped objective
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO与裁剪目标
- en: 'First, let us recall the objective function of TRPO. We learned that the TRPO
    objective function is given as:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下TRPO的目标函数。我们了解到，TRPO的目标函数表示为：
- en: '![](img/B15558_13_245.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_245.png)'
- en: It implies that we try to maximize our policy along with the constraint that
    the old policy and the new policy stays within the trust region, that is, the
    KL divergence between the old policy and new policy should be less than or equal
    to ![](img/B15558_13_246.png).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们试图在约束条件下最大化我们的策略，即旧策略和新策略应保持在信任区域内，即旧策略与新策略之间的KL散度应小于或等于 ![](img/B15558_13_246.png)。
- en: 'Let us take only the objective without the constraint and write the PPO objective
    function as:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们只考虑目标函数，不考虑约束条件，将PPO目标函数写为：
- en: '![](img/B15558_13_247.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_247.png)'
- en: 'In the preceding equation, the term ![](img/B15558_13_248.png) implies the
    probability ratio, that is, the ratio of the new policy to the old policy. Let
    us denote this using ![](img/B15558_13_249.png) and write the PPO objective function
    as:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，项 ![](img/B15558_13_248.png) 表示概率比率，即新策略与旧策略的比率。我们用 ![](img/B15558_13_249.png)
    来表示这个比率，并将PPO目标函数写为：
- en: '![](img/B15558_13_250.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_250.png)'
- en: 'If we update the policy using the preceding objective function then the policy
    updates will not be in the trust region. So, to ensure that our policy updates
    are in the trust region (that the new policy is not far from the old policy),
    we modify our objective function by adding a new function called the clipping
    function and rewrite our objective function as:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用前面的目标函数来更新策略，那么策略更新将不在信任区域内。因此，为了确保我们的策略更新在信任区域内（即新策略不会偏离旧策略太远），我们通过添加一个新的裁剪函数来修改目标函数，并将目标函数重写为：
- en: '![](img/B15558_13_251.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_251.png)'
- en: 'The preceding function implies that we take the minimum of two terms: one is
    ![](img/B15558_13_252.png) and the other is ![](img/B15558_13_253.png).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数意味着我们取两个项中的最小值：一个是 ![](img/B15558_13_252.png)，另一个是 ![](img/B15558_13_253.png)。
- en: We know that the first term ![](img/B15558_13_254.png) is basically our objective,
    see equation (20), and the second term is called the clipped objective. Thus,
    our final objective function is just the minimum of the unclipped and clipped
    objectives. But what's the use of this? How does adding this clipped objective
    help us in keeping our new policy not far away from the old policy?
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，第一个项 ![](img/B15558_13_254.png) 基本上就是我们的目标，见方程（20），第二个项称为裁剪目标。因此，我们的最终目标函数就是未裁剪目标和裁剪目标中的最小值。那么，这有什么用呢？加入这个裁剪目标是如何帮助我们确保新策略不会偏离旧策略太远的呢？
- en: 'Let''s understand this by taking a closer look:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过仔细研究来理解这一点：
- en: '![](img/B15558_13_251.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_251.png)'
- en: 'We know that the first term (unclipped objective) is just given by equation
    (20). So, let''s take a look into the second term, the clipped objective. It is
    given as:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，第一个项（未裁剪目标）正是由方程（20）给出。那么，我们来看看第二个项——裁剪目标。它表示为：
- en: '![](img/B15558_13_256.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_256.png)'
- en: By looking at the preceding term, we can say that we are clipping the probability
    ratio ![](img/B15558_13_257.png) in the range of ![](img/B15558_13_258.png). But
    why do we have to clip ![](img/B15558_13_259.png)? This can be explained by considering
    two cases of the advantage function—when the advantage is positive and when it
    is negative.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察前面的项，我们可以说我们在将概率比率 ![](img/B15558_13_257.png) 限制在 ![](img/B15558_13_258.png)
    范围内。但为什么我们需要对 ![](img/B15558_13_259.png) 进行裁剪呢？这可以通过考虑优势函数的两种情况来解释——当优势为正时和当优势为负时。
- en: '**Case 1: When the advantage is positive**'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1: 当优势为正时**'
- en: When the advantage is positive, ![](img/B15558_13_260.png), then it means that
    the corresponding action should be preferred over the average of all other actions.
    So, we can increase the value of ![](img/B15558_13_261.png) for that action so
    that it will have a greater chance of being selected. However, while increasing
    the value of ![](img/B15558_13_257.png), we should not increase it too much that
    it goes far away from the old policy. So, to prevent this, we clip ![](img/B15558_13_261.png)
    at ![](img/B15558_13_264.png).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 当优势为正时，![](img/B15558_13_260.png)，这意味着对应的动作应该比所有其他动作的平均值更受偏好。因此，我们可以增加该动作的 ![](img/B15558_13_261.png)
    值，以便它有更大的被选择的机会。然而，在增加 ![](img/B15558_13_257.png) 的值时，我们不应增加得太多，以免远离旧的策略。因此，为了防止这种情况，我们将
    ![](img/B15558_13_261.png) 的值裁剪到 ![](img/B15558_13_264.png)。
- en: '*Figure 13.10* shows how we increase the value of ![](img/B15558_13_261.png)
    when the advantage is positive and how we clip it at ![](img/B15558_13_266.png):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.10* 显示了当优势为正时我们如何增加 ![](img/B15558_13_261.png) 的值，以及如何将其裁剪到 ![](img/B15558_13_266.png)：'
- en: '![](img/B15558_13_10.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_10.png)'
- en: 'Figure 13.10: Value of ![](img/B15558_13_267.png) when the advantage is positive'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.10: 当优势为正时，![](img/B15558_13_267.png) 的值'
- en: '**Case 2: When the advantage is negative**'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 2: 当优势为负时**'
- en: When the advantage is negative, ![](img/B15558_13_268.png), then it means that
    the corresponding action should not be preferred over the average of all other
    actions. So, we can decrease the value of ![](img/B15558_13_257.png) for that
    action so that it will have a lower chance of being selected. However, while decreasing
    the value of ![](img/B15558_13_257.png), we should not decrease it too much that
    it goes far away from the old policy. So, in order to prevent that, we clip ![](img/B15558_13_257.png)
    at ![](img/B15558_13_272.png).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 当优势为负时，![](img/B15558_13_268.png)，这意味着对应的动作不应该比所有其他动作的平均值更受偏好。因此，我们可以降低该动作的
    ![](img/B15558_13_257.png) 值，以便它有更小的被选择的机会。然而，在降低 ![](img/B15558_13_257.png) 的值时，我们不应降低得太多，以免远离旧的策略。因此，为了防止这种情况，我们将
    ![](img/B15558_13_257.png) 的值裁剪到 ![](img/B15558_13_272.png)。
- en: '*Figure 13.11* shows how we decrease the value of ![](img/B15558_13_273.png)
    when the advantage is negative and how we clip it at ![](img/B15558_13_272.png):'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.11* 显示了当优势为负时我们如何降低 ![](img/B15558_13_273.png) 的值，以及如何将其裁剪到 ![](img/B15558_13_272.png)：'
- en: '![](img/B15558_13_11.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_11.png)'
- en: 'Figure 13.11: Value of ![](img/B15558_13_275.png) when the advantage is negative'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13.11: 当优势为负时，![](img/B15558_13_275.png) 的值'
- en: 'The value of ![](img/B15558_13_276.png) is usually set to 0.1 or 0.2\. Thus,
    we learned that the clipped objective keeps our policy updates close to the old
    policy by clipping at ![](img/B15558_13_266.png) and ![](img/B15558_04_123.png)
    based on the advantage function. So, our final objective function takes the minimum
    value of the unclipped and clipped objectives as:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15558_13_276.png) 的值通常设置为 0.1 或 0.2。因此，我们了解到，裁剪后的目标通过根据优势函数将我们的策略更新保持接近旧策略，裁剪点为
    ![](img/B15558_13_266.png) 和 ![](img/B15558_04_123.png)。因此，我们的最终目标函数取未裁剪目标和裁剪目标中的最小值，公式如下：'
- en: '![](img/B15558_13_251.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_251.png)'
- en: Now that we have learned how the PPO algorithm with a clipped objective works,
    let's look into the algorithm in the next section.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了带裁剪目标的 PPO 算法如何工作，接下来让我们研究下一节中的算法。
- en: Algorithm – PPO-clipped
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – PPO 裁剪版
- en: 'The steps involved in the PPO-clipped algorithm are given as follows:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 裁剪版算法的步骤如下：
- en: Initialize the policy network parameter ![](img/B15558_09_002.png) and value
    network parameter ![](img/B15558_12_213.png)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_002.png) 和价值网络参数 ![](img/B15558_12_213.png)
- en: Collect *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_120.png)
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集 *N* 个符合策略 ![](img/B15558_10_058.png) 的轨迹 ![](img/B15558_10_120.png)
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励目标） *R*[t]
- en: Compute the gradient of the objective function ![](img/B15558_13_284.png)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_13_284.png)
- en: Update the policy network parameter ![](img/B15558_09_054.png) using gradient
    ascent as ![](img/B15558_13_286.png)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升法更新策略网络参数 ![](img/B15558_09_054.png)，如 ![](img/B15558_13_286.png)
- en: Compute the mean squared error of the value network:![](img/B15558_10_149.png)
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算价值网络的均方误差: ![](img/B15558_10_149.png)'
- en: Compute the gradient of the value network ![](img/B15558_10_093.png)
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算价值网络的梯度 ![](img/B15558_10_093.png)
- en: Update the value network parameter ![](img/B15558_13_289.png) using gradient
    descent as ![](img/B15558_10_150.png)
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新值网络参数 ![](img/B15558_13_289.png)，如 ![](img/B15558_10_150.png) 所示：
- en: Repeat steps 2 to 8 for several iterations
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 到 8 多次迭代：
- en: Implementing the PPO-clipped method
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 PPO 剪切方法：
- en: Let's implement the PPO-clipped method for the swing-up pendulum task. The code
    used in this section is adapted from one of the very good PPO implementations
    ([https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proximal_Policy_Optimization](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proxima))
    by Morvan.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为摆动摆任务实现 PPO 剪切方法。本节中使用的代码改编自 Morvan 的一个很好的 PPO 实现（[https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proximal_Policy_Optimization](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proxima)）：
- en: 'First, let''s import the necessary libraries:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE0]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating the Gym environment
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Gym 环境：
- en: 'Let''s create a pendulum environment using Gym:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Gym 创建一个摆动环境：
- en: '[PRE1]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Get the state shape of the environment:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的状态形状：
- en: '[PRE2]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the action shape of the environment:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的动作形状：
- en: '[PRE3]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the pendulum is a continuous environment and thus our action space
    consists of continuous values. So, we get the bound of our action space:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，摆动摆的环境是连续的，因此我们的动作空间由连续值组成。所以，我们获取动作空间的边界：
- en: '[PRE4]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Set the epsilon value that is used in the clipped objective:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 设置在剪切目标中使用的 epsilon 值：
- en: '[PRE5]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Defining the PPO class
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义 PPO 类：
- en: 'Let''s define a class called `PPO` where we will implement the PPO algorithm.
    For a clear understanding, let''s take a look into the code line by line:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为 `PPO` 的类，在其中实现 PPO 算法。为了更清楚地理解，我们逐行查看代码：
- en: '[PRE6]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Defining the init method
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义 `init` 方法：
- en: 'First, let''s define the `init` method:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义 `init` 方法：
- en: '[PRE7]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Start the TensorFlow session:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话：
- en: '[PRE8]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the placeholder for the state:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE9]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, let''s build the value network that returns the value of a state:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建返回状态值的值网络：
- en: '[PRE10]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the placeholder for the Q value:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 Q 值的占位符：
- en: '[PRE11]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the advantage value as the difference between the Q value and the state
    value:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 定义优势值为 Q 值与状态值之间的差：
- en: '[PRE12]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Compute the loss of the value network:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 计算值网络的损失：
- en: '[PRE13]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Train the value network by minimizing the loss using the Adam optimizer:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Adam 优化器最小化损失来训练值网络：
- en: '[PRE14]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we obtain the new policy and its parameter from the policy network:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从策略网络获取新的策略及其参数：
- en: '[PRE15]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Obtain the old policy and its parameter from the policy network:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 从策略网络获取旧策略及其参数：
- en: '[PRE16]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Sample an action from the new policy:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 从新策略中采样一个动作：
- en: '[PRE17]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Update the parameters of the old policy:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 更新旧策略的参数：
- en: '[PRE18]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define the placeholder for the action:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 定义动作的占位符：
- en: '[PRE19]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the placeholder for the advantage:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 定义优势的占位符：
- en: '[PRE20]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s define our surrogate objective function of the policy network:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义策略网络的代理目标函数：
- en: '[PRE21]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We learned that the objective of the policy network is:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到策略网络的目标是：
- en: '![](img/B15558_13_251.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_251.png)'
- en: 'First, let''s define the ratio ![](img/B15558_13_257.png) as ![](img/B15558_13_248.png):'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义比率 ![](img/B15558_13_257.png) 为 ![](img/B15558_13_248.png)：
- en: '[PRE22]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define the objective by multiplying the ratio ![](img/B15558_13_257.png) and
    the advantage value *A*[t]:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将比率 ![](img/B15558_13_257.png) 和优势值 *A*[t] 相乘来定义目标：
- en: '[PRE23]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the objective function with the clipped and unclipped objectives:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 使用剪切和未剪切的目标定义目标函数：
- en: '[PRE24]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can compute the gradient and maximize the objective function using
    gradient ascent. However, instead of doing that, we can convert the preceding
    maximization objective into the minimization objective by just adding a negative
    sign. So, we can denote the loss of the policy network as:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算梯度并使用梯度上升法最大化目标函数。然而，实际上我们可以通过添加一个负号将上述最大化目标转换为最小化目标。所以，我们可以将策略网络的损失表示为：
- en: '[PRE25]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Train the policy network by minimizing the loss using the Adam optimizer:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 Adam 优化器最小化损失来训练策略网络：
- en: '[PRE26]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有 TensorFlow 变量：
- en: '[PRE27]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Defining the train function
  id: totrans-411
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义训练函数：
- en: 'Now, let''s define the `train` function:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义 `train` 函数：
- en: '[PRE28]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Update the old policy:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 更新旧策略：
- en: '[PRE29]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compute the advantage value:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 计算优势值：
- en: '[PRE30]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Train the policy network:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略网络：
- en: '[PRE31]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Train the value network:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 训练值网络：
- en: '[PRE32]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Building the policy network
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建策略网络：
- en: 'We define a function called `build_policy_network` for building the policy
    network. Note that our action space is continuous here, so our policy network
    returns the mean and variance of the action as an output and then we generate
    a normal distribution using this mean and variance and select an action by sampling
    from this normal distribution:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个名为`build_policy_network`的函数，用于构建策略网络。请注意，这里我们的动作空间是连续的，因此我们的策略网络返回动作的均值和方差作为输出，然后我们使用这个均值和方差生成一个正态分布，并通过从这个正态分布中采样来选择一个动作：
- en: '[PRE33]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Define the layer of the network:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 定义网络的层：
- en: '[PRE34]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Compute the mean:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 计算均值：
- en: '[PRE35]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Compute the standard deviation:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 计算标准差：
- en: '[PRE36]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Compute the normal distribution:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 计算正态分布：
- en: '[PRE37]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Get the parameters of the policy network:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 获取策略网络的参数：
- en: '[PRE38]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Selecting the action
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择动作
- en: 'Let''s define a function called `select_action` for selecting the action:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`select_action`的函数，用于选择动作：
- en: '[PRE39]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Sample an action from the normal distribution generated by the policy network:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 从策略网络生成的正态分布中采样一个动作：
- en: '[PRE40]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We clip the action so that it lies within the action bounds and then we return
    the action:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将动作裁剪到动作范围内，然后返回该动作：
- en: '[PRE41]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Computing the state value
  id: totrans-442
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算状态值
- en: 'We define a function called `get_state_value` to obtain the value of the state
    computed by the value network:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义一个名为`get_state_value`的函数，用于获取通过价值网络计算的状态值：
- en: '[PRE42]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Training the network
  id: totrans-445
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Now, let''s start training the network. First, let''s create an object for
    our PPO class:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。首先，创建一个PPO类的对象：
- en: '[PRE43]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Set the number of episodes:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数：
- en: '[PRE44]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Set the number of time steps in each episode:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 设置每回合的时间步数：
- en: '[PRE45]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 设置折扣因子，![](img/B15558_03_190.png)：
- en: '[PRE46]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Set the batch size:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 设置批量大小：
- en: '[PRE47]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For each episode:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个回合：
- en: '[PRE48]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE49]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Initialize the lists for holding the states, actions, and rewards obtained
    in the episode:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化用于保存回合中获得的状态、动作和奖励的列表：
- en: '[PRE50]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Initialize the return:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化回报：
- en: '[PRE51]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For every step:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步：
- en: '[PRE52]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Render the environment:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE53]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Select the action:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作：
- en: '[PRE54]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Perform the selected action:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所选择的动作：
- en: '[PRE55]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Store the state, action, and reward in the list:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态、动作和奖励存储在列表中：
- en: '[PRE56]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Update the state to the next state:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 更新状态到下一个状态：
- en: '[PRE57]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Update the return:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 更新回报：
- en: '[PRE58]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'If we reached the batch size or if we reached the final step of the episode:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们达到了批量大小或达到了回合的最后一步：
- en: '[PRE59]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compute the value of the next state:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 计算下一个状态的值：
- en: '[PRE60]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Compute the Q value as ![](img/B15558_13_296.png):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Q值为 ![](img/B15558_13_296.png)：
- en: '[PRE61]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Stack the episodic states, actions, and rewards:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠回合状态、动作和奖励：
- en: '[PRE62]'
  id: totrans-485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Train the network:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE63]'
  id: totrans-487
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Empty the lists:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 清空列表：
- en: '[PRE64]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Print the return for every 10 episodes:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 每10回合打印一次回报：
- en: '[PRE65]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Now that we have learned how PPO with a clipped objective works and how to implement
    it, in the next section we will learn about another interesting type of PPO algorithm
    called PPO with a penalized objective.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了带剪切目标的PPO是如何工作的以及如何实现它，在接下来的部分，我们将学习另一种有趣的PPO算法类型，称为带惩罚目标的PPO。
- en: PPO with a penalized objective
  id: totrans-493
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带惩罚目标的PPO
- en: 'In the PPO-penalty method, we convert the constraint term into a penalty term.
    First, let us recall the objective function of TRPO. We learned that the TRPO
    objective function is given as:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 在PPO惩罚方法中，我们将约束项转换为惩罚项。首先，让我们回顾一下TRPO的目标函数。我们知道TRPO的目标函数为：
- en: '![](img/B15558_13_245.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_245.png)'
- en: 'In the PPO-penalty method, we can rewrite the preceding objective by converting
    the KL constraint term into a penalty term as shown here:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在PPO惩罚方法中，我们可以通过将KL约束项转换为惩罚项，像这样重写前面的目标：
- en: '![](img/B15558_13_298.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_298.png)'
- en: Where ![](img/B15558_06_030.png) is called the penalty coefficient.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_06_030.png) 被称为惩罚系数。
- en: 'Let ![](img/B15558_13_300.png) and let ![](img/B15558_13_301.png) be the target
    KL divergence; then, we set the value of ![](img/B15558_09_151.png) adaptively
    as:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![](img/B15558_13_300.png) 和 ![](img/B15558_13_301.png) 为目标KL散度；然后，我们自适应地设置
    ![](img/B15558_09_151.png) 的值为：
- en: If *d* is greater than or equal to ![](img/B15558_13_303.png), then we set ![](img/B15558_13_304.png)
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*d*大于或等于 ![](img/B15558_13_303.png)，那么我们设置 ![](img/B15558_13_304.png)
- en: If *d* is less than or equal to ![](img/B15558_13_305.png), then we set ![](img/B15558_13_306.png)
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果*d*小于或等于 ![](img/B15558_13_305.png)，那么我们设置 ![](img/B15558_13_306.png)
- en: We can understand how exactly this works by looking into the PPO-penalty algorithm
    in the next section.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看接下来的部分中的PPO惩罚算法，来准确理解它是如何工作的。
- en: Algorithm – PPO-penalty
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – PPO-penalty
- en: 'The steps involved in the PPO-penalty algorithm are:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: PPO-penalty 算法的步骤如下：
- en: Initialize the policy network parameter ![](img/B15558_09_008.png) and the value
    network parameter ![](img/B15558_13_308.png), and initialize the penalty coefficient
    ![](img/B15558_13_309.png) and the target KL divergence ![](img/B15558_13_310.png)
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数 ![](img/B15558_09_008.png) 和值网络参数 ![](img/B15558_13_308.png)，并初始化惩罚系数
    ![](img/B15558_13_309.png) 和目标 KL 散度 ![](img/B15558_13_310.png)
- en: 'For iterations ![](img/B15558_13_311.png):'
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于迭代 ![](img/B15558_13_311.png)：
- en: Collect *N* number of trajectories following the policy ![](img/B15558_10_111.png)
  id: totrans-507
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照策略 ![](img/B15558_10_111.png) 收集 *N* 条轨迹
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-508
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（奖励） *R*[t]
- en: Compute ![](img/B15558_13_313.png)
  id: totrans-509
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 ![](img/B15558_13_313.png)
- en: Compute the gradient of the objective function ![](img/B15558_13_284.png)
  id: totrans-510
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算目标函数的梯度 ![](img/B15558_13_284.png)
- en: Update the policy network parameter ![](img/B15558_09_054.png) using gradient
    ascent as ![](img/B15558_13_316.png)
  id: totrans-511
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升法更新策略网络参数 ![](img/B15558_09_054.png)，如 ![](img/B15558_13_316.png)
- en: If *d* is greater than or equal to ![](img/B15558_13_317.png), then we set ![](img/B15558_13_318.png);
    if *d* is less than or equal to ![](img/B15558_13_319.png), then we set ![](img/B15558_13_306.png)
  id: totrans-512
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *d* 大于或等于 ![](img/B15558_13_317.png)，则设置 ![](img/B15558_13_318.png)；如果 *d*
    小于或等于 ![](img/B15558_13_319.png)，则设置 ![](img/B15558_13_306.png)
- en: Compute the mean squared error of the value network:![](img/B15558_13_321.png)
  id: totrans-513
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的均方误差：![](img/B15558_13_321.png)
- en: Compute the gradients of the value network ![](img/B15558_10_093.png)
  id: totrans-514
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的梯度 ![](img/B15558_10_093.png)
- en: Update the value network parameter ![](img/B15558_13_234.png) using gradient
    descent as ![](img/B15558_10_150.png)
  id: totrans-515
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降法更新值网络参数 ![](img/B15558_13_234.png)，如 ![](img/B15558_10_150.png)
- en: Thus, we learned how PPO-clipped and PPO-penalized objectives work. In general,
    PPO with a clipped objective is used more often than the PPO method with a penalized
    objective.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们了解了 PPO-clipped 和 PPO-penalized 目标的工作原理。一般而言，带有裁剪目标的 PPO 方法比带有惩罚目标的 PPO
    方法使用得更多。
- en: In the next section, we will learn another interesting algorithm called ACKTR.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习另一个有趣的算法，称为 ACKTR。
- en: Actor-critic using Kronecker-factored trust region
  id: totrans-518
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kronecker 分解信任域的演员-评论家
- en: ACKTR, as the name suggests, is the actor-critic algorithm based on the Kronecker
    factorization and trust region.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ACKTR，如其名所示，是基于 Kronecker 分解和信任区域的演员-评论家算法。
- en: 'We know that the actor-critic architecture consists of the actor and critic
    networks, where the role of the actor is to produce a policy and the role of the
    critic is to evaluate the policy produced by the actor network. We learned that
    in the actor network (policy network), we compute gradients and update the parameter
    of the actor network using gradient ascent:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，演员-评论家架构由演员网络和评论家网络组成，演员的角色是生成策略，而评论家的角色是评估演员网络生成的策略。我们已经了解到，在演员网络（策略网络）中，我们通过计算梯度并使用梯度上升更新演员网络的参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: 'Instead of updating our actor network parameter using the preceding update
    rule, we can also update it by computing the natural gradients as:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算自然梯度来更新演员网络的参数，而不是使用前述更新规则：
- en: '![](img/B15558_13_326.png)'
  id: totrans-523
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_326.png)'
- en: 'Where *F* is called the Fisher information matrix. Thus, the natural gradient
    is just the product of the inverse of the Fisher matrix and standard gradient:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *F* 称为费舍尔信息矩阵。因此，自然梯度就是费舍尔矩阵的逆与标准梯度的乘积：
- en: '![](img/B15558_13_14.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_14.png)'
- en: The use of the natural gradient is that it guarantees a monotonic improvement
    in the policy. However, updating the actor network (policy network) parameter
    using the preceding update rule is a computationally expensive task, because computing
    the Fisher information matrix and then taking its inverse is a computationally
    expensive task. So, to avoid this tedious computation, we can just approximate
    the value of ![](img/B15558_13_327.png) using a Kronecker-factored approximation.
    Once we approximate ![](img/B15558_13_327.png) using a Kronecker-factored approximation,
    then we can just update our policy network parameter using the natural gradient
    update rule given in equation (21), and while updating the policy network parameter,
    we also ensure that the policy updates are in the trust region so that the new
    policy is not far from the old policy. This is the main idea behind the ACKTR
    algorithm.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 自然梯度的使用在于它能保证策略的单调改进。然而，使用上述更新规则更新演员网络（策略网络）参数是一项计算量大的任务，因为计算费舍尔信息矩阵并求其逆是一个计算量巨大的过程。因此，为了避免这项繁重的计算，我们可以通过克罗内克分解近似来逼近
    ![](img/B15558_13_327.png) 的值。一旦我们使用克罗内克分解近似了 ![](img/B15558_13_327.png)，就可以按照公式（21）中给出的自然梯度更新规则更新我们的策略网络参数，在更新策略网络参数时，我们还要确保策略更新位于信任区域内，这样新策略就不会偏离旧策略太远。这就是
    ACKTR 算法的核心思想。
- en: Now that we have a basic understanding of what ACKTR is, let us understand how
    this works exactly in detail. First, we will understand what Kronecker factorization
    is, then we will learn how it is used in the actor-critic setting, and later we
    will learn how to incorporate the trust region in the policy updates.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经对 ACKTR 有了基本的了解，接下来让我们详细了解它是如何工作的。首先，我们将理解克罗内克分解是什么，然后我们将学习它如何在演员-评论家设置中使用，之后我们将学习如何在策略更新中结合信任区域。
- en: Before going ahead, let's learn several math concepts that are required to understand
    ACKTR.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们学习一些理解 ACKTR 所必需的数学概念。
- en: Math essentials
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数学基础
- en: 'To understand how Kronecker factorization works, we will learn the following
    important concepts:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解克罗内克分解如何工作，我们将学习以下重要概念：
- en: Block matrix
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块矩阵
- en: Block diagonal matrix
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 块对角矩阵
- en: The Kronecker product
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克罗内克积
- en: The vec operator
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vec 运算符
- en: Properties of the Kronecker product
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克罗内克积的性质
- en: Block matrix
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块矩阵
- en: 'A block matrix is defined as a matrix that can be broken down into submatrices
    called blocks, or we can say a block matrix is formed by a set of submatrices
    or blocks. For instance, let''s consider a block matrix *A* as shown here:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 块矩阵被定义为一个可以分解为称为块的子矩阵的矩阵，或者我们可以说，块矩阵是由一组子矩阵或块构成的。例如，假设我们考虑一个如图所示的块矩阵 *A*：
- en: '![](img/B15558_13_329.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_329.png)'
- en: 'The matrix *A* can be broken into four ![](img/B15558_13_330.png) submatrices
    as shown here:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 *A* 可以分解成四个子矩阵，如下所示： ![](img/B15558_13_330.png)
- en: '![](img/B15558_13_331.png)'
  id: totrans-540
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_331.png)'
- en: 'Now, we can simply write our block matrix *A* as:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地将块矩阵 *A* 写成：
- en: '![](img/B15558_13_332.png)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_332.png)'
- en: Block diagonal matrix
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 块对角矩阵
- en: 'A block diagonal matrix is a block matrix that consists of a square matrix
    on the diagonals, and off-diagonal elements are set to 0\. A block diagonal matrix
    *A* is represented as:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 块对角矩阵是一个包含方阵对角线元素，非对角线元素设为 0 的块矩阵。块对角矩阵 *A* 可表示为：
- en: '![](img/B15558_13_333.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_333.png)'
- en: Where the diagonals ![](img/B15558_13_334.png) are the square matrices.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 其中对角线部分 ![](img/B15558_13_334.png) 是方阵。
- en: 'An example of a block diagonal matrix is shown here:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 一个块对角矩阵的例子如下所示：
- en: '![](img/B15558_13_335.png)'
  id: totrans-548
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_335.png)'
- en: 'As we can see, the diagonals are basically the square matrix and off-diagonal
    elements are set to zero:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，对角线部分基本上是一个方阵，而非对角线元素被设为零：
- en: '![](img/B15558_13_15.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_15.png)'
- en: 'Thus, we can write:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以写成：
- en: '![](img/B15558_13_336.png)'
  id: totrans-552
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_336.png)'
- en: 'Now, we can simply denote our block diagonal matrix *A* as:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地将我们的块对角矩阵 *A* 表示为：
- en: '![](img/B15558_13_337.png)'
  id: totrans-554
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_337.png)'
- en: The Kronecker product
  id: totrans-555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 克罗内克积
- en: 'The Kronecker product is an operation performed between two matrices. The Kronecker
    product is not the same as matrix multiplication. When we perform the Kronecker
    product between two matrices, it will output the block matrix. The Kronecker product
    is denoted by ![](img/B15558_13_338.png). Let us say we have a matrix *A* of order
    ![](img/B15558_13_339.png) and a matrix *B* of order ![](img/B15558_13_340.png);
    the Kronecker product of matrices *A* and *B* is expressed as:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 克罗内克积是两个矩阵之间执行的运算。克罗内克积不同于矩阵乘法。当我们执行两个矩阵之间的克罗内克积时，它会输出块矩阵。克罗内克积用![](img/B15558_13_338.png)表示。假设我们有一个阶数为![](img/B15558_13_339.png)的矩阵*A*和一个阶数为![](img/B15558_13_340.png)的矩阵*B*，那么矩阵*A*和*B*的克罗内克积表示为：
- en: '![](img/B15558_13_341.png)'
  id: totrans-557
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_341.png)'
- en: 'This implies that we multiply every element in matrix *A* by matrix *B*. Let
    us understand this with an example. Say we have two matrices *A* and *B* as shown
    here:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将矩阵*A*中的每个元素与矩阵*B*相乘。让我们通过一个例子来理解这一点。假设我们有两个矩阵*A*和*B*，如下所示：
- en: '![](img/B15558_13_342.png)'
  id: totrans-559
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_342.png)'
- en: 'Then the Kronecker product of matrices *A* and *B* is given as:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，矩阵*A*和*B*的克罗内克积表示为：
- en: '![](img/B15558_13_16.png)'
  id: totrans-561
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_16.png)'
- en: The vec operator
  id: totrans-562
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: vec运算符
- en: 'The vec operator creates a column vector by stacking all the columns in a matrix
    below one another. For instance, let''s consider a matrix *A* as shown here:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: vec运算符通过将矩阵中的所有列堆叠在一起，创建一个列向量。例如，假设我们有如下所示的矩阵*A*：
- en: '![](img/B15558_13_343.png)'
  id: totrans-564
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_343.png)'
- en: 'Applying the vec operator on *A* stacks all the columns in the matrix one below
    the other as follows:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 对* A *应用vec运算符会将矩阵中的所有列堆叠在一起，形成如下所示的矩阵：
- en: '![](img/B15558_13_344.png)'
  id: totrans-566
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_344.png)'
- en: Properties of the Kronecker product
  id: totrans-567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 克罗内克积的性质
- en: 'The Kronecker product has several useful properties; these include:'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 克罗内克积有几个有用的性质，包括：
- en: '![](img/B15558_13_345.png)'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_345.png)'
- en: '![](img/B15558_13_346.png)'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_346.png)'
- en: '![](img/B15558_13_347.png)'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_347.png)'
- en: Now that we have learned several important concepts, let's understand what Kronecker
    factorization is.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了几个重要概念，接下来让我们理解什么是克罗内克因式分解。
- en: Kronecker-Factored Approximate Curvature (K-FAC)
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克罗内克因式近似曲率（K-FAC）
- en: 'Let''s suppose we have a neural network parametrized by ![](img/B15558_09_054.png)
    and we train the neural network using gradient descent. We can write our update
    rule, including the natural gradient, as:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由![](img/B15558_09_054.png)参数化的神经网络，并且我们使用梯度下降法训练神经网络。我们可以写出更新规则，包括自然梯度，如下所示：
- en: '![](img/B15558_13_349.png)'
  id: totrans-575
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_349.png)'
- en: Where *F* is the Fisher information matrix. The problem is that computing *F*
    and finding its inverse is an expensive task. So to avoid that, we use a Kronecker-factored
    approximation to approximate the value of ![](img/B15558_13_350.png).
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*F*是费舍尔信息矩阵。问题在于计算*F*并求解其逆是一个昂贵的任务。为了避免这种情况，我们使用克罗内克因式分解近似来估算![](img/B15558_13_350.png)的值。
- en: 'Let''s learn how we approximate ![](img/B15558_13_350.png) using Kronecker
    factors. Say our network has ![](img/B15558_13_352.png) layers and the weight
    of the network is represented by ![](img/B15558_09_123.png). Thus, ![](img/B15558_13_354.png)
    denotes the weights of the layers ![](img/B15558_13_355.png) respectively. Let
    ![](img/B15558_13_356.png) denote the output distribution of the network, and
    we will use the negative log-likelihood as the loss function *J*:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何使用克罗内克因式近似![](img/B15558_13_350.png)。假设我们的网络有![](img/B15558_13_352.png)层，网络的权重表示为![](img/B15558_09_123.png)。因此，![](img/B15558_13_354.png)表示层![](img/B15558_13_355.png)的权重。让![](img/B15558_13_356.png)表示网络的输出分布，我们将使用负对数似然作为损失函数*J*：
- en: '![](img/B15558_13_357.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_357.png)'
- en: 'Then, the Fisher information matrix can be written as:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，费舍尔信息矩阵可以写成：
- en: '![](img/B15558_13_358.png)'
  id: totrans-580
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_358.png)'
- en: 'K-FAC approximates the Fisher information matrix, *F*, as a block diagonal
    matrix where each block refers to the gradients of the loss with respect to the
    weights of a particular layer. For example, the block *F*[1] denotes the gradients
    of the loss with respect to the weights of layer 1\. The block *F*[2] denotes
    the gradients of the loss with respect to the weights of layer 2\. The block *F*[l]
    denotes the gradients of the loss with respect to the weights of layer *l*:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: K-FAC近似费舍尔信息矩阵*F*，将其表示为块对角矩阵，每个块代表特定层的权重梯度。例如，块*F*[1]表示损失函数相对于第1层权重的梯度。块*F*[2]表示损失函数相对于第2层权重的梯度。块*F*[l]表示损失函数相对于第*l*层权重的梯度：
- en: '![](img/B15558_13_359.png)'
  id: totrans-582
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_359.png)'
- en: 'That is, ![](img/B15558_13_360.png), where:'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 即，![](img/B15558_13_360.png)，其中：
- en: '![](img/B15558_13_361.png)'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_361.png)'
- en: '![](img/B15558_13_362.png)'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_362.png)'
- en: '![](img/B15558_13_363.png)'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_363.png)'
- en: '![](img/B15558_13_364.png)'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B15558_13_364.png)'
- en: As we can observe, each block *F*[1] to *F*[L] contains the derivatives of loss
    *J* with respect to the weights of the corresponding layer. Okay, how can we compute
    each block? That is, how can the values in the preceding block diagonal matrix
    be computed?
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，每个块*F*[1]到*F*[L]包含了损失*J*相对于相应层权重的导数。那么，我们如何计算每个块呢？也就是说，如何计算前面这个对角矩阵中的值？
- en: To understand this, let's just take one block, say, *F*[l], and learn how it
    is computed. Let's take a layer *l*. Let *a* be the input activation vector, let
    ![](img/B15558_13_365.png) be the weights of the layer, and let *s* be the output
    pre-activation vector, and it can be sent to the next layer *l* + 1.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，让我们先看一个块，假设是*F*[l]，并学习它是如何计算的。假设我们有一层*l*，让*a*是输入激活向量，![](img/B15558_13_365.png)是该层的权重，*s*是输出前激活向量，它可以传递给下一层*l*
    + 1。
- en: 'We know that in the neural network, we multiply the activation vector by weights
    and send that to the next layer; so, we can write:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在神经网络中，我们将激活向量与权重相乘并将其传递到下一层；因此，我们可以写成：
- en: '![](img/B15558_13_366.png)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_366.png)'
- en: 'We can approximate the block *F*[l] corresponding to layer *l* as:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将对应于层*l*的块*F*[l]近似为：
- en: '![](img/B15558_13_367.png)'
  id: totrans-593
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_367.png)'
- en: The preceding equation *F*[l] denotes the gradient of the loss with respect
    to the weights of layer *l*.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程*F*[l]表示损失对层*l*权重的梯度。
- en: 'From (22), the partial derivative of the loss function *J* with respect to
    weights ![](img/B15558_13_368.png) in layer *l* can be written as:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 从(22)可得，损失函数*J*相对于层*l*的权重![](img/B15558_13_368.png)的偏导数可以写为：
- en: '![](img/B15558_13_369.png)'
  id: totrans-596
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_369.png)'
- en: 'Substituting (24) in (23), we can write:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 将(24)代入(23)，我们可以写为：
- en: '![](img/B15558_13_370.png)'
  id: totrans-598
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_370.png)'
- en: 'The preceding equation implies that *F*[l] is just the expected value of the
    Kronecker product. So, we can rewrite it as the Kronecker product of the expected
    value; that is, *F*[l] can be approximated as the Kronecker product of the expected
    value:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程表明*F*[l]仅仅是克罗内克积的期望值。因此，我们可以将其重新写为期望值的克罗内克积；也就是说，*F*[l]可以近似为期望值的克罗内克积：
- en: '![](img/B15558_13_371.png)'
  id: totrans-600
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_371.png)'
- en: 'Let ![](img/B15558_13_372.png) and ![](img/B15558_13_373.png). We can write:'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 令![](img/B15558_13_372.png)和![](img/B15558_13_373.png)。我们可以写成：
- en: '![](img/B15558_13_374.png)'
  id: totrans-602
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_374.png)'
- en: This is known as Kronecker factorization and *A* and *S* are called the Kronecker
    factors. Now that we have learned how to compute the block *F*[l], let's learn
    how to update the weights ![](img/B15558_13_368.png) of the layer *l*.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为克罗内克分解，*A*和*S*被称为克罗内克因子。现在我们已经了解了如何计算块*F*[l]，接下来我们来学习如何更新层*l*的权重![](img/B15558_13_368.png)。
- en: 'The update rule for updating the weights ![](img/B15558_13_376.png) of the
    layer *l* is given as:'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 更新层*l*权重![](img/B15558_13_376.png)的更新规则为：
- en: '![](img/B15558_13_377.png)'
  id: totrans-605
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_377.png)'
- en: 'Let ![](img/B15558_13_378.png). We can write:'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 令![](img/B15558_13_378.png)。我们可以写成：
- en: '![](img/B15558_13_379.png)'
  id: totrans-607
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_379.png)'
- en: 'Let''s see how to compute the value of ![](img/B15558_13_380.png):'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何计算![](img/B15558_13_380.png)的值：
- en: '![](img/B15558_13_378.png)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_378.png)'
- en: 'Applying the vec operator on both sides, we can write:'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 对两边应用vec运算符，我们可以写成：
- en: '![](img/B15558_13_382.png)'
  id: totrans-611
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_382.png)'
- en: 'From (25), we can substitute the value of *F*[l] and write:'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 从(25)我们可以代入*F*[l]的值并写成：
- en: '![](img/B15558_13_383.png)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_383.png)'
- en: 'Using the properties ![](img/B15558_13_384.png) and ![](img/B15558_13_347.png),
    we can write:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 使用属性![](img/B15558_13_384.png)和![](img/B15558_13_347.png)，我们可以写成：
- en: '![](img/B15558_13_386.png)'
  id: totrans-615
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_386.png)'
- en: 'As you may observe, we have computed the value of ![](img/B15558_13_387.png)
    without expensive computation of the inverse of the Fisher information matrix
    using Kronecker factors. Now, using the value of ![](img/B15558_13_388.png) we
    just derived, we can update the weights ![](img/B15558_13_389.png) of the layer
    *l* as:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们已经在不需要通过克罗内克因子进行昂贵的费舍尔信息矩阵逆运算的情况下计算了![](img/B15558_13_387.png)的值。现在，利用我们刚推导出的![](img/B15558_13_388.png)的值，我们可以更新层*l*的权重![](img/B15558_13_389.png)：
- en: '![](img/B15558_13_379.png)'
  id: totrans-617
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_379.png)'
- en: In a nutshell, K-FAC approximates the Fisher information matrix as a block diagonal
    matrix where each block contains the derivatives. Then, each block is approximated
    as the Kronecker product of two matrices, which is known as Kronecker factorization.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，K-FAC将Fisher信息矩阵近似为一个块对角矩阵，其中每个块包含导数。然后，每个块被近似为两个矩阵的Kronecker积，这就是所谓的Kronecker因式分解。
- en: Thus, we have learned how to approximate the natural gradient using Kronecker
    factors. In the next section, we will learn how to apply this in an actor-critic
    setting.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经学会了如何使用Kronecker因子近似自然梯度。在下一节中，我们将学习如何在演员-评论者方法中应用这一点。
- en: K-FAC in actor-critic
  id: totrans-620
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-FAC 在演员-评论者方法中
- en: We know that in the actor-critic method, we have actor and critic networks.
    The role of the actor is to produce the policy and the role of the critic is to
    evaluate the policy produced by the actor network.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在演员-评论者方法中，有演员网络和评论者网络。演员的角色是生成策略，评论者的角色是评估演员网络生成的策略。
- en: 'First, let''s take a look at the actor network. In the actor network, our goal
    is to find the optimal policy. So, we try to find the optimal parameter ![](img/B15558_09_087.png)
    with which we can obtain the optimal policy. We compute gradients and update the
    parameter of the actor network using gradient ascent:'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看演员网络。在演员网络中，我们的目标是找到最优策略。因此，我们试图找到最优参数 ![](img/B15558_09_087.png)，通过该参数可以获得最优策略。我们计算梯度并使用梯度上升法更新演员网络的参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: 'Instead of updating the actor network parameter using the preceding update
    rule, we can also update the parameter of the actor network by computing the natural
    gradients as:'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过计算自然梯度来更新演员网络的参数，而不是使用前述的更新规则，具体方法如下：
- en: '![](img/B15558_13_393.png)'
  id: totrans-625
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_393.png)'
- en: 'But computing ![](img/B15558_13_350.png) is an expensive task. So, we can use
    Kronecker factorization for approximating the value of ![](img/B15558_13_350.png).
    We can define the Fisher information matrix for the actor network *F* as:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 但是计算 ![](img/B15558_13_350.png) 是一项昂贵的任务。因此，我们可以使用Kronecker因式分解来近似 ![](img/B15558_13_350.png)
    的值。我们可以将演员网络的Fisher信息矩阵 *F* 定义为：
- en: '![](img/B15558_13_396.png)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_396.png)'
- en: Now, just as we learned in the previous section, we can approximate the Fisher
    information matrix as a block diagonal matrix where each block contains the derivatives,
    and then we can approximate each block as the Kronecker product of two matrices.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所学到的，我们可以将Fisher信息矩阵近似为一个块对角矩阵，其中每个块包含导数，然后我们可以将每个块近似为两个矩阵的Kronecker积。
- en: 'Let ![](img/B15558_13_397.png). We can write:'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 令 ![](img/B15558_13_397.png)。我们可以写为：
- en: '![](img/B15558_13_156.png)'
  id: totrans-630
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_156.png)'
- en: 'The value of ![](img/B15558_13_399.png) can be computed using Kronecker factorization
    as:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B15558_13_399.png) 的值可以通过Kronecker因式分解计算为：'
- en: '![](img/B15558_13_400.png)'
  id: totrans-632
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_400.png)'
- en: This is exactly the same as what we learned in the previous section.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在上一节中学到的内容。
- en: Now, let's look at the critic network. We know that the critic evaluates the
    policy produced by the actor network by estimating the Q function. So, we train
    the critic by minimizing the mean squared error between the target value and predicted
    value.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下评论者网络。我们知道，评论者通过估计Q函数来评估由演员网络生成的策略。因此，我们通过最小化目标值和预测值之间的均方误差来训练评论者。
- en: 'We minimize the loss using gradient descent and update the critic network parameter
    ![](img/B15558_12_213.png) as:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用梯度下降法最小化损失，并更新评论者网络的参数 ![](img/B15558_12_213.png) ，具体方法如下：
- en: '![](img/B15558_10_150.png)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_150.png)'
- en: Where ![](img/B15558_10_093.png) is the standard first-order gradient.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_10_093.png) 是标准的一阶梯度。
- en: 'Instead of using the first-order gradient, can we use the second-order gradient
    and update the critic network parameter ![](img/B15558_10_148.png), similar to
    what we did with the actor? Yes, in settings like least squares (MSE), we can
    use an algorithm called the Gauss-Newton method for finding the second-order derivative.
    You can learn more about the Gauss-Newton method here: [http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf](http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf).
    Let''s represent our error as ![](img/B15558_13_405.png). According to the Gauss-Newton
    method, the update rule for updating the critic network parameter ![](img/B15558_13_406.png)
    is given as:'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否不使用一阶梯度，而是使用二阶梯度来更新评论员网络的参数 ![](img/B15558_10_148.png)，类似于我们对演员所做的操作？是的，在像最小二乘法（MSE）这样的设置中，我们可以使用一个叫做高斯-牛顿法的算法来找到二阶导数。你可以在这里了解更多关于高斯-牛顿法的信息：[http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf](http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf)。让我们用
    ![](img/B15558_13_405.png) 表示我们的误差。根据高斯-牛顿法，更新评论员网络参数 ![](img/B15558_13_406.png)
    的更新规则为：
- en: '![](img/B15558_13_407.png)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_407.png)'
- en: Where *G* is called the Gauss-Newton matrix, and it is given as ![](img/B15558_13_408.png),
    and *J* is the Jacobian matrix. (A Jacobian matrix is a matrix that contains a
    first-order partial derivative for a vector-valued function.)
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *G* 被称为高斯-牛顿矩阵，其形式为 ![](img/B15558_13_408.png)，而 *J* 是雅可比矩阵。（雅可比矩阵是一个包含向量值函数的一阶偏导数的矩阵。）
- en: If you look at the preceding equation, computing ![](img/B15558_13_409.png)
    is equivalent to computing the ![](img/B15558_13_350.png) we saw in the actor
    network. That is, computing the inverse of the Gauss-Newton matrix is equivalent
    to computing the inverse of the Fisher information matrix. So, we can use the
    Kronecker factor (K-FAC) to approximate the value of ![](img/B15558_13_409.png)
    just like we approximated the value of ![](img/B15558_13_412.png).
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看前面的方程式，计算 ![](img/B15558_13_409.png) 相当于计算我们在演员网络中看到的 ![](img/B15558_13_350.png)。也就是说，计算高斯-牛顿矩阵的逆相当于计算费舍尔信息矩阵的逆。因此，我们可以使用克罗内克因子（K-FAC）来近似计算
    ![](img/B15558_13_409.png) 的值，就像我们近似计算 ![](img/B15558_13_412.png) 的值一样。
- en: Instead of applying K-FAC to the actor and critic separately, we can also apply
    them in a shared mode. As specified in the paper **Scalable trust-region method
    for deep reinforcement learning using Kronecker-factored approximation** by Yuhuai
    Wu, Elman Mansimov, Shun Liao, Roger Grosse, Jimmy Ba ([https://arxiv.org/pdf/1708.05144.pdf](https://arxiv.org/pdf/1708.05144.pdf)),
    "*We can have a single architecture where both actor and critic share the lower
    layer representations but they have different output layers*."
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不仅仅单独应用 K-FAC 于演员和评论员网络，还可以在共享模式下应用。正如 Yuhuai Wu、Elman Mansimov、Shun Liao、Roger
    Grosse 和 Jimmy Ba 在论文 **Scalable trust-region method for deep reinforcement learning
    using Kronecker-factored approximation** 中所述，"*我们可以设计一个单一架构，其中演员和评论员共享底层表示，但它们具有不同的输出层*。"
    该论文可访问：[https://arxiv.org/pdf/1708.05144.pdf](https://arxiv.org/pdf/1708.05144.pdf)
- en: In a nutshell, in the ACKTR method, we update the parameters of the actor and
    critic networks by computing the second-order derivatives. Since computing the
    second-order derivative is an expensive task, we use a method called Kronecker-factored
    approximation to approximate the second-order derivative.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在 ACKTR 方法中，我们通过计算二阶导数来更新演员和评论员网络的参数。由于计算二阶导数是一项昂贵的任务，我们使用了一种叫做克罗内克因子近似的方法来近似二阶导数。
- en: In the next section, we will learn how to incorporate the trust region into
    our update rule so that our new and old policy updates will not be too far apart.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何将信任区域融入我们的更新规则，使得新旧策略更新不会相差太远。
- en: Incorporating the trust region
  id: totrans-645
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 融入信任区域
- en: 'We learned that we can update the parameter of our network with a natural gradient
    as:'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何使用自然梯度来更新我们网络的参数，如下所示：
- en: '![](img/B15558_13_413.png)'
  id: totrans-647
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_13_413.png)'
- en: In the previous section, we learned how we can use K-FAC to approximate the
    ![](img/B15558_13_350.png) matrix. While updating the policy, we need to make
    sure that our policy updates are in the trust region; that is, our new policy
    should not be too far away from the old policy. So to ensure this, we can choose
    the step size ![](img/B15558_09_143.png) as ![](img/B15558_13_416.png), where
    ![](img/B15558_09_143.png) and the trust region radius ![](img/B15558_09_135.png)
    are the hyperparameters, as mentioned in the ACKTR paper (refer to the *Further
    reading* section). Updating our network parameters with this step size ensures
    that our policy updates are in the trust region.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何使用K-FAC来近似![](img/B15558_13_350.png)矩阵。在更新策略时，我们需要确保策略更新处于信任区域内；也就是说，新策略不应与旧策略相差太远。为了确保这一点，我们可以选择步长![](img/B15558_09_143.png)作为![](img/B15558_13_416.png)，其中![](img/B15558_09_143.png)和信任区域半径![](img/B15558_09_135.png)是超参数，如ACKTR论文中所提到的（参见*进一步阅读*部分）。使用此步长更新网络参数可确保我们的策略更新处于信任区域内。
- en: Summary
  id: totrans-649
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by understanding what TRPO is and how it acts as
    an improvement to the policy gradient algorithm. We learned that when the new
    policy and old policy vary greatly then it causes model collapse.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从了解TRPO是什么以及它如何作为策略梯度算法的改进开始本章内容。我们了解到，当新旧策略差异过大时，会导致模型崩溃。
- en: So in TRPO, we make a policy update while imposing the constraint that the parameters
    of the old and new policies should stay within the trust region. We also learned
    that TRPO guarantees monotonic policy improvement; that is, it guarantees that
    there will always be a policy improvement on every iteration.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在TRPO中，我们在更新策略时强加一个约束，要求新旧策略的参数保持在信任区域内。我们还了解到，TRPO保证了单调的策略改进；也就是说，它确保每次迭代都会有策略改进。
- en: 'Later, we learned about the PPO algorithm, which acts as an improvement to
    the TRPO algorithm. We learned about two types of PPO algorithm: PPO-clipped and
    PPO-penalty. In the PPO-clipped method, in order to ensure that the policy updates
    are in the trust region, PPO adds a new function called the clipping function
    that ensures the new and old policies are not far away from each other. In the
    PPO-penalty method, we modify our objective function by converting the KL constraint
    term to a penalty term and update the penalty coefficient adaptively during training
    by ensuring that the policy updates are in the trust region.'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们学习了PPO算法，它是TRPO算法的改进版。我们了解了两种PPO算法：PPO-clipped和PPO-penalty。在PPO-clipped方法中，为了确保策略更新处于信任区域，PPO添加了一个名为裁剪函数的新功能，以确保新旧策略不会相差太远。在PPO-penalty方法中，我们通过将KL约束项转化为惩罚项来修改目标函数，并在训练过程中自适应地更新惩罚系数，确保策略更新位于信任区域内。
- en: At the end of the chapter, we learned about ACKTR. In the ACKTR method, we update
    the parameters of the actor and critic networks by computing the second-order
    derivative. Since computing the second-order derivative is an expensive task,
    we use a method called Kronecker-factored approximation to approximate the second-order
    derivatives, and while updating the policy network parameter, we also ensure that
    the policy updates are in the trust region so that the new policy is not far from
    the old policy.
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们学习了ACKTR。在ACKTR方法中，我们通过计算二阶导数来更新演员网络和评论员网络的参数。由于计算二阶导数是一项昂贵的任务，我们使用一种叫做Kronecker-分解近似法的方法来近似二阶导数，在更新策略网络参数时，我们还确保策略更新位于信任区域内，以确保新策略不会与旧策略相差太远。
- en: In the next chapter, we will learn about several interesting distributional
    reinforcement learning algorithms.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一些有趣的分布式强化学习算法。
- en: Questions
  id: totrans-655
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our understanding of the algorithms we learned in this chapter.
    Try answering the following questions:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们评估一下我们对本章所学算法的理解。试着回答以下问题：
- en: What is a trust region?
  id: totrans-657
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是信任区域？
- en: Why is TRPO useful?
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么TRPO有用？
- en: How does the conjugate gradient method differ from gradient descent?
  id: totrans-659
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 共轭梯度法与梯度下降法有何不同？
- en: What is the update rule of TRPO?
  id: totrans-660
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TRPO的更新规则是什么？
- en: How does PPO differ from TRPO?
  id: totrans-661
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPO与TRPO有何不同？
- en: Explain the PPO-clipped method.
  id: totrans-662
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释PPO-clipped方法。
- en: What is Kronecker factorization?
  id: totrans-663
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是Kronecker分解？
- en: Further reading
  id: totrans-664
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, refer to the following papers:'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参考以下论文：
- en: '**Trust Region Policy Optimization** by *John Schulman*, *Sergey Levine, Philipp
    Moritz*, *Michael I. Jordan*, *Pieter Abbeel*, [https://arxiv.org/pdf/1502.05477.pdf](https://arxiv.org/pdf/1502.05477.pdf)'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信赖域策略优化** 由 *约翰·舒尔曼*、*谢尔盖·莱文*、*菲利普·莫里茨*、*迈克尔·I·乔丹*、*皮特·阿贝尔* 发表，[https://arxiv.org/pdf/1502.05477.pdf](https://arxiv.org/pdf/1502.05477.pdf)'
- en: '**Proximal Policy Optimization Algorithms** by *John Schulman*, *Filip Wolski*,
    *Prafulla Dhariwal*, *Alec Radford*, *Oleg Klimov*, [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**邻近策略优化算法** 由 *约翰·舒尔曼*、*菲利普·沃尔斯基*、*普拉夫拉·达里瓦尔*、*阿列克·拉德福德*、*奥列格·克利莫夫* 发表，[https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)'
- en: '**Scalable trust-region method for deep reinforcement learning using Kronecker-factored
    approximation** by*Yuhuai Wu*, *Elman Mansimov*, *Shun Liao*, *Roger Grosse*,
    *Jimmy Ba*, [https://arxiv.org/pdf/1708.05144.pdf](https://arxiv.org/pdf/1708.05144.pdf)'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用克罗内克因子近似的可扩展信赖域方法应用于深度强化学习** 由 *余槐·吴*、*埃尔曼·曼西莫夫*、*邵·廖*、*罗杰·格罗斯*、*吉米·巴*
    发表，[https://arxiv.org/pdf/1708.05144.pdf](https://arxiv.org/pdf/1708.05144.pdf)'
