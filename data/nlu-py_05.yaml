- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Natural Language Data – Finding and Preparing Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言数据 – 查找和准备数据
- en: This chapter will teach you how to identify and prepare data for processing
    with natural language understanding techniques. It will discuss data from databases,
    the web, and different kinds of documents, as well as privacy and ethics considerations.
    The Wizard of Oz technique will be covered briefly. If you don’t have access to
    your own data, or if you wish to compare your results to those of other researchers,
    this chapter will also discuss generally available and frequently used corpora.
    It will then go on to discuss preprocessing steps such as stemming and lemmatization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将教你如何识别并准备适用于自然语言理解技术的数据处理。它将讨论来自数据库、网络以及不同种类文档的数据，还会涉及隐私和伦理考虑。我们将简要介绍“奥兹巫师”技术。如果你没有自己的数据，或者希望将自己的结果与其他研究者的结果进行对比，本章还将讨论一些通用可用且常用的语料库。接着，我们将讨论诸如词干提取和词形还原等预处理步骤。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Sources of data and annotation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据来源与标注
- en: Ensuring privacy and observing ethical considerations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保隐私并遵循伦理考虑
- en: Generally available corpora
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用可用的语料库
- en: Preprocessing data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理数据
- en: Application-specific types of preprocessing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对特定应用的预处理类型
- en: Choosing among preprocessing techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预处理技术中进行选择
- en: Finding sources of data and annotating it
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找数据来源并进行注释
- en: Data is where all **natural language processing** (**NLP**) projects start.
    Data can be in the form of written texts or transcribed speech. The purpose of
    data is to teach an NLP system what it should do when it’s given similar data
    in the future. Specific collections of data are also called *corpora* or *datasets*,
    and we will often use these terms interchangeably. Very recently, large pretrained
    models have been developed that greatly reduce the need for data in many applications.
    However, these pretrained models, which will be discussed in detail in [*Chapter
    11*](B19005_11.xhtml#_idTextAnchor193), do not in most cases eliminate the need
    for application-specific data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是所有**自然语言处理**（**NLP**）项目的起点。数据可以是书面文本或转录的语音。数据的目的是教会NLP系统在未来遇到类似数据时应该做什么。特定的数据集也被称为*语料库*或*数据集*，我们通常会交替使用这些术语。最近，已经开发出了大型的预训练模型，它们大大减少了许多应用中对数据的需求。然而，这些预训练模型（将在[*第11章*](B19005_11.xhtml#_idTextAnchor193)中详细讨论）在大多数情况下并不会消除对特定应用数据的需求。
- en: Written language data can be of any length, ranging from very short texts such
    as tweets to multi-page documents or even books. Written language can be interactive,
    such as a record of a chatbot session between a user and a system, or it can be
    non-interactive, such as a newspaper article or blog. Similarly, spoken language
    can be long or short. Like written language, it can be interactive, such as a
    transcription of a conversation between two people, or non-interactive, such as
    broadcast news. What all NLP data has in common is that it is language, and it
    consists of words, in one or more human languages, that are used by people to
    communicate with each other. The goal of every NLP project is to take that data,
    process it by using specific algorithms, and gain information about what the author
    or authors of the data had in mind when they created the data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 书面语言数据可以有任何长度，从非常短的文本（如推文）到多页文档甚至书籍。书面语言可以是互动性的，例如用户与系统之间的聊天记录；也可以是非互动性的，如新闻文章或博客。类似地，口语数据可以长也可以短。与书面语言一样，它可以是互动性的，如两人对话的转录，也可以是非互动性的，如广播新闻。所有NLP数据的共同点是它是语言，由一个或多个自然语言的词汇组成，用于人们之间的交流。每个NLP项目的目标是将这些数据进行处理，使用特定算法获取关于数据创作者意图的信息。
- en: One of the first steps in any NLP project is finding the right data. For that,
    you’ll want to consider your goals in doing the project.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 任何NLP项目的第一步之一是找到合适的数据。为此，你需要考虑你进行该项目的目标。
- en: Finding data for your own application
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找适用于自己应用的数据
- en: If you have a specific, practical application in mind that you’d like to build,
    it’s often easy to know what kind of data you need. For example, to build an enterprise
    assistant (one of the types of interactive applications shown in *Figure 1**.3*),
    you’ll need examples of conversations between users and either human agents or
    interactive systems of the kind you would like to build. These examples could
    already be available in the form of existing call center records.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个特定的实际应用想要构建，通常很容易知道你需要什么样的数据。例如，要构建一个企业助手（如*图1.3*中显示的交互式应用之一），你需要用户与人类代理或你想要构建的交互系统之间对话的示例。这些示例可能已经存在，形式可能是现有的呼叫中心记录。
- en: 'The examples we will consider in the following subsections are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在接下来的子章节中将考虑以下示例：
- en: Conversations in call centers
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 呼叫中心对话
- en: Chat logs
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天记录
- en: Databases
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据库
- en: Message boards and customer reviews
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息板和客户评价
- en: Let’s begin!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Conversations in call centers
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 呼叫中心对话
- en: If you’re planning to build a voice assistant that performs customer support,
    in many cases, the goal is to offload some work from an existing call center.
    In that case, there will frequently be many transcripts of previous calls between
    human agents and customers that can serve as data for training an application.
    The customers’ questions will be examples of what the application will need to
    understand, and the agents’ responses will be examples of how the system should
    respond. There will need to be an annotation process that assigns an overall intent,
    or customer goal, to each customer utterance. Most of the time, the annotation
    process will also need to label entities within the utterance. Before annotation
    of the intents and entities begins, there should be an initial design step where
    the intents and entities are determined.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划构建一个执行客户支持的语音助手，在许多情况下，目标是将一些工作从现有的呼叫中心转移到语音助手上。在这种情况下，通常会有很多之前人类客服代表与客户之间通话的转录文本，这些文本可以作为训练应用程序的数据。客户的问题将作为应用程序需要理解的示例，而客服代表的回答将作为系统应如何回应的示例。在这过程中，需要有一个标注步骤，为每个客户的发言分配一个总体意图或客户目标。大多数时候，标注过程还需要标记发言中的实体。在开始标注意图和实体之前，应该有一个初步设计步骤，确定意图和实体。
- en: Once the data has been annotated, it can be used as training data for applications.
    The training process will vary significantly depending on the technologies used.
    We will cover the use of the data in training in detail in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173)
    and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被标注，它就可以作为应用程序的训练数据。训练过程将根据使用的技术有所不同。我们将在[*第9章*](B19005_09.xhtml#_idTextAnchor173)和[*第10章*](B19005_10.xhtml#_idTextAnchor184)中详细讨论数据在训练中的应用。
- en: Chat logs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聊天记录
- en: If you have a website that includes a chat window, the questions typed by customers
    can serve as training data, just like transcripts from call center conversations.
    The only difference in the data is that call center data is based on speech rather
    than typing. Otherwise, the annotation, design, and training processes will be
    very similar. The data itself will be a bit different since typed inputs tend
    to be shorter than spoken inputs and will contain spelling errors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个包含聊天窗口的网站，客户输入的问题就可以作为训练数据，就像呼叫中心对话的转录本一样。数据之间的唯一区别是，呼叫中心的数据是基于语音的，而不是输入的文字。否则，标注、设计和训练过程将非常相似。数据本身会有一些不同，因为输入的文字通常比语音输入短，并且可能包含拼写错误。
- en: Databases
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据库
- en: Databases can often be a good source of enterprise data. Databases often contain
    free text fields, where a user can enter any information they like. Free text
    fields are used for information such as narrative summaries of incident reports,
    and they often contain rich information that’s not captured elsewhere in the other
    database fields. NLP can be extremely valuable for learning from this rich information
    because the contents of free text fields can be classified and analyzed using
    NLP techniques. This analysis can provide additional insight into the topic of
    the database.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库通常是企业数据的一个很好的来源。数据库中经常包含自由文本字段，用户可以在其中输入任何信息。自由文本字段用于诸如事件报告的叙述摘要等信息，并且通常包含其他数据库字段中未捕获的丰富信息。自然语言处理（NLP）在从这些丰富的信息中学习方面非常有价值，因为自由文本字段的内容可以通过NLP技术进行分类和分析。这种分析可以为数据库主题提供更多的洞见。
- en: Message boards and customer reviews
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息板和客户评价
- en: Like free text fields, message boards and customer support forums contain unformatted
    inputs from customers. Customer support message boards and customer product reviews
    can be valuable sources of data about product failures as well as customer attitudes
    about products. Although this information can be reviewed by human analysts, human
    analysis is time-consuming and expensive. In many cases, this information is abundant
    and can be used as the basis of very useful NLP applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 像自由文本字段一样，留言板和客户支持论坛包含了来自客户的未格式化输入。客户支持留言板和客户产品评论可以成为有关产品故障以及客户对产品态度的宝贵数据来源。尽管这些信息可以由人工分析，但人工分析既费时又昂贵。在许多情况下，这些信息非常丰富，可以作为非常有用的
    NLP 应用的基础。
- en: So far, we’ve talked about application-specific data and how you can find and
    analyze it for the purposes of the application. On the other hand, sometimes you
    will be interested in analyzing data as part of a research project. The next section
    discusses where you can obtain data for a research project.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了特定应用的数据以及如何为该应用的目的找到并分析这些数据。另一方面，有时你可能会对分析数据作为研究项目的一部分感兴趣。下一部分将讨论你可以从哪里获得研究项目的数据。
- en: Finding data for a research project
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找研究项目的数据
- en: If your goal is to contribute to the science of NLP, or if you just want to
    be able to compare your algorithms to other researchers’ work, the kind of data
    you need will be quite different from the data discussed previously. Rather than
    finding data (possibly proprietary data inside your enterprise) that no one’s
    ever used before, you will want to use data that’s freely available to other researchers.
    Ideally, this data will already be public, but if not, it’s important for you
    to make it available to others in the NLP research community so that they can
    replicate your work. Scientific conferences or journals will nearly always require
    that any newly collected data be made available before a paper can be presented
    or published. We will now discuss several ways of collecting new data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的目标是为 NLP 科学做出贡献，或者如果你只是希望能够将自己的算法与其他研究者的工作进行比较，那么你所需要的数据将与之前讨论的数据大不相同。与其寻找没人使用过的数据（可能是你企业内部的专有数据），你更希望使用那些其他研究人员可以自由访问的数据。理想情况下，这些数据应该是公开的，但如果不是，重要的是你要将其提供给
    NLP 研究社区中的其他人，以便他们能够复现你的工作。科学会议或期刊几乎总是要求任何新收集的数据在论文展示或发布之前必须公开。接下来我们将讨论几种收集新数据的方法。
- en: Collecting data
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集数据
- en: Although there are many sources of pre-existing data, sometimes you will not
    find exactly what you need from currently available resources. Perhaps you want
    to work on a very specific technical topic, or perhaps you’re interested in a
    rapidly changing topic such as COVID-19\. You may need data that’s specific to
    a particular local area or seasonal data that’s only applicable to certain times
    of the year. For all of these reasons, it may be necessary to collect data specifically
    for your project.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有很多现成的数据来源，但有时你可能无法从现有资源中找到完全符合需求的数据。也许你想处理一个非常具体的技术话题，或者你对一个快速变化的话题感兴趣，比如
    COVID-19。你可能需要特定于某个地区的数据，或是某些季节性的数据，这些数据只适用于一年中的特定时段。由于这些原因，你可能需要专门为你的项目收集数据。
- en: There are several good ways to collect data under these circumstances, including
    **application programming interfaces** (**APIs**), crowdsourcing data, and Wizard
    of Oz. We will review these in the following sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，有几种很好的方法可以收集数据，包括**应用程序编程接口**（**API**）、众包数据和“奥兹巫师”方法。我们将在接下来的部分中回顾这些方法。
- en: APIs
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: API
- en: Some social media services have feeds that can be accessed from APIs. Twitter,
    for example, has an API that developers can use to access Twitter services ([https://developer.twitter.com/en/docs/twitter-api](https://developer.twitter.com/en/docs/twitter-api)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一些社交媒体服务提供可以通过 API 访问的动态信息。例如，Twitter 提供了一个 API，开发者可以用它访问 Twitter 服务（[https://developer.twitter.com/en/docs/twitter-api](https://developer.twitter.com/en/docs/twitter-api)）。
- en: Crowdsourcing data
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 众包数据
- en: 'Some data can be generated by human workers using platforms such as Amazon’s
    Mechanical Turk ([https://www.mturk.com/](https://www.mturk.com/)). The data to
    be generated has to be clearly described for crowdworkers, along with any parameters
    or constraints on the data to be generated. Data that is easy for the average
    person to understand, as opposed to technical or specialized scientific data,
    is especially suitable for crowdsourcing. Crowdsourcing can be an effective way
    to obtain data; however, for this data to be useful, some precautions have to
    be taken:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据可以通过像亚马逊的机械土耳其人平台（[https://www.mturk.com/](https://www.mturk.com/)）这样的工具由人工工人生成。需要生成的数据必须清楚地描述给众包工作者，并包括任何关于数据生成的参数或约束条件。对于普通人容易理解的数据，而不是技术性或专业的科学数据，尤其适合用于众包。众包可以是获取数据的有效方式；然而，为了使这些数据有用，必须采取一些预防措施：
- en: It’s important to make sure that the crowdworkers have adequate instructions
    so that the data they create is sufficiently similar to the real data that the
    system will encounter during deployment. Crowdworker data has to be monitored
    to make sure that the crowdworkers are properly following instructions.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保众包工作者有足够的指示，以确保他们创建的数据与系统在部署过程中将遇到的真实数据足够相似，这是非常重要的。必须监控众包工作者的数据，以确保他们正确遵循指示。
- en: Crowdworkers have to have the right knowledge in order to create data that’s
    appropriate for specialized applications. For example, if the crowdworkers are
    to generate medical reports, they have to have medical backgrounds.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众包工作者必须具备适当的知识，以便为专业应用生成合适的数据。例如，如果众包工作者需要生成医学报告，他们必须具备医学背景。
- en: Crowdworkers have to have sufficient knowledge of the language that they’re
    generating data for in order to ensure that the data is representative of the
    real data. It is not necessary for the data to be perfectly grammatical – language
    encountered during deployment is not, especially speech data. Insisting on perfectly
    grammatical data can lead to stilted, unrealistic data.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 众包工作者必须具备足够的语言知识，以确保他们生成的数据能够代表真实数据。这些数据不必完全符合语法规范——部署过程中遇到的语言，尤其是语音数据，并不一定完美。过分坚持语法规范可能导致数据过于生硬、不现实。
- en: Wizard of Oz
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 奥兹巫师法
- en: 'The **Wizard of Oz** (**WoZ**) method of collecting data is based on setting
    up a computer-human interaction situation where a system appears to be processing
    the user’s inputs, but in fact, the processing is done by a human behind the scenes.
    The *Wizard of Oz* reference is from the line in the movie where the wizard says,
    “*Pay no attention to the man behind the curtain*,” who is actually controlling
    a projection of what is supposed to be the wizard. The idea behind the technique
    is that if the user believes that a system is doing the processing, the user’s
    behavior will represent how they would behave with an actual system. While the
    WoZ method can provide very high-quality data, it is expensive, since the setup
    must be carefully arranged so that subjects in the experiment are unaware that
    they’re interacting with an automated system. You can find details about the WoZ
    paradigm at [https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment](https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment),
    and more information about conducting a WoZ experiment here: [https://www.answerlab.com/insights/wizard-of-oz-testing](https://www.answerlab.com/insights/wizard-of-oz-testing).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**奥兹巫师法**（**WoZ**）是一种数据收集方法，其基本原理是设置一种计算机与人类互动的情境，系统看似在处理用户的输入，实际上，处理工作是由幕后的人类完成的。*奥兹巫师*这一名称源于电影中的一句台词：“*不要注意幕布后的那个人*”，实际上那个人在控制一个本应是巫师的投影。该技术背后的理念是，如果用户认为系统在处理数据，那么他们的行为将反映出他们在与实际系统互动时的行为。虽然WoZ方法可以提供高质量的数据，但它成本较高，因为设置必须小心安排，以确保实验对象没有意识到他们正在与自动化系统互动。你可以在[https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment](https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment)了解关于WoZ范式的详细信息，更多关于进行WoZ实验的信息请参考：[https://www.answerlab.com/insights/wizard-of-oz-testing](https://www.answerlab.com/insights/wizard-of-oz-testing)。'
- en: In data collection, we are not only interested in the language itself but also
    in additional information that describes the data, or *metadata*. The next section
    will describe metadata in general, and then continue discussing an extremely important
    type of metadata, *annotation*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集过程中，我们不仅关注语言本身，还关注描述数据的附加信息，或者说是*元数据*。下一节将介绍元数据的基本概念，然后继续讨论一种非常重要的元数据类型——*注释*。
- en: Metadata
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据
- en: Datasets often include metadata. Metadata refers to information about the data
    rather than the data itself. Almost any information that data providers think
    might be useful in further processing can be included as metadata. Some of the
    most common types of metadata include the human language of the data, the speaker
    and time and place of speech for spoken data, and the author of written data.
    If spoken data is the result of a speech recognition process, the speech recognizer’s
    confidence is usually included as metadata. The next section covers annotation,
    which is probably the most important type of metadata.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集通常包括元数据。元数据指的是关于数据的信息，而不是数据本身。几乎任何数据提供者认为可能对后续处理有用的信息都可以作为元数据包含在内。一些最常见的元数据类型包括数据的自然语言、口语数据的讲者、发言时间和地点，以及书面数据的作者。如果口语数据是通过语音识别过程获得的，语音识别器的置信度通常会作为元数据包含。接下来的部分将介绍注释，这是可能是最重要的一种元数据类型。
- en: Annotation
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注释
- en: One of the most important types of metadata is the intended NLP result for a
    text, or the annotation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的元数据类型之一是文本的预期自然语言处理结果，或称为注释。
- en: For the most part, newly collected data will need to be annotated, unless the
    experiment to be done involves unsupervised learning (more on unsupervised learning
    in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)). Annotation is the process
    of associating an input with the NLP result that the trained system is intended
    to produce. The system *learns* how to analyze data by processing the annotated
    examples, and then applies that learning to new, unannotated examples.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，新收集的数据需要进行注释，除非要进行的实验涉及无监督学习（关于无监督学习的更多内容，请参考[*第12章*](B19005_12.xhtml#_idTextAnchor217)）。注释是将输入与训练系统预期生成的自然语言处理结果相关联的过程。系统通过处理带注释的示例来*学习*如何分析数据，然后将这种学习应用于新的、没有注释的示例。
- en: Since annotation is actually the *supervision* in supervised learning, data
    used in unsupervised learning experiments does not need to be annotated with the
    intended NLP result.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注释实际上是监督学习中的*监督*，因此用于无监督学习实验的数据不需要带有预期的自然语言处理结果的注释。
- en: There are several software tools that can be used to annotate NLP text data.
    For example, the **General Architecture for Text Engineering** (**GATE**) ([https://gate.ac.uk/](https://gate.ac.uk/))
    includes a well-tested user interface that enables annotators to assign meanings
    to documents and parts of documents.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种软件工具可以用于注释自然语言处理文本数据。例如，**文本工程通用架构**（**GATE**）([https://gate.ac.uk/](https://gate.ac.uk/))包括一个经过充分测试的用户界面，使得注释者可以为文档及其部分赋予意义。
- en: In the following sections, we will “learn about the” transcription of speech
    data and the question of inter-annotator agreement.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将“了解”语音数据的转录以及标注者之间一致性的问题。
- en: Transcription
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 转录
- en: Transcription, or converting the speech contained in audio files to its written
    form, is a necessary annotation step for speech data. If the speech does not contain
    significant amounts of noise, commercial **automatic speech recognition** (**ASR**)
    systems such as Nuance Dragon ([https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html](https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html))
    can provide a fairly accurate first pass at transcribing audio files. If you do
    use commercial ASR for transcription, the results should still be reviewed by
    a researcher in order to catch and correct any errors made by the ASR system.
    On the other hand, if the speech is very noisy or quiet, or if it contains speech
    from several people talking over each other, commercial ASR systems will probably
    make too many errors for the automatic transcription results to be useful. In
    that situation, manual transcription software such as TranscriberAG ([http://transag.sourceforge.net/](http://transag.sourceforge.net/))
    can be used. You should keep in mind that manual transcription of noisy or otherwise
    problematic speech is likely to be a slow process because the transcriber has
    to first understand the speech in order to transcribe it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 转录，即将音频文件中的语音转换为书面形式，是语音数据的重要标注步骤。如果语音中没有显著的噪声，商业**自动语音识别**（**ASR**）系统，如Nuance
    Dragon（[https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html](https://www.nuance.com/dragon/business-solutions/dragon-professional-individual.html)），可以提供相当准确的音频文件初步转录结果。如果你使用商业ASR进行转录，结果仍然需要由研究人员审查，以便捕捉并纠正ASR系统可能出现的错误。另一方面，如果语音非常嘈杂或安静，或者包含几个人同时交谈的内容，商业ASR系统可能会出现太多错误，导致自动转录结果不具有参考价值。在这种情况下，可以使用手动转录软件，如TranscriberAG（[http://transag.sourceforge.net/](http://transag.sourceforge.net/)）。你需要记住，转录嘈杂或其他问题语音的手动过程可能会非常缓慢，因为转录员必须先理解语音内容才能进行转录。
- en: Inter-annotator agreement
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标注者一致性
- en: Annotators will not always agree on the correct annotation for any given data
    item. There can be general differences in opinion about the correct annotation,
    especially if the annotation instructions are unclear. In addition, annotators
    might not be thinking carefully about what the annotations should be, or the data
    that the annotators are being asked to annotate might be inherently subjective.
    For these reasons, annotation of the same data is often done by several annotators,
    especially if the data is subjective. Annotating an emotion associated with text
    or speech is a good example of data that has a lot of potential for disagreement.
    The degree of agreement among annotators is called **inter-annotator agreement**
    and is measured by what is known as the **kappa statistic**. The kappa statistic
    is preferable to just computing a percentage agreement because it takes into account
    the possibility that the annotators might agree by chance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 标注者对于任何给定数据项的正确标注并不总是达成一致。特别是当标注说明不清晰时，关于正确标注的意见可能会有较大差异。此外，标注者可能没有仔细思考标注的具体要求，或者标注者需要标注的数据可能本身就带有主观性。基于这些原因，同一数据项的标注通常由多个标注者完成，尤其是当数据具有主观性时。例如，标注与文本或语音相关的情感，就是一种具有高度分歧潜力的数据。标注者之间的同意度被称为**标注者一致性**，通常通过所谓的**卡帕统计量**来衡量。卡帕统计量比单纯计算百分比一致性更为优越，因为它考虑了标注者可能仅仅是偶然达成一致的情况。
- en: '`nltk.metrics.agreement`, that can be used to calculate inter-annotator agreement.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.metrics.agreement`，可以用来计算标注者一致性。'
- en: So far, we have discussed the process of obtaining your own data, but there
    are also many pre-existing datasets available that have already been annotated
    and are freely available. We’ll discuss pre-existing data in the next section.
    When we start working directly with data in later chapters, we will be using pre-existing
    datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了获取自己数据的过程，但也有许多现成的、已经标注并且可以自由获取的数据集。我们将在下一节中讨论现成的数据。之后我们在实际操作中会直接使用这些现成数据集。
- en: Generally available corpora
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用可用语料库
- en: The easiest way to get data is to use pre-existing corpora that cover the kind
    of problem that you’re trying to solve. With pre-existing corpora, you will not
    need to collect the data, and you will probably not need to annotate it unless
    the pre-existing annotations are not suitable for your problem. Any privacy questions
    will have been addressed before the dataset was published. An added advantage
    to using pre-existing corpora is that other researchers have probably published
    papers describing their work on the corpus, which you can compare to your own
    work.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 获取数据的最简单方法是使用覆盖你所要解决问题类型的现有语料库。使用现有语料库，你不需要收集数据，除非现有的注释不适合你的问题，否则你可能不需要重新注释。任何隐私问题在数据集发布之前都会得到解决。使用现有语料库的一个附加优点是，其他研究人员可能已经发布了描述他们在语料库上的工作论文，你可以将这些与自己的工作进行比较。
- en: Fortunately, there are many standard preexisting datasets that you can download
    and work with, covering almost any NLP problem. Some are free and others are available
    for a fee.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，存在许多标准的现有数据集，你可以下载并用于几乎任何自然语言处理问题。一些是免费的，其他则需要付费。
- en: 'Preexisting datasets are available from a number of organizations, such as
    the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织提供现有数据集，例如以下几个：
- en: 'Linguistic Data Consortium ([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/)):
    Provides a wide variety of text and speech data in many languages and also manages
    donated data.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '语言数据联盟 ([https://www.ldc.upenn.edu/](https://www.ldc.upenn.edu/)): 提供多种语言的文本和语音数据，并管理捐赠的数据。'
- en: 'Hugging Face ([https://huggingface.co/](https://huggingface.co/)): Provides
    datasets in many languages, as well as NLP models. Some of the popular datasets
    available from Hugging Face include movie reviews, product reviews, and Twitter
    emotion categories.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hugging Face ([https://huggingface.co/](https://huggingface.co/)): 提供多语言的数据集以及自然语言处理模型。Hugging
    Face 提供的一些热门数据集包括电影评论、产品评论和 Twitter 情感分类。'
- en: 'Kaggle ([https://www.kaggle.com/](https://www.kaggle.com/)): Provides many
    datasets, including user-contributed datasets.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaggle ([https://www.kaggle.com/](https://www.kaggle.com/)): 提供多个数据集，包括用户贡献的数据集。'
- en: '**European language resources association** (**ELRA**) ([http://www.elra.info/en/](http://www.elra.info/en/)):
    A European organization that provides multilingual data.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧洲语言资源协会**（**ELRA**）([http://www.elra.info/en/](http://www.elra.info/en/)):
    一个提供多语言数据的欧洲组织。'
- en: 'Data distributed with NLP libraries such as NLTK and spaCy: NLP libraries include
    dozens of corpora of all sizes and languages. Much of the data is annotated in
    support of many different types of applications.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与自然语言处理库（如 NLTK 和 spaCy）一起分发的数据：自然语言处理库包含数十个不同规模和语言的语料库。很多数据都带有注释，支持多种不同类型的应用。
- en: 'Government data: Governments collect vast amounts of data, including text data,
    which is often publicly available and can be used for research.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政府数据：政府收集大量数据，包括文本数据，这些数据通常是公开的，可以用于研究。
- en: 'Librispeech [https://www.openslr.org/12](https://www.openslr.org/12): A large
    dataset of read speech, based on audiobooks read for people with visual impairments.
    Primarily used for speech projects.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Librispeech [https://www.openslr.org/12](https://www.openslr.org/12): 一个大型的阅读语音数据集，基于为视力障碍人士朗读的有声书，主要用于语音项目。'
- en: Up to this point, we have covered the topics of obtaining data and adding metadata,
    including annotation. Before data is used in NLP applications, we also need to
    ensure that it is used ethically. Ethical considerations are the topic of the
    next section.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了获取数据和添加元数据（包括注释）的话题。在数据用于自然语言处理应用之前，我们还需要确保其使用符合伦理。伦理考虑将是下一部分的主题。
- en: Ensuring privacy and observing ethical considerations
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保隐私并遵循伦理考虑
- en: 'Language data, especially data internal to an enterprise, may contain sensitive
    information. Examples that come to mind right away are medical and financial data.
    When an application deals with these kinds of topics, it is very likely to contain
    sensitive information about health or finances. Information can become even more
    sensitive if it is associated with a specific person. This is called **personally
    identifiable information** (**PII**), which is defined by the United States Department
    of Labor as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 语言数据，尤其是企业内部的数据，可能包含敏感信息。立刻想到的例子有医疗和金融数据。当一个应用涉及这些话题时，它很可能包含有关健康或财务的敏感信息。如果信息与特定人关联，它将变得更加敏感。这种信息被称为**个人可识别信息**（**PII**），由美国劳工部定义如下：
- en: “*Any representation of information that permits the identity of an individual
    to whom the information applies to be reasonably inferred by either direct or
    indirect means*” ([https://www.dol.gov/general/ppii](https://www.dol.gov/general/ppii)).
    This is a broad and complex issue, a full treatment of which is out of the scope
    of this book. However, it’s worth discussing a few important points specific to
    NLP applications that should be considered if you need to deal with any kind of
    sensitive data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: “*任何可以通过直接或间接手段合理推断出信息适用对象身份的资料表示*”([https://www.dol.gov/general/ppii](https://www.dol.gov/general/ppii))。这是一个广泛且复杂的问题，完整的讨论超出了本书的范围。然而，值得讨论的是一些针对NLP应用的关键点，如果你需要处理任何形式的敏感数据，应该考虑这些点。
- en: Ensuring the privacy of training data
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保训练数据的隐私
- en: In the case of generally available corpora, the data has typically been prepared
    so that sensitive information has been removed. If you are dealing with data you
    have obtained yourself, on the other hand, you will have to consider how to deal
    with sensitive information that you may encounter in the raw data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般可用的语料库，数据通常已被处理，以便移除敏感信息。另一方面，如果你处理的是自己获取的数据，你需要考虑如何处理在原始数据中可能遇到的敏感信息。
- en: One common strategy is to replace sensitive data with placeholders, such as
    `<NAME>`, `<LOCATION>`, and `<PHONENUMBER>`. This allows the training process
    to learn how to process the natural language without exposing any sensitive data.
    This should not affect the ability of the trained system to process natural language
    because it would be rare that an application would classify utterances differently
    depending on specific names or locations. If the classification depends on more
    specific information than just, for example, `<LOCATION>`, a more specific placeholder
    can be used, such as a city or country name. It is also actually helpful to use
    placeholders in training because it reduces the chances of overfitting the trained
    model on the specific names in the training data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的策略是用占位符替换敏感数据，例如`<NAME>`、`<LOCATION>`和`<PHONENUMBER>`。这样可以让训练过程在不暴露任何敏感数据的情况下学习如何处理自然语言。这不应影响训练系统处理自然语言的能力，因为很少有应用程序会根据具体的姓名或地点对语句做出不同的分类。如果分类依赖于比`<LOCATION>`更具体的信息，可以使用更具体的占位符，例如城市名或国家名。实际上，在训练中使用占位符是有帮助的，因为它减少了训练模型对训练数据中特定名称过拟合的可能性。
- en: Ensuring the privacy of runtime data
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保运行时数据的隐私
- en: At runtime, when an application is deployed, incoming data will naturally contain
    sensitive data that the users enter in order to accomplish their goals. Any precautions
    that are normally taken to secure data entered with forms (non-NLP data such as
    social security numbers or credit card numbers) will of course apply to data entered
    with natural language text and speech. In some cases, the handling of this data
    will be subject to regulations and legislation, which you will need to be aware
    of and follow.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，当应用程序部署时，进入的数据自然会包含用户为实现目标而输入的敏感数据。通常对通过表单输入的数据（如社会安全号码或信用卡号码）采取的任何预防措施，当然也适用于通过自然语言文本和语音输入的数据。在某些情况下，处理这些数据将受到法规和立法的约束，你需要了解并遵守相关规定。
- en: Treating human subjects ethically
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理地对待人类受试者
- en: Natural language and speech data can be collected in the course of an experiment,
    such as a WoZ study, when human users or subjects provide speech and text for
    research. Universities and other research institutions have committees that review
    the planned procedures for any experiments with human subjects to ensure that
    the subjects are treated ethically. For example, subjects must provide informed
    consent to the experiment, they must not be harmed, their anonymity must be protected,
    deceptive practices must be avoided, and they must be able to withdraw from the
    study at any time. If you are collecting data in an experimental context, make
    sure to find out about the rules at your institution regarding approval by your
    human subjects committee or the equivalent. You may need to allocate extra lead
    time for the approval process to go through.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验过程中可以收集自然语言和语音数据，例如在 WoZ 研究中，当人类用户或受试者提供语音和文本进行研究时。大学和其他研究机构有委员会审查所有涉及人类受试者的实验程序，以确保受试者受到伦理对待。例如，受试者必须提供知情同意书，不能受到伤害，必须保护他们的匿名性，避免欺骗行为，并且他们必须能够随时退出研究。如果你在实验环境中收集数据，请确保了解你所在机构关于人类受试者委员会或类似机构审批程序的规则。你可能需要为审批过程预留额外的时间。
- en: Treating crowdworkers ethically
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公平对待众包工人
- en: If crowdworkers such as Amazon Mechanical Turk workers are used to create or
    annotate data, it is important to remember to treat them fairly – most importantly,
    to pay them fairly and on time – but also to make sure they have the right tools
    to do their jobs and to listen respectfully to any concerns or questions that
    they might have about their tasks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用众包工人（例如 Amazon Mechanical Turk 工人）来创建或注释数据，重要的是要记住公平对待他们——最重要的是按时支付公平报酬——还要确保他们拥有完成任务所需的工具，并尊重地听取他们可能对任务提出的任何疑虑或问题。
- en: Now that we have identified a source of data and discussed ethical issues, let’s
    move on to what we can do to get the data ready to be used in an application,
    or preprocessing. We will first cover general topics in preprocessing in the next
    section, and then discuss preprocessing techniques for specific applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了数据来源并讨论了伦理问题，让我们继续讨论如何准备数据以便在应用程序中使用，或称为预处理。在下一节中，我们将首先介绍预处理的常见主题，然后讨论针对特定应用程序的预处理技术。
- en: Preprocessing data
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理数据
- en: Once the data is available, it usually needs to be cleaned or preprocessed before
    the actual natural language processing begins.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据可用，通常需要在实际的自然语言处理开始之前进行清洗或预处理。
- en: There are two major goals in preprocessing data. The first goal is to remove
    items that can’t be processed by the system – these might include items such as
    emojis, HTML markup, spelling errors, foreign words, or some Unicode characters
    such as *smart quotes*. There are a number of existing Python libraries that can
    help with this, and we’ll be showing how to use them in the next section, *Removing
    non-text*. The second goal is addressed in the section called *Regularizing text*.
    We regularize text so that differences among words in the text that are not relevant
    to the application’s goal can be ignored. For example, in some applications, we
    might want to ignore the differences between uppercase and lowercase.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理数据有两个主要目标。第一个目标是去除系统无法处理的项目——这些项目可能包括表情符号、HTML标记、拼写错误、外语词汇或某些 Unicode 字符，如*智能引号*。有许多现有的
    Python 库可以帮助完成这些任务，我们将在下一节*去除非文本*中展示如何使用它们。第二个目标在名为*正则化文本*的部分中讨论。我们通过正则化文本，忽略文本中与应用目标无关的词语差异。例如，在某些应用中，我们可能希望忽略大小写的差异。
- en: There are many possible preprocessing tasks that can be helpful in preparing
    natural language data. Some are almost universally done, such as tokenization,
    while others are only done in particular types of applications. We will be discussing
    both types of preprocessing tasks in this section.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可能有助于准备自然语言数据的预处理任务。有些几乎是普遍需要的，如分词，而其他则仅在特定类型的应用中使用。在本节中，我们将讨论这两种类型的预处理任务。
- en: Different applications will need different kinds of preprocessing. Other than
    the most common preprocessing tasks, such as tokenization, exactly what kinds
    of preprocessing need to be done has to be carefully considered. A useful preprocessing
    step for one kind of application can completely remove essential information that’s
    needed in another kind of application. Consequently, for each preprocessing step,
    it’s important to think through its purpose and how it will contribute to the
    effectiveness of the application. In particular, you should use preprocessing
    steps thoughtfully if you will be using **large language models** (**LLMs**) in
    your NLP application. Since they are trained on normal (unregularized) text, regularizing
    input text to LLMs will make the input text less similar to the training text.
    This usually causes problems with machine learning models such as LLMs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用需要不同种类的预处理。除了最常见的预处理任务，如分词，究竟需要做哪些具体的预处理需要仔细考虑。对于某种应用而言有用的预处理步骤，可能会完全删除另一种应用所需的重要信息。因此，对于每个预处理步骤，必须考虑其目的以及它如何提高应用的有效性。特别是，如果你打算在NLP应用中使用**大型语言模型**（**LLMs**），你应当深思熟虑地使用预处理步骤。因为这些模型是在正常（未正则化的）文本上进行训练的，将输入文本正则化会使其与训练文本的相似度降低，通常会导致像LLM这样的机器学习模型出现问题。
- en: Removing non-text
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除非文本
- en: Many natural language components are only able to process textual characters,
    but documents can also contain characters that are not text. Depending on the
    purpose of the application, you can either remove them from the text completely
    or replace them with equivalent characters that the system is able to process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 许多自然语言组件只能处理文本字符，但文档中也可能包含非文本字符。根据应用的目的，你可以选择将它们完全删除，或者用系统能够处理的等效字符替换它们。
- en: Note that the idea of *non-text* is not an all-or-none concept. What is considered
    *non-text* is to some extent application-specific. While the standard characters
    in the ASCII character set ([https://www.ascii-code.com/](https://www.ascii-code.com/))
    are clear examples of text, other characters can sometimes be considered non-text.
    For example, your application’s *non-text* could include such items as currency
    symbols, math symbols, or texts written in scripts other than the main script
    of the rest of the text (such as a Chinese word in an otherwise English document).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*非文本*的概念并不是非黑即白的。什么被视为*非文本*在某种程度上是特定于应用的。尽管ASCII字符集中的标准字符（[https://www.ascii-code.com/](https://www.ascii-code.com/)）是典型的文本示例，其他字符有时也可以被认为是非文本。例如，你的应用中的*非文本*可能包括货币符号、数学符号，或者以与其他文本主脚本不同的文字书写的文本（例如，在一篇英文文档中出现的中文单词）。
- en: 'In the next two sections, we will look at removing or replacing two common
    examples of non-text: emojis and smart quotes. These examples should provide a
    general framework for removing other types of non-text.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，我们将讨论删除或替换两种常见的非文本类型：表情符号和智能引号。这些示例应该为删除其他类型的非文本提供一个通用框架。
- en: Removing emojis
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除表情符号
- en: One of the most common types of non-text is emojis. Social media posts are very
    likely to contain emojis, but they are a very interesting kind of text for NLP
    processing. If the natural language tools being used in your application don’t
    support emojis, the emojis can either be removed or replaced with their text equivalents.
    In most applications, it is likely that you will want to remove or replace emojis,
    but it is also possible in some cases that your NLP application will be able to
    interpret them directly. In that case, you won’t want to remove them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 非文本的最常见类型之一是表情符号。社交媒体帖子很可能包含表情符号，但它们对自然语言处理（NLP）来说是非常有趣的一类文本。如果你在应用中使用的自然语言工具不支持表情符号，可以将表情符号删除或用它们的文本等价物替换。在大多数应用中，你可能希望删除或替换表情符号，但在某些情况下，你的NLP应用可能能够直接解析它们。在这种情况下，你就不需要删除它们了。
- en: One way to replace emojis is to use regular expressions that search for the
    Unicode (see [https://home.unicode.org/](https://home.unicode.org/) for more information
    about Unicode) representations of emojis in the text. Still, the set of emojis
    is constantly expanding, so it is difficult to write a regular expression that
    covers all possibilities. Another approach is to use a Python library that directly
    accesses the Unicode data from [unicode.org](https://unicode.org), which defines
    standard emojis ([https://home.unicode.org/](https://home.unicode.org/)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 替换表情符号的一种方法是使用正则表达式，搜索文本中表情符号的 Unicode（有关 Unicode 的更多信息，请参见 [https://home.unicode.org/](https://home.unicode.org/)）。然而，表情符号的集合不断扩展，因此很难编写一个能涵盖所有可能性的正则表达式。另一种方法是使用一个
    Python 库，直接从 [unicode.org](https://unicode.org) 访问 Unicode 数据，它定义了标准的表情符号（[https://home.unicode.org/](https://home.unicode.org/)）。
- en: One package that can be used to remove or replace emojis is `demoji` ([https://pypi.org/project/demoji/](https://pypi.org/project/demoji/)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以用来删除或替换表情符号的包是 `demoji` （[https://pypi.org/project/demoji/](https://pypi.org/project/demoji/)）。
- en: 'Using `demoji` is just a matter of installing it and running over text that
    may contain undesired emojis:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `demoji` 只需要安装它，然后对可能包含不需要的表情符号的文本进行处理：
- en: 'First, install `demoji`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装 `demoji`：
- en: '[PRE0]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, given a text that contains emojis, run to replace the emojis with descriptions
    using the following code:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，给定一个包含表情符号的文本，使用以下代码将表情符号替换为描述：
- en: '[PRE1]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or, if you want to remove the emojis completely or replace them with a specific
    alternative of your choosing, you can run the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你想完全删除表情符号或将其替换为你选择的特定替代项，可以运行以下代码：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For example, *Figure 5.1* shows a text that includes an emoji of a birthday
    cake and shows how this can be replaced with the description `:birthday cake:`,
    or simply removed:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图 5.1* 显示了一个包含生日蛋糕表情符号的文本，并展示了如何用描述 `:birthday cake:` 替换它，或者干脆删除它：
- en: '![Figure 5.1 – Replacing or removing emojis](img/B19005_05_01.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 替换或删除表情符号](img/B19005_05_01.jpg)'
- en: Figure 5.1 – Replacing or removing emojis
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 替换或删除表情符号
- en: Even if the emoji doesn’t cause problems with running the software, leaving
    the emoji in place means that any meaning associated with the emoji will be ignored,
    because the NLP software will not understand the meaning of the emoji. If the
    emoji is replaced with a description, some of its meaning (for example, that the
    emoji represents a birthday cake) can be taken into account.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 即使表情符号不会导致软件运行问题，保留表情符号意味着与表情符号相关的任何含义将被忽略，因为 NLP 软件无法理解表情符号的含义。如果表情符号被替换为描述，则可以考虑其部分含义（例如，表情符号代表生日蛋糕）。
- en: Removing smart quotes
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除智能引号
- en: 'Word processing programs sometimes automatically change typed quotation marks
    into *smart quotes*, or *curly quotes*, which look better than straight quotes,
    but which other software may not be prepared for. Smart quotes can cause problems
    with some NLP software that is not expecting smart quotes. If your text contains
    smart quotes, they can easily be replaced with the normal Python string replacement
    method, as in the following code:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 文字处理程序有时会自动将输入的引号转换为 *智能引号* 或 *弯引号*，这些引号看起来比直引号更美观，但其他软件可能无法处理这些引号。智能引号可能会导致一些自然语言处理（NLP）软件出现问题，因为这些软件可能没有预设智能引号。如果文本中包含智能引号，可以通过正常的
    Python 字符串替换方法轻松替换，如以下代码所示：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the replacement straight quotes in the `replace` method need to be
    escaped with a backslash, like any use of literal quotes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 `replace` 方法中替换的直引号需要用反斜杠转义，就像任何字面量引号的使用一样。
- en: The previous two sections covered removing non-text items such as emojis and
    smart quotes. We will now talk about some techniques for regularizing text or
    modifying it to make it more uniform.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 前两节内容介绍了如何去除非文本项，如表情符号和智能引号。接下来我们将讨论一些规范化文本或修改文本以使其更统一的技巧。
- en: Regularizing text
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规范化文本
- en: In this section, we will be covering the most important techniques for regularizing
    text. We will talk about the goals of each technique and how to apply it in Python.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一些规范化文本的最重要技巧。我们将讨论每种技巧的目标，以及如何在 Python 中应用它。
- en: Tokenization
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: 'Nearly all NLP software operates on the level of words, so the text needs to
    be broken into words for processing to work. In many languages, the primary way
    of separating text into words is by whitespaces, but there are many special cases
    where this heuristic doesn’t work. In *Figure 5**.2*, you can see the code for
    splitting on whitespace and the code for tokenization using NLTK:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的 NLP 软件都在单词级别上操作，因此文本需要被拆分成单词才能进行处理。在许多语言中，分隔文本为单词的主要方式是空格，但也有许多特殊情况使得这个启发式方法不起作用。在*图
    5.2*中，您可以看到用于空格拆分的代码以及使用 NLTK 进行分词的代码：
- en: '![Figure 5.2 – Python code for tokenization by splitting on whitespace and
    using NLTK’s tokenization](img/B19005_05_02.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 使用 Python 代码进行空格拆分并应用 NLTK 分词](img/B19005_05_02.jpg)'
- en: Figure 5.2 – Python code for tokenization by splitting on whitespace and using
    NLTK’s tokenization
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 使用 Python 代码进行空格拆分并应用 NLTK 分词
- en: 'Running the code in *Figure 5**.2* results in the tokenizations shown in *Table
    5.1*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 运行*图 5.2*中的代码将产生*表 5.1*中显示的分词结果：
- en: '| **Example** | **Issue** | **Result from splitting** **on whitespace** | **NLTK
    result** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **示例** | **问题** | **根据空格拆分的结果** | **NLTK 结果** |'
- en: '| `Walk here.` | Punctuation should not be included in the token | `[''``Walk'',
    ''here.'']` | `[''Walk'', ''``here'', ''.'']` |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `Walk here.` | 标点符号不应包含在标记中 | `[''``Walk'', ''here.'']` | `[''Walk'', ''``here'',
    ''.'']` |'
- en: '| `Walk` `here.` | Extra whitespace should not count as a token | `[''Walk'',
    '''', ''``here.'']` | `[''Walk'', ''``here'', ''.'']` |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `Walk` `here.` | 多余的空格不应计为一个标记 | `[''Walk'', '''', ''``here.'']` | `[''Walk'',
    ''``here'', ''.'']` |'
- en: '| `Don''t` `walk here.` | The contraction “don’t” should count as two tokens
    | `["Don''t", ''``walk'', ''here.'']` | `[''Do'', "n''t", ''walk'', ''``here'',
    ''.'']` |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `Don''t` `walk here.` | 缩写“don’t”应视为两个标记 | `["Don''t", ''``walk'', ''here.'']`
    | `[''Do'', "n''t", ''walk'', ''``here'', ''.'']` |'
- en: '| `$``100` | The “$” should be a separate token | `[''$``100'']` | `[''$'',
    ''``100'']` |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| `$``100` | “$”应作为一个单独的标记 | `[''$``100'']` | `[''$'', ''``100'']` |'
- en: Table 5.1 – Tokenization results by splitting on whitespace and using NLTK’s
    tokenization
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.1 – 通过拆分空格并使用 NLTK 的分词方法的分词结果
- en: 'Looking at *Table 5.1*, we can see that the simple heuristic of splitting the
    text on whitespace results in errors in some cases:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从*表 5.1*中，我们可以看到，仅通过空格拆分文本的简单启发式方法，在某些情况下会导致错误：
- en: In the first row of *Table 5.1*, we can compare the two approaches when punctuation
    occurs at the end of a token, but there is no whitespace between the token and
    punctuation. This means that just separating tokens on whitespace results in incorrectly
    including the punctuation in the token. This means that `walk`, `walk,`, `walk.`,
    `walk?`, `walk;`, `walk:`, and `walk!` will all be considered to be different
    words. If all of those words appear to be different, any generalizations found
    in training based on one version of the word won’t apply to the other versions.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在*表 5.1*的第一行中，我们可以比较两种方法，当标点符号出现在标记的末尾，但标记与标点符号之间没有空格时。这样仅通过空格拆分标记会导致标点符号被错误地包含在标记中。这意味着`walk`、`walk,`、`walk.`、`walk?`、`walk;`、`walk:`和`walk!`将被视为不同的单词。如果这些单词看起来不同，那么在训练中基于某个版本的单词得出的任何泛化结果将无法应用于其他版本。
- en: In the second row, we can see that if split on whitespace, two whitespaces in
    a row will result in an extra blank token. The result of this will be to throw
    off any algorithms that take into account the fact that two words are next to
    each other.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二行中，我们可以看到，如果根据空格进行拆分，连续的两个空格将导致一个额外的空白标记。这会导致任何考虑到两个单词相邻的算法产生偏差。
- en: Contractions also cause problems when splitting on whitespace is used for tokenization.
    Contractions of two words won’t be separated into their components, which means
    that the **natural language understanding** (**NLU**) algorithms won’t be able
    to take into account that *do not* and *don’t* have the same meaning.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩写也会导致问题，当使用空格拆分进行分词时。两个单词的缩写不会被分开，这意味着**自然语言理解**（**NLU**）算法无法考虑到*do not*和*don’t*有相同的含义。
- en: Finally, when presented with words with monetary amounts or other measurements,
    the algorithms won’t be able to take into account that *$100* and *100 dollars*
    have the same meaning.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，当遇到带有货币金额或其他度量单位的词时，算法无法考虑到*100美元*和*100 dollars*有相同的含义。
- en: It is tempting to try to write regular expressions to take care of these exceptions
    to the generalization that most words are surrounded by whitespace. However, in
    practice, it is very hard to capture all of the cases. As we attempt to cover
    more cases with regular expressions, the regular expressions will become more
    complex and difficult to maintain. For that reason, it is preferable to use a
    library such as NLTK’s, which has been developed over many years and has been
    thoroughly tested. You can try different texts with this code to see what kinds
    of results you get with different tokenization approaches.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易尝试写正则表达式来处理这些与一般化规则的例外情况，即大多数单词周围有空格。然而，在实践中，很难捕捉到所有的情况。随着我们尝试用正则表达式涵盖更多的情况，正则表达式会变得越来越复杂，也更难维护。因此，最好使用像NLTK这样的库，它经过多年开发并经过充分测试。你可以尝试使用不同的文本和代码，看看使用不同的分词方法会得到什么样的结果。
- en: As *Table 5.1* shows, either way, the result will be a list of strings, which
    is a convenient form for further processing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如*表 5.1*所示，不管哪种方式，结果都会是一个字符串列表，这是一种便于进一步处理的格式。
- en: Lower casing
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小写化
- en: 'In languages that use uppercase and lowercase in their writing systems, most
    documents contain words with upper and lowercase letters. As in the case of tokenization,
    having the same word written in slightly different formats means that data from
    one format won’t apply to data in the other formats. For example, *Walk*, *walk*,
    and *WALK* will all count as different words. In order to make them all count
    as the same word, the text is normally all in lowercase. This can be done by looping
    over a list of word tokens and applying the `lower()` Python function, as shown
    in *Figure 5**.3*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用大写和小写字母的语言中，大多数文档包含大写和小写字母的单词。就像分词一样，具有略微不同格式的相同单词意味着来自一种格式的数据不能适用于另一种格式的数据。例如，*Walk*、*walk*和*WALK*都会被视为不同的单词。为了将它们视为相同的单词，文本通常会被全部转换为小写。这可以通过循环遍历单词令牌列表并应用`lower()`的Python函数来实现，如*图
    5.3*所示：
- en: '![Figure 5.3 – Converting text to all lowercase](img/B19005_05_03.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 将文本转换为小写](img/B19005_05_03.jpg)'
- en: Figure 5.3 – Converting text to all lowercase
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 将文本转换为小写
- en: Converting all words to lowercase has some drawbacks, however. Case differences
    are sometimes important for meaning. The biggest example of this is that it will
    make it hard for NLP software to tell the difference between proper names and
    ordinary words, which differ in case. This can cause errors in **part of speech**
    (**POS**) tagging or **named entity recognition** (**NER**) if the tagger or named
    entity recognizer is trained on data that includes case differences. Similarly,
    words can sometimes be written in all caps for emphasis. This may indicate something
    about the sentiment expressed in the sentence – perhaps the writer is excited
    or angry – and this information could be helpful in sentiment analysis. For these
    reasons, the position of each preprocessing step in a pipeline should be considered
    to make sure no information is removed before it’s needed. This will be discussed
    in more detail in the *Text preprocessing pipeline* section later in this chapter.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有单词转换为小写确实有一些缺点。大小写的差异有时对意义很重要。最大的例子是，这会使得自然语言处理（NLP）软件难以区分专有名词和普通单词，因为它们在大小写上有所不同。如果标注器或命名实体识别器在包含大小写差异的数据上进行训练，这可能会导致**词性标注**（**POS**）或**命名实体识别**（**NER**）的错误。同样，单词有时会全大写以示强调。这可能表示句子中表达的情感——也许作者感到兴奋或愤怒——而这类信息在情感分析中可能很有帮助。出于这些原因，应该考虑每个预处理步骤在管道中的位置，以确保在需要之前不会丢失任何信息。关于这一点，将在本章后面的*文本预处理管道*部分详细讨论。
- en: Stemming
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: Words in many languages appear in different forms depending on how they’re used
    in a sentence. For example, English nouns have different forms depending on whether
    they’re singular or plural, and English verbs have different forms depending on
    their tense. English has only a few variations, but other languages sometimes
    have many more. For example, Spanish has dozens of verb forms that indicate past,
    present, or future tenses or whether the subject of the verb is first, second,
    or third person, or singular or plural. Technically, in linguistics, these different
    forms are referred to as **inflectional morphology**. The endings themselves are
    referred to as **inflectional morphemes**.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 许多语言中的单词根据它们在句子中的使用方式会有不同的形式。例如，英语名词根据是单数还是复数而有不同的形式，英语动词根据时态不同而有不同的形式。英语的变化形式较少，但其他语言有时会有更多。例如，西班牙语有几十种动词形式，用于表示过去、现在或未来的时态，或者动词的主语是第一、第二还是第三人称，单数还是复数。从语言学角度讲，这些不同的形式被称为**屈折形态学**。这些词尾被称为**屈折语素**。
- en: Of course, in speech or text that is directed toward another person, these different
    forms are very important in conveying the speaker’s meaning. However, if the goal
    of our NLP application is to classify documents into different categories, paying
    attention to different forms of a word is likely not to be necessary. Just as
    with punctuation, different forms of words can cause them to be treated as completely
    separate words by NLU processors, despite the fact that the words are very similar
    in meaning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在对话或文本交流中，这些不同的形式对传达说话者的意思非常重要。然而，如果我们的NLP应用目标是将文档分类为不同的类别，那么关注单词的不同形式可能就不那么必要了。就像标点符号一样，单词的不同形式可能会导致它们在自然语言理解处理器中被视为完全不同的单词，尽管这些单词在意义上非常相似。
- en: '**Stemming** and **lemmatization** are two similar methods of regularizing
    these different forms. Stemming is the simpler approach, so we will look at that
    first. Stemming basically means removing specific letters that end some words
    and that are frequently, but not always, inflectional morphemes – for example,
    the *s* at the end of *walks*, or the *ed* at the end of *walked*. Stemming algorithms
    don’t have any knowledge of the actual words in a language; they’re just guessing
    what may or may not be an ending. For that reason, they make a lot of mistakes.
    They can make mistakes by either removing too many letters or not enough letters,
    resulting in two words being collapsed that are actually different words, or not
    collapsing words that should be treated as the same word.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取**和**词形还原**是两种类似的正则化方法，用于处理这些不同的形式。词干提取是更简单的方法，因此我们首先讨论它。词干提取基本上意味着移除一些单词末尾的特定字母，这些字母通常但不总是屈折语素——例如，*walks*
    末尾的 *s*，或 *walked* 末尾的 *ed*。词干提取算法并不了解语言中的实际单词；它们只是猜测哪些可能是词尾。因此，它们会犯很多错误。它们可能会犯错，要么删除太多字母，要么删除的字母不够，导致将实际不同的单词合并为一个，或者没有将应该视为相同单词的词合并。'
- en: '`PorterStemmer` is a widely used stemming tool and is built into NLTK. It can
    be used as shown in *Figure 5**.4*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`PorterStemmer`是一个广泛使用的词干提取工具，并且已内置于NLTK中。它可以像*图5.4*所示那样使用：'
- en: '![Figure 5.4 – Results from stemming with PorterStemmer](img/B19005_05_04.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 使用PorterStemmer进行词干提取的结果](img/B19005_05_04.jpg)'
- en: Figure 5.4 – Results from stemming with PorterStemmer
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 使用PorterStemmer进行词干提取的结果
- en: 'Note that the results include a number of mistakes. While `walked` becomes
    `walk`, and `going` becomes `go`, which are good results, the other changes that
    the stemmer made are errors:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，结果中包含了一些错误。虽然`walked`变成了`walk`，`going`变成了`go`，这些是正确的结果，但词干提取器做出的其他更改则是错误的：
- en: '`exercise` ![](img/01.png) `exercis`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exercise` ![](img/01.png) `exercis`'
- en: '`every` ![](img/01.png) `everi`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`every` ![](img/01.png) `everi`'
- en: '`evening` ![](img/01.png) `even`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evening` ![](img/01.png) `even`'
- en: '`this` ![](img/01.png) `thi`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`this` ![](img/01.png) `thi`'
- en: The Porter stemmer also only works for English because its stemming algorithm
    includes heuristics, such as removing the *s* at the end of words, that only apply
    to English. NLTK also includes a multilingual stemmer, called the Snowball stemmer,
    that can be used with more languages, including Arabic, Danish, Dutch, English,
    Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian,
    Russian, Spanish, and Swedish.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Porter词干提取器也仅适用于英语，因为其词干提取算法包含了一些启发式规则，例如移除词尾的*s*，这些规则只适用于英语。NLTK还包括一个多语言词干提取器，叫做Snowball词干提取器，可以用于更多语言，包括阿拉伯语、丹麦语、荷兰语、英语、芬兰语、法语、德语、匈牙利语、意大利语、挪威语、葡萄牙语、罗马尼亚语、俄语、西班牙语和瑞典语。
- en: However, since these stemmers don’t have any specific knowledge of the words
    of the languages they’re applied to, they can make mistakes, as we have seen.
    A similar but more accurate approach actually makes use of a dictionary, so that
    it doesn’t make errors like the ones listed previously. This approach is called
    **lemmatization**.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于这些词干提取器并没有对它们所应用语言的单词有具体的了解，它们可能会犯错，正如我们所看到的那样。一个类似但更准确的方法实际上是使用字典，以避免像前面列出的错误。这种方法称为
    **词形还原**。
- en: Lemmatizing and part of speech tagging
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词形还原和词性标注
- en: Lemmatization, like stemming, has the goal of reducing variation in the words
    that occur in the text. However, lemmatization actually replaces each word with
    its root word (found by looking the word up in a computational dictionary) rather
    than simply removing what looks like suffixes. However, identifying the root word
    often depends on the part of speech, and lemmatization can be inaccurate if it
    doesn’t know the word’s part of speech. The part of speech can be identified through
    **part of speech tagging**, covered in [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059),
    which assigns the most probable part of speech to each word in a text. For that
    reason, lemmatization and part of speech tagging are often performed together.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原与词干提取一样，旨在减少文本中出现单词的变异。然而，词形还原实际上是用单词的词根替换每个单词（通过查阅计算词典找到词根），而不仅仅是去除看似是后缀的部分。然而，识别词根通常取决于词性，如果不知道单词的词性，词形还原可能会不准确。词性可以通过
    **词性标注** 来识别，词性标注会将每个单词最可能的词性分配给文本中的每个单词，这在 [*第 3 章*](B19005_03.xhtml#_idTextAnchor059)
    中有所介绍。正因为如此，词形还原和词性标注通常会一起执行。
- en: For the dictionary in this example, we’ll use WordNet, developed at Princeton
    University ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)),
    an important source of information about words and their parts of speech. The
    original WordNet was developed for English, but WordNets for other languages have
    also been developed. We briefly mentioned WordNet in the *Semantic analysis* section
    of [*Chapter 3*](B19005_03.xhtml#_idTextAnchor059), because WordNet contains semantic
    information as well as information about parts of speech.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用由普林斯顿大学开发的 WordNet（[https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)），这是一个关于单词及其词性的重要信息来源。原版
    WordNet 是为英语开发的，但也有针对其他语言的 WordNet。我们在 [*第 3 章*](B19005_03.xhtml#_idTextAnchor059)
    的 *语义分析* 部分简要提到过 WordNet，因为 WordNet 包含了语义信息以及关于词性的资料。
- en: 'In this example, we’ll just use the part of speech information, not the semantic
    information. *Figure 5**.5* shows importing the WordNet lemmatizer, the tokenizer,
    and the part of speech tagger. We then have to align the names of the parts of
    speech between WordNet and the part of speech tagger, because the part of speech
    tagger and WordNet don’t use the same names for the parts of speech. We then go
    through the text, lemmatizing each word:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将仅使用词性信息，而不使用语义信息。*图 5.5* 显示了导入 WordNet 词形还原器、分词器和词性标注器的过程。接着，我们需要对齐
    WordNet 和词性标注器之间的词性名称，因为词性标注器和 WordNet 使用的词性名称不完全相同。然后，我们遍历文本，对每个单词进行词形还原：
- en: '![Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve
    walked every evening this week”](img/B19005_05_05.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 对“going for a walk is the best exercise. i’ve walked every evening
    this week”进行词形还原](img/B19005_05_05.jpg)'
- en: Figure 5.5 – Lemmatization for “going for a walk is the best exercise. i’ve
    walked every evening this week”
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 对“going for a walk is the best exercise. i’ve walked every evening this
    week”进行词形还原
- en: As the lemmatized result shows, many of the words in the input text have been
    replaced by their lemmas – `going` is replaced with `go`, `is` is replaced with
    `be`, and `walked` is replaced with `walk`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如词形还原结果所示，输入文本中的许多单词已被替换为它们的词根——`going` 被替换为 `go`，`is` 被替换为 `be`，`walked` 被替换为
    `walk`。
- en: Note that `evening` hasn’t been replaced by *even*, as it was in the stemming
    example. If *evening* had been the present participle of the verb *even*, it would
    have been replaced by `even`, but *even* isn’t the root word for *evening* here.
    In this case, *evening* just refers to the time of day.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`evening` 没有像在词干提取示例中那样被替换为 *even*。如果 *evening* 是动词 *even* 的现在分词，它将被替换为 `even`，但在这里，*even*
    不是 *evening* 的词根。在这种情况下，*evening* 仅指一天中的时间。
- en: Stopword removal
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停用词移除
- en: '**Stopwords** are extremely common words that are not helpful in distinguishing
    documents and so they are often removed in classification applications. However,
    if the application involves any kind of detailed sentence analysis, these common
    words are needed so that the system can figure out what the analysis should be.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**停用词**是非常常见的词，对于区分文档没有帮助，因此在分类应用中常常被移除。然而，如果应用涉及任何形式的详细句子分析，这些常见的词是需要的，以便系统能够理解分析的内容。'
- en: Normally, stopwords include words such as pronouns, prepositions, articles,
    and conjunctions. Which words should be considered to be stopwords for a particular
    language is a matter of judgment. For example, spaCy has many more English stopwords
    (326) than NLTK (179). These specific stopwords were chosen by the spaCy and NLTK
    developers because they thought those stopwords would be useful in practice. You
    can use whichever one you find more convenient.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，停用词包括代词、介词、冠词和连词等词。哪些词应该被认为是某种语言的停用词，这个问题需要判断。例如，spaCy的英文停用词比NLTK多得多（326个
    vs. 179个）。这些特定的停用词是由spaCy和NLTK的开发者选择的，因为他们认为这些停用词在实际应用中会很有用。你可以选择你认为更方便的一个。
- en: Let’s take a look at the stopwords provided by each system.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看每个系统提供的停用词。
- en: 'First, to run the NLTK and spaCy systems, you may need to do some preliminary
    setup. If you are working in a command-line or terminal environment, you can ensure
    that NLTK and spaCy are available by entering the following commands on the command
    line:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了运行NLTK和spaCy系统，你可能需要做一些初步设置。如果你在命令行或终端环境中工作，你可以通过在命令行中输入以下命令来确保NLTK和spaCy可用：
- en: '`pip install -U pip` `setuptools wheel`'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pip install -U pip` `setuptools wheel`'
- en: '`pip install -``U spacy`'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pip install -U spacy`'
- en: '`python -m spacy` `download en_core_web_sm`'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`python -m spacy` `download en_core_web_sm`'
- en: On the other hand, if you’re working in the (recommended) Jupyter Notebook environment
    covered in [*Chapter 4*](B19005_04.xhtml#_idTextAnchor085), you can enter the
    same commands in a Jupyter code cell but precede each command with `!`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你在（推荐的）Jupyter Notebook环境中工作，环境的设置在[**第4章**](B19005_04.xhtml#_idTextAnchor085)中有详细介绍，你可以在Jupyter代码单元中输入相同的命令，但在每个命令前加上`!`。
- en: 'Once you’ve confirmed that your NLTK and spaCy environments are set up, you
    can look at the NLTK stopwords by running the code in *Figure 5**.6*:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确认了你的NLTK和spaCy环境已经设置好，你可以通过运行*图5**.6*中的代码来查看NLTK的停用词：
- en: '![Figure 5.6 – Viewing the first few stopwords for NLTK](img/B19005_05_06.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 查看NLTK的前几个停用词](img/B19005_05_06.jpg)'
- en: Figure 5.6 – Viewing the first few stopwords for NLTK
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 查看NLTK的前几个停用词
- en: Note that *Figure 5**.6* just shows the first few stopwords. You can see them
    all by running the code in your environment.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*图5**.6*仅显示了前几个停用词。你可以通过在你的环境中运行代码来查看所有停用词。
- en: 'To see the spaCy stopwords, run the code in *Figure 5**.7*, which also just
    shows the first few stopwords:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看spaCy的停用词，请运行*图5**.7*中的代码，该代码也仅显示前几个停用词：
- en: '![Figure 5.7 – Viewing the first few stopwords for spaCy](img/B19005_05_07.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 查看spaCy的前几个停用词](img/B19005_05_07.jpg)'
- en: Figure 5.7 – Viewing the first few stopwords for spaCy
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 查看spaCy的前几个停用词
- en: Comparing the stopwords provided by both packages, we can see that the two sets
    have a lot in common, but there are differences as well. In the end, the stopwords
    you use are up to you. Both sets work well in practice.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个包提供的停用词，我们可以看到这两个集合有很多相同之处，但也存在差异。最终，你使用哪个停用词集合取决于你。两个集合在实践中都表现良好。
- en: Removing punctuation
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: Removing punctuation can also be useful since punctuation, like stopwords, appears
    in most documents and therefore doesn’t help distinguish document categories.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号也非常有用，因为标点符号和停用词一样，出现在大多数文档中，因此并不能帮助区分文档类别。
- en: Punctuation can be removed by defining a string of punctuation symbols and removing
    items in that string with regular expressions, or by removing every non-alphanumeric
    word in the text. The latter approach is more robust because it’s easy to overlook
    an uncommon punctuation symbol.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 标点符号可以通过定义一个标点符号字符串，并使用正则表达式移除字符串中的项，或者通过移除文本中每个非字母数字的词来去除。后一种方法更具鲁棒性，因为容易忽视不常见的标点符号。
- en: 'The code to remove punctuation can be seen in *Figure 5**.8*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 删除标点符号的代码可以在*图5**.8*中看到：
- en: '![Figure 5.8 – Removing punctuation](img/B19005_05_08.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 删除标点符号](img/B19005_05_08.jpg)'
- en: Figure 5.8 – Removing punctuation
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 删除标点符号
- en: The original text is the value of the `text_to_remove_punct` variable, in *Figure
    5**.8*, which contains several punctuation marks – specifically, an exclamation
    mark, a comma, and a period. The result is the value of the `tokens_no_punct`
    variable, shown in the last line.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本是`text_to_remove_punct`变量的值，位于*图 5.8*中，其中包含几个标点符号——具体来说，是感叹号、逗号和句号。结果是`tokens_no_punct`变量的值，显示在最后一行。
- en: Spelling correction
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拼写更正
- en: 'Correcting misspelled words is another way of removing noise and regularizing
    text input. Misspelled words are much less likely to have occurred in training
    data than correctly spelled words, so they will be harder to recognize when a
    new text is being processed. In addition, any training that includes the correctly
    spelled version of the word will not recognize or be able to make use of data
    including the incorrectly spelled word. This means that it’s worth considering
    adding a spelling correction preprocessing step to the NLP pipeline. However,
    we don’t want to automatically apply spelling correction in every project. Some
    of the reasons not to use spelling correction are the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 更正拼写错误是去除噪音并规范化文本输入的另一种方式。拼写错误的单词在训练数据中出现的可能性远低于正确拼写的单词，因此在处理新的文本时，它们会更难识别。此外，任何包含正确拼写单词版本的训练将无法识别或利用包含拼写错误单词的数据。这意味着值得考虑在NLP流程中添加拼写更正预处理步骤。然而，我们不希望在每个项目中自动应用拼写更正。以下是一些不使用拼写更正的原因：
- en: Some types of text will naturally be full of spelling mistakes – for example,
    social media posts. Because spelling mistakes will occur in texts that need to
    be processed by the application, it might be a good idea not to try to correct
    spelling mistakes in either the training or runtime data.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些类型的文本自然充满了拼写错误——例如，社交媒体帖子。由于拼写错误会出现在需要被应用程序处理的文本中，因此在训练或运行时数据中可能不应尝试更正拼写错误。
- en: Spelling error correction doesn’t always do the right thing, and a spelling
    correction that results in the wrong word won’t be helpful. It will just introduce
    errors into the processing.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写错误更正并不总是做到正确的事情，导致错误单词的拼写更正是没有帮助的。它只会给处理过程引入错误。
- en: Some types of texts include many proper names or foreign words that aren’t known
    to the spell checker, which will try to correct them. Again, this will just introduce
    errors.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有些类型的文本包含许多拼写检查器不知道的专有名词或外来词，这些词会被检查器尝试更正。再一次，这只会引入错误。
- en: 'If you do choose to use spelling corrections, there are many spell checkers
    available in Python. One recommended spell checker is `pyspellchecker`, which
    can be installed as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择使用拼写更正，Python中有许多拼写检查工具可供使用。一个推荐的拼写检查器是`pyspellchecker`，可以按如下方式安装：
- en: '[PRE4]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The spell-checking code and the result can be seen in *Figure 5**.9*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写检查代码和结果可以在*图 5.9*中看到：
- en: '![Figure 5.9 – Spell-checking with pyspellchecker](img/B19005_05_09.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 使用pyspellchecker进行拼写检查](img/B19005_05_09.jpg)'
- en: Figure 5.9 – Spell-checking with pyspellchecker
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 使用pyspellchecker进行拼写检查
- en: You can see from *Figure 5**.9* that it’s easy to make mistakes with spell-checking.
    `pyspellchecker` correctly changes `agains` to `against`, but it also made a mistake
    correcting `Ms.` to `is` and it didn’t know anything about the name `Ramalingam`,
    so it didn’t have a correction for that.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 5.9*中可以看出，拼写检查容易出错。`pyspellchecker`正确地将`agains`更正为`against`，但它也犯了错误，将`Ms.`更正为`is`，而且它对`Ramalingam`这个名字一无所知，所以没有做出任何更正。
- en: Keep in mind that input generated by **ASR** will not contain spelling errors,
    because ASR can only output words in their dictionaries, which are all correctly
    spelled. Of course, ASR output can contain mistakes, but those mistakes will just
    be the result of substituting a wrong word for the word that was actually spoken,
    and they can’t be corrected by spelling correction.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，由**ASR**生成的输入不会包含拼写错误，因为ASR只能输出字典中的单词，这些单词都是正确拼写的。当然，ASR的输出可能会包含错误，但这些错误仅仅是由于错误的单词替代了实际说出的单词，而这些错误无法通过拼写更正来修复。
- en: Also note that stemming and lemmatization can result in tokens that aren’t real
    words, which you don’t want to be corrected. If spelling correction is used in
    a pipeline, make sure that it occurs before stemming and lemmatization.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，词干提取和词形还原可能会导致出现不是实际单词的标记，而这些标记你不希望被更正。如果在处理流程中使用拼写更正，请确保它发生在词干提取和词形还原之前。
- en: Expanding contractions
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 展开缩写词
- en: Another way to increase the uniformity of data is to expand contractions – that
    is, words such as *don’t* would be expanded to their full form, *do not*. This
    will allow the system to recognize an occurrence of *do* and an occurrence of
    *not* when it finds *don’t*.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 增加数据统一性的另一种方法是展开缩写词——也就是说，将像*don’t*这样的词展开为其完整形式*do not*。这样，系统就能在遇到*don’t*时识别出*do*和*not*。
- en: So far, we have reviewed many generic preprocessing techniques. Next, let’s
    move on to more specific techniques that are only applicable to certain types
    of applications.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了许多通用的预处理技术。接下来，让我们转向一些仅适用于特定类型应用的更具体的技术。
- en: Application-specific types of preprocessing
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用特定类型的预处理
- en: The preprocessing topics we have covered in the previous sections are generally
    applicable to many types of text in many applications. Additional preprocessing
    steps can also be used in specific applications, and we will cover these in the
    next sections.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面部分讨论的预处理技术通常适用于许多应用中的各种文本。对于特定应用，还可以使用额外的预处理步骤，我们将在接下来的部分中介绍这些步骤。
- en: Substituting class labels for words and numbers
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换类标签为词语和数字
- en: 'Sometimes data includes specific words or tokens that have equivalent semantics.
    For example, a text corpus might include the names of US states, but for the purposes
    of the application, we only care that *some* state was mentioned – we don’t care
    which one. In that case, we can substitute a *class token* for the specific state
    name. Consider the interaction in *Figure 5**.10*:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据中会包含一些语义等价的特定词语或标记。例如，一个文本语料库可能包括美国各州的名称，但对于该应用而言，我们只关心提到了*某个*州，而不在乎是哪一个。在这种情况下，我们可以用*类标记*替代特定的州名。请参考*图
    5.10*中的交互：
- en: '![Figure 5.10 – Class token substitution](img/B19005_05_10New.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – 类标记替换](img/B19005_05_10New.jpg)'
- en: Figure 5.10 – Class token substitution
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 类标记替换
- en: If we substitute the class token, `<state_name>`, for `Texas`, all of the other
    state names will be easier to recognize, because instead of having to learn 50
    states, the system will only have to learn about the general class, `<state_name>`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将类标记`<state_name>`替换为`Texas`，那么其他州名就更容易识别了，因为系统只需要学习关于通用类`<state_name>`的内容，而不是学习所有50个州的名称。
- en: Another reason to use class tokens is if the texts contain alphanumeric tokens
    such as dates, phone numbers, or social security numbers, especially if there
    are too many to enumerate. Class tokens such as `<social_security_number>` can
    be substituted for the actual numbers. This has the added benefit of masking,
    or *redacting*, sensitive information such as specific social security numbers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类标记的另一个原因是，如果文本包含字母数字标记，如日期、电话号码或社会安全号码，尤其是当有很多这种信息需要枚举时，可以用像`<social_security_number>`这样的类标记来替代实际的号码。这还有一个附加好处，那就是可以遮掩或*删减*敏感信息，如具体的社会安全号码。
- en: Redaction
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删减
- en: As we discussed in the *Ensuring privacy and observing ethical considerations*
    section, data can contain sensitive information such as people’s names, health
    information, social security numbers, or telephone numbers. This information should
    be redacted before the data is used in training.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*确保隐私和遵守伦理考量*一节中讨论的那样，数据可能包含敏感信息，如人名、健康信息、社会安全号码或电话号码。在使用数据进行训练之前，应该先对这些信息进行删减。
- en: Domain-specific stopwords
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特定领域的停用词
- en: Both NLTK and spaCy have the capability to add and remove stopwords from their
    lists. For example, if your application has some very common domain-specific words
    that aren’t on the built-in stopword list, you can add these words to the application-specific
    stopword list. Conversely, if some words that are normally stopwords are actually
    meaningful in the application, these words can be removed from the stopwords.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 和 spaCy 都具备从它们的列表中添加和删除停用词的功能。例如，如果你的应用程序中有一些非常常见的特定领域词汇，而这些词不在内建的停用词列表中，你可以将这些词添加到应用特定的停用词列表中。相反，如果一些通常被认为是停用词的词在应用程序中有实际意义，可以将这些词从停用词中移除。
- en: A good example is the word *not*, which is a stopword for both NLTK and spaCy.
    In many document classification applications, it’s fine to consider *not* as a
    stopword; however, in applications such as sentiment analysis, *not* and other
    related words (for example, *nothing* or *none*) can be important clues for a
    negative sentiment. Removing them can cause errors if sentences such as *I do
    not like this product* becomes *I do like this product*. In that case, you should
    remove *not* and other negative words from the list of stopwords you are using.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是词语*not*，在 NLTK 和 spaCy 中都是停用词。在许多文档分类应用中，将*not*视为停用词是可以接受的；然而，在情感分析等应用中，*not*及其他相关词语（例如*nothing*或*none*）可以是负面情感的重要线索。如果移除它们，则可能会导致错误，例如句子*I
    do not like this product* 变为 *I do like this product*。在这种情况下，您应该从您正在使用的停用词列表中移除*not*和其他否定词语。
- en: Remove HTML markup
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 移除 HTML 标记
- en: If the application is based on web pages, they will contain HTML formatting
    tags that aren’t useful in NLP. The Beautiful Soup library ([https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/))
    can perform the task of removing HTML tags. While Beautiful Soup has many functions
    for working with HTML documents, for our purposes, the most useful function is
    `get_text()`, which extracts the text from an HTML document.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序基于网页，则其将包含在 NLP 中无用的 HTML 格式标记。Beautiful Soup 库（[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)）可以执行删除
    HTML 标签的任务。虽然 Beautiful Soup 有许多用于处理 HTML 文档的功能，但对于我们的目的来说，最有用的功能是 `get_text()`，它从
    HTML 文档中提取文本。
- en: Data imbalance
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据不平衡
- en: Text classification, where the task is to assign each document to one of a set
    of classes, is one of the most common types of NLP applications. In every real-life
    classification dataset, some of the classes will have more examples than others.
    This problem is called **data imbalance**. If the data is severely imbalanced,
    this will cause problems with machine learning algorithms. Two common techniques
    for addressing data imbalance are **oversampling** and **undersampling**. Oversampling
    means that some of the items in the less frequent classes are duplicated, and
    undersampling means that some of the items in the more common classes are removed.
    Both approaches can be used at the same time – frequent classes can be undersampled
    while infrequent classes can be oversampled. We will be discussing this topic
    in detail in [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类的任务是将每个文档分配给一组类别中的一个，这是最常见的 NLP 应用之一。在每个真实的分类数据集中，一些类别将比其他类别拥有更多的示例。这个问题称为**数据不平衡**。如果数据严重不平衡，这将导致机器学习算法出现问题。解决数据不平衡的两种常见技术是**过采样**和**欠采样**。过采样意味着复制较少频繁类别中的一些项，而欠采样意味着删除更常见类别中的一些项。这两种方法可以同时使用
    - 可以对频繁类别进行欠采样，而对不频繁类别进行过采样。我们将在[*第14章*](B19005_14.xhtml#_idTextAnchor248)中详细讨论这个话题。
- en: Using text preprocessing pipelines
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用文本预处理流水线
- en: Getting your data ready for NLP often involves multiple steps, and each step
    takes the output of the previous step and adds new information, or in general,
    gets the data further prepared for NLP. A sequence of preprocessing steps like
    this is called a **pipeline**. For example, an NLP pipeline could include tokenization
    followed by lemmatization and then stopword removal. By adding and removing steps
    in a pipeline, you can easily experiment with different preprocessing steps and
    see whether they make a difference in the results.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为 NLP 准备您的数据通常涉及多个步骤，每个步骤都将前一步骤的输出添加新信息，或者通常是为 NLP 进一步准备数据。这样的预处理步骤序列称为**流水线**。例如，一个
    NLP 流水线可以包括分词，然后是词形还原，最后是停用词移除。通过在流水线中添加和删除步骤，您可以轻松地尝试不同的预处理步骤，并查看它们是否会对结果产生影响。
- en: Pipelines can be used to prepare both training data to be used in learning a
    model, as well as test data to be used at runtime or during testing. In general,
    if a preprocessing step is always going to be needed (for example, tokenization),
    it’s worth considering using it on the training data once and then saving the
    resulting data as a dataset. This will save time if you’re running experiments
    with different configurations of preprocessing steps to find the best configuration.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线可用于准备用于模型学习的训练数据，以及在运行时或测试期间使用的测试数据。通常情况下，如果始终需要预处理步骤（例如分词），则值得考虑在训练数据上执行一次并保存结果数据集。如果您正在使用不同的预处理步骤配置运行实验以找到最佳配置，则这将节省时间。
- en: Choosing among preprocessing techniques
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择预处理技术
- en: '*Table 5.2* is a summary of the preprocessing techniques described in this
    chapter, along with their advantages and disadvantages. It is important for every
    project to consider which techniques will lead to improved results:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.2* 是本章中描述的预处理技术的总结，包括它们的优缺点。每个项目都应该考虑哪些技术能够带来更好的结果：'
- en: '![](img/Table_01.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Table_01.jpg)'
- en: Table 5.2 – Advantages and disadvantages of preprocessing techniques
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.2 – 预处理技术的优缺点
- en: Many techniques, such as spelling correction, have the potential to introduce
    errors because the technology is not perfect. This is particularly true for less
    well-studied languages, for which the relevant algorithms can be less mature than
    those of better-studied languages.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 许多技术（如拼写校正）有可能引入错误，因为技术并不完美。对于一些较少研究的语言，相关算法的成熟度可能不如研究较多的语言。
- en: It is worth starting with an initial test with only the most necessary techniques
    (such as tokenization) and introducing additional techniques only if the results
    of the initial test are not good enough. Sometimes, the errors introduced by preprocessing
    can cause the overall results to get worse. It is important to keep evaluating
    results during the investigation to make sure that results aren’t getting worse.
    Evaluation will be discussed in detail in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，首先进行仅使用最基本技术（如分词）的初步测试，只有在初步测试结果不够好时，才引入额外的技术。有时，预处理引入的错误会导致整体结果变差。在调查过程中，保持持续评估结果非常重要，以确保结果不会变差。评估将在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中详细讨论。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered how to find and use natural language data, including
    finding data for a specific application as well as using generally available corpora.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何找到和使用自然语言数据，包括为特定应用查找数据以及使用通用可用语料库。
- en: We discussed a wide variety of techniques for preparing data for NLP, including
    annotation, which provides the foundation for supervised learning. We also discussed
    common preprocessing steps that remove noise and decrease variation in the data
    and allow machine learning algorithms to focus on the most informative differences
    among different categories of texts. Another important set of topics covered in
    this chapter had to do with privacy and ethics – how to ensure the privacy of
    information included in text data and how to ensure that crowdsourcing workers
    who are generating data or who are annotating data are treated fairly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了各种为自然语言处理准备数据的技术，包括注释，它为监督学习奠定了基础。我们还讨论了常见的预处理步骤，这些步骤去除了数据中的噪声并减少了数据的变化，使机器学习算法能够专注于不同类别文本之间最具信息量的差异。本章还涉及了隐私和伦理相关的议题——如何确保文本数据中包含的信息的隐私性，如何确保生成数据或标注数据的众包工作者得到公平对待。
- en: The next chapter will discuss exploratory techniques for getting an overall
    picture of a dataset, such as summary statistics (word frequencies, category frequencies,
    and so on). It will also discuss visualization tools (such as matplotlib) that
    can provide the kinds of insights that can be best obtained by looking at graphical
    representations of text data. Finally, it will discuss the kinds of decisions
    that can be made based on visualization and statistical results.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论探索性技术，用于全面了解数据集，例如摘要统计（词频、类别频率等）。还将讨论可视化工具（如matplotlib），这些工具可以提供通过查看文本数据的图形表示所能获得的最佳见解。最后，它将讨论基于可视化和统计结果可以做出的决策。
