- en: ­
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: ­
- en: '*Chapter 10:* Deploying a Deep Learning Network'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章：* 部署深度学习网络'
- en: In the previous sections of this book, we covered the training of deep neural
    networks for many different use cases, starting with an autoencoder for fraud
    detection, through **Long Short-Term Memory** (**LSTM**) networks for energy consumption
    prediction and free text generation, all the way to cancer cell classification.
    But training the network is not the only part of a project. Once a deep learning
    network is trained, the next step is to deploy it.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几个部分，我们涵盖了多种不同用例下的深度神经网络训练，从用于欺诈检测的自编码器，到用于能耗预测和自由文本生成的 **长短期记忆**（**LSTM**）网络，再到癌细胞分类。但训练网络并不是项目的唯一部分。一旦深度学习网络训练完成，下一步就是进行部署。
- en: During the exploration of some of the use cases, a second workflow has already
    been introduced, to deploy the network to work on real-world data. So, you have
    already seen some deployment examples. In this last section of the book, however,
    we focus on the many deployment options for machine learning models in general,
    and for trained deep learning networks in particular.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索一些用例时，已经介绍了第二个工作流，用于将网络部署到实际数据中进行工作。因此，你已经看到了一些部署示例。然而，在本书的最后一部分，我们将重点关注机器学习模型的多种部署选项，特别是对于已训练的深度学习网络。
- en: Usually, a second workflow is built and dedicated to deployment. This workflow
    reads the trained model and the new real-world data, it preprocesses this data
    in exactly the same way as for the training data, then it applies the trained
    deep learning network on is transformed data and produces the results according
    to the project's requirements.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，会构建一个专门用于部署的第二个工作流。该工作流读取训练好的模型和新的实际数据，按照与训练数据相同的方式预处理这些数据，然后将训练好的深度学习网络应用于转换后的数据，并根据项目的需求生成结果。
- en: This chapter focuses on the reading, writing, and preprocessing of the data
    in a deployment workflow.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了部署工作流中数据的读取、写入和预处理。
- en: This chapter starts with a review of the features for saving, reading, and converting
    a trained network. This is followed by two examples of how the preprocessing for
    our sentiment analysis use case can also be implemented in a deployment workflow.
    Finally, the chapter shows how to improve execution speed by enabling GPU support.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先回顾了保存、读取和转换训练网络的功能。接下来，介绍了如何在部署工作流中实现情感分析用例的预处理的两个示例。最后，本章展示了如何通过启用 GPU
    支持来提高执行速度。
- en: 'The chapter consists of the following sections:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下部分：
- en: Conversion of the Network Structure
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网络结构的转换
- en: Building a Simple Deployment Workflow
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个简单的部署工作流
- en: Improving Scalability – GPU Execution
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提高可扩展性——GPU 执行
- en: Conversion of the Network Structure
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络结构的转换
- en: The goal of a deployment workflow is to apply a trained network to new real-world
    data. Therefore, the last step of the training workflow must be to save the trained
    network.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 部署工作流的目标是将已训练的网络应用于新的实际数据。因此，训练工作流的最后一步必须是保存训练好的网络。
- en: Saving a Trained Network
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存已训练的网络
- en: All networks described in this book have been trained using the Keras libraries,
    relying on TensorFlow as the backend. So, the most natural way to save a network
    is to continue using the Keras libraries and therefore to use the `.h5` file.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中描述的所有网络都使用 Keras 库进行训练，并依赖 TensorFlow 作为后端。因此，保存网络的最自然方法是继续使用 Keras 库，并使用
    `.h5` 文件。
- en: However, Keras-formatted networks can only be interpreted and executed via the
    Keras libraries. This is already one level on top of the TensorFlow libraries.
    Executing the network application on the TensorFlow Java API directly, rather
    than on a Python kernel via the Keras Python API, makes execution faster. The
    good news is that KNIME Analytics Platform also has nodes for TensorFlow execution
    in addition to the nodes based on Keras libraries.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Keras 格式的网络只能通过 Keras 库进行解释和执行。这已经比 TensorFlow 库多了一层。在 TensorFlow Java API
    上直接执行网络应用，而不是通过 Keras Python API 在 Python 内核上执行，可以提高执行速度。好消息是，KNIME 分析平台除了基于 Keras
    库的节点外，还提供了用于 TensorFlow 执行的节点。
- en: Thus, if faster execution is needed, the Keras network should be converted into
    a TensorFlow network using the `SavedModel` file, a compressed `zip` file. A `SavedModel`
    file contains a complete TensorFlow program, including weights and computation.
    It does not require the original model building code to run, which makes it useful
    for sharing or deploying.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果需要更快的执行，应该将 Keras 网络转换为 TensorFlow 网络，使用 `SavedModel` 文件，即一个压缩的 `zip` 文件。`SavedModel`
    文件包含完整的 TensorFlow 程序，包括权重和计算。它不需要原始的模型构建代码就能运行，这使得它在共享或部署时非常有用。
- en: The first step in a deployment network is to read a trained network.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 部署网络的第一步是读取一个已训练的网络。
- en: Reading a Trained Network
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 读取已训练的网络
- en: 'KNIME Analytics Platform provides many nodes for reading a trained neural network,
    such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME Analytics Platform 提供了许多节点用于读取已训练的神经网络，以下是其中一些：
- en: Keras Network Reader
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 网络读取器
- en: TensorFlow Network Reader (and TensorFlow 2 Network Reader)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 网络读取器（以及 TensorFlow 2 网络读取器）
- en: DL Python Network Creator
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL Python 网络创建器
- en: ONNX Network Reader
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ONNX 网络读取器
- en: The `.h5` file) or just a network architecture definition without weights (a
    `.json` or `.yaml` file). You can use the node to read networks trained with KNIME
    Analytics Platform or networks trained directly with Keras, such as pretrained
    Keras networks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`.h5` 文件）或者仅仅是没有权重的网络架构定义（如 `.json` 或 `.yaml` 文件）。你可以使用该节点读取通过 KNIME Analytics
    Platform 训练的网络，或者直接用 Keras 训练的网络，比如预训练的 Keras 网络。'
- en: The `zip` file. If reading from a directory, it has to be a valid `SavedModel`
    folder. If reading from a `zip` file, it must contain a valid `SavedModel` folder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`zip` 文件。如果是从目录读取，它必须是有效的 `SavedModel` 文件夹。如果是从 `zip` 文件读取，它必须包含一个有效的 `SavedModel`
    文件夹。'
- en: Tip
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The TensorFlow Network Reader node allows us to select a tag and a signature
    in its configuration window. Tags are used to identify the meta graph definition
    to load. Signatures are `SavedModel` can have multiple tags as well as multiple
    signatures per tag. A network saved with KNIME Analytics Platform has only one
    tag and one signature. In the **Advanced** tab of the configuration window, you
    can define your own signature by defining the input and output of the model by
    selecting one of the hidden layers as output, for example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 网络读取器节点允许我们在其配置窗口中选择一个标签和一个签名。标签用于识别要加载的元图定义。`SavedModel` 的签名可以有多个标签，并且每个标签下可以有多个签名。通过
    KNIME Analytics Platform 保存的网络只有一个标签和一个签名。在配置窗口的 **高级** 标签页中，你可以通过选择一个隐藏层作为输出，定义模型的输入和输出，从而定义自己的签名。
- en: Another node, which allows you to read pretrained networks without writing a
    single line of code, is the **ONNX Network Reader** node. **ONNX** stands for
    **Open Neural Network Exchange** and is a standard format for neural networks
    developed by Microsoft and Facebook. Since it is a standard format, it is portable
    across machine learning frameworks such as PyTorch, Caffe2, TensorFlow, and more.
    You can download pretrained networks from the ONNX Model Zoo ([https://github.com/onnx/models#vision](https://github.com/onnx/models#vision))
    and read them with the ONNX Network Reader node. The ONNX networks can also be
    converted into TensorFlow networks using the **ONNX to TensorFlow Network Converter**
    node, and then executed with the TensorFlow Network Executor node.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个节点是 **ONNX 网络读取器** 节点，它允许你在无需写任何代码的情况下读取预训练的网络。**ONNX** 代表 **开放神经网络交换**，是由微软和
    Facebook 开发的神经网络标准格式。由于它是标准格式，ONNX 网络可以跨不同的机器学习框架使用，如 PyTorch、Caffe2、TensorFlow
    等。你可以从 ONNX 模型库（[https://github.com/onnx/models#vision](https://github.com/onnx/models#vision)）下载预训练的网络，并使用
    ONNX 网络读取器节点读取它们。ONNX 网络还可以通过 **ONNX 到 TensorFlow 网络转换器** 节点转换为 TensorFlow 网络，然后使用
    TensorFlow 网络执行器节点进行执行。
- en: Tip
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: To use the ONNX nodes, you need to install the **KNIME Deep Learning – ONNX
    Integration** extension.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 ONNX 节点，你需要安装 **KNIME 深度学习 – ONNX 集成** 扩展。
- en: Another option for reading a network using Python code is the **DL Python Network
    Creator** node, which can be used to read pretrained neural networks using a few
    lines of Python code.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用 Python 代码读取网络的方式是 **DL Python 网络创建器** 节点，它可以通过几行 Python 代码读取预训练的神经网络。
- en: Tip
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The DL Python Network Creator node can also be used in training workflows to
    define the network architecture using Python code instead of layer nodes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DL Python 网络创建器节点也可以在训练工作流中使用，使用 Python 代码定义网络架构，而不是使用层节点。
- en: So far, we have used Keras-based nodes with TensorFlow 1 as the backend. There
    are also nodes that use TensorFlow 2 as the backend to implement similar operations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了基于 Keras 的节点，TensorFlow 1 作为后端。也有使用 TensorFlow 2 作为后端的节点来实现类似的操作。
- en: Using TensorFlow 2
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2
- en: For all the examples in this book, we have used Keras-based nodes that run TensorFlow
    1 as the backend. TensorFlow 2 is also supported since the release of KNIME Analytics
    Platform 4.2\. On the KNIME Hub, you can find many examples of how to use TensorFlow
    2 integration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有示例都使用了基于 Keras 的节点，TensorFlow 1 作为后端。自 KNIME Analytics Platform 4.2 版本发布以来，TensorFlow
    2 也得到了支持。在 KNIME Hub 上，您可以找到许多如何使用 TensorFlow 2 集成的示例。
- en: 'The TensorFlow 2 integration comes with three nodes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 集成提供了三个节点：
- en: '**The TensorFlow 2 Network Executor** node'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 2 网络执行器**节点'
- en: '**The TensorFlow 2 Network Reader** node'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 2 网络读取器**节点'
- en: '**The TensorFlow 2 Network Writer** node'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 2 网络写入器**节点'
- en: To train a deep learning model using TensorFlow 2 you can use the **DL Python
    Network Learner** node.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 TensorFlow 2 训练深度学习模型，可以使用 **DL Python 网络学习器**节点。
- en: Now that we have reviewed the many options to save and read neural networks,
    let's focus on building a simple deployment workflow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了保存和读取神经网络的多种选项，让我们专注于构建一个简单的部署工作流。
- en: Building a Simple Deployment Workflow
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个简单的部署工作流
- en: So far, in all the case studies we have explored, we have always performed some
    kind of preprocessing of the input data, such as encoding categorical features,
    encoding text, or normalizing data, to name just some of the adopted preprocessing
    steps. During deployment, the new incoming data must be prepared with the exact
    same preprocessing as the training data in order to be consistent with the task
    and with the input that the network expects.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们探讨的所有案例研究中，我们总是对输入数据进行某种预处理，例如对分类特征进行编码、对文本进行编码或对数据进行归一化，仅举一些采用的预处理步骤。部署期间，新的输入数据必须经过与训练数据完全相同的预处理，以确保与任务以及网络期望的输入一致。
- en: In this section, we use the sentiment analysis case study shown in [*Chapter
    7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing NLP Applications*,
    as an example, and we build two deployment workflows for it. The goal of both
    workflows is to read new movie reviews from a database, predict the sentiment,
    and write the prediction into the database.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用[*第 7 章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)中的情感分析案例研究，*实施
    NLP 应用程序*，作为示例，构建两个部署工作流。两个工作流的目标是从数据库中读取新的电影评论，预测情感，并将预测写入数据库。
- en: In the first example, the preprocessing steps are implemented manually into
    the deployment workflow. In the second example, the **Integrated Deployment**
    feature is used.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，预处理步骤被手动实现到部署工作流中。在第二个示例中，使用了**集成部署**功能。
- en: Building a Deployment Workflow Manually, without Integrated Deployment
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手动构建部署工作流，无需集成部署
- en: The deployment workflow should access new reviews from a table in a database,
    apply the trained network, write the reviews with the corresponding predictions
    into another table in the database, and delete the reviews from the first table.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 部署工作流应从数据库中的表格获取新的评论，应用训练好的网络，将带有相应预测的评论写入数据库中的另一张表，并删除第一张表中的评论。
- en: 'These steps are performed by the workflow in *Figure 10.1*, which you can download
    from the KNIME Hub at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤由*图 10.1*中的工作流执行，您可以从 KNIME Hub 下载，网址为：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/)：
- en: '![Figure 10.1 – Deployment workflow for the sentiment analysis case study from
    Chapter 7, Implementing NLP Applications](img/B16391_10_001.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 来自第 7 章的情感分析案例研究的部署工作流，实施 NLP 应用程序](img/B16391_10_001.jpg)'
- en: Figure 10.1 – Deployment workflow for the sentiment analysis case study from
    [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), Implementing NLP
    Applications
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 来自[*第 7 章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)的情感分析案例研究的部署工作流，实施
    NLP 应用程序
- en: The workflow first connects to a SQLite database, where the new movie reviews
    are stored, using the **SQLite Connector** node.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流首先使用**SQLite 连接器**节点连接到 SQLite 数据库，数据库中存储了新的电影评论。
- en: Next, the **SELECT** SQL statement to read the new reviews from the table named
    **new_reviews** is implemented by the **DB Table Selector** node.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，**SELECT** SQL 语句通过**DB Table Selector**节点来读取名为**new_reviews**的表中的新评论。
- en: The SQL statement is then executed through the **DB Reader** node. As a result,
    we have the new reviews in a data table at the output port of the node.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 语句通过**DB Reader**节点执行。因此，我们在节点的输出端口得到了包含新评论的数据表。
- en: Tip
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In [*Chapter 2*](B16391_02_Final_SK_ePUB.xhtml#_idTextAnchor051), *Data Access
    and Preprocessing with KNIME Analytics Platform*, the database extension was introduced
    in detail. Remember that the database nodes create a SQL statement at their output
    brown-squared port.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 2 章*](B16391_02_Final_SK_ePUB.xhtml#_idTextAnchor051)中，*使用 KNIME 分析平台进行数据访问和预处理*详细介绍了数据库扩展。请记住，数据库节点在其输出的棕色方形端口上创建
    SQL 语句。
- en: Before applying the network to these new reviews, we need to perform the same
    transformations as in the training workflow. In the training workflow, reported
    in [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230), *Implementing
    NLP Applications*, there was a metanode named **Preprocess test set** where all
    the required preprocessing steps were applied to the test data. We used this metanode
    as the basis for creating the preprocessing steps for the incoming data in the
    deployment workflow.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在将网络应用于这些新评论之前，我们需要执行与训练工作流中相同的转换。在[*第 7 章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)中报告的训练工作流中，存在一个名为**Preprocess
    test set**的元节点，所有必要的预处理步骤都在其中应用于测试数据。我们使用这个元节点作为创建部署工作流中传入数据预处理步骤的基础。
- en: '*Figure 10.2* shows the content of this metanode, which is dedicated to the
    preprocessing of the test set:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.2*显示了该元节点的内容，它专门用于测试集的预处理：'
- en: '![Figure 10.2 – Preprocessing of the test data in the training workflow of
    the sentiment analysis case study from Chapter 7, Implementing NLP Applications](img/B16391_10_002.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – 第 7 章情感分析案例研究中训练工作流的测试数据预处理](img/B16391_10_002.jpg)'
- en: Figure 10.2 – Preprocessing of the test data in the training workflow of the
    sentiment analysis case study from [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    Implementing NLP Applications
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 第 7 章情感分析案例研究中训练工作流的测试数据预处理
- en: In the deployment workflow in *Figure 10.1*, the dictionary, created during
    training is read first; then the preprocessing steps are implemented in the **Preprocessing**
    metanode.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 10.1*的部署工作流中，首先读取训练过程中创建的字典；然后在**Preprocessing**元节点中执行预处理步骤。
- en: '*Figure 10.3* shows you the workflow snippet inside this metanode:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.3*向您展示了此元节点内的工作流片段：'
- en: '![Figure 10.3 – Workflow snippet inside the Preprocessing metanode of the deployment
    workflow](img/B16391_10_003.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 部署工作流中预处理元节点内的工作流片段](img/B16391_10_003.jpg)'
- en: Figure 10.3 – Workflow snippet inside the Preprocessing metanode of the deployment
    workflow
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 部署工作流中预处理元节点内的工作流片段
- en: If we compare the workflow snippets in *Figure 10.2* and *Figure 10.3*, you
    can see that they contain the same preprocessing steps, as was expected.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对比*图 10.2*和*图 10.3*中的工作流片段，可以看到它们包含了相同的预处理步骤，正如预期的那样。
- en: Now that the same preprocessing as for the training data has been applied to
    the deployment data, the trained network can be introduced through the **Keras
    Network Reader** node (*Figure 10.1*).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，已经对部署数据应用了与训练数据相同的预处理，训练好的网络可以通过**Keras Network Reader**节点引入（*图 10.1*）。
- en: 'Next, the trained network runs on the preprocessed deployment reviews using
    the **Keras Network Executor** node. The output of the network is the probability
    of the sentiment being equal to 1, where 1 encodes a positive movie review. The
    same threshold as during training is also applied here through the **Rule Engine**
    node: a threshold of ![](img/Formula_B16391_10_001.png).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，训练好的网络通过**Keras Network Executor**节点在预处理过的部署评论上运行。网络的输出是情感为1的概率，其中1表示正面的电影评论。与训练时相同的阈值也通过**Rule
    Engine**节点应用在此：阈值为 ![](img/Formula_B16391_10_001.png)。
- en: In the last step, the tables in the database are updated. First, the **DB Delete**
    node deletes the reviews we just analyzed from the **new_reviews** table. Then,
    the **DB Writer** node appends the new movie reviews with their predictions to
    another table in the database, named **review-with-sentiment**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，数据库中的表格被更新。首先，**DB Delete** 节点从 **new_reviews** 表中删除我们刚刚分析的评论。然后，**DB
    Writer** 节点将带有预测的新电影评论附加到数据库中的另一个表格，名为 **review-with-sentiment**。
- en: This is the first example of the deployment of a neural network using KNIME
    Analytics Platform. This workflow should be executed on a regular basis to predict
    the sentiment for all new incoming movie reviews.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 KNIME Analytics Platform 部署神经网络的第一个示例。这个工作流应该定期执行，以预测所有新进电影评论的情感。
- en: Tip
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: KNIME Server can schedule the execution of workflows, so you can trigger their
    execution automatically on a regular schedule.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME Server 可以调度工作流的执行，因此你可以定期自动触发它们的执行。
- en: This approach has one disadvantage. If the model is retrained on more data or
    with different settings (for example, if more or fewer terms are taken into account
    during training or the threshold for the Rule Engine node is changed) we need
    to remember to also update the preprocessing steps in the deployment workflow.
    And since we are forgetful humans, we might forget or make mistakes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一个缺点。如果模型在更多数据上重新训练或使用不同的设置（例如，在训练过程中考虑的术语更多或更少，或规则引擎节点的阈值发生变化），我们需要记得更新部署工作流中的预处理步骤。由于我们是健忘的人类，可能会忘记或犯错。
- en: A solution to overcome this issue is the concept of **Integrated Deployment**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这个问题的解决方案是 **集成部署** 的概念。
- en: Building a Deployment Workflow Automatically with Integrated Deployment
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用集成部署自动构建部署工作流
- en: Until KNIME Analytics Platform 4.2, as well as in other tools, a common approach
    was to implement data blending, data transformation, and network execution manually
    in the deployment workflow. This means that you need to copy the different preprocessing
    snippets, parameters, and network executor nodes from the training workflow to
    the deployment workflow, making sure that all settings remain unaltered.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 直到 KNIME Analytics Platform 4.2 版本，以及其他工具中，常见的做法是在部署工作流中手动实现数据混合、数据转换和网络执行。这意味着你需要从训练工作流中复制不同的预处理代码段、参数和网络执行节点到部署工作流中，确保所有设置保持不变。
- en: This manual step slows down the process and can easily lead to mistakes. Automating
    the construction of parts of the deployment workflow can be a safer option, especially
    if the models are changed often, for example, every day or even every hour.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一手动步骤会减慢过程，并且很容易导致错误。自动构建部署工作流的部分内容可以是一个更安全的选择，尤其是在模型经常改变的情况下，例如每天甚至每小时都可能变化。
- en: Important note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Other common names for the training process are data science creation or modeling
    workflow.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的其他常见名称是数据科学创建或建模工作流。
- en: The nodes from the Integrated Deployment extension close the gap between creating
    and deploying data science.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 来自集成部署扩展的节点弥合了创建和部署数据科学之间的差距。
- en: The Integrated Deployment Extension
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成部署扩展
- en: The Integrated Deployment extension allows data scientists to combine the model
    training and deployment into one single workflow. The idea is to capture parts
    of the training workflow and to automatically write them into the deployment workflow
    during the execution of the training workflow.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 集成部署扩展允许数据科学家将模型训练和部署合并成一个单一的工作流。其思想是在执行训练工作流的过程中，捕捉训练工作流的部分内容，并自动将它们写入部署工作流中。
- en: Instead of copying the preprocessing parts manually, one by one, the required
    parts from the training workflow are captured in between the **Capture Workflow
    Start** and **Capture Workflow End** nodes. The captured workflow part in the
    middle can then be written into a new workflow with a **Workflow Writer** node.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 不必逐个手动复制预处理部分，训练工作流中所需的部分会在 **Capture Workflow Start** 和 **Capture Workflow
    End** 节点之间捕捉。然后，通过 **Workflow Writer** 节点，捕捉到的工作流部分可以写入新的工作流。
- en: Using the Integrated Deployment Extension in the Training Workflow
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在训练工作流中使用集成部署扩展
- en: Let's consider again the deployment workflow for the sentiment analysis case
    study described in [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*. In the training workflow, we have introduced
    the **Capture Workflow Start** node and the **Capture Workflow End** node to isolate
    the workflow snippet that we want to reproduce exactly in the deployment workflow.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次考虑情感分析案例的部署工作流，该工作流在 [*第 7 章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)《实施
    NLP 应用程序》中进行了描述。在训练工作流中，我们引入了 **Capture Workflow Start** 节点和 **Capture Workflow
    End** 节点，以便精确地隔离我们希望在部署工作流中重现的工作流片段。
- en: 'This includes the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括以下内容：
- en: The metanode named **Preprocessing test set**, including all required preprocessing
    steps
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名为 **Preprocessing test set** 的元节点，包括所有所需的预处理步骤
- en: The **Keras Network Executor** node to apply the trained network on the deployment
    transformed data
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras 网络执行器** 节点，用于在部署后的数据上应用训练好的网络'
- en: The **Rule Engine** node, which decides on the positive or the negative class
    based on a threshold applied to the output class' probability
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rule Engine** 节点，根据应用于输出类别概率的阈值决定正类或负类'
- en: 'The workflow in *Figure 10.4* shows you this example based on the sentiment
    analysis case study. You can download the workflow from the KNIME Hub at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 *10.4* 展示了基于情感分析案例的工作流示例。你可以从 KNIME Hub 下载该工作流：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/)：
- en: '![Figure 10.4 – Training workflow that automatically creates a deployment workflow
    using Integrated Deployment](img/B16391_10_004.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 使用集成部署自动创建部署工作流的训练工作流](img/B16391_10_004.jpg)'
- en: Figure 10.4 – Training workflow that automatically creates a deployment workflow
    using Integrated Deployment
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 使用集成部署自动创建部署工作流的训练工作流
- en: The part in the thick box is the captured workflow snippet. The **Capture Workflow
    Start** node defines the beginning and the **Capture Workflow End** node defines
    the end of the workflow snippet to capture.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 粗体框中的部分是捕获的工作流片段。**Capture Workflow Start** 节点定义了工作流片段的开始，**Capture Workflow
    End** 节点定义了工作流片段的结束。
- en: 'The start node doesn''t need any configuration. *Figure 10.5* shows the configuration
    window of the **Capture Workflow End** node:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 起始节点不需要任何配置。图 *10.5* 显示了 **Capture Workflow End** 节点的配置窗口：
- en: '![Figure 10.5 – Configuration window of the Capture Workflow End node](img/B16391_10_005.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – **Capture Workflow End** 节点的配置窗口](img/B16391_10_005.jpg)'
- en: Figure 10.5 – Configuration window of the Capture Workflow End node
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – **Capture Workflow End** 节点的配置窗口
- en: In the configuration window, you can set the name of the captured workflow snippet.
    You can also set whether the captured snippet should be stored with the data and,
    if yes, the maximum number of data rows to include. We will see in a second why
    it can be helpful to store some data in the captured workflow snippet.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口中，你可以设置捕获的工作流片段的名称。你还可以设置是否将捕获的片段与数据一起存储，如果是，设置应包含的最大数据行数。稍后我们将看到，存储一些数据在捕获的工作流片段中是如何有帮助的。
- en: The captured workflow snippet, with or without data, is then exported via the
    output port (the black square) of the **Capture Workflow End** node. In the workflow
    in *Figure 10.4*, the workflow snippet is then collected by the **Workflow Writer**
    node and written into the deployment workflow, with unaltered settings and configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获的工作流片段，是否包含数据，将通过 **Capture Workflow End** 节点的输出端口（黑色方块）导出。在图 *10.4* 中，工作流片段随后被
    **Workflow Writer** 节点收集并写入部署工作流，设置和配置保持不变。
- en: '*Figure 10.6* shows the configuration window of the **Workflow Writer** node:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.6* 显示了 **Workflow Writer** 节点的配置窗口：'
- en: '![Figure 10.6 – The Workflow Writer node and its configuration window](img/B16391_10_006.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – Workflow Writer 节点及其配置窗口](img/B16391_10_006.jpg)'
- en: Figure 10.6 – The Workflow Writer node and its configuration window
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – Workflow Writer 节点及其配置窗口
- en: At the top, you can set the location of the folder of the destination workflow
    (**Output location**).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，你可以设置目标工作流的文件夹位置（**输出位置**）。
- en: Next, you need to set the name of the destination workflow. The node automatically
    proposes a default name, which you can customize via the **Use custom workflow
    name** option. If the name you choose refers to a workflow that already exists,
    you can let the writer node fail or overwrite.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要设置目标工作流的名称。该节点会自动提供默认名称，您可以通过**使用自定义工作流名称**选项进行自定义。如果您选择的名称已经指向一个现有的工作流，您可以选择让写入节点失败或者覆盖。
- en: 'At the bottom, you can select the deployment option for the destination workflow:
    just create it, create it and open it, or save it as a `.knwf` file to export.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部，您可以选择目标工作流的部署选项：仅创建、创建并打开，或保存为`.knwf`文件进行导出。
- en: 'The next figure, *Figure 10.7*, shows you the automatically generated deployment
    workflow by the **Workflow Writer** node:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下图，*图 10.7*，展示了由**工作流写入器**节点自动生成的部署工作流：
- en: '![Figure 10.7 – Automatically created deployment workflow from the workflow
    snippet captured via Integrated Deployment](img/B16391_10_007.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 通过集成部署捕获的工作流片段自动生成的部署工作流](img/B16391_10_007.jpg)'
- en: Figure 10.7 – Automatically created deployment workflow from the workflow snippet
    captured via Integrated Deployment
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 通过集成部署捕获的工作流片段自动生成的部署工作流
- en: 'In the captured workflow you can see the **Preprocessing test set** metanode,
    as well as the **Keras Network Executor**, **Rule Engine**, and **Column Filter**
    nodes. Additionally, the whole Integrated Deployment process has added the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕获的工作流中，您可以看到**预处理测试集**元节点，以及**Keras 网络执行器**、**规则引擎**和**列过滤器**节点。此外，整个集成部署过程还添加了以下内容：
- en: Two **Reference Reader** nodes. They are generic reader nodes, loading the connection
    information of static parameters not found in the captured workflow snippet.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个**引用读取器**节点。它们是通用的读取器节点，加载在捕获的工作流片段中未找到的静态参数的连接信息。
- en: A **Container Input (Table)** and a **Container Output (Table)** node in order
    to accept input data and to send output data respectively from and to other applications.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器输入（表格）**和**容器输出（表格）**节点用于接受输入数据和分别将输出数据发送至其他应用程序。'
- en: The execution of this deployment workflow can be triggered either by another
    workflow using the **Call Workflow (Table)** node or via a REST service if the
    workflow has been deployed on a KNIMEs Server. In the next chapter, we will talk
    about the REST calls and REST services in detail.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过另一个工作流使用**调用工作流（基于表格）**节点或通过 REST 服务触发此部署工作流，如果工作流已经部署在 KNIME 服务器上。在下一章中，我们将详细讨论
    REST 调用和 REST 服务。
- en: In *Figure 10.7*, the example deployment workflow reads two entities at the
    top of the workflow using the two reader nodes without an icon inside them. The
    left one provides the dictionary table based on the training data, and the right
    one provides the trained neural network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 10.7*中，示例部署工作流在工作流顶部使用两个读取器节点读取两个实体，这两个节点内部没有图标。左侧节点基于训练数据提供字典表，右侧节点提供训练好的神经网络。
- en: In addition, you can see two more new nodes, which are the **Container Input
    (Table)** and **Container Output (Table)** nodes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以看到另外两个新节点，即**容器输入（表格）**和**容器输出（表格）**节点。
- en: The **Container Input (Table)** node receives a data table from an external
    caller (that is, the **Call Workflow (Table Based)** node) and makes it available
    on the output port. A configuration parameter enables the external caller to send
    a data table to the **Container Input (Table)** node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器输入（表格）**节点从外部调用者（即**调用工作流（基于表格）**节点）接收数据表，并在输出端口上提供。配置参数使外部调用者能够向**容器输入（表格）**节点发送数据表。'
- en: The **Container Input (Table)** node also has an optional input port (represented
    by an unfilled input port). If a data table is connected to the optional input,
    the node will simply forward this table to the next node; if a table is supplied
    via a REST API, then the supplied table will be available on the output port.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器输入（表格）**节点还有一个可选的输入端口（用未填充的输入端口表示）。如果连接了数据表到可选输入端口，节点将简单地将此表转发到下一个节点；如果通过
    REST API 提供了表格，则提供的表格将在输出端口上可用。'
- en: If no input is given, a default template table will be provided on the output
    of the node. Here, the **Store input tables** setting from the **Capture Workflow
    End** node comes in. If you select to store some data rows, they are used to define
    this default template table.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有输入，则节点输出默认模板表。这里涉及到来自**捕获工作流结束**节点的**存储输入表格**设置。如果选择存储某些数据行，则它们用于定义此默认模板表。
- en: The **Container Output (Table)** node sends a KNIME data table to an external
    caller.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器输出（表格）**节点将KNIME数据表发送到外部调用者。'
- en: Let's now find out how the automatically created workflow can be used to predict
    the sentiment of new reviews during deployment.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解一下如何在部署过程中使用自动创建的工作流来预测新评论的情感。
- en: Using the Automatically Created Workflow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用自动创建的工作流
- en: Let's have a look now at how the deployment workflow can be consumed.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何使用部署工作流。
- en: '*Figure 10.8* shows you an example of how the automatically created deployment
    workflow can be consumed to classify the sentiment of new movie reviews, and you
    can download it from the KNIME Hub to try it out, at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/)
    :'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.8*展示了如何使用自动创建的部署工作流来分类新电影评论的情感，您可以从KNIME Hub下载该工作流进行试用，网址：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter_10/)：'
- en: ': Figure 10.8 – Workflow calling the automatically created deployment workflow'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ': 图 10.8 – 调用自动创建的部署工作流的工作流'
- en: '![Figure 10.8 – Workflow calling the automatically created deployment workflow](img/B16391_10_008.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 调用自动创建的部署工作流的工作流](img/B16391_10_008.jpg)'
- en: The workflow connects to the database and reads the incoming new movie reviews.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作流连接到数据库并读取新到的电影评论。
- en: 'Then, the **Call Workflow (Table Based)** node calls the deployment workflow
    (*Figure 10.7*), the one that was automatically built. The **Call Workflow (Table
    Based)** node indeed calls other workflows residing on your local workspace or
    on a mounted KNIME server. The called workflow must contain at least one Container
    Input node and one Container Output node to define the interface between the two
    workflows: the called and the caller workflows.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**调用工作流（基于表格）**节点调用部署工作流（*图 10.7*），即自动构建的那个。**调用工作流（基于表格）**节点实际上会调用存储在本地工作区或挂载的KNIME服务器上的其他工作流。被调用的工作流必须至少包含一个容器输入节点和一个容器输出节点，以定义两个工作流之间的接口：被调用工作流和调用工作流。
- en: Via the **Call Workflow (Table Based)** node, we send the new movie reviews
    to the deployment workflow to feed the **Container Input (Table)** node. The deployment
    workflow is then executed, and the predictions are sent back to the caller workflow
    and made available via the output port of the **Call Workflow (Table Based)**
    node.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**调用工作流（基于表格）**节点，我们将新的电影评论发送到部署工作流，以供**容器输入（表格）**节点使用。然后，部署工作流被执行，预测结果被发送回调用工作流，并通过**调用工作流（基于表格）**节点的输出端口提供。
- en: A great advantage of this strategy is the ensured consistency between the data
    operations in the training workflow and the data operations in the deployment
    workflow. If we now change any settings in the data operations in the training
    workflow, for example, the value of the threshold in the **Rule Engine** node
    (*Figure 10.4*), and we re-execute the training workflow, these changes are automatically
    imported into the new version of the deployment workflow (*Figure 10.7*) and used
    by any workflow relying on it (*Figure 10.8*).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的一个大优点是确保了训练工作流和部署工作流中的数据操作一致性。如果我们现在更改训练工作流中的数据操作设置，例如更改**规则引擎**节点中的阈值（*图
    10.4*），并重新执行训练工作流，这些更改将自动导入到部署工作流的新版本中（*图 10.7*），并被任何依赖于该工作流的工作流使用（*图 10.8*）。
- en: Tip
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Another great node of the **Integrated Deployment** extension is the **Workflow
    Combiner** node, which allows us to combine workflow snippets from different original
    workflows.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成部署**扩展的另一个优秀节点是**工作流合并器**节点，它允许我们将不同原始工作流中的工作流片段合并在一起。'
- en: We have reached the last section of this chapter, which is on scalability and
    GPU execution.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已进入本章的最后一部分，内容涉及可扩展性和GPU执行。
- en: Improving Scalability – GPU Execution
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高可扩展性 – GPU 执行
- en: For the case studies described in this book, we have used relatively small datasets
    and small networks. This allowed us to train the networks within hours using only
    CPU-based execution. However, training tasks that take minutes or hours on small
    datasets can easily take days or weeks on larger datasets; small network architectures
    can quickly increase in size and execution times can quickly become prohibitive.
    In general, when working with deep neural networks, the training phase is the
    most resource-intensive task.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中描述的案例研究中，我们使用了相对较小的数据集和小型网络。这使我们能够仅使用基于 CPU 的执行在几个小时内训练网络。然而，原本在小数据集上需要几分钟或几个小时的训练任务，在大数据集上可能需要几天或几周；小型网络架构很快就会增大，执行时间也会迅速变得不可承受。通常，在处理深度神经网络时，训练阶段是最为消耗资源的任务。
- en: GPUs have been designed to handle multiple computations simultaneously. This
    paradigm suits the intensive computations required to train a deep learning network.
    Hence, GPUs are an alternative option to train large deep learning networks efficiently
    and effectively on large datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 设计用于同时处理多个计算。这个范式非常适合训练深度学习网络所需的密集计算。因此，GPU 是在大数据集上高效有效地训练大规模深度学习网络的替代方案。
- en: Some Keras libraries can exploit the computational power of NVIDIA®-compatible
    GPUs via the TensorFlow paradigms. As a consequence, **KNIME Keras integration**
    can also exploit the computational power of GPUs to train deep learning networks
    more quickly.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 Keras 库可以通过 TensorFlow 范式利用 NVIDIA® 兼容 GPU 的计算能力。因此，**KNIME Keras 集成** 也可以利用
    GPU 的计算能力来更快速地训练深度学习网络。
- en: In [*Chapter 1*](B16391_01_Final_NM_ePUB.xhtml#_idTextAnchor016), *Introduction
    to Deep Learning with KNIME Analytics Platform*, we introduced how to set up Python
    for KNIME Keras integration and KNIME TensorFlow integration. In order to run
    the KNIME Keras integration on the GPU rather than on the CPU, you do not need
    to take many extra steps.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第一章*](B16391_01_Final_NM_ePUB.xhtml#_idTextAnchor016)，*使用 KNIME 分析平台介绍深度学习*
    中，我们介绍了如何为 KNIME Keras 集成和 KNIME TensorFlow 集成设置 Python。为了在 GPU 上运行 KNIME Keras
    集成而不是在 CPU 上，您无需额外采取很多步骤。
- en: Of course, you need a GPU-enabled computer. TensorFlow 1.12 requires an NVIDIA
    GPU card with a CUDA compute capability of 3.5 or higher.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您需要一台支持 GPU 的计算机。TensorFlow 1.12 需要一张具有 CUDA 计算能力 3.5 或更高版本的 NVIDIA GPU 显卡。
- en: Besides that, most of the required dependencies (that is, CUDA® and cuDNN) will
    be automatically installed by Anaconda when installing the conda `tensorflow=1.12`
    and `keras-gpu=2.2.4`. packages
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数所需的依赖项（即 CUDA® 和 cuDNN）将在安装 conda `tensorflow=1.12` 和 `keras-gpu=2.2.4`
    软件包时由 Anaconda 自动安装。
- en: The only extra step at installation is the latest version of the NVIDIA® GPU
    driver, to be installed manually.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程中唯一的额外步骤是手动安装最新版本的 NVIDIA® GPU 驱动程序。
- en: At installation time, by selecting `keras-gpu=2.2.4` is created.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装时，通过选择 `keras-gpu=2.2.4` 创建。
- en: When using the TensorFlow integration, it is also possible to execute on the
    GPU to read and execute TensorFlow's SavedModel.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 TensorFlow 集成时，您还可以通过 GPU 执行以读取和执行 TensorFlow 的 SavedModel。
- en: Important note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The GPU support for the **KNIME TensorFlow integration** (which uses the TensorFlow
    Java API) is generally independent of the GPU support for the **KNIME Keras integration**
    (which uses Python). Hence, the two GPU supports must be set up individually.
    Due to the limitations of TensorFlow, the GPU support for the KNIME TensorFlow
    integration can only run on Windows and Linux, and not on Mac.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**KNIME TensorFlow 集成**（使用 TensorFlow Java API）的 GPU 支持通常与 **KNIME Keras 集成**（使用
    Python）的 GPU 支持是独立的。因此，两个 GPU 支持必须单独设置。由于 TensorFlow 的限制，KNIME TensorFlow 集成的
    GPU 支持只能在 Windows 和 Linux 上运行，不能在 Mac 上运行。'
- en: At the time of writing, the following GPU configuration is recommended by KNIME.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，KNIME 推荐以下 GPU 配置。
- en: 'The KNIME TensorFlow integration uses TensorFlow version 1.13.1, which requires
    the following NVIDIA® software to be installed on your system:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME TensorFlow 集成使用 TensorFlow 版本 1.13.1，这要求系统上安装以下 NVIDIA® 软件：
- en: 'NVIDIA® GPU drivers: CUDA® 10.0 requires 410.x or higher.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA® GPU 驱动程序：CUDA® 10.0 需要 410.x 或更高版本。
- en: 'CUDA® Toolkit: TensorFlow (≥ 1.13.0) supports CUDA® 10.0\.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CUDA® 工具包：TensorFlow（≥ 1.13.0）支持 CUDA® 10.0。
- en: 'cuDNN (version ≥ 7.4.1): Select cuDNN v7.6.0 (May 20, 2019) for CUDA® 10.0.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuDNN（版本 ≥ 7.4.1）：选择 cuDNN v7.6.0（2019 年 5 月 20 日）用于 CUDA® 10.0。
- en: For detailed instructions and the most recent updates, please check the KNIME
    documentation ([https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration](https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有关详细的说明和最新的更新，请查看 KNIME 文档 ([https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration](https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#tensorflow-integration))。
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have covered three different topics. We started with a summary
    of the many options for reading, converting, and writing neural networks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们涵盖了三个不同的主题。我们从总结阅读、转换和写入神经网络的多种选项开始。
- en: 'We then moved on to the deployment of neural networks, using the sentiment
    analysis case study from [*Chapter 7*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *Implementing NLP Applications*, as an example. The goal here was to build a workflow
    that uses the trained neural network to predict the sentiment of new reviews stored
    in the database. We have shown that a deployment workflow can be assembled in
    two ways: manually or automatically with Integrated Deployment.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续讨论神经网络的部署，使用来自 [*第7章*](B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230)的情感分析案例研究作为示例，*实现
    NLP 应用程序*。这里的目标是构建一个工作流，使用训练好的神经网络预测存储在数据库中的新评论的情感。我们已经展示了可以通过两种方式组装部署工作流：手动或通过集成部署自动完成。
- en: The last section of the chapter dealt with the scalability of network training
    and execution. In particular, it showed how to exploit the computational power
    of GPUs when training a neural network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后部分讨论了网络训练和执行的可扩展性，特别是展示了如何在训练神经网络时利用 GPU 的计算能力。
- en: In the next and last chapter of this book, we will explore further deployment
    options and best practices when working with deep learning.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章也是最后一章，我们将进一步探索部署选项以及在深度学习中工作的最佳实践。
- en: Questions and Exercises
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题和练习
- en: Which network conversions are available in KNIME Analytics Platform?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KNIME 分析平台支持哪些网络转换？
- en: a) Keras to TensorFlow network conversion
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) Keras 到 TensorFlow 网络转换
- en: b) TensorFlow to Keras network conversion
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) TensorFlow 到 Keras 网络转换
- en: c) ONNX to Keras network conversion
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) ONNX 到 Keras 网络转换
- en: d) Keras to ONNX network conversion
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) Keras 到 ONNX 网络转换
- en: Which statements regarding Integrated Deployment are true (two statements are
    correct)?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于集成部署，哪些陈述是正确的（有两个陈述是正确的）？
- en: a) Integrated Deployment allows us to retrain a model during execution.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 集成部署允许我们在执行期间重新训练模型。
- en: b) The execution of the automatically generated workflow can be triggered by
    another workflow.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 自动生成的工作流的执行可以由另一个工作流触发。
- en: c) The execution of the training workflow is triggered by the deployment workflow.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 训练工作流的执行由部署工作流触发。
- en: d) Integrated Deployment closes the gap between training and deployment.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 集成部署缩小了训练和部署之间的差距。
