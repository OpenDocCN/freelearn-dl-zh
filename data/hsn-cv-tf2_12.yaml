- en: Optimizing Models and Deploying on Mobile Devices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化模型并部署到移动设备
- en: Computer vision applications are various and multifaceted. While most of the
    training steps take place on a server or a computer, deep learning models are
    used on a variety of frontend devices, such as mobile phones, self-driving cars,
    and **Internet-of-Things** (**IoT**) devices. With limited computing power, performance
    optimization becomes paramount.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉应用种类繁多、涉及面广。虽然大多数训练步骤发生在服务器或计算机上，但深度学习模型也广泛应用于各种前端设备，如手机、自动驾驶汽车和**物联网**（**IoT**）设备。在有限的计算能力下，性能优化变得尤为重要。
- en: In this chapter, we will introduce techniques to limit your model size and improve
    inference speed while maintaining good prediction quality. As a practical example,
    we will create a simple mobile application to recognize facial expressions on
    iOS and Android devices, as well as in the browser.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一些技术，帮助你限制模型大小并提高推理速度，同时保持良好的预测质量。作为一个实际示例，我们将创建一个简单的移动应用，识别 iOS 和
    Android 设备上的面部表情，以及在浏览器中运行。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: How to reduce model size and boost speed without impacting accuracy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何减少模型大小并提升速度，同时保持准确性
- en: Analyzing model computational performance in depth
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入分析模型计算性能
- en: Running models on mobile phones (iOS and Android)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在移动设备（iOS 和 Android）上运行模型
- en: Introducing TensorFlow.js to run models in the browser
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 TensorFlow.js 在浏览器中运行模型
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is available from [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以从 [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter09)
    获取。
- en: When developing applications for mobile phones, you will need knowledge of **Swift**
    (for iOS) or **Java** (for Android). For computer vision in the browser, you will
    require knowledge of **JavaScript**. The examples in this chapter are simple and
    thoroughly explained, making it easy to understand for developers who are more
    familiar with Python.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在为移动设备开发应用时，你需要掌握 **Swift**（用于 iOS）或 **Java**（用于 Android）。如果在浏览器中进行计算机视觉开发，你需要了解
    **JavaScript**。本章中的示例简单且有详细解释，即便你更熟悉 Python，理解起来也非常容易。
- en: Moreover, to run the example iOS app, you will need a compatible device as well
    as a Mac computer with Xcode installed. To run the Android app, you will need
    an Android device.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，要运行示例 iOS 应用，你需要一台兼容的设备以及安装了 Xcode 的 Mac 电脑。要运行 Android 应用，你需要一台 Android
    设备。
- en: Optimizing computational and disk footprints
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化计算和磁盘占用
- en: When using a computer vision model, some characteristics are crucial. Optimizing
    a model for *speed* may allow it to run in real time, opening up many new uses.
    Improving a model's *accuracy* by even a few percent may make the difference between
    a toy model and a real-life application.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算机视觉模型时，有些特性至关重要。优化模型的*速度*可能使它实现实时运行，开启许多新的应用场景。即使是提高模型的*准确性*几个百分点，也可能决定一个模型是玩具级别还是能够实际应用的。
- en: Another important characteristic is *size*, which impacts how much storage the
    model will use and how long it will take to download it. For some platforms, such
    as mobile phones or web browsers, the size of the model matters to the end user.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要特征是*大小*，它影响模型所需的存储空间以及下载时间。对于一些平台，如手机或网页浏览器，模型的大小对最终用户来说至关重要。
- en: In this section, we will describe techniques to improve the model inference
    speed and how to reduce its size.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍提升模型推理速度的技术，并讨论如何减少模型的大小。
- en: Measuring inference speed
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量推理速度
- en: '**Inference** describes the process of using a deep learning model to get predictions.
    It is measured in images per second or seconds per image. Models must run between
    5 and 30 images per second to be considered real-time processing. Before we can
    improve inference speed, we need to measure it properly.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理**描述了使用深度学习模型进行预测的过程。推理速度通常以每秒图像数或每张图像的秒数来衡量。模型必须在每秒处理 5 到 30 张图像之间，才能算作实时处理。在提高推理速度之前，我们需要正确地测量它。'
- en: If a model can process *i* images per second, we can always run *N* inference
    pipelines simultaneously to boost performance—the model will then be able to process
    *N* × *i* images per second. While parallelism benefits many applications, it
    would not work for real-time applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型每秒能处理*i*张图像，我们可以同时运行*N*个推理管道来提高性能——这样，模型将能够每秒处理*N* × *i*张图像。虽然并行化对许多应用有益，但它并不适用于实时应用。
- en: In a real-time context, such as with a self-driving car, no matter how many
    images can be processed in parallel, what matters is **latency**—how long it takes
    to compute predictions for a single image. Therefore, for real-time applications,
    we only measure the latency of a model—how much time it takes to *process a single
    image*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时场景中，例如自动驾驶汽车，无论能并行处理多少图像，真正重要的是**延迟**——为单张图像计算预测所需的时间。因此，对于实时应用，我们只测量模型的延迟——处理*单张图像*所需的时间。
- en: For non-real-time applications, you can run as many inference processes in parallel
    as necessary. For instance, for a video, you can analyze *N* chunks of video in
    parallel and concatenate the predictions at the end of the process. The only impact
    will be in terms of financial cost, as you will need more hardware to process
    the frames in parallel.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非实时应用，您可以并行运行任意多的推理进程。例如，对于一个视频，您可以并行分析*N*段视频，并在过程结束时将预测结果连接起来。唯一的影响是财务成本，因为您需要更多的硬件来并行处理这些帧。
- en: Measuring latency
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量延迟
- en: As stated, to measure how fast a model performs, we want to compute the time
    it takes to process a *single image*. However, to minimize measuring error, we
    will actually measure the processing time for several images. We will then divide
    the time obtained by the number of images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，要衡量模型的性能，我们希望计算处理*单张图像*所需的时间。然而，为了最小化测量误差，我们实际上会测量多个图像的处理时间。然后，我们将获得的时间除以图像的数量。
- en: We are not measuring the computing time over a single image for several reasons.
    First, we want to remove measurement error. When running the inference for the
    first time, the machine could be busy, the GPU might not be initialized yet, or
    many other technicalities could be causing the slowdown. Running several times
    allows us to minimize this error.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不测量单张图像的计算时间，原因有几个。首先，我们希望消除测量误差。第一次运行推理时，机器可能正在忙碌，GPU可能尚未初始化，或者其他许多技术因素可能导致性能下降。多次运行可以帮助我们减少这种误差。
- en: The second reason is TensorFlow's and CUDA's warmup. When running an operation
    for the first time, deep learning frameworks are usually slower—they have to initialize
    variables, allocate memory, move data around, and so on. Moreover, when running
    repetitive operations, they usually automatically optimize for it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是TensorFlow和CUDA的预热。当第一次运行某个操作时，深度学习框架通常会比较慢——它们需要初始化变量、分配内存、移动数据等。此外，在执行重复操作时，它们通常会自动优化。
- en: For all those reasons, it is recommended to measure inference time with multiple
    images to simulate a real environment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于以上原因，建议使用多个图像来测量推理时间，以模拟实际环境。
- en: When measuring inference time, it is also very important to include data loading,
    data preprocessing, and post-processing times, as these can be significant.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在测量推理时间时，包含数据加载、数据预处理和后处理时间也非常重要，因为这些可能占有相当大的比例。
- en: Using tracing tools to understand computational performance
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用跟踪工具来理解计算性能
- en: While measuring the total inference time of a model informs you of the feasibility
    of an application, you might sometimes need a more detailed performance report.
    To do so, TensorFlow offers several tools. In this section, we will discuss the
    **trace tool**, which is part of the TensorFlow summary package.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然测量模型的总推理时间可以告诉您应用的可行性，但有时您可能需要更详细的性能报告。为此，TensorFlow提供了多个工具。在本节中，我们将讨论**跟踪工具**，它是TensorFlow摘要包的一部分。
- en: In [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training on Complex
    and Scarce Datasets*, we described how to analyze the performance of input pipelines.
    Refer to this chapter to monitor preprocessing and data ingestion performance.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)，《复杂和稀缺数据集上的训练》中，我们描述了如何分析输入管道的性能。请参考本章以监控预处理和数据摄取的性能。
- en: 'To use it, call `trace_on` and set `profiler` to `True`. You can then run TensorFlow
    or Keras operations and export the trace to a folder:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，调用`trace_on`并将`profiler`设置为`True`。然后，您可以运行TensorFlow或Keras操作，并将跟踪信息导出到一个文件夹：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Omitting the call to `create_file_writer` and `with writer.as_default()` will
    still create a trace of the operations. However, the model graph representation
    will not be written to disk.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略调用 `create_file_writer` 和 `with writer.as_default()` 仍然会生成操作的追踪信息。然而，模型图表示将不会写入磁盘。
- en: 'Once the model starts running with tracing enabled, we can point TensorBoard
    to this folder by executing the following command in the command line:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开始运行并启用追踪，我们可以通过在命令行执行以下命令来将 TensorBoard 指向该文件夹：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After opening TensorBoard in the browser and clicking on the **Profile** tab,
    we can then review the operations:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中打开 TensorBoard 并点击 **Profile** 标签后，我们可以查看操作信息：
- en: '![](img/436d7ed2-5384-4bb7-881e-7012bf36af70.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/436d7ed2-5384-4bb7-881e-7012bf36af70.png)'
- en: Figure 9-1: Trace of the operations for a simple fully connected model over
    multiple batches of data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-1：在多个数据批次上进行简单全连接模型的操作追踪
- en: 'As seen in the preceding timeline, the model is composed of many small operations.
    By clicking on an operation, we can obtain its name and its duration. For instance,
    here are the details for a dense matrix multiplication (a fully-connected layer):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，模型由许多小的操作组成。通过点击某个操作，我们可以获取它的名称及持续时间。例如，以下是一个密集矩阵乘法（全连接层）的详细信息：
- en: '![](img/47f42293-835f-4e3e-9640-9919d2b3ff8b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/47f42293-835f-4e3e-9640-9919d2b3ff8b.png)'
- en: 'Figure 9-2: The details of a matrix multiplication operation'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-2：矩阵乘法操作的细节
- en: TensorFlow traces can end up taking a large amount of disk space. For this reason,
    we recommend running the operations you want to trace on a few batches of data
    only.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 追踪可能会占用大量磁盘空间。因此，我们建议仅在少量数据批次上运行你希望追踪的操作。
- en: On the TPU, a dedicated Capture Profile button is available in TensorBoard.
    The TPU name, IP, and the trace recording time need to be specified.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TPU 上，TensorBoard 提供了一个专用的 Capture Profile 按钮。需要指定 TPU 名称、IP 地址以及追踪记录的时间。
- en: 'In practice, the tracing tool is used on much larger models to determine the
    following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，追踪工具通常用于更大的模型，以确定以下信息：
- en: Which layers are taking the most computing time.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些层占用了最多的计算时间。
- en: Why a model is taking more time than usual after a modification to the architecture.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在修改架构后模型需要更多时间。
- en: Whether TensorFlow is always computing numbers or is waiting on data. This can
    happen if preprocessing takes too long or if there is a lot of back and forth
    between CPUs.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判断 TensorFlow 是否始终在计算数字，或者是否在等待数据。这可能是因为数据预处理时间过长，或者 CPU 之间存在大量的数据交换。
- en: We encourage you to trace the models you are using to get a better understanding
    of the computational performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励你追踪所使用的模型，以更好地理解计算性能。
- en: Improving model inference speed
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高模型推理速度
- en: Now that we know how to properly measure a model inference speed, we can use
    several approaches to improve it. Some involve changing the hardware used, while
    others imply changing the model architecture itself.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何正确衡量模型推理速度，接下来可以使用几种方法来提高推理速度。有些方法涉及更换硬件，而其他方法则需要改变模型架构本身。
- en: Optimizing for hardware
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对硬件进行优化
- en: 'As we saw previously, the hardware used for inference is crucial for speed.
    From the slowest option to the fastest, it is recommended to use the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，推理使用的硬件对速度至关重要。从最慢的选项到最快的选项，推荐使用以下硬件：
- en: '**CPU**: While slower, it is often the cheapest option.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU**：虽然较慢，但通常是最便宜的选择。'
- en: '**GPU**: Faster but more expensive. Many smartphones have integrated GPUs that
    can be used for real-time applications.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU**：速度较快，但价格较贵。许多智能手机都配备了集成 GPU，可用于实时应用。'
- en: '**Specialized hardware**: For instance, Google''s *TPU* (for servers), Apple''s
    *Neural Engine* (on mobile), or *NVIDIA Jetson* (for portable hardware). They
    are chips made specifically for running deep learning operations.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用硬件**：例如，Google 的 *TPU*（用于服务器）、Apple 的 *Neural Engine*（用于移动设备）或 *NVIDIA
    Jetson*（用于便携式硬件）。这些都是专门为运行深度学习操作设计的芯片。'
- en: If speed is crucial for your application, it is important to use the fastest
    hardware available and to adapt your code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果速度对你的应用至关重要，那么使用最快的硬件并调整你的代码是很重要的。
- en: Optimizing on CPUs
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 CPU 上优化
- en: Modern Intel CPUs can compute matrix operations more quickly through special
    instructions. This is done using the **Math Kernel Library for Deep Neural Networks**
    (**MKL-DNN**). Out-of-the-box TensorFlow does not exploit those instructions.
    Using them requires either compiling TensorFlow with the right options or installing
    a special build of TensorFlow called `tensorflow-mkl`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现代英特尔 CPU 通过特殊指令可以更快速地计算矩阵运算。这是通过**深度神经网络数学核心库**（**MKL-DNN**）实现的。TensorFlow
    默认并未利用这些指令。使用这些指令需要重新编译 TensorFlow 并启用正确的选项，或安装一个名为 `tensorflow-mkl` 的特殊版本。
- en: Information on how to build TensorFlow with MKL-DNN is available at [https://www.tensorflow.org/](https://www.tensorflow.org/).
    Note that the toolkit currently only works on Linux.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用 MKL-DNN 构建 TensorFlow 的相关信息可以在[https://www.tensorflow.org/](https://www.tensorflow.org/)上找到。请注意，该工具包目前仅适用于
    Linux。
- en: Optimizing on GPUs
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 GPU 上的操作
- en: To run models on NVIDIA GPUs, two libraries are mandatory—`CUDA` and `cuDNN`.
    TensorFlow natively exploits the speed-up offered by those libraries.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 NVIDIA GPU 上运行模型，两个库是必需的——`CUDA` 和 `cuDNN`。TensorFlow 天生就能利用这些库提供的加速。
- en: To properly run operations on the GPU, the `tensorflow-gpu` package must be
    installed. Moreover, the CUDA version of `tensorflow-gpu` must match the one installed
    on the computer.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确在 GPU 上运行操作，必须安装 `tensorflow-gpu` 包。此外，`tensorflow-gpu` 的 CUDA 版本必须与计算机上安装的版本匹配。
- en: Some modern GPUs offer **Floating Point 16** (**FP16**) instructions. The idea
    is to use reduced precision floats (16 bits instead of the 32 bits commonly used)
    in order to speed up inference while not impacting the output quality by much.
    Not all GPUs are compatible with FP16.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现代 GPU 提供**16位浮点**（**FP16**）指令。其理念是使用较低精度的浮点数（16 位而不是常用的 32 位），以加速推理，而不会对输出质量产生太大影响。并非所有
    GPU 都支持 FP16。
- en: Optimizing on specialized hardware
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在专用硬件上优化
- en: Since every chip is different, the techniques ensuring faster inference vary
    from one manufacturer to another. The steps necessary for running a model are
    well documented by the manufacturer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个芯片都不同，确保更快推理的技术在不同的制造商之间有所不同。运行模型所需的步骤由制造商进行了详细文档化。
- en: A rule of thumb is to not use exotic operations. If one of the layers is running
    operations that include conditions or branching, it is likely that the chip will
    not support it. The operations will have to run on the CPU, making the whole process
    slower. It is therefore recommended to *only use standard operations*—convolution,
    pooling, and fully connected layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一个经验法则是避免使用特殊操作。如果某一层运行的操作包含条件判断或分支，那么该芯片很可能不支持它。操作将不得不在 CPU 上运行，从而使整个过程变慢。因此，建议*仅使用标准操作*——卷积、池化和全连接层。
- en: Optimizing input
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化输入
- en: The inference speed of a computer vision model is directly proportional to the
    size of the input image. Moreover, dividing the dimensions of an image by two
    means four times fewer pixels for the model to process. Therefore, using *smaller
    images improves inference speed*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉模型的推理速度与输入图像的大小成正比。此外，将图像的维度除以二意味着模型需要处理的像素减少四倍。因此，使用*较小的图像可以提高推理速度*。
- en: When using smaller images, the model has less information and fewer details
    to work with. This often has an impact on the quality of the results. It is necessary
    to experiment with image size to find a good *trade-off between speed and accuracy*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较小的图像时，模型可以获取的信息较少，细节也更少。这通常会影响结果的质量。需要通过实验图像大小来找到一个良好的*速度与准确性之间的平衡*。
- en: Optimizing post-processing
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化后处理
- en: As we saw previously in the book, most models require post-processing operations.
    If implemented using the wrong tools, post-processing can take a lot of time.
    While most post-processing happens on the CPU, it is sometimes possible to run
    some operations on the GPU.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书前面看到的，大多数模型需要后处理操作。如果使用不合适的工具实现，后处理可能会耗费大量时间。虽然大多数后处理发生在 CPU 上，但有时也可以将部分操作放在
    GPU 上执行。
- en: 'Using tracing tools, we can analyze the time taken by post-processing to optimize
    it. **Non-Maximum Suppression** (**NMS**) is an operation that can take a lot
    of time if not implemented correctly (refer to [Chapter 5](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml),
    *Object Detection Models*):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用追踪工具，我们可以分析后处理所需的时间，从而对其进行优化。**非极大值抑制**（**NMS**）是一项操作，如果实现不当，可能会消耗大量时间（请参见[第
    5 章](593ada62-2ff4-4085-a15e-44f8f5e3d071.xhtml)，*目标检测模型*）：
- en: '![](img/abea7881-df99-4226-b65b-e3686c106fd0.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abea7881-df99-4226-b65b-e3686c106fd0.png)'
- en: Figure 9-3: Evolution of NMS computing time with the number of boxes
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-3：NMS 计算时间与框数的关系
- en: Notice in the preceding diagram that the slow implementation takes linear computing
    time, while the fast implementation is almost constant. Though four milliseconds
    may seem quite low, keep in mind that some models can return an even larger number
    of boxes, resulting in a post-processing time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意前面的图表，慢速实现采用线性计算时间，而快速实现则几乎是常数。虽然四毫秒看起来非常短，但请记住，一些模型可能返回更多的框，从而导致后处理时间。
- en: When the model is still too slow
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当模型仍然太慢时
- en: Once the model has been optimized for speed, it can sometimes still be too slow
    for real-time applications. There are a few techniques to work around the slowness
    while maintaining a real-time feeling for the user.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型在速度上得到了优化，它有时仍然太慢，无法满足实时应用的要求。有几种技术可以绕过这种慢速问题，同时保持用户的实时体验。
- en: Interpolating and tracking
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插值与跟踪
- en: Object detection models are notoriously computationally-intensive. Running on
    every frame of a video is sometimes impractical. A common technique is to use
    the model every few frames only. In-between frames, linear interpolation is used
    to follow the tracked object.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测模型通常计算量非常大。在每一帧视频上运行有时是不切实际的。常用的技术是只在每几帧运行一次模型。在中间帧之间，使用线性插值来跟踪目标物体。
- en: While this technique does not work for real-time applications, another one that
    is commonly used is **object tracking.** Once an object is detected with a deep
    learning model, a more simple model is used to follow the boundaries of the object.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种技术不适用于实时应用，但另一种常用的技术是**目标跟踪**。一旦通过深度学习模型检测到目标，就使用一个更简单的模型来跟踪物体的边界。
- en: Object tracking can work on almost any kind of object as long as it is well
    distinguishable from its background and its shape does not change excessively.
    There are many object tracking algorithms (some of them are available through
    OpenCV's tracker module,  documented here [https://docs.opencv.org/master/d9/df8/group__tracking.html](https://docs.opencv.org/master/d9/df8/group__tracking.html));
    many of them are available for mobile applications.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪几乎适用于任何类型的物体，只要它能与背景清晰区分，并且其形状不会发生过度变化。有许多目标跟踪算法（其中一些可以通过 OpenCV 的 tracker
    模块获得，文档可在此查看 [https://docs.opencv.org/master/d9/df8/group__tracking.html](https://docs.opencv.org/master/d9/df8/group__tracking.html)）；其中许多算法也适用于移动应用。
- en: Model distillation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型蒸馏
- en: When none of the other techniques work, one last option is **model distillation**.
    The general idea is to train a small model to learn the output of a bigger model.
    Instead of training the small model to learn the raw labels (we could use the
    data for this), we train it to learn the output of the bigger model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当其他技术都不起作用时，最后的选择是**模型蒸馏**。其基本思路是训练一个小模型，让它学习更大模型的输出。我们不是让小模型学习原始标签（我们可以用数据来做到这一点），而是让它学习更大模型的输出。
- en: 'Let''s see an example—we trained a very large network to predict an animal''s
    breed from a picture. The output is as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子——我们训练了一个非常大的网络，通过图片预测动物的品种。输出如下：
- en: '![](img/e9ab31a5-6241-4236-9da3-948a0d32c6b6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9ab31a5-6241-4236-9da3-948a0d32c6b6.png)'
- en: Figure 9-4: Examples of predictions made by our network
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-4：我们网络所做的预测示例
- en: Because our model is too large to run on mobile, we decided to train a smaller
    model. Instead of training it with the labels we have, we decided to distill the
    knowledge of the larger network. To do so, we will use the output of the larger
    network as targets.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型太大，无法在移动设备上运行，因此我们决定训练一个较小的模型。我们没有直接用已有的标签来训练它，而是决定将大网络的知识进行蒸馏。为此，我们将使用大网络的输出作为目标。
- en: For the first picture, instead of training the new model with a target of *[1,
    0, 0]*, we will use the output of the larger network, a target of *[0.9, 0.7,
    0.1]*. This new target is called a **soft target**. This way, the smaller network
    will be taught that, while the animal in the first picture is not a husky, it
    does look similar to one according to more advanced models, as the picture has
    a score of *0.7* for the *husky* class.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一张图片，我们不再用* [1, 0, 0] *作为新模型的目标，而是使用大网络的输出，目标为* [0.9, 0.7, 0.1] *。这个新目标被称为**软目标**。通过这种方式，小网络将被教导，尽管第一张图片中的动物不是哈士奇，但根据更先进的模型，它看起来与哈士奇相似，因为这张图片在*哈士奇*类别中的得分为*0.7*。
- en: The larger network managed to directly learn from the original labels (*[1,
    0, 0]* in our example) because it has more computing and memory power. During
    training, it was able to deduce that breeds of dogs look like one another but
    belong to different classes. A smaller model would not have the capacity to learn
    such abstract relations in the data by itself, but it can be guided by the other
    network. Following the aforementioned procedure, the inferred knowledge from the
    first model will be passed to the new one, hence the name **knowledge distillation**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的网络能够直接从原始标签中学习（例如我们示例中的 *[1, 0, 0]*），因为它拥有更多的计算和内存能力。在训练过程中，它能够推测出不同品种的狗虽然长得相似，但属于不同的类别。一个较小的模型自己可能无法学习到这样的抽象关系，但可以通过其他网络进行引导。按照上述过程，第一个模型推断出的知识将传递给新的模型，这就是**知识蒸馏**的名称由来。
- en: Reducing model size
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少模型大小
- en: 'When using a deep learning model in the browser or on mobile, the model needs
    to be downloaded on the device. It needs to be as lightweight as possible for
    the following reasons:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器或移动设备上使用深度学习模型时，模型需要下载到设备上。为了以下原因，模型必须尽可能轻量：
- en: Users are often using their phone on a cellular connection that is sometimes
    metered.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户通常使用手机在移动网络下连接，且这个网络有时是计量的。
- en: The connection can also be slow.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接也可能较慢。
- en: Models can be frequently updated.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可能需要频繁更新。
- en: Disk space on portable devices is sometimes limited.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 便携设备的磁盘空间有时是有限的。
- en: With hundreds of millions of parameters, deep learning models are notoriously
    disk space-consuming. Thankfully, there are techniques to reduce their size.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型拥有数亿个参数，这使得它们非常占用磁盘空间。幸运的是，有一些技术可以减少它们的大小。
- en: Quantization
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: The most common technique is to reduce the precision of the parameters. Instead
    of storing them as 32-bit floats, we can store them as 16- or 8-bit floats. There
    have been experiments for using binary parameters, taking only 1 bit to store.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的技术是减少参数的精度。我们可以将参数存储为16位或8位浮点数，而不是32位浮点数。已经有实验使用二进制参数，仅用1位存储。
- en: '**Quantization** is often done at the end of training, when converting the
    model for use on the device. This conversion impacts the accuracy of the model.
    Because of this, it is very important to evaluate the model after quantization.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**量化**通常在训练结束时进行，当模型转换为在设备上使用时进行。这一转换会影响模型的准确性。因此，在量化后评估模型非常重要。'
- en: Among all the compression techniques, quantization is often the one with the
    highest impact on size and the least impact on performance. It is also very easy
    to implement.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有的压缩技术中，量化通常对模型大小的影响最大，而对性能的影响最小。它也非常容易实现。
- en: Channel pruning and weight sparsification
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通道剪枝和权重稀疏化
- en: Other techniques exist but can be harder to implement. There is no straightforward
    way to apply them because they rely mostly on trial and error.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些技术存在，但实现起来可能更困难。因为这些技术主要依赖于反复试验，所以没有直接的应用方法。
- en: The first one, **channel pruning**, consists of removing some convolutional
    filters or some channels. Convolutional layers usually have between 16 and 512
    different filters. At the end of the training phase, it often appears that some
    of them are not useful. We can remove them to avoid storing weights that will
    not help the model performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法，**通道剪枝**，包括去除一些卷积滤波器或通道。卷积层通常有16到512个不同的滤波器。在训练阶段结束时，通常会发现其中一些滤波器是无用的。我们可以去除这些滤波器，以避免存储那些对模型性能无帮助的权重。
- en: The second one is called **weight sparsification**. Instead of storing weights
    for the whole matrix, we can store only the ones that are deemed important or
    not close to zero.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法叫做**权重稀疏化**。我们可以只存储那些被认为重要或远离零值的权重，而不是存储整个矩阵的权重。
- en: For instance, instead of storing a weight vector such as *[0.1, 0.9, 0.05, 0.01,
    0.7, 0.001]*, we could keep weights that are not close to zero. The result is
    a list of tuples in the form *(position, value)*. In our example, it would be
    *[(1, 0.9), (4, 0.7)]*. If many of the vector's values are close to zero, we could
    expect a large reduction in stored weights.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以存储一个权重向量，如*[0.1, 0.9, 0.05, 0.01, 0.7, 0.001]*，而不是存储所有权重。我们可以保留那些远离零值的权重。最终结果是一个元组列表，形式为*(位置,
    值)*。在我们的例子中，它会是*[(1, 0.9), (4, 0.7)]*。如果向量中的许多值接近零，我们可以预期存储的权重大幅减少。
- en: On-device machine learning
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设备端机器学习
- en: 'Due to their high computational requirements, deep learning algorithms are
    most commonly run on powerful servers. They are computers specifically designed
    for this task. For latency, privacy, or cost reasons, it is sometimes more interesting
    to run inference on customers'' devices: smartphones, connected objects, cars,
    or microcomputers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习算法对计算资源的高要求，它们通常在强大的服务器上运行。这些计算机是专门为此任务设计的。出于延迟、隐私或成本的原因，有时在客户的设备上运行推理更具吸引力：智能手机、连接设备、汽车或微型计算机。
- en: What all those devices have in common are lower computational power and low
    power requirements. Because they are at the end of the data life cycle, on-device
    machine learning is also referred to as **edge computing** or **machine learning
    on the edge**.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些设备的共同特点是较低的计算能力和低功耗要求。由于它们处于数据生命周期的末端，设备端机器学习也被称为**边缘计算**或**边缘机器学习**。
- en: With regular machine learning, the computation usually happens in the data center.
    For instance, when you upload a photo to Facebook, a deep learning model is run
    in Facebook's data center to detect your friends' faces and help you tag them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常规机器学习时，计算通常发生在数据中心。例如，当你将照片上传到 Facebook 时，一个深度学习模型会在 Facebook 的数据中心运行，以检测你朋友的面孔并帮助你标记他们。
- en: 'With on-device machine learning, the inference happens on your device. A common
    example is Snapchat face filters—the model that detects your face position is
    run directly on the device. However, model training still happens in data centers—the
    device uses a trained model fetched from the server:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设备端机器学习时，推理发生在你的设备上。一个常见的例子是 Snapchat 面部滤镜——检测你面部位置的模型直接在设备上运行。然而，模型训练仍然发生在数据中心——设备使用的是从服务器获取的训练模型：
- en: '![](img/62deba1a-1bb9-4848-a42a-f56c56320bdc.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62deba1a-1bb9-4848-a42a-f56c56320bdc.png)'
- en: Figure 9-5: Diagram comparing on-device machine learning with conventional machine
    learning
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9-5: 比较设备端机器学习与传统机器学习的示意图'
- en: Most on-device machine learning happens for inference. The training of models
    is still mostly done on dedicated servers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数设备端机器学习都是用于推理。模型的训练仍然主要在专用服务器上进行。
- en: Considerations of on-device machine learning
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设备端机器学习的考虑因素
- en: The use of **on-device machine learning** (**on-device ML**) is usually motivated
    by a combination of reasons, but also has its limitations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**设备端机器学习**（**on-device ML**）通常是由多种原因推动的，但也有其局限性。
- en: Benefits of on-device ML
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设备端机器学习的好处
- en: The following paragraphs list the main benefits of running machine learning
    algorithms directly on users' devices.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下段落列出了直接在用户设备上运行机器学习算法的主要好处。
- en: Latency
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟
- en: The most common motivation is **latency**. Because sending data to a server
    for processing takes time, real-time applications make it impossible to use conventional
    machine learning. The most striking illustration is self-driving cars. To react
    quickly to its environment, the car must have the lowest latency possible. Therefore,
    it is crucial to run the model in the car. Moreover, some devices are used in
    places where internet access is simply not available.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的动机是**延迟**。因为将数据发送到服务器进行处理需要时间，实时应用使得使用传统机器学习变得不可能。最引人注目的例子是自动驾驶汽车。为了快速反应环境，汽车必须拥有尽可能低的延迟。因此，在汽车内运行模型至关重要。此外，一些设备被使用在没有互联网连接的地方。
- en: Privacy
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私
- en: As consumers care more and more about their privacy, companies are devising
    techniques to run deep learning models while respecting this demand.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随着消费者对隐私的关注越来越高，企业正在设计技术，以在尊重这一需求的同时运行深度学习模型。
- en: Let's use a large-scale example from Apple. When browsing through photos on
    an iOS device, you may notice that it is possible to search for objects or things—`cat`,
    `bottle`, `car` will return the corresponding images. This is the case even if
    the pictures are not sent to the cloud. For Apple, it was important to make that
    feature available while respecting the privacy of its users. Sending pictures
    for processing without the users' consent would have been impossible.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个来自 Apple 的大规模示例。当你在 iOS 设备上浏览照片时，你可能会注意到可以搜索对象或事物——`cat`、`bottle`、`car`
    会返回相应的图片。即使这些图片没有发送到云端，也能实现这一点。对于 Apple 来说，在尊重用户隐私的同时提供这个功能非常重要。如果没有用户的同意，发送照片进行处理是不可能的。
- en: Therefore, Apple decided to use on-device ML. Every night, when the phone is
    charging, a computer vision model is run on the iPhone to detect objects in the
    image and make this feature available.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，苹果决定使用设备端机器学习。每晚，当手机在充电时，iPhone 上会运行计算机视觉模型，检测图像中的物体并启用此功能。
- en: Cost
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本
- en: On top of respecting user privacy, this feature also reduces costs for Apple
    because the company does not have to pay the bill for servers to process the hundreds
    of millions of images that their customers produce.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了尊重用户隐私外，这项功能还帮助苹果公司降低了成本，因为公司无需支付服务器费用来处理其客户生成的数亿张图片。
- en: On a much smaller scale, it is now possible to run some deep learning models
    in the browser. This is especially useful for demos—by running the models on the
    user's computer, you can avoid paying for a costly GPU-enabled server to run inference
    at scale. Moreover, there will not be any overloading issues because the more
    users that access the page, the more computing power that is available.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在更小的规模上，现在已经可以在浏览器中运行一些深度学习模型。这对于演示特别有用——通过在用户的计算机上运行模型，您可以避免为大规模推理支付高昂的 GPU
    服务器费用。此外，不会出现过载问题，因为页面访问的用户越多，所能使用的计算能力也越多。
- en: Limitations of on-device ML
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设备端机器学习的局限性
- en: While it has many benefits, this concept also has a number of limitations. First
    of all, the limited computing power of devices means that some of the most powerful
    models cannot be considered.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它有许多好处，但这一概念也存在一些局限性。首先，设备的计算能力有限，意味着一些最强大的模型无法被考虑。
- en: Also, many on-device deep learning frameworks are not compatible with the most
    innovative or the most complex layers. For instance, TensorFlow Lite is not compatible
    with custom LSTM layers, making it hard to port advanced recurrent neural networks
    on mobile using this framework.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多设备端深度学习框架与最创新或最复杂的层不兼容。例如，TensorFlow Lite 不兼容自定义 LSTM 层，这使得使用该框架在移动设备上移植高级循环神经网络变得困难。
- en: Finally, making models available on devices implies sharing the weights and
    the architecture with users. While encryption and obfuscation methods exist, it
    increases the risk of reverse engineering or model theft.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使模型在设备上可用意味着要与用户共享权重和架构。尽管存在加密和混淆方法，但这增加了反向工程或模型盗窃的风险。
- en: Practical on-device computer vision
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际的设备端计算机视觉
- en: Before discussing the practical application of on-device computer vision, we
    will have a look at the general considerations for running deep learning models
    on mobile devices.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论设备端计算机视觉的实际应用之前，我们先来看一下在移动设备上运行深度学习模型的一般考虑因素。
- en: On-device computer vision particularities
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设备端计算机视觉的特点
- en: 'When running computer vision models on mobile devices, the focus switches from
    raw performance metrics to user experience. On mobile phones, this means minimizing
    battery and disk usage: we don''t want to drain the phone''s battery in minutes
    or fill up all the available space on the device. When running on mobile, it is
    recommended to use smaller models. As they contain fewer parameters, they use
    less disk space. Moreover, as they require fewer operations, this leads to reduced
    battery usage.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在移动设备上运行计算机视觉模型时，重点从原始性能指标转向用户体验。在手机上，这意味着要尽量减少电池和磁盘的使用：我们不希望在几分钟内耗尽手机电池或填满设备的所有可用空间。在移动设备上运行时，建议使用较小的模型。由于它们包含更少的参数，因此占用的磁盘空间更少。此外，由于所需的操作更少，这也减少了电池的使用。
- en: Another particularity of mobile phones is orientation. In training datasets,
    most pictures are provided with the correct orientation. While we sometimes change
    this orientation during data augmentation, the images are rarely upside down or
    completely sideways. However, there are many ways to hold a mobile phone. For
    this reason, we must monitor the device's orientation to make sure that we are
    feeding the model with images that are correctly oriented.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 手机的另一个特点是方向。在训练数据集中，大多数图片都是正确方向的。尽管我们有时在数据增强过程中会改变方向，但图像很少会完全倒置或侧向。然而，手机的持握方式多种多样。因此，我们必须监控设备的方向，以确保我们向模型输入的图像方向正确。
- en: Generating a SavedModel
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成 SavedModel
- en: As we mentioned earlier, on-device machine learning is typically used for inference.
    Therefore, a prerequisite is to have a *trained model*. Hopefully, this book will
    have given you a good idea of how to implement and prepare your network. We now
    need to convert the model to an intermediate file format. It will then be converted
    by a library for mobile use.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，设备端机器学习通常用于推理。因此，前提是需要有一个*训练好的模型*。希望本书能让你对如何实现和准备网络有一个好的了解。我们现在需要将模型转换为中间文件格式。然后，它将通过一个库转换为移动端使用。
- en: In TensorFlow 2, the intermediate format of choice is **SavedModel**. A SavedModel
    contains the model architecture (the graph) and the weights.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2中，首选的中间格式是**SavedModel**。一个SavedModel包含了模型架构（图）和权重。
- en: 'Most TensorFlow objects can be exported as a SavedModel. For instance, the
    following code exports a trained Keras model:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数TensorFlow对象都可以导出为SavedModel。例如，以下代码导出一个训练过的Keras模型：
- en: '[PRE2]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Generating a frozen graph
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成冻结图
- en: Before introducing the **SavedModel** API, TensorFlow mainly used the **frozen
    graphs** format. In practice, a SavedModel is a wrapper around a frozen graph.
    The former includes more metadata and can include the preprocessing function needed
    to serve the model. While SavedModel is gaining in popularity, some libraries
    still require frozen models.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入**SavedModel** API之前，TensorFlow主要使用**冻结图**格式。实际上，SavedModel是一个冻结图的封装。前者包含更多的元数据，并且可以包含服务模型所需的预处理函数。虽然SavedModel越来越受欢迎，但一些库仍然要求使用冻结模型。
- en: 'To convert a SavedModel to a frozen graph, the following code can be used:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要将SavedModel转换为冻结图，可以使用以下代码：
- en: '[PRE3]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: On top of specifying the input and output, we also need to specify `output_node_names`.
    Indeed, it is not always clear what the inference output of a model is. For instance,
    image detection models have several outputs—the box coordinates, the scores, and
    the classes. We need to specify which one(s) to use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 除了指定输入和输出之外，我们还需要指定`output_node_names`。事实上，模型的推理输出并不总是很清楚。例如，图像检测模型有多个输出——框坐标、分数和类别。我们需要指定使用哪个（些）输出。
- en: Note that many arguments are `False` or `None` because this function can accept
    many different formats, and SavedModel is only one of them.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，许多参数的值是`False`或`None`，因为这个函数可以接受许多不同的格式，而SavedModel只是其中之一。
- en: Importance of preprocessing
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理的重要性
- en: 'As explained in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, input images have to be **preprocessed**. The most common preprocessing
    method is to divide each channel by *127.5* (*127.5* = *255/2* = middle value
    of an image pixel) and subtract 1\. This way, we represent images with values
    between -1 and 1:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)中所解释的，*现代神经网络*，输入图像必须进行**预处理**。最常见的预处理方法是将每个通道的值除以*127.5*（*127.5*
    = *255/2* = 图像像素的中间值），并减去1。这样，我们将图像的值表示为-1到1之间的数值：
- en: '![](img/2d8034f3-e309-4bc6-836f-a34d00cb4057.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d8034f3-e309-4bc6-836f-a34d00cb4057.png)'
- en: Figure 9-6: Example of preprocessing for a *3 x 3* image with a single channel
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-6： 单通道*3 x 3*图像的预处理示例
- en: 'However, there are many ways to represent images, depending on the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，表示图像的方法有很多种，具体取决于以下几个方面：
- en: 'The order of the channels: RGB or BGR'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通道的顺序：RGB 或 BGR
- en: Whether the image is between *0* and *1*, *-1* and *1*, or *0* and *255*
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的范围是*0*到*1*，* -1*到*1*，还是*0*到*255*？
- en: 'The order of the dimensions: *[W, H, C]* or *[C, W, H]*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度的顺序：*[W, H, C]* 或 *[C, W, H]*
- en: The orientation of the image
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的方向
- en: When porting a model, it is paramount to use the *exact same preprocessing*
    on a device as during training. Failing to do so will lead the model to infer
    poorly, sometimes even to fail completely, as the input data will be too different
    compared with the training data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在移植模型时，至关重要的是在设备上使用与训练时*完全相同的预处理*。如果没有做到这一点，模型的推理效果会很差，有时甚至会完全失败，因为输入数据与训练数据之间的差异太大。
- en: All mobile deep learning frameworks provide some options to specify preprocessing
    settings. It is up to you to set the correct parameters.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的移动深度学习框架都提供了一些选项来指定预处理设置。由你来设置正确的参数。
- en: Now that we have obtained a **SavedModel** and that we know the importance of
    pre-processing, we are ready to use our model on different devices.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了**SavedModel**，并且了解了预处理的重要性，我们可以在不同的设备上使用我们的模型了。
- en: Example app – recognizing facial expressions
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例应用 – 识别面部表情
- en: To directly apply the notions presented in this chapter, we will develop an
    app making use of a lightweight computer vision model, and we will deploy it to
    various platforms.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直接应用本章介绍的概念，我们将开发一个使用轻量级计算机视觉模型的应用程序，并将其部署到各种平台。
- en: 'We will build an app that classifies facial expressions. When pointed to a
    person''s face, it will output the expression of that person—happy, sad, surprised,
    disgusted, angry, or neutral. We will train our model on the **Facial Expression
    Recognition** (**FER**) dataset available at [https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge),
    put together by Pierre-Luc Carrier and Aaron Courville. It is composed of 28,709
    grayscale images of *48 × 48* in size:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个分类面部表情的应用程序。当指向一个人的脸时，它会输出该人的表情——高兴、悲伤、惊讶、厌恶、生气或中立。我们将在**面部表情识别**（**FER**）数据集上训练我们的模型，该数据集可在[https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge)上获得，由Pierre-Luc
    Carrier和Aaron Courville整理。它由28,709张灰度图像组成，大小为*48 × 48*：
- en: '![](img/c5a7d006-6b39-4efd-8d63-1c6381414dee.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5a7d006-6b39-4efd-8d63-1c6381414dee.png)'
- en: Figure 9-7: Images sampled from the FER dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-7：从FER数据集中采样的图像
- en: 'Inside the app, the naive approach would be to capture images with the camera
    and then feed them directly to our trained model. However, this would yield poor
    results as objects in the environment would impair the quality of the prediction.
    We need to crop the face of the user before feeding it to the user:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序中，最简单的方法是使用相机捕捉图像，然后将其直接输入到训练好的模型中。然而，这样会导致结果不佳，因为环境中的物体会影响预测的质量。我们需要在将图像输入给用户之前，先裁剪出用户的面部。
- en: '![](img/9e7666d7-4bd5-49f6-a429-f33dc2299d60.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7666d7-4bd5-49f6-a429-f33dc2299d60.png)'
- en: Figure 9-8: Two-step flow of our facial expression classification app
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9-8：我们的面部表情分类应用的两步流程
- en: While we could build our own model for the first step (face detection), it is
    much more convenient to use out-of-the-box APIs. They are available natively on
    iOS and through libraries on Android and in the browser. The second step, expression
    classification, will be performed using our custom model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以为第一步（人脸检测）构建自己的模型，但使用现成的API要方便得多。它们在iOS上原生支持，在Android和浏览器中通过库提供支持。第二步，表情分类，将使用我们的自定义模型进行。
- en: Introducing MobileNet
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 MobileNet
- en: 'The architecture we will use for classification is named **MobileNet**. It
    is a convolutional model designed to run on mobile. Introduced in 2017, in the
    paper *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications*,
    by Andrew G Howard et al.*,* it uses a special kind of convolution to reduce the
    number of parameters as well as the computations necessary to generate predictions.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将用于分类的架构被命名为**MobileNet**。它是一个为移动设备设计的卷积模型。该模型在2017年由Andrew G Howard等人提出，论文名为《MobileNets:
    Efficient Convolutional Neural Networks for Mobile Vision Applications》，它使用一种特殊类型的卷积来减少生成预测所需的参数数量和计算量。'
- en: 'MobileNet uses **depthwise separable** convolutions. In practice, this means
    that the architecture is composed of an alternation of two types of convolutions:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet 使用**深度可分离**卷积。实际上，这意味着该架构由两种类型的卷积交替组成：
- en: '**Pointwise** **convolutions**: These are just like regular convolutions, but
    with a *1* × *1* kernel. The purpose of pointwise convolutions is to combine the
    different channels of the input. Applied to an RGB image, they will compute a
    weighted sum of all channels.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逐点** **卷积**：这些与常规卷积类似，但使用的是*1* × *1* 卷积核。逐点卷积的目的是结合输入的不同通道。应用于RGB图像时，它们将计算所有通道的加权和。'
- en: '**Depthwise** **convolutions**: These are like regular convolutions, but do
    not combine channels. The role of depthwise convolutions is to filter the content
    of the input (detect lines or patterns). Applied to an RGB image, they will compute
    a feature map for each channel.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**深度** **卷积**：这些卷积与常规卷积类似，但不合并通道。深度卷积的作用是过滤输入的内容（检测线条或模式）。应用于RGB图像时，它们将为每个通道计算一个特征图。'
- en: When combined, these two types of convolutions perform similarly to regular
    convolutions. However, due to the small size of their kernels, they require fewer
    parameters and computational power, making this architecture suited for mobile
    devices.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的卷积结合使用时，表现与常规卷积相似。然而，由于它们的卷积核较小，所需的参数和计算量较少，因此这种架构非常适合移动设备。
- en: Deploying models on-device
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在设备上部署模型
- en: To illustrate on-device machine learning, we will port a model to iOS and Android
    devices, as well as for web browsers. We will also describe the other types of
    devices available.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明设备端的机器学习，我们将把一个模型移植到iOS和Android设备，以及网页浏览器上。我们还将描述其他可用的设备类型。
- en: Running on iOS devices using Core ML
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在iOS设备上使用Core ML运行
- en: With the release of its latest devices, Apple is putting the emphasis on machine
    learning. They designed a custom chip—the **neural engine**. This can achieve
    fast deep learning operations while maintaining a low power usage. To fully benefit
    from this chip, developers must use a set of official APIs called **Core ML**
    (refer to the documentation at [https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最新设备的发布，苹果公司将重点放在了机器学习上。他们设计了一款定制的芯片——**神经引擎**。该芯片可以实现快速的深度学习操作，同时保持低功耗。为了充分利用这款芯片，开发者必须使用一组官方API，称为**Core
    ML**（请参阅[https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)中的文档）。
- en: To use an existing model with Core ML, developers need to convert it to the
    `.mlmodel` format. Thankfully, Apple provides Python tools to convert from Keras
    or TensorFlow.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Core ML中使用现有模型，开发者需要将其转换为`.mlmodel`格式。幸运的是，苹果提供了Python工具，用于将Keras或TensorFlow模型转换为该格式。
- en: In addition to speed and energy efficiency, one of the strengths of Core ML
    is its integration with other iOS APIs. Powerful native methods exist for augmented
    reality, face detection, object tracking, and much more.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了速度和能效，Core ML的一个优势是它与其他iOS API的集成。存在强大的本地方法，用于增强现实、面部检测、物体追踪等多个功能。
- en: While TensorFlow Lite supports iOS, as of now, we still recommend using Core
    ML. This allows faster inference time and broader feature compatibility.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TensorFlow Lite支持iOS，但目前我们仍然推荐使用Core ML。它能够提供更快的推理时间和更广泛的功能兼容性。
- en: Converting from TensorFlow or Keras
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从TensorFlow或Keras转换
- en: To convert our model from Keras or TensorFlow, another tool is needed—`tf-coreml`
    ([https://github.com/tf-coreml/tf-coreml](https://github.com/tf-coreml/tf-coreml)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的模型从Keras或TensorFlow转换，另一个工具是必需的——`tf-coreml` ([https://github.com/tf-coreml/tf-coreml](https://github.com/tf-coreml/tf-coreml))。
- en: At the time of writing, `tf-coreml` is not compatible with TensorFlow 2\. We
    have provided a modified version while the library's developers are updating it.
    Refer to the chapter's notebook for the latest installation instructions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，`tf-coreml`与TensorFlow 2不兼容。我们提供了一个修改版本，直到该库的开发者更新它。请参阅本章的笔记本，以获取最新的安装说明。
- en: 'We can then convert our model to `.mlmodel`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型转换为`.mlmodel`格式：
- en: '[PRE4]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A few arguments are important:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参数是重要的：
- en: '`class_labels`: The list of labels. Without this, we would end up with class
    IDs instead of readable text.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class_labels`：标签列表。如果没有这个，我们最终得到的将是类ID，而不是可读的文本。'
- en: '`input_names`: The name of the input layer.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_names`：输入层的名称。'
- en: '`image_input_names`: This is used to specify to the Core ML framework that
    our input is an image. This will be useful later on because the library will handle
    all preprocessing for us.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_input_names`：这是用于指定Core ML框架我们的输入是图像。这个设置稍后会很有用，因为库将为我们处理所有的预处理工作。'
- en: '`output_feature_names`: As with the frozen model conversion, we need to specify
    the outputs we will target in our model. In this case, they are not operations
    but outputs. Therefore, `:0` must be appended to the name.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_feature_names`：与冻结模型转换一样，我们需要指定模型中要目标的输出。在这种情况下，它们不是操作，而是输出。因此，必须在名称后附加`:0`。'
- en: '`image_scale`: The scale used for preprocessing.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_scale`：用于预处理的缩放比例。'
- en: '`bias`: The bias of the preprocessing for each color.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias`：每种颜色的预处理偏差。'
- en: '`is_bgr`: Must be `True` if channels are in the BGR order, or `False` if RGB.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_bgr`：如果通道是BGR顺序，则必须为`True`，如果是RGB顺序，则为`False`。'
- en: As stated earlier, `scale`, `bias`, and `is_bgr` must match the ones used during
    training.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`scale`、`bias`和`is_bgr`必须与训练时使用的设置相匹配。
- en: 'After converting the model to a `.mlmodel` file, it can be opened in Xcode:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型转换为`.mlmodel`文件后，可以在Xcode中打开它：
- en: '![](img/35e480af-9e51-41fb-8a8c-92ad059ebdaa.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35e480af-9e51-41fb-8a8c-92ad059ebdaa.png)'
- en: Figure 9-9: Screenshot of Xcode showing the details of a model
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-9：Xcode截屏，显示模型的详细信息
- en: Note that the input is recognized as an `Image` since we specified `image_input_names`.
    Thanks to this, Core ML will be able to handle the preprocessing of the image
    for us.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们指定了`image_input_names`，输入被识别为`Image`。因此，Core ML将能够为我们处理图像的预处理工作。
- en: Loading the model
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载模型
- en: 'The full app is available in the chapter repository. A Mac computer and an
    iOS device are necessary to build and run it. Let''s briefly detail the steps
    to get predictions from the model. Note that the following code is written in
    Swift. It has a similar syntax to Python:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的应用程序可以在章节仓库中找到。构建和运行它需要一台 Mac 计算机和一台 iOS 设备。让我们简要介绍一下如何从模型中获取预测的步骤。请注意，以下代码是用
    Swift 编写的，它与 Python 语法类似：
- en: '[PRE5]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The code consists of three main steps:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 代码由三个主要步骤组成：
- en: Loading the model. All the information about it is available in the `.mlmodel`
    file.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型。有关模型的所有信息都包含在 `.mlmodel` 文件中。
- en: Setting a custom callback. In our case, after the image is classified, we will
    call `processClassifications`.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置自定义回调。在我们的情况下，图像分类完成后，我们将调用 `processClassifications`。
- en: Setting `imageCropAndScaleOption`. Our model was designed to accept square images,
    but the input often has a different ratio. Therefore, we configure Core ML to
    crop the center of the image by setting it to `centerCrop`.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 `imageCropAndScaleOption`。我们的模型设计为接受正方形图像，但输入图像的比例通常不同。因此，我们配置 Core ML 通过将其设置为
    `centerCrop` 来裁剪图像的中心部分。
- en: 'We also load the model used for face detection using the native `VNDetectFaceRectanglesRequest`
    and `VNSequenceRequestHandler` functions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用本机的 `VNDetectFaceRectanglesRequest` 和 `VNSequenceRequestHandler` 函数加载用于面部检测的模型：
- en: '[PRE6]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Using the model
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用模型
- en: 'As an input, we access `pixelBuffer`, which contains the pixel of the video
    feed from the camera of the device. We run our face detection model and obtain
    `faceObservations`. This will contain the detection results. If the variable is
    empty, it means that no face was detected and we do not go further in the function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为输入，我们访问 `pixelBuffer`，它包含设备摄像头的视频流中的像素。我们运行面部检测模型并获得 `faceObservations`。这将包含检测结果。如果该变量为空，则表示未检测到面部，我们将不会进一步处理该函数：
- en: '[PRE7]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, for each `faceObservation` in `faceObservations`, we classify the area
    containing the face:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个 `faceObservation` 在 `faceObservations` 中，我们将对包含面部的区域进行分类：
- en: '[PRE8]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To do so, we specify `regionOfInterest` of the request. This notifies the Core
    ML framework that the input is this specific area of the image. This is very convenient
    as we do not have to crop and resize the image—the framework handles this for
    us. Finally, we call the `classificationHandler.perform` native method.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们指定请求的 `regionOfInterest`。这会通知 Core ML 框架，输入是图像的这个特定区域。这非常方便，因为我们不需要裁剪和调整图像的大小——框架会为我们处理这些。最后，我们调用
    `classificationHandler.perform` 本机方法。
- en: Note that we had to change the coordinate system. The face coordinates are returned
    with an origin at the top left of the image, while `regionOfInterest` must be
    specified with an origin in the bottom left.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要更改坐标系统。面部坐标是以图像的左上角为原点返回的，而 `regionOfInterest` 必须以左下角为原点来指定。
- en: Once the predictions are generated, our custom callback, `processClassifications`,
    will be called with the results. We will then be able to display the results to
    the user. This part is covered in the full application available in the book's
    GitHub repository.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成预测，我们的自定义回调 `processClassifications` 将会被调用并传入结果。然后，我们将能够将结果显示给用户。这部分内容在本书的
    GitHub 仓库中的完整应用程序中有介绍。
- en: Running on Android using TensorFlow Lite
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Android 上使用 TensorFlow Lite 运行
- en: '**TensorFlow Lite** is a mobile framework that allows you to run TensorFlow
    models on mobile and embedded devices. It supports Android, iOS, and Raspberry
    Pi. Unlike Core ML on iOS devices, it is not a native library but an external
    dependency that must be added to your app.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Lite** 是一个移动框架，可以在移动设备和嵌入式设备上运行 TensorFlow 模型。它支持 Android、iOS
    和 Raspberry Pi。与 iOS 设备上的 Core ML 不同，它不是一个本地库，而是一个必须添加到应用中的外部依赖项。'
- en: While Core ML was optimized for iOS device hardware, TensorFlow Lite performance
    may vary from device to device. On some Android devices, it can use the GPU to
    improve inference speed.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Core ML 已为 iOS 设备硬件进行了优化，但 TensorFlow Lite 的性能可能因设备而异。在某些 Android 设备上，它可以使用
    GPU 来提高推理速度。
- en: To use TensorFlow Lite for our example application, we will first convert our
    model to the library's format using the TensorFlow Lite converter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的示例应用中使用 TensorFlow Lite，我们首先需要使用 TensorFlow Lite 转换器将模型转换为该库的格式。
- en: Converting the model from TensorFlow or Keras
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 TensorFlow 或 Keras 转换模型
- en: 'TensorFlow integrates a function to transform a SavedModel model to the TF
    Lite format. To do so, we first create a TensorFlow Lite converter object:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 集成了一个功能，用于将 SavedModel 模型转换为 TF Lite 格式。为此，我们首先创建一个 TensorFlow Lite
    转换器对象：
- en: '[PRE9]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, the model is saved to disk:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将模型保存到磁盘：
- en: '[PRE10]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You will notice that the TensorFlow Lite function offers fewer options than
    the Apple Core ML equivalent. Indeed, TensorFlow Lite does not handle preprocessing
    and resizing of the images automatically. This has to be handled by the developer
    in the Android app.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，TensorFlow Lite的功能提供的选项比Apple Core ML少。确实，TensorFlow Lite不自动处理图像的预处理和调整大小。这些需要在Android应用中由开发者手动处理。
- en: Loading the model
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载模型
- en: 'After converting the model to the `.tflite` format, we can add it to the assets
    folder of our Android application. We can then load the model using a helper function, `loadModelFile`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型转换为`.tflite`格式后，我们可以将其添加到Android应用的assets文件夹中。然后，我们可以使用辅助函数`loadModelFile`加载模型：
- en: '[PRE11]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Because our model is in the assets folder of our app, we need to pass the current
    activity. If you are not familiar with Android app development, you can think
    of an activity as a specific screen of an app.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型在应用的assets文件夹中，我们需要传递当前活动。如果你不熟悉Android应用开发，可以将活动理解为应用中的一个特定屏幕。
- en: 'We can then create `Interpreter`. In TensorFlow Lite, the interpreter is necessary
    to run a model and return predictions. In our example, we pass the default `Options` constructor.
    The `Options` constructor could be used to change the number of threads or the
    precision of the model:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以创建`Interpreter`。在TensorFlow Lite中，解释器用于运行模型并返回预测结果。在我们的示例中，我们传递默认的`Options`构造函数。`Options`构造函数可以用来改变线程数或模型的精度：
- en: '[PRE12]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will create `ByteBuffer`. This is a data structure that contains
    the input image data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将创建`ByteBuffer`。这是一种包含输入图像数据的数据结构：
- en: '[PRE13]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`ByteBuffer` is an array that will contain the pixels of the image. Its size
    depends on the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`ByteBuffer`是一个数组，将包含图像的像素数据。其大小取决于以下因素：'
- en: The batch size—in our case, 1.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批次大小——在我们的例子中为1。
- en: The dimensions of the input image.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像的维度。
- en: The number of channels (`DIM_PIXEL_SIZE`)—3 for RGB, 1 for grayscale.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通道数（`DIM_PIXEL_SIZE`）——RGB为3，灰度为1。
- en: Finally, the number of bytes per channel. As *1 byte = 8 bits*, a 32-bits input
    will require 4 bytes. If using quantization, an 8-bits input will require 1 byte.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，每个通道的字节数。因为*1字节 = 8位*，所以一个32位的输入需要4个字节。如果使用量化，8位输入需要1个字节。
- en: 'To process predictions, we will later fill this `imgData` buffer and pass it
    to the interpreter. Our facial expression detection model is ready to be used.
    Before we can start using our full pipeline, we only need to instantiate the face
    detector:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理预测，我们稍后将填充这个`imgData`缓冲区并传递给解释器。我们的面部表情检测模型已经准备好使用。在开始使用我们的完整流程之前，我们只需要实例化人脸检测器：
- en: '[PRE14]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that this `FaceDetector` class comes from the Google Vision framework and
    has nothing to do with TensorFlow Lite.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个`FaceDetector`类来自Google Vision框架，与TensorFlow Lite无关。
- en: Using the model
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用该模型
- en: 'For our example app, we will work with bitmap images. You can see bitmaps as
    a matrix of raw pixels. They are compatible with most of the image libraries on
    Android. We obtain this bitmap from the view that displays the video feed from
    the camera, called `textureView`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例应用，我们将处理位图图像。你可以将位图视为一个原始像素矩阵。它们与Android上的大多数图像库兼容。我们从显示相机视频流的视图`textureView`获取这个位图：
- en: '[PRE15]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We do not capture the bitmap at full resolution. Instead, we divide its dimensions
    by `4` (this number was picked by trial and error). Choosing a size that's too
    large would result in very slow face detection, reducing the inference time of
    our pipeline.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并没有以全分辨率捕获位图。相反，我们将其尺寸除以`4`（这个数字是通过反复试验选出来的）。选择过大的尺寸会导致人脸检测非常慢，从而增加我们流程的推理时间。
- en: 'We then proceed to create `vision.Frame` from the bitmap. This step is necessary
    to pass the image to `faceDetector`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从位图创建`vision.Frame`。这个步骤是必要的，以便将图像传递给`faceDetector`：
- en: '[PRE16]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, for each `face` in `faces`, we can crop the face of the user in the bitmap.
    Provided in the GitHub repository, the `cropFaceInBitmap` helper function does
    precisely this—it accepts the coordinates of the face and crops the corresponding
    area in the bitmap:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于`faces`中的每个`face`，我们可以在位图中裁剪出用户的面部。在GitHub仓库中提供的`cropFaceInBitmap`辅助函数正是执行此操作——它接受面部坐标并裁剪位图中的相应区域：
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After resizing the bitmap to fit the input of our model, we fill `imgData`,
    `ByteBuffer`, which `Interpreter` accepts:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整位图大小以适应模型输入后，我们填充`imgData`，即`ByteBuffer`，它是`Interpreter`接受的格式：
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can see, we iterate over the bitmap''s pixels to add them to `imgData`.
    To do so, we use `addPixelValue`. This function handles the preprocessing of each
    pixel. It will be different depending on the characteristics of the model. In
    our case, the model is using a grayscale image. We must therefore convert each
    pixel from color to grayscale:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们遍历位图的像素并将其添加到`imgData`中。为此，我们使用`addPixelValue`。这个函数处理每个像素的预处理。它会根据模型的特性有所不同。在我们的案例中，模型使用的是灰度图像。因此，我们必须将每个像素从彩色转换为灰度：
- en: '[PRE19]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this function, we are using bit-wise operations to compute the mean of the
    three colors of each pixel. We then divide it by `127.5` and subtract `1` as this
    is the preprocessing step of our model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，我们使用位运算来计算每个像素的三种颜色的平均值。然后将其除以`127.5`并减去`1`，这是我们模型的预处理步骤。
- en: 'At the end of this process, `imgData` contains the input information in the
    correct format. Finally, we can run the inference:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程的最后，`imgData`包含了正确格式的输入信息。最后，我们可以运行推理：
- en: '[PRE20]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The predictions will be inside `labelProbArray`. We can then process and display
    them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果将存放在`labelProbArray`中。然后我们可以处理并显示它们。
- en: Running in the browser using TensorFlow.js
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在浏览器中使用TensorFlow.js运行
- en: 'With web browsers packing more and more features every year, it was only a
    matter of time before they could run deep learning models. Running models in the
    browser has many advantages:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 随着每年浏览器功能的不断增加，能够运行深度学习模型只是时间问题。在浏览器中运行模型有许多优势：
- en: The user does not have anything to install.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户无需安装任何东西。
- en: The computing is done on the user's machine (mobile or computer).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算在用户的机器上进行（无论是手机还是电脑）。
- en: The model can sometimes make use of the device's GPU.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型有时可以使用设备的GPU。
- en: The library to run in the browser is called TensorFlow.js (refer to the documentation
    at [https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs)).
    We will implement our face expression classification application using it.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中运行的库称为TensorFlow.js（请参考文档：[https://github.com/tensorflow/tfjs](https://github.com/tensorflow/tfjs)）。我们将使用它来实现我们的面部表情分类应用。
- en: While TensorFlow cannot take advantage of non-NVIDIA GPUs, TensorFlow.js can
    use GPUs on almost any device. GPU support in the browser was first implemented
    to display graphical animations through WebGL (a computer graphics API for web
    applications, based on OpenGL). Since it involves matrix calculus, it was then
    repurposed to run deep learning operations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TensorFlow不能利用非NVIDIA的GPU，但TensorFlow.js可以在几乎任何设备上使用GPU。浏览器中的GPU支持最初是为了通过WebGL（一个用于Web应用程序的计算机图形API，基于OpenGL）显示图形动画而实现的。由于它涉及矩阵运算，后来被重新用于运行深度学习操作。
- en: Converting the model to the TensorFlow.js format
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型转换为TensorFlow.js格式
- en: To use TensorFlow.js, the model must first be converted to the correct format
    using `tfjs-converter`. It can convert Keras models, frozen models, and SavedModels.
    Installation instructions are provided in the GitHub repository.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TensorFlow.js，必须先通过`tfjs-converter`将模型转换为正确的格式。它可以转换Keras模型、冻结的模型和SavedModels。安装说明可在GitHub仓库中找到。
- en: 'Then, converting a model is very similar to the process done for TensorFlow
    Lite. Instead of being done in Python, it is done from the command line:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，转换模型的过程与TensorFlow Lite的过程非常相似。不同之处在于，它不是在Python中完成，而是通过命令行完成：
- en: '[PRE21]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Similar to TensorFlow Lite, we need to specify the names of the output nodes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于TensorFlow Lite，我们需要指定输出节点的名称。
- en: 'The output is composed of multiple files:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出由多个文件组成：
- en: '`optimized_model.pb`: Contains the model graph'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimized_model.pb`：包含模型图'
- en: '`weights_manifest.json`: Contains information about the list of weights'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights_manifest.json`：包含权重列表的信息'
- en: '`group1-shard1of5`, `group1-shard2of5`, ..., `group1-shard5of5`: Contains the
    weights of the model split into multiple files'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group1-shard1of5`，`group1-shard2of5`，...，`group1-shard5of5`：包含模型的权重，分成多个文件'
- en: The model is split into multiple files because parallel downloads are usually
    faster.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 模型被分割成多个文件，因为并行下载通常更快。
- en: Using the model
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用该模型
- en: 'In our JavaScript app, after importing TensorFlow.js, we can load our model.
    Note that the following code is in JavaScript. It has a similar syntax to Python:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的JavaScript应用程序中，导入TensorFlow.js后，我们可以加载模型。请注意，以下代码是用JavaScript编写的。它的语法类似于Python：
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will also use a library called `face-api.js` to extract faces:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用一个名为`face-api.js`的库来提取人脸：
- en: '[PRE23]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once both models are loaded, we can start processing images from the user:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦两个模型加载完毕，我们就可以开始处理用户的图像：
- en: '[PRE24]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, we grab a frame from the `video` element displaying the webcam of the
    user. The `face-api.js` library will attempt to detect a face in this frame. If
    it detects a frame, the part of the image containing the frame is extracted and
    fed to our model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从`video`元素中抓取一帧显示用户网络摄像头的图像。`face-api.js`库将尝试检测该帧中的人脸。如果它检测到人脸，则提取图像中包含人脸的部分，并将其输入到我们的模型中。
- en: 'The `predict` function handles the preprocessing of the image and the classification.
    This is what it looks like:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`函数处理图像的预处理和分类。它的具体操作如下：'
- en: '[PRE25]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We first resize the image using `resizeBilinear` and convert it from color to
    grayscale using `mean`. We then preprocess the pixels, normalizing them between
    `-1` and `1`. Finally, we run the data through `model.predict` to get predictions.
    At the end of this pipeline, we end up with predictions that we can display to
    the user.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用`resizeBilinear`调整图像大小，并通过`mean`将其从彩色转换为灰度。然后，我们对像素进行预处理，将其标准化到`-1`到`1`之间。最后，我们将数据传递给`model.predict`进行预测。在这个流程的末端，我们得到了可以展示给用户的预测结果。
- en: Note the use of `tf.tidy`. This is very important because TensorFlow.js creates
    intermediate tensors that might never be removed from memory. Wrapping our operations
    inside `tf.tidy` automatically purges intermediate elements from memory.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 注意使用`tf.tidy`。这一点非常重要，因为TensorFlow.js会创建一些中间张量，而这些张量可能永远不会从内存中清除。将我们的操作包装在`tf.tidy`中可以自动清除内存中的中间元素。
- en: Over the last few years, technology improvements have made new applications
    in the browser possible—image classification, text generation, style transfer,
    and pose estimation are now available to anyone without needing to install anything.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，技术进步使得浏览器中的新应用成为可能——图像分类、文本生成、风格迁移和姿势估计现在对任何人都可以使用，无需安装任何软件。
- en: Running on other devices
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在其他设备上运行
- en: We have covered the conversion of models to run in the browser, and on iOS and
    Android devices. TensorFlow Lite can also run on the Raspberry Pi, a pocket-sized
    computer running Linux.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了如何将模型转换为浏览器、iOS和Android设备上运行的内容。TensorFlow Lite也可以在运行Linux的袖珍计算机——树莓派上运行。
- en: 'Moreover, devices designed specifically to run deep learning models started
    to emerge over the years. Here are a few examples:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，专门为运行深度学习模型设计的设备也在这些年中陆续出现。以下是一些例子：
- en: '**NVIDIA Jetson TX2**: The size of a palm; it is often used for robotics applications.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**英伟达Jetson TX2**：大小相当于手掌，常用于机器人应用。'
- en: '**Google Edge TPU**: A chip designed by Google for IoT applications. It is
    the size of a nail, and is available with a developer kit.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌Edge TPU**：谷歌为物联网应用设计的一款芯片，大小类似于指甲，并提供开发者工具包。'
- en: '**Intel Neural Compute Stick**: The size of a USB flash drive; it can be connected
    to any computer (including the Raspberry Pi) to improve its machine learning capabilities.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**英特尔神经计算棒**：大小类似于USB闪存驱动器；可以连接到任何计算机（包括树莓派），以提高其机器学习能力。'
- en: These devices all focus on maximizing computing power while minimizing power
    consumption. With each generation getting more powerful, the on-device ML field
    is moving extremely quickly, opening new applications every year.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设备都专注于最大化计算能力，同时最小化功耗。随着每一代设备的不断增强，设备端机器学习领域发展迅速，每年都在开启新的应用。
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: 'In this chapter, we covered several topics on performance. First, we learned
    how to properly measure the inference speed of a model, and then we went through
    techniques to reduce inference time: choosing the right hardware and the right
    libraries, optimizing input size, and optimizing post-processing. We covered techniques
    to make a slower model appear, to the user, as if it were processing in real time,
    and to reduce the model size.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了多个性能相关的话题。首先，我们学习了如何正确地测量模型的推理速度，然后我们介绍了减少推理时间的技巧：选择合适的硬件和库、优化输入大小以及优化后处理。我们还介绍了使较慢的模型看起来像是实时处理的技巧，并减少了模型的大小。
- en: Then, we introduced on-device ML, along with its benefits and limitations. We
    learned how to convert TensorFlow and Keras models to a format that's compatible
    with on-device deep learning frameworks. With examples on iOS and Android, and
    in the browser, we covered a wide range of devices. We also introduced some existing
    embedded devices.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们介绍了设备端机器学习（on-device ML），以及其优缺点。我们学习了如何将TensorFlow和Keras模型转换为与设备端深度学习框架兼容的格式。通过iOS、Android和浏览器的实例，我们涵盖了广泛的设备。我们还介绍了一些现有的嵌入式设备。
- en: Throughout this book, we have presented TensorFlow 2 in detail, applying it
    to multiple computer vision tasks. We have covered a variety of state-of-the-art
    solutions, providing both a theoretical background and some practical implementations.
    With this last chapter tackling their deployment, it is now up to you to harness
    the power of TensorFlow 2 and to develop computer vision applications for the
    use cases of your choice!
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们详细介绍了TensorFlow 2，并将其应用于多个计算机视觉任务。我们涵盖了多种先进的解决方案，提供了理论背景和一些实际实现。最后一章涉及模型的部署，现在由你来利用TensorFlow
    2的强大功能，开发适用于你选择的用例的计算机视觉应用！
- en: Questions
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: When measuring a model's inference speed, should you measure with a single image
    or multiple ones?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在衡量模型推理速度时，是应该使用单张图像还是多张图像来进行测量？
- en: Is a model with *float32* weights smaller or larger than one with *float16*
    weights?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 权重为*float32*的模型比权重为*float16*的模型更小还是更大？
- en: On iOS devices, should Core ML or TensorFlow Lite be favored? What about Android
    devices?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在iOS设备上，应该偏向使用Core ML还是TensorFlow Lite？那在Android设备上呢？
- en: What are the benefits and limitations of running a model in the browser?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中运行模型有哪些好处和局限性？
- en: What is the most important requirement for embedded devices running deep learning
    algorithms?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入式设备运行深度学习算法的最重要要求是什么？
