- en: Chapter 4. Using Reinforcement Learning for Predictive Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 使用强化学习进行预测分析
- en: As a human being, we learn from past experiences. We haven't become so charming
    by accident. Years of positive compliments as well as negative criticism have
    all helped shape us into who we are today. We learn how to ride a bike by trying
    out different muscle movements until it just clicks. When you perform actions,
    you're sometimes rewarded immediately. This is all about **reinforcement learning**
    (**RL**).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们从过去的经验中学习。我们并不是偶然变得如此迷人。多年的正面赞美以及负面批评帮助我们塑造了今天的自己。我们通过尝试不同的肌肉动作直到掌握了骑自行车的方法。当你执行某些动作时，某些时候会立刻得到奖励。这就是**强化学习**（**RL**）。
- en: This lesson is all about designing a machine learning system driven by criticisms
    and rewards. We will see how to apply reinforcement learning algorithms for the
    predictive model on real-life datasets.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本课程的核心是设计一个由批评和奖励驱动的机器学习系统。我们将看到如何将强化学习算法应用于实际数据集的预测模型。
- en: 'In a nutshell, the following topics will be covered throughout this lesson:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本课程将涵盖以下主题：
- en: Reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning for predictive analytics
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预测分析的强化学习
- en: Notation, policy, and utility in RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习中的符号、策略和效用
- en: Developing a multiarmed bandit's predictive model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发多臂赌博机的预测模型
- en: Developing a stock price predictive model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发股票价格预测模型
- en: Reinforcement Learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: From a technical perspective, whereas supervised and unsupervised learning appears
    at opposite ends of the spectrum, RL exists somewhere in the middle. It's not
    supervised learning because the training data comes from the algorithm deciding
    between exploration and exploitation. And it's not unsupervised because the algorithm
    receives feedback from the environment. As long as you are in a situation where
    performing an action in a state produces a reward, you can use reinforcement learning
    to discover a good sequence of actions to take the maximum expected rewards.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，监督学习和无监督学习分别位于两端，而强化学习则处于中间。它不是监督学习，因为训练数据来自算法在探索与利用之间的选择。它也不是无监督学习，因为算法会从环境中接收反馈。只要你处于一个在某个状态下执行某个动作会产生奖励的情况，你就可以使用强化学习来发现一系列好的动作，以获得最大期望奖励。
- en: The goal of an RL agent will be to maximize the total reward that it receives
    in the long run. The third main sub element is the `value` function.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习智能体的目标是最大化它在长期内所获得的总奖励。第三个主要子元素是`价值`函数。
- en: While the rewards determine an immediate desirability of the states, the values
    indicate the long-term desirability of states, taking into account the states
    that may follow and the available rewards in these states. The `value` function
    is specified with respect to the chosen policy. During the learning phase, an
    agent tries actions that determine the states with the highest value, because
    these actions will get the best amount of reward in the long run.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励决定了状态的即时可取性，而价值则表示了状态的长期可取性，考虑到可能随之而来的状态以及这些状态中的可用奖励。`价值`函数是根据所选策略来指定的。在学习阶段，智能体会尝试采取能够确定具有最高价值的状态的行动，因为这些行动在长期内将获得最佳奖励。
- en: Reinforcement Learning in Predictive Analytics
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测分析中的强化学习
- en: 'Figure 1 shows a person making decisions to arrive at their destination. Moreover,
    suppose on your drive from home to work you always choose the same route. But
    one day your curiosity takes over and you decide to try a different path in hopes
    for a shorter commute. This dilemma of trying out new routes or sticking to the
    best-known route is an example of exploration versus exploitation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1显示了一个人做决策以到达目的地。此外，假设你从家到工作一直选择相同的路线。但有一天，你的好奇心占了上风，你决定尝试一条不同的路，希望能缩短通勤时间。这种尝试新路线还是坚持最熟悉路线的困境，就是探索与利用的例子：
- en: '![Reinforcement Learning in Predictive Analytics](img/04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![预测分析中的强化学习](img/04_01.jpg)'
- en: 'Figure 1: An agent always try to reach the destination passing through route'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：智能体总是试图通过路线到达目的地
- en: Reinforcement learning techniques are being used in many areas. A general idea
    that is being pursued right now is creating an algorithm that doesn't need anything
    apart from a description of its task. When this kind of performance is achieved,
    it will be applied virtually everywhere.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习技术正在被应用于许多领域。目前一个正在追求的总体目标是创建一个只需要任务描述的算法。一旦这种性能实现，它将几乎在所有地方得到应用。
- en: Notation, Policy, and Utility in RL
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的符号、政策和效用
- en: You may notice that reinforcement learning jargon involves anthropomorphizing
    the algorithm into taking actions in situations to receive rewards. In fact, the
    algorithm is often referred to as an agent that acts with the environment. You
    can just think of it like an intelligent hardware agent sensing with sensors and
    interacting with the environment using its actuators.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，强化学习的术语涉及到将算法人格化，使其在特定情境下采取行动以获得奖励。事实上，算法通常被称为一个代理，它与环境互动。你可以将它看作是一个智能硬件代理，利用传感器感知并通过执行器与环境互动。
- en: 'Therefore, it shouldn''t be a surprise that much of RL theory is applied in
    robotics. Figure 2 demonstrates the interplay between states, actions, and rewards.
    If you start at state **s1**, you can perform action **a1** to obtain a reward
    r (**s1**, **a1**). Actions are represented by arrows, and states are represented
    by circles:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，强化学习理论被广泛应用于机器人技术并不奇怪。图2展示了状态、行动和奖励之间的相互作用。如果你从状态**s1**开始，你可以执行行动**a1**来获得奖励r（**s1**，**a1**）。行动由箭头表示，状态由圆圈表示：
- en: '![Notation, Policy, and Utility in RL](img/04_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![强化学习中的符号、政策和效用](img/04_02.jpg)'
- en: 'Figure 2: An agent is performing an action on a state produces a reward'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个代理在某个状态上执行一个行动产生奖励
- en: A robot performs actions to change between different states. But how does it
    decide which action to take? Well, it's all about using different or a concrete
    policy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人通过执行行动在不同的状态之间转换。但它是如何决定采取哪种行动的呢？这完全依赖于使用不同的或具体的策略。
- en: Policy
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 政策
- en: 'In reinforcement learning lingo, we call the strategy a policy. The goal of
    reinforcement learning is to discover a good strategy. One of the most common
    ways to solve it is by observing the long-term consequences of actions in each
    state. The short-term consequence is easy to calculate: that''s just the reward.
    Although performing an action yields an immediate reward, it''s not always a good
    idea to greedily choose the action with the best reward.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的术语中，我们称这种策略为策略。强化学习的目标是发现一种有效的策略。解决问题的最常见方法之一是观察每个状态下行动的长期后果。短期后果很容易计算：那就是奖励。虽然执行一个行动会立即获得奖励，但贪婪地选择最佳奖励的行动并不总是一个好主意。
- en: 'That''s a lesson in life too because the most immediate best thing to do might
    not always be the most satisfying in the long run. The best possible policy is
    called the optimal policy, and it''s often the holy grail of RL as shown in figure
    3, which shows the optimal action given any state:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是生活中的一课，因为最立即的最佳选择可能并不总是长期来看最令人满意的。最佳的策略被称为最优策略，它通常是强化学习的圣杯，如图3所示，显示了在任何状态下的最优行动：
- en: '![Policy](img/04_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![政策](img/04_03.jpg)'
- en: 'Figure 3: A policy defines an action to be taken in a given state'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：一个策略定义了在给定状态下要采取的行动
- en: We've so far seen one type of policy where the agent always chooses the action
    with the greatest immediate reward, called a greedy policy. Another simple example
    of a policy is arbitrarily choosing an action, called random policy. If you come
    up with a policy to solve a reinforcement learning problem, it's often a good
    idea to double-check that your learned policy performs better than both the random
    and greedy policies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一个策略的例子，其中代理总是选择立即获得最大奖励的行动，这被称为贪婪策略。另一个简单的策略例子是随机选择一个行动，这被称为随机策略。如果你为强化学习问题设计了一个策略，通常建议你再次检查所学到的策略是否优于随机策略和贪婪策略。
- en: In addition, we will also see how to develop another robust policy called policy
    gradients, where a neural network learns a policy for picking actions by adjusting
    its weights through gradient descent using feedback from the environment. We will
    see that although both the approaches are used, policy gradient is more direct
    and optimistic.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将看到如何开发另一个强健的策略，称为策略梯度，其中神经网络通过调整其权重来学习选择行动的策略，方法是通过梯度下降使用来自环境的反馈。我们将看到，尽管这两种方法都在使用，但策略梯度更直接且更为乐观。
- en: Utility
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 效用
- en: 'The long-term reward is called a utility. It turns out that, if we know the
    utility of performing an action at a state, then it''s easy to solve reinforcement
    learning. For example, to decide which action to take, we simply select the action
    that produces the highest utility. However, uncovering these utility values is
    the harder part to be sorted out. The utility of performing an action *a* at a
    state **s** is written as a function *Q(s, a)*, called the utility function that
    predicts the expected immediate reward plus rewards following an optimal policy
    gave the state-action input which is shown in figure 4:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 长期奖励称为效用。事实证明，如果我们知道在某个状态下执行某个行动的效用，那么强化学习就变得容易解决。例如，要决定采取哪个行动，我们只需选择产生最高效用的行动。然而，揭示这些效用值才是更难解决的部分。执行行动*a*在状态**s**下的效用写作一个函数*Q(s,
    a)*，称为效用函数，它预测期望的即时奖励加上遵循最优策略的奖励，该策略根据状态-行动输入计算，如图4所示：
- en: '![Utility](img/04_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Utility](img/04_04.jpg)'
- en: 'Figure 4: Using a utility function'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用效用函数
- en: 'Most reinforcement learning algorithms boil down to just three main steps:
    infer, do, and learn. During the first step, the algorithm selects the best action
    (*a*) given a state (*s*) using the knowledge it has so far. Next, it does the
    action to find out the reward (*r*) as well as the next state (*s''*). Then it
    improves its understanding of the world using the newly acquired knowledge *(s,
    r, a, s'')*. However, this is just a naive way to calculate the utility; you would
    agree on this too.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习算法归结为三个主要步骤：推理、执行和学习。在第一步中，算法使用到目前为止所获得的知识，从给定的状态(*s*)中选择最佳行动(*a*)。接下来，它执行该行动以获得奖励(*r*)以及下一个状态(*s'*)。然后，它使用新获得的知识*(s,
    r, a, s')*来改进对世界的理解。然而，这只是计算效用的一种简单方法，你也会同意这一点。
- en: 'Now, the question is what could be a more robust way to compute it? Here are
    two cents from my side. We can calculate the utility of a particular state-action
    pair *(s, a)* by recursively considering the utilities of future actions. The
    utility of your current action is influenced not just by the immediate reward,
    but also the next best action, as shown in the following formula:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是有什么更稳健的方式来计算它吗？这是我的一些看法。我们可以通过递归地考虑未来行动的效用，来计算特定状态-行动对*(s, a)*的效用。你当前行动的效用不仅受到即时奖励的影响，还受到下一个最佳行动的影响，如下公式所示：
- en: '![Utility](img/04_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Utility](img/04_05.jpg)'
- en: In the previous formula, *s'* denotes the next state, and *a'* denotes the next
    action. The reward of taking action *a* in state *s* is denoted by *r(s, a)*.
    Here, *γ* is a hyperparameter that you get to choose, called the discount factor.
    If *γ* is *0*, then the agent chooses the action that maximizes the immediate
    reward. Higher values of *γ* will make the agent put more importance in considering
    long-term consequences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的公式中，*s'*表示下一个状态，*a'*表示下一个行动。采取行动*a*在状态*s*中的奖励表示为*r(s, a)*。这里，*γ*是一个超参数，你可以选择，称为折扣因子。如果*γ*为*0*，那么代理将选择最大化即时奖励的行动。更高的*γ*值会让代理更重视考虑长期后果。
- en: 'In practice, we have more hyperparameters to be considered. For example, if
    a vacuum cleaner robot is expected to learn to solve tasks quickly but not necessarily
    optimally, we might want to set a faster learning rate. Alternatively, if a robot
    is allowed more time to explore and exploit, we might tune down the learning rate.
    Let''s call the learning rate **α**, and change our utility function as follows
    (note that when *α = 1*, both the equations are identical):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们需要考虑更多的超参数。例如，如果预期吸尘机器人能够快速学习解决任务，但不一定是最优的，我们可能希望设置一个更快的学习率。相反，如果允许机器人有更多时间来探索和利用，我们可能会降低学习率。我们将学习率称为**α**，并按以下方式修改我们的效用函数（注意，当*α
    = 1*时，这两个公式是相同的）：
- en: '![Utility](img/04_06.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![Utility](img/04_06.jpg)'
- en: In summary, an RL problem can be solved if we know this *Q(s, a)* function.
    Here comes the machine learning strategy called neural networks, which are a way
    to approximate functions given enough training data. Also, TensorFlow is the perfect
    tool to deal with neural networks because it comes with many essential algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们知道这个*Q(s, a)*函数，就可以解决强化学习问题。这里出现了一个叫做神经网络的机器学习策略，神经网络是一种通过足够的训练数据来逼近函数的方式。此外，TensorFlow是处理神经网络的完美工具，因为它包含了许多必要的算法。
- en: In the next two sections, we will see two examples of such implementation with
    TensorFlow. The first example is a naïve way of developing a multiarmed bandit
    agent for the predictive model. Then, the second example is a bit more advanced
    using neural network implementation for stock price prediction.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个部分中，我们将看到两个使用 TensorFlow 实现的示例。第一个示例是开发一个多臂强盗智能体的简单方法，用于预测模型。接着，第二个示例使用神经网络实现来进行股票价格预测，稍微复杂一些。
- en: Developing a Multiarmed Bandit's Predictive Model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发多臂强盗的预测模型
- en: One of the simplest RL problems is called n-armed bandits. The thing is there
    are n-many slot machines but each has different fixed payout probability. The
    goal is to maximize the profit by always choosing the machine with the best payout.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的强化学习问题之一被称为 n 臂强盗问题。问题在于，有 n 个老虎机，但每个老虎机的固定奖励概率不同。目标是通过总是选择奖励最好的机器来最大化利润。
- en: As mentioned earlier, we will also see how to use policy gradient that produces
    explicit outputs. For our multiarmed bandits, we don't need to formalize these
    outputs on any particular state. To be simpler, we can design our network such
    that it will consist of just a set of weights that are corresponding to each of
    the possible arms to be pulled in the bandit. Then, we will represent how good
    an agent thinks to pull each arm to make maximum profit. A naive way is to initialize
    these weights to 1 so that the agent will be optimistic about each arm's potential
    reward.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们还将看到如何使用策略梯度来产生显式的输出。对于我们的多臂强盗问题，我们不需要在任何特定状态上正规化这些输出。为了简单起见，我们可以设计我们的网络，使其仅由一组权重组成，这些权重对应于强盗中每个可能的臂。然后，我们将表示智能体认为拉取每个臂以获得最大利润的好坏。一个简单的方法是将这些权重初始化为
    1，这样智能体会对每个臂的潜在奖励持乐观态度。
- en: To update the network, we can try choosing an arm with a greedy policy that
    we discussed earlier. Our policy is such that the agent receives a reward of either
    `1` or `-1` once it has issued an action. I know this is not a realistic imagination
    but most of the time the agent will choose an action randomly that corresponds
    to the largest expected value.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新网络，我们可以尝试选择一个具有贪婪策略的臂，这在前面已经讨论过。我们的策略是，智能体每次执行一个动作后，都会收到 `1` 或 `-1` 的奖励。我知道这不是一个现实的设想，但大多数时候，智能体会选择一个随机的动作，对应于最大的期望值。
- en: We will start developing a simple but effective bandit agent incrementally for
    solving multiarmed bandit problems. At first, there will be no state, that is,
    we will have a stateless agent. Then, we will see that using a stateless bandit
    agent to solve a complex problem is so biased that we cannot use it in real life.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步开发一个简单但有效的强盗智能体，以解决多臂强盗问题。最初，不会有状态，即我们将拥有一个无状态的智能体。然后，我们将看到，使用无状态的强盗智能体来解决复杂问题会出现偏差，以至于我们无法在实际生活中使用它。
- en: 'Then we will increase the agent complexity by adding or converting the sample
    bandits into contextual bandits. The contextual bandits then will be a state full
    agent so can solve our predicting problem more efficiently. Finally, we will further
    increase the agent complexity by converting the textual bandits to full RL agent
    before deploying it:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将通过添加或将样本强盗转换为上下文强盗来增加智能体的复杂度。上下文强盗将成为一个有状态的智能体，从而可以更高效地解决我们的预测问题。最后，我们将通过将文本强盗转换为完整的强化学习智能体，进一步增加智能体的复杂度，直到部署：
- en: Load the required library.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所需的库。
- en: 'Load the required library and packages/modules needed:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加载所需的库和所需的包/模块：
- en: '[PRE0]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining bandits.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义强盗。
- en: 'For this example, I am using a four-armed bandit. The `getBandit` function
    that generates a random number from a normal distribution has the mean of 0\.
    The lower the Bandit number, the more likely a positive reward will be awarded.
    As stated earlier, this is just a naïve but greedy way to train the agent so that
    it learns to choose a bandit that will generate not only the positive but also
    the maximum reward. Here I have listed the bandits so that Bandit 4 most often
    provides a positive reward:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个示例中，我使用的是四臂强盗。`getBandit` 函数从正态分布中生成一个随机数，均值为 0。Bandit 数字越低，获得正奖励的概率越大。如前所述，这只是一种简单但贪婪的方式来训练智能体，使其学会选择一个不仅能产生正奖励，而且能产生最大奖励的强盗。这里，我列出了强盗的列表，其中
    Bandit 4 最常提供正奖励：
- en: '[PRE1]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Developing an agent for the bandits.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为强盗问题开发一个智能体。
- en: 'The following code creates a very simple neural agent consisting of a set of
    values for each of the `bandits`. Each value is estimated to be 1 based on the
    return value from the `bandits`. We use a policy gradient method to update the
    agent by moving the value for the selected action toward the received reward.
    At first, we need to reset the graph as follows:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码创建了一个非常简单的神经代理，它包含一个为每个`bandits`设置的值。每个值根据`bandits`的回报值估算为1。我们使用策略梯度方法通过将所选行动的值朝着获得的奖励方向更新代理。首先，我们需要重置图形，如下所示：
- en: '[PRE2]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, the next two lines do the actual choosing by establishing the feed-forward
    part of the network:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，接下来的两行代码实际完成选择操作，建立网络的前馈部分：
- en: '[PRE3]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, before starting the training process, we need to initiate the training
    process itself. Since we already know the reward, now it''s time to feed them
    and choose an action in the network to compute the loss and use it to update the
    network:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，在开始训练过程之前，我们需要启动训练过程本身。因为我们已经知道奖励，现在是时候将奖励输入并选择网络中的行动来计算损失，并用它来更新网络：
- en: '[PRE4]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We need to define the objective function that is loss:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要定义目标函数，即损失函数：
- en: '[PRE5]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And then let''s make the training process slow to make it exhaustive utilizing
    the learning rate:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后让我们将训练过程放慢，以便更全面地利用学习率：
- en: '[PRE6]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then use the gradient descent `optimizer` and instantiate the `training`
    operation:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后我们使用梯度下降`optimizer`并实例化`training`操作：
- en: '[PRE7]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, it''s time to define the training parameters such as a total number of
    iterations to train the agent, `reward` function, and a `random` action. The reward
    here sets the scoreboard for `bandits` to `0`, and by choosing a random action,
    we set the probability of taking a random action:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，是时候定义训练参数了，比如训练代理的总迭代次数、`reward`函数和一个`random`行动。这里的奖励将`bandits`的计分板设置为`0`，通过选择随机行动，我们设置采取随机行动的概率：
- en: '[PRE8]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we initialize the global variables:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们初始化全局变量：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training the agent.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练代理。
- en: 'We need to train the agent by taking actions to the environment and receiving
    rewards. We start by creating a TensorFlow session and launch the TensorFlow graph.
    Then, iterate the training process up to a total number of iterations. Then, we
    choose either a random act or one from the network. We then compute the reward
    from picking one of the `bandits`. Then, we make the training process consistent
    and update the network. Finally, we update the scoreboard:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要通过采取行动与环境互动并获得奖励来训练代理。我们从创建一个TensorFlow会话并启动TensorFlow图开始。接着，迭代训练过程直到达到总迭代次数。然后，我们选择一个随机行为或从网络中选择一个行为。接下来，我们计算选择一个`bandits`后的奖励。然后，我们使训练过程保持一致并更新网络。最后，我们更新计分板：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s evaluate the above model as follows:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在让我们按如下方式评估上述模型：
- en: '[PRE11]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first iteration generates the following output:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一次迭代生成以下输出：
- en: '[PRE12]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The second iteration generates a different result as follows:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二次迭代生成一个不同的结果，如下所示：
- en: '[PRE13]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that if you see the limitation of this agent being a stateless agent so
    randomly predicts which bandits to choose. In that situation, there are no environmental
    states, and the agent must simply learn to choose which action is best to take.
    To get rid of this problem, we can think of developing contextual bandits.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，如果你看到这个代理是一个无状态的代理，它随机预测选择哪个bandits。在这种情况下，没有环境状态，代理必须简单地学习选择最好的行动。为了解决这个问题，我们可以考虑开发上下文bandits。
- en: Using the contextual bandits, we can introduce and make the proper utilization
    of the state. The state consists of an explanation of the environment that the
    agent can use to make more intelligent and informed actions. The thing is that
    instead of using a single bandit we can chain multiple bandits together. So what
    would be the function of the state? Well, the state of the environment tells the
    agent to choose a bandit from the available list. On the other hand, the goal
    of the agent is to learn the best action for any number of bandits.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用上下文bandits，我们可以引入并合理利用状态。状态由代理可以用来做出更智能、更有信息的行动的环境说明组成。关键是，与其使用单个bandit，我们可以将多个bandits串联在一起。那么，状态的作用是什么呢？环境的状态告诉代理从可用列表中选择一个bandit。另一方面，代理的目标是学习对于任意数量的bandits，最好的行动。
- en: 'This way, the agent faces an issue since each bandit may have different reward
    probabilities for each arm and agent needs to learn how to perform an action on
    the state of the environment. Otherwise, the agent cannot achieve the maximum
    reward possible:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这样，代理会遇到一个问题，因为每个强盗对每个手臂的奖励概率可能不同，而代理需要学会如何在环境状态下执行动作。否则，代理无法实现最大可能的奖励：
- en: '![Developing a Multiarmed Bandit''s Predictive Model](img/04_07.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![开发多臂强盗的预测模型](img/04_07.jpg)'
- en: 'Figure 5: Stateless versus contextual bandits'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5：无状态与上下文强盗
- en: As mentioned earlier, to get rid of this issue, we can build a single-layer
    neural network so that it can take a state and yield an action. Now, similar to
    the random bandits, we can use a policy-gradient update method too so that the
    network update is easier to take actions for maximizing the reward. This simplified
    way of posting an RL problem is referred to as the contextual bandit.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，为了解决这个问题，我们可以构建一个单层神经网络，使其能够接受状态并输出动作。现在，类似于随机强盗，我们也可以使用策略梯度更新方法，使得网络更新更容易采取动作以最大化奖励。这种简化的强化学习问题表示方式被称为上下文强盗。
- en: Developing contextual bandits.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发上下文强盗。
- en: 'This example was adopted and extended based on "Simple Reinforcement Learning
    with TensorFlow Part 1.5: Contextual Bandits" By Arthur Juliani published at [https://medium.com/](https://medium.com/).'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本例基于 Arthur Juliani 发表在 [https://medium.com/](https://medium.com/) 的《简单强化学习与
    TensorFlow 第1.5部分：上下文强盗》进行了改编和扩展。
- en: At first, let's define our contextual bandits. For this example, we will see
    how to use three four-armed bandits, that is, each Bandit has four arms that can
    be pulled to make an action. Since each bandit is contextual and has a state,
    so their arms have different success probabilities. This requires different actions
    to be performed to yield the best predictive result.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们的上下文强盗。在这个例子中，我们将看到如何使用三个四臂强盗，也就是说，每个强盗有四个手臂可以拉动以执行动作。由于每个强盗都是上下文相关的并且有一个状态，因此它们的手臂有不同的成功概率。这需要执行不同的动作来获得最佳的预测结果。
- en: 'Here, we define a class named `contextual_bandit()` consisting of a constructor
    and two user defined functions: `getBandit()` and `pullArm()`. The `getBandit()`
    function generates a random number from a normal distribution with a mean of `0`.
    The lower the Bandit number, the more likely a positive reward will be returned
    to be utilized. We want our agent to learn to choose the banditarm that will most
    often give a positive reward. Of course, it depends on the bandit presented. This
    constructor lists out all of our bandits. We assume the current state being armed
    `4`, `2`, `3`, and `1` that is the most optimal respectively.'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个名为 `contextual_bandit()` 的类，其中包含一个构造函数和两个用户定义的函数：`getBandit()` 和
    `pullArm()`。`getBandit()` 函数从均值为 `0` 的正态分布中生成一个随机数。Bandit 数字越低，返回正奖励的可能性越大。我们希望我们的代理学会选择最常返回正奖励的强盗手臂。当然，这取决于呈现的强盗。该构造函数列出了我们所有的强盗。我们假设当前状态为武装
    `4`、`2`、`3` 和 `1`，这些是最优的选择。
- en: 'Also, if you see carefully, most of the reinforcement learning algorithms follow
    similar implementation patterns. Thus, it''s a good idea to create a class with
    the relevant methods to reference later, such as an abstract class or interface:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，如果你仔细观察，大多数强化学习算法遵循类似的实现模式。因此，创建一个包含相关方法的类是个好主意，以便以后引用，例如抽象类或接口：
- en: '[PRE14]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Developing a policy-based agent.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发基于策略的代理。
- en: The following class `ContextualAgent` helps develop our simple, but very effective
    neural and contextual agent. We supply the current state as input and it then
    returns an action that is conditioned on the state of the environment. This is
    the most important step toward making a stateless agent a stateful one to be able
    to solve a full RL problem.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下类 `ContextualAgent` 帮助开发我们的简单但非常有效的神经和上下文代理。我们提供当前状态作为输入，代理会根据环境的状态返回一个动作。这是将无状态代理转变为有状态代理并能够解决完整强化学习问题的最重要一步。
- en: 'Here, I tried to develop this agent such that it uses a single set of weights
    for choosing a particular arm given a bandit. The policy gradient method is used
    to update the agent by moving the value for a particular action toward achieving
    maximum reward:'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我尝试开发一个代理，使其在给定强盗的情况下使用一组权重来选择特定的手臂。使用策略梯度方法通过将特定动作的值朝最大奖励的方向移动来更新代理：
- en: '[PRE15]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Training the contextual bandit agent.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练上下文强盗代理。
- en: 'At first, we clear the default TensorFlow graph:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们清除默认的 TensorFlow 图：
- en: '[PRE16]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we define some parameters that will be used to train the agent:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们定义一些参数，供训练代理时使用：
- en: '[PRE17]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, before starting the training, we need to load the bandits and then our
    agent:'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，在开始训练之前，我们需要加载强盗，并接着加载我们的代理：
- en: '[PRE18]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, to maximize the objective function toward total rewards, `weights` is
    used to evaluate to look into the network. We also set the scoreboard for `bandits`
    to `0` initially:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，为了最大化目标函数对总回报的影响，我们使用`weights`来评估网络。我们还将`bandits`的记分板初始设置为`0`：
- en: '[PRE19]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we initialize all the variables using `global_variables_initializer()function`:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们使用`global_variables_initializer()`函数初始化所有变量：
- en: '[PRE20]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we will start the training. The training is similar to the random
    one we have done in the preceding example. However, here the main objective of
    the training is to compute the mean reward for each of the bandits so that we
    can evaluate the agent''s prediction accuracy later on by utilizing them:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们将开始训练。这个训练与我们在前面示例中进行的随机训练类似。然而，在这里，训练的主要目标是计算每个强盗的平均回报，以便我们稍后可以通过利用它们来评估代理的预测准确性：
- en: '[PRE21]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Evaluating the agent.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估代理。
- en: 'Now, that we have the mean reward for all the four bandits, it''s time to utilize
    them to predict something interesting, that is, which bandit''s arm will maximize
    the reward. Well, at first we can initialize some variables to estimate the prediction
    accuracy as well:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们已经有了所有四个强盗的平均回报，是时候利用它们来预测一些有趣的事情，也就是哪个强盗的臂将最大化回报。首先，我们可以初始化一些变量来估计预测准确性：
- en: '[PRE22]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then let''s start evaluating the agent''s prediction performance:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后让我们开始评估代理的预测性能：
- en: '[PRE23]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As you can see, all the predictions made are right predictions. Now we can
    compute the accuracy as follows:'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，所有的预测都是正确的预测。现在我们可以按如下方式计算准确率：
- en: '[PRE24]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Fantastic, well done! We have managed to design and develop a more robust bandit
    agent by means of a contextual agent that can accurately predict which arm, that
    is, the action of a bandit that would help to achieve the maximum reward, that
    is, profit.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，做得好！我们通过一个上下文代理成功设计并开发了一个更强大的强盗代理，能够准确预测哪个臂，即哪个行动的强盗能够帮助实现最大回报，也就是利润。
- en: In the next section, we will see another interesting but very useful application
    for stock price prediction, where we will see how to develop a policy-based Q
    Learning agent out of the box of the RL.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到另一个非常有趣但又非常实用的股票价格预测应用，我们将展示如何从强化学习的框架中开发一个基于策略的Q学习代理。
- en: Developing a Stock Price Predictive Model
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发股票价格预测模型
- en: 'An emerging area for applying is the stock market trading, where a trader acts
    like a reinforcement agent since buying and selling (that is, action) particular
    stock changes the state of the trader by generating profit or loss, that is, reward.
    The following figure shows some of the most active stocks on July 15, 2017 (for
    an example):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个新兴的应用领域是股市交易，其中交易员像一个强化学习代理，因为买卖（即行动）特定股票会通过产生利润或亏损来改变交易员的状态，也就是回报。下图展示了2017年7月15日一些最活跃的股票（例如）：
- en: '![Developing a Stock Price Predictive Model](img/04_08.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_08.jpg)'
- en: 'Figure 6: [https://finance.yahoo.com/](https://finance.yahoo.com/)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: [https://finance.yahoo.com/](https://finance.yahoo.com/)'
- en: Now, we want to develop an intelligent agent that will predict stock prices
    such that a trader will buy at a low price and sell at a high price. However,
    this type of prediction is not so easy and is dependent on several parameters
    such as the current number of stocks, recent historical prices, and most importantly,
    on the available budget to be invested for buying and selling.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望开发一个智能代理，预测股票价格，以便交易员能够以低价买入并以高价卖出。然而，这种预测并不容易，它依赖于多个参数，比如当前的股票数量、最近的历史价格，以及最重要的，买卖时可用的投资预算。
- en: 'The states in this situation are a vector containing information about the
    current budget, current number of stocks, and a recent history of stock prices
    (the last 200 stock prices). So each state is a 202-dimensional vector. For simplicity,
    there are only three actions to be performed by a stock market agent: buy, sell,
    and hold.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，状态是一个包含当前预算、当前股票数量和最近的股价历史（最近200个股价）信息的向量。所以每个状态是一个202维的向量。为了简化，股市代理需要执行的动作只有三种：买入、卖出和持有。
- en: 'So, we have the state and action, what else do you need? Policy, right? Yes,
    we should have a good policy, so based on that an action will be performed in
    a state. A simple policy can consist of the following rules:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经有了状态和动作，那还缺什么？策略，对吧？是的，我们应该有一个好的策略，这样基于该策略，才能在某个状态下执行动作。一个简单的策略可以包含以下规则：
- en: Buying (that is, action) a stock at the current stock price (that is, state)
    decreases the budget while incrementing the current stock count
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在当前股价（即状态）下购买（即动作）股票会减少预算，同时增加当前股票的数量
- en: Selling a stock trades it in for money at the current share price
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卖出股票则按当前股价将股票换成现金
- en: Holding does neither, and performing this action simply waits for a particular
    time period and yields no reward
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持有既不减少预算也不增加股票数量，执行该动作只是等待一个特定的时间段，并且不会获得任何奖励
- en: 'To find the stock prices, we can use the `yahoo_finance` library in Python.
    A general warning you might experience is "**HTTPError: HTTP Error 400: Bad Request**".
    But keep trying.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '要查找股票价格，我们可以使用Python中的 `yahoo_finance` 库。你可能会遇到一个常见的警告：“**HTTPError: HTTP Error
    400: Bad Request**”。但请继续尝试。'
- en: 'Now, let''s try to get familiar with this module:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试熟悉这个模块：
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: So as of July 14, 2017, the stock price of Microsoft Inc. went higher, from
    72.24 to 72.78, which means about a 7.5% increase. However, this small and just
    one-day data doesn't give us any significant information. But, at least we got
    to know the present state for this particular stock or instrument.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 所以截至2017年7月14日，微软公司的股价从72.24上涨至72.78，这意味着大约7.5%的涨幅。然而，这一小段仅一天的数据并未给我们任何显著的信息。但至少我们了解了这只特定股票或工具的当前状态。
- en: 'To install `yahoo_finance`, issue the following command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `yahoo_finance`，请执行以下命令：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now it would be worth looking at the historical data. The following function
    helps us get the historical data for Microsoft Inc:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，值得查看一下历史数据。以下函数帮助我们获取微软公司的历史数据：
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `get_prices()` method takes several parameters such as the share symbol
    of an instrument in the stock market, the opening date, and the end date. You
    will also like to specify and cache the historical data to avoid repeated downloading.
    Once you have downloaded the data, it's time to plot the data to get some insights.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_prices()` 方法接受几个参数，如股市中某个工具的股票符号、开始日期和结束日期。你还可以指定并缓存历史数据，以避免重复下载。一旦下载了数据，就可以绘制图表以获得一些洞察。'
- en: 'The following function helps us to plot the price:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数帮助我们绘制价格：
- en: '[PRE28]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we can call these two functions by specifying a real argument as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过指定实际参数来调用这两个函数，如下所示：
- en: '[PRE29]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here I have chosen a wide range for the historical data of 17 years to get
    a better insight. Now, let''s take a look at the output of this data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我选择了17年的广泛历史数据，以获得更好的洞察。现在，让我们来看看这些数据的输出：
- en: '![Developing a Stock Price Predictive Model](img/04_09.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_09.jpg)'
- en: 'Figure 7: Historical stock price data of Microsoft Inc. from 2000 to 2017'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：微软公司2000年至2017年的历史股价数据
- en: 'The goal is to learn a policy that gains the maximum net worth from trading
    in the stock market. So what will a trading agent be achieving in the end? Figure
    8 gives you some clue:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习一种策略，从股票市场交易中获得最大净资产。那么，最终交易代理会实现什么目标呢？图8给了你一些线索：
- en: '![Developing a Stock Price Predictive Model](img/04_10.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_10.jpg)'
- en: 'Figure 8: Some insight and a clue that shows, based on the current price, up
    to $160 profit can be made'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：一些洞察和线索，显示根据当前价格，最多可以获得$160的利润
- en: Well, figure 8 shows that if the agent buys a certain instrument with price
    $20 and sells at a peak price say at $180, it will be able to make $160 reward,
    that is, profit.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，图8显示，如果代理以$20购买某个工具，并在峰值价格如$180时卖出，它将能够获得$160的奖励，也就是利润。
- en: So, implementing such an intelligent agent using RL algorithms is a cool idea?
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，使用强化学习算法实现这样一个智能代理是个不错的主意吗？
- en: 'From the previous example, we have seen that for a successful RL agent, we
    need two operations well defined, which are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的例子中，我们已经看到，要实现一个成功的强化学习代理，我们需要定义两个操作，分别如下：
- en: How to select an action
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择一个动作
- en: How to improve the utility Q-function
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何提高效用 Q 函数
- en: To be more specific, given a state, the decision policy will calculate the next
    action to take. On the other hand, improve Q-function from a new experience of
    taking an action.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，给定一个状态，决策策略将计算出下一步的动作。另一方面，通过从执行某个动作中获得的新经验来改进Q函数。
- en: 'Also, most reinforcement learning algorithms boil down to just three main steps:
    infer, perform, and learn. During the first step, the algorithm selects the best
    action (*a*) given a state (*s*) using the knowledge it has so far. Next, it performs
    the action to find out the reward (*r*) as well as the next state (*s''*).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大多数强化学习算法归结为三个主要步骤：推理、执行和学习。在第一步中，算法根据当前状态(*s*)使用迄今为止所获得的知识选择最佳行动(*a*)。接下来，它执行该行动以找到奖励(*r*)以及下一个状态(*s'*)。
- en: 'Then, it improves its understanding of the world using the newly acquired knowledge
    *(s, r, a, s'')* as shown in the following figure:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它使用新获得的知识*(s, r, a, s')*来改善对世界的理解，如下图所示：
- en: '![Developing a Stock Price Predictive Model](img/04_11.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_11.jpg)'
- en: 'Figure 9: Steps to be performed for implementing an intelligent stock price
    prediction agent'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：实现智能股票价格预测代理的步骤
- en: Now, let's start implementing the decision policy based on which action will
    be taken for buying, selling, or holding a stock item. Again, we will do it an
    incremental way. At first, we will create a random decision policy and evaluate
    the agent's performance.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始实现决策策略，基于该策略采取购买、卖出或持有股票的行动。同样，我们将以增量方式进行。首先，我们将创建一个随机决策策略并评估代理的性能。
- en: 'But before that, let''s create an abstract class so that we can implement it
    accordingly:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，让我们创建一个抽象类，以便我们可以相应地实现它：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The next task that can be performed is to inherit from this superclass to implement
    a random decision policy:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的任务是从这个父类继承，实施一个随机决策策略：
- en: '[PRE31]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The previous class did nothing except defining a function named `select_action
    ()`, which will randomly pick an action without even looking at the state.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个类什么也没做，只定义了一个名为`select_action()`的函数，它将随机选择一个行动，甚至不查看状态。
- en: 'Now, if you would like to use this policy, you can run it on the real-world
    stock price data. This function takes care of exploration and exploitation at
    each interval of time, as shown in the following figure that form states S1, S2,
    and S3\. The policy suggests an action to be taken, which we may either choose
    to exploit or otherwise randomly explore another action. As we get rewards for
    performing an action, we can update the policy function over time:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想使用这个策略，可以将其应用于现实世界的股价数据。此功能在每个时间间隔处理探索与开发，正如下面的图所示，形成状态S1、S2和S3。该策略建议采取某种行动，我们可以选择执行该行动或随机探索另一个行动。随着我们为执行某个行动而获得奖励，我们可以随着时间推移更新策略函数：
- en: '![Developing a Stock Price Predictive Model](img/04_12.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_12.jpg)'
- en: 'Figure 10: A rolling window of some size iterates through the stock prices
    over time'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：一个滚动窗口以某种大小在股票价格上迭代
- en: 'Fantastic, so we have the policy and now it''s time to utilize this policy
    to make decisions and return the performance. Now, imagine a real scenario—suppose
    you''re trading on Forex or ForTrade platform, then you can recall that you also
    need to compute the portfolio and the current profit or loss, that is, reward.
    Typically, these can be calculated as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，现在我们有了策略，接下来就是利用这个策略做决策并返回性能指标。现在，假设一个实际场景——假设你在外汇或ForTrade平台上进行交易，那么你也需要计算投资组合和当前的利润或损失，即奖励。通常，这些可以按以下方式计算：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'At first, we can initialize values that depend on computing the net worth of
    a portfolio, where the state is a `hist+2`dimensional vector. In our case, it
    would be 202 dimensional. Then we define the range of tuning the range up to:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，我们可以初始化依赖于计算投资组合净资产的值，其中状态是一个`hist+2`维向量。在我们的例子中，它将是202维的。然后我们定义调节范围的范围，直到：
- en: Length of the prices selected by the user query – (history + 1), since we start
    from 0, we subtract 1 instead. Then, we should calculate the updated value of
    the portfolio and from the portfolio, we can calculate the value of the reward,
    that is, profit.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 用户查询所选价格的长度 - （历史 + 1），由于我们从0开始，因此应该减去1。接下来，我们应该计算投资组合的更新值，并从投资组合中计算奖励值，即利润。
- en: 'Also, we have already defined our random policy, so we can then select an action
    from the current policy. Then, we repeatedly update the portfolio values based
    on the action in each iteration and the new portfolio value after taking the action
    can be calculated. Then, we need to compute the reward from taking an action at
    a state. Nevertheless, we also need to update the policy after experiencing a
    new action. Finally, we compute the final portfolio worth:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们已经定义了随机策略，因此可以从当前策略中选择一个动作。然后，我们在每次迭代中根据动作更新投资组合的值，并可以计算出采取该动作后的新投资组合值。接着，我们需要计算在某个状态下采取行动所获得的奖励。然而，我们还需要在经历新动作后更新策略。最后，我们计算最终的投资组合价值：
- en: '[PRE33]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The previous simulation predicts a somewhat good result; however, it produces
    random results too often. Thus, to obtain a more robust measurement of success,
    let''s run the simulation a couple of times and average the results. Doing so
    may take a while to complete, say 100 times, but the results will be more reliable:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的仿真预测出了一个相对不错的结果；然而，它也经常产生随机结果。因此，为了获得更可靠的成功度量，让我们多次运行仿真并对结果进行平均。这样做可能需要一段时间，比如
    100 次，但结果会更可靠：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The previous function computes the average portfolio and the standard deviation
    by iterating the previous simulation function 100 times. Now, it''s time to evaluate
    the previous agent. As already stated, there will be three possible actions to
    be taken by the stock trading agent such as buy, sell, and hold. We have a state
    vector of 202 dimension and budget only $`1000`. Then, the evaluation goes as
    follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的函数通过迭代先前的仿真函数 100 次来计算平均投资组合和标准差。现在，是时候评估之前的代理了。正如之前所说，股票交易代理将采取三种可能的行动，如买入、卖出和持有。我们有一个
    202 维的状态向量和仅 $`1000` 的预算。接下来，评估过程如下：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The first one is the mean and the second one is the standard deviation of the
    final portfolio. So, our stock prediction agent predicts that as a trader you/we
    could make a profit about $513\. Not bad. However, the problem is that since we
    have utilized a random decision policy, the result is not so reliable. To be more
    specific, the second execution will definitely produce a different result:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是均值，第二个是最终投资组合的标准差。所以，我们的股票预测代理预测作为交易者的你/我们可以赚取大约 $513。不错。然而，问题在于，由于我们使用了随机决策策略，结果并不是很可靠。更具体地说，第二次执行肯定会产生不同的结果：
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Therefore, we should develop a more robust decision policy. Here comes the
    use of neural network-based QLearning for decision policy. Next, we will see a
    new hyperparameter epsilon to keep the solution from getting stuck when applying
    the same action over and over. The lesser its value, the more often it will randomly
    explore new actions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们应该开发一个更稳健的决策策略。这里就需要使用基于神经网络的 QLearning 来进行决策策略的改进。接下来，我们将看到一个新的超参数 epsilon，它可以防止在重复应用相同的动作时解决方案陷入困境。它的值越小，随机探索新动作的频率就越高：
- en: '![Developing a Stock Price Predictive Model](img/04_13.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![开发股票价格预测模型](img/04_13.jpg)'
- en: 'Figure 11: The input is the state space vector with three outputs, one for
    each output''s Q-value'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：输入是具有三个输出的状态空间向量，每个输出对应一个 Q 值
- en: 'Next, I am going to write a class containing their functions:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将编写一个包含其功能的类：
- en: '`Constructor`: This helps to set the hyperparameters from the Q-function. It
    also helps to set the number of hidden nodes in the neural networks. Once we have
    these two, it helps to define the input and output tensors. It then defines the
    structure of the neural network. Further, it defines the operations to compute
    the utility. Then, it uses an optimizer to update model parameters to minimize
    the loss and sets up the session and initializes variables.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Constructor`：该函数帮助从 Q 函数设置超参数。它还帮助设置神经网络中隐藏节点的数量。一旦我们有了这两个参数，它有助于定义输入和输出张量。然后，它定义了神经网络的结构。此外，它定义了计算效用的操作。接着，它使用优化器来更新模型参数，以最小化损失，并设置会话和初始化变量。'
- en: '`select_action`: This function exploits the best option with probability 1-epsilon.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select_action`：该函数以 1-epsilon 的概率利用最佳选择。'
- en: '`update_q`: This updates the Q-function by updating its model parameters.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_q`：该函数通过更新模型参数来更新 Q 函数。'
- en: '[PRE37]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Refer to the following code:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下代码：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this lesson, we have discussed a wonderful field of machine learning called
    reinforcement learning with TensorFlow. We have discussed it from the theoretical
    as well as practical point of view. Reinforcement learning is the natural tool
    when a problem can be framed by states that change due to actions that can be
    taken by an agent to discover rewards. There are three primary steps in implementing
    the algorithm: infer the best action from the current state, perform the action,
    and learn from the results.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本课中，我们讨论了一个叫做强化学习的机器学习领域，使用了TensorFlow。我们从理论和实践的角度都进行了讨论。强化学习是解决当一个问题可以通过状态来框定，而这些状态因代理采取的行动而变化，并且这些行动会带来奖励的自然工具。实现该算法有三个主要步骤：从当前状态推断出最佳行动，执行该行动，并从结果中学习。
- en: We have seen how to implement RL agents for making predictions by knowing the
    `action`, `state`, `policy`, and `utility` functions. We have seen how to develop
    RL-based agents using random policy as well as neural network-based QLearning
    policy. QLearning is an approach to solve reinforcement learning, where you develop
    an algorithm to approximate the utility function (`Q`-function). Once a good enough
    approximation is found, you can start inferring best actions to take from each
    state. In particular, we have seen two step-by-step examples that show how we
    could develop a multiarmed bandit agent and a stock price prediction agent with
    very good accuracy. But, be advised that the actual stock market is a much more
    complicated beast, and the techniques used in this lesson generalize too many
    situations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何通过了解`action`、`state`、`policy`和`utility`函数来实现RL代理进行预测。我们已经看到了如何使用随机策略以及基于神经网络的QLearning策略来开发基于RL的代理。QLearning是一种解决强化学习问题的方法，它通过开发一个算法来近似效用函数（`Q`-函数）。一旦找到足够好的近似值，就可以开始推断从每个状态应该采取的最佳行动。特别地，我们已经看到了两个逐步的例子，展示了我们如何开发一个多臂赌博机代理和一个股价预测代理，并且取得了很好的准确性。但请注意，实际的股市要复杂得多，本课中使用的技术适用于许多情况。
- en: This is more or less the end of our little journey with TensorFlow. I hope you'd
    a smooth journey and gained a lot of knowledge on TensorFlow.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这大致是我们与TensorFlow的小旅程的结束。我希望你有一个顺利的旅程，并且学到了很多关于TensorFlow的知识。
- en: I wish you all the best for your future projects. Keep learning and exploring!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你未来的项目一切顺利，继续学习和探索！
- en: Assessments
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: In reinforcement learning lingo, we call the ______ a policy.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在强化学习术语中，我们称 ______ 为策略。
- en: 'State whether the following statement is True or False: The goal of reinforcement
    learning is to discover a good strategy. One of the most common ways to solve
    it is by observing the long-term consequences of actions in each state.'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错：强化学习的目标是发现一种好的策略。解决这个问题的最常见方法之一是通过观察在每个状态下行动的长期后果。
- en: We need to train the agent by taking actions to the environment and receiving
    ______.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要通过让代理采取行动并从环境中接收 ______ 来训练代理。
- en: 'State whether the following statement is True or False: Using the contextual
    bandits, we cannot introduce and make the proper utilization of the state.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断以下陈述是对还是错：使用上下文赌博者，我们不能引入并正确利用状态。
- en: To find the stock prices, we can use the _______ library in Python.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找股票价格，我们可以在Python中使用 _______ 库。
- en: get_prices
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: get_prices
- en: plot_prices
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: plot_prices
- en: yahoo_finance
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: yahoo_finance
- en: finance_yahoo
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: finance_yahoo
