- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'This chapter introduces **Reinforcement Learning** (**RL**)—the least explored
    and yet most promising learning paradigm. Reinforcement learning is very different
    from the supervised and unsupervised learning models we covered in earlier chapters.
    Starting from a clean slate (that is, having no prior information), the RL agent
    can go through multiple stages of trial and error, and learn to achieve a goal,
    all the while the only input being the feedback from the environment. The research
    in RL by OpenAI seems to suggest that continuous competition can be a cause for
    the evolution of intelligence. Many deep learning practitioners believe that RL
    will play an important role in the big AI dream: **Artificial General Intelligence**
    (**AGI**). This chapter will delve into different RL algorithms. The following
    topics will be covered:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了**强化学习**（**RL**）——一种最不被探索但最有前景的学习范式。强化学习与我们在之前章节中讨论的监督学习和无监督学习模型大不相同。从一张白纸开始（也就是没有先前的信息），RL
    代理可以通过多次试错的阶段，并学会实现一个目标，在此过程中唯一的输入是来自环境的反馈。OpenAI 在强化学习方面的研究似乎表明，持续的竞争可能是智力进化的一个原因。许多深度学习从业者认为，RL
    将在大规模人工智能梦想中扮演重要角色：**人工通用智能**（**AGI**）。本章将深入探讨不同的 RL 算法。以下主题将被涵盖：
- en: What RL is and its lingo
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 RL 及其术语
- en: Learn how to use the OpenAI Gym interface
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用 OpenAI Gym 接口
- en: Applications of RL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL 的应用
- en: Deep Q-Networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 Q 网络
- en: Policy gradients
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp11](https://packt.link/dltfchp11).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp11](https://packt.link/dltfchp11)找到。
- en: An introduction to RL
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RL 简介
- en: 'What is common between a baby learning to walk, birds learning to fly, and
    an RL agent learning to play an Atari game? Well, all three involve:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 学习走路的婴儿、学习飞行的鸟和学习玩 Atari 游戏的强化学习（RL）代理之间有什么共同点？嗯，三者都有：
- en: '**Trial and error**: The child (or the bird) tries various ways, fails many
    times, and succeeds in some ways before it can really walk (or fly). The RL agent
    plays many games, winning some and losing many, before it can become reliably
    successful.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**试错法**：孩子（或鸟）尝试多种方式，失败多次，最终在某些方式上成功，才能真正学会走路（或飞行）。RL 代理玩许多游戏，赢得一些，输掉许多，直到它能够可靠地成功。'
- en: '**Goal**: The child has the goal to walk, the bird to fly, and the RL agent
    to win the game.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标**：孩子的目标是走路，鸟的目标是飞行，而RL代理的目标是赢得游戏。'
- en: '**Interaction with the environment**: The only feedback they have is from their
    environment.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与环境的互动**：它们唯一的反馈来自于环境。'
- en: So, the first questions that arise are what is RL, and how is it different from
    supervised and unsupervised learning? Anyone who owns a pet knows that the best
    strategy to train a pet is rewarding it for desirable behavior and disciplining
    it for bad behavior. RL, also called **learning with a critic**, is a learning
    paradigm where the agent learns in the same manner. The agent here corresponds
    to our network (program); it can perform a set of **actions** (**a**), which brings
    about a change in the **state** (**s**) of the environment, and, in turn, the
    agent receives a reward or punishment from the environment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，首先产生的问题是，什么是 RL，它与监督学习和无监督学习有什么区别？任何养宠物的人都知道，训练宠物的最佳策略是奖励它的良好行为，并惩罚它的不良行为。RL，也叫做**带有评论员的学习**，是一种学习范式，在这种范式中，代理以相同的方式进行学习。这里的代理对应于我们的网络（程序）；它可以执行一组**动作**（**a**），这会导致环境的**状态**（**s**）发生变化，进而代理从环境中获得奖励或惩罚。
- en: 'For example, consider the case of training a dog to fetch a ball: here, the
    dog is our agent, the voluntary muscle movements that the dog makes are the actions,
    and the ground (as well as the person and ball) is the environment; the dog perceives
    our reaction to its action in terms of giving it a treat as a reward. RL can be
    defined as a computational approach to goal-directed learning and decision making,
    from interaction with the environment, under some idealized conditions. The agent
    can sense the state of the environment, and the agent can perform specific well-defined
    actions on the environment. This causes two things: first, a change in the state
    of the environment, and second, a reward is generated (under ideal conditions).
    This cycle continues, and in theory the agent learns how to more frequently generate
    a reward over time:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑训练一只狗去捡球的情况：在这里，狗是我们的智能体，狗做出的自愿肌肉运动是行动，而地面（以及人和球）是环境；狗通过给予它奖励的反应来感知我们的行动。强化学习（RL）可以定义为一种通过与环境互动，在一些理想化条件下，进行目标导向学习和决策的计算方法。智能体可以感知环境的状态，并且可以对环境执行特定的、明确定义的动作。这会导致两件事：首先，环境的状态发生变化；其次，生成奖励（在理想条件下）。这个循环持续进行，理论上智能体随着时间的推移学习如何更频繁地生成奖励：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B18331_11_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图，描述自动生成的图片，信心较高](img/B18331_11_01.png)'
- en: 'Figure 11.1: Reinforcement learning: interaction between agent and environment'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：强化学习：智能体与环境的互动
- en: Unlike supervised learning, the agent is not presented with any training examples;
    it does not know what the correct action is.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，智能体没有被提供任何训练示例；它不知道正确的行动是什么。
- en: And unlike unsupervised learning, the agent’s goal is not to find some inherent
    structure in the input (the learning may find some structure, but that isn’t the
    goal); instead, its only goal is to maximize the rewards (in the long run) and
    reduce the punishments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与无监督学习不同，智能体的目标不是在输入中找到某种内在的结构（虽然学习过程可能会发现一些结构，但那并不是目标）；相反，它唯一的目标是最大化奖励（从长远来看）并减少惩罚。
- en: RL lingo
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习术语
- en: 'Before learning various RL algorithms, it is important we understand a few
    important terms. We will illustrate the terms with the help of two examples, first
    a robot in a maze, and second an agent controlling the wheels of a **Self-Driving
    Car** (**SDC**). The two RL agents are shown as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习各种强化学习算法之前，了解一些重要术语非常重要。我们将通过两个例子来说明这些术语，第一个是迷宫中的机器人，第二个是控制**自动驾驶汽车**（**SDC**）轮子的智能体。两个强化学习智能体如下所示：
- en: '![A picture containing diagram  Description automatically generated](img/B18331_11_02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片，描述自动生成的图片](img/B18331_11_02.png)'
- en: 'Figure 11.2: State for a robot trying to find a path in a maze (LHS). State
    for an agent trying to control the steering wheel of a self-driving car (RHS)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：机器人在迷宫中寻找路径的状态（左侧）。智能体控制自动驾驶汽车方向盘的状态（右侧）
- en: '*Figure 11.2* shows the two examples we will be considering. Let us start with
    the terms:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.2* 显示了我们将要考虑的两个例子。让我们从术语开始：'
- en: '**State**, *S*: State is the set of tokens (or representations) that can define
    all of the possible states the environment can be in. It can be continuous or
    discrete. In the case of the robot finding its path through a maze, the state
    can be represented by a 4×4 matrix, with elements indicating whether that block
    is empty, occupied, or blocked. A block with a value of 1 means it is occupied
    by the robot, 0 means it is empty, and *X* represents that the block is impassable.
    Each element in this array, *S*, can have one of these three discrete values,
    so the state is discrete in nature. Next, consider the agent controlling the steering
    wheel of a self-driving car. The agent takes as input the front-view image. The
    image contains continuous valued pixels, so here the state is continuous.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**，*S*：状态是一组可以定义环境可能处于的所有状态的标记（或表示）。它可以是连续的，也可以是离散的。在机器人通过迷宫寻找路径的例子中，状态可以用一个
    4×4 的矩阵来表示，矩阵元素表示该块是空的、被占用的，还是被阻挡的。值为 1 的块表示被机器人占据，值为 0 表示空的，*X* 表示该块不可通行。这个数组中的每个元素，*S*，可以有这三种离散值之一，因此状态在本质上是离散的。接下来，考虑智能体控制自动驾驶汽车的方向盘。智能体以前视图图像作为输入。图像包含连续值的像素，因此在这里状态是连续的。'
- en: '**Action**, *A(S)*: Actions are the set of all possible things that the agent
    can do in a particular state. The set of possible actions, *A*, depends on the
    present state, *S*. Actions may or may not result in a change of state. Like states,
    they can be discrete or continuous. The robot finding a path in the maze can perform
    five discrete actions [**up**, **down**, **left**, **right**, **no change**].
    The SDC agent, on the other hand, can rotate the steering wheel at a continuous
    range of angles.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**，*A(S)*：动作是代理在某一特定状态下可以采取的所有可能行为的集合。可能的动作集合*A*取决于当前状态*S*。动作可能会导致状态的变化，也可能不会。与状态一样，动作可以是离散的或连续的。比如，机器人在迷宫中寻找路径时，可以执行五个离散的动作
    [**上**, **下**, **左**, **右**, **不变**]。而自动驾驶代理则可以在一个连续的角度范围内旋转方向盘。'
- en: '**Reward** *R(S,A,S’)*: Rewards are a scalar value returned by the environment
    based on the agent’s action(s). Here *S* is the present state and *S’* is the
    state of the environment after action *A* is taken. It is determined by the goal;
    the agent gets a higher reward if the action brings it near the goal, and a low
    (or even negative) reward otherwise. How we define a reward is totally up to us—in
    the case of the maze, we can define the reward as the Euclidean distance between
    the agent’s current position and goal. The SDC agent reward can be that the car
    is on the road (positive reward) or off the road (negative reward).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励** *R(S,A,S’)*：奖励是环境根据代理的动作返回的标量值。这里*S*是当前状态，*S’*是执行动作*A*后环境的状态。奖励由目标决定；如果动作将代理带近目标，代理将获得较高的奖励，否则会得到较低（甚至负）的奖励。我们如何定义奖励完全由我们决定——以迷宫为例，我们可以定义奖励为代理当前位置与目标之间的欧几里得距离。自动驾驶代理的奖励可以是车在路上（正奖励）或偏离道路（负奖励）。'
- en: '**Policy** ![](img/B18331_11_001.png): Policy defines a mapping between each
    state and the action to take in that state. The policy can be *deterministic*,
    that is, for each state, there is a well-defined policy. In the case of the maze
    robot, a policy can be that if the top block is empty, move up. The policy can
    also be *stochastic*, that is, where an action is taken by some probability. It
    can be implemented as a simple look-up table, or it can be a function dependent
    on the present state. The policy is the core of the RL agent. In this chapter,
    we’ll learn about different algorithms that help the agent to learn the policy.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略** ![](img/B18331_11_001.png)：策略定义了每个状态与在该状态下应该采取的动作之间的映射。策略可以是*确定性*的，即对于每个状态，都有一个明确的策略。例如，对于迷宫机器人，如果上方的方块为空，就向上移动。策略也可以是*随机*的，即动作是按一定概率采取的。它可以实现为一个简单的查找表，或者作为一个依赖于当前状态的函数。策略是强化学习代理的核心。在本章中，我们将学习帮助代理学习策略的不同算法。'
- en: '**Return** *G*[t]: This is the discounted sum of all future rewards starting
    from the current time, mathematically defined as:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回报** *G*[t]：这是从当前时刻开始，所有未来奖励的折扣总和，数学定义如下：'
- en: '![](img/B18331_11_002.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_002.png)'
- en: Here *R*[t] is the reward at time *t* and ![](img/B18331_11_003.png) is the
    discount factor; its value lies between 0 and 1\. The discount factor determines
    how important future rewards are in deciding the policy. If it is near zero, the
    agent gives importance to the immediate rewards. A high discount factor, however,
    means the agent is looking far into the future. It may give up immediate reward
    in favor of high future rewards, just as in the game chess, you may sacrifice
    a pawn to later checkmate the opponent.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里 *R*[t] 是时刻 *t* 的奖励，且 ![](img/B18331_11_003.png) 是折扣因子；它的值介于 0 和 1 之间。折扣因子决定了在决策策略时未来奖励的重要性。如果它接近零，代理会更加注重即时奖励。然而，高折扣因子意味着代理会更长远地考虑未来。它可能会放弃即时奖励，以换取较高的未来奖励，就像在国际象棋中，你可能会牺牲一个兵，以换取后续的将死对方。
- en: '**Value function** *V(S)*: This defines the “goodness” of a state in the long
    run. It can be thought of as the total amount of reward the agent can expect to
    accumulate over time, starting from the state, *S*. You can think of it as long-term
    good, as opposed to an immediate but short-lived good. What do you think is more
    important, maximizing the immediate reward or the value function? You probably
    guessed right: just as in chess, we sometimes lose a pawn to win the game a few
    steps later, and so the agent should try to maximize the value function.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数** *V(S)*：它定义了一个状态在长远来看“好坏”的程度。可以把它看作是代理从状态*S*出发，在一段时间内可以期望积累的奖励总量。你可以把它看作是长远的好处，而不是短期且短暂的好处。你认为哪个更重要，是最大化即时奖励还是价值函数？你可能猜对了：就像在国际象棋中，我们有时会牺牲一个兵，以便在几步后赢得比赛，因此代理应该尝试最大化价值函数。'
- en: 'Normally, the value is defined either as the **state-value function** ![](img/B18331_11_004.png)
    or the **action-value function** ![](img/B18331_11_005.png), where ![](img/B18331_11_006.png)
    is the policy followed. The state-value function is the expected return from the
    state *S* after following policy ![](img/B18331_11_006.png):'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，值定义为**状态价值函数** ![](img/B18331_11_004.png) 或 **动作价值函数** ![](img/B18331_11_005.png)，其中
    ![](img/B18331_11_006.png) 是所遵循的策略。状态价值函数是遵循策略 ![](img/B18331_11_006.png) 后，从状态
    *S* 得到的期望回报：
- en: '![](img/B18331_11_008.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_008.png)'
- en: 'Here *E* is the expectation, and *S*[t]*=s* is the state at time *t*. The action-value
    function is the expected return from the state *S*, taking an action *A=a* and
    following the policy ![](img/B18331_11_006.png):'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的 *E* 是期望，*S*[t]*=s* 是时间 *t* 时的状态。动作值函数是从状态 *S* 开始，采取动作 *A=a* 并遵循策略 ![](img/B18331_11_006.png)
    所得到的期望回报：
- en: '![](img/B18331_11_010.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_010.png)'
- en: '**Model of the environment**: This is an optional element. It mimics the behavior
    of the environment, and it contains the physics of the environment; in other words,
    it indicates how the environment will behave. The model of the environment is
    defined by the transition probability to the next state. This is an optional component;
    we can have **model-free** reinforcement learning as well where the transition
    probability is not needed to define the RL process.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境模型**：这是一个可选元素。它模拟环境的行为，并包含环境的物理特性；换句话说，它表示环境将如何表现。环境模型通过转移到下一个状态的转移概率来定义。这是一个可选组件；我们也可以采用**无模型**的强化学习，在这种情况下，不需要转移概率来定义RL过程。'
- en: In RL, we assume that the state of the environment follows the **Markov property**,
    that is, each state is dependent solely on the preceding state, the action taken
    from the action space, and the corresponding reward.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在RL中，我们假设环境的状态遵循**马尔可夫性质**，即每个状态仅依赖于前一个状态、从动作空间中采取的动作以及相应的奖励。
- en: 'That is, if *S*^t^(+1) is the state of the environment at time *t+1*, then
    it is a function of *S*^t state at time *t*, *A*^t is the action taken at time
    *t*, and *R*^t is the corresponding reward received at time *t*, no prior history
    is needed. If *P(S*^t^(+1)|*S*^t*)* is the transition probability, mathematically
    the Markov property can be written as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 即，如果 *S*^t^(+1) 是时间 *t+1* 时环境的状态，那么它是时间 *t* 时 *S*^t 状态、时间 *t* 时采取的动作 *A*^t 和相应的奖励
    *R*^t 的函数，不需要前置历史。如果 *P(S*^t^(+1)|*S*^t*)* 是转移概率，从数学上讲，马尔可夫性质可以写成：
- en: '![](img/B18331_11_011.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_011.png)'
- en: And thus, RL can be assumed to be a **Markov Decision Process** (**MDP**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RL可以被认为是一个**马尔可夫决策过程**（**MDP**）。
- en: Deep reinforcement learning algorithms
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习算法
- en: 'The basic idea of **Deep Reinforcement Learning** (**DRL**) is that we can
    use a deep neural network to approximate either the policy function or the value
    function. In this chapter, we will be studying some popular DRL algorithms. These
    algorithms can be classified into two classes, depending upon what they approximate:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**DRL**）的基本思想是，我们可以使用深度神经网络来逼近策略函数或价值函数。在本章中，我们将研究一些流行的DRL算法。这些算法可以根据它们逼近的内容分为两类：'
- en: '**Value-based methods**: In these methods, the algorithms take the action that
    maximizes the value function. The agent here learns to predict how good a given
    state or action would be. An example of the value-based method is the Deep Q-Network.
    Consider, for example, our robot in a maze: assuming that the value of each state
    is the negative of the number of steps needed to go from that box to the goal,
    then, at each time step, the agent will choose the action that takes it to a state
    with optimal value, as in the following diagram. So, starting from a value of
    **-6**, it’ll move to **-5**, **-4**, **-3**, **-2**, **-1**, and eventually reach
    the goal with the value **0**:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于价值的方法**：在这些方法中，算法会采取最大化价值函数的动作。这里的智能体学习预测给定状态或动作的好坏。基于价值的方法的一个例子是深度Q网络。例如，考虑我们的迷宫中的机器人：假设每个状态的值是从该位置到目标所需步数的负数，那么，在每个时间步，智能体将选择采取能将其带到具有最佳值的状态的动作，如下图所示。因此，从
    **-6** 的值开始，它将移动到 **-5**、**-4**、**-3**、**-2**、**-1**，最终到达目标，值为 **0**：'
- en: '![Table  Description automatically generated](img/B18331_11_03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_11_03.png)'
- en: 'Figure 11.3: Demo value function values for the maze-finding robot'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：迷宫寻路机器人的值函数值演示
- en: '**Policy-based methods**: In these methods, the algorithms predict the optimal
    policy (the one that maximizes the expected return), without maintaining the value
    function estimates. The aim is to find the optimal policy, instead of the optimal
    action. An example of the policy-based method is policy gradients. Here, we approximate
    the policy function, which allows us to map each state to the best corresponding
    action. One advantage of policy-based methods over value-based is that we can
    use them even for continuous action spaces.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的方法**：在这些方法中，算法预测最优策略（即最大化期望回报的策略），而不维护价值函数的估计。目标是找到最优策略，而不是最优动作。基于策略的方法的一个例子是策略梯度。在这里，我们近似策略函数，从而将每个状态映射到最佳的相应动作。基于策略的方法相对于基于价值的方法的一个优势是，它们甚至可以用于连续动作空间。'
- en: Besides the algorithms approximating either policy or value, there are a few
    questions we need to answer to make reinforcement learning work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了近似策略或价值的算法外，我们还需要回答一些问题，以便让强化学习有效地工作。
- en: How does the agent choose its actions, especially when untrained?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理如何选择其动作，尤其是在未训练的情况下？
- en: When the agent starts learning, it has no idea what the best way in which to
    determine an action is, or which action will provide the best *Q* value. So how
    do we go about it? We take a leaf out of nature’s book. Like bees and ants, the
    agent makes a balance between exploring new actions and exploiting learned ones.
    Initially, when the agent starts, it has no idea which action among the possible
    actions is better, so it makes random choices, but as it learns, it starts making
    use of the learned policy. This is called the **exploration vs exploitation**
    [2] tradeoff. Using exploration, the agent gathers more information, and later
    exploits the gathered information to make the best decision.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当代理开始学习时，它并不知道如何确定一个动作是最好的，或者哪个动作将提供最佳的*Q*值。那么我们该如何做呢？我们借鉴自然界的做法。就像蜜蜂和蚂蚁一样，代理在探索新动作和利用已学得动作之间做出平衡。最初，当代理开始时，它不知道在所有可能的动作中哪个更好，因此它会做出随机选择，但随着学习的进行，它开始利用已学得的策略。这被称为**探索与利用**[2]的权衡。通过探索，代理收集更多的信息，之后利用这些收集到的信息做出最佳决策。
- en: How does the agent maintain a balance between exploration and exploitation?
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理如何在探索与利用之间保持平衡？
- en: 'There are various strategies; one of the most employed is the **epsilon-greedy**
    (![](img/B18331_11_012.png)) policy. Here, the agent explores unceasingly, and
    depending upon the value of ![](img/B18331_11_013.png), at each step the agent
    selects a random action with probability ![](img/B18331_11_014.png), and with
    probability ![](img/B18331_11_015.png) selects an action that maximizes the value
    function. Normally, the value of ![](img/B18331_11_014.png) decreases asymptotically.
    In Python the ![](img/B18331_11_012.png) policy can be implemented as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种策略，其中最常用的一种是**epsilon-贪心** (![](img/B18331_11_012.png)) 策略。在这里，代理持续探索，并且根据![](img/B18331_11_013.png)的值，在每一步，代理以概率![](img/B18331_11_014.png)选择一个随机动作，并以概率![](img/B18331_11_015.png)选择一个最大化价值函数的动作。通常，![](img/B18331_11_014.png)的值会渐进下降。在Python中，![](img/B18331_11_012.png)策略可以实现为：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: where `model` is the deep neural network approximating the value/policy function,
    `a` is the action chosen from the action space of size `action_size`, and `s`
    is the state. Another way to perform exploration is to use noise; researchers
    have experimented with both Gaussian and Ornstein-Uhlenbeck noise with success.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`model`是近似价值/策略函数的深度神经网络，`a`是从大小为`action_size`的动作空间中选择的动作，`s`是状态。另一种进行探索的方式是使用噪声；研究人员成功地实验了高斯噪声和奥恩斯坦-乌伦贝克噪声。
- en: How to deal with the highly correlated input state space
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何处理高度相关的输入状态空间？
- en: The input to our RL model is the present state of the environment. Each action
    results in some change in the environment; however, the correlation between two
    consecutive states is very high. Now if we make our network learn based on the
    sequential states, the high correlation between consecutive inputs results in
    what is known as **catastrophic forgetting**. To mitigate the effect of catastrophic
    forgetting, in 2018, David Isele and Akansel Cosgun proposed the **experience
    replay** method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的强化学习模型的输入是环境的当前状态。每个动作都会引起环境的某些变化；然而，两个连续状态之间的相关性非常高。如果我们让网络基于连续的状态进行学习，两个连续输入之间的高相关性就会导致所谓的**灾难性遗忘**。为了缓解灾难性遗忘的影响，David
    Isele和Akansel Cosgun在2018年提出了**经验回放**方法。
- en: 'In simplest terms, the learning algorithm first stores the MDP tuple—state,
    action, reward, and next state *<S, A, R, S’>*—in a buffer/memory. Once a significant
    amount of memory is built, a batch is selected randomly to train the agent. The
    memory is continuously refreshed with new additions and old deletions. The use
    of experience replay provides three benefits:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，学习算法首先将MDP元组——状态、动作、奖励和下一个状态 *<S, A, R, S’>*——存储在缓冲区/记忆中。一旦积累了足够的记忆，就会随机选择一批数据来训练代理。记忆会不断地通过新的添加和旧的删除来刷新。使用经验回放提供了三个好处：
- en: First, it allows the same experience to be potentially used in many weight updates,
    hence increasing data efficiency.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它允许相同的经验在多个权重更新中被潜在地使用，从而提高了数据效率。
- en: Second, the random selection of batches of experience removes the correlations
    between consecutive states presented to the network for training.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，随机选择经验批次可以去除连续状态之间的相关性，从而避免网络训练时出现偏差。
- en: Third, it stops any unwanted feedback loops that may arise and cause the network
    to get stuck in local minima or diverge.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，它可以防止任何不希望出现的反馈循环，这些循环可能导致网络陷入局部最小值或发散。
- en: A modified version of experience replay is the **Prioritized Experience Replay**
    (**PER**). Introduced in 2015 by Tom Schaul et al. [4], it derives from the idea
    that not all experiences (or, you might say, attempts) are equally important.
    Some attempts are better lessons than others. Thus, instead of selecting the experiences
    randomly, it will be much more efficient to assign higher priority to more educational
    experiences in selection for training. In the Schaul paper, it was proposed that
    experiences in which the difference between the prediction and target is high
    should be given priority, as the agent could learn a lot in these cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放的一个修改版本是**优先经验回放**（**PER**）。该方法由Tom Schaul等人于2015年提出[4]，其理念来源于并非所有经验（或者说，尝试）都同等重要的观点。有些尝试比其他尝试能提供更好的教训。因此，与其随机选择经验，不如为更多教育意义的经验分配更高的优先级，从而提高选择训练的效率。在Schaul的论文中，提出应该优先选择那些预测和目标之间差异较大的经验，因为在这些情况下，代理能够学到更多。
- en: How to deal with the problem of moving targets
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何处理移动目标的问题
- en: Unlike supervised learning, the target is not previously known in RL. With a
    moving target, the agent tries to maximize the expected return, but the maximum
    value goes on changing as the agent learns. In essence, this is like trying to
    catch a butterfly yet each time you approach it, it moves to a new location. The
    major reason to have a moving target is that the same networks are used to estimate
    the action and the target values, and this can cause oscillations in learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，RL中的目标并不是事先已知的。在移动目标的情况下，代理试图最大化期望回报，但最大值会随着代理学习的进行而不断变化。实际上，这就像试图捕捉一只蝴蝶，每次靠近它时，它都会飞到一个新的位置。移动目标的主要原因是，同一网络用来估计动作和目标值，这可能会导致学习中的震荡。
- en: A solution to this was proposed by the DeepMind team in their 2015 paper, titled
    *Human-level Control through Deep Reinforcement Learning*, published in Nature.
    The solution is that now, instead of a moving target, the agent has short-term
    fixed targets. The agent now maintains two networks, both are exactly the same
    in architecture, one called the local network, which is used at each step to estimate
    the present action, and one the target network, which is used to get the target
    value. However, both networks have their own set of weights. At each time step,
    the local network learns in the direction such that its estimate and target are
    near to each other. After some number of time steps, the target network weights
    are updated. The update can be a **hard update**, where the weights of the local
    network are copied completely to the target network after *N* time steps, or it
    can be a **soft update**, in which the target network slowly (by a factor of Tau
    ![](img/B18331_11_018.png)) moves its weight toward the local network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的解决方案是由DeepMind团队在2015年发表的论文《*通过深度强化学习实现人类水平的控制*》中提出的，该论文发表于《自然》杂志。该解决方案是，现在代理不再面对一个移动的目标，而是拥有短期固定的目标。代理现在保持两个网络，它们在架构上完全相同，一个叫做局部网络，用于在每一步估计当前的动作，另一个是目标网络，用来获取目标值。然而，两个网络各自有自己的一组权重。在每一个时间步，局部网络学习的方向是使其估计值和目标值尽可能接近。经过若干个时间步后，目标网络的权重会被更新。更新可以是**硬更新**，即在*N*个时间步之后，将局部网络的权重完全复制到目标网络，或者是**软更新**，在这种情况下，目标网络缓慢地（通过Tau因子！[](img/B18331_11_018.png)）将其权重向局部网络靠近。
- en: Reinforcement success in recent years
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 近年来强化学习的成功
- en: 'In the last few years, DRL has been successfully used in a variety of tasks,
    especially in game playing and robotics. Let us acquaint ourselves with some success
    stories of RL before learning its algorithms:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，深度强化学习（DRL）已成功应用于各种任务，特别是在游戏和机器人领域。让我们在学习算法之前，先了解一些强化学习的成功案例：
- en: '**AlphaGo Zero**: Developed by Google’s DeepMind team, the AlphaGo Zero paper
    *Mastering the game of Go without any human knowledge* starts from an absolutely
    blank slate (**tabula rasa**). The AlphaGo Zero uses one neural network to approximate
    both the move probabilities and value.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AlphaGo Zero**：由谷歌的DeepMind团队开发，AlphaGo Zero的论文《*无需任何人类知识即可掌握围棋*》从一张完全空白的白纸（**tabula
    rasa**）开始。AlphaGo Zero使用一个神经网络来近似计算棋步概率和价值。'
- en: 'This neural network takes as an input the raw board representation. It uses
    a Monte Carlo tree search guided by the neural network to select the moves. The
    reinforcement learning algorithm incorporates a look-ahead search inside the training
    loop. It was trained for 40 days using a 40-block residual CNN and, over the course
    of training, it played about 29 million games (a big number!). The neural network
    was optimized on Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU
    parameter servers. You can access the paper here: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个神经网络以原始棋盘表示作为输入。它使用一个由神经网络引导的蒙特卡洛树搜索来选择棋步。强化学习算法在训练循环中结合了前瞻性搜索。它使用40块残差CNN训练了40天，在训练过程中，它玩了大约2900万盘棋（这是一个非常大的数字！）。该神经网络在Google
    Cloud上使用TensorFlow进行优化，配备了64个GPU工作节点和19个CPU参数服务器。你可以在这里查看论文：[https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270)。
- en: '**AI-controlled sailplanes**: Microsoft has developed a controller system that
    can run on many different autopilot hardware platforms, such as Pixhawk and Raspberry
    Pi 3\. It can keep the sailplane in the air without using a motor, by autonomously
    finding and catching rides on naturally occurring thermals. The controller helps
    the sailplane to operate on its own by detecting and using these thermals to travel
    without the aid of a motor or a person. They implemented it as a partially observable
    Markov decision process. They employed Bayesian reinforcement learning and used
    the Monte Carlo tree search to search for the best action. They’ve divided the
    whole system into level planners—a high-level planner that makes a decision based
    on experience and a low-level planner that uses Bayesian reinforcement learning
    to detect and latch onto thermals in real time. You can see the sailplane in action
    at Microsoft News: [https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/](https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI控制的滑翔机**：微软开发了一套控制系统，可以在多种不同的自动驾驶硬件平台上运行，如Pixhawk和Raspberry Pi 3。它能够通过自主寻找并利用自然气流，将滑翔机保持在空中，无需使用发动机。控制系统帮助滑翔机在没有电机或人工干预的情况下，仅通过检测并利用气流来飞行。他们将其实现为一个部分可观察的马尔可夫决策过程（MDP）。他们采用了贝叶斯强化学习，并使用蒙特卡洛树搜索来寻找最佳行动。他们将整个系统分为多个层次的规划器——一个高层规划器根据经验做出决策，一个低层规划器则使用贝叶斯强化学习实时检测并锁定气流。你可以在微软新闻中看到这款滑翔机的实际操作：[https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/](https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/)。'
- en: '**Locomotion behavior**: In the paper *Emergence of Locomotion Behaviours in
    Rich Environments* ([https://arxiv.org/pdf/1707.02286.pdf](https://arxiv.org/pdf/1707.02286.pdf)),
    DeepMind researchers provided the agents with rich and diverse environments. The
    environments presented a spectrum of challenges at different levels of difficulty.
    The agent was provided with difficulties in increasing order; this led the agent
    to learn sophisticated locomotion skills without performing any reward engineering
    (that is, designing special reward functions).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运动行为**：在论文《*在丰富环境中运动行为的出现*》([https://arxiv.org/pdf/1707.02286.pdf](https://arxiv.org/pdf/1707.02286.pdf))中，DeepMind的研究人员为智能体提供了丰富多样的环境。环境呈现出不同难度级别的挑战。智能体在不断增加的难度下进行训练，这促使其在没有进行任何奖励设计（即没有设计特定奖励函数）的情况下，学习到复杂的运动技能。'
- en: '**Data center cooling using reinforcement learning**: Data centers are workhorses
    of the present digital/internet revolution. With their large servers and networking
    devices, they facilitate data storage, data transfer, and the processing of information
    over the internet. Data centers account for about ~1.5% of all global energy consumption
    and if nothing is done about it, the consumption will only increase. DeepMind,
    along with Google Research in 2016, employed reinforcement learning models to
    reduce the energy consumption of their data centers by 40%. Using the historical
    data collected from the sensors within the data center, they trained a deep neural
    network to predict future energy efficiency and propose optimal action. You can
    read the details of the models and approach in the paper *Data center cooling
    using model-predictive control* ([https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf)).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用强化学习进行数据中心冷却**：数据中心是当今数字/互联网革命的主力军。凭借其庞大的服务器和网络设备，它们促进了数据存储、数据传输以及信息处理。数据中心约占全球能源消耗的1.5%左右，如果不采取措施，消耗量只会增加。DeepMind与Google
    Research在2016年采用强化学习模型，成功将其数据中心的能耗减少了40%。通过使用数据中心传感器收集的历史数据，他们训练了一个深度神经网络来预测未来的能效并提出最佳行动方案。你可以在这篇论文中阅读该模型和方法的详细信息：*使用模型预测控制进行数据中心冷却*（[https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf)）。'
- en: '**Controlling nuclear fusion plasma**: A recent (2022) and interesting application
    of RL is in controlling nuclear fusion plasma with the help of reinforcement learning.
    The results are published in a Nature paper: *Magnetic control of tokamak plasmas
    through reinforcement learning*.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制核聚变等离子体**：RL的一个最近（2022年）且有趣的应用是借助强化学习来控制核聚变等离子体。相关成果已发布在《自然》期刊的论文中：*通过强化学习实现托卡马克等离子体的磁控*。'
- en: It is really amazing to see how the DRL agent, without any implicit knowledge,
    learns to perform, and even beat, humans – in many specialized tasks. In the coming
    sections, we will explore these fabulous DRL algorithms and see them play games
    with almost human efficiency within a few thousand epochs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 看到DRL代理如何在没有任何隐性知识的情况下学习执行任务，甚至在许多专业任务中超越人类，真是令人惊叹。在接下来的章节中，我们将探索这些神奇的DRL算法，看看它们如何在几千个训练周期内，以几乎人类的效率玩游戏。
- en: Simulation environments for RL
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的仿真环境
- en: As mentioned earlier, **trial and error** is an important component of any RL
    algorithm. Therefore, it makes sense to train our RL agent firstly in a simulated
    environment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，**试错**是任何RL算法的重要组成部分。因此，在模拟环境中首先训练我们的RL代理是有意义的。
- en: 'Today there exists a large number of platforms that can be used for the creation
    of an environment. Some popular ones are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，已经有大量平台可以用来创建环境。一些流行的包括：
- en: '**OpenAI Gym**: This contains a collection of environments that we can use
    to train our RL agents. In this chapter, we’ll be using the OpenAI Gym interface.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI Gym**：它包含了一系列环境，我们可以用来训练RL代理。在本章中，我们将使用OpenAI Gym接口。'
- en: '**Unity ML-Agents SDK**: It allows developers to transform games and simulations
    created using the Unity editor into environments where intelligent agents can
    be trained using DRL, evolutionary strategies, or other machine learning methods
    through a simple-to-use Python API. It works with TensorFlow and provides the
    ability to train intelligent agents for 2D/3D and VR/AR games. You can learn more
    about it here: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unity ML-Agents SDK**：它允许开发者通过一个简单易用的Python API，将使用Unity编辑器创建的游戏和仿真转化为智能代理可以训练的环境，使用DRL、进化策略或其他机器学习方法。它与TensorFlow兼容，能够训练适用于2D/3D以及VR/AR游戏的智能代理。你可以在这里了解更多：[https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)。'
- en: '**Gazebo**: In Gazebo, we can build three-dimensional worlds with physics-based
    simulation. The `gym-gazebo` toolkit uses Gazebo along with the **Robot Operating
    System** (**ROS**) and the OpenAI Gym interface and can be used to train RL agents.
    To find out more about this, you can refer to the white paper: [https://arxiv.org/abs/1608.05742](https://arxiv.org/abs/1608.05742).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gazebo**：在 Gazebo 中，我们可以构建具有基于物理的仿真功能的三维世界。`gym-gazebo` 工具包结合 Gazebo、**机器人操作系统**（**ROS**）和
    OpenAI Gym 接口，可以用于训练 RL 代理。有关更多信息，您可以参考白皮书：[https://arxiv.org/abs/1608.05742](https://arxiv.org/abs/1608.05742)。'
- en: '**Blender learning environment**: This is a Python interface for the Blender
    game engine, and it also works with OpenAI Gym. It has at its base Blender: a
    free 3D modeling software with an integrated game engine. This provides an easy-to-use,
    powerful set of tools for creating games. It provides an interface to the Blender
    game engine, and the games themselves are designed in Blender. We can then create
    a custom virtual environment to train an RL agent on a specific problem ([https://github.com/LouisFoucard/gym-blender](https://github.com/LouisFoucard/gym-blender)).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Blender 学习环境**：这是一个用于 Blender 游戏引擎的 Python 接口，它也可以与 OpenAI Gym 配合使用。其基础是
    Blender：一款免费的 3D 建模软件，内置游戏引擎。这为创建游戏提供了一个易用且强大的工具集。它提供了与 Blender 游戏引擎的接口，游戏本身是在
    Blender 中设计的。然后，我们可以创建一个自定义虚拟环境，在特定问题上训练 RL 代理（[https://github.com/LouisFoucard/gym-blender](https://github.com/LouisFoucard/gym-blender)）。'
- en: '**Malmo**: Built by the Microsoft team, Malmo is a platform for AI experimentation
    and research built on top of Minecraft. It provides a simple API for creating
    tasks and missions. You can learn more about Project Malmo here: [https://www.microsoft.com/en-us/research/project/project-malmo/](https://www.microsoft.com/en-us/research/project/project-malmo/).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Malmo**：由微软团队构建的 Malmo 是一个基于 Minecraft 的 AI 实验和研究平台。它提供了一个简单的 API，用于创建任务和任务。您可以在这里了解更多关于
    Project Malmo 的信息：[https://www.microsoft.com/en-us/research/project/project-malmo/](https://www.microsoft.com/en-us/research/project/project-malmo/)。'
- en: An introduction to OpenAI Gym
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAI Gym 介绍
- en: We will be using OpenAI Gym to provide an environment for our agent. OpenAI
    Gym is an open source toolkit to develop and compare RL algorithms. It contains
    a variety of simulated environments that can be used to train agents and develop
    new RL algorithms.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OpenAI Gym 来为我们的代理提供环境。OpenAI Gym 是一个开源工具包，用于开发和比较 RL 算法。它包含了多种可用于训练代理并开发新
    RL 算法的仿真环境。
- en: 'The first thing to do is install OpenAI Gym. The following command will install
    the minimal `gym` package:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要做的是安装 OpenAI Gym。以下命令将安装最小的 `gym` 包：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If you want to install all (free) `gym` modules, add `[all]` after it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想安装所有（免费）`gym` 模块，可以在后面加上 `[all]`：
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The MuJoCo environment requires a purchasing license. For Atari-based games,
    you will need to install Atari dependencies (Box2D and ROM):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: MuJoCo 环境需要购买许可证。对于基于 Atari 的游戏，您需要安装 Atari 依赖项（Box2D 和 ROM）：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'OpenAI Gym provides a variety of environments, from simple text-based to three-dimensional
    games. The environments supported can be grouped as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 提供了多种环境，从简单的基于文本的到三维游戏。支持的环境可以按如下方式分组：
- en: '**Algorithms**: Contains environments that involve performing computations
    such as addition. While we can easily perform the computations on a computer,
    what makes these problems interesting as RL problems is that the agent learns
    these tasks purely by example.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法**：包含涉及执行计算任务（如加法）的环境。虽然我们可以轻松地在计算机上执行这些计算，但使这些问题成为 RL 问题的有趣之处在于，代理仅通过示例学习这些任务。'
- en: '**Atari**: This environment provides a wide variety of classic Atari/arcade
    games.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atari**：此环境提供了种类繁多的经典 Atari/街机游戏。'
- en: '**Box2D**: Contains robotics tasks in two dimensions such as a car racing agent
    or bipedal robot walk.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Box2D**：包含二维机器人工具任务，如赛车代理或双足机器人走路。'
- en: '**Classic control**: This contains the classical control theory problems, such
    as balancing a cart pole.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经典控制**：包含经典控制理论问题，例如平衡小车杆。'
- en: '**MuJoCo**: This is proprietary (you can get a one-month free trial). It supports
    various robot simulation tasks. The environment includes a physics engine; hence,
    it’s used for training robotic tasks.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MuJoCo**：这是专有的（您可以获得一个月的免费试用）。它支持各种机器人仿真任务。该环境包括物理引擎，因此用于训练机器人任务。'
- en: '**Robotics**: This environment also uses the physics engine of MuJoCo. It simulates
    goal-based tasks for fetch and shadow-hand robots.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人学**：此环境也使用 MuJoCo 的物理引擎。它模拟了以目标为导向的任务，适用于取物和影像手机器人。'
- en: '**Toy text**: A simple text-based environment—very good for beginners.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Toy text**：一个简单的基于文本的环境——非常适合初学者。'
- en: 'You can get a complete list of environments from the Gym website: [https://gym.openai.com](https://gym.openai.com).
    To find a list of all available environments in your installation, you can use
    the following code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 Gym 网站获取完整的环境列表：[https://gym.openai.com](https://gym.openai.com)。要查看安装中所有可用环境的列表，你可以使用以下代码：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At the time of writing this book, it resulted in 859, that is, there are 859
    different environments present in the `gym` module. Let us see more details of
    these environments. Each environment is created by using the `make` function.
    Associated with each environment is a unique ID, its observation space, its action
    space, and a default reward range. Gym allows you to access them through dot notation,
    as shown in the following code. We go through all the environments in the `envall`
    list and note down its unique ID, which is used to create the environment using
    the `make` method, its observation space, reward range, and the action space:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，结果是 859，即 `gym` 模块中存在 859 个不同的环境。让我们看看这些环境的更多细节。每个环境都是通过使用 `make` 函数创建的。每个环境都有一个唯一的
    ID、其观察空间、动作空间和默认奖励范围。Gym 允许你通过点符号访问它们，如下代码所示。我们遍历 `envall` 列表中的所有环境，并记录下其唯一 ID，ID
    用于通过 `make` 方法创建环境，观察空间、奖励范围和动作空间：
- en: '[PRE5]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 11.4* shows a random sample from the list:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.4* 显示了列表中的一个随机样本：'
- en: '![Table  Description automatically generated](img/B18331_11_04.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![表格 描述自动生成](img/B18331_11_04.png)'
- en: 'Figure 11.4: Random list of environments available in OpenAI Gym'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：OpenAI Gym 中可用环境的随机列表
- en: 'You can use these commands to find out details about any environment in Gym.
    For example, the following code prints details of the MountainCar environment:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这些命令来查看 Gym 中任何环境的详细信息。例如，以下代码打印出 MountainCar 环境的详细信息：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The core interface provided by OpenAI Gym is the unified environment interface.
    The agent can interact with the environment using three basic methods, that is,
    `reset`, `step`, and `render`. The `reset` method resets the environment and returns
    the observation. The `step` method steps the environment by one time step and
    returns `new_obs`, `reward`, `done`, and `info`. The `render` method renders one
    frame of the environment, like popping a window. Let us try and view some different
    environments and view their initial frame:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 提供的核心接口是统一的环境接口。代理可以使用三种基本方法与环境进行交互，即 `reset`、`step` 和 `render`。`reset`
    方法重置环境并返回观察值。`step` 方法使环境按一个时间步长前进，并返回 `new_obs`、`reward`、`done` 和 `info`。`render`
    方法渲染环境的一帧，类似于弹出一个窗口。让我们尝试查看一些不同的环境并查看它们的初始帧：
- en: '| **Physics Engine** | **Classic Control** | **Atari** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| **物理引擎** | **经典控制** | **Atari** |'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ![Icon  Description automatically generated](img/B18331_11_05.png) | ![Chart,
    box and whisker chart  Description automatically generated](img/B18331_11_06.png)
    | ![A picture containing text  Description automatically generated](img/B18331_11_07.png)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ![图标 描述自动生成](img/B18331_11_05.png) | ![图表，箱线图 描述自动生成](img/B18331_11_06.png)
    | ![包含文本的图片 描述自动生成](img/B18331_11_07.png) |'
- en: 'Table 11.1: Different environments of OpenAI Gym and their initial state'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1：OpenAI Gym 的不同环境及其初始状态
- en: 'The preceding code uses Matplotlib to display the environment; alternatively,
    you can directly use the `render` method:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用 Matplotlib 显示环境；你也可以直接使用 `render` 方法：
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see the Breakout environment in *Figure 11.5*; the `render` function
    pops up the environment window:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 11.5*中看到 Breakout 环境；`render` 函数会弹出环境窗口：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_08.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_11_08.png)'
- en: 'Figure 11.5: Initial state of the Breakout environment'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：Breakout 环境的初始状态
- en: 'We can use `env.observation_space` and `env.action_space` to find out more
    about the state space and action space for the Breakout game. The results show
    the state consists of a three-channel image of size 210 × 160, and the action
    space is discrete with four possible actions. Once you are done, do not forget
    to close OpenAI using:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `env.observation_space` 和 `env.action_space` 来了解 Breakout 游戏的状态空间和动作空间。结果显示，状态由一个
    210 × 160 大小的三通道图像组成，动作空间是离散的，有四个可能的动作。完成后，别忘了使用以下命令关闭 OpenAI：
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Random agent playing Breakout
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机代理玩 Breakout
- en: Let’s have some fun and play the Breakout game. When I first played the game,
    I had no idea of the rules or how to play, so I randomly chose the control buttons.
    Our novice agent will do the same; it will choose the actions randomly from the
    action space. Gym provides a function called `sample()`, which chooses a random
    action from the action space – we will be using this function. Also, we can save
    a replay of the game, to view it later. There are two ways to save the play, one
    using Matplotlib and another using an OpenAI Gym Monitor wrapper. Let us first
    see the Matplotlib method.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩得开心，玩一下Breakout游戏。当我第一次玩这个游戏时，我根本不知道规则是什么，也不知道该怎么玩，所以我随机选择了控制按钮。我们的新手智能体也将做同样的事情；它会从动作空间中随机选择动作。Gym提供了一个名为`sample()`的函数，它从动作空间中选择一个随机动作——我们将使用这个函数。同时，我们还可以保存游戏的回放，以便稍后查看。保存回放有两种方式，一种是使用Matplotlib，另一种是使用OpenAI
    Gym的Monitor包装器。让我们先看看Matplotlib方法。
- en: 'We will first import the necessary modules; we will only need `gym` and `matplotlib`
    for now, as the agent will be playing random moves:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入必要的模块；目前我们只需要`gym`和`matplotlib`，因为智能体将进行随机操作：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We create the Gym environment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建Gym环境：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we will run the game, one step at a time, choosing a random action, either
    for 300 steps or until the game is finished (whichever is earlier). The environment
    state (observation) space is saved at each step in the list `frames`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将一步一步运行游戏，选择一个随机动作，无论是300步还是直到游戏结束（以较早的为准）。环境状态（观察）空间将在每一步保存到列表`frames`中：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now comes the part of combining all the frames into a GIF image using Matplotlib
    Animation. We create an image object, patch, and then define a function that sets
    image data to a particular frame index. The function is used by the Matplotlib
    `Animation` class to create an animation, which we finally save in the file `random_agent.gif`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们进入将所有帧合成GIF图像的部分，使用Matplotlib Animation。我们创建一个图像对象、补丁，然后定义一个函数，将图像数据设置为特定的帧索引。该函数由Matplotlib的`Animation`类使用，用来创建动画，最后我们将其保存在文件`random_agent.gif`中：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The code above will generate a GIF image. Below are some screen grabs from
    the image:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将生成一个GIF图像。下面是从该图像中截取的一些屏幕截图：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_09.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序说明自动生成](img/B18331_11_09.png)'
- en: 'Figure 11.6: Some screenshots from the saved GIF image'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6：从保存的GIF图像中截取的一些屏幕截图
- en: Now that we are familiar with OpenAI Gym, we’ll move on to wrappers—which you
    can use to create your own custom environments.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了OpenAI Gym，我们将继续介绍包装器——你可以用它来创建自定义环境。
- en: Wrappers in Gym
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gym中的包装器
- en: 'Gym provides various wrappers for us to modify the existing environment. For
    example, if you have image-based inputs with the RGB intensity value lying between
    0 and 255, but the RL agent you use is a neural network, which works best if the
    input is in the range 0-1, you can use the Gym wrapper class to preprocess the
    state space. Below we define a wrapper that concatenates observations:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Gym为我们提供了多种包装器来修改现有环境。例如，如果你有图像输入，RGB强度值在0到255之间，而你使用的RL智能体是神经网络，最佳输入范围是0到1，那么你可以使用Gym包装器类来预处理状态空间。下面我们定义了一个包装器，用于连接观察：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see that we need to change the default `reset` function, `step` function,
    and observation function `_get_obs`. We also need to modify the default observation
    space.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们需要更改默认的`reset`函数、`step`函数和观察函数`_get_obs`。我们还需要修改默认的观察空间。
- en: 'Let us see how it works. If you take the `"BreakoutNoFrameskip-v4"` environment,
    then the initial observation space is 210 x 160 x 3:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的。如果你选择`"BreakoutNoFrameskip-v4"`环境，那么初始的观察空间是210 x 160 x 3：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And now if you use the wrapper we just created:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你使用我们刚刚创建的包装器：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You can see that now a dimension is added—it has four frames, with each frame
    of size 210 x 160 x 3\. You can use a wrapper to modify the rewards as well. In
    this case, you use the superclass `RewardWrapper`. Below is sample code that can
    clip the reward to lie within the range [-10, 10]:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到现在添加了一个维度——它有四个帧，每个帧的大小是210 x 160 x 3。你也可以使用包装器来修改奖励。在这种情况下，你使用父类`RewardWrapper`。下面是一个示例代码，可以将奖励裁剪到[-10,
    10]的范围内：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let us try using it in the CartPole environment, which has the reward range
    ![](img/B18331_11_019.png):'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在CartPole环境中使用它，该环境的奖励范围是 ![](img/B18331_11_019.png)：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Another useful application of wrappers is when you want to save the state space
    as an agent is learning. Normally, an RL agent requires lots of steps for proper
    training, and as a result, it is not feasible to store the state space at each
    step. Instead, we can choose to store after every 500th step (or any other number
    you wish) in the preceding algorithm. OpenAI Gym provides the `Wrapper Monitor`
    class to save the game as a video. To do so, we need to first import wrappers,
    then create the environment, and finally use `Monitor`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 包装器的另一个有用应用是，当你想在智能体学习时保存状态空间。通常，RL智能体需要大量的步骤来进行适当的训练，因此在每一步保存状态空间并不可行。相反，我们可以选择在每500步后（或者你希望的任何其他步数）存储状态空间，正如前述算法所示。OpenAI
    Gym提供了`Wrapper Monitor`类来将游戏保存为视频。为此，我们需要首先导入包装器，然后创建环境，最后使用`Monitor`。
- en: 'By default, it will store the video of 1, 8, 27, 64, (episode numbers with
    perfect cubes), and so on, and then every 1,000th episode; each training, by default,
    is saved in one folder. The code to do this is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它将存储1、8、27、64（完美立方数的剧集编号）等视频，然后每1,000个剧集；每次训练，默认保存在一个文件夹中。执行此操作的代码如下：
- en: '[PRE27]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: For `Monitor` to work, we require FFmpeg support. We may need to install it
    depending upon our OS, if it is missing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要使`Monitor`正常工作，我们需要FFmpeg的支持。根据操作系统的不同，可能需要安装它，如果缺少的话。
- en: This will save the videos in `.mp4` format in the folder `recording`. An important
    thing to note here is that you have to set the `force=True` option if you want
    to use the same folder for the next training session.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把视频以`.mp4`格式保存在`recording`文件夹中。这里需要注意的是，如果你想使用同一个文件夹进行下次训练，会需要设置`force=True`选项。
- en: 'If you want to train your agent on Google Colab, you will need to add the following
    drivers to be able to visualize the Gym output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在Google Colab上训练你的智能体，你需要添加以下驱动程序，以便能够可视化Gym的输出：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'After installing the Python virtual display, you need to start it—Gym uses
    the virtual display to set observations. The following code can help you in starting
    a display of size 600 x 400:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Python虚拟显示后，你需要启动它——Gym使用虚拟显示来设置观察。以下代码可以帮助你启动一个大小为600 x 400的显示：
- en: '[PRE29]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And to be able to play around with Atari games, use:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 并且要能够玩Atari游戏，使用：
- en: '[PRE30]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Deep Q-networks
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: '**Deep Q-Networks**, **DQNs** for short, are deep learning neural networks
    designed to approximate the Q-function (value-state function). They are one of
    the most popular value-based reinforcement learning algorithms. The model was
    proposed by Google’s DeepMind in NeurIPS 2013, in the paper entitled *Playing
    Atari with Deep Reinforcement Learning*. The most important contribution of this
    paper was that they used the raw state space directly as input to the network;
    the input features were not hand-crafted as done in earlier RL implementations.
    Also, they could train the agent with exactly the same architecture to play different
    Atari games and obtain state-of-the-art results.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度Q网络**，简称**DQNs**，是深度学习神经网络，旨在逼近Q函数（价值-状态函数）。它们是最流行的基于价值的强化学习算法之一。该模型由Google的DeepMind在2013年NeurIPS会议上提出，论文标题为*使用深度强化学习玩Atari游戏*。这篇论文的最重要贡献是，他们直接将原始状态空间作为输入传递给网络；输入特征不像早期的RL实现那样是人工设计的。此外，他们能够使用完全相同的架构训练智能体进行不同的Atari游戏，并取得最先进的结果。'
- en: 'This model is an extension of the simple Q-learning algorithm. In Q-learning
    algorithms, a Q-table is maintained as a cheat sheet. After each action, the Q-table
    is updated using the Bellman equation [5]:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是简单Q学习算法的扩展。在Q学习算法中，会维持一个Q表作为备忘单。每次执行动作后，Q表会使用贝尔曼方程[5]进行更新：
- en: '![](img/B18331_11_020.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_020.png)'
- en: '![](img/B18331_11_021.png) is the learning rate, and its value lies in the
    range [0,1]. The first term represents the component of the old *Q* value and
    the second term the target *Q* value. Q-learning is good if the number of states
    and the number of possible actions is small, but for large state spaces and action
    spaces, Q-learning is simply not scalable. A better alternative would be to use
    a deep neural network as a function approximator, approximating the target Q-function
    for each possible action. The weights of the deep neural network in this case
    store the Q-table information. There is a separate output unit for each possible
    action. The network takes the state as its input and returns the predicted target
    *Q* value for all possible actions. The question arises: how do we train this
    network, and what should be the loss function? Well, since our network has to
    predict the target *Q* value:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B18331_11_021.png) 是学习率，它的值位于[0,1]范围内。第一项表示旧的*Q*值的组成部分，第二项表示目标*Q*值。Q学习适用于状态数量和可能的动作数量较少的情况，但对于较大的状态空间和动作空间，Q学习的扩展性较差。一个更好的替代方法是使用深度神经网络作为函数近似器，逼近每个可能动作的目标Q函数。在这种情况下，深度神经网络的权重存储了Q表信息。每个可能的动作都有一个单独的输出单元。网络将状态作为输入，并返回所有可能动作的预测目标*Q*值。问题来了：我们如何训练这个网络，损失函数应该是什么？好吧，由于我们的网络必须预测目标*Q*值：'
- en: '![](img/B18331_11_022.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_022.png)'
- en: 'the loss function should try and reduce the difference between the *Q* value
    predicted, *Q*[predicted], and the target *Q*, *Q*[target]. We can do this by
    defining the loss function as:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数应该尽量减少预测的*Q*值（*Q*[predicted]）与目标*Q*值（*Q*[target]）之间的差异。我们可以通过将损失函数定义为：
- en: '![](img/B18331_11_023.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_023.png)'
- en: where *W* is the training parameters of our deep *Q* network, learned using
    gradient descent, such that the loss function is minimized.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*W*是我们深度*Q*网络的训练参数，通过梯度下降学习，以使损失函数最小化。
- en: 'The following is the general architecture of a DQN. The network takes the *n*-dimensional
    state as input and outputs the *Q* value of each possible action in the *m*-dimensional
    action space. Each layer (including the input) can be a convolutional layer (if
    we are taking the raw pixels as input, convolutional layers make more sense) or
    a dense layer:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DQN的一般架构。网络将*n*维状态作为输入，并输出*Q*值，对于*m*维动作空间中的每个可能动作。每一层（包括输入层）可以是卷积层（如果我们采用原始像素作为输入，卷积层更有意义）或密集层：
- en: '![A picture containing icon  Description automatically generated](img/B18331_11_10.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing icon  Description automatically generated](img/B18331_11_10.png)'
- en: 'Figure 11.7: The figure shows a simple DQN network, the input layer taking
    State vector S, and the output predicting Q for all possible actions for the state'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：图示显示了一个简单的DQN网络，输入层接收状态向量S，输出层预测该状态下所有可能动作的Q值
- en: In the next section, we will try training a DQN. Our agent task will be to stabilize
    a pole on a cart. The agent can move the cart left or right to maintain balance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将尝试训练一个DQN。我们的智能体任务是使小车上的杆子保持稳定。智能体可以左右移动小车以保持平衡。
- en: DQN for CartPole
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole的DQN
- en: 'CartPole is a classic OpenAI problem with continuous state space and discrete
    action space. In it, a pole is attached by an un-actuated joint to a cart; the
    cart moves along a frictionless track. The goal is to keep the pole standing on
    the cart by moving the cart left or right. A reward of +1 is given for each time
    step the pole is standing. Once the pole is more than 15 degrees from the vertical,
    or the cart moves beyond 2.4 units from the center, the game is over:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole是一个经典的OpenAI问题，具有连续的状态空间和离散的动作空间。在这个问题中，一根杆子通过一个不受控制的关节连接到一个小车上；小车沿着一个无摩擦的轨道移动。目标是通过左右移动小车来保持杆子在小车上站立。每当杆子站立时，奖励为+1。若杆子与垂直方向的角度超过15度，或小车从中心位置移动超过2.4个单位，游戏结束：
- en: '![Box and whisker chart  Description automatically generated](img/B18331_11_11.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![Box and whisker chart  Description automatically generated](img/B18331_11_11.png)'
- en: 'Figure 11.8: A screenshot from the CartPole Gym environment'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：来自CartPole Gym环境的截图
- en: 'You can check the leaderboard of OpenAI Gym for some cool entries for the CartPole
    environment: [https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看OpenAI Gym的排行榜，看看CartPole环境中的一些酷炫条目：[https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0)。
- en: 'We start with importing the necessary modules. We require `gym`, obviously,
    to provide us with the CartPole environment, and `tensorflow` to build our DQN
    network. Besides these, we need the `random` and `numpy` modules:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的模块。我们显然需要`gym`来为我们提供CartPole环境，`tensorflow`来构建我们的DQN网络。除此之外，我们还需要`random`和`numpy`模块：
- en: '[PRE31]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We set up the global values for the maximum episodes for which we will be training
    the agent (`EPOCHS`), the threshold value when we consider the environment solved
    (`THRESHOLD`), and a bool to indicate if we want to record the training or not
    (`MONITOR`). Please note that as per the official OpenAI documentation, the CartPole
    environment is considered solved when the agent is able to maintain the pole in
    the vertical position for 195 time steps (ticks). In the following code, for the
    sake of time, we have reduced the `THRESHOLD` to 45:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了训练代理的最大训练回合数（`EPOCHS`）、我们认为环境已解决的阈值（`THRESHOLD`），以及一个布尔值来表示是否希望记录训练过程（`MONITOR`）。请注意，根据官方OpenAI文档，CartPole环境在代理能够在195个时间步（ticks）内保持杆处于竖直位置时被认为已解决。在以下代码中，为了节省时间，我们将`THRESHOLD`降低到45：
- en: '[PRE32]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now let us build our DQN. We declare a class `DQN` and in its `__init__()`
    function declare all the hyperparameters and our model. We are also creating the
    environment inside the `DQN` class. As you can see, the class is quite general,
    and you can use it to train any Gym environment whose state space information
    can be encompassed in a 1D array:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建我们的DQN。我们声明一个`DQN`类，并在其`__init__()`函数中声明所有超参数和模型。我们还在`DQN`类内部创建了环境。如你所见，这个类非常通用，你可以用它来训练任何Gym环境，只要其状态空间信息可以被包含在一个一维数组中：
- en: '[PRE33]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The DQN that we have built is a three-layered perceptron; in the following
    output, you can see the model summary. We use the Adam optimizer with learning
    rate decay:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的DQN是一个三层感知机；在以下输出中，你可以看到模型摘要。我们使用带有学习率衰减的Adam优化器：
- en: '[PRE34]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The variable list `self.memory` will contain our experience replay buffer.
    We need to add a method for saving the *<S,A,R,S’>* tuple into the memory and
    a method to get random samples from it in batches to train the agent. We perform
    these two functions by defining the class methods `remember` and `replay`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 变量列表`self.memory`将包含我们的经验回放缓冲区。我们需要添加一个方法，将*S,A,R,S’*元组保存到内存中，并添加一个方法，从中批量获取随机样本以训练代理。我们通过定义类方法`remember`和`replay`来实现这两个功能：
- en: '[PRE35]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Our agent will use the **epsilon-greedy policy** when choosing the action.
    This is implemented in the following method:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理将在选择动作时使用**epsilon-贪婪策略**。该策略在以下方法中实现：
- en: '[PRE36]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we write a method to train the agent. We define two lists to keep track
    of the scores. First, we fill the experience replay buffer and then we choose
    some samples from it to train the agent and hope that the agent will slowly learn
    to do better:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个方法来训练代理。我们定义了两个列表来跟踪分数。首先，我们填充经验回放缓冲区，然后从中选择一些样本来训练代理，并希望代理能够逐渐学会做得更好：
- en: '[PRE37]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that all necessary functions are done, we just need one more helper function
    to reshape the state of the CartPole environment so that the input to the model
    is in the correct shape. The state of the environment is described by four continuous
    variables: cart position ([-2.4-2.4]), cart velocity, pole angle ([-41.8o-41.8o]),
    and pole velocity :'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有必要的函数已经完成，我们只需要一个辅助函数来重新塑造CartPole环境的状态，以便模型的输入是正确的形状。环境的状态由四个连续变量描述：小车位置（[-2.4-2.4]），小车速度，杆角度（[-41.8o-41.8o]），和杆速度：
- en: '[PRE38]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let us now instantiate our agent for the CartPole environment and train it:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实例化我们的代理，应用于CartPole环境并进行训练：
- en: '[PRE39]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s plot the average reward as the agent learns:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制代理学习过程中获得的平均奖励：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '*Figure 11.9* shows the agent being trained on my system. The agent was able
    to achieve our set threshold of 45 in 254 steps:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.9*显示了代理在我的系统上进行训练的过程。代理在254步内达到了我们设定的45的阈值：'
- en: '`![Chart  Description automatically generated](img/B18331_11_12.png)`'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`![图表 描述自动生成](img/B18331_11_12.png)`'
- en: 'Figure 11.9: Average agent reward plot'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9：代理平均奖励图
- en: 'Once the training is done, you can close the environment:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你可以关闭环境：
- en: '[PRE42]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: You can see that starting with no information about how to balance the pole,
    the agent, using a DQN, is able to balance the pole for more and more time (on
    average) as it learns. Starting from a blank slate, the agent is able to build
    information/knowledge to fulfill the required goal. Remarkable!
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，从一开始没有任何平衡杆的平衡信息，到代理使用 DQN 逐渐能够平衡杆越来越长时间（平均值），随着学习的进行。起初一片空白，代理能够构建信息/知识来完成目标，真是了不起！
- en: DQN to play a game of Atari
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN 用于玩 Atari 游戏
- en: In the preceding section, we trained a DQN to balance a pole in CartPole. It
    was a simple problem, and thus we could solve it using a perceptron model. But
    imagine if the environment state was just the CartPole visual as we humans see
    it. With raw pixel values as the input state space, our previous DQN will not
    work. What we need is a convolutional neural network. Next, we build one based
    on the seminal paper on DQNs, *Playing Atari with Deep Reinforcement Learning*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们训练了一个 DQN 来平衡 CartPole。这个问题比较简单，因此我们可以使用感知机模型来解决。但是，假设环境状态仅仅是我们人类看到的
    CartPole 视觉画面。如果输入状态空间是原始的像素值，我们之前的 DQN 将无法工作。我们需要的是卷积神经网络（CNN）。接下来，我们将基于 DQN
    经典论文《*通过深度强化学习玩 Atari*》来构建一个模型。
- en: Most of the code will be similar to the DQN for CartPole, but there will be
    significant changes in the DQN network itself, and how we preprocess the state
    that we obtain from the environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分代码将与用于 CartPole 的 DQN 相似，但 DQN 网络本身以及我们如何处理从环境中获得的状态将发生显著变化。
- en: 'First, let us see the change in the way state space is processed. *Figure 11.10*
    shows one of the Atari games, Breakout:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看状态空间处理方式的变化。*图 11.10* 显示了其中一款 Atari 游戏——Breakout：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_13.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B18331_11_13.png)'
- en: 'Figure 11.10: A screenshot of the Atari game, Breakout'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：Atari 游戏 Breakout 的截图
- en: 'Now, if you look at the image, not all of it contains relevant information:
    the top part has redundant information about the score, the bottom part has unnecessary
    blank space, and the image is colored. To reduce the burden on our model, it is
    best to remove the unnecessary information, so we crop the image, convert it to
    grayscale, and make it a square of size 84 × 84 (as in the paper). Here is the
    code to preprocess the input raw pixels:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你看看这张图像，并非所有部分都包含相关信息：顶部有关于分数的冗余信息，底部有不必要的空白区域，图像还带有颜色。为了减轻模型的负担，最好移除不必要的信息，因此我们将裁剪图像，转换为灰度图，并将其调整为
    84 × 84 的正方形（如论文中所示）。以下是预处理输入原始像素的代码：
- en: '[PRE43]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Another important issue is that just by looking at the image at one time step,
    how can the agent know whether the ball is going up or down? One way could be
    to use LSTM along with a CNN to keep a record of the past and hence the ball movement.
    The paper, however, used a simple technique. Instead of a single state frame,
    it concatenated the state space for the past four time steps together as one input
    to the CNN; that is, the network sees four past frames of the environment as its
    input. The following is the code for combining the present and previous states:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要问题是，仅通过一次观察图像，代理如何知道小球是上升还是下降？一种方法是使用 LSTM 和 CNN 结合，记录过去的状态，从而追踪小球的运动。然而，论文中采用了一种简单的技术：它将过去四个时间步的状态空间连接在一起作为
    CNN 的输入，而不是单一的状态帧；也就是说，网络将过去四帧环境画面作为输入。以下是将当前状态和过去状态组合的代码：
- en: '[PRE44]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The model was defined in the `__init__` function. We modify the function to
    now have a CNN with an input of (84 × 84 × 4) representing four state frames each
    of size 84 × 84:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是在 `__init__` 函数中定义的。我们修改该函数，使其现在具有一个输入为（84 × 84 × 4）的 CNN，表示四个大小为 84 × 84
    的状态帧：
- en: '[PRE45]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Lastly, we will need to make a minor change in the `train` function. We will
    need to call the new `preprocess` function, along with the `combine_images` function
    to ensure that four frames are concatenated:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要在 `train` 函数中做一个小改动。我们需要调用新的 `preprocess` 函数，并使用 `combine_images` 函数来确保四帧图像被连接在一起：
- en: '[PRE46]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: That’s all. We can now train the agent for playing Breakout. The complete code
    is available on GitHub repository ([https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11](https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11))
    of this chapter in the file `DQN_Atari_v2.ipynb`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们现在可以训练智能体来玩 Breakout 游戏。本章节的完整代码可以在 GitHub 仓库中找到，链接为 [https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11](https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11)，文件名为
    `DQN_Atari_v2.ipynb`。
- en: DQN variants
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DQN 变种
- en: After the unprecedented success of DQNs, the interest in RL increased and many
    new RL algorithms came into being. Next, we see some of the algorithms that are
    based on DQNs. They all use DQNs as the base and build upon it.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DQN 取得前所未有的成功之后，强化学习（RL）领域的关注度增加，许多新的 RL 算法应运而生。接下来，我们将看到一些基于 DQN 的算法。它们都以
    DQN 为基础，并在此基础上进行扩展。
- en: Double DQN
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Double DQN
- en: In DQNs, the agent uses the same *Q* value to both select and evaluate an action.
    This can cause a maximization bias in learning. For example, let us consider that
    for a state, *S*, all possible actions have true *Q* values of zero. Now, our
    DQN estimates will have some values above and some values below zero, and since
    we are choosing the action with the maximum *Q* value and later evaluating the
    *Q* value of each action using the same (maximized) estimated value function,
    we are overestimating *Q*—or in other words, our agent is over-optimistic. This
    can lead to unstable training and a low-quality policy. To deal with this issue,
    Hasselt et al. from DeepMind proposed the Double DQN algorithm in their paper
    *Deep Reinforcement Learning with Double Q-Learning*. In Double DQN, we have two
    Q-networks with the same architecture but different weights. One of the Q-networks
    is used to determine the action using the epsilon-greedy policy and the other
    is used to determine its value (Q-target).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DQN 中，智能体使用相同的 *Q* 值来选择和评估一个动作。这可能会导致学习中的最大化偏差。例如，假设对于某个状态 *S*，所有可能的动作的真实
    *Q* 值都为零。那么，我们的 DQN 估计值将有些高于零，有些低于零，并且由于我们选择具有最大 *Q* 值的动作，然后使用相同的（最大化的）估计值函数来评估每个动作的
    *Q* 值，我们就在高估 *Q* 值——换句话说，我们的智能体过于乐观。这可能导致训练不稳定和低质量的策略。为了解决这个问题，DeepMind 的 Hasselt
    等人在论文 *Deep Reinforcement Learning with Double Q-Learning* 中提出了 Double DQN 算法。在
    Double DQN 中，我们有两个具有相同架构但不同权重的 Q 网络。一个 Q 网络用于通过 epsilon-greedy 策略确定动作，另一个用于计算其值（Q
    目标）。
- en: 'If you recall in DQNs, the Q-target was given by:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在 DQN 中，Q 目标是通过以下公式给出的：
- en: '![](img/B18331_11_024.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_024.png)'
- en: 'Here, the action *A* was selected using the same DQN, *Q(S,A; W)*, where *W*
    is the training parameters of the network; that is, we are writing the *Q* value
    function along with its training parameter to emphasize the difference between
    vanilla DQNs and Double DQN:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，使用相同的 DQN *Q(S,A; W)* 选择了动作 *A*，其中 *W* 是网络的训练参数；也就是说，我们在编写 *Q* 值函数时，连同其训练参数一起，以强调
    vanilla DQN 和 Double DQN 之间的区别：
- en: '![](img/B18331_11_025.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_025.png)'
- en: 'In Double DQN, the equation for the target will now change. Now, the DQN *Q(S,A;W)*
    is used for determining the action and the DQN *Q(S,A;W’)* is used for calculating
    the target (notice the different weights). So, the preceding equation will change
    to:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Double DQN 中，目标的计算公式现在将发生变化。现在，DQN *Q(S,A;W)* 用于确定动作，而 DQN *Q(S,A;W’)* 用于计算目标（注意权重不同）。因此，前面的公式将变为：
- en: '![](img/B18331_11_026.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_026.png)'
- en: This simple change reduces the overestimation and helps us to train the agent
    faster and more reliably.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的变化减少了高估的情况，并帮助我们更快速、更可靠地训练智能体。
- en: Dueling DQN
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dueling DQN
- en: This architecture was proposed by Wang et al. in their paper *Dueling Network
    Architectures for Deep Reinforcement Learning* in 2015\. Like the DQN and Double
    DQN, it is also a model-free algorithm.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这个架构是 Wang 等人在 2015 年的论文 *Dueling Network Architectures for Deep Reinforcement
    Learning* 中提出的。与 DQN 和 Double DQN 一样，它也是一种无模型算法。
- en: 'Dueling DQN decouples the Q-function into the value function and advantage
    function. The value function, which we discussed earlier, represents the value
    of the state independent of any action. The advantage function, on the other hand,
    provides a relative measure of the utility (advantage/goodness) of action *A*
    in the state *S*. The Dueling DQN uses convolutional networks in the initial layers
    to extract the features from raw pixels. However, in the later stages, it is separated
    into two different networks, one approximating the value and another approximating
    the advantage. This ensures that the network produces separate estimates for the
    value function and the advantage function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: Dueling DQN 将 Q 函数解耦为价值函数和优势函数。我们之前讨论过的价值函数表示状态的价值，而不考虑任何动作。另一方面，优势函数提供了动作 *A*
    在状态 *S* 中的相对效用（优势/好处）度量。Dueling DQN 在初始层使用卷积网络从原始像素中提取特征。然而，在后期阶段，它被分为两个不同的网络，一个用于近似价值，另一个用于近似优势。这样可以确保网络为价值函数和优势函数提供独立的估计。
- en: '![](img/B18331_11_027.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_027.png)'
- en: Here, ![](img/B18331_10_024.png) is an array of the training parameters of the
    shared convolutional network (it is shared by both *V* and *A*), and ![](img/B18331_11_021.png)
    and ![](img/B18331_11_030.png) are the training parameters for the *Advantage*
    and *Value* estimator networks. Later, the two networks are recombined using an
    aggregating layer to estimate the *Q* value.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B18331_10_024.png) 是共享卷积网络的训练参数数组（它被 *V* 和 *A* 共享），而 ![](img/B18331_11_021.png)
    和 ![](img/B18331_11_030.png) 是 *Advantage* 和 *Value* 估计器网络的训练参数。稍后，两个网络通过聚合层重新组合，以估算
    *Q* 值。
- en: 'In *Figure 11.11*, you can see the architecture of Dueling DQN:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 11.11* 中，你可以看到 Dueling DQN 的架构：
- en: '![Graphical user interface  Description automatically generated](img/B18331_11_14.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面 自动生成的描述](img/B18331_11_14.png)'
- en: 'Figure 11.11: Visualizing the architecture of a Dueling DQN'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：可视化 Dueling DQN 的架构
- en: 'You may be wondering, what is the advantage of doing all of this? Why decompose
    *Q* if we will just be putting it back together? Well, decoupling the value and
    advantage functions allows us to know which states are valuable, without having
    to take into account the effect of each action for each state. There are many
    states that, irrespective of the action taken, are good or bad states: for example,
    having breakfast with your loved ones in a good resort is always a good state,
    and being admitted to a hospital emergency ward is always a bad state. Thus, separating
    value and advantage allows one to get a more robust approximation of the value
    function. Next, you can see a figure from the paper highlighting how in the Atari
    game Enduro, the value network learns to pay attention to the road, and the advantage
    network learns to pay attention only when there are cars immediately in front,
    so as to avoid a collision:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，这样做有什么好处？如果我们最后还要将它们组合起来，为什么要解构 *Q* 呢？其实，解耦价值和优势函数可以让我们知道哪些状态是有价值的，而不需要考虑每个状态下每个动作的影响。有很多状态，无论采取什么行动，都是好或坏的状态：例如，在一个好的度假村与亲人一起吃早餐始终是一个好状态，而被送进医院急诊室始终是一个坏状态。因此，分开价值和优势可以获得更稳健的价值函数近似。接下来，你可以看到来自论文的一幅图，展示了在Atari游戏《Enduro》中，价值网络学会关注道路，而优势网络只在前方有车时才关注，以避免碰撞：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_15.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 自动生成的描述](img/B18331_11_15.png)'
- en: 'Figure 11.12: In the Atari game Enduro, the value network learns to pay attention
    to the road (red spot), and the advantage network focuses only when other vehicles
    are immediately in front. Image source: https://arxiv.org/pdf/1511.06581.pdf'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：在 Atari 游戏 Enduro 中，价值网络学会关注道路（红点），而优势网络只在有其他车辆立即在前方时才关注。图像来源：https://arxiv.org/pdf/1511.06581.pdf
- en: 'The aggregate layer is implemented in a manner that allows one to recover both
    *V* and *A* from the given *Q*. This is achieved by enforcing that the advantage
    function estimator has zero advantage at the chosen action:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合层的实现方式使得可以从给定的 *Q* 中恢复 *V* 和 *A*。这是通过强制使优势函数估计器在选择的动作下具有零优势来实现的：
- en: '![](img/B18331_11_031.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_11_031.png)'
- en: In the paper, Wang et al. reported that the network is more stable if the max
    operation is replaced by the average operation. This is so because the speed of
    change in advantage is now the same as the change in average, instead of the optimal
    (max) value.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，Wang 等人报告称，如果将最大值操作替换为平均值操作，网络会更加稳定。原因是，优势的变化速度现在与平均值的变化速度相同，而不是与最优（最大）值的变化速度相同。
- en: Rainbow
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Rainbow
- en: 'Rainbow is the current state-of-the-art DQN variant. Technically, to call it
    a DQN variant would be wrong. In essence, it is an ensemble of many DQN variants
    combined together into a single algorithm. It modifies the distributional RL [6]
    loss to multi-step loss and combines it with Double DQN using a greedy action.
    Quoting from the paper:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow 是当前最先进的 DQN 变体。从技术上讲，称其为 DQN 变体是不准确的。实际上，它是许多 DQN 变体的集成，组合成一个单一的算法。它将分布式强化学习[6]损失修改为多步损失，并将其与
    Double DQN 和贪心策略相结合。论文中引用如下：
- en: The network architecture is a dueling network architecture adapted for use with
    return distributions. The network has a shared representation ![](img/B18331_11_032.png),
    which is then fed into a value stream ![](img/B18331_11_033.png) with *N*[atoms]
    outputs, and into an advantage stream ![](img/B18331_11_034.png) with *N*[atoms]×*N*[actions]
    outputs, where a1ξ(fξ(s), a)will denote the output corresponding to atom i and
    action a. For each atom *z*[i], the value and advantage streams are aggregated,
    as in Dueling DQN, and then passed through a softmax layer to obtain the normalised
    parametric distributions used to estimate the returns’ distributions.
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 网络架构是一个适用于回报分布的对抗性网络架构。该网络具有一个共享的表示 ![](img/B18331_11_032.png)，然后被输入到具有 *N*[原子]
    输出的值流 ![](img/B18331_11_033.png)，以及具有 *N*[原子]×*N*[动作] 输出的优势流 ![](img/B18331_11_034.png)，其中
    a1ξ(fξ(s), a) 表示与原子 i 和动作 a 对应的输出。对于每个原子 *z*[i]，值流和优势流按 Dueling DQN 的方式聚合，然后通过
    softmax 层以获得用于估计回报分布的标准化参数分布。
- en: 'Rainbow combines six different RL algorithms:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow 集成了六种不同的强化学习算法：
- en: N-step returns
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N步回报
- en: Distributional state-action value learning
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式状态-动作值学习
- en: Dueling networks
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗网络
- en: Noisy networks
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声网络
- en: Double DQN
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双重 DQN
- en: Prioritized experience replay
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: Till now, we’ve considered value-based reinforcement learning algorithms. In
    the next section, we will learn about policy-based reinforcement learning algorithms.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了基于价值的强化学习算法。在下一节中，我们将学习基于策略的强化学习算法。
- en: Deep deterministic policy gradient
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度确定性策略梯度
- en: 'The DQN and its variants have been very successful in solving problems where
    the state space is continuous and action space is discrete. For example, in Atari
    games, the input space consists of raw pixels, but actions are discrete—[**up**,
    **down**, **left**, **right**, **no-op**]. How do we solve a problem with continuous
    action space? For instance, say an RL agent driving a car needs to turn its wheels:
    this action has a continuous action space.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: DQN 及其变体在解决状态空间连续且动作空间离散的问题上非常成功。例如，在 Atari 游戏中，输入空间由原始像素构成，但动作是离散的—[**上**，**下**，**左**，**右**，**无操作**]。如何解决具有连续动作空间的问题呢？例如，假设一个强化学习智能体驾驶一辆车需要转动车轮：这个动作具有连续的动作空间。
- en: One way to handle this situation is by discretizing the action space and continuing
    with a DQN or its variants. However, a better solution would be to use a policy
    gradient algorithm. In policy gradient methods, the policy ![](img/B18331_11_035.png)
    is approximated directly.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种情况的一种方法是通过离散化动作空间并继续使用 DQN 或其变体。然而，更好的解决方案是使用策略梯度算法。在策略梯度方法中，策略 ![](img/B18331_11_035.png)
    直接进行近似。
- en: 'A neural network is used to approximate the policy; in the simplest form, the
    neural network learns a policy for selecting actions that maximize the rewards
    by adjusting its weights using the steepest gradient ascent, hence the name: policy
    gradients.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络用于近似策略；在最简单的形式下，神经网络学习一种选择动作的策略，以通过调整权重来最大化回报，这一过程使用最陡峭的梯度上升，因此得名：策略梯度。
- en: In this section, we will focus on the **Deep Deterministic Policy Gradient**
    (**DDPG**) algorithm, another successful RL algorithm by Google’s DeepMind in
    2015\. DDPG is implemented using two networks; one called the actor network and
    the other called the critic network.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点介绍 **深度确定性策略梯度**（**DDPG**）算法，这是 Google DeepMind 于 2015 年提出的另一种成功的强化学习算法。DDPG
    使用两个网络进行实现；一个叫做演员网络，另一个叫做评论员网络。
- en: 'The actor network approximates the optimal policy deterministically, that is,
    it outputs the most preferred action for any given input state. In essence, the
    actor is learning. The critic on the other hand evaluates the optimal action value
    function using the actor’s most preferred action. Before going further, let us
    contrast this with the DQN algorithm that we discussed in the preceding section.
    In *Figure 11.13*, you can see the general architecture of DDPG:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络确定性地逼近最优策略，也就是说，它为任何给定的输入状态输出最优的动作。从本质上讲，演员在学习。评论员则使用演员最优选择的动作来评估最优动作值函数。在继续之前，让我们将其与前一节讨论的DQN算法进行对比。在*图11.13*中，你可以看到DDPG的整体架构：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B18331_11_16.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![一张计算机截图  自动生成的描述可信度较低](img/B18331_11_16.png)'
- en: 'Figure 11.13: Architecture of the DDPG model'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13：DDPG模型架构
- en: On the left-hand side of *Figure 11.13* is the critic network, it takes as input
    the state vector, *S*, and action taken, *A*. The output of the network is the
    *Q* value for that state and action. The right-hand figure shows the actor network.
    It takes as input the state vector, S, and predicts the optimum action, A, to
    be taken. In the figure, we have shown both the actor and critic to be of four
    layers. This is only for demonstration purposes.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图11.13*的左侧是评论员网络，它以状态向量*S*和所采取的动作*A*为输入。网络的输出是该状态和动作对应的*Q*值。右侧的图示则展示了演员网络。它以状态向量S为输入，并预测要采取的最优动作A。在图中，我们展示了演员和评论员网络各有四层，这仅是为了演示目的。
- en: The actor network outputs the most preferred action; the critic takes as input
    both the input state and action taken and evaluates its *Q* value. To train the
    critic network, we follow the same procedure as with a DQN; that is, we try to
    minimize the difference between the estimated *Q* value and the target *Q* value.
    The gradient of the *Q* value over actions is then propagated back to train the
    actor network. So, if the critic is good enough, it will force the actor to choose
    actions with optimal value functions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络输出最优的动作；评论员网络则以输入状态和所采取的动作为输入，并评估其*Q*值。为了训练评论员网络，我们遵循与DQN相同的过程；也就是说，我们尝试最小化估计的*Q*值与目标*Q*值之间的差异。然后，动作的*Q*值的梯度会被传播回去，训练演员网络。因此，如果评论员足够优秀，它将迫使演员选择具有最优价值函数的动作。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Reinforcement learning has in recent years seen a lot of progress. To summarize
    all of that in a single chapter is not possible. However, in this chapter, we
    focused on the recent successful RL algorithms. The chapter started by introducing
    the important concepts in the RL field, its challenges, and the solutions to move
    forward. Next, we delved into two important RL algorithms: the DQN and DDPG algorithms.
    Toward the end of this chapter, we covered important topics in the field of deep
    learning.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，强化学习取得了显著进展。将所有这些进展总结在一章中是不可能的。然而，在这一章中，我们重点讨论了近年来成功的RL算法。本章从介绍RL领域的重要概念、挑战以及前进的解决方案开始。接下来，我们深入探讨了两个重要的RL算法：DQN和DDPG算法。章末，我们还涵盖了深度学习领域的一些重要话题。
- en: In the next chapter, we will move on to applying what we have learned to production.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续将所学应用于实际生产。
- en: References
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'MIT Technology Review covers OpenAI experiments on reinforcement learning:
    [https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/](https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/)'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 《MIT技术评论》报道了OpenAI在强化学习方面的实验：[https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/](https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/)
- en: Coggan, Melanie. (2014). *Exploration and Exploitation in Reinforcement Learning*.
    Research supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Coggan, Melanie. (2014). *强化学习中的探索与利用*。由Doina Precup教授监督的研究，McGill大学CRA-W DMP项目。
- en: Lin, Long-Ji. (1993). *Reinforcement learning for robots using neural networks*.
    No. CMU-CS-93-103\. Carnegie-Mellon University Pittsburgh PA School of Computer
    Science.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lin, Long-Ji. (1993). *使用神经网络的机器人强化学习*。编号CMU-CS-93-103。卡内基梅隆大学计算机科学学院，宾夕法尼亚州，匹兹堡。
- en: Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. (2015). *Prioritized
    Experience Replay*. arXiv preprint arXiv:1511.05952
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schaul, Tom, John Quan, Ioannis Antonoglou, 和 David Silver. (2015). *优先经验回放*。arXiv预印本arXiv:1511.05952
- en: 'Sutton R., Barto A. *Chapter 4, Reinforcement Learning*. MIT Press: [https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutton R.、Barto A. *第四章，强化学习*。MIT出版社：[https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
- en: Dabney W., Rowland M., Bellemare M G., and Munos R. (2018). *Distributional
    Reinforcement Learning with Quantile Regression*. In Thirty-Second AAAI Conference
    on Artificial Intelligence.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dabney W.、Rowland M.、Bellemare M G. 和 Munos R.（2018）。*基于分位数回归的分布式强化学习*。发表于第三十二届AAAI人工智能会议。
- en: 'Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). *Rainbow: Combining
    improvements in Deep Reinforcement Learning*. In Thirty-Second AAAI Conference
    on Artificial Intelligence.'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hessel, M.、Modayil, J.、Van Hasselt, H.、Schaul, T.、Ostrovski, G.、Dabney, W.、Horgan,
    D.、Piot, B.、Azar, M. 和 Silver, D.（2018）。*Rainbow：结合深度强化学习中的改进*。发表于第三十二届AAAI人工智能会议。
- en: Details about different environments can be obtained from [https://www.gymlibrary.ml/](https://www.gymlibrary.ml/)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于不同环境的详细信息可以在[https://www.gymlibrary.ml/](https://www.gymlibrary.ml/)找到
- en: Wiki pages are maintained for some environments at [https://github.com/openai/gym/wiki](https://github.com/openai/gym/wiki)
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些环境的维基页面可以在[https://github.com/openai/gym/wiki](https://github.com/openai/gym/wiki)找到
- en: Details regarding installation instructions and dependencies can be obtained
    from [https://github.com/openai/gym](https://github.com/openai/gym)
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于安装说明和依赖项的详细信息可以在[https://github.com/openai/gym](https://github.com/openai/gym)找到
- en: 'Link to the paper by DeepMind, *Asynchronous Methods for Deep Reinforcement
    Learning*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DeepMind的论文《*深度强化学习的异步方法*》链接：[https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)
- en: 'This is a blog post by Andrej Karpathy on reinforcement learning: [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是Andrej Karpathy关于强化学习的博客文章：[http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)
- en: 'Glorot X. and Bengio Y. (2010). *Understanding the difficulty of training deep
    feedforward neural networks*. Proceedings of the Thirteenth International Conference
    on Artificial Intelligence and Statistics: [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Glorot X. 和 Bengio Y.（2010）。*理解训练深度前馈神经网络的困难*。第十三届国际人工智能与统计会议论文集：[http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
- en: 'A good read on why RL is still hard to crack: [https://www.alexirpan.com/2018/02/14/rl-hard.xhtml](https://www.alexirpan.com/2018/02/14/rl-hard.xhtml)'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于为什么强化学习仍然很难突破的精彩读物：[https://www.alexirpan.com/2018/02/14/rl-hard.xhtml](https://www.alexirpan.com/2018/02/14/rl-hard.xhtml)
- en: Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
    ... & Wierstra, D. (2015). *Continuous control with deep reinforcement learning.
    arXiv preprint arXiv:1509.02971*.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lillicrap, T. P.、Hunt, J. J.、Pritzel, A.、Heess, N.、Erez, T.、Tassa, Y.、... &
    Wierstra, D.（2015）。*使用深度强化学习进行连续控制。arXiv预印本arXiv:1509.02971*。
- en: Join our book’s Discord space
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，结识志同道合的人，并与超过2000名成员一起学习：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
