- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Neural Network Foundations with TF
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TF 构建神经网络基础
- en: In this chapter, we learn the basics of TensorFlow, an open-source library developed
    by Google for machine learning and deep learning. In addition, we introduce the
    basics of neural networks and deep learning, two areas of machine learning that
    have had incredible Cambrian growth during the last few years. The idea behind
    this chapter is to provide all the tools needed to do basic but fully hands-on
    deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习 TensorFlow 的基础知识，这是 Google 开发的一个开源库，用于机器学习和深度学习。此外，我们还将介绍神经网络和深度学习的基础知识，这两个领域在过去几年中经历了令人惊讶的爆发性增长。本章的目的是提供进行基本但完全动手操作的深度学习所需的所有工具。
- en: 'We will learn:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习：
- en: What TensorFlow and Keras are
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 是什么
- en: An introduction to neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: What the perceptron and multi-layer perceptron are
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知机和多层感知机是什么
- en: 'A real example: recognizing handwritten digits'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个实际的例子：识别手写数字
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp1](https://packt.link/dltfchp1).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 [https://packt.link/dltfchp1](https://packt.link/dltfchp1) 找到。
- en: Let’s begin!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What is TensorFlow (TF)?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow（TF）？
- en: TensorFlow is a powerful open-source software library developed by the Google
    Brain Team for deep neural networks, the topic covered in this book. It was first
    made available under the Apache 2.0 License in November 2015 and has since grown
    rapidly; as of May 2022, its GitHub repository ([https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow))
    has more than 129,000 commits, with roughly 3,100 contributors. This in itself
    provides a measure of the popularity of TensorFlow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是 Google Brain 团队开发的一个强大的开源软件库，用于深度神经网络，这是本书所涉及的主题。它于 2015 年 11 月首次以
    Apache 2.0 许可证发布，并迅速发展；截至 2022 年 5 月，它的 GitHub 仓库（[https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)）已有超过
    129,000 次提交，约有 3,100 名贡献者。仅此可以作为衡量 TensorFlow 受欢迎程度的标准。
- en: 'Let us first learn what exactly TensorFlow is and why it is so popular among
    deep neural network researchers and engineers. Google calls it “an open-source
    software library for machine intelligence,” but since there are so many other
    deep learning libraries like PyTorch ([https://pytorch.org/](https://pytorch.org/)),
    Caffe ([https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/)),
    and MXNet ([https://mxnet.apache.org/](https://mxnet.apache.org/)), what makes
    TensorFlow special? Most other deep learning libraries, like TensorFlow, have
    auto-differentiation (a useful mathematical tool used for optimization), many
    are open-source platforms. Most of them support the CPU/GPU option, have pretrained
    models, and support commonly used NN architectures like recurrent neural networks,
    convolutional neural networks, and deep belief networks. So, what else is there
    in TensorFlow? Let me list the top features:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解 TensorFlow 究竟是什么，以及它为何在深度神经网络研究人员和工程师中如此受欢迎。Google 称其为“机器智能的开源软件库”，但由于还有很多其他的深度学习库，如
    PyTorch（[https://pytorch.org/](https://pytorch.org/)）、Caffe（[https://caffe.berkeleyvision.org/](https://caffe.berkeleyvision.org/)）和
    MXNet（[https://mxnet.apache.org/](https://mxnet.apache.org/)），那么是什么让 TensorFlow
    与众不同呢？像 TensorFlow 这样的深度学习库大多数都有自动微分（一个用于优化的有用数学工具），许多都是开源平台。它们大多数支持 CPU/GPU 选项，拥有预训练模型，并支持常用的神经网络架构，如递归神经网络、卷积神经网络和深度信念网络。那么，TensorFlow
    还有什么特别之处呢？让我列举一下它的主要特点：
- en: It works with all popular languages such as Python, C++, Java, R, and Go. TensorFlow
    provides stable Python and C++ APIs, as well as a non-guaranteed backward-compatible
    API for other languages.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它适用于所有流行的编程语言，如 Python、C++、Java、R 和 Go。TensorFlow 提供了稳定的 Python 和 C++ API，以及一个对其他语言不保证向后兼容的
    API。
- en: Keras – a high-level neural network API that has been integrated with TensorFlow
    (in 2.0 Keras became the standard API for interacting with TensorFlow). This API
    specifies how software components should interact.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras – 一个高级神经网络 API，已经与 TensorFlow 集成（在 2.0 版本中，Keras 成为与 TensorFlow 交互的标准
    API）。该 API 指定了软件组件应如何交互。
- en: TensorFlow allows model deployment and ease of use in production.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 允许在生产环境中部署模型，并提供易用性。
- en: Most importantly, TensorFlow has very good community support.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是，TensorFlow 拥有非常好的社区支持。
- en: 'The number of stars on GitHub (see *Figure 1.1*) is a measure of popularity
    for all open-source projects. As of May 2022, TensorFlow, Keras, and PyTorch have
    165K, 55K, and 56K stars respectively, which makes TensorFlow the most popular
    framework for machine learning:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 上的星标数（见 *图 1.1*）是衡量所有开源项目受欢迎程度的指标。截止到2022年5月，TensorFlow、Keras 和 PyTorch
    的星标数分别为165K、55K和56K，使得 TensorFlow 成为最受欢迎的机器学习框架：
- en: '![](img/B18331_01_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_01.png)'
- en: 'Figure 1.1: Number of stars for various deep learning projects on GitHub'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：GitHub 上各个深度学习项目的星标数
- en: What is Keras?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Keras？
- en: Keras is a beautiful API for composing building blocks to create and train deep
    learning models. Keras can be integrated with multiple deep learning engines including
    Google TensorFlow, Microsoft CNTK, Amazon MXNet, and Theano. Starting with TensorFlow
    2.0, Keras, the API developed by François Chollet, has been adopted as the standard
    high-level API, largely simplifying coding and making programming more intuitive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个优美的 API，用于组合构建模块以创建和训练深度学习模型。Keras 可以与多个深度学习引擎集成，包括 Google TensorFlow、Microsoft
    CNTK、Amazon MXNet 和 Theano。从 TensorFlow 2.0 开始，由 François Chollet 开发的 Keras 被作为标准的高级
    API 采纳，极大简化了编码过程，使编程变得更加直观。
- en: Introduction to neural networks
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络简介
- en: Artificial neural networks (briefly, “nets” or ANNs) represent a class of machine
    learning models loosely inspired by studies about the central nervous systems
    of mammals. Each ANN is made up of several interconnected “neurons,” organized
    in “layers.” Neurons in one layer pass messages to neurons in the next layer (they
    “fire,” in jargon terms) and this is how the network computes things. Initial
    studies were started in the early 1950s with the introduction of the “perceptron”
    [1], a two-layer network used for simple operations, and further expanded in the
    late 1960s with the introduction of the “back-propagation” algorithm used for
    efficient multi-layer network training (according to [2] and [3]). Some studies
    argue that these techniques have roots dating further back than normally cited
    [4].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（简称“神经网络”或 ANN）代表了一类机器学习模型，灵感来自于对哺乳动物中枢神经系统的研究。每个神经网络由几个互相连接的“神经元”组成，这些神经元按“层”组织。一个层中的神经元将信息传递给下一个层中的神经元（在术语中称为“激活”），这就是网络进行计算的方式。最初的研究始于20世纪50年代初，当时引入了“感知机”[1]，这是一种用于简单操作的两层网络，随后在60年代末期引入了“反向传播”算法，用于高效的多层网络训练（参考[2]和[3]）。一些研究认为，这些技术的起源可能比通常所说的还要更早[4]。
- en: 'Neural networks were a topic of intensive academic studies up until the 1980s,
    at which point other simpler approaches became more relevant. However, there has
    been a resurgence of interest since the mid 2000s, mainly thanks to three factors:
    a breakthrough fast learning algorithm proposed by G. Hinton [3], [5], and [6];
    the introduction of GPUs around 2011 for massive numeric computation; and the
    availability of big collections of data for training.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络曾是1980年代前的学术研究重点，之后其他更简单的方法变得更为相关。然而，自2000年代中期以来，随着三个因素的推动，神经网络再次引起了人们的广泛关注：G.
    Hinton 提出的突破性快速学习算法[3]，[5]，[6]；2011年左右引入的用于大规模数值计算的GPU；以及可用于训练的大量数据集。
- en: These improvements opened the route for modern “deep learning,” a class of neural
    networks characterized by a significant number of layers of neurons that are able
    to learn rather sophisticated models, based on progressive levels of abstraction.
    People began referring to it as “deep” when it started utilizing 3–5 layers a
    few years ago. Now, networks with more than 200 layers are commonplace!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改进为现代“深度学习”铺平了道路，深度学习是一类神经网络，具有大量的神经元层，能够基于渐进的抽象层次学习复杂的模型。几年前，人们开始称其为“深度”学习，当时它利用了3到5层的神经网络。而现在，超过200层的网络已经很常见！
- en: This learning via progressive abstraction resembles vision models that have
    evolved over millions of years within the human brain. Indeed, the human visual
    system is organized into different layers. First, our eyes are connected to an
    area of the brain named the visual cortex (V1), which is located in the lower
    posterior part of our brain. This area is common to many mammals and has the role
    of discriminating basic properties like small changes in visual orientation, spatial
    frequencies, and colors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通过渐进抽象进行学习的方式类似于人类大脑中数百万年来进化的视觉模型。事实上，人类的视觉系统被组织成不同的层次。首先，我们的眼睛与大脑中的一个区域——视觉皮层（V1）相连接，这个区域位于大脑后部的下方。这个区域在许多哺乳动物中都很常见，负责区分基本的视觉属性，如视觉方向的微小变化、空间频率和颜色。
- en: 'It has been estimated that V1 consists of about 140 million neurons, with tens
    of billions of connections between them. V1 is then connected to other areas,
    V2, V3, V4, V5, and V6 doing progressively more complex image processing and recognition
    of more sophisticated concepts, such as shapes, faces, animals, and many more.
    It has been estimated that there are ~16 billion human cortical neurons and about
    10–25% of the human cortex is devoted to vision [7]. Deep learning has taken some
    inspiration from this layer-based organization of the human visual system: early
    artificial neuron layers learn basic properties of images while deeper layers
    learn more sophisticated concepts.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 据估计，V1 由大约 1.4 亿个神经元组成，神经元之间有数十亿个连接。V1 然后与其他区域相连，V2、V3、V4、V5 和 V6 逐步进行更复杂的图像处理和更高级概念的识别，如形状、面孔、动物等。估计人类皮层约有
    160 亿个神经元，人类皮层的约 10-25% 用于视觉处理 [7]。深度学习从人类视觉系统的这种分层组织中汲取了一些灵感：早期的人工神经元层学习图像的基本特性，而较深的层学习更复杂的概念。
- en: This book covers several major aspects of neural networks by providing working
    nets in TensorFlow. So, let’s start!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本书通过提供在 TensorFlow 中工作的神经网络，涵盖了神经网络的几个主要方面。那么，让我们开始吧！
- en: Perceptron
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机
- en: 'The “perceptron” is a simple algorithm that, given an input vector *x* of *m*
    values (*x*[1], *x*[2],..., and *x*[m]), often called input features or simply
    features, outputs either a *1* (“yes”) or a *0* (“no”). Mathematically, we define
    a function:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: “感知机”是一个简单的算法，给定一个包含 *m* 个值（*x*[1], *x*[2], ..., 和 *x*[m]）的输入向量 *x*，通常称为输入特征或简而言之特征，它输出
    *1*（“是”）或 *0*（“否”）。从数学上讲，我们定义一个函数：
- en: '![](img/B18331_01_001.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_001.png)'
- en: 'Where *w* is a vector of weights, ![](img/B18331_01_002.png) is the dot product
    ![](img/B18331_01_003.png), and *b* is the bias. If you remember elementary geometry,
    *wx* + *b* defines a boundary hyperplane that changes position according to the
    values assigned to *w* and *b*. Note that a hyperplane is a subspace whose dimension
    is one fewer than that of its ambient space. See (*Figure 1.2*) for an example:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w* 是权重向量，![](img/B18331_01_002.png) 是点积 ![](img/B18331_01_003.png)，*b* 是偏置。如果你记得初等几何，*wx*
    + *b* 定义了一个边界超平面，该超平面根据分配给 *w* 和 *b* 的值而改变位置。请注意，超平面是一个子空间，其维度比它所在的环境空间少一维。请参见（*图
    1.2*）以获得示例：
- en: '![](img/B18331_01_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_02.png)'
- en: 'Figure 1.2: An example of a hyperplane'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：超平面的示例
- en: In other words, this is a very simple but effective algorithm! For example,
    given three input features, the amounts of red, green, and blue in a color, the
    perceptron could try to decide whether the color is “white” or not.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这是一个非常简单但有效的算法！例如，给定三个输入特征，即颜色中的红色、绿色和蓝色的量，感知机可以尝试判断该颜色是否是“白色”。
- en: Note that the perceptron cannot express a “*maybe*” answer. It can answer “yes”
    (1) or “no” (0) if we understand how to define *w* and *b*. This is the “training”
    process that will be discussed in the following sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，感知机无法表达“*可能*”的答案。如果我们了解如何定义 *w* 和 *b*，它可以回答“是”（1）或“否”（0）。这就是接下来将讨论的“训练”过程。
- en: Our first example of TensorFlow code
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们的第一个 TensorFlow 代码示例
- en: 'There are three ways of creating a model in `tf.keras`: sequential API, functional
    API, and model subclassing. In this chapter, we will use the simplest one, `Sequential()`,
    while the other two are discussed in *Chapter 2*, *Regression and Classification*.
    A `Sequential()` model is a linear pipeline (a stack) of neural network layers.
    This code fragment defines a single layer with 10 artificial neurons that expect
    784 input variables (also known as features). Note that the net is “dense,” meaning
    that each neuron in a layer is connected to all neurons located in the previous
    layer, and to all the neurons in the following layer:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `tf.keras` 中有三种创建模型的方式：顺序 API、函数式 API 和模型子类化。在本章中，我们将使用最简单的一个，`Sequential()`，而另外两种将在
    *第 2 章*，*回归与分类* 中讨论。`Sequential()` 模型是一个神经网络层的线性管道（堆栈）。该代码片段定义了一个单层模型，其中包含 10
    个人工神经元，期望 784 个输入变量（也称为特征）。请注意，该网络是“密集”的，这意味着层中的每个神经元都与前一层中的所有神经元相连，并且与下一层中的所有神经元相连：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each neuron can be initialized with specific weights via the `''kernel_initializer''`
    parameter. There are a few choices, the most common of which are listed below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元可以通过 `'kernel_initializer'` 参数使用特定的权重进行初始化。这里有一些选择，最常见的几种如下：
- en: '`random_uniform`: Weights are initialized to uniformly random small values
    in the range (-0.05, 0.05).'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_uniform`：权重被初始化为在范围（-0.05，0.05）内均匀随机的小值。'
- en: '`random_normal`: Weights are initialized according to a Gaussian distribution,
    with zero mean and a small standard deviation of 0.05\. For those of you who are
    not familiar with a Gaussian distribution, think about a symmetric “bell curve”
    shape.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_normal`：权重根据高斯分布初始化，均值为零，标准差较小，为0.05。对于不熟悉高斯分布的朋友，可以将其想象成一个对称的“钟形曲线”。'
- en: '`zero`: All weights are initialized to zero.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zero`：所有权重初始化为零。'
- en: A full list is available online ([https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的列表可以在线查看（[https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)）。
- en: 'Multi-layer perceptron: our first example of a network'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知机：我们的第一个网络示例
- en: 'In this chapter, we present our first example of a network with multiple dense
    layers. Historically, “perceptron” was the name given to the model having one
    single linear layer, and as a consequence, if it has multiple layers, we call
    it a **Multi-Layer Perceptron** (**MLP**). Note that the input and the output
    layers are visible from the outside, while all the other layers in the middle
    are hidden – hence the name *hidden layers*. In this context, a single layer is
    simply a linear function and the MLP is therefore obtained by stacking multiple
    single layers one after the other:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了一个具有多个密集层的网络的第一个示例。历史上，“感知机”是指具有单一线性层的模型，因此，如果它具有多个层，我们称之为**多层感知机**（**MLP**）。请注意，输入层和输出层可以从外部看到，而其他所有位于中间的层都是隐藏的——因此有了*隐藏层*这个名称。在这个背景下，单一层仅仅是一个线性函数，因此，MLP是通过将多个单一层按顺序堆叠而成的：
- en: '![Diagram  Description automatically generated](img/Image5021.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/Image5021.png)'
- en: 'Figure 1.3: An example of multiple layer perceptron'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：多层感知机示例
- en: In *Figure 1.3* each node in the first hidden layer receives an input and “fires”
    (0,1) according to the values of the associated linear function. Then the output
    of the first hidden layer is passed to the second layer where another linear function
    is applied, the results of which are passed to the final output layer consisting
    of one single neuron. It is interesting to note that this layered organization
    vaguely resembles the organization of the human vision system, as we discussed
    earlier.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图1.3*中，第一隐藏层中的每个节点接收一个输入，并根据其相关的线性函数的值“激活” (0,1)。然后，第一隐藏层的输出被传递到第二层，应用另一个线性函数，结果再传递到最终的输出层，该输出层由一个神经元组成。有趣的是，这种层次结构在某种程度上类似于人类视觉系统的组织，正如我们之前讨论的。
- en: Problems in training the perceptron and solution
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机训练中的问题及解决方案
- en: Let’s consider a single neuron; what are the best choices for the weight *w*
    and the bias *b*? Ideally, we would like to provide a set of training examples
    and let the computer adjust the weight and the bias in such a way that the errors
    produced in the output are minimized.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个单一的神经元；*w*和*b*的最佳选择是什么？理想情况下，我们希望提供一组训练示例，并让计算机调整权重和偏置，使得输出中的误差最小化。
- en: In order to make this a bit more concrete, let’s suppose that we have a set
    of images of cats and another separate set of images not containing cats. Suppose
    that each neuron receives input from the value of a single pixel in the images.
    While the computer processes those images, we would like our neuron to adjust
    its weights and its bias so that we have fewer and fewer images wrongly recognized.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个问题更加具体化，假设我们有一组猫的图像和另一组不包含猫的图像。假设每个神经元接收来自图像中单个像素的输入。当计算机处理这些图像时，我们希望我们的神经元调整它的权重和偏置，使得错误识别的图像越来越少。
- en: 'This approach seems very intuitive, but it requires that a small change in
    the weights (or the bias) causes only a small change in the outputs. Think about
    it: if we have a big output jump, we cannot learn *progressively*. After all,
    kids learn little by little. Unfortunately, the perceptron does not show this
    “little-by-little” behavior. A perceptron is either a 0 or 1, and that’s a big
    jump that will not help in learning (see *Figure 1.4*):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法看起来很直观，但它要求权重（或偏置）的微小变化仅导致输出的微小变化。想一想：如果输出跳跃很大，我们就无法*逐步学习*。毕竟，孩子们是一点一点地学习的。不幸的是，感知机并没有表现出这种“逐步”行为。感知机要么是0，要么是1，这是一个大跳跃，无法帮助学习（参见*图1.4*）：
- en: '![](img/B18331_01_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_04.png)'
- en: 'Figure 1.4: Example of a perceptron – either a 0 or 1'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：感知机示例——0或1
- en: We need something different, something smoother. We need a function that progressively
    changes from 0 to 1 with no discontinuity. Mathematically, this means that we
    need a continuous function that allows us to compute the derivative. You might
    remember that in mathematics the derivative is the amount by which a function
    is changing at a given point. For functions with input given by real numbers,
    the derivative is the slope of the tangent line at a point on a graph. Later in
    this chapter we will see why derivatives are important for learning, when we will
    talk about gradient descent.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些不同的东西，一些更加平滑的东西。我们需要一个从 0 到 1 逐渐变化且没有间断的函数。从数学角度来看，这意味着我们需要一个连续函数，允许我们计算导数。你可能还记得，在数学中，导数是一个函数在某一点上变化的量。对于输入是实数的函数，导数是图中某一点切线的斜率。在本章稍后，我们将讨论为什么导数对学习很重要，尤其是我们会讲到梯度下降。
- en: 'Activation function: sigmoid'
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数：Sigmoid
- en: 'The sigmoid function, defined as ![](img/B18331_01_004.png) and represented
    in the image below, has small output changes in the range (0, 1) when the input
    varies in the range ![](img/B18331_01_005.png). Mathematically the function is
    continuous. A typical sigmoid function is represented in *Figure 1.5*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数，定义为 ![](img/B18331_01_004.png)，如下面的图所示，当输入在 ![](img/B18331_01_005.png)
    范围内变化时，其输出在 (0, 1) 范围内变化较小。从数学角度看，该函数是连续的。一个典型的 Sigmoid 函数在 *图 1.5* 中表示：
- en: '![](img/B18331_01_05.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_05.png)'
- en: 'Figure 1.5: A sigmoid function with output in the range (0,1)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：输出范围为 (0,1) 的 Sigmoid 函数
- en: A neuron can use the sigmoid for computing the nonlinear function ![](img/B18331_01_006.png).
    Note that if *z* = *wx* + *b* is very large and positive, then ![](img/B18331_01_007.png)
    so ![](img/B18331_01_008.png), while if *z* = *wx* + *b* is very large and negative,
    then ![](img/B18331_01_009.png) so ![](img/B18331_01_010.png). In other words,
    a neuron with sigmoid activation has a behavior similar to the perceptron, but
    the changes are gradual and output values such as 0.5539 or 0.123191 are perfectly
    legitimate. In this sense a sigmoid neuron can answer “maybe.”
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元可以使用 Sigmoid 来计算非线性函数 ![](img/B18331_01_006.png)。注意，如果 *z* = *wx* + *b*
    非常大且为正数，那么 ![](img/B18331_01_007.png)，因此 ![](img/B18331_01_008.png)；而如果 *z* =
    *wx* + *b* 非常大且为负数，那么 ![](img/B18331_01_009.png)，因此 ![](img/B18331_01_010.png)。换句话说，具有
    Sigmoid 激活的神经元行为类似于感知器，但变化是渐进的，像 0.5539 或 0.123191 这样的输出值是完全合法的。从这个意义上说，Sigmoid
    神经元可以回答“也许”。
- en: 'Activation function: tanh'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数：tanh
- en: 'Another useful activation function is tanh. It is defined as ![](img/B18331_01_011.png)
    whose shape is shown in *Figure 1.6*. Its outputs range from -1 to 1:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的激活函数是 tanh。它的定义为 ![](img/B18331_01_011.png)，其形状如 *图 1.6* 所示。其输出范围从 -1
    到 1：
- en: '![](img/B18331_01_06.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_06.png)'
- en: 'Figure 1.6: Tanh activation function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：Tanh 激活函数
- en: 'Activation function: ReLU'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数：ReLU
- en: The “sigmoid” is not the only kind of smooth activation function used for neural
    networks. Recently, a very simple function named **ReLU** (**REctified Linear
    Unit**) became very popular because it helps address some problems of optimizations
    observed with sigmoids. We will discuss these problems in more detail when we
    talk about vanishing gradient in *Chapter 5*, *Recurrent Neural Networks*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: “Sigmoid” 并不是用于神经网络的唯一平滑激活函数。最近，一个非常简单的函数叫做 **ReLU**（**修正线性单元**）变得非常流行，因为它有助于解决
    Sigmoid 中优化时出现的一些问题。我们将在 *第五章* 中更详细地讨论这些问题，特别是在讨论梯度消失时，我们会提到递归神经网络。
- en: 'A ReLU is simply defined as *f*(*x*) = *max*(0, *x*) and the nonlinear function
    is represented in *Figure 1.7*. As we can see, the function is zero for negative
    values and it grows linearly for positive values. The ReLU is also very simple
    to implement (generally three instructions are enough), while the sigmoid is a
    few orders of magnitude more. This helps to squeeze the neural networks onto an
    early GPU:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 被简单地定义为 *f*(*x*) = *max*(0, *x*)，其非线性函数在 *图 1.7* 中表示。如我们所见，该函数对于负值为零，对于正值线性增长。ReLU
    也非常容易实现（通常只需要三条指令），而 Sigmoid 实现起来复杂得多，可能需要几个数量级的计算。这有助于将神经网络压缩到早期的 GPU 上：
- en: '![](img/B18331_01_07.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_07.png)'
- en: 'Figure 1.7: A ReLU function'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：ReLU 函数
- en: 'Two additional activation functions: ELU and Leaky ReLU'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个额外的激活函数：ELU 和 Leaky ReLU
- en: Sigmoid and ReLU are not the only activation functions used for learning.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 和 ReLU 并不是用于学习的唯一激活函数。
- en: '**Exponential Linear Unit** (**ELU**) is defined as ![](img/B18331_01_012.png)
    for ![](img/B18331_01_013.png) and its plot is represented in *Figure 1.8*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**指数线性单元**（**ELU**）定义为！[](img/B18331_01_012.png)，当![](img/B18331_01_013.png)时，其图像如*图
    1.8*所示：'
- en: '![](img/B18331_01_08.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_08.png)'
- en: 'Figure 1.8: An ELU function'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：一个ELU函数
- en: 'LeakyReLU is defined as ![](img/B18331_01_014.png) for ![](img/B18331_01_013.png)
    and its plot is represented in *Figure 1.9*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LeakyReLU定义为！[](img/B18331_01_014.png)，当![](img/B18331_01_013.png)时，其图像如*图 1.9*所示：
- en: '![](img/B18331_01_09.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_09.png)'
- en: 'Figure 1.9: A LeakyReLU function'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：一个LeakyReLU函数
- en: Both the functions allow small updates if *x* is negative, which might be useful
    in certain conditions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数允许在*x*为负时进行小幅更新，这在某些条件下可能会很有用。
- en: Activation functions
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'Sigmoid, Tanh, ELU, Leaky ReLU, and ReLU are generally called *activation functions*
    in neural network jargon. In the gradient descent section, we will see that those
    gradual changes typical of sigmoid and ReLU functions are the basic building blocks
    to developing a learning algorithm that adapts little by little, by progressively
    reducing the mistakes made by our nets. An example of using the activation function
    ![](img/B18331_01_016.png) with the (*x*[1], *x*[2],..., *x*[m]) input vector,
    the (*w*[1], *w*[2],..., *w*[m]) weight vector, the *b* bias, and the ![](img/B18331_01_017.png)
    summation is given in *Figure 1.10* (note that TensorFlow supports many activation
    functions, a full list of which is available online):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid、Tanh、ELU、Leaky ReLU 和 ReLU 通常被称为神经网络术语中的*激活函数*。在梯度下降部分，我们将看到这些渐变变化，尤其是Sigmoid和ReLU函数的渐变变化，是开发一种学习算法的基本构建块，这种算法通过逐渐减少网络所犯的错误，从而一点点地进行调整。一个使用激活函数的示例！[](img/B18331_01_016.png)，输入向量为(*x*[1],
    *x*[2],..., *x*[m])，权重向量为(*w*[1], *w*[2],..., *w*[m])，偏置为*b*，汇总为！[](img/B18331_01_017.png)，如*图
    1.10*所示（请注意，TensorFlow支持许多激活函数，完整的函数列表可以在线查看）：
- en: '![Diagram  Description automatically generated](img/Image5194.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/Image5194.png)'
- en: 'Figure 1.10: An example of an activation function applied after a linear function'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.10：激活函数在线性函数后的应用示例
- en: 'In short: what are neural networks after all?'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简而言之：神经网络到底是什么？
- en: In one sentence, machine learning models are a way to compute a function that
    maps some inputs to their corresponding outputs. The function is nothing more
    than a number of addition and multiplication operations. However, when combined
    with a nonlinear activation and stacked in multiple layers, these functions can
    learn almost anything [8]. We also need a meaningful metric capturing what we
    want to optimize (this being the so-called loss function that we will cover later
    in the book), enough data to learn from, and sufficient computational power.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 用一句话来说，机器学习模型是一种计算函数的方式，它将某些输入映射到对应的输出。这个函数不过是一些加法和乘法操作。然而，当与非线性激活函数结合并堆叠成多层时，这些函数几乎可以学习任何东西[8]。我们还需要一个有意义的度量，来捕捉我们希望优化的内容（即所谓的损失函数，我们将在后面的章节中介绍），足够的数据来学习，以及足够的计算能力。
- en: Now, it might be beneficial to stop one moment and ask ourselves what “learning”
    really is? Well, we can say for our purposes that learning is essentially a process
    aimed at generalizing established observations [9] to predict future results.
    So, in short, this is exactly the goal we want to achieve with neural networks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可能有必要停下来问自己，“学习”究竟是什么？好吧，从我们的角度来看，学习本质上是一个旨在将已建立的观察结果[9]泛化，从而预测未来结果的过程。所以，简而言之，这正是我们希望通过神经网络实现的目标。
- en: 'A real example: recognizing handwritten digits'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个实际的例子：识别手写数字
- en: In this section we will build a network that can recognize handwritten numbers.
    To achieve this goal, we use MNIST ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)),
    a database of handwritten digits made up of a training set of 60,000 examples,
    and a test set of 10,000 examples. The training examples are annotated by humans
    with the correct answer. For instance, if the handwritten digit is the number
    “3,” then 3 is simply the label associated with that example.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个可以识别手写数字的网络。为了实现这个目标，我们使用MNIST（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)），这是一个包含60,000个训练示例和10,000个测试示例的手写数字数据库。训练示例由人工标注，标注的是正确的答案。例如，如果手写数字是“3”，那么3就是与该示例关联的标签。
- en: In machine learning, when a dataset with correct answers is available, we say
    that we can perform a form of *supervised learning*. In this case we can use training
    examples for improving our net. Testing examples also have the correct answer
    associated with each digit. In this case, however, the idea is to pretend that
    the label is unknown, let the network do the prediction, and then later on reconsider
    the label to evaluate how well our neural network has learned to recognize digits.
    Unsurprisingly, testing examples are just used to test the performance of our
    net.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，当有一个包含正确答案的数据集时，我们称之为可以执行某种形式的 *监督学习*。在这种情况下，我们可以使用训练示例来改进我们的网络。测试示例也有与每个数字相关的正确答案。然而，在这种情况下，目的是假装标签是未知的，让网络进行预测，然后再重新考虑标签，以评估我们的神经网络在识别数字方面的学习效果。毫不奇怪，测试示例仅用于测试我们网络的性能。
- en: 'Each MNIST image is in grayscale and consists of 28 x 28 pixels. A subset of
    these images of numbers is shown in *Figure 1.11*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每张 MNIST 图像为灰度图，包含 28 x 28 个像素。以下是一些数字图像的子集，显示在 *图 1.11* 中：
- en: '![mnist.png](img/B18331_01_11.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![mnist.png](img/B18331_01_11.png)'
- en: 'Figure 1.11: A collection of MNIST images'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：一组 MNIST 图像
- en: One hot-encoding (OHE)
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: One hot 编码（OHE）
- en: We will use OHE as a simple tool to encode information used inside neural networks.
    In many applications, it is convenient to transform categorical (non-numerical)
    features into numerical variables. For instance, the categorical feature *digit*
    with value *d* in [0–9] can be encoded into a binary vector with 10 positions,
    which has always a 0 value except the *d - th* position where a 1 is present.
    For example, the digit 3 can be encoded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 OHE 作为一个简单的工具来编码神经网络中使用的信息。在许多应用中，将类别（非数值型）特征转换为数值变量是很方便的。例如，类别特征*digit*的值为
    *d*，其中 *d* 属于[0–9]，可以被编码成一个具有 10 个位置的二进制向量，除了 *d-th* 位置外，其他位置的值始终为 0，而 *d-th*
    位置的值为 1。例如，数字 3 可以编码为 [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]。
- en: This type of representation is called **One-Hot-Encoding** (**OHE**) or sometimes
    simply one-hot, and is very common in data mining when the learning algorithm
    is specialized in dealing with numerical functions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方法被称为 **One-Hot-Encoding**（**OHE**），有时也简称为 one-hot，在数据挖掘中非常常见，当学习算法专门用于处理数值函数时，通常会使用这种表示方式。
- en: Defining a simple neural net in TensorFlow
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义一个简单的神经网络
- en: In this section we use TensorFlow to define a network that recognizes MNIST
    handwritten digits. We start with a very simple neural network and then progressively
    improve it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 TensorFlow 定义一个识别 MNIST 手写数字的网络。我们从一个非常简单的神经网络开始，然后逐步改进它。
- en: Following Keras’ style, TensorFlow provides suitable libraries ([https://www.tensorflow.org/api_docs/python/tf/keras/datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets))
    for loading the dataset and splits it into training sets, `X_train`*,* used for
    fine-tuning our net, and test sets, `X_test`*,* used for assessing the performance.
    Later in the chapter, we are going to formally define what a training set, a validation
    set, and a test set are. For now, we just need to know that a training set is
    the dataset used to let our neural network learn from data examples. Data is converted
    into `float32` to use 32-bit precision when training a neural network and normalized
    to the range [0,1]. In addition, we load the true labels into `Y_train` and `Y_test`
    respectively, and perform one-hot encoding on them. Let’s see the code.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 Keras 的风格，TensorFlow 提供了合适的库（[https://www.tensorflow.org/api_docs/python/tf/keras/datasets](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)）用于加载数据集，并将其划分为训练集
    `X_train`*，* 用于微调我们的网络，以及测试集 `X_test`*，* 用于评估性能。在本章后续内容中，我们将正式定义什么是训练集、验证集和测试集。现在，我们只需要知道训练集是用来让神经网络从数据示例中学习的。数据会被转换为
    `float32`，以便在训练神经网络时使用 32 位精度，并被归一化到 [0,1] 范围内。此外，我们还将真实标签分别加载到 `Y_train` 和 `Y_test`
    中，并对其进行一-hot 编码。让我们看看代码。
- en: 'For now, do not focus too much on understanding why certain parameters have
    specific assigned values, as these choices will be discussed throughout the rest
    of the book. Intuitively, an epoch defines how long the training should last,
    `BATCH_SIZE` is the number of samples you feed in your network at a time, and
    the validation sample is the amount of data reserved for checking or proving the
    validity of the training process. The reason why we picked `EPOCHS = 200`, `BATCH_SIZE
    = 128`, `VALIDATION_SPLIT=0.2`, and `N_HIDDEN = 128` will be clearer later in
    this chapter when we will explore different values and discuss hyperparameters
    optimization. Let’s see our first code fragment of a neural network in TensorFlow.
    Reading is intuitive but you will find a detailed explanation in the upcoming
    pages:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，暂时不需要过于关注为什么某些参数有特定的赋值，这些选择将在本书的其余部分中讨论。直观地说，一个epoch定义了训练的持续时间，`BATCH_SIZE`是你每次输入网络的样本数量，验证样本是用于检查或验证训练过程有效性的数据量。我们选择`EPOCHS
    = 200`，`BATCH_SIZE = 128`，`VALIDATION_SPLIT=0.2`，以及`N_HIDDEN = 128`的原因将在本章后续内容中更加清楚，当时我们会探索不同的值并讨论超参数优化。让我们来看看我们在TensorFlow中的第一个神经网络代码片段。阅读上去直观，但你会在接下来的页面中找到详细的解释：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see from the above code that the input layer has a neuron associated
    to each pixel in the image for a total of 28 x 28=784 neurons, one for each pixel
    in the MNIST images.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码可以看出，输入层与图像中的每个像素相关联，共有28 x 28 = 784个神经元，每个神经元对应MNIST图像中的一个像素。
- en: Typically, the values associated with each pixel are normalized in the range
    [0,1] (which means that the intensity of each pixel is divided by 255, the maximum
    intensity value). The output can be one of ten classes, with one class for each
    digit.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个像素的值会被归一化到[0,1]范围内（这意味着每个像素的强度值会被255（最大强度值）除）。输出可以是十个类之一，每个类对应一个数字。
- en: 'The final layer is a single neuron with the activation function `''``softmax''`,
    which is a generalization of the sigmoid function. As discussed earlier, a sigmoid
    function output is in the range (0, 1) when the input varies in the range ![](img/B18331_01_005.png).
    Similarly, a softmax “squashes” a K-dimensional vector of arbitrary real values
    into a K-dimensional vector of real values in the range (0, 1), so that they all
    add up to 1\. In our case, it aggregates ten answers provided by the previous
    layer with ten neurons. What we have just described is implemented with the following
    code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是一个单神经元，激活函数为`'softmax'`，它是sigmoid函数的一种推广。如前所述，当输入在范围![](img/B18331_01_005.png)内变化时，sigmoid函数的输出在(0,
    1)范围内。同样，softmax将一个K维的任意实值向量“压缩”成一个K维的实值向量，范围在(0, 1)之间，并且它们的总和为1。在我们的例子中，它将前一层的十个神经元提供的十个答案汇总。我们刚刚描述的内容可以通过以下代码实现：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we define the model, we have to compile it so that it can be executed by
    TensorFlow. There are a few choices to be made during compilation. Firstly, we
    need to select an *optimizer*, which is the specific algorithm used to update
    weights while we train our model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模型，就必须对其进行编译，以便它可以被TensorFlow执行。在编译过程中有一些选择。首先，我们需要选择一个*优化器*，它是用于在训练模型时更新权重的特定算法。
- en: A complete list of optimizers is at [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).
    Second, we need to select an *objective function*,which is used by the optimizer
    to navigate the space of weights (frequently objective functions are called either
    *loss functions* or *cost functions* and the process of optimization is defined
    as a process of loss *minimization*). Third, we need to evaluate the trained model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的优化器列表可以在[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)找到。其次，我们需要选择一个*目标函数*，它被优化器用来在权重空间中导航（通常目标函数被称为*损失函数*或*代价函数*，而优化过程被定义为损失*最小化*的过程）。第三，我们需要评估训练好的模型。
- en: 'Some common choices for objective functions (a complete list of loss functions
    is at [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses))
    are:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的目标函数选择（完整的损失函数列表可以在[https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)找到）有：
- en: '`mse`, which defines the mean squared error between the predictions and the
    true values. Mathematically if *d* is a vector of predictions and *y* is the vector
    of *n* observed values, then ![](img/B18331_01_019.png). Note that this objective
    function is the average of all the mistakes made in each prediction. If a prediction
    is far off from the true value, then this distance is made more evident by the
    squaring operation. In addition, the square can add up the error regardless of
    whether a given value is positive or negative.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mse`，定义为预测值与真实值之间的均方误差。从数学上讲，如果*d*是预测值向量，*y*是包含*n*个观测值的向量，那么 ![](img/B18331_01_019.png)。请注意，这个目标函数是每个预测中所有错误的平均值。如果预测值与真实值相差较大，那么这个差距会通过平方操作变得更加明显。此外，平方操作能够加总误差，无论给定值是正数还是负数。'
- en: '`binary_crossentropy`, which defines the binary logarithmic loss. Suppose that
    our model predicts *p* while the target is *c*, then the binary cross-entropy
    is defined as ![](img/B18331_01_020.png). Note that this objective function is
    suitable for binary labels prediction.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`binary_crossentropy`，定义为二分类对数损失。假设我们的模型预测值为*p*，而目标值为*c*，则二分类交叉熵定义为 ![](img/B18331_01_020.png)。请注意，这个目标函数适用于二分类标签预测。'
- en: '`categorical_crossentropy`, which defines the multiclass logarithmic loss.
    Categorical cross-entropy compares the distribution of the predictions with the
    true distribution, with the probability of the true class set to 1 and 0 for the
    other classes. If the true class is *c* and the prediction is *y*, then the categorical
    cross-entropy is defined as:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categorical_crossentropy`，定义为多类对数损失。类别交叉熵比较预测分布与真实分布，其中真实类别的概率设置为1，其他类别的概率设置为0。如果真实类别是*c*，而预测为*y*，那么类别交叉熵定义为：'
- en: '![](img/B18331_01_021.png)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B18331_01_021.png)'
- en: One way to think about multi-class logarithm loss is to consider the true class
    represented as a one-hot encoded vector, and the closer the model’s outputs are
    to that vector, the lower the loss. Note that this objective function is suitable
    for multi-class label predictions. It is also the default choice with softmax
    activation. A complete list of loss functions is at [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses).
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑多类对数损失的一种方式是将真实类别表示为独热编码向量，并且模型输出越接近该向量，损失越低。请注意，这个目标函数适用于多分类标签预测，且在使用softmax激活函数时为默认选择。完整的损失函数列表请参见
    [https://www.tensorflow.org/api_docs/python/tf/keras/losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)。
- en: 'Some common choices for metrics (a complete list of metrics is at [https://www.tensorflow.org/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics))
    are:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的度量标准（完整的度量标准列表请参见 [https://www.tensorflow.org/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)）包括：
- en: Accuracy, defined as the proportion of correct predictions with respect to the
    total number of predictions.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率，定义为正确预测相对于总预测数量的比例。
- en: Precision, defined as the proportion of correct positive predictions with respect
    to the number of correct and incorrect positive predictions.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度，定义为正确的正向预测相对于正确和错误的正向预测数量的比例。
- en: Recall, defined as the proportion of correct positive predictions with respect
    to the actual number of positive predictions.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率，定义为正确的正向预测相对于实际正向预测数量的比例。
- en: A complete list of metrics is at [https://www.tensorflow.org/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics).
    Metrics are similar to objective functions, with the only difference being that
    they are not used for training a model, only for evaluating the model. However,
    it is important to understand the difference between metrics and objective functions.
    As discussed, the loss function is used to optimize your network. This is the
    function minimized by the selected optimizer. Instead, a metric is used to judge
    the performance of your network. This is only for you to run an evaluation, and
    it should be separated from the optimization process. On some occasions, it would
    be ideal to directly optimize for a specific metric. However, some metrics are
    not differentiable with respect to their inputs, which precludes them from being
    used directly.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的度量标准列表可以在[https://www.tensorflow.org/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)查看。度量标准与目标函数类似，不同之处在于它们不用于训练模型，只用于评估模型。然而，理解度量标准与目标函数的区别非常重要。如前所述，损失函数用于优化你的网络，这是被选定优化器最小化的函数。而度量标准则用于判断你的网络性能。这仅供你进行评估，应该与优化过程分开。在某些情况下，理想的做法是直接优化特定的度量标准。然而，一些度量标准对于其输入是不可导的，因此不能直接用于优化。
- en: 'When compiling a model in TensorFlow, it is possible to select the optimizer,
    the loss function, and the metric used together with a given model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中编译模型时，可以选择优化器、损失函数和度量标准，它们将与给定的模型一起使用：
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Stochastic Gradient Descent** (**SGD**) is a particular kind of optimization
    algorithm used to reduce the mistakes made by neural networks after each training
    epoch. We will review SGD and other optimization algorithms in the next chapters.
    Once the model is compiled, it can then be trained with the `fit()` method, which
    specifies a few parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机梯度下降法**（**SGD**）是一种特殊的优化算法，用于减少神经网络在每次训练周期后的错误。我们将在接下来的章节中回顾SGD和其他优化算法。模型编译完成后，可以使用`fit()`方法进行训练，该方法指定了一些参数：'
- en: '`epochs` is the number of times the model is exposed to the training set. At
    each iteration the optimizer tries to adjust the weights so that the objective
    function is minimized.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`是指模型暴露于训练集的次数。在每次迭代中，优化器会尝试调整权重，以最小化目标函数。'
- en: '`batch_size` is the number of training instances observed before the optimizer
    performs a weight update; there are usually many batches per epoch.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`是指在优化器进行权重更新之前观察到的训练实例的数量；每个周期通常会有多个批次。'
- en: 'Training a model in TensorFlow is very simple:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中训练模型非常简单：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we’ve reserved part of the training set for validation. The key idea
    is that we reserve a part of the training data for measuring the performance on
    the validation while training. This is a good practice to follow for any machine
    learning task, and one that we will adopt in all of our examples. Please note
    that we will return to validation later in this chapter when we will talk about
    overfitting.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经为验证保留了部分训练集。关键思想是，在训练过程中，我们保留部分训练数据用于验证集的性能评估。这是一种良好的实践，适用于任何机器学习任务，并且我们将在所有示例中采用这种方法。请注意，当我们在本章后面讨论过拟合时，会再次提到验证。
- en: Once the model is trained, we can evaluate it on the test set that contains
    new examples never seen by the model during the training phase.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以在测试集上进行评估，该测试集包含在训练阶段从未见过的新示例。
- en: 'Note that, of course, the training set and the test set are rigorously separated.
    There is no point in evaluating a model on an example that was already used for
    training. In TF we can use the method `evaluate(X_test, Y_test)` to compute the
    `test_loss` and the `test_acc`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，训练集和测试集是严格分开的。没有必要在已经用于训练的示例上评估模型。在TensorFlow中，我们可以使用`evaluate(X_test, Y_test)`方法来计算`test_loss`和`test_acc`：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Congratulations! You have just defined your first neural network in TensorFlow.
    A few lines of code and your computer should be able to recognize handwritten
    numbers. Let’s run the code and see what the performance is.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚定义了你的第一个神经网络。只需几行代码，你的计算机就能够识别手写数字。让我们运行代码并看看性能如何。
- en: Running a simple TensorFlow net and establishing a baseline
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行一个简单的TensorFlow网络并建立基准
- en: 'So, let’s see what happens when we run the code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们看看当我们运行代码时会发生什么：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First the net architecture is dumped and we can see the different types of
    layers used, their output shape, how many parameters (i.e., how many weights)
    they need to optimize, and how they are connected. Then, the network is trained
    on 48K samples, and 12K are reserved for validation. Once the neural model is
    built, it is then tested on 10K samples. For now we won’t go into the internals
    of how the training happens, but we can see that the program runs for 200 iterations
    and each time accuracy improves. When the training ends, we test our model on
    the test set and we achieve about 89.96% accuracy on the training dataset, 90.70%
    on validation, and 90.71% on test:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，网络架构被输出，我们可以看到使用的不同类型的层、它们的输出形状、需要优化的参数数量（即需要优化的权重数量）以及它们是如何连接的。然后，网络在48K样本上进行训练，12K样本用于验证。一旦神经模型构建完成，它会在10K样本上进行测试。现在我们不深入探讨训练如何进行，但可以看到程序运行了200次，每次准确率都有所提高。训练结束后，我们在测试集上测试我们的模型，最终得到了在训练数据集上的约89.96%的准确率，在验证集上的90.70%，以及在测试集上的90.71%：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This means that nearly 1 in 10 images are incorrectly classified. We can certainly
    do better than that.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着大约每10张图片中就有1张被错误分类。我们肯定能做得比这更好。
- en: Improving the simple net in TensorFlow with hidden layers
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow通过隐藏层改进简单网络
- en: Okay, we have a baseline of accuracy of 89.96% on the training dataset, 90.70%
    on validation, and 90.71% on test. It is a good starting point, but we can improve
    it. Let’s see how.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们在训练数据集上的准确率基准为89.96%，在验证集上为90.70%，在测试集上为90.71%。这是一个不错的起点，但我们可以改进它。让我们看看怎么做。
- en: 'An initial improvement is to add additional layers to our network because these
    additional neurons might intuitively help to learn more complex patterns in the
    training data. In other words, additional layers add more parameters, potentially
    allowing a model to memorize more complex patterns. So, after the input layer,
    we have a first dense layer with `N_HIDDEN` neurons and an activation function
    `''relu''`. This additional layer is considered *hidden* because it is not directly
    connected either with the input or with the output. After the first hidden layer
    we have a second hidden layer, again with `N_HIDDEN` neurons, followed by an output
    layer with ten neurons, each one of which will fire when the relative digit is
    recognized. The following code defines this new network:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一项初步的改进是向我们的网络中添加更多层，因为这些额外的神经元可能直观地有助于学习训练数据中的更复杂模式。换句话说，额外的层增加了更多参数，可能让模型记住更复杂的模式。所以，在输入层之后，我们添加了第一层密集层，其中有`N_HIDDEN`个神经元，并使用激活函数`'relu'`。这个额外的层被认为是*隐藏层*，因为它既不与输入层直接连接，也不与输出层直接连接。在第一个隐藏层之后，我们添加了第二个隐藏层，同样有`N_HIDDEN`个神经元，然后是一个输出层，包含10个神经元，每个神经元会在识别到相应的数字时被激活。以下代码定义了这个新的网络：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that `to_categorical(Y_train, NB_CLASSES)` converts the array `Y_train`
    into a matrix with as many columns as there are classes. The number of rows stays
    the same. So, for instance, if we have:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`to_categorical(Y_train, NB_CLASSES)`将数组`Y_train`转换为一个矩阵，列数等于类别数，而行数保持不变。比如，假设我们有：
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'then:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 那么：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s run the code and see what results we get with this multi-layer network:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行代码，看看这个多层网络得到什么结果：
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The previous output shows the initial steps of the run while the following output
    shows the conclusion. Not bad. As seen in the following output, by adding two
    hidden layers we reached 90.81% on the training dataset, 91.40% on validation,
    and 91.18% on test. This means that we have increased accuracy on the test dataset
    with respect to the previous network, and we have reduced the number of iterations
    from 200 to 50\. That’s good, but we want more.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个输出展示了运行的初始步骤，而以下输出展示了结论。不坏。从以下输出可以看到，通过添加两层隐藏层，我们在训练数据集上的准确率达到了90.81%，在验证集上为91.40%，在测试集上为91.18%。这意味着，相比之前的网络，我们提高了测试数据集上的准确率，并且将迭代次数从200次减少到50次。这很好，但我们还想要更多。
- en: 'If you want, you can play by yourself and see what happens if you add only
    one hidden layer instead of two or if you add more than two layers. I leave this
    experiment as an exercise:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以自己尝试，看看如果你只添加一个隐藏层而不是两个，或者如果你添加超过两个层，会发生什么。我把这个实验留给你作为练习：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that improvement stops (or it become almost imperceptible) after a certain
    number of epochs. In machine learning this is a phenomenon called *convergence*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，改进会在一定数量的迭代后停止（或者变得几乎不可察觉）。在机器学习中，这种现象被称为*收敛*。
- en: Further improving the simple net in TensorFlow with dropout
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow通过Dropout进一步改进简单网络
- en: 'Now our baseline is 90.81% on training set, 91.40% on validation, and 91.18%
    on test. A second improvement is very simple. We decide to randomly drop – with
    the `DROPOUT` probability – some of the values propagated inside our internal
    dense network of hidden layers during training. In machine learning this is a
    well-known form of regularization. Surprisingly enough, this idea of randomly
    dropping a few values can improve our performance. The idea behind this improvement
    is that random dropouts *force* the network to learn redundant patterns that are
    useful for better generalization:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的基准是训练集准确率90.81%，验证集准确率91.40%，测试集准确率91.18%。第二个改进非常简单。我们决定在训练过程中，以 `DROPOUT`
    概率随机丢弃一些值，这些值在我们的内部密集网络的隐藏层中传播。在机器学习中，这是一种广为人知的正则化方法。令人惊讶的是，随机丢弃一些值的这一想法竟然能改善我们的性能。其背后的思想是，随机丢弃
    *迫使* 网络学习到冗余的模式，这些模式对更好的泛化有帮助：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s run the code for 200 iterations as before and we see that this net achieves
    an accuracy of 91.70% on training, 94.42% on validation, and 94.15% on testing:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像之前一样运行200次迭代，可以看到该网络在训练集上的准确率为91.70%，在验证集上的准确率为94.42%，在测试集上的准确率为94.15%：
- en: '[PRE14]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that it has been frequently observed that networks with random dropouts
    in internal hidden layers can “generalize” better on unseen examples contained
    in test sets. Intuitively we can consider this phenomenon as each neuron becoming
    more capable because it knows it cannot depend on its neighbors. Also, it forces
    information to be stored in a redundant way. During testing there is no dropout,
    so we are now using all our highly tuned neurons. In short, it is generally a
    good approach to test how a net performs when some dropout function is adopted.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，已经多次观察到，在内部隐藏层中具有随机丢弃（dropout）功能的网络可以在未见过的测试集样本上“泛化”得更好。直观地说，我们可以将这一现象理解为每个神经元变得更有能力，因为它知道自己不能依赖于邻近的神经元。此外，这还迫使信息以冗余的方式进行存储。在测试时没有丢弃，因此我们现在使用的是所有经过高度调优的神经元。简而言之，当采用某种丢弃功能时，通常可以通过测试网络的表现来验证其有效性。
- en: Besides that, note that training accuracy should still be above test accuracy;
    otherwise, we might be not training for long enough. This is the case in our example
    and therefore we should increase the number of epochs. However, before performing
    this attempt we need to introduce a few other concepts that allow the training
    to converge faster. Let’s talk about optimizers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，训练的准确率应该仍然高于测试的准确率；否则，我们可能训练的时间还不够长。这在我们的例子中确实是这样，因此我们应该增加训练的轮次。然而，在尝试这样做之前，我们需要引入一些其他概念，这些概念可以加速训练的收敛过程。接下来我们讨论优化器。
- en: Testing different optimizers in TensorFlow
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中测试不同的优化器
- en: 'Now that we have defined and used a network, it is useful to start developing
    some intuition about how networks are trained, using an analogy. Let us focus
    on one popular training technique known as **gradient descent** (**GD**). Imagine
    a generic cost function *C*(*w*) in one single variable *w* like in *Figure 1.12*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义并使用了一个网络，接下来可以通过一个类比来帮助我们更好地理解网络是如何训练的。让我们关注一种流行的训练技术——**梯度下降法**（**GD**）。假设有一个通用的代价函数
    *C*(*w*)，它是一个关于单一变量 *w* 的函数，如*图1.12*所示：
- en: '![](img/B18331_01_12.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_12.png)'
- en: 'Figure 1.12: An example of GD optimization'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：梯度下降优化的示例
- en: GD can be seen as a hiker who needs to navigate down a steep slope and aims
    to enter a ditch. The slope represents the function *C* while the ditch represents
    the minimum *C*[min]. The hiker has a starting point *w*[0]. The hiker moves little
    by little; imagine that there is almost zero visibility, so the hiker cannot see
    where to go automatically, and they proceed in a zigzag. At each step *r*, the
    gradient is the direction of maximum increase.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法可以看作是一个需要沿着陡峭的坡道向下行进的登山者，目标是进入一个沟壑。坡道代表着代价函数 *C*，而沟壑代表着最小值 *C*[min]。登山者有一个起始点
    *w*[0]，并逐步前进；可以想象，这里几乎没有可见性，所以登山者无法自动看到前进的方向，而是采取锯齿形的路径。在每一步 *r*，梯度是最大增量的方向。
- en: Mathematically this direction is the value of the partial derivative ![](img/B18331_01_022.png)
    evaluated at point *w*[r], reached at step *r*. Therefore, by taking the opposite
    direction ![](img/B18331_01_023.png) the hiker can move toward the ditch.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，这个方向是偏导数的值 ![](img/B18331_01_022.png)，它在步长 *r* 时的 *w*[r] 点被评估出来。因此，通过采取相反的方向
    ![](img/B18331_01_023.png)，登山者可以朝向沟壑移动。
- en: At each step the hiker can decide how big a stride to take before the next stop.
    This is the so-called “learning rate” ![](img/B18331_01_024.png) in GD jargon.
    Note that if ![](img/B18331_01_025.png) is too small, then the hiker will move
    slowly. However, if ![](img/B18331_01_025.png) is too high, then the hiker will
    possibly miss the ditch by stepping over it.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，徒步旅行者可以决定在下次停下之前走多大一步。这就是 GD 术语中的所谓“学习率” ![](img/B18331_01_024.png)。请注意，如果
    ![](img/B18331_01_025.png) 太小，那么徒步旅行者的步伐会非常缓慢。然而，如果 ![](img/B18331_01_025.png)
    太大，那么徒步旅行者可能会跳过沟渠。
- en: Now you should remember that a sigmoid is a continuous function and it is possible
    to compute the derivative. It can be proven that the sigmoid ![](img/B18331_01_027.png)
    has the derivative ![](img/B18331_01_028.png).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该记得，sigmoid 是一个连续函数，并且可以计算它的导数。可以证明，sigmoid ![](img/B18331_01_027.png) 的导数是
    ![](img/B18331_01_028.png)。
- en: ReLU is not differentiable at 0\. We can however extend the first derivative
    at 0 to a function over the whole domain by defining it to be either a 0 or 1\.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 在 0 处不可导。然而，我们可以通过将 0 处的导数扩展到整个领域来定义它为 0 或 1，从而使其成为一个函数。
- en: The piecewise derivative of ReLU ![](img/B18331_01_029.png) is ![](img/B18331_01_030.png).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 的分段导数 ![](img/B18331_01_029.png) 是 ![](img/B18331_01_030.png)。
- en: Once we have the derivative, it is possible to optimize the nets with a GD technique.
    TensorFlow computes the derivative on our behalf so we don’t need to worry about
    implementing or computing it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到导数，就可以使用 GD 技术优化网络。TensorFlow 为我们计算导数，所以我们不需要担心实现或计算它。
- en: A neural network is essentially a composition of multiple derivable functions
    with thousands and sometimes millions of parameters. Each network layer computes
    a function, the error of which should be minimized in order to improve the accuracy
    observed during the learning phase. When we discuss backpropagation, we will discover
    that the minimization game is a bit more complex than our toy example. However,
    it is still based on the same intuition of descending a slope to reach a ditch.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络本质上是由多个可导函数组成，具有成千上万，甚至数百万个参数。每一层网络计算一个函数，其误差应该最小化，以提高学习阶段观察到的准确性。当我们讨论反向传播时，我们将发现最小化问题比我们的简化例子要复杂一些。然而，它仍然基于相同的直觉，即沿着坡度下降，直到到达沟渠。
- en: TensorFlow implements a fast variant of GD known as **Stochastic Gradient Descent**
    (**SGD**) and many more advanced optimization techniques such as RMSProp and Adam.
    RMSProp and Adam include the concept of momentum (a velocity component) in addition
    to the acceleration component that SGD has. This allows faster convergence at
    the cost of more computation. Think about a hiker who starts to move in one direction
    and then decides to change direction but remembers previous choices. It can be
    proven that momentum helps accelerate SGD in the relevant direction and dampens
    oscillations [10].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 实现了一种快速的 GD 变体，称为 **随机梯度下降** (**SGD**)，以及许多更先进的优化技术，如 RMSProp 和 Adam。RMSProp
    和 Adam 除了拥有 SGD 的加速组件外，还引入了动量（速度分量）概念。这使得收敛速度更快，但也需要更多的计算。可以想象，一名徒步旅行者开始朝一个方向行进，然后决定改变方向，但会记住之前的选择。可以证明，动量有助于在相关方向上加速
    SGD，同时减缓振荡[10]。
- en: 'SGD was our default choice so far. So now let’s try the other two. It is very
    simple; we just need to change a few lines:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，SGD 是我们默认的选择。那么现在让我们尝试另外两种方法。非常简单；我们只需要修改几行代码：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s it. Let’s test it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些，来测试一下吧。
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As you can see, RMSProp is faster than SDG since we are able to achieve in
    only 10 epochs an accuracy of 97.43% on the training dataset, 97.62% on validation,
    and 97.64% on test. That’s a significant improvement on SDG. Now that we have
    a very fast optimizer, let us try to increase significantly the number of epochs
    up to 250, and we get 98.99% accuracy on the training dataset, 97.66% on validation,
    and 97.77% on test:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，RMSProp 比 SDG 更快，因为我们仅在 10 次训练中就能在训练数据集上达到 97.43% 的准确率，在验证集上为 97.62%，在测试集上为
    97.64%。这是 SDG 的显著改进。现在我们有了一个非常快速的优化器，接下来我们试着将训练轮数显著增加到 250 次，结果在训练数据集上达到了 98.99%
    的准确率，在验证集上为 97.66%，在测试集上为 97.77%：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It is useful to observe how accuracy increases on training and test sets when
    the number of epochs increases (see *Figure 1.13*). As you can see, these two
    curves touch at about 15 epochs and therefore there is no need to train further
    after that point:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 观察随着训练轮数增加，训练集和测试集上的准确率变化非常有用（见 *图 1.13*）。正如你所看到的，这两条曲线在大约 15 次训练后相交，因此在此之后就不再需要继续训练：
- en: '![](img/B18331_01_13.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_13.png)'
- en: 'Figure 1.13: An example of accuracy and loss with RMSProp'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13：使用 RMSProp 的准确率和损失示例
- en: 'Okay, let’s try the other optimizer, `Adam()`. It’s pretty simple to implement:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们尝试另一个优化器，`Adam()`。实现起来相当简单：
- en: '[PRE18]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we see, `Adam()` is slightly better. With Adam we achieve 98.94% accuracy
    on the training dataset, 97.89% on validation, and 97.82% on test with 50 iterations:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`Adam()`略好一些。使用 Adam，我们在训练数据集上的准确率为 98.94%，在验证集上的准确率为 97.89%，在测试集上的准确率为
    97.82%，使用了 50 次迭代：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'One more time, let’s plot how accuracy increases on training and test sets
    when the number of epochs increases (see *Figure 1.14*). You’ll notice that by
    choosing Adam as an optimizer we are able to stop after just about 12 epochs or
    steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们绘制当迭代次数增加时，训练集和测试集上的准确率是如何变化的（见*图 1.14*）。你会注意到，通过选择 Adam 作为优化器，我们能够在大约
    12 次迭代或步骤后就停止：
- en: '![](img/B18331_01_14.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_14.png)'
- en: 'Figure 1.14: An example of accuracy and loss with Adam'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14：使用 Adam 的准确率和损失示例
- en: 'Note that this is our fifth variant and remember that our initial baseline
    was at 90.71% on the test dataset. So far, we’ve made progressive improvements.
    However, gains are now more and more difficult to obtain. Note that we are optimizing
    with a dropout of 30%. For the sake of completeness, it could be useful to report
    the accuracy of the test dataset for different dropout values (see *Figure 1.15*).
    In this example, we selected Adam as the optimizer. Note that the choice of optimizer
    isn’t a rule of thumb and we can get different performance depending on the problem-optimizer
    combination:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是我们的第五个变体，并记住我们最初的基线在测试数据集上的准确率为 90.71%。到目前为止，我们已逐步改进。然而，收益现在变得越来越难以获得。请注意，我们正在使用
    30% 的丢弃率进行优化。为了完整性，报告不同丢弃率下测试数据集的准确率可能会很有用（见*图 1.15*）。在这个例子中，我们选择了 Adam 作为优化器。请注意，优化器的选择不是一成不变的，我们可以根据问题和优化器的组合获得不同的性能：
- en: '![Chart](img/B18331_01_15.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_15.png)'
- en: 'Figure 1.15: An example of changes in accuracy for different dropout values'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15：不同丢弃率下的准确率变化示例
- en: Increasing the number of epochs
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加迭代次数
- en: 'Let’s make another attempt and increase the number of epochs used for training
    from 20 to 200\. Unfortunately, this choice increases our computation time tenfold,
    yet gives us no gain. The experiment is unsuccessful, but we have learned that
    if we spend more time learning, we will not necessarily improve the result. Learning
    is more about adopting smart techniques and not necessarily about the time spent
    in computations. Let’s keep track of our five variants in the following graph:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再试一次，将用于训练的迭代次数从 20 增加到 200。遗憾的是，这个选择将我们的计算时间增加了十倍，但并没有带来任何好处。实验失败了，但我们已经学到，如果我们花更多时间学习，结果不一定会改善。学习更多的是采用智能的技巧，而不一定是花费在计算上的时间。让我们在接下来的图表中跟踪我们的五种变体：
- en: '![Chart](img/B18331_01_16.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_16.png)'
- en: 'Figure 1.16: Accuracy for different models and optimizers'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16：不同模型和优化器的准确率
- en: Controlling the optimizer learning rate
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制优化器的学习率
- en: 'There is another approach we can take that involves changing the learning parameter
    for our optimizer. As you can see in *Figure 1.17*, the best value reached by
    our three experiments [lr=0.1, lr=0.01, and lr=0.001] is 0.1, which is the default
    learning rate for the optimizer. Good! Adam works well out of the box:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种方法是改变优化器的学习参数。正如你在*图 1.17*中看到的，我们的三个实验 [lr=0.1, lr=0.01, 和 lr=0.001] 达到的最佳值是
    0.1，这是优化器的默认学习率。很好！Adam 开箱即用：
- en: '![Chart](img/B18331_01_17.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_17.png)'
- en: 'Figure 1.17: Accuracy for different learning rates'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17：不同学习率下的准确率
- en: Increasing the number of internal hidden neurons
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加内部隐藏神经元的数量
- en: 'Yet another approach involves changing the number of internal hidden neurons.
    We report the results of the experiments with an increasing number of hidden neurons.
    We see that by increasing the complexity of the model, the runtime increases significantly
    because there are more and more parameters to optimize. However, the gains that
    we are getting by increasing the size of the network decrease more and more as
    the network grows (see *Figure 1.18*, *Figure 1.19*, and *Figure 1.20*):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是改变内部隐藏神经元的数量。我们报告了在增加隐藏神经元数量时的实验结果。我们发现，通过增加模型的复杂性，运行时间显著增加，因为需要优化的参数越来越多。然而，随着网络规模的增大，我们通过增加网络大小获得的收益越来越少（见*图
    1.18*、*图 1.19* 和 *图 1.20*）：
- en: '![Chart](img/B18331_01_18.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_18.png)'
- en: 'Figure 1.18: Number of parameters for the increasing values of internal hidden
    neurons'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.18：内部隐藏神经元数量增加时的参数数量
- en: 'On the other hand, the time needed increases as the size of the internal network
    increases (see *Figure 1.19*):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，随着内部网络大小的增加，所需的时间也会增加（见*图 1.19*）：
- en: '![Chart](img/B18331_01_19.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_19.png)'
- en: 'Figure 1.19: Seconds of computation time for the increasing values of internal
    hidden neurons'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.19：内部隐藏神经元数量增加时的计算时间（秒）
- en: 'Note that increasing the number of hidden neurons after a certain value can
    reduce the accuracy because the network might not be able to generalize well (as
    shown in *Figure 1.20*):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在某个值之后，增加隐藏神经元的数量可能会降低准确率，因为网络可能无法很好地泛化（如*图 1.20*所示）：
- en: '![Chart](img/B18331_01_20.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_20.png)'
- en: 'Figure 1.20: Test accuracy for the increasing values of internal hidden neurons'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20：内部隐藏神经元数量增加时的测试准确率
- en: Increasing the size of batch computation
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加批处理计算的大小
- en: 'GD tries to minimize the cost function on all the examples provided in the
    training sets and, at the same time, for all the features provided as input. SGD
    is a much less expensive variant that considers only `BATCH_SIZE` examples. So,
    let us see how it behaves when we change this parameter. As you can see, the best
    accuracy value is reached for a `BATCH_SIZE=64` in our four experiments (see *Figure
    1.21*):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GD 尝试最小化在训练集中提供的所有示例上的成本函数，同时考虑所有作为输入提供的特征。SGD 是一个更便宜的变体，只考虑 `BATCH_SIZE` 个示例。那么，让我们看看当我们更改此参数时它的表现。正如你所看到的，在我们的四个实验中，最佳准确率值出现在
    `BATCH_SIZE=64` 时（见*图 1.21*）：
- en: '![Chart](img/B18331_01_21.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图表](img/B18331_01_21.png)'
- en: 'Figure 1.21: Test accuracy for different batch values'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.21：不同批处理值下的测试准确率
- en: Summarizing experiments run to recognizing handwritten digits
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结识别手写数字的实验
- en: 'So, let’s summarize: with five different variants, we were able to improve
    our performance from 90.71% to 97.82%. First, we defined a simple layer network
    in TensorFlow. Then, we improved the performance by adding some hidden layers.
    After that, we improved the performance on the test set by adding a few random
    dropouts in our network, and then by experimenting with different types of optimizers:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们总结一下：通过五种不同的变体，我们能够将性能从 90.71% 提升到 97.82%。首先，我们在 TensorFlow 中定义了一个简单的层网络。然后，我们通过添加一些隐藏层来提高性能。之后，我们通过在网络中添加一些随机丢弃来改善测试集上的表现，接着通过实验不同类型的优化器来进一步提升性能：
- en: '|  | **Accuracy** |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | **准确率** |'
- en: '| **Model** | **Training** | **Validation** | **Test** |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **训练** | **验证** | **测试** |'
- en: '| **Simple** | 89.96% | 90.70% | 90.71% |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **简单** | 89.96% | 90.70% | 90.71% |'
- en: '| **Two hidden layers (128)** | 90.81% | 91.40% | 91.18% |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **两层隐藏层（128）** | 90.81% | 91.40% | 91.18% |'
- en: '| **Dropout (30%)** | 91.70% | 94.42% | 94.15% (200 epochs) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **Dropout（30%）** | 91.70% | 94.42% | 94.15%（200 轮） |'
- en: '| **RMSProp** | 97.43% | 97.62% | 97.64% (10 epochs) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **RMSProp** | 97.43% | 97.62% | 97.64%（10 轮） |'
- en: '| **Adam** | 98.94% | 97.89% | 97.82% (10 epochs) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **Adam** | 98.94% | 97.89% | 97.82%（10 轮） |'
- en: 'Table 1.1: Summary of experiments with various levels of accuracy'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1：不同准确率水平的实验总结
- en: However, the next two experiments (not shown in *Table 1.1*) were not providing
    significant improvements. Increasing the number of internal neurons creates more
    complex models and requires more expensive computations, but it provides only
    marginal gains. We have the same experience if we increase the number of training
    epochs. A final experiment consisted of changing the `BATCH_SIZE` for our optimizer.
    This also provided marginal results.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，接下来的两个实验（未显示在*表 1.1*中）并未提供显著的改进。增加内部神经元的数量会创建更复杂的模型，并需要更多昂贵的计算，但它只提供了边际的增益。如果我们增加训练轮次，也会有相同的体验。最后一个实验是改变优化器的`BATCH_SIZE`。这也提供了边际的结果。
- en: Regularization
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: In this section we will review a few best practices for improving the training
    phase. In particular, regularization and batch normalization will be discussed.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾一些改进训练阶段的最佳实践。特别是，将讨论正则化和批归一化。
- en: Adopting regularization to avoid overfitting
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用正则化以避免过拟合
- en: 'Intuitively, a good machine learning model should achieve low error on training
    data. Mathematically this is equivalent to minimizing the loss function on the
    training data given the model:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，一个好的机器学习模型应该在训练数据上实现低误差。从数学上讲，这等同于在给定模型的情况下最小化训练数据上的损失函数：
- en: '![](img/B18331_01_031.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_031.png)'
- en: However, this might not be enough. A model can become excessively complex in
    order to capture all the relations inherently expressed by the training data.
    This increase in complexity might have two negative consequences. First, a complex
    model might require a significant amount of time to be executed. Second, a complex
    model might achieve very good performance on training data but perform quite badly
    on validation data. This is because the model is able to contrive relationships
    between many parameters in the specific training context, but these relationships
    in fact do not exist within a more generalized context. Causing a model to lose
    its ability to generalize in this manner is termed “overfitting. “ Again, learning
    is more about generalization than memorization. Another phenomenon to consider
    is “underfitting.”
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能还不够。一个模型可能会变得过于复杂，以便捕捉训练数据中固有表达的所有关系。这种复杂度的增加可能带来两个负面后果。首先，复杂的模型可能需要大量时间来执行。其次，复杂的模型可能在训练数据上表现非常好，但在验证数据上表现很差。这是因为模型能够在特定的训练上下文中制造多个参数之间的关系，但这些关系在更一般的上下文中实际上并不存在。这种导致模型丧失泛化能力的现象被称为“过拟合”。再次强调，学习更重要的是关于泛化，而非记忆。另一个需要考虑的现象是“欠拟合”。
- en: 'This happens when a data model cannot capture the relationship between the
    input and output variables accurately, with a high error rate on both the training
    set and new unseen data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况发生在数据模型无法准确捕捉输入与输出变量之间的关系时，训练集和新未见数据的误差率都很高：
- en: '![](img/B18331_01_22.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_22.png)'
- en: 'Figure 1.22: Loss function and overfitting'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.22：损失函数与过拟合
- en: As a rule of thumb, if during the training we see that the loss increases on
    validation, after an initial decrease, then we have a problem of model complexity
    that overfits the training data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果在训练过程中，我们看到验证集的损失在初步下降后开始增加，那么我们遇到了模型复杂度的问题，导致模型过拟合训练数据。
- en: 'In order to solve the overfitting problem, we need a way to capture the complexity
    of a model, i.e. how complex a model can be. What could the solution be? Well,
    a model is nothing more than a vector of weights. Each weight affects the output,
    except for those which are zero, or very close to it. Therefore, the complexity
    of a model can be conveniently represented as the number of non-zero weights.
    In other words, if we have two models M1 and M2 achieving pretty much the same
    performance in terms of a loss function, then we should choose the simplest model,
    the one which has the minimum number of non-zero weights. We can use a hyperparameter
    ![](img/B18331_01_032.png) for controlling the importance of having a simple model,
    as in this formula:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决过拟合问题，我们需要一种方法来捕捉模型的复杂度，也就是模型可以有多复杂。解决方案是什么呢？实际上，模型不过是一个权重向量。每个权重都会影响输出，除了那些为零或接近零的权重。因此，模型的复杂度可以方便地用非零权重的数量来表示。换句话说，如果我们有两个模型
    M1 和 M2，在损失函数方面的表现几乎相同，那么我们应该选择最简单的模型，即非零权重数量最少的那个模型。我们可以使用一个超参数 ![](img/B18331_01_032.png)
    来控制保持简单模型的重要性，如下公式所示：
- en: '![](img/B18331_01_033.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_033.png)'
- en: 'There are three different types of regularization used in machine learning:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有三种不同类型的正则化方法：
- en: L1 regularization (also known as LASSO). The complexity of the model is expressed
    as the sum of the absolute values of the weights.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化（也称为 LASSO）。模型的复杂度表示为权重绝对值的和。
- en: L2 regularization (also known as Ridge). The complexity of the model is expressed
    as the sum of the squares of the weights.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化（也称为 Ridge）。模型的复杂度表示为权重平方和。
- en: ElasticNet regularization. The complexity of the model is captured by a combination
    of the two techniques above.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ElasticNet 正则化。模型的复杂度通过上述两种技术的结合来表示。
- en: Note that playing with regularization can be a good way to increase the generalization
    performance of a network, particularly when there is an evident situation of overfitting.
    This set of experiments is left as an exercise to the interested reader.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调整正则化可以是一种提升网络泛化性能的好方法，特别是在过拟合明显的情况下。这组实验留给有兴趣的读者自行完成。
- en: 'Also note that TensorFlow supports L1, L2, and ElasticNet regularization. A
    complete list of regularizers is at [https://www.tensorflow.org/api_docs/python/tf/keras/regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers).
    Adding regularization is easy:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，值得注意的是，TensorFlow 支持 L1、L2 和 ElasticNet 正则化。完整的正则化器列表可以在 [https://www.tensorflow.org/api_docs/python/tf/keras/regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)
    中找到。添加正则化非常简单：
- en: '[PRE20]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Understanding batch normalization
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解批量归一化
- en: Batch normalization is another form of regularization and one of the most effective
    improvements proposed during the last few years. Batch normalization enables us
    to accelerate training, in some cases by halving the training epochs, and it offers
    some regularization. During training, the weights in early layers naturally change
    and therefore the inputs of later layers can significantly change. In other words,
    each layer must continuously re-adjust its weights to the different distribution
    for every batch. This may slow down the model’s training greatly. The key idea
    is to make layer inputs more similar in distribution, batch after batch and epoch
    after epoch.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是另一种正则化方法，也是近年来提出的最有效的改进之一。批量归一化可以加速训练，在某些情况下将训练周期缩短一半，并提供一定的正则化效果。在训练过程中，前面层的权重会自然变化，因此后续层的输入可能会发生显著变化。换句话说，每一层必须不断地重新调整其权重，以适应每个批次的不同分布。这可能会大大减慢模型的训练速度。关键思想是使每一层的输入在每个批次和每个周期中具有更相似的分布。
- en: Another issue is that the sigmoid activation function works very well close
    to zero but tends to “get stuck” when values get sufficiently far away from zero.
    If, occasionally, neuron outputs fluctuate far away from the sigmoid zero, then
    said neuron becomes unable to update its own weights.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，sigmoid 激活函数在接近零时效果很好，但当值远离零时，它往往会“卡住”。如果神经元的输出偶尔远离 sigmoid 的零点，则该神经元将无法更新其权重。
- en: The other key idea is therefore to transform the layer outputs into a Gaussian
    distribution unit close to zero. This way, layers will have significantly less
    variation from batch to batch. Mathematically, the formula is very simple. The
    activation input x is centered around zero by subtracting the batch mean ![](img/B18331_01_034.png)
    from it. Then the result is divided by ![](img/B18331_01_035.png), the sum of
    batch variance ![](img/B18331_01_016.png), and a small number ![](img/B18331_01_037.png)
    to prevent division by zero. Then, we use a linear transformation ![](img/B18331_01_038.png)
    to make sure that the normalizing effect is applied during training.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键思想是将层的输出转换为接近零的高斯分布单位。这样，层之间的变化将显著减少。数学上，公式非常简单。通过从激活输入 x 中减去批次均值 ![](img/B18331_01_034.png)，使其围绕零进行居中。然后将结果除以
    ![](img/B18331_01_035.png)，即批次方差的和 ![](img/B18331_01_016.png)，并加上一个小数 ![](img/B18331_01_037.png)，以防止除以零。接着，我们使用线性变换
    ![](img/B18331_01_038.png) 来确保在训练过程中应用归一化效果。
- en: In this way, ![](img/B18331_01_039.png) and ![](img/B18331_01_040.png) are parameters
    that get optimized during the training phase in a way similar to any other layer.
    Batch normalization has been proven to be a very effective way to increase both
    the speed of training and accuracy, because it helps to prevent activations becoming
    either too small and vanishing or too big and exploding.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，![](img/B18331_01_039.png) 和 ![](img/B18331_01_040.png) 是在训练阶段优化的参数，优化方式类似于任何其他层。批量归一化已被证明是一种非常有效的方法，可以提高训练速度和准确性，因为它有助于防止激活值过小而消失或过大而爆炸。
- en: 'Playing with Google Colab: CPUs, GPUs, and TPUs'
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Google Colab：CPUs、GPUs 和 TPUs
- en: Google offers a truly intuitive tool for training neural networks and for playing
    with TensorFlow at no cost. You can find an actual Colab, which can be freely
    accessed, at [https://colab.research.google.com/](https://colab.research.google.com/)
    and if you are familiar with Jupyter notebooks you will find a very familiar web-based
    environment here. **Colab** stands for **Colaboratory** and is a Google research
    project created to help disseminate machine learning education and research. We
    will see the difference between CPUs, GPUs, and TPUs in *Chapter 15*, *Tensor
    Processing Unit*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: Google 提供了一款直观的工具，用于训练神经网络并免费体验 TensorFlow。你可以访问一个实际的 Colab，免费使用，地址为 [https://colab.research.google.com/](https://colab.research.google.com/)，如果你熟悉
    Jupyter notebooks，你会发现这里是一个非常熟悉的基于网页的环境。**Colab** 代表 **Colaboratory**，这是一个 Google
    的研究项目，旨在帮助传播机器学习教育和研究。我们将在*第 15 章*，*张量处理单元*中了解 CPU、GPU 和 TPU 之间的区别。
- en: 'For now, it’s important to know that CPUs are generic processing units, while
    GPUs and TPUs are accelerators, specific processing units suitable for deep learning.
    Let’s see how it works, starting with the screenshot shown in *Figure 1.23*:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，重要的是要知道，CPU 是通用处理单元，而 GPU 和 TPU 是加速器，专门用于深度学习的处理单元。让我们从 *图 1.23* 中显示的截图开始，看看它是如何工作的：
- en: '![](img/B18331_01_23.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_23.png)'
- en: 'Figure 1.23: An example of notebooks in Colab'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.23：Colab 中的笔记本示例
- en: By accessing Colab, we can either check a listing of notebooks generated in
    the past or we can create a new notebook. Different versions of Python are supported.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问 Colab，我们可以查看以前生成的笔记本列表，也可以创建新的笔记本。支持不同版本的 Python。
- en: 'When we create a new notebook, we can also select if we want to run it on CPUs,
    GPUs, or in Google’s TPUs as shown in *Figure 1.24*:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建一个新的笔记本时，我们还可以选择是否在 CPU、GPU 或 Google 的 TPU 上运行，如 *图 1.24* 所示：
- en: '![](img/B18331_01_24.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_24.png)'
- en: 'Figure 1.24: Selecting the desired hardware accelerator (None, GPUs, or TPUs)
    – the first step'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24：选择所需的硬件加速器（无、GPU 或 TPU）——第一步
- en: By accessing the **Notebook settings** option contained in the **Edit** menu
    (see *Figure 1.24* and *Figure 1.25*), we can select the desired hardware accelerator
    (**None**, **GPUs**, or **TPUs**). Google will allocate the resources at no cost,
    although they can be withdrawn at any time, for example during periods of a particularly
    heavy load. In my experience, this is a very rare event, and you can access Colab
    pretty much any time. However, be polite and do not do something like start mining
    bitcoins at no cost – you will almost certainly get evicted!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问**笔记本设置**选项，该选项位于**编辑**菜单中（见 *图 1.24* 和 *图 1.25*），我们可以选择所需的硬件加速器（**无**、**GPU**
    或 **TPU**）。谷歌会免费分配这些资源，尽管它们可能会随时撤回，例如在负载特别重的期间。根据我的经验，这种情况非常罕见，你几乎随时都可以使用 Colab。然而，还是请保持礼貌，不要做类似免费挖比特币的事情——你几乎肯定会被踢出去！
- en: '![](img/B18331_01_25.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_25.png)'
- en: 'Figure 1.25: Selecting the desired hardware accelerator (None, GPUs, or TPUs)
    – the second step'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25：选择所需的硬件加速器（无、GPU 或 TPU）——第二步
- en: 'The next step is to insert your code (see *Figure 1.26*) in the appropriate
    Colab notebook cells and *voila!* You are good to go. Execute the code and happy
    deep learning without the hassle of buying very expensive hardware to start your
    experiments! *Figure 1.26* contains an example of code in a Google notebook:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将你的代码插入到适当的 Colab 笔记本单元格中（见 *图 1.26*），然后 *瞧！* 你就准备好了。执行代码，享受深度学习的乐趣，无需购买非常昂贵的硬件即可开始实验！*图
    1.26* 展示了 Google 笔记本中的代码示例：
- en: '![](img/B18331_01_26.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_26.png)'
- en: 'Figure 1.26: An example of code in a notebook'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.26：笔记本中的代码示例
- en: Sentiment analysis
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'What is the code we used to test Colab? It is an example of sentiment analysis
    developed on top of the IMDB dataset. The IMDB dataset contains the text of 50,000
    movie reviews from the Internet Movie Database. Each review is either positive
    or negative (for example, thumbs up or thumbs down). The dataset is split into
    25,000 reviews for training and 25,000 reviews for testing. Our goal is to build
    a classifier that can predict the binary judgment given the text. We can easily
    load IMDB via `tf.keras` and the sequences of words in the reviews have been converted
    to sequences of integers, where each integer represents a specific word in a dictionary.
    We also have a convenient way of padding sentences to `max_len`, so that we can
    use all sentences, whether short or long, as inputs to a neural network with an
    input vector of fixed size:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来测试 Colab 的代码是什么？它是一个基于 IMDB 数据集开发的情感分析示例。IMDB 数据集包含来自互联网电影数据库的 50,000 条电影评论文本。每条评论要么是正面的，要么是负面的（例如，点赞或点踩）。数据集分为
    25,000 条用于训练的评论和 25,000 条用于测试的评论。我们的目标是构建一个分类器，根据文本预测二元判断。我们可以通过 `tf.keras` 轻松加载
    IMDB，评论中的单词序列已被转换为整数序列，其中每个整数代表字典中的一个特定单词。我们还可以方便地将句子填充至 `max_len`，这样我们就可以将所有句子（无论长短）作为固定大小输入向量，输入到神经网络中：
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now let’s build a model. We are going to use a few layers that will be explained
    in detail in *Chapter 4*, *Word Embeddings*. For now, let’s assume that the `embedding()`
    layer will map the sparse space of words contained in the reviews into a denser
    space. This will make computation easier. In addition, we will use a `GlobalMaxPooling1D()`
    layer, which takes the maximum value of either feature vector from each of the
    `n_words` features. In addition, we have two `Dense()` layers. The last one is
    made up of a single neuron with a sigmoid activation function for making the final
    binary estimation:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们构建一个模型。我们将使用几个在*第4章*中详细解释的层，*词嵌入*。现在假设`embedding()`层将把评论中包含的稀疏词空间映射到一个更密集的空间中，这将使计算变得更加容易。此外，我们将使用`GlobalMaxPooling1D()`层，它从每个`n_words`特征的特征向量中取最大值。此外，我们有两个`Dense()`层，最后一个层由一个带有sigmoid激活函数的神经元组成，用于进行最终的二分类估计：
- en: '[PRE22]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now we need to train our model and this piece of code is very similar to what
    we have done with MNIST. Let’s see:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要训练我们的模型，这段代码与我们之前在MNIST上做的非常相似。让我们来看一下：
- en: '[PRE23]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s see the network and then run a few iterations:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下网络，然后运行几个迭代：
- en: '[PRE24]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As shown in the following output, we reach accuracy of 85%, which is not bad
    at all for a simple network:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下输出所示，我们达到了85%的准确率，对于一个简单的网络来说，这已经相当不错了：
- en: '[PRE25]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The next section is devoted to tuning hyperparameters and AutoML.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将专门讨论超参数调优和AutoML。
- en: Hyperparameter tuning and AutoML
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调优和AutoML
- en: The experiments defined above give some opportunities for fine-tuning a net.
    However, what works for this example will not necessarily work for other examples.
    For a given neural network, there are indeed multiple parameters that can be optimized
    (such as the number of hidden neurons, batch size, number of epochs, and many
    more according to the complexity of the net itself). These parameters are called
    “hyperparameters” to distinguish them from the parameters of the network itself,
    i.e. the values of the weights and biases.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 上述定义的实验为微调网络提供了一些机会。然而，适用于这个例子的做法不一定适用于其他例子。对于一个给定的神经网络，确实有多个参数可以优化（例如隐藏神经元的数量、批次大小、训练周期数等，具体取决于网络本身的复杂性）。这些参数被称为“超参数”，以区别于网络本身的参数，即权重和偏差的值。
- en: Hyperparameter tuning is the process of finding the optimal combination of those
    hyperparameters that minimize cost functions. The key idea is that if we have
    *n* hyperparameters, then we can imagine that they define a space with *n* dimensions,
    and the goal is to find the point in this space that corresponds to an optimal
    value for the cost function. One way to achieve this goal is to create a grid
    in this space and systematically check the value assumed by the cost function
    for each grid vertex. In other words, the hyperparameters are divided into buckets
    and different combinations of values are checked via a brute-force approach.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优是寻找能够最小化成本函数的超参数最佳组合的过程。核心思想是，如果我们有*n*个超参数，那么我们可以想象它们定义了一个具有*n*个维度的空间，目标是找到这个空间中对应于成本函数最优值的点。实现这一目标的一种方式是创建一个网格，并系统地检查每个网格顶点处成本函数的值。换句话说，超参数被划分为不同的区间，并通过穷举法检查不同的组合值。
- en: If you think that this process of fine-tuning the hyperparameters is manual
    and expensive then you are absolutely right! However, during the last few years,
    we have seen significant results in AutoML, a set of research techniques aimed
    at both automatically tuning hyperparameters and searching automatically for optimal
    network architecture. We will discuss more about this in *Chapter 13*, *An Introduction
    to AutoML*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为这个微调超参数的过程是手动且昂贵的，那么你完全正确！然而，在过去的几年里，我们在AutoML领域看到了显著的成果，AutoML是一套旨在自动调优超参数并自动搜索最优网络架构的研究技术。我们将在*第13章*中进一步讨论这一内容，*AutoML简介*。
- en: Predicting output
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测输出
- en: 'Once a net is trained, it can of course be used for making predictions. In
    TensorFlow, this is very simple. We can use this method:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络训练完成，当然可以用来进行预测。在TensorFlow中，这非常简单。我们可以使用以下方法：
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: For a given input, several types of output can be computed including a method
    `model.evaluate()` used to compute the loss values, a method `model.predict_classes()`
    used to compute category outputs, and a method `model.predict_proba()` used to
    compute class probabilities.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入，可以计算出几种类型的输出，包括用于计算损失值的`model.evaluate()`方法，用于计算类别输出的`model.predict_classes()`方法，以及用于计算类别概率的`model.predict_proba()`方法。
- en: A practical overview of backpropagation
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播的实际概述
- en: Multi-layer perceptrons learn from training data through a process called backpropagation.
    In this paragraph we will give an intuition while more details are in *Chapter
    14*, *The Math Behind Deep Learning*. The process can be described as a way of
    progressively correcting mistakes as soon as they are detected. Let’s see how
    this works.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机通过一种叫做反向传播的过程从训练数据中学习。在这一段中，我们将给出一个直观的理解，更多细节见*第14章*，*深度学习背后的数学*。这个过程可以描述为一种在错误被检测到后逐步纠正的方式。让我们看看这个是如何运作的。
- en: Remember that each neural network layer has an associated set of weights that
    determine the output values for a given set of inputs. Additionally, remember
    that a neural network can have multiple hidden layers.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，每个神经网络层都有一组权重，这些权重决定了给定输入集的输出值。此外，记住神经网络可以有多个隐藏层。
- en: 'At the beginning, all the weights have some random assignment. Then the neural
    network is activated for each input in the training set: values are propagated
    *forward* from the input stage through the hidden stages to the output stage where
    a prediction is made.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，所有权重都有一些随机的赋值。然后，对于训练集中的每个输入，神经网络会被激活：值从输入阶段通过隐藏阶段传播*前向*到输出阶段，在输出阶段进行预测。
- en: 'Note that we keep *Figure 1.27* below simple by only representing a few values
    with green dotted lines but in reality, all the values are propagated forward
    through the network:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过仅用绿色虚线表示少量值来保持*图 1.27*的简单性，但实际上，所有的值都会通过网络前向传播：
- en: '![](img/B18331_01_27.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_27.png)'
- en: 'Figure 1.27: Forward step in backpropagation'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27：反向传播中的前向步骤
- en: 'Since we know the true observed value in the training set, it is possible to
    calculate the error made in the prediction. The key intuition for backtracking
    is to propagate the error back (see *Figure 1.28*), using an appropriate optimizer
    algorithm such as a GD to adjust the neural network weights with the goal of reducing
    the error (again for the sake of simplicity only a few error values are represented
    here):'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道训练集中的真实观察值，因此可以计算出预测中的误差。反向传播的关键直觉是将误差反向传播（见*图 1.28*），使用适当的优化算法，如梯度下降（GD），调整神经网络的权重，目的是减少误差（为了简化，这里只表示了一些误差值）：
- en: '![](img/B18331_01_28.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_28.png)'
- en: 'Figure 1.28: Backward step in backpropagation'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.28：反向传播中的反向步骤
- en: 'The process of forward propagation from input to output and backward propagation
    of errors is repeated several times until the error gets below a predefined threshold.
    The whole process is represented in *Figure 1.29*:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入到输出的前向传播过程和误差的反向传播过程会重复多次，直到误差低于预设的阈值。整个过程在*图 1.29*中表示：
- en: '![](img/B18331_01_29.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_01_29.png)'
- en: 'Figure 1.29: Forward propagation and backward propagation'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.29：前向传播和反向传播
- en: The features represent the input, and the labels are used here to drive the
    learning process. The model is updated in such a way that the loss function is
    progressively minimized. In a neural network, what really matters is not the output
    of a single neuron but the collective weights adjusted in each layer. Therefore,
    the network progressively adjusts its internal weights in such a way that the
    prediction increases the number of correctly forecasted labels. Of course, using
    the right set of features and having quality labeled data is fundamental to minimizing
    the bias during the learning process.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 特征表示输入，而标签在这里用于驱动学习过程。模型以一种方式进行更新，使得损失函数逐步最小化。在神经网络中，真正重要的不是单个神经元的输出，而是每一层中调整的集体权重。因此，网络逐步调整其内部权重，以便使得预测增加正确预测标签的数量。当然，使用正确的特征集并拥有高质量的标签数据是最小化学习过程中偏差的基础。
- en: What have we learned so far?
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 到目前为止我们学到了什么？
- en: In this chapter, we have learned the basics of neural networks. More specifically,
    we have learned what a perceptron is and what a multi-layer perceptron is, how
    to define neural networks in TensorFlow, how to progressively improve metrics
    once a good baseline is established, and how to fine-tune the hyperparameter space.
    In addition to that, we also have a good idea of useful activation functions (sigmoid
    and ReLU) available, and how to train a network with backpropagation algorithms
    based on either GD, SGD, or more sophisticated approaches, such as Adam and RMSProp.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经学会了神经网络的基础知识。更具体地说，我们学习了什么是感知器，什么是多层感知器，如何在 TensorFlow 中定义神经网络，如何在建立良好的基准后逐步改善指标，以及如何微调超参数空间。除此之外，我们还对有用的激活函数（sigmoid
    和 ReLU）有了很好的了解，以及如何通过基于 GD、SGD 或更复杂的方法（如 Adam 和 RMSProp）的反向传播算法训练网络。
- en: Toward a deep learning approach
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向深度学习方法迈进
- en: While playing with handwritten digit recognition, we came to the conclusion
    that the closer we get to an accuracy of 99%, the more difficult it is to improve.
    If we want more improvement, we definitely need a new idea. What are we missing?
    Think about it.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在玩手写数字识别时，我们得出结论：当我们接近 99% 的准确率时，改进变得更加困难。如果我们希望有更多改进，显然需要一个新思路。我们缺少什么？思考一下。
- en: 'The fundamental intuition is that in our examples so far, we are not making
    use of the local spatial structure of images, which means we will use the fact
    that an image can be described as a matrix with data locality. In particular,
    this piece of code transforms the bitmap representing each written digit into
    a flat vector where the local spatial structure (the fact that some pixels are
    closer to each other) is gone:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的直觉是，在我们到目前为止的例子中，我们没有利用图像的局部空间结构，这意味着我们将使用图像可以作为具有数据局部性的矩阵来描述的事实。特别是，这段代码将表示每个手写数字的位图转换为一个平坦的向量，其中局部空间结构（即某些像素相互靠近的事实）消失了：
- en: '[PRE27]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: However, this is not how our brain works. Remember that our vision is based
    on multiple cortex levels, each one recognizing more and more structured information
    while still preserving the locality. First, we see single pixels, then from that,
    we recognize simple geometric forms and then more and more sophisticated elements
    such as objects, faces, human bodies, animals, and so on.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是我们大脑的工作方式。请记住，我们的视觉是基于多个皮层层次的，每一层识别更多结构化的信息，同时仍然保留局部性。首先，我们看到单个像素，然后从这些像素中识别简单的几何形状，接着是越来越复杂的元素，如物体、面孔、人类身体、动物等。
- en: 'In *Chapter 3*, we will see that a particular type of deep learning network
    known as the **Convolutional Neural Network** (**CNN**) has been developed by
    taking into account both the idea of preserving the local spatial structure in
    images (and more generally in any type of information that has a spatial structure)
    and the idea of learning via progressive levels of abstraction: with one layer
    you can only learn simple patterns; with more than one layer you can learn multiple
    patterns. Before discussing CNN, we need to discuss some aspects of TensorFlow
    architecture and have a practical introduction to a few additional machine learning
    concepts.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*中，我们将看到一种特定类型的深度学习网络，即**卷积神经网络**（**CNN**）。这种网络通过考虑图像中的局部空间结构（更广泛地说，任何具有空间结构的信息）以及通过逐步抽象层次进行学习的思想而被开发出来：通过一层只能学习简单模式；而通过多层可以学习多个模式。在讨论
    CNN 之前，我们需要讨论 TensorFlow 架构的一些方面，并对一些额外的机器学习概念做一个实践性的介绍。
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we learned what TensorFlow and Keras are and introduced neural
    networks with the perceptron and the multi-layer perceptron. Then, we saw a real
    example of recognizing handwritten digits with several optimizations.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了什么是 TensorFlow 和 Keras，并介绍了感知器和多层感知器的神经网络。随后，我们通过几个优化方法看到了一个识别手写数字的实际例子。
- en: The next chapter is devoted to regression and classification.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将专注于回归与分类。
- en: References
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Rosenblatt, F. (1958). *The perceptron: a probabilistic model for information
    storage and organization in the brain*.Psychol. Rev, vol. 65, pp. 386–408.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rosenblatt, F. (1958). *感知器：一种用于大脑信息存储和组织的概率模型*。心理学评论，65 卷，第 386-408 页。
- en: 'Werbos, P. J. (1990). *Backpropagation through time: what it does and how to
    do it*. Proc. IEEE, vol. 78, pp. 1550–1560.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Werbos, P. J. (1990). *通过时间的反向传播：它的作用及如何实现*。IEEE 会议论文，78 卷，第 1550-1560 页。
- en: Hinton, G. E., Osindero, S., and Teh, Y. W. (2006). *A fast learning algorithm
    for deep belief nets*. Neural Comput, vol. 18, pp. 1527–1554.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G. E., Osindero, S., 和 Teh, Y. W. (2006). *深度置信网络的快速学习算法*。《神经计算》，第18卷，第1527–1554页。
- en: 'Schmidhuber, J. (2015). *Deep learning in neural networks: an overview*.Neural
    Networks : Off. J. Int. Neural Netw. Soc., vol. 61, pp. 85–117.'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schmidhuber, J. (2015). *神经网络中的深度学习：概述*。《神经网络：国际神经网络学会官方期刊》，第61卷，第85–117页。
- en: 'Leven, S. (1996). *The roots of backpropagation: From ordered derivatives to
    neural networks and political forecasting*.Neural Networks, vol. 9.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Leven, S. (1996). *反向传播的起源：从有序导数到神经网络和政治预测*。《神经网络》，第9卷。
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). *Learning representations
    by back-propagating errors*.Nature, vol. 323.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1986). *通过反向传播误差学习表示*。《自然》，第323卷。
- en: 'Herculano-Houzel, S. (2009). *The human brain in numbers: a linearly scaled-up
    primate brain*. Front. Hum. Neurosci., vol. 3.'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Herculano-Houzel, S. (2009). *人类大脑的数字：线性放大的灵长类动物大脑*。《前沿人类神经科学》，第3卷。
- en: Hornick, K., Stinchcombe, M., and White, H. (1989). *Multilayer feedforward
    networks are universal approximators*. Neural Networks Volume 2, Issue 5\. Pages
    359–366.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hornick, K., Stinchcombe, M., 和 White, H. (1989). *多层前馈网络是通用逼近器*。《神经网络》，第2卷，第5期，第359–366页。
- en: Vapnik, V. N. (2013). *The nature of statistical learning theory*.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vapnik, V. N. (2013). *统计学习理论的本质*。
- en: Sutskever, I., Martens, J., Dahl, G., Hinton, G., (2013). *On the importance
    of initialization and momentum in deep learning*. 30th International Conference
    on Machine Learning, ICML.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutskever, I., Martens, J., Dahl, G., Hinton, G. (2013). *初始化和动量在深度学习中的重要性*。第30届国际机器学习大会，ICML。
- en: Join our book’s Discord space
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区，与志同道合的人一起学习，和2000多名成员共同进步：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
