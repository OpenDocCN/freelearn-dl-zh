- en: Understanding Rewards-Based Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于奖励的学习
- en: The world is consumed with the machine learning revolution and, in particular,
    the search for a functional **artificial general intelligence** or **AGI**. Not
    to be confused with a conscious AI, AGI is a broader definition of machine intelligence
    that seeks to apply generalized methods of learning and knowledge to a broad range
    of tasks, much like the ability we have with our brains—or even small rodents
    have, for that matter. Rewards-based learning and, in particular, **reinforcement
    learning** (**RL**) are seen as the next steps to a more generalized intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 世界正被机器学习革命所吞噬，特别是对功能性**通用人工智能**或**AGI**的寻找。不要与有意识的AI混淆，AGI是对机器智能的更广泛定义，它寻求将通用学习方法应用于广泛的任务，就像我们用大脑的能力一样——甚至小老鼠也有这样的能力。基于奖励的学习，特别是**强化学习**（**RL**），被视为迈向更通用智能的下一步。
- en: '"Short-term AGI is a serious possibility."'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '"短期AGI是一个严肃的可能性。"'
- en: – OpenAI Co-founder and Chief Scientist, **Ilya Sutskever**
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – OpenAI联合创始人兼首席科学家，**伊利亚·苏茨克维**
- en: In this book, we start from the beginning of rewards-based learning and RL with
    its history to modern inception and its use in gaming and simulation. RL and,
    specifically, deep RL are gaining popularity in both research and use. In just
    a few years, the advances in RL have been dramatic, which have made it both impressive
    but, at the same time, difficult to keep up with and make sense of. With this
    book, we will unravel the abstract terminology that plagues this multi-branch
    and complicated topic in detail. By the end of this book, you should be able to
    consider yourself a confident practitioner of RL and deep RL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们从基于奖励的学习和强化学习的历史开始，探讨其从现代起源到在游戏和模拟中的应用。强化学习，尤其是深度强化学习，在研究和应用中都越来越受欢迎。仅仅几年时间，强化学习的发展就非常显著，这使得它既令人印象深刻，同时，也难以跟上并理解。通过本书，我们将详细揭示困扰这个多分支和复杂主题的抽象术语。到本书结束时，你应该能够认为自己是一个自信的强化学习和深度强化学习实践者。
- en: 'For this first chapter, we will start with an overview of RL and look at the
    terminology, history, and basic concepts. In this chapter, the high-level topics
    we will cover are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将从强化学习的概述开始，探讨术语、历史和基本概念。在本章中，我们将涵盖以下高级主题：
- en: Understanding rewards-based learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于奖励的学习
- en: Introducing the Markov decision process
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍马尔可夫决策过程
- en: Using value learning with multi-armed bandits
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多臂赌博机的价值学习
- en: Exploring Q-learning with contextual bandits
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索带有上下文赌博机的Q学习
- en: We want to mention some important technical requirements before continuing in
    the next section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，我们想提及一些重要的技术要求。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This book is a hands-on one, which means there are plenty of code examples to
    work through and discover on your own. The code for this book can be found in
    the following GitHub repository: [https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是一本实践性很强的书，这意味着有很多代码示例供你亲自操作和探索。本书的代码可以在以下GitHub仓库中找到：[https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)
- en: As such, be sure to have a working Python coding environment set up. Anaconda,
    which is a cross-platform wrapper framework for both Python and R, is the recommended
    platform to use for this book. We also recommend Visual Studio Code or Visual
    Studio Professional with the Python tools as good **Integrated development editors**,
    or **IDEs**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，请确保你已经设置了一个可工作的Python编码环境。Anaconda，这是一个跨平台的Python和R的包装框架，是本书推荐的平台。我们还推荐使用Visual
    Studio Code或带有Python工具的Visual Studio Professional作为好的**集成开发环境**或**IDE**。
- en: Anaconda, recommended for this book, can be downloaded from [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书推荐使用Anaconda，可以从[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)下载。
- en: With that out of the way, we can move on to learning the basics of RL and, in
    the next section, look at why rewards-based learning works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题解决之后，我们可以继续学习强化学习（RL）的基础知识，在下一节中，我们将探讨基于奖励的学习为何有效。
- en: Understanding rewards-based learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于奖励的学习
- en: 'Machine learning is quickly becoming a broad and growing category, with many
    forms of learning systems addressed. We categorize learning based on the form
    of a problem and how we need to prepare it for a machine to process. In the case
    of supervised machine learning, data is first labeled before it is fed into the
    machine. Examples of this type of learning are simple image classification systems
    that are trained to recognize a cat or dog from a prelabeled set of cat and dog
    images. Supervised learning is the most popular and intuitive type of learning
    system. Other forms of learning that are becoming increasingly powerful are unsupervised
    and semi-supervised learning. Both of these methods eliminate the need for labels
    or, in the case of semi-supervised learning, require the labels to be defined
    more abstractly. The following diagram shows these learning methods and how they
    process data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习正在迅速成为一个广泛且不断发展的类别，涵盖了多种学习系统。我们根据问题的形式以及我们如何为机器处理它进行分类。在监督机器学习的案例中，数据在输入机器之前首先被标记。这类学习的例子包括简单的图像分类系统，这些系统被训练从预先标记的猫和狗图像集中识别猫或狗。监督学习是最受欢迎且直观的学习系统类型。其他越来越强大的学习形式是无监督学习和半监督学习。这两种方法都消除了对标签的需求，或者在半监督学习的案例中，需要更抽象地定义标签。以下图示展示了这些学习方法和它们如何处理数据：
- en: '![](img/b9ba18f3-498c-4f72-89b7-88ff6d76e79a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9ba18f3-498c-4f72-89b7-88ff6d76e79a.png)'
- en: Variations of supervised learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的变体
- en: A couple of recent papers on [arXiv.org](http://arxiv.org) (pronounced archive.org)
    suggest the use of semi-supervised learning to solve RL tasks. While the papers
    suggest no use of external rewards, they do talk about internal updates or feedback
    signals. This suggests a method of using internal reward RL, which, as we mentioned
    before, is a thing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最近在 [arXiv.org](http://arxiv.org)（发音为archive.org）上的论文建议使用半监督学习来解决强化学习任务。虽然论文建议不使用外部奖励，但它们确实谈到了内部更新或反馈信号。这表明了一种使用内部奖励强化学习的方法，正如我们之前提到的，这是一个存在的事物。
- en: 'While this family of supervised learning methods has made impressive progress
    in just the last few years, they still lack the necessary planning and intelligence
    we expect from a truly intelligent machine. This is where RL picks up and differentiates
    itself. RL systems learn from interacting and making selections in the environment
    the agent resides in. The classic diagram of an RL system is shown here:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个监督学习方法的家族在过去几年中取得了令人印象深刻的进步，但它们仍然缺乏我们从真正智能机器中期望的必要规划和智能。这就是强化学习发挥作用并区别于其他方法的地方。强化学习系统通过在与代理所在的环境中进行交互和选择来学习。一个典型的强化学习系统图示如下：
- en: '![](img/af027863-1265-4af3-947a-b74dad43a9ec.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af027863-1265-4af3-947a-b74dad43a9ec.png)'
- en: An RL system
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强化学习系统
- en: 'In the preceding diagram, you can identify the main components of an RL system:
    the **Agent** and **Environment**, where the **Agent** represents the RL system,
    and the **Environment** could be representative of a game board, game screen,
    and/or possibly streaming data. Connecting these components are three primary
    signals, the **State**, **Reward**, and **Action**. The **State** signal is essentially
    a snapshot of the current state of **Environment**. The **Reward** signal may
    be externally provided by the **Environment** and provides feedback to the agent,
    either bad or good. Finally, the **Action** signal is the action the **Agent**
    selects at each time step in the environment. An action could be as simple as
    *jump* or a more complex set of controls operating servos. Either way, another
    key difference in RL is the ability for the agent to interact with, and change,
    the **Environment**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您可以识别出强化学习系统的主要组成部分：**代理**和**环境**，其中**代理**代表强化学习系统，而**环境**可能是游戏板、游戏屏幕，以及/或可能是流数据。连接这些组件的是三个主要信号：**状态**、**奖励**和**动作**。**状态**信号基本上是**环境**当前状态的快照。**奖励**信号可能由**环境**外部提供，并为代理提供反馈，无论是好是坏。最后，**动作**信号是代理在每个时间步在环境中选择的动作。一个动作可能像*跳跃*一样简单，或者是一组更复杂的控制伺服机构。无论如何，强化学习中的另一个关键区别是代理能够与**环境**交互并改变它。
- en: Now, don't worry if this all seems a little muddled still—early researchers
    often encountered trouble differentiating between supervised learning and RL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果这一切仍然显得有些混乱，请不要担心——早期研究人员经常在区分监督学习和强化学习之间遇到麻烦。
- en: In the next section, we look at more RL terminology and explore the basic elements
    of an RL agent.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨更多的强化学习术语，并探讨强化学习代理的基本要素。
- en: The elements of RL
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的要素
- en: 'Every RL agent is comprised of four main elements. These are **policy**, **reward
    function**, **value function**, and, optionally, **model**. Let''s now explore
    what each of these terms means in more detail:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个强化学习代理都由四个主要元素组成。这些是**策略**、**奖励函数**、**价值函数**，以及可选的**模型**。现在让我们更详细地探讨这些术语的含义：
- en: '**The policy**: A policy represents the decision and planning process of the
    agent. The policy is what decides the actions the agent will take during a step.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：策略代表了代理的决策和规划过程。策略决定了代理在每一步将采取哪些动作。'
- en: '**The reward function**: The reward function determines what amount of reward
    an agent receives after completing a series of actions or an action. Generally,
    a reward is given to an agent externally but, as we will see, there are internal
    reward systems as well.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励函数**：奖励函数决定了代理在完成一系列动作或单个动作后所获得的奖励量。通常，奖励是由外部给予代理的，但正如我们将看到的，也存在内部奖励系统。'
- en: '**The value function**: A value function determines the value of a state over
    the long term. Determining the value of a state is fundamental to RL and our first
    exercise will be determining state values.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**：价值函数决定了长期状态下状态的值。确定状态的值是强化学习的基础，我们的第一个练习将是确定状态值。'
- en: '**The model**: A model represents the environment in full. In the case of a
    game of tic-tac-toe, this may represent all possible game states. For more advanced
    RL algorithms, we use the concept of a partially observable state that allows
    us to do away with a full model of the environment. Some environments that we
    will tackle in this book have more states than the number of atoms in the universe.
    Yes, you read that right. In massive environments like that, we could never hope
    to model the entire environment state.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型代表了整个环境。在井字棋游戏中，这可能代表所有可能的游戏状态。对于更高级的强化学习算法，我们使用部分可观察状态的概念，这允许我们摆脱环境的完整模型。在这本书中我们将要解决的问题中，有些环境的状态数量比宇宙中的原子数量还要多。是的，您没有看错。在如此庞大的环境中，我们永远无法希望对整个环境状态进行建模。'
- en: We will spend the next several chapters covering each of these terms in excruciating
    detail, so don't worry if things feel a bit abstract still. In the next section,
    we will take a look at the history of RL.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几章中详细探讨这些术语，所以如果感觉有些抽象，请不要担心。在下一节中，我们将回顾强化学习的历史。
- en: The history of RL
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的历史
- en: '*An Introduction to RL*, by Sutton and Barto (1998), discusses the origins
    of modern RL being derived from two main threads with a later joining thread.
    The two main threads are trial and error-based learning and dynamic programming,
    with the third thread arriving later in the form of temporal difference learning.
    The primary thread founded by Sutton, trial and error, is based on animal psychology.
    As for the other methods, we will look at each in far more detail in their respective
    chapters. A diagram showing how these three threads converged to form modern RL
    is shown here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton和Barto（1998年）的《强化学习导论》讨论了现代强化学习的起源，它源自两个主要线索，后来又加入了一个线索。这两个主要线索是基于试错的学习和动态规划，第三个线索以时间差分学习的形式在后来出现。Sutton创立的主要线索，即试错，基于动物心理学。至于其他方法，我们将在各自的章节中更详细地探讨。下面是一个展示这三个线索如何汇聚形成现代强化学习的图表：
- en: '![](img/34897072-41fb-41c9-bbfa-14d3712d417f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/34897072-41fb-41c9-bbfa-14d3712d417f.png)'
- en: The history of modern RL
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现代强化学习的历史
- en: Dr. Richard S. Sutton, a distinguished research scientist for DeepMind and renowned
    professor from the University of Alberta, is considered the father of modern RL.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Dr. Richard S. Sutton，DeepMind的杰出研究科学家，同时也是阿尔伯塔大学的著名教授，被认为是现代强化学习（RL）之父。
- en: Lastly, before we jump in and start unraveling RL, let's look at why it makes
    sense to use this form of learning with games in the next section.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在我们深入探讨强化学习之前，让我们在下一节中看看为什么使用这种学习形式与游戏相结合是有意义的。
- en: Why RL in games?
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么是游戏中的强化学习？
- en: Various forms of machine learning systems have been used in gaming, with supervised
    learning being the primary choice. While these methods can be made to look intelligent,
    they are still limited by working on labeled or categorized data. While **generative
    adversarial networks** (**GANs**) show a particular promise in level and other
    asset generation, these families of algorithms cannot plan and make sense of long-term
    decision making. AI systems that replicate planning and interactive behavior in
    games are now typically done with hardcoded state machine systems such as finite
    state machines or behavior trees. Being able to develop agents that can learn
    for themselves the best moves or actions for an environment is literally game-changing,
    not only for the games industry, of course, but this should surely cause repercussions
    in every industry globally.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏领域，已经使用了各种形式的机器学习系统，其中监督学习是首选。虽然这些方法可以表现得像智能系统，但它们仍然受限于处理标记或分类的数据。尽管**生成对抗网络**（GANs）在关卡和其他资产生成方面显示出特别的潜力，但这些算法家族无法规划和理解长期决策。现在，在游戏中复制规划和交互行为的AI系统通常是通过硬编码的状态机系统，如有限状态机或行为树来实现的。能够开发出能够自己学习最佳移动或行为的智能体，这在实际上可以说是改变游戏规则，不仅对游戏产业如此，而且这肯定会在全球的每个产业中引起反响。
- en: In the next section, we take a look at the foundation of the RL system, the
    Markov decision process.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨强化学习系统的基础，即马尔可夫决策过程。
- en: Introducing the Markov decision process
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍马尔可夫决策过程
- en: In RL, the agent learns from the environment by interpreting the state signal.
    The state signal from the environment needs to define a discrete slice of the
    environment at that time. For example, if our agent was controlling a rocket,
    each state signal would define an exact position of the rocket in time. State,
    in that case, may be defined by the rocket's position and velocity. We define
    this state signal from the environment as a Markov state. The Markov state is
    not enough to make decisions from, and the agent needs to understand previous
    states, possible actions, and any future rewards. All of these additional properties
    may converge to form a Markov property, which we will discuss further in the next
    section.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体通过解释状态信号从环境中学习。环境的状态信号需要定义当时环境的离散切片。例如，如果我们的智能体正在控制一枚火箭，每个状态信号都会定义火箭在时间上的确切位置。在这种情况下，状态可能由火箭的位置和速度定义。我们将从环境中定义的这个状态信号称为马尔可夫状态。马尔可夫状态不足以做出决策，智能体需要理解先前状态、可能的行为以及任何未来的奖励。所有这些附加属性可能汇聚形成一个马尔可夫性质，我们将在下一节中进一步讨论。
- en: The Markov property and MDP
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫性质和马尔可夫决策过程
- en: 'An RL problem fulfills the Markov property if all Markov signals/states predict
    a future state. Subsequently, a Markov signal or state is considered a Markov
    property if it enables the agent to predict values from that state. Likewise,
    a learning task that is a Markov property and is finite is called a finite **Markov
    decision process**, or **MDP**. A very classic example of an MDP used to often
    explain RL is shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有马尔可夫信号/状态都预测未来状态，那么一个强化学习问题就满足了马尔可夫性质。随后，如果一个马尔可夫信号或状态能够使智能体从该状态预测值，那么它就被认为是具有马尔可夫性质的。同样，如果一个既是马尔可夫性质又是有限的学习任务，那么它被称为有限**马尔可夫决策过程**或**MDP**。这里展示了一个非常经典的MDP例子，经常用来解释强化学习：
- en: '![](img/de46c636-7f35-454c-8044-41e0fbce202a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de46c636-7f35-454c-8044-41e0fbce202a.png)'
- en: The Markov decision process (Dr. David Silver)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（Dr. David Silver）
- en: The preceding diagram was taken from the excellent online lecture by Dr. David
    Silver on YouTube ([https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)).
    Dr. Silver, a former student of Dr. Sutton, has since gone on to great fame by
    being the brains that power most of DeepMind's early achievements in RL.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表取自YouTube上David Silver博士的优秀在线讲座（[https://www.youtube.com/watch?v=2pWv7GOvuf0](https://www.youtube.com/watch?v=2pWv7GOvuf0)）。Silver博士是Sutton博士的前学生，后来因成为DeepMind在强化学习（RL）早期成就背后的智慧大脑而声名鹊起。
- en: The diagram is an example of a finite discrete MDP for a post-secondary student
    trying to optimize their actions for maximum reward. The student has the option
    of attending class, going to the gym, hanging out on Instagram or whatever, passing
    and/or sleeping. States are denoted by circles and the text defines the activity.
    In addition to this, the numbers next to each path from a circle denote the probability
    of using that path. Note how all of the values around a single circle sum to 1.0
    or 100% probability. The R= denotes the reward or output of the reward function
    when the student is in that state. To solidify this abstract concept further,
    let's build our own MDP in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该图是一个有限离散MDP的例子，用于一个试图优化其动作以获得最大奖励的大学后学生。学生可以选择上课、去健身房、在Instagram上闲逛，或者做其他事情，通过考试和/或睡觉。状态用圆圈表示，文本定义了活动。此外，每个圆圈旁边数字表示使用该路径的概率。注意，围绕单个圆圈的所有值总和为1.0或100%的概率。R=表示当学生在该状态下时，奖励函数的奖励或输出。为了进一步巩固这个抽象概念，让我们在下一节构建我们自己的MDP。
- en: Building an MDP
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个MDP
- en: 'In this hands-on exercise, we will build an MDP using a task from your own
    daily life or experience. This should allow you to better apply this abstract
    concept to something more tangible. Let''s begin as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个动手练习中，我们将使用你日常生活中或经验中的任务来构建一个马尔可夫决策过程（MDP）。这应该能让你更好地将这个抽象概念应用到更具体的事物上。让我们开始吧：
- en: Think of a daily task you do that may encompass six or so states. Examples of
    this may be going to school, getting dressed, eating, showering, browsing Facebook,
    and traveling.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想想你每天做的可能包含六个左右状态的任务。这类任务的例子可能包括上学、穿衣、吃饭、洗澡、浏览Facebook和旅行。
- en: Write each state within a circle on a full sheet of paper or perhaps some digital
    drawing app.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一张完整的纸张上或在某个数字绘图应用程序中，将每个状态写在圆圈内。
- en: Connect the states with the actions you feel most appropriate. For example,
    don't get dressed before you shower.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你认为最合适的动作连接各个状态。例如，不要在洗澡之前穿衣。
- en: Assign the probability you would use to take each action. For example, if you
    have two actions leaving a state, you could make them both 50/50 or 0.5/0.5, or
    some other combination that adds up to 1.0.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配你将用于采取每个动作的概率。例如，如果你有两个离开当前状态的动作，你可以使它们都是50/50或0.5/0.5，或者任何其他总和为1.0的组合。
- en: Assign the reward. Decide what rewards you would receive for being within each
    state and mark those on your diagram.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配奖励。决定你在每个状态下会得到什么奖励，并在你的图上标记出来。
- en: Compare your completed diagram with the preceding example. How did you do?
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你完成的图与前面的例子进行比较。你做得怎么样？
- en: Before we get to solving your MDP or others, we first need to understand some
    background on calculating values. We will uncover this in the next section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解决你的MDP或其他MDP之前，我们首先需要了解一些关于计算值的基础知识。我们将在下一节中揭示这一点。
- en: Using value learning with multi-armed bandits
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多臂老虎机进行价值学习
- en: 'Solving a full MDP and, hence, the full RL problem first requires us to understand
    values and how we calculate the value of a state with a value function. Recall
    that the value function was a primary element of the RL system. Instead of using
    a full MDP to explain this, we instead rely on a simpler single-state problem
    known as the multi-armed bandit problem. This is named after the one-armed slot
    machines often referred to as bandits by their patrons but, in this case, the
    machine has multiple arms. That is, we now consider a single-state or stationary
    problem with multiple actions that lead to terminal states providing constant
    rewards. More simply, our agent is going to play a multi-arm slot machine that
    will give either a win or loss based on the arm pulled, with each arm always returning
    the same reward. An example of our agent playing this machine is shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先解决一个完整的MDP，从而解决完整的强化学习（RL）问题，需要我们了解值以及如何使用价值函数计算状态的价值。回想一下，价值函数是RL系统的一个主要元素。我们不是使用完整的MDP来解释这一点，而是依赖于一个更简单的单状态问题，即多臂老虎机问题。这个名字来源于那些被称为“强盗”的顾客通常提到的一臂老虎机，但在这个案例中，机器有多个臂。也就是说，我们现在考虑一个单状态或稳态问题，有多个动作导致终端状态提供恒定的奖励。更简单地说，我们的代理将玩一个多臂老虎机，根据拉动的臂提供胜利或失败，每个臂总是返回相同的奖励。这里展示了我们的代理玩这个机器的例子：
- en: '![](img/4a1eab02-d933-4189-9134-27881ca733af.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4a1eab02-d933-4189-9134-27881ca733af.png)'
- en: Illustration of an agent playing multi-armed bandits
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机的代理演示
- en: 'We can consider the value for a single state to be dependent on the next action,
    provided we also understand the reward provided by that action. Mathematically,
    we can define a simple value equation for learning like so:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以考虑单个状态的值依赖于下一个动作，前提是我们还了解该动作提供的奖励。数学上，我们可以定义一个简单的学习价值方程如下：
- en: '![](img/af99e6f2-9a84-45e7-b895-e27d70a13847.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af99e6f2-9a84-45e7-b895-e27d70a13847.png)'
- en: 'In this equation, we have the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，我们有以下内容：
- en: '*V(a)*: the value for a given action'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*V(a)*：给定动作的值'
- en: '*a*: action'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a*：动作'
- en: '*α*: alpha or the learning rate'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α*：alpha 或学习率'
- en: '*r*: reward'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r*：奖励'
- en: Notice the addition of a new variable called α (alpha) or the learning rate.
    This learning rate denotes how fast the agent needs to learn the value from pull
    to pull. The smaller the learning rate (0.1), the slower the agent learns. This
    method of action-value learning is fundamental to RL. Let's code up this simple
    example to solidify further in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意新增了一个名为 α（alpha）或学习率的变量。这个学习率表示代理需要多快从拉动到拉动学习价值。学习率越小（0.1），代理的学习速度越慢。这种动作价值学习方法对于强化学习（RL）是基本的。让我们在下一节中通过编写这个简单的示例来进一步巩固。
- en: Coding a value learner
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写价值学习器
- en: 'Since this is our first example, make sure your Python environment is set to
    go. Again for simplicity, we prefer Anaconda. Make sure you are comfortable coding
    with your chosen IDE and open up the code example, `Chapter_1_1.py`, and follow
    along:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们第一个示例，请确保您的 Python 环境已经设置好。为了简单起见，我们更喜欢使用 Anaconda。请确保您对所选 IDE 的编码感到舒适，并打开代码示例
    `Chapter_1_1.py`，并跟随操作：
- en: 'Let''s examine the first section of the code, as shown here:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查代码的第一个部分，如下所示：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We first start by doing `import` of `random`. We will use `random` to randomly
    select an arm during each training episode.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先开始通过 `import` 导入 `random`。我们将在每个训练轮次中使用 `random` 随机选择一个臂。
- en: Next, we define a list of rewards, `reward`. This list defines the reward for
    each arm (action) and hence defines the number of arms/actions on the bandit.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个奖励列表，`reward`。这个列表定义了每个臂（动作）的奖励，从而定义了在老虎机上臂/动作的数量。
- en: Then, we determine the number of arms using the `len()` function.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们使用 `len()` 函数确定臂的数量。
- en: After that, we set the number of training episodes our agent will use to evaluate
    the value of each arm.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们设置代理将使用的训练轮数来评估每个臂的价值。
- en: Set the `learning_rate` value to `.1`. This means the agent will learn slowly
    the value of each pull.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `learning_rate` 值设置为 `.1`。这意味着代理将缓慢地学习每个拉动的价值。
- en: 'Next, we initialize the value for each action in a list called `Value`, using
    the following code:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码在一个名为 `Value` 的列表中初始化每个动作的值：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, we print the `Value` list to the console, making sure all of the values
    are 0.0.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将 `Value` 列表打印到控制台，确保所有值都是 0.0。
- en: 'The first section of code initialized our rewards, number of arms, learning
    rate, and value list. Now, we need to implement the training cycle where our agent/algorithm
    will learn the value of each pull. Let''s jump back into the code for `Chapter_1_1.py`
    and look to the next section:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第一个部分初始化了我们的奖励、臂的数量、学习率和值列表。现在，我们需要实现训练周期，在这个周期中，我们的代理/算法将学习每个拉动的价值。让我们回到
    `Chapter_1_1.py` 的代码中，查看下一部分：
- en: 'The next section of code in the listing we want to focus on is entitled `agent
    learns` and is shown here for reference:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列表中我们想要关注的下一部分代码是标题为 `agent learns` 的部分，如下所示供参考：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We start by first defining a `for` loop that loops through `0` to our number
    of episodes. For each episode, we let the agent pull an arm and use the reward
    from that pull to update its determination of value for that action or arm.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义一个 `for` 循环，该循环从 `0` 到我们的轮数。对于每个轮次，我们让代理拉动一个臂，并使用该拉动的奖励来更新其对该动作或臂的价值判断。
- en: 'Next, we want to determine the action or arm the agent pulls randomly using
    the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们想要确定代理随机拉动的动作或臂，使用以下代码：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code just selects a random arm/action number based on the total number of
    arms on the bandit (minus one to allow for proper indexing).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码只是根据老虎机上臂的总数（减一以允许正确的索引）随机选择一个臂/动作编号。
- en: 'This then allows us to determine the value of the pull by using the next line
    of code, which mirrors very well our previous value equation:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这允许我们使用下一行代码确定拉动的价值，该代码与我们的先前价值方程非常相似：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That line of code clearly resembles the math for our previous `Value` equation.
    Now, think about how `learning_rate` is getting applied during each iteration
    of an episode. Notice that, with a rate of `.1`, our agent is learning or applying
    1/10^(th) of what `reward` the agent receives minus the `Value` function the agent
    previously equated. This little trick has the effect of averaging out the values
    across the episodes.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这行代码明显类似于我们之前的`Value`方程的数学公式。现在，思考一下`learning_rate`是如何在剧集的每次迭代中应用的。注意，以`.1`的速率，我们的智能体正在学习或应用智能体收到的`reward`与智能体之前等价的`Value`函数之差的1/10^(th)。这个小技巧的效果是在剧集之间平均化值。
- en: Finally, after the looping completes and all of the episodes are run, we print
    the updated `Value` function for each action.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在循环完成后，并运行了所有剧集后，我们打印出每个动作更新的`Value`函数。
- en: 'Run the code from the command line or your favorite Python editor. In Visual
    Studio, this is as simple as hitting the play button. After the code has completed
    running, you should see something similar to the following, but not the exact
    output:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过命令行或您喜欢的Python编辑器运行代码。在Visual Studio中，这就像按一下播放按钮那么简单。代码运行完成后，你应该会看到以下类似的内容，但不是确切的输出：
- en: '![](img/56ea32c7-778f-4fa6-b871-c537ea378028.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56ea32c7-778f-4fa6-b871-c537ea378028.png)'
- en: Output from Chapter_1_1.py
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`Chapter_1_1.py`的输出
- en: You will most certainly see different output values since the random action
    selections on your computer will be different. Python has many ways to set static
    values for random seeds but that isn't something we want to worry about quite
    yet.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你肯定会看到不同的输出值，因为你在电脑上的随机动作选择将是不同的。Python有许多方法可以设置随机种子的静态值，但我们现在还不必担心这个问题。
- en: Now, think back and compare those output values to the rewards set for each
    arm. Are they the same or different and if so, by how much? Generally, the learned
    values after only 100 episodes should indicate a clear value but likely not the
    finite value. This means the values will be smaller than the final rewards but
    they should still indicate a preference.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回想一下，并将这些输出值与为每个杠杆设定的奖励进行比较。它们是否相同或不同？以及如果不同，差异有多大？一般来说，仅经过100个剧集的学习后，应该可以清楚地显示出价值，但可能不是最终的价值。这意味着这些值将小于最终奖励，但它们应该仍然显示出偏好。
- en: The solution we show here is an example of **trial and error** learning; it's
    that first thread we talked about back in the history of RL section. As you can
    see, the agent learns by randomly pulling an arm and determining the value. However,
    at no time does our agent learn to make better decisions based on those updated
    values. The agent always just pulls randomly. Our agent currently has no decision
    mechanism or what we call a **policy** in RL. We will look at how to implement
    a basic greedy policy in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里展示的解决方案是一个**试错学习**的例子；它就是我们在RL历史部分提到的那第一条线索。正如你所见，智能体通过随机拉动杠杆并确定其价值来学习。然而，我们的智能体从未学会根据那些更新的价值做出更好的决策。智能体总是随机拉动。我们的智能体目前还没有决策机制，或者我们称之为RL中的**策略**。我们将在下一节中探讨如何实现一个基本的贪婪策略。
- en: Implementing a greedy policy
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现贪婪策略
- en: 'Our current value learner is not really learning aside from finding the optimum
    calculated value or the reward for each action over several episodes. Since our
    agent is not learning, it also makes it a less efficient learner as well. After
    all, the agent is just randomly picking any arm each episode when it could be
    using its acquired knowledge, which is the `Value` function, to determine it''s
    next best choice. We can code this up in a very simple policy called a greedy
    policy in the next exercise:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的价值学习器除了在几个剧集内找到每个动作的最优计算值或奖励之外，并没有真正学习。由于我们的智能体没有学习，这也使得它成为一个效率较低的智能体。毕竟，智能体在每一期中只是随机选择任何杠杆，而它本可以使用其获得的知识，即`Value`函数，来确定其下一个最佳选择。我们可以在下一个练习中通过一个非常简单的策略，即贪婪策略来实现这一点：
- en: 'Open up the `Chapter_1_2.py` example. The code is basically the same as our
    last example except for the episode iteration and, in particular, the selection
    of action or arm. The full listing can be seen here—note the new highlighted sections:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_1_2.py`示例。代码基本上与我们的上一个示例相同，除了剧集迭代和特别地，动作或杠杆的选择。完整的列表可以在这里看到——注意新的高亮部分：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice the inclusion of a new `greedy()` function. This function will always
    select the action with the highest value and return the corresponding index/action
    index. This function is essentially our agent's policy.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到新加入的`greedy()`函数。这个函数将始终选择具有最高价值的动作，并返回相应的索引/动作索引。这个函数本质上就是我们的智能体的策略。
- en: 'Scrolling down in the code, notice inside the training loop how we are now
    using the `greedy()` function to select our action, as shown here:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码中向下滚动，注意在训练循环中我们现在是如何使用 `greedy()` 函数来选择我们的动作，如下所示：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Again, run the code and look at the output. Is it what you expected? What went
    wrong?
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，运行代码并查看输出。这是你预期的结果吗？出了什么问题？
- en: Looking at your output likely shows that the agent calculated the maximum reward
    arm correctly, but likely didn't determine the correct values for the other arms.
    The reason for this is that, as soon as the agent found the most valuable arm,
    it kept pulling that arm. Essentially the agent finds the best path and sticks
    with it, which is okay in this single step or stationary environment but certainly
    won't work over a many step problem requiring multiple decisions. Instead, we
    need to balance the agents need to explore and find new paths, versus maximizing
    the immediate optimum reward. This problem is called the **exploration versus
    exploitation** dilemma in RL and something we will explore in the next section.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 查看你的输出可能表明智能体正确地计算了最大奖励臂，但可能没有确定其他臂的正确价值。原因是，一旦智能体找到了最有价值的臂，它就会一直拉这个臂。本质上，智能体找到了最佳路径并坚持它，这在单步或静态环境中是可以的，但在需要多个决策的多个步骤问题中肯定不行。相反，我们需要平衡智能体探索和寻找新路径的需求，与最大化即时最优奖励。这个问题在强化学习中被称为**探索与利用**的困境，我们将在下一节中探讨。
- en: Exploration versus exploitation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: 'As we have seen, having our agent always make the best choice limits their
    ability to learn the full values of a single state never mind multiple connected
    states. This also severely limits an agent''s ability to learn, especially in
    environments where multiple states converge and diverge. What we need, therefore,
    is a way for our agent to choose an action based on a policy that favors more
    equal action/value distribution. Essentially, we need a policy that allows our
    agent to explore as well as exploit its knowledge to maximize learning. There
    are multiple variations and ways of balancing the trade-off between exploration
    and exploitation. Much of this will depend on the particular environment as well
    as the specific RL implementation you are using. We would never use an absolute
    greedy policy but, instead, some variation of greedy or another method entirely.
    In our next exercise, we show how to implement an initial optimistic value method,
    which can be effective:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，让我们的智能体总是做出最佳选择限制了它们学习单个状态（更不用说多个相连状态）的完整价值的能力。这也严重限制了智能体学习的能力，尤其是在多个状态汇聚和发散的环境中。因此，我们需要一种让我们的智能体根据一个偏向于更均匀的动作/价值分布的策略来选择动作的方法。本质上，我们需要一种策略，允许我们的智能体探索并利用其知识以最大化学习。探索与利用之间的权衡有多种变体和方式，这很大程度上取决于特定的环境和您使用的特定强化学习实现。我们永远不会使用绝对贪婪策略，而是使用贪婪策略的某种变体或完全不同的方法。在我们的下一个练习中，我们将展示如何实现一个初始乐观值方法，这可能非常有效：
- en: 'Open `Chapter_1_3.py` and look at the highlighted lines shown here:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 `Chapter_1_3.py` 并查看这里显示的突出显示的行：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we have increased the number of `episodes` to `10000`. This will allow
    us to confirm that our new policy is converging to some appropriate solution.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将 `episodes` 的数量增加到 `10000`。这将使我们能够确认我们的新策略正在收敛到某个适当的解决方案。
- en: Next, we set the initial value of the `Value` list to `5.0`. Note that this
    value is well above the reward value maximum of `1.0`. Using a higher value than
    our reward forces our agent to always explore the most valuable path, which now
    becomes any path it hasn't explored, hence ensuring our agent will always explore
    each action or arm at least once.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将 `Value` 列表的初始值设置为 `5.0`。请注意，这个值远高于奖励值的最大值 `1.0`。使用高于我们奖励值的高值迫使我们的智能体始终探索最有价值的路径，现在变成任何它尚未探索的路径，从而确保我们的智能体将始终至少探索每个动作或臂一次。
- en: 'There are no more code changes and you can run the example as you normally
    would. The output of the example is shown here:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有更多的代码更改，你可以像平常一样运行示例。示例的输出如下所示：
- en: '![](img/ab73e287-c06e-4544-8969-b6f6809d00fb.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab73e287-c06e-4544-8969-b6f6809d00fb.png)'
- en: Output from Chapter_1_3.py
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter_1_3.py 的输出
- en: Your output may vary slightly but it likely will show very similar values. Notice
    how the calculated values are now more relative. That is, the value of `1.0` clearly
    indicates the best course of action, the arm with a reward of `1.0`, but the other
    values are less indicative of the actual reward. Initial option value methods
    are effective but will force an agent to explore all paths, which are not so efficient
    in larger environments. There are of course a multitude of other methods you can
    use to balance exploration versus exploitation and we will cover a new method
    in the next section, where we introduce solving the full RL problem with Q-learning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您的输出可能略有不同，但很可能显示非常相似的价值。注意计算出的值现在更加相对。也就是说，`1.0`的价值清楚地表明了最佳的行动方案，即奖励为`1.0`的臂，但其他值对实际奖励的指示性较弱。初始选项价值方法有效，但会迫使智能体探索所有路径，这在较大的环境中效率不高。当然，还有许多其他方法可以用来平衡探索与利用，我们将在下一节中介绍一种新方法，其中我们将介绍如何使用Q学习解决完整的RL问题。
- en: Exploring Q-learning with contextual bandits
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有上下文老虎机的探索Q学习
- en: 'Now that we understand how to calculate values and the delicate balance of
    exploration and exploitation, we can move on to solving an entire MDP. As we will
    see, various solutions work better or worse depending on the RL problem and environment.
    That is actually the basis for the next several chapters. For now, though, we
    just want to introduce a method that is basic enough to solve the full RL problem.
    We describe the full RL problem as the non-stationary or contextual multi-armed
    bandit problem, that is, an agent that moves across a different bandit each episode
    and chooses a single arm from multiple arms. Each bandit now represents a different
    state and we no longer want to determine just the value of an action but the quality.
    We can calculate the quality of an action given a state using the Q-learning equation
    shown here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何计算值以及探索和利用之间的微妙平衡，我们可以继续解决整个MDP问题。正如我们将看到的，不同的解决方案在RL问题和环境中表现得好或不好。这实际上是下一几个章节的基础。然而，目前我们只想介绍一种足够基本的方法，可以解决完整的RL问题。我们将完整的RL问题描述为非平稳或上下文化的多臂老虎机问题，即在每个回合中移动到不同的老虎机并从多个臂中选择一个臂的智能体。现在每个老虎机代表一个不同的状态，我们不再只想确定动作的价值，而是其质量。我们可以使用这里显示的Q学习方程来计算给定状态的动作质量：
- en: '![](img/04ab42e6-5e3e-4b4e-a48c-14760d0e428f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/04ab42e6-5e3e-4b4e-a48c-14760d0e428f.png)'
- en: 'In the preceding equation, we have the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '![](img/6d397c65-4e23-4b49-9c8f-6a0036816a5b.png): state'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![状态](img/6d397c65-4e23-4b49-9c8f-6a0036816a5b.png)'
- en: '![](img/24f39736-665b-475e-ae52-d143b709d7f9.png): current state'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![当前状态](img/24f39736-665b-475e-ae52-d143b709d7f9.png)：状态'
- en: '![](img/66980f9a-9d74-4324-95f1-6787c2118fca.png): next action'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![下一动作](img/66980f9a-9d74-4324-95f1-6787c2118fca.png)'
- en: '![](img/696a9e64-bb73-4824-bf1d-83c7a5cf0e36.png): current action'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![当前动作](img/696a9e64-bb73-4824-bf1d-83c7a5cf0e36.png)'
- en: 'ϒ: gamma—reward discount'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ϒ：gamma—奖励折扣
- en: 'α: alpha—learning rate'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α：alpha—学习率
- en: 'r: reward'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: r：奖励
- en: '![](img/226fbd43-f55d-4b2c-96bb-12aa1fc6797e.png): next reward'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![下一奖励](img/226fbd43-f55d-4b2c-96bb-12aa1fc6797e.png)：奖励折扣'
- en: '![](img/989607da-b947-4a5f-b610-1ada3e3fe922.png): quality'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![质量](img/989607da-b947-4a5f-b610-1ada3e3fe922.png)：质量'
- en: Now, don't get overly concerned if all of these terms are a little foreign and
    this equation appears overwhelming. This is the Q-learning equation developed
    by Chris Watkins in 1989 and is a method that simplifies the solving of a **Finite
    Markov Decision Process** or **FMDP**. The important thing to observe about the
    equation at this point is to understand the similarities it shares with the earlier
    action-value equation. In [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml),
    *Dynamic Programming and the Bellman Equation*, we will learn in more detail how
    this equation is derived and functions. For now, the important concept to grasp
    is that we are now calculating a quality-based value on previous states and rewards
    based on actions rather than just a single action-value. This, in turn, allows
    our agent to make better planning for multiple states. We will implement a Q-learning
    agent that can play several multi-armed bandits and be able to maximize rewards
    in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果所有这些术语都有些陌生，这个方程看起来有些令人望而生畏，请不要过分担心。这是由克里斯·沃特金斯在 1989 年开发的 Q 学习方程，它是一种简化求解**有限马尔可夫决策过程**或**FMDP**的方法。在这个阶段，关于这个方程的重要观察点是理解它与我们之前看到的动作值方程的相似之处。在[第
    2 章](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml)《动态规划和贝尔曼方程》中，我们将更详细地学习这个方程是如何推导和工作的。现在，我们需要掌握的重要概念是，我们现在正在根据之前的状态和奖励以及动作来计算一个基于质量的值，而不仅仅是单个动作值。这反过来又允许我们的智能体为多个状态做出更好的规划。在下一节中，我们将实现一个可以玩多个多臂老虎机的
    Q 学习智能体，并能够最大化奖励。
- en: Implementing a Q-learning agent
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个 Q 学习智能体
- en: 'While that Q-learning equation may seem a lot more complex, actually implementing
    the equation is not unlike building our agent that just learned values earlier.
    To keep things simpler, we will use the same base of code but turn it into a Q-learning
    example. Open up the code example, `Chapter_1_4.py`, and follow the exercise here:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然那个 Q 学习方程可能看起来复杂得多，但实际上实现这个方程并不像我们之前学习值时构建我们的智能体那样。为了使事情更简单，我们将使用相同的代码基础，但将其转换为
    Q 学习示例。打开代码示例，`Chapter_1_4.py`，并按照这里的练习进行：
- en: 'Here is the full code listing for reference:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是完整的代码列表，供参考：
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'All of the highlighted sections of code are new and worth paying closer attention
    to. Let''s take a look at each section in more detail here:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有加亮的代码部分都是新的，值得仔细关注。让我们更详细地看看每个部分：
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We start by initializing the `arms` variable to `7` then a new `bandits` variable
    to `7` as well. Recall that `arms` is analogous to `actions` and `bandits` likewise
    is to `state`. The last new variable, `gamma`, is a new learning parameter used
    to discount rewards. We will explore this discount factor concept in future chapters:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将 `arms` 变量初始化为 `7`，然后创建一个新的 `bandits` 变量也是 `7`。回想一下，`arms` 类似于 `actions`，而
    `bandits` 同样是 `state`。最后一个新变量 `gamma` 是一个新的学习参数，用于折现奖励。我们将在未来的章节中探讨这个折现因子概念：
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The next section of code builds up the reward table matrix as a set of random
    values from -1 to 1\. We use a list of lists in this example to better represent
    the separate concepts:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一节代码构建了一个奖励表矩阵，其中包含从 -1 到 1 的随机值。在这个例子中，我们使用列表的列表来更好地表示单独的概念：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following section is very similar and this time sets up a Q table matrix
    to hold our calculated quality values. Notice how we initialize our starting Q
    value to 10.0\. We do this to account for subtle changes in the math, again something
    we will discuss later.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个部分非常相似，这次设置了一个 Q 表矩阵来存储我们计算的质量值。注意我们如何将初始 Q 值初始化为 10.0。我们这样做是为了考虑到数学中的细微变化，我们将在后面讨论这一点。
- en: 'Since our states and actions can be all mapped onto a matrix/table, we refer
    to our RL system as using a model. A model represents all actions and states of
    an environment:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们的状态和动作都可以映射到一个矩阵/表中，我们将我们的强化学习系统称为使用模型。模型代表环境中的所有动作和状态：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We next define a new function called `learn`. This new function is just a straight
    implementation of the Q equation we observed earlier:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个新的函数，称为 `learn`。这个新函数只是我们之前观察到的 Q 方程的直接实现：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Finally, the agent learning section is updated significantly with new code.
    This new code sets up the parameters we need for the new learn function we looked
    at earlier. Notice how the bandit or state is getting randomly selected each time.
    Essentially, this means our agent is just randomly walking from bandit to bandit.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，智能体学习部分通过新的代码进行了显著更新。这段新代码设置了我们需要的新学习函数的参数。注意老虎机或状态每次都是随机选择的。本质上，这意味着我们的智能体只是在老虎机之间随机漫步。
- en: Run the code as you normally would and notice the new calculated Q values printed
    out at the end. Do they match the rewards for each of the arm pulls?
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正常运行代码，并注意最后打印出的新计算出的Q值。它们是否与每次拉动臂杆的奖励相匹配？
- en: Likely, a few of your arms don't match up with their respective reward values.
    This is because the new Q-learning equation solves the entire MDP but our agent
    is NOT moving in an MDP. Instead, our agent is just randomly moving from state
    to state with no care on which state it saw before. Think back to our example
    and you will realize since our current state does not affect our future state,
    it fails to be a Markov property and hence is not an MDP. However, that doesn't
    mean we can't successfully solve this problem and we will look to do that in the
    next section.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，你的几个臂杆与相应的奖励值不匹配。这是因为新的Q学习方程解决了整个MDP，但我们的智能体并没有在MDP中移动。相反，我们的智能体只是在状态之间随机移动，而不关心它之前看到了哪个状态。回想一下我们的例子，你就会意识到，由于我们的当前状态不会影响我们的未来状态，它没有满足马尔可夫属性，因此不是一个MDP。然而，这并不意味着我们不能成功地解决这个问题，我们将在下一节中探讨这一点。
- en: Removing discounted rewards
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除折现奖励
- en: 'The problem with our current solution and using the full Q-learning equation
    is that the equation assumes any state our agent is in affects future states.
    Except, remember in our example, the agent just walked randomly from bandit to
    bandit. This means using any previous state information would be useless, as we
    saw. Fortunately, we can easily fix this by removing the concept of discounted
    rewards. Recall that new variable, gamma, that appeared in this complicated term: ![](img/3372b23b-e6dc-4ac3-b188-96f0350de6a6.png).
    Gamma and this term are a way of discounting future rewards and something we will
    discuss at length starting in [Chapter 2](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml), *Dynamic
    Programming and the Bellman Equation*. For now, though, we can fix this sample
    up by just removing that term from our learn function. Let''s open up code example, `Chapter_1_5.py`,
    and follow the exercise here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前解决方案和全Q学习方程的问题在于，该方程假设我们的智能体（agent）所处的任何状态都会影响未来的状态。然而，记住在我们的例子中，智能体只是随机地从强盗移动到强盗。这意味着使用任何先前状态信息将是无用的，正如我们所看到的。幸运的是，我们可以通过移除折现奖励的概念来轻松解决这个问题。回想一下，在这个复杂项中出现的新的变量gamma： ![](img/3372b23b-e6dc-4ac3-b188-96f0350de6a6.png)。Gamma和这个项是折现未来奖励的一种方式，我们将在[第二章](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml)，“动态规划和贝尔曼方程”中详细讨论。现在，尽管如此，我们可以通过从我们的学习函数中移除这个项来修复这个样本。让我们打开代码示例，`Chapter_1_5.py`，并遵循这里的练习：
- en: 'The only section of code we really need to focus on is the updated `learn`
    function, here:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们真正需要关注的代码部分只有更新的`learn`函数，如下所示：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first line of code in the function is responsible for discounting the future
    reward of the next state. Since none of the states in our example are connected,
    we can just comment out that line. We create a new initializer for `q = 0` in
    the next line.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数中的第一行代码负责折现下一个状态的未来奖励。由于我们例子中的所有状态都没有连接，我们只需注释掉那一行。在下一行，我们为`q = 0`创建一个新的初始化器。
- en: Run the code as you normally would. Now you should see very close values closely
    matching their respective rewards.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正常运行代码。现在你应该会看到非常接近的值与各自的奖励非常接近。
- en: By omitting the discounted rewards part of the calculation, hopefully, you can
    appreciate that this would just revert to a value calculation problem. Alternatively,
    you may also realize that if our bandits were connected. That is, pulling an arm
    led to another one arm machine with more actions and so on. We could then use
    the Q-learning equation to solve the problem as well.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过省略计算中的折现奖励部分，希望你能理解这将仅仅回归到一个价值计算问题。或者，你也可能意识到，如果我们的强盗（bandits）是相互连接的。也就是说，拉动一个臂杆会导致另一个具有更多动作的臂杆机器，如此类推。那么我们可以使用Q学习方程来解决这个问题。
- en: That concludes a very basic introduction to the primary components and elements
    of RL. Throughout the rest of this book, we will dig into the nuances of policies,
    values, actions, and rewards.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对强化学习（RL）的主要组件和元素的非常基础的介绍。在这本书的其余部分，我们将深入探讨策略、价值、动作和奖励的细微差别。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we first introduced ourselves to the world of RL. We looked
    at what makes RL so unique and why it makes sense for games. After that, we explored
    the basic terminology and history of modern RL. From there, we looked to the foundations
    of RL and the Markov decision process, where we discovered what makes an RL problem.
    Then we looked to building our first learner a value learner that calculated the
    values of states on an action. This led us to uncover the need for exploration
    and exploitation and the dilemma that constantly challenges RL implementers. Next,
    we jumped in and discovered the full Q-learning equation and how to build a Q-learner,
    where we later realized that the full Q equation was beyond what we needed for
    our unconnected state environment. We then reverted our Q learned back into a
    value learner and watched it solve the contextual bandit problem.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先向RL的世界介绍了自己。我们探讨了为什么RL如此独特以及为什么它对游戏来说是有意义的。之后，我们研究了现代RL的基本术语和历史。从那里，我们转向RL的基础和马尔可夫决策过程，我们发现是什么构成了RL问题。然后我们转向构建我们的第一个学习者——一个价值学习者，它计算状态在动作上的值。这使我们发现了探索和利用的需求以及不断挑战RL实施者的困境。接下来，我们深入研究了完整的Q学习方程以及如何构建Q学习者，后来我们意识到完整的Q方程超出了我们未连接状态环境的需求。然后我们将我们的Q学习重新转换为价值学习者，并观察它解决上下文投币机问题。
- en: In the next chapter, we will continue from where we left off and look into how
    rewards are discounted with the Bellman equation, as well as look at the many
    other improvements dynamic programming has introduced to RL.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们留下的内容，探讨如何使用贝尔曼方程对奖励进行折现，以及查看动态规划为RL引入的许多其他改进。
- en: Questions
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Use these questions and exercises to reinforce the material you just learned.
    The exercises may be fun to attempt, so be sure to try atleast two to four questions/exercises:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些问题和练习来巩固你刚刚学到的内容。这些练习可能很有趣，所以请确保尝试至少两到四个问题/练习：
- en: 'Questions:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: What are the names of the main components of an RL system? Hint, the first one
    is **Environment**.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习（RL）系统的主要组成部分有哪些名称？提示：第一个是**环境**。
- en: Name the four elements of an RL system. Remember that one element is optional.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出强化学习（RL）系统的四个要素。请记住，其中一个要素是可选的。
- en: Name the three main threads that compose modern RL.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出组成现代强化学习（RL）的三个主要线程。
- en: What makes a Markov state a Markov property?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么使马尔可夫状态具有马尔可夫性质？
- en: What is a policy?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 政策是什么？
- en: 'Exercises:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 练习：
- en: Using `Chapter_1_2.py`, alter the code so the agent pulls from a bandit with
    1,000 arms. What code changes do you need to make?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Chapter_1_2.py`，修改代码使代理从一个有1,000个臂的投币机中抽取。你需要做出哪些代码更改？
- en: Using `Chapter_1_3.py`, alter the code so that the agent pulls from the average
    value, not greedy/max. How did this affect the agent's exploration?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Chapter_1_3.py`，修改代码使代理从平均值而非贪婪/最大值中抽取。这如何影响了代理的探索？
- en: Using `Chapter_1_3.py`, alter the `learning_rate` variable to determine how
    fast or slow you can make the agent learn. How few episodes are you required to
    run for the agent to solve the problem?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Chapter_1_3.py`，修改 `learning_rate` 变量以确定你可以使代理学习得多快或多慢。你需要运行多少个剧集才能使代理解决问题？
- en: Using `Chapter_1_5.py`, alter the code so that the agent uses a different policy
    (either the greedy policy or something else). Take points off yourself if you
    look ahead in this book or online for solutions.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Chapter_1_5.py`，修改代码使代理使用不同的策略（无论是贪婪策略还是其他策略）。如果你在这本书或网上查找解决方案，请扣分。
- en: Using `Chapter_1_4.py`, alter the code so that the bandits are connected. Hence,
    when an agent pulls an arm, they receive a reward and are transported to another
    specific bandit, no longer at random. **Hint:** This likely will require a new
    destination table to be built and you will now need to include the discounted
    reward term we removed.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `Chapter_1_4.py`，修改代码以便使投币机连接起来。因此，当代理拉动一个臂时，他们会获得奖励并被传输到另一个特定的投币机，不再是随机的。**提示**：这很可能需要构建一个新的目的地表，并且你现在需要包括我们之前移除的折现奖励项。
- en: Even completing a few of these questions and/or exercises will make a huge difference
    to your learning this material. This is a hands-on book after all.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 即使完成这些问题和/或练习中的几个，也会对你的学习产生巨大影响。毕竟，这是一本实践性很强的书。
