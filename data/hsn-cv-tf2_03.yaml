- en: TensorFlow Basics and Training a Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow基础与模型训练
- en: '**TensorFlow** is a numerical processing library used by researchers and machine
    learning practitioners. While you can perform any numerical operation with TensorFlow,
    it is mostly used to train and run deep neural networks. This chapter will introduce
    you to the core concepts of TensorFlow 2 and walk you through a simple example.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow**是一个用于数值处理的库，供研究人员和机器学习从业者使用。虽然你可以使用TensorFlow执行任何数值运算，但它主要用于训练和运行深度神经网络。本章将介绍TensorFlow
    2的核心概念，并带你通过一个简单的示例。'
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Getting started with TensorFlow 2 and Keras
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2和Keras入门
- en: Creating and training a simple computer vision model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并训练一个简单的计算机视觉模型
- en: TensorFlow and Keras core concepts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow和Keras核心概念
- en: The TensorFlow ecosystem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow生态系统
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Throughout this book, we will use TensorFlow 2\. You can find detailed installation
    instructions for the different platforms at [https://www.tensorflow.org/install](https://www.tensorflow.org/install/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中将使用TensorFlow 2。你可以在[https://www.tensorflow.org/install](https://www.tensorflow.org/install/)上找到适用于不同平台的详细安装说明。
- en: If you plan on using your machine's GPU, make sure you install the corresponding
    version, `tensorflow-gpu`. It must be installed along with the CUDA Toolkit, a
    library provided by NVIDIA ([https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用机器的GPU，请确保安装相应的版本，`tensorflow-gpu`。它必须与CUDA工具包一起安装，CUDA工具包是NVIDIA提供的一个库（[https://developer.nvidia.com/cuda-zone](https://developer.nvidia.com/cuda-zone)）。
- en: Installation instructions are also available in the README on GitHub at [https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 安装说明也可以在GitHub的README中找到，链接为[https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02)。
- en: Getting started with TensorFlow 2 and Keras
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow 2和Keras入门
- en: Before detailing the core concepts of TensorFlow, we will start with a brief
    introduction of the framework and a basic example.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细介绍TensorFlow的核心概念之前，我们将简要介绍该框架，并提供一个基本示例。
- en: Introducing TensorFlow
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍TensorFlow
- en: TensorFlow was originally developed at Google to allow researchers and developers
    to conduct machine learning research. It was originally defined as *an interface
    for expressing machine learning algorithms, and an implementation for executing
    such algorithms.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow最初由Google开发，用于让研究人员和开发人员进行机器学习研究。它最初被定义为*表达机器学习算法的接口，以及执行这些算法的实现*。
- en: TensorFlow primarily offers to simplify the deployment of machine learning solutions
    on various platforms—computer CPUs, computer GPUs, mobile devices, and, more recently,
    in the browser. On top of that, TensorFlow offers many useful functions for creating
    machine learning models and running them at scale. In 2019, TensorFlow 2 was released
    with a focus on ease of use while maintaining good performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow主要用于简化在各种平台上部署机器学习解决方案——计算机CPU、计算机GPU、移动设备，以及最近的浏览器平台。此外，TensorFlow还提供了许多有用的功能，用于创建机器学习模型并进行大规模运行。2019年，TensorFlow
    2发布，重点是易用性，同时保持良好的性能。
- en: An introduction to TensorFlow 1.0's concepts is available in [Appendix](59767fa2-b254-47a4-a39a-3f8c826490fa.xhtml), *Migrating
    from TensorFlow 1 to TensorFlow 2* of this book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关于TensorFlow 1.0概念的介绍可以在本书的[附录](59767fa2-b254-47a4-a39a-3f8c826490fa.xhtml)中的《*从TensorFlow
    1迁移到TensorFlow 2*》找到。
- en: The library was open sourced in November 2015\. Since then, it has been improved
    and used by users all around the world. It is considered one of the platforms
    of choice for research. It is also one of the most active deep learning frameworks
    in terms of GitHub activity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该库于2015年11月开源。从那时起，它得到了不断的改进，并被全球用户广泛使用。它被认为是研究中最受欢迎的平台之一。它也是GitHub活动中最活跃的深度学习框架之一。
- en: TensorFlow can be used by beginners as well as experts. The TensorFlow API has
    different levels of complexity, allowing newcomers to start with a simple API
    and experts to create very complex models at the same time. Let's explore those
    different levels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow既适合初学者，也适合专家使用。TensorFlow API具有不同的复杂度级别，允许新手从简单的API开始，而专家则可以同时创建非常复杂的模型。让我们来探索这些不同的级别。
- en: TensorFlow's main architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow的主要架构
- en: 'TensorFlow''s architecture has several levels of abstraction. Let''s first
    introduce the lowest layer and find our way to the uppermost layer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的架构具有多个抽象层次。我们首先介绍最底层，然后逐层走向最顶层：
- en: '![](img/554cef8f-62a0-4b4c-abb8-9ab79075c69f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/554cef8f-62a0-4b4c-abb8-9ab79075c69f.png)'
- en: 'Figure 2.1: Diagram of the TensorFlow architecture'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：TensorFlow架构图
- en: Most deep learning computations are coded in C++. To run operations on the GPU,
    TensorFlow uses a library developed by NVIDIA called **CUDA**. This is the reason
    you need to install CUDA if you want to exploit GPU capabilities and why you cannot
    use GPUs from another hardware manufacturer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习计算是用C++编写的。为了在GPU上运行操作，TensorFlow使用了NVIDIA开发的一个库，叫做**CUDA**。这也是你需要安装CUDA，如果希望利用GPU功能的原因，同时也解释了为何不能使用来自其他硬件制造商的GPU。
- en: The Python **low-level** **API** then wraps the C++ sources. When you call a
    Python method in TensorFlow, it usually invokes C++ code behind the scenes. This
    wrapper layer allows users to work more quickly because Python is considered easier
    to use than C++ and does not require compilation. This Python wrapper makes it
    possible to perform extremely basic operations such as matrix multiplication and
    addition.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Python**低级** **API**然后对C++源代码进行了封装。当你在TensorFlow中调用Python方法时，通常会在后台调用C++代码。这个封装层让用户能够更快地工作，因为Python被认为比C++更容易使用且不需要编译。这个Python封装使得执行一些非常基础的操作（如矩阵乘法和加法）成为可能。
- en: At the top sits the **high-level API**, made of two components—Keras and the
    Estimator API. **Keras** is a user-friendly, modular, and extensible wrapper for
    TensorFlow. We will introduce it in the next section. The **Estimator API** contains
    several pre-made components that allow you to build your machine learning model
    easily. You can consider them building blocks or templates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在最顶层是**高级API**，由两个组件组成——Keras和Estimator API。**Keras**是一个用户友好、模块化、可扩展的TensorFlow封装器。我们将在下一节介绍它。**Estimator
    API**包含若干预先构建的组件，允许你轻松构建机器学习模型。你可以将它们视为构建块或模板。
- en: In deep learning, a **model** usually refers to a neural network that was trained
    on data. A model is composed of an architecture, matrix weights, and parameters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，**模型**通常指的是一个在数据上训练过的神经网络。一个模型由架构、矩阵权重和参数组成。
- en: Introducing Keras
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Keras
- en: First released in 2015, Keras was designed as an interface to enable fast experimentation
    with neural networks. As such, it relied on TensorFlow or **Theano** (another
    deep learning framework, now deprecated) to run deep learning operations. Known
    for its user-friendliness, it was the library of choice for beginners.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Keras于2015年首次发布，旨在作为一个接口，便于快速进行神经网络实验。因此，它依赖TensorFlow或**Theano**（另一种已废弃的深度学习框架）来执行深度学习操作。Keras以其用户友好性著称，是初学者的首选库。
- en: 'Since 2017, TensorFlow has integrated Keras fully, meaning that you can use
    it without installing anything other than TensorFlow. Throughout this book, we
    will rely on `tf.keras` instead of the standalone version of Keras. There are
    a few minor differences between the two versions, such as compatibility with TensorFlow''s
    other modules and the way models are saved. For this reason, readers must make
    sure to use the correct version, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年起，TensorFlow已完全集成Keras，这意味着你只需安装TensorFlow即可使用它，无需额外安装其他内容。在本书中，我们将依赖`tf.keras`而不是Keras的独立版本。两者之间有一些小的差异，比如与TensorFlow其他模块的兼容性以及模型保存方式。因此，读者必须确保使用正确的版本，具体如下：
- en: In your code, import `tf.keras` and not `keras`.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的代码中，导入`tf.keras`而不是`keras`。
- en: Go through the `tf.keras` documentation on TensorFlow's website and not the *keras.io*
    documentation.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读TensorFlow网站上的`tf.keras`文档，而不是*keras.io*文档。
- en: When using external Keras libraries, make sure they are compatible with `tf.keras`.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用外部Keras库时，确保它们与`tf.keras`兼容。
- en: Some saved models might not be compatible between different versions of Keras.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些保存的模型可能不兼容不同版本的Keras。
- en: The two versions will continue to co-exist for the foreseeable future, and `tf.keras`
    will become more and more integrated with TensorFlow. To illustrate the power
    and simplicity of Keras, we will now use it to implement a simple neural network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个版本将会在可预见的未来继续共存，并且`tf.keras`将会越来越多地与TensorFlow集成。为了展示Keras的强大和简洁，我们现在将用它实现一个简单的神经网络。
- en: A simple computer vision model using Keras
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras构建的简单计算机视觉模型
- en: Before we delve into the core concepts of TensorFlow, let's start with a classical
    example of computer vision—digit recognition with the **Modified National Institute
    of Standards and Technology** (**MNIST**) dataset. The dataset was introduced
    in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer Vision and
    Neural Networks*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨TensorFlow的核心概念之前，让我们从一个经典的计算机视觉例子——数字识别，使用**美国国家标准与技术研究所**（**MNIST**）数据集开始。该数据集在[第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)中介绍，*计算机视觉与神经网络*。
- en: Preparing the data
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'First, we import the data. It is made up of 60,000 images for the training
    set and 10,000 images for the test set:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入数据。数据由60,000张图像组成作为训练集，10,000张图像作为测试集：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is common practice to import TensorFlow with the alias `tf` for faster reading
    and typing. It is also common to use `x` to denote input data, and `y` to represent
    labels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，通常使用别名`tf`进行导入，以便更快速地读取和输入。通常也会用`x`表示输入数据，用`y`表示标签。
- en: The `tf.keras.datasets` module provides quick access to download and instantiate
    a number of classical datasets. After importing the data using `load_data`, notice
    that we divide the array by `255.0` to get a number in the range [*0, 1*] instead
    of [*0, 255*]. It is common practice to normalize data, either in the [*0, 1*]
    range or in the [*-1, 1*] range.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.datasets`模块提供了快速访问、下载并实例化多个经典数据集的方法。在使用`load_data`导入数据后，注意我们将数组除以`255.0`，以将数据范围缩小到[*0,
    1*]而非[*0, 255*]。通常我们会对数据进行归一化处理，范围可以是[*0, 1*]或[*-1, 1*]。'
- en: Building the model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'We can now move on to building the actual model. We will use a very simple
    architecture composed of two **fully connected** (also called **dense**) layers.
    Before we explore the architecture, let''s have a look at the code. As you can
    see, Keras code is very concise:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始构建实际的模型了。我们将使用一个非常简单的架构，由两个**全连接**（也叫**密集**）层组成。在我们探索架构之前，先来看看代码。正如您所看到的，Keras代码非常简洁：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since our model is a linear stack of layers, we start by calling the `Sequential`
    function. We then add each layer one after the other. Our model is composed of
    two fully connected layers. We build it layer by layer:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型是一个线性堆叠的层次结构，我们从调用`Sequential`函数开始。然后逐个添加每一层。我们的模型由两个全连接层组成，我们逐层构建它：
- en: '**Flatten**: This will take the 2D matrix representing the image pixels and
    turn it into a 1D array. We need to do this before adding a fully connected layer.
    The *28* × *28* images are turned into a vector of size *784*.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flatten**：这个操作将代表图像像素的2D矩阵转换为1D数组。我们需要在添加全连接层之前进行此操作。*28* × *28* 的图像被转换为一个大小为
    *784* 的向量。'
- en: '**Dense** of size *128*: This will turn the *784* pixel values into 128 activations
    using a weight matrix of size *128* × *784* and a bias matrix of size *128*. In
    total, this means *100,480* parameters.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dense**大小为*128*：这个层将*784*个像素值转化为128个激活值，使用一个大小为*128* × *784*的权重矩阵和一个大小为*128*的偏置矩阵。总的来说，这意味着*100,480*个参数。'
- en: '**Dense** of size *10*: This will turn the *128* activations into our final
    prediction. Notice that because we want probabilities to sum to *1*, we will use
    the `softmax` activation function.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dense**大小为*10*：这个层将*128*个激活值转化为我们的最终预测。请注意，由于我们希望概率的总和为*1*，我们将使用`softmax`激活函数。'
- en: The `softmax` function takes the output of a layer and returns probabilities
    that sum up to `1`. It is the activation of choice for the last layer of a classification
    model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`softmax`函数接收一个层的输出，并返回使其总和为`1`的概率。这是分类模型最后一层的首选激活函数。'
- en: 'Note that you can get a description of the model, the outputs, and their weights
    using `model.summary()`. Here is the output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以通过`model.summary()`获取模型的描述、输出及其权重。以下是输出：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With its architecture set and weights initialized, the model is now ready to
    be trained for the chosen task.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已设置架构并初始化权重，可以开始训练以完成选择的任务。
- en: Training the model
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Keras makes training extremely simple:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使得训练变得异常简单：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Calling `.compile()` on the model we just created is a mandatory step. A few
    arguments must be specified:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 调用我们刚创建的模型的`.compile()`是一个必须的步骤。必须指定几个参数：
- en: '`optimizer`: This is the component that will perform the gradient descent.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：这是执行梯度下降的组件。'
- en: '`loss`: This is the metric we will optimize. In our case, we choose cross-entropy,
    just like in the previous chapter.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：这是我们要优化的度量。在我们的例子中，我们选择交叉熵，就像在上一章中一样。'
- en: '`metrics`: These are additional metric functions evaluated during training
    to provide further visibility of the model''s performance (unlike `loss`, they
    are not used in the optimization process).'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics`：这些是训练过程中评估的附加指标函数，用于进一步展示模型的性能（与`loss`不同，它们不用于优化过程）。'
- en: The Keras `loss` named `sparse_categorical_crossentropy` performs the same cross-entropy
    operation as `categorical_crossentropy`, but the former directly takes the ground
    truth labels as inputs, while the latter requires the ground truth labels to be
    *one-hot* encoded already before hand. Using the `sparse_...` loss thus saves
    us from manually having to transform the labels.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中名为`loss`的`sparse_categorical_crossentropy`执行与`categorical_crossentropy`相同的交叉熵操作，但前者直接将真实标签作为输入，而后者要求真实标签事先进行*one-hot*编码。因此，使用`sparse_...`损失函数可以避免我们手动转换标签。
- en: Passing `'sgd'` to Keras is equivalent to passing `tf.keras.optimizers.SGD()`.
    The former option is easier to read, while the latter makes it possible to specify
    parameters such as a custom learning rate. The same goes for the loss, metrics,
    and most arguments passed to Keras methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `'sgd'` 传递给 Keras 相当于传递 `tf.keras.optimizers.SGD()`。前者更易读，而后者则可以指定诸如自定义学习率等参数。损失函数、指标以及大多数传递给
    Keras 方法的参数也是如此。
- en: Then, we call the `.fit()` method. It is very similar to the interface used
    in **scikit-learn**, another popular machine learning library. We will train for
    five epochs, meaning that we will iterate over the whole train dataset five times.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用 `.fit()` 方法。它与 **scikit-learn** 这一流行的机器学习库使用的接口非常相似。我们将训练五个周期，这意味着我们会在整个训练数据集上迭代五次。
- en: 'Notice that we set `verbose` to `1`. This will allow us to get a progress bar
    with the metrics we chose earlier, the loss, and the **Estimated Time of Arrival**
    (**ETA**). The ETA is an estimate of the remaining time before the end of the
    epoch. Here is what the progress bar looks like:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将`verbose`设置为`1`。这将允许我们看到包含先前选择的指标、损失值以及**预计到达时间**（**ETA**）的进度条。ETA是对当前周期结束前剩余时间的估算。以下是进度条的显示效果：
- en: '![](img/59872f2a-0192-4997-98c9-8bb6fd4c1b1a.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59872f2a-0192-4997-98c9-8bb6fd4c1b1a.png)'
- en: 'Figure 2.2: Screenshot of the progress bar displayed by Keras in verbose mode'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：Keras 在 verbose 模式下显示的进度条截图
- en: Model performance
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能
- en: As described in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer
    Vision and Neural Networks*, you will notice that our model is overfitting—training
    accuracy is greater than test accuracy. If we train the model for five epochs,
    we end up with an accuracy of 97% on the test set. This is about 2% better than
    in the previous chapter, where we achieved 95%. State-of-the-art algorithms attain
    99.79% accuracy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)《计算机视觉与神经网络》所述，你会注意到我们的模型发生了过拟合——训练准确率高于测试准确率。如果我们训练模型五个周期，测试集上的准确率会达到97%。这比上一章的95%提高了大约2%。最先进的算法可以达到99.79%的准确率。
- en: 'We followed three main steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循了三个主要步骤：
- en: '**Loading the data**: In this case, the dataset was already available. During
    future projects, you may need additional steps to gather and clean the data.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载数据**：在这种情况下，数据集已经准备好了。在未来的项目中，可能需要额外的步骤来收集和清理数据。'
- en: '**Creating the model**: This step was made easy by using Keras—we defined the
    architecture of the model by adding sequential layers. Then, we selected a loss,
    an optimizer, and a metric to monitor.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建模型**：通过使用 Keras，这一步变得非常简单——我们通过添加顺序层来定义模型的架构。然后，我们选择了损失函数、优化器和监控指标。'
- en: '**Training the model**: Our model worked pretty well the first time. On more
    complex datasets, you will usually need to fine-tune parameters during training.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练模型**：我们的模型第一次运行得相当好。在更复杂的数据集上，通常需要在训练过程中对参数进行微调。'
- en: The whole process was extremely simple thanks to Keras, the high-level API of
    TensorFlow. Behind this simple API, the library hides a lot of the complexity.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有了 Keras 这个 TensorFlow 的高级 API，整个过程变得非常简单。在这个简单的 API 后面，库隐藏了很多复杂性。
- en: TensorFlow 2 and Keras in detail
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2 和 Keras 详解
- en: We have introduced the general architecture of TensorFlow and trained our first
    model using Keras. Let's now walk through the main concepts of TensorFlow 2\.
    We will explain several core concepts of TensorFlow that feature in this book,
    followed by some advanced notions. While we may not employ all of them in the
    remainder of the book, you might find it useful to understand some open source
    models that are available on GitHub or to get a deeper understanding of the library.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了 TensorFlow 的总体架构，并使用 Keras 训练了我们的第一个模型。现在，让我们逐步介绍 TensorFlow 2 的主要概念。我们将解释本书中涉及的
    TensorFlow 核心概念，并随后介绍一些高级概念。虽然在本书的后续部分我们可能不会使用所有这些概念，但理解它们可能对你了解一些在 GitHub 上可用的开源模型或深入理解该库有所帮助。
- en: Core concepts
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心概念
- en: Released in spring 2019, the new version of the framework is focused on simplicity
    and ease of use. In this section, we will introduce the concepts that TensorFlow
    relies on and cover how they evolved from version 1 to version 2.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 新版本的框架于 2019 年春季发布，重点是简化和易用性。在这一部分中，我们将介绍 TensorFlow 依赖的概念，并讲解它们从版本 1 到版本 2
    的演变。
- en: Introducing tensors
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍张量
- en: TensorFlow takes its name from a mathematical object called a **tensor**. You
    can imagine tensors as N-dimensional arrays. A tensor could be a scalar, a vector,
    a 3D matrix, or an N-dimensional matrix.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 得名于一种数学对象 **tensor**。你可以将张量想象为 N 维数组。一个张量可以是标量、向量、3D 矩阵或 N 维矩阵。
- en: A fundamental component of TensorFlow, the `Tensor` object is used to store
    mathematical values. It can contain fixed values (created using `tf.constant`)
    or changing values (created using `tf.Variable`).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的一个基础组件，`Tensor` 对象用于存储数学值。它可以包含固定值（使用 `tf.constant` 创建）或变化值（使用 `tf.Variable`
    创建）。
- en: In this book, *tensor* denotes the mathematical concept, while *Tensor* (with
    a capital *T*) corresponds to the TensorFlow object.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，*tensor* 指代数学概念，而 *Tensor*（大写 *T*）则对应 TensorFlow 对象。
- en: 'Each `Tensor` object has the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `Tensor` 对象具有以下内容：
- en: '**Type**: `string`, `float32`, `float16`, or `int8`, among others.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**：`string`、`float32`、`float16` 或 `int8` 等。'
- en: '**Shape**: The dimensions of the data. For instance, the shape would be `()`
    for a scalar, `(n)` for a vector of size *n*, and `(n, m)` for a 2D matrix of
    size *n* × *m*.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状**：数据的维度。例如，标量的形状为 `()`，大小为 *n* 的向量的形状为 `(n)`，大小为 *n* × *m* 的 2D 矩阵的形状为
    `(n, m)`。'
- en: '**Rank**: The number of dimensions, *0* for a scalar, `1` for a vector, and *2*
    for a 2D matrix.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**：维度的数量，*0* 表示标量，`1` 表示向量，*2* 表示 2D 矩阵。'
- en: Some tensors can have partially unknown shapes. For instance, a model accepting
    images of variable sizes could have an input shape of `(None, None, 3)`. Since
    the height and the width of the images are not known in advance, the first two
    dimensions are set to `None`. However, the number of channels (`3`, corresponding
    to red, blue, and green) is known and is therefore set.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些张量可能具有部分未知的形状。例如，一个接受可变大小图像的模型，其输入形状可能是 `(None, None, 3)`。由于图像的高度和宽度事先未知，因此前两个维度被设置为
    `None`。然而，通道数（`3`，对应红色、蓝色和绿色）是已知的，因此被设置为固定值。
- en: TensorFlow graphs
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 图
- en: TensorFlow uses tensors as inputs as well as outputs. A component that transforms
    input into output is called an **operation**. A computer vision model is therefore
    composed of multiple operations.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 使用张量作为输入和输出。将输入转化为输出的组件称为 **操作**。因此，一个计算机视觉模型是由多个操作组成的。
- en: TensorFlow represents these operations using a **directed acyclic graph** (**DAC**),
    also referred to as a **graph**. In TensorFlow 2, graph operations have disappeared
    under the hood to make the framework easier to use. Nevertheless, the graph concept
    remains important to understand how TensorFlow really works.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 使用 **有向无环图**（**DAC**）表示这些操作，也称为 **图**。在 TensorFlow 2 中，图操作已被隐藏，以便使框架更易于使用。然而，图的概念仍然是理解
    TensorFlow 工作原理的重要部分。
- en: 'When building the previous example using Keras, TensorFlow actually built a
    graph:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Keras 构建之前的示例时，TensorFlow 实际上构建了一个图：
- en: '![](img/12472b05-6c87-4dd4-a778-1c3344dd07f1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/12472b05-6c87-4dd4-a778-1c3344dd07f1.png)'
- en: Figure 2.3: A simplified graph corresponding to our model. In practice, each
    node is composed of smaller operations (such as matrix multiplications and additions)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：对应我们模型的简化图。在实践中，每个节点由更小的操作（如矩阵乘法和加法）组成。
- en: 'While very simple, this graph represents the different layers of our model
    in the form of operations. Relying on graphs has many advantages, allowing TensorFlow
    to do the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然非常简单，这个图以操作的形式表示了我们模型的不同层。依赖图有许多优势，使TensorFlow能够执行以下操作：
- en: Run part of the operations on the CPU and another part on the GPU
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU上运行一部分操作，在GPU上运行另一部分操作
- en: Run different parts of the graph on different machines in the case of a distributed
    model
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式模型的情况下，在不同的机器上运行图的不同部分
- en: Optimize the graph to avoid unnecessary operations, leading to better computational
    performance
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化图以避免不必要的操作，从而提高计算性能
- en: Moreover, the graph concept allows TensorFlow models to be portable. A single
    graph definition can be run on any kind of device.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图的概念使得TensorFlow模型具有可移植性。一个图的定义可以在任何设备上运行。
- en: In TensorFlow 2, graph creation is no longer handled by the user. While managing
    graphs used to be a complex task in TensorFlow 1, the new version greatly improves
    usability while still maintaining performance. In the next section, we will peek
    into the inner workings of TensorFlow and briefly explore how graphs are created.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2中，图的创建不再由用户处理。虽然在TensorFlow 1中管理图是一个复杂的任务，但新版本极大地提高了可用性，同时仍保持性能。在下一部分，我们将窥探TensorFlow的内部工作原理，简要探索图的创建过程。
- en: Comparing lazy execution to eager execution
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较延迟执行和急切执行
- en: The main change in TensorFlow 2 is **eager execution**. Historically, TensorFlow
    1 always used **lazy execution** by default. It is called *lazy* because operations
    are not run by the framework until asked specifically to do so.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2的主要变化是**急切执行**。历史上，TensorFlow 1默认总是使用**延迟执行**。它被称为*延迟*，因为在框架没有被明确要求之前，操作不会被执行。
- en: 'Let''s start with a very simple example to illustrate the difference between
    lazy and eager execution, summing the values of two vectors:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单的示例开始，来说明延迟执行和急切执行的区别，求和两个向量的值：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that `tf.add(a, b)` could be replaced by `a + b` since TensorFlow overloads
    many Python operators.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`tf.add(a, b)`可以被`a + b`替代，因为TensorFlow重载了许多Python运算符。
- en: 'The output of the previous code depends on the TensorFlow version. With TensorFlow
    1 (where lazy execution is the default mode), the output would be this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出取决于TensorFlow的版本。在TensorFlow 1中（默认模式为延迟执行），输出将是这样的：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'However, with TensorFlow 2 (where eager execution is the default mode), you
    would get the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在TensorFlow 2中（急切执行是默认模式），你将获得以下输出：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In both cases, the output is a Tensor. In the second case, the operation has
    been run eagerly and we can observe directly that the Tensor contains the result
    (`[1 2 4]`). In the first case, the Tensor contains information about the addition
    operation (`Add:0`), but not the result of the operation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种情况下，输出都是一个Tensor。在第二种情况下，操作已经被急切执行，我们可以直接观察到Tensor包含了结果（`[1 2 4]`）。而在第一种情况下，Tensor包含了关于加法操作的信息（`Add:0`），但没有操作的结果。
- en: In eager mode, you can access the value of a Tensor by calling the `.numpy()`
    method. In our example, calling `c.numpy()` returns `[1 2 4]` (as a NumPy array).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在急切模式下，你可以通过调用`.numpy()`方法来获取Tensor的值。在我们的示例中，调用`c.numpy()`会返回`[1 2 4]`（作为NumPy数组）。
- en: In TensorFlow 1, more code would be needed to compute the result, making the
    development process more complex. Eager execution makes code easier to debug (as
    developers can peak at the value of a Tensor at any time) and easier to develop.
    In the next section, we will detail the inner workings of TensorFlow and look
    at how it builds graphs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 1中，计算结果需要更多的代码，这使得开发过程更加复杂。急切执行使得代码更易于调试（因为开发者可以随时查看Tensor的值）并且更易于开发。在下一部分，我们将详细介绍TensorFlow的内部工作原理，并研究它是如何构建图的。
- en: Creating graphs in TensorFlow 2
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow 2中创建图
- en: 'We''ll start with a simple example to illustrate graph creation and optimization:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的示例开始，来说明图的创建和优化：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Assuming `a`, `b`, and `c` are Tensor matrices, this code computes two new values: `d` and
    `e`. Using eager execution, TensorFlow would compute the value for `d` and then
    compute the value for `e`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`a`、`b`和`c`是Tensor矩阵，这段代码计算两个新值：`d`和`e`。使用急切执行，TensorFlow将先计算`d`的值，然后计算`e`的值。
- en: Using lazy execution, TensorFlow would create a graph of operations. Before
    running the graph to get the result, a **graph optimizer** would be run. To avoid
    computing `a * b` twice, the optimizer would **cache** the result and reuse it
    when necessary. For more complex operations, the optimizer could enable **parallelism**
    to make computation faster. Both techniques are important when running large and
    complex models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用延迟执行，TensorFlow 会创建一个操作图。在运行图以获得结果之前，会运行一个**图形优化器**。为了避免重复计算 `a * b`，优化器会**缓存**结果，并在需要时重用它。对于更复杂的操作，优化器还可以启用**并行性**，以加快计算速度。这两种技术在运行大型和复杂模型时非常重要。
- en: As we saw, running in eager mode implies that every operation is run when defined.
    Therefore, such optimizations cannot be applied. Thankfully, TensorFlow includes
    a module to work around this—TensorFlow **AutoGraph**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，运行在 eager 模式下意味着每个操作在定义时都会执行。因此，无法应用这样的优化。幸运的是，TensorFlow 包括一个模块来绕过这一点——TensorFlow
    **AutoGraph**。
- en: Introducing TensorFlow AutoGraph and tf.function
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 TensorFlow AutoGraph 和 tf.function
- en: 'The TensorFlow AutoGraph module makes it easy to turn eager code into a graph,
    allowing automatic optimization. To do so, the easiest way is to add the `tf.function`
    decorator on top of your function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow AutoGraph 模块使得将 eager 代码转换为图形变得简单，从而实现自动优化。为了做到这一点，最简单的方法是在函数上方添加
    `tf.function` 装饰器：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: A **Python decorator** is a concept that allows functions to be wrapped, adding
    functionalities or altering them. Decorators start with an `@` (the "at" symbol).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python 装饰器**是一种概念，它允许函数被包装，添加功能或修改其行为。装饰器以`@`符号（"at"符号）开始。'
- en: 'When we call the `compute` function for the first time, TensorFlow will transparently
    create the following graph:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们第一次调用 `compute` 函数时，TensorFlow 会透明地创建以下图形：
- en: '![](img/da22bed4-9aa1-4c0d-b5f3-18addb45a933.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da22bed4-9aa1-4c0d-b5f3-18addb45a933.png)'
- en: 'Figure 2.4: The graph automatically generated by TensorFlow when calling the
    compute function for the first time'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：第一次调用 `compute` 函数时 TensorFlow 自动生成的图形
- en: 'TensorFlow AutoGraph can convert most Python statements, such as `for` loops,
    `while` loops, `if` statements, and iterations. Thanks to graph optimizations,
    graph execution can sometimes be faster than eager code. More generally, AutoGraph
    should be used in the following scenarios:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow AutoGraph 可以转换大多数 Python 语句，如 `for` 循环、`while` 循环、`if` 语句和迭代。由于图形优化，图形执行有时可能比
    eager 代码更快。更一般而言，AutoGraph 应该在以下场景中使用：
- en: When the model needs to be exported to other devices
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型需要导出到其他设备时
- en: When performance is paramount and graph optimizations can lead to speed improvements
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当性能至关重要且图形优化能够带来速度提升时
- en: Another advantage of graphs is their **automatic differentiation**. Knowing
    the full list of operations, TensorFlow can easily compute the gradient for each
    variable.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图形的另一个优势是其**自动求导**。知道所有操作的完整列表后，TensorFlow 可以轻松地计算每个变量的梯度。
- en: Note that in order to compute the gradient, the operations need to be **differentiable**.
    Some of them, such as `tf.math.argmax`, are not. Using them in a `loss` function
    will most likely cause the automatic differentiation to fail. It is up to the
    user to make sure that the loss is differentiable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了计算梯度，操作必须是**可微分**的。某些操作，如 `tf.math.argmax`，则不是。在 `loss` 函数中使用它们可能会导致自动求导失败。用户需要确保损失函数是可微分的。
- en: However, since, in eager mode, each operation is independent from one another,
    automatic differentiation is not possible by default. Thankfully, TensorFlow 2
    provides a way to perform automatic differentiation while still using eager mode—the
    **gradient tape**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于在 eager 模式下，每个操作都是相互独立的，因此默认情况下无法进行自动求导。幸运的是，TensorFlow 2 提供了一种方法，在仍使用
    eager 模式的情况下执行自动求导——**梯度带**。
- en: Backpropagating errors using the gradient tape
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度带回传误差
- en: The gradient tape allows easy backpropagation in eager mode. To illustrate this,
    we will use a simple example. Let's assume that we want to solve the equation
    *A* × *X = B*, where *A* and *B* are constants. We want to find the value of *X*
    to solve the equation. To do so, we will try to minimize a simple loss, *abs(A
    × X - B)*.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度带允许在 eager 模式下轻松进行反向传播。为了说明这一点，我们将使用一个简单的示例。假设我们想解方程 *A* × *X = B*，其中 *A*
    和 *B* 是常数。我们想找到 *X* 的值以解方程。为此，我们将尝试最小化一个简单的损失函数，*abs(A × X - B)*。
- en: 'In code, this translates to the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，这转换为以下内容：
- en: '[PRE9]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, to update the value of *X*, we would like to compute the gradient of the
    loss with respect to *X*. However, when printing the content of the loss, we obtain
    the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了更新*X*的值，我们希望计算损失函数关于*X*的梯度。然而，当打印损失的内容时，我们得到如下结果：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In eager mode, TensorFlow computed the result of the operation instead of storing
    the operation! With no information on the operation and its inputs, it would be
    impossible to automatically differentiate the `loss` operation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在eager模式下，TensorFlow计算了操作的结果，而不是存储操作！没有操作及其输入的信息，将无法自动微分`loss`操作。
- en: 'That is where the gradient tape comes in handy. By running our loss computation
    in the context of `tf.GradientTape`, TensorFlow will automatically record all
    operations and allow us to replay them backward afterward:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，梯度带就派上用场了。通过在`tf.GradientTape`上下文中运行我们的损失计算，TensorFlow将自动记录所有操作，并允许我们在之后回放它们：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The previous code defines a single training step. Every time `train_step` is
    called, the loss is computed in the context of the gradient tape. The context
    is then used to compute the gradient. The *X* variable is then updated. Indeed,
    we can see *X* converging toward the value that solves the equation:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码定义了一个训练步骤。每次调用`train_step`时，损失都会在梯度带的上下文中计算。然后，使用该上下文来计算梯度。*X*变量随后会被更新。事实上，我们可以看到*X*逐渐逼近解决方程的值：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You will notice that in the very first example of this chapter, we did not make
    use of the gradient tape. This is because Keras models encapsulate training inside
    the `.fit()` function—there's no need to update the variables manually. Nevertheless,
    for innovative models or when experimenting, the gradient tape is a powerful tool
    that allows automatic differentiation without much effort. Readers can find a
    more practical use of the gradient tape in the regularization notebook of [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在本章的第一个示例中，我们并没有使用梯度带。这是因为Keras模型将训练封装在`.fit()`函数中——不需要手动更新变量。然而，对于创新性模型或实验时，梯度带是一个强大的工具，它允许我们在几乎不费力的情况下进行自动微分。读者可以在[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)的正则化笔记本中找到梯度带的更实际应用，*现代神经网络*。
- en: Keras models and layers
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras模型和层
- en: 'In the first section of this chapter, we built a simple Keras Sequential model.
    The resulting `Model` object contains numerous useful methods and properties:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们构建了一个简单的Keras Sequential模型。生成的`Model`对象包含了许多有用的方法和属性：
- en: '`.inputs` and `.outputs`: Provide access to the inputs and outputs of the model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.inputs`和`.outputs`：提供对模型输入和输出的访问。'
- en: '`.layers`: Lists the model''s layers as well as their shape.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.layers`：列出模型的所有层以及它们的形状。'
- en: '`.summary()`: Prints the architecture of the model.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.summary()`：打印模型的架构。'
- en: '`.save()`: Saves the model, its architecture, and the current state of training.
    It is very useful for resuming training later on. Models can be instantiated from
    a file using `tf.keras.models.load_model()`.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.save()`：保存模型、其架构和当前的训练状态。对于稍后恢复训练非常有用。可以使用`tf.keras.models.load_model()`从文件中实例化模型。'
- en: '`.save_weights()`: Only saves the weights of the model.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.save_weights()`：仅保存模型的权重。'
- en: While there is only one type of Keras model object, they can be built in a variety
    of ways.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Keras模型对象只有一种类型，但可以通过多种方式构建它们。
- en: Sequential and functional APIs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sequential和函数式API
- en: 'Instead of employing the Sequential API, like at the beginning of this chapter,
    you can instead use the functional API:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用函数式API来代替本章开始时使用的Sequential API：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Notice that the code is slightly longer than it previously was. Nevertheless,
    the functional API is much more versatile and expressive than the Sequential API.
    The former allows for branching models (that is, for building architectures with
    multiple parallel layers for instance), while the latter can only be used for
    linear models. For even more flexibility, Keras also offers the possibility to
    subclass the `Model` class, as described in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml),
    *Modern Neural Networks*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到代码比之前稍长了一些。然而，函数式API比Sequential API更加灵活和富有表现力。前者允许构建分支模型（即构建具有多个并行层的架构），而后者只能用于线性模型。为了更大的灵活性，Keras还提供了子类化`Model`类的可能性，如[第3章](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)中所述，*现代神经网络*。
- en: Regardless of how a `Model` object is built, it is composed of layers. A layer
    can be seen as a node that accepts one or several inputs and returns one or several
    outputs, similar to a TensorFlow operation. Its weights can be accessed using
    `.get_weights()` and set using `.set_weights()`. Keras provides pre-made layers
    for the most common deep learning operations. For more innovative or complex models,
    `tf.keras.layers.Layer` can also be subclassed.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 无论`Model`对象是如何构建的，它都是由层组成的。一个层可以看作是一个节点，接受一个或多个输入并返回一个或多个输出，类似于TensorFlow操作。它的权重可以通过`.get_weights()`访问，并通过`.set_weights()`设置。Keras提供了用于最常见深度学习操作的预制层。对于更具创新性或复杂的模型，`tf.keras.layers.Layer`也可以被子类化。
- en: Callbacks
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回调函数
- en: '**Keras callbacks** are utility functions that you can pass to a Keras model''s
    `.fit()` method to add functionality to its default behavior. Multiple callbacks
    can be defined, which will be called by Keras either before or after each batch
    iteration, each epoch, or the whole training procedure. Predefined Keras callbacks
    include the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras回调函数**是一些实用函数，可以传递给Keras模型的`.fit()`方法，以增加其默认行为的功能。可以定义多个回调函数，Keras会在每个批次迭代、每个epoch或整个训练过程中，在回调函数之前或之后调用它们。预定义的Keras回调函数包括以下内容：'
- en: '`CSVLogger`: Logs training information in a CSV file.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CSVLogger`：将训练信息记录到CSV文件中。'
- en: '`EarlyStopping`: Stops training if the loss or a metric stops improving. It
    can be useful in avoiding overfitting.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EarlyStopping`：如果损失或某个度量停止改善，它会停止训练。它可以在避免过拟合方面发挥作用。'
- en: '`LearningRateScheduler`: Changes the learning rate on each epoch according
    to a schedule.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LearningRateScheduler`：根据计划在每个epoch改变学习率。'
- en: '`ReduceLROnPlateau`: Automatically reduces the learning rate when the loss
    or a metric stops improving.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ReduceLROnPlateau`：当损失或某个度量停止改善时，自动降低学习率。'
- en: It is also possible to create custom callbacks by subclassing `tf.keras.callbacks.Callback`,
    as demonstrated in later chapters and their code samples.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以通过子类化`tf.keras.callbacks.Callback`来创建自定义回调函数，如后续章节和代码示例中所示。
- en: Advanced concepts
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级概念
- en: In summary, the AutoGraph module, the `tf.function` decorator, and the gradient
    tape context make graph creation and management very simple—if not invisible.
    However, a lot of the complexity is hidden from the user. In this section, we
    will explore the inner workings of these modules.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，AutoGraph模块、`tf.function`装饰器和梯度带上下文使得图的创建和管理变得非常简单——如果不是不可见的话。然而，很多复杂性对用户来说是隐藏的。在这一部分中，我们将探索这些模块的内部工作原理。
- en: This section presents advanced concepts that are not required throughout the
    book, but it may be useful for you to understand more complex TensorFlow code.
    More impatient readers can skip this part and come back to it later.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一些高级概念，这些概念在全书中并不需要，但它们可能有助于你理解更复杂的TensorFlow代码。更急于学习的读者可以跳过这一部分，稍后再回来查看。
- en: How tf.function works
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.function的工作原理
- en: As mentioned earlier, when calling a function decorated with `tf.function` for
    the first time, TensorFlow will create a graph corresponding to the function's
    operations. TensorFlow will then cache the graph so that the next time the function
    is called, graph creation will not be necessary.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当第一次调用用`tf.function`装饰的函数时，TensorFlow将创建一个对应函数操作的图。TensorFlow会缓存这个图，以便下次调用该函数时无需重新创建图。
- en: 'To illustrate this, let''s create a simple `identity` function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们创建一个简单的`identity`函数：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This function will print a message every time TensorFlow creates a graph corresponding
    to its operation. In this case, since TensorFlow is caching the graph, it will
    print something only the first time it is run:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将在TensorFlow每次创建与其操作对应的图时打印一条消息。在这种情况下，由于TensorFlow缓存了图，它只会在第一次运行时打印一些信息：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'However, note that if we change the input type, TensorFlow will recreate a
    graph:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，如果我们更改输入类型，TensorFlow将重新创建图：
- en: '[PRE16]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This behavior is explained by the fact that TensorFlow graphs are defined by
    their operations and the shapes and types of the tensors they receive as inputs.
    Therefore, when the input type changes, a new graph needs to be created. In TensorFlow
    vocabulary, when a `tf.function` function has defined input types, it becomes
    a **concrete function**.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行为的解释在于，TensorFlow图是通过它们的操作以及它们接收的输入张量的形状和类型来定义的。因此，当输入类型发生变化时，需要创建一个新的图。在TensorFlow术语中，当`tf.function`函数定义了输入类型时，它就变成了**具体函数**。
- en: To summarize, every time a decorated function is run for the first time, TensorFlow
    caches the graph corresponding to the input types and input shapes. If the function
    is run with inputs of a different type, TensorFlow will create a new graph and
    cache it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，每次一个装饰过的函数首次运行时，TensorFlow会缓存与输入类型和输入形状对应的图。如果函数使用不同类型的输入运行，TensorFlow将创建一个新的图并进行缓存。
- en: 'Nevertheless, it might be useful to log information every time a concrete function
    is run and not just the first time. To do so, use `tf.print`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，每次执行具体函数时记录信息可能会很有用，而不仅仅是第一次执行时。为此，可以使用`tf.print`：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Instead of printing information only the first time, this function will print
    `Running identity` every single time it is run.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将不会仅仅在第一次打印信息，而是每次运行时都会打印`Running identity`。
- en: Variables in TensorFlow 2
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2中的变量
- en: 'To hold the model weights, TensorFlow uses `Variable` instances. In our Keras
    example, we can list the content of the model by accessing `model.variables`.
    It will return the list of all variables contained in our model:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow使用`Variable`实例来存储模型权重。在我们的Keras示例中，我们可以通过访问`model.variables`列出模型的内容。它将返回模型中包含的所有变量的列表：
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In our example, variable management (including naming) has been entirely handled
    by Keras. As we saw earlier, it is also possible to create our own variables:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，变量管理（包括命名）完全由Keras处理。如前所述，我们也可以创建自己的变量：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that for large projects, it is recommended to name variables to clarify
    the code and ease debugging. To change the value of a variable, use the `Variable.assign`
    method:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于大型项目，建议为变量命名以明确代码的含义并简化调试。要更改变量的值，可以使用`Variable.assign`方法：
- en: '[PRE20]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Failing to use the `.assign()` method would create a new `Tensor` method:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不使用`.assign()`方法，将会创建一个新的`Tensor`方法：
- en: '[PRE21]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Finally, deleting the Python reference to a variable will remove the object
    itself from the active memory, releasing space for other variables to be created.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，删除Python对变量的引用将会把该对象从活动内存中移除，为其他变量创建腾出空间。
- en: Distribution strategies
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式策略
- en: We trained a simple model on a very small dataset. When using larger models
    and datasets, more computing power is necessary—this often implies multiple servers.
    The `tf.distribute.Strategy` API defines how multiple machines communicate together
    to train a model efficiently.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个非常小的数据集上训练了一个简单的模型。在使用更大的模型和数据集时，需要更多的计算能力——这通常意味着需要多个服务器。`tf.distribute.Strategy`
    API定义了多个机器如何协同工作以高效训练模型。
- en: 'Some of the strategies defined by TensorFlow are as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow定义的一些策略如下：
- en: '`MirroredStrategy`: For training on multiple GPUs on a single machine. Model
    weights are kept in sync between each device.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MirroredStrategy`：用于在单台机器上训练多个GPU。模型的权重会在各个设备之间保持同步。'
- en: '`MultiWorkerMirroredStrategy`: Similar to `MirroredStategy`, but for training
    on multiple machines.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiWorkerMirroredStrategy`：类似于`MirroredStrategy`，但用于在多台机器上训练。'
- en: '`ParameterServerStrategy`: For training on multiple machines. Instead of syncing
    the weights on each device, they are kept on a parameter server.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ParameterServerStrategy`：用于在多台机器上训练。不同于在每个设备上同步权重，权重会保存在参数服务器上。'
- en: '`TPUStrategy`: For training on Google''s **Tensor Processing Unit** (**TPU**)
    chip.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TPUStrategy`：用于在Google的**张量处理单元**（**TPU**）芯片上进行训练。'
- en: The TPU is a custom chip made by Google, similar to a GPU, designed specifically
    to run neural network computations. It is available through Google Cloud.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: TPU是Google定制的芯片，类似于GPU，专门设计用于运行神经网络计算。它可以通过Google Cloud访问。
- en: 'To use a distribution strategy, create and compile your model in its scope:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用分布式策略，在其作用域内创建并编译模型：
- en: '[PRE22]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that you will probably have to increase the batch size, as each device
    will now receive a small subset of each batch. Depending on your model, you may
    also have to change the learning rate.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你可能需要增加批处理大小，因为每个设备现在只会接收每个批次的小部分数据。根据你的模型，你可能还需要调整学习率。
- en: Using the Estimator API
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Estimator API
- en: We saw in the first part of this chapter that the Estimator API is a high-level
    alternative to the Keras API. Estimators simplify training, evaluation, prediction,
    and serving.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的第一部分看到，Estimator API是Keras API的高级替代方法。Estimator简化了训练、评估、预测和服务过程。
- en: There are two types of Estimators. Pre-made Estimators are very simple models
    provided by TensorFlow, allowing you to quickly try out machine learning architectures.
    The second type is custom Estimators, which can be created using any model architecture.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器有两种类型。预制估计器是由TensorFlow提供的非常简单的模型，允许你快速尝试机器学习架构。第二种类型是自定义估计器，可以使用任何模型架构创建。
- en: Estimators handle all the small details of a model's life cycle—data queues,
    exception handling, recovering from failure, periodic checkpoints, and many more.
    While using Estimators was considered best practice in TensorFlow 1, in version
    2, it is recommended to use the Keras API.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 估计器处理模型生命周期的所有小细节——数据队列、异常处理、从失败中恢复、周期性检查点等。在TensorFlow 1中，使用估计器被认为是最佳实践，而在版本2中，建议使用Keras
    API。
- en: Available pre-made Estimators
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用的预制估计器
- en: At the time of writing, the available pre-made Estimators are `DNNClassifier`,
    `DNNRegressor`, `LinearClassifier`, and `LinearRegressor`. Here, DNN stands for
    **deep neural network**. Combined Estimators based on both architectures are also
    available—`DNNLinearCombinedClassifier` and `DNNLinearCombinedRegressor`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，现有的预制估计器有`DNNClassifier`、`DNNRegressor`、`LinearClassifier`和`LinearRegressor`。其中，DNN代表**深度神经网络**。还提供了基于两种架构的组合估计器——`DNNLinearCombinedClassifier`和`DNNLinearCombinedRegressor`。
- en: In machine learning, classification is the process of predicting a discrete
    category, while regression is the process of predicting a continuous number.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，分类是预测离散类别的过程，而回归是预测连续数字的过程。
- en: '**Combined Estimators**, also called **deep-n-wide models**, make use of a
    linear model (for memorization) and a deep model (for generalization). They are
    mostly used for recommendation or ranking models.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**组合估计器**，也称为**深度宽度模型**，利用线性模型（用于记忆）和深度模型（用于泛化）。它们主要用于推荐或排序模型。'
- en: Pre-made Estimators are suitable for some machine learning problems. However,
    they are not suitable for computer vision problems, as there are no pre-made Estimators
    with convolutions, a powerful type of layer described in the next chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 预制估计器适用于一些机器学习问题。然而，它们不适用于计算机视觉问题，因为没有带有卷积的预制估计器，卷积是一种强大的层类型，将在下一章中描述。
- en: Training a custom Estimator
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练自定义估计器
- en: 'The easiest way to create an Estimator is to convert a Keras model. After the
    model has been compiled, call `tf.keras.estimator.model_to_estimator()`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 创建估计器的最简单方法是转换Keras模型。在模型编译后，调用`tf.keras.estimator.model_to_estimator()`：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `model_dir` argument allows you to specify a location where the checkpoints
    of the model will be saved. As mentioned earlier, Estimators will automatically
    save checkpoints for our models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`model_dir`参数允许你指定一个位置，在该位置保存模型的检查点。如前所述，估计器将自动保存我们的模型检查点。'
- en: 'Training an Estimator requires the use of an **input function**—a function
    that returns data in a specific format. One of the accepted formats is a TensorFlow
    dataset. The dataset API is described in depth in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*. For now, we''ll define the following
    function, which returns the dataset defined in the first part of this chapter
    in the correct format, in batches of *32* samples:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练估计器需要使用**输入函数**——一个返回特定格式数据的函数。接受的格式之一是TensorFlow数据集。数据集API在[第7章](337ec077-c215-4782-b56c-beae4d94d718.xhtml)中有详细描述，*复杂和稀缺数据集的训练*。现在，我们将定义以下函数，该函数以正确的格式批量返回本章第一部分定义的数据集，每批包含*32*个样本：
- en: '[PRE24]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once this function is defined, we can launch the training with the Estimator:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了这个函数，我们可以启动训练与估计器：
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Just like Keras, the training part is very simple, as Estimators handle the
    heavy lifting.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Keras一样，训练部分非常简单，因为估计器处理了繁重的工作。
- en: The TensorFlow ecosystem
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow生态系统
- en: In addition to the main library, TensorFlow offers numerous tools that are useful
    for machine learning. While some of them are shipped with TensorFlow, others are
    grouped under **TensorFlow Extended** (**TFX**) and **TensorFlow Addons**. We
    will now introduce the most commonly used tools.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 除了主要库外，TensorFlow还提供了许多对机器学习有用的工具。虽然其中一些工具随TensorFlow一起提供，但其他工具被归类在**TensorFlow扩展**（**TFX**）和**TensorFlow附加组件**（**TensorFlow
    Addons**）下。我们将介绍最常用的工具。
- en: TensorBoard
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard
- en: 'While the progress bar we used in the first example of this chapter displayed
    useful information, we might want to access more detailed graphs. TensorFlow provides
    a powerful tool for monitoring—**TensorBoard**. Installed by default with TensorFlow,
    it is also very easy to use when combined with Keras''s callbacks:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在本章第一个示例中使用的进度条提供了有用的信息，但我们可能希望访问更详细的图表。TensorFlow 提供了一个强大的监控工具——**TensorBoard**。它在安装
    TensorFlow 时默认包含，并且与 Keras 的回调函数结合使用时非常简单：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In this updated code, we pass the TensorBoard callback to the `model.fit()`
    method. By default, TensorFlow will automatically write the loss and the metrics
    to the folder we specified. We can then launch TensorBoard from the command line:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段更新后的代码中，我们将 TensorBoard 回调传递给 `model.fit()` 方法。默认情况下，TensorFlow 会自动将损失值和指标写入我们指定的文件夹中。然后，我们可以从命令行启动
    TensorBoard：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This command outputs a URL that we can then open to display the TensorBoard
    interface. In the Scalars tab, we can find graphs displaying the loss and the
    accuracy:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令会输出一个 URL，我们可以打开它来显示 TensorBoard 界面。在 Scalars 选项卡中，我们可以找到显示损失值和准确率的图表：
- en: '![](img/49b87d26-016f-4802-ae6b-13175e0ce6f3.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49b87d26-016f-4802-ae6b-13175e0ce6f3.png)'
- en: 'Figure 2.5: Two graphs displayed by TensorBoard during training'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：训练期间由 TensorBoard 显示的两个图表
- en: 'As you will see in this book, training a deep learning model requires a lot
    of fine-tuning. Therefore, it is essential to monitor how your model is performing.
    TensorBoard allows you to do precisely this. The most common use case is to monitor
    the evolution of the loss of your model over time. But you can also do the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在本书中将看到的，训练一个深度学习模型需要大量的微调。因此，监控模型的表现至关重要。TensorBoard 让你可以精确地完成这项任务。最常见的使用场景是监控模型损失的变化过程。但你还可以执行以下操作：
- en: Plot any metric (such as accuracy)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制任何指标（如准确率）
- en: Display input and output images
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示输入和输出图像
- en: Display the execution time
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示执行时间
- en: Draw your model's graph representation
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绘制模型的图形表示
- en: 'TensorBoard is very versatile, and there are many ways to use it. Each piece
    of information is stored in `tf.summary`—this can be scalars, images, histograms,
    or text. For instance, to log a scalar you might first create a summary writer
    and log information using the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 非常灵活，有许多使用方法。每条信息都存储在 `tf.summary` 中——这可以是标量、图像、直方图或文本。例如，要记录标量，你可以先创建一个摘要写入器，然后使用以下方法记录信息：
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding code, we specify the step—it could be the epoch number, the
    batch number, or custom information. It will correspond to the *x *axis in TensorBoard
    figures. TensorFlow also provides tools for generating aggregates. To manually
    log accuracy, you could use the following:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们指定了步骤——它可以是 epoch 数、batch 数量或自定义信息。它将对应于 TensorBoard 图表中的 *x* 轴。TensorFlow
    还提供了生成汇总的工具。为了手动记录准确率，你可以使用以下方法：
- en: '[PRE29]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Other metrics are available, such as `Mean`, `Recall`, and `TruePositives`.
    While setting up the logging of metrics in TensorBoard may seem a bit complicated
    and time-consuming, it is an essential part of the TensorFlow toolkit. It will
    save you countless hours of debugging and manual logging.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可用的指标，例如 `Mean`、`Recall` 和 `TruePositives`。虽然在 TensorBoard 中设置指标的日志记录可能看起来有些复杂且耗时，但它是
    TensorFlow 工具包中的一个重要部分。它将节省你无数小时的调试和手动日志记录工作。
- en: TensorFlow Addons and TensorFlow Extended
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 附加组件和 TensorFlow 扩展
- en: '**TensorFlow Addons** is a collection of extra functionalities gathered into
    a single repository ([https://github.com/tensorflow/addons](https://github.com/tensorflow/addons)).
    It hosts some of the newer advancements in deep learning that are too unstable
    or not used by enough people to justify adding them to the main TensorFlow library.
    It also acts as a replacement for `tf.contrib`, which was removed from TensorFlow
    1.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 附加组件**是一个将额外功能收集到单一库中的集合 ([https://github.com/tensorflow/addons](https://github.com/tensorflow/addons))。它包含一些较新的深度学习进展，这些进展由于不够稳定或使用人数不足，无法被加入到
    TensorFlow 主库中。它也作为 `tf.contrib` 的替代品，后者已从 TensorFlow 1 中移除。'
- en: '**TensorFlow Extended** is an end-to-end machine learning platform for TensorFlow.
    It offers several useful tools:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 扩展**是一个为 TensorFlow 提供端到端机器学习平台的工具。它提供了几个有用的工具：'
- en: '**TensorFlow Data Validation**: A library for exploring and validating machine
    learning data. You can use it before even building your model.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 数据验证**：用于探索和验证机器学习数据的库。你可以在构建模型之前使用它。'
- en: '**TensorFlow Transform**: A library for preprocessing data. It allows you to
    make sure training and evaluation data are processed the same way.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Transform**：一个用于数据预处理的库。它确保训练数据和评估数据的处理方式一致。'
- en: '**TensorFlow Model Analysis**: A library for evaluating TensorFlow models.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow模型分析**：用于评估TensorFlow模型的库。'
- en: '**TensorFlow Serving**: A serving system for machine learning models. Serving
    is the process of delivering predictions from a model, usually through a REST
    API:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Serving**：一个用于机器学习模型的服务系统。服务是通过REST API从模型提供预测的过程：'
- en: '![](img/009f5850-47f5-4c19-af52-9861f8716370.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/009f5850-47f5-4c19-af52-9861f8716370.png)'
- en: 'Figure 2.6: End-to-end process of creating and using a deep learning model'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：创建和使用深度学习模型的端到端过程
- en: As seen in *Figure 2.6*, these tools fulfill the goal of being end to end, covering
    every step of the process of building and using a deep learning model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图2.6*所示，这些工具实现了端到端的目标，涵盖了构建和使用深度学习模型的每个步骤。
- en: TensorFlow Lite and TensorFlow.js
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Lite和TensorFlow.js
- en: The main version of TensorFlow is designed for Windows, Linux, and Mac computers.
    To operate on other devices, a different version of TensorFlow is necessary. **TensorFlow
    Lite** is designed to run model predictions (inference) on mobile phones and embedded
    devices. It is composed of a converter transforming TensorFlow models to the required
    `.tflite` format and an interpreter that can be installed on mobile devices to
    run inferences.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的主要版本设计用于Windows、Linux和Mac计算机。要在其他设备上运行，则需要不同版本的TensorFlow。**TensorFlow
    Lite**旨在在手机和嵌入式设备上运行模型预测（推断）。它包括一个转换器，将TensorFlow模型转换为所需的`.tflite`格式，并包含一个可以安装在移动设备上进行推断的解释器。
- en: More recently, **TensorFlow.js** (also referred to as **tfjs**) was developed
    to empower almost any web browser with deep learning. It does not require any
    installation from the user and can sometimes make use of the device's GPU acceleration.
    We detail the use of TensorFlow Lite and TensorFlow.js in [Chapter 9](e8935e55-c3b5-419e-a86a-43eba3ff4dad.xhtml),
    *Optimizing Models and Deploying on Mobile Devices*.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，开发了**TensorFlow.js**（也称为**tfjs**），它使几乎任何Web浏览器都能进行深度学习。它不需要用户安装，且有时可以利用设备的GPU加速。我们在[第9章](e8935e55-c3b5-419e-a86a-43eba3ff4dad.xhtml)《优化模型并在移动设备上部署》中详细介绍了TensorFlow
    Lite和TensorFlow.js的使用，*优化模型并部署到移动设备上*。
- en: Where to run your model
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型运行的位置
- en: As computer vision models process large amounts of data, they take a long time
    to train. Because of this, training on your local computer can take a considerable
    amount of time. You will also notice that creating efficient models requires a
    lot of iterations. Those two insights will drive your decision regarding where
    to train and run your models. In this section, we will compare the different options
    available to train and use your model.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算机视觉模型处理大量数据，因此训练需要较长时间。因此，在本地计算机上训练可能会花费相当多的时间。你还会发现，创建高效的模型需要很多次迭代。这两个观点将影响你决定在哪里训练和运行模型。在本节中，我们将比较不同的训练和使用模型的选项。
- en: On a local machine
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地机器上
- en: Coding your model on your computer is often the fastest way to get started.
    As you have access to a familiar environment, you can easily change your code
    as often as needed. However, personal computers, especially laptops, lack the
    computing power to train a computer vision model. Training on a GPU may be between
    10 and 100 times faster than using a CPU. This is why it is recommended to use
    a GPU.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的计算机上编码模型通常是开始的最快方式。由于你可以使用熟悉的环境，你可以根据需要轻松修改代码。然而，个人计算机，尤其是笔记本电脑，缺乏训练计算机视觉模型的计算能力。使用GPU进行训练的速度可能比使用CPU快10到100倍。这就是为什么建议使用GPU的原因。
- en: Even if your computer has a GPU, only very specific models can run TensorFlow.
    Your GPU must be compatible with CUDA, NVIDIA's computing library. At the time
    of writing, the latest version of TensorFlow requires a CUDA compute capability
    of 3.5 or higher.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的计算机拥有GPU，也只有非常特定的模型可以运行TensorFlow。你的GPU必须与CUDA兼容，CUDA是NVIDIA的计算库。撰写本文时，TensorFlow的最新版本要求CUDA计算能力为3.5或更高。
- en: Some laptops are compatible with external GPU enclosures, but this defeats the
    purpose of a portable computer. Instead, a practical way is to run your model
    on a remote computer that has a GPU.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一些笔记本电脑支持外接GPU机箱，但这违背了便携电脑的初衷。一个更实际的方法是将你的模型运行在一台远程计算机上，该计算机拥有GPU。
- en: On a remote machine
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在远程机器上
- en: Nowadays, you can rent powerful machines with GPUs by the hour. Pricing varies,
    depending on the GPU power and the provider. It usually costs around $1 per hour
    for a single GPU machine, with the price going down every day. If you commit to
    renting the machine for the month, you can get good computing power for around
    $100 per month. Considering the time you will save waiting for the model to train,
    it often makes economic sense to rent a remote machine.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，你可以按小时租用带 GPU 的强大计算机。定价因 GPU 性能和供应商而异。单个 GPU 机器的费用通常为每小时约 $1，且价格每日有所下降。如果你承诺租用整个月份的机器，每月大约
    $100 就可以获得良好的计算性能。考虑到你在训练模型时节省的时间，租用远程机器通常从经济角度来说是明智的选择。
- en: Another option is to build your own deep learning server. Note that this requires
    investment and assembly, and that GPUs consume large amounts of electricity.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是搭建你自己的深度学习服务器。请注意，这需要投资和组装，并且 GPU 消耗大量电力。
- en: 'Once you have secured access to a remote machine, you have two options:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你获得了远程机器的访问权限，你有两个选择：
- en: Run Jupyter Notebook on the remote server. Jupyter Lab or Jupyter Notebook will
    then be accessible using your browser, anywhere on the planet. It is a very convenient
    way of performing deep learning.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在远程服务器上运行 Jupyter Notebook。然后，你可以通过浏览器在全球任何地方访问 Jupyter Lab 或 Jupyter Notebook。这是一种非常方便的深度学习执行方式。
- en: Sync your local development folder and run your code remotely. Most IDEs have
    a feature to sync your local code with a remote server. This allows you to code
    in your favorite IDE while still enjoying a powerful machine.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步本地开发文件夹并远程运行代码。大多数 IDE 都有将本地代码与远程服务器同步的功能。这使得你可以在自己喜欢的 IDE 中编程，同时仍然享受强大的计算机性能。
- en: Google Colab, based on Jupyter notebooks, allows you to run notebooks in the
    cloud for *free*. You can even enable GPU mode. Colab has limited storage space
    and a limit of 8 hours of consecutive running time. While it is the perfect tool
    for getting started or experimenting, it is not convenient for larger models.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Jupyter 笔记本的 Google Colab 允许你在云端运行笔记本，*免费*使用。你甚至可以启用 GPU 模式。Colab 有存储空间限制，并且连续运行时间限制为
    8 小时。虽然它是一个非常适合入门或实验的工具，但对于更大的模型来说并不方便。
- en: On Google Cloud
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Google Cloud 上
- en: To run TensorFlow on a remote machine, you will need to manage it yourself—installing
    the correct software, making sure it is up to date, and turning the server on
    and off. While it is still possible to do so for one machine, and you sometimes
    need to distribute the training among numerous GPUs, using Google Cloud ML to
    run TensorFlow allows you to focus on your model and not on operations.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要在远程机器上运行 TensorFlow，你需要自己管理——安装正确的软件，确保其更新，并开启和关闭服务器。虽然对于一台机器来说仍然可以这样做，有时你还需要将训练分配到多个
    GPU 上，但使用 Google Cloud ML 来运行 TensorFlow 可以让你专注于模型而不是操作。
- en: 'You will find that Google Cloud ML is useful for the following:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现 Google Cloud ML 对以下内容非常有用：
- en: Training your model quickly thanks to elastic resources in the cloud
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过云中的弹性资源快速训练模型
- en: Looking for the best model parameters in the shortest amount of time possible
    using parallelization
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用并行化方法在最短时间内找到最佳模型参数
- en: Once your model is ready, serving predictions without having to run your own
    prediction server
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型准备好，你可以在无需自己运行预测服务器的情况下提供预测
- en: All the details for packaging, sending, and running your model are available
    in the Google Cloud ML documentation ([https://cloud.google.com/ml-engine/docs/](https://cloud.google.com/ml-engine/docs/)).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 有关打包、发送和运行模型的所有细节都可以在 Google Cloud ML 文档中找到（[https://cloud.google.com/ml-engine/docs/](https://cloud.google.com/ml-engine/docs/)）。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by training a basic computer vision model using
    the Keras API. We introduced the main concepts behind TensorFlow 2—tensors, graphs,
    AutoGraph, eager execution, and the gradient tape. We also detailed some of the
    more advanced concepts of the framework. We went through the main tools surrounding
    the use of deep learning with the library, from TensorBoard for monitoring, to
    TFX for preprocessing and model analysis. Finally, we covered where to run your
    model depending on your needs.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先使用 Keras API 训练了一个基本的计算机视觉模型。我们介绍了 TensorFlow 2 背后的主要概念——张量、图、AutoGraph、即时执行和梯度带。我们还详细介绍了框架中的一些更高级的概念。我们通过库周围的主要工具进行了讲解，从用于监控的
    TensorBoard 到用于预处理和模型分析的 TFX。最后，我们讨论了根据需求选择运行模型的位置。
- en: With these powerful tools in hand, you are now ready to discover modern computer
    vision models in the next chapter.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些强大工具后，你现在准备好在下一章探索现代计算机视觉模型了。
- en: Questions
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is Keras in relation to TensorFlow, and what is its purpose?
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 在 TensorFlow 中的作用是什么，它的目的是什么？
- en: Why does TensorFlow use graphs, and how do you create them manually?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 TensorFlow 使用图形？如何手动创建图形？
- en: What is the difference between eager execution mode and lazy execution mode?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 急切执行模式和懒惰执行模式有什么区别？
- en: How do you log information in TensorBoard, and how do you display it?
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在 TensorBoard 中记录信息，并如何显示它？
- en: What are the main differences between TensorFlow 1 and TensorFlow 2?
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow 1 和 TensorFlow 2 之间的主要区别是什么？
