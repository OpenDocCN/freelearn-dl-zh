- en: Attention Mechanism for CNN and Visual Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 和视觉模型中的注意力机制
- en: Not everything in an image or text—or in general, any data—is equally relevant
    from the perspective of insights that we need to draw from it. For example, consider
    a task where we are trying to predict the next word in a sequence of a verbose
    statement like *Alice and Alya are friends. Alice lives in France and works in
    Paris. Alya is British and works in London. Alice prefers to buy books written
    in French, whereas Alya prefers books in _____.*
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '并非图像或文本中的每一部分——或者一般来说，任何数据——从我们需要获得的洞察角度来看都是同等相关的。例如，考虑一个任务，我们需要预测一个冗长陈述序列中的下一个单词，如
    *Alice and Alya are friends. Alice lives in France and works in Paris. Alya is
    British and works in London. Alice prefers to buy books written in French, whereas
    Alya prefers books in _____.*  '
- en: 'When this example is given to a human, even a child with decent language proficiency
    can very well predict the next word will most probably be *English*. Mathematically,
    and in the context of deep learning, this can similarly be ascertained by creating
    a vector embedding of these words and then computing the results using vector
    mathematics, as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个例子给人类时，即使是语言能力相对较好的孩子也能很好地预测下一个词很可能是 *English*。从数学角度出发，在深度学习的背景下，我们同样可以通过创建这些单词的向量嵌入，并使用向量数学计算结果，来得出类似的结论，具体如下：
- en: '![](img/7834171b-7edd-4d0f-84b3-cc116bcd796a.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7834171b-7edd-4d0f-84b3-cc116bcd796a.png)'
- en: Here, *V(Word)* is the vector embedding for the required word; similarly, *V(French)*,
    *V(Paris)*, and *V(London)* are the required vector embeddings for the words *French*,
    *Paris*, and *London*, respectively.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V(Word)* 是所需单词的向量嵌入；类似地，*V(French)*、*V(Paris)* 和 *V(London)* 分别是单词 *French*、*Paris*
    和 *London* 的向量嵌入。
- en: Embeddings are (often) lower dimensional and dense (numerical) vector representations
    of inputs or indexes of inputs (for non-numerical data); in this case, text.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入通常是低维且密集（数值型）的向量表示，用于表示输入或输入的索引（对于非数值数据）；在此情况下为文本。
- en: Algorithms such as `Word2Vec` and `glove` can be used to get word embeddings.
    Pretrained variants of these models for general texts are available in popular
    Python-based NLP libraries, such as SpaCy, Gensim and others can also be trained
    using most deep learning libraries, such as Keras, TensorFlow, and so on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如 `Word2Vec` 和 `glove` 等算法可以用来获得单词嵌入。这些模型的预训练变种可在流行的基于 Python 的 NLP 库中找到，如
    SpaCy、Gensim 等，也可以使用大多数深度学习库（如 Keras、TensorFlow 等）进行训练。
- en: The concept of embeddings is as much relevant to vision and images as it is
    to text.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入（embeddings）这一概念与视觉和图像领域的相关性与文本领域同样重要。
- en: There may not be an existing vector exactly matching the vector we obtained
    just now in the form of ![](img/c3f164b3-d260-4473-9ec8-892bf9703ee6.png); but
    if we try to find the one closest to the so obtained ![](img/a8e0ee9f-386b-48a2-a6df-61e883dde986.png)
    that exists and find the representative word using reverse indexing, that word
    would most likely be the same as what we as humans thought of earlier, that is,
    *English*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 可能没有现成的向量与我们刚刚得到的向量完全匹配，如 ![](img/c3f164b3-d260-4473-9ec8-892bf9703ee6.png)；但是，如果我们尝试找到最接近的现有向量，如
    ![](img/a8e0ee9f-386b-48a2-a6df-61e883dde986.png)，并使用反向索引查找代表词，那么这个词很可能与我们人类之前想到的词相同，即
    *English*。
- en: Algorithms such as cosine similarity can be used to get the vector closest to
    the computed one.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如余弦相似度等算法可以用来找到与计算出的向量最接近的向量。
- en: For implementation, a computationally more efficient way of finding the closest
    vector would be **approximate nearest neighbor** (**ANN**), as available in Python's
    `annoy` library.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实现来说，找到最接近向量的计算效率更高的方式是 **近似最近邻** (**ANN**)，可以通过 Python 的 `annoy` 库来实现。
- en: Though we have helped get the same results, both cognitively and through deep
    learning approaches, the input in both the cases was not the same. To humans,
    we had given the exact sentence as to the computer, but for deep learning applications,
    we had carefully picked the correct words (*French*, *Paris*, and *London*) and
    their right position in the equation to get the results. Imagine how we can very
    easily realize the right words to pay attention to in order to understand the
    correct context, and hence we have the results; but in the current form, it was
    not possible for our deep learning approach to do the same.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通过认知和深度学习方法帮助得出了相同的结果，但两者的输入并不相同。对于人类，我们给出的句子与计算机相同，但对于深度学习应用，我们仔细选择了正确的单词（*French*、*Paris*
    和 *London*）及其在方程中的正确位置以获得结果。想象一下，我们如何能轻松地意识到需要关注的正确单词，以理解正确的上下文，从而得出结果；但在当前的形式下，我们的深度学习方法无法做到这一点。
- en: Now there are quite sophisticated algorithms in language modeling using different
    variants and architectures of RNN, such as LSTM and Seq2Seq, respectively. These
    could have solved this problem and got the right solution, but they are most effective
    in shorter and more direct sentences, such as *Paris is to French what London
    is to _____*. In order to correctly understand a long sentence and generate the
    correct result, it is important to have a mechanism to teach the architecture
    whereby specific words need to be paid more attention to in a long sequence of
    words. This is called the **attention mechanism** in deep learning, and it is
    applicable to many types of deep learning applications but in slightly different
    ways.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用不同变体和架构的RNN（如LSTM和Seq2Seq）的语言建模算法已经相当复杂。这些算法本来可以解决这个问题并得到正确的答案，但它们在较短且直接的句子中最为有效，例如*巴黎对于法语来说，就像伦敦对于_____一样*。为了正确理解一个长句并生成正确的结果，重要的是要有一种机制来教导架构，使得在一长串单词中需要特别关注某些特定的词。这就是深度学习中的**注意力机制**，它适用于许多类型的深度学习应用，但方式略有不同。
- en: '**RNN** stands for **recurrent neural networks** and is used to depict a temporal
    sequence of data in deep learning. Due to the vanishing gradient problem, RNN
    is seldom used directly; instead, its variants, such as **LSTM** (**Long-Short
    Term Memory**) and **GRU** (**Gated Recurrent Unit**) are more popular in actual
    implementations.**Seq2Seq** stands for **Sequence-to-Sequence** models and comprises
    two RNN (or variant) networks (hence it is called **Seq2Seq**, where each RNN
    network represents a sequence); one acts as an encoder and the other as a decoder.
    The two RNN networks can be multi-layer or stacked RNN networks, and they are
    connected via a thought or context vector. Additionally, Seq2Seq models may use
    the attention mechanism to improve performance, especially for longer sequences.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNN**代表**循环神经网络**，用于表示深度学习中的时间序列数据。由于梯度消失问题，RNN很少被直接使用；相反，其变体，如**LSTM**（**长短期记忆网络**）和**GRU**（**门控循环单元**）在实际应用中更为流行。**Seq2Seq**代表**序列到序列**模型，由两个RNN（或其变体）网络组成（因此被称为**Seq2Seq**，其中每个RNN网络表示一个序列）；一个作为编码器，另一个作为解码器。这两个RNN网络可以是多层或堆叠的RNN网络，并通过一个思维或上下文向量连接。此外，Seq2Seq模型可以使用注意力机制来提高性能，特别是对于更长的序列。'
- en: In fact, to be more precise, even we had to process the preceding information
    in layers, first understanding that the last sentence is about Alya. Then we can
    identify and extract Alya's city, then that for Alice, and so on. Such a layered
    way of human thinking is analogous to stacking in deep learning, and hence in
    similar applications, stacked architectures are quite common.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，更准确地说，即使我们需要分层处理前面的信息，首先要理解最后一句话是关于Alya的。然后我们可以识别并提取Alya所在的城市，接着是Alice的城市，依此类推。人类这种分层思维方式类似于深度学习中的堆叠，因此在类似的应用中，堆叠架构非常常见。
- en: To know more about how stacking works in deep learning, especially with sequence-based
    architectures, explore topics such as stacked RNN and stacked attention networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 若想更了解堆叠在深度学习中的运作方式，特别是在基于序列的架构中，可以探讨一些话题，如堆叠RNN和堆叠注意力网络。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Attention mechanism for image captioning
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像字幕生成的注意力机制
- en: Types of attention (Hard, and Soft Attentions)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制的类型（硬注意力和软注意力）
- en: Using attention to improve visual models
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用注意力机制改善视觉模型
- en: Recurrent models of visual attention
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉注意力的递归模型
- en: Attention mechanism for image captioning
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像字幕生成的注意力机制
- en: From the introduction, so far, it must be clear to you that the attention mechanism
    works on a sequence of objects, assigning each element in the sequence a weight
    for a specific iteration of a required output. With every next step, not only
    the sequence but also the weights in the attention mechanism can change. So, attention-based
    architectures are essentially sequence networks, best implemented in deep learning
    using RNNs (or their variants).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从介绍中，到目前为止，您一定已经清楚注意力机制是在一系列对象上工作的，为每个序列中的元素分配一个特定迭代所需输出的权重。随着每一步的推进，不仅序列会改变，注意力机制中的权重也会发生变化。因此，基于注意力的架构本质上是序列网络，最适合使用RNN（或其变体）在深度学习中实现。
- en: 'The question now is: how do we implement a sequence-based attention on a static
    image, especially the one represented in a **convolutional neural network** (**CNN**)?
    Well, let''s take an example that sits right in between a text and image to understand
    this. Assume that we need to caption an image with respect to its contents.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：我们如何在静态图像上实现基于序列的注意力，尤其是在**卷积神经网络**（**CNN**）中表示的图像上？好吧，让我们通过一个介于文本和图像之间的例子来理解这个问题。假设我们需要根据图像内容为其生成标题。
- en: 'We have some images with captions provided by humans as training data and using
    this, we need to create a system that can provide a decent caption for any new
    image not seen earlier by the model. As seen earlier, let''s take an example and
    see how we, as humans, will perceive this task and the analogous process to it
    that needs to be implemented in deep learning and CNN. Let''s consider the following
    image and conceive some plausible captions for it. We''ll also rank them heuristically
    using human judgment:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一些带有人工提供的图像和标题作为训练数据，基于这些数据，我们需要创建一个系统，能够为任何新图像生成合适的标题，且这些图像此前未曾被模型见过。如前所述，我们可以举个例子，看看作为人类我们如何理解这个任务，以及需要在深度学习和卷积神经网络（CNN）中实现的类似过程。让我们考虑以下这张图像，并为其构思一些合理的标题。我们还将通过人工判断对这些标题进行启发式排序：
- en: '![](img/4769fd23-01c6-49b2-a5bb-003b7d071e8b.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4769fd23-01c6-49b2-a5bb-003b7d071e8b.jpeg)'
- en: 'Some probable captions (in order of most likely to least likely) are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一些可能的标题（从最可能到最不可能的顺序）是：
- en: Woman seeing dog in snow forest
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 女人在雪林中看着狗
- en: Brown dog in snow
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 棕色狗在雪中
- en: A person wearing cap in woods and white land
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位戴帽子的人在森林和白雪覆盖的土地上
- en: Dog, tree, snow, person, and sunshine
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗、树、雪、人、阳光
- en: 'An important thing to note here is that, despite the fact that the woman is
    central to the image and the dog is not the biggest object in the image, the caption
    we sought probable focused on them and then their surroundings here. This is because
    we consider them as important entities here (given no prior context). So as humans,
    how we reached this conclusion is as follows: we first glanced the whole image,
    and then we focused towards the woman, in high resolution, while putting everything
    in the background (assume a **Bokeh** effect in a dual-camera phone). We identified
    the caption part for that, and then the dog in high resolution while putting everything
    else in low resolution; and we appended the caption part. Finally, we did the
    same for the surroundings and caption part for that.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，尽管女人在图像中是中心对象，而狗并不是图像中最大的物体，但我们寻求的标题显然集中在它们及其周围环境。这是因为我们认为它们在此处是重要的实体（假设没有其他上下文）。作为人类，我们如何得出这个结论的过程如下：我们首先快速浏览了整张图像，然后将焦点集中在女人身上，并以高分辨率呈现她，同时将背景的其他部分（假设是**散景**效果）置于低分辨率。我们为此部分确定了标题，然后将狗呈现为高分辨率，背景其他部分则保持低分辨率；接着我们补充了与狗相关的标题部分。最后，我们对周围环境做了相同处理并为之添加了标题部分。
- en: 'So essentially, we saw it in this sequence to reach to the first caption:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，本质上，我们通过以下顺序得出第一个标题：
- en: '![](img/2f25f7ff-82b8-48db-b230-bbb01e70cc2e.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f25f7ff-82b8-48db-b230-bbb01e70cc2e.jpg)'
- en: 'Image 1: Glance the image first'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像1：先快速浏览图像
- en: '![](img/c27a5da0-d904-4809-87fd-45f43765afcd.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c27a5da0-d904-4809-87fd-45f43765afcd.jpg)'
- en: 'Image 2: Focus on woman'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图像2：聚焦于女人
- en: '![](img/75e4944d-5be6-42ca-b241-941152165171.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75e4944d-5be6-42ca-b241-941152165171.jpg)'
- en: Image 3: Focus on dog
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图像3：聚焦于狗
- en: '![](img/1c6594a7-aeca-4723-b445-37b172ec0a1c.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c6594a7-aeca-4723-b445-37b172ec0a1c.jpg)'
- en: 'Image 4: Focus on snow'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图像4：聚焦于雪
- en: '![](img/cdf6808c-9cef-4710-bcf3-a959a2ab1810.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cdf6808c-9cef-4710-bcf3-a959a2ab1810.jpg)'
- en: Image 5: Focus on forest
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图像5：聚焦于森林
- en: 'In terms of weight of attention or focus, after glancing the image, we focus
    on the first most important object: the woman here. This is analogous to creating
    a mental frame in which we put the part of the image with the woman in high-resolution
    and the remaining part of the image in low-resolution.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从注意力或焦点的权重来看，在快速浏览图像之后，我们会将注意力集中在图像中最重要的第一个物体上：这里是女人。这个过程类似于在创建一个心理框架，在这个框架中，我们将包含女人的图像部分置于高分辨率，而图像的其余部分则保持低分辨率。
- en: In a deep learning reference, the attention sequence will have the highest weight
    for the vector (embedding) representing the concept of the woman for this part
    of the sequence. In the next step of the output/sequence, the weight will shift
    more towards the vector representation for the dog and so on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的参考中，注意力序列将会为表示女人概念的向量（嵌入）分配最高的权重。在输出/序列的下一步中，权重将更多地向代表狗的向量偏移，依此类推。
- en: To understand this intuitively, we convert the image represented in the form
    of CNN into a flattened vector or some other similar structure; then we create
    different splices of the image or sequences with different parts in varying resolutions.
    Also, as we understand now from our discussion in [Chapter 7](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml),
    *Object-Detection & Instance-Segmentation with CNN*, we must have the relevant
    portions that we need to detect in varying scales as well for effective detection.
    The same concept applies here too, and besides resolution, we also vary the scale;
    but for now, we will keep it simple and ignore the scale part for intuitive understanding.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观理解这一点，我们将以CNN形式表示的图像转换为一个扁平化的向量或其他类似的结构；然后，我们创建图像的不同切片或具有不同部分和不同分辨率的序列。同时，正如我们在[第7章](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml)中讨论的那样，*使用CNN进行目标检测与实例分割*，我们必须拥有需要检测的相关部分，并且这些部分也必须在不同的尺度下进行有效的检测。这个概念在这里同样适用，除了分辨率，我们还会改变尺度；但为了直观理解，我们暂时忽略尺度部分，保持简单。
- en: These splices or sequences of images now act as a sequence of words, as in our
    earlier example, and hence they can be treated inside an RNN/LSTM or similar sequence-based
    architecture for the purpose of attention. This is done to get the best-suited
    word as the output in every iteration. So the first iteration of the sequence
    leads to woman (from the weights of a sequence representing an object represented
    as a *Woman* in *Image 2*) → then the next iteration as → *seeing* (from a sequence
    identifying the back of the *Woman* as in *Image 2*) → *Dog* (sequence as in *Image
    3*) → *in* (from a sequence where everything is blurred generating *filler* words
    transitioning from entities to surroundings) → *Snow* (sequence as in *Image 4*) →
    *Forest* (sequence as in *Image 5*).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像切片或序列现在充当一系列单词，就像我们之前的例子一样，因此它们可以在RNN/LSTM或类似的基于序列的架构中进行处理，用于注意力机制。这样做的目的是在每次迭代中获得最合适的单词作为输出。因此，序列的第一次迭代会得到woman（来自表示*Woman*的序列在*Image
    2*中的权重）→ 然后下一次迭代得到→ *seeing*（来自表示*Woman*背面的序列，如*Image 2*）→ *Dog*（来自*Image 3*中的序列）→
    *in*（来自所有模糊像素生成的序列，生成*填充词*，从实体过渡到环境）→ *Snow*（来自*Image 4*中的序列）→ *Forest*（来自*Image
    5*中的序列）。
- en: Filler words such as *in* and action words such as *seeing* can also be automatically
    learned when the best image splice/sequence mapping to human-generated captions
    is done across several images. But for the simpler version, a caption such as *Woman*,
    *Dog*, *Snow*, and *Forest* can also be a good depiction of entities and surroundings
    in the image.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 填充词如*in*和动作词如*seeing*也可以通过在人类生成的字幕与多张图像之间进行最佳图像切片/序列映射时自动学习。但在更简单的版本中，像*Woman*、*Dog*、*Snow*和*Forest*这样的字幕也能很好地描述图像中的实体和周围环境。
- en: Types of Attention
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力的类型
- en: 'There are two types attention mechanisms. They are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制有两种类型。它们如下：
- en: Hard attention
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬性注意力
- en: Soft attention
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软性注意力
- en: Let's now take a look at each one in detail in the following sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细了解每一种类型。
- en: Hard Attention
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬性注意力
- en: In reality, in our recent image caption example, several more pictures would
    be selected, but due to our training with the handwritten captions, those would
    never be weighted higher. However, the essential thing to understand is how the
    system would understand what all pixels (or more precisely, the CNN representations
    of them) the system focuses on to draw these high-resolution images of different
    aspects and then how to choose the next pixel to repeat the process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在我们最近的图像描述示例中，会选择更多的图片，但由于我们用手写字幕进行训练，这些图片的权重永远不会更高。然而，重要的是要理解系统如何理解所有像素（或者更精确地说，是它们的CNN表示），系统聚焦于这些像素，以绘制不同方面的高分辨率图像，并且如何选择下一个像素以重复这一过程。
- en: In the preceding example, the points are chosen at random from a distribution
    and the process is repeated. Also, which pixels around this point get a higher
    resolution is decided inside the attention network. This type of attention is
    known as **hard attention**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，点是从分布中随机选择的，并且该过程会重复进行。而且，哪些像素会获得更高的分辨率是由注意力网络内部决定的。这种类型的注意力被称为**硬性注意力**。
- en: Hard attention has something called the **differentiability problem**. Let's
    spend some time understanding this. We know that in deep learning the networks
    have to be trained and to train them we iterate across training batches in order
    to minimize the loss function. We can minimize the loss function by changing the
    weights in the direction of the gradient of the minima, which in turn is arrived
    at after differentiating the loss function*.*
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 硬注意力存在所谓的**可微性问题**。我们花些时间来理解这个问题。我们知道，在深度学习中，网络需要训练，而训练它们的方式是通过遍历训练批次来最小化损失函数。我们可以通过沿着最小值的梯度方向改变权重，从而最小化损失函数，这样就可以得到最小值，而这个过程是通过对损失函数进行微分得到的。*
- en: This process of minimizing losses across layers of a deep network, starting
    from the last layer to the first, is known as **back-propagation**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程，即从最后一层到第一层在深度网络中最小化损失，是**反向传播**。
- en: Examples of some differentiable loss functions used in deep learning and machine
    learning are the log-likelihood loss function, squared-error loss function, binomial
    and multinominal cross-entropy, and so on.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和机器学习中使用的一些可微损失函数示例包括对数似然损失函数、平方误差损失函数、二项式和多项式交叉熵等。
- en: However, since the points are chosen randomly in each iteration in hard attention—and
    since such a random pixel choosing mechanism is not a differentiable function—we
    essentially cannot train this attention mechanism, as explained. This problem
    is overcome either by using **Reinforcement Learning** (**RL**) or by switching
    to soft attention.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于在每次迭代中硬注意力是随机选择点的——而且这种随机像素选择机制不是一个可微函数——我们本质上无法训练这种注意力机制，正如前文所解释的。这个问题可以通过使用**强化学习**（**RL**）或切换到软注意力来解决。
- en: RL involves mechanisms of solving two problems, either separately or in combination.
    The first is called the **control problem**, which determines the most optimal
    action that the agent should take in each step given its state, and the second
    is the **prediction problem**, which determines the optimal *value* of the state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习涉及解决两个问题的机制，可能是分别解决，也可能是结合解决。第一个问题叫做**控制问题**，它决定了在给定状态下，代理在每一步应该采取的最优动作；第二个问题叫做**预测问题**，它决定了状态的最优*值*。
- en: Soft Attention
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软注意力
- en: As introduced in the preceding sub-section on hard attention, soft attention
    uses RL to progressively train and determine where to seek next (control problem).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的硬注意力小节中介绍的那样，软注意力使用强化学习逐步训练并确定下一步寻找的地方（控制问题）。
- en: 'There exist two major problems with using the combination of hard attention
    and RL to achieve the required objective:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用硬注意力和强化学习（RL）结合来实现所需目标存在两个主要问题：
- en: It becomes slightly complicated to involve RL and train an RL agent and an RNN/deep
    network based on it separately.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将强化学习和训练基于强化学习的代理及递归神经网络（RNN）/深度网络分开处理会稍显复杂。
- en: The variance in the gradient of the policy function is not only high (as in
    **A3C** model), but also has a computational complexity of *O(N)*, where *N* is
    the number of units in the network. This increases the computation load for such
    approaches massively. Also, given that the attention mechanism adds more value
    in overly long sequences (of words or image embedding splices)—and to train networks
    involving longer sequences requires larger memory, and hence much deeper networks—this
    approach is computationally not very efficient.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略函数的梯度方差不仅很高（如**A3C**模型），而且其计算复杂度为*O(N)*，其中*N*是网络中单元的数量。这大大增加了此类方法的计算负担。此外，由于注意力机制在过长的序列（无论是词语还是图像嵌入片段）中能提供更多价值——而且训练涉及更长序列的网络需要更大的内存，因此需要更深的网络——这种方法在计算上并不高效。
- en: The **Policy Function **in RL, determined as *Q(a,s)*, is the function used
    to determine the optimal policy or the action *(a)* that should be taken in any
    given state *(s)* to maximize the rewards.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的**策略函数**，表示为*Q(a,s)*，是用来确定最优策略或在给定状态*(s)*下应该采取的动作*(a)*，以最大化奖励的函数。
- en: So what is the alternative? As we discussed, the problem arose because the mechanism
    that we were choosing for attention led to a non-differentiable function, because
    of which we had to go with RL. So let's take a different approach here. Taking
    an analogy of our language modeling problem example (as in the A*ttention Mechanism
    - Intuition* section) earlier, we assume that we have the vector of the tokens
    for the objects/ words present in the attention network. Also, in same vector
    space (say in the embedding hyperspace) we bring the tokens for the object/ words
    in the required query of the particular sequence step. On taking this approach,
    finding the right attention weights for the tokens in the attention network with
    the respect to the tokens in query space is as easy as computing the vector similarity
    between them; for example, a cosine distance. Fortunately, most vector distance
    and similarity functions are differentiable; hence the loss function derived by
    using such vector distance/similarity functions in such space is also differentiable,
    and our back-propagation can work in this scenario.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，替代方案是什么呢？正如我们所讨论的，问题的出现是因为我们为注意力机制选择的方式导致了一个不可微的函数，因此我们不得不使用强化学习（RL）。所以让我们在这里采取不同的方式。通过类比我们在前面提到的语言建模问题示例（如在*A*ttention机制
    - 直觉*部分），我们假设我们已经有了注意力网络中对象/词语的向量。此外，在同一向量空间中（比如嵌入超空间），我们将特定序列步骤中的查询所需的对象/词语的标记引入。采用这种方法，找到注意力网络中标记的正确注意力权重与查询空间中的标记之间的关系，就像计算它们之间的向量相似度一样简单；例如，计算余弦距离。幸运的是，大多数向量距离和相似度函数都是可微的；因此，使用这种向量距离/相似度函数在该空间中推导出的损失函数也是可微的，我们的反向传播可以在这种情况下正常工作。
- en: The cosine distance between two vectors, say ![](img/a1f3eff3-207d-4afa-89ea-46df50ec41d7.png),
    and ![](img/4d0ee39a-0f58-4d78-9d0c-9ac88bd3089f.png), in a multi-dimensional
    (three in this example) vector space is given as:![](img/4ae1832d-60bb-4a85-b664-748a097ef5c4.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的余弦距离，例如 ![](img/a1f3eff3-207d-4afa-89ea-46df50ec41d7.png) 和 ![](img/4d0ee39a-0f58-4d78-9d0c-9ac88bd3089f.png)，在一个多维（此例为三维）向量空间中的计算公式为：![](img/4ae1832d-60bb-4a85-b664-748a097ef5c4.png)
- en: This approach of using a differentiable loss function for training an attention
    network is known as **soft attention**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可微损失函数训练注意力网络的方法被称为**软注意力**。
- en: Using attention to improve visual models
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用注意力来改善视觉模型
- en: As we discovered in the NLP example covered in the earlier section on Attention
    Mechanism - Intuition, Attention did help us a lot in both achieving new use-cases,
    not optimally feasible with conventional NLP, and vastly improving the performance
    of the existing NLP mechanism. Similar is the usage of Attention in CNN and Visual
    Models as well
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的注意力机制 - 直觉部分的NLP示例中发现的，注意力确实帮助我们在实现新的用例方面取得了巨大进展，这些用例是传统NLP无法高效实现的，同时也大大提高了现有NLP机制的性能。在CNN和视觉模型中，注意力的使用也是类似的。
- en: In the earlier chapter [Chapter 7](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml),
    *Object-Detection & Instance-Segmentation with CNN*, we discovered how Attention
    (like) mechanism are used as Region Proposal Networks for networks like Faster
    R-CNN and Mask R-CNN, to greatly enhance and optimize the proposed regions, and
    enable the generation of segment masks. This corresponds to the first part of
    the discussion. In this section, we will cover the second part of the discussion,
    where we will use 'Attention' mechanism to improve the performance of our CNNs,
    even under extreme conditions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章[第七章](20952d99-3977-420f-a5c7-a3320b96bed6.xhtml)，*基于CNN的目标检测与实例分割*中，我们发现了如何使用类似注意力机制的区域提议网络（如Faster
    R-CNN和Mask R-CNN），大大增强和优化了提议区域，并生成分割掩码。这对应于讨论的第一部分。在本节中，我们将讨论第二部分，我们将使用“注意力”机制来提高我们CNN的性能，即使在极端条件下。
- en: Reasons for sub-optimal performance of visual CNN models
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉CNN模型性能不佳的原因
- en: 'The performance of a CNN network can be improved to a certain extent by adopting
    proper tuning and setup mechanisms such as: data pre-processing, batch normalization,
    optimal pre-initialization of weights; choosing the correct activation function;
    using techniques such as regularization to avoid overfitting; using an optimal
    optimization function; and training with plenty of (quality) data.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用适当的调优和设置机制，CNN网络的性能可以在一定程度上得到改善，这些机制包括：数据预处理、批归一化、权重的最佳预初始化；选择正确的激活函数；使用正则化技术来避免过拟合；使用最佳的优化函数；以及使用大量（优质）数据进行训练。
- en: 'Beyond these training and architecture-related decisions, there are image-related
    nuances because of which the performance of visual models may be impacted. Even
    after controlling the aforementioned training and architectural factors, the conventional
    CNN-based image classifier does not work well under some of the following conditions
    related to the underlying images:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些训练和架构相关的决策外，还有与图像相关的细节，这些细节可能影响视觉模型的表现。即便在控制了上述训练和架构因素后，传统的基于 CNN 的图像分类器在以下一些与底层图像相关的条件下表现不佳：
- en: Very big images
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常大的图像
- en: Highly cluttered images with a number of classification entities
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含多个分类实体的高度杂乱的图像
- en: Very noisy images
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常嘈杂的图像
- en: Let's try to understand the reasons behind the sub-optimal performance under
    these conditions, and then we will logically understand what may fix the problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解在这些条件下性能不佳的原因，然后我们将从逻辑上理解可能修复问题的方案。
- en: In conventional CNN-based models, even after a downsizing across layers, the
    computational complexity is quite high. In fact, the complexity is of the order
    of ![](img/9576a375-e104-439d-8c29-614bc5121d3a.png), where *L* and *W* are the length
    and width of the image in inches, and *PPI* is pixels per inch (pixel density).
    This translates into a linear complexity with respect to the total number of pixels
    (*P*) in the image, or *O(P)*. This directly answers the first point of the challenge;
    for higher *L*, *W*, or *PPI*, we need much higher computational power and time
    to train the network.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的基于 CNN 的模型中，即便是经过层间下采样，计算复杂度仍然相当高。实际上，复杂度是以 ![](img/9576a375-e104-439d-8c29-614bc5121d3a.png)
    为量级，其中 *L* 和 *W* 是图像的长度和宽度（以英寸为单位），*PPI* 是每英寸的像素数（像素密度）。这意味着计算复杂度与图像中总像素数（*P*）线性相关，即
    *O(P)*。这直接回答了挑战中的第一个问题；对于更高的 *L*、*W* 或 *PPI*，我们需要更高的计算能力和时间来训练网络。
- en: Operations such as max-pooling, average-pooling, and so on help downsize the
    computational load drastically vis-a-vis all the computations across all the layers
    performed on the actual image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 操作如最大池化、平均池化等有助于大幅减少计算负担，相比于在所有层中对实际图像进行的所有计算。
- en: 'If we visualize the patterns formed in each of the layers of our CNN, we would
    understand the intuition behind the working of the CNN and why it needs to be
    deep. In each subsequent layer, the CNN trains higher conceptual features, which
    may progressively better help understand the objects in the image layer after
    layer. So, in the case of MNIST, the first layer may only identify boundaries,
    the second the diagonals and straight-line-based shapes of the boundaries, and
    so on:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可视化 CNN 中每一层形成的模式，我们将理解 CNN 工作原理背后的直觉，并且明白为什么它需要是深层的。在每一层中，CNN 训练更高层次的概念特征，这些特征逐层帮助更好地理解图像中的物体。所以，在
    MNIST 的情况下，第一层可能只识别边界，第二层识别基于边界的对角线和直线形状，以此类推：
- en: '![](img/84b09c81-669a-48a9-a78a-a9dc717454a5.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84b09c81-669a-48a9-a78a-a9dc717454a5.jpg)'
- en: Illustrative conceptual features formed in different (initial) layers of CNN
    for MNIST
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 的不同（初始）层中形成的 MNIST 相关的概念特征
- en: '![](img/7917f980-20eb-4bb3-9d7b-8c5add0c914a.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7917f980-20eb-4bb3-9d7b-8c5add0c914a.jpg)'
- en: 'MNIST is a simple dataset, whereas real-life images are quite complex; this
    requires higher conceptual features to distinguish them, and hence more complex
    and much deeper networks. Moreover, in MNIST, we are trying to distinguish between
    similar types of objects (all handwritten numbers). Whereas in real life, the
    objects might differ widely, and hence the different types of features that may
    be required to model all such objects will be very high:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 是一个简单的数据集，而现实生活中的图像则相当复杂；这需要更高层次的概念特征来区分它们，因此需要更复杂且更深的网络。此外，在 MNIST 中，我们试图区分相似类型的物体（所有都是手写数字）。而在现实生活中，物体可能差异很大，因此需要的不同特征类型也非常多：
- en: '![](img/902073a5-cb0b-4ff7-ac0a-7710d216b9de.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/902073a5-cb0b-4ff7-ac0a-7710d216b9de.jpeg)'
- en: This brings us to our second challenge. A cluttered image with too many objects
    would require a very complex network to model all these objects. Also, since there
    are too many objects to identify, the image resolution needs to be good to correctly
    extract and map the features for each object, which in turn means that the image
    size and the number of pixels need to be high for an effective classification.
    This, in turn, increases the complexity exponentially by combining the first two
    challenges.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了我们的第二个挑战。一个包含过多物体的杂乱图像需要一个非常复杂的网络来建模所有这些物体。此外，由于需要识别的物体太多，图像分辨率需要足够高才能正确提取和映射每个物体的特征，这也意味着图像大小和像素数量需要足够高，以便进行有效的分类。这反过来会通过结合前两个挑战，成倍增加复杂性。
- en: The number of layers, and hence the complexity of popular CNN architectures
    used in ImageNet challenges, have been increasing over the years. Some examples
    are VGG16 – Oxford (2014) with 16 layers, GoogLeNet (2014) with 19 layers, and
    ResNet (2015) with 152 layers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在ImageNet挑战赛中使用的流行CNN架构的层数，以及因此而增加的复杂性，近年来不断增加。一些例子包括：VGG16（2014）有16层，GoogLeNet（2014）有19层，ResNet（2015）有152层。
- en: 'Not all images are perfect SLR quality. Often, because of low light, image
    processing, low resolution, lack of stabilization, and so on, there may be a lot
    of noise introduced in the image. This is just one form of noise, one that is
    easier to understand. From the perspective of CNN, another form of noise can be
    image transition, rotation, or transformation:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有图像都具有完美的单反相机（SLR）质量。通常，由于低光照、图像处理、低分辨率、缺乏稳定性等原因，图像中可能会引入大量噪声。这只是噪声的一种形式，是比较容易理解的一种。从卷积神经网络（CNN）的角度来看，噪声的另一种形式可能是图像过渡、旋转或变换：
- en: '![](img/9a998d97-d806-4d65-ade6-9cb2aeab0b4e.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9a998d97-d806-4d65-ade6-9cb2aeab0b4e.jpeg)'
- en: Image without noise
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 没有噪声的图像
- en: '![](img/843cf705-b4bc-4751-b10e-77f64284478f.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/843cf705-b4bc-4751-b10e-77f64284478f.jpg)'
- en: Same image with added noise
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声后的同一图像
- en: In the preceding images, try reading the newspaper title *Business* in the image
    without and with noise, or identify the mobile in both the images. Difficult to
    do that in the image with noise, right? Similar is the detection/classification
    challenge with our CNN in the case of noisy images.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，试着在带噪声和不带噪声的图像中阅读报纸标题*Business*，或者在两张图像中识别手机。带噪声的图像中很难做到这一点，对吧？这就像在噪声图像的情况下，CNN的检测/分类挑战一样。
- en: Even with exhaustive training, perfect hyperparameter adjustment, and techniques
    such as dropouts and others, these real-life challenges continue to diminish the
    image recognition accuracy of CNN networks. Now that we've understood the causes
    and intuition behind the lack of accuracy and performance in our CNNs, let's explore
    some ways and architectures to alleviate these challenges using visual attention.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 即使经过大量训练，完美的超参数调整，以及诸如丢弃法等技术，这些现实中的挑战依然会降低CNN网络的图像识别准确性。现在我们已经理解了导致CNN准确性和性能不足的原因和直觉，让我们探讨一些使用视觉注意力来缓解这些挑战的方法和架构。
- en: Recurrent models of visual attention
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉注意力的递归模型
- en: '*Recurrent models of visual attention* can be used to answer some of the challenges
    we covered in the earlier section. These models use the hard attention method,
    as covered in an earlier (*Types of attention*) section. Here we use one of the
    popular variants of recurrent models of visual attention, the **Recurrent Attention
    Model** (**RAM**).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*视觉注意力的递归模型*可以用来解决我们在前面部分提到的一些挑战。这些模型使用硬注意力方法，正如在之前的（*注意力类型*）部分中所讲述的那样。在这里，我们使用的是一种流行的视觉注意力递归模型变体——**递归注意力模型**（**RAM**）。'
- en: As covered earlier, hard attention problems are non-differentiable and have
    to use RL for the control problem. The RAM thus uses RL for this optimization.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，硬注意力问题是不可微分的，因此必须使用强化学习（RL）来解决控制问题。因此，RAM使用强化学习来进行此优化。
- en: A recurrent model of visual attention does not process the entire image, or
    even a sliding-window-based bounding box, at once. It mimics the human eye and
    works on the concept of *Fixation* of *Gaze* at different locations of an image;
    with each *Fixation*, it incrementally combines information of importance to dynamically
    build up an internal representation of scenes in the image. It uses an RNN to
    do this in a sequential manner.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉注意力的递归模型不会一次性处理整个图像，甚至不会处理基于滑动窗口的边界框。它模仿人眼的工作方式，基于图像中不同位置的*注视*，并结合每次*注视*所获得的重要信息，逐步建立起图像场景的内部表示。它使用递归神经网络（RNN）以顺序方式进行处理。
- en: The model selects the next location to Fixate to based on the RL agents control
    policy to maximize the reward based on the current state. The current state, in
    turn, is a function of all the past information and the demands of the task. Thus,
    it finds the next coordinate for fixation so that it can maximize the reward (demands
    of the task), given the information collected until now across the previous gazes
    in the memory snapshot of the RNN and the previously visited coordinate.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 模型根据RL智能体的控制策略选择下一个要固定的位置信息，以最大化基于当前状态的奖励。当前状态又是所有过去信息和任务需求的函数。因此，它找到下一个固定坐标，以便在已经收集的信息基础上（通过RNN的记忆快照和先前访问的坐标）最大化奖励（任务需求）。
- en: Most RL mechanisms use the **Markov Decision Process** (**MDP**), in which the
    next action is determined only by the current state, irrespective of the states
    visited earlier. By using RNN here, important information from previous *Fixations*
    can be combined in the present state itself.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数RL机制使用**马尔可夫决策过程**(**MDP**)，其中下一个动作仅由当前状态决定，而与之前访问的状态无关。在这里使用RNN，能够将来自先前*固定视点*的重要信息结合到当前状态中。
- en: The preceding mechanism solves the last two problems highlighted in CNN in the
    earlier section. Also, in the RAM, the number of parameters and amount of computation
    it performs can be controlled independently of the size of the input image, thus
    solving the first problem as well.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 上述机制解决了CNN在前面部分中强调的最后两个问题。此外，在RAM中，参数的数量和计算量可以独立于输入图像的大小进行控制，从而也解决了第一个问题。
- en: Applying the RAM on a noisy MNIST sample
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在噪声MNIST样本上应用RAM
- en: 'To understand the working of the RAM in greater detail, let''s try to create
    an MNIST sample incorporating some of the problems as highlighted in the earlier
    section:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地理解RAM的工作原理，让我们尝试创建一个包含一些早期部分所提到的问题的MNIST样本：
- en: '![](img/a2c083ac-a490-449a-9c63-bf7c251244d2.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2c083ac-a490-449a-9c63-bf7c251244d2.jpg)'
- en: Larger image of noisy and distorted MNIST
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的噪声和失真的MNIST图像
- en: The preceding image represents a larger image/collage using an actual and slightly
    noisy sample of an MNIST image (of number **2**), and a lot of other distortions
    and snippets of other partial samples. Also, the actual digit **2** here is not
    centered. This example represents all the previously stated problems, yet it is
    simple enough to understand the working of the RAM.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个较大的图像/拼贴，使用了一个实际且略微噪声化的MNIST图像（数字**2**），以及其他一些失真和部分样本的片段。此外，实际的数字**2**并未居中。此示例代表了之前所述的所有问题，但足够简单，便于理解RAM的工作原理。
- en: 'The RAM uses the concept of a **Glimpse Sensor**. The RL agent fixes its gaze
    at a particular coordinate (*l*) and particular time (*t-1*). The coordinate at
    time t-1, *l[t-1]* of the image *x[t ]*and uses the **Glimpse Sensor** to extract
    retina-like multiple-resolution patches of the image with *l[t-1]* as the center.
    These representations, extracted at time *t-1*, are collectively called *p(x[t]*,
    *l[t-1])*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: RAM使用**瞥视传感器**的概念。RL智能体将目光固定在特定的坐标(*l*)和特定的时间(*t-1*)。在时间t-1时刻，坐标*l[t-1]*和图像*x[t]*的内容，通过**瞥视传感器**提取以*l[t-1]*为中心的类似视网膜的多分辨率图像补丁。这些在时间*t-1*提取的表示
    collectively 被称为*p(x[t]*, *l[t-1])*：
- en: '![](img/619b6dcc-4967-458b-8628-3becc0bafd72.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/619b6dcc-4967-458b-8628-3becc0bafd72.jpg)'
- en: The concept of the Glimpse Sensor
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 瞥视传感器的概念
- en: '![](img/a545ac17-e9dd-4116-b679-169f3fb2ec46.jpg)  ;![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a545ac17-e9dd-4116-b679-169f3fb2ec46.jpg)  ;![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)'
- en: These images show the representations of our image across two fixations using
    the **Glimpse Sensor**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像展示了我们的图像在两个固定视点下使用**瞥视传感器**的表示。
- en: 'The representations obtained from the **Glimpse Sensor** are passes through
    the ''Glimpse Network, which flattens the representation at two stages. In the
    first stage, the representations from the **Glimpse Sensor** and the **Glimpse
    Network** are flattened separately (![](img/7963a240-8377-44d3-ae47-0ddb6b5cd3e3.png)),
    and then they are combined into a single flattened layer (![](img/f6d21a4f-d1e9-4fbf-a645-7f785778784e.png))
    to generate the output representation *g[t] *for time *t*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从**瞥视传感器**获得的表示经过'瞥视网络'处理，表示会在两个阶段被展平。在第一阶段，**瞥视传感器**和**瞥视网络**的表示分别被展平（![](img/7963a240-8377-44d3-ae47-0ddb6b5cd3e3.png)），然后它们合并为一个单一的展平层（![](img/f6d21a4f-d1e9-4fbf-a645-7f785778784e.png)），以生成时间*t*的输出表示*g[t]*：
- en: '![](img/c09406b3-37cb-4929-a9c1-42e08c8742e7.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c09406b3-37cb-4929-a9c1-42e08c8742e7.jpg)'
- en: The concept of the Glimpse Network
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Glimpse网络的概念
- en: 'These output representations are then passed through the RNN model architecture.
    The fixation for the next step in the iteration is determined by the RL agent
    to maximize the reward from this architecture:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些输出表示然后传递通过RNN模型架构。下一步的固定点由RL代理决定，以最大化来自此架构的奖励：
- en: '![](img/41cb0399-2275-462d-b820-135b3243e91d.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41cb0399-2275-462d-b820-135b3243e91d.jpg)'
- en: Model architecture (RNN)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构（RNN）
- en: 'As can be intuitively understood, the Glimpse Sensor captures important information
    across fixations, which can help identify important concepts. For example, the
    multiple resolution (here 3) representations at the Fixation represented by our
    second sample image have three resolutions as marked (red, green, and blue in
    order of decreasing resolution). As can be seen, even if these are used directly,
    we have got a varying capability to detect the right digit represented by this
    noisy collage:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如直观理解所示，Glimpse传感器捕捉了跨越注视点的重要信息，这有助于识别重要的概念。例如，我们第二个示例图像中表示的Fixation处的多个分辨率（此处为3）具有标记的三种分辨率（按分辨率递减顺序为红色、绿色和蓝色）。如图所示，即使这些被直接使用，我们依然能够检测到由这一噪声拼贴表示的正确数字：
- en: '![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)![](img/a15e4752-6c1d-4d40-b966-cacf871b6701.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c44ab12e-cd94-4a50-ae69-1d5876ba0978.jpg)![](img/a15e4752-6c1d-4d40-b966-cacf871b6701.jpg)'
- en: Glimpse Sensor in code
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Glimpse传感器代码
- en: As discussed in the earlier section, the Glimpse Sensor is a powerful concept.
    Combined with other concepts, such as RNN and RL, as discussed earlier, it is
    at the heart of improving the performance of visual models.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所讨论，Glimpse传感器是一个强大的概念。结合RNN和RL等其他概念，如前所述，它是提高视觉模型性能的核心。
- en: 'Let''s see this in greater detail here. The code is commented at every line
    for easy understanding and is self-explanatory:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里更详细地查看。代码的每一行都有注释，方便理解，并且自解释：
- en: '[PRE0]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: References
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
    Richard S. Zemel, Yoshua Bengio, Show, Attend and Tell: *Neural Image Caption
    Generation with Visual Attention*, CoRR, arXiv:1502.03044, 2015.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan
    Salakhutdinov, Richard S. Zemel, Yoshua Bengio, Show, Attend and Tell: *基于视觉注意力的神经图像描述生成*，CoRR，arXiv:1502.03044，2015年。'
- en: Karl Moritz Hermann, Tom's Kocisk, Edward Grefenstette, Lasse Espeholt, Will
    Kay, Mustafa Suleyman, Phil Blunsom, *Teaching Machines to Read and Comprehend*,
    CoRR, arXiv:1506.03340, 2015.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karl Moritz Hermann, Tom's Kocisk, Edward Grefenstette, Lasse Espeholt, Will
    Kay, Mustafa Suleyman, Phil Blunsom, *教机器阅读与理解*，CoRR，arXiv:1506.03340，2015年。
- en: Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, *Recurrent Models
    of Visual Attention*, CoRR, arXiv:1406.6247, 2014.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu, *视觉注意力的递归模型*，CoRR，arXiv:1406.6247，2014年。
- en: Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Tat-Seng Chua, SCA-CNN: *Spatial
    and Channel-wise Attention in Convolutional Networks for Image Captioning*, CoRR,
    arXiv:1611.05594, 2016.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Tat-Seng Chua,
    SCA-CNN: *卷积网络中的空间与通道注意力用于图像描述*，CoRR，arXiv:1611.05594，2016年。'
- en: Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN: *An
    Attention Based Convolutional Neural Network for Visual Question Answering*, CoRR,
    arXiv:1511.05960, 2015.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kan Chen, Jiang Wang, Liang-Chieh Chen, Haoyuan Gao, Wei Xu, Ram Nevatia, ABC-CNN:
    *一种基于注意力的卷积神经网络用于视觉问答*，CoRR，arXiv:1511.05960，2015年。'
- en: Wenpeng Yin, Sebastian Ebert, Hinrich Schutze, *Attention-Based Convolutional
    Neural Network for Machine Comprehension*, CoRR, arXiv:1602.04341, 2016.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wenpeng Yin, Sebastian Ebert, Hinrich Schutze, *基于注意力的卷积神经网络用于机器理解*，CoRR，arXiv:1602.04341，2016年。
- en: Wenpeng Yin, Hinrich Schutze, Bing Xiang, Bowen Zhou, ABCNN: *Attention-Based
    Convolutional Neural Network for Modeling Sentence Pairs*, CoRR, arXiv:1512.05193,
    2015.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Wenpeng Yin, Hinrich Schutze, Bing Xiang, Bowen Zhou, ABCNN: *基于注意力的卷积神经网络用于建模句子对*，CoRR，arXiv:1512.05193，2015年。'
- en: Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alexander J. Smola, *Stacked
    Attention Networks for Image Question Answering*, CoRR, arXiv:1511.02274, 2015.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, Alexander J. Smola, *用于图像问答的堆叠注意力网络*，CoRR，arXiv:1511.02274，2015年。
- en: Y. Chen, D. Zhao, L. Lv and C. Li, *A visual attention based convolutional neural
    network for image classification*, *2016 12th World Congress on Intelligent Control
    and Automation (WCICA)*, Guilin, 2016, pp. 764-769.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Chen, D. Zhao, L. Lv 和 C. Li，*一种基于视觉注意力的卷积神经网络用于图像分类*，*2016年第12届世界智能控制与自动化大会（WCICA）*，桂林，2016年，第764-769页。
- en: H. Zheng, J. Fu, T. Mei and J. Luo, *Learning Multi-attention Convolutional
    Neural Network for Fine-Grained Image Recognition*, *2017 IEEE International Conference
    on Computer Vision (ICCV)*, Venice, 2017, pp. 5219-5227.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H. Zheng，J. Fu，T. Mei 和 J. Luo，*学习多注意力卷积神经网络用于细粒度图像识别*，*2017年IEEE国际计算机视觉会议（ICCV）*，威尼斯，2017年，5219-5227页。
- en: Tianjun Xiao, Yichong Xu, Kuiyuan Yang, Jiaxing Zhang, Yuxin Peng, Zheng Zhang, *The
    Application of Two-level Attention Models in Deep Convolutional Neural Network
    for Fine-grained Image Classification*, CoRR, arXiv:1411.6447, 2014.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 肖天俊、徐一冲、杨奎远、张家兴、彭宇欣、张正，*两级注意力模型在深度卷积神经网络中的应用：用于细粒度图像分类*，CoRR，arXiv:1411.6447，2014年。
- en: Jlindsey15, *A TensorFlow implementation of the recurrent attention model*,
    GitHub, [https://github.com/jlindsey15/RAM](https://github.com/jlindsey15/RAM),
    Feb 2018.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jlindsey15，*循环注意力模型的TensorFlow实现*，GitHub，[https://github.com/jlindsey15/RAM](https://github.com/jlindsey15/RAM)，2018年2月。
- en: QihongL, *A TensorFlow implementation of the recurrent attention model*, GitHub, [https://github.com/QihongL/RAM](https://github.com/QihongL/RAM),
    Feb 2018.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: QihongL，*循环注意力模型的TensorFlow实现*，GitHub，[https://github.com/QihongL/RAM](https://github.com/QihongL/RAM)，2018年2月。
- en: Amasky, *Recurrent Attention Model*, GitHub, [https://github.com/amasky/ram](https://github.com/amasky/ram),
    Feb 2018.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Amasky，*循环注意力模型*，GitHub，[https://github.com/amasky/ram](https://github.com/amasky/ram)，2018年2月。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The attention mechanism is the hottest topic in deep learning today and is conceived
    to be in the center of most of the cutting-edge algorithms under current research,
    and in probable future applications. Problems such as image captioning, visual
    question answering, and many more have gotten great solutions by using this approach.
    In fact, attention is not limited to visual tasks and was conceived earlier for
    problems such as neural machine translations and other sophisticated NLP problems.
    Thus, understanding the attention mechanism is vital to mastering many advanced
    deep learning techniques.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制是当今深度学习中最热门的话题，被认为是当前研究中大多数前沿算法的核心，并且在未来的应用中也可能处于中心地位。像图像描述、视觉问答等问题，已经通过这种方法得到了很好的解决。事实上，注意力机制不仅限于视觉任务，早期也被应用于神经机器翻译等复杂的自然语言处理问题。因此，理解注意力机制对于掌握许多高级深度学习技术至关重要。
- en: CNNs are used not only for vision but also for many good applications with attention
    for solving complex NLP problems, such as **modeling sentence pairs and machine
    translation**. This chapter covered the attention mechanism and its application
    to some NLP problems, along with image captioning and recurrent vision models.
    In RAMs, we did not use CNN; instead, we applied RNN and attention to reduced-size
    representations of an image from the Glimpse Sensor. But there are recent works
    to apply attention to CNN-based visual models as well.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）不仅用于视觉任务，还在许多应用中与注意力机制结合，用于解决复杂的自然语言处理问题，如**建模句子对和机器翻译**。本章介绍了注意力机制及其在一些自然语言处理问题中的应用，以及图像描述和循环视觉模型。在RAM中，我们没有使用CNN，而是将RNN和注意力机制应用于从Glimpse传感器获得的图像缩小表示。然而，最近的研究也开始将注意力机制应用于基于CNN的视觉模型。
- en: Readers are highly encouraged to go through the original papers in the references
    and also explore advanced concepts in using attention, such as multi-level attention,
    stacked attention models, and the use of RL models (such as the **Asynchronous
    Advantage Actor-Critic** (**A3C**) model for the hard attention control problem).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 强烈建议读者参考文献中的原始论文，并探索使用注意力机制的高级概念，如多层次注意力、堆叠注意力模型以及使用RL模型（例如**异步优势行为者-批评家**（**A3C**）模型解决硬注意力控制问题）。
