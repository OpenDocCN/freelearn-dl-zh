- en: Machine Learning for IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网的机器学习
- en: The term **machine learning** (**ML**) refers to computer programs that can
    automatically detect meaningful patterns in data and improve with experience.
    Though it isn't a new field, it's presently at the peak of its hype cycle. This
    chapter introduces the reader to standard ML algorithms and their applications
    in the field of IoT.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）指的是能够自动检测数据中有意义的模式并通过经验不断改进的计算机程序。尽管这不是一个新兴领域，但目前正处于其兴奋周期的顶峰。本章将向读者介绍标准的机器学习算法及其在物联网领域的应用。'
- en: 'After reading this chapter, you will know about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，您将了解以下内容：
- en: What ML is and the role it plays in the IoT pipeline
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习（ML）是什么，它在物联网（IoT）管道中扮演的角色
- en: Supervised and unsupervised learning paradigms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习的范式
- en: Regression and how to perform linear regression using TensorFlow and Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归分析以及如何使用TensorFlow和Keras进行线性回归
- en: Popular ML classifiers and implementing them in TensorFlow and Keras
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行的机器学习分类器，并在TensorFlow和Keras中实现它们
- en: Decision trees, random forests, and techniques to perform boosting and how to
    write code for them
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树、随机森林以及执行提升的技术和如何为它们编写代码
- en: Tips and tricks to improve the system performance and model limitations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升系统性能和模型局限性的技巧与方法
- en: ML and IoT
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与物联网（IoT）
- en: ML, a subset of artificial intelligence, aims to build computer programs with
    an ability to automatically learn and improve from experience without being explicitly
    programmed. In this age of big data, with data being generated at break-neck speed,
    it isn't humanly possible to go through all of the data and understand it manually.
    According to an estimate by Cisco, a leading company in the field of IT and networking,
    IoT will generate 400 zettabytes of data a year by 2018\. This suggests that we
    need to look into automatic means of understanding this enormous data, and this
    is where ML comes in.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）是人工智能的一个子集，旨在构建能够通过经验自动学习和改进的计算机程序，而无需明确编程。在大数据时代，随着数据生成速度惊人地加快，人类无法逐一处理和理解所有数据。根据思科（Cisco）公司的估算，这家公司是IT和网络领域的领先企业，到2018年，物联网（IoT）将每年产生400泽字节的数据。这表明我们需要寻找自动化的手段来理解这些庞大的数据，这正是机器学习的作用所在。
- en: The complete Cisco report, released on February 1, 2018, can be accessed at [https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html).
    It forecasts data traffic and cloud service trends in light of the amalgamation
    of IoT, robotics, AI, and telecommunication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的思科报告，发布于2018年2月1日，可以通过[https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html](https://www.cisco.com/c/en/us/solutions/collateral/service-provider/global-cloud-index-gci/white-paper-c11-738085.html)访问。报告预测了物联网、机器人技术、人工智能和电信结合下的数据流量和云服务趋势。
- en: Every year, Gartner, a research and advisory firm, releases a graphical representation
    providing a visual and conceptual presentation of the maturity of emerging technologies
    through five phases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，Gartner这家研究和咨询公司都会发布一份图示，提供新兴技术成熟度的视觉和概念性展示，分为五个阶段。
- en: You can find the image of *Gartner Hype Cycle for Emerging Technologies* in
    the year 2018 at [https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/](https://www.gartner.com/smarterwithgartner/5-trends-emerge-in-gartner-hype-cycle-for-emerging-technologies-2018/)找到2018年*Gartner新兴技术兴奋周期*的图片。
- en: We can see that both IoT platforms and ML are at the Peak of Inflated Expectations.
    What does it mean? The Peak of Inflated Expectations is the stage in the lifetime
    of technology when there's over enthusiasm about the technology. A large number
    of vendors and startups invest in the technology present at the peak crest. A
    growing number of business establishments explore how the new technology may fit
    within their business strategies. In short, it's the time to jump in to the technology.
    You can hear investors joking at venture fund events that *if you just include
    machine learning in your pitch, you can add a zero on to the end of your valuation*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，物联网平台和机器学习都处于“虚 inflated期”的顶峰。那意味着什么呢？虚 inflated期是技术生命周期中的一个阶段，通常伴随着对技术的过度热情。大量供应商和初创公司投资于处于顶峰的技术。越来越多的商业机构探索新技术如何融入其商业战略。简而言之，这是跳入该技术的时机。你会听到投资者在风险投资活动中开玩笑说，*只要你在推介中提到机器学习，你就可以在估值后面加一个零*。
- en: So, fasten your seat belts and let's dive deeper into ML technology.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，系好安全带，让我们深入探索机器学习技术。
- en: Learning paradigms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习范式
- en: 'ML algorithms can be classified based on the method they use as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以根据其使用的方法进行如下分类：
- en: Probabilistic versus non-probabilistic
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率与非概率
- en: Modeling versus optimization
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模与优化
- en: Supervised versus unsupervised
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: 'In this book, we classify our ML algorithms as supervised versus unsupervised.
    The distinction between these two depends on how the model learns and the type
    of data that''s provided to the model to learn:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将我们的机器学习算法分为监督学习和无监督学习。两者之间的区别取决于模型是如何学习的，以及提供给模型的数据类型：
- en: '**Supervised learning**: Let''s say I give you a series and ask you to predict
    the next element:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：假设我给你一个序列，并要求你预测下一个元素：'
- en: '*(1, 4, 9, 16, 25*,...)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*(1, 4, 9, 16, 25*,...)'
- en: 'You guessed right: the next number will be 36, followed by 49 and so on. This
    is supervised learning, also called **learning by example**; you weren''t told
    that the series represents the square of positive integers—you were able to guess
    it from the five examples provided.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你猜对了：下一个数字是36，接下来是49，依此类推。这就是监督学习，也叫**通过示例学习**；你并没有被告知这个序列表示的是正整数的平方——你通过提供的五个示例自己猜出了这一点。
- en: In a similar manner, in supervised learning, the machine learns from example.
    It's provided with a training data consisting of a set of pairs (*X*, *Y*) where
    *X* is the input (it can be a single number or an input value with a large number
    of features) and *Y* is the expected output for the given input. Once trained
    on the example data, the model should be able to reach an accurate conclusion
    when presented with a new data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在监督学习中，机器从示例中学习。它提供了一组包含(*X*, *Y*)的训练数据，其中*X*是输入（可以是一个单一数字或具有大量特征的输入值），*Y*是给定输入的预期输出。一旦在示例数据上训练完成，模型就应该能够在面对新数据时得出准确的结论。
- en: The supervised learning is used to predict, given set of inputs, either a real-valued
    output (regression) or a discrete label (classification). We'll explore both regression
    and classification algorithms in the coming sections.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习用于根据一组输入预测一个实值输出（回归）或一个离散标签（分类）。我们将在接下来的章节中探讨回归和分类算法。
- en: '**Unsupervised learning**: Let''s say you''re given with eight circular blocks
    of different radii and colors, and you are asked to arrange or group them in an
    order. What will you do?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：假设你得到了八个不同半径和颜色的圆形块，并要求你按照某种顺序排列或分组它们。你会怎么做？'
- en: Some may arrange them in increasing or decreasing order of radii, some may group
    them according to color. There are so many ways, and for each one of us, it will
    be dependent on what internal representation of the data we had while grouping.
    This is unsupervised learning, and a majority of human learning lies in this category.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会根据半径的增减顺序排列它们，有些人可能会根据颜色将它们分组。方法有很多，而对我们每个人来说，这取决于我们在分组时对数据的内部表征。这就是无监督学习，大多数人类的学习也属于这一类别。
- en: In unsupervised learning, the model is just given the data (*X*) but isn't told
    anything about it; the model learns by itself the underlying patterns and relationships
    in the data. Unsupervised learning is normally used for clustering and dimensionality
    reduction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，模型仅给定数据(*X*)，但并未告诉它任何相关信息；模型通过自己学习数据中的潜在模式和关系。无监督学习通常用于聚类和降维。
- en: Though we use TensorFlow for most of the algorithms in this book, in this chapter,
    due to the efficiently built scikit library for ML algorithms, we'll use the functions
    and methods provided by scikit wherever they provide more flexibility and features.
    The aim is to provide you, the reader, with to use AI/ML techniques on the data
    generated by IoT, not to reinvent the wheel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本书中的大部分算法中使用的是TensorFlow，但在本章中，由于scikit库高效地构建了机器学习算法，我们将在提供更多灵活性和功能的情况下使用scikit提供的函数和方法。目标是为你，读者，提供使用AI/ML技术处理物联网生成的数据，而不是重新发明轮子。
- en: Prediction using linear regression
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用线性回归进行预测
- en: Aaron, a friend of mine, is a little sloppy with money and is never able to
    estimate how much his monthly credit card bill will be. Can we do something to
    help him? Well, yes, linear regression can help us to predict a monthly credit
    card bill if we have sufficient data. Thanks to the digital economy, all of his
    monetary transactions for the last five years are available online. We extracted
    his monthly expenditure on groceries, stationery, and travel and his monthly income.
    Linear regression helped not only in predicting his monthly credit card bill,
    it also gave an insight into which factor was most responsible for his spending.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我的朋友Aaron对钱的管理有点马虎，总是无法估算出他的每月信用卡账单会是多少。我们能做点什么来帮助他吗？当然，线性回归可以帮助我们预测每月的信用卡账单，只要我们有足够的数据。得益于数字经济，他过去五年的所有货币交易都可以在网上找到。我们提取了他的每月食品杂货、文具和旅行支出以及他的月收入。线性回归不仅帮助预测了他的月信用卡账单，还揭示了最主要的消费因素。
- en: This was just one example; linear regression can be used in many similar tasks.
    In this section, we'll learn how we can perform linear regression on our data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个例子；线性回归可以用于许多类似的任务。在本节中，我们将学习如何在数据上执行线性回归。
- en: Linear regression is a supervised learning task. It's one of the most basic,
    simple, and extensively used ML techniques for prediction. The goal of regression
    is to find a function *F*(*x, W*), for a given input-output pair (*x*, *y*), so
    that *y* = *F*(*x, W*). In the (*x*, *y*) pair, *x* is the independent variable
    and *y* the dependent variable, and both of them are continuous variables. It
    helps us to find the relationship between the dependent variable *y* and the independent
    variable(s) *x*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一个监督学习任务。它是最基础、最简单且应用广泛的机器学习预测技术之一。回归的目标是为给定的输入输出对(*x*, *y*)找到一个函数*F*(*x,
    W*)，使得*y* = *F*(*x, W*)。在(*x*, *y*)对中，*x*是自变量，*y*是因变量，且它们都是连续变量。它帮助我们找到因变量*y*和自变量*x*之间的关系。
- en: The input *x* can be a single input variable or many input variables. When *F*(*x,
    W*) maps a single input variable *x*, it's called **simple linear regression**;
    for multiple input variables, it's called **multiple linear regression**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输入*x*可以是单一的输入变量，也可以是多个输入变量。当*F*(*x, W*)映射一个单一输入变量*x*时，这叫做**简单线性回归**；当有多个输入变量时，则称为**多元线性回归**。
- en: 'The function *F*(*x, W*) is approximated using the following expression:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*F*(*x, W*)可以通过以下表达式来近似：
- en: '![](img/b866dc12-794c-4ae7-804a-5e1d8ef703e5.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b866dc12-794c-4ae7-804a-5e1d8ef703e5.png)'
- en: 'In this expression, *d* is the dimensions of *x* (number of independent variables),
    and *W* is the weight associated with each component of *x*. To find the function
    *F*(*x, W*), we need to determine the weights. The natural choice is to find the
    weights that reduce the squared error, hence our objective function is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中，*d*是*x*的维度（独立变量的数量），*W*是与*x*的每个分量相关的权重。为了找到函数*F*(*x, W*)，我们需要确定权重。自然的选择是找到能够减少平方误差的权重，因此我们的目标函数如下：
- en: '![](img/c2da2354-6889-4776-96d6-062c18f9b2cf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2da2354-6889-4776-96d6-062c18f9b2cf.png)'
- en: 'In the preceding function, *N* is the total number of the input-output pair
    presented. To find the weights, we differentiate the objective function with respect
    to weight and equate it to *0*. In matrix notation, we can write the solution
    for the column vector *W* = (*W*[0], *W*[1], *W*[2], ..., *W*[d])^T as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的函数中，*N*是所提供的输入输出对的总数。为了找到权重，我们对目标函数关于权重进行求导并使其等于*0*。在矩阵表示法中，我们可以将列向量*W*
    = (*W*[0], *W*[1], *W*[2], ..., *W*[d])^T的解写为如下：
- en: '![](img/9d0fdad2-b84a-4896-868c-3718dc47f9af.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d0fdad2-b84a-4896-868c-3718dc47f9af.png)'
- en: 'On differentiating and simplifying, we get the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过求导并简化，我们得到以下结果：
- en: '![](img/d758faaa-14ab-4347-8655-6c3ed714d269.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d758faaa-14ab-4347-8655-6c3ed714d269.png)'
- en: '*X* is the input vector of size [*N*, *d*] and *Y* the output vector of size
    [*N*, 1]. The weights can be found if (*X^TX*)^(-1) exists, that''s if all of
    the rows and columns of *X* are linearly independent. To ensure this, the number
    of input-output samples (*N*) should be much greater than the number of input
    features (*d*).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*X*是大小为[*N*, *d*]的输入向量，*Y*是大小为[*N*, 1]的输出向量。如果(*X^TX*)^(-1)存在，即*X*的所有行和列是线性独立的，那么可以找到权重。为了确保这一点，输入输出样本的数量（*N*）应该远大于输入特征的数量（*d*）。'
- en: An important thing to remember is that *Y*, the dependent variable, isn't linear
    with respect to the dependent variable X; instead, it's linear with respect to
    the model parameter *W*, the weights. And so we can model relationships such as
    exponential or even sinusoidal (between *Y* and *X*) using linear regression.
    In this case, we generalize the problem to finding weights *W*, so that *y* =
    *F*(*g*(*x*), *W*), where *g*(*x*) is a non-linear function of *X.*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一件重要事情是，*Y*，即因变量，并不是与自变量*X*线性相关的；相反，它是与模型参数*W*，即权重，线性相关的。因此，我们可以通过线性回归建模诸如指数型甚至正弦型的关系（在*Y*和*X*之间）。在这种情况下，我们将问题推广到寻找权重*W*，使得*y*
    = *F*(*g*(*x*), *W*)，其中*g*(*x*)是*X*的非线性函数。
- en: Electrical power output prediction using regression
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归进行电力输出预测
- en: Now that you've understood the basics of linear regression, let's use it to
    predict the electrical power output of a combined cycle power plant. We described
    this dataset in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml), *Principles
    and Foundations of AI and IoT*; here, we'll use TensorFlow and its automatic gradient
    to find the solution. The dataset can be downloaded from the UCI ML archive ([http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant](http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)).
    The complete code is available on GitHub ([https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT))
    under the filename `ElectricalPowerOutputPredictionUsingRegression.ipynb`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了线性回归的基础知识，让我们用它来预测联合循环电厂的电力输出。我们在[第1章](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml)中描述了这个数据集，*人工智能与物联网的原理与基础*；在这里，我们将使用TensorFlow及其自动梯度来找到解决方案。该数据集可以从UCI
    ML档案下载（[http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant](http://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant)）。完整的代码可以在GitHub上找到（[https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT](https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-IoT)），文件名为`ElectricalPowerOutputPredictionUsingRegression.ipynb`。
- en: 'Let''s understand the execution of code in the following steps:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解以下步骤中的代码执行：
- en: 'We import `tensorflow`, `numpy`, `pandas`, `matplotlib`, and some useful functions
    of scikit-learn:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入了`tensorflow`、`numpy`、`pandas`、`matplotlib`和一些`scikit-learn`的有用函数：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The data file is loaded and analyzed:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据文件已加载并进行分析：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Since the data isn''t normalized, before using it, we need to normalize it
    using the `MinMaxScaler` of `sklearn`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据没有标准化，在使用它之前，我们需要使用`sklearn`的`MinMaxScaler`进行标准化：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we define a class, `LinearRegressor`; this is the class where all of the
    real work happens. The class initialization defines the computational graph and
    initializes all of the `Variables` (weights and bias). The class has the `function`
    method, which models the function *y* = *F*(*X*,*W*); the `fit` method performs
    the auto gradient and updates the weights and bias, the `predict` method is used
    to get the output *y* for a given input *X*, and the `get_weights` method returns
    the learned weights and bias:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个类`LinearRegressor`；这是所有实际工作发生的地方。类初始化定义了计算图并初始化所有`Variables`（权重和偏差）。该类有一个`function`方法，用于建模函数*y*
    = *F*(*X*,*W*)；`fit`方法执行自动梯度并更新权重和偏差，`predict`方法用于获取给定输入*X*的输出*y*，`get_weights`方法返回学习到的权重和偏差：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We use the previous class to create our linear regression model and train it:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用前面定义的类来创建线性回归模型并进行训练：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s see the performance of our trained linear regressor. A plot of mean
    square error with **Epochs** shows that the network tried to reach a minimum value
    of mean square error:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看训练好的线性回归器的性能。带有**Epochs**的均方误差图显示，网络试图达到均方误差的最小值：
- en: '![](img/08d71db2-d508-4a8a-a136-ca9176a05f18.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08d71db2-d508-4a8a-a136-ca9176a05f18.png)'
- en: On the test dataset, we achieved an *R²* value of *0.768* and mean square error
    of *0.011*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据集上，我们实现了*R²*值为*0.768*，均方误差为*0.011*。
- en: Logistic regression for classification
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的逻辑回归
- en: 'In the previous section, we learned how to predict. There''s another common
    task in ML: the task of classification. Separating dogs from cats and spam from
    not spam, or even identifying the different objects in a room or scene—all of
    these are classification tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何进行预测。在机器学习中，还有一个常见的任务：分类任务。将狗和猫分开、垃圾邮件和非垃圾邮件分开，甚至识别房间或场景中的不同物体——这些都是分类任务。
- en: 'Logistic regression is an old classification technique. It provides the probability
    of an event taking place, given an input value. The events are represented as
    categorical dependent variables, and the probability of a particular dependent variable
    being *1* is given using the logit function:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种经典的分类技术。它提供了一个事件发生的概率，给定输入值。事件被表示为类别型的因变量，而某一特定因变量为 *1* 的概率通过 logit 函数表示：
- en: '![](img/13aab929-f33c-4e8b-8bdc-33726bf818d8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13aab929-f33c-4e8b-8bdc-33726bf818d8.png)'
- en: 'Before going into the details of how we can use logistic regression for classification,
    let''s examine the logit function (also called the **sigmoid** function because
    of its S-shaped curve). The following diagram shows the logit function and its
    derivative varies with respect to the input *X,* the Sigmoidal function (blue)
    and its derivative (orange):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论如何使用逻辑回归进行分类之前，我们先来看看 logit 函数（也被称为 **sigmoid** 函数，因其 S 形曲线）。下图展示了 logit
    函数及其导数随着输入 *X* 的变化，Sigmoid 函数（蓝色）及其导数（橙色）：
- en: '![](img/ae962533-66ec-4952-997b-524596c97b5f.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae962533-66ec-4952-997b-524596c97b5f.png)'
- en: 'A few important things to note from this diagram are the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从该图中需要注意的几点如下：
- en: The value of sigmoid (and hence *Y[pred]*) lies between (*0*, *1*)
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数的值（因此 *Y[pred]*）介于 (*0*, *1*) 之间。
- en: The derivative of the sigmoid is highest when *W^TX + b = 0.0* and the highest
    value of the derivative is just *0.25* (the sigmoid at same place has a value
    *0.5*)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 *W^TX + b = 0.0* 时，sigmoid 的导数最大，且该导数的最大值为 *0.25*（在同一点上，sigmoid 的值为 *0.5*）
- en: The slope by which the sigmoid varies depends on the weights, and the position
    where we'll have the peak of derivative depends on the bias
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数变化的斜率取决于权重，而我们将得到导数峰值的位置则取决于偏置。
- en: I would suggest you play around with the `Sigmoid_function.ipynb` program available
    at this book's GitHub repository, to get a feel of how the sigmoid function changes
    as the weight and bias changes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你尝试在本书的 GitHub 仓库中使用 `Sigmoid_function.ipynb` 程序，感受当权重和偏置发生变化时，sigmoid 函数如何变化。
- en: Cross-entropy loss function
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉熵损失函数
- en: 'Logistic regression aims to find weights *W* and bias *b*, so that each input
    vector, *X[i]*, in the input feature space is classified correctly to its class,
    *y[i]*. In other words, *y[i]* and *![](img/e1e5c9a3-577d-4910-997f-dbdb5cd0cd59.png)*
    should have a similar distribution for the given *x[i]*. We first consider a binary
    classification problem; in this case, the data point *y[i]* can have value *1*
    or *0*. Since logistic regression is a supervised learning algorithm, we give
    as input the training data pair (*X[i]*, *Y[i]*) and let *![](img/62f0973e-bd28-4c44-9a70-93a322422a4a.png)*
    be the probability that *P*(*y*=*1*|*X*=*X[i]*); then, for *p* training data points,
    the total average loss is defined as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的目标是找到权重 *W* 和偏置 *b*，使得每个输入向量 *X[i]* 在输入特征空间中能够正确分类到其所属的类别 *y[i]*。换句话说，*y[i]*
    和 *![](img/e1e5c9a3-577d-4910-997f-dbdb5cd0cd59.png)* 应该对给定的 *x[i]* 有类似的分布。我们首先考虑一个二分类问题；在这种情况下，数据点
    *y[i]* 的值可以是 *1* 或 *0*。由于逻辑回归是一种监督学习算法，我们将训练数据对 (*X[i]*, *Y[i]*) 作为输入，并让 *![](img/62f0973e-bd28-4c44-9a70-93a322422a4a.png)*
    是 *P*(*y*=*1*|*X*=*X[i]*) 的概率；然后，对于 *p* 个训练数据点，总的平均损失定义如下：
- en: '![](img/f45cc770-d660-465d-bf48-ed5d7d8eef35.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f45cc770-d660-465d-bf48-ed5d7d8eef35.png)'
- en: Hence, for every data pair, for *Y[i]* = *1*, the first term will contribute
    to the loss term, with the contribution changing from infinity to *0* as ![](img/05c8ae04-6675-4cc7-b35a-5caa58800101.png)varies
    from *0* to *1*, respectively. Similarly, for *Y[i]* = *0*, the second term will
    contribute to the loss term, with the contribution changing from infinity to zero
    as ![](img/4295ffc5-6a17-4ece-b58d-ea27a6688761.png)varies from *1* to *0*, respectively.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于每个数据对，当 *Y[i]* = *1* 时，第一项将对损失项做出贡献，且随着 ![](img/05c8ae04-6675-4cc7-b35a-5caa58800101.png)
    从 *0* 到 *1* 变化时，其贡献从无穷大到 *0*；同样地，对于 *Y[i]* = *0* 时，第二项将对损失项做出贡献，且随着 ![](img/4295ffc5-6a17-4ece-b58d-ea27a6688761.png)
    从 *1* 到 *0* 变化时，其贡献从无穷大变为零。
- en: 'For multiclass classification, the loss term is generalized to the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多分类问题，损失项被推广为以下形式：
- en: '![](img/0f18ab66-73bb-4fd7-ac3d-3015a548ef93.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f18ab66-73bb-4fd7-ac3d-3015a548ef93.png)'
- en: In the preceding, *K* is the number of classes. An important thing to note is
    that, while for binary classification the output *Y[i]* and *Y[pred]* were single
    values, for multiclass problems, both *Y[i]* and *Y*[*pred*] are now vectors of
    *K* dimensions, with one component for each category.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述内容中，*K*是类别的数量。需要注意的是，虽然对于二分类问题，输出的*Y[i]*和*Y[pred]*是单一值，但对于多类问题，*Y[i]*和*Y[pred]*现在是*K*维的向量，每个类别有一个分量。
- en: Classifying wine using logistic regressor
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归器分类酒质
- en: 'Let''s now use what we''ve learned to classify wine quality. I can hear you
    thinking: *What wine quality? No way!* Let''s see how our logistic regressor fares
    as compared to professional wine tasters. We''ll be using the wine quality dataset
    ([https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality));
    details about the dataset are given in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundation of AI and IoT*. The full code is in the file named `Wine_quality_using_logistic_regressor.ipynb`
    at the GitHub repository. Let''s understand the code step by step:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们利用所学的知识来分类酒的质量。我能听到你在想：*什么酒质？不可能！*让我们看看我们的逻辑回归器与专业品酒师相比表现如何。我们将使用酒质数据集([https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality))；数据集的详细信息请参见[第1章](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml)，*人工智能与物联网的原理与基础*。完整代码位于GitHub仓库中的`Wine_quality_using_logistic_regressor.ipynb`文件中。让我们一步步理解代码：
- en: 'The first step is loading all of the modules:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是加载所有模块：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We read the data; in the present code, we are analyzing only the red wine,
    so we read data from the `winequality-red.csv` file. The file contains the data
    values separated not by commas, but instead by semicolons, so we need to specify
    the separator argument:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取数据；在当前的代码中，我们只分析红酒，所以我们从`winequality-red.csv`文件中读取数据。该文件中的数据值不是通过逗号分隔的，而是通过分号分隔，因此我们需要指定分隔符参数：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We separate from the data file input features and target quality. In the file,
    the target, wine quality is given on a scale from 0—10\. Here, for simplicity,
    we divide it into three classes, so if the initial quality is less than five,
    we make it the third class (signifying bad); between five and eight, we consider
    it `ok` (second class); and above eight, we consider it `good` (the first class).
    We also normalize the input features and split the data into training and test
    datasets:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据文件中分离出输入特征和目标质量。在文件中，目标酒质以0到10的尺度表示。为了简化处理，我们将其分为三类，因此如果初始质量小于5，我们将其归为第三类（表示差）；在5到8之间，我们认为它是`ok`（第二类）；大于8时，我们认为它是`good`（第一类）。我们还会对输入特征进行归一化，并将数据集分为训练集和测试集：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The main part of the code is the `LogisticRegressor` class; at first glance,
    you''ll think that it''s similar to the `LinearRegressor` class we made earlier.
    The class is defined in the Python file, `LogisticRegressor.py`. It is indeed,
    but there are a few important differences: the `Y` `output` is replaced by `Y[pred]`,
    which instead of having a single value, now is a three-dimensional categorical
    value, each dimension specifying the probability of three categories. The weights
    here have dimensions of *d × n*, where `d` is the number of input features and
    `n` the number of output categories. The bias too now is three-dimensional. Another
    important change is the change in the loss function:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的主要部分是`LogisticRegressor`类；乍一看，你可能会认为它与我们之前做的`LinearRegressor`类相似。该类定义在Python文件`LogisticRegressor.py`中。它确实类似，但有几个重要的不同之处：`Y`
    `output`被`Y[pred]`替代，后者不再是单一值，而是一个三维的分类值，每个维度指定了三类的概率。这里的权重具有*d × n*的维度，其中`d`是输入特征的数量，`n`是输出类别的数量。偏差也变成了三维。另一个重要变化是损失函数的变化：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we simply train our model and predict the output. The learned model gives
    us an accuracy of ~85% on the test dataset. Pretty impressive!
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们只需训练我们的模型并预测输出。学到的模型在测试数据集上的准确率约为85%。相当令人印象深刻！
- en: 'Using ML, we can also identify what ingredients make wine good quality. A company
    called IntelligentX recently started brewing beer based on user feedback; it uses
    AI to get the recipe for the tastiest beer. You can read about the work in this
    *Forbes* article: [https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3](https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习，我们还可以识别哪些成分使酒的质量更好。名为IntelligentX的公司最近开始根据用户反馈酿造啤酒；它使用人工智能来获得最美味的啤酒配方。你可以在这篇*Forbes*文章中了解该项目：[https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3](https://www.forbes.com/sites/emmasandler/2016/07/07/you-can-now-drink-beer-brewed-by-artificial-intelligence/#21fd11cc74c3)。
- en: Classification using support vector machines
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机进行分类
- en: '**Support Vector Machines** (**SVMs**) is arguably the most used ML technique
    for classification. The main idea behind SVM is that we find an optimal hyperplane
    with maximum margin separating the two classes. If the data is linearly separable,
    the process of finding the hyperplane is straightforward, but if it isn''t linearly
    separable, then kernel trick is used to make the data linearly separable in some
    transformed high-dimensional feature space.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVMs**）可以说是最常用的机器学习分类技术。SVM的主要思想是找到一个最大间隔的最优超平面来分隔两个类别。如果数据是线性可分的，那么寻找超平面的过程是直接的，但如果数据不可线性分隔，则使用核技巧将数据转化到某个高维特征空间中，使其线性可分。'
- en: 'SVM is considered a non-parametric supervised learning algorithm. The main
    idea of SVM is to find a **maximal margin separator**: a separating hyperplane
    that is farthest from the training samples presented.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）被认为是一种非参数化的监督学习算法。SVM的主要思想是寻找一个**最大间隔分隔器**：一个与训练样本最远的分隔超平面。
- en: 'Consider the following diagram; the red dots represent class 1 for which the
    output should be 1, and the blue dots represent the class 2 for which the output
    should be -1\. There can be many lines which can separate the red dots from the
    blue ones; the diagram demonstrates three such lines: **A**, **B**, and **C**
    respectively. Which of the three lines do you think will be the best choice? Intuitively,
    the best choice is line B, because it''s farthest from the examples of both classes,
    and hence ensures the least error in classification:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图示；红色的点表示类别1，其输出应为1，蓝色的点表示类别2，其输出应为-1。可以有多条直线将红点与蓝点分开；图示展示了三条这样的直线：**A**、**B**和**C**。你认为哪一条线是最好的选择？直观上，最好的选择是B线，因为它距离两个类别的样本最远，从而确保分类错误最少：
- en: '![](img/900315da-37fc-4a5b-a32f-78220b239342.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/900315da-37fc-4a5b-a32f-78220b239342.png)'
- en: In the following section, we'll learn the basic maths behind finding the maximal-separator
    hyperplane. Though the maths here is mostly basic, if you don't like maths you
    can simply skip to the implementation section where we use SVM to classify wine
    again! Cheers!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习如何寻找最大分隔超平面的基本数学。虽然这里的数学大多是基础的，但如果你不喜欢数学，你可以跳过这部分，直接进入实现部分，看看我们如何再次使用SVM进行酒类分类！干杯！
- en: Maximum margin hyperplane
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大间隔超平面
- en: 'From our knowledge of linear algebra, we know that the equation of a plane
    is given by the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们对线性代数的了解，我们知道平面的方程由以下内容给出：
- en: '![](img/9f9a231d-0111-405f-9996-da15704d42da.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9a231d-0111-405f-9996-da15704d42da.png)'
- en: 'In SVM, this plane should separate the positive classes (*y*= *1*) from the
    negative classes (*y*=*-1*), and there''s an additional constrain: the distance
    (margin) of this hyperplane from the closest positive and negative training vectors
    (*X[pos]* and *X[neg]* respectively) should be maximum. Hence, the plane is called
    the maximum margin separator.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM中，这个平面应该将正类（*y*=*1*）与负类（*y*=*-1*）分开，并且有一个附加约束：这个超平面与最靠近的正负训练向量（*X[pos]*
    和 *X[neg]*）之间的距离（间隔）应该是最大值。因此，这个平面被称为最大间隔分隔器。
- en: The vectors *X*[*pos* ]and *X*[*neg* ]are called **support vectors,** and they
    play an important role in defining the SVM model.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 向量*X*[*pos*]和*X*[*neg*]被称为**支持向量**，它们在定义SVM模型中起着重要作用。
- en: 'Mathematically, this means that the following is true:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度看，这意味着以下内容成立：
- en: '![](img/297a2f4c-fa15-4849-b82f-6966b1dd6eb9.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/297a2f4c-fa15-4849-b82f-6966b1dd6eb9.png)'
- en: 'And, so is this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，以下也是如此：
- en: '![](img/1203bf35-5600-4a16-8e6c-1a320d09f32a.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1203bf35-5600-4a16-8e6c-1a320d09f32a.png)'
- en: 'From these two equations, we get the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两个方程，我们得到以下结果：
- en: '![](img/5a6caae8-8666-4d86-a2fe-409fdb573500.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a6caae8-8666-4d86-a2fe-409fdb573500.png)'
- en: 'Dividing by the weight vector length into both sides, we get the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 将权重向量的长度除以两边，我们得到以下结果：
- en: '![](img/fee17cdc-d92b-4cab-a6bd-bb1b5093899d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fee17cdc-d92b-4cab-a6bd-bb1b5093899d.png)'
- en: 'So we need to find a separator so that the margin between positive and negative
    support vectors is maximum, that is: ![](img/825f71a2-a026-4d06-9122-5a60ef4e70ee.png)
    is maximum, while at the same time all the points are classified correctly, such
    as the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要找到一个分隔器，使得正负支持向量之间的间隔最大，也就是说：![](img/825f71a2-a026-4d06-9122-5a60ef4e70ee.png)最大，同时确保所有点都被正确分类，如下所示：
- en: '![](img/0a4af9f3-b099-47f9-867e-d1003b8823ae.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a4af9f3-b099-47f9-867e-d1003b8823ae.png)'
- en: 'Using a little maths, which we''ll not go into in this book, the preceding
    condition can be represented as finding an optimal solution to the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些数学知识，尽管本书不会深入讲解，前述条件可以表示为以下最优解的求解问题：
- en: '![](img/bef553ea-90e1-4f0b-a4e8-3bfae275d27a.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bef553ea-90e1-4f0b-a4e8-3bfae275d27a.png)'
- en: 'Subject to the constraints that:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 需要满足以下约束条件：
- en: '![](img/4fe746d0-2a30-48cd-9ed6-783275dd1486.png)![](img/f7738f34-ca7f-4970-80ff-2439f0bf2464.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fe746d0-2a30-48cd-9ed6-783275dd1486.png)![](img/f7738f34-ca7f-4970-80ff-2439f0bf2464.png)'
- en: 'From the values of alpha, we can get weights *W* from *α*, the vector of coefficients,
    using the following equation:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从α的值中，我们可以使用以下公式从*α*（系数向量）中得到权重*W*：
- en: '![](img/4098e849-68a2-46d8-8c8a-6ce87f78a05b.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4098e849-68a2-46d8-8c8a-6ce87f78a05b.png)'
- en: This is a standard quadratic programming optimization problem. Most ML libraries
    have built-in functions to solve it, so you need not worry about how to do so.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标准的二次规划优化问题。大多数机器学习库都有内置的函数来解决这个问题，所以你不需要担心如何处理。
- en: For the reader interested in knowing more about SVMs and the math behind it,
    the book *The Nature of Statistical Learning Theory* by Vladimir Vapnik, published
    by *Springer Science+Business Media*, 2013, is an excellent reference.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有兴趣了解更多关于SVM及其背后数学的读者，Vladimir Vapnik的《统计学习理论的本质》（The Nature of Statistical
    Learning Theory），由*Springer Science+Business Media*出版，2013年，是一本非常好的参考书。
- en: Kernel trick
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核技巧
- en: 'The previous method works fine when the input feature space is linearly separable.
    What should we do when it isn''t? One simple way is to transform the data (*X*)
    into a higher dimensional space where it''s linearly separable and find a maximal
    margin hyperplane in that high-dimensional space. Let''s see how; our hyperplane
    in terms of *α* is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入特征空间是线性可分时，上述方法效果很好。那么当输入特征空间不可线性分隔时，我们该怎么办呢？一种简单的方法是将数据（*X*）转换到一个更高维的空间，在这个高维空间中数据是线性可分的，然后在该高维空间中找到一个最大间隔的超平面。让我们来看看；我们关于*α*的超平面如下：
- en: '*![](img/1cfc0edb-71e9-44d3-9b6d-500bfccdbac2.png)*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*![](img/1cfc0edb-71e9-44d3-9b6d-500bfccdbac2.png)*'
- en: Let *φ* be the transform, then we can replace *X* by *φ*(*X*) and hence its
    dot product *X^(T )X^((i))* with a function K(*X^T*, *X*^((*i*))) = *φ*(*X*)*^T*
    *φ*(*X*^((*i*))) called **kernel**. So we now just preprocess the data by applying
    the transform *φ* and then find a linear separator in the transformed space as
    before.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 设*φ*为变换，那么我们可以用*φ*(*X*)替换*X*，从而将其点积*X^(T )X^((i))*替换为一个函数K(*X^T*, *X*^((*i*)))
    = *φ*(*X*)*^T* *φ*(*X*^((*i*)))，这个函数叫做**核**。因此，我们现在只需要通过应用变换*φ*对数据进行预处理，然后在变换后的空间中像之前一样找到一个线性分隔器。
- en: 'The most commonly used kernel function is the **Gaussian kernel**, also called
    **radial basis function**, defined as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的核函数是**高斯核**，也叫**径向基函数**，定义如下：
- en: '![](img/1c95c021-9f0e-4d3f-8f22-7f0425e2e684.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c95c021-9f0e-4d3f-8f22-7f0425e2e684.png)'
- en: Classifying wine using SVM
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM进行葡萄酒分类
- en: We'll use the `svm.SVC` function provided by the scikit library for the task.
    The reason to do so is that the TensorFlow library provides us, as of the time
    of writing, with only a linear implementation of SVM, and it works only for binary
    classification. We can make our own SVM using the maths we learned in previously
    in TensorFlow, and `SVM_TensorFlow.ipynb` in the GitHub repository contains the
    implementation in TensorFlow. The following code can be found in the `Wine_quality_using_SVM.ipynb`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit库提供的`svm.SVC`函数来完成这个任务。这样做的原因是，直到目前为止，TensorFlow库只提供了SVM的线性实现，并且它只适用于二分类。我们可以利用之前在TensorFlow中学到的数学知识自己实现SVM，并且GitHub仓库中的`SVM_TensorFlow.ipynb`包含了TensorFlow中的实现。以下代码可以在`Wine_quality_using_SVM.ipynb`中找到。
- en: 'The SVC classifier of scikit is a support vector classifier. It can also handle
    multiclass support using a one-versus-one scheme. Some of the optional parameters
    of the method are as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: scikit的SVC分类器是一个支持向量分类器。它也可以通过一对一的方式处理多类问题。该方法的一些可选参数如下：
- en: '`C`: It''s a parameter specifying the penalty term (default value is `1.0`).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：它是一个指定惩罚项的参数（默认值为`1.0`）。'
- en: '`kernel`: It specifies the kernel to be used (default is `rbf`). The possible
    choices are `linear`, `poly`, `rbf`, `sigmoid`, `precomputed`, and `callable`.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel`：指定要使用的核函数（默认为`rbf`）。可选值包括`linear`、`poly`、`rbf`、`sigmoid`、`precomputed`和`callable`。'
- en: '`gamma`: It specifies the kernel coefficient for `rbf`, `poly`, and `sigmoid` and
    the default value (the default is `auto`).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：它指定`rbf`、`poly`和`sigmoid`的核函数系数，默认值为`auto`。'
- en: '`random_state`: It sets the seed of the pseudo-random number generator to use
    when shuffling the data.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`：它设置伪随机数生成器的种子，用于数据洗牌时。'
- en: 'Follow the given steps to create our SVM model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤创建我们的SVM模型：
- en: 'Let''s load all of the modules we''ll need for the code. Note that we aren''t
    importing TensorFlow here and instead have imported certain modules from the `scikit`
    library:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载代码中需要的所有模块。请注意，我们没有在这里导入TensorFlow，而是从`scikit`库中导入了某些模块：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We read the data file, preprocess it, and separate it into test and training
    datasets. This time, for simplicity, we''re dividing into two classes, `good`
    and `bad`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们读取数据文件，进行预处理，并将数据分为测试集和训练集。这次，为了简便起见，我们将数据分为两类：`good`和`bad`：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we use the `SVC` classifier and train it on our training dataset with the
    `fit` method:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用`SVC`分类器，并通过`fit`方法在我们的训练数据集上训练它：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let''s now predict the output for the test dataset:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们预测测试数据集的输出：
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The model gave an accuracy of `67.5%` and the confusion matrix is as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型的准确率为`67.5%`，混淆矩阵如下：
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/a5309c9f-56b6-401b-9897-c3259f565a76.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5309c9f-56b6-401b-9897-c3259f565a76.png)'
- en: 'The preceding code uses the binary classification; we can change the code to
    work for more than two classes as well. For example, in the second step, we can
    replace the code with the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用的是二分类；我们可以修改代码，使其适用于多个类别。例如，在第二步中，我们可以将代码替换为以下内容：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we have three categories just as our previous logistic classifier, and
    the accuracy is 65.9%. And the confusion matrix is as follows:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们得到三个类别，就像我们之前的逻辑回归分类器一样，准确率为65.9%。混淆矩阵如下：
- en: 'In the three-class case, the training data distribution is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在三类情况下，训练数据的分布如下：
- en: '`good` `855`'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`good` `855`'
- en: '`ok` `734`'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ok` `734`'
- en: '`bad` `10`'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bad` `10`'
- en: Since the number of samples in the `bad` class (corresponding to `0` in the
    confusion matrix) is only `10`, the model isn't able to learn what parameters
    contribute to bad wine quality. Hence, data should be uniformly distributed among
    all classes of the classifiers that we explore in this chapter.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`bad`类（对应混淆矩阵中的`0`）的样本数量仅为`10`，模型无法学习哪些参数会影响酒的质量。因此，数据应该在我们本章中探索的所有分类器的各个类之间均匀分布。
- en: Naive Bayes
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'Naive Bayes is one of the simplest and fastest ML algorithms. This too belongs
    to the class of supervised learning algorithms. It''s based on the Bayes probability
    theorem. One important assumption that we make in the case of the Naive Bayes
    classifier is that all of the features of the input vector are **independent and
    identically distributed** (**iid**). The goal is to learn a conditional probability
    model for each class *C*[*k*] in the training dataset:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是最简单和最快的机器学习算法之一。它也属于监督学习算法的范畴。它基于贝叶斯概率定理。在朴素贝叶斯分类器的情况下，我们做出的一个重要假设是输入向量的所有特征都是**独立同分布**（**iid**）。目标是为每个类*C*[*k*]学习一个条件概率模型，该模型在训练数据集中进行训练：
- en: '![](img/9796dc8e-c201-401f-b6a8-4427c915c783.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9796dc8e-c201-401f-b6a8-4427c915c783.png)'
- en: 'Under the iid assumption, and using the Bayes theorem, this can be expressed
    in terms of the joint probability distribution *p*(*C[k]*, *X*):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在iid假设下，并使用贝叶斯定理，这可以表示为联合概率分布*p*(*C[k]*, *X*)：
- en: '![](img/11e80a0f-f697-4acb-bc36-98b77039a6bf.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11e80a0f-f697-4acb-bc36-98b77039a6bf.png)'
- en: 'We pick the class that maximizes this term ***Maximum A Posteriori* **(**MAP**):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择最大化此项的类 ***Maximum A Posteriori* **（**MAP**）：
- en: '![](img/4981d980-6d92-4ae7-8acf-a96f36da3805.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4981d980-6d92-4ae7-8acf-a96f36da3805.png)'
- en: There can be different Naive Bayes algorithms, depending upon the distribution
    of *p*(*x[i]*|*C[k]*). The common choices are Gaussian in the case of real-valued
    data, Bernoulli for binary data, and MultiNomial when the data contains the frequency
    of a certain event (such as document classification).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 根据*p*(*x[i]*|*C[k]*)的分布，可能会有不同的朴素贝叶斯算法。常见的选择有：对于实数值数据使用高斯分布，对于二进制数据使用伯努利分布，对于数据中包含某事件频率的情况（例如文档分类）使用多项式分布。
- en: Let's now see whether we can classify the wine using Naive Bayes. For the sake
    of simplicity and efficiency, we'll use the scikit built-in Naive Bayes distributions.
    Since the features values we have in our data are continuous-valued—we'll assume
    that they have a Gaussian distribution, and we'll use `GaussianNB` of scikit-learn.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看是否能使用朴素贝叶斯对葡萄酒进行分类。为了简单和高效起见，我们将使用scikit内置的朴素贝叶斯分布。由于我们数据中的特征值是连续的——我们将假设它们符合高斯分布，因此我们将使用scikit-learn的`GaussianNB`。
- en: Gaussian Naive Bayes for wine quality
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用高斯朴素贝叶斯预测葡萄酒质量
- en: 'The scikit-learn Naive Bayes module supports three Naive Bayes distributions.
    We can choose either of them depending on our input feature data type. The three
    Naive Bayes available in scikit-learn are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的朴素贝叶斯模块支持三种朴素贝叶斯分布。我们可以根据输入特征的数据类型选择其中之一。scikit-learn中可用的三种朴素贝叶斯如下：
- en: '`GaussianNB`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GaussianNB`'
- en: '`MultinomialNB`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultinomialNB`'
- en: '`BernoulliNB`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BernoulliNB`'
- en: The wine data, as we have already seen, is a continuous data type. Hence, it
    will be good if we use Gaussian distribution for *p*(*x[i]*|*C[k]*)—that is, the
    `GaussianNB` module, and so we'll add `from sklearn.naive_bayes import GaussianNB` in
    the import cell of the Notebook. You can read more details about the `GaussianNB`
    module from the is scikit-learn link: [http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，葡萄酒数据是一种连续数据类型。因此，如果我们使用高斯分布来表示*p*(*x[i]*|*C[k]*)——也就是`GaussianNB`模块，那么效果会更好，所以我们需要在Notebook的导入单元中加入`from
    sklearn.naive_bayes import GaussianNB`。你可以通过这个scikit-learn的链接了解更多关于`GaussianNB`模块的详细信息：[http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)。
- en: 'The first two steps will remain the same as in the SVM case. But now, instead
    of declaring an `SVM` classifier, we''ll declare a `GaussianNB` classifier and
    we''ll use its `fit` method to learn the training examples. The result from the
    learned model is obtained using the `predict` method. So follow these steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前两步将与SVM案例中的相同。但现在，我们不会声明一个`SVM`分类器，而是声明一个`GaussianNB`分类器，并使用它的`fit`方法来学习训练样本。通过`predict`方法可以得到从学习的模型中获得的结果。所以，按照以下步骤操作：
- en: 'Import the necessary modules. Note that now we''re importing `GaussianNB` from
    the `scikit` library:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的模块。请注意，现在我们正在从`scikit`库中导入`GaussianNB`：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Read the data file and preprocess it:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据文件并进行预处理：
- en: '[PRE16]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we declare a Gaussian Naive Bayes, train it on the training dataset, and
    use the trained model to predict the wine quality on the test dataset:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们声明一个高斯朴素贝叶斯，使用训练数据集对其进行训练，并使用训练好的模型来预测测试数据集中的葡萄酒质量：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That''s all, folks; our model is ready and kicking. The accuracy of this model
    is 71.25% for the binary classification case. In the following screenshot, you
    can a the heatmap of the confusion matrix:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，大家；我们的模型已经准备好了并开始运作。在二分类的情况下，这个模型的准确率为71.25%。在下面的截图中，你可以看到混淆矩阵的热力图：
- en: '![](img/556bf535-7318-4f9b-9ee5-78f824c7035e.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/556bf535-7318-4f9b-9ee5-78f824c7035e.png)'
- en: 'Before you conclude that Naive Bayes is best, let''s be aware of some of its
    pitfalls:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在你得出朴素贝叶斯是最好的结论之前，我们先要注意它的一些陷阱：
- en: Naive Bayes makes the prediction based on the frequency-based probability; therefore,
    it's strongly dependent on the data we use for training.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是基于频率的概率进行预测的，因此它在很大程度上依赖于我们用于训练的数据。
- en: Another issue is that we made the iid assumption about input feature space;
    this isn't always true.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是，我们假设输入特征空间是独立同分布（iid）的，但这并不总是成立。
- en: Decision trees
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: 'In this section, you''ll learn about another ML algorithm that''s very popular
    and fast—decision trees. In decision trees, we build a tree-like structure of
    decisions; we start with the root, choose a feature and split into branches, and
    continue till we reach the leaves, which represent the predicted class or value.
    The algorithm of decision trees involves two main steps:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将学习另一种非常流行且快速的机器学习算法——决策树。在决策树中，我们构建一个类似树状的决策结构；从根节点开始，选择一个特征并分裂成分支，继续分裂直到到达叶节点，叶节点表示预测的类别或数值。决策树的算法涉及两个主要步骤：
- en: Decide which features to choose and what conditions to use for splitting
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定选择哪些特征以及使用什么条件进行划分
- en: Know when to stop
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道何时停止
- en: 'Let''s understand it with an example. Consider a sample of 40 students; we
    have three variables: the gender (boy or girl; discrete), class (XI or XII; discrete),
    and height (5 to 6 feet; continuous). Eighteen students prefer to go to the library
    in their spare time and rest prefer to play. We can build a decision tree to predict
    who will be going to the library and who will be going to the playground in their
    leisure time. To build the decision tree, we''ll need to separate the students
    who go to library/playground based on the highly significant input variable among
    the three input variables. The following diagram gives the split based on each
    input variable:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个例子来理解。假设有一个由40名学生组成的样本；我们有三个变量：性别（男孩或女孩；离散型），班级（XI 或 XII；离散型），身高（5到6英尺；连续型）。其中18名学生在空闲时间倾向于去图书馆，其他学生则喜欢玩耍。我们可以建立一个决策树来预测哪些学生会去图书馆，哪些学生会去操场。为了构建决策树，我们需要根据三个输入变量中最具显著性的变量来将去图书馆和去操场的学生区分开。下图展示了基于每个输入变量的划分：
- en: '![](img/2bebf484-66d9-4936-83c1-79dd982f6e57.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bebf484-66d9-4936-83c1-79dd982f6e57.png)'
- en: 'We consider all of the features and choose the one that gives us the maximum
    information. In the previous example, we can see that a split over the feature
    height generates the most homogeneous groups, with the group **Height > 5.5 ft**
    containing 80% students who play and 20% who go to the library in the leisure
    time and the group **Height < 5.5 ft** containing 13% students who play and 86%
    who go to the library in their spare time. Hence, we''ll make our first split
    on the feature height. We''ll continue the split in this manner and finally reach
    the decision (leaf node) telling us whether the student will play or go to the
    library in their spare time. The following diagram shows the decision tree structure;
    the black circle is the **Root** **Node**, the blue circles are the **Decision**
    **Nodes**, and the green circles are the **Leaf** **Nodes**:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑所有特征，选择能给我们最大信息量的那个特征。在前面的例子中，我们可以看到，基于身高特征的划分生成了最均匀的组，其中**身高 > 5.5 英尺**的组包含了80%的学生玩耍，20%的学生去图书馆，而**身高
    < 5.5 英尺**的组包含了13%的学生玩耍，86%的学生去图书馆。因此，我们将基于身高特征进行第一次划分。我们将继续以这种方式划分，最终得到决策（叶节点），告诉我们学生在空闲时间是去玩耍还是去图书馆。下图展示了决策树的结构；黑色圆圈是**根**
    **节点**，蓝色圆圈是**决策** **节点**，绿色圆圈是**叶** **节点**：
- en: '![](img/cc3007bb-01fb-4b84-ac7c-9a850c58f8bb.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc3007bb-01fb-4b84-ac7c-9a850c58f8bb.png)'
- en: 'The decision trees belong to the family of greedy algorithms. To find the most
    homogeneous split, we define our cost function so that it tries to maximize the
    same class input values in a particular group. For regression, we generally use
    the mean square error cost function:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树属于贪心算法的一类。为了找到最均匀的划分，我们定义了代价函数，使其尽量在特定组内最大化同类输入值的比例。对于回归问题，我们通常使用均方误差代价函数：
- en: '![](img/89ebe80b-22b8-431f-a110-be4f8f7ce105.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89ebe80b-22b8-431f-a110-be4f8f7ce105.png)'
- en: Here, *y* and *y[pred]* represent the given and predicted output values for
    the input values (*i*); we find the split that minimizes this loss.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 和 *y[pred]* 表示给定的输出值和预测的输出值，分别对应输入值（*i*）；我们找到最小化该损失的划分。
- en: 'For classification, we use either the *gini* impurity or cross-entropy as the
    loss function:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，我们使用*基尼*不纯度或交叉熵作为损失函数：
- en: '![](img/51381f73-fa84-40ff-bf3f-6c81da1bea53.png)![](img/104b5b6f-3a29-4cbd-a8af-358bc8d9e867.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51381f73-fa84-40ff-bf3f-6c81da1bea53.png)![](img/104b5b6f-3a29-4cbd-a8af-358bc8d9e867.png)'
- en: In the preceding, *c[k]* defines the proportion of same class input values present
    in a particular group.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述中，*c[k]* 定义了特定组中同类输入值的比例。
- en: 'Some good resources to learn more about decision trees are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些学习决策树的好资源：
- en: L. Breiman, J. Friedman, R. Olshen, and C. Stone: *Classification and Regression
    Trees,* Wadsworth, Belmont, CA, 1984
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'L. Breiman, J. Friedman, R. Olshen 和 C. Stone: *分类与回归树*，Wadsworth，Belmont，CA，1984'
- en: 'J.R. Quinlan: *C4\. 5: programs for ML,* Morgan Kaufmann, 1993'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'J.R. Quinlan: *C4.5：机器学习程序*，Morgan Kaufmann，1993'
- en: T. Hastie, R. Tibshirani and J. Friedman: *Elements of Statistical Learning*,
    Springer, 2009
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'T. Hastie, R. Tibshirani 和 J. Friedman: *统计学习的元素*，Springer，2009'
- en: Decision trees in scikit
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit 中的决策树
- en: 'The `scikit` library provides `DecisionTreeRegressor` and `DecisionTreeClassifier`
    to implement regression and classification. Both can be imported from `sklearn.tree`.
    `DecisionTreeRegressor` is defined as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit` 库提供了 `DecisionTreeRegressor` 和 `DecisionTreeClassifier` 来实现回归和分类。两者都可以从
    `sklearn.tree` 导入。`DecisionTreeRegressor` 定义如下：'
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The different arguments are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的参数如下：
- en: '`criterion`: It defines which loss function to use to determine the split.
    The default value is mean square error (`mse`). The library supports the use of `friedman_mse` and
    mean absolute error (`mae`) as loss functions.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`：它定义了用于确定拆分的损失函数。默认值是均方误差（`mse`）。该库支持使用 `friedman_mse` 和均值绝对误差（`mae`）作为损失函数。'
- en: '`splitter`: We use this to decide whether to use the greedy strategy and go
    for the best split (default) or we can use random `splitter` to choose the best
    random split.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`splitter`：我们用它来决定是否使用贪婪策略并选择最佳拆分（默认值），或者可以使用随机 `splitter` 来选择最佳随机拆分。'
- en: '`max_depth`: It defines the maximum depth of the tree.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：它定义了树的最大深度。'
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it defines the percentage
    of minimum samples needed for the split).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：它定义了拆分内部节点所需的最小样本数。它可以是整数或浮动数（在这种情况下，它定义了拆分所需的最小样本百分比）。'
- en: '`DecisionTreeClassifier` is defined as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 定义如下：'
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The different arguments are as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的参数如下：
- en: '`criterion`: It tells which loss function to use to determine the split. The
    default value for the classifier is the `gini`. The library supports the use of `entropy`
    as loss functions.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`：它指示用于确定拆分的损失函数。分类器的默认值是 `gini`。该库支持使用 `entropy` 作为损失函数。'
- en: '`splitter`: We use this to decide how to choose the split (default value is
    the best split) or we can use random `splitter` to choose the best random split.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`splitter`：我们用它来决定如何选择拆分（默认值是最佳拆分），或者可以使用随机 `splitter` 来选择最佳随机拆分。'
- en: '`max_depth`: It defines the maximum depth of the tree. When the input feature
    space is large, we use this to restrict the maximum depth and take care of overfitting.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：它定义了树的最大深度。当输入特征空间很大时，我们用它来限制最大深度并避免过拟合。'
- en: '`min_samples_split`: It defines the minimum number of samples required to split
    an internal node. It can be integer or float (in this case it tells the percentage
    of minimum samples needed for the split).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：它定义了拆分内部节点所需的最小样本数。它可以是整数或浮动数（在这种情况下，它指示拆分所需的最小样本百分比）。'
- en: 'We''ve listed only the commonly used preceding arguments; details regarding
    the remaining parameters of the two can be read on the scikit-learn website: [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) and [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅列出了常用的前导参数；有关两个参数的其余细节，请参见 scikit-learn 网站：[http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
    和 [http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
- en: Decision trees in action
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的实际应用
- en: 'We''ll use a decision tree regressor to predict electrical power output first.
    The dataset and its description have already been introduced in [Chapter 1](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml),
    *Principles and Foundations of IoT and AI*. The code is available at the GitHub repository
    in the file named `ElectricalPowerOutputPredictionUsingDecisionTrees.ipynb`:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用决策树回归器来预测电力输出。数据集及其描述已在 [第1章](fa0444a6-ce5c-4bed-8d9d-ddab846fe571.xhtml)，*物联网和人工智能的原理与基础*
    中介绍。代码可在 GitHub 仓库中的文件 `ElectricalPowerOutputPredictionUsingDecisionTrees.ipynb`
    中找到：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We get an R-square value of 0.90 and mean square error of 0.0047 on the test
    data; it''s a significant improvement over the prediction results obtained using
    linear regressor (R-square: `0.77`;mse: `0.012`).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上，我们得到了0.90的R平方值和0.0047的均方误差；这相比使用线性回归器的预测结果（R平方:`0.77`; mse:`0.012`）有了显著的提高。
- en: 'Let''s also see the performance of decision trees in the classification task;
    we use it for the wine quality classification as before. The code is available
    in the `Wine_quality_using_DecisionTrees.ipynb` file in the GitHub repository:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看决策树在分类任务中的表现；我们依旧使用葡萄酒质量分类。代码可在GitHub仓库中的`Wine_quality_using_DecisionTrees.ipynb`文件中找到：
- en: '[PRE21]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The decision tree generates a classification accuracy of around 70%. We can
    see that, for small data size, we can use both decision trees and Naive Bayes
    with almost equal success. Decision trees suffer from overfitting, which can be
    taken care of by restricting the maximum depth or setting a minimum number of
    training inputs. They, like Naive Bayes, are unstable—a little variation in the
    data can result in a completely different tree; this can be resolved by making
    use of bagging and boosting techniques. Last, but not least, since it's a greedy
    algorithm, there's no guarantee that it returns a globally optimal solution.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树产生的分类准确率约为70%。我们可以看到，对于小规模数据，决策树和朴素贝叶斯几乎可以同样成功。决策树容易过拟合，可以通过限制最大深度或设置最小训练输入数量来解决。像朴素贝叶斯一样，决策树也是不稳定的——数据的微小变化可能会导致完全不同的树结构；这可以通过使用装袋法和提升法来解决。最后，由于它是贪心算法，无法保证它能返回全局最优解。
- en: Ensemble learning
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成学习
- en: In our daily life, when we have to make a decision, we take guidance not from
    one person, but from many individuals whose wisdom we trust. The same can be applied
    in ML; instead of depending upon one single model, we can use a group of models
    (ensemble) to make a prediction or classification decision. This form of learning
    is called **ensemble learning**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的日常生活中，当我们需要做出决定时，我们不仅依赖一个人的指导，而是依赖许多我们信任的智者。在机器学习中也可以应用同样的思想；我们可以使用一组模型（集成）来进行预测或分类，而不是依赖于单一模型。这种学习方式称为**集成学习**。
- en: 'Conventionally, ensemble learning is used as the last step in many ML projects.
    It works best when the models are as independent of one another as possible. The
    following diagram gives a graphical representation of ensemble learning:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，集成学习作为许多机器学习项目中的最后一步使用。当各模型尽可能独立时，它的效果最佳。以下图展示了集成学习的图示：
- en: '![](img/73af0f0f-00be-4082-a4a5-7d4540b67e24.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73af0f0f-00be-4082-a4a5-7d4540b67e24.png)'
- en: 'The training of different models can take place either sequentially or in parallel.
    There are various ways to implement ensemble learning: voting, bagging and pasting,
    and random forest. Let''s see what each of these techniques and how we can implement
    them.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 不同模型的训练可以是顺序进行，也可以是并行进行。集成学习有多种实现方式：投票法、装袋法、拼接法和随机森林。让我们来看看这些技术是什么，以及如何实现它们。
- en: Voting classifier
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投票分类器
- en: 'The voting classifier follows the majority; it aggregates the prediction of
    all the classifiers and chooses the class with maximum votes. For example, in
    the following screenshot, the voting classifier will predict the input instance
    to belong to class **1**:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 投票分类器遵循多数规则；它会汇总所有分类器的预测，并选择票数最多的类别。例如，在下面的截图中，投票分类器将预测输入实例属于类别**1**：
- en: '![](img/2f60022e-2236-4291-a411-268ed5152367.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f60022e-2236-4291-a411-268ed5152367.png)'
- en: 'scikit has the `VotingClassifier` class to implement this. Using ensemble learning
    on wine quality classification, we reach an accuracy score of 74%, higher than
    any of the models considered alone. The complete code is in the `Wine_quality_using_Ensemble_learning.ipynb`
    file. The following is the main code to perform ensemble learning using voting:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn提供了`VotingClassifier`类来实现这一功能。通过在葡萄酒质量分类中使用集成学习，我们达到了74%的准确度，超过了单独使用任何模型的结果。完整代码在`Wine_quality_using_Ensemble_learning.ipynb`文件中。以下是使用投票法进行集成学习的主要代码：
- en: '[PRE22]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Bagging and pasting
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装袋法与拼接法
- en: 'In voting, we used different algorithms for training on the same dataset. We
    can also achieve ensemble learning by using different models with the same learning
    algorithm, but we train them on different training data subsets. The training
    subset is sampled randomly. The sampling can be done with replacement (bagging)
    or without replacement (pasting):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在投票法中，我们使用不同的算法对相同的数据集进行训练。我们也可以通过使用不同的模型与相同的学习算法来实现集成学习，但我们将它们训练在不同的训练数据子集上。训练子集是随机抽样的。抽样可以是有放回的（袋装法）或无放回的（拼接法）：
- en: '**Bagging**: In it, additional data for training is generated from the original
    dataset using combinations with repetitions. This helps in decreasing the variance
    of different models.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**袋装法**：在袋装法中，额外的训练数据是通过对原始数据集进行带重复的组合生成的。这有助于减少不同模型的方差。'
- en: '**Pasting**: Since pasting is without replacement, each subset of the training
    data can be used at most once. It''s more suitable if the original dataset is
    large.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拼接法**：由于拼接法是无放回的，因此每个训练数据的子集最多只能使用一次。如果原始数据集较大，拼接法更为合适。'
- en: 'The `scikit` library has a method for performing bagging and pasting; from
    `sklearn.ensemble`, we can import `BaggingClassifier` and use it. The following
    code estimates `500` decision tree classifiers, each with `1000` training samples
    using bagging (for pasting, keep `bootstrap=False`):'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit` 库提供了执行袋装法（bagging）和拼接法（pasting）的方法；通过 `sklearn.ensemble`，我们可以导入 `BaggingClassifier`
    并使用它。以下代码使用袋装法估算 `500` 个决策树分类器，每个分类器使用 `1000` 个训练样本（对于拼接法，将 `bootstrap=False`）：'
- en: '[PRE23]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This results in an accuracy of 77% for wine quality classification. The last
    argument to `BaggingClassifier`, `n_jobs`, defines how many CPU cores to use (that's
    the number of jobs to run in parallel); when its value is set to `-1`, then it
    uses all of the available CPU cores.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致葡萄酒质量分类的准确率为 77%。`BaggingClassifier` 的最后一个参数 `n_jobs` 定义了使用多少个 CPU 核心（即并行运行的任务数）；当它的值设置为
    `-1` 时，将使用所有可用的 CPU 核心。
- en: An ensemble of only decision trees is called **random forest**. And so what
    we've implemented previously is a random forest. We can directly implement random
    forest in scikit using the `RandomForestClassifier` class. The advantage of using
    the class is that it introduces extra randomness while building the tree. While
    splitting, it searches for the best feature to split among a random subset of
    features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 只有决策树的集成被称为 **随机森林**。因此，我们之前实现的就是一个随机森林。我们可以直接在 scikit 中使用 `RandomForestClassifier`
    类实现随机森林。使用该类的优势在于它在构建树时引入了额外的随机性。在分裂时，它会在随机子集的特征中寻找最佳的分裂特征。
- en: Improving your model – tips and tricks
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升你的模型 – 提示与技巧
- en: In this chapter, we've learned a large number of ML algorithms, each with its
    own pros and cons. In this section, we'll look into some common problems and ways
    to resolve them.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了大量的机器学习算法，每种算法都有其优缺点。在本节中，我们将探讨一些常见的问题及其解决方法。
- en: Feature scaling to resolve uneven data scale
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过特征缩放解决不均匀的数据尺度问题
- en: 'The data that''s collected normally doesn''t have the same scale; for example,
    one feature may be varying in the range 10–100 and another one may be only distributed
    in range 2–5\. This uneven data scale can have an adverse effect on learning.
    To resolve this, we use the method of feature scaling (normalization). The choice
    of normalization has been found to drastically affect the performance of certain
    algorithms. Two common normalization methods (also called standardization in some
    books) are as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到的数据通常没有相同的尺度；例如，一个特征可能在 10 到 100 之间变化，而另一个特征则可能仅在 2 到 5 之间分布。这种不均匀的数据尺度可能对学习产生不利影响。为了解决这个问题，我们使用特征缩放（标准化）方法。标准化方法的选择已被发现会极大地影响某些算法的性能。两种常见的标准化方法（在某些书籍中也称为标准化）如下：
- en: '**Z-score normalization**: In z-score normalization, each individual feature
    is scaled so that it has the properties of a standard normal distribution, that
    is, a mean of *0* and variance of *1*. If *μ* is the mean and *σ* the variance,
    we can compute Z-score normalization by making the following linear transformation
    on each feature as follows:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Z-score 标准化**：在 Z-score 标准化中，每个单独的特征都会被缩放，使其具备标准正态分布的特性，即均值为 *0*，方差为 *1*。如果
    *μ* 是均值，*σ* 是方差，我们可以通过对每个特征进行如下线性变换来计算 Z-score 标准化：'
- en: '![](img/40dddd5e-f8af-4ab1-8b59-5a708d51b61b.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40dddd5e-f8af-4ab1-8b59-5a708d51b61b.png)'
- en: '**Min-max normalization**: The min-max normalization rescales the input features
    so that they lie in the range between *0* and *1*. It results in reducing the
    standard deviation in the data and hence suppresses the effect of outliers. To
    achieve min-max normalization, we find the maximum and minimum value of the feature
    (*x[max]* and *x[min]* respectively), and perform the following linear transformation:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小-最大归一化**：最小-最大归一化将输入特征重新缩放，使其位于*0*和*1*之间。这会减少数据的标准差，从而抑制异常值的影响。为了实现最小-最大归一化，我们找到特征的最大值和最小值（分别为*x[max]*和*x[min]*），然后执行以下线性变换：'
- en: '![](img/724c7e3c-3d4d-4643-a5b5-bf3c29c82b30.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/724c7e3c-3d4d-4643-a5b5-bf3c29c82b30.png)'
- en: We can use the `scikit` library `StandardScaler` or `MinMaxscaler` methods to
    normalize the data. In all of the examples in this chapter, we've used `MinMaxScaler`;
    you can try changing it to `StandardScalar` and observe if the performance changes.
    In the next chapter, we'll also learn how to perform these normalizations in TensorFlow.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit`库中的`StandardScaler`或`MinMaxScaler`方法对数据进行归一化。在本章中的所有示例中，我们使用了`MinMaxScaler`；你可以尝试将其更改为`StandardScaler`并观察性能是否有所变化。在下一章中，我们还将学习如何在TensorFlow中执行这些归一化操作。
- en: Overfitting
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'Sometimes the model tries to overfit the training dataset; in doing so, it
    loses its ability to generalize and hence performs badly on the validation dataset;
    this in turn will affect its performance on unseen data values. There are two
    standard ways to take care of overfitting: regularization and cross-validation.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，模型会尝试过拟合训练数据集；这样做会失去其泛化能力，因此在验证数据集上的表现较差；这反过来会影响模型在未见数据上的表现。有两种标准方式可以解决过拟合问题：正则化和交叉验证。
- en: Regularization
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: 'Regularization adds a term in the loss function to ensure that the cost increases
    as the model increases the number of features. Hence, we force the model to stay
    simpler. If *L(X*, *Y)* was the loss function earlier, we replace it with the
    following:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化在损失函数中添加了一项，确保当模型增加特征数时，代价会增加。因此，我们强制模型保持简单。如果*L(X*, *Y)*是之前的损失函数，我们将其替换为以下公式：
- en: '![](img/7c6cd641-a45d-4cb1-abc4-902e2e23e1ac.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c6cd641-a45d-4cb1-abc4-902e2e23e1ac.png)'
- en: 'In the preceding, *N* can be *L*[1] norm, *L*[2] norm, or a combination of
    the two, and *λ* is the regularization coefficient. Regularization helps in reducing
    the model variance, without losing any important properties of the data distribution:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面，*N*可以是*L*[1]范数、*L*[2]范数或两者的组合，*λ*是正则化系数。正则化有助于减少模型的方差，同时不失去数据分布的任何重要属性：
- en: '**Lasso regularization**: In this case, the *N* is *L*[1] norm. It uses the
    modulus of weight as the penalty term *N:*'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**套索回归正则化**：在这种情况下，*N*是*L*[1]范数。它使用权重的模作为惩罚项*N*：'
- en: '![](img/76d82420-39b8-4956-bc07-a4fc8d6c1000.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76d82420-39b8-4956-bc07-a4fc8d6c1000.png)'
- en: '**Ridge regularization**: In this case, the *N* is *L2* norm, given by the
    following:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**岭回归正则化**：在这种情况下，*N*是*L2*范数，表示为以下公式：'
- en: '![](img/c03bc8ca-e933-4a96-bd00-4bfc0c492b73.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c03bc8ca-e933-4a96-bd00-4bfc0c492b73.png)'
- en: Cross-validation
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Using cross-validation can also help in reducing the problem of overfitting.
    In *k*-fold cross-validation, data is divided into *k*-subsets, called **folds**.
    Then it trains and evaluates the model *k*-times; each time, it picks one of the
    folds for validation and the rest for training the model. We can perform the cross-validation
    when the data is less and training time is small. scikit provides a `cross_val_score`
    method to implement the k-folds. Let `classifier` be the model we want to cross-validate,
    then we can use the following code to perform cross-validation on `10` folds:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证还可以帮助减少过拟合问题。在*k*-折交叉验证中，数据被分成*k*个子集，称为**折**。然后，它训练和评估模型*k*次；每次，选择一个折进行验证，其余部分用于训练模型。当数据较少且训练时间较短时，我们可以执行交叉验证。scikit提供了一个`cross_val_score`方法来实现k折交叉验证。假设`classifier`是我们要交叉验证的模型，那么我们可以使用以下代码在`10`折上执行交叉验证：
- en: '[PRE24]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The result of this is an average mean and variance value. A good model should
    have a high average and low variance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个平均均值和方差值。一个好的模型应该有较高的平均值和较低的方差。
- en: No Free Lunch theorem
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无免费午餐定理
- en: With so many models, one always wonders which one to use. Wolpert, in his famous
    paper *The Lack of A Priori Distinctions Between Learning*, explored this issue
    and showed that if we make no prior assumption about the input data, then there's
    no reason to prefer one model over any other. This is known as the **No Free Lunch**
    **theorem**.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 面对如此多的模型，人们总是会想，应该使用哪个模型。沃尔珀特（Wolpert）在他著名的论文《学习中缺乏先验区分》中探讨了这个问题，并证明了如果我们对输入数据没有任何先验假设，那么就没有理由偏向某一个模型。这就是**无免费午餐定理**。
- en: This means that there's no model hat can be *a* priori guaranteed to work better.
    The only way we can ascertain which model is best is by evaluating them all. But,
    practically, it isn't possible to evaluate all of the models and so, in practice,
    we make reasonable assumptions about the data and evaluate a few relevant models.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着没有任何模型可以*先验*地保证其表现更好。我们能确定哪个模型最好的唯一方法是评估它们所有。但实际上，评估所有模型是不可能的，因此在实际操作中，我们会对数据做出合理的假设，并评估一些相关的模型。
- en: Hyperparameter tuning and grid search
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整和网格搜索
- en: 'Different models have different hyperparameters; for example, in linear regressor,
    the learning rate was a hyperparameter; if we''re using regularization, then the
    regularizing parameter λ is a hyperparameter. What should be their value? While
    there''s a rule of thumb for some hyperparameters, most of the time we make either
    a guess or use grid search to perform a sequential search for the best hyperparameters.
    In the following, we present the code to perform hyperparameter search in the
    case of SVM using the `scikit` library; in the next chapter, we''ll see how we
    can use TensorFlow to perform hyperparameter tuning:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的模型有不同的超参数；例如，在线性回归中，学习率就是一个超参数；如果我们使用正则化，那么正则化参数λ就是超参数。它们的值应该是多少？虽然有些超参数有经验法则，但大多数时候我们只能做出猜测，或者使用网格搜索来进行有序的搜索，以找到最佳的超参数。接下来，我们将展示使用`scikit`库在支持向量机（SVM）中进行超参数搜索的代码；在下一章中，我们将看到如何使用TensorFlow进行超参数调整：
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`GridSearchCV` will provide us with the hyperparameters that produce the best
    results for the SVM classifier.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`将为我们提供产生最佳结果的支持向量机分类器的超参数。'
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The goal of this chapter was to provide you with  intuitive understanding of
    different standard ML algorithms so that you can make an informed choice. We covered
    the popular ML algorithms used for classification and regression.We also learnt
    how supervised and unsupervised learning are different from each other. Linear
    regression, logistic regression, SVM, Naive Bayes, and decision trees were introduced
    along with the fundamental principles involved in each. We used the regression
    methods to predict electrical power production of a thermal station and classification
    methods to classify wine as good or bad. Lastly, we covered the common problems
    with different ML algorithms and some tips and tricks to solve them.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为你提供不同标准机器学习算法的直观理解，以便你能够做出明智的选择。我们介绍了用于分类和回归的流行机器学习算法。我们还学习了有监督学习和无监督学习之间的区别。我们介绍了线性回归、逻辑回归、支持向量机（SVM）、朴素贝叶斯和决策树，并探讨了每种算法所涉及的基本原理。我们使用回归方法预测了热电站的电力生产，并使用分类方法将葡萄酒分类为好或坏。最后，我们讨论了不同机器学习算法的常见问题以及一些解决这些问题的技巧。
- en: In the next chapter, we'll study different deep learning models and learn how
    to use them to analyze our data and make predictions.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究不同的深度学习模型，并学习如何使用它们来分析我们的数据并做出预测。
