- en: Predicting Future Stock Prices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测未来股价
- en: The financial market is a very important part of any economy. For an economy
    to thrive, its financial market must be solid. Since the advent of machine learning,
    companies have begun to adopt algorithmic trading in the purchase of stocks and
    other financial assets. There has been proven successful with this method, and
    it has risen in prominence over time. Given its rise, several machine models have
    been developed and adopted for algorithmic trading. One popular machine learning
    model for trading is the time series analysis. You have already learned about
    reinforcement learning and Keras, and in this chapter, they will be used to develop
    a model that can predict stock prices.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 金融市场是任何经济的重要组成部分。一个经济要繁荣，其金融市场必须稳固。自从机器学习的出现以来，许多公司开始在股票和其他金融资产的购买中采用算法交易。这个方法已被证明是成功的，并且随着时间的推移而变得更加重要。由于其迅速崛起，已经开发并采用了多种机器学习模型来进行算法交易。一种流行的用于交易的机器学习模型是时间序列分析。你已经学习过强化学习和Keras，在本章中，它们将被用来开发一个可以预测股价的模型。
- en: Background problem
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景问题
- en: Automation is taking over in almost every sector, and the financial market is
    no exception. Creating automated algorithmic trading models will provide for a
    faster and more accurate analysis of stocks before purchase. Multiple indicators
    can be analyzed at a speed that humans are incapable of. Also, in trading, it
    is dangerous to operate with emotions. Machine learning models can solve that
    problem. There is also a reduction in transaction costs, as there is no need for
    continuous supervision.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化几乎渗透到了每个行业，金融市场也不例外。创建自动化的算法交易模型可以在购买前对股票进行更快速、更准确的分析。多个指标可以以人类无法达到的速度进行分析。而且，在交易中，情绪化的操作是危险的。机器学习模型可以解决这个问题。同时，交易成本也减少了，因为不再需要持续的监督。
- en: In this tutorial, you will learn how to combine reinforcement learning with
    time series modeling, in order to predict the prices of stocks, based on real-life
    data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你将学习如何将强化学习与时间序列建模结合起来，以便基于真实数据预测股票价格。
- en: Data used
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的数据
- en: The data that we will use will be the standard and poor's 500\. According to
    Wikipedia, it is *An American stock market index based on the market capitalizations of
    500 large companies having common stock listed on the NYSE or NASDAQ.* Here is
    a link to the data ([https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC](https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据是标准普尔500指数。根据维基百科，它是 *基于500家大型公司的市值的美国股票市场指数，这些公司在纽约证券交易所或纳斯达克上市并拥有普通股。*
    这里是数据的链接（[https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC](https://ca.finance.yahoo.com/quote/%255EGSPC/history?p=%255EGSPC)）。
- en: 'The data has the following columns:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包含以下列：
- en: '**Date**: This indicates the date under consideration'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**日期**：这表示考虑的日期'
- en: '**Open**: This indicates the price at which the market opens on the date'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开盘价**：这表示当天市场开盘时的价格'
- en: '**High**: This indicates the highest market price on the date'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最高价**：这表示当天市场的最高价格'
- en: '**Low**: This indicates the lowest market price on the date'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**低价**：这表示当天市场的最低价格'
- en: '**Close**: This indicates the price at which the market closes on the date,
    adjusted for the split'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收盘价**：这表示当天市场收盘时的价格，经过拆股调整'
- en: '**Adj Close**: This indicates the adjusted closing price for both the split
    and dividends'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**调整后收盘价**：这表示经过拆股和分红调整后的收盘价'
- en: '**Volume**: This indicates the total volume of shares available'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**交易量**：这表示可用的股票总量'
- en: 'The date under consideration for training the data is as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练数据的日期如下：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'On the website, filter the date as follows, and download the dataset:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站上，按以下方式筛选日期，并下载数据集：
- en: '![](img/cd47755a-4b1b-423b-802d-a4121658a46e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd47755a-4b1b-423b-802d-a4121658a46e.png)'
- en: 'For testing, we will use the following date range:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试，我们将使用以下日期范围：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Change the dates on the website accordingly, and download the dataset for testing,
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地更改网站上的日期，并下载用于测试的数据集，如下所示：
- en: '![](img/f9367771-52bb-4db0-8eda-5a61cf78a297.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9367771-52bb-4db0-8eda-5a61cf78a297.png)'
- en: In the next section, we will define some possible actions that the agent can
    carry out.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将定义一些代理可以执行的可能操作。
- en: Step-by-step guide
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分步指南
- en: 'Our solution uses an actor-critic reinforcement learning model, along with
    an infused time series, to help us predict the best action, based on the stock
    prices. The possible actions are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案使用了一个基于演员-评论员的强化学习模型，并结合了时间序列，帮助我们基于股票价格预测最佳操作。可能的操作如下：
- en: '**Hold**: This means that based on the price and projected profit, the trader
    should hold a stock'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**持有**：这意味着根据价格和预期利润，交易者应该持有股票。'
- en: '**Sell**: This means that based on the price and projected profit, the trader
    should sell a stock'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卖出**：这意味着根据价格和预期利润，交易者应该卖出股票。'
- en: '**Buy**: This means that based on the price and projected profit, the trader
    should buy a stock'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**购买**：这意味着根据价格和预期利润，交易者应该购买股票。'
- en: 'The actor-critic network is a family of reinforcement learning methods premised
    on two interacting network models. These models have two components: the actor
    and the critic. In our case, the network models that we will use will be neural
    networks. We will use the Keras package, which you have already learned about,
    to create the neural networks. The reward function that we are looking to improve
    is the profit.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论员网络是一类基于两个交互网络模型的强化学习方法。这些模型有两个组成部分：演员和评论员。在我们的案例中，我们将使用神经网络作为网络模型。我们将使用你已经学过的Keras包来创建神经网络。我们希望改进的奖励函数是利润。
- en: The actor takes in the state of the environment, then returns the best action,
    or a policy that refers to a probability distribution over actions. This seems
    like a natural way to perform reinforcement learning, as policies are directly
    returned as a function of the state.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 演员接受环境状态，然后返回最佳动作，或者返回一个指向动作的概率分布的策略。这似乎是执行强化学习的一种自然方式，因为策略是作为状态的函数直接返回的。
- en: The critic evaluates the actions returned by the actor-network. This is similar
    to the traditional deep Q network; in the environment state and an action to return
    a score representing the value of taking that action given the state. The job
    of the critic is to compute an approximation, which is then used to update the
    actor in the direction of its gradient. The critic is trained itself temporal
    difference algorithm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 评论员评估由演员网络返回的动作。这类似于传统的深度Q网络；在环境状态和一个动作下，返回一个分数，表示在给定状态下采取该动作的价值。评论员的工作是计算一个近似值，然后用它来根据梯度更新演员。评论员本身是通过时序差分算法进行训练的。
- en: These two networks are trained simultaneously. With time, the critic network
    is able to improve its `Q_value` prediction, and the actor also learns how to
    make better decisions, given the state.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络是同时训练的。随着时间的推移，评论员网络能够改善其`Q_value`预测，演员也学会了如何根据状态做出更好的决策。
- en: There are five scripts that make up this solution, and they will be described
    in the next sections.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案由五个脚本组成，它们将在接下来的章节中描述。
- en: Actor script
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员脚本
- en: 'The actor script is where the policy model is defined. We begin by importing
    certain modules from Keras: layers, optimizers, models, and the backend. These
    modules will help us to construct our neural network: Let''s start by importing
    the required functions from Keras.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 演员脚本是在这里定义策略模型的。我们首先从Keras中导入某些模块：layers、optimizers、models和backend。这些模块将帮助我们构建神经网络：让我们从导入Keras的所需函数开始。
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We create a class called `Actor`, whose object takes in the parameters of the
    `state` and `action` size:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`Actor`的类，它的对象接受`state`和`action`大小的参数：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code shows the state size, which represents the dimension of
    each state, and the action size, which represents the dimensions of the actions.
    Next, call a function to build the model, as follows:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码展示了状态大小，表示每个状态的维度，以及动作大小，表示动作的维度。接下来，调用一个函数来构建模型，如下所示：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Build a policy model that maps the states to actions, and start by defining
    the input layer, as follows:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个策略模型，将状态映射到动作，并从定义输入层开始，如下所示：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add hidden layers to the model. There are two dense layers, each one followed
    by a batch normalization and an activation layer. The dense layers are regularized.
    The two layers have 16 and 32 hidden units, respectively:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型中添加隐藏层。共有两层密集层，每一层后面跟着批归一化和激活层。这些密集层是正则化的。两层分别有16个和32个隐藏单元：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The final output layer will predict the action probabilities that have an activation
    of `softmax`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终的输出层将预测具有`softmax`激活函数的动作概率：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the loss function by using the action value (`Q_value`) gradients, as
    follows:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用动作值（`Q_value`）的梯度来定义损失函数，如下所示：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the `optimizer` and training function, as follows:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`optimizer`和训练函数，如下所示：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The custom training function for the actor-network that makes use of the Q gradients
    with respect to the action probabilities. With this custom function, the training
    aims to maximize the profits (in other words, minimize the negatives of the `Q_values`).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 演员网络的自定义训练函数，利用与动作概率相关的 Q 梯度进行训练。通过这个自定义函数，训练的目标是最大化利润（换句话说，最小化`Q_values`的负值）。
- en: Critic script
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评论家脚本
- en: 'We begin by importing certain modules from Keras: layers, optimizers, models,
    and the backend. These modules will help us to construct our neural network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 Keras 的一些模块：layers、optimizers、models 和 backend。这些模块将帮助我们构建神经网络：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We create a class called `Critic`, whose object takes in the following parameters:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个名为`Critic`的类，其对象接收以下参数：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Build a critic (value) network that maps `state` and `action` pairs (`Q_values`),
    and define input layers, as follows:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个评论家（价值）网络，它将`state`和`action`对（`Q_values`）映射，并定义输入层，如下所示：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add the hidden layers for the state pathway, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为状态路径添加隐藏层，如下所示：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Add the hidden layers for the action pathway, as follows:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为动作路径添加隐藏层，如下所示：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Combine the state and action pathways, as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并状态路径和动作路径，如下所示：
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Add the final output layer to produce the action values (`Q_values`):'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最终输出层，以产生动作值（`Q_values`）：
- en: '[PRE16]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the Keras model, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Keras 模型，如下所示：
- en: '[PRE17]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the `optimizer` and compile a model for training with the built-in loss
    function:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`optimizer`并编译一个模型以进行训练，使用内置的损失函数：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Compute the action gradients (the derivative of `Q_values`, with respect to
    `actions`):'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算动作梯度（`Q_values`相对于`actions`的导数）：
- en: '[PRE19]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define an additional function to fetch the action gradients (to be used by
    the actor model), as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个附加函数来获取动作梯度（供演员模型使用），如下所示：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This concludes the critic script.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，评论家脚本已完成。
- en: Agent script
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理脚本
- en: 'In this section, we will train an agent that will perform reinforcement learning
    based on the actor and critic networks. We will perform the following steps to
    achieve this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将训练一个代理，该代理将基于演员和评论家网络执行强化学习。我们将执行以下步骤以实现此目标：
- en: Create an agent class whose initial function takes in the batch size, state
    size, and an evaluation Boolean function, to check whether the training is ongoing.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个代理类，其初始化函数接收批次大小、状态大小和一个评估布尔函数，用于检查训练是否正在进行中。
- en: 'In the agent class, create the following methods:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理类中，创建以下方法：
- en: 'Import the `actor` and `critic` scripts:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`actor`和`critic`脚本：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Import `numpy`, `random`, `namedtuple`, and `deque` from the `collections`
    package:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`、`random`、`namedtuple`和`deque`，这些模块来自`collections`包：
- en: '[PRE22]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a `ReplayBuffer` class that adds, samples, and evaluates a buffer:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ReplayBuffer`类，该类负责添加、抽样和评估缓冲区：
- en: '[PRE23]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Add a new experience to the replay buffer memory:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向重放缓冲区记忆中添加一个新的经验：
- en: '[PRE24]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Randomly sample a batch of experienced tuples from the memory. In the following
    function, we randomly sample states from a memory buffer. We do this so that the
    states that we feed to the model are not temporally correlated. This will reduce
    overfitting:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机从记忆中抽取一批经验元组。在以下函数中，我们从记忆缓冲区中随机抽取状态。这样做是为了确保我们输入模型的状态在时间上没有相关性，从而减少过拟合：
- en: '[PRE25]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Return the current size of the buffer memory, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回当前缓冲区内存的大小，如下所示：
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The reinforcement learning agent that learns using the actor-critic network
    is as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用演员-评论家网络进行学习的强化学习代理如下所示：
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The number of actions are defined as 3: sit, buy, sell
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 动作数量定义为3：坐下、买入、卖出
- en: '[PRE28]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Define the replay memory size
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义重放记忆的大小：
- en: '[PRE29]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define whether or not training is ongoing. This variable will be changed during
    the training and evaluation phase:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练是否正在进行中的变量。在训练和评估阶段，此变量将会变化：
- en: '[PRE30]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Discount factor in Bellman equation:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bellman 方程中的折扣因子：
- en: '[PRE31]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'A soft update of the actor and critic networks can be done as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以通过以下方式对演员和评论家网络进行软更新：
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The actor policy model maps states to actions and instantiates the actor networks
    (local and target models, for soft updates of parameters):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员策略模型将状态映射到动作，并实例化演员网络（本地和目标模型，用于软更新参数）：
- en: '[PRE33]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The critic (value) model that maps the state-action pairs to `Q_values` is
    as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评论家（价值）模型，它将状态-动作对映射到`Q_values`，如下所示：
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Instantiate the critic model (the local and target models are utilized to allow
    for soft updates), as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化评论模型（使用本地和目标模型以允许软更新），如下所示：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following code sets the target model parameters to local model parameters:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将目标模型参数设置为本地模型参数：
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Returns an action, given a state, using the actor (policy network) and the output
    of the `softmax` layer of the actor-network, returning the probability for each
    action. An action method that returns an action, given a state, using the actor
    (policy network) is as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个状态，使用演员（策略网络）和演员网络的`softmax`层输出返回一个动作，返回每个动作的概率。一个返回给定状态的动作的方法如下：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Returns a stochastic policy, based on the action probabilities in the training
    model and a deterministic action corresponding to the maximum probability during
    testing. There is a set of actions to be carried out by the agent at every step
    of the episode. A method (step) that returns the set of actions to be carried
    out by the agent at every step of the episode is as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于训练模型中的动作概率返回随机策略，并在测试期间返回与最大概率对应的确定性动作。智能体在每一步执行的动作集如下所示：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following code adds a new experience to the memory:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码向记忆中添加一个新经验：
- en: '[PRE39]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following code asserts that enough experiences are present in the memory
    to train:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码断言记忆中有足够的经验以进行训练：
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following code samples a random batch from the memory to train:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码从记忆中随机抽取一批样本进行训练：
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Learn from the sampled experiences, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从采样的经验中学习，如下所示：
- en: '[PRE42]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following code updates the state to the next state:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码将状态更新到下一个状态：
- en: '[PRE43]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Learning from the sampled experiences through the actor and the critic. Create
    a method to learn from the sampled experiences through the actor and the critic,
    as follows:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过演员和评论者从采样的经验中学习。创建一个方法，通过演员和评论者从采样的经验中学习，如下所示：
- en: '[PRE44]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Return a separate array for each experience in the replay component and predict
    actions based on the next states, as follows:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回回放组件中每个经验的独立数组，并基于下一个状态预测动作，如下所示：
- en: '[PRE45]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Predict the `Q_value` of the actor output for the next state, as follows:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测演员输出的`Q_value`，用于下一个状态，如下所示：
- en: '[PRE46]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Target the `Q_value` to serve as a label for the critic network, based on the
    temporal difference, as follows:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以基于时间差的`Q_value`作为评论网络的标签，如下所示：
- en: '[PRE47]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Fit the critic model to the time difference of the target, as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将评论模型拟合到目标的时间差，如下所示：
- en: '[PRE48]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Train the actor model (local) using the gradient of the critic network output
    with respect to the action probabilities fed from the actor-network:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用评论网络输出关于动作概率的梯度训练演员模型（本地模型）：
- en: '[PRE49]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, define a custom training function, as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个自定义的训练函数，如下所示：
- en: '[PRE50]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, initiate a soft update of the parameters of both networks, as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，初始化两个网络的参数软更新，如下所示：
- en: '[PRE51]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'This performs soft updates on the model parameters, based on the parameter
    `tau` to avoid drastic model changes. A method that updates the model by performing
    soft updates on the model parameters, based on the parameter `tau` (to avoid drastic
    model changes), is as follows:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法根据参数`tau`对模型参数进行软更新，以避免模型发生剧烈变化。通过执行基于参数`tau`的软更新来更新模型（以避免剧烈的模型变化）的方法如下：
- en: '[PRE52]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This concludes the agent script.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了智能体脚本。
- en: Helper script
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 辅助脚本
- en: 'In this script, we will create functions that will be helpful for training,
    via the following steps:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们将通过以下步骤创建一些有助于训练的函数：
- en: 'Import the `numpy` and `math` modules, as follows:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`和`math`模块，如下所示：
- en: '[PRE53]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, define a function to format the price to two decimal places, to reduce
    the ambiguity of the data:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个将价格格式化为两位小数的函数，以减少数据的歧义性：
- en: '[PRE54]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Return a vector of stock data from the CSV file. Convert the closing stock
    prices from the data to vectors, and return a vector of all stock prices, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从CSV文件中返回一个股票数据向量。将数据中的收盘股价转换为向量，并返回所有股价的向量，如下所示：
- en: '[PRE55]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, define a function to generate states from the input vector. Create the
    time series by generating the states from the vectors created in the previous
    step. The function for this takes three parameters: the data; a time, *t* (the
    day that you want to predict); and a window (how many days to go back in time).
    The rate of change between these vectors will then be measured and based on the
    sigmoid function:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数来根据输入向量生成状态。通过生成上一阶段创建的向量中的状态来创建时间序列。这个函数有三个参数：数据；时间 *t*（你想预测的日期）；以及窗口（回溯多少天）。然后，将衡量这些向量之间的变化率，并基于
    sigmoid 函数：
- en: '[PRE56]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, scale the state vector from 0 to 1 with a sigmoid function. The sigmoid
    function can map any input value, from 0 to 1\. This helps to normalize the values
    to probabilities:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，用 sigmoid 函数将状态向量从 0 到 1 进行缩放。sigmoid 函数可以将任何输入值映射到 0 到 1 范围内。这有助于将值标准化为概率：
- en: '[PRE57]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: All of the necessary functions and classes are now defined, so we can start
    the training process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所有必要的函数和类现在都已定义，因此我们可以开始训练过程。
- en: Training the data
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: We will proceed to train the data, based on our agent and helper methods. This
    will provide us with one of three actions, based on the states of the stock prices
    at the end of the day. These states can be to buy, sell, or hold. During training,
    the prescribed action for each day is predicted, and the price (profit, loss,
    or unchanged) of the action is calculated. The cumulative sum will be calculated
    at the end of the training period, and we will see whether there has been a profit
    or a loss. The aim is to maximize the total profit.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将基于代理和辅助方法来训练数据。这将为我们提供三种操作之一，基于当天股票价格的状态。这些状态可以是买入、卖出或保持。在训练过程中，将预测每一天的预定操作，并计算该操作的价格（利润、损失或不变）。在训练期结束时，将计算累计总和，我们将看到是否有盈利或亏损。目标是最大化总利润。
- en: 'Let''s start with the imports, as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入开始，如下所示：
- en: '[PRE58]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, define the number of market days to consider as the window size, and
    define the batch size with which the neural network will be trained, as follows:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义要考虑的市场交易天数作为窗口大小，并定义神经网络训练的批量大小，如下所示：
- en: '[PRE59]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Instantiate the stock agent with the window size and batch size, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用窗口大小和批量大小实例化股票代理，如下所示：
- en: '[PRE60]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, read the training data from the CSV file, using the helper function:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从 CSV 文件中读取训练数据，使用辅助函数：
- en: '[PRE61]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, the episode count is defined as `300`. The agent will look at the data
    for so many numbers of times. An episode represents a complete pass over the data:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将回合数定义为`300`。代理将在数据上查看这么多次。一个回合表示对数据的完整遍历：
- en: '[PRE62]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we can start to iterate through the episodes, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以开始遍历回合，如下所示：
- en: '[PRE63]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Each episode has to be started with a state based on the data and window size.
    The inventory of stocks is initialized before going through the data:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个回合必须以基于数据和窗口大小的状态开始。库存初始化后，再遍历数据：
- en: '[PRE64]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, start to iterate over every day of the stock data. The action probability
    is predicted by the agent, based on the `state`:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，开始遍历每一天的股票数据。基于`state`，代理预测动作的概率：
- en: '[PRE65]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The `action` can be held, if the agent decides not to do anything with the
    stock. Another possible action is to buy (hence, the stock will be added to the
    inventory), as follows:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果代理决定不对股票进行任何操作，则`action`可以保持不变。另一个可能的操作是买入（因此，股票将被加入库存），如下所示：
- en: '[PRE66]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'If the `action` is `2`, the agent sells the stocks and removes it from the
    inventory. Based on the sale, the profit (or loss) is calculated:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`action`为`2`，代理卖出股票并将其从库存中移除。根据销售情况，计算利润（或损失）：
- en: '[PRE67]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'You can see logs similar to those that follow during the training process.
    The stocks are bought and sold at certain prices:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练过程中，你可以看到类似下面的日志。股票在特定的价格下被买入和卖出：
- en: '[PRE68]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Next, the test data is read from the CSV file. The initial state is inferred
    from the data. The steps are very similar to a single episode of the training
    process:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从 CSV 文件中读取测试数据。初始状态由数据推断得出。这些步骤与训练过程中的单一回合非常相似：
- en: '[PRE69]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The profit starts at `0`. The agent is initialized with a zero inventory and
    in test mode:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 利润从`0`开始。代理初始化时库存为零，并且处于测试模式：
- en: '[PRE70]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, every day of trading is iterated, and the agent can act upon the data.
    Every day, the agent decides an action. Based on the action, the stock is held,
    sold, or bought:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，迭代交易的每一天，代理可以根据数据做出决策。每天，代理决定一个动作。根据动作，股票会被持有、卖出或买入：
- en: '[PRE71]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'If the action is `0`, then there is no trade. The state can be called **holding**
    during that period:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果操作是`0`，则没有交易。在这期间，状态可以被称为**持有**。
- en: '[PRE72]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'If the action is `1`, buy the stock by adding it to the inventory, as follows:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果操作是`1`，则通过将股票加入库存来购买股票，操作如下：
- en: '[PRE73]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'If the action is `2`, the agent sells the stock by removing it from the inventory.
    The difference in price is recorded as a profit or a loss:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果操作是`2`，代理通过将股票从库存中移除来卖出股票。价格差异被记录为利润或亏损：
- en: '[PRE74]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once the script starts to run, the model will get better over time through
    training. You can see the logs, as follows:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦脚本开始运行，模型将随着训练逐步变得更好。你可以查看日志，内容如下：
- en: '[PRE75]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The model has traded and made a total profit of $10,427\. Please note that this
    style of trading is not suitable for the real world, as trading involves more
    costs and uncertainty; hence, this trading style could have adverse effects.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已进行了交易，总利润为`$10,427`。请注意，这种交易方式不适合现实世界，因为交易涉及更多成本和不确定性；因此，这种交易风格可能会产生不利影响。
- en: Final result
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终结果
- en: After training the data, we tested it against the `test` dataset. Our model
    resulted in a total profit of `$10427.24`. The best thing about the model was
    that the profits kept improving over time, indicating that it was learning well
    and taking better actions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据之后，我们用`test`数据集对模型进行了测试。我们的模型总共获得了`$10427.24`的利润。模型的最佳之处在于，利润随着时间的推移不断增加，表明它正在有效学习并采取更好的行动。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In conclusion, machine learning can be applied to several industries and can
    be applied very efficiently in financial markets, as you saw in this chapter.
    We can combine different models, as we did with reinforcement learning and time
    series, to produce stronger models that suit our use cases. We discussed the use
    of reinforcement learning and time series to predict the stock market. We worked
    with an actor-critic model that determined the best action, based on the state
    of the stock prices, with the aim of maximizing profits. In the end, we obtained
    a result that boasted an overall profit and included increasing profits over time,
    indicating that the agent learned more with each state.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，机器学习可以应用于多个行业，并且在金融市场中可以非常高效地应用，正如你在本章中所见。我们可以结合不同的模型，正如我们结合了强化学习和时间序列，以生成更强大的模型来满足我们的用例。我们讨论了使用强化学习和时间序列来预测股市。我们使用了一个演员-评论家模型，根据股票价格的状态来决定最佳行动，目的是最大化利润。最终，我们获得了一个结果，展示了总体利润，并且利润随时间增加，表明代理在每个状态中学得更多。
- en: In the next chapter, you will learn about the future areas of work.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习未来的工作领域。
