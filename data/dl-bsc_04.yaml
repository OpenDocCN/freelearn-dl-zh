- en: 3\. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 神经网络
- en: We learned about perceptrons in the previous chapter, and there is both good
    news and bad news. The good news is that perceptrons are likely to represent complicated
    functions. For example, the perceptron can (theoretically) represent complicated
    processes performed by a computer, as described in the previous chapter. The bad
    news is that weights must be defined manually first before the appropriate weights
    are determined in order to meet the expected inputs and outputs. In the previous
    chapter, we used the truth tables with AND and OR gates to determine the appropriate
    weights manually.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章学习了感知机，其中有好消息也有坏消息。好消息是，感知机很可能能够表示复杂的函数。例如，感知机（理论上）能够表示计算机执行的复杂过程，正如上一章所描述的那样。坏消息是，在确定合适的权重之前，必须先手动定义权重，以满足预期的输入和输出。在上一章中，我们使用了与
    AND 和 OR 门相关的真值表来手动确定适当的权重。
- en: Neural networks exist to solve the bad news. More specifically, one important
    property of a neural network is that it can learn appropriate weight parameters
    from data automatically. This chapter provides an overview of neural networks
    and focuses on what distinguishes them. The next chapter will describe how it
    learns weight parameters from data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的存在是为了解决坏消息。更具体地说，神经网络的一个重要特性是，它可以自动从数据中学习合适的权重参数。本章概述了神经网络，并着重介绍了它们的区别。下一章将描述它如何从数据中学习权重参数。
- en: From Perceptrons to Neural Networks
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从感知机到神经网络
- en: A neural network is similar to the perceptron described in the previous chapter
    in many ways. How a neural network works, as well as how it differs from a perceptron,
    will be described in this section.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在许多方面与上一章中描述的感知机相似。神经网络是如何工作的，以及它如何与感知机有所不同，将在本节中进行描述。
- en: Neural Network Example
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络示例
- en: '*Figure 3.1* shows a neural network example. Here, the left column is called
    an **input layer**, the right column is called an **output layer**, and the center
    column is called the **middle layer**. The middle layer is also known as a hidden
    layer. "Hidden" means that the neurons in the hidden layer are invisible (unlike
    those in the input and output layers). In this book, we''ll call the layers layer
    0, layer 1, and layer 2 from the input layer to the output layer (layer numbers
    start from layer 0 because doing so is convenient when the layers are implemented
    in Python later). In *Figure 3.1*, layer 0 is the input layer, layer 1 is the
    middle layer, and layer 2 is the output layer:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.1* 显示了一个神经网络示例。这里，左列称为**输入层**，右列称为**输出层**，中间列称为**中间层**。中间层也叫做隐藏层。“隐藏”意味着隐藏层中的神经元是不可见的（与输入层和输出层中的神经元不同）。在本书中，我们将这些层依次称为第
    0 层、第 1 层和第 2 层（层编号从第 0 层开始，因为在后面使用 Python 实现层时这样做较为方便）。在*图 3.1*中，第 0 层是输入层，第
    1 层是中间层，第 2 层是输出层：'
- en: '![Figure 3.1: Neural network example'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：神经网络示例'
- en: '](img/fig03_1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_1.jpg)'
- en: 'Figure 3.1: Neural network example'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：神经网络示例
- en: Note
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Although the network in *Figure 3.1* consists of three layers, we call it a
    "two-layer network" because it has two layers with weights. Some books call it
    a "three-layer network" based on the number of layers that constitute the network,
    but in this book, the network name is based on the number of layers that have
    weights (that is, the total number of input, hidden, and output layers, minus
    1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*图 3.1*中的网络包含三层，但我们称其为“二层网络”，因为它有两个带权重的层。有些书籍根据构成网络的层数称其为“三层网络”，但在本书中，网络的名称是根据具有权重的层数来命名的（即输入层、隐藏层和输出层的总层数减去
    1）。
- en: The neural network in *Figure 3.1* is similar to the perceptron in the previous
    chapter in terms of its shape. In fact, in terms of how neurons are connected,
    it is no different from the perceptron we saw in the previous chapter. So, how
    are signals transmitted in a neural network?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.1* 中的神经网络在形态上类似于上一章中的感知机。实际上，在神经元连接方式上，它与我们在上一章中看到的感知机没有什么不同。那么，信号是如何在神经网络中传递的呢？'
- en: Reviewing the Perceptron
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回顾感知机
- en: 'To answer this question, we first need to review the perceptron. Consider a
    network that has the following structure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们首先需要回顾感知机。考虑一个具有以下结构的网络：
- en: '![Figure 3.2: Reviewing the perceptron'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.2：回顾感知机'
- en: '](img/fig03_2.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_2.jpg)'
- en: 'Figure 3.2: Reviewing the perceptron'
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.2：回顾感知机
- en: '*Figure 3.2* shows a perceptron that receives two input signals (x1 and x2)
    and outputs y. As described earlier, the perceptron in *Figure 3.2* is represented
    by equation (3.1):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.2* 展示了一个感知器，它接收两个输入信号 (x1 和 x2)，并输出 y。如前所述，*图 3.2* 中的感知器由方程式 (3.1) 表示：'
- en: '| ![4](img/Figure_3.2a.png) | (3.1) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| ![4](img/Figure_3.2a.png) | (3.1) |'
- en: Here, b is a parameter called "bias" and controls how easily the neuron fires.
    Meanwhile, w1 and w2 are the parameters that indicate the "weights" of individual
    signals to control their importance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，b 是一个称为“偏置”的参数，控制神经元触发的容易程度。同时，w1 和 w2 是表示单个信号“权重”的参数，用于控制它们的重要性。
- en: 'You may have noticed that the network in *Figure 3.2* has no bias, b. We can
    indicate the bias shown in *Figure 3.3*, if we want to. A signal of weight b and
    input 1 has been added in *Figure 3.3*. This perceptron receives three signals
    (x1, x2, and 1) as the inputs to the neuron, and multiplies the signals by each
    weight before transmitting them to the next neuron. The next neuron sums the weighted
    signals and then outputs 1 if the sum exceeds 0\. It outputs 0 if it doesn''t.
    The neuron in the following diagram is shown in solid gray to distinguish it from
    other neurons. This is because the input signal of the bias is always 1:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，*图 3.2* 中的网络没有偏置 b。如果需要，我们可以在 *图 3.3* 中表示偏置。在 *图 3.3* 中添加了一个权重为 b，输入为
    1 的信号。这个感知器接收三个信号 (x1, x2 和 1) 作为神经元的输入，并将每个信号与相应的权重相乘后传递到下一个神经元。下一个神经元将加权信号求和，如果总和超过
    0，则输出 1。如果没有超过 0，则输出 0。以下图中的神经元用实心灰色表示，以便与其他神经元区分开来。这是因为偏置的输入信号始终为 1：
- en: '![Figure 3.3: Showing the bias explicitly'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.3：明确展示偏置](img/Figure_3.3a.png)'
- en: '](img/fig03_3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_3.jpg)'
- en: 'Figure 3.3: Showing the bias explicitly'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.3：明确展示偏置
- en: 'Now, we want to simplify equation (3.1). To do that, we use a single function
    to express the condition, where `1` is the output if the sum exceeds 0, and 0
    is the output if it does not. Here, we will introduce a new function, *h*(*x*),
    and rewrite equation (3.1) to equations (3.2) and (3.3) shown here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想简化方程式 (3.1)。为此，我们使用一个单一的函数来表示这个条件，其中 `1` 是当和超过 0 时的输出，如果不超过则输出 0。在这里，我们引入一个新函数
    *h*(*x*)，并将方程式 (3.1) 重写为方程式 (3.2) 和 (3.3)，如下所示：
- en: '| ![5](img/Figure_3.3a.png) | (3.2) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ![5](img/Figure_3.3a.png) | (3.2) |'
- en: '| ![5a](img/Figure_3.3b.png) | (3.3) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ![5a](img/Figure_3.3b.png) | (3.3) |'
- en: Equation (3.2) indicates that the *h*(*x*) function converts the sum of input
    signals into the output, y. The *h*(*x*) function represented by equation (3.3)
    returns 1 if the input exceeds 0 and returns 0 if it does not. Therefore, equations
    (3.2) and (3.3) operate in the same way as equation (3.1).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 (3.2) 表示 *h*(*x*) 函数将输入信号的和转换为输出 y。方程式 (3.3) 中表示的 *h*(*x*) 函数当输入超过 0 时返回
    1，否则返回 0。因此，方程式 (3.2) 和 (3.3) 与方程式 (3.1) 的操作方式相同。
- en: Introducing an Activation Function
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 引入激活函数
- en: The *h*(*x*) function that appears here is generally called an **activation
    function**. It converts the sum of input signals into an output signal. As the
    name "activation" indicates, the activation function determines how the sum of
    the input signals activates (that is, how it fires).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现的 *h*(*x*) 函数通常称为 **激活函数**。它将输入信号的和转换为输出信号。正如“激活”这个名字所示，激活函数决定了输入信号的和如何激活（即它如何触发）。
- en: 'Now, we can rewrite equation (3.2) again. Equation (3.2) performs two processes:
    the weighted input signals are summed, and the sum is converted by the activation
    function. Therefore, you can divide equation (3.2) into the following two equations:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次重写方程式 (3.2)。方程式 (3.2) 执行两个过程：加权输入信号求和，然后通过激活函数转换该和。因此，可以将方程式 (3.2)
    分解为以下两个方程式：
- en: '| ![7](img/Figure_3.3c.png) | (3.4) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| ![7](img/Figure_3.3c.png) | (3.4) |'
- en: '| ![8](img/Figure_3.3d.png) | (3.5) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![8](img/Figure_3.3d.png) | (3.5) |'
- en: In equation (3.4), the sum of the weighted input signals and biases becomes
    a. In equation (3.5), a is converted by *h()*, and *y* is output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程式 (3.4) 中，加权输入信号和偏置的和变为 a。在方程式 (3.5) 中，a 被 *h()* 转换，*y* 被输出。
- en: 'So far, a neuron has been shown as one circle. *Figure 3.4* shows equations
    (3.4) and (3.5) explicitly:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一个神经元已被表示为一个圆形。*图 3.4* 明确展示了方程式 (3.4) 和 (3.5)：
- en: '![Figure 3.4: Showing the process performed by the activation function explicitly'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.4：明确展示激活函数执行的过程](img/Figure_3.4a.png)'
- en: '](img/fig03_4.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_4.jpg)'
- en: 'Figure 3.4: Showing the process performed by the activation function explicitly'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.4：明确展示激活函数执行的过程
- en: '*Figure 3.4* explicitly shows the process that is performed by the activation
    function in the circle of the neuron. We can clearly see that the sum of the weighted
    signals becomes node *a* and that it is converted into node *y* by the activation
    function, *h*(). In this book, the terms "neuron" and "node" are used interchangeably.
    Here, circles *a* and *y* are called "nodes," which are used in the same sense
    as "neurons" that were used earlier.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.4* 明确展示了激活函数在神经元圆圈内执行的过程。我们可以清楚地看到，带权重的信号之和成为节点 *a*，并通过激活函数 *h*() 转换为节点
    *y*。在本书中，“神经元”和“节点”是可以互换使用的术语。这里，圆圈 *a* 和 *y* 被称为“节点”，与之前使用的“神经元”意义相同。  '
- en: 'We will continue to show a neuron as one circle, as shown on the left of *Figure
    3.5*. In this book, we will also show the activation process (to the right of
    *Figure 3.5*), if the behavior of the neural network can be clarified:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将继续将神经元表示为一个圆圈，如 *图 3.5* 左侧所示。在本书中，我们还将展示激活过程（如 *图 3.5* 右侧所示），以便能够阐明神经网络的行为：  '
- en: '![Figure 3.5: The left-hand image is an ordinary image that shows a neuron,
    while the right-hand image explicitly shows the process of activation in a neuron
    (a is the sum of input signals, h() is the activation function, and y is the output)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5：左侧的图像是一个普通的神经元图像，而右侧的图像则明确展示了神经元激活的过程（a 是输入信号的总和，h() 是激活函数，y 是输出）  '
- en: '](img/fig03_5.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_5.jpg)  '
- en: 'Figure 3.5: The left-hand image is an ordinary image that shows a neuron, while
    the right-hand image explicitly shows the process of activation in a neuron (a
    is the sum of input signals, h() is the activation function, and y is the output)'
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.5：左侧的图像是一个普通的神经元图像，而右侧的图像则明确展示了神经元激活的过程（a 是输入信号的总和，h() 是激活函数，y 是输出）  '
- en: Now, let's focus on the activation function, which serves as the bridge from
    a perceptron to a neural network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，让我们专注于激活函数，它是从感知机到神经网络的桥梁。  '
- en: Note
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '注意  '
- en: In this book, the algorithm indicated by the word "perceptron" is not strictly
    defined. Generally, a "simple perceptron" is a single-layer network where a step
    function that changes the output values at a threshold is used as the activation
    function. A "multilayer perceptron" usually means a neural network that contains
    multiple layers and uses a smooth activation function, such as a sigmoid function.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '在本书中，"感知机"一词所表示的算法并没有严格定义。通常，“简单感知机”是一个单层网络，其中使用一个在阈值处改变输出值的阶跃函数作为激活函数。“多层感知机”通常指包含多个层并使用平滑激活函数（如
    sigmoid 函数）的神经网络。  '
- en: Activation Function
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: The activation function represented by equation (3.3) changes output values
    at a threshold and is called a "step function" or a "staircase function." Therefore,
    we can say, "a perceptron uses a step function as the activation function." In
    other words, a perceptron chooses a "step function" as the activation function
    from many candidate functions. When a perceptron uses a step function as the activation
    function, what happens if a function other than a step function is used as the
    activation function? Well, by changing the activation function from a step function
    to another function, we can move to the world of a neural network. The next section
    will introduce an activation function for a neural network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '由方程（3.3）表示的激活函数在阈值处改变输出值，称为“阶跃函数”或“阶梯函数”。因此，我们可以说，“感知机使用阶跃函数作为激活函数”。换句话说，感知机从众多候选函数中选择“阶跃函数”作为激活函数。如果感知机使用了阶跃函数作为激活函数，那么如果使用其他函数作为激活函数会发生什么呢？好吧，通过将激活函数从阶跃函数更改为其他函数，我们可以进入神经网络的世界。下一节将介绍神经网络的激活函数。  '
- en: Sigmoid Function
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Sigmoid 函数  '
- en: 'One of the activation functions often used in neural networks is the **sigmoid
    function**, represented by equation (3.6):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络中常用的激活函数之一是 **sigmoid 函数**，其由方程（3.6）表示：  '
- en: '| ![9](img/Figure_3.5a.png) | (3.6) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ![9](img/Figure_3.5a.png) | (3.6) |  '
- en: exp(-*x*) in equation (3.6) indicates *e*-x. The real number, *e*, is Napier's
    number, 2.7182... The sigmoid function represented by equation (3.6) seems complicated,
    but it is only a "function." A function is a converter that returns output when
    input is provided. For example, when a value such as 1.0 and 2.0 is provided to
    the sigmoid function, values such as *h*(1.0) = 0.731… and *h*(2.0) = 0.880… are
    returned.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 方程(3.6)中的`exp(-*x*)`表示*e*-x。实数*e*是自然常数，即2.7182... 由方程(3.6)表示的Sigmoid函数看起来很复杂，但它其实只是一个“函数”。一个函数是一个转换器，当提供输入时，它会返回输出。例如，当提供像1.0和2.0这样的值到Sigmoid函数时，返回的值如*h*(1.0)
    = 0.731... 和*h*(2.0) = 0.880...。
- en: In a neural network, a sigmoid function is often used as the activation function
    to convert signals, and the converted signals are transmitted to the next neuron.
    In fact, the main difference between the perceptron described in the previous
    chapter and the neural network described here is the activation function. Other
    aspects, such as the structure where neurons are connected in multiple layers
    and how signals are transmitted, are basically the same as they are for perceptrons.
    Now, let's look more closely at a sigmoid function (used as the activation function)
    by comparing it with a step function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，Sigmoid函数通常用作激活函数来转换信号，并将转换后的信号传递给下一个神经元。事实上，上一章所描述的感知机和这里描述的神经网络的主要区别就是激活函数。其他方面，如神经元的多层连接结构和信号如何传递，基本上与感知机是一样的。现在，让我们通过与阶跃函数的比较，更深入地了解作为激活函数使用的Sigmoid函数。
- en: Implementing a Step Function
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现阶跃函数
- en: 'Here, we will use Python to show the graph of a step function. As represented
    by equation (3.3), the step function outputs 1 when the input exceeds 0 and outputs
    0 if it does not. The following shows a simple implementation of the step function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用Python显示阶跃函数的图。正如方程(3.3)所表示的，阶跃函数当输入超过0时输出1，否则输出0。以下是阶跃函数的简单实现：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This implementation is simple and easy to understand, but it only takes a real
    number (a floating-point number) as argument `x`. Therefore, `step_function(3.0)`
    is allowed. However, the function cannot take a NumPy array as the argument. Thus,
    `step_function(np.array([1.0, 2.0]))` is not allowed. Here, we want to change
    to the future implementation so that it can take a NumPy array. For that purpose,
    we can write an implementation like the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现简单易懂，但它只接受一个实数（浮点数）作为参数`x`。因此，`step_function(3.0)`是允许的。但是，这个函数不能接受NumPy数组作为参数。因此，`step_function(np.array([1.0,
    2.0]))`是不允许的。在这里，我们希望修改为未来的实现，以便它可以接受NumPy数组。为此，我们可以编写类似以下的实现：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Although the preceding function contains only two lines, it may be a little
    difficult to understand because it uses a useful "trick" from NumPy. Here, the
    following example from the Python interpreter is used to describe what kind of
    trick is used. In this example, the NumPy `x` array is provided. For the NumPy
    array, a comparison operator is conducted:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的函数只有两行，但它可能有些难以理解，因为它使用了NumPy的一个有用“技巧”。在这里，使用以下Python解释器的示例来描述使用了什么样的技巧。在这个示例中，提供了NumPy的`x`数组。对于NumPy数组，进行比较运算符操作：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When a *greater than* comparison is conducted for a NumPy array, each element
    in the array is compared to generate a Boolean array. Here, each element in the
    `x` array is converted into `True` when it exceeds 0 or into `False` when it does
    not. Then, the new array, `y`, is generated.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当对NumPy数组进行“大于”比较时，数组中的每个元素都会被比较，以生成一个布尔数组。在这里，当`x`数组中的每个元素超过0时，它会被转换为`True`，否则为`False`。然后，生成新的数组`y`。
- en: 'The `y` array is Boolean, and the desired step function must return `0` or
    `1` of the `int` type. Therefore, we convert the type of elements of array `y`
    from Boolean into `int`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`y`数组是布尔类型，所需的阶跃函数必须返回`0`或`1`的整数类型。因此，我们将数组`y`的元素类型从布尔型转换为整数型：'
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As shown here, the `astype()` method is used to convert the type of the NumPy
    array. The `astype()` method takes the desired type (`np.int`, in this example)
    as the argument. In Python, `True` is converted into `1`, and `False` is converted
    into `0` by converting the Boolean type into the int type. The preceding code
    explains NumPy's "trick" that's used when implementing the step function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，`astype()`方法用于转换NumPy数组的类型。`astype()`方法将所需的类型（在此示例中为`np.int`）作为参数。在Python中，`True`会被转换为`1`，`False`会被转换为`0`，通过将布尔类型转换为整数类型。前面的代码解释了在实现阶跃函数时NumPy所使用的“技巧”。
- en: Step Function Graph
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶跃函数图
- en: 'Now, let''s draw the graph of the step function we defined previously. To do
    that, we need to use the Matplotlib library:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制之前定义的阶跃函数的图像。为此，我们需要使用 Matplotlib 库：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`np.arange(-5.0, 5.0, 0.1)` generates a NumPy array containing values from
    `-5.0` to `5.0` in `0.1` steps, `([-5.0, -4.9, …, 4.9]). step_function()` takes
    a NumPy array as the argument. It executes the step function for each element
    in the array and returns an array as the result. When these `x` and `y` arrays
    are plotted, the graph shown in *Figure 3.6* is displayed:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`np.arange(-5.0, 5.0, 0.1)` 会生成一个包含从 `-5.0` 到 `5.0`，步长为 `0.1` 的 NumPy 数组（`[-5.0,
    -4.9, …, 4.9]`）。`step_function()` 接受一个 NumPy 数组作为参数。它会对数组中的每个元素执行阶跃函数，并返回一个数组作为结果。当这些
    `x` 和 `y` 数组被绘制时，会显示出*图 3.6*所示的图形：'
- en: '![Figure 3.6: Step function graph'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6：阶跃函数图'
- en: '](img/fig03_6.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_6.jpg)'
- en: 'Figure 3.6: Step function graph'
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.6：阶跃函数图
- en: As shown in *Figure 3.6*, the output of the step function changes from 0 to
    1 (or 1 to 0) at the threshold of 0\. A step function is sometimes called a "staircase
    function" because the output represents the steps of stairs, as shown in *Figure
    3.6*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 3.6*所示，阶跃函数的输出在 0 的阈值处从 0 变为 1（或从 1 变为 0）。阶跃函数有时被称为“阶梯函数”，因为它的输出就像楼梯的台阶，正如*图
    3.6*所示。
- en: Implementing a Sigmoid Function
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 sigmoid 函数
- en: 'Now, let''s implement a sigmoid function. We can write the sigmoid function
    of equation (3.6) in Python as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来实现一个 sigmoid 函数。我们可以将公式（3.6）中的 sigmoid 函数用 Python 编写如下：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, `np.exp(-x)` corresponds to `exp(−x)` in the equation. This implementation
    is not very difficult. The correct results are returned even when a NumPy array
    is provided as the `x` argument. When this sigmoid function receives a NumPy array,
    it calculates correctly, as shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`np.exp(-x)` 对应公式中的 `exp(−x)`。这个实现并不复杂。当将 NumPy 数组作为 `x` 参数传递时，仍然能够返回正确的结果。该
    sigmoid 函数接收 NumPy 数组时，会按正确的方式计算，如下所示：
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The implementation of the sigmoid function supports a NumPy array due to NumPy's
    broadcasting (refer to the *Broadcasting* section in *Chapter 1*, *Introduction
    to Python* for details). When an operation is performed on a scalar and a NumPy
    array, thanks to the broadcast, the operation is performed between the scalar
    and each element of the NumPy array.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 函数的实现支持 NumPy 数组，这得益于 NumPy 的广播功能（有关详情，请参阅*第 1 章：Python 简介*中的*广播*部分）。当对标量和
    NumPy 数组进行操作时，借助广播机制，操作会在标量和 NumPy 数组的每个元素之间执行。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding example, arithmetic operations (such as `+` and `/`) are performed
    between the scalar value (1.0 here) and the NumPy array. As a result, the scalar
    value and each element of the NumPy array is used in the operations, and the results
    are output as a NumPy array. In this implementation of the sigmoid function, because
    `np.exp(-x)` generates a NumPy array, `1 / (1 + np.exp(-x))` also uses each element
    of the NumPy array for the operation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，算术运算（如 `+` 和 `/`）是在标量值（此处为 1.0）和 NumPy 数组之间进行的。结果是，标量值和 NumPy 数组的每个元素都参与了运算，结果作为
    NumPy 数组输出。在这个 sigmoid 函数的实现中，因为 `np.exp(-x)` 会生成一个 NumPy 数组，`1 / (1 + np.exp(-x))`
    也会对 NumPy 数组的每个元素进行操作。
- en: 'Now, let''s draw the graph of the sigmoid function. The code for drawing is
    almost the same as the code for the step function. The only difference is that
    the function that outputs `y` is changed to the sigmoid function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来绘制 sigmoid 函数的图像。绘制代码几乎与阶跃函数的代码相同。唯一的区别是输出 `y` 的函数被更改为 sigmoid 函数：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code creates the graph shown in *Figure 3.7* when it is executed:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码执行时会生成*图 3.7*所示的图形：
- en: '![Figure 3.7: Graph of the sigmoid function'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7：sigmoid 函数图'
- en: '](img/fig03_7.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_7.jpg)'
- en: 'Figure 3.7: Graph of the sigmoid function'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.7：sigmoid 函数图
- en: Comparing the Sigmoid Function and the Step Function
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较 sigmoid 函数和阶跃函数
- en: Let's compare the sigmoid function and the step function. *Figure 3.8* shows
    the sigmoid function and the step function. In what ways are the two functions
    different? In what ways are they alike? We can consider *Figure 3.8* and think
    about this for a moment.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下 sigmoid 函数和阶跃函数。*图 3.8* 显示了 sigmoid 函数和阶跃函数。两者有什么不同？又有哪些相似之处？我们可以考虑一下*图
    3.8*，并思考一下这个问题。
- en: 'When you look at *Figure 3.8*, you may notice the difference in smoothness.
    The sigmoid function is a smooth curve, where the output changes continuously
    based on the input. On the other hand, the output of the step function changes
    suddenly at `0`. This smoothness of the sigmoid function has an important meaning
    when training neural networks:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看*图3.8*时，你可能会注意到平滑度上的差异。Sigmoid函数是一条平滑曲线，其输出会基于输入连续变化。另一方面，阶跃函数的输出在`0`时会突然变化。Sigmoid函数的平滑性在训练神经网络时具有重要意义：
- en: '![Figure 3.8: Step function and sigmoid function (the dashed line shows the
    step function)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.8：阶跃函数和Sigmoid函数（虚线表示阶跃函数）'
- en: '](img/fig03_8.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_8.jpg)'
- en: 'Figure 3.8: Step function and sigmoid function (the dashed line shows the step
    function)'
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.8：阶跃函数和Sigmoid函数（虚线表示阶跃函数）
- en: In connection with the smoothness mentioned previously, they are different in
    that the step function returns only 0 or 1, while the sigmoid function returns
    real numbers such as 0.731... and 0.880... That is, binary signals of 0 and 1
    flow among neurons in a perceptron, while signals of continuous real numbers flow
    in a neural network.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前提到的平滑度相关，它们的不同之处在于阶跃函数仅返回0或1，而Sigmoid函数返回如0.731...和0.880...等实数。也就是说，在感知机中，0和1的二进制信号在神经元之间流动，而在神经网络中，连续实数信号在神经元之间流动。
- en: 'When we use "water" to describe the behaviors of these two functions, the step
    function can be compared to a "shishi-odoshi" (a bamboo tube that clacks against
    a stone after water flows out of a tube), and the sigmoid function can be compared
    to a "waterwheel." The step function conducts two actions: it drains or stores
    water (0 or 1), while the sigmoid function controls the flow of water like a "waterwheel"
    based on the amount of water that reaches it.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用“水”来描述这两个函数的行为时，阶跃函数可以比作“石榴水竹”（水流出管道后竹管撞击石头的声音），而Sigmoid函数则可以比作“水车”。阶跃函数执行两个动作：排水或储水（0或1），而Sigmoid函数则像“水车”一样，基于达到它的水量来控制水流。
- en: Now, consider the ways in which the step and sigmoid functions are similar.
    They are different in "smoothness," but you may notice that they are similar in
    shape when you view *Figure 3.8* from a broader perspective. Actually, both of
    them output a value near/of 0 when the input is small, and, as the input becomes
    larger, the output approaches/reaches 1\. The step and sigmoid functions output
    a large value when the input signal contains important information and output
    a small value when it don't. They are also similar in that they output a value
    between 0 and 1, no matter how small or large the value of the input signal is.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一下阶跃函数和Sigmoid函数的相似之处。它们在“平滑度”上有所不同，但从更广泛的角度来看，当你查看*图3.8*时，你可能会发现它们在形状上是相似的。实际上，它们都在输入较小时输出接近/为0的值，而随着输入增大，输出趋近/达到1。阶跃函数和Sigmoid函数在输入信号包含重要信息时输出较大的值，而在没有重要信息时输出较小的值。它们还有一个相似之处，就是无论输入信号的值多么小或大，它们都会输出介于0和1之间的值。
- en: Nonlinear Function
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性函数
- en: The step and sigmoid functions are similar in another way. One important similarity
    is that they are both **nonlinear functions**. The sigmoid function is represented
    by a curve, while the step function is represented by straight lines that look
    like stairs. They are both classified as nonlinear functions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 阶跃函数和Sigmoid函数在另一个方面也相似。一个重要的相似点是它们都是**非线性函数**。Sigmoid函数由一条曲线表示，而阶跃函数则由看起来像楼梯的直线表示。它们都被归类为非线性函数。
- en: Note
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The terms "nonlinear function" and "linear function" often appear when an activation
    function is used. A function is a "converter" that returns a value when a value
    is provided. A function that outputs the input values multiplied by a constant
    is called a linear function (represented by the equation *h*(*x*) = *cx*, where
    *c* is a constant). Therefore, the graph of a linear function is a straight line.
    Meanwhile, as its name suggests, the graph of a nonlinear function is not a simple
    straight line.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: “非线性函数”和“线性函数”这两个术语通常出现在激活函数中。一个函数是一个“转换器”，当提供一个值时返回一个结果。一个输出为输入值乘以常数的函数称为线性函数（其表示式为*h*（*x*）
    = *cx*，其中*c*为常数）。因此，线性函数的图像是一条直线。与此相对，顾名思义，非线性函数的图像不是一条简单的直线。
- en: In a neural network, a nonlinear function must be used as the activation function.
    In other words, a linear function may not be used as the activation function.
    Why may a linear function not be used? The reason is that increasing the number
    of layers in a neural network becomes useless if a linear function is used.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，必须使用非线性函数作为激活函数。换句话说，线性函数不能作为激活函数使用。为什么线性函数不能使用？原因是如果使用线性函数，增加神经网络中的层数将变得毫无意义。
- en: The problem with a linear function is caused by the fact that a "network without
    a hidden layer" that does the same task always exists, no matter how many layers
    are added. To understand this specifically (and somewhat intuitively), let's consider
    a simple example. Here, a linear function, *h*(*x*) = *cx*, is used as the activation
    function and the calculation of *y*(*x*) = *h*(*h*(*h*(*x*))) is performed as
    in a three-layer network. It contains multiplications of y(x) = c×c×c×x, and the
    same operation can be represented by one multiplication of *y*(*x*) = *ax* (where
    *a* = *c*3). Thus, it can be represented by a network without a hidden layer.
    As this example shows, using a linear function offsets the advantage of multiple
    layers. Therefore, to take advantage of multiple layers, a nonlinear function
    must be used as the activation function.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数的问题在于，无论增加多少层，总有一个“没有隐藏层的网络”能够完成相同的任务。为了具体理解这一点（并且稍微直观一些），我们来看一个简单的例子。这里，使用线性函数
    *h*(*x*) = *cx* 作为激活函数，并像三层网络一样计算 *y*(*x*) = *h*(*h*(*h*(*x*)))。它包含了 y(x) = c×c×c×x
    的乘法，而同样的操作可以通过一次乘法 *y*(*x*) = *ax*（其中 *a* = *c*³）来表示。因此，它可以通过一个没有隐藏层的网络来表示。正如这个例子所示，使用线性函数会抵消多层的优势。因此，为了利用多层的优势，必须使用非线性函数作为激活函数。
- en: ReLU Function
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU 函数
- en: Thus far, we have learned about step and sigmoid functions as activation functions.
    While a sigmoid function has been used for a long time in the history of neural
    networks, a function called **Rectified Linear Unit** (**ReLU**) is mainly used
    these days.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了作为激活函数的阶跃函数和 sigmoid 函数。虽然 sigmoid 函数在神经网络历史上已经使用了很长时间，但如今主要使用一种叫做**整流线性单元**（**ReLU**）的函数。
- en: 'If the input exceeds 0, the ReLU function outputs the input as it is. If the
    input is equal to or smaller than 0, it outputs 0 (see *Figure 3.9*):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入超过 0，ReLU 函数将直接输出输入值。如果输入小于或等于 0，它将输出 0（见 *图 3.9*）：
- en: '![Figure 3.9: ReLU function'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.9：ReLU 函数'
- en: '](img/fig03_9.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_9.jpg)'
- en: 'Figure 3.9: ReLU function'
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.9：ReLU 函数
- en: 'Equation (3.7) represents the ReLU function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (3.7) 表示 ReLU 函数：
- en: '| ![10](img/Figure_3.9a.png) | (3.7) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![10](img/Figure_3.9a.png) | (3.7) |'
- en: 'As the graph and the equation shows, the ReLU function is very simple. Therefore,
    we can also implement it easily, as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如图和方程所示，ReLU 函数非常简单。因此，我们也可以轻松实现它，如下所示：
- en: '[PRE9]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, NumPy's maximum function is used. It outputs the larger of the input values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了 NumPy 的最大值函数。它输出输入值中较大的那个。
- en: While a sigmoid function will be used as the activation function later in this
    chapter, the ReLU function is mainly used in the latter half of this book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本章后续将使用 sigmoid 函数作为激活函数，但 ReLU 函数主要在本书后半部分使用。
- en: Calculating Multidimensional Arrays
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算多维数组
- en: If you learn how to calculate multidimensional arrays using NumPy, you will
    be able to implement a neural network efficiently. First, we will look at how
    to use NumPy to calculate multidimensional arrays. Then, we will implement a neural
    network.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你学会了如何使用 NumPy 计算多维数组，你将能够高效地实现一个神经网络。首先，我们将学习如何使用 NumPy 计算多维数组。然后，我们将实现一个神经网络。
- en: Multidimensional Arrays
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多维数组
- en: 'Simply put, a multidimensional array is "a set of numbers" arranged in a line,
    in a rectangle, in three dimensions, or (more generally) in N dimensions, called
    a multidimensional array. Let''s use NumPy to create a multidimensional array.
    First, we will create a one-dimensional array, as described so far:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，多维数组是“一个由数字组成的集合”，这些数字可以排成一行、一个矩形、三维的，或者（更一般地）N维的，这种集合被称为多维数组。我们将使用 NumPy
    来创建一个多维数组。首先，我们将创建一个一维数组，正如我们到目前为止所描述的那样：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As shown here, you can use the `np.ndim()` function to obtain the number of
    dimensions of an array. You can also use the instance variable, `shape`, to obtain
    the shape of the array. The preceding example shows that `A` is a one-dimensional
    array consisting of four elements. Please note that the result of `A.shape` is
    a tuple. This is because the result is returned in the same format both for a
    one-dimensional array and for a multidimensional array. For example, a (4,3) tuple
    is returned for a two-dimensional array, and a (4,3,2) tuple is returned for a
    three-dimensional one. Therefore, a tuple is also returned for a one-dimensional
    array. Now, let''s create a two-dimensional array:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，你可以使用`np.ndim()`函数来获取数组的维度数量。你还可以使用实例变量`shape`来获取数组的形状。前面的例子显示，`A`是一个由四个元素组成的一维数组。请注意，`A.shape`的结果是一个元组。这是因为该结果的返回格式对于一维数组和多维数组都是相同的。例如，对于二维数组返回一个(4,3)的元组，对于三维数组返回(4,3,2)的元组。因此，对于一维数组也会返回一个元组。现在，让我们创建一个二维数组：
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here, a 3x2 array, B, is created. A 3x2 array means that it has three elements
    in the first dimension and two elements in the next dimension. The first dimension
    is dimension 0, and the next dimension is dimension 1 (an index starts from 0
    in Python). A two-dimensional array is called a matrix. As shown in *Figure 3.10*,
    a horizontal sequence in an array is called a **row**, while a vertical sequence
    is called a **column**:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，创建了一个3x2的数组B。3x2数组意味着它在第一维上有三个元素，在下一维上有两个元素。第一维是维度0，下一维是维度1（在Python中索引从0开始）。二维数组称为矩阵。如*图3.10*所示，数组中的水平序列称为**行**，垂直序列称为**列**：
- en: '![Figure 3.10: A horizontal sequence is called a "row," and a vertical one
    is called a "column"'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10：水平序列称为“行”，垂直序列称为“列”'
- en: '](img/fig03_10.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_10.jpg)'
- en: 'Figure 3.10: A horizontal sequence is called a "row," and a vertical one is
    called a "column"'
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.10：水平序列称为“行”，垂直序列称为“列”
- en: Matrix Multiplication
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩阵乘法
- en: 'Now, consider the product of matrices (two-dimensional arrays). For 2x2 matrices,
    matrix multiplication is calculated as shown in *Figure 3.11* (defined as the
    calculation in this procedure):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑矩阵（二维数组）的乘积。对于2x2矩阵，矩阵乘法的计算如*图3.11*所示（定义为此过程中的计算）：
- en: '![Figure 3.11: Calculating matrix multiplication'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.11：计算矩阵乘法'
- en: '](img/fig03_11.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_11.jpg)'
- en: 'Figure 3.11: Calculating matrix multiplication'
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.11：计算矩阵乘法
- en: 'As this example indicates, matrix multiplication is calculated by multiplying
    the elements between the (horizontal) rows of the left matrix and the (vertical)
    columns of the right matrix and adding the results. The calculation result is
    stored as the elements of a new multidimensional array. For example, the result
    between A''s first row and B''s first column becomes the first element in the
    first row, while the result between A''s second row and B''s first column becomes
    the first element in the second row. In this book, a matrix in an equation is
    shown in bold. For example, a matrix is shown as `A` to differentiate it from
    a scalar value (for example, a or b) with one element. This calculation is implemented
    in Python as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如本例所示，矩阵乘法通过将左矩阵的（水平）行与右矩阵的（垂直）列的元素相乘并加和来计算。计算结果被存储为新多维数组的元素。例如，A的第一行与B的第一列的结果成为第一行的第一个元素，而A的第二行与B的第一列的结果成为第二行的第一个元素。在本书中，方程中的矩阵用粗体显示。例如，矩阵用`A`表示，以区别于只有一个元素的标量值（例如，a或b）。这个计算在Python中实现如下：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A and B are 2x2 matrices. NumPy's `np.dot()` function is used to calculate the
    product of matrices A and B (the "dot" here indicates a dot product). `np.dot
    (dot product)` calculates the inner product of vectors for one-dimensional arrays
    and matrix multiplication for two-dimensional arrays. You should note that `np.dot(A,
    B)` and `np.dot(B, A)` can return different values. Unlike regular operations
    (+, *, and so on), the product of matrices becomes different when the order of
    operands (A and B) is different.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: A和B是2x2矩阵。使用NumPy的`np.dot()`函数计算矩阵A和B的乘积（这里的“dot”表示点积）。`np.dot`（点积）计算一维数组的内积以及二维数组的矩阵乘法。你需要注意的是，`np.dot(A,
    B)`和`np.dot(B, A)`可能返回不同的值。与常规运算（+，*等）不同，矩阵的乘积在操作数（A和B）的顺序不同的情况下会有所不同。
- en: 'The preceding example shows the product of 2x2 matrices. You can also calculate
    the product of matrices in different shapes. For example, the product of 2x3 and
    3x2 matrices can be implemented in Python as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例展示了2x2矩阵的乘积。你也可以计算不同形状矩阵的乘积。例如，2x3矩阵和3x2矩阵的乘积可以通过以下Python代码实现：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code shows how the product of the 2x3 matrix A and the 3x2 matrix
    B can be implemented. Here, you must be careful about the "shapes of matrices."
    Specifically, the number of elements (number of columns) in dimension 1 of matrix
    A must be the same as the number of elements (number of rows) in dimension 0 of
    matrix B. Actually, in the preceding example, matrix A is 2x3, and matrix B is
    3x2\. The number of elements in dimension 1 of matrix A (3) is the same as the
    number of elements in dimension 0 of matrix B (3). If they are different, the
    product of the matrices cannot be calculated. So then, if you try to calculate
    the product of the 2x3 matrix A and the 2x2 matrix C in Python, the following
    error occurs:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码展示了如何实现2x3矩阵A和3x2矩阵B的乘积。在这里，你必须注意“矩阵的形状”。具体来说，矩阵A的维度1中的元素数量（列数）必须与矩阵B的维度0中的元素数量（行数）相同。实际上，在上述示例中，矩阵A是2x3，矩阵B是3x2，矩阵A维度1中的元素数量（3）与矩阵B维度0中的元素数量（3）相同。如果它们不同，则无法计算矩阵的乘积。那么，如果你尝试计算2x3矩阵A和2x2矩阵C的乘积，以下错误将发生：
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Traceback (most recent call last):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 回溯（最近的调用最后）：
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This error says that dimension 1 of matrix A and dimension 0 of matrix C are
    different in terms of the numbers of their elements (the index of a dimension
    starts from zero). In other words, to calculate the product of a multidimensional
    array, the number of elements in the corresponding dimensions of two matrices
    must be the same. Because this is an important point, let''s check it again in
    *Figure 3.12*:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误提示矩阵A的维度1和矩阵C的维度0在元素数量上不相同（维度的索引从零开始）。换句话说，要计算多维数组的乘积，两个矩阵的相关维度的元素数量必须相同。因为这是一个重要的点，让我们在*图3.12*中再检查一遍：
- en: '![Figure 3.12: The number of elements in corresponding dimensions must be the'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.12：矩阵乘法时相关维度的元素数量必须相同'
- en: same for matrix multiplication
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法相同
- en: '](img/fig03_12.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_12.jpg)'
- en: 'Figure 3.12: The number of elements in corresponding dimensions must be the
    same for matrix multiplication'
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.12：矩阵乘法时相关维度的元素数量必须相同
- en: '*Figure 3.12* shows an example of the product of the 3x2 matrix A and the 2x4
    matrix B, resulting in the 3x4 matrix C. As we can see, the number of elements
    in the corresponding dimensions of matrices A and B must be the same. The resulting
    matrix, C, consists of as many rows as matrix A and as many columns as matrix
    B. This is also important.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.12*展示了3x2矩阵A和2x4矩阵B的乘积结果，得到3x4矩阵C。如我们所见，矩阵A和矩阵B的相关维度的元素数量必须相同。结果矩阵C的行数与矩阵A相同，列数与矩阵B相同。这一点也很重要。'
- en: 'Even when A is a two-dimensional matrix and B is a one-dimensional array, the
    same principle (that the number of elements in the corresponding dimensions must
    be the same) applies, as shown in *Figure 3.13*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 即使A是二维矩阵而B是一维数组，依然适用相同的原则（即相关维度的元素数量必须相同），如*图3.13*所示：
- en: '![Figure 3.13: The number of elements in the corresponding dimensions must
    be the same, even when A is a two-dimensional matrix and B is a one-dimensional
    array'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.13：即使A是二维矩阵而B是一维数组，相关维度的元素数量也必须相同'
- en: '](img/fig03_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_13.jpg)'
- en: 'Figure 3.13: The number of elements in the corresponding dimensions must be
    the same, even when A is a two-dimensional matrix and B is a one-dimensional array'
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.13：即使A是二维矩阵而B是一维数组，相关维度的元素数量也必须相同
- en: 'The sample in *Figure 3.13* can be implemented in Python as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.13*中的示例可以通过以下Python代码实现：'
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Matrix Multiplication in a Neural Network
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络中的矩阵乘法
- en: Now, let's use NumPy matrices to implement a neural network, as shown in *Figure
    3.14*. Let's assume that the neural network only has weights. Bias and an activation
    function have been omitted.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用NumPy矩阵实现神经网络，如*图3.14*所示。假设神经网络仅有权重，偏置和激活函数已省略。
- en: '![Figure 3.14: Using matrix multiplication to calculate a neural network'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.14：使用矩阵乘法计算神经网络'
- en: '](img/fig03_14.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_14.jpg)'
- en: 'Figure 3.14: Using matrix multiplication to calculate a neural network'
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.14：使用矩阵乘法计算神经网络
- en: 'In this implementation, we must be careful about the shapes of `X`, `W`, and
    `Y`. It is very important that the number of elements in the corresponding dimensions
    of `X` and `W` are the same:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实现中，我们必须注意`X`、`W`和`Y`的形状。非常重要的一点是，`X`和`W`对应维度中的元素数量必须相同：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As shown here, you can use `np.dot` (dot product of multidimensional matrices)
    to calculate the result, `Y`, at one time. This means that, even if the number
    of elements of `Y` is 100 or 1,000, you can calculate it all at once. Without
    `np.dot`, you must take out each element of `Y` (and use a `for` statement) for
    calculation, which is very tiresome. Therefore, we can say that the technique
    of using matrix multiplication to calculate the product of multidimensional matrices
    is very important.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，你可以使用`np.dot`（多维矩阵的点积）一次性计算结果`Y`。这意味着，即使`Y`的元素数量是100或1000，你也可以一次性计算出来。如果没有`np.dot`，你就必须逐个提取`Y`的元素（并使用`for`语句）进行计算，这样非常繁琐。因此，我们可以说，使用矩阵乘法来计算多维矩阵的乘积这一技巧非常重要。
- en: Implementing a Three-Layer Neural Network
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现一个三层神经网络
- en: Now, let's implement a "practical" neural network. Here, we will implement the
    process from its input to its output (a process in the forward direction) in the
    three-layer neural network shown in *Figure 3.15*. We will use NumPy's multidimensional
    arrays (as described in the previous section) for implementation. By making good
    use of NumPy arrays, you can write some short code for a forward process in the
    neural network.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个“实际的”神经网络。在这里，我们将实现从输入到输出的过程（一个前向传播过程），并且这个过程使用的是*图3.15*中所示的三层神经网络。我们将使用NumPy的多维数组（如前一节所述）来实现。通过充分利用NumPy数组，你可以为神经网络的前向传播过程编写简洁的代码。
- en: Examining the Symbols
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查符号
- en: 'Here, we will use symbols such as ![5c](img/Figure_3.15a_-_Copy.png) and ![5d](img/Figure_3.15b.png)
    to explain the processes performed in the neural network. They may seem a little
    complicated. You can skim through this section because the symbols are only used
    here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用像![5c](img/Figure_3.15a_-_Copy.png)和![5d](img/Figure_3.15b.png)这样的符号来解释神经网络中执行的过程。它们可能看起来有点复杂。你可以快速浏览这一部分，因为这些符号仅在此处使用：
- en: '![Figure 3.15: A three-layer neural network consisting of two neurons in the
    input layer (layer 0), three neurons in the first hidden layer (layer 1), two
    neurons in the second hidden layer (layer 2), and two neurons in the output layer
    (layer 3)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.15：一个三层神经网络，包括输入层（层0）中的两个神经元，第一个隐藏层（层1）中的三个神经元，第二个隐藏层（层2）中的两个神经元，以及输出层（层3）中的两个神经元'
- en: '](img/fig03_15.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_15.jpg)'
- en: 'Figure 3.15: A three-layer neural network consisting of two neurons in the
    input layer (layer 0), three neurons in the first hidden layer (layer 1), two
    neurons in the second hidden layer (layer 2), and two neurons in the output layer
    (layer 3)'
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.15：一个三层神经网络，包括输入层（层0）中的两个神经元，第一个隐藏层（层1）中的三个神经元，第二个隐藏层（层2）中的两个神经元，以及输出层（层3）中的两个神经元
- en: Note
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: What is important in this section is that calculating a neural network can be
    conducted collectively as a matrix calculation. Calculating each layer in a neural
    network can be conducted collectively using matrix multiplication (this can be
    considered from a larger viewpoint). So, there is no problem in understanding
    subsequent explanations, even if you forget the detailed rules relating to these symbols.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中重要的是，神经网络的计算可以作为矩阵计算来统一进行。神经网络中每一层的计算可以通过矩阵乘法统一进行（从更广泛的视角来看，这也是合理的）。因此，即使你忘记了与这些符号相关的详细规则，也不会影响后续的理解。
- en: Let's begin by defining the symbols. Look at *Figure 3.16*. This diagram illustrates
    the weight from the input layer x2 to the neuron *a![5e](img/Figure_3.15c.png)*
    in the next layer.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义符号开始。请查看*图3.16*。该图展示了从输入层x2到下一层神经元*a![5e](img/Figure_3.15c.png)*的权重。
- en: 'As shown in *Figure 3.16*, "(1)" is placed at the upper right of a weight or
    a hidden layer neuron. This number indicates the weight or neuron of layer 1\.
    A weight has two numbers at the lower right, which are the index numbers of the
    next and previous layer neurons. For example, *![5f](img/Figure_3.15d.png)* indicates
    that it is the weight from the second neuron (*x*2) in the previous layer to the
    first neuron (*![5g](img/Figure_3.15e.png)*) in the next layer. The index numbers
    at the lower right of weight must be in the order of "the number for the next
    layer and the number for the previous layer":'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.16* 所示，“(1)”位于权重或隐藏层神经元的右上角。这个数字表示第 1 层的权重或神经元。一个权重在右下角有两个数字，表示下一层和上一层神经元的索引。例如，*![5f](img/Figure_3.15d.png)*
    表示它是上一层第二个神经元（*x*2）到下一层第一个神经元（*![5g](img/Figure_3.15e.png)*) 的权重。权重右下角的索引数字必须按照“下一层的编号和上一层的编号”顺序排列：
- en: '![Figure 3.16: Weight symbols'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.16：权重符号'
- en: '](img/fig03_16.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_16.jpg)'
- en: 'Figure 3.16: Weight symbols'
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.16：权重符号
- en: Implementing Signal Transmission in Each Layer
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现每层信号传递
- en: 'Now, let''s look at transmitting signals from the input layer to "the first
    neuron in layer 1." *Figure 3.17* shows this graphically:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来看一下从输入层到“第 1 层的第一个神经元”的信号传递。*图 3.17* 以图示方式展示了这一过程：
- en: '![Figure 3.17: Transmitting signals from the input layer to layer 1'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.17：从输入层到第 1 层的信号传递'
- en: '](img/fig03_17.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_17.jpg)'
- en: 'Figure 3.17: Transmitting signals from the input layer to layer 1'
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.17：从输入层到第 1 层的信号传递
- en: 'As shown in *Figure 3.17*, ① is added as a neuron for a bias. Note that there
    is only one index at the lower right of the bias. This is because only one bias
    neuron (① neuron) exists in the previous layer. Now, let''s express *![11](img/Figure_3.15g.png)*
    as an equation to review what we have learned so far. *![12](img/Figure_3.15f.png)*
    is the sum of the weighted signals and the bias and is calculated as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.17* 所示，① 被添加为偏置神经元。请注意，偏置的右下角只有一个索引。这是因为在上一层中只有一个偏置神经元（① 神经元）。现在，让我们将
    *![11](img/Figure_3.15g.png)* 表达为一个方程式，回顾一下我们迄今为止所学的内容。*![12](img/Figure_3.15f.png)*
    是加权信号和偏置的总和，并按照以下方式计算：
- en: '| ![13](img/Figure_3.17h.png) | (3.8) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ![13](img/Figure_3.17h.png) | (3.8) |'
- en: 'By using matrix multiplication, you can express "the weighted sum" of layer
    1 collectively as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用矩阵乘法，可以将第 1 层的“加权和”整体表示为：
- en: '| ![14](img/Figure_3.17i.png) | (3.9) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| ![14](img/Figure_3.17i.png) | (3.9) |'
- en: 'Here, A(1), X, B(1), and W(1) are as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，A(1)、X、B(1) 和 W(1) 如下所示：
- en: '![15](img/Figure_3.17j.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![15](img/Figure_3.17j.png)'
- en: 'Now, let''s use NumPy''s multidimensional arrays to implement equation (3.9).
    Arbitrary values are set for input signals, weights, and biases here:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用 NumPy 的多维数组来实现方程（3.9）。这里为输入信号、权重和偏置设置了任意值：
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This calculation is the same as the one in the previous section. W1 is a 2x3
    array and X is a one-dimensional array with two elements. Also, in this case,
    the number of elements in the corresponding dimensions of W1 and X are the same.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该计算与上一节中的计算相同。W1 是一个 2x3 的数组，X 是一个包含两个元素的单维数组。此外，在这种情况下，W1 和 X 的对应维度中的元素个数相同。
- en: Now, consider the processes performed by the activation function in layer 1\.
    *Figure 3.18* shows these processes graphically.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑激活函数在第 1 层中执行的过程。*图 3.18* 以图示方式展示了这些过程。
- en: 'As shown in *Figure 3.18*, the weighted sums in a hidden layer (the total of
    the weighted signals and the biases) are shown as *a''*s, and the signals converted
    with the activation function are shown as *z*''s. Here, the activation function
    is shown as *h*() using a sigmoid function:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.18* 所示，隐藏层中的加权和（加权信号和偏置的总和）表示为 *a'*，通过激活函数转换的信号表示为 *z*'。这里，激活函数表示为 *h*()，使用的是
    sigmoid 函数：
- en: '![Figure 3.18: Transmitting signals from the input layer to layer 1'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.18：从输入层到第 1 层的信号传递'
- en: '](img/fig03_18.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_18.jpg)'
- en: 'Figure 3.18: Transmitting signals from the input layer to layer 1'
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.18：从输入层到第 1 层的信号传递
- en: 'This process is implemented in Python as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在 Python 中的实现如下：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This `sigmoid()` function is the one we defined previously. It takes a NumPy
    array and returns a NumPy array with the same number of elements.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `sigmoid()` 函数是我们之前定义的。它接受一个 NumPy 数组并返回一个具有相同元素个数的 NumPy 数组。
- en: 'Let''s now move on to the implementation from layer 1 to layer 2 (*Figure 3.19*):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们继续讲解从第 1 层到第 2 层的实现（*图 3.19*）：
- en: '![Figure 3.19: Transmitting signals from layer 1 to layer 2'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.19：从第 1 层到第 2 层的信号传递'
- en: '](img/fig03_19.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_19.jpg)'
- en: 'Figure 3.19: Transmitting signals from layer 1 to layer 2'
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.19：从第一层到第二层的信号传递
- en: 'This implementation is the same as the previous one, except that the output
    of layer 1 (Z1) is the input of layer 2\. As you can see, you can write the transmission
    of signals from one layer to another easily by using NumPy arrays:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现与之前的实现相同，不同之处在于第一层的输出（Z1）是第二层的输入。如你所见，通过使用NumPy数组，可以轻松地实现信号从一层到另一层的传递：
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, let''s implement the transmission of signals from layer 2 to the output
    layer (*Figure 3.20*). You can implement the output layer almost in the same way
    as the other implementations we''ve looked at so far. Only the last activation
    function is different from that of the hidden layers we''ve seen so far:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现从第二层到输出层的信号传递（*图 3.20*）。你可以几乎以与我们之前看到的其他实现相同的方式实现输出层。唯一不同的是最后的激活函数与我们之前看到的隐藏层的激活函数不同：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, we will define a function named `identity_function()` and use it as the
    activation function for the output layer. An identity function outputs the input
    as it is. Although you do not need to define `identity_function()` in this example,
    this implementation is used so that it is consistent with the previous ones. In
    *Figure 3.20*, the activation function of the output layer is shown as `σ()` to
    indicate that it is different from the activation function, *h*(), of the hidden
    layers (`σ` is called **sigma**):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将定义一个名为`identity_function()`的函数，并将其作为输出层的激活函数。单位函数将输入原样输出。尽管在这个例子中不需要定义`identity_function()`，但为了与之前的实现保持一致，使用了该实现。在*图
    3.20*中，输出层的激活函数显示为`σ()`，以表明它与隐藏层的激活函数*h*()不同（`σ`被称为**sigma**）：
- en: '![Figure 3.20: Transmitting signals from layer 2 to the output layer'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.20：从第二层到输出层的信号传递'
- en: '](img/fig03_20.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_20.jpg)'
- en: 'Figure 3.20: Transmitting signals from layer 2 to the output layer'
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.20：从第二层到输出层的信号传递
- en: You can select the activation function used in the output layer, depending on
    what type of problem you wish to solve. Generally, an identity function is used
    for a regression problem, a sigmoid function for a two-class classification problem,
    and a softmax function for a multi-class classification problem. The activation
    function for an output layer will be explained in detail in the next section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据希望解决的问题类型选择输出层使用的激活函数。通常，对于回归问题使用单位函数，对于二分类问题使用sigmoid函数，对于多分类问题使用softmax函数。输出层的激活函数将在下一节中详细解释。
- en: Implementation Summary
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现总结
- en: 'This completes our investigation of a three-layer neural network. The following
    summarizes the implementation we''ve performed so far. As is customary in the
    implementation of a neural network, only weights are written in uppercase (for
    example, W1), while other items (such as a bias and intermediate result) are written
    in lowercase:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这也完成了我们对三层神经网络的研究。以下总结了我们到目前为止的实现过程。按照神经网络实现的惯例，只有权重采用大写字母（例如，W1），而其他项（如偏置和中间结果）则采用小写字母：
- en: '[PRE22]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, the `init_network()` and `forward()` functions are defined. The `init_network()`
    function initializes the weights and biases and stores them in a dictionary type
    variable, `network` which stores the parameters required for individual layers,
    weights, and biases. The `forward()` function collectively implements the process
    of converting an input signal into an output signal.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这里定义了`init_network()`和`forward()`函数。`init_network()`函数初始化权重和偏置，并将其存储在字典类型的变量`network`中，该变量存储了各个层所需的参数、权重和偏置。`forward()`函数则实现了将输入信号转化为输出信号的过程。
- en: The word "forward" here indicates the transmission process from an input to
    an output. Later, we will look at the process in the backward direction (from
    output to input) when we train a neural network.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的“forward”表示从输入到输出的传递过程。稍后，在我们训练神经网络时，我们将研究反向过程（从输出到输入）。
- en: This completes the implementation of a three-layer neural network in the forward
    direction. By using NumPy's multidimensional arrays, we were able to implement
    a neural network efficiently.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了前向传播方向上三层神经网络的实现。通过使用NumPy的多维数组，我们能够高效地实现神经网络。
- en: Designing the Output Layer
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计输出层
- en: You can use a neural network both for a classification problem and for a regression
    problem. However, you must change the activation function of the output layer,
    depending on which of the problems you use a neural network for. Usually, an identity
    function is used for a regression problem, and a softmax function is used for
    a classification problem.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将神经网络同时用于分类问题和回归问题。然而，你必须根据所处理的问题更改输出层的激活函数。通常，回归问题使用恒等函数，分类问题使用 softmax
    函数。
- en: Note
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Machine learning problems can be broadly divided into "classification problems"
    and "regression problems." A classification problem is a problem of identifying
    which class the data belongs to—for example, classifying the person in an image
    as a man or a woman—while a regression problem is a problem of predicting a (continuous)
    number from certain input data—for example, predicting the weight of the person
    in an image.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题可以大致分为“分类问题”和“回归问题”。分类问题是指识别数据属于哪个类别——例如，将图像中的人分类为男性或女性——而回归问题则是从某些输入数据中预测一个（连续的）数值——例如，预测图像中人的体重。
- en: Identity Function and Softmax Function
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恒等函数与 Softmax 函数
- en: 'An identity function outputs the input as it is. The function that outputs
    what is entered without doing anything is an identity function. Therefore, when
    an identity function is used for the output layer, an input signal is returned
    as-is. Using the diagram of the neural network we''ve used so far, you can represent
    the process by an identity function as shown in *Figure 3.21*. The process of
    conversion by the identity function can be represented with one arrow, in the
    same way in the same way as the activation function we have seen so far:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 恒等函数的输出就是输入本身。一个不进行任何处理、直接输出输入内容的函数就是恒等函数。因此，当输出层使用恒等函数时，输入信号会原样返回。利用我们迄今使用的神经网络图，你可以像
    *图 3.21* 所示一样，用恒等函数表示这个过程。恒等函数的转换过程可以通过一条箭头来表示，方式与我们之前见过的激活函数类似：
- en: '![Figure 3.21: Identity function'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.21：恒等函数](img/Figure_3.21a.png)'
- en: '](img/fig03_21.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_21.jpg)'
- en: 'Figure 3.21: Identity function'
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.21：恒等函数
- en: 'The softmax function, which is used for a classification problem, is expressed
    by the following equation:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类问题的 softmax 函数可以通过以下方程表示：
- en: '| ![16](img/Figure_3.21a.png) | (3.10) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ![16](img/Figure_3.21a.png) | (3.10) |'
- en: '`exp(x)` is an exponential function that indicates ex (e is Napier''s constant,
    2.7182…). Assuming the total number of output layers is n, the equation provides
    the k-th output, yk. As shown in equation (3.10), the numerator of the softmax
    function is the exponential function of the input signal, *a*k, and the denominator
    is the sum of the exponential functions of all the input signals.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`exp(x)` 是一个指数函数，表示 ex（e 是自然常数，约为 2.7182…）。假设输出层的总数为 n，方程提供了第 k 个输出，yk。如方程（3.10）所示，softmax
    函数的分子是输入信号 *a*k 的指数函数，分母是所有输入信号指数函数的总和。'
- en: '*Figure 3.22* shows the softmax function graphically. As you can see, the output
    of the softmax function is connected from all the input signals with arrows. As
    the denominator of equation (3.10) indicates, each neuron of the output is affected
    by all the input signals:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.22* 通过图形方式展示了 softmax 函数。如你所见，softmax 函数的输出是通过箭头与所有输入信号连接的。正如方程（3.10）所示，输出的每个神经元都受到所有输入信号的影响：'
- en: '![Figure 3.22: Softmax function'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.22：Softmax 函数](img/fig03_22.jpg)'
- en: '](img/fig03_22.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_22.jpg)'
- en: 'Figure 3.22: Softmax function'
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.22：Softmax 函数
- en: 'Now, let''s implement the softmax function,using the Python interpreter to
    check the results, one by one:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现 softmax 函数，并使用 Python 解释器逐个检查结果：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This implementation represents the softmax function of equation (3.10) with
    Python. Therefore, no special description will be required. When considering the
    use of the softmax function later, we will define it as a Python function, as
    follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该实现通过 Python 表示了方程（3.10）的 softmax 函数。因此，不需要额外的描述。当我们以后使用 softmax 函数时，我们将其定义为一个
    Python 函数，如下所示：
- en: '[PRE24]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Issues when Implementing the Softmax Function
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 Softmax 函数时的问题
- en: The preceding implementation of the softmax function represents equation (3.10)
    correctly, but it is defective for computer calculations. This defect is an overflow
    problem. Implementing the softmax function involves calculating the exponential
    functions, and the value of an exponential function can be very large. For example,
    *e*10 is larger than 20,000, and *e*100 is a large value that has more than 40
    digits. The result of *e*1000 returns `inf`, which indicates an infinite value.
    Dividing these large values returns an "unstable" result.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 上述实现的 softmax 函数正确地表示了方程 (3.10)，但在计算机计算中存在缺陷。这个缺陷是溢出问题。实现 softmax 函数涉及计算指数函数，而指数函数的值可能非常大。例如，*e*10
    的值大于 20,000，*e*100 是一个超过 40 位的巨大值。*e*1000 的结果返回 `inf`，表示无限大值。将这些大值相除会得到一个“不稳定”的结果。
- en: Note
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: When a computer handles a "number," it is stored in finite data width, such
    as four or eight bytes. This means that a number has a number of significant figures.
    The range of a number that can be represented is limited. Therefore, there is
    a problem in that a very large value cannot be expressed. This is called an overflow,
    so we must be careful when we use a computer for calculation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算机处理“数字”时，它会以有限的数据宽度存储，例如四个或八个字节。这意味着一个数字有一定的有效数字位数。一个数字能够表示的范围是有限的。因此，会存在无法表达非常大值的问题，这称为溢出，所以我们在使用计算机进行计算时必须小心。
- en: 'Improved implementation of the softmax function is obtained from the following equation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: softmax 函数的改进实现来源于以下方程：
- en: '| ![17](img/Figure_3.22a.png) | (3.11) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| ![17](img/Figure_3.22a.png) | (3.11) |'
- en: First, equation (3.11) is transformed by multiplying both the numerator and
    the denominator by an arbitrary constant, *C* (the same calculations are performed
    because both the numerator and the denominator are multiplied by the same constant).
    Then, C is moved into the exponential function (exp) as log *C*. Finally, log
    *C* is replaced with another symbol, *C'*.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过将分子和分母同时乘以一个任意常数 *C* 来变换方程 (3.11)（由于分子和分母都乘以相同的常数，因此进行的计算是相同的）。然后，将 C 移入指数函数
    (exp)，表示为 log *C*。最后，将 log *C* 替换为另一个符号 *C'*。
- en: 'Equation (3.11) says that adding or subtracting a certain constant does not
    change the result when the exponential functions in the softmax function are calculated.
    Although you can use any number as *C''* here, the largest value from the input
    signals is usually used to prevent an overflow. Consider the following example:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 (3.11) 表明，在计算 softmax 函数中的指数函数时，加上或减去某个常数不会改变结果。虽然在这里你可以使用任何数作为 *C'*，但通常使用输入信号中的最大值来防止溢出。考虑以下示例：
- en: '[PRE25]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As this example indicates, when the largest value of the input signals (*c*,
    here) is subtracted, you can calculate the function properly. Otherwise, nan (not
    a number: unstable) values are returned. Based on this description, we can implement
    the softmax function as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如本例所示，当输入信号的最大值 (*c*，此处) 被减去时，你可以正确计算该函数。否则，将返回 nan（非数：不稳定）值。基于此描述，我们可以按如下方式实现
    softmax 函数：
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Characteristics of the Softmax Function
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 函数的特点
- en: 'You can use the `softmax()` function to calculate the output of the neural
    network, as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 `softmax()` 函数来计算神经网络的输出，如下所示：
- en: '[PRE27]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The softmax function outputs a real number between 0 and 1.0\. The total of
    the outputs of the softmax function is 1\. The fact that the total is 1 is an
    important characteristic of the softmax function as it means we can interpret
    the output of the softmax function as "probability."
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 函数输出一个介于 0 和 1.0 之间的实数。softmax 函数的输出总和为 1。总和为 1 是 softmax 函数的一个重要特性，因为它意味着我们可以将
    softmax 函数的输出解释为“概率”。
- en: 'For instance, in the preceding example, we could interpret the probability
    of `y[0]` as `0.018` (1.8%), the probability of `y[1]` as `0.245` (24.5%), and
    the probability of `y[2]` as `0.737` (73.7%). From these probabilities, we can
    say, "because the second element is the most probable, the answer is the second
    class." We can even answer probabilistically: "the answer is the second class
    with a probability of 74%, the first class with a probability of 25%, and the
    zeroth class with a probability of 1%." Thus, you can use the softmax function
    to handle a problem probabilistically (statistically).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，在前面的例子中，我们可以将 `y[0]` 的概率解释为 `0.018`（1.8%），`y[1]` 的概率为 `0.245`（24.5%），`y[2]`
    的概率为 `0.737`（73.7%）。根据这些概率，我们可以说，“由于第二个元素的概率最高，答案是第二类。”我们甚至可以用概率性地回答：“答案是第二类，概率为
    74%，第一类的概率为 25%，零类的概率为 1%。”因此，你可以使用 softmax 函数来概率性地（统计学上）处理问题。
- en: We should note that applying the softmax function does not change the order
    of the elements. This is because an exponential function, *(y = exp(x))*, increases
    monotonically. Actually, in the preceding example, the order of the elements in
    `a` is the same as those of the elements in `y`. The largest value in `a` is the
    second element, and the largest value in `y` is also the second element.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，应用 softmax 函数不会改变元素的顺序。这是因为指数函数，*(y = exp(x))*，是单调递增的。实际上，在前面的例子中，`a`
    中元素的顺序与 `y` 中元素的顺序相同。`a` 中的最大值是第二个元素，而 `y` 中的最大值也是第二个元素。
- en: Generally, class classification by a neural network recognizes only the class
    that corresponds to the neuron with the largest output. Using the softmax function
    does not change the position of the neuron of the largest output. Therefore, you
    can omit the softmax function for the output layer from neural network classification.
    In reality, the softmax function for the output layer is usually omitted because
    the exponential function requires some computation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络的分类任务只会识别与最大输出对应的类别。使用 softmax 函数并不会改变最大输出对应的神经元位置。因此，你可以在神经网络分类中省略输出层的
    softmax 函数。实际上，由于指数函数需要一些计算，输出层的 softmax 函数通常会被省略。
- en: Note
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'The procedure for solving a machine learning problem consists of two phases:
    "training" and "predicting." First, you train a model in the training phase and
    then use the trained model to predict (classify) unknown data in the inference
    phase. As described earlier, the softmax function for the output layer is usually
    omitted in the inference phase. The reason we use the softmax function for the
    output layer will be relevant when the neural network trains (for more details,
    refer to the next chapter).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 解决机器学习问题的过程分为两个阶段：“训练”和“预测”。首先，在训练阶段你训练一个模型，然后使用训练好的模型在推理阶段对未知数据进行预测（分类）。如前所述，推理阶段通常省略输出层的
    softmax 函数。我们之所以在输出层使用 softmax 函数，是因为它对神经网络的训练有重要作用（更多细节请参考下一章）。
- en: Number of Neurons in the Output Layer
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出层中的神经元数量
- en: 'You must determine the number of neurons in the output layer as appropriate,
    depending on the problem to solve. For classification problems, the number of
    classes to classify is usually used as the number of neurons in the output layer.
    For example, to predict a number from `0` to `9` from an input image (10-class
    classification), 10 neurons are placed in the output layer, as shown in *Figure
    3.23*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须根据要解决的问题来确定输出层中神经元的数量。对于分类问题，分类的类别数通常作为输出层神经元的数量。例如，要从输入图像中预测一个从 `0` 到 `9`
    的数字（10 类分类），输出层会有 10 个神经元，如 *图 3.23* 所示：
- en: '![Figure 3.23: The neuron in the output layer corresponds to each number'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.23：输出层中的神经元对应于每个数字'
- en: '](img/fig03_23.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_23.jpg)'
- en: 'Figure 3.23: The neuron in the output layer corresponds to each number'
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.23：输出层中的神经元对应于每个数字
- en: As shown in *Figure 3.23*, the neurons in the output layer correspond to the
    numbers 0, 1, ..., 9 from the top. Here, the various shades of gray represent
    the values of the neurons in the output layer. In this example, the color of *y*2
    is the darkest because the *y*2 neuron outputs the largest value. It shows that
    this neural network predicts that the input belongs to the class that corresponds
    to *y*2; that is, "2."
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.23* 所示，输出层中的神经元从上到下分别对应数字 0、1、...、9。这里，各种灰度的不同深浅表示输出层神经元的值。在这个例子中，*y*2
    的颜色最深，因为 *y*2 神经元输出的值最大。这表明该神经网络预测输入属于与 *y*2 对应的类别；也就是“2”。
- en: Handwritten Digit Recognition
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手写数字识别
- en: Now that we have covered the mechanisms of a neural network, let's consider
    a practical problem. We will classify some handwritten digit images. Assuming
    that training has already been completed, we will use trained parameters to implement
    "inference" in the neural network. This inference is also called forward propagation
    in a neural network.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了神经网络的机制，让我们考虑一个实际的问题。我们将对一些手写数字图像进行分类。假设训练已经完成，我们将使用训练好的参数在神经网络中实现“推理”。在神经网络中，这种推理也称为前向传播。
- en: Note
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In the same way as the procedure for solving a machine learning problem (which
    consists of two phases, "training" and "inference"), to solve a problem using
    a neural network, we will use training data to train the weight parameters and
    then use the trained parameters while predicting to classify the input data.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与解决机器学习问题的过程相同（包括“训练”和“推理”两个阶段），要使用神经网络解决问题，我们将使用训练数据训练权重参数，然后在预测时使用训练好的参数对输入数据进行分类。
- en: MNIST Dataset
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MNIST数据集
- en: Here, we will use a set of images of handwritten digits called MNIST. MNIST
    is one of the most famous datasets in the field of machine learning and is used
    in various ways, from simple experiments to research. When you read research papers
    on image recognition or machine learning, you will often notice that the MNIST
    dataset is used as experimental data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用称为MNIST的手写数字图像集。MNIST是机器学习领域中最著名的数据集之一，并以从简单实验到研究的各种方式使用。当您阅读有关图像识别或机器学习的研究论文时，您经常会注意到MNIST数据集作为实验数据的使用。
- en: 'The MNIST dataset consists of images of numbers from 0 to 9 (*Figure 3\. 24*).
    It contains 60,000 training images and 10,000 test images, and they are used for
    training and inference. When we use the MNIST dataset, we usually use training
    images for training and measure how correctly the trained model can classify the
    test images:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含从0到9的数字图像（*Figure 3\. 24*）。它包含60,000张训练图像和10,000张测试图像，用于训练和推理。当我们使用MNIST数据集时，通常使用训练图像进行训练，并测量训练模型如何正确分类测试图像：
- en: '![Figure 3.24: Examples from the MNIST image dataset'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.24：MNIST图像数据集示例'
- en: '](img/fig03_24.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[img/fig03_24.jpg)'
- en: 'Figure 3.24: Examples from the MNIST image dataset'
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.24：MNIST图像数据集示例
- en: MNIST's image data is a 28x28 gray image (one channel), and each pixel has a
    value from 0 to 255\. Each image data is labeled, such as "7", "2", and "1."
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST的图像数据是28x28的灰度图像（一个通道），每个像素值从0到255。每个图像数据都有标签，如“7”，“2”和“1”。
- en: 'This book provides a convenient Python script, `mnist.py`, which is located
    in the `dataset` directory. It supports downloading the MNIST dataset and converting
    image data into NumPy arrays. To use the `mnist.py` script, the current directory
    must be the `ch01`, `ch02`, `ch03`, ..., or `ch08` directory. By using the `load_mnist()`
    function in `mnist.py`, you can load the MNIST data easily, as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本书提供了一个方便的Python脚本`mnist.py`，位于`dataset`目录中。它支持下载MNIST数据集并将图像数据转换为NumPy数组。要使用`mnist.py`脚本，当前目录必须是`ch01`、`ch02`、`ch03`、...
    或 `ch08`目录。通过在`mnist.py`中使用`load_mnist()`函数，您可以轻松加载MNIST数据，如下所示：
- en: '[PRE28]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: First, configure the details for importing the files in the parent directory.
    Then, import the `load_mnist` function from `dataset`/`mnist.py`. Finally, use
    the imported `load_mnist` function to load the MNIST dataset. When you call `load_mnist`
    for the first time, an internet connection is required to download the MNIST data.
    A subsequent call completes immediately because it only loads the locally saved
    files (pickle files).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，配置导入父目录中文件的详细信息。然后，从`dataset`/`mnist.py`导入`load_mnist`函数。最后，使用导入的`load_mnist`函数加载MNIST数据集。第一次调用`load_mnist`时，需要互联网连接下载MNIST数据。后续调用因为仅加载本地保存的文件（pickle文件），所以完成时间很快。
- en: Note
  id: totrans-274
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The files for loading the MNIST images exist in the dataset directory of the
    source code provided in this book. It is assumed that this MNIST dataset is used
    only from the `ch01`, `ch02`, `ch03`, ..., or `ch08` directory. Therefore, to
    use the dataset, the `sys.path.append(os.pardir)` statement is required. This
    is because the files in the parent directory (dataset directory) must be imported.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 用于加载MNIST图像的文件位于本书提供的源代码的数据集目录中。假定此MNIST数据集仅从`ch01`、`ch02`、`ch03`、... 或 `ch08`目录中使用。因此，要使用数据集，需要`sys.path.append(os.pardir)`语句。这是因为必须导入父目录（数据集目录）中的文件。
- en: 'The `load_mnist` function returns the loaded MNIST data in the format of `(training
    image, training label), (test image, test label)`. It can take three arguments:
    `load_mnist(normalize=True, flatten=True, one_hot_label=False)`. The first argument,
    `normalize`, specifies whether to normalize the input image between 0.0 and 1.0\.
    If `False` is set, the pixel of the input image remains between 0 and 255\. The
    second argument, `flatten`, specifies whether to flatten the input image (convert
    it into a one-dimensional array). If `False` is set, the input image is stored
    as an array with three dimensions (1 × 28 × 28). If `True` is set, it is stored
    as a one-dimensional array with 784 elements. The third argument, `one_hot_label`,
    specifies whether to store the label using one-hot encoding. In a one-hot encoded
    array, only the element for the correct label is 1 and the other elements are
    0, such as in [0,0,1,0,0,0,0,0,0,0]. When `one_hot_label` is `False`, only the
    correct label, such as 7 or 2, is stored. If `one_hot_label` is `True`, the labels
    are stored as a one-hot encoded array.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_mnist` 函数返回加载的 MNIST 数据，格式为 `(训练图像，训练标签)，(测试图像，测试标签)`。它可以接受三个参数：`load_mnist(normalize=True,
    flatten=True, one_hot_label=False)`。第一个参数 `normalize` 指定是否将输入图像归一化到 0.0 到 1.0
    之间。如果设置为 `False`，输入图像的像素值将保持在 0 到 255 之间。第二个参数 `flatten` 指定是否将输入图像展平（将其转换为一维数组）。如果设置为
    `False`，输入图像将以三维数组（1 × 28 × 28）存储。如果设置为 `True`，图像将以一维数组形式存储，包含 784 个元素。第三个参数 `one_hot_label`
    指定是否使用独热编码存储标签。在独热编码的数组中，只有正确标签的元素为 1，其他元素为 0，例如 [0,0,1,0,0,0,0,0,0,0]。当 `one_hot_label`
    为 `False` 时，标签将仅存储正确的标签值，如 7 或 2；当 `one_hot_label` 为 `True` 时，标签将存储为独热编码数组。'
- en: Note
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Python has a convenient feature called pickle, which saves objects as files
    while a program is being executed. By loading the saved pickle file, you can immediately
    restore the object that was used during the execution of the program. The `load_mnist()`
    function, which loads the MNIST dataset, also uses pickle (for the second or subsequent
    loading phases). By using pickle's feature, you can prepare the MNIST data quickly.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Python 有一个方便的功能，叫做 pickle，它可以在程序执行过程中将对象保存为文件。通过加载保存的 pickle 文件，你可以立即恢复程序执行过程中使用的对象。`load_mnist()`
    函数（用于加载 MNIST 数据集）也使用 pickle（用于第二次或之后的加载阶段）。通过使用 pickle 的功能，你可以快速准备 MNIST 数据。
- en: 'Now, let''s display MNIST images to check the data. We will use the `ch03/mnist_show.py`):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们显示 MNIST 图像以检查数据。我们将使用 `ch03/mnist_show.py`。
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This results in the following output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 3.25: Displaying an MNIST image'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.25：显示 MNIST 图像'
- en: '](img/fig03_25.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_25.jpg)'
- en: 'Figure 3.25: Displaying an MNIST image'
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.25：显示 MNIST 图像
- en: Please note that when `flatten=True`, the loaded image is stored as a NumPy
    array in a line (one-dimensionally). Therefore, to display the image, you must
    reshape it into its original 28x28 size. You can use the `reshape()` method to
    reshape a NumPy array by specifying the desired shape with an argument. You must
    also convert the image data stored as a NumPy array into the data object for PIL.
    You can use `Image.fromarray()` for this conversion.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当 `flatten=True` 时，加载的图像会作为一维的 NumPy 数组存储。因此，为了显示图像，你必须将其重新调整为原始的 28x28
    大小。你可以使用 `reshape()` 方法，通过指定所需的形状来调整 NumPy 数组的形状。你还需要将以 NumPy 数组存储的图像数据转换为 PIL
    所需的数据对象。你可以使用 `Image.fromarray()` 进行此转换。
- en: Inference for Neural Network
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络推理
- en: 'Now, let''s implement a neural network that predicts over this MNIST dataset.
    The network consists of an input layer containing 784 neurons and an output layer
    containing 10 neurons. The number 784 for the input layer comes from the image
    size (28 x 28 = 784), while the number 10 for the output layer comes from 10-class
    classification (10 classes of numbers 0 to 9). There are two hidden layers: the
    first one has 50 neurons, and the second one has 100 neurons. You can change the
    numbers 50 and 100 as you like. First, let''s define the three functions, `get_data()`,
    `init_network()`, and `predict()` (the following source code is located at `ch03/neuralnet_mnist.py`):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个神经网络，用于预测 MNIST 数据集上的数据。该网络由一个包含 784 个神经元的输入层和一个包含 10 个神经元的输出层组成。输入层的
    784 来自于图像的大小（28 x 28 = 784），而输出层的 10 来自于 10 类分类（数字 0 到 9 的 10 个类别）。有两个隐藏层：第一个隐藏层有
    50 个神经元，第二个隐藏层有 100 个神经元。你可以根据需要更改 50 和 100 的值。首先，我们定义三个函数：`get_data()`、`init_network()`
    和 `predict()`（以下源代码位于 `ch03/neuralnet_mnist.py`）：
- en: '[PRE30]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `init_network()` function loads the trained weight parameters that are
    stored in the pickle file `sample_weight.pkl`. This file contains weight and bias
    parameters as a dictionary type variable. The remaining two functions are almost
    the same as in the implementations described so far, so these don''t need to be
    described. Now, we will use these three functions to predict using a neural network.
    We want to evaluate the recognition precision—that is, how correctly it can classify:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_network()`函数加载了存储在pickle文件`sample_weight.pkl`中的训练权重参数。这个文件包含了权重和偏差参数，并以字典类型变量的形式存储。剩下的两个函数与之前描述的实现几乎相同，因此不需要再次描述。现在，我们将使用这三个函数来进行神经网络的预测。我们想要评估识别精度——即它能够多正确地分类：'
- en: '[PRE31]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, we will obtain the MNIST dataset and build a network, then use a `for`
    statement to get each image data stored in `x` and use the `predict()` function
    to classify it. The `predict()` function returns a NumPy array containing the
    probability of each label. For example, an array such as [0.1, 0.3, 0.2, …, 0.04]
    is returned, which indicates that the probability of "0" is 0.1, that of "1" is
    0.3, and so on. The index with the largest value in this probability list, which
    indicates the most probable element, is obtained as the prediction result. You
    can use `np.argmax(x)` to obtain the index of the largest element in an array.
    It returns the index of the largest element in the array specified by the `x`
    argument. Finally, the answers predicted by the neural network and the correct
    labels are compared, and the rate of correct predictions is displayed as the recognition
    precision (accuracy).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将获取MNIST数据集并构建一个网络，然后使用`for`语句获取存储在`x`中的每个图像数据，并使用`predict()`函数进行分类。`predict()`函数返回一个NumPy数组，其中包含每个标签的概率。例如，返回一个像[0.1,
    0.3, 0.2, …, 0.04]这样的数组，表示"0"的概率为0.1，"1"的概率为0.3，依此类推。通过找到这个概率列表中最大值的索引，我们可以得到最可能的元素作为预测结果。你可以使用`np.argmax(x)`来获取数组中最大元素的索引。它返回`x`参数指定数组中最大元素的索引。最后，神经网络预测的答案与正确的标签进行比较，正确预测的比率将显示为识别精度（准确率）。
- en: When the preceding code is executed, `Accuracy:0.9352` is displayed. This shows
    that 93.52% of the classifications were correct. We will not discuss the recognition
    accuracy here as our goal is to run a trained neural network, but later in this
    book, we will improve the structure and training method of the neural network
    to gain higher recognition accuracy. In fact, the accuracy will exceed 99%.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 当前面的代码执行时，`Accuracy:0.9352`会显示。这表示93.52%的分类是正确的。我们在这里不讨论识别准确率，因为我们的目标是运行一个训练好的神经网络，但在本书的后面部分，我们将改进神经网络的结构和训练方法，以获得更高的识别准确率。实际上，准确率将超过99%。
- en: In this example, the argument of the `load_mnist` function, `normalize`, is
    set to `True`. When `normalize` is `True`, the function divides the value of each
    pixel in the image by 255 so that the data values are between 0.0 and 1.0\. Converting
    data so that it fits in a certain range is called **normalization**, while converting
    the input data for a neural network in a defined way is called **pre-processing**.
    Here, the input image data was normalized as pre-processing.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`load_mnist`函数的参数`normalize`被设置为`True`。当`normalize`为`True`时，函数会将图像中每个像素的值除以255，使得数据值处于0.0到1.0之间。将数据转换为某个特定范围内的值叫做**归一化**，而以特定方式转换神经网络的输入数据叫做**预处理**。在这里，输入图像数据在预处理阶段进行了归一化。
- en: Note
  id: totrans-294
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In practical usage, pre-processing is often used in a neural network (deep learning).
    The validity of pre-processing, as in improved discrimination and faster learning,
    has been proven through experiments. In the preceding example, simple normalization
    was conducted by dividing the value of each pixel by 255 using pre-processing.
    Actually, pre-processing is often conducted while considering the distribution
    of the whole data. Normalization is conducted by using the average and standard
    deviation of the whole data so that all of the data is distributed around 0 or
    fits in a certain range. In addition, **whitening** is also conducted so that
    all the data is distributed more evenly.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，预处理常常用于神经网络（深度学习）中。通过实验已经证明了预处理的有效性，例如提高了区分度和加快了学习速度。在前面的例子中，简单的归一化是通过将每个像素的值除以255来进行的预处理。实际上，预处理通常是在考虑整个数据分布的基础上进行的。归一化是通过使用整个数据的平均值和标准差来进行的，使得所有数据都围绕0分布或符合某个特定的范围。此外，**白化**也会被执行，以便所有数据分布更加均匀。
- en: Batch Processing
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量处理
- en: This process is all about implementing a neural network using the MNIST dataset.
    Here, we will re-examine the preceding implementation while paying attention to
    the "shapes" of the input data and weight parameters.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程是使用MNIST数据集实现神经网络的过程。在这里，我们将重新审视前面的实现，并重点关注输入数据和权重参数的“形状”。
- en: 'Let''s use the Python interpreter to output the shape of the weights for each
    layer in the preceding neural network:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python解释器输出前面神经网络中每一层权重的形状：
- en: '[PRE32]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s check that the number of elements in the corresponding dimensions of
    the multidimensional arrays are the same as they are in the preceding result (biases
    are omitted). *Figure 3.26* shows this graphically. Here, the number of elements
    in the corresponding dimensions of the multidimensional arrays are the same. Verify
    that a one-dimensional array with 10 elements, y, is returned as the final result:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查多维数组的对应维度的元素数量是否与前面的结果一致（偏差被省略）。*图3.26*以图形化方式展示了这一点。在这里，多维数组的对应维度的元素数量是一致的。请验证一个包含10个元素的单维数组y是否作为最终结果返回：
- en: '![Figure 3.26: Transition of the shapes of arrays'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.26：数组形状变化'
- en: '](img/fig03_26.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_26.jpg)'
- en: 'Figure 3.26: Transition of the shapes of arrays'
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.26：数组形状变化
- en: '*Figure 3.26* shows the flow where a one-dimensional array with 784 elements
    (originally a two-dimensional 28x28 array) is provided, and a one-dimensional
    array with 10 elements is returned. This is the process when a single image is
    input.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.26*展示了一个一维数组（原本为28x28的二维数组）包含784个元素输入，并返回一个包含10个元素的一维数组的过程。这是单张图片输入时的处理流程。'
- en: 'Now, let''s think about the process when multiple images are entered at once.
    For example, let''s assume that you want to use the `predict()` function to process
    100 images at one time. To do that, you can change the shape of `x` to `100×784`
    so that you can enter 100 images collectively as input data. *Figure 3.27* shows
    this graphically:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们思考一下当多张图片同时输入时的处理过程。例如，假设你想使用`predict()`函数一次性处理100张图片。为了实现这一点，你可以将`x`的形状更改为`100×784`，这样就可以将100张图片作为输入数据一起输入。*图3.27*以图形化方式展示了这一点：
- en: '![Figure 3.27: Transition of the shapes of arrays in batch processing'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.27：批处理中的数组形状变化'
- en: '](img/fig03_27.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig03_27.jpg)'
- en: 'Figure 3.27: Transition of the shapes of arrays in batch processing'
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.27：批处理中的数组形状变化
- en: As shown in *Figure 3.27*, the shape of the input data is 100x784, and that
    of the output data is 100x10\. This indicates that the results for the input data
    of 100 images are returned in one go. For example, `x[0]` and `y[0]` store the
    image and predict the result of the 0th image, `x[1],` and `y[1]` store the image
    and predicting the result of the first image, and so on.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.27*所示，输入数据的形状为100x784，输出数据的形状为100x10\。这表示100张图片的输入数据会一次性返回结果。例如，`x[0]`和`y[0]`存储第0张图片的图像和预测结果，`x[1]`和`y[1]`存储第一张图片的图像和预测结果，以此类推。
- en: An organized set of input data, as described here, is called a **batch**. A
    batch is a stack of images, such as a wad of bills.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这种有组织的输入数据集，如这里所述，称为**批次**。批次就像一堆叠起来的图片，类似于一叠钞票。
- en: Note
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Batch processing has a big advantage in computer calculation. It can greatly
    reduce the processing time of each image since many of the libraries that handle
    numerical calculations are highly optimized so that large arrays can be calculated
    efficiently. When data transfer causes a bottleneck in neural network calculation,
    batch processing can reduce the load on the bus band (i.e.: the ratio of operations
    to data loading can be increased). Although batch processing requires a large
    array to be calculated, calculating a large array in one go is faster than calculating
    by dividing small arrays little by little.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理在计算机计算中具有巨大优势。由于许多处理数值计算的库经过高度优化，因此可以高效地计算大型数组，批处理大大减少了每张图片的处理时间。当数据传输成为神经网络计算的瓶颈时，批处理可以减轻总线带宽的负载（即：操作与数据加载的比例可以增加）。虽然批处理需要计算一个大型数组，但一次性计算整个数组比一点一点地分割计算小数组要快。
- en: 'Now, let''s use batch processing for our implementation. Here, the differences
    from the previous code are shown in bold:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在实现中使用批处理。这里，和之前代码的不同之处以粗体标出：
- en: '[PRE33]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we will describe each section shown in bold. First, let''s look at the
    `range()` function. You can use the `range()` function, such as `range(start,
    end)`, to generate a list of integers from `start` to `end-1`. By specifying three
    integers, as in `range(start, end, step)`, you can generate a list of integers
    where values are incremented by the value specified with the `step`, as in the
    following example:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将描述每个加粗的部分。首先，让我们看一下 `range()` 函数。您可以使用 `range()` 函数，例如 `range(start, end)`，生成一个从
    `start` 到 `end-1` 的整数列表。通过指定三个整数，如 `range(start, end, step)`，您可以生成一个按 `step` 指定的值递增的整数列表，如下例所示：
- en: '[PRE34]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Based on the list returned by the `range()` function, `x[i:i+batch_size]` is
    used to extract a batch from the input data. `x[i:i+batch_n]` obtains from the
    `i-`th to `i+batch_n-`th data in the input data. In this example, 100 items of
    data are obtained from the beginning, such as x[0:100], x[100:200], …
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 `range()` 函数返回的列表，`x[i:i+batch_size]` 用于从输入数据中提取一个批次。`x[i:i+batch_n]` 从输入数据中获取第
    `i-` 个到 `i+batch_n-` 个的数据。在这个例子中，数据从开始提取 100 项，比如 x[0:100]，x[100:200]，…
- en: 'Then, `argmax()` obtains the index of the largest value. Please note that an
    argument, `axis=1`, is specified here. It indicates that, in a 100x10 array, the
    index of the largest value is found among the elements in dimension 1 (the axis
    is almost the same as the dimension), as shown below:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`argmax()` 获取最大值的索引。请注意，这里指定了一个参数 `axis=1`。这意味着，在一个 100x10 的数组中，最大值的索引是在维度
    1（轴几乎等同于维度）中的元素中找到的，如下所示：
- en: '[PRE35]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Lastly, the classification results for each batch are compared with the actual
    answers. To do that, a comparison operator (`==`) is used to compare the NumPy
    arrays. A Boolean array of `True`/`False` is returned, and the number of Trues
    is calculated, as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将每一批次的分类结果与实际答案进行比较。为此，使用比较运算符（`==`）来比较 NumPy 数组。返回一个布尔值数组 `True`/`False`，并计算
    `True` 的数量，如下所示：
- en: '[PRE36]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: That's it for implementation using batch processing. Batch processing enables
    fast and efficient processing. When we learn about neural networks in the next
    chapter, batches of image data will be used for training. There, we will also
    build an implementation of batch processing, just like we did in this chapter.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用批处理实现的内容。批处理使得处理快速且高效。当我们在下一章学习神经网络时，将会使用一批批的图像数据进行训练。到时候，我们也会像本章一样构建一个批处理实现。
- en: Summary
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter described forward propagation in a neural network. The neural
    network we explained in this chapter is the same as a perceptron in the previous
    chapter in that the signals of the neurons are transmitted hierarchically. However,
    a large difference exists in the activation functions that change signals when
    they are transmitted to the next neurons. As an activation function, a neural
    network uses a sigmoid function, which changes signals smoothly, and a perceptron
    uses a step function, which changes signals sharply. This difference is important
    in neural network training and will be described in the next chapter. This chapter
    covered the following points:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了神经网络中的前向传播。本章中解释的神经网络与上一章的感知器相同，都以层次化的方式传递神经元信号。然而，在激活函数方面有很大差异，这些激活函数会在信号传递到下一个神经元时改变信号。在激活函数中，神经网络使用的是
    Sigmoid 函数，该函数平滑地改变信号，而感知器使用的是步进函数，该函数会突然改变信号。这一差异在神经网络训练中非常重要，并将在下一章中进行描述。本章涵盖了以下内容：
- en: A neural network uses a function that changes smoothly, such as a sigmoid function
    or a ReLU function, as an activation function.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络使用平滑变化的函数作为激活函数，如 Sigmoid 函数或 ReLU 函数。
- en: By using NumPy's multidimensional arrays, you can implement a neural network efficiently.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 NumPy 的多维数组，您可以高效地实现神经网络。
- en: Machine learning problems can be broadly divided into classification problems
    and regression problems.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习问题大致可以分为分类问题和回归问题。
- en: When using the activation function for the output layer, an identity function
    is often used for a regression problem, and a softmax function is used for a classification
    problem.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在为输出层使用激活函数时，回归问题通常使用恒等函数，分类问题则使用 softmax 函数。
- en: For a classification problem, the number of classes to classify is used as the
    number of neurons in the output layer.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于分类问题，用来分类的类别数作为输出层神经元的数量。
- en: A set of input data is called a batch. Predicting per batch accelerates the
    calculation process.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组输入数据被称为一个批次。按批次进行预测可以加速计算过程。
