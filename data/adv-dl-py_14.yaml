- en: Meta Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元学习
- en: In [Chapter 9](66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml), *Emerging Neural
    Network Designs*, we introduced new **neural network** (**NN**) architectures
    to tackle some of the limitations of existing **deep learning** (**DL**) algorithms.
    We discussed graph neural networks that are used to process structured data, represented
    as graphs. We also introduced memory augmented neural networks, which allow networks
    to use external memory. In this chapter, we'll look at how to improve DL algorithms
    by giving them the ability to learn more information using fewer training samples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第9章](66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml)，《*新兴神经网络设计*》中，我们介绍了新的**神经网络**（**NN**）架构，以解决现有**深度学习**（**DL**）算法的一些局限性。我们讨论了用于处理结构化数据的图神经网络，这些数据表示为图形。我们还介绍了增强记忆的神经网络，允许网络使用外部记忆。在本章中，我们将探讨如何通过赋予DL算法更多的学习能力，以便在使用更少的训练样本的情况下学习更多的信息。
- en: Let's illustrate this problem with an example. Imagine that a person has never
    seen a certain type of object, say a car (I know—highly unlikely). They will only
    need to see a car once to be able to recognize other cars as well. But this is
    not the case with DL algorithms. A DNN needs a lot of training samples (and sometimes
    data augmentation as well), to be able to recognize a certain class of object.
    Even the relatively small CIFAR-10 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    dataset contains 50,000 training images for only 10 classes of objects, the equivalent
    of 5,000 images per class.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来说明这个问题。假设一个人从未见过某种类型的物体，比如一辆车（我知道——这不太可能）。他们只需要看到一辆车一次，就能够识别其他的车。但是，这对于深度学习（DL）算法并非如此。一个深度神经网络（DNN）需要大量的训练样本（有时还需要数据增强），才能识别某一类物体。即便是相对较小的CIFAR-10
    ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    数据集，也包含了50,000张用于10类物体的训练图片，相当于每类5,000张图片。
- en: 'Meta learning, also referred to as learning to learn, allows **machine learning**
    (**ML**) algorithms to leverage and channel knowledge, gained over multiple training
    tasks, to improve its training efficiency over a new task. Hopefully, in this
    way, the algorithm will require fewer training samples to learn the new task.
    The ability to train with fewer samples has two advantages: reduced training time
    and good performance when there is not enough training data. In that regard, the
    goals of meta learning are similar to the transfer learning mechanism that we
    introduced in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),*Understanding
    Convolutional Networks*. In fact, we can think of transfer learning as a meta
    learning algorithm. But there are multiple approaches to meta learning. In this
    chapter, we''ll discuss some of them.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习，也称为“学习如何学习”，使得**机器学习**（**ML**）算法能够利用并传递在多个训练任务中获得的知识，从而提高其在新任务上的训练效率。希望通过这种方式，算法在学习新任务时所需的训练样本更少。用更少的样本进行训练有两个优势：缩短训练时间和在训练数据不足时仍能取得良好的性能。在这方面，元学习的目标与我们在[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)《*理解卷积网络*》中介绍的迁移学习机制类似。实际上，我们可以将迁移学习视为一种元学习算法。但元学习有多种方法。在本章中，我们将讨论其中的一些方法。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to meta learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元学习简介
- en: Metric-based meta learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于度量的元学习
- en: Optimization-based meta learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于优化的元学习
- en: Introduction to meta learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元学习简介
- en: 'As we mentioned in the introduction, the goal of meta learning is to allow
    an ML algorithm (in our case, NN) to learn from relatively fewer training samples
    compared to standard supervised training. Some meta learning algorithms try to
    achieve this goal by finding a mapping between their existing knowledge of the
    domain of a well-known task to the domain of a new task. Other algorithms are
    simply designed from scratch to learn from fewer training samples. Yet another
    category of algorithms introduce new optimization training techniques, designed
    specifically with meta learning in mind. But before we discuss these topics, let''s
    introduce some basic meta learning paradigms. In a standard ML supervised learning
    task, we aim to minimize the cost function *J(θ)* across a training dataset *D* by
    updating the model parameters *θ *(network weights, in the case of NNs). As we
    mentioned in the introduction, in meta learning we usually work with multiple
    datasets. Therefore, in a meta learning scenario, we can extend this definition
    by saying that we aim to minimize *J(θ) *over a distribution of these datasets
    *P(D)*:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在介绍中提到的，元学习的目标是让机器学习算法（在我们的例子中是神经网络）相较于标准的监督学习，通过较少的训练样本进行学习。一些元学习算法通过在已知任务领域的现有知识与新任务领域之间找到映射来实现这一目标。其他算法则是从零开始设计，旨在通过较少的训练样本进行学习。还有一些算法引入了新的优化训练技术，专门为元学习设计。但在我们讨论这些主题之前，先介绍一些基本的元学习范式。在标准的机器学习监督学习任务中，我们的目标是通过更新模型参数*θ*（在神经网络中是网络权重），在训练数据集*D*上最小化成本函数*J(θ)*。正如我们在介绍中提到的，在元学习中，我们通常会处理多个数据集。因此，在元学习情景下，我们可以通过以下方式扩展这个定义：我们旨在通过这些数据集的分布*P(D)*最小化*J(θ)*：
- en: '![](img/8fb8d948-a381-4a5c-a510-044386944a60.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fb8d948-a381-4a5c-a510-044386944a60.png)'
- en: Here, ![](img/7c90bf15-eefe-4035-9a88-d268ac605628.png) is the optimal model
    parameters and ![](img/031dc6dc-63e5-4e8e-a5da-207139d104da.png) is the cost function,
    which now depends on the current dataset as well as the model parameters. In other
    words, the goal is to find model parameters ![](img/43cc2369-d0da-4210-a0a1-3b6f68952bfc.png) such
    that the expected value (as described in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, in the *Random variables and probability
    distributions* section) of the cost across all datasets ![](img/8348ecdc-8752-4afb-8f4d-4607fa921b82.png) is
    minimized. We can think of this scenario as training over a single dataset whose
    training samples are themselves datasets.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/7c90bf15-eefe-4035-9a88-d268ac605628.png)是最优模型参数，![](img/031dc6dc-63e5-4e8e-a5da-207139d104da.png)是成本函数，现在它依赖于当前数据集和模型参数。换句话说，目标是找到模型参数![](img/43cc2369-d0da-4210-a0a1-3b6f68952bfc.png)，使得成本在所有数据集![](img/8348ecdc-8752-4afb-8f4d-4607fa921b82.png)上的期望值（如在[第一章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中所描述，*神经网络的基础*部分，*随机变量和概率分布*章节）被最小化。我们可以将这种情景看作是在单个数据集上进行训练，而这个数据集的训练样本本身也是数据集。
- en: 'Next, let''s continue by expanding the expression *fewer training samples* that
    we used in the introduction. In supervised training, we can refer to this scenario
    of scarce training data as ***k*-shot learning**, where *k* can be 0, 1, 2, and
    so on. Let''s assume that our training dataset consists of labeled samples distributed
    among *n* classes. In *k*-shot learning, we have *k* labeled training samples
    for each of the *n* classes (the total number of labeled samples is *n × k*).
    We refer to this dataset as the **support set**, and we''ll denote it with *S*. We
    also have a **query set** *Q*, which contains unlabeled samples that belong to
    one of the *n* classes. Our goal is to correctly classify the samples of the query
    set. There are three types of *k*-shot learning: zero-shot, one-shot, and few-shot.
    Let''s start with zero-shot learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们继续扩展我们在介绍中使用的表达式*较少的训练样本*。在监督训练中，我们可以将这种稀缺训练数据的情景称为***k*-shot学习**，其中*k*可以是0、1、2，依此类推。假设我们的训练数据集包含分布在*n*个类别中的标记样本。在*k*-shot学习中，我们为每个*n*个类别提供*k*个标记训练样本（总的标记样本数为*n
    × k*）。我们将这个数据集称为**支持集**，用*S*表示。我们还有一个**查询集** *Q*，其中包含属于*n*个类别之一的未标记样本。我们的目标是正确分类查询集中的样本。有三种类型的*k*-shot学习：零-shot、one-shot和few-shot。让我们从零-shot学习开始。
- en: Zero-shot learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 零-shot学习
- en: We'll begin with zero-shot learning (*k* = 0), where we know that a particular
    class exists, but we don't have any labeled samples of that class (that is, there
    is no support set). At first, this sounds impossible—how can we classify something
    we have never seen before? But in meta learning, this is not exactly the case.
    Recall that we leverage knowledge of previously learned tasks (let's denote them
    with *a*) over the task at hand (*b*). In that regard, zero-shot learning is a
    form of transfer learning. To understand how this works, let's imagine that a
    person has never seen an elephant (another highly unlikely example), yet they
    have to recognize one when they see a picture of it (new task *b*). However, the
    person has read in a book that the elephant is large, gray, has four legs, large
    ears, and a trunk (previous task *a*). Given this description, they'll easily
    recognize an elephant when they see it. In this example, the person applied their
    knowledge from the domain of a previously learned task (reading a book) to the
    domain of the new task (image classification).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从零-shot学习（*k* = 0）开始，在这种情况下，我们知道某个特定类别存在，但我们没有该类别的任何标记样本（即没有支持集）。一开始，这听起来不可能——我们如何分类我们从未见过的东西？但在元学习中，情况并非完全如此。回想一下，我们通过以前学过的任务的知识（我们用*a*表示它们）来处理当前任务（*b*）。从这个角度来看，零-shot学习是一种迁移学习。为了理解它是如何工作的，假设一个人从未见过大象（另一个极不可能的例子），但他们必须在看到大象的图片时进行识别（新任务*b*）。然而，这个人在书中读到大象是很大、灰色的，四条腿、大耳朵，还有一个象鼻（先前任务*a*）。根据这个描述，当他们看到大象时，他们会很容易识别出来。在这个例子中，这个人将他们在以前学到的任务领域（读书）中的知识应用到新任务（图像分类）领域。
- en: 'In the context of ML, these features can be encoded as nonhuman readable embedding
    vectors. We can replicate the elephant recognition example in the NN realm by
    using language-modeling techniques, such as word2vec or transformers to encode
    a context-based embedding vector of the word *elephant*. We can also use a convolutional
    network (CNN) to produce an embedding vector **h**[*b*] of an image of an elephant.
    Let''s look at how to implement this step by step:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，这些特征可以编码为非人类可读的嵌入向量。我们可以通过使用语言建模技术（如word2vec或transformers）来编码单词*elephant*的基于上下文的嵌入向量，从而在神经网络领域复制象识别的示例。我们还可以使用卷积神经网络（CNN）来生成一张象的图像的嵌入向量**h**[*b*]。让我们一步步来看如何实现：
- en: Apply encoders *f* and g (NNs) over labeled and unlabeled samples *a* and *b *to
    produce embeddings **h**[*a*] and **h**[*b*] respectively.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对标记和未标记样本*a*和*b*应用编码器*f*和g（神经网络），分别生成嵌入**h**[*a*]和**h**[*b*]。
- en: Use a mapping function to transform **h**[*b*] to the vector space of the embeddings **h**[*a**] of
    the known samples. The mapping function could be an NN as well. Furthermore, the
    encoders and the mapping could be combined in a single model and learned jointly.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用映射函数将**h**[*b*]转换到已知样本的嵌入**h**[*a*]的向量空间。映射函数也可以是一个神经网络。此外，编码器和映射函数可以结合在一个模型中并联合学习。
- en: 'Once we have the transformed representation of the query sample, we can compare
    it to all representations **h**[*a*]* using a similarity measure (for example,
    cosine similarity). We then assume that the query sample''s class is the same
    as the class of the support sample most closely related to the query. The following
    diagram illustrates this scenario:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们得到了查询样本的转化表示，我们可以将其与所有表示**h**[*a*]*进行比较，使用相似度度量（例如余弦相似度）。然后，我们假设查询样本的类别与与查询最相关的支持样本的类别相同。下图说明了这一场景：
- en: '![](img/8e41022d-9fb4-492d-bb62-63c299a64fe8.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e41022d-9fb4-492d-bb62-63c299a64fe8.png)'
- en: Zero-shot learning is possible thanks to transfer learning. Inspired by Chapter
    15 of http://www.deeplearningbook.org/
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot学习之所以可能，是因为迁移学习。灵感来源于《深度学习》第15章，网址为 [http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)
- en: Let's formalize the zero-shot learning scenario. In a traditional classification
    task with a single dataset, the NN represents the conditional probability ![](img/d7cc052e-9841-434d-923f-3b137dd1f47f.png),
    where *y* is the label of input sample **x** and *θ* is the model parameters.
    In meta learning, **x** and *y* belong to the traditional dataset, but we introduce
    a random variable *T* that describes the new task we're interested in. In our
    example, **x** would be the context (surrounding words) of the word *elephant*,
    and the label *y* is a one-hot encoding of the class elephant. On the other hand,
    *T* will be an image we're interested in; therefore, the meta learning model represents
    a new conditional probability ![](img/20ce23f4-8e8d-43a4-a4ff-aca3bbf6a943.png).
    The zero-shot scenario we just described is part of so-called metric-based meta
    learning (we'll see more examples of this later in the chapter). For now, let's
    move on to one-shot learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正式化零次学习场景。在传统的分类任务中，使用单一数据集，神经网络表示条件概率 ![](img/d7cc052e-9841-434d-923f-3b137dd1f47f.png)，其中*y*是输入样本**x**的标签，*θ*是模型参数。在元学习中，**x**和*y*属于传统数据集，但我们引入了一个随机变量*T*，它描述了我们感兴趣的新任务。在我们的例子中，**x**将是词语*elephant*的上下文（周围的词），而标签*y*是类别elephant的独热编码。另一方面，*T*将是我们感兴趣的一张图片；因此，元学习模型表示一个新的条件概率
    ![](img/20ce23f4-8e8d-43a4-a4ff-aca3bbf6a943.png)。我们刚才描述的零次学习场景是所谓的基于度量的元学习的一部分（稍后我们将在本章中看到更多示例）。现在，让我们继续讨论单次学习。
- en: One-shot learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单次学习
- en: In this section, we'll be looking at **one-shot learning** (*k = 1*) and its
    generalization **few-shot learning** (*k > 1*). In this case, the support set
    is not empty and we have one or more labeled samples of each class. This is an
    advantage over the zero-shot scenario because we can rely on labeled samples from
    the same domain instead of using a mapping from the labeled samples of another
    domain. Therefore, we have a single encoder *f* and no need for additional mapping.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论**单次学习**（*k = 1*）及其推广的**少量学习**（*k > 1*）。在这种情况下，支持集不是空的，我们有每个类别的一或多个标记样本。这相比于零次学习场景是一种优势，因为我们可以依赖来自同一领域的标记样本，而不是使用来自另一个领域的标记样本映射。因此，我们只需要一个编码器*f*，无需额外的映射。
- en: An example of a one-shot learning task is a company's facial recognition system.
    This system should be able to recognize the identity of an employee based on a
    single photo. It should be possible to add new employees with a single photo as
    well. Let's note that in this scenario, adding a new employee is equivalent to
    adding a new class that has already been seen (the photo itself), but which is
    otherwise unknown. This is in contrast to zero-shot learning, where we had unseen,
    but known classes. A naive way to solve this task is with a classification **feed-forward
    network** (**FFN**), which takes the photo as input and ends with a softmax output,
    where each class represents one employee. This system will have two major disadvantages.
    First, every time we add a new employee, we have to retrain the whole model using
    the full dataset of employees. And second, we need multiple images per employee
    to train the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一种典型的单次学习任务是公司的面部识别系统。该系统应该能够基于单张照片识别员工的身份。同样，也应该能够通过单张照片添加新员工。值得注意的是，在这种情况下，添加新员工相当于添加一个已经见过的新类别（照片本身），但其他方面是未知的。这与零次学习相对，后者是我们有未见过的但已知的类别。解决这个任务的一种简单方法是使用分类**前馈神经网络**（**FFN**），该网络将照片作为输入，并通过一个softmax输出，其中每个类别代表一个员工。这个系统将有两个主要缺点。首先，每次添加新员工时，我们都必须使用所有员工的完整数据集重新训练整个模型。其次，我们需要每个员工的多张图片来训练模型。
- en: 'The following description is based on the method introduced in *Matching Networks
    for One Shot Learning* ([https://arxiv.org/abs/1606.04080](https://arxiv.org/abs/1606.04080)).
    The paper has two major contributions: a novel one-shot training procedure and
    a special network architecture. In this section, we''ll discuss the training procedure
    and we''ll describe the network architecture in the *Matching networks* section.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述基于在*Matching Networks for One Shot Learning*（[https://arxiv.org/abs/1606.04080](https://arxiv.org/abs/1606.04080)）中提出的方法。该论文有两个主要贡献：一种新的单次训练过程和一种特殊的网络架构。在本节中，我们将讨论训练过程，并在*Matching
    networks*部分描述网络架构。
- en: We can also solve this task within the one-shot learning framework. The first
    thing we'll need is a pretrained network that can produce embedding vectors of
    the employee images. We'll assume that the pretraining allows the network to produce
    a sufficiently unique embedding **h** for each photo. We'll also store all employee
    photos in some external database. For performance reasons, we can apply the network
    to all photos and then store the embedding of each image as well. Let's focus
    on the use case where the system has to identify an existing employee when he
    or she tries to authenticate with a new photo. We'll use the network to produce
    an embedding of that photo and then we'll compare it to the embeddings in the
    database. We'll identify the employee by taking the database embedding that most
    closely matches the embedding of the current photo.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在一-shot 学习框架内解决这个任务。我们首先需要的是一个经过预训练的网络，该网络能够生成员工图像的嵌入向量。我们假设预训练使得网络可以为每张照片生成一个足够独特的嵌入
    **h**。我们还将所有员工的照片存储在某个外部数据库中。为了提高性能，我们可以将网络应用于所有照片，然后将每张图像的嵌入也存储下来。我们将重点关注这样一种使用场景：系统必须识别一个现有员工，当他或她尝试用一张新照片进行身份验证时。我们将使用网络生成该照片的嵌入，然后将其与数据库中的嵌入进行比较。通过将数据库中的嵌入与当前照片的嵌入进行最匹配的比较，我们就能识别出该员工。
- en: Next, let's look at the use case when a new employee is added to the system.
    Here, we'll simply take a photo of that employee and store it in the database.
    In this way, every time the employee tries to authenticate, their current photo
    will be compared to the initial one (along with all other photos). In this way,
    we have added a new class (the employee) without any changes to the network. We
    can think of the employee photo/identification database as a support set ![](img/3697813c-caa0-4554-9008-6da67efc3a5f.png).
    The goal of the task is to map this support set to a classifier ![](img/a86cb797-f60e-4e0d-b95f-b79c0d2c5c18.png),
    which outputs a probability distribution over the labels ![](img/c54b34bd-26a6-4570-aa0a-778c3befbdfe.png),
    given a previously unseen query sample ![](img/2c4ff6d5-f273-40d9-a492-11c0bd853084.png).
    In our case, the ![](img/654a154a-a2e7-4893-8094-7a99365c1c59.png) pairs represent
    new employees (that is, new query samples and new classes) that were not part
    of the system before.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下在系统中添加新员工时的使用案例。在这里，我们只需要拍一张该员工的照片并将其存储在数据库中。这样，每次员工尝试进行身份验证时，系统都会将他们当前的照片与初始照片进行对比（以及所有其他照片）。通过这种方式，我们在不改变网络的情况下添加了一个新类别（员工）。我们可以将员工照片/身份数据库视为一个支持集
    ![](img/3697813c-caa0-4554-9008-6da67efc3a5f.png)。任务的目标是将这个支持集映射到一个分类器 ![](img/a86cb797-f60e-4e0d-b95f-b79c0d2c5c18.png)，该分类器会根据之前未见过的查询样本
    ![](img/2c4ff6d5-f273-40d9-a492-11c0bd853084.png) 输出一个标签的概率分布 ![](img/c54b34bd-26a6-4570-aa0a-778c3befbdfe.png)。在我们的例子中，
    ![](img/654a154a-a2e7-4893-8094-7a99365c1c59.png) 配对表示之前不在系统中的新员工（即新的查询样本和新类别）。
- en: In other words, we want to be able to predict never before seen classes with
    the help of the existing support set. We'll define the mapping ![](img/f3fa7d27-d701-4c0e-867e-2532b9c61574.png) as
    a conditional probability ![](img/a3e6dc4c-5998-418e-a878-f7af427e9d09.png), implemented
    by a neural network with weights *θ*. Additionally, we can also plug a new support
    set ![](img/e3146769-fbf0-4f1a-9a2a-35757e5babac.png) into the same network, which
    would lead to a new probability distribution ![](img/ee987cf4-2602-45a3-88a7-8a6349fca99a.png).
    In this way, we can condition the outputs over the new training data without changing
    the network weights *θ*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们希望能够在现有支持集的帮助下预测以前从未见过的类别。我们将映射 ![](img/f3fa7d27-d701-4c0e-867e-2532b9c61574.png)
    定义为一个条件概率 ![](img/a3e6dc4c-5998-418e-a878-f7af427e9d09.png)，由一个具有权重 *θ* 的神经网络实现。此外，我们还可以将一个新的支持集
    ![](img/e3146769-fbf0-4f1a-9a2a-35757e5babac.png) 输入到同一个网络中，这将导致一个新的概率分布 ![](img/ee987cf4-2602-45a3-88a7-8a6349fca99a.png)。通过这种方式，我们可以在不改变网络权重
    *θ* 的情况下对输出进行条件化，以适应新的训练数据。
- en: Now that we are familiar with *k*-shot learning, let's look at how to train
    an algorithm with few-shot datasets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了 *k*-shot 学习，让我们来看一下如何在少量样本的数据集上训练一个算法。
- en: Meta-training and meta-testing
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元训练和元测试
- en: The scenarios we described in the *Zero-shot learning* and *One-shot learning* sections are
    referred to as **meta-testing** **phases**. In this phase, we leverage the knowledge
    of a pretrained network and apply it to predict previously unseen labels with
    the help of only a small support set (or no support set at all). We also have
    a **meta-training phase**, where we train a network from scratch in a few-shot
    context. The authors of *Matching Networks for One Shot Learning* introduce a
    meta-training algorithm that closely matches the meta-testing. This is necessary
    so that we can train the model under the same conditions that we expect it to
    work in the testing phase. Since we train the network from scratch, the training
    set (denoted with ***D***) is not a few-shot dataset, and instead contains a sufficient
    number of labeled examples of each class. Nevertheless, the training process simulates
    a few-shot dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*零-shot学习*和*单-shot学习*部分描述的场景被称为**元测试****阶段**。在这个阶段，我们利用预训练网络的知识，并通过仅使用少量支持集（或者根本不使用支持集）来预测以前未见过的标签。我们还有一个**元训练阶段**，在这个阶段我们在少-shot环境下从头开始训练一个网络。《*匹配网络用于单-shot学习*》的作者们介绍了一种与元测试密切匹配的元训练算法。这是必要的，因为我们需要在期望的测试阶段条件下训练模型。由于我们从头开始训练网络，训练集（记作***D***）不是一个少-shot数据集，而是包含每个类别的足够多的标注样本。尽管如此，训练过程仍然模拟了一个少-shot数据集。
- en: 'Here''s how it works:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是其工作原理：
- en: Sample a set of labels ![](img/0f33b03f-3446-4a31-97fb-7857d111590b.png), where
    *T* is the set of all labels in *D*. To clarify, *L* contains only part of all
    labels *T*. In this way, the training mimics the testing when the model sees just
    a couple of samples. For example, adding a new employee to the facial recognition
    system requires a single image and a label.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标签集中采样一个标签集 ![](img/0f33b03f-3446-4a31-97fb-7857d111590b.png)，其中*T*是*D*中所有标签的集合。为了澄清，*L*仅包含*T*中部分标签。这样，训练过程在模型仅看到少量样本时就能模拟测试。例如，向人脸识别系统中添加一名新员工只需要一张图片和一个标签。
- en: Sample a support set ![](img/aa4bc1ea-b5c3-4afd-84af-822b26901b15.png), where
    the labels of all samples in ![](img/f8a6dbad-5521-4176-9c67-7d093f01da14.png) are
    only part of L ![](img/7fc335ba-2d84-4a81-85e1-5021be3694c6.png). The support
    set contains *k* samples of each label.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从支持集采样 ![](img/aa4bc1ea-b5c3-4afd-84af-822b26901b15.png)，其中所有样本的标签来自于![](img/f8a6dbad-5521-4176-9c67-7d093f01da14.png)，并且只包含L的一部分
    ![](img/7fc335ba-2d84-4a81-85e1-5021be3694c6.png)。支持集包含每个标签的*k*个样本。
- en: Sample a training batch ![](img/97335221-af7f-4e5c-8804-f9a15409fb29.png), where
    ![](img/13951691-7fcb-47c8-b849-1d6a6c15df90.png) (the same as the support set). The
    combination of ![](img/780e0957-6fd5-415d-8921-da00cad5e2ac.png) and ![](img/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png) represents
    one training **episode**. We can think of the episode as a separate learning **task**
    with its corresponding dataset. Alternatively, in supervised learning, one episode
    is simply a single training sample.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从训练批次采样 ![](img/97335221-af7f-4e5c-8804-f9a15409fb29.png)，其中 ![](img/13951691-7fcb-47c8-b849-1d6a6c15df90.png)（与支持集相同）。![](img/780e0957-6fd5-415d-8921-da00cad5e2ac.png)和![](img/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png)的组合表示一次训练**回合**。我们可以将回合视为一个独立的学习**任务**，并对应一个数据集。或者，在监督学习中，一次回合仅是一个训练样本。
- en: 'Optimize the network weights over the episode. The network represents the probability
    ![](img/fce68f2a-12ac-4a4f-aa9d-bd1229ed21b1.png) and uses both ![](img/780e0957-6fd5-415d-8921-da00cad5e2ac.png) and ![](img/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png) as
    inputs. To clarify, the set ![](img/bc78000b-ce6c-45b3-b80e-6ce86ee4ff22.png) consists
    of the ![](img/8a63630d-66b4-4e85-aa2b-b4e96e56969e.png) tuples, conditioned on
    the support set ![](img/4c0b25dd-e5ba-49e3-816a-84287b8da55a.png). This is the
    "meta" part of the training process, because the model learns to learn from a
    support set to minimize the loss over the full batch. The model uses the following
    cross-entropy objective:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在回合中优化网络权重。网络表示概率 ![](img/fce68f2a-12ac-4a4f-aa9d-bd1229ed21b1.png)，并使用![](img/780e0957-6fd5-415d-8921-da00cad5e2ac.png)和![](img/464ecc9a-6411-46fb-b8ad-4a48dd54cc34.png)作为输入。为了澄清，集合![](img/bc78000b-ce6c-45b3-b80e-6ce86ee4ff22.png)由![](img/8a63630d-66b4-4e85-aa2b-b4e96e56969e.png)元组组成，这些元组以支持集![](img/4c0b25dd-e5ba-49e3-816a-84287b8da55a.png)为条件。这是训练过程中的“元”部分，因为模型学会了从支持集学习，以最小化整个批次的损失。模型使用以下交叉熵目标：
- en: '![](img/9da8b85a-03fa-4a0e-85d6-780c27e5ee2a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9da8b85a-03fa-4a0e-85d6-780c27e5ee2a.png)'
- en: 'Here, ![](img/ea5bddf9-9d3e-49be-a5a7-73a24bb0ec6f.png) and ![](img/6c29607a-e8a2-4892-b43e-0215125e28b2.png) reflect
    the sampling of labels and examples respectively. Let''s compare this to the same
    task, but in a classic supervised learning scenario. In this case, we sample mini
    batches *B* from the dataset *D* and there is no support set. The sampling is
    random and doesn''t depend on the labels. Then, the preceding formula would transform
    to the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/ea5bddf9-9d3e-49be-a5a7-73a24bb0ec6f.png) 和 ![](img/6c29607a-e8a2-4892-b43e-0215125e28b2.png) 分别反映了标签和示例的采样。我们将其与经典的监督学习场景中的相同任务进行比较。在这种情况下，我们从数据集*D*中采样小批量*B*，并且没有支持集。采样是随机的，不依赖于标签。然后，前面的公式将转化为以下形式：
- en: '![](img/971e0294-1966-44bc-93e6-5b948cb613fb.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/971e0294-1966-44bc-93e6-5b948cb613fb.png)'
- en: 'Meta learning algorithms can be classified into three main categories: metric-based,
    model-based, and optimization-based. In this chapter, we''ll focus on the metric-
    and optimization- based approaches (excluding model-based). Model-based meta learning
    algorithms don''t impose any restrictions on the type of ML algorithm that implements
    the probability ![](img/732a9825-aa4b-4429-9700-d77602b5935d.png). That is, there
    is no requirement for encoder and mapping functions. Instead, they rely on network
    architectures specifically adapted to work with a small number of labeled samples. You
    may recall that in [Chapter 9](66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml), *Emerging
    Neural Network Designs*, we introduced one such model when we looked at the *One-shot
    Learning with Memory-Augmented Neural Networks* paper([https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)).
    As the name suggests, the paper demonstrates the use of memory-augmented neural
    networks in a one-shot learning framework. Since we have already discussed the
    network architecture, and the training process is similar to the one we described
    in this section, we won''t include another model-based example in this chapter.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习算法可以分为三大类：基于度量的、基于模型的和基于优化的。在本章中，我们将专注于基于度量和基于优化的方法（不包括基于模型的）。基于模型的元学习算法对实现概率 ![](img/732a9825-aa4b-4429-9700-d77602b5935d.png)的机器学习算法类型没有任何限制。也就是说，不要求编码器和映射函数。相反，它们依赖于专门适配的小样本标注的网络架构。你可能还记得在[第9章](66956576-0f67-49a6-9ba8-1a782baa6b24.xhtml)，《*新兴神经网络设计*》中，我们讨论了当时我们在分析*使用记忆增强神经网络进行单次学习*论文时引入的一个模型（[https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)）。正如其名所示，这篇论文展示了在单次学习框架中使用记忆增强神经网络的应用。由于我们已经讨论过网络架构，而且训练过程与本节描述的相似，因此我们不会在本章中再提供一个基于模型的示例。
- en: Now that we've introduced the basics of meta learning, in the following section
    we'll focus on metric-based learning algorithms.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经介绍了元学习的基本概念，接下来的部分将专注于基于度量的学习算法。
- en: Metric-based meta learning
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于度量的元学习
- en: 'We mentioned a metric-based approach when we discussed the one-shot scenario
    in the *Introduction to meta learning *section, but this approach applies to *k*-shot
    learning in general. The idea is to measure the similarity between the unlabeled
    query sample ![](img/0737f559-b5df-47e5-8f94-61c64b233435.png) and all other samples
    ![](img/428554e4-a2f5-4329-bb51-48ba85405fa2.png) of the support set. Using these
    similarity scores, we can compute a probability distribution ![](img/e3047114-b1db-449c-91ad-4a8c07c441ad.png).
    The following formula reflects this mechanism:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论*元学习简介*章节中的单次学习场景时提到了基于度量的方法，但这种方法通常适用于*k*-次学习。其思想是衡量无标签查询样本 ![](img/0737f559-b5df-47e5-8f94-61c64b233435.png) 与支持集中的所有其他样本 ![](img/428554e4-a2f5-4329-bb51-48ba85405fa2.png) 之间的相似度。利用这些相似度得分，我们可以计算概率分布 ![](img/e3047114-b1db-449c-91ad-4a8c07c441ad.png)。以下公式反映了这一机制：
- en: '![](img/843c91d4-f4f7-4593-bb4d-d806d7dfdb94.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/843c91d4-f4f7-4593-bb4d-d806d7dfdb94.png)'
- en: 'Here, *α* is the similarity measure between the query samples and ![](img/eabcb9e2-b8e4-439c-bf46-8a7522301878.png) is
    the size of the support set with *n* classes and *k* samples of each class. To
    clarify, the label of the query sample is simply a linear combination of all samples
    of the support set. The classes of the samples with higher similarities will have
    higher contributions to the distribution of the label of the query sample. We
    can implement *α* as a clustering algorithm (for example, *k*-nearest neighbors)
    or an attention model (as we''ll see later in the upcoming section). In the case
    of zero-shot learning, this process has two formal steps: compute sample embeddings
    and then compute the similarity between the embeddings. But the preceding formula
    is a generalized combination of the two steps, and computes the similarity directly
    from the query samples (although internally, the steps could still be separate).
    The two-step metric-based learning (including the encoders *f* and *g*) is illustrated
    in the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*α* 是查询样本与 ![](img/eabcb9e2-b8e4-439c-bf46-8a7522301878.png) 之间的相似度度量，表示带有
    *n* 类别和每个类别 *k* 个样本的支持集的大小。为了明确，查询样本的标签仅仅是支持集所有样本的线性组合。与查询样本相似度更高的样本类别会对查询样本标签的分布产生更大的贡献。我们可以将
    *α* 实现为聚类算法（例如 *k*-最近邻）或注意力模型（正如我们在接下来的部分中将看到的）。在零-shot学习的情况下，这个过程有两个正式步骤：计算样本的嵌入表示，然后计算嵌入表示之间的相似度。但前面的公式是这两个步骤的广义组合，它直接从查询样本中计算相似度（尽管在内部，这些步骤可能仍然是分开的）。两步度量基础学习（包括编码器
    *f* 和 *g*）在下图中进行了说明：
- en: '![](img/c1ee8ed7-cf32-4dba-a869-a43efc4ee63c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1ee8ed7-cf32-4dba-a869-a43efc4ee63c.png)'
- en: Generic metric-based learning algorithm
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通用度量基础学习算法
- en: In the next few sections, we'll discuss some of the more popular metric meta-learning
    algorithms.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几部分中，我们将讨论一些更受欢迎的度量元学习算法。
- en: Matching networks for one-shot learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次性学习的匹配网络
- en: 'We already discussed the training procedure that was introduced alongside matching
    networks in the *Introduction to* *meta learning* section. Now, let''s focus on
    the actual model, starting with the similarity measure, which we outlined in the *Metric-based
    meta learning *section*. *One way to implement this is with cosine similarity (denoted
    with *c*), followed by softmax:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了在《元学习介绍》部分中与匹配网络一起引入的训练过程。现在，让我们关注实际的模型，从我们在*度量基础元学习*部分概述的相似度度量开始。*实现这一点的一种方法是使用余弦相似度（记作
    *c*），然后是 softmax：*
- en: '![](img/3f743c02-83f5-46ed-9fe9-dc2607309fc4.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f743c02-83f5-46ed-9fe9-dc2607309fc4.png)'
- en: Here, *f* and *g* are encoders of the samples of the new task and the support
    set respectively (as we discussed, it's possible that *f* and *g* are the same
    function). The encoders could be CNNs for image inputs or word embeddings, such
    as word2vec in the case of natural language processing tasks. This formula is
    very similar to the attention mechanism that we introduced in [Chapter 8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml),*Sequence-to-Sequence
    Models and Attention. *
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 和 *g* 分别是新任务样本和支持集样本的编码器（正如我们讨论的，*f* 和 *g* 可能是相同的函数）。这些编码器可以是用于图像输入的卷积神经网络（CNN）或在自然语言处理任务中使用的词嵌入，例如
    word2vec。这个公式与我们在[第8章](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml)《序列到序列模型与注意力机制》中介绍的注意力机制非常相似。
- en: 'With the current definition, the encoder *g* only encodes one support sample
    at a time, independently of the other samples of the support set. However, it''s
    possible that the embeddings ![](img/129f9541-1bbb-43b2-bebe-4310d53a8b00.png) and ![](img/b2aa5d9c-f3c1-4932-8a0a-187f78564751.png) of
    two samples *i* and *j* are very close in the embedding feature space, but that
    the two samples have different labels. The authors of the paper propose modifying *g*
    to take the whole support set *S* as additional input: ![](img/8b93e6d0-a0b6-4e9f-ac1a-850ab142a1b6.png).
    In this way, the encoder could condition the embedding vector of ![](img/2af7ad11-148a-4894-8d30-c181bd5585f5.png) on
    *S* and avoid this problem. We can apply similar logic to the encoder *f* as well.
    The paper refers to new embedding functions as **full context embeddings**.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 按照当前定义，编码器 *g* 每次仅对一个支持样本进行编码，独立于支持集中的其他样本。然而，*i* 和 *j* 两个样本的嵌入 ![](img/129f9541-1bbb-43b2-bebe-4310d53a8b00.png)
    和 ![](img/b2aa5d9c-f3c1-4932-8a0a-187f78564751.png) 可能在嵌入特征空间中非常接近，但这两个样本可能具有不同的标签。论文的作者建议修改
    *g*，使其将整个支持集 *S* 作为额外输入：![](img/8b93e6d0-a0b6-4e9f-ac1a-850ab142a1b6.png)。通过这种方式，编码器可以基于
    *S* 条件化 ![](img/2af7ad11-148a-4894-8d30-c181bd5585f5.png) 的嵌入向量，避免这个问题。我们也可以对编码器
    *f* 应用类似的逻辑。论文将这种新的嵌入函数称为 **完整上下文嵌入**。
- en: Let's look at how to implement full context embeddings over *f*. First, we'll
    introduce a new function ![](img/c46e1950-117a-43cf-84ce-592498727f9a.png), which
    is similar to the old encoder (before including *S* as input)—that is, *f'* could
    be a CNN or word-embedding model, which creates sample embedding, independently
    of the support set. The result of ![](img/c46e1950-117a-43cf-84ce-592498727f9a.png) will
    serve as the input for the full embedding function ![](img/ca72fc39-4d7a-448e-a67d-2c7336d2977b.png). We'll
    treat the support set as a sequence, which allows us to embed it using long short-term
    memory (LSTM). Because of this, computing the embedding vector is a sequential
    process of multiple steps.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 *f* 上实现完整的上下文嵌入。首先，我们将引入一个新函数 ![](img/c46e1950-117a-43cf-84ce-592498727f9a.png)，它类似于旧的编码器（在包括
    *S* 作为输入之前）——也就是说，*f'* 可以是 CNN 或词嵌入模型，它独立于支持集创建样本嵌入。![](img/c46e1950-117a-43cf-84ce-592498727f9a.png)
    的结果将作为完整嵌入函数 ![](img/ca72fc39-4d7a-448e-a67d-2c7336d2977b.png) 的输入。我们将支持集视为一个序列，这使我们可以使用长短期记忆（LSTM）对其进行嵌入。因此，计算嵌入向量是一个多步骤的顺序过程。
- en: However, *S* is a set, which implies that the order of samples in the sequence
    is not relevant. To reflect this, the algorithm also uses a special attention
    mechanism over the elements of the support set. In this way, the embedding function
    can attend to all previous elements of the sequence, regardless of their order.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*S* 是一个集合，这意味着序列中样本的顺序并不重要。为了反映这一点，算法还使用了一个特殊的注意力机制来处理支持集中的元素。通过这种方式，嵌入函数可以关注序列中所有先前的元素，而不管它们的顺序如何。
- en: 'Let''s see how one step of the encoder works:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看编码器的一个步骤是如何工作的：
- en: '![](img/fd51771f-facb-41bb-8b25-e16bc4ec5776.png), where *t* is the current
    element of the input sequence, ![](img/a9ea670b-72b1-4af1-be30-1fe93e02d555.png) is
    an intermediate hidden state, ![](img/5da17d1b-16a9-4da0-a99d-e363b302f1c4.png) is
    the hidden state at step *t-1*, and ![](img/d172852d-a434-4c56-b861-395e3685bf5b.png) is
    the cell state. The attention mechanism is implemented with a vector ![](img/17b4899b-1150-432e-b153-34bbc012cc11.png),
    which is concatenated to the hidden state ![](img/91a1c38c-e597-47da-8840-8f6e1c369c71.png).'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/fd51771f-facb-41bb-8b25-e16bc4ec5776.png)，其中 *t* 是输入序列的当前元素，![](img/a9ea670b-72b1-4af1-be30-1fe93e02d555.png)
    是一个中间隐藏状态，![](img/5da17d1b-16a9-4da0-a99d-e363b302f1c4.png) 是步骤 *t-1* 时的隐藏状态，![](img/d172852d-a434-4c56-b861-395e3685bf5b.png)
    是细胞状态。注意力机制通过一个向量 ![](img/17b4899b-1150-432e-b153-34bbc012cc11.png) 实现，它与隐藏状态
    ![](img/91a1c38c-e597-47da-8840-8f6e1c369c71.png) 连接。'
- en: '![](img/0bbe5c21-503b-4a53-b89c-db4eb7ca5db4.png), where ![](img/1d7d2444-6565-4fe0-aeef-6b627dd6bb21.png) is
    the final hidden state at step *t*.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/0bbe5c21-503b-4a53-b89c-db4eb7ca5db4.png)，其中 ![](img/1d7d2444-6565-4fe0-aeef-6b627dd6bb21.png)
    是步骤 *t* 时的最终隐藏状态。'
- en: '![](img/2211a28c-c0fa-4804-898c-f10d7e2304cd.png), where ![](img/d0a84215-84c6-4555-ad04-18d6f3fab826.png) is
    the size of the support set, *g* is an embedding function for the support set,
    and α is a similarity measure, which is defined as multiplicative attention, followed
    by a softmax:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '![](img/2211a28c-c0fa-4804-898c-f10d7e2304cd.png)，其中 ![](img/d0a84215-84c6-4555-ad04-18d6f3fab826.png)
    是支持集的大小，*g* 是支持集的嵌入函数，α 是一个相似性度量，定义为乘法注意力，随后进行 softmax：'
- en: '![](img/f482152a-6913-4870-ac20-eb0219de356b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f482152a-6913-4870-ac20-eb0219de356b.png)'
- en: 'The process continues for *T* steps (*T* is a parameter). We can summarize
    it with the following formula:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程持续进行*T*步（*T*是一个参数）。我们可以用以下公式总结：
- en: '![](img/16204eed-bf27-42d7-a7c3-c5c673afb6b9.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16204eed-bf27-42d7-a7c3-c5c673afb6b9.png)'
- en: 'Next, let''s focus on the full context embeddings of *g*. Like *f*, we''ll
    introduce a new function, ![](img/3f28a031-ed39-4bab-a7a5-d88d8d243ebf.png), which
    is similar to the old encoder (before including *S* as input). The authors propose
    to use a bidirectional LSTM encoder, defined as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注*g*的完整上下文嵌入。像*f*一样，我们将引入一个新函数，![](img/3f28a031-ed39-4bab-a7a5-d88d8d243ebf.png)，它类似于旧的编码器（在包括*S*作为输入之前）。作者建议使用双向LSTM编码器，定义如下：
- en: '![](img/9bfdab3d-e575-4900-ae76-2a37fc18c20e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bfdab3d-e575-4900-ae76-2a37fc18c20e.png)'
- en: 'Here, ![](img/20547e60-5396-43af-8168-185a072ca9e1.png) and ![](img/40fe9ba8-571b-4e0c-98f0-3232fac5e237.png) are
    the cell hidden states in both directions. We can define them as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/20547e60-5396-43af-8168-185a072ca9e1.png)和![](img/40fe9ba8-571b-4e0c-98f0-3232fac5e237.png)是两个方向上的单元隐藏状态。我们可以按如下方式定义它们：
- en: '![](img/04b0f06b-7947-4297-ad2d-8eb11f6e736a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/04b0f06b-7947-4297-ad2d-8eb11f6e736a.png)'
- en: In the next section, we'll discuss another metric-based learning approach called
    Siamese networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论另一种基于度量的学习方法——孪生网络。
- en: Siamese networks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 孪生网络
- en: 'In this section, we''ll discuss the *Siamese Neural Networks for One-shot Image
    Recognition* paper ([https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)).
    A Siamese network is a system of two identical base networks, as illustrated in
    the following diagram:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论*孪生神经网络用于一次性图像识别*论文（[https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)）。孪生网络是由两个相同的基础网络组成的系统，如下图所示：
- en: '![](img/a472a2ae-9a98-44be-9c15-e7c2018a8144.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a472a2ae-9a98-44be-9c15-e7c2018a8144.png)'
- en: Siamese networks
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络
- en: 'The two networks are identical in the sense that they share the same architecture
    and the same parameters (weights). Each network is fed a single input sample and
    the last hidden layer produces an embedding vector of that sample. The two embeddings
    are fed to a distance measure. The distance is further processed to produce the
    final output of the system, which is binary and represents a verification of whether
    the two samples are from the same class. The distance measure itself is differentiable,
    which allows us to train the networks as a single system. The authors of the paper
    recommend using *L1* distance:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络在共享相同架构和相同参数（权重）方面是相同的。每个网络接收一个输入样本，最后一个隐藏层生成该样本的嵌入向量。这两个嵌入向量被输入到一个距离度量中。这个距离经过进一步处理，产生系统的最终输出，该输出为二进制值，表示两个样本是否来自同一类。该距离度量本身是可微的，这使我们能够将这两个网络作为一个整体进行训练。论文的作者建议使用*L1*距离：
- en: '![](img/bf11009d-9829-442f-87e0-b5733908126e.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf11009d-9829-442f-87e0-b5733908126e.png)'
- en: Here, ![](img/c5191441-03d4-4710-be85-b879e8b75f25.png) is the base network.
    Using Siamese networks in a one-shot learning scenario follows the same general
    idea we described in the *Meta-training and meta-testing *section, but in this
    case, the task is simplified because we always have only two classes (same or
    not same), regardless of the actual number of classes in the dataset. In the meta-training
    phase, we train the system with a large labeled dataset. We do this by generating
    samples of image pairs and binary labels with either the same or a different class.
    In the meta-testing phase, we have a single query sample and a support set. We
    then create multiple pairs of images, where each pair contains the query sample
    and a single sample of the support set. We have as many image pairs as the size
    of the support set. Then, we feed all pairs to the Siamese system and we pick
    the pair with the smallest distance. The class of the query image is determined
    by the class of the support sample of that pair.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/c5191441-03d4-4710-be85-b879e8b75f25.png)是基础网络。在一次性学习场景中使用孪生网络遵循我们在*元训练和元测试*部分中描述的相同基本思路，但在这种情况下，任务被简化，因为我们始终只有两个类别（相同或不同），无论数据集中的实际类别数是多少。在元训练阶段，我们使用一个大型标注数据集来训练系统。我们通过生成图像对和二进制标签样本来完成这一点，标签可以是相同的或不同的类别。在元测试阶段，我们有一个查询样本和一个支持集。然后，我们创建多个图像对，每对图像包含查询样本和支持集中的一个样本。图像对的数量与支持集的大小相同。接着，我们将所有图像对输入到孪生系统中，并选择距离最小的那一对。查询图像的类别由该对的支持样本的类别决定。
- en: Implementing Siamese networks
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现孪生网络
- en: In this section, we'll use Keras to implement a simple example of Siamese networks,
    which will verify whether two MNIST images are from the same class or not. It
    is partially based on [https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py](https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将使用 Keras 实现一个简单的孪生网络示例，验证两张 MNIST 图像是否属于同一类别。它部分基于 [https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py](https://github.com/keras-team/keras/blob/master/examples/mnist_siamese.py)。
- en: 'Let''s look at how to do this step by step:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来看如何实现：
- en: 'We''ll start with the import statements:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入语句开始：
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we''ll implement the `create_pairs` function to create the train/test
    dataset (both for training and testing):'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现 `create_pairs` 函数来创建训练/测试数据集（用于训练和测试）：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each dataset sample consists of an input pair of two MNIST images and a binary
    label, which indicates whether they are from the same class. The function creates
    an equal number of true/false samples distributed over all classes (digits).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据集样本由一对 MNIST 图像和一个二进制标签组成，标签表示它们是否来自同一类别。该函数创建了一个在所有类别（数字）中分布的相等数量的真/假样本。
- en: 'Next, let''s implement the `create_base_network` function, which defines one
    branch of the Siamese network:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现 `create_base_network` 函数，它定义了孪生网络的一个分支：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The branch represents the base network that starts from the input and goes to
    the last hidden layer, before the distance measure. We'll use a simple NN of three
    fully connected layers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该分支表示从输入到最后一个隐藏层的基础网络，然后进行距离度量。我们将使用一个由三个全连接层组成的简单神经网络。
- en: 'Next, let''s build the whole training system, starting from the MNIST dataset:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们从 MNIST 数据集开始，构建整个训练系统：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll use the raw dataset to create the actual train and test verification
    datasets:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用原始数据集来创建实际的训练和测试验证数据集：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we''ll build the base portion of the Siamese network:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将构建孪生网络的基础部分：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `base_network` object is shared between the two forks of the Siamese system.
    In this way, we ensure that the weights are the same in the two branches.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`base_network` 对象在孪生系统的两个分支之间共享。通过这种方式，我们确保两个分支中的权重是相同的。'
- en: 'Next, let''s create the two branches:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们创建两个分支：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we''ll create the L1 distance, which uses the outputs of `encoder_a`
    and `encoder_b`. It is implemented as a `tf.keras.layers.Lambda` layer:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建 L1 距离，它使用 `encoder_a` 和 `encoder_b` 的输出。它作为 `tf.keras.layers.Lambda`
    层实现：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we''ll create the final fully connected layer, which takes the output
    of the distance and compresses it to a single sigmoid output:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建最终的全连接层，该层接受距离的输出并将其压缩为一个单一的 sigmoid 输出：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can build the model and initiate the training for 20 epochs:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以构建模型并开始训练 20 个周期：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If everything goes alright, the model will achieve around 98% accuracy.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，模型将达到大约 98% 的准确率。
- en: Next, we'll discuss yet another metric learning method called prototypical networks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论另一种称为原型网络的度量学习方法。
- en: Prototypical networks
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原型网络
- en: In a few short learning scenarios, it would be very easy for a high-capacity
    model (an NN with many layers and parameters) to overfit. Prototypical networks (as
    discussed in the *Prototypical Networks for Few-shot Learning* paper, [https://arxiv.org/abs/1703.05175](https://arxiv.org/abs/1703.05175))
    address this issue by computing a special prototype vector of each label, which
    is based on all samples of that label. The same prototypical network computes
    an embedding of the query samples as well. Then, we measure the distance between
    the query embedding and the prototypes and assign the query class accordingly
    (more details on this later in the section).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在少-shot 学习场景中，高容量模型（具有许多层和参数的神经网络）很容易发生过拟合。原型网络（如在 *Prototypical Networks for
    Few-shot Learning* 论文中讨论的，[https://arxiv.org/abs/1703.05175](https://arxiv.org/abs/1703.05175)）通过计算每个标签的特殊原型向量来解决这个问题，该向量是基于该标签的所有样本的。相同的原型网络还计算查询样本的嵌入。然后，我们测量查询嵌入与原型之间的距离，并根据此距离分配查询类别（有关更多细节，请参见本节后面）。
- en: 'Prototypical networks work for both zero-shot and few-shot learning, as illustrated
    in the following diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络适用于零-shot 和少-shot 学习，如下图所示：
- en: '![](img/58be1cd6-d9e8-438d-a162-e36d75404460.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58be1cd6-d9e8-438d-a162-e36d75404460.png)'
- en: 'Left: Few-shot learning; Right: Zero-shot learning. Source: https://arxiv.org/abs/1703.05175'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：少-shot 学习；右图：零-shot 学习。来源：[https://arxiv.org/abs/1703.05175](https://arxiv.org/abs/1703.05175)
- en: 'Let''s start with the few-shot learning scenario, where the prototype vector ![](img/e72f4c55-df9a-4d9e-95c8-81f9244a499d.png)
    of each class *k* is computed as the element-wise mean value of all samples of
    that class:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从少样本学习场景开始，其中每个类*k*的原型向量![](img/e72f4c55-df9a-4d9e-95c8-81f9244a499d.png)是该类所有样本的逐元素均值：
- en: '![](img/16491fe9-1061-41e5-81c4-1a0731c13384.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16491fe9-1061-41e5-81c4-1a0731c13384.png)'
- en: 'Here, ![](img/30bb12e9-f333-4761-adbd-0d6528c3606e.png) is the number of samples
    in the support set of class *k* and ![](img/97739005-895a-4f7e-a9b0-91cc57aaa616.png) is
    the prototypical network with parameters *θ*. In the zero-shot learning scenario,
    the prototype is computed as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/30bb12e9-f333-4761-adbd-0d6528c3606e.png)是类*k*在支持集中的样本数量，![](img/97739005-895a-4f7e-a9b0-91cc57aaa616.png)是带有参数*θ*的原型网络。在零-shot学习场景下，原型的计算方式如下：
- en: '![](img/05302326-8610-4da4-b8e0-5e6ccb3a58e4.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05302326-8610-4da4-b8e0-5e6ccb3a58e4.png)'
- en: Here, ![](img/770cd824-65a0-4b46-b1cb-ce9cf587643c.png)is a metadata vector,
    which gives a high-level description of the label, and ![](img/6d0952ea-11c9-4b62-90cb-5a891c014b8f.png) is
    the embedding function (encoder) of that vector. The metadata vectors could be
    given in advance or computed.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/770cd824-65a0-4b46-b1cb-ce9cf587643c.png)是元数据向量，它提供了标签的高层描述，![](img/6d0952ea-11c9-4b62-90cb-5a891c014b8f.png)是该向量的嵌入函数（编码器）。元数据向量可以提前给定或计算得出。
- en: 'Each new query sample is classified as a softmax over the distance between
    the sample embedding and all prototypes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新的查询样本都通过样本嵌入与所有原型之间的距离进行softmax分类：
- en: '![](img/0af436ea-87c0-4572-a362-0249a8fa36ee.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0af436ea-87c0-4572-a362-0249a8fa36ee.png)'
- en: Here, *d* is a distance measure (for example, the linear Euclidean distance).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*d*是距离度量（例如，线性欧氏距离）。
- en: Now that we have an overview of the main idea behind prototype networks, let's
    focus on how to train them (the procedure is similar to the training we outlined
    in the *Introduction to meta learning *section).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了原型网络背后的主要思想，接下来让我们聚焦于如何训练它们（这个过程类似于我们在*元学习简介*部分中概述的训练方法）。
- en: 'Before we start, we''ll introduce some notations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们先介绍一些符号：
- en: '*D* is the few-shot training set.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*是少样本训练集。'
- en: '* D[k]* is the training samples of *D* of class *k*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D[k]*是类*k*在*D*中的训练样本。'
- en: '*T* is the total number of classes in the dataset.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*是数据集中的类总数。'
- en: '![](img/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png) is the subset of labels,
    selected for each training episode.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png)是每个训练轮次中选定的标签子集。'
- en: '*N[S]* is the number of support samples per class per episode.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[S]*是每一轮每个类的支持样本数量。'
- en: '*N[Q]* is the number of query samples per episode.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N[Q]*是每一轮查询样本的数量。'
- en: 'The algorithm starts with the training set *D* and outputs the result of the
    cost function *J.* Let''s look at how it works step by step:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从训练集*D*开始，并输出代价函数*J*的结果。让我们逐步看看它是如何工作的：
- en: Sample a set of labels ![](img/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从标签集中采样！[](img/aeebc6f9-55ef-4356-a4c8-aa68957472ca.png)。
- en: 'For each class *k* in *L*, do the following:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*L*中的每个类*k*，执行以下操作：
- en: Sample support set ![](img/f281ab2d-23e9-4cae-8c30-b2108591b259.png), where ![](img/fc7488e4-b34b-4096-8a86-9de46e6b63df.png).
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样支持集![](img/f281ab2d-23e9-4cae-8c30-b2108591b259.png)，其中![](img/fc7488e4-b34b-4096-8a86-9de46e6b63df.png)。
- en: Sample query set ![](img/dd31ea70-5b05-4b41-8f2f-f8c618b9f6eb.png), where ![](img/0a4e6495-2547-4855-9112-13020afe2afc.png).
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 采样查询集![](img/dd31ea70-5b05-4b41-8f2f-f8c618b9f6eb.png)，其中![](img/0a4e6495-2547-4855-9112-13020afe2afc.png)。
- en: 'Compute the class prototype from the support set:'
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从支持集计算类原型：
- en: '![](img/16491fe9-1061-41e5-81c4-1a0731c13384.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16491fe9-1061-41e5-81c4-1a0731c13384.png)'
- en: Initialize the cost function ![](img/e65dadec-23f7-4f15-b24d-34dc8a1d25b2.png).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化代价函数![](img/e65dadec-23f7-4f15-b24d-34dc8a1d25b2.png)。
- en: 'For each class *k* in *L*, do the following:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*L*中的每个类*k*，执行以下操作：
- en: 'For each query sample ![](img/41a130c0-a158-4900-8769-5e1fc8adf9f0.png), update
    the cost function as follows:'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个查询样本![](img/41a130c0-a158-4900-8769-5e1fc8adf9f0.png)，更新代价函数如下：
- en: '![](img/9c041ae9-5e63-4a39-a70e-aa10926261f9.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c041ae9-5e63-4a39-a70e-aa10926261f9.png)'
- en: Intuitively, the first component (in the square braces) minimizes the distance
    between the query and its corresponding prototype of the same class. The second
    term maximizes the sum of the distance between the query and the prototypes of
    the other classes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，第一个组成部分（方括号中的部分）最小化查询样本与其对应类的原型之间的距离。第二项则最大化查询样本与其他类原型之间距离的和。
- en: The authors of the paper demonstrated their work on the Omniglot dataset ([https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot)),
    which contains 1,623 images of handwritten characters collected from 50 alphabets.
    There are 20 examples associated with each character, where each example is drawn
    by a different human subject. The goal is to classify a new character as one of
    the 1,623 classes. They trained prototypical networks using Euclidean distance,
    one-shot, and five-shot scenarios, and training episodes with 60 classes and 5
    query points per class. The following screenshot shows a *t*-SNE ([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    visualization of the embeddings of a subset of similar (but not the same) characters
    of the same alphabet, learned by the prototypical network.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者们在Omniglot数据集上展示了他们的工作（[https://github.com/brendenlake/omniglot](https://github.com/brendenlake/omniglot)），该数据集包含1,623张手写字符的图像，收集自50种字母表。每个字符有20个示例，其中每个示例由不同的人类主试绘制。目标是将一个新的字符分类为1,623个类别中的一个。他们使用欧几里得距离训练原型网络，采用一次性学习和五次性学习的场景，并使用60个类别和每个类别5个查询点的训练集。以下截图展示了由原型网络学习的同一字母表的一个相似（但不相同）字符子集的*t*-SNE可视化（[https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)）。
- en: 'Even though the visualized characters are minor variations of each other, the
    network is able to cluster the hand-drawn characters closely around the class
    prototypes. Several misclassified characters are highlighted in rectangles, along
    with arrows pointing to the correct prototype:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些可视化的字符只是彼此的微小变化，网络仍能将手绘字符紧密地聚类到类别原型周围。几个被误分类的字符已被用矩形框出，并且箭头指向正确的原型：
- en: '![](img/28516082-2f11-4246-bb25-2bee44a03d0c.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28516082-2f11-4246-bb25-2bee44a03d0c.png)'
- en: A *t*-SNE visualization of the embeddings of a subset of similar characters,
    learned by the network; source: https://arxiv.org/abs/1703.05175
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由网络学习到的相似字符子集的*t*-SNE可视化；来源： [https://arxiv.org/abs/1703.05175](https://arxiv.org/abs/1703.05175)
- en: This concludes our description of prototypical networks and metric-based meta
    learning as well. Next, we'll focus on model-based methods.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这也结束了我们对原型网络和基于度量的元学习的描述。接下来，我们将重点讨论基于模型的方法。
- en: Optimization-based learning
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于优化的学习
- en: So far, we have discussed metric-based learning, which uses a special similarity
    measure (which is hard to overfit) to adapt the representational power of NNs
    with the ability to learn from datasets with few training samples. Alternatively,
    model-based approaches rely on improved network architectures (for example, memory
    augmented networks) to solve the same issue. In this section, we'll discuss optimization-based
    approaches, which adjust the training framework to adapt to the few-shot learning
    requirements. More specifically, we'll focus on a particular algorithm called **model-agnostic
    meta learning** (MAML; *Model-Agnostic Meta-Learning for Fast Adaptation of Deep
    Networks*, [https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)).
    As the name suggests, MAML can be applied over any learning problem and model
    that is trained with gradient descent.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了基于度量的学习方法，它使用一种特殊的相似度度量（这种度量难以过拟合）来调整神经网络的表示能力，并且能够从少量训练样本的数据集中学习。另一种方法是基于模型的方式，依靠改进的网络架构（例如，增强记忆网络）来解决相同的问题。在这一部分，我们将讨论基于优化的方法，它通过调整训练框架以适应少样本学习的需求。更具体地说，我们将重点介绍一种叫做**模型无关元学习**（MAML；*Model-Agnostic
    Meta-Learning for Fast Adaptation of Deep Networks*，[https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400))的算法。顾名思义，MAML可以应用于任何通过梯度下降训练的学习问题和模型。
- en: 'To quote the original paper:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 引用原文：
- en: The key idea underlying our method is to train the model’s initial parameters
    such that the model has maximal performance on a new task after the parameters
    have been updated through one or more gradient steps computed with a small amount
    of data from that new task.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的核心思想是训练模型的初始参数，使得在通过少量数据从新任务中进行一次或多次梯度更新后，模型能够在新任务上实现最佳性能。
- en: '...'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: 'The process of training a model’s parameters such that a few gradient steps,
    or even a single gradient step, can produce good results on a new task can be
    viewed from a feature learning standpoint as building an internal representation
    that is broadly suitable for many tasks. If the internal representation is suitable
    to many tasks, simply fine-tuning the parameters slightly (e.g. by primarily modifying
    the top layer weights in a feedforward model) can produce good results. In effect,
    our procedure optimizes for models that are easy and fast to fine-tune, allowing
    the adaptation to happen in the right space for fast learning. From a dynamical
    systems standpoint, our learning process can be viewed as maximizing the sensitivity
    of the loss functions of new tasks with respect to the parameters: when the sensitivity
    is high, small local changes to the parameters can lead to large improvements
    in the task loss.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型参数的过程，使得通过少量的梯度更新，甚至单次梯度更新，就能在新任务上取得良好结果，可以从特征学习的角度看作是构建一个对多任务广泛适用的内部表示。如果这个内部表示适用于多任务，那么仅通过微调参数（例如，主要修改前馈模型中的顶层权重）就能得到良好的结果。实际上，我们的过程是优化那些容易且快速微调的模型，允许在适合快速学习的空间内发生适应。从动态系统的角度来看，我们的学习过程可以被视为最大化新任务的损失函数对参数的敏感度：当敏感度高时，对参数的小的局部变化可以导致任务损失的大幅改善。
- en: The primary contribution of this work is a simple model and task-agnostic algorithm
    for meta-learning that trains a model’s parameters such that a small number of
    gradient updates will lead to fast learning on a new task. We demonstrate the
    algorithm on different model types, including fully connected and convolutional
    networks, and in several distinct domains, including few-shot regression, image
    classification, and reinforcement learning. Our evaluation shows that our meta-learning
    algorithm compares favorably to state-of-the-art one-shot learning methods designed
    specifically for supervised classification, while using fewer parameters, but
    that it can also be readily applied to regression and can accelerate reinforcement
    learning in the presence of task variability, substantially outperforming direct
    pretraining as initialization.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献是一个简单的、与任务无关的元学习算法，它训练模型的参数，使得少量的梯度更新就能在新任务上快速学习。我们在不同的模型类型（包括全连接网络和卷积网络）以及多个不同领域（包括少样本回归、图像分类和强化学习）中展示了该算法。我们的评估表明，尽管使用较少的参数，我们的元学习算法与专门为监督分类设计的一次学习方法相比较具有优势，并且还可以轻松应用于回归，并在任务变异性存在时加速强化学习，显著超越作为初始化的直接预训练。
- en: 'To understand MAML, we''ll introduce some paper-specific notations (some of
    them overlap with notations from the preceding sections, but I prefer to preserve
    the originals from the paper):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解MAML，我们将介绍一些特定于论文的符号（其中一些与前面章节中的符号重叠，但我更倾向于保留论文中的原始符号）：
- en: We'll denote the model (neural network) with *![](img/bd356c54-ccf7-4925-a927-44ad2593e014.png),*
    which maps inputs **x** to outputs **a**.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用*![](img/bd356c54-ccf7-4925-a927-44ad2593e014.png),*表示模型（神经网络），它将输入**x**映射到输出**a**。
- en: We'll denote the full training set with ![](img/fccdc6d6-2215-4a4a-9a04-679fc9fcf47a.png) (equivalent
    to the dataset *D*). Similar to the meta-training of the *Meta-training and meta-testing *section,
    we sample tasks ![](img/405c5e5e-1ef4-4528-95f0-f7408e991380.png)(equivalent to
    episodes) from ![](img/57f3e407-f1aa-423a-92fe-e7b52b477953.png)during training.
    This process is defined as a distribution over tasks ![](img/fe7b2420-8146-4563-95d5-e219560c8b13.png).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用![](img/fccdc6d6-2215-4a4a-9a04-679fc9fcf47a.png)表示完整的训练集（等同于数据集*D*）。与*元训练和元测试*部分的元训练类似，我们在训练过程中从![](img/57f3e407-f1aa-423a-92fe-e7b52b477953.png)中采样任务![](img/405c5e5e-1ef4-4528-95f0-f7408e991380.png)（等同于回合）。该过程被定义为一个关于任务的分布![](img/fe7b2420-8146-4563-95d5-e219560c8b13.png)。
- en: We'll denote one task (an episode) with ![](img/54b1269a-0a1a-424c-ae52-2bc0e9ac97f0.png).
    It is defined by a loss function ![](img/0431e13d-3789-4612-b937-aa14bed7d0cd.png) (equivalent
    to the loss *J*), a distribution over the initial observations ![](img/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png),
    a transition distribution ![](img/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png), and
    length *H*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用![](img/54b1269a-0a1a-424c-ae52-2bc0e9ac97f0.png)表示一个任务（一个回合）。它通过损失函数![](img/0431e13d-3789-4612-b937-aa14bed7d0cd.png)（等同于损失*J*）、初始观察分布![](img/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png)、转移分布![](img/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png)和长度*H*来定义。
- en: 'To understand some components of the MAML task definition, let''s note that
    besides supervised problems, MAML can be applied to **reinforcement learning**
    (**RL**) tasks as well. In the RL framework, we have an environment and an agent,
    which continuously interact with each other. At each step, the agent takes an
    action (from a number of possible actions) and the environment provides it with
    feedback. The feedback consists of a reward (which could be negative) and the
    new state of the environment after the agent''s action. Then the agent takes a
    new action, and so on, as illustrated in the following diagram:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解MAML任务定义的一些组成部分，我们需要注意，除了监督学习问题外，MAML也可以应用于**强化学习**（**RL**）任务。在强化学习框架中，我们有一个环境和一个代理，它们不断相互作用。在每个步骤中，代理采取一个动作（从多个可能的动作中选择），环境则提供反馈。反馈包括奖励（可能是负奖励）以及代理动作后的新环境状态。然后，代理再采取新的动作，以此类推，如下图所示：
- en: '![](img/db858706-4511-4407-8c5e-5fbd55b4bf0f.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db858706-4511-4407-8c5e-5fbd55b4bf0f.png)'
- en: The RL framework
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习框架
- en: Many real-world tasks can be represented as RL problems, including games, where
    the agent is the player and the environment is the game universe.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的任务可以表示为强化学习（RL）问题，包括游戏，其中代理是玩家，环境是游戏宇宙。
- en: 'We can view task ![](img/56d99c2b-8167-4080-9483-ed465ed1f3d3.png) in both
    supervised and RL contexts. With a supervised task, we have labeled training samples ![](img/df638237-5175-4d54-bbda-b4eafb34a6a5.png) in
    no particular order. But in an RL context, we can view the inputs **x** as the
    environment state and the outputs **a** as the agent''s action. In this scenario,
    the task is sequential—state **x***[1]* leads to action **a**[*1*], which in turn
    leads to state **x**[*2*], and so on. The initial state of the environment is
    denoted as ![](img/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png). This means that ![](img/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png) is
    the probability of a new environment state ![](img/d1ac7d91-b821-4f28-a9af-0ac7326c59b7.png),
    given the previous state **x***[t]* and the agent''s action **a**[*t*]. The loss ![](img/aded729e-1ffc-4689-9b56-719691cfaa4b.png) can
    be viewed in both contexts as well: a misclassification loss in the supervised
    scenario and a cost function (the one that provides rewards) in the RL scenario.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在监督学习和强化学习的背景下看待任务！[](img/56d99c2b-8167-4080-9483-ed465ed1f3d3.png)。在监督学习任务中，我们有标注的训练样本！[](img/df638237-5175-4d54-bbda-b4eafb34a6a5.png)，这些样本的顺序没有特定要求。但在强化学习的背景下，我们可以将输入**x**视为环境状态，将输出**a**视为代理的动作。在这种情况下，任务是顺序的——状态**x**[*1*]导致动作**a**[*1*]，进而导致状态**x**[*2*]，依此类推。环境的初始状态表示为！[](img/ad65abcb-0d83-4ffe-aae4-b8e50ef72d3d.png)。这意味着！[](img/1aaba41c-f1ed-4a83-9010-848ce3f9438e.png)是给定前一个状态**x**[*t*]和代理的动作**a**[*t*]时，新环境状态**x**[*t+1*]的概率。损失函数！[](img/aded729e-1ffc-4689-9b56-719691cfaa4b.png)在这两种情况下也可以这样理解：在监督学习场景中是误分类损失，而在强化学习场景中则是成本函数（即提供奖励的函数）。
- en: 'Now that we''re familiar with the notation, let''s focus on the MAML algorithm.
    To understand how it works, we''ll look at another quote from the original paper:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了符号表示，让我们聚焦于MAML算法。为了理解它是如何工作的，我们将看一下原始论文中的另一个引用：
- en: 'We propose a method that can learn the parameters of any standard model via
    meta-learning in such a way as to prepare that model for fast adaptation. The
    intuition behind this approach is that some internal representations are more
    transferable than others. For example, a neural network might learn internal features
    that are broadly applicable to all tasks in ![](img/8d889142-b146-4f41-a47e-dd57238882d0.png),
    rather than a single individual task. How can we encourage the emergence of such
    general-purpose representations? We take an explicit approach to this problem:
    since the model will be fine-tuned using a gradient-based learning rule on a new
    task, we will aim to learn a model in such a way that this gradient-based learning
    rule can make rapid progress on new tasks drawn from ![](img/231436d9-6ab9-4ff4-a63f-62fda7143215.png),
    without overfitting. In effect, we will aim to find model parameters that are
    sensitive to changes in the task, such that small changes in the parameters will
    produce large improvements on the loss function of any task drawn from ![](img/1be30fb7-8c51-469d-8e47-c2ffb6895e1c.png),
    when altered in the direction of the gradient of that loss.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种方法，通过元学习可以学习任何标准模型的参数，以便为该模型的快速适应做准备。该方法背后的直觉是，一些内部表示比其他表示更具可迁移性。例如，神经网络可能会学习到一些广泛适用于所有任务的内部特征，而不仅仅是针对某个特定任务。我们如何鼓励此类通用表示的出现？我们采取了一种明确的方法来解决这个问题：由于模型将使用基于梯度的学习规则在新任务上进行微调，我们的目标是以这样的方式学习模型，使得该基于梯度的学习规则能够在从
    ![](img/231436d9-6ab9-4ff4-a63f-62fda7143215.png) 中抽取的新任务上迅速取得进展，而不至于过拟合。实际上，我们的目标是找到对任务变化敏感的模型参数，使得当参数朝着该损失函数的梯度方向变化时，任何从
    ![](img/1be30fb7-8c51-469d-8e47-c2ffb6895e1c.png) 中抽取的任务的损失函数都会大幅改进。
- en: 'After all this suspense, let''s check the MAML algorithm, illustrated by the
    following pseudocode:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些悬念后，让我们看看MAML算法，其伪代码如下所示：
- en: '![](img/8ed9c91a-6d4d-4dbd-8a5d-bb0c9013be39.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ed9c91a-6d4d-4dbd-8a5d-bb0c9013be39.png)'
- en: The MAML algorithm: source: https://arxiv.org/abs/1703.03400
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: MAML算法：来源：[https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)
- en: 'The algorithm has an outer (line 2) and an inner loop (line 4). We''ll start
    with the inner loop, which iterates over a number of tasks, sampled from the task
    distribution ![](img/17f4abf2-e718-48f2-b619-f07ca66f52ff.png). Let''s focus on
    a single loop iteration, which handles a single task ![](img/07a0cc41-5a51-4412-bf2c-6eee6e86c686.png) with ![](img/bdd38bab-6c83-4fbb-8f26-a4ffcdb2b52e.png) training
    samples, where ![](img/95d50486-9933-4695-ab69-4170bc0a56ba.png) is the support
    set of the task. The training samples are processed as batches in the following
    steps (lines 4 through 7 in the preceding screenshot):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法有一个外部循环（第2行）和一个内部循环（第4行）。我们将从内部循环开始，它遍历一系列任务，这些任务是从任务分布中抽样得到的 ![](img/17f4abf2-e718-48f2-b619-f07ca66f52ff.png)。让我们关注单次循环迭代，它处理一个单独的任务
    ![](img/07a0cc41-5a51-4412-bf2c-6eee6e86c686.png)，该任务有 ![](img/bdd38bab-6c83-4fbb-8f26-a4ffcdb2b52e.png)
    训练样本，其中 ![](img/95d50486-9933-4695-ab69-4170bc0a56ba.png) 是该任务的支持集。训练样本将在以下步骤中作为批处理进行处理（参见前面的截图第4行至第7行）：
- en: Propagate the samples through the model and compute the loss ![](img/b6d1c143-b5c5-4d67-adbc-d3353ecb6fc8.png).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将样本通过模型传播并计算损失 ![](img/b6d1c143-b5c5-4d67-adbc-d3353ecb6fc8.png)。
- en: Compute the error gradient ![](img/2ce06d60-e158-48ce-8c69-1aa1ecf91ac2.png) with
    respect to the initial parameters *θ.*
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于初始参数 *θ* 的误差梯度 ![](img/2ce06d60-e158-48ce-8c69-1aa1ecf91ac2.png)。
- en: Propagate the gradient backward and compute the updated model parameters ![](img/3949829b-221a-4175-851d-8f886214d0b5.png),
    where α is the learning rate. Note that the parameters ![](img/02942f5e-041d-42b8-af9a-d45b069de7d8.png) are
    auxiliary and are specific for each task. To clarify, whenever the inner loop
    starts a new iteration for a new task ![](img/79232bbc-2d38-44cd-aca8-30c1f1fe9df1.png),
    the model always starts with the same initial parameters *θ**.* At the same time,
    each task stores its updated weights as an additional variable ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png) without
    actually modifying the initial parameters *θ* (we'll update the original model
    in the outer loop).
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向后传播梯度并计算更新后的模型参数 ![](img/3949829b-221a-4175-851d-8f886214d0b5.png)，其中 α 是学习率。请注意，参数
    ![](img/02942f5e-041d-42b8-af9a-d45b069de7d8.png) 是辅助参数，特定于每个任务。为了澄清，每当内部循环为一个新任务
    ![](img/79232bbc-2d38-44cd-aca8-30c1f1fe9df1.png) 开始一个新迭代时，模型总是从相同的初始参数 *θ** 开始。与此同时，每个任务会将其更新的权重存储为一个附加变量
    ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png)，而不会实际修改初始参数 *θ*（我们将在外部循环中更新原始模型）。
- en: 'We can perform such gradient updates multiple times over the same task. Think
    of it as training over multiple batches, implemented with an additional nested
    loop in the inner loop. In this scenario, the algorithm starts each iteration
    *i* with the weights ![](img/5744edb7-fe78-47a9-97df-9f4b87dc23bc.png) of the
    last iteration and not with the initial parameters *θ*, as shown in the following
    formula:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以对同一任务执行多次这样的梯度更新。可以把它当作在多个批次上训练，只不过在内部循环中额外嵌套了一个循环。在这种情况下，算法在每次迭代 *i* 时，使用的是上一次迭代的权重
    ![](img/5744edb7-fe78-47a9-97df-9f4b87dc23bc.png)，而不是初始参数 *θ*，如下公式所示：
- en: '![](img/fcd96c3f-47ce-4d6a-b59b-084e6d9fa2ca.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcd96c3f-47ce-4d6a-b59b-084e6d9fa2ca.png)'
- en: In the case of multiple iterations, only the latest weights ![](img/bb684ec6-d41a-4ca2-bee9-b765b06e8689.png) are
    preserved.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在多次迭代的情况下，仅保留最新的权重 ![](img/bb684ec6-d41a-4ca2-bee9-b765b06e8689.png)。
- en: 'Only after the inner loop is done can we proceed to update the initial parameters *θ*
    of the original model, based on the feedback of all tasks ![](img/cb429e50-f798-426e-9792-5557e2699e3c.png).
    To understand why this is necessary, let''s take a look at the following diagram:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在内部循环完成后，我们才能基于所有任务的反馈 ![](img/cb429e50-f798-426e-9792-5557e2699e3c.png) 来更新原始模型的初始参数
    *θ*。为了理解为什么这是必要的，下面是一个图示：
- en: '![](img/185047d5-b9ac-4aa4-897c-6285fe0566d0.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/185047d5-b9ac-4aa4-897c-6285fe0566d0.png)'
- en: MAML that optimizes for parameters *θ* that can quickly adapt to new tasks: source: https://arxiv.org/abs/1703.03400
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 'MAML优化能够快速适应新任务的参数 *θ*：来源: [https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)'
- en: 'It shows the error gradients of three tasks ![](img/f67b0a3a-58bc-4c2c-be06-8adfbb56484e.png) along
    the global error gradient. Let''s assume that, instead of an inner/outer loop
    mechanism, we iterate sequentially over each task and simply update the original
    model parameters *θ* after each mini batch*.* We can see that the gradients of
    the different loss functions would push the model in completely different directions;
    for example, the error gradient of task 2 will contradict the gradient of task
    1\. MAML solves this problem by aggregating (but not applying) the updated weights
    for each task from the inner loop (the auxiliary parameters ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png)).
    Then, we can compute the outer loop cost function (referred to as the meta-objective),
    combining the updated weights of all tasks all at once (this is a meta optimization
    across all tasks):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了三个任务 ![](img/f67b0a3a-58bc-4c2c-be06-8adfbb56484e.png) 沿全局误差梯度的误差梯度。假设我们不使用内部/外部循环机制，而是按顺序遍历每个任务，并在每个小批次后简单更新原始模型参数
    *θ*。我们可以看到，不同损失函数的梯度会将模型推向完全不同的方向；例如，任务2的误差梯度与任务1的梯度会相互对立。MAML通过汇总（但不应用）来自内部循环的每个任务的更新权重（辅助参数
    ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png)）来解决这个问题。然后，我们可以计算外部循环的代价函数（称为元目标），结合所有任务的更新权重一次性进行优化（这是跨任务的元优化）：
- en: '![](img/0ece93d6-c678-40b6-8b7d-9fd721f250f9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ece93d6-c678-40b6-8b7d-9fd721f250f9.png)'
- en: 'We use the following formula for the weight update of the main model parameters:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下公式来更新主模型参数的权重：
- en: '![](img/b4d82576-378d-4629-a4f6-b3fa3259db10.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4d82576-378d-4629-a4f6-b3fa3259db10.png)'
- en: Here, β is the learning rate. The outer loop tasks ![](img/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png) (line
    8 of the MAML pseudocode program) are not the same as the ones from the inner
    loop (line 3). We can think of inner loop tasks as the training set and the tasks
    of the outer loop as the validation set. Note that we use task-specific parameters ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png) to
    compute the losses, but we compute the loss gradient with respect to the initial
    parameters *θ*. To clarify, this means that we backpropagate through the outer
    loop and the inner loop as well. This is referred to as a second-order gradient,
    because we compute a gradient over the gradient (second derivative). This makes
    it possible to learn parameter that can generalize over a task even after a number
    of updates.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，β 是学习率。外部循环任务 ![](img/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png)（MAML伪代码程序的第8行）与内部循环任务（第3行）不同。我们可以将内部循环任务看作训练集，外部循环任务看作验证集。需要注意的是，我们使用特定任务的参数
    ![](img/8d2390a5-3ae9-479d-94f5-6c1e50126ca6.png) 来计算损失，但我们计算的损失梯度是相对于初始参数 *θ*
    的。为了澄清，这意味着我们需要通过外部循环和内部循环进行反向传播。这被称为二阶梯度，因为我们计算的是梯度的梯度（二阶导数）。这使得即使经过多次更新，模型仍然可以学习到能够泛化到任务的参数。
- en: 'One disadvantage of MAML is that backpropagation through the full computational
    graph (the outer loop and inner loop) is computationally expensive. In addition,
    because of the large number of backpropagation steps, it can suffer from vanishing
    or exploding gradients. To better understand this, let''s assume that we have
    a single task ![](img/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png) (we''ll omit it
    from the formulas); we perform a single gradient step (one inner loop iteration)
    for that task, and the inner loop''s updated parameters are ![](img/b5d8c89a-4eb0-42b2-8562-58268f1bc207.png).
    That is, we change the notation of the loss function from ![](img/14233372-0ec2-4bdf-a55a-62c8d16a6764.png) to
    ![](img/cdb3b688-6555-483c-a3c6-eb1e13d32ca0.png). Then, the parameter update
    rule of the outer loop becomes the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: MAML 的一个缺点是通过完整计算图（外循环和内循环）进行反向传播是计算密集型的。此外，由于反向传播步骤众多，它可能会遭遇梯度消失或爆炸的问题。为了更好地理解这一点，我们假设有一个单一任务![](img/cafe3eca-c354-4d70-9cdc-77c5f072b84d.png)（我们将在公式中省略它）；我们对该任务执行一次梯度步骤（一次内循环迭代），内循环更新后的参数是![](img/b5d8c89a-4eb0-42b2-8562-58268f1bc207.png)。即，我们将损失函数的符号从![](img/14233372-0ec2-4bdf-a55a-62c8d16a6764.png)改为![](img/cdb3b688-6555-483c-a3c6-eb1e13d32ca0.png)。然后，外循环的参数更新规则变为：
- en: '![](img/f64fb8b8-0784-453e-ae82-c4712fb0b196.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f64fb8b8-0784-453e-ae82-c4712fb0b196.png)'
- en: 'We can compute the gradient of the loss with respect to the initial parameters *θ*
    with the help of the chain rule (see [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts Of Neural Networks*):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以借助链式法则计算损失相对于初始参数*θ*的梯度（见[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)，*神经网络的基础*）：
- en: '![](img/dedd5d9b-fbff-41a2-9aad-e9fefa467305.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dedd5d9b-fbff-41a2-9aad-e9fefa467305.png)'
- en: 'We can see that the formula includes second-degree derivative ![](img/01fbf8b5-14a9-4822-badf-d77723f72555.png).
    The authors of MAML have proposed the so-called **first-order MAML** (**FOMAML**),
    which simply ignores the term ![](img/9a4d1637-422f-4a4e-8b17-ebebecac8549.png).
    With this, we have ![](img/30598cc8-79a9-41cb-9d04-bcea6c45d59f.png) and the FOMAML
    gradient becomes:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到公式中包含了二阶导数![](img/01fbf8b5-14a9-4822-badf-d77723f72555.png)。MAML 的作者提出了所谓的**一阶
    MAML**（**FOMAML**），即简单地忽略项![](img/9a4d1637-422f-4a4e-8b17-ebebecac8549.png)。这样，我们得到![](img/30598cc8-79a9-41cb-9d04-bcea6c45d59f.png)，FOMAML
    的梯度变为：
- en: '![](img/c3fd34a5-d556-428d-b7e1-e7d316a681e4.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3fd34a5-d556-428d-b7e1-e7d316a681e4.png)'
- en: This simplified formula excludes the computationally expensive second-order
    gradient.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简化的公式排除了计算量大的二阶梯度。
- en: 'So far, we have looked at the generic MAML algorithm, which applies for both
    supervised and RL settings. Next, let''s focus on the supervised version. Let''s
    recall that in the supervised case, each task is a list of unrelated input/label
    pairs and the episode length *H* is *1*. We can see the MAML algorithm for few-shot
    supervised learning in the following pseudocode (it''s similar to the generic
    algorithm):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了通用的 MAML 算法，它适用于监督学习和强化学习环境。接下来，让我们专注于监督学习版本。我们可以回顾一下，在监督学习的情况下，每个任务都是一组不相关的输入/标签对，且每个任务的回合长度*H*为*1*。我们可以在以下伪代码中看到少样本监督学习的
    MAML 算法（它与通用算法相似）：
- en: '![](img/66cf802b-e94d-417b-a945-c2c575b9ccfe.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66cf802b-e94d-417b-a945-c2c575b9ccfe.png)'
- en: MAML for few-shot supervised learning: source: https://arxiv.org/abs/1703.03400
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: MAML 用于少样本监督学习：来源：[https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)
- en: In the preceding code, equations 2 and 3 both refer to cross-entropy losses
    for classification tasks or mean square errors for regression tasks, ![](img/84101fed-0c4c-4d2d-b671-b4cd6dd43ee5.png) refers
    to the inner loop training set, and ![](img/fb149d2e-5e1d-4262-8e82-0e463ff8033e.png) refers
    to the validation set of the outer loop.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，方程式 2 和 3 都指的是分类任务的交叉熵损失或回归任务的均方误差，![](img/84101fed-0c4c-4d2d-b671-b4cd6dd43ee5.png)表示内部循环训练集，![](img/fb149d2e-5e1d-4262-8e82-0e463ff8033e.png)表示外部循环的验证集。
- en: 'Finally, let''s discuss the RL scenario, as illustrated by the following pseudocode:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论一下强化学习场景，如下伪代码所示：
- en: '![](img/2a79b67c-b4c1-4ea9-88e9-14cb933aa8dc.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a79b67c-b4c1-4ea9-88e9-14cb933aa8dc.png)'
- en: MAML for few-shot reinforcement learning: source: https://arxiv.org/abs/1703.03400
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MAML 用于少样本强化学习：来源：[https://arxiv.org/abs/1703.03400](https://arxiv.org/abs/1703.03400)
- en: Each sample ![](img/22fd3afd-a08e-4a6e-8351-81ea3a82d65e.png) represents a trajectory
    of one game episode, where the environment presents the agent with its current
    state ![](img/6995f43b-5308-45bb-a6f7-ee41a2a6ce75.png) at step *t*. In turn, the
    agent (NN) samples use **policy** ![](img/5365dd0a-8fb5-43fa-be6e-98e72aace46f.png) to
    map states ![](img/cbf56b62-9c7a-463d-881c-0c27324811d0.png) to a distribution
    over actions ![](img/b0c3bb4e-5d4e-4a33-8e1a-e878aabb91d0.png). The model uses
    a special type of loss function, which aims to train the network to maximize rewards
    over all steps of the episode.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本 ![](img/22fd3afd-a08e-4a6e-8351-81ea3a82d65e.png) 代表一场游戏回合的轨迹，其中环境在步骤 *t*
    处展示了代理的当前状态 ![](img/6995f43b-5308-45bb-a6f7-ee41a2a6ce75.png)。然后，代理（NN）使用 **策略** ![](img/5365dd0a-8fb5-43fa-be6e-98e72aace46f.png)
    将状态 ![](img/cbf56b62-9c7a-463d-881c-0c27324811d0.png) 映射到一个动作的分布 ![](img/b0c3bb4e-5d4e-4a33-8e1a-e878aabb91d0.png)。该模型使用一种特殊类型的损失函数，旨在训练网络以最大化整个回合步骤中的奖励。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at the field of meta learning, which can be described
    as learning to learn. We started with an introduction to meta learning. More specifically,
    we talked about zero-shot and few-shot learning, as well as meta training and
    meta testing. Then, we focused on several metric-based learning approaches. We
    looked at matching networks, implemented an example of a Siamese network, and
    we introduced prototypical networks. Next, we focused on optimization-based learning,
    where we introduced the MAML algorithm.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们研究了元学习领域，这可以描述为学习如何学习。我们从元学习的介绍开始。更具体地，我们讨论了零-shot学习和少-shot学习，以及元训练和元测试。然后，我们重点介绍了几种基于度量的学习方法。我们研究了匹配网络，实施了一个孪生网络的示例，并介绍了原型网络。接下来，我们集中讨论了基于优化的学习方法，并介绍了MAML算法。
- en: 'In the next chapter, we''ll learn about an exciting topic: automated vehicles.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一个激动人心的主题：自动驾驶车辆。
