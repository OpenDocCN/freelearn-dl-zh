- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Deploying Deep Learning Models to Production
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境
- en: In the previous chapters, we delved into the intricacies of data preparation,
    **deep learning** (**DL**) model development, and how to deliver insightful outcomes
    from our DL models. Through meticulous data analysis, feature engineering, model
    optimization, and model analysis, we have learned the techniques to ensure our
    DL models can perform well and as desired. As we transition into the next phase
    of our journey, the focus now shifts toward deploying these DL models in production
    environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们深入探讨了数据准备、**深度学习**（**DL**）模型开发的复杂性，以及如何从我们的DL模型中获得有价值的见解。通过细致的数据分析、特征工程、模型优化和模型分析，我们已经掌握了确保DL模型表现良好并按预期运行的技术。随着我们进入下一个阶段，焦点将转向在生产环境中部署这些DL模型。
- en: Reaching the stage of deploying a DL model to production is a significant accomplishment,
    considering that most models don’t make it that far. If your project has reached
    this milestone, it signifies that you have successfully satisfied stakeholders,
    presented valuable insights, and performed thorough value and metric analysis.
    Congratulations, as you are now one step closer to joining the small percentage
    of successful projects amidst countless attempts. It’s worth noting that, according
    to a 2022 Gartner survey highlighted by VentureBeat, which was executed online
    from October to December 2021 with 699 respondents from organizations in the US,
    Germany, and the UK, only around half (54%) of AI models make it into production.
    Furthermore, the 2023 State of AI Infrastructure Survey, published by Run AI,
    an AI resource management solutions provider, reported that in over 88% of the
    companies surveyed, less than half of the AI models reached the production stage.
    This involved 450 industry professionals across the US and Western Europe. These
    two surveys emphasize the challenges faced in this process and the significance
    of reaching this stage.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境的阶段是一个重要的成就，因为大多数模型无法走到这一步。如果你的项目已经达到了这一里程碑，这意味着你已经成功满足了利益相关者的需求，提供了有价值的见解，并进行了全面的价值和指标分析。恭喜你，你离加入成功项目的少数群体又近了一步。值得注意的是，根据2022年Gartner的一项调查，VentureBeat进行了报道，这项调查在线进行，时间为2021年10月至12月，收集了来自美国、德国和英国的699名受访者的数据，约有一半（54%）的AI模型最终进入生产阶段。此外，由AI资源管理解决方案提供商Run
    AI发布的2023年《AI基础设施状态调查》报告显示，在超过88%的受访公司中，进入生产阶段的AI模型不到一半。该调查涉及450名来自美国和西欧的行业专业人士。这两项调查突显了这个过程中的挑战，以及达到这一阶段的重要性。
- en: The ultimate goal here is to make these DL models accessible to end users, in
    an intuitive way, enabling them to harness the full potential of DL in real-world
    applications. In this chapter, we will explore the various strategies, tools,
    and best practices to seamlessly integrate our DL models into production systems,
    ensuring scalability, reliability, and ease of use for a diverse range of users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最终目标是让这些DL模型以直观的方式对最终用户可用，使他们能够在实际应用中充分发挥DL的潜力。在本章中，我们将探讨将DL模型无缝集成到生产系统中的各种策略、工具和最佳实践，确保可扩展性、可靠性和用户友好性，适应各种不同的用户需求。
- en: 'Specifically, we will be going through the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将讨论以下主题：
- en: Exploring the crucial components for DL model deployment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习模型部署的关键组件
- en: Identifying key DL model deployment requirements
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定深度学习模型部署的关键需求
- en: Choosing the right DL model deployment options
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择正确的深度学习模型部署选项
- en: Exploring deployment decisions based on practical use cases
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨基于实际使用案例的部署决策
- en: Discovering general recommendations for DL deployment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索深度学习（DL）模型部署的通用建议
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ONNX、TensorRT和NVIDIA Triton服务器部署语言模型
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will have a practical topic in the last section of this chapter. This tutorial
    requires you to have a Linux machine with an NVIDIA GPU device ideally in Ubuntu
    with Python 3.10 and the `nvidia-docker` tool installed. Additionally, we will
    require the following Python libraries to be installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后一部分，我们将进行一个实践性主题。本教程要求你拥有一台配备NVIDIA GPU设备的Linux机器，最好是在Ubuntu上安装Python
    3.10和`nvidia-docker`工具。此外，我们还需要安装以下Python库：
- en: '`numpy`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`transformers==4.21.3`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.21.3`'
- en: '`nvidia-tensorrt==8.4.1.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nvidia-tensorrt==8.4.1.5`'
- en: '`torch==1.12.0`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch==1.12.0`'
- en: '`transformers-deploy`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers-deploy`'
- en: '`tritonclient`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tritonclient`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可在 GitHub 上获取：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15)。
- en: Exploring the crucial components for DL model deployment
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度学习模型部署的关键组件
- en: So, what does it take to deploy a DL model? It starts with having a holistic
    view of each required component and defining clear requirements that guide decision-making
    for every aspect. This approach ensures alignment with the business goals and
    requirements, maximizing the chances of a successful deployment. With careful
    planning, diligent execution, and a focus on meeting the needs of the business,
    you can increase the likelihood of successfully deploying your DL model and unlocking
    its value for users. We will start by discovering components that are required
    to deploy a DL model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，部署深度学习模型需要什么？它从全面了解每个必需的组件开始，并定义清晰的需求，以指导每个方面的决策。这种方法确保与业务目标和需求保持一致，最大化成功部署的机会。通过精心规划、严谨执行和专注于满足业务需求，你可以增加成功部署深度学习模型并为用户释放其价值的可能性。我们将从探索部署深度学习模型所需的组件开始。
- en: 'Deploying a DL model to production involves more than just the trained model
    itself. It requires seamless collaboration among various components, working together
    to enable users to effectively extract value from the model’s predictions. These
    components are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习模型部署到生产环境不仅仅涉及训练好的模型本身。它需要各个组件之间的无缝协作，共同工作以帮助用户有效地从模型的预测中提取价值。以下是这些组件：
- en: '**Architectural choices**: The overall design and structure of the deployment
    system. Should the model be implemented as a separate service, microservice, or
    directly part of an existing service? Should the model be hosted on the cloud
    or on-premises? Another aspect to consider is whether to use container orchestration
    platforms, such as Kubernetes, Docker Swarm, or Apache Mesos, to manage and scale
    deployments of deep learning models in containerized applications.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构选择**：部署系统的整体设计和结构。模型应该作为一个独立的服务、微服务，还是直接作为现有服务的一部分来实现？模型应该托管在云端还是本地？另一个需要考虑的方面是，是否使用容器编排平台，如
    Kubernetes、Docker Swarm 或 Apache Mesos，来管理和扩展深度学习模型在容器化应用中的部署。'
- en: These platforms provide flexible deployment across multiple machines, cloud
    providers, or on-premises infrastructure, and can be used in conjunction with
    other tools and services for efficient management of containerized applications
    and microservices.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些平台提供灵活的部署方式，可以跨多台机器、云服务提供商或本地基础设施进行部署，并可与其他工具和服务结合使用，以高效管理容器化应用程序和微服务。
- en: '**Hardware/physical infrastructure choices**: This involves the decision of
    which physical computing device you want to use and the choice of each of the
    components that make up the computing device. Should the model be run on a CPU,
    GPU, TPU, or an **Artificial Neural Engine** (**ANE**) in an iPhone?'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件/物理基础设施选择**：这涉及到选择你希望使用的物理计算设备，以及组成该计算设备的各个组件的选择。模型应该在 CPU、GPU、TPU，还是 iPhone
    中的 **人工神经引擎**（**ANE**）上运行？'
- en: '**Model packaging methods and frameworks**: This is a component that involves
    serializing the model’s architecture, weights, and configuration into a file or
    container format, allowing for easy distribution, deployment, and usage across
    various environments. Usually, the DL framework will provide out-of-the-box support
    for model packaging. Do you have architectural choices and hardware infrastructure
    choices or preferences that require the model to be packaged in a specific way?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型打包方法和框架**：这是一个涉及将模型的架构、权重和配置序列化为文件或容器格式的组件，从而实现模型在各种环境中的分发、部署和使用。通常，深度学习框架会提供开箱即用的模型打包支持。你是否有架构选择和硬件基础设施偏好，要求模型以特定方式进行打包？'
- en: '**Model safety, trust, and reliability component**: This encompasses the measures
    taken to ensure that the deployed model is secure, trustworthy, and reliable in
    making accurate predictions. It involves implementing guardrails to prevent misuse
    or unintended behavior, ensuring model consistency, monitoring model performance,
    and providing prediction explanations to help users understand and trust the model’s
    output. Ensuring data privacy and compliance with relevant regulations is also
    a critical aspect of this component. Are there any specific safety, trust, or
    reliability requirements that must be met for the deployment of your DL model?'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型安全性、可信度和可靠性组件**：这包括采取措施确保部署的模型在做出准确预测时是安全、可信和可靠的。它涉及到实施防护措施以防止滥用或意外行为，确保模型的一致性，监控模型性能，并提供预测解释，以帮助用户理解和信任模型的输出。确保数据隐私并遵守相关法规也是该组件的一个关键方面。是否有任何特定的安全性、可信度或可靠性要求，必须在你的深度学习模型部署中得到满足？'
- en: '**Security and authentication methods**: These involve protecting your DL model
    and its associated infrastructure, as well as controlling access to the model
    by implementing suitable authentication, authorization, and encryption mechanisms.
    This ensures that only authorized users can access and interact with the model,
    preventing unauthorized access, data breaches, and potential misuse of the model.
    What are the necessary security and authentication requirements for your DL model
    deployment, and how will they be integrated into your system?'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性和身份验证方法**：这些涉及到保护你的深度学习模型及其相关基础设施，以及通过实施合适的身份验证、授权和加密机制来控制对模型的访问。这确保只有授权用户才能访问和与模型进行交互，防止未授权访问、数据泄露以及潜在的模型滥用。你的深度学习模型部署需要哪些安全性和身份验证要求？这些要求将如何集成到你的系统中？'
- en: '**Communication protocols**: These define the rules and formats for exchanging
    data between the deployed model and other components or users in the system. It
    involves selecting appropriate protocols based on the requirements, such as latency,
    reliability, and data formats. Examples of communication protocols are HTTP, RESTful
    APIs, gRPC, server-sent events, and WebSockets. What communication protocols best
    suit your DL model deployment, and how will they be implemented to enable seamless
    interaction between the model and its users?'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信协议**：这些定义了在部署的模型和系统中的其他组件或用户之间交换数据的规则和格式。它涉及根据要求选择适当的协议，例如延迟、可靠性和数据格式。通信协议的示例包括HTTP、RESTful
    API、gRPC、服务器推送事件和WebSocket。哪些通信协议最适合你的深度学习模型部署？它们将如何实现，以便模型和用户之间能够无缝互动？'
- en: '**User interfaces**: These are the visual components and interaction methods
    that allow users or downstream systems to access, interact with, and obtain predictions
    from the deployed DL model. User interfaces can be web-based, mobile, desktop
    applications, APIs, or even voice-activated systems, depending on the use case
    and target audience. Designing user-friendly and intuitive interfaces is essential
    to ensure that users can easily understand and make the most of the model’s predictions.
    What type of user interface is best suited for your DL model deployment, and how
    will it be designed to provide an optimal user experience while effectively delivering
    the model’s capabilities? Here are some examples of user interface design challenges
    specific to DL models:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：这些是视觉组件和交互方式，允许用户或下游系统访问、与部署的深度学习模型进行交互并获取预测。用户界面可以是基于网页、移动端、桌面应用程序、API，甚至是语音激活系统，具体取决于使用场景和目标用户群体。设计用户友好且直观的界面至关重要，以确保用户能够轻松理解并充分利用模型的预测功能。什么样的用户界面最适合你的深度学习模型部署？如何设计界面，以提供最佳的用户体验，同时有效地传递模型的能力？以下是一些特定于深度学习模型的用户界面设计挑战示例：'
- en: '**Visualizing complex data**: DL models often work with multi-dimensional data,
    which can be challenging to display in a user-friendly manner. Designers may need
    to devise innovative ways to visualize and represent such data, making it accessible
    and understandable for users.'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化复杂数据**：深度学习模型通常处理多维数据，这可能会让数据以用户友好的方式展示变得具有挑战性。设计人员可能需要设计创新的方式来可视化和呈现这些数据，使用户能够访问并理解这些数据。'
- en: '**Handling real-time data**: In scenarios where DL models process and analyze
    real-time data, the user interface must efficiently manage data streaming and
    updates, ensuring that users receive timely and accurate information without being
    overwhelmed.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理实时数据**：在深度学习模型处理和分析实时数据的场景中，用户界面必须有效地管理数据流和更新，确保用户及时获得准确的信息，同时避免信息过载。'
- en: '**Facilitating model interactions**: Users may need to interact with the DL
    model to adjust parameters, provide feedback, or request additional information.
    Designing intuitive UI elements for these interactions is crucial to ensure users
    can effectively engage with the model.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**促进模型交互**：用户可能需要与深度学习模型互动，以调整参数、提供反馈或请求额外信息。设计直观的用户界面元素以支持这些交互至关重要，确保用户能够有效地与模型互动。'
- en: '**Interpreting model output**: DL models can produce complex and nuanced output,
    which may be challenging for users to understand and act upon. Designers must
    find ways to present model predictions in a clear and actionable manner while
    also providing contextual information to help users interpret the results.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解释模型输出**：深度学习模型可能会产生复杂且微妙的输出，这可能会使用户理解和操作变得困难。设计者必须找到方法，以清晰且可操作的方式呈现模型预测结果，同时提供上下文信息，帮助用户解释结果。'
- en: '**Managing uncertainty**: DL models may produce predictions with varying degrees
    of confidence or uncertainty. Designers should consider how to communicate this
    uncertainty to users, ensuring that they are aware of the limitations and potential
    risks associated with the model’s output.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理不确定性**：深度学习模型可能会产生具有不同置信度或不确定性的预测。设计者应考虑如何将这种不确定性传达给用户，确保他们意识到模型输出的局限性和潜在风险。'
- en: '**Accessibility and inclusivity**: User interfaces for DL models should cater
    to a diverse range of users, including those with different abilities, languages,
    and cultural backgrounds. Designers must ensure that their interfaces are accessible
    and inclusive, taking into account various user needs and preferences.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性与包容性**：深度学习模型的用户界面应当适应各种不同背景的用户，包括有不同能力、语言和文化背景的用户。设计师必须确保他们的界面具有可访问性和包容性，考虑到各种用户需求和偏好。'
- en: '**Monitoring and logging components**: These tools allow you to track the performance,
    usage, and health of your DL model in real time. By collecting and analyzing relevant
    metrics, logs, and alerts, this component helps identify potential issues, optimize
    the model’s performance, and ensure a stable deployment environment. How will
    you implement monitoring and logging to track your DL model’s health and performance,
    and what metrics will be crucial to measure its success?'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控与日志组件**：这些工具允许你实时跟踪深度学习模型的性能、使用情况和健康状况。通过收集和分析相关的指标、日志和警报，该组件有助于发现潜在问题、优化模型性能，并确保稳定的部署环境。你将如何实施监控和日志记录，以追踪深度学习模型的健康状况和性能，并且哪些指标对衡量其成功至关重要？'
- en: '**Continuous integration/continuous deployment** (**CI/CD**): This process
    involves the automated building, testing, and deployment of your DL model whenever
    changes are made to its code, data, or infrastructure. CI/CD streamlines the development
    life cycle, enabling faster iterations and improvements while ensuring that the
    deployed model remains up-to-date and reliable. What CI/CD practices and tools
    will you adopt to maintain a seamless deployment pipeline for your DL model?'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续集成/持续部署** (**CI/CD**)：该过程涉及每当模型的代码、数据或基础设施发生变化时，自动构建、测试和部署你的深度学习模型。CI/CD简化了开发生命周期，实现了更快速的迭代和改进，同时确保已部署的模型保持最新并且可靠。你将采用哪些CI/CD实践和工具，以维持你的深度学习模型的无缝部署流程？'
- en: With numerous options available for each of these components, it’s essential
    to have a strategy to decide which ones to use. The first logical step in this
    process is to define the specific requirements that will guide decision-making
    for each component. In the next section, we will discuss how to establish these
    requirements, ensuring that your choices align with your business goals.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个组件都有众多选项的情况下，制定决策策略至关重要。这个过程的第一步是定义具体的需求，以指导每个组件的决策。在下一节中，我们将讨论如何建立这些需求，确保你的选择与业务目标相符。
- en: Identifying key DL model deployment requirements
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定深度学习模型部署的关键要求
- en: 'To determine the most suitable deployment strategy from a variety of options,
    it is essential to identify and define seven key requirements. These are latency
    and availability, cost, scalability, model hardware, data privacy, safety, and
    trust and reliability requirements. Let’s dive into each of these requirements
    in detail:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从多种选项中确定最合适的部署策略，必须识别并定义七个关键要求。这些要求包括延迟和可用性、成本、可扩展性、模型硬件、数据隐私、安全性，以及信任和可靠性要求。让我们详细探讨每一个要求：
- en: '**Latency and availability requirements**: These are two closely connected
    components and should be defined together. Availability requirements refer to
    the desired level of uptime and accessibility of the model’s prediction. Latency
    requirements refer to the maximum acceptable delay or response time that the models
    must meet to provide timely predictions or results. A deployment with a low availability
    requirement usually can tolerate high latency predictions, and vice versa. One
    reason is that a low-latency capable infrastructure can’t ensure low latency if
    it is not available when model predictions are requested. However, there are edge
    cases that can require complete availability and low latency only for a short
    period but can be unavailable for the rest of the time, which is considered low
    availability but with low latency requirements. Here are a few best practices
    when determining latency and availability requirements:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**延迟和可用性要求**：这两者是紧密相关的组件，应一同定义。可用性要求指的是模型预测的期望运行时间和可访问性水平。延迟要求指的是模型必须满足的最大可接受延迟或响应时间，以提供及时的预测或结果。如果一个部署具有较低的可用性要求，通常可以容忍较高的延迟预测，反之亦然。原因之一是，如果低延迟基础设施在请求模型预测时不可用，那么它就无法确保低延迟。然而，也有一些特殊情况，只在短时间内需要完全可用且低延迟，而其他时间可以不可用，这被视为低可用性但具有低延迟要求。以下是确定延迟和可用性要求时的一些最佳实践：'
- en: Consider the expectations and needs of the end users or applications utilizing
    the DL model. Consult with stakeholders to understand the desired response times
    and availability levels they expect.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用 DL 模型的最终用户或应用程序的期望和需求。与相关方沟通，了解他们期望的响应时间和可用性水平。
- en: Assess the impact of latency and availability on the overall system or business
    process. Identify critical points where delays can significantly affect user experience
    or business operations. Is waiting for a minimum of 1 hour for predictions going
    to provide the value the business wanted?
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估延迟和可用性对整体系统或业务流程的影响。识别延迟可能显著影响用户体验或业务运营的关键点。等待至少 1 小时才能获得预测，是否能够提供业务所需的价值？
- en: Identify time windows or periods where availability is particularly crucial.
    Determine if the DL model needs to be available 24/7 or if there are specific
    hours or events when high availability is essential.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定可用性特别重要的时间窗口或时段。确定 DL 模型是否需要全天候 24/7 可用，或是否有特定的时段或事件需要高可用性。
- en: Set both the ideal and maximum latency and availability thresholds. The maximum
    is usually where a significant value can still be obtained, and an ideal condition
    would just slightly increase that value.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置理想和最大延迟与可用性阈值。最大值通常是指仍能获得显著价值的水平，而理想条件则是稍微提高该值的情况。
- en: '**Cost requirements**: Budget constraints are a critical consideration in any
    business, and it is essential to determine the maximum cost you are willing to
    allocate for deploying a machine learning model based on the expected value it
    will bring. To ensure that the expenses do not exceed what the organization is
    willing to invest, it is advisable to conduct a cost-benefit analysis. This analysis
    will involve evaluating the cost implications of various components within the
    deployment infrastructure, including achieving higher levels of latency and availability.
    By carefully balancing the desired requirements against the associated infrastructure
    costs and operational complexities, you can make informed decisions that align
    with the overall financial goals of your organization while still leveraging the
    benefits of machine learning.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本要求**：预算限制是任何企业的关键考虑因素，因此必须确定您愿意为部署机器学习模型分配的最大成本，以便根据模型预期带来的价值进行规划。为了确保费用不会超过组织愿意投入的金额，建议进行成本效益分析。此分析将评估部署基础设施中各个组件的成本影响，包括如何实现更高的延迟和可用性水平。通过仔细平衡所需要求与相关基础设施成本及操作复杂性，您可以做出与组织整体财务目标相一致的明智决策，同时依然能利用机器学习的优势。'
- en: '**Scalability requirements**: Scalability is the ability of a deployment infrastructure
    to handle an increase or decrease in workload demands without compromising performance
    or quality. It is essential to determine the scalability requirements of your
    deep learning model, as this will impact the choice of deployment strategy and
    infrastructure. Do you expect the model usage to grow over time? How fast do you
    expect it to grow? Do you need to scale horizontally (adding more instances of
    the model) or vertically (increasing the resources of existing instances)? Having
    an expectation regarding the utilization growth rate will allow you to choose
    appropriate components and decisions in operationalizing your model.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性要求**：可扩展性是指部署基础设施能够在不影响性能或质量的情况下，应对工作负载需求的增加或减少。确定深度学习模型的可扩展性要求至关重要，因为这将影响部署策略和基础设施的选择。您是否预期模型使用量会随着时间增长？您预期它会增长多快？您需要横向扩展（增加更多模型实例）还是纵向扩展（增加现有实例的资源）？对使用增长速率的预期将帮助您在模型落地时做出适当的组件选择和决策。'
- en: '**Model hardware requirements**: The choice of hardware for deploying a DL
    model is crucial as it can significantly impact the performance, latency, and
    cost of the overall deployment. To properly identify hardware requirements, consider
    the following:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型硬件要求**：选择用于部署深度学习模型的硬件至关重要，因为它会显著影响整体部署的性能、延迟和成本。为了正确识别硬件需求，请考虑以下几点：'
- en: '**Compatibility**: Ensure the chosen hardware is compatible with the frameworks
    and libraries used to develop the DL model. This includes checking if the hardware
    can support specific functions, such as GPU acceleration, that may be essential
    for model performance.'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兼容性**：确保所选硬件与用于开发深度学习模型的框架和库兼容。这包括检查硬件是否能支持特定功能，例如可能对模型性能至关重要的GPU加速。'
- en: '**Processing power**: Evaluate the processing power required to efficiently
    run the model, including the number of cores, memory, and storage. Consider how
    the model’s complexity and size may impact hardware requirements.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理能力**：评估高效运行模型所需的处理能力，包括核心数量、内存和存储。考虑模型的复杂性和大小可能对硬件要求的影响。'
- en: '**Power consumption and heat dissipation**: The power consumption and heat
    dissipation of the chosen hardware can affect the overall operational cost and
    the environmental footprint of the deployment. Choose hardware that balances performance
    with energy efficiency.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功耗和散热**：所选硬件的功耗和散热情况会影响整体运营成本和部署的环境足迹。选择能在性能与能源效率之间找到平衡的硬件。'
- en: '**Future-proofing**: Consider the expected lifespan of the hardware and its
    ability to accommodate future updates or improvements to the model. Opt for hardware
    that can easily be upgraded or replaced if necessary.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未来适应性**：考虑硬件的预期使用寿命及其是否能够适应模型未来的更新或改进。选择可以轻松升级或更换的硬件，以应对可能的需求。'
- en: '**Integration**: Ensure the hardware can be seamlessly integrated with the
    rest of the deployment infrastructure and any other relevant systems or components.'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：确保硬件能够无缝集成到其他部署基础设施及相关系统或组件中。'
- en: By thoroughly assessing model hardware requirements, you can make informed decisions
    that ensure optimal performance while minimizing costs and potential bottlenecks
    in your DL model deployment.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过彻底评估模型的硬件要求，您可以做出明智的决策，确保在优化性能的同时，最小化成本和潜在的瓶颈。
- en: '**Data privacy requirements**: Ensuring the privacy and security of data used
    in the DL model and predictions by it is crucial as it can impact the trust and
    compliance of the deployment. To identify and address data privacy requirements,
    consider the following:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据隐私要求**：确保深度学习模型及其预测所使用数据的隐私和安全至关重要，因为这会影响部署的信任和合规性。为识别并解决数据隐私要求，请考虑以下方面：'
- en: '**Regulatory compliance**: Understand the data protection regulations and industry
    standards applicable to your organization, such as GDPR, HIPAA, or CCPA. Ensure
    that the deployment strategy and infrastructure comply with these regulations.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性要求**：了解适用于您组织的数据保护法规和行业标准，如GDPR、HIPAA或CCPA。确保部署策略和基础设施符合这些法规。'
- en: '**Data storage and processing locations**: Assess where the data will be stored
    and processed during the deployment. Determine if any data residency requirements
    or restrictions exist, such as the need to store data in a specific geographic
    region.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储和处理地点**：评估数据在部署过程中存储和处理的位置。确定是否存在数据驻留要求或限制，如需要将数据存储在特定的地理区域。'
- en: '**Data access controls**: All DL applications should have the requirement to
    implement appropriate access controls to ensure that only authorized users or
    systems can access the data. This includes implementing authentication, authorization,
    and encryption mechanisms.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问控制**：所有深度学习应用都应要求实施适当的访问控制，以确保只有授权的用户或系统可以访问数据。这包括实现身份验证、授权和加密机制。'
- en: '**Data retention and deletion policies**: Check if there are legal and regulatory
    requirements for data retention and deletion. Ensure that the deployment infrastructure
    supports these policies and allows for secure data disposal if necessary.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据保留和删除政策**：检查是否有数据保留和删除的法律和合规要求。确保部署基础设施支持这些政策，并在必要时允许安全删除数据。'
- en: '**Data monitoring and auditing**: Check if there is a need to implement monitoring
    and auditing mechanisms to track data usage and access throughout the deployment.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据监控与审计**：检查是否需要实施监控和审计机制，以跟踪部署过程中数据的使用和访问。'
- en: '**Data breach response plan**: Such a plan should include roles and responsibilities,
    communication channels, and remediation actions. Check if there is a need to develop
    a data breach response plan that outlines the steps to be taken when there’s a
    data breach or a security incident.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据泄露响应计划**：此类计划应包括角色与职责、沟通渠道和修复措施。检查是否需要制定数据泄露响应计划，列出发生数据泄露或安全事件时的应对步骤。'
- en: '**Safety requirements**: Reflect on the potential legal and ethical boundaries
    that the model must comply with in the specific region you want to deploy your
    model.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性要求**：反思模型在特定地区部署时必须遵守的法律和伦理边界。'
- en: '**Trust and reliability requirements**: Trust and reliability for machine learning
    models refer to the confidence in a model’s consistent performance, accuracy,
    and adherence to ethical and regulatory standards during its deployment and operation.
    Consider these questions when determining requirements:'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任与可靠性要求**：机器学习模型的信任与可靠性指的是对模型在部署和运行过程中保持一致的性能、准确性，以及遵守伦理和合规标准的信心。在确定要求时，请考虑以下问题：'
- en: How frequently will the model be updated or modified?
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型多久更新或修改一次？
- en: Is tracking multiple model versions necessary?
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有必要追踪多个模型版本？
- en: Will the model face concept or data drift in its operating environment?
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在其运行环境中是否会面临概念漂移或数据漂移？
- en: How important is efficient error detection and resolution?
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效的错误检测和解决有多重要？
- en: How often will the model receive updates or new features?
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型多久会收到更新或新增功能？
- en: Is adapting to user feedback or changing requirements essential?
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应用户反馈或变化的需求是否至关重要？
- en: Are there opportunities to leverage advances in DL to improve the model?
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有机会利用深度学习的进展来改进模型？
- en: Is maintaining a stable and secure production environment a priority?
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护稳定和安全的生产环境是否是优先事项？
- en: How critical is the model’s performance to its users or business functions?
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能对其用户或业务功能有多关键？
- en: Are there strict SLAs or regulatory requirements related to performance?
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有与性能相关的严格服务水平协议（SLA）或监管要求？
- en: Is consistent performance across different environments and configurations important?
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同环境和配置中，性能一致性是否重要？
- en: Do the model’s predictions have significant consequences, making consistency
    essential for user trust and success?
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的预测是否具有重大影响，从而使得一致性对用户信任和成功至关重要？
- en: Some of these requirements are best determined early on in the planning stage.
    For instance, defining latency requirements from the outset allows you to select
    an appropriate model that ensures runtime duration falls within the specified
    latency constraints. Having explored the types of requirements that need to be
    defined and the approximate methods for defining them, we are now prepared to
    discuss choosing the right deployment options.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些要求最好在规划阶段早期确定。例如，从一开始就定义延迟要求，可以帮助您选择一个合适的模型，以确保运行时持续时间符合指定的延迟限制。在探讨了需要定义的要求类型和定义这些要求的大致方法后，我们现在可以讨论如何选择正确的部署选项。
- en: Choosing the right DL model deployment options
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的深度学习（DL）模型部署选项
- en: Selecting the right deployment options for your DL model is a crucial step in
    ensuring optimal performance, scalability, and cost-effectiveness. To assist you
    in making an informed decision, we will explore recommended options based on different
    requirements. These recommendations encompass various aspects, such as hardware
    and physical infrastructure, monitoring and logging components, and deployment
    strategies. By carefully evaluating your model’s characteristics, resource constraints,
    and desired outcomes, you should be able to identify the most suitable deployment
    solution that aligns with your objectives while maximizing efficiency and return
    on investment through this guide. The tangible deployment components we will explore
    here are architectural decisions, computing hardware, model packaging and frameworks,
    communication protocols, and user interfaces. Let’s dive into each component one
    by one, starting with architectural choices.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为深度学习（DL）模型选择正确的部署选项是确保最佳性能、可扩展性和成本效益的关键步骤。为了帮助您做出明智的决策，我们将根据不同的需求探讨推荐的选项。这些建议涵盖多个方面，例如硬件和物理基础设施、监控和日志记录组件，以及部署策略。通过仔细评估模型的特性、资源限制和预期结果，您应该能够识别最合适的部署解决方案，以最大化效率并通过本指南实现投资回报率。我们将在这里探讨的实际部署组件包括架构决策、计算硬件、模型打包与框架、通信协议和用户接口。让我们一一深入探讨每个组件，从架构选择开始。
- en: Architectural choices
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构选择
- en: 'Architectural choices for a machine learning service involve designing the
    infrastructure, data pipelines, and deployment methods for efficient and reliable
    operations. We will start with service placement considerations:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习服务的架构选择涉及设计基础设施、数据管道和部署方法，以确保高效和可靠的操作。我们将从服务部署的考虑因素开始：
- en: '**Microservice**: **Deploy the Deep Learning** (**DL**) model as a small, loosely
    coupled, and independently deployable service with its own APIs. A microservice
    is a software architecture design pattern where an application is structured as
    a collection of small, loosely coupled, and independently deployable services.
    Each microservice is responsible for a specific functionality or domain within
    the application and communicates with other microservices through well-defined
    **Application Programming Interfaces** (**APIs**). So, when deploying as a microservice,
    a prerequisite is that other components are also implemented as a microservice.
    Its advantages are as follows:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微服务**：**将深度学习（DL）模型**部署为一个小型、松散耦合、可独立部署的服务，并拥有自己的API。微服务是一种软件架构设计模式，将应用程序构建为一组小型、松散耦合且可独立部署的服务。每个微服务负责应用程序中的特定功能或领域，并通过明确定义的**应用程序接口（API）**与其他微服务进行通信。因此，当以微服务形式部署时，前提是其他组件也必须以微服务的方式实现。其优势如下：'
- en: Better scalability
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的可扩展性
- en: Easier updates and maintenance
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更便捷的更新和维护
- en: Higher resilience
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更高的弹性
- en: Flexibility in technology choices
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术选择的灵活性
- en: 'Choose this microservice in the following circumstances:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择该微服务：
- en: When model usage is expected to grow
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预计模型使用量会增长时
- en: When frequent updates are needed
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要频繁更新时
- en: When integration with various external systems is required
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当需要与各种外部系统集成时
- en: When high resilience is crucial
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当高可用性至关重要时
- en: '**Standalone service:** Deploy the DL model as a separate, independent service,
    that is not a microservice. Consider a movie recommendation application - a microservice
    approach would be to create a **Review Analysis Service** microservice that processes
    movie reviews using a DL model. It has its own API, data storage, and deployment
    pipeline, operating independently from other services in the application. For
    a separate service approach in the same application, a **Movie Recommendation
    Service** combines user preference management, movie review analysis (using the
    DL model), and recommendation generation. It''s more monolithic, combining related
    functionalities, with its own API but no separate microservice for review analysis.
    Its advantages are as follows:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立服务**：将深度学习模型作为一个独立的服务部署，而不是微服务。以电影推荐应用为例——微服务方法是创建一个**评论分析服务**微服务，使用深度学习模型处理电影评论。它有自己的
    API、数据存储和部署管道，并且独立于应用中的其他服务运行。在同一个应用中采用独立服务方法，**电影推荐服务**结合了用户偏好管理、电影评论分析（使用深度学习模型）和推荐生成。它更具单体结构，整合了相关功能，拥有自己的
    API，但没有为评论分析单独设置微服务。其优点如下：'
- en: Easier management and administration
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易的管理和维护
- en: Better suited for complex applications
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更适合复杂的应用
- en: Consolidated resources and data access
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中的资源和数据访问
- en: Simplified communication between components
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件之间的简化通信
- en: More predictable performance
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更可预测的性能
- en: 'Choose this standalone service in the following circumstances:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择此独立服务：
- en: When the application has a limited number of services
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当应用中服务数量有限时
- en: When the model is complex and requires a more monolithic approach
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型复杂并且需要更单体化的方法时
- en: When the scope of the model does not change frequently
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型的范围变化不频繁时
- en: When a balance between resilience and complexity is preferred
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当优先考虑弹性与复杂性之间的平衡时
- en: '**Part of the existing service**: Integrate the DL model into an existing service
    of an application or system. The advantages are as follows:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有服务的一部分**：将深度学习模型集成到应用或系统的现有服务中。其优点如下：'
- en: Less complexity
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的复杂性
- en: Improved performance
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进的性能
- en: Easier data synchronization
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易的数据同步
- en: Potential cost savings
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在的成本节约
- en: 'Choose to integrate with an existing service in the following circumstances:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下情况下选择与现有服务集成：
- en: When model usage growth is limited
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型使用量增长有限时
- en: When infrequent updates or modifications are needed
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当不需要频繁更新或修改时
- en: When you have limited integration with external systems
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与外部系统的集成有限时
- en: When high resilience is not crucial
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当高弹性不是关键要求时
- en: 'Decide between microservice or integrating with an existing service by considering
    scalability, update frequency, integration requirements, and resilience. Align
    these factors with your specific requirements to make the best decision for your
    DL model deployment. Next, we will go through recommendations for choosing the
    physical deployment environment:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在微服务和与现有服务集成之间做出决定时，应考虑可扩展性、更新频率、集成需求和弹性等因素。根据这些因素与您的具体需求对齐，以便做出最佳的深度学习模型部署决策。接下来，我们将讨论选择物理部署环境的建议：
- en: '**Cloud**: Cloud deployments are suitable when you require high availability
    and can tolerate moderate latency. They minimize upfront costs and offer flexible
    pay-as-you-go pricing models. Cloud-based infrastructure provides virtually unlimited
    resources, allows for rapid auto-scaling, and typically offers high uptime guarantees
    and managed services. However, you need to carefully evaluate the cloud providers’
    security offerings and ensure compatibility with your DL framework and libraries.
    A few companies that offer GPU are AWS, GCP, Microsoft Azure, and IBM Cloud.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云**：当您需要高可用性并且能容忍适度的延迟时，云部署是合适的。它们减少了前期成本，并提供灵活的按需付费定价模型。基于云的基础设施提供几乎无限的资源，允许快速自动扩展，并且通常提供高可用性的保证和托管服务。然而，您需要仔细评估云服务提供商的安全性，并确保其与您的深度学习框架和库兼容。提供
    GPU 服务的公司包括 AWS、GCP、Microsoft Azure 和 IBM Cloud。'
- en: '**Server on-premises**: Server on-premises deployments give you more control
    over your hardware and network resources, making them ideal for low latency and
    high availability within a specific geographical region. They require an upfront
    investment in terms of hardware and maintenance but can provide long-term cost
    savings, especially if you have high and consistent resource demands. On-premises
    deployments also offer more control over security measures and data privacy but
    require more effort in maintaining and updating security measures. Ensure compatibility
    with your DL framework and libraries.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地服务器**：本地服务器部署让你对硬件和网络资源拥有更多的控制权，非常适合在特定地理区域内实现低延迟和高可用性。这类部署需要在硬件和维护方面的前期投资，但如果你有高且稳定的资源需求，长期来看可以节省成本。与云端部署相比，本地部署在安全措施和数据隐私方面提供更多的控制权，但也需要更多的精力来维护和更新安全措施。确保与深度学习框架及库的兼容性。'
- en: '**Edge on-premises**: Also known as edge computing, this approach processes
    data close to the source, offering extremely low latency and improved security
    and data privacy. Edge deployments are suitable when data processing and storage
    need to happen close to the source, and they can reduce data transfer costs. However,
    managing security across multiple edge devices and ensuring compatibility with
    your DL framework and libraries can be challenging. Edge deployments offer scalability
    in terms of distributing processing across multiple edge devices but may require
    more management and maintenance efforts.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边缘本地部署**：也叫做边缘计算，这种方法将数据处理靠近数据源，提供极低的延迟、提升的安全性和数据隐私保护。边缘部署适用于需要将数据处理和存储靠近数据源的场景，并且能够减少数据传输成本。然而，跨多个边缘设备管理安全性，并确保与深度学习框架及库的兼容性，可能是一个挑战。边缘部署在分布式处理方面具有可扩展性，但可能需要更多的管理和维护工作。'
- en: 'Next, we will dive into container orchestration platforms, which have a significant
    impact on how applications and services are designed, deployed, and managed within
    a system. A container is a lightweight, standalone, and executable software package
    that includes everything needed to run a piece of software, including the code,
    runtime, system tools, libraries, and settings. Containers are isolated from each
    other and from the host system, allowing them to run consistently across different
    computing environments. There are two main types of container technologies: Docker
    containers and Linux containers (LXC).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨容器编排平台，它们对应用程序和服务在系统中的设计、部署和管理方式产生了深远影响。容器是一个轻量级、独立的可执行软件包，包含了运行软件所需的所有内容，包括代码、运行时、系统工具、库和设置。容器彼此隔离，也与主机系统隔离，使它们能够在不同的计算环境中一致地运行。容器技术主要有两种类型：Docker
    容器和 Linux 容器（LXC）。
- en: 'Container orchestration platforms help manage and scale deployments of deep
    learning models in containerized applications, utilizing technologies such as
    Docker containers or LXC. These platforms provide flexible deployment across multiple
    machines, cloud providers, or on-premises infrastructure. They can be used in
    conjunction with other tools and services, enabling efficient management of containerized
    applications and microservices. Some popular container orchestration platforms
    to choose among are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 容器编排平台帮助管理和扩展容器化应用程序中深度学习模型的部署，利用 Docker 容器或 LXC 等技术。这些平台提供跨多台机器、云服务商或本地基础设施的灵活部署。它们可以与其他工具和服务结合使用，从而实现对容器化应用和微服务的高效管理。以下是一些流行的容器编排平台：
- en: '**Kubernetes (open source):** Kubernetes is an open source container orchestration
    platform that automates the deployment, scaling, and management of containerized
    applications, including deep learning models. It works with various container
    technologies, including Docker and LXC.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes（开源）**：Kubernetes 是一个开源的容器编排平台，能够自动化部署、扩展和管理容器化应用程序，包括深度学习模型。它与多种容器技术兼容，包括
    Docker 和 LXC。'
- en: '**Docker Swarm (open source):** Docker Swarm is a native clustering and scheduling
    tool for Docker containers. It is tightly integrated with the Docker ecosystem,
    providing a simple way to deploy and manage containerized applications. While
    not as feature-rich as Kubernetes, Docker Swarm is known for its ease of use and
    faster setup.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Swarm（开源）**：Docker Swarm 是 Docker 容器的原生集群和调度工具。它与 Docker 生态系统紧密集成，提供了一种简单的方式来部署和管理容器化应用程序。虽然功能上不如
    Kubernetes 丰富，Docker Swarm 以其易用性和更快速的设置而闻名。'
- en: '**Apache Mesos (open source):** Apache Mesos is a distributed systems kernel
    that abstracts CPU, memory, and storage resources away from machines, enabling
    fault-tolerant and elastic distributed systems. It can be used in conjunction
    with other frameworks such as Marathon or DC/OS to provide container orchestration
    capabilities for deploying and managing deep learning models.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos（开源）**：Apache Mesos是一个分布式系统内核，它将CPU、内存和存储资源从机器中抽象出来，使得系统具有容错性和弹性。它可以与其他框架（如Marathon或DC/OS）结合使用，提供容器编排功能，用于部署和管理深度学习模型。'
- en: '**Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Service
    (ECS) (paid-for services):** These are managed container orchestration services
    provided by AWS. EKS is a managed Kubernetes service, while ECS is a proprietary
    container orchestration platform from AWS. Both services simplify the deployment,
    scaling, and management of containerized applications on AWS infrastructure.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon Elastic Kubernetes Service (EKS) 和 Amazon Elastic Container Service
    (ECS)（收费服务）**：这些是AWS提供的托管容器编排服务。EKS是一个托管的Kubernetes服务，而ECS是AWS自有的容器编排平台。这两项服务简化了在AWS基础设施上部署、扩展和管理容器化应用程序的过程。'
- en: Choose a container orchestration platform that best suits your deep learning
    deployment requirements, such as flexibility, scalability, compatibility with
    your preferred container technology, cloud provider, and integration with other
    tools and services.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最适合你深度学习部署需求的容器编排平台，例如灵活性、可扩展性、与你偏好的容器技术的兼容性、云提供商以及与其他工具和服务的集成。
- en: 'Next, we will dive into architectural trade-offs between real-time and batch
    predictions:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨实时预测和批量预测之间的架构权衡：
- en: '**Real-time predictions**: It’s recommended to have the model always loaded
    in memory to reduce latency and respond quickly to requests. This setup is suitable
    for applications where immediate response is critical, such as autonomous vehicles,
    live chatbots, or fraud detection systems. Here are some recommendations when
    using this option:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时预测**：建议将模型始终加载到内存中，以减少延迟并快速响应请求。此设置适用于即时响应至关重要的应用场景，如自动驾驶汽车、实时聊天机器人或欺诈检测系统。使用此选项时，以下是一些建议：'
- en: Use a dedicated server or cloud instance with enough memory and processing power
    to handle the model and concurrent requests
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一台专用服务器或云实例，确保有足够的内存和处理能力来处理模型和并发请求
- en: Optimize the model for inference by using techniques such as quantization, pruning,
    or model distillation
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用量化、剪枝或模型蒸馏等技术优化模型推理
- en: Implement a load balancer if necessary to distribute incoming requests across
    multiple instances of the model
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，实施负载均衡器以将传入请求分配到多个模型实例
- en: Monitor resource usage and performance to ensure the system meets real-time
    requirements and scales as needed
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控资源使用情况和性能，确保系统满足实时要求，并根据需要进行扩展
- en: Have a queue system to ensure workers are not overloaded or implement autoscaling
    to handle overload cases
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用队列系统确保工作人员不会过载，或者实施自动扩展来处理超载情况
- en: '**On-demand batch predictions**: Batch predictions are suitable for scenarios
    where real-time responses are not crucial, and predictions can be processed in
    groups. This setup requires extra time to spin up worker infrastructure, initialize
    the model, and load trained model weights. Here are some recommendations when
    using this option:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**按需批量预测**：批量预测适用于实时响应不重要的场景，且预测可以按批次处理。此设置需要额外的时间来启动工作基础设施、初始化模型并加载训练好的模型权重。使用此选项时，以下是一些建议：'
- en: Use a queue system such as RabbitMQ or Amazon SQS to manage incoming prediction
    requests
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用队列系统，如RabbitMQ或Amazon SQS，来管理传入的预测请求
- en: Set up a batch processing system that initializes the model and loads weights
    when processing starts
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置批处理系统，当处理开始时初始化模型并加载权重
- en: Optimize the batch size to balance processing time and resource usage
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化批处理大小，以平衡处理时间和资源使用
- en: Implement auto-scaling to handle variable workloads and ensure efficient use
    of resources
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施自动扩展来处理变化的工作负载并确保资源的高效使用
- en: Next, we will explore computer hardware choices and recommendations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨计算硬件选择和建议。
- en: Computing hardware choices
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算硬件选择
- en: 'Selecting hardware to carry out model computations is all about trading off
    cost, availability, and runtime. Let’s explore the different options, along with
    recommendations on when to opt for each option:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 选择硬件以执行模型计算是关于成本、可用性和运行时之间的权衡。我们来探讨不同的选项，并附上使用每种选项的建议：
- en: '**CPU**: CPUs are a versatile and cost-effective option for deploying DL models.
    They are compatible with most frameworks and libraries and provide decent performance
    for less complex models. CPUs are a good choice when cost constraints are a priority,
    and you don’t require the high processing power that GPUs or TPUs offer.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CPU**：CPU 是部署深度学习模型的多功能且具有成本效益的选项。它们与大多数框架和库兼容，并为不太复杂的模型提供不错的性能。当成本是首要考虑因素，并且你不需要
    GPU 或 TPU 提供的高处理能力时，CPU 是一个不错的选择。'
- en: '**GPU**: GPUs provide faster processing and better parallelization, significantly
    reducing latency and improving performance. They are ideal for complex models
    that demand high processing power. GPUs are an excellent choice when you require
    low latency and high availability, but they come with higher costs compared to
    CPUs.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPU**：GPU 提供更快的处理速度和更好的并行化，显著降低延迟并提高性能。它们非常适合需要高处理能力的复杂模型。当你需要低延迟和高可用性时，GPU
    是一个极好的选择，但它们的成本高于 CPU。'
- en: '**TPU**: TPUs are specialized hardware designed for machine learning tasks,
    offering high performance and efficient processing. They are particularly suitable
    for large models or computationally intensive tasks. TPUs are a great option when
    you need exceptional processing power and low latency but be aware of the potential
    higher costs and that it is only available in GCP and usable only in TensorFlow!'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TPU**：TPU 是为机器学习任务设计的专用硬件，提供高性能和高效处理。它们特别适用于大型模型或计算密集型任务。当你需要卓越的处理能力和低延迟时，TPU
    是一个不错的选择，但要注意可能会有较高的成本，而且它仅在 GCP 中可用，并且只能在 TensorFlow 中使用！'
- en: '**Artificial Neural Engines** (**ANEs**): ANEs are specialized AI accelerators
    found in devices such as iPhones. They provide efficient processing for DL tasks
    on edge devices, offering low latency and energy-efficient performance. ANEs are
    a good choice when your application requires user interface requirements on an
    iPhone, which is an edge device. Note that it is only compatible with the CoreML
    framework and that the ONNX weights format is needed to convert weights easily
    to CoreML.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工神经引擎** (**ANE**)：ANE 是专门的 AI 加速器，通常出现在如 iPhone 等设备中。它们为边缘设备上的深度学习任务提供高效的处理，具有低延迟和能效高的性能。当你的应用需要在
    iPhone 这样的边缘设备上满足用户界面需求时，ANE 是一个不错的选择。需要注意的是，它仅与 CoreML 框架兼容，且需要使用 ONNX 权重格式来轻松转换权重到
    CoreML。'
- en: '**FPGA**: FPGAs are highly customizable (its hardware circuitry can be programmed!)
    and energy-efficient hardware and are suitable for deploying DL models that require
    low latency and adaptability. The con here is the need to have deep expertise
    in the FPGA programming language and circuit development to successfully allow
    inference with a trained neural network efficiently. This is an out-of-bounds
    device for most teams.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FPGA**：FPGA 是高度可定制的（其硬件电路可以编程！），同时具有能效高的特性，适用于部署需要低延迟和适应性的深度学习模型。这里的缺点是需要具备深厚的
    FPGA 编程语言和电路开发专业知识，才能成功地高效地进行已训练神经网络的推理。这对于大多数团队来说是一个超出范围的设备。'
- en: Next, we will explore model packaging and framework choices and recommendations.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨模型打包和框架选择以及建议。
- en: Model packaging and frameworks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型打包和框架
- en: 'This controls how DL models are executed and where recommendations can depend
    on the compute hardware used, as well as portability and runtime requirements.
    Here are some popular examples, along with recommendations on when to use them:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这决定了深度学习模型的执行方式，且推荐的选择可能取决于所使用的计算硬件，以及可移植性和运行时需求。以下是一些常见示例，并附有使用建议：
- en: '**Original framework packaging**: You can take advantage of specific optimizations
    and features provided by the framework, potentially improving performance. However,
    certain cases may require compatibility with specific hardware options, such as
    using a TPU, which is only supported by the TensorFlow framework, so if you have
    a TPU and you stick with PyTorch, you will not be able to use the TPU.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原始框架包装**：你可以利用框架提供的特定优化和功能，从而提高性能。然而，在某些情况下，可能需要与特定硬件选项兼容，例如使用 TPU，而 TPU
    仅支持 TensorFlow 框架。因此，如果你有 TPU 而选择使用 PyTorch，就无法使用 TPU。'
- en: '**Open Neural Network Exchange** (**ONNX**) **framework**: ONNX provides an
    open standard for representing DL models, allowing you to convert your model to
    different frameworks and run it on various hardware platforms. Using ONNX can
    increase the flexibility and portability of your model, enabling you to choose
    from a wider range of hardware and infrastructure options. Moreover, it allows
    you to leverage optimizations and features provided by different DL frameworks.
    A convenient and general solution to address packaging issues is to convert your
    model into the ONNX format, which can then be easily converted into other formats
    as needed. This approach streamlines the process and ensures smooth integration
    with various hardware and framework options, such as leveraging ANE in an iPhone
    to accelerate your deep learning model within an app.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放神经网络交换** (**ONNX**) **框架**：ONNX 提供了一种开放标准，用于表示深度学习模型，允许你将模型转换为不同的框架，并在各种硬件平台上运行。使用
    ONNX 可以提高模型的灵活性和可移植性，使你能够从更广泛的硬件和基础设施选项中进行选择。此外，它还可以让你利用不同深度学习框架提供的优化和功能。一种方便且通用的解决方案是将模型转换为
    ONNX 格式，之后可以根据需要轻松转换为其他格式。这种方法简化了过程，并确保与各种硬件和框架选项的顺利集成，例如在 iPhone 中利用 ANE 加速应用中的深度学习模型。'
- en: '**ONNX Runtime**: This is an inference accelerator that’s designed to accelerate
    DL model inference in any hardware by leveraging compute and memory optimizations.
    It is faster to run a model in ONNX Runtime than to run it in their native DL
    framework, such as TensorFlow or PyTorch.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ONNX Runtime**：这是一种推理加速器，旨在通过利用计算和内存优化来加速在任何硬件上的深度学习模型推理。在 ONNX Runtime 中运行模型比在本地深度学习框架（如
    TensorFlow 或 PyTorch）中运行更快。'
- en: '**TensorRT**: This is a high-performance DL inference optimizer and runtime/compiler
    library from NVIDIA that’s designed to accelerate DL model inference on NVIDIA
    GPUs. It supports TensorFlow and ONNX and provides easy ways to convert model
    weights so that they’re compatible with its framework, offering fast and efficient
    model deployment. TensorRT allows faster model inference speed in GPUs by collectively
    tuning the model at a lower level, leveraging different GPU internal hardware
    capabilities to maximize the model efficiency during inference. As ONNX weights
    are compatible with TensorRT, a typical path to convert PyTorch model weights
    into a TensorRT-compatible weight format is to convert PyTorch model weights into
    ONNX weights. On an NVIDIA GPU, TensorRT is known to be faster than ONNX Runtime.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorRT**：这是 NVIDIA 提供的高性能深度学习推理优化器和运行时/编译器库，旨在加速 NVIDIA GPU 上的深度学习模型推理。它支持
    TensorFlow 和 ONNX，并提供便捷的方式将模型权重转换为与其框架兼容的格式，从而实现快速高效的模型部署。TensorRT 通过在较低层次调优模型，利用不同
    GPU 内部硬件能力，提高 GPU 上的模型推理速度，最大化推理时的模型效率。由于 ONNX 权重与 TensorRT 兼容，将 PyTorch 模型权重转换为
    TensorRT 兼容的权重格式的典型路径是先将 PyTorch 模型权重转换为 ONNX 权重。在 NVIDIA GPU 上，TensorRT 的推理速度被认为比
    ONNX Runtime 更快。'
- en: '**Open Visual Inference & Neural Network Optimization** (**OpenVINO**): This
    is a toolkit from Intel that accelerates DL model inference across Intel hardware,
    including CPUs, GPUs, and FPGAs. It supports TensorFlow, ONNX, and other frameworks,
    offering optimized model deployment in diverse environments.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开放视觉推理与神经网络优化** (**OpenVINO**)：这是英特尔提供的一款工具包，旨在加速深度学习模型在英特尔硬件（包括 CPU、GPU
    和 FPGA）上的推理。它支持 TensorFlow、ONNX 以及其他框架，提供在多种环境中的优化模型部署。'
- en: Next, we will explore communication protocol choices and recommendations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨通信协议的选择和建议。
- en: Communication protocols to use
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的通信协议
- en: 'The protocol you should use can depend on the runtime requirements, network
    load requirements, user interface chosen, mode of deployment, and compute requirements.
    Here are some examples, along with their recommendations:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用的协议取决于运行时需求、网络负载需求、所选的用户界面、部署模式以及计算需求。以下是一些示例及其推荐：
- en: '**MQTT**: Use MQTT when you need a lightweight, low-latency protocol for devices
    with limited resources, such as IoT devices, and real-time communication and status
    updates are essential for your application. Power consumption and heat dissipation
    are important factors.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MQTT**：当你需要一个轻量级、低延迟的协议，适用于资源有限的设备（如物联网设备），并且实时通信和状态更新对你的应用至关重要时，可以使用 MQTT。功耗和散热是重要的考虑因素。'
- en: '**HTTP or REST API**: Choose this when you require a well-supported and easy-to-implement
    protocol for web services and data exchange, your application follows a request-response
    communication pattern, and finally where compliance with data protection regulations
    and data privacy is crucial.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP 或 REST API**：当你需要一个受支持良好、易于实现的协议来处理网页服务和数据交换，且你的应用遵循请求-响应通信模式，并且需要符合数据保护法规和数据隐私要求时，选择此协议。'
- en: '**gRPC**: Opt for gRPC when you need a high-performance, low-latency protocol
    for large-scale distributed systems or microservices, bidirectional streaming,
    and support for multiple programming languages are essential.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**gRPC**：当你需要高性能、低延迟协议来处理大规模分布式系统或微服务时，选择 gRPC，同时它还支持双向流和多种编程语言的支持。'
- en: '**Server-Sent Events** (**SSE**) **or WebSockets**: Use them when real-time
    notifications or live updates are critical for your web application. If you require
    unidirectional communication between server and client, use SSE. If you require
    bidirectional communication between server and client, use WebSockets. A notable
    domain that requires these communication protocols is live collaborative tools
    with machine learning. Here are some examples:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器推送事件**（**SSE**）**或 WebSockets**：当实时通知或实时更新对你的网页应用至关重要时，请使用它们。如果你需要服务器与客户端之间的单向通信，请使用
    SSE。如果你需要服务器与客户端之间的双向通信，请使用 WebSockets。一个需要这些通信协议的显著领域是具有机器学习功能的实时协作工具。以下是一些示例：'
- en: Grammarly uses Websockets
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grammarly 使用 WebSockets
- en: ChatGPT uses SSE
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT 使用 SSE
- en: Next, we will explore user interface choices and recommendations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨用户界面选择和推荐。
- en: User interfaces
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户界面
- en: 'When designing user interfaces for machine learning applications, it is essential
    to consider factors such as user experience, accessibility, responsiveness, and
    adaptability. Here are some recommendations for user interfaces:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计机器学习应用的用户界面时，必须考虑用户体验、可访问性、响应性和适应性等因素。以下是一些用户界面的建议：
- en: '**Web applications**:'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网页应用程序**：'
- en: They are suitable for cross-platform access as users can access the application
    through a web browser
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们适用于跨平台访问，因为用户可以通过网页浏览器访问应用程序
- en: Use popular web development frameworks such as React, Angular, or Vue.js to
    build responsive and interactive user interfaces
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流行的网页开发框架，如 React、Angular 或 Vue.js 来构建响应式和互动性强的用户界面
- en: '**Example use case**: A sentiment analysis tool that allows users to input
    text and receive sentiment scores by interacting with a machine learning model
    through a web-based interface'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个情感分析工具，允许用户输入文本并通过与机器学习模型的互动获得情感评分，界面基于网页'
- en: For web applications, you need to also choose a web framework wisely according
    to the benefits it provides, along with the latency trade-offs. Refer to https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db
    for an estimate of the latency you will get for different web frameworks for a
    single web API query.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于网页应用程序，你还需要根据提供的好处以及延迟权衡来明智地选择网页框架。请参考 https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&hw=ph&test=db
    获取不同网页框架在单个网页 API 查询中的延迟估算。
- en: '**Mobile applications**:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动应用程序**：'
- en: Ideal for on-the-go access to machine learning features through smartphones
    and tablets
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常适合通过智能手机和平板电脑随时访问机器学习功能
- en: Develop native apps for iOS and Android platforms using Swift or Kotlin, or
    use cross-platform frameworks such as React Native or Flutter
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Swift 或 Kotlin 开发 iOS 和 Android 平台的原生应用，或使用跨平台框架如 React Native 或 Flutter
- en: '**Example use case**: A mobile app that uses a machine learning model for image
    recognition to identify plants or animals by analyzing user-captured photos'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个移动应用，利用机器学习模型进行图像识别，通过分析用户拍摄的照片来识别植物或动物'
- en: '**Desktop applications**:'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桌面应用程序**：'
- en: Suitable for users who require a dedicated, platform-specific application with
    offline functionality
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于需要专用、平台特定应用并具备离线功能的用户
- en: Use technologies such as Electron or Qt for cross-platform desktop applications
    or platform-specific languages such as C# for Windows or Swift for macOS
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Electron 或 Qt 等技术来构建跨平台桌面应用程序，或使用平台特定的语言，如 Windows 的 C# 或 macOS 的 Swift
- en: '**Example use case**: A video editing software with built-in machine learning-powered
    features such as object tracking, automatic color grading, or scene detection'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个视频编辑软件，内置机器学习驱动的功能，如物体跟踪、自动色彩校正或场景检测'
- en: '**Voice User** **Interfaces** (**VUI**):'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音用户界面**（**VUI**）：'
- en: Ideal for hands-free interaction with machine learning-powered services through
    voice commands
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于通过语音命令与机器学习驱动的服务进行免提交互
- en: Integrate with popular voice assistant platforms such as Amazon Alexa, Google
    Assistant, or Apple Siri
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成流行的语音助手平台，如亚马逊 Alexa、谷歌助手或苹果 Siri
- en: '**Example use case**: A voice-activated home automation system that uses natural
    language processing to control smart devices based on user commands'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个语音激活的家庭自动化系统，使用自然语言处理根据用户命令控制智能设备'
- en: '**Conversational** **UI (chatbots)**:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话式** **UI（聊天机器人）**：'
- en: Suitable for engaging users more naturally and interactively through text or
    voice conversations
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于通过文本或语音对话更自然、互动地吸引用户
- en: Use chatbot development platforms such as Dialogflow, Rasa, or Microsoft Bot
    Framework
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用聊天机器人开发平台，如 Dialogflow、Rasa 或 Microsoft Bot Framework
- en: '**Example use case**: A customer support chatbot that uses machine learning-powered
    natural language understanding to answer user queries and provide assistance'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个客户支持聊天机器人，使用机器学习驱动的自然语言理解来回答用户查询并提供帮助'
- en: '**Augmented reality** (**AR**) **and virtual** **reality** (**VR**):'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强现实** (**AR**) **和虚拟** **现实** (**VR**)：'
- en: Ideal for immersive and interactive experiences that combine the real and digital
    worlds
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于将现实世界和数字世界结合的沉浸式和互动式体验
- en: Use AR/VR development platforms such as Unity or Unreal Engine and integrate
    machine learning models for object recognition, motion tracking, or scene understanding
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AR/VR 开发平台，如 Unity 或 Unreal Engine，并集成机器学习模型进行物体识别、运动跟踪或场景理解
- en: '**Example use case**: A virtual training simulator that uses deep learning
    models to analyze and assess user performance in real time. In this AR/VR application,
    users can practice various skills, such as medical procedures, mechanical repairs,
    or emergency response scenarios. The deep learning model evaluates the user’s
    actions through visual input, provides instant feedback, and offers personalized
    guidance for improvement, enhancing the learning experience and accelerating skill
    development.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个虚拟训练模拟器，使用深度学习模型实时分析和评估用户表现。在这个 AR/VR 应用中，用户可以练习各种技能，如医疗程序、机械修理或紧急响应场景。深度学习模型通过视觉输入评估用户的操作，提供即时反馈，并为改进提供个性化指导，增强学习体验并加速技能发展。'
- en: '**API-based user interface**: An API-based user interface provides a flexible
    and scalable way to integrate your machine learning model with various applications,
    platforms, and services. This approach allows developers to build custom user
    interfaces or incorporate machine learning-powered features into existing applications,
    expanding the reach and impact of your model. This is suitable for enabling other
    applications, systems, or services to access and interact with your machine learning
    model programmatically. Two recommendations for this approach are as follows:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于 API 的用户界面**：基于 API 的用户界面提供了一种灵活且可扩展的方式，将你的机器学习模型与各种应用、平台和服务进行集成。这种方法允许开发者构建自定义用户界面或将机器学习驱动的功能整合到现有应用中，从而扩展模型的覆盖面和影响力。这适用于使其他应用、系统或服务能够以编程方式访问和与机器学习模型进行交互。以下是该方法的两个建议：'
- en: Use REST, gRPC, SSE, Websockets, or MQTT to create well-structured and documented
    APIs that expose the machine learning model’s functionality to external clients
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 REST、gRPC、SSE、Websockets 或 MQTT 创建结构良好且有文档的 API，将机器学习模型的功能暴露给外部客户端
- en: Implement authentication and authorization mechanisms (for example, API keys
    and OAuth) to ensure secure access to the API
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现身份验证和授权机制（例如，API 密钥和 OAuth）以确保对 API 的安全访问
- en: '**Example use case**: A sentiment analysis API that allows developers to integrate
    machine learning-powered sentiment analysis into their applications by sending
    text data and receiving sentiment scores through API calls'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例用例**：一个情感分析 API，允许开发者通过 API 调用发送文本数据并接收情感评分，将机器学习驱动的情感分析集成到他们的应用中'
- en: Choosing the right deployment options for your DL model involves carefully evaluating
    architectural choices, hardware options, communication protocols, and user interfaces
    that best align with your specific requirements and objectives. By considering
    factors such as scalability, update frequency, integration needs, and resilience,
    you can select the most suitable deployment solution that maximizes efficiency
    and ROI.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合的部署选项涉及仔细评估架构选择、硬件选项、通信协议和用户界面，确保它们最能与特定需求和目标对接。通过考虑可扩展性、更新频率、集成需求和韧性等因素，你可以选择最合适的部署解决方案，从而最大化效率和投资回报率。
- en: Next, let’s discuss some practical examples for deciding on components when
    deploying DL models in production.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论在部署深度学习模型时决策组件的一些实际示例。
- en: Exploring deployment decisions based on practical use cases
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于实际用例探讨部署决策
- en: 'In this section, we will explore practical deployment decisions for DL models
    in production, focusing on two distinct use cases: a sentiment analysis application
    for an e-commerce company and a face detection and recognition system for security
    cameras. By examining these real-world scenarios, we will gain valuable insights
    into establishing robust deployment strategies tailored to specific needs and
    objectives.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨深度学习（DL）模型在生产环境中的实际部署决策，重点关注两个不同的使用案例：一个用于电子商务公司的情感分析应用和一个用于安全摄像头的面部检测与识别系统。通过分析这些真实世界的场景，我们将获得建立稳健部署策略的宝贵见解，以满足特定需求和目标。
- en: Exploring deployment decisions for a sentiment analysis application
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨情感分析应用的部署决策
- en: 'Suppose you are developing a sentiment analysis application to be used by an
    e-commerce company to analyze customer reviews in real-time. The system needs
    to process a large number of reviews every day, and low latency is essential to
    provide immediate insights for the company. In this case, your choices could be
    as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在开发一个情感分析应用，供电子商务公司用于实时分析客户评论。该系统需要每天处理大量评论，并且低延迟至关重要，以便为公司提供即时的洞察。在这种情况下，你的选择可能如下：
- en: '**Architectural choice**: As an independent service, as it would allow better
    scalability and easier updates to handle the growing number of requests.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构选择**：作为独立服务部署，因为它可以提供更好的可扩展性，并且在处理不断增长的请求时更易于更新。'
- en: '**Hardware/infrastructure choice**: GPU on a cloud service, as it provides
    better parallelization and processing power for a large number of simultaneous
    requests.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件/基础设施选择**：云服务上的GPU，因为它提供更好的并行处理能力和大规模并发请求的处理能力。'
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型包装与框架**：ONNX和TensorRT，因为它们提供高效的模型部署和推理加速。'
- en: '**Safety, trust, and reliability**: Implement monitoring for data drift and
    model performance, regularly retrain the model on updated data, and ensure compliance
    with data privacy regulations. For example, anonymize user information and avoid
    storing **personally identifiable information** (**PII**) in the analysis as it
    can infringe upon data protection regulations, such as the GDPR in the European
    Union or the CCPA in the United States, depending on the country the application
    is intended to be deployed in.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性、信任与可靠性**：实施数据漂移和模型性能的监控，定期用更新后的数据重新训练模型，并确保符合数据隐私法规。例如，对用户信息进行匿名化处理，并避免在分析中存储**个人身份信息**（**PII**），因为这可能会违反数据保护法规，如欧盟的GDPR或美国的CCPA，具体取决于应用部署的国家。'
- en: '**Communication protocol**: RESTful APIs or gRPC, as they are well-suited for
    web services and can handle a large number of requests with low latency.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信协议**：RESTful API或gRPC，因为它们非常适合用于Web服务，并能以低延迟处理大量请求。'
- en: '**User interface**: A web-based dashboard where the company’s staff can monitor
    the sentiment analysis results in real time.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：基于网页的仪表板，公司的员工可以在其中实时监控情感分析结果。'
- en: Exploring deployment decisions for a face detection and recognition system for
    security cameras
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探讨安全摄像头的面部检测与识别系统的部署决策
- en: 'Suppose you are building an object detection system for security cameras that
    need to detect intruders in real time. In this case, your choices could be as
    follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在为需要实时检测入侵者的安全摄像头构建物体检测系统。在这种情况下，你的选择可能如下：
- en: '**Architectural choice**: Edge on-premises, as it provides low latency and
    improved security by processing data close to the source. This choice also reduces
    the time needed for data to travel through the network, as no video streaming
    to some cloud server is needed.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构选择**：本地边缘部署，因为它通过在数据源附近处理数据来提供低延迟和增强的安全性。此选择还减少了数据在网络中传输所需的时间，因为不需要将视频流传输到某个云服务器。'
- en: '**Hardware/infrastructure choice**: GPU or TPU on the edge device, depending
    on the compatibility with the DL framework and the model’s complexity.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件/基础设施选择**：根据与深度学习框架的兼容性和模型的复杂性，在边缘设备上选择 GPU 或 TPU。'
- en: '**Model packaging and framework**: ONNX and TensorRT, as they offer efficient
    model deployment and inference acceleration.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型包装和框架**：ONNX 和 TensorRT，因为它们提供高效的模型部署和推理加速。'
- en: '**Safety, trust, and reliability**: Implement monitoring for model performance
    and ensure compliance with local regulations related to video surveillance such
    as data privacy, retention policies, and consent requirements, to maintain ethical
    and legal standards in video analytics. For example, the facial images shouldn’t
    be stored, only the extracted facial features, as this can infringe personal data
    protection-related regulations, depending on the country it is intended to be
    deployed in.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性、可信度和可靠性**：实现对模型性能的监控，并确保遵守与视频监控相关的本地法规，如数据隐私、存储政策和同意要求，以维持视频分析的伦理和法律标准。例如，面部图像不应被存储，仅应存储提取的面部特征，因为这可能违反与个人数据保护相关的法规，具体取决于部署的国家。'
- en: '**Communication protocol**: MQTT or WebSockets, as they provide low-latency
    communication between edge devices and the central monitoring system.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通信协议**：MQTT 或 WebSockets，因为它们提供边缘设备和中央监控系统之间的低延迟通信。'
- en: '**User interface**: A desktop application that displays real-time video feeds
    with object detection overlays for security personnel to monitor.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户界面**：一个桌面应用程序，显示带有物体检测叠加层的实时视频流，供安保人员监控。'
- en: By considering the specific requirements of each use case, you can make informed
    decisions on the components required for deploying DL models in production. Now,
    let’s move on to some general recommendations for successful DL model deployment.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑每个用例的具体需求，您可以就生产环境中部署深度学习模型所需的组件做出明智的决策。接下来，让我们进入一些成功深度学习模型部署的通用建议。
- en: Discovering general recommendations for DL deployment
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索深度学习部署的通用建议
- en: Here, we will discover DL deployment recommendations related to three verticals,
    namely model safety, trust, and reliability assurance, model latency optimization,
    and tools that help abstract model deployment-related decisions and ease the model
    deployment process. We will dive into the three verticals one by one.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将发现与三大垂直领域相关的深度学习部署建议，即模型安全、可信度和可靠性保障，模型延迟优化，以及帮助抽象模型部署相关决策并简化模型部署过程的工具。我们将逐一深入探讨这三大领域。
- en: Model safety, trust, and reliability assurance
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型安全、可信度和可靠性保障
- en: Ensuring model safety, trust, and reliability is a crucial aspect of deploying
    DL systems. In this section, we will explore various recommendations and best
    practices to help you establish a robust framework for maintaining the integrity
    of your models. This includes compliance with regulations, implementing guardrails,
    prediction consistency, comprehensive testing, staging and production deployment
    strategies, usability tests, retraining and updating deployed models, human-in-the-loop
    decision-making, and model governance. By adopting these measures, you can effectively
    mitigate risks, enhance performance, and foster user trust in your DL deployment.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型安全、可信度和可靠性是部署深度学习系统的关键环节。在本节中，我们将探索各种建议和最佳实践，帮助您建立一个强大的框架，以维护模型的完整性。这包括遵守规定、实施安全防护措施、预测一致性、全面测试、分阶段和生产部署策略、可用性测试、重新训练和更新已部署的模型、人工干预决策和模型治理。通过采取这些措施，您可以有效地降低风险、提升性能，并增强用户对您深度学习部署的信任。
- en: Comply with regulations and implement guardrails
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遵守规定并实施安全防护措施
- en: 'Regulatory compliance and guardrails are essential components of responsible
    deep learning deployment, ensuring that your model adheres to relevant laws, industry
    standards, and ethical guidelines. Implementing a robust compliance framework
    not only mitigates legal and reputational risks but also fosters trust among users
    and stakeholders. It’s a very broad topic, so here are a few examples to learn
    from:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 法规遵从性和保障措施是负责任的深度学习部署的重要组成部分，确保你的模型遵守相关法律、行业标准和道德准则。实施强有力的合规框架不仅可以减少法律和声誉风险，还能增强用户和利益相关者的信任。这是一个非常广泛的话题，以下是一些可以借鉴的例子：
- en: '**Content moderation for social media platforms**: Compliance with community
    guidelines and regional laws can be achieved by implementing AI-powered filters
    for detecting and flagging inappropriate content, setting up a human review process
    for ambiguous cases, and providing users with a transparent mechanism to appeal
    decisions.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体平台内容审核**：遵守社区准则和地区法律可以通过实施 AI 驱动的过滤器来检测和标记不当内容，设立人工审查流程处理模糊案件，并为用户提供透明的上诉机制。'
- en: '**AI-powered recruitment tools**: Compliance with anti-discrimination laws
    can involve steps such as monitoring bias and fairness metric performance and
    ensuring that any automated decisions are transparent and explainable to both
    employers and applicants.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI 驱动的招聘工具**：遵守反歧视法可以采取措施，例如监控偏见和公平性指标的表现，确保任何自动化决策对雇主和申请者都是透明且可解释的。'
- en: '**Facial recognition systems**: Compliance with privacy and ethical guidelines
    can be achieved through steps such as obtaining explicit consent from individuals
    before collecting and processing their biometric data, implementing robust data
    security measures, and ensuring transparency about the system’s capabilities and
    limitations.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人脸识别系统**：遵守隐私和道德准则可以通过以下步骤实现，例如在收集和处理个人生物识别数据之前获得明确同意，实施强大的数据安全措施，并确保关于系统能力和局限性的透明度。'
- en: '**DL-based video surveillance systems, such as people detection**: Compliance
    with privacy and ethical guidelines can be achieved through measures such as setting
    up clear signage to inform the public about the presence of surveillance cameras,
    restricting data access to authorized personnel, and adhering to data retention
    and deletion policies as per local regulations.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于深度学习的视频监控系统，例如人员检测**：遵守隐私和道德准则可以通过措施实现，例如设置明确的标识牌，告知公众存在监控摄像头，限制数据访问仅限授权人员，并根据当地法规遵守数据保留和删除政策。'
- en: '**Recommendation systems (YouTube, Netflix, and Tiktok)**: Ensuring compliance
    with data protection regulations can involve steps such as implementing privacy-preserving
    data processing techniques, providing users with the ability to opt out of personalized
    recommendations, and being transparent about data collection and usage policies.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统（YouTube、Netflix 和 Tiktok）**：确保符合数据保护法规可以采取措施，例如实施隐私保护数据处理技术、为用户提供选择退出个性化推荐的功能，以及在数据收集和使用政策方面保持透明。'
- en: '**Generative AI**: Compliance can be achieved by using content filtering mechanisms
    to prevent harmful content generation, which includes hate speech, explicit material,
    and content that encourages criminal activities, or prevent dangerous recommendations
    about medical issues.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成式 AI**：合规性可以通过使用内容过滤机制来实现，以防止有害内容的生成，包括仇恨言论、淫秽材料和鼓励犯罪活动的内容，或者防止关于医疗问题的危险推荐。'
- en: As we continue to explore model safety, trust, and reliability assurance, let’s
    examine the vital aspect of ensuring prediction consistency in DL deployment.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续探索模型安全性、信任和可靠性保障，让我们来审视确保深度学习部署中预测一致性的关键方面。
- en: Ensure prediction consistency
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确保预测一致性
- en: Prediction consistency is all about a model’s ability to generate the same predictions
    when faced with the same input data, no matter the hardware, pre/post serialization
    and loading, infrastructure, or whether it’s a single row or a random batch. Inconsistent
    predictions can lead to mismatched expectations of a model’s accuracy and overall
    performance. To maintain consistency across various factors, it’s essential to
    track and replicate the environmental dependencies involved in training, evaluation,
    and inference. Tools such as Docker can help create isolated environments with
    specific dependencies, ensuring a seamless experience and eliminating potential
    issues. Additionally, consider making automated tests to objectively prevent any
    inconsistency from going through, essentially working as a guardrail.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 预测一致性是指模型在面对相同的输入数据时，能够生成相同的预测，无论硬件、序列化/反序列化过程、基础设施，还是单条数据还是随机批次。一致性差的预测可能导致对模型准确性和整体性能的预期偏差。为了在各种因素中保持一致性，必须跟踪并复制训练、评估和推理过程中涉及的环境依赖关系。像Docker这样的工具可以帮助创建具有特定依赖项的隔离环境，确保无缝体验并消除潜在问题。此外，考虑设置自动化测试，客观地防止任何不一致通过，基本上充当一个保护屏障。
- en: Moving forward, we will discuss the significance of comprehensive testing in
    maintaining a reliable DL deployment.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论全面测试在保持可靠的深度学习部署中的重要性。
- en: Testing
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试
- en: 'Other than prediction consistency tests, generally, comprehensive testing will
    ensure that your DL model and system perform as expected at all times and meet
    user requirements. DL systems are essentially software systems and require similar
    things to ensure a successful deployment. The test components are as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测一致性测试，全面的测试通常可以确保你的深度学习模型和系统在任何时候都能按预期运行，并满足用户需求。深度学习系统本质上是软件系统，需要类似的措施来确保成功部署。测试组件如下：
- en: '**Unit, integration, and functional tests**: Unit, integration, and functional
    testing are essential for ensuring the reliability, maintainability, and overall
    quality of software components. Here’s why they are important:'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元、集成和功能测试**：单元测试、集成测试和功能测试对确保软件组件的可靠性、可维护性和整体质量至关重要。以下是它们的重要性：'
- en: '**Unit testing**: This focuses on individual components or functions, verifying
    their correctness and isolating potential issues early in development. This helps
    catch bugs before they propagate, reduces debugging time, and improves code maintainability.'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单元测试**：侧重于个别组件或功能，验证其正确性并在开发早期隔离潜在问题。这有助于在问题蔓延前捕捉到错误，减少调试时间，提高代码可维护性。'
- en: '**Integration testing**: This validates the interactions between different
    components, ensuring they work together as intended. This helps identify interface
    issues, data flow problems, and inconsistencies that can arise when combining
    components, ensuring a smooth integration.'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成测试**：验证不同组件之间的交互，确保它们按预期协同工作。这有助于识别界面问题、数据流问题以及在组合组件时可能出现的不一致，确保顺利集成。'
- en: '**Functional testing**: This assesses the software’s ability to fulfill its
    intended purpose and meet user requirements. Testing end-to-end functionality
    ensures that the software operates correctly in real-world scenarios and delivers
    a positive user experience.'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功能测试**：评估软件是否能够实现其预定目的并满足用户需求。端到端功能测试确保软件在实际场景中正常运行，并提供良好的用户体验。'
- en: '**Failover and recovery testing**: Verify the model’s ability to recover from
    failures, such as hardware or software crashes, and maintain high availability
    in the face of unexpected disruptions.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障切换与恢复测试**：验证模型在遇到硬件或软件崩溃等故障后，能否恢复并保持在意外中断情况下的高可用性。'
- en: '**Load stress testing**: Evaluate the model’s performance under various load
    conditions to identify bottlenecks and ensure it can handle the expected user
    traffic. These tests can also help you catch errors such as GPU memory overflow,
    CPU overload, or insufficient storage.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载压力测试**：评估模型在不同负载条件下的表现，识别瓶颈并确保其能够处理预期的用户流量。这些测试还能帮助你发现错误，如GPU内存溢出、CPU过载或存储不足等。'
- en: '**Broad and diverse testing**: The model may not be able to handle unexpected
    input data, edge cases, or system failures gracefully, causing crashes or undesired
    behavior. Thinking up all the possible ways the system will be used can help you
    catch issues with the system.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**广泛且多样化的测试**：模型可能无法优雅地处理意外的输入数据、边缘情况或系统故障，导致崩溃或不期望的行为。考虑所有可能的系统使用方式有助于你捕捉到系统中的问题。'
- en: '**Adopting the staging and production deployment steps**: Embracing a staging
    and production strategy in DL production deployment is highly beneficial for ensuring
    model reliability and performance. This approach involves setting up separate
    environments for testing (staging) and final deployment (production), allowing
    you to validate your model’s behavior and identify potential issues before the
    model goes live. By adopting this strategy, you can minimize the risks associated
    with deploying untested models, streamline the process of identifying and resolving
    issues, and enhance the overall reliability of your DL solutions. Ensure the pipeline
    can continuously be in production for 24 hours without failure.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采用分阶段部署和生产部署步骤**：在深度学习（DL）生产部署中采用分阶段和生产部署策略，对于确保模型的可靠性和性能非常有益。这种方法包括为测试（阶段）和最终部署（生产）设置单独的环境，使你能够验证模型的行为并在模型上线之前识别潜在问题。通过采纳这一策略，可以最小化部署未经测试模型的风险，简化问题识别和解决的过程，并提高整个DL解决方案的可靠性。确保流水线能够在24小时内持续无故障地运行。'
- en: '**Usability tests**: Usability tests focus on ensuring that software applications
    deliver an effective, efficient, and satisfying user experience. Both automated
    and manual tests are useful and complementary to each other:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可用性测试**：可用性测试的重点是确保软件应用程序能够提供高效、有效且令人满意的用户体验。自动化和手动测试互为补充，各有其独特的作用：'
- en: '**Manual usability testing**: This involves real users interacting with the
    software to identify potential usability issues, understand user behavior, and
    gather qualitative feedback. Manual testing helps uncover problems that may not
    be detectable through automated testing, such as confusing navigation, unclear
    instructions, or subjective preferences. This human-centric approach provides
    valuable insights into how users perceive the software and identifies areas for
    improvement.'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动可用性测试**：这涉及真实用户与软件的互动，以识别潜在的可用性问题、了解用户行为并收集定性反馈。手动测试有助于发现自动化测试可能无法检测到的问题，例如导航混乱、指令不清或主观偏好等。这种以人为中心的方法提供了关于用户如何感知软件的宝贵见解，并确定了改进的领域。'
- en: '**Automated usability testing**: This complements manual testing by using tools
    and scripts to simulate user interactions, validate user interface elements, and
    check for accessibility and responsiveness. Automated testing offers several advantages,
    including increased efficiency, speed, and coverage, as well as the ability to
    consistently test across multiple devices, platforms, and browsers. This helps
    with identifying usability issues that may not be apparent during manual testing,
    ensuring a consistent and high-quality user experience.'
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化可用性测试**：自动化可用性测试通过使用工具和脚本来模拟用户互动、验证用户界面元素并检查可访问性和响应性，从而补充手动测试。自动化测试具有多个优势，包括提高效率、速度和覆盖面，以及能够在多个设备、平台和浏览器上进行一致的测试。这有助于识别在手动测试中可能不显现的可用性问题，从而确保一致且高质量的用户体验。'
- en: Next, let’s consider the importance of retraining and updating the deployed
    model to ensure its continued effectiveness and relevance.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来讨论重新训练和更新已部署模型的重要性，以确保其持续的有效性和相关性。
- en: Retraining and updating the deployed model
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新训练和更新已部署的模型
- en: A retraining and updating strategy is crucial for maintaining the effectiveness
    of your DL model as it addresses the potential need for regular updates in response
    to changing data patterns. By periodically retraining your model on fresh, relevant
    data, you can ensure it stays current and continues to deliver accurate predictions.
    This not only helps maintain the model’s performance but also keeps it in tune
    with evolving trends and user requirements. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will delve deeper into the importance of
    retraining and updating, exploring its benefits and best practices to help you
    successfully implement this strategy in your DL deployment practically.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练和更新策略对于保持深度学习模型的有效性至关重要，因为它解决了随着数据模式变化可能需要定期更新的问题。通过定期在最新、相关的数据上重新训练模型，您可以确保它始终保持最新，并持续提供准确的预测。这不仅有助于保持模型的性能，还能让它与不断变化的趋势和用户需求保持同步。在[*第16章*](B18187_16.xhtml#_idTextAnchor238)《深度学习模型治理》中，我们将深入探讨重新训练和更新的重要性，探索其好处和最佳实践，帮助您在实际的深度学习部署中成功实施这一策略。
- en: To further enhance our DL deployment, we will explore the benefits of adopting
    a human-in-the-loop decision-making flow.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提升我们的深度学习部署效果，我们将探索采用人类决策参与的决策流程的好处。
- en: Adopting a human-in-the-loop decision-making flow
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采用人类决策参与的决策流程
- en: Incorporating human-in-the-loop scenarios into your DL deployment can greatly
    enhance model performance and reliability, either as a permanent solution or by
    triggering alerts when certain conditions are met. By involving human experts
    in the decision-making process, you can bridge the gap between the model’s predictions
    and real-world complexities, allowing for more accurate and nuanced decisions.
    This collaborative approach enables continuous improvement by leveraging human
    expertise to validate, correct, and fine-tune the model’s output. Additionally,
    human-in-the-loop systems foster trust and accountability as users can be confident
    that complex or high-stakes decisions are not made solely by algorithms but are
    also supported by human judgment and oversight.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 将人类决策参与场景纳入您的深度学习部署可以极大提高模型的性能和可靠性，无论是作为长期解决方案，还是在满足某些条件时触发警报。通过让人类专家参与决策过程，您可以弥合模型预测与现实世界复杂性之间的差距，从而做出更准确、细致的决策。这种合作方法通过利用人类的专业知识来验证、纠正和调整模型输出，推动持续改进。此外，人类决策参与系统还能够增强信任和责任感，因为用户可以放心地知道，复杂或高风险的决策不仅由算法做出，还得到了人类判断和监督的支持。
- en: Lastly, we will delve into the crucial role of model governance in overseeing
    and managing the overall DL deployment process.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将深入探讨模型治理在监督和管理整体深度学习部署过程中的关键作用。
- en: Model governance
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型治理
- en: Monitoring and governance play a pivotal role in ensuring the ongoing effectiveness
    and reliability of your deep learning deployment. By tracking various aspects
    of your model, such as data drift and concept drift monitoring, you can identify
    and address issues that may affect its performance over time. Data drift monitoring
    helps detect changes in the underlying data distribution, while concept drift
    monitoring focuses on shifts in the relationships between input features and target
    variables. Establishing a robust monitoring and governance framework enables you
    to proactively manage your model’s performance and maintain its accuracy in the
    face of evolving trends and conditions. In [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238),
    *Governing Deep Learning Models*, we will explore these aspects in greater detail,
    along with other critical components of model monitoring and governance, to help
    you develop a comprehensive strategy for maintaining your DL deployment’s effectiveness.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和治理在确保深度学习部署持续有效性和可靠性方面发挥着关键作用。通过跟踪模型的各个方面，如数据漂移和概念漂移监控，您可以识别并解决可能影响其长期表现的问题。数据漂移监控有助于检测底层数据分布的变化，而概念漂移监控则关注输入特征与目标变量之间关系的变化。建立一个强大的监控和治理框架使您能够主动管理模型的性能，并在面临不断变化的趋势和条件时保持其准确性。在[*第16章*](B18187_16.xhtml#_idTextAnchor238)《深度学习模型治理》中，我们将更详细地探讨这些方面，以及模型监控和治理的其他关键组件，帮助您制定一个全面的策略，保持深度学习部署的有效性。
- en: Next, we will explore the recommendations for model latency optimization.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨模型延迟优化的建议。
- en: Optimizing model latency
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化模型延迟
- en: 'Assuming you have chosen an ideal architecture, trained a model, extracted
    insights, selected the inference model compiler/acceleration framework, and selected
    the target hardware infrastructure and architecture for hosting your models, there
    are additional steps you can take to improve model latency at this stage. The
    following techniques can be employed:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经选择了理想的架构，训练了一个模型，提取了洞察，选择了推理模型编译器/加速框架，并为托管模型选择了目标硬件基础设施和架构，那么你可以采取额外的步骤来提高此阶段的模型延迟。可以采用以下技术：
- en: '**Model pruning**: Remove unnecessary neurons or weights in the neural network
    without affecting the overall performance significantly. Pruning techniques include
    weight pruning, neuron pruning, and filter pruning. This can reduce model size
    and computational requirements, resulting in faster inference times.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型剪枝**：去除神经网络中不必要的神经元或权重，而不会显著影响整体性能。剪枝技术包括权重剪枝、神经元剪枝和滤波器剪枝。这可以减小模型大小和计算要求，从而提高推理速度。'
- en: '**Model quantization**: Reduce the precision of model parameters (for example,
    weights and biases) from 32-bit floating-point numbers to lower bit-width representations
    such as 16-bit or 8-bit integers. Quantization can accelerate model inference
    without significant loss in accuracy, especially when deploying DL models on hardware
    with limited computational resources.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型量化**：将模型参数（例如权重和偏置）的精度从32位浮动点数降低到较低位宽的表示方式，如16位或8位整数。量化可以加速模型推理，同时不会显著损失准确性，特别是在将深度学习模型部署到计算资源有限的硬件上时。'
- en: '**Model distillation**: Train a smaller, faster “student” model to mimic the
    behavior of a larger, slower “teacher” model. The student model learns from the
    teacher model’s outputs, achieving comparable performance with reduced complexity
    and faster inference times. This method was demonstrated in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型蒸馏**：训练一个更小、更快的“学生”模型来模仿较大、较慢的“教师”模型的行为。学生模型从教师模型的输出中学习，以较低的复杂度和更快的推理速度实现相似的性能。此方法在[*第13章*](B18187_13.xhtml#_idTextAnchor196)中进行了展示，*探索偏差*和*公平性*。'
- en: '**Model parallelism**: In model parallelism, different parts of a neural network
    are distributed across multiple devices or processors, allowing concurrent computation
    on different portions of the model. For huge models that cannot fit entirely within
    the memory of a single GPU, this method is an essential step. For models that
    have highly parallel operations, latency can be reduced significantly. Model parallelism
    can be achieved at various parallelism levels, such as layer-level, pipeline-level,
    or tensor-slicing level parallelism.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行**：在模型并行中，神经网络的不同部分被分配到多个设备或处理器上，从而在不同的模型部分上进行并行计算。对于无法完全容纳在单个GPU内存中的巨大模型，这种方法是必不可少的。对于具有高度并行操作的模型，延迟可以显著减少。模型并行可以在各种并行级别上实现，例如层级并行、流水线级并行或张量切片级并行。'
- en: '**Batch inference**: Process multiple input samples simultaneously through
    batch processing, enabling the model to make better use of the underlying hardware,
    leading to faster overall inference times.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量推理**：通过批处理同时处理多个输入样本，使模型能够更好地利用底层硬件，从而提高整体推理速度。'
- en: Next, we will explore tools that abstract deployment.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨一些抽象化部署的工具。
- en: Tools that abstract deployment
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象化部署的工具
- en: 'There are numerous tools and platforms available that help abstract the model
    deployment process, making it easier and more efficient to deploy machine learning
    models in various environments. Here’s an overview of some popular tools and platforms,
    including both open source and paid-for tools:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具和平台可用，帮助抽象化模型部署过程，使得在不同环境中部署机器学习模型变得更简单、更高效。以下是一些流行工具和平台的概述，包括开源工具和收费工具：
- en: '**TensorFlow serving (open source tool)**: A flexible, high-performance serving
    system for deploying TensorFlow models in a production environment that provides
    out-of-the-box support for model versioning, REST and gRPC APIs, and efficient
    model serving on GPUs and CPUs.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Serving（开源工具）**：一个灵活的高性能服务系统，用于在生产环境中部署TensorFlow模型，提供开箱即用的支持模型版本控制、REST和gRPC
    API，以及在GPU和CPU上的高效模型服务。'
- en: '**TorchServe (open source tool)**: The PyTorch equivalent of TensorFlow Serving.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TorchServe（开源工具）**：PyTorch的等效工具，类似于TensorFlow Serving。'
- en: '**TensorFlow Extended (TFX) (open source tool)**: An end-to-end platform for
    deploying, managing, and maintaining machine learning pipelines in production.
    TFX integrates with TensorFlow, TensorFlow Serving, and other tools to provide
    a seamless deployment experience.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Extended (TFX)（开源工具）**：一个端到端的平台，用于在生产环境中部署、管理和维护机器学习管道。TFX 与 TensorFlow、TensorFlow
    Serving 及其他工具集成，提供无缝的部署体验。'
- en: '**MLflow (open source tool)**: An open source platform that streamlines the
    end-to-end machine learning life cycle, including experimentation, reproducibility,
    deployment, and monitoring. It supports multiple languages and machine learning
    libraries, making it a versatile choice for diverse projects.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow（开源工具）**：一个开源平台，简化了端到端机器学习生命周期，包括实验、可重现性、部署和监控。它支持多种编程语言和机器学习库，是多样化项目的多功能选择。'
- en: '**Kubeflow (open source tool)**: A Kubernetes-integrated solution that’s designed
    to facilitate the creation, coordination, deployment, and execution of adaptable
    and transportable machine learning tasks. It simplifies the deployment process
    by providing a consistent and unified environment across different cloud providers
    and on-premises infrastructure.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow（开源工具）**：一个与 Kubernetes 集成的解决方案，旨在促进可适应、可传输的机器学习任务的创建、协调、部署和执行。通过为不同的云服务提供商和本地基础设施提供一致的统一环境，它简化了部署过程。'
- en: '**Streamlit (open source tool)**: A Python library that enables developers
    to quickly build and deploy custom web applications for machine learning and data
    science projects. Streamlit simplifies the process of creating interactive web
    apps with minimal coding, making it easier to share and deploy models through
    web apps.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Streamlit（开源工具）**：一个 Python 库，使开发者能够快速构建和部署机器学习与数据科学项目的定制化 web 应用。Streamlit
    简化了创建交互式 web 应用的过程，几乎无需编码，从而更容易通过 web 应用共享和部署模型。'
- en: '**NVIDIA Triton (open source tool)**: An open source tool that can be used
    to deploy DL models. It natively supports many frameworks, most notably TensorRT,
    Pytorch, ONNX Runtime, OpenVINO, and a general Python backend that allows you
    to wrap and run any DL framework and Python code. It provides predictions through
    HTTP REST APIs and the gRPC protocol. It also natively provides the Prometheus-compatible
    and standard time series performance metric logs, which can be subsequently used
    for model monitoring in the Grafana dashboard. It also allows us to configure
    custom metrics in its C API. Most relevantly, it eases multiple GPU utilization
    and GPU memory assignments. We will be exploring this tool practically in the
    next section.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NVIDIA Triton（开源工具）**：一个开源工具，可用于部署深度学习（DL）模型。它原生支持许多框架，特别是 TensorRT、Pytorch、ONNX
    Runtime、OpenVINO，以及一个通用的 Python 后端，允许你包装和运行任何 DL 框架和 Python 代码。它通过 HTTP REST API
    和 gRPC 协议提供预测。它还原生提供与 Prometheus 兼容的标准时间序列性能指标日志，之后可用于在 Grafana 仪表板中进行模型监控。它还允许我们在其
    C API 中配置自定义指标。最重要的是，它简化了多个 GPU 的利用和 GPU 内存分配。我们将在下一节中实际探讨该工具的使用。'
- en: '**Azure ML deployment (paid-for tool)**: Microsoft Azure’s machine learning
    service that simplifies model deployment in the cloud. It provides tools for managing,
    monitoring, and scaling deployed models, and supports popular frameworks such
    as TensorFlow and PyTorch.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure ML 部署（付费工具）**：微软 Azure 的机器学习服务，简化了云端模型的部署。它提供了管理、监控和扩展已部署模型的工具，并支持如
    TensorFlow 和 PyTorch 等流行框架。'
- en: '**DataRobot (paid-for tool)**: DataRobot is an automated machine learning platform
    that simplifies the process of building, deploying, and maintaining machine learning
    models. It provides a wide range of tools and features, including customization,
    model versioning, monitoring, and collaboration. We will be exploring the usage
    of this platform in [*Chapter 18*](B18187_18.xhtml#_idTextAnchor265), *Exploring
    the DataRobot* *AI Platform*.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataRobot（付费工具）**：DataRobot 是一个自动化机器学习平台，简化了构建、部署和维护机器学习模型的过程。它提供了多种工具和功能，包括定制、模型版本控制、监控和协作。我们将在
    [*第 18 章*](B18187_18.xhtml#_idTextAnchor265) 中探索该平台的使用，*探索 DataRobot* *AI 平台*。'
- en: '**Google Vertex AI (paid-for tool)**: A managed machine learning platform from
    Google Cloud that streamlines the end-to-end machine learning workflow, including
    model training, deployment, and management. It integrates with TensorFlow, PyTorch,
    and other popular frameworks.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Vertex AI（付费工具）**：谷歌云提供的托管机器学习平台，简化了端到端的机器学习工作流，包括模型训练、部署和管理。它与 TensorFlow、PyTorch
    以及其他流行框架集成。'
- en: '**Amazon SageMaker (paid-for tool)**: A fully managed machine learning service
    from AWS that allows developers to build, train, and deploy machine learning models
    quickly and easily. It supports multiple frameworks and provides tools for model
    versioning, monitoring, and scaling.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon SageMaker（付费工具）**：AWS 提供的一项完全托管的机器学习服务，允许开发者快速轻松地构建、训练和部署机器学习模型。它支持多种框架，并提供模型版本管理、监控和扩展工具。'
- en: These tools and platforms help simplify and streamline the model deployment
    process, enabling developers to efficiently deploy their machine learning models
    in various environments.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具和平台有助于简化和优化模型部署过程，使开发者能够高效地在各种环境中部署他们的机器学习模型。
- en: Successful DL deployment requires addressing key aspects such as safety, trust,
    reliability, and latency optimization while leveraging tools and platforms that
    simplify the process. By adhering to these recommendations and utilizing appropriate
    tools, developers can effectively deploy and manage their DL models in various
    environments, ensuring consistent and reliable performance.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的 DL 部署需要解决关键方面，例如安全性、信任、可靠性和延迟优化，同时利用简化流程的工具和平台。通过遵循这些建议并使用合适的工具，开发者可以有效地部署和管理他们的
    DL 模型，确保在各种环境中始终如一、可靠的性能。
- en: It should be apparent by now that many trade-offs and criteria need to be evaluated
    and considered before you can make a DL deployment system component choice. However,
    if you don’t have access to a paid tool, have a DL model, and have access to a
    GPU machine that has enough RAM to host your model, three tools are a no-brainer
    to choose from. In the next section, we will dive into a topic that both reveals
    those three tools and practically uses them.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经能明显看出，在选择 DL 部署系统组件之前，需要评估和考虑许多折衷和标准。然而，如果你没有付费工具，拥有一个 DL 模型，并且可以访问一个具有足够内存来托管模型的
    GPU 机器，那么这三种工具就是不容置疑的选择。接下来的部分，我们将深入探讨这三种工具，并实际使用它们。
- en: Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ONNX、TensorRT 和 NVIDIA Triton Server 部署语言模型
- en: The three tools are ONNX, TensorRT, and NVIDIA Triton Server. ONNX and TensorRT
    are meant to perform GPU-based inference acceleration, while NVIDIA Triton Server
    is meant to host HTTP or GRPC APIs. We will explore these three tools practically
    in this section. TensorRT is known to perform the best model optimization toward
    the GPU to speed up inference, while NVIDIA Triton Server is a battle-tested tool
    for hosting DP models that have compatibility with TensorRT natively. ONNX, on
    the other hand, is an intermediate framework in the setup, which we will use primarily
    to host the weight formats that are directly supported by TensorRT.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种工具是 ONNX、TensorRT 和 NVIDIA Triton Server。ONNX 和 TensorRT 旨在执行基于 GPU 的推理加速，而
    NVIDIA Triton Server 用于托管 HTTP 或 GRPC API。我们将在本节中实践探索这三种工具。TensorRT 被认为是最擅长优化模型以加速
    GPU 推理的工具，而 NVIDIA Triton Server 是一款经过实践考验的托管深度学习（DP）模型的工具，并且与 TensorRT 原生兼容。另一方面，ONNX
    是设置过程中的一个中间框架，我们主要用它来托管 TensorRT 直接支持的权重格式。
- en: 'In this practical tutorial, we will be deploying a Hugging Face-sourced language
    model that can be supported on most NVIDIA GPU devices. We will be converting
    our PyTorch-based language model from Hugging Face into ONNX weights, which will
    allow TensorRT to load the Hugging Face language model. Then, we will create the
    code and configuration required by the NVIDIA Triton Server framework to host
    the language model. NVIDIA Triton Server supports two ways of deploying a model,
    which is to deploy the DL model with its pre-processing and post-processing methods
    as a single pipeline all embedded into a Python class, and to deploy the DL model
    by logically separating the pipeline into separate components. *Figure 15**.1*
    depicts both approaches with a pipeline that requires two models:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实践教程中，我们将部署一个来源于 Hugging Face 的语言模型，该模型可以在大多数 NVIDIA GPU 设备上运行。我们将把基于 PyTorch
    的 Hugging Face 语言模型转换为 ONNX 权重，这将允许 TensorRT 加载 Hugging Face 语言模型。接下来，我们将创建代码和配置，要求
    NVIDIA Triton Server 框架托管该语言模型。NVIDIA Triton Server 支持两种方式来部署模型，一种是将深度学习（DL）模型与其预处理和后处理方法作为一个单一管道部署，所有这些都嵌入在一个
    Python 类中，另一种是通过逻辑上分离管道的不同组件来部署 DL 模型。*图 15.1* 描述了这两种方法，它们涉及需要两个模型的管道：
- en: '![Figure 15.1 – Two approaches for configuring the model deployment with two
    models in the pipeline](img/B18187_15_01.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图 15.1 – 配置模型部署的两种方法，其中管道中有两个模型](img/B18187_15_01.jpg)'
- en: Figure 15.1 – Two approaches for configuring the model deployment with two models
    in the pipeline
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 – 配置具有两个模型的管道中模型部署的两种方法
- en: 'An intuitive and straightforward way to deploy your DL model pipeline is to
    go with the first approach. However, the second approach, which involves breaking
    down and separating each component into its configuration, provides multiple benefits:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 部署深度学习模型管道的一种直观且简单的方式是使用第一种方法。然而，第二种方法，通过将每个组件分开并拆分为独立的配置，提供了多种好处：
- en: '**Modularity and reusability**: The modularity aspect allows the individual
    parts to be reused across different pipelines or projects. Additionally, it allows
    easier component swapping while maintaining other components in the pipeline.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化和可重用性**：模块化的特点使得各个部分可以在不同的管道或项目中重用。此外，它还允许在保持其他组件的同时，轻松地交换组件。'
- en: '**Scalability and flexibility**: This allows you to deploy different components
    to different GPUs and assign different instances of each component running at
    one time. Additionally, this method allows CPU-bound methods to not get tied to
    a GPU.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和灵活性**：这使得您可以将不同的组件部署到不同的GPU，并为每个组件分配不同的实例进行同时运行。此外，这种方法允许与CPU绑定的方法不受GPU限制。'
- en: '**Parallelism and latency reduction**: Native parallelism can be enabled through
    parallel branches instead of you needing to implement it in Python code.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性和延迟减少**：可以通过并行分支启用本地并行性，而不需要在Python代码中实现它。'
- en: 'Consider the following two approaches as ways to organize a factory assembly
    line:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下两种方法视为组织工厂流水线的方式：
- en: In the first approach, the entire assembly process is combined into a single
    pipeline. This means that all components are processed and assembled sequentially
    in one integrated process. This can be easier to set up and manage, but it may
    not be as flexible or scalable as the second approach.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一种方法中，整个组装过程被合并为一个单一的管道。这意味着所有组件都会按顺序处理并组合成一个集成过程。这种方法可能更容易设置和管理，但它的灵活性和可扩展性可能不如第二种方法。
- en: In the second approach, the assembly process is broken down into separate, modular
    components that can be individually managed and optimized. This allows for greater
    flexibility and scalability as each component can be fine-tuned or replaced without
    it affecting the entire pipeline. Additionally, this approach enables parallel
    processing, where multiple components can be processed simultaneously, potentially
    reducing overall latency and improving efficiency.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种方法中，组装过程被拆分为独立的模块化组件，这些组件可以单独管理和优化。这样可以提高灵活性和可扩展性，因为每个组件可以独立调整或更换，而不会影响整个管道。此外，这种方法支持并行处理，多个组件可以同时处理，可能会减少整体延迟并提高效率。
- en: To make things simple, we will be demonstrating the first approach here.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化演示，我们将在这里展示第一种方法。
- en: Practically deploying a DL model with the single pipeline approach
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际使用单一管道方法部署深度学习模型。
- en: 'In this tutorial, we will explore the process of deploying a DL model using
    ONNX, TensorRT, and NVIDIA Triton Server. While deploying the model using NVIDIA
    Triton Server, you may encounter issues related to model loading, configuration,
    or inference. Here are some troubleshooting tips:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将探索使用ONNX、TensorRT和NVIDIA Triton Server部署深度学习模型的过程。在使用NVIDIA Triton
    Server部署模型时，您可能会遇到与模型加载、配置或推理相关的问题。以下是一些故障排除建议：
- en: Verify that the model files, configuration files, and other required files are
    in the correct locations and have the proper file permissions
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证模型文件、配置文件和其他必需文件是否位于正确位置，并且具有正确的文件权限。
- en: Ensure that the model configuration file (`config.pbtxt`) has the correct settings,
    such as input and output tensor names, data types, and dimensions
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保模型配置文件（`config.pbtxt`）具有正确的设置，例如输入和输出张量名称、数据类型和维度。
- en: Check the NVIDIA Triton Server logs for any error messages or warnings that
    could provide insights into the issue
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查NVIDIA Triton Server日志，查看是否有任何错误消息或警告，这些可能为问题提供线索。
- en: Make sure that the necessary dependencies, such as the DL framework, ONNX, and
    TensorRT, are installed and compatible with your system and hardware
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保必要的依赖项，如深度学习框架、ONNX和TensorRT，已安装并与您的系统和硬件兼容。
- en: 'Let’s start the practical tutorial in a step-by-step manner:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以逐步的方式开始实际操作教程：
- en: 'First, we need to install the `transformer-deploy` repository by running the
    following code:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要通过运行以下代码来安装`transformer-deploy`仓库：
- en: '[PRE0]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Alternatively, we can use the following command:'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，我们可以使用以下命令：
- en: '[PRE1]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with torch.no_grad():'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'with torch.no_grad():'
- en: torch.onnx.export(
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: torch.onnx.export(
- en: model_pytorch,
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: model_pytorch,
- en: args=tuple(inputs_pytorch.values()),
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: args=tuple(inputs_pytorch.values()),
- en: f=output_path,
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f=output_path,
- en: opset_version=13,
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: opset_version=13,
- en: do_constant_folding=True,
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: do_constant_folding=True,
- en: input_names=input_names,
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: input_names=input_names,
- en: output_names=output_names,
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: output_names=output_names,
- en: dynamic_axes=dynamic_axis,
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: dynamic_axes=dynamic_axis,
- en: training=TrainingMode.EVAL,
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: training=TrainingMode.EVAL,
- en: )
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: )
- en: 'of this process. This is what happens when we use the transformer-deploy tool:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们使用 transformer-deploy 工具时发生的情况：
- en: '[PRE2]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: trt_engine = builder.build_serialized_network(network_def, config)
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: trt_engine = builder.build_serialized_network(network_def, config)
- en: 'engine: ICudaEngine = runtime.deserialize_cuda_engine(trt_engine)'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'engine: ICudaEngine = runtime.deserialize_cuda_engine(trt_engine)'
- en: 'with open(engine_file_path, "wb") as f:'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'with open(engine_file_path, "wb") as f:'
- en: f.write(engine.serialize())
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: f.write(engine.serialize())
- en: '[PRE3]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we will execute the command that will convert the chosen Hugging Face
    text generation model into a TensorRT serialized engine:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将执行命令，将选定的 Hugging Face 文本生成模型转换为 TensorRT 序列化引擎：
- en: '[PRE4]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result of running the command in *step 4* is as follows:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中运行命令的结果如下：
- en: '[PRE5]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, we will move the model into new folders at the following path:'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将把模型移到以下路径的新文件夹中：
- en: '[PRE6]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, it is time to create the code and configurations required to host this
    TensorRT model in NVIDIA Triton Server. Following approach 1 from *Figure 15**.1*,
    we need to define a Python class to initialize and perform inference using the
    Hugging Face tokenizer and the TensorRT engine. Let’s start by importing the necessary
    Python libraries into this deployment Python code file:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候创建托管此 TensorRT 模型在 NVIDIA Triton Server 上所需的代码和配置了。按照 *图 15**.1* 中的方式
    1，我们需要定义一个 Python 类，用于初始化并使用 Hugging Face 分词器和 TensorRT 引擎进行推理。让我们开始在这个部署 Python
    代码文件中导入必要的 Python 库：
- en: '[PRE7]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we must specify the model and path where we stored the serialized TensorRT
    model:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须指定模型和我们存储序列化 TensorRT 模型的路径：
- en: '[PRE8]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we must define the `TritonPythonModel` class interface, starting with
    the initialization method:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须定义 `TritonPythonModel` 类接口，从初始化方法开始：
- en: '[PRE9]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The class name must be the same as the initialization method name – that is,
    `initialize`. This code loads the pre-trained tokenizer from the Hugging Face
    library methods and downloads the tokenizer from the internet. Note that for production
    deployment of a model, it is advised to have a managed instance of the tokenizer
    weights or any model weights somewhere to ensure a reliable deployment process.
    Additionally, the code loads the serialized TensorRT engine.
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 类名必须与初始化方法名相同，即 `initialize`。此代码从 Hugging Face 库方法中加载预训练的分词器，并从互联网下载分词器。需要注意的是，对于生产环境中部署的模型，建议将分词器权重或任何模型权重的管理实例存储在某个位置，以确保可靠的部署过程。此外，代码加载了序列化的
    TensorRT 引擎。
- en: 'Next, we need to define the actual inference part of the tokenizer and model,
    as follows:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义分词器和模型的实际推理部分，如下所示：
- en: '[PRE10]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code should live under the Python code `model.py` file under the `models/transformer_tensorrt_text_generation/1/model.py`
    path.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码应位于 `models/transformer_tensorrt_text_generation/1/model.py` 路径下的 Python
    代码 `model.py` 文件中。
- en: The folder named `1` is to symbolize the version of the `transformer_tensorrt_text_generation`
    model name.
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 名为 `1` 的文件夹是为了表示 `transformer_tensorrt_text_generation` 模型名称的版本。
- en: 'The final file we need is a configuration file that specifies the name of the
    model, the max batch size of the model, the backend type of the model (in this
    case, Python), the input type, the name and dimensions of the model, the output
    type, the name and dimensions of the model, the number of instances of this pipeline,
    and finally whether to use GPU or CPU. The file needs to be named `config.pbtxt`.
    The content of this file for our usage is as follows:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要的最终文件是一个配置文件，该文件指定了模型的名称、模型的最大批量大小、模型的后端类型（在本例中是 Python）、输入类型、模型的名称和维度、输出类型、模型的名称和维度、该管道的实例数量，以及是否使用
    GPU 或 CPU。该文件需要命名为 `config.pbtxt`。我们使用的该文件内容如下：
- en: '[PRE11]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This should be stored under the following file path:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应该将其存储在以下文件路径下：
- en: '[PRE12]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we have all the code and configuration needed to run NVIDIA Triton Server
    and deploy our language model, which is an easy-to-use `nvidia-docker`-based deployment
    with a publicly available and downloadable image. The language model can be deployed
    on NVIDIA Triton Server by executing the following command:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们有了运行 NVIDIA Triton Server 和部署我们的语言模型所需的所有代码和配置，这是一种基于 `nvidia-docker` 的易于使用的部署，且有公开可用和可下载的镜像。可以通过执行以下命令将语言模型部署到
    NVIDIA Triton Server：
- en: '[PRE13]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: import tritonclient.http as httpclient
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入tritonclient.http作为httpclient
- en: '[PRE14]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also need to specify the model name defined in the `config.pbtxt` file from
    *step 12*, along with the model version, as follows:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要指定在*第12步*中定义的`config.pbtxt`文件中的模型名称以及模型版本，如下所示：
- en: '[PRE15]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we will define the client using the `httpclient` helper tool, define the
    input data, configure the output data so that it’s obtained according to the `config.pbtxt`
    specified output name, and print the generated text:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用`httpclient`助手工具定义客户端，定义输入数据，配置输出数据，以便根据`config.pbtxt`指定的输出名称获取，并打印生成的文本：
- en: '[PRE16]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running `python triton_client.py` in the command line will return the following
    response:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中运行`python triton_client.py`将返回以下响应：
- en: '[PRE17]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: And with that, we are done with the practical tutorial!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们完成了实践教程！
- en: This topic serves to show the minimal workflow needed to deploy a language model
    with acceleration with NVIDIA Triton Server, which is not too different from an
    audio DL model or a computer vision DL model. Try the workflow presented here
    using other language models and try to play around with all the settings! Note
    that there can be some issues with either the conversion or the optimization stage
    due to highly custom layers from new language models, so you will either need
    to work on fixing it in the base libraries themselves or raise it to the respective
    teams and wait for it to be upgraded.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 本主题旨在展示使用NVIDIA Triton Server加速部署语言模型所需的最小工作流，这与音频DL模型或计算机视觉DL模型并无太大区别。尝试使用这里展示的工作流与其他语言模型进行实验，并尝试调整所有设置！请注意，由于新语言模型中的高度自定义层，转换或优化阶段可能会遇到一些问题，因此你可能需要在基础库中修复这些问题，或者将其提交给相关团队并等待更新。
- en: 'When deploying specifically with language models, there are a few more tools
    that can be used for deployment that might be worth considering due to their high-level
    abstraction of the Hugging Face models and out-of-the-box official support of
    selected LLMs. These are as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在专门使用语言模型进行部署时，还有一些额外的工具可以用于部署，它们由于对Hugging Face模型的高层次抽象以及对选定LLM的开箱即用官方支持，可能值得考虑。具体如下：
- en: '`baichuan-inc/Baichuan-7B`), BLOOM (`bigscience/bloom` and `bigscience/bloomz`),
    GPT-2 (`gpt2` and `gpt2-xl`), GPT BigCode (`bigcode/starcoder` and `bigcode/gpt_bigcode-santacoder`),
    GPT-J (`EleutherAI/gpt-j-6b` and `nomic-ai/gpt4all-j`), GPT-NeoX (`EleutherAI/gpt-neox-20b`,
    `databricks/dolly-v2-12b`, and `stabilityai/stablelm-tuned-alpha-7b`), LLaMA and
    LLaMA-2 (`meta-llama/Llama-2-70b-hf`, `lmsys/vicuna-13b-v1.3`, `young-geng/koala`,
    and `openlm-research/open_llama_13b`), MPT (`mosaicml/mpt-7b` and `mosaicml/mpt-30b`),
    and OPT (`facebook/opt-66b` and `facebook/opt-iml-max-30b`)'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`baichuan-inc/Baichuan-7B`）、BLOOM（`bigscience/bloom` 和 `bigscience/bloomz`）、GPT-2（`gpt2`
    和 `gpt2-xl`）、GPT BigCode（`bigcode/starcoder` 和 `bigcode/gpt_bigcode-santacoder`）、GPT-J（`EleutherAI/gpt-j-6b`
    和 `nomic-ai/gpt4all-j`）、GPT-NeoX（`EleutherAI/gpt-neox-20b`、`databricks/dolly-v2-12b`
    和 `stabilityai/stablelm-tuned-alpha-7b`）、LLaMA和LLaMA-2（`meta-llama/Llama-2-70b-hf`、`lmsys/vicuna-13b-v1.3`、`young-geng/koala`
    和 `openlm-research/open_llama_13b`）、MPT（`mosaicml/mpt-7b` 和 `mosaicml/mpt-30b`）、OPT（`facebook/opt-66b`
    和 `facebook/opt-iml-max-30b`）'
- en: '**CTranslate2** (https://github.com/OpenNMT/CTranslate2): This boasts efficient
    inference with support of the following models:'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CTranslate2**（https://github.com/OpenNMT/CTranslate2）：这款工具以高效的推理性能为特点，支持以下模型：'
- en: '**Encoder-decoder models**: Transformer base/big, M2M-100, NLLB, BART, mBART,
    Pegasus, T5, and Whisper'
  id: totrans-357
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器-解码器模型**：Transformer基础/大模型、M2M-100、NLLB、BART、mBART、Pegasus、T5和Whisper'
- en: '**Decoder-only models**: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, CodeGen,
    GPTBigCode, and Falcon'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅解码器模型**：GPT-2、GPT-J、GPT-NeoX、OPT、BLOOM、MPT、Llama、CodeGen、GPTBigCode和Falcon'
- en: '**Encoder-only** **models**: BERT'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅编码器模型**：BERT'
- en: '**Text-generation-interface** (https://github.com/huggingface/text-generation-inference):
    This is not as efficient without an accelerator but it provides manually performed
    offline optimizations for the following models: BLOOM, FLAN-T5, Galactica, GPT-Neox,
    Llama, OPT, SantaCoder, Starcoder, Falcon 7B, Falcon 40B, MPT, and Llama V2'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成接口**（https://github.com/huggingface/text-generation-inference）：如果没有加速器，这个工具效率较低，但它为以下模型提供了手动执行的离线优化：BLOOM、FLAN-T5、Galactica、GPT-Neox、Llama、OPT、SantaCoder、Starcoder、Falcon
    7B、Falcon 40B、MPT和Llama V2'
- en: '**OpenLLM** ([https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)):
    This boasts integration with Langchain and Hugging Face agents but without using
    acceleration/compiler libraries'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenLLM**（[https://github.com/bentoml/OpenLLM](https://github.com/bentoml/OpenLLM)）：这款工具集成了Langchain和Hugging
    Face代理，但没有使用加速器/编译器库'
- en: '**Mlc-llm** ([https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)):
    This boasts support on a variety of devices, such as mobile phones'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mlc-llm** ([https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm))：这支持多种设备，如手机'
- en: As a final point, the practical deployment example presented here wouldn’t be
    as effective without us following the recommendations and guidelines presented
    in the previous topic, so be sure to follow through with every one of them!
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点，如果我们没有遵循上一个主题中提出的建议和指南，这里呈现的实际部署示例将不会如此有效，因此请务必全面执行每一个建议！
- en: Summary
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the various aspects of deploying DL models in production
    environments, focusing on key components, requirements, and strategies. We discussed
    architectural choices, hardware infrastructure, model packaging, safety, trust,
    reliability, security, authentication, communication protocols, user interfaces,
    monitoring, and logging components, along with continuous integration and deployment.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在生产环境中部署深度学习模型的各个方面，重点关注了关键组件、需求和策略。我们讨论了架构选择、硬件基础设施、模型打包、安全性、信任度、可靠性、安全性、认证、通信协议、用户界面、监控和日志记录组件，以及持续集成和部署。
- en: This chapter also provided a step-by-step guide for choosing the right deployment
    options based on specific needs, such as latency, availability, scalability, cost,
    model hardware, data privacy, and safety requirements. We also explored general
    recommendations for ensuring model safety, trust, and reliability, optimizing
    model latency, and utilizing tools that simplify the deployment process.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还为根据特定需求选择正确部署选项提供了逐步指南，例如延迟、可用性、可扩展性、成本、模型硬件、数据隐私和安全需求。我们还探讨了确保模型安全性、信任度和可靠性的一般建议，优化模型延迟，并利用简化部署流程的工具。
- en: A practical tutorial on deploying a language model with ONNX, TensorRT, and
    NVIDIA Triton Server was presented, showcasing a minimal workflow needed for accelerated
    deployment using NVIDIA Triton Server.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ONNX、TensorRT 和 NVIDIA Triton Server 部署语言模型的实际教程展示了加速部署所需的最小工作流程，展示了使用 NVIDIA
    Triton Server 的加速部署的实例。
- en: By understanding and implementing the strategies and best practices presented
    in this chapter, you can successfully deploy DL models in production with the
    most sensible choice for each component required and unlock their full potential.
    To build on this success path, we need to make sure we don’t forget about our
    model after we deploy it and always consider monitoring our deployed model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解并实施本章提出的策略和最佳实践，您可以成功地在生产环境中部署深度学习模型，并为每个所需组件的最明智选择发挥其全部潜力。要在成功的道路上进一步发展，我们需要确保在部署模型后不要忘记我们的模型，并始终考虑监控我们部署的模型。
- en: In the next chapter, we will dive into the many aspects of monitoring that we
    need to consider to ensure the continued success of our machine learning use case.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨我们需要考虑的监控许多方面，以确保我们机器学习用例的持续成功。
