- en: Chapter 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章
- en: Bayesian Inference in the Age of Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习时代的贝叶斯推理
- en: Over the last fifteen years, **machine learning** (**ML**) has gone from a relatively
    little-known field to a buzzword in the tech community. This is due in no small
    part to the impressive feats of **neural networks** (**NNs**). Once a niche underdog
    in the field, **deep learning**’s accomplishments in almost every conceivable
    application have resulted in a near-meteoric rise in its popularity. Its success
    has been so pervasive that, rather than being impressed by features afforded by
    deep learning, we’ve come to *expect* them. From applying filters in social networking
    apps, through to relying on Google Translate when on vacation abroad, it’s undeniable
    that deep learning is now well and truly embedded in the technology landscape.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十五年里，**机器学习**（**ML**）从一个相对不为人知的领域，迅速成为科技圈的流行词。这在很大程度上归功于**神经网络**（**NNs**）的惊人成就。曾经是该领域中的一个冷门角色，**深度学习**在几乎每个可想象的应用中的成就，导致其人气的迅速飙升。它的成功如此广泛，以至于我们不再因深度学习所带来的特性而感到惊讶，反而开始*期望*它们的出现。从社交应用中的滤镜，到在海外度假时依赖谷歌翻译，深度学习无疑已经深深嵌入到技术领域中。
- en: But, despite all of its impressive accomplishments, and the variety of products
    and features it’s afforded us, deep learning has not yet surmounted its final
    hurdle. As sophisticated neural networks are increasingly applied in mission-critical
    and safety-critical applications, the questions around their robustness become
    more and more pertinent. The black-box nature of many deep learning algorithms
    makes them daunting candidates for safety-savvy solutions architects - so much
    so that many would prefer sub-standard performance over the potential risks of
    an opaque system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习取得了令人印象深刻的成就，并为我们带来了丰富的产品和特性，但它仍然没有跨越最后的障碍。随着复杂的神经网络越来越多地应用于关键任务和安全任务，关于它们稳健性的疑问变得越来越重要。许多深度学习算法的“黑箱”特性使得它们成为安全意识强的解决方案架构师的难题——以至于许多人宁愿接受次优的表现，也不愿承担使用不透明系统的潜在风险。
- en: So, how can we conquer the apprehension surrounding deep learning and ensure
    that we create more robust, trustworthy models? While some of the answers to this
    lie down the path of **explainable artificial intelligence** (**XAI**), an important
    building block lies in the field of **Bayesian deep** **learning** (**BDL**).
    Through this book, you will discover the fundamental principles behind BDL through
    practical examples, allowing you to develop a strong understanding of the field,
    and equipping you with the knowledge and tools you need to build your own BDL
    models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何克服围绕深度学习的疑虑，确保构建更加稳健、值得信赖的模型呢？虽然一些答案可以从**可解释人工智能**（**XAI**）的路径中找到，但一个重要的构建块在于**贝叶斯深度学习**（**BDL**）领域。通过本书，你将通过实际示例了解BDL背后的基本原理，帮助你深入理解该领域，并为你提供所需的知识和工具，以构建自己的BDL模型。
- en: But, before we get started, let’s delve deeper into the justifications of BDL,
    and why typical deep learning methods may not be as robust as we’d like. In this
    chapter, we’ll learn about some of the key successes and failures of deep learning,
    and how BDL can help us to avoid the potentially tragic consequences of standard
    deep models. We’ll then outline the core topics of the rest of the book, before
    introducing you to the libraries and data that we’ll be using in practical examples.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 但在开始之前，让我们更深入地探讨贝叶斯深度学习（BDL）的理论依据，为什么典型的深度学习方法可能没有我们想象中的那么稳健。在本章中，我们将了解深度学习的一些关键成功与失败案例，以及BDL如何帮助我们避免标准深度模型可能带来的悲剧性后果。然后，我们将概述本书其余章节的核心内容，并介绍我们将在实际示例中使用的库和数据。
- en: 'These topics will be covered in the following sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主题将在接下来的章节中讨论：
- en: Wonders of the deep learning age
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习时代的奇迹
- en: Understanding the limitations of deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习的局限性
- en: Core topics
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心主题
- en: Setting up the work environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置工作环境
- en: 1.1 Technical requirements
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 技术要求
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)。
- en: 1.2 Wonders of the deep learning age
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 深度学习时代的奇迹
- en: Over the last 10 to 15 years, we’ve seen a dramatic shift in the landscape of
    ML thanks to the enormous success of deep learning. Perhaps one of the most impressive
    feats of the universal impact of deep learning is that it has affected fields
    from medical imaging and manufacturing all the way through to tools for translation
    and content creation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的10到15年里，由于深度学习的巨大成功，我们见证了机器学习领域的剧变。深度学习影响力最广泛的成就之一，或许就是它已经渗透到从医学影像学、制造业到翻译工具和内容创作等各个领域。
- en: 'While deep learning has only seen great success over recent years, many of
    its core principles are already well established. Researchers have been working
    with neural networks for some time – in fact, one could argue that the first neural
    network was introduced by Frank Rosenblatt as early as 1957! This, of course,
    wasn’t as sophisticated as the models we have today, but it was an important component
    of these models: the perceptron, as shown in *Figure* *1.1*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在最近几年才取得了巨大成功，但它的许多核心原理已经得到了很好的确立。研究人员已经在神经网络领域工作了一段时间——实际上，早在1957年，Frank
    Rosenblatt就提出了第一个神经网络！当然，这比我们今天的模型要简单得多，但它是这些模型的重要组成部分：感知器，如*图1.1*所示。
- en: '![PIC](img/perceptron_diagram.JPG)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/perceptron_diagram.JPG)'
- en: 'Figure 1.1: Diagram of a single perceptron'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：单一感知器的示意图
- en: 'The 1980s saw the introduction of many now-familiar concepts, with the introduction
    of **convolutional neural networks** (**CNNs**) by Kunihiko Fukushima in 1980,
    and the development of the **recurrent neural network** (**RNN**) by John Hopfield
    in 1982\. The 1980s and 1990s saw further maturation of these technologies: Yann
    LeCun famously applied back-propagation to create a CNN capable of recognizing
    hand-written digits in 1989, and the crucial concept of long short-term memory
    RNNs was introduced by Hochreiter and Schmidhuber in 1997.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1980年代引入了许多如今已熟知的概念，其中包括Kunihiko Fukushima于1980年提出的**卷积神经网络**（**CNNs**），以及John
    Hopfield于1982年开发的**递归神经网络**（**RNN**）。1980年代和1990年代，这些技术进一步成熟：Yann LeCun在1989年将反向传播应用于创建能够识别手写数字的CNN，而长短期记忆（LSTM）RNN的关键概念则由Hochreiter和Schmidhuber于1997年提出。
- en: But, while we had the foundation of today’s powerful models before the turn
    of the century, it wasn’t until the introduction of modern GPUs that the field
    really took off. With the introduction of accelerated training and inference afforded
    by GPUs, it became possible to develop networks with dozens (or even hundreds)
    of layers. This opened the door to incredibly sophisticated neural network architectures
    capable of learning compact feature representations of complex, high-dimensional
    data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管在世纪之交之前我们已经有了今天强大模型的基础，直到现代GPU的引入，整个领域才真正蓬勃发展。随着GPU带来的加速训练和推理，开发包含几十层（甚至几百层）网络成为可能。这为非常复杂的神经网络架构开辟了大门，这些架构能够学习高维数据的紧凑特征表示。
- en: '![PIC](img/file4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file4.png)'
- en: 'Figure 1.2: Diagram of AlexNet'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：AlexNet架构图
- en: One of the first highly influential network architectures was AlexNet. This
    network, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, comprised
    11 layers and was capable of classifying images into one of 1,000 possible classes.
    It achieved unprecedented performance on the ImageNet Large Scale Visual Recognition
    Challenge in 2012, illustrating the power of deep networks. AlexNet was the first
    of an array of influential neural network architectures, and the following years
    saw the introduction of many now-familiar architectures, including VGG Net, the
    Inception architectures, ResNet, EfficientNet, YOLO... The list goes on!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最早具有重大影响的网络架构之一是AlexNet。这一网络由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton开发，包含11层，能够将图像分类为1000种可能的类别之一。它在2012年的ImageNet大规模视觉识别挑战赛中取得了前所未有的成绩，展示了深度网络的强大能力。AlexNet是许多具有影响力的神经网络架构中的第一个，接下来的几年里，许多现在熟知的架构相继问世，包括VGG
    Net、Inception架构、ResNet、EfficientNet、YOLO……这个列表还在继续！
- en: But NNs weren’t just successful in computer vision applications. In 2014, work
    by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio demonstrated that end-to-end
    NN models could be used to obtain state-of-the-art results in machine translation.
    This was a watershed moment for the field, and large-scale machine translation
    services quickly adopted these end-to-end networks, spurring further advancements
    in natural language processing. Fast-forward to today, and these concepts have
    matured to produce the **transformer** architecture – an architecture that has
    had a dramatic effect on deep learning through its ability to learn rich feature
    embeddings through self-supervised learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是神经网络不仅仅在计算机视觉应用中取得了成功。2014年，Dzmitry Bahdanau、Kyunghyun Cho和Yoshua Bengio的研究表明，端到端神经网络模型可以用于在机器翻译中取得最先进的成果。这是该领域的一个分水岭时刻，随后大规模的机器翻译服务迅速采用了这些端到端网络，进一步推动了自然语言处理领域的发展。时至今日，这些概念已经发展成熟，产生了**Transformer**架构——一种通过自监督学习获取丰富特征嵌入能力的架构，极大地推动了深度学习的发展。
- en: With the impressive flexibility granted to them by the wide variety of architectures,
    neural networks have now achieved state-of-the-art performance in applications
    across almost every conceivable field, and they’re now a familiar part of our
    daily lives. Whether it’s the facial recognition we use on our mobile devices,
    translation services such as Google Translate, or speech recognition in our smart
    devices, it’s clear that these networks are not just competitive in image classification
    challenges, they’re now an important part of the technologies we’re developing,
    and they’re even capable of *outperforming* *humans*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 借助各种架构赋予的令人印象深刻的灵活性，神经网络如今已在几乎每个可想象的领域的应用中达到了最先进的性能，并且它们已成为我们日常生活中熟悉的一部分。无论是我们在移动设备上使用的面部识别，还是像谷歌翻译这样的翻译服务，亦或是智能设备中的语音识别，显然这些网络不仅在图像分类挑战中具有竞争力，它们现在是我们开发的技术中不可或缺的一部分，甚至能够*超越*
    *人类*。
- en: While reports of deep learning models outperforming human experts are becoming
    more and more frequent, the most profound examples are perhaps those in medical
    imaging. In 2020, a network developed by researchers at Imperial College London
    and Google Health outperformed six radiologists when detecting breast cancer from
    mammograms. A few months later, a study from February 2021 demonstrated that a
    deep learning model was able to outperform two human experts in diagnosing gallbladder
    disorders. Another study published later that year showed that a CNN outperformed
    157 dermatologists in detecting melanoma from images of skin abnormalities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习模型超越人类专家的报道越来越频繁，最深刻的例子或许是在医学影像领域。2020年，由伦敦帝国学院和谷歌健康研究人员开发的一个网络在从乳腺X光片中检测乳腺癌时超越了六位放射科医生。几个月后，2021年2月的一项研究表明，一个深度学习模型能够超越两位人类专家，诊断胆囊疾病。另一项在同年晚些时候发布的研究显示，一个卷积神经网络（CNN）在检测皮肤异常图像中的黑色素瘤时超越了157位皮肤科医生。
- en: All of the applications we’ve discussed so far have been supervised applications
    of ML, in which models have been trained for classification or regression problems.
    However, some of the most impressive feats of deep learning are found in other
    applications, including generative modeling and reinforcement learning. Perhaps
    one of the most famous examples of the latter is **AlphaGo**, a reinforcement
    learning model developed by DeepMind. The algorithm, as indicated in its name,
    was trained to play the game Go via reinforcement learning. Unlike some games,
    such as chess, which can be solved via fairly straightforward artificial intelligence
    methods, Go is far more challenging from a computational standpoint. This is due
    to the sophisticated nature of the game – the many possible combinations of moves
    are difficult for more traditional approaches. Thus, when AlphaGo successfully
    beat Go champions Fan Hui and Lee Sedol, in 2015 and 2016 respectively, this was
    big news.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的所有应用都是监督式的机器学习应用，其中模型被训练用于分类或回归问题。然而，深度学习最令人印象深刻的成就之一出现在其他应用中，包括生成建模和强化学习。也许最著名的强化学习例子之一是**AlphaGo**，这是DeepMind开发的一个强化学习模型。顾名思义，这个算法通过强化学习训练来下围棋。与象棋等一些游戏不同，象棋可以通过相对直接的人工智能方法来解决，而围棋在计算上要复杂得多。这是因为围棋的复杂性——众多可能的着法组合使得传统方法难以应对。因此，当AlphaGo在2015年和2016年分别战胜围棋冠军范辉和李世石时，这成了轰动一时的新闻。
- en: DeepMind went on to further refine AlphaGo by creating a version which learned
    by playing games against itself – AlphaGo Zero. This model was superior to any
    previous model, achieving superhuman performance in Go. The algorithm at the core
    of its success, AlphaZero, went on to achieve superhuman performance in a range
    of other games, proving the algorithm’s ability to generalize to other applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind通过创建一个自我对弈的版本进一步改进了AlphaGo——AlphaGo Zero。这个模型优于任何以前的模型，在围棋中达到了超人类的表现。其成功的核心算法AlphaZero也在其他一系列游戏中实现了超人类的表现，证明了该算法能够推广到其他应用领域。
- en: Another significant milestone for deep learning over the last decade was the
    introduction of **Generative Adversarial Networks**, or **GANs**. GANs work by
    employing two networks. The goal of the first network is to generate data with
    the same statistical qualities as a training set. The goal of the second network
    is to classify the output of the first network, using what it has learned from
    the dataset. Because the first network is not trained directly on the data, it
    does not learn to simply replicate data – instead, it effectively learns to deceive
    the second network. This is why the term *adversarial* is used. Through this process,
    the first network is able to learn which kinds of outputs successfully deceive
    the second network, and thus is able to generate content that matches the data
    distribution.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 过去十年，深度学习的另一个重要里程碑是**生成对抗网络**（**GANs**）的出现。GAN通过使用两个网络来工作。第一个网络的目标是生成与训练集具有相同统计特征的数据。第二个网络的目标是根据从数据集中学到的内容对第一个网络的输出进行分类。因为第一个网络并没有直接在数据上进行训练，它不会简单地复制数据——而是通过学习欺骗第二个网络来工作。这就是为什么使用*对抗*这一术语的原因。通过这一过程，第一个网络能够学习哪些输出能够成功欺骗第二个网络，从而生成符合数据分布的内容。
- en: 'GANs can produce particularly impressive outputs. For example, the following
    image was generated by the StyleGAN2 model:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GAN能够生成特别令人印象深刻的输出。例如，以下图像是由StyleGAN2模型生成的：
- en: '![PIC](img/file5.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file5.jpg)'
- en: 'Figure 1.3: Face generated by StyleGAN2 from thispersondoesnotexist.com.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：由StyleGAN2从thispersondoesnotexist.com生成的面孔。
- en: But GANs aren’t just useful for generating realistic faces; they have practical
    applications in many other fields, such as suggesting molecular combinations for
    drug discovery. They are also a powerful tool for improving other ML methods through
    data augmentation – using the GAN-generated data to augment datasets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但GAN不仅仅用于生成真实的面孔；它们在许多其他领域也有实际应用，比如为药物发现建议分子组合。它们还是通过数据增强提升其他机器学习方法的强大工具——使用GAN生成的数据来扩充数据集。
- en: All these successes may make deep learning seem infallible. While its achievements
    are impressive, they don’t tell the whole story. In the next section, we’ll learn
    about some of deep learning’s failings, and start to understand how Bayesian approaches
    may help to avoid these in the future.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些成功可能让深度学习看起来无懈可击。虽然它的成就令人印象深刻，但它们并没有讲述整个故事。在下一节中，我们将了解深度学习的一些失败，并开始理解贝叶斯方法如何帮助我们避免这些问题。
- en: 1.3 Understanding the limitations of deep learning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 理解深度学习的局限性
- en: 'As we’ve seen, deep learning has achieved some remarkable feats, and it’s undeniable
    that it’s revolutionizing the way that we deal with data and predictive modeling.
    But deep learning’s short history also comprises darker tales: stories that bring
    with them crucial lessons for developing systems that are more robust, and, crucially,
    safer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，深度学习取得了一些显著的成就，不可否认它正在革新我们处理数据和预测建模的方式。但深度学习的短暂历史中也充满了更黑暗的故事：这些故事带来了重要的教训，帮助我们开发出更强大、更安全的系统。
- en: In this section, we’ll introduce a couple of key cases in which deep learning
    failed, and we will discuss how a Bayesian perspective could have helped to produce
    a better outcome.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些深度学习失败的关键案例，并讨论贝叶斯视角如何有助于产生更好的结果。
- en: 1.3.1 Bias in deep learning systems
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.1 深度学习系统中的偏差
- en: 'We’ll start with a textbook example of **bias**, a crucial problem faced by
    data-driven methods. This example centers around Amazon. Now a household name,
    the e-commerce company started out by revolutionizing the world of book retail,
    before becoming literally *the* one-stop shop for just about anything: from garden
    furniture to a new laptop, or even a home security system, if you can imagine
    it, you can probably purchase it on Amazon. The company has also been responsible
    for significant strides technologically, often as a means of improving its infrastructure
    in order to enable its expansion. From hardware infrastructure to theoretical
    and technological leaps in optimization methods, what started out as an e-commerce
    organization has now become one of the key figures in technology.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个教科书式的**偏差**例子开始，这是数据驱动方法面临的一个关键问题。这个例子围绕着亚马逊展开。如今已经是家喻户晓的名字，亚马逊最初通过彻底改变书籍零售的世界而起步，随后成为了几乎任何商品的*一站式购物平台*：从花园家具到新笔记本电脑，甚至是家庭安全系统，如果你能想到它，你大概可以在亚马逊上购买到。该公司还在技术上取得了显著进展，通常是通过改进基础设施以促进其扩展。从硬件基础设施到优化方法中的理论和技术飞跃，最初的电子商务公司如今已经成为技术领域的关键人物之一。
- en: 'While these technological leaps often set the standard for the industry, this
    example did the opposite: demonstrating a key weakness of data-driven methods.
    The case we’re referring to is that of Amazon’s AI recruiting software. With automation
    playing such a key role in so much of Amazon’s success, it made sense to expand
    this automation to reviewing resumes. In 2014, Amazon’s ML engineers deployed
    a tool to do just that. Trained on the previous 10 year’s worth of applicants,
    the tool was designed to learn to identify favorable traits from the company’s
    enormous pool of applicants. However, in 2015 it became clear that it had latched
    onto certain features that resulted in deeply undesirable behavior.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些技术飞跃通常会设定行业标准，但这个例子却恰恰相反：它展示了数据驱动方法的一个关键弱点。我们所指的案例是亚马逊的AI招聘软件。由于自动化在亚马逊成功中的关键作用，将这一自动化扩展到简历审查是合情合理的。2014年，亚马逊的机器学习工程师部署了一个工具来实现这一目标。该工具基于过去十年的申请者数据进行训练，旨在从公司庞大的申请者池中识别出有利的特征。然而，2015年时，大家发现它学会了依赖某些特征，从而导致了深具负面影响的行为。
- en: 'The issue was largely due to the underlying data: because of the nature of
    the tech industry at the time, Amazon’s dataset of resumes was dominated by male
    applicants. This resulted in tremendous inequity in the model’s predictions: it
    effectively learned to favor men, becoming hugely biased against female applicants.
    The discriminatory behavior of the model resulted in the project being abandoned
    by Amazon, and it now serves as a key example of bias for the AI community.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在很大程度上源于底层数据：由于当时科技行业的特性，亚马逊的简历数据集主要由男性申请者主导。这导致了模型预测中的巨大不平衡：它实际上学会了偏向男性，变得极其偏向男性申请者，严重歧视女性申请者。模型的歧视行为导致亚马逊放弃了这个项目，至今它已成为AI社区中偏差问题的一个重要案例。
- en: 'An important factor to consider in the problem presented here is that this
    bias isn’t just driven by *explicit* information, such as a person’s name (which
    could be a clue as to their gender): algorithms learn latent information, which
    can then drive bias. This means the problem can’t simply be solved by anonymizing
    people – it’s up to the engineers and scientists to ensure that bias is evaluated
    comprehensively so that the algorithms we deploy are fair. While Bayesian methods
    can’t make bias disappear, they present us with a range of tools that can help
    with these problems. As we’ll see later in the book, Bayesian methods give us
    the ability to determine whether data is in-distribution or **out-of-distribution**
    (**OOD**). In this case, Amazon could have used this capability of Bayesian methods:
    separating the OOD data and analyzing it to understand why it was OOD. Was it
    picking up on things that were relevant, such as applicants with the wrong kind
    of experience? Or was it picking up on something irrelevant and discriminatory,
    such as the applicant’s gender? This could have helped Amazon’s ML team to spot
    the undesirable behavior early, allowing them to develop an unbiased solution.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里提出的问题中，一个重要的因素是，这种偏见不仅仅是由*显式*信息驱动的，比如一个人的名字（这可能是性别的线索）：算法会学习潜在的信息，进而驱动偏见。这意味着，问题不能仅仅通过匿名化人们来解决——需要工程师和科学家确保对偏见进行全面评估，以确保我们部署的算法是公平的。尽管贝叶斯方法无法消除偏见，但它为我们提供了一系列工具，帮助我们解决这些问题。正如我们在本书后面将看到的，贝叶斯方法使我们能够确定数据是否处于分布内（in-distribution）还是**分布外**（**OOD**）。在这种情况下，亚马逊本可以利用贝叶斯方法的这一能力：将分布外（OOD）数据分离出来，并进行分析，了解为何这些数据是OOD的。是因为它关注到了一些相关因素，比如经验不匹配的申请者？还是关注到了某些无关且具有歧视性的因素，比如申请者的性别？这本可以帮助亚马逊的机器学习团队尽早发现不良行为，从而开发出无偏的解决方案。
- en: 1.3.2 The danger of over-confident predictions
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.2 过度自信预测的危险
- en: 'Another widely referenced example of a deep learning failure is illustrated
    in the paper *Robust Physical-World Attacks on Deep Learning Visual Classification*
    by Kevin Eykholt *et al.* ( [https://arxiv.org/abs/1707.08945](https://arxiv.org/abs/1707.08945)).
    This paper played an important role in highlighting the issue of **adversarial
    attacks** on deep learning models: slightly modifying input data so that the model
    produces an incorrect prediction. In one of the key examples from their paper,
    they stick white and black stickers to a stop sign. While the modifications to
    the sign were subtle, the computer vision model interpreted the modified sign
    as a Speed Limit 45 sign.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛引用的深度学习失败案例，出现在Kevin Eykholt *等人*的论文《深度学习视觉分类的稳健物理世界攻击》中（[https://arxiv.org/abs/1707.08945](https://arxiv.org/abs/1707.08945)）。这篇论文在揭示深度学习模型的**对抗攻击**问题上起到了重要作用：稍微修改输入数据，让模型做出错误预测。在他们论文中的一个关键例子中，他们在停车标志上粘贴了白色和黑色的贴纸。尽管对标志的修改非常微妙，计算机视觉模型却将修改后的标志误解为限速
    45 标志。
- en: '![PIC](img/adversarial_illustration.PNG)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/adversarial_illustration.PNG)'
- en: 'Figure 1.4: Illustration of the effect of a simple adversarial attack on a
    model interpreting a stop sign.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4：展示了一个简单对抗攻击对模型解读停车标志的影响。
- en: At first, this may seem inconsequential, but if we take a step back and consider
    the amount of work that Tesla, Uber, and others have dedicated towards self-driving
    cars, it’s easy to see how this sort of adversarial perturbation could lead to
    catastrophic consequences. In the case of this sign, this misclassification could
    lead to a self-driving car bypassing a stop sign, hurtling into traffic at an
    intersection. This would obviously not be good for the passengers or other road
    users. In fact, an incident not too dissimilar to what we’re describing here happened
    in 2016 when a Tesla Model S collided with a truck in northern Florida ( [https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC](https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC)).
    According to Tesla, the trailer being pulled by the truck wasn’t detected by Tesla’s
    autopilot as it couldn’t distinguish it from the backdrop of the bright sky behind
    the trailer. The driver also didn’t notice the trailer, ultimately resulting in
    a fatal collision. But what if the decision processes used by the autopilot were
    more sophisticated? One of the key themes throughout this book is that of making
    *robust* decisions with our ML systems, particularly in mission-critical or safety-critical
    applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，这看起来可能无关紧要，但如果我们退一步，考虑特斯拉、优步等公司在自动驾驶汽车方面投入的巨大工作，就不难看出这种对抗性扰动如何导致灾难性后果。在这个标志的案例中，误分类可能导致自动驾驶汽车忽略停车标志，冲入交叉路口的交通中。这显然对乘客或其他道路使用者来说都不好。事实上，2016年发生过一个与我们所描述的非常相似的事件，当时一辆特斯拉Model
    S在佛罗里达州北部与一辆卡车发生碰撞（[https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC](https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC)）。据特斯拉称，由于无法将卡车后面明亮的天空与拖车区分开来，特斯拉的自动驾驶系统没有检测到拖车。驾驶员也未能注意到拖车，最终导致了致命的碰撞。但如果自动驾驶系统的决策过程更为复杂呢？本书的一个关键主题是如何在我们的机器学习系统中做出*鲁棒*的决策，特别是在任务关键或安全关键的应用中。
- en: While this traffic sign example provides an intuitive illustration of the dangers
    associated with misclassifications, this applies to a vast range of other scenarios,
    from robotics equipment used for manufacturing through to automated surgical procedures.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个交通标志的例子直观地展示了误分类带来的危险，但这一点同样适用于广泛的其他场景，从用于制造的机器人设备到自动化手术程序。
- en: 'Having some idea of confidence (or uncertainty) is an important step towards
    improving the robustness of these systems and ensuring consistently safe behavior.
    In the case of the stop sign, simply having a model that ”knows when it doesn’t
    know” can prevent potentially tragic outcomes. As we’ll see later in the book,
    BDL methods allow us to detect adversarial inputs through their uncertainty estimates.
    In our self-driving car example, this could be incorporated in the logic so that,
    if the model is uncertain, the car safely comes to a stop, switching to manual
    mode to allow the driver to safely navigate the situation. This is the *wisdom*
    that comes with uncertainty-aware models: allowing us to design models that know
    their limitations, and thus are more robust in unexpected scenarios.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些系统而言，了解一定程度的信心（或不确定性）是提高其鲁棒性并确保一致性安全行为的重要步骤。在停车标志的案例中，拥有一个“知道自己不知道”的模型可以防止潜在的悲剧性后果。正如我们将在本书后面看到的，BDL方法通过不确定性估计可以帮助我们检测对抗性输入。在我们的自动驾驶汽车示例中，可以将其纳入逻辑中，以便在模型不确定时，汽车安全停车并切换到手动模式，让驾驶员能够安全地应对这一情况。这就是具有不确定性感知模型的*智慧*：让我们设计出了解自己局限性的模型，从而在意外情况下更加鲁棒。
- en: 1.3.3 Shifting trends
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3.3 趋势变化
- en: Our last examples look at the challenge of dealing with data that changes over
    time – a common problem in real-world applications. The first problem we’ll consider,
    typically referred to as **dataset shift** or **covariate shift**, occurs when
    the data encountered by a model at inference time changes relative to the data
    the model was trained on. This is often due to the dynamic nature of real-world
    problems and the fact that training sets – even very large training sets – rarely
    represent the total variation present in the phenomena they represent. An important
    example of this can be found in the paper *Systematic Review of Approaches to
    Preserve Machine Learning* *Performance in the Presence of Temporal Dataset Shift
    in Clinical Medicine*, in which Lin Lawrence Guo *et al.* highlight concerns around
    dataset shift ( [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/)).
    Their work shows that there is relatively little literature on tackling issues
    related to dataset shift in ML models applied in clinical settings. This is problematic
    because clinical data is dynamic. Let’s consider an example.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的例子探讨了应对数据随时间变化的挑战——这是现实世界应用中常见的问题。我们将考虑的第一个问题，通常被称为**数据集漂移**或**协变量漂移**，发生在模型推理时遇到的数据相对于训练时的数据发生了变化。这通常是由于现实世界问题的动态性以及训练集——即使是非常大的训练集——也很少能代表它们所代表现象的全部变化。一个重要的例子可以在论文《*Systematic
    Review of Approaches to Preserve Machine Learning* *Performance in the Presence
    of Temporal Dataset Shift in Clinical Medicine*》中找到，在这篇论文中，Lin Lawrence Guo *等*人强调了数据集漂移的问题（[https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/)）。他们的研究表明，关于如何解决临床环境中应用的机器学习模型中的数据集漂移问题的文献相对较少。这是一个问题，因为临床数据是动态的。让我们来看一个例子。
- en: 'In our example, we have a model that’s trained to automatically prescribe medication
    for a patient given their symptoms. A patient complains to a physician about respiratory
    symptoms, and the physician uses the model to prescribe medication. Because of
    the data presented to the model, it prescribes antibiotics. This works for many
    patients for a while, but over time something changes: a new disease becomes prevalent
    in the population. The new disease happens to have very similar symptoms to the
    bacterial infection that was going around previously, but this is caused by a
    virus. Because the model isn’t capable of adapting to dataset shift, it continues
    recommending antibiotics. Not only will they not help the patients, but it could
    contribute to antibiotic resistance within the local population.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们有一个模型，它被训练用来根据病人的症状自动开药。当病人向医生抱怨呼吸道症状时，医生使用该模型开药。由于模型接收到的数据，模型开出了抗生素。这个方法对许多病人有效，但随着时间的推移，情况发生了变化：一种新疾病在群体中变得流行。这种新疾病恰好与之前流行的细菌感染症状非常相似，但它是由病毒引起的。由于模型无法适应数据集的变化，它继续推荐抗生素。这不仅无法帮助病人，还可能导致局部群体中抗生素耐药性的产生。
- en: In order to be robust to these shifts in real-world data, models need to be
    sensitive to dataset shift. One way to do this is through the use of Bayesian
    methods, which provide uncertainty estimates. Applying this to our automatic prescriber
    example, the model becomes sensitive to small changes in the data when capable
    of producing uncertainty estimates. For example, there may be subtle differences
    in symptoms, such as a different type of cough, associated with our new viral
    infection. This will cause the uncertainty associated with the model predictions
    to rise, indicating that the model needs to be updated with new data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够应对现实世界数据中的这些变化，模型需要对数据集漂移保持敏感性。做到这一点的一种方法是通过使用贝叶斯方法，这些方法提供不确定性估计。将其应用到我们的自动开药示例中，当模型能够生成不确定性估计时，它对数据中的微小变化变得敏感。例如，可能会有一些微妙的症状差异，比如与我们新的病毒感染相关的不同类型的咳嗽。这将导致模型预测的不确定性上升，表明模型需要通过新数据进行更新。
- en: 'A related issue, referred to as **catastrophic forgetting**, is caused by models
    adapting to changes in data. Given our example, this sounds like a good thing:
    if models are adapting to changes in the data, then they’re always up to date,
    right? Unfortunately, it’s not quite so simple. Catastrophic forgetting occurs
    when models learn from new data, but ”forget” about past data in the process.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关问题，被称为**灾难性遗忘**，是由模型适应数据变化引起的。根据我们的例子，这听起来像是件好事：如果模型正在适应数据变化，那它们就永远是最新的，对吧？不幸的是，事情并没有那么简单。灾难性遗忘发生在模型从新数据中学习时，但在此过程中“忘记”了过去的数据。
- en: 'For example, say an ML algorithm is developed to identify fraudulent documents.
    It may work very well at first, but fraudsters will quickly notice that methods
    that used to fool automated document verification no longer work, so they develop
    new methods. While a few of these methods get through, the model – using its uncertainty
    estimates – notices that it needs to adapt to the new data. The model updates
    its dataset, focusing on the current popular attack methods, and runs a few more
    training iterations. Once again, it successfully thwarts the fraudsters, but,
    much to the surprise of the model’s designers, the model has started letting through
    older, less sophisticated attacks: attacks that used to be easy for it to identify.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设开发了一种机器学习算法来识别欺诈性文档。它可能一开始表现得非常好，但欺诈者很快会注意到曾经能够欺骗自动文档验证的方法不再有效，因此他们开发了新方法。虽然其中一些方法能成功突破，但模型通过其不确定性估计注意到它需要适应新数据。模型更新了其数据集，专注于当前流行的攻击方法，并进行几轮额外的训练。再次，它成功地阻止了欺诈者，但让模型设计师吃惊的是，模型开始放行一些较老、更不复杂的攻击：这些攻击曾经很容易被模型识别出来。
- en: In training on the new data, the model’s parameters have changed. Because there
    wasn’t sufficient support for the old data in the updated dataset, the model has
    lost information about old associations between the inputs (documents) and their
    classification (whether or not they’re fraudulent).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在新数据的训练中，模型的参数发生了变化。由于更新后的数据集中没有足够的旧数据支持，模型丧失了关于输入（文档）和其分类（是否欺诈）的旧关联信息。
- en: While this example used uncertainty estimates to tackle the issue of dataset
    shift, it could have further leveraged them to ensure that its dataset was balanced.
    This can be done using methods such as **uncertainty sampling**, which look to
    sample from uncertain regions, ensuring that the dataset used to train the model
    captures all available information from current and past data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个例子使用不确定性估计来解决数据集漂移问题，但它本可以进一步利用这些估计来确保数据集的平衡性。这可以通过**不确定性采样**等方法来实现，这些方法旨在从不确定区域进行采样，确保用于训练模型的数据集涵盖当前和过去数据中的所有可用信息。
- en: 1.4 Core topics
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 核心主题
- en: The aim of this book is to provide you with the tools and knowledge you need
    to develop your own BDL solutions. To this end, while we assume some familiarity
    with concepts of statistical learning and deep learning, we will still provide
    a refresher of these fundamental concepts.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的目标是为您提供开发自己BDL解决方案所需的工具和知识。为此，尽管我们假设您对统计学习和深度学习的概念有所了解，但我们仍然会提供这些基础概念的复习。
- en: In [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002),
    we’ll go over some of the key concepts from Bayesian inference, including probabilities
    and model uncertainty estimates. In [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003), we’ll cover important key aspects of
    deep learning, including learning via backpropagation, and popular varieties of
    NNs. With these fundamentals covered, we’ll start to explore BDL in [*Chapter 4*](CH4.xhtml#x1-490004),
    [*Introducing Bayesian Deep Learning*](CH4.xhtml#x1-490004). In *Chapters 5* and
    *6* we’ll delve deeper into BDL; we’ll first learn about principled methods, before
    going on to understand more practical methods for approximating Bayesian neural
    networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中，我们将回顾贝叶斯推断的一些关键概念，包括概率和模型不确定性估计。在[*第3章*](CH3.xhtml#x1-350003)，[*深度学习基础*](CH3.xhtml#x1-350003)中，我们将介绍深度学习的几个重要方面，包括通过反向传播学习以及神经网络的常见变种。在掌握这些基础后，我们将在[*第4章*](CH4.xhtml#x1-490004)，[*引入贝叶斯深度学习*](CH4.xhtml#x1-490004)中开始探索BDL。在*第5章*和*第6章*中，我们将深入探讨BDL；首先学习一些有原则的方法，然后继续了解更多用于逼近贝叶斯神经网络的实用方法。
- en: In [*Chapter 7*](CH7.xhtml#x1-1130007), [*Practical Considerations for Bayesian
    Deep Learning*](CH7.xhtml#x1-1130007), we’ll explore some practical considerations
    for BDL, helping us to understand how best to apply these methods to real-world
    problems. By [*Chapter 8*](CH8.xhtml#x1-1320008), [*Applying Bayesian* *Deep Learning*](CH8.xhtml#x1-1320008),
    we should have a strong understanding of the core BDL methods, and we’ll cement
    this with a number of practical examples. Finally, [*Chapter 9*](CH9.xhtml#x1-1780009),
    [*Next Steps in Bayesian Deep Learning*](CH9.xhtml#x1-1780009) will provide an
    overview of the current challenges within the field of BDL and give you an idea
    of where the technology is headed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 7 章*](CH7.xhtml#x1-1130007) [*贝叶斯深度学习的实际考虑*](CH7.xhtml#x1-1130007) 中，我们将探讨一些贝叶斯深度学习的实际应用考虑，帮助我们理解如何最好地将这些方法应用于现实问题。在
    [*第 8 章*](CH8.xhtml#x1-1320008) [*应用贝叶斯深度学习*](CH8.xhtml#x1-1320008) 中，我们应该已经对核心的贝叶斯深度学习方法有了扎实的理解，并通过一系列实际的示例进一步巩固这一知识。最后，在
    [*第 9 章*](CH9.xhtml#x1-1780009) [*贝叶斯深度学习的下一步*](CH9.xhtml#x1-1780009) 中，我们将概述贝叶斯深度学习领域当前面临的挑战，并对技术的发展方向有所了解。
- en: Throughout most of the book, the theory will be accompanied by hands-on examples,
    allowing you to develop a strong understanding by implementing these methods yourself.
    In order to follow these coding examples, you will need to have a Python environment
    set up with the necessary prerequisites. We’ll go over these in the next section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的大多数章节中，理论内容将配以实践示例，帮助你通过自己动手实现这些方法来加深理解。为了跟随这些编码示例，你需要设置好 Python 环境并安装所需的先决条件。我们将在下一节中介绍这些内容。
- en: 1.5 Setting up the work environment
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.5 设置工作环境
- en: To complete the practical elements of the book, you’ll need a Python 3.9 environment
    with the necessary prerequisites. We recommend using `conda`, a Python package
    manager specifically designed for scientific computing applications. To install
    `conda`, simply head to [https://conda.io/projects/conda/en/latest/user-guide/install/index.html](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)
    and follow the instructions for your operating system.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本书中的实践部分，你需要一个包含必要先决条件的 Python 3.9 环境。我们建议使用 `conda`，它是一个专为科学计算应用设计的 Python
    包管理器。要安装 `conda`，只需访问 [https://conda.io/projects/conda/en/latest/user-guide/install/index.html](https://conda.io/projects/conda/en/latest/user-guide/install/index.html)，并按照你的操作系统的安装说明进行操作。
- en: 'With `conda` installed, you can set up the `conda` environment that you’ll
    use for the book:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了 `conda` 后，你可以设置用于本书的 `conda` 环境：
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When you hit *Enter* to execute this command, you’ll be asked if you wish to
    continue installing the required packages; simply type `y` and hit **Enter**.
    `conda` will now proceed to install the core packages.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你按下 *Enter* 键执行此命令时，系统会询问是否继续安装所需的软件包；只需输入 `y` 并按 **Enter**。`conda` 将继续安装核心软件包。
- en: 'You can now activate your environment by typing the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以通过输入以下命令激活你的环境：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You’ll now see that your shell prompt contains `bdl`, indicating that your
    `conda` environment is active. Now you’re ready to install the prerequisites for
    the book. The key libraries required for the book are as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在会看到你的 shell 提示符中包含 `bdl`，这表示你的 `conda` 环境已激活。现在，你可以开始安装本书所需的先决条件了。本书所需的关键库如下：
- en: '**NumPy**: Numerical Python, or NumPy, is the core package for numerical programming
    in Python. You’re likely very familiar with this already.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**：数值 Python，或 NumPy，是 Python 中进行数值编程的核心库。你可能已经非常熟悉这个库了。'
- en: '**SciPy**: SciPy, or Scientific Python, provides the fundamental packages for
    scientific computing applications. The full scientific computing stack comprising
    SciPy, matplotlib, NumPy, and other libraries, is often referred to as the SciPy
    stack.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SciPy**：SciPy，或科学 Python，提供了科学计算应用程序所需的基础包。整个科学计算栈，包括 SciPy、matplotlib、NumPy
    和其他库，通常被称为 SciPy 栈。'
- en: '**scikit-learn**: This is the core Python machine learning library. Built on
    the SciPy stack, it provides easy-to-use implementations of many popular ML methods.
    It also provides a substantial number of helper classes and functions for data
    loading and processing, which we’ll use throughout the book.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scikit-learn**：这是核心的 Python 机器学习库。基于 SciPy 栈，它提供了许多流行机器学习方法的易用实现。它还提供了大量的辅助类和函数用于数据加载和处理，我们将在本书中多次使用。'
- en: '**TensorFlow**: TensorFlow, along with PyTorch and JAX, is one of the popular
    Python deep learning frameworks. It provides the tools necessary for developing
    deep learning models, and it will provide the foundation for many of the programming
    examples throughout the book.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**：TensorFlow 与 PyTorch 和 JAX 一起，是流行的 Python 深度学习框架之一。它提供了开发深度学习模型所需的工具，并将在本书中的许多编程示例中提供基础。'
- en: '**TensorFlow Probability**: Built on TensorFlow, TensorFlow Probability provides
    the tools necessary for working with probabilistic neural networks. We’ll be using
    this along with TensorFlow for many of the Bayesian neural network examples.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow Probability**：基于 TensorFlow，TensorFlow Probability 提供了处理概率神经网络所需的工具。我们将在本书中使用它与
    TensorFlow 一起进行许多贝叶斯神经网络的示例。'
- en: 'To install the full list of dependencies required for the book, with your `conda`
    environment activated, enter the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装本书所需的所有依赖项，请在激活 `conda` 环境后输入以下命令：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s summarize what we have learned.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下所学内容。
- en: 1.6 Summary
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.6 总结
- en: 'In this chapter, we’ve revisited the successes of deep learning, renewing our
    understanding of its enormous potential, and its ubiquity within today’s technology.
    We’ve also explored some key examples of its shortcomings: scenarios in which
    deep learning has failed us, demonstrating the potential for catastrophic consequences.
    While BDL can’t eliminate these risks, it can allow us to build more robust ML
    systems that incorporate both the flexibility of deep learning and the caution
    of Bayesian inference.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了深度学习的成功，重新认识了它巨大的潜力以及它在当今技术中的普遍存在。我们还探索了一些它不足之处的关键例子：深度学习未能解决的场景，展示了潜在的灾难性后果。虽然
    BDL 无法消除这些风险，但它可以帮助我们构建更强健的机器学习系统，结合深度学习的灵活性与贝叶斯推理的谨慎。
- en: In the next chapter, we’ll dive deeper into the latter as we cover some of the
    core concepts of Bayesian inference and probability, in preparation for our foray
    into BDL.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨贝叶斯推理和概率的一些核心概念，为我们进入 BDL 做准备。
