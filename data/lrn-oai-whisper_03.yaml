- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Diving into the Whisper Architecture
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 Whisper 架构
- en: As we embark on the third chapter of our journey into the world of OpenAI’s
    Whisper, we’ll delve deeper into the architectural intricacies that underpin this
    advanced ASR system. This chapter, aptly titled *Diving into the Whisper Architecture*,
    is designed to provide a comprehensive understanding of the transformer model
    that forms the backbone of Whisper.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们踏上深入 OpenAI Whisper 世界的第三章时，我们将进一步探讨支撑这一先进语音识别系统的架构细节。本章名为 *深入探讨 Whisper
    架构*，旨在全面了解构成 Whisper 背后的 Transformer 模型。
- en: The transformer model, a concept that has revolutionized the field of machine
    learning, is a critical component of Whisper’s architecture. It is the engine
    that drives the system’s ability to convert spoken language into written text
    accurately. Understanding the transformer model is akin to understanding the heart
    of Whisper, and this chapter aims to guide you through its complexities with clarity
    and precision.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型，一个彻底改变了机器学习领域的概念，是 Whisper 架构中的关键组件。它是驱动系统将口语语言准确转换为书面文本的引擎。理解
    Transformer 模型就像理解 Whisper 的核心，本章旨在通过清晰和精准的解释帮助你深入理解其复杂性。
- en: We’ll begin by introducing transformers and explaining their role and significance
    in the context of Whisper. We’ll provide a broad understanding of the model, setting
    the stage for a more detailed exploration of its mechanics. Then, we’ll delve
    into the encoder-decoder mechanics, a vital aspect of the transformer model. This
    section will elucidate how the model processes and transforms input data, providing
    you with insights into the inner workings of Whisper’s speech recognition capabilities.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍 Transformer 模型，并解释其在 Whisper 中的作用和意义。我们将提供对该模型的广泛理解，为更详细地探索其机制打下基础。接着，我们将深入分析编码器-解码器机制，这是
    Transformer 模型的一个重要方面。本节将阐明该模型如何处理和转换输入数据，帮助你深入了解 Whisper 的语音识别能力。
- en: As we navigate the architecture of Whisper, we’ll also discuss how the transformer
    model drives effective speech recognition. We’ll highlight the model’s role in
    enhancing the accuracy and efficiency of Whisper, providing you with a deeper
    understanding of how the system achieves its impressive performance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们导航 Whisper 架构的过程中，我们还将讨论 Transformer 模型如何推动有效的语音识别。我们将突出该模型在提高 Whisper 准确性和效率方面的作用，帮助你更深入理解系统如何实现其卓越的性能。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the transformer model in Whisper
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Whisper 中的 Transformer 模型
- en: Exploring the multitasking and multilingual capabilities of Whisper
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索 Whisper 的多任务和多语言能力
- en: Training Whisper with weak supervision on large-scale data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大规模数据上使用弱监督训练 Whisper
- en: Gaining insights into data, annotation, and model training
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解数据、注释和模型训练
- en: Integrating Whisper with other OpenAI technologies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Whisper 与其他 OpenAI 技术集成
- en: By the end of this chapter, you will have gained a comprehensive understanding
    of the transformer model and its role in Whisper. You will have delved into Whisper’s
    architecture, comprehending its encoder-decoder mechanics and how it drives effective
    speech recognition. This knowledge will help you better understand the subsequent
    chapters, where we’ll explore Whisper’s multitasking and multilingual capabilities,
    the methods of training Whisper with weak supervision on large-scale data, and
    integrating Whisper with other OpenAI technologies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将全面理解 Transformer 模型及其在 Whisper 中的作用。你将深入探索 Whisper 的架构，理解其编码器-解码器机制，以及它如何推动有效的语音识别。这些知识将帮助你更好地理解随后的章节，在这些章节中，我们将探讨
    Whisper 的多任务和多语言能力、在大规模数据上使用弱监督训练 Whisper 的方法，以及如何将 Whisper 与其他 OpenAI 技术集成。
- en: As we continue our journey into the world of Whisper, remember that understanding
    the architecture of an ASR system such as Whisper is about more than just comprehending
    its technical aspects. It’s about appreciating the transformative potential of
    such technologies. It’s about envisioning a future where voice technologies are
    deeply woven into the fabric of our daily lives, driving efficiency, accessibility,
    and innovation. So, as we dive into the architecture of Whisper, we’ll also ponder
    on the transformative potential of this technology and how we can harness it to
    shape a better future.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续探索Whisper的世界时，请记住，理解像Whisper这样的ASR系统的架构，不仅仅是理解其技术层面。它还包括欣赏这种技术的变革潜力。它是关于想象一个声音技术深深融入我们日常生活的未来，推动效率、可访问性和创新。因此，当我们深入探讨Whisper的架构时，我们也会思考这种技术的变革潜力，以及如何利用它塑造更美好的未来。
- en: In the words of the great architect Louis Kahn, “*A great building must begin
    with the unmeasurable, must go through measurable means when it is being designed,
    and in the end must be unmeasurable*.” Similarly, as we delve into the measurable
    aspects of Whisper’s architecture in this chapter, let’s keep sight of this technology’s
    unmeasurable potential. Let’s dive in!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 引用伟大的建筑师Louis Kahn的话：“*伟大的建筑必须从不可衡量的开始，在设计时必须通过可衡量的手段，而最终必须是不可衡量的*。”同样，正如我们在本章中深入研究Whisper架构的可衡量方面时，让我们保持对这项技术不可衡量潜力的关注。让我们一起深入探索！
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, we will leverage Google Colaboratory’s accessibility and economy.
    Whisper’s small model requires at least 12 GB of GPU memory. Thus, we must try
    to secure a decent GPU for our Colab! Unfortunately, accessing a good GPU with
    the free version of Google Colab (with the free version, we get a Tesla T4 16
    GB) is becoming much harder. However, with Google Colab Pro, we should have no
    issues in being allocated a V100 or P100 GPU.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将利用Google Colaboratory的可访问性和经济性。Whisper的小型模型需要至少12 GB的GPU内存。因此，我们必须尽力确保为我们的Colab获得一个不错的GPU！不幸的是，使用Google
    Colab的免费版（免费版提供的是特斯拉T4 16 GB）越来越难以获取一个好的GPU。然而，使用Google Colab Pro，我们应该不会遇到问题，可以分配到V100或P100
    GPU。
- en: To get a GPU, within Google Colab’s main menu, click **Runtime** | **Change
    runtime type**, then change the **Hardware accelerator** from **None** to **GPU**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取GPU，在Google Colab的主菜单中，点击**运行时** | **更改运行时类型**，然后将**硬件加速器**从**无**更改为**GPU**。
- en: 'We can verify that we’ve been assigned a GPU and view its specifications by
    running the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过运行以下代码来验证是否已分配GPU，并查看其规格：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s the output:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 3.1 – Example of the output from gpu_info in Google Colab](img/B21020_03_1.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – Google Colab中gpu_info输出的示例](img/B21020_03_1.jpg)'
- en: Figure 3.1 – Example of the output from gpu_info in Google Colab
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – Google Colab中gpu_info输出的示例
- en: Of course, feel free to run in your preferred environment. A Jupyter notebook
    and link to Google Colab can be found at [https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter03](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter03).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，可以在你喜欢的环境中运行。Jupyter笔记本和Google Colab的链接可以在[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter03](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter03)找到。
- en: 'The notebook for this chapter serves as an essential companion. It’s designed
    not merely as a supplement but as an integral part of your learning journey through
    Whisper’s world. This notebook offers a hands-on exploration of how to work with
    audio data while leveraging the Hugging Face ecosystem, which is foundational
    for anyone looking to implement Whisper effectively. The notebook encompasses
    the following key learning objectives:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的笔记本是一个重要的伴侣。它不仅仅作为补充，而是作为你学习Whisper世界过程中不可或缺的一部分。这个笔记本提供了如何处理音频数据并利用Hugging
    Face生态系统的实践探索，任何希望有效实现Whisper的人都需要掌握这些内容。该笔记本涵盖了以下关键学习目标：
- en: An introduction to handling audio data with Hugging Face, showcasing how theoretical
    concepts from this chapter translate into practical coding exercises
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍如何使用Hugging Face处理音频数据，展示本章的理论概念如何转化为实际的编码练习。
- en: Demonstrating basic audio processing techniques, such as loading, playing, and
    visualizing audio files – skills crucial for anyone working with Whisper or any
    ASR technology
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演示基本的音频处理技巧，如加载、播放和可视化音频文件——这些技巧对任何使用Whisper或任何自动语音识别（ASR）技术的人来说都至关重要。
- en: Preliminary steps toward more advanced applications, including the preprocessing
    necessary for fine-tuning Whisper models – a topic that will be expanded upon
    in [*Chapter 4*](B21020_04.xhtml#_idTextAnchor113)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向更高级应用的初步步骤，包括微调Whisper模型所需的预处理——这一话题将在[*第4章*](B21020_04.xhtml#_idTextAnchor113)中进一步展开。
- en: Through this notebook, you’ll gain practical experience that complements the
    theoretical knowledge from this chapter and prepares you for the more advanced
    techniques of fine-tuning Whisper models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本笔记本，您将获得与本章理论知识互补的实践经验，为更高级的Whisper模型微调技术打下基础。
- en: Understanding the transformer model in Whisper
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Whisper中的transformer模型
- en: In this section, we’ll explore how the transformer model empowered a breakthrough
    in NLP and understand its mechanics, enabling Whisper to accurately transform
    spoken utterances into written phrases. We’ll walk through the specifics of its
    encoder-decoder structure, along with its optimizations, making it unmatched for
    speech processing tasks. By the end, we’ll have insight into the inner workings
    of this advanced model architecture, comprehending how it drives Whisper’s prowess
    and unlocking applications across languages.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索transformer模型如何在自然语言处理（NLP）领域带来突破，并理解其机制，使得Whisper能够准确地将口语转换为书面语言。我们将详细介绍其编码器-解码器结构以及优化方法，使其在语音处理任务中无可匹敌。通过学习，我们将深入了解这一先进模型架构的内部工作原理，理解它如何驱动Whisper的强大功能，并解锁跨语言的应用。
- en: Introducing the transformer model
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入transformer模型
- en: 'As an expert in OpenAI’s Whisper, I am often asked, “What makes this **ASR**
    system so advanced?” The answer lies in its backbone: the pioneering transformer
    model architecture. It all started with the paper *Attention Is All You Need*,
    by Vaswani et al., published in 2017\. Introducing the transformer model marked
    a significant paradigm shift in NLP. Before this, the dominant models for sequence
    transduction, or converting sequences from one domain to another, were based on
    RNNs, including LSTM networks and CNNs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为OpenAI Whisper的专家，我经常被问到：“是什么让这个**ASR**系统如此先进？”答案就在于它的核心：开创性的transformer模型架构。一切始于2017年Vaswani等人发表的论文《Attention
    Is All You Need》。引入transformer模型标志着NLP领域的一个重要范式转变。在此之前，序列转换模型（即将一个领域的序列转换为另一个领域的序列）主要基于RNN，包括LSTM网络和CNN。
- en: RNNs and LSTMs process data sequentially, allowing them to maintain a form of
    memory by passing information from one sequence step to the next. However, they
    have limitations, such as difficulty parallelizing the operations (since each
    step depends on the previous one) and difficulty learning long-range dependencies
    within sequences due to problems such as vanishing gradients.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RNN和LSTM按顺序处理数据，它们通过将信息从一个序列步骤传递到下一个步骤来保持某种形式的记忆。然而，它们也有局限性，例如操作难以并行化（因为每一步都依赖于前一步）以及由于梯度消失等问题，难以学习序列中的长程依赖关系。
- en: The transformer model introduced a new architecture that relies entirely on
    attention mechanisms, dispensing with recurrence and convolutions. This was a
    significant departure from the previous paradigms, which often used complex arrangements
    of RNNs or CNNs with attention mechanisms to connect the encoder and decoder components
    of the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: transformer模型引入了一种全新的架构，它完全依赖于注意力机制，摒弃了循环和卷积操作。这是与之前的范式的显著区别，后者通常使用复杂的RNN或CNN与注意力机制相结合来连接模型的编码器和解码器组件。
- en: The attention mechanism allows the transformer model to focus on different parts
    of the input sequence when predicting each part of the output sequence, effectively
    capturing the input *context* regardless of its position. This is particularly
    important for tasks such as translation, where the relevance of a word can depend
    heavily on words elsewhere in the sentence.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制使得transformer模型在预测输出序列的每一部分时能够聚焦于输入序列的不同部分，从而有效地捕捉输入的*上下文*，无论它的位置如何。这对于像翻译这样的任务尤其重要，因为一个单词的相关性可能会严重依赖于句子中其他地方的单词。
- en: The transformer’s self-attention mechanism enables it to weigh the relevance
    of each part of the input sequence when producing the output, which is crucial
    for interpreting spoken language correctly. This allows the model to process all
    parts of the input sequence in parallel, significantly improving training efficiency
    and the ability to learn long-range dependencies more effectively. To illustrate
    this, let’s consider a practical example of a sentence translation task. Suppose
    we have the sentence, “I arrived at the bank after crossing the river.” In this
    context, the word “bank” refers to the edge of a river. However, “bank” can also
    mean a financial institution. The correct interpretation of “bank” depends on
    its context within the sentence, specifically the presence of the word “river.”
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的自注意力机制使得它在生成输出时，能够衡量输入序列每个部分的相关性，这对于正确解读口语语言至关重要。这使得模型能够并行处理输入序列的所有部分，显著提高训练效率，并更有效地学习长程依赖关系。为了说明这一点，我们可以考虑一个实际的句子翻译任务。假设我们有句子：“I
    arrived at the bank after crossing the river.” 在这个上下文中，单词“bank”指的是河岸。然而，“bank”也可以指金融机构。“bank”的正确解释取决于它在句子中的上下文，特别是单词“river”的存在。
- en: A transformer model uses self-attention to weigh the relevance of each word
    in the sentence when translating it. When the model processes the word “bank,”
    it assigns higher attention scores to related words (“arrived,” “crossing,” “river”)
    that help determine the correct meaning of “bank.” This way, the model can correctly
    translate the sentence into another language, preserving the intended meaning
    of “bank.”
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型使用自注意力机制来衡量句子中每个单词的相关性，以便在翻译时使用。当模型处理单词“bank”时，它会为相关的单词（“arrived”，“crossing”，“river”）分配更高的注意力分数，以帮助确定“bank”的正确含义。通过这种方式，模型能够正确地将句子翻译成另一种语言，并保持“bank”一词的原意。
- en: This mechanism also allows the model to process all parts of the input sequence
    in parallel, significantly improving training efficiency. Traditional sequence-to-sequence
    models, such as RNNs, process input sequences step-by-step, which can be time-consuming
    for long sequences. In contrast, transformers can simultaneously process all words
    in the input sequence, leading to faster training times.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种机制还允许模型并行处理输入序列的所有部分，从而显著提高训练效率。传统的序列到序列模型，如 RNN，会一步一步地处理输入序列，这对于长序列来说可能非常耗时。相反，Transformer
    可以同时处理输入序列中的所有单词，从而缩短训练时间。
- en: Moreover, the self-attention mechanism helps the model learn long-range dependencies
    in the data more effectively. In our example, even though the words “bank” and
    “river” are separated by several other words, the model can still understand their
    relationship. This ability is crucial for tasks such as text summarization or
    question answering, where understanding the entire context is essential.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，自注意力机制帮助模型更有效地学习数据中的长程依赖关系。在我们的例子中，尽管单词“bank”和“river”被其他几个单词分隔开，但模型仍然能够理解它们之间的关系。这种能力对于文本摘要或问答等任务至关重要，在这些任务中，理解整个上下文是非常重要的。
- en: The self-attention mechanism
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制
- en: The self-attention mechanism enables the transformer model to understand the
    context within the input data. It calculates attention scores, determining how
    much focus each input part should be given when predicting a particular output
    element. This mechanism is crucial for accurately transcribing speech because
    it allows the model to consider the entire context of a sentence or conversation
    rather than processing words in isolation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制使得 Transformer 模型能够理解输入数据中的上下文。它通过计算注意力分数，确定在预测特定输出元素时每个输入部分应该给予多少关注。这个机制对于准确转录语音至关重要，因为它使得模型能够考虑句子或对话的整个上下文，而不是单独处理每个单词。
- en: Introducing transformers has led to state-of-the-art performance in various
    tasks, including machine translation, text summarization, and question-answering.
    It has also paved the way for developing subsequent models such as BERT, GPT,
    and others, further pushing what’s possible in NLP.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 引入 Transformer 已经在各种任务中带来了最先进的性能，包括机器翻译、文本摘要和问答。它还为后续模型（如 BERT、GPT 等）的发展铺平了道路，进一步推动了自然语言处理的可能性。
- en: The shift to transformer models has been so significant that it has redefined
    the best practices in NLP, moving from sequential processing to a more parallel
    and context-aware approach. This has improved performance on benchmark tasks and
    opened up new possibilities for NLP applications, making it a truly transformative
    moment in the field.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 向变换器模型的转变是如此显著，以至于它重新定义了NLP领域的最佳实践，改变了从顺序处理到更具并行性和上下文感知的方法。这不仅提升了基准任务的表现，还为NLP应用开辟了新天地，成为该领域一个真正具有变革性的时刻。
- en: Examining the transformer model framework
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审视变换器模型框架
- en: 'The transformer model contains an encoder and decoder. The encoder processes
    the input audio frames while the decoder generates the transcribed text output.
    Both the encoder and decoder have repeated blocks containing the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型包含编码器和解码器。编码器处理输入的音频帧，而解码器生成转录后的文本输出。编码器和解码器都有重复的块，其中包含以下内容：
- en: '**Multihead self-attention layers**: These allow the model to understand the
    context and weigh the relevance of each word when transcribing. This is key for
    interpreting spoken language correctly.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头自注意力层**：这些层使得模型能够理解上下文，并在转录时衡量每个单词的相关性。这对正确解释口语语言至关重要。'
- en: '**Position-wise feedforward layers**: These process features from the attention
    layers and propagate information throughout the model.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置相关的前馈层**：这些层处理来自注意力层的特征，并将信息在整个模型中传播。'
- en: Unlike previous sequence models, self-attention layers let the model consider
    the whole context when transcribing each word. This gives us substantial performance
    improvements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的序列模型不同，自注意力层使得模型在转录每个单词时能够考虑到整个上下文。这为我们带来了显著的性能提升。
- en: 'The following diagram illustrates the steps of *auto-regressive* generation
    in encoder-decoder models found in transformers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了变换器中编码器-解码器模型的*自回归*生成步骤：
- en: '![Figure 3.2 – The transformer encoder-decoder model (Transformers-based Encoder-Decoder
    Models. Patrick von Platen. October 10, 2020\. https://huggingface.co/blog/encoder-decoder)](img/B21020_03_2.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 变换器编码器-解码器模型（基于变换器的编码器-解码器模型。Patrick von Platen. 2020年10月10日。https://huggingface.co/blog/encoder-decoder)](img/B21020_03_2.jpg)'
- en: Figure 3.2 – The transformer encoder-decoder model (Transformers-based Encoder-Decoder
    Models. Patrick von Platen. October 10, 2020\. https://huggingface.co/blog/encoder-decoder)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 变换器编码器-解码器模型（基于变换器的编码器-解码器模型。Patrick von Platen. 2020年10月10日。https://huggingface.co/blog/encoder-decoder）
- en: 'In the preceding figure, the encoder, depicted in green, and the decoder, shown
    in orange, demonstrate translating the English phrase “My cat is hungry” into
    Spanish as “Mi gato tiene hambre.” The translation involves a series of steps,
    as detailed here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述图中，编码器用绿色表示，解码器用橙色表示，演示了将英语短语“My cat is hungry”翻译成西班牙语“Mi gato tiene hambre”。翻译涉及一系列步骤，具体如下：
- en: '**Step 1**: Initially, the encoder analyzes the entire input sequence of **a1:5**
    = “my cat is hungry” (visualized through light green vectors and converting it
    into a series of context-aware encoded vectors, **A1:5**. For instance, the vector
    **a2** captures an encoding that reflects not just the word “cat” but also incorporates
    the contextual relevance of the surrounding words “My” “cat” “is” and “hungry”
    and the end-of-sentence marker, “EOS”.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：最初，编码器分析整个输入序列**a1:5** = “my cat is hungry”（通过浅绿色向量可视化，并将其转换为一系列上下文感知的编码向量**A1:5**）。例如，向量**a2**捕捉到的编码不仅仅反映了“cat”这个单词，还包含了周围单词“My”，“cat”，“is”和“hungry”的上下文相关性，以及句子结束标记“EOS”。'
- en: '**Step 2**: Subsequently, this encoded sequence, **A1:5**, along with the beginning-of-sentence
    (BOS) vector, denoted as **b0**, is introduced to the decoder. The decoder then
    interprets these inputs to generate the first logit, **B1** (represented in a
    deeper shade of orange), establishing the conditional probability for the initial
    target vector, **b1**.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**：接着，这个编码序列**A1:5**，以及句子起始（BOS）向量**b0**，被输入到解码器中。解码器随后解读这些输入，生成第一个logit值**B1**（以更深的橙色表示），从而建立初始目标向量**b1**的条件概率。'
- en: '**Step 3**: Following this, the first target vector, **b1**, corresponding
    to “Mi,” is derived from the probability distribution (indicated by the grey arrow)
    and reintroduced into the decoder. At this juncture, the decoder evaluates both
    **b0 = “BOS”** and **b1 = “Mi”** to ascertain the conditional probability for
    the subsequent target vector, **b2**.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**：接下来，第一个目标向量 **b1**，对应于“Mi”，由概率分布（由灰色箭头表示）得出并重新引入解码器。此时，解码器会评估 **b0
    = “BOS”** 和 **b1 = “Mi”**，以确定下一个目标向量 **b2** 的条件概率。'
- en: '**Step 3…n**: This process is continued iteratively, after which the next target
    vector, **b2 = “gato”**, is obtained. The procedure is maintained in an auto-regressive
    manner until the **end-of-sentence** (**EOS**) vector is identified at the sixth
    step, continuing in this sequential manner.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3…n**：这个过程会持续迭代进行，直到获得下一个目标向量 **b2 = “gato”**。该过程保持自回归方式，直到在第六步识别出 **句末**（**EOS**）向量，并以此顺序继续进行。'
- en: It is crucial to recognize that the encoder’s role is confined to the initial
    pass, where it transforms **a1:n** into **A1:n**. In the subsequent pass, the
    decoder directly utilizes the pre-computed encodings of **A1:n**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 需要明确的是，编码器的作用仅限于初步处理，它将**a1:n** 转换为 **A1:n**。在随后的步骤中，解码器直接使用已经预先计算好的 **A1:n**
    编码。
- en: Optimizing for automated speech recognition
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化自动语音识别
- en: When applied to ASR in Whisper, the transformer leverages vast datasets to handle
    multiple languages and tasks. For training, **connectionist temporal classification**
    (**CTC**) neatly aligns audio inputs to text outputs without needing explicit
    alignment annotations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Whisper 中应用于 ASR 时，transformer 利用大量数据集处理多种语言和任务。对于训练，**连接主义时间分类**（**CTC**）巧妙地将音频输入与文本输出对齐，而无需显式的对齐注释。
- en: This makes the model robust to speech variations such as pace or pausing. Unlike
    previous deep learning models, the transformer handles speaker overlap in conversations.
    Together, these optimizations enable Whisper to transcribe real-world speech accurately.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得该模型能够适应语音变化，如语速或停顿。与以往的深度学习模型不同，transformer 可以处理对话中的说话人重叠。通过这些优化，Whisper
    能够准确地转录现实世界的语音。
- en: 'Whisper uses a sequence-to-sequence model with a transformer encoder-decoder
    architecture. This maps audio to text in stages. First, the raw audio is converted
    into a **log-Mel spectrogram** showing speech frequencies. The encoder then processes
    this spectrogram to extract essential features. Finally, the decoder uses those
    features to predict the text transcription one word at a time. Whisper can convert
    speech into text automatically by optimizing the mappings between audio and text.
    This step-by-step pipeline enables the model to learn alignments between the input
    audio and output text. *Figure 3**.3* summarizes the Whisper model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 使用的是一个序列到序列模型，采用 transformer 编码器-解码器架构。这将音频映射到文本的过程中分阶段进行。首先，原始音频被转换成显示语音频率的**对数梅尔频谱图**。然后，编码器处理该频谱图以提取基本特征。最后，解码器使用这些特征逐个预测文本转录。Whisper
    可以通过优化音频与文本之间的映射，自动将语音转换为文本。这个逐步的处理流程使得模型能够学习输入音频和输出文本之间的对齐。*图 3.3* 总结了 Whisper
    模型：
- en: '![Figure 3.3 – The Whisper model. The model applies a standard transformer
    encoder-decoder architecture. Log-Mel spectrograms of audio are input to the encoder.
    The encoder passes learned features to the decoder. The decoder then predictively
    transcribes the speech one word at a time based on the audio features and previous
    words (https://cdn.openai.com/papers/whisper.pdf)](img/B21020_03_3.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – Whisper 模型。该模型采用标准的 transformer 编码器-解码器架构。音频的对数梅尔频谱图输入到编码器。编码器将学习到的特征传递给解码器。解码器然后基于音频特征和前一个单词逐个预测地转录语音（https://cdn.openai.com/papers/whisper.pdf）](img/B21020_03_3.jpg)'
- en: Figure 3.3 – The Whisper model. The model applies a standard transformer encoder-decoder
    architecture. Log-Mel spectrograms of audio are input to the encoder. The encoder
    passes learned features to the decoder. The decoder then predictively transcribes
    the speech one word at a time based on the audio features and previous words (https://cdn.openai.com/papers/whisper.pdf)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Whisper 模型。该模型采用标准的 transformer 编码器-解码器架构。音频的对数梅尔频谱图输入到编码器。编码器将学习到的特征传递给解码器。解码器然后基于音频特征和前一个单词逐个预测地转录语音（https://cdn.openai.com/papers/whisper.pdf）
- en: Sequence-to-sequence models for speech recognition utilize an encoder-decoder
    architecture. The encoder extracts noticeable features from the audio speech inputs
    and encodes them into hidden state representations. The decoder acts as an internal
    language model, processing these representations to generate transcriptions of
    the spoken text. Incorporating the language model within the model is known as
    deep fusion. This contrasts with shallow fusion approaches, which combine an external
    language model with a separate encoder (for example, connecting a CTC encoder
    with an n-gram language model; see the research paper at [https://arxiv.org/pdf/2011.01991.pdf](https://arxiv.org/pdf/2011.01991.pdf)).
    Deep fusion trains the full model end-to-end, using the same data and loss function.
    This facilitates more flexible training and performs better than shallow fusion
    techniques, as benchmarks show (see the research paper at [https://arxiv.org/abs/2210.13352](https://arxiv.org/abs/2210.13352)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 用于语音识别的序列到序列模型采用了编码器-解码器架构。编码器从音频语音输入中提取显著特征，并将其编码为隐藏状态表示。解码器充当内部语言模型，处理这些表示以生成口语文本的转录。将语言模型融入模型中被称为深度融合（deep
    fusion）。这与浅层融合方法（例如，将外部语言模型与单独的编码器结合，连接CTC编码器与n-gram语言模型；请参见研究论文：[https://arxiv.org/pdf/2011.01991.pdf](https://arxiv.org/pdf/2011.01991.pdf)）形成对比。深度融合对整个模型进行端到端训练，使用相同的数据和损失函数。这种方法提供了更灵活的训练，并比浅层融合技术表现更好，正如基准测试所示（请参见研究论文：[https://arxiv.org/abs/2210.13352](https://arxiv.org/abs/2210.13352)）。
- en: By leveraging deep learning breakthroughs and abundantly available training
    data, Whisper pushes the boundaries of ASR using the transformer architecture.
    As the model continues improving, so will this system’s versatility. Understanding
    these mechanics provides valuable insight into Whisper’s impressive capabilities
    compared to other speech technology.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用深度学习的突破和丰富可用的训练数据，Whisper采用变换器架构推动了自动语音识别（ASR）的边界。随着模型的不断改进，系统的多功能性也将提升。理解这些机制有助于深入了解Whisper在与其他语音技术相比时的出色能力。
- en: Examining the role of the transformer model in Whisper
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究变换器模型在Whisper中的作用
- en: The transformer model is integral to OpenAI’s Whisper and is based on a deep
    learning architecture that leverages self-attention mechanisms to process sequential
    data, such as speech, in a way that captures the context and nuances of language.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型是OpenAI的Whisper的重要组成部分，它基于一种深度学习架构，利用自注意力机制以一种捕捉语言上下文和细微差别的方式处理序列数据，如语音。
- en: Whisper’s transformer model encodes the input data corresponding to spoken words
    in speech recognition. The input audio is split into chunks, typically 30 seconds
    long, and converted into a log-Mel spectrogram. This spectrogram is then passed
    through the encoder, which uses self-attention to weigh the importance of each
    part of the input sequence when producing the output.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的变换器模型对语音识别中的输入数据（即与口语单词相对应的数据）进行编码。输入音频被分割成通常为30秒的块，并转换为对数梅尔频谱图。然后，这个频谱图被传递给编码器，编码器使用自注意力机制在生成输出时对输入序列的每个部分的相对重要性进行加权。
- en: The decoder is trained to predict the corresponding text caption for the processed
    audio input. It does this by generating one word at a time, considering the entire
    sequence processed by the encoder to maintain the context. The decoder also uses
    self-attention to focus on different parts of the input sequence when predicting
    each part of the output sequence.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器经过训练，以预测与处理过的音频输入相对应的文本字幕。它通过逐个生成单词的方式进行预测，并考虑编码器处理的整个序列，以保持上下文的连贯性。解码器还使用自注意力机制，在预测输出序列的每个部分时，集中关注输入序列的不同部分。
- en: The transformer model’s role in Whisper is significant because it effectively
    drives the system’s ability to convert spoken language into written text. Its
    architecture, particularly the self-attention mechanism, allows Whisper to capture
    the context and meaning of spoken words, which is essential for accurate transcription.
    The model’s scalability and ability to learn from large datasets contribute to
    Whisper’s robustness and adaptability, making it a powerful tool for speech recognition
    across various languages and applications.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型在Whisper中的作用至关重要，因为它有效地驱动了系统将口语语言转换为书面文本的能力。其架构，尤其是自注意力机制，使得Whisper能够捕捉到口语单词的上下文和含义，这对准确的转录至关重要。该模型的可扩展性和从大规模数据集学习的能力为Whisper的强大和适应性提供了支持，使其成为一种强大的语音识别工具，适用于各种语言和应用场景。
- en: Having examined the transformer model’s pivotal role in Whisper’s advanced speech
    recognition, let’s delve deeper into this technology’s core—the encoder-decoder
    mechanics—and unravel how these components work in concert to interpret and transform
    spoken language into written text accurately.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了变换器模型在Whisper先进语音识别中的关键作用后，让我们更深入地了解这项技术的核心——编码器-解码器机制，并揭示这些组件是如何协同工作，将口语转化为准确的书面文本的。
- en: Deciphering the encoder-decoder mechanics
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解读编码器-解码器机制
- en: Like other transformer models, Whisper’s architecture is based on an encoder-decoder
    mechanism. As shown in *Figure 3**.3*, the encoder-decoder mechanism is a two-step
    process. The encoder takes the input data (in this case, speech) and converts
    it into vectors, representing the data in a way the model can understand. These
    vectors capture the contextual information of the input data. The decoder then
    takes these vectors and generates the output data (in this case, text) step by
    step.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他变换器模型一样，Whisper的架构是基于编码器-解码器机制。如*图3.3*所示，编码器-解码器机制是一个两步过程。编码器接受输入数据（在此为语音），并将其转换为向量，以模型可以理解的方式表示数据。这些向量捕捉了输入数据的上下文信息。然后，解码器利用这些向量，逐步生成输出数据（在此为文本）。
- en: Encoding in Whisper
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Whisper中的编码
- en: In the context of Whisper, the encoder takes the spoken language as input and
    converts it into a sequence of vectors. This sequence captures the contextual
    information of the speech, such as the order of the words and the phonetic details.
    The decoder then takes this sequence and generates the corresponding text, one
    word at a time, maintaining the order of the words and the context.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Whisper的框架中，编码器将口语作为输入，并将其转换为一系列向量。这个序列捕捉了语音的上下文信息，如单词顺序和语音细节。接着，解码器利用这一序列，逐字生成相应的文本，同时保持单词的顺序和上下文。
- en: The encoder processes the input data in stages, each adding a level of abstraction.
    It starts by converting the raw audio into a sequence of feature vectors, which
    are then passed through several layers of the transformer model. Each layer consists
    of self-attention mechanisms and feed-forward neural networks, which help capture
    the input data’s complex patterns and dependencies.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器按阶段处理输入数据，每一阶段都增加了一层抽象。它首先将原始音频转换为一系列特征向量，然后通过多个变换器模型的层进行处理。每一层都包含自注意力机制和前馈神经网络，帮助捕捉输入数据中的复杂模式和依赖关系。
- en: The encoder’s output is a sequence of context-sensitive representations of the
    input data. These representations capture the information in the corresponding
    input feature vector and the information from the entire input sequence. This
    allows the decoder to generate accurate transcriptions, even in the presence of
    noise or other distortions in the input data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出是一个上下文敏感的输入数据表示序列。这些表示捕捉了相应输入特征向量中的信息以及来自整个输入序列的信息。这使得解码器能够生成准确的转录结果，即使输入数据中存在噪声或其他失真。
- en: The encoder’s ability to handle multiple languages and tasks simultaneously
    is another critical feature of Whisper, making it a versatile tool for various
    applications, from transcription services to voice assistants.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器同时处理多种语言和任务的能力是Whisper的另一关键特性，使其成为多种应用的多功能工具，从转录服务到语音助手。
- en: Decoding in Whisper
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Whisper中的解码
- en: 'The decoder in Whisper’s transformer model works in tandem with the encoder
    to perform the task of speech recognition. While the encoder processes the input
    audio and creates a contextual representation, the decoder uses this representation
    to predict the corresponding text output. Here are the fundamental processing
    phases that are performed by the decoder:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在Whisper的变换器模型中，解码器与编码器协同工作，执行语音识别任务。编码器处理输入音频并创建上下文表示，解码器则利用该表示预测相应的文本输出。以下是解码器执行的基本处理阶段：
- en: '**Predicting text**: The decoder is trained to predict text captions from the
    encoded representations of the audio input. It does this by generating one word
    at a time, considering the entire sequence processed by the encoder to maintain
    the context of the spoken language.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测文本**：解码器通过对音频输入的编码表示进行训练，预测文本字幕。它通过逐字生成的方式来实现，并且考虑由编码器处理的整个序列，以保持口语的上下文。'
- en: '**Handling special tokens**: Whisper’s decoder also utilizes unique tokens
    to perform several tasks, such as providing phrase-level timestamps and indicating
    different functions within the transcription process. These tokens are part of
    the model’s vocabulary and direct the model’s behavior during the decoding phase.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理特殊标记**：Whisper的解码器还利用独特的标记来执行多个任务，如提供短语级时间戳和指示转录过程中的不同功能。这些标记是模型词汇的一部分，并在解码阶段引导模型的行为。'
- en: '**Coupling input-output representations**: The decoder employs coupled input-output
    token representations and learned position embeddings. This allows the model to
    understand the sequence and position of words within the context of the entire
    sentence or conversation.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**耦合输入输出表示**：解码器采用耦合的输入输出标记表示和学习的位置嵌入。这使得模型能够理解单词在整个句子或对话中的顺序和位置。'
- en: '**Performing autoregressive generation**: The architecture follows a classic
    encoder-decoder structure, meaning the decoder relies on an autoregressive generation
    process. This process involves predicting each subsequent word based on the previous
    words generated, ensuring that the output text is coherent and contextually relevant.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行自回归生成**：该架构遵循经典的编码器-解码器结构，这意味着解码器依赖于自回归生成过程。该过程涉及根据之前生成的单词预测每个后续单词，确保输出文本具有连贯性并与上下文相关。'
- en: '**Handling errors**: The decoder’s design and training allow it to handle variations
    in speech, such as accents, background noise, and technical language. This robustness
    is partly due to the large and diverse dataset on which Whisper is trained, which
    includes a wide range of languages and audio conditions.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**处理错误**：解码器的设计和训练使其能够处理语音中的变异，如口音、背景噪音和专业术语。这种鲁棒性部分归功于Whisper所训练的大型多样化数据集，该数据集包括广泛的语言和音频条件。'
- en: In summary, the decoder in Whisper’s architecture generates the written text
    from the encoded audio input. It is a sophisticated component that uses learned
    patterns, unique tokens, and an autoregressive generation process to produce accurate
    transcriptions that reflect the context and nuances of the spoken language. The
    effectiveness of the decoder is a testament to the transformer model’s ability
    to handle complex tasks such as speech recognition and translation, making Whisper
    a powerful tool in the field of ASR.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Whisper架构中的解码器根据编码的音频输入生成书面文本。它是一个复杂的组件，使用学习到的模式、独特的标记和自回归生成过程，生成准确的转录文本，反映出口语语言的上下文和细微差别。解码器的有效性证明了Transformer模型能够处理复杂任务，如语音识别和翻译，使Whisper成为语音识别领域的强大工具。
- en: The following section explores the technical innovations behind speech recognition
    systems adapting between domains such as translation, summarization, and keyword
    identification. We’ll walk through Whisper’s optimized model architecture, extensive
    multilingual datasets, and intriguing zero-shot transfer learning capabilities
    that facilitate its linguistic flexibility.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨语音识别系统如何在翻译、摘要和关键词识别等领域之间进行适配的技术创新。我们将深入了解Whisper优化的模型架构、广泛的多语言数据集，以及其引人注目的零-shot迁移学习能力，这些都为其语言灵活性提供了支持。
- en: Exploring the multitasking and multilingual capabilities of Whisper
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Whisper的多任务和多语言能力
- en: As we saw in the previous section, the transformer model architecture is central
    to empowering Whisper’s advanced speech recognition capabilities. However, the
    story does not end there. Whisper possesses remarkable versatility beyond just
    transcribing English audio into text. Its flexible design supports seamlessly
    switching between diverse tasks such as translation, summarization, and keyword
    identification across 90 languages. This ability to adaptably multitask in linguistically
    diverse environments significantly expands the practical applicability of Whisper
    for global business and consumer needs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，Transformer模型架构是赋能Whisper高级语音识别能力的核心。然而，故事并不止于此。Whisper拥有令人瞩目的多功能性，不仅仅是将英语音频转录为文本。其灵活的设计支持在90种语言之间无缝切换，执行翻译、摘要和关键词识别等多种任务。这种能够在语言多样化环境中适应性地进行多任务处理的能力，显著扩展了Whisper在全球商业和消费者需求中的实际应用。
- en: In the following sections, we will explore the technical innovations that drive
    Whisper’s versatility, including its optimized model architecture for multitasking,
    extensive multilingual training data, and intriguing zero-shot transfer learning
    abilities. Understanding these capabilities provides valuable insight for integrating
    Whisper effectively into cross-cultural and multifunctional speech recognition
    projects, from voice assistant solutions to reporting systems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨驱动 Whisper 多功能性的技术创新，包括其优化的多任务模型架构、广泛的多语言训练数据，以及引人注目的零-shot 转移学习能力。理解这些能力为将
    Whisper 有效集成到跨文化和多功能语音识别项目中提供了宝贵的见解，从语音助手解决方案到报告系统。
- en: Assessing Whisper’s ability to handle multiple tasks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 Whisper 处理多任务的能力
- en: When I first learned about Whisper’s multitasking capabilities, I’ll admit –
    I was stunned. As experienced tech professionals, we know that most AI systems
    specialize in a single purpose. Language models generate text. Computer vision
    models analyze images. Speech recognition tools transcribe audio.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我第一次了解到 Whisper 的多任务能力时，我不得不承认——我感到震惊。作为有经验的技术专业人员，我们知道大多数 AI 系统都专注于单一任务。语言模型生成文本。计算机视觉模型分析图像。语音识别工具转录音频。
- en: But Whisper breaks this pattern. Its architecture supports performing multiple
    types of speech processing tasks from the same model, a remarkable capability
    that sets a new standard for versatility in speech AI systems.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是 Whisper 打破了这一模式。它的架构支持从同一个模型执行多种类型的语音处理任务，这一非凡的能力为语音 AI 系统的多功能性树立了新的标准。
- en: So, how does Whisper pull off this magic trick? This revelation sent me on an
    intriguing exploration to uncover the secrets behind its flexible design. And
    what I discovered only deepened my appreciation for its elegant innovations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Whisper 是如何实现这一魔法的呢？这一发现让我开始了一个引人入胜的探索，揭开其灵活设计背后的秘密。而我所发现的仅仅加深了我对其优雅创新的欣赏。
- en: Revealing latent connections
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 揭示潜在连接
- en: The critical insight is that, at their core, all speech tasks rely on understanding
    language. So, by training Whisper’s model on diverse speech data for multiple
    tasks, it learns the connections between the tasks at an abstract, latent level.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的洞察是，从本质上讲，所有语音任务都依赖于理解语言。因此，通过在多任务的多样语音数据上训练 Whisper 的模型，它能在抽象的潜在层面上学习任务之间的连接。
- en: '**Latent connections** in OpenAI’s Whisper ASR system are crucial for improving
    speech recognition accuracy. These connections are part of the transformer model
    architecture that underpins Whisper. The transformer model is known for its encoder-decoder
    structure, which uses self-attention mechanisms to weigh the importance of different
    parts of the input data.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 的 Whisper ASR 系统中的**潜在连接**对于提高语音识别准确性至关重要。这些连接是支撑 Whisper 的 Transformer
    模型架构的一部分。Transformer 模型以其编码器-解码器结构著称，利用自注意力机制来衡量输入数据中不同部分的重要性。
- en: In speech recognition, latent connections help the model capture the dependencies
    between different parts of the speech input, even when they are far apart in the
    sequence. This is particularly important in speech recognition, where the meaning
    of a word can depend on the context provided by words that occurred much earlier
    or later in the conversation. By effectively capturing these dependencies, latent
    connections help to improve the accuracy of the transcriptions produced by the
    Whisper ASR system.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音识别中，潜在连接帮助模型捕捉语音输入中不同部分之间的依赖关系，即使这些部分在序列中相隔较远。这在语音识别中尤为重要，因为一个词的意思可能依赖于对话中早些时候或稍后出现的词提供的上下文。通过有效地捕捉这些依赖关系，潜在连接有助于提高
    Whisper ASR 系统生成的转录准确性。
- en: Moreover, the transformer model in Whisper is trained using weak supervision
    on large-scale data. This method involves training the model on a large amount
    of data with limited annotation, allowing it to learn from a broader context and
    improve its performance even in complex or ambiguous situations. This training
    methodology, combined with the power of latent connections in capturing long-range
    dependencies, contributes to Whisper’s high accuracy in speech recognition tasks.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Whisper 中的 Transformer 模型是在大规模数据上通过弱监督训练的。这种方法涉及在大量带有限制标注的数据上训练模型，让它从更广泛的上下文中学习，并在复杂或模糊的情况下提高其性能。这种训练方法与潜在连接在捕捉长距离依赖关系上的强大能力相结合，促成了
    Whisper 在语音识别任务中的高准确性。
- en: For example, transcribing Spanish audio requires understanding Spanish vocabulary
    and grammar. Translating Spanish speech into English relies on mapping between
    the languages. Summarizing a Spanish conversation demands picking out critical
    semantic concepts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，转录西班牙语音频需要理解西班牙语的词汇和语法。将西班牙语语音翻译成英语依赖于两种语言之间的映射。总结西班牙语对话要求挑选出关键的语义概念。
- en: Although superficially different, all these tasks tap into the meaning behind
    spoken words—what linguists call semantics. Exposing Whisper to a variety of verbal
    tasks implicitly makes these critical connections through self-supervised learning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管表面上有所不同，所有这些任务都涉及到对口语单词背后意义的探究——这就是语言学家所说的语义学。通过自监督学习，Whisper在接触各种语言任务时，隐性地建立起这些关键的连接。
- en: Linguistic semantics
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学语义学
- en: Linguistic semantics is the study of meaning used to understand human expression
    through language. It involves interpreting the meanings of words, phrases, and,
    ultimately, entire texts. Semantics considers the relationships between words
    and how they create meaning, often focusing on denotations (direct or dictionary
    meanings) and connotations (ideas or feelings that a word invokes). In AI and
    machine learning, understanding semantics is crucial for NLP tasks such as language
    translation, sentiment analysis, and information extraction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 语言学语义学是研究意义的学科，用以理解人类通过语言表达的意思。它涉及对单词、短语，最终是整个文本的意义进行解释。语义学关注单词之间的关系，以及它们如何共同创造意义，通常重点讨论指称（直接的或词典意义）和隐含意义（单词所引发的想法或感情）。在人工智能和机器学习中，理解语义对自然语言处理（NLP）任务至关重要，如语言翻译、情感分析和信息提取。
- en: Architectural supports for adaptability
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 适应性支持的架构
- en: 'But soaking up lots of training data isn’t enough alone. Whisper’s architecture
    crucially supports adaptable, versatile applications of the knowledge it gains:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，仅仅吸收大量的训练数据是不够的。Whisper的架构在至关重要的方面支持知识的可适应性和多样化应用：
- en: Self-attention allows the model to weigh the context around each word when transcribing.
    This enables correctly interpreting words such as *right* based on the whole sentence’s
    meaning, which improves accuracy.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力机制使模型在转录时能够考虑每个单词周围的上下文。这使得像*right*这样的词能够根据整个句子的意思进行正确解读，从而提高准确性。
- en: Multilingual training exposes Whisper to vocabulary, grammar, and pronunciation
    diversity across languages. Recognizing these cross-linguistic patterns enables
    better generalization of new languages not explicitly seen during training.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多语种训练使Whisper能够接触到跨语言的词汇、语法和发音多样性。识别这些跨语言的模式使得模型在面对训练时未明确出现的新语言时能更好地进行泛化。
- en: The encoder-decoder structure is well-suited to translating input audio across
    domains such as languages or tasks. Flexibility is the key. This capability is
    called **soft alignment**.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器结构非常适合跨领域（如语言或任务）转换输入音频。灵活性是关键。这一能力被称为**软对齐**。
- en: Soft alignment
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 软对齐
- en: Soft alignment is used during training to align the input audio with the corresponding
    transcription. This alignment is *soft* because it’s probabilistic, meaning it’s
    based on the likelihood of certain parts of the audio corresponding to certain
    parts of the transcription. Soft alignment during training means the model doesn’t
    make rigid assumptions about strict input-output pairings. This enables handling
    more free-form, variable real-world speech.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中使用软对齐来将输入音频与对应的转录文本对齐。这个对齐是*软的*，因为它是基于概率的，意味着它依据某些音频部分与转录文本某些部分对应的可能性来进行。训练中的软对齐意味着模型不会对严格的输入-输出配对做出僵硬假设。这使得模型能够处理更自由、变化多端的现实世界语音。
- en: In Whisper, the model is trained on many multilingual and multitask supervised
    data collected from the web. The model uses a variant of the CTC loss function,
    which allows it to handle alignment between the input audio and its corresponding
    transcription in a *soft* or probabilistic manner. This soft alignment enables
    the model to handle variations in speech rate and other temporal variations in
    the audio data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在Whisper中，模型在许多来自网络的多语种和多任务监督数据上进行训练。模型使用一种变体的CTC损失函数，这使得它能够以*软的*或概率的方式处理输入音频与其对应转录之间的对齐。这种软对齐使得模型能够处理语音速率和音频数据中的其他时间变化。
- en: The advantage of this approach is that it doesn’t require explicit segmentation
    or alignment of the audio data, which can be a challenging task in ASR. In traditional
    ASR systems, aligning audio data with its corresponding transcription often requires
    precise segmentation, breaking the audio into smaller, manageable segments corresponding
    to speech units, such as words or phonemes. This process can be complex and error-prone,
    especially when dealing with variations in speech, such as different accents,
    speech rates, and background noises.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于它不需要显式的音频数据分段或对齐，而这在自动语音识别（ASR）中是一个具有挑战性的任务。在传统的 ASR 系统中，将音频数据与其对应的转录文本对齐通常需要精确的分段，将音频划分成较小、可管理的片段，对应于语音单元，例如单词或音素。这个过程既复杂又容易出错，特别是在处理语音变化时，如不同的口音、语速和背景噪声。
- en: Instead, Whisper adopts a probabilistic approach by employing soft alignment
    through the CTC loss function. This approach is based on the likelihood of certain
    parts of the audio corresponding to specific parts of the transcription rather
    than rigidly trying to align fixed audio segments to text. This method allows
    the model to handle a wide range of real-world speech variabilities, such as changes
    in speech rate and other temporal variations in the audio data. As a result, the
    model learns to implicitly align the audio and text data during training, leading
    to more robust and accurate speech recognition without the need for complex and
    labor-intensive explicit segmentation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 取而代之的是，Whisper 采用了一种概率性的方法，通过使用 CTC 损失函数进行软对齐。这种方法基于音频的某些部分与转录文本的特定部分相对应的可能性，而不是僵硬地将固定的音频段与文本对齐。这种方法使得模型能够处理广泛的现实世界语音变异性，如语速变化以及音频数据中的其他时间变化。因此，模型在训练过程中学习到隐式地对齐音频和文本数据，从而实现更强大、更准确的语音识别，而无需复杂且劳动密集的显式分段。
- en: Exploring Whisper’s multilingual capabilities deeper
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更深入地探索 Whisper 的多语言能力
- en: When explored under the hood, Whisper’s method for instilling remarkable multilingual
    skills revealed masterful AI engineering. The spark igniting Whisper’s flexible
    language skills starts with its data. Whisper `large-v3` was trained on 1 million
    hours of weakly labeled audio and 4 million hours of pseudo-labeled audio collected
    using `large-v2`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入探索时，Whisper 在赋予其出色的多语言能力方面展现了高超的 AI 工程技术。激发 Whisper 灵活语言能力的火花源自于其数据。Whisper
    `large-v3` 是在 100 万小时的弱标注音频和 400 万小时的伪标注音频上训练的，这些音频是使用 `large-v2` 收集的。
- en: Pseudo-labeling
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签
- en: Pseudo-labeling is a semi-supervised learning technique used to improve the
    performance of a machine-learning model. In training the latest Whisper model
    version 3, pseudo-labeling involves using the model’s predictions on unlabeled
    data to generate pseudo labels. Pseudo-labeling is particularly useful in scenarios
    where there is a large amount of unlabeled data and a relatively small amount
    of labeled data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 伪标签是一种半监督学习技术，用于提高机器学习模型的性能。在训练最新的 Whisper 模型版本 3 时，伪标签技术涉及利用模型对无标签数据的预测来生成伪标签。伪标签在无标签数据量庞大而标注数据相对较少的场景中尤为有用。
- en: Imagine we have an extensive collection of audio recordings in various languages,
    but many don’t have corresponding text labels indicating what is being said. To
    train Whisper, we initially used a previous model version (`large-v2`) to process
    these unlabeled recordings. The `large-v2` model listens to the audio and makes
    its best guess at transcribing the speech, effectively creating *pseudo* labels
    for these recordings.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拥有大量各种语言的音频录音，但其中许多并没有相应的文本标签来指示所说内容。为了训练 Whisper，我们最初使用了之前的模型版本（`large-v2`）来处理这些无标签的录音。`large-v2`
    模型监听音频并尽最大努力转录语音，有效地为这些录音生成 *伪* 标签。
- en: Though not perfectly accurate, these pseudo labels provide a starting point
    for training the next version of the model (`large-v3`). The `large-v3` model
    then learns from this expanded dataset, including the original labeled data and
    the new pseudo-labeled data. This approach allows the model to improve its understanding
    and recognition of speech in various languages, even when there’s a lack of perfectly
    labeled data. This technique of using the model’s predictions on unlabeled data
    to create new training material is called pseudo-labeling when training Whisper’s
    latest model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不完全准确，这些伪标签为训练下一版本模型（`large-v3`）提供了起点。然后，`large-v3` 模型从这个扩展的数据集中学习，包括原始的标注数据和新的伪标注数据。该方法使模型能够在缺乏完全标注数据的情况下，改善其对多种语言语音的理解和识别。当使用模型对未标注数据的预测来创建新的训练材料时，这种技术被称为伪标签化，应用于训练
    Whisper 的最新模型。
- en: Crucially, this data encompassed 90 languages – exposing the model to unprecedented
    linguistic diversity. By leveraging web-scale data and cutting-edge techniques,
    Whisper soaks up vocabulary, grammar, accents, and other linguistic nuances spanning
    geographic regions and language families.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这些数据涵盖了90种语言——使得模型接触到了前所未有的语言多样性。通过利用互联网规模的数据和先进技术，Whisper 吸收了词汇、语法、口音以及跨地理区域和语言家族的其他语言细微差别。
- en: This sheer scale and variety massages innate connections between solving speech
    tasks across languages – transforming what the model implicitly understands as
    an abstract *language* itself.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种庞大的数据规模和多样性打通了跨语言解决语音任务之间的固有联系——转化了模型隐式理解的抽象*语言*本身。
- en: Optimizing the model architecture
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化模型架构
- en: But voluminous data alone isn’t enough – that also needs balancing with optimized
    model design. Whisper leverages the versatile transformer architecture we explored
    earlier for adaptable encoding and decoding between input audio and output text.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但单纯的大量数据是不够的——还需要与优化的模型设计相平衡。Whisper 利用我们之前探讨的多功能变换器架构，进行输入音频和输出文本之间的可调编码与解码。
- en: Unique to speech recognition, Whisper employs a time-restricted self-attention
    window during training. This considers local context when transcribing words,
    helping improve accuracy and computational efficiency over lengthy sequences.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别中的独特之处在于，Whisper 在训练过程中采用了一个时间限制的自注意力窗口。这在转录单词时考虑了局部上下文，帮助提高了在长序列中进行准确性和计算效率。
- en: Furthermore, adding **stochastic depth** and **dropout** gives randomness during
    training, helping Whisper generalize better by reducing reliance on any specific
    neurons. Together with multitasking learning across objectives such as transcription,
    translation, and identification, the model develops flexible linguistic dexterity.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，加入**随机深度**和**丢弃法**在训练过程中引入随机性，帮助 Whisper 更好地泛化，减少对任何特定神经元的依赖。结合跨目标的多任务学习，例如转录、翻译和识别，模型发展出了灵活的语言处理能力。
- en: Stochastic depth and dropout
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随机深度和丢弃法
- en: Stochastic depth and dropout are two techniques used to introduce randomness
    during the training of machine learning models, including Whisper ASR, to prevent
    overfitting and improve generalization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 随机深度和丢弃法是两种技术，用于在训练机器学习模型时引入随机性，包括 Whisper ASR，以防止过拟合并改善泛化能力。
- en: Stochastic depth is a regularization technique that randomly omits (or *drops*)
    specific layers in a deep neural network during training. The key idea is to reduce
    the network’s complexity during training by skipping some layers while still using
    the entire network at test time. This approach can help prevent overfitting, especially
    in deep networks, by adding noise to the training process and encouraging the
    network to learn more robust features. It also has the added benefit of reducing
    the computational cost of training.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随机深度是一种正则化技术，它在训练过程中随机省略（或*丢弃*）深度神经网络中的特定层。其核心思想是在训练时通过跳过某些层来减少网络的复杂性，同时在测试时仍然使用整个网络。这种方法有助于防止过拟合，特别是在深度网络中，通过向训练过程中添加噪声并鼓励网络学习更强大的特征。它还有降低训练计算成本的附加好处。
- en: Dropout, on the other hand, is a technique that randomly *drops out* (that is,
    sets to zero) the outputs of some neurons during training. Like stochastic depth,
    dropout is a form of regularization designed to prevent overfitting. By randomly
    dropping out neurons, dropout forces the network to learn redundant representations,
    making it more robust to the loss of specific neurons and improving its ability
    to generalize from the training data to unseen data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Dropout 是一种在训练过程中随机*丢弃*（即设置为零）某些神经元输出的技术。与随机深度类似，Dropout 是一种正则化方法，旨在防止过拟合。通过随机丢弃神经元，Dropout
    强迫网络学习冗余的表示，从而使其对特定神经元丢失更具鲁棒性，并提高其从训练数据到未见数据的泛化能力。
- en: In the context of Whisper ASR, these techniques can improve the robustness and
    generalization of the trained models. Introducing randomness into the training
    process can help the models better handle the variability and unpredictability
    of real-world speech data.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Whisper ASR 的背景下，这些技术可以提高训练模型的鲁棒性和泛化能力。将随机性引入训练过程可以帮助模型更好地处理现实世界语音数据的多变性和不可预测性。
- en: Zero-shot transfer across languages
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跨语言的零样本迁移
- en: The synergy of data and technique to unlock Whisper’s most sci-fi capability
    – recognizing languages never explicitly seen during training! This is known as
    **zero-shot transfer learning** in speech recognition. Through exposure to sufficient
    diversity in its training data across multiple languages, Whisper learns to generalize
    linguistic structures and decode new languages it has never seen before.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与技术的协同作用解锁了 Whisper 最具科幻色彩的能力——识别在训练过程中从未明确接触过的语言！这在语音识别中被称为 **零样本迁移学习**。通过接触足够多样化的训练数据，Whisper
    学会了概括语言结构，并解码它从未见过的新语言。
- en: 'This cross-lingual transfer ability allows the model to be deployed for practical
    speech recognition tasks without needing custom training data for every new language
    of interest. It is an efficient method that imitates humans’ capacity to infer
    meanings and patterns in unfamiliar languages after learning multiple tongues.
    This technique pushes the boundaries on the versatility and broad applicability
    of speech AI systems such as Whisper to diverse global audiences. Thus, the Whisper
    model can remarkably adapt to languages not explicitly covered in its training
    process. This adaptability stems from the model’s exposure to multilingual training,
    where it learns connections between languages. As a result, even without direct
    training in specific languages, the model can effectively handle unseen languages.
    This multilingual training approach offers a significant advantage: it allows
    for efficient deployment to new target languages without costly data collection
    and retraining.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这种跨语言的迁移能力使得该模型可以用于实际的语音识别任务，而无需为每种新语言准备定制的训练数据。这是一种高效的方法，模仿了人类在学习多种语言后，推断不熟悉语言的含义和模式的能力。这项技术推动了语音
    AI 系统（如 Whisper）在全球不同语言观众中的适应性和广泛应用的边界。因此，Whisper 模型可以显著适应那些在其训练过程中未明确覆盖的语言。这种适应性源于模型对多语言训练的接触，在此过程中它学习了语言之间的联系。因此，即使没有对特定语言的直接训练，模型也能有效处理未见过的语言。这种多语言训练方法具有显著的优势：它允许在不需要昂贵的数据收集和重新训练的情况下高效地部署到新的目标语言。
- en: Under the hood, Whisper doesn’t memorize vocabulary but discovers deeper universal
    structures permeating all human speech. Linguists hypothesize common cognitive
    facilities shape spoken languages – patterns Whisper extracts through exposure
    to sufficient diversity. This permits an almost wizardly adaptability to unfamiliar
    languages – a remarkable achievement pushing the boundaries of multilingual speech
    AI!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术层面来看，Whisper 并不记忆词汇，而是发现贯穿所有人类语言的更深层次的普遍结构。语言学家假设，常见的认知能力塑造了口语语言——这些模式是 Whisper
    通过接触足够多样化的语言数据提取出来的。这使得 Whisper 对陌生语言具有几乎像魔法一样的适应能力——这是推动多语言语音 AI 边界的一个显著成就！
- en: By efficiently generalizing to unseen languages without explicit examples, zero-shot
    transfer learning makes deploying Whisper more accessible for diverse global use
    cases. This technique pushes boundaries on the versatility and broad applicability
    of speech AI systems to serve users speaking thousands of languages worldwide.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在没有明确示例的情况下高效地对未见过的语言进行泛化，零样本迁移学习使得部署 Whisper 更加方便，适用于各种全球使用场景。这项技术推动了语音 AI
    系统的多功能性和广泛适用性的边界，以服务全球讲述数千种语言的用户。
- en: Appreciating the importance of multitasking and multilingual capabilities in
    ASR systems
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认识到多任务处理和多语言能力在 ASR 系统中的重要性
- en: As we wrap up our exploration of Whisper’s remarkable multitasking and multilingual
    skills, it’s worth appreciating why these capabilities are vital for speech recognition
    systems to handle real-world scenarios.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束我们对 Whisper 出色的多任务处理和多语言技能的探讨时，值得思考这些能力为何对于语音识别系统在应对现实场景中至关重要。
- en: Meeting diverse end-user needs
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 满足不同最终用户的需求。
- en: Simply put, the unpredictable variability of human speech necessitates flexible,
    versatile ASR models. Whether it’s diverse languages, technical vocabulary, acoustic
    conditions, or multiple verbal tasks, end users have diverse needs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，人类语言的不可预测变异性要求具备灵活多样的自动语音识别（ASR）模型。无论是多种语言、技术词汇、声学条件，还是多种语言任务，最终用户都有多样化的需求。
- en: Whisper provides multilanguage support for 90 languages, spanning multiple language
    families such as Romance, Germanic, Slavic, and more. This breadth handles international
    user bases communicating in different tongues. The model architecture also permits
    zero-shot transfer – recognizing new languages without explicit training data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 提供 90 种语言的多语言支持，涵盖了浪漫语言、日耳曼语言、斯拉夫语言等多个语言家族。这种广泛性能够处理在不同语言之间进行交流的国际用户群体。该模型架构还支持零-shot
    转移——无需明确的训练数据，就能够识别新语言。
- en: Moreover, with the appropriate parameters, Whisper can handle niche vocabularies,
    such as medical terminology or legal jargon, that users frequently need to interpret
    accurately. The model acquires broad lexical coverage beyond common phrases by
    training on diverse web datasets.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过适当的参数设置，Whisper 可以处理诸如医学术语或法律术语等用户经常需要准确解读的专业词汇。该模型通过训练多样化的网络数据集，获得了超出常用短语的广泛词汇覆盖。
- en: Excelling at multitasking
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 擅长多任务处理。
- en: 'On a technical level, Whisper owes its versatile multitasking skills to specific
    architectural optimizations:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，Whisper 之所以能够具备多样的多任务能力，归功于其特定的架构优化。
- en: Soft alignment during training prevents overfitting on strict input-output alignments,
    improving generalization.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中的软对齐可以防止严格的输入输出对齐导致过拟合，从而提高模型的泛化能力。
- en: Multitask learning exposes the model to connections between related tasks, allowing
    for flexible knowledge transfer.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多任务学习使得模型能够接触到相关任务之间的联系，从而实现灵活的知识转移。
- en: Stochastic depth and dropout provide randomness to reduce reliance on specific
    neurons, improving robustness.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机深度和 dropout 提供了随机性，减少了对特定神经元的依赖，从而提高了模型的鲁棒性。
- en: These methods enable a single model to skillfully adapt between transcription,
    translation, sentiment analysis, keyword identification, and other speech processing
    objectives without losing accuracy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法使得单一模型能够熟练地在转录、翻译、情感分析、关键词识别及其他语音处理目标之间灵活转换，同时不失准确性。
- en: Future-proofing investments against shifting trends
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使投资具有未来保障，以应对趋势的变化。
- en: Speech recognition models are long-term investments intended to scale across
    regions over the years. Given the current pace of technological change, flexibility
    is vital to protecting value. Whisper’s multilingual zero-shot abilities and multitasking
    design proactively future-proof systems against new demands that arise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别模型是长期投资，旨在随着时间的推移在各个地区扩展。鉴于当前技术变化的速度，灵活性对于保护价值至关重要。Whisper 的多语言零-shot 能力和多任务设计能够主动为系统的未来需求提供保障。
- en: Whether there’s unexpected language growth in emerging markets or novel speech
    use cases, Whisper provides insurance against getting locked into fixed assumptions.
    This adaptability ensures companies don’t risk systems becoming outdated white
    elephants over shifting trends.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是在新兴市场中语言的意外增长，还是新型语音使用场景，Whisper 都能提供避免被固定假设束缚的保险。这种适应性确保了企业不会因为趋势变化而使系统成为过时的“白象”。
- en: Paving the way for more capable conversational agents
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为更强大的对话代理铺平道路。
- en: Finally, by showcasing sophisticated handling of linguistic and acoustic diversity
    with Whisper, OpenAI raises bars across speech recognition research. These impressive
    capabilities inspire others to push boundaries about assumptions of needing distinct
    narrow systems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过展示 Whisper 在语言和声学多样性处理方面的高超能力，OpenAI 提升了语音识别研究的标准。这些令人印象深刻的能力激励他人突破常规，重新审视是否需要独立、专门的系统。
- en: The era of learning a single language or task in isolation is ending. Users
    deserve and increasingly expect holistic speech solutions. Moving forward, integrated
    multifunctional models such as Whisper will pave the way for more capable conversational
    agents that understand natural language in all its glories and challenges!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 单一语言或任务孤立学习的时代即将结束。用户应得的且日益期待的是全面的语音解决方案。未来，像Whisper这样的集成功能模型将为更强大的对话代理铺平道路，使其能够理解自然语言中的各种精彩与挑战！
- en: Next, we’ll explore Whisper’s training methodology using weak supervision strategies
    to leverage large datasets effectively – even with limited human annotations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨Whisper使用弱监督策略的训练方法，如何有效利用大规模数据集——即使在人工标注有限的情况下。
- en: Training Whisper with weak supervision on large-scale data
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用弱监督在大规模数据上训练Whisper
- en: With Whisper’s multitasking transformer architecture covered, we’ll now explore
    the intricate training strategies that instilled its advanced speech recognition
    skills. Rather than just small, exquisitely annotated datasets, Whisper leverages
    terabytes of web speech data with semi-supervised techniques.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了Whisper的多任务变压器架构后，我们将进一步探讨其复杂的训练策略，这些策略赋予了Whisper先进的语音识别能力。Whisper不仅仅依赖小规模、精心注释的数据集，它还利用了数TB的网络语音数据，并结合半监督技术。
- en: The following sections will dive into Whisper’s web-scale data accumulation,
    pseudo-labeling via machine teachers, and architectural supports, which facilitate
    learning from noisy labels. We’ll walk through data programming paradigms and
    innovations on self-training, stochastic depth, and pretraining, all of which
    were instrumental to Whispher’s success. By the end, you’ll grasp how weak supervision
    enabled unmatched speech comprehension – unlocking customization for accents and
    vocabulary where getting robust annotation at scale remains impractical.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分将深入探讨Whisper的网页规模数据积累、通过机器教师进行伪标注以及架构支持，这些都促进了从噪声标签中学习的过程。我们将逐步介绍数据编程范式、自我训练、随机深度和预训练方面的创新，这些都是Whisper成功的关键。最终，你将理解弱监督如何使得语音理解能力无与伦比——为方言和词汇的定制化开辟了道路，而大规模的强标注在现实中依然难以实现。
- en: Introducing weak supervision
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入弱监督
- en: The traditional supervised learning paradigm has long been the gold standard
    in machine learning. It involves training models on a large amount of labeled
    data, where both the input and the desired output are provided. However, this
    approach has its limitations. Labeling data is time-consuming and often expensive,
    and obtaining a large amount of labeled data for every task is only sometimes
    feasible. This is where weak supervision comes into play.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的监督学习范式长期以来一直是机器学习的金标准。它通过大量标注数据训练模型，其中既有输入也有期望输出。然而，这种方法也有其局限性。标注数据既费时又昂贵，并且为每个任务获取大量标注数据并不总是可行的。这就是弱监督发挥作用的地方。
- en: What is weak supervision?
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 什么是弱监督？
- en: Weak supervision is a machine learning paradigm that leverages less accurate
    or *noisy* labels to train models. These labels can be generated using various
    methods, such as heuristics, crowdsourcing, or data augmentation. The key idea
    behind weak supervision is to use these noisy labels as a proxy for the true labels,
    with the understanding that they may not be 100% accurate.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督是一种机器学习范式，它利用不太准确或*噪声*标签来训练模型。这些标签可以通过多种方式生成，如启发式方法、众包或数据增强。弱监督的关键思想是将这些噪声标签作为真实标签的代理，前提是理解它们可能并不100%准确。
- en: The advantage of weak supervision is that it allows us to train models on a
    much larger scale than would be possible with fully supervised learning. By leveraging
    weakly labeled data, we can train models on millions or even billions of examples,
    leading to significantly improved performance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督的优势在于，它使我们能够在比完全监督学习更大规模的基础上训练模型。通过利用弱标注数据，我们可以在数百万甚至数十亿的样本上训练模型，从而显著提升性能。
- en: 'The concept of weak supervision, while advantageous for training models such
    as OpenAI’s Whisper, does have certain drawbacks that are important to consider:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督的概念，虽然对于训练像OpenAI的Whisper这样的模型有很大优势，但也有一些缺点是值得考虑的：
- en: '**Accuracy**: Weakly supervised models may be less accurate than fully supervised
    learning. The model might learn incorrect patterns or associations, leading to
    suboptimal performance.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：弱监督模型的准确性可能低于完全监督学习。模型可能会学习到不正确的模式或关联，导致性能不理想。'
- en: '**Model complexity**: Weak supervision often necessitates more complex models
    and training procedures. These models need to account for the noise in the labels,
    which can increase the complexity of the model and the computational resources
    required for training.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂性**：弱监督通常需要更复杂的模型和训练过程。这些模型需要处理标签中的噪声，这可能增加模型的复杂性以及训练所需的计算资源。'
- en: '**Evaluation difficulty**: Evaluating the performance of models trained with
    weak supervision can be challenging due to the absence of ground truth labels.
    This makes it hard to accurately assess and compare the model’s performance with
    other models.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估困难**：由于缺乏真实标签，评估使用弱监督训练的模型性能可能非常具有挑战性。这使得准确评估和比较模型的表现变得困难。'
- en: '**Bias in training data**: If the weak labels are biased in any way, this bias
    can be propagated to the model, leading to biased predictions. This issue is common
    in machine learning and can be particularly problematic in weak supervision, where
    the labels are less reliable.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据中的偏差**：如果弱标签存在任何偏差，这种偏差可能会传递到模型中，导致偏差预测。这一问题在机器学习中较为常见，在弱监督中尤其成问题，因为标签的可靠性较差。'
- en: '**Dependency on labeling functions**: Weak supervision relies heavily on labeling
    functions, which can vary in reliability and accuracy.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖标签函数**：弱监督在很大程度上依赖于标签函数，而这些标签函数的可靠性和准确性可能存在差异。'
- en: These considerations highlight the importance of being mindful of weak supervision’s
    potential limitations and challenges, especially in training sophisticated models
    such as Whisper.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些考虑因素突出了在训练复杂模型（如 Whisper）时，特别需要注意弱监督潜在的局限性和挑战。
- en: Frameworks and techniques in weak supervision
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 弱监督中的框架和技术
- en: In weak supervision, several technical frameworks and methodologies are employed
    to enhance the training process and improve model performance.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在弱监督中，采用了多个技术框架和方法论来增强训练过程并提高模型性能。
- en: One of the critical frameworks that are used in weak supervision is the **data
    programming paradigm**. This approach involves creating a set of labeling functions,
    which are heuristic rules or distant supervision techniques, to label a large,
    unlabeled dataset. These labeling functions can be noisy and may conflict with
    each other, but they are combined using a generative model to produce probabilistic
    labels for the training data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在弱监督中，使用的关键框架之一是**数据编程范式**。这种方法涉及创建一组标签函数，它们是启发式规则或远程监督技术，用于标记一个大型未标记的数据集。这些标签函数可能包含噪声并且可能相互冲突，但它们通过生成模型结合起来，为训练数据生成概率标签。
- en: Another essential technique is **multitask learning**, where a model is trained
    on multiple related tasks simultaneously to improve generalization by leveraging
    the commonalities and differences among the tasks. This is particularly useful
    in weak supervision scenarios, where data for some functions may be limited or
    noisy.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的技术是**多任务学习**，即在多个相关任务上同时训练模型，通过利用任务之间的共性和差异来提高泛化能力。这在弱监督场景中特别有用，因为某些函数的数据可能有限或存在噪声。
- en: '**Transfer learning** is also a crucial technique in weak supervision. It involves
    training a model on a large, labeled dataset (the source task) and then fine-tuning
    it on a smaller, related dataset (the target task). This approach allows the model
    to leverage the knowledge gained from the source task to improve performance on
    the target task, which is particularly useful when labeled data for the target
    task is scarce.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**也是弱监督中的一个关键技术。它涉及在一个大型带标签数据集（源任务）上训练模型，然后在一个较小的、相关的数据集（目标任务）上进行微调。这种方法允许模型利用源任务中获得的知识，提升在目标任务上的表现，尤其在目标任务的带标签数据稀缺时，特别有用。'
- en: In addition to these, several other techniques are used in weak supervision,
    such as **self-training** (where the model is used to label its training data),
    **co-training** (where two models are trained on different views of the data and
    used to label each other’s data), and **active learning** (where the model actively
    selects the most informative examples for labeling).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，弱监督中还使用了其他几种技术，如**自训练**（模型用于标注其训练数据）、**协同训练**（两个模型在数据的不同视角上进行训练，并相互标注对方的数据）、以及**主动学习**（模型主动选择最具信息量的示例进行标注）。
- en: These frameworks and methodologies are not mutually exclusive and are often
    combined to achieve the best results in weak supervision scenarios. They represent
    some of the most advanced techniques in machine learning and are at the forefront
    of research under weak supervision. ’However, it’s important to note that while
    these techniques are commonly used in weak supervision scenarios, the specific
    application of all these frameworks in Whisper is not explicitly detailed in the
    documents from OpenAI. Whisper’s training methodology, as discussed previously,
    leverages the principles of weak supervision, but whether it employs every single
    one of these techniques is not clearly stated.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架和方法论并非互斥，通常会结合使用，以在弱监督场景中取得最佳效果。它们代表了机器学习领域中一些最先进的技术，并处于弱监督研究的前沿。然而，需要注意的是，尽管这些技术在弱监督场景中被广泛使用，但OpenAI的文档中并未明确详细说明Whisper中是否应用了所有这些框架。正如之前讨论的，Whisper的训练方法论利用了弱监督的原则，但是否使用了其中的每一种技术并未明确说明。
- en: 'Of course, there are several challenges associated with using weak supervision
    in machine learning:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用弱监督训练机器学习模型也有若干挑战：
- en: '**Quality of labels**: The primary challenge with weak supervision is the quality
    of the labels. Since the labels are less precise and accurate than those used
    in fully supervised learning, the model may learn incorrect patterns or associations,
    leading to suboptimal performance.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签质量**：弱监督的主要挑战是标签的质量。由于标签不像完全监督学习中使用的标签那样精确和准确，模型可能学习到错误的模式或关联，导致次优的表现。'
- en: '**Model complexity**: Weak supervision often requires more complex models and
    training procedures. For instance, models may need to account for the noise in
    the labels, which can increase their complexity and the computational resources
    required for training.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂性**：弱监督通常需要更复杂的模型和训练过程。例如，模型可能需要考虑标签中的噪声，这会增加它们的复杂性并且需要更多的计算资源来进行训练。'
- en: '**Evaluation difficulty**: Evaluating the performance of models trained with
    weak supervision can be challenging. Since the ground truth labels are unavailable,
    it can be difficult to accurately assess and compare the model’s performance with
    other models.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估困难**：评估使用弱监督训练的模型的性能可能会面临挑战。由于没有真实标签，准确评估并与其他模型进行比较可能变得困难。'
- en: '**Bias in training data**: If the weak labels are biased in some way, this
    bias can be propagated to the model, leading to biased predictions. This is a
    common issue in machine learning and can be particularly problematic in weak supervision,
    where the labels are less reliable.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据中的偏差**：如果弱标签存在某种偏差，这种偏差可能会传递到模型中，导致偏差预测。这是机器学习中的一个常见问题，在弱监督中尤为突出，因为标签的可靠性较差。'
- en: '**Dependency on labeling functions**: In weak supervision, labeling functions
    generate weak labels. These functions can introduce their own biases and errors,
    and the quality of the weak labels is highly dependent on the quality of these
    functions.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签函数的依赖性**：在弱监督中，标签函数生成弱标签。这些函数可能引入自身的偏差和错误，弱标签的质量高度依赖于这些函数的质量。'
- en: Despite these challenges, weak supervision remains a promising approach for
    training machine learning models when large amounts of labeled data are unavailable.
    It’s crucial to carefully consider these challenges and develop strategies to
    mitigate them when using weak supervision.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，弱监督仍然是训练机器学习模型的一种有前景的方法，尤其在大量标注数据不可用时。使用弱监督时，仔细考虑这些挑战并制定缓解策略至关重要。
- en: Understanding the role of weak supervision in training Whisper
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解弱监督在训练Whisper中的作用
- en: Weak supervision was integral to training Whisper’s state-of-the-art speech
    recognition capabilities. By adopting this semi-supervised method, the model’s
    architects could utilize more speech training data harvested from the internet
    with no need for accurate labeling. This was essential for embedding a deep understanding
    of real-world linguistic nuances into the system. In the following sections, we’ll
    delve into how weak supervision functions within Whisper and how it’s critical
    to instilling real-world linguistic comprehension. We’ll also explore various
    strategies to manage label noise effectively during the training process. Later,
    we will expand our understanding of the data programming pipeline in the *Recognizing
    the benefits of using large-scale data for* *training* section.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督在训练Whisper的最先进语音识别能力中发挥了重要作用。通过采用这种半监督方法，模型的设计者能够利用更多来自互联网的语音训练数据，而无需准确标注。这对于将对现实世界语言细微差别的深刻理解嵌入系统至关重要。在接下来的部分，我们将深入探讨弱监督如何在Whisper中发挥作用，以及它如何对灌输现实世界语言理解至关重要。我们还将探索在训练过程中有效管理标签噪声的各种策略。之后，我们将在*认识使用大规模数据进行*
    *训练*的好处部分进一步扩展我们对数据编程流程的理解。
- en: Gathering diverse speech data
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集多样化的语音数据
- en: 'The starting point for weakly supervised training is assembling a massive,
    heterogeneous speech dataset scraped from public web sources: podcasts, audiobooks,
    YouTube videos, discussion forums, educational lectures, and movie dialogue corpus,
    to name a few.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督训练的起点是收集来自公共网络源的大规模异构语音数据集：播客、有声书、YouTube 视频、讨论论坛、教育讲座以及电影对话语料库等等。
- en: This exposes Whisper to far more acoustic patterns from vastly more speakers
    than smaller read-speech datasets. Natural pacing, overlapping dialogue, technical
    vocabulary – these real-world elements prepare Whisper for practical usage.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，Whisper能够接触到比较小的朗读语音数据集更多的语音模式，来自更多说话者的语音。自然的语速、重叠的对话、技术性词汇——这些现实世界的元素为Whisper的实际应用做好了准备。
- en: Weak supervision critically relied on quickly aggregating terabytes of public
    web data rather than costly human annotation. However, maximizing diversity along
    dimensions such as language, speaker demographics, and topics remained an engineering
    challenge. Custom web crawlers with heuristic sampling addressed this to collect
    heterogeneous training candidates.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督依赖于快速聚合公网上的TB级数据，而不是昂贵的人工标注。然而，在语言、说话者人口统计和话题等维度上最大化多样性仍然是一个工程挑战。定制的网页爬虫和启发式采样方法解决了这一问题，收集异构的训练数据。
- en: Generating noisy labels programmatically
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 程序化生成噪声标签
- en: 'With abundant unlabeled speech data gathered, the next phase creates *good
    enough* labels programmatically to facilitate training. As we covered earlier,
    that process is called pseudo-labeling. The process of pseudo-labeling involves
    several steps:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 收集到大量未标注的语音数据后，下一阶段是通过编程方式生成*足够好*的标签，以便于训练。正如我们之前提到的，这个过程叫做伪标签化。伪标签化过程包括几个步骤：
- en: The model is initially trained on a small amount of labeled data.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型最初是在少量标注数据上进行训练的。
- en: The trained model then predicts labels for the unlabeled data, creating pseudo
    labels.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，训练好的模型会为未标注的数据预测标签，从而生成伪标签。
- en: The model is retrained by combining the original labeled data and the newly
    pseudo-labeled data.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将原始标注数据和新生成的伪标签数据结合起来，重新训练模型。
- en: These techniques act as heuristic labeling functions, using associated text,
    metadata cues, or classification models to derive noisy labels judiciously. The
    uncertainty levels vary significantly between sources – translation tools produce
    approximate phrase alignment, while keyword extractors give precise but sparse
    signals.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术作为启发式标签函数，通过关联文本、元数据提示或分类模型来谨慎地推导噪声标签。不同来源之间的无确定性程度差异显著——翻译工具提供的是近似的短语对齐，而关键词提取器则提供准确但稀疏的信号。
- en: Whisper captured dependencies between heuristic labeling approaches by orchestrating
    varied label generators using a probabilistic graphical model. This guided aggregating
    the imperfect sources into consensus training labels with calibrated confidence
    scores.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper通过使用概率图模型协调不同的标签生成器，捕获启发式标签方法之间的依赖关系。这指导了将不完美的来源聚合为具有校准信心评分的一致性训练标签。
- en: Supporting semi-supervised learning
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持半监督学习
- en: 'Crucially, Whisper uses the following architectural innovations that support
    semi-supervised objectives critical for weak supervision approaches:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，Whisper使用了以下架构创新，支持对弱监督方法至关重要的半监督目标：
- en: '**Self-training**: This approach involves progressively growing labeled data
    by re-training the model on its predictions. The process stays confined to high-confidence
    regions to minimize noise accumulation, and active learning queries are used to
    identify error-prone candidates needing human verification. This method is effective
    in semi-supervised learning, allowing the model to learn from its high-confidence
    predictions, gradually improving its accuracy.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自我训练**：这种方法通过重新训练模型来逐步增长标签数据，训练过程仅限于高置信度区域，以最小化噪声积累，且使用主动学习查询来识别需要人工验证的易错候选。这种方法在半监督学习中非常有效，允许模型从其高置信度的预测中学习，逐步提高其准确性。'
- en: '**Stochastic depth**: Incorporating unique stochastic depth layers involves
    randomly dropping model blocks during training. This strategy prevents the model
    from overly relying on specific parameters, improving its resilience to noisy
    labels. It’s a beneficial technique for handling the inherent uncertainties and
    variabilities in semi-supervised learning environments.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机深度**：引入独特的随机深度层，意味着在训练过程中随机丢弃模型的某些模块。这一策略可以防止模型过度依赖特定参数，提升其对噪声标签的鲁棒性。在半监督学习环境中，处理固有的不确定性和变异性时，这是一种有益的技术。'
- en: '**Intermediate pre-training**: This involves intermediate self-supervised pre-training
    on reconstruction tasks, such as masking. The intermediate pre-training step provides
    functional regularization and helps learn robust data representations before the
    model undergoes label-aware tuning. It’s beneficial in reducing overfitting errors
    in weakly supervised data, a common challenge in semi-supervised learning scenarios.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中间预训练**：这涉及在重建任务（如掩蔽）上的中间自监督预训练。中间预训练步骤提供了功能性正则化，并帮助在模型进行标签感知调优之前学习稳健的数据表示。它有助于减少在弱监督数据中常见的过拟合错误，这是半监督学习场景中的一大挑战。'
- en: Collectively, these innovations enhance Whisper’s capability to handle the challenges
    of semi-supervised learning, particularly in contexts where labeled data is scarce
    or noisy. Each technique improves the model’s overall robustness and accuracy,
    making it well-suited for practical applications where fully supervised learning
    may not be feasible.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，这些创新提高了 Whisper 在应对半监督学习挑战方面的能力，尤其是在标签数据稀缺或噪声较大的情况下。每项技术都提升了模型的整体鲁棒性和准确性，使其非常适合于在完全监督学习不可行的实际应用中使用。
- en: Let’s understand how these architectural innovations translate into measurable
    performance improvements.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解这些架构创新如何转化为可衡量的性能提升。
- en: Benchmarking performance improvements
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能提升的基准测试
- en: 'Weak supervision training strategies have shown to be highly beneficial in
    ASR systems, as evidenced by comparing metrics on the standard LibriSpeech test
    set. The following table highlights two different training approaches and their
    corresponding **word error** **rates** (**WERs**):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督训练策略在 ASR 系统中已被证明非常有效，通过对标准 LibriSpeech 测试集上的指标进行比较得到了验证。下表突出显示了两种不同的训练方法及其相应的**词错误率**（**WERs**）：
- en: '| **Training Approach** | **WER** |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **训练方法** | **WER** |'
- en: '| --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Fully Supervised (Clean Data Only) | 5.8% |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督（仅清洁数据） | 5.8% |'
- en: '| Weak Supervision (Noisy Web Data) | 3.2% |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 弱监督（噪声网络数据） | 3.2% |'
- en: Table 3.1 – WERs of two different training approaches
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.1 – 两种不同训练方法的词错误率（WER）
- en: The fully supervised approach, which relies on clean, well-annotated data, achieved
    a WER of 5.8%. In contrast, the weak supervision approach, which utilizes noisy
    web data, significantly outperformed the fully supervised method with a WER of
    3.2%. This substantial improvement underscores the effectiveness of weak supervision
    in ASR.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 完全监督方法依赖于清洁且标注准确的数据，获得了 5.8% 的 WER。相比之下，利用噪声网络数据的弱监督方法，显著优于完全监督方法，达到了 3.2% 的
    WER。这一显著改进凸显了弱监督在 ASR 中的有效性。
- en: The LibriSpeech test
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: LibriSpeech 测试集
- en: The LibriSpeech test set collects English speech data from audiobooks in the
    public domain. It is part of the larger LibriSpeech corpus, a widespread ASR research
    dataset. The test set is explicitly used to evaluate the performance of ASR models,
    providing a standard benchmark for comparison across different systems.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: LibriSpeech 测试集收集了来自公共领域有声书的英语语音数据。它是更大规模的 LibriSpeech 语料库的一部分，该语料库是一个广泛使用的
    ASR 研究数据集。该测试集专门用于评估 ASR 模型的性能，为不同系统之间的比较提供标准基准。
- en: 'The LibriSpeech test set is divided into two subsets: *test-clean* and *test-other*.
    The *test-clean* subset contains cleaner recordings with less background noise
    and is generally easier for ASR models to transcribe. On the other hand, the *test-other*
    subset contains more challenging recordings with various types of noise and distortions.
    These subsets allow researchers to evaluate how well their ASR models perform
    under different conditions.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: LibriSpeech 测试集被分为两个子集：*test-clean* 和 *test-other*。*test-clean* 子集包含较清晰的录音，背景噪声较少，一般对
    ASR 模型的转录较为容易。另一方面，*test-other* 子集包含更具挑战性的录音，具有各种类型的噪声和失真。这些子集让研究人员能够评估他们的 ASR
    模型在不同条件下的表现。
- en: The LibriSpeech test set measures an ASR model’s WER in speech recognition research.
    WER is a standard metric in ASR that calculates the percentage of words incorrectly
    transcribed by the model. By comparing the WER on the LibriSpeech test set, researchers
    can gauge the relative performance of different ASR models or versions of the
    same model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: LibriSpeech 测试集衡量的是自动语音识别（ASR）模型在语音识别研究中的字错误率（WER）。WER 是 ASR 中的一个标准度量，用于计算模型转录错误的单词百分比。通过比较
    LibriSpeech 测试集上的 WER，研究人员可以评估不同 ASR 模型或同一模型不同版本的相对性能。
- en: 'Weak supervision leverages large-scale datasets that may contain inaccuracies
    or less precise annotations. Despite the potential noise in the data, the volume
    and diversity of the dataset enable the model to learn robust representations
    of speech. This method is particularly advantageous when it is impractical or
    too costly to obtain a large amount of fully annotated data. The success of weak
    supervision in reducing WER can be attributed to several factors:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督利用包含不准确或标注不精确的大规模数据集。尽管数据中可能存在噪声，但数据集的量和多样性使得模型能够学习到稳健的语音表示。这种方法在获取大量完全标注数据既不实际又过于昂贵时尤其具有优势。弱监督在减少
    WER 上的成功可以归因于几个因素：
- en: '**Diversity of data**: Noisy web data often includes various accents, dialects,
    and speaking styles, which can help the model generalize better to real-world
    scenarios.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据的多样性**：嘈杂的网络数据通常包括各种口音、方言和说话风格，这有助于模型更好地在实际场景中进行泛化。'
- en: '**Quantity over quality**: The sheer amount of data available for weak supervision
    compensates for the lower quality of individual data points. Through exposure
    to numerous examples, the model can discern patterns and correct errors.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数量重于质量**：可用于弱监督的大量数据弥补了单个数据点质量较低的缺点。通过接触大量示例，模型能够辨别模式并纠正错误。'
- en: '**Regularization effect**: Training on noisy data can have a regularizing effect,
    preventing the model from overfitting to the idiosyncrasies of a smaller, cleaner
    dataset.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化效应**：在嘈杂数据上训练可以起到正则化作用，防止模型对较小、较干净数据集的特异性过拟合。'
- en: '**Cost-effectiveness**: Weak supervision allows for the utilization of readily
    available web data, reducing the need for expensive and time-consuming data labeling
    processes.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本效益**：弱监督利用现成的网络数据，减少了昂贵且耗时的数据标注过程。'
- en: '**Innovative training techniques**: Data programming, multitask learning, and
    transfer learning are often employed in weak supervision to handle the noise in
    the data and improve learning efficiency.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创新的训练技术**：数据编程、多任务学习和迁移学习常常在弱监督中使用，以应对数据中的噪声并提高学习效率。'
- en: The results from the LibriSpeech test set demonstrate that weak supervision
    is a viable alternative to fully supervised learning and can lead to superior
    performance in ASR tasks. This finding is particularly relevant for developing
    ASR systems such as Whisper, where the ability to accurately transcribe speech
    in various conditions is paramount. Weak supervision in training such models is
    a promising direction that can lead to more accurate, resilient, and versatile
    ASR systems.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 LibriSpeech 测试集的结果表明，弱监督是完全监督学习的可行替代方案，并且在 ASR 任务中可能带来更优的表现。这个发现对于开发像 Whisper
    这样的 ASR 系统特别相关，其中在各种条件下准确转录语音的能力至关重要。在训练这类模型时采用弱监督是一条有前景的方向，可以使 ASR 系统更加准确、稳健和多功能。
- en: So, in summary, web-scale weak supervision was integral to unlocking Whisper’s
    advanced speech recognition prowess. Strategically aggregating imperfect labeling
    functions facilitated efficient access to massive, noisy datasets. Custom model
    architectures then isolated practical knowledge despite uncertainty – culminating
    in state-of-the-art performance.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，网络规模的弱监督是解锁 Whisper 高级语音识别能力的关键。战略性地聚合不完美的标签函数，便于高效地获取庞大而杂乱的数据集。然后，定制的模型架构能够在不确定的情况下提取实际的知识，最终实现了最先进的性能。
- en: 'As we appreciate the nuances of semi-supervised learning in enhancing Whisper’s
    capabilities, we must focus on another pivotal aspect of this technology’s advancement:
    the utilization of extensive datasets. This brings us to our next key topic: *Recognizing
    the benefits of using large-scale data* *for training*.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们意识到半监督学习在提升 Whisper 能力方面的细微差别时，我们必须关注技术进步的另一个关键方面：利用广泛的数据集。这引出了下一个关键话题：*认识到使用大规模数据进行训练的好处*。
- en: Recognizing the benefits of using large-scale data for training
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认识到使用大规模数据进行训练的好处
- en: Using large-scale data for training models such as OpenAI’s Whisper in ASR offers
    unprecedented benefits. Contrary to traditional methods that rely on smaller,
    meticulously labeled datasets, this approach hinges on the principle that exposure
    to vast, diverse datasets can significantly enhance a model’s ability to understand
    and interpret human speech in all its complexity.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大规模数据训练像 OpenAI 的 Whisper 这样的自动语音识别（ASR）模型带来了前所未有的好处。与依赖较小、精确标注数据集的传统方法相反，这种方法基于一个原则：接触庞大、丰富的数据集可以显著提升模型理解和解读人类语音的能力，涵盖其所有复杂性。
- en: One of the paramount benefits of using large-scale data is capturing the rich
    tapestry of language diversity. Human speech is incredibly varied, not just in
    terms of languages but also in accents, dialects, and colloquialisms. By feeding
    Whisper with extensive datasets encompassing these variations, the model becomes
    adept at understanding and transcribing speech from various linguistic backgrounds.
    This is akin to growing up in a multicultural environment, organically learning
    to understand different linguistic variations and accents, even in noisy environments.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大规模数据的一个最重要的好处是能够捕捉到丰富的语言多样性。人类的语言多样性不仅体现在不同的语言上，还包括口音、方言和俚语等方面。通过为 Whisper
    提供包含这些变化的大规模数据集，模型能够熟练地理解并转录来自不同语言背景的语音。这就像是在多元文化的环境中成长，自然而然地学习理解不同语言的变化和口音，即使是在嘈杂的环境中。
- en: Navigating noisy realms
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在嘈杂环境中导航
- en: Real-world speech is rarely clean and noise-free. Large-scale datasets typically
    include audio with background noises, overlapping conversations, and varying sound
    quality. Training Whisper on such data equips it to perform robustly in real-life
    scenarios, where ideal recording conditions are the exception rather than the
    norm. This robustness is crucial for practical applications in bustling city streets
    or office environments.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现实中的语音很少是干净无噪声的。大规模数据集通常包含背景噪音、重叠的对话以及不同的音质。对这些数据进行训练使得 Whisper 在现实场景中能够强大地表现出来，因为理想的录音条件通常是例外，而不是常态。这种强大的适应性对于繁忙的城市街道或办公环境中的实际应用至关重要。
- en: Human conversations are complex. They involve interruptions, non-linear discourse,
    and a range of emotions and intonations. Large datasets often contain such conversational
    intricacies, allowing Whisper to learn and adapt to the natural flow of human
    communication. This learning is not just about understanding the words but also
    about grasping the context, the emotional undertones, and the unspoken nuances
    of speech.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 人类对话是复杂的。它们涉及到打断、非线性的话语结构以及各种情感和语调。大规模的数据集通常包含这样的对话复杂性，使得 Whisper 能够学习并适应人类沟通的自然流动。这种学习不仅仅是理解单词，还包括把握上下文、情感暗示以及言语中未言明的细微差别。
- en: Embracing global linguistic variations
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拥抱全球语言变体
- en: Training Whisper on large-scale datasets also exposes it to various global linguistic
    variations. This exposure is essential in today’s interconnected world, where
    ASR systems are increasingly required to understand and transcribe multilingual
    content. From podcasts in European languages to YouTube videos in Asian dialects,
    each piece of data enriches Whisper’s linguistic repertoire.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模数据集上训练 Whisper 还使其能够接触到各种全球语言变体。这种接触在今天互联互通的世界中至关重要，因为 ASR 系统越来越需要理解并转录多语言内容。从欧洲语言的播客到亚洲方言的
    YouTube 视频，每一份数据都丰富了 Whisper 的语言库。
- en: An intriguing aspect of large-scale data is that not all data needs to be labeled
    perfectly. Whisper can learn from imperfect, *noisy* data, making the training
    process more akin to how humans learn languages – through exposure and contextual
    understanding rather than rote learning. This method also circumvents the extensive
    resources required for meticulously labeling vast datasets.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模数据的一个有趣之处是，并非所有数据都需要完美标注。Whisper可以从不完美的、*嘈杂*数据中学习，使训练过程更像人类学习语言——通过接触和上下文理解，而不是死记硬背。这种方法还避免了为大量数据集进行精细标注所需的庞大资源。
- en: Different industries often use specific jargon and terminologies. Large datasets,
    especially those sourced from specialized domains such as legal or medical fields,
    provide Whisper with the necessary exposure to this sector-specific language.
    This makes it an invaluable tool for professionals who require accurate transcription
    services that understand their industry’s language nuances.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 不同行业通常使用特定的行话和术语。大规模数据集，尤其是那些来自法律或医疗等专业领域的数据，为Whisper提供了必要的行业特定语言的暴露。这使得它成为需要准确转录服务的专业人士的宝贵工具，它能够理解行业语言的细微差别。
- en: Training Whisper with large-scale data is akin to preparing it for a journey
    through the diverse landscape of human speech. Just as a well-traveled individual
    gains a rich understanding of different cultures and languages, Whisper becomes
    adept at navigating the complexities of human communication through its exposure
    to vast and varied datasets. This journey, fueled by the power of large-scale
    data, is not just about building an efficient ASR system but creating a technology
    that understands and interacts with the human voice as naturally and accurately
    as possible.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大规模数据训练Whisper就像是在为它准备一场穿越人类语言多样化景观的旅程。就像一个旅行丰富的人对不同的文化和语言有着深刻理解一样，Whisper通过接触庞大且多样的数据集，熟练地应对人类交流的复杂性。这场由大规模数据驱动的旅程，不仅仅是构建一个高效的ASR系统，更是在创造一项尽可能自然、准确地理解和与人类声音互动的技术。
- en: Now that you understand these semi-supervised training strategies, the next
    step is digging deeper into the data – including annotation, utilization, and
    model optimization processes. In the upcoming section, we will unpack principles
    for curating optimal datasets for speech recognition systems. You’ll gain practical
    skills for assembling domain-specific corpora, efficiently labeling relevant examples,
    and fine-tuning models such as Whisper to maximize accuracy on target application
    scenarios.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了这些半监督训练策略，接下来的步骤是深入挖掘数据——包括标注、利用和模型优化过程。在接下来的部分中，我们将阐述为语音识别系统策划最佳数据集的原则。你将获得组建特定领域语料库、有效标注相关示例以及微调模型（如Whisper）以最大化目标应用场景中准确性的实际技能。
- en: Gaining insights into data, annotation, and model training
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据、标注和模型训练的洞察
- en: Now that we’ve covered Whisper’s semi-supervised training methodology, the next
    step is to dive deeper into curating optimal data for driving targeted performance
    gains. While web-scale corpora provide a strong starting point, fine-tuning for
    niche applications requires customized dataset development.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讲解了Whisper的半监督训练方法，接下来的步骤是深入挖掘如何策划最佳数据，以推动针对性性能提升。虽然Web规模的语料库提供了一个强有力的起点，但针对特定应用的微调需要定制数据集的开发。
- en: 'Keep in mind the concepts we already learned about regarding how transformers
    process sequences. Traditional sequence-to-sequence models, such as RNNs, process
    input sequences step by step, which can be time-consuming for long sequences.
    In contrast, transformers can simultaneously process all words in the input sequence,
    leading to faster training times. Whisper’s transformer sequence-to-sequence model
    is trained on various speech processing tasks, including multilingual speech recognition,
    translation, spoken language identification, and voice activity detection. As
    shown in *Figure 3**.4*, these tasks are jointly represented as a sequence of
    tokens to be predicted by the decoder, allowing a single model to replace many
    stages of a traditional speech-processing pipeline. The multitask training format
    uses a set of unique tokens that serve as task specifiers or classification targets:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们之前学过的关于变压器如何处理序列的概念。传统的序列到序列模型（例如RNN）是逐步处理输入序列的，这对于长序列来说可能比较耗时。而变压器则可以同时处理输入序列中的所有词语，从而加快训练时间。Whisper的变压器序列到序列模型已经在多种语音处理任务上进行了训练，包括多语言语音识别、翻译、口语语言识别和语音活动检测。如*图
    3.4*所示，这些任务通过一组标记共同表示，由解码器进行预测，从而使单一模型能够取代传统语音处理管道中的多个阶段。多任务训练格式使用一组独特的标记作为任务指示符或分类目标：
- en: '![Figure 3.4 – Whisper sequence-to-sequence training approach using transformers
    (Whisper’s GitHub repository. https://github.com/openai/whisper/tree/main)](img/B21020_03_4.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 使用变压器的Whisper序列到序列训练方法（Whisper的GitHub仓库：https://github.com/openai/whisper/tree/main)](img/B21020_03_4.jpg)'
- en: Figure 3.4 – Whisper sequence-to-sequence training approach using transformers
    (Whisper’s GitHub repository. https://github.com/openai/whisper/tree/main)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 使用变压器的Whisper序列到序列训练方法（Whisper的GitHub仓库：https://github.com/openai/whisper/tree/main）
- en: The following sections will unlock best practices for collecting in-domain data,
    efficiently annotating minimally viable samples, and tracking metrics to ensure
    integrity. We’ll cover precise monitoring of audio conditions, speaker attributes,
    and label distributions that maximize model learning. By the end, you’ll have
    actionable skills for assembling domain-adapted datasets – facilitating customizable
    speech recognition where industry terminology or specialized acoustic environments
    necessitate precision tuning.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几节将解锁收集领域数据的最佳实践、高效注释最小可行样本的技巧，以及跟踪指标以确保数据完整性。我们将涵盖音频条件、说话人特征和标签分布的精确监控，以最大化模型的学习效果。到最后，您将掌握组建领域适应数据集的可操作技能——为行业术语或特定声学环境需要精确调优的语音识别提供支持。
- en: Understanding the importance of data selection and annotation
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据选择和注释的重要性
- en: As we unpack principles for optimizing Whisper’s performance, an integral place
    to start is understanding best practices for curating training data tailored to
    speech recognition objectives. While weak supervision facilitates leveraging available
    web speech data, fine-tuning for niche applications necessitates more customized
    data curation.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入优化Whisper性能的原则时，理解针对语音识别目标量身定制的训练数据策划最佳实践是一个至关重要的起点。虽然弱监督有助于利用现有的网络语音数据，但针对细分应用的微调则需要更多定制化的数据策划。
- en: In this section, we’ll explore considerations around assembling domain-specific
    datasets, efficiently prioritizing labeling efforts, and methodologies for annotation
    – unraveling why these elements are vital to unlocking Whisper’s full potential.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨组建领域特定数据集时的考虑因素、如何高效地优先进行标注工作，以及注释方法——揭示这些要素为何对释放Whisper的全部潜力至关重要。
- en: Gathering in-domain training examples
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集领域内的训练样本
- en: While pre-training on large web corpora provides Whisper with strong general
    speech comprehension, optimal performance for specialized use cases requires in-domain
    training data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在大规模网络语料上进行预训练为Whisper提供了强大的通用语音理解能力，但在专业应用场景下，最佳性能仍然需要领域内的训练数据。
- en: For instance, a medical voice assistant needs exposure to terminology-heavy
    doctor-patient dialogue with ambient hospital noises to reliably transcribe examinations.
    News transcription models, on the other hand, demand political press conference
    recordings in international English dialects.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个医疗语音助手需要接触到术语密集的医患对话，并且还要考虑到医院的环境噪音，才能可靠地转录医学检查内容。而新闻转录模型则需要国际英语方言中的政治记者发布会录音。
- en: In-domain data matching target deployment environments expose Whisper to necessary
    vocabulary, acoustics, and linguistic patterns – driving 30-50% accuracy gains
    over web pre-training.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 领域内数据与目标部署环境的匹配，使Whisper接触到必要的词汇、语音特征和语言模式——相比网络预训练，这将带来30%-50%的准确度提升。
- en: However, collecting niche datasets can prove challenging. Recording real patient
    conversations requires navigating strict healthcare privacy policies while news
    agencies closely guard internal media assets.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，收集小众数据集可能会面临挑战。记录真实的患者对话需要遵循严格的医疗隐私政策，而新闻机构则严格保护内部媒体资产。
- en: Here, data programming strategies used in weak supervision facilitate tapping
    into niche data. Assembling synthetic in-domain training sets by mixing and corrupting
    web data provides a pragmatic alternative.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，弱监督下的数据编程策略有助于挖掘小众数据。通过混合和破坏网络数据，组装合成的领域内训练集提供了一种务实的替代方案。
- en: Prioritizing relevant data for annotation
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优先标注相关数据
- en: 'When training OpenAI’s Whisper, choosing the correct annotated data is vital.
    Annotation is like labeling: we tell the system what each piece of data means.
    This step is crucial in helping Whisper understand and interpret speech correctly.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练OpenAI的Whisper时，选择正确的标注数据至关重要。标注就像是给数据贴标签：我们告诉系统每一条数据的含义。这一步对于帮助Whisper正确理解和解读语音至关重要。
- en: Imagine we have a vast puzzle of different sounds and words. Picking the most
    distinct puzzle pieces first will help complete the picture faster, and selecting
    specific data for annotation will make training Whisper more efficient. This means
    we don’t need to label every sound; we focus on the ones that teach Whisper the
    most.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们有一个庞大的拼图，拼图中包含了不同的声音和单词。首先挑选最具辨识度的拼图块将有助于更快地完成图像，选择特定数据进行标注将使Whisper的训练更加高效。这意味着我们不需要标注每一个声音，而是专注于那些能帮助Whisper学习最多的声音。
- en: One exciting aspect is discovering *classes* in the data. Think of these as
    groups or categories that share standard features. For instance, Whisper might
    encounter various English accents. Each accent can be seen as a different class.
    By focusing on annotating representative samples of these accents, we help Whisper
    learn to recognize and understand them more accurately.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人兴奋的方面是发现数据中的*类别*。可以把这些看作是具有共同特征的群体或类别。例如，Whisper可能会遇到各种不同的英语口音。每种口音都可以看作是一个不同的类别。通过聚焦标注这些口音的代表性样本，我们帮助Whisper更准确地学习并理解这些口音。
- en: Focusing on annotation is about being wise with our resources. Instead of labeling
    everything, we strategically pick data representing different classes or groups.
    This way, Whisper learns a broad range of speech patterns without getting overwhelmed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于标注是关于如何明智地利用我们的资源。我们不是标注所有内容，而是有策略地挑选代表不同类别或群体的数据。通过这种方式，Whisper可以在不被淹没的情况下学习广泛的语音模式。
- en: In summary, prioritizing data for annotation means choosing the most informative
    and diverse examples that help Whisper learn the complexities of human speech
    more effectively. It’s like teaching a child by showing them various examples
    – this way, they learn to recognize and understand the world around them in all
    its diversity.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，优先考虑标注数据意味着选择最具信息性和多样性的样本，这些样本能够帮助Whisper更有效地学习人类语音的复杂性。这就像通过展示各种例子来教孩子——通过这种方式，他们学会了识别并理解周围世界的多样性。
- en: Employing efficient and accurate annotation methodologies
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采用高效准确的标注方法
- en: 'In the meticulous process of training Whisper, annotation plays a pivotal role.
    This section delves into how efficient and accurate annotation methodologies are
    vital for transforming raw audio into a richly annotated dataset. Best practice
    speech annotation involves the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在Whisper训练的精细过程里，标注起着至关重要的作用。本节将深入探讨高效且准确的标注方法如何对将原始音频转化为丰富标注数据集至关重要。最佳实践语音标注包括以下内容：
- en: '**Audio segmentation**: Consider a complex audio file as a continuous data
    stream. Our first audio segmentation task involves partitioning this stream into
    smaller, manageable units. This is akin to segmenting a lengthy code base into
    functional modules for better readability and maintenance. Each audio segment
    is accurately timestamped, ensuring a precise start and end. This meticulous process
    is supported by language change detection tools, similar to syntax highlighting
    in programming, helping annotators identify language transitions within the audio.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音频分段**：将一个复杂的音频文件视为一个连续的数据流。我们的第一个音频分段任务是将该流分割成更小、更易管理的单元。这类似于将冗长的代码库划分为功能模块，以便更好地阅读和维护。每个音频段都有准确的时间戳，确保了精确的开始和结束。这个精细的过程得到了语言变更检测工具的支持，类似于编程中的语法高亮，帮助标注员识别音频中的语言转换。'
- en: '**Two-pass transcription**: The annotation process for Whisper employs a two-pass
    transcription method. In the first pass, annotators transcribe the audio segments,
    akin to writing a preliminary draft in coding, focusing on getting the structure
    right. The second pass involves revisiting these transcriptions for refinement,
    akin to code review and debugging, where context is fully considered to ensure
    semantic coherence and accuracy.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二次转录**：Whisper的标注过程采用了二次转录方法。在第一次转录时，标注员将音频段转录下来，类似于编写代码时的初步草稿，关注于结构的准确性。第二次转录则是对这些转录内容进行精细修订，类似于代码审查和调试，确保在考虑语境的同时，语义的一致性和准确性得到保证。'
- en: '**Resolution tracking**: In software development, tracking changes and decisions
    is crucial for understanding the evolution of a project. Similarly, in Whisper’s
    annotation process, every decision made during transcription, especially in resolving
    ambiguities, is meticulously logged. This provides a comprehensive audit trail,
    offering insights into the nuances of language processing and helping to refine
    the model’s accuracy.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案追踪**：在软件开发中，追踪变化和决策对于理解项目的演变至关重要。同样，在Whisper的标注过程中，每一个在转录过程中作出的决策，尤其是在解决歧义时，都会被仔细记录。这提供了全面的审计追踪，提供了对语言处理细节的深入理解，帮助完善模型的准确性。'
- en: These techniques ensure accurate and consistent label quality – a must for speech
    recognition where discrepancies severely impact integrity.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术确保了标注质量的准确性和一致性——这是语音识别的必要条件，因为任何偏差都会严重影响准确性。
- en: Finally, interfacing labelers with intuitive interfaces increases throughput
    over tedious documentation. Expanding on the concept of grids displaying audio
    waveforms, imagine a complex dashboard in a data analysis tool. These grids offer
    a detailed visual representation of the audio’s waveform, similar to a plot graph
    representing data points in a statistical analysis. Annotators use these waveforms,
    which depict aspects such as intonation and rhythm, to make informed decisions
    on segmenting and annotating the audio. Accompanied by editing tools and searchable
    segment lists, this setup provides high control and precision, allowing annotators
    to navigate the audio data efficiently, akin to a data analyst sifting through
    large datasets using advanced querying and visualization tools.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，直观的界面使标注员能够提高处理繁琐文档的效率。以显示音频波形的网格概念为基础，想象一个复杂的数据显示工具面板。这些网格提供音频波形的详细视觉表现，类似于统计分析中表示数据点的图表。标注员使用这些波形，波形展现了语调、节奏等方面，帮助他们做出更明智的音频分段和标注决策。配合编辑工具和可搜索的分段列表，这种设置提供了高度的控制力和精确度，使标注员能够高效地导航音频数据，类似于数据分析师利用先进的查询和可视化工具筛选大数据集。
- en: The culmination of strategic data gathering, selective annotation, and interface
    tooling ultimately allows for the delivery of training sets purpose-built to expand
    Whisper’s specialized linguistic skills efficiently. Comprehensive coverage of
    niche vocabularies, acoustics, and conversations paves the way for extraordinary
    transcription prowess over complex speech frontiers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 战略性数据收集、选择性标注和界面工具的最终结合，能够高效地交付专为扩展Whisper语言技能而构建的训练集。对专业词汇、音响学和对话的全面覆盖，为跨越复杂语音领域的卓越转录能力铺平了道路。
- en: Now that we’ve explored how efficient and accurate annotation methods enhance
    Whisper’s learning process, let’s dive into how this expertly annotated data plays
    a pivotal role in training Whisper to understand and interpret our world of diverse
    sounds and languages.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了高效准确的标注方法如何提升Whisper的学习过程，接下来我们来深入了解这些精确标注的数据如何在训练Whisper理解和解读我们多样化的声音与语言世界中发挥关键作用。
- en: Learning how data is utilized in training Whisper
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习数据如何在训练Whisper时被利用
- en: Now that we’ve covered considerations for curating optimized datasets, the integral
    next question is, how is speech data consumed during Whisper’s training process?
    Understanding the intricacies of data utilization uncovers methodologies for translating
    annotated datasets into enhanced transcription prowess.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了优化数据集策划的考虑因素，接下来要解决的核心问题是，Whisper在训练过程中如何消费语音数据？理解数据利用的复杂性揭示了将标注数据集转化为增强转录能力的方法。
- en: In this section, we’ll unpack the key phases of ingestion, transformation, and
    model integration to demystify how recordings ultimately manifest as linguistic
    comprehension. Tracing this journey will also reveal techniques for monitoring
    data utilization signals to ensure integrity.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解构数据摄取、转换和模型集成的关键阶段，揭示录音如何最终表现为语言理解。追溯这一过程还将揭示监控数据利用信号的技术，以确保数据的完整性。
- en: Ingesting data from heterogeneous formats
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从异构格式中摄取数据
- en: The first step involves aggregating speech data from sources and providing varied
    audio encodings, such as MP3, WAV, and M4A, alongside text transcriptions in document
    formats such as Word, text files, or spreadsheets.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是从各个来源聚合语音数据，并提供多种音频编码格式，如MP3、WAV和M4A，同时提供如Word、文本文件或电子表格等格式的文本转录。
- en: These raw ingestion payloads pass through normalization pipelines, transforming
    them into optimized machine-readable tensors for learning. Audio gets decoded
    into consistent formats and then segmented into fixed windows (for example, 30
    seconds), which are easier for models to digest. Text gets cleaned of artifacts
    and broken into word/character tokens.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原始的摄取负载会经过归一化管道，转化为优化的机器可读张量用于学习。音频被解码为一致的格式，然后被切分成固定时长的窗口（例如30秒），这使得模型更容易处理。文本去除杂项并被分解为单词/字符令牌。
- en: For optional auxiliary modeling, accompanying metadata such as speaker age,
    gender, ethnicity, and so on is also cataloged. The output homogenized, machine-ready
    datasets facilitate efficient data loading and batching during training.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可选的辅助建模，还会记录伴随的元数据，如说话者的年龄、性别、种族等。输出的同质化、机器可读的数据集促进了训练过程中高效的数据加载和批处理。
- en: Applying augmentation to enhance variety
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用增强以提升多样性
- en: Domain-specific data post-ingestion still risks overfitting models to narrow
    data distributions that fail to generalize. Applying augmentations enhances diversity.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 域特定数据在摄取后仍然有可能使模型过拟合到狭窄的数据分布，导致无法泛化。应用数据增强可以提升多样性。
- en: '*Mixing background noises* provides acoustic robustness training by simulating
    public environments. *Modulating pitch and tempo* reduces reliance on narrow speaking
    style assumptions. *Synthesizing combinations of raw web speech pieces* better
    replicate natural dialogue dynamics.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '*混合背景噪音*通过模拟公共环境提供声学鲁棒性训练。*调节音高和节奏*减少了对狭窄语音风格假设的依赖。*合成原始网页语音片段的组合*更好地再现了自然对话的动态。'
- en: 'Strategically distorting training data forces models to focus more on linguistic
    patterns than memorization, improving generalizability. The companion Colab notebook
    for this chapter provides an example of using the Hugging Face `transformers`
    class to facilitate the massive augmentation of audio datasets:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 战略性地扭曲训练数据迫使模型更多地关注语言模式而非记忆，从而提高了模型的泛化能力。本章节的配套Colab笔记本提供了一个示例，展示如何使用Hugging
    Face的`transformers`类来便捷地进行大规模的音频数据集增强：
- en: '[PRE1]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this code snippet, we first load `WhisperFeatureExtractor` from the `transformers`
    library. Then, we define a `prepare_dataset` function that takes an example from
    our dataset, extracts its audio, and applies the feature extractor. Finally, we
    use the `map` function to apply this preprocessing step to the entire dataset,
    transforming each audio file into a format suitable for the Whisper model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们首先从`transformers`库加载`WhisperFeatureExtractor`。然后，我们定义一个`prepare_dataset`函数，该函数从我们的数据集中提取一个示例音频，并应用特征提取器。最后，我们使用`map`函数将此预处理步骤应用于整个数据集，将每个音频文件转化为适合Whisper模型的格式。
- en: Monitoring utilization to ensure integrity
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控数据利用以确保完整性
- en: Without care, defects in data ingestion or augmentation can fatally disrupt
    integrity. Missing transcripts, mismatched audio, out-of-sync segments, or excessive
    augmentation noise can undermine learning and performance. So, it’s imperative
    to understand the pivotal role of monitoring utilization in ensuring the integrity
    of the training process. This involves meticulously overseeing the data as it
    transforms into valuable insights, akin to a skilled artisan guaranteeing the
    quality of their craft.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不加以关注，数据采集或增强中的缺陷可能会致命地破坏完整性。缺失的转录、错误的音频匹配、不同步的片段或过度增强的噪音都可能破坏学习和表现。因此，理解监控使用在确保训练过程完整性中的关键作用是至关重要的。这涉及到精心监督数据的转换过程，类似于一位技艺精湛的工匠保证他们工艺的质量。
- en: Imagine that we are crafting a mosaic. Each tile represents a unique sound or
    phrase in our vast dataset. To create a mosaic that genuinely represents the diversity
    of human speech, we must ensure that no single color or pattern dominates the
    picture. This is where coverage metrics come into play in Whisper’s training.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们正在打造一幅马赛克，每一块瓷砖代表着我们庞大数据集中独特的声音或短语。为了创造一个真正代表人类言语多样性的马赛克，我们必须确保没有任何单一的颜色或图案主导了整幅画面。这正是覆盖率指标在Whisper训练中发挥作用的地方。
- en: '*Coverage metrics* act like a meticulous curator, scrutinizing our mosaic for
    balance and diversity. They help us identify if certain accents or dialects are
    underrepresented, ensuring that Whisper understands speech as colorful and varied
    as the mosaic we envision. For instance, if our coverage metrics reveal an underrepresentation
    of rural dialects, we can enrich the dataset accordingly. This ensures that Whisper’s
    comprehension is not confined to urban eloquence but is also attuned to the rustic
    nuances of rural speech.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*覆盖率指标*就像是一位细致的策展人，审视我们的马赛克是否均衡多样。它们帮助我们识别某些口音或方言是否被低估，确保Whisper理解的言语像我们设想的马赛克一样丰富多彩。例如，如果我们的覆盖率指标揭示出乡村方言的代表性不足，我们可以相应地丰富数据集。这确保了Whisper的理解不仅限于城市的流利表达，还能适应乡村言语中的朴素细微差别。'
- en: And then there’s the delicate art of augmentation – it’s about enriching the
    dataset without distorting the essence of the speech. *Augmentation caps* are
    like a dance of precision and restraint. We introduce variations in background
    noise, pitch, and tempo, but always within a carefully calibrated spectrum. This
    ensures that Whisper learns to navigate the cacophony of the natural world without
    losing the melody of the speech it seeks to understand.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是增强的微妙艺术——它是关于在不扭曲言语本质的前提下丰富数据集。*增强封顶*就像是一场精确与克制的舞蹈。我们引入背景噪音、音高和节奏的变化，但始终在精心校准的范围内。这确保了Whisper能够在不失去其想要理解的言语旋律的情况下，学会在自然世界的噪音中找到方向。
- en: Imagine our mosaics under the meticulous scrutiny of an expert artisan, where
    every tile is examined for its quality and fit. Human spot-checks in *data validation*
    serve this purpose in Whisper’s training. They involve keen-eyed experts who meticulously
    examine the data, catching subtle nuances and errors that automated systems might
    overlook. This process is like a final touch of craftsmanship, ensuring that each
    aspect of the training data aligns perfectly with the desired outcome. It’s a
    testament to the art of combining human intuition with technological precision,
    refining Whisper’s ability to interpret the myriad subtleties of human speech.
    You can prevent excessive distortion from losing meaning.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们的马赛克在一位专家工匠的细致审查下，每一块瓷砖都被仔细检查其质量和贴合度。人工抽查在*数据验证*中有着相似的作用，服务于Whisper的训练。它们涉及眼光敏锐的专家，他们细心地审查数据，捕捉那些自动化系统可能忽略的微妙差异和错误。这个过程就像是工艺的最后润色，确保每一个训练数据的细节都与预期结果完美契合。这是将人类直觉与技术精确结合的艺术，精细化Whisper对人类言语中千变万化细微之处的解读能力。这样可以防止过度的扭曲导致意义的丧失。
- en: Together, these inspection measures verify the coordinated delivery of quality
    input speech and supervision, something that’s critical for drawing correct connections
    between speech signals and language.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些检查措施共同验证了高质量输入语音和监督的协调交付，这是在语音信号与语言之间建立正确联系的关键。
- en: Employing sampling and order randomization
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用采样和顺序随机化
- en: As models process terabytes of speech data spanning millions of samples, feeding
    data sequentially risks skewing learning. Sample ordering biases or curriculum
    assumptions that emerge can distort model understanding. In machine learning,
    curriculum assumptions involve structured, progressive exposure of a model to
    training data based on the notion that specific sequences or complexities of data
    are more conducive to effective learning. These assumptions influence the order
    and complexity of the data fed to the model during its training phase. Still,
    they must be applied thoughtfully in the context of Whisper training to avoid
    imposing unnecessary limitations on the model’s learning potential.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型处理跨越数百万样本的数TB语音数据时，按顺序馈送数据可能会导致学习的偏斜。样本顺序偏差或课程假设的出现可能会扭曲模型的理解。在机器学习中，课程假设是指根据特定数据序列或复杂度的假设，结构化地逐步暴露模型于训练数据，这种做法认为特定的序列或数据复杂度有助于更有效的学习。这些假设会影响在训练阶段向模型提供数据的顺序和复杂度。但在Whisper训练的背景下，仍需谨慎应用，以避免对模型学习潜力施加不必要的限制。
- en: '**Stochastic data shuffling** is a technique for randomizing the order of data
    samples across epochs during training. This method helps prevent the model from
    learning any potential order patterns in the data that could lead to biased predictions.
    By randomizing the order of data, the model is exposed to a more diverse range
    of samples in each epoch, which can help it learn more generalized representations
    of the data.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机数据洗牌**是一种在训练过程中随机打乱数据样本顺序的技术。此方法有助于防止模型学习到数据中的任何潜在顺序模式，避免导致偏向预测。通过随机化数据的顺序，模型在每个训练轮次中能够接触到更多样化的样本，这有助于它学习数据的更具泛化性的表示。'
- en: '**Negative sampling**, on the other hand, is a technique used within training
    batches to help the model better discriminate between positive and negative examples.
    In this context, *positive* examples are those that align with the desired output,
    while *negative* examples are those that do not. By including these contrasting
    *negative* samples in the training batches, the model is challenged to learn more
    robust representations that can better handle edge cases.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '**负采样**则是训练批次中使用的一种技术，帮助模型更好地区分正例和负例。在这种情况下，*正例*是与期望输出对齐的样本，而*负例*则是那些不符合期望的样本。通过在训练批次中包括这些对比性的*负样本*，模型被挑战去学习更具鲁棒性的表示，从而更好地应对边界情况。'
- en: Using stochastic data shuffling and negative sampling in machine learning models
    is a powerful strategy for enhancing their robustness and generalizability, mainly
    when dealing with large and diverse datasets. These techniques are crucial for
    avoiding biases and ensuring the models can handle various data scenarios effectively.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中使用随机数据洗牌和负采样是一种增强其鲁棒性和泛化能力的有效策略，特别是在处理大型和多样化数据集时。这些技术对于避免偏差并确保模型能够有效处理各种数据场景至关重要。
- en: Tracking metrics such as perplexity over epochs
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪诸如困惑度等指标
- en: In training AI models such as OpenAI’s Whisper, tracking metrics such as **perplexity**
    over epochs is crucial. These metrics are proxy indicators of the model’s learning
    progress and ability to consume and learn from the data provided effectively.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练像OpenAI的Whisper这样的AI模型时，跟踪诸如**困惑度**等指标至关重要。这些指标是模型学习进度的代理指标，能够有效反映模型如何消化和从所提供的数据中学习。
- en: Perplexity
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度
- en: Perplexity, in the context of language modeling objectives, measures how surprised
    or uncertain the model is when encountering the text labels aligned with speech
    segments. A decreasing perplexity over time indicates that the model is improving
    its understanding of the coherence between learned audio patterns and textual
    representations. This means the model is becoming less *surprised* by the data
    it encounters, suggesting it is learning effectively from the training data.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言建模目标的背景下，困惑度（Perplexity）衡量的是模型在遇到与语音片段对齐的文本标签时有多么惊讶或不确定。困惑度随时间的降低表明模型在提高对学习到的音频模式与文本表示之间连贯性的理解。这意味着模型对所遇到的数据变得不那么*惊讶*，暗示它在有效学习训练数据。
- en: In addition to perplexity, **accuracy** is another important metric, particularly
    for classification tasks such as speech-to-text. Accuracy measures how well the
    model utilizes the annotations provided in the training data. A high accuracy
    indicates that the model effectively learns the correct audio and text data associations.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 除了困惑度，**准确率**是另一个重要指标，特别适用于如语音转文本等分类任务。准确率衡量模型如何利用训练数据中提供的注释。较高的准确率意味着模型有效地学习了正确的音频和文本数据关联。
- en: WER is a fundamental metric in speech recognition. It measures the percentage
    of errors in a model’s transcribed text compared to a reference transcription.
    It’s crucial for assessing Whisper’s accuracy in understanding and transcribing
    spoken language.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: WER 是语音识别中的基本指标。它衡量模型转录文本与参考转录文本之间的错误百分比。对于评估 Whisper 在理解和转录口语语言的准确性至关重要。
- en: Now, in scenarios where we have imbalanced classes, the **F1 score** is the
    harmonic mean of precision and recall. It provides a more nuanced understanding
    of Whisper’s performance, especially when false positives or negatives carry significant
    consequences. Less known metrics are the **receiver operating characteristic**
    (**ROC**) **curve** and the **area under the curve** (**AUC**). They are used
    to evaluate the performance of classification models at various threshold settings.
    ROC AUC is handy for dealing with probabilistic outputs, providing insight into
    the trade-off between true and false favorable rates.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在类别不平衡的场景中，**F1 分数**是精确度和召回率的调和平均值。它提供了对 Whisper 性能的更深入理解，特别是当假阳性或假阴性带来重大后果时。较少为人知的指标有**受试者工作特征**（**ROC**）**曲线**和**曲线下面积**（**AUC**）。它们用于评估分类模型在不同阈值设置下的性能。ROC
    AUC 对于处理概率输出非常有用，它提供了对真实与假阳性率之间权衡的洞察。
- en: On the other end of the popularity spectrum, **confusion matrix** tools are
    commonly used to visualize the performance of a classification algorithm. It shows
    the actual versus predicted classifications and helps us understand how well the
    model distinguishes between different classes. The same could be said for **mean
    squared error** (**MSE**) and **root mean squared error** (**RMSE**); they provide
    measurements of how well the model differentiates classes. Regression tasks within
    Whisper, MSE, and RMSE are critical for quantifying the average squared difference
    between the estimated and actual values. They are crucial indicators of the model’s
    predictive accuracy.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在受欢迎程度的另一端，**混淆矩阵**工具通常用于可视化分类算法的性能。它展示了实际分类与预测分类的对比，帮助我们理解模型在区分不同类别方面的表现。同样的说法适用于**均方误差**（**MSE**）和**均方根误差**（**RMSE**）；它们提供了模型区分类别的表现度量。在
    Whisper 中，回归任务、MSE 和 RMSE 对量化估计值与实际值之间的平均平方差至关重要。它们是模型预测准确性的关键指标。
- en: '**Data utilization histograms** are another tool used to diagnose areas of
    neglect across samples. These histograms can help identify parts of the data that
    the model is not effectively learning from, allowing for targeted improvements
    in the training process. Complementing histograms and monitoring the **norms of
    the gradients** and the **learning rates** can help diagnose training issues.
    For example, vanishing or exploding gradients can be identified, enabling adjustments
    to the learning process.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据利用直方图**是另一种用于诊断样本中忽视区域的工具。这些直方图有助于识别模型未能有效学习的数据部分，从而为训练过程中的针对性改进提供支持。与直方图一起监控**梯度的范数**和**学习率**可以帮助诊断训练问题。例如，可以识别消失或爆炸的梯度，从而调整学习过程。'
- en: 'Together, these metrics and tools help ensure that the models fully leverage
    the datasets and can guide attention to areas needing improvement. Visualizing
    audio signals can provide valuable insights into their characteristics. Here’s
    an example of how to plot the waveform of an audio sample using the `librosa`
    library in Python:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标和工具一起帮助确保模型充分利用数据集，并能够引导注意力集中在需要改进的领域。可视化音频信号可以提供关于其特征的宝贵洞察。以下是如何使用 Python
    中的 `librosa` 库绘制音频样本波形的示例：
- en: '[PRE2]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code snippet takes an audio example from the dataset, extracts the audio
    array and sampling rate, and then uses the `librosa` library’s `display.waveshow()`
    function to plot the waveform. The resulting visualization (*Figure 3**.5*) helps
    us observe the audio signal’s amplitude variations over time, which is useful
    for understanding the audio data’s structure and identifying patterns:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码从数据集中提取一个音频示例，获取音频数组和采样率，然后使用`librosa`库的`display.waveshow()`函数绘制波形图。生成的可视化效果（*图3.5*）帮助我们观察音频信号在时间上的振幅变化，这对理解音频数据的结构和识别模式非常有用：
- en: '![Figure 3.5 – Plot of an audio waveform](img/B21020_03_5.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – 音频波形图](img/B21020_03_5.jpg)'
- en: Figure 3.5 – Plot of an audio waveform
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – 音频波形图
- en: This data ingestion, transformation, and integration process enables Whisper
    to be imbued with annotated linguistic knowledge. Carefully managing this process
    allows for more effective learning at scale, translating painstaking human signals
    into extraordinary speech comprehension prowess.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 这一数据摄取、转化和集成过程使Whisper能够被赋予标注的语言知识。精心管理这个过程有助于更有效地进行大规模学习，将艰辛的人类信号转化为非凡的语音理解能力。
- en: Having delved into the nuances of data utilization in Whisper’s training, let’s
    pivot to uncover the intricate process of how this model is meticulously trained,
    a journey that further amplifies its remarkable speech recognition capabilities.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解Whisper训练中的数据利用细节后，我们将转向揭示模型如何精心训练的复杂过程，这一过程进一步提升了其卓越的语音识别能力。
- en: Exploring the process of model training in Whisper
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索Whisper模型训练的过程
- en: We’ve now reached an intriguing inflection point. With our translated datasets
    in hand, the next step is actively imparting accumulated speech-language comprehension
    into Whisper. This knowledge transfer occurs by iteratively tuning model parameters
    over training steps – molding linguistic connections.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达一个有趣的转折点。手头有了翻译后的数据集，下一步就是将积累的语音语言理解能力积极传递给Whisper。这一知识转移通过在训练步骤中不断调整模型参数来实现——塑造语言连接。
- en: Understanding this runtime *optimization process* is valuable for monitoring
    healthy progress and diagnosing issues. We’ll walk through critical phases, from
    configuring training regimes to tracking evaluation signals, culminating in comprehensive
    speech mastery.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个运行时的*优化过程*对监控健康的进展和诊断问题非常有价值。我们将从配置训练方案到跟踪评估信号，逐步走过关键阶段，最终实现全面的语音掌握。
- en: Configuring training parameters and infrastructure
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置训练参数和基础设施
- en: 'Launching a training session for a machine learning model involves a delicate
    balance between configuring hyperparameters, which control the learning dynamics,
    and the computational resources available. This balance is crucial to ensure efficient
    learning and optimal model performance. The most significant hyperparameters are
    batch size, learning rate, training steps, and enabling hardware acceleration.
    Let’s examine each in more detail:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 启动机器学习模型的训练会涉及到一个微妙的平衡，即配置控制学习动态的超参数与可用计算资源之间的平衡。这个平衡至关重要，以确保高效的学习和最佳的模型性能。最重要的超参数包括批量大小、学习率、训练步骤和启用硬件加速。我们来详细看看每一个：
- en: '**Batch size**: The batch size is a critical hyperparameter that determines
    the number of samples to be processed before the model updates its internal parameters.
    It represents a trade-off between computational efficiency and learning stability.
    A larger batch size allows the model to process more samples per update, which
    can lead to faster training. However, it also requires more memory and may lead
    to less stable learning due to having to average the gradients over a more significant
    number of samples. Conversely, a smaller batch size can lead to more stable learning
    and better generalization, but at the cost of slower training.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量大小**：批量大小是一个关键的超参数，决定了在模型更新其内部参数之前处理的样本数量。它代表了计算效率与学习稳定性之间的权衡。较大的批量大小可以让模型每次更新时处理更多的样本，从而加速训练。然而，这也需要更多的内存，并可能由于需要对更多样本的梯度进行平均，导致学习不稳定。相反，较小的批量大小可能导致更稳定的学习和更好的泛化能力，但会以训练速度较慢为代价。'
- en: '**Learning rate**: The learning rate is another crucial hyperparameter determining
    the step size at which the model updates its parameters. It controls the aggressiveness
    of the model updates. A high learning rate can cause the model to converge quickly,
    but it may also lead to overshooting the optimal solution. On the other hand,
    a low learning rate can lead to more precise convergence, but it may also cause
    the model to get stuck in suboptimal solutions or to converge very slowly.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：学习率是另一个关键超参数，决定了模型更新参数时的步长。它控制了模型更新的激进程度。较高的学习率可能使模型快速收敛，但也可能导致超过最优解。另一方面，较低的学习率可以导致更精确的收敛，但也可能使模型陷入次优解或收敛速度过慢。'
- en: '**Training steps**: The number of training steps is a hyperparameter that determines
    the duration of the training process. It represents a trade-off between computational
    resources and model performance. A more significant number of training steps allows
    the model to learn more complex patterns, but it also requires more computational
    resources and may lead to overfitting. Conversely, fewer training steps can save
    computational resources but may lead to underfitting.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练步数**：训练步数是一个超参数，决定了训练过程的持续时间。它代表了计算资源与模型性能之间的权衡。更多的训练步数使模型能够学习更复杂的模式，但也需要更多的计算资源，并可能导致过拟合。相反，较少的训练步数可以节省计算资源，但可能导致欠拟合。'
- en: '**Hardware acceleration**: Hardware accelerators such as **graphics processing
    units** (**GPUs**) and **tensor processing units** (**TPUs**) can significantly
    speed up the training process. These devices are designed to perform parallel
    computations efficiently, a common requirement in machine learning tasks. Using
    hardware accelerators can, therefore, lead to more efficient use of computational
    resources.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件加速**：硬件加速器，如**图形处理单元**（**GPU**）和**张量处理单元**（**TPU**），可以显著加快训练过程。这些设备旨在高效地执行并行计算，这是机器学习任务中的常见要求。因此，使用硬件加速器可以更高效地利用计算资源。'
- en: Incorrectly configuring these parameters can cause the learning process to diverge
    or progress very slowly, wasting valuable time that could be used for parameter
    tweaking. To avoid this, it is often beneficial to profile small runs first or
    to inherit hyperparameter settings from reference models. This approach can help
    streamline the setup stage and ensure that the learning process converges efficiently.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 错误配置这些参数可能导致学习过程偏离或进展缓慢，浪费本可以用来调整参数的宝贵时间。为避免这种情况，通常有益于先对小规模的运行进行性能分析，或者从参考模型中继承超参数设置。这种方法有助于简化设置阶段，确保学习过程高效收敛。
- en: Kickstarting with checkpoints
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从检查点启动
- en: In generative AI, the ability to harness pre-existing knowledge is a game-changer.
    This is where OpenAI’s Whisper shines, offering initialization checkpoints – pre-trained
    models that encapsulate the hard-won general speech knowledge from its original
    training. These checkpoints are not just static snapshots; they are dynamic knowledge
    repositories embodying the essence of Whisper’s learning journey.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能中，利用现有知识的能力是一个改变游戏规则的因素。OpenAI的Whisper就是在这一点上表现出色，它提供了初始化检查点——这些预训练模型概括了原始训练中所获得的宝贵通用语音知识。这些检查点不仅仅是静态快照；它们是动态的知识库，体现了Whisper学习过程的精髓。
- en: Whisper leverages these checkpoints instead of starting from scratch to warm-start
    its learning process. This approach transfers an innate understanding of speech
    and language, effectively sidestepping the heavy lifting needed to acquire essential
    linguistic competencies. In essence, these checkpoints serve as a springboard,
    accelerating the process of targeted specialization.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper利用这些检查点，而不是从头开始，以启动其学习过程。这种方法转移了对语音和语言的内在理解，有效避开了获得基本语言能力所需的繁重工作。实质上，这些检查点充当了跳板，加速了针对性专业化的过程。
- en: This transfer of knowledge via checkpoints is akin to the principles of continual
    learning techniques in machine learning. It provides a valuable head start, saving
    hours to days that would otherwise be spent rediscovering elemental speech concepts.
    This is not just a time-saving measure; it’s a strategic move that allows Whisper
    to focus on refining its capabilities and expanding its knowledge base.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查点进行的知识转移类似于机器学习中持续学习技术的原理。它提供了一个宝贵的起点，节省了本来需要花费数小时到数天来重新发现基本语音概念的时间。这不仅仅是节省时间的措施；它是一种战略性举措，使Whisper能够集中精力改进其能力并扩展其知识库。
- en: The power of checkpoints lies in their ability to encapsulate and transfer knowledge.
    They embody Whisper’s learning journey, encapsulating the lessons learned, the
    challenges overcome, and the knowledge gained. By leveraging these checkpoints,
    Whisper can hit the ground running, focusing on refining and expanding its capabilities
    rather than starting from scratch.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点的力量在于其封装和传递知识的能力。它们体现了 Whisper 的学习历程， encapsulating 了所学的教训、克服的挑战以及获得的知识。通过利用这些检查点，Whisper
    可以迅速投入到优化和扩展其能力的工作中，而不是从头开始。
- en: Tracking training dynamics
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪训练动态
- en: Training dynamics in machine learning models involve interconnected processes
    crucial for the model’s performance. These processes are initiated with the forward
    propagation of batches through the encoder and decoder layers, which generate
    predictions. The next step involves quantifying the loss, which is the discrepancy
    between the model’s predictions and target labels. This loss is then backpropagated
    to update the model parameters to minimize the loss. This cycle is repeated over
    the entire dataset for one training epoch.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型中的训练动态涉及许多相互关联的过程，这些过程对模型的性能至关重要。这些过程从批量数据通过编码器和解码器层的前向传播开始，生成预测结果。接下来是量化损失，即模型预测与目标标签之间的差距。然后，损失会被反向传播，以更新模型参数，从而最小化损失。这个过程会在整个数据集上重复进行一个训练周期。
- en: Actively monitoring metrics such as losses and prediction accuracies over epochs
    is essential. It provides a diagnostic *pulse* on the model’s learning progress
    and can alert us to potential issues, such as overfitting or label noise, which
    could compromise the model’s performance.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 积极监控诸如损失值和预测准确率等指标在训练周期中的变化至关重要。它为模型的学习进度提供了诊断性的*脉搏*，并能提醒我们潜在的问题，如过拟合或标签噪声，这些问题可能会影响模型的性能。
- en: In addition to these core processes, supplementary techniques can be incorporated
    to *regularize* the training process and optimize the model’s effectiveness. These
    include introducing noise into the model with stochastic depth and dropouts to
    prevent the model from relying on fragile patterns. **Ensembling**, which involves
    selecting robust solutions across model checkpoints, can also enhance the model’s
    performance.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些核心过程之外，还可以引入补充技术来*规范*训练过程并优化模型的效果。这些技术包括通过随机深度和丢弃法向模型中引入噪声，以防止模型依赖于脆弱的模式。**集成方法**，即从模型检查点中选择强健的解决方案，也可以提升模型性能。
- en: 'Furthermore, employing cyclical learning rates allows for rapid solution space
    exploration and a more focused refinement of the model parameters. Here’s how
    employing cyclical learning rates is helpful in training models such as Whisper:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，采用循环学习率可以快速探索解决方案空间，并更专注地优化模型参数。以下是采用循环学习率在训练如 Whisper 这样的模型中的优势：
- en: '**Overcoming local minima**: One of the significant challenges in training
    deep learning models is avoiding getting stuck in local minima—points in the training
    landscape that are not the optimal solution. Cyclical learning rates help by allowing
    the model to jump out of these local minima. When the learning rate is increased,
    it gives the model a boost of energy to escape these suboptimal points.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**克服局部最小值**：训练深度学习模型时的一个重大挑战是避免陷入局部最小值——训练过程中的一些不是最优解的点。循环学习率通过允许模型跳出这些局部最小值来帮助解决这个问题。当学习率增加时，它为模型提供了一股能量，帮助它摆脱这些次优点。'
- en: '**Faster convergence**: Traditional learning rate schedules typically start
    high and decrease over time. While this is generally effective, it can be slow.
    Cyclical learning rates can lead to faster convergence by periodically increasing
    the learning rate, encouraging more rapid solution space exploration.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加速收敛**：传统的学习率调度通常从较高的学习率开始，随着时间推移逐渐减小。虽然这种方式通常有效，但速度较慢。循环学习率通过周期性地提高学习率来促使解决方案空间的更快速探索，从而加速了收敛过程。'
- en: '**Reducing the need for fine-tuned learning rate scheduling**: Finding the
    proper learning rate schedule can be tedious and require much experimentation.
    By their nature, cyclical learning rates reduce the need for this fine-tuning.
    The cyclical approach automatically adjusts the learning rate, helping to find
    a good balance between exploration and exploitation of the solution space.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少对微调学习率调度的需求**：找到合适的学习率调度方案可能是繁琐的，并且需要大量实验。由于其本身的特点，循环学习率减少了这种微调的需求。循环方法会自动调整学习率，帮助在探索和利用解决方案空间之间找到良好的平衡。'
- en: '**Improved generalization**: By oscillating the learning rate, the model is
    exposed to a broader range of training scenarios. This can lead to a more robust
    model that generalizes unseen data better as it is not overly optimized for the
    specific characteristics of the training data.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的泛化能力**：通过调节学习率，模型能够接触到更广泛的训练场景。这可以使模型更强大，能够更好地泛化未见过的数据，因为它不会过度优化训练数据的特定特征。'
- en: '**Adaptability to various parts of the training process**: Cyclical learning
    rates can be beneficial in different training phases. For example, a higher learning
    rate can be used for faster convergence during the initial phase. A lower learning
    rate in later stages can help fine-tune the model’s parameters.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对训练过程各阶段的适应性**：周期性学习率在不同的训练阶段都有可能带来益处。例如，较高的学习率可以用于初期的快速收敛。较低的学习率可以帮助在后期阶段微调模型的参数。'
- en: All these techniques are creative cushions that help us overcome optimization
    sticking points, leading to a more versatile understanding of the data.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些技术都是创造性的缓冲区，帮助我们克服优化的瓶颈，从而更全面地理解数据。
- en: Ensembling
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 集成
- en: 'Ensembling refers to combining multiple predictive models to produce a single
    model that is often more accurate than any of the individual models alone. This
    approach is based on the idea that by aggregating the predictions of several models,
    the errors of one model are likely to be compensated for by the others, leading
    to improved overall performance. Ensembling methods in machine learning can be
    categorized into two broad types: sequential ensemble techniques and parallel
    ensemble techniques.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 集成指的是将多个预测模型组合成一个模型，这个模型通常比单个模型更准确。这种方法基于这样一个观点：通过聚合多个模型的预测结果，一个模型的错误可能会被其他模型弥补，从而提高整体性能。机器学习中的集成方法可以分为两大类：序列集成技术和并行集成技术。
- en: Sequential ensemble techniques, such as **Adaptive Boosting** (**AdaBoost**),
    generate base learners in a sequence where the predecessors’ performance influences
    each learner. The learners are weighted based on accuracy, and the final prediction
    is based on a weighted vote.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 序列集成技术，如**自适应提升**（**AdaBoost**），按顺序生成基学习器，其中前一个学习器的性能会影响每个后续学习器。学习器根据准确性加权，最终的预测是基于加权投票。
- en: Parallel ensemble techniques, such as random forest, generate base learners
    independently of each other, which encourages diversity among the learners. The
    final prediction is typically made by averaging the predictions of all the learners
    (for regression tasks) or by majority voting (for classification tasks).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 并行集成技术，如随机森林，通过相互独立地生成基学习器，鼓励学习器之间的多样性。最终的预测通常是通过平均所有学习器的预测结果（回归任务）或通过多数投票（分类任务）来进行的。
- en: Monitoring evaluation sets
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监控评估集
- en: In machine learning and specifically in training models such as Whisper, evaluation
    datasets serve as a critical benchmark for assessing capabilities outside the
    training environment. These datasets estimate the model’s generalizable performance,
    acting as a litmus test for how well it can apply its learned knowledge to new,
    unseen data.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，特别是在训练像 Whisper 这样的模型时，评估数据集作为评估训练环境外能力的关键基准。通过这些数据集，可以估算模型的泛化性能，作为模型能否将其学习到的知识应用于新的、未见过的数据的试金石。
- en: Keeping a close eye on metrics derived from these evaluation sets is essential
    for determining the right moment to conclude the training process. Evaluation
    datasets play a critical role in training Whisper, serving as a vital indicator
    of the model’s readiness for real-world application. These datasets, distinct
    from the training sets, are crucial for assessing Whisper’s ability to handle
    unseen data, ensuring its performance is not confined to the scenarios it was
    trained on.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 密切关注这些评估集所衍生的指标对于确定训练过程何时结束至关重要。评估数据集在 Whisper 的训练中起着至关重要的作用，作为模型是否准备好用于实际应用的重要指示器。这些数据集与训练集不同，对于评估
    Whisper 处理未见数据的能力至关重要，确保其性能不仅限于它所训练的场景。
- en: The primary use of these datasets is to monitor for overfitting, a condition
    where the model excels on training data but performs poorly on new, unseen data.
    Regular testing against evaluation datasets helps identify any signs of overfitting,
    ensuring that the model remains robust and generalizable.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集的主要用途是监控过拟合现象，过拟合指的是模型在训练数据上表现出色，但在新的、未见过的数据上表现较差。通过定期对评估数据集进行测试，帮助发现任何过拟合的迹象，确保模型保持稳健和具有良好的泛化能力。
- en: The performance on evaluation datasets also informs us when to conclude the
    training. If Whisper’s performance plateaus or declines on these sets, further
    training may not yield significant improvements, signaling readiness for deployment.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 评估数据集上的表现也告诉我们何时结束训练。如果Whisper在这些数据集上的表现达到平台或下降，进一步的训练可能不会带来显著改进，表明已准备好部署。
- en: Additionally, evaluation datasets assist in fine-tuning Whisper’s parameters
    for optimal performance. They help ensure that the model meets the necessary standards
    for accuracy and reliability before being deployed in practical applications.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，评估数据集帮助微调Whisper的参数以实现最佳性能。它们有助于确保模型在部署到实际应用程序之前达到必要的准确性和可靠性标准。
- en: These datasets are instrumental in fine-tuning Whisper to its optimal performance,
    guaranteeing its effectiveness and reliability in diverse real-world scenarios.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集在微调Whisper以达到最佳性能方面起到关键作用，保证其在多种实际场景中的效果和可靠性。
- en: Exporting deployment-ready checkpoints
  id: totrans-345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出部署就绪的检查点
- en: The final step in the training journey involves exporting the top-performing
    snapshots that have been saved throughout the training epochs. These checkpoints,
    which contain the model’s parameters, represent the culmination of the model’s
    learning and are ready for deployment in client applications.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 训练旅程的最后一步包括导出在训练周期中保存的表现最佳的快照。这些检查点包含模型的参数，代表模型学习的顶峰，并准备好在客户应用程序中部署。
- en: These exported checkpoints are not just static artifacts but the encoded essence
    of Whisper’s linguistic mastery. When deployed, they unlock the actual value of
    Whisper’s service, bringing its extraordinary speech recognition capabilities
    directly to the end users at the customer’s edge.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导出的检查点不仅仅是静态的工件，而是Whisper语言掌握的编码精髓。一旦部署，它们将直接向最终用户提供Whisper卓越的语音识别能力，为客户边缘带来实际价值。
- en: Moreover, the journey of improvement doesn’t end with deployment. The model
    can undergo retraining and refinement as new data becomes available, continuously
    enhancing its transcription abilities. This iterative process ensures that Whisper
    maintains a competitive edge as an ASR provider, adapting and evolving with the
    ever-changing landscape of speech and language.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着新数据的出现，改进之旅并不会在部署后结束。该模型可以进行重新训练和优化，持续提升其转录能力。这种迭代过程确保Whisper作为ASR提供者保持竞争优势，随着语音和语言领域不断变化而不断调整和进化。
- en: The culmination of meticulous configuration, tight feedback loops, and strategic
    regularization techniques ensures that models such as Whisper extract maximum
    value from the datasets they are trained on. This comprehensive approach translates
    vast amounts of speech data into highly performant speech recognition engines
    that are ready to meet and exceed user needs on a scale.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 细致配置、紧密反馈循环和战略正则化技术的结合确保了像Whisper这样的模型从其训练的数据中提取最大价值。这种全面的方法将大量的语音数据转化为高性能的语音识别引擎，以在各个规模上满足并超越用户需求。
- en: Now that we have a thorough understanding of Whisper’s training intricacies,
    let’s explore its synergistic potential when integrated with other pioneering
    technologies from OpenAI, opening doors to a realm of enhanced capabilities and
    applications.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经全面了解了Whisper的训练复杂性，让我们探索当其与OpenAI的其他开创性技术集成时所展现的协同潜力，打开了增强能力和应用的领域之门。
- en: Integrating Whisper with other OpenAI technologies
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Whisper与其他OpenAI技术集成
- en: As we unravel Whisper’s capabilities, an enticing new frontier emerges – synergizing
    its speech prowess with other cutting-edge AI technologies from OpenAI. Beyond
    operating in isolation, integrating Whisper unlocks new possibilities at the intersection
    of modalities such as vision, language, and acoustic understanding.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们揭示Whisper的能力时，一个引人入胜的新领域出现了——将其语音能力与OpenAI的其他尖端AI技术进行协同。集成Whisper不仅仅是孤立操作，还解锁了在视觉、语言和声学理解等多种模式交汇处的新可能性。
- en: The following sections explore the technical glue enabling these fused systems
    to drive more advanced applications. We’ll cover strategies for concatenating
    representations, cascading natural language tasks, and even steering generative
    imagery with speech context. By the end, you’ll have an expanded imagination for
    bringing Whisper with tools such as DALL-E and CLIP to bolster performance and
    unlock experiences enhanced with multisensory contextualization.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将探讨使这些融合系统驱动更先进应用的技术纽带。我们将介绍连接表示、级联自然语言任务，甚至通过语音上下文引导生成图像的策略。到最后，你将能拓展想象力，将Whisper与DALL-E、CLIP等工具结合，从而提高性能并解锁多感官情境化增强的体验。
- en: Understanding the synergies between AI models
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解AI模型之间的协同作用
- en: As we conclude unraveling Whisper’s inner workings, new frontiers await, synergizing
    its speech prowess with other cutting-edge OpenAI technologies. Diverse toolkits,
    from code-writing GitHub Copilot to creative DALL-E image generators, promise
    intriguing possibilities when interconnected with Whisper.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们逐步解开Whisper的内在工作原理，新的前沿领域正等待着它与其他前沿OpenAI技术的协同。多种工具包，从代码编写的GitHub Copilot到创意图像生成的DALL-E，当与Whisper互联时，承载着令人兴奋的可能性。
- en: But what stands explicitly to benefit from this cross-pollination? First, let’s
    ground our exploration by understanding possible synergies when combining modalities
    such as vision, language, and speech recognition. This cross-disciplinary vantage
    point reveals adjacent problems that Whisper integration helps advance.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪些领域会从这种跨学科融合中显著受益呢？首先，我们通过理解结合视觉、语言和语音识别等多模态时可能的协同效应来扎实我们的探索。这个跨学科的视角揭示了Whisper集成能够推动的相关问题。
- en: Enriching situational context for visual understanding
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 丰富情境背景以增强视觉理解
- en: 'Humans seamlessly integrate visual and auditory signals to reason about environments
    holistically. Yet historically, computer vision and speech comprehension advance
    in silos, unable to close this gap. However, fusing Whisper’s speech representations
    with visual analysis tools such as **Contrastive Language-Image Pretraining**
    (**CLIP**) allows us to transcend reliance purely on pixels. This promises more
    contextual visual intelligence applications:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够无缝整合视觉和听觉信号，从而全面推理环境。然而，历史上，计算机视觉和语音理解往往各自发展，无法弥合这个鸿沟。然而，将Whisper的语音表示与视觉分析工具如**对比语言-图像预训练**（**CLIP**）结合，能够突破单纯依赖像素的局限。这预示着更多的上下文视觉智能应用：
- en: '**Localizing noise sources**: Using speech cues to pinpoint defective machines'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定位噪音源**：利用语音线索定位故障机器'
- en: '**Understanding social dynamics**: Leveraging conversational details to refine
    relationship graphs'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解社会动态**：利用对话细节来完善关系图谱'
- en: This way, Whisper helps progress contextual visual understanding closer to human
    parity.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，Whisper帮助将上下文的视觉理解向人类水平逼近。
- en: CLIP
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP
- en: OpenAI’s CLIP is a model that uniquely connects vision and language. It is trained
    on various internet text paired with images, but unlike most AI models, it does
    not require the direct pairing of an image and its description during training.
    Instead, it learns to associate images and texts more broadly, allowing it to
    understand and generate descriptions for images it has never seen before.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的CLIP是一个独特地连接视觉和语言的模型。它在各种互联网文本与图像的配对数据上进行训练，但与大多数AI模型不同，它不需要在训练过程中直接将图像与其描述配对。相反，它通过广泛地关联图像和文本来学习，使其能够理解并生成未曾见过的图像描述。
- en: The synergy between CLIP and Whisper lies in their complementary capabilities.
    While Whisper can convert spoken words into written text, CLIP can understand
    and generate image descriptions based on that text. This combination can be particularly
    powerful in speech recognition and image understanding applications.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP和Whisper之间的协同作用体现在它们互补的能力上。Whisper可以将口语转换为书面文本，而CLIP可以根据这些文本理解并生成图像描述。这种结合在语音识别和图像理解应用中尤其强大。
- en: Let’s explore a scenario that illustrates this application. Imagine a visually
    impaired individual navigating a public museum. They are equipped with a wearable
    device integrating Whisper’s speech recognition and CLIP’s language-image understanding.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一个情境，来说明这个应用。假设有一个视力障碍者在公共博物馆中导航。他们佩戴着集成了Whisper语音识别和CLIP语言-图像理解的可穿戴设备。
- en: As the individual walks through different exhibit sections, they can ask questions
    about their surroundings, such as “*What is in front of me?*” or make specific
    requests, such as “*Describe the painting I’m facing*.” Whisper accurately transcribes
    these spoken queries into text. The wearable device has a camera that captures
    images of the individual’s surroundings. CLIP processes these images and understands
    the content based on the textual description it has been trained on. For instance,
    it can recognize and understand a painting, sculpture, or any other exhibit item
    in view.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当个体走过不同的展览区时，他们可以询问关于周围环境的问题，如“*我面前是什么？*”或提出具体的请求，例如“*描述我面前的画作*”。Whisper准确地将这些口头查询转录为文本。可穿戴设备上有一台相机，能够捕捉到个体周围环境的图像。CLIP处理这些图像，并根据其训练过的文本描述理解图像内容。例如，它可以识别并理解一幅画、一件雕塑或任何其他展品。
- en: The combined system then correlates the spoken queries with the visual context.
    For the statement “Describe the painting I’m facing,” Whisper’s transcribed text
    guides CLIP to focus on the specific object (the painting) within its visual frame.
    CLIP then provides a detailed description of the painting, which is converted
    back into speech and relayed to the user through an earpiece.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 该组合系统将口头查询与视觉上下文关联起来。例如，对于“描述我面前的画作”这一陈述，Whisper转录的文本引导CLIP专注于视觉框架中的特定物体（画作）。然后，CLIP提供画作的详细描述，转化为语音并通过耳机传递给用户。
- en: 'The benefits are apparent: the visually impaired individual receives real-time,
    context-aware descriptions of their surroundings, enhancing their experience and
    interaction with the environment. Essentially, the combination of Whisper and
    CLIP allows for a more natural and interactive way of accessing information as
    the user can speak to inquire about their surroundings. This technology can be
    extended to various environments, such as outdoor landmarks, educational settings,
    or everyday street navigation, providing enriched situational awareness for visually
    impaired users.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这些好处显而易见：视障人士可以实时获得与环境相关的描述，增强了他们与周围环境的互动体验。本质上，Whisper与CLIP的结合提供了一种更自然、互动的方式来获取信息，用户可以通过说话询问自己周围的情况。这项技术可以扩展到各种环境，如户外地标、教育场所或日常街道导航，为视障用户提供更丰富的情境意识。
- en: Advancing natural dialogue systems
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推动自然对话系统的发展
- en: Speech recognition provides critical infrastructure for conversational agents
    to intake questions or commands. This is often the starting point before downstream
    NLP, such as text generation or semantics analysis.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别为对话代理提供了关键的基础设施，用于接收问题或命令。这通常是下游自然语言处理（如文本生成或语义分析）的起点。
- en: Whisper’s capabilities extend beyond mere transcription of spoken words into
    text. It captures and interprets subtle elements of speech that are often overlooked
    but play a crucial role in communication. These include pause lengths, interruptions,
    and soft confirmations, which provide valuable context to the conversation.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper的功能不仅仅局限于将口语转换为文本。它捕捉并解读了语音中常被忽视的微妙元素，而这些元素在沟通中扮演着重要角色。这些元素包括停顿长度、打断和轻微的确认，它们为对话提供了宝贵的上下文。
- en: Integrating Whisper with models such as GPT promises a more organic dialogue
    flow. This results in more engaging and human-like interactions, transforming
    our interactions with AI systems.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 将Whisper与像GPT这样的模型整合，预示着更自然的对话流。这将带来更具吸引力和人性化的互动，改变我们与AI系统的互动方式。
- en: Unlocking multimodal personas and narratives
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解锁多模态人格和叙事
- en: The ability to process and interpret multimodal data is one of Whisper’s most
    powerful features. This capability allows for a more comprehensive understanding
    of the context and content of dialogues, thereby enhancing the quality and relevance
    of the generated responses.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 处理和解释多模态数据的能力是Whisper最强大的特点之一。这一能力使得对话的上下文和内容能得到更全面的理解，从而提升生成回答的质量和相关性。
- en: Whisper’s ability to preserve essential auditory essences is a critical feature
    that differentiates it from other ASR systems. While other systems might overlook
    the nuances of human speech, Whisper is designed to capture and interpret these
    subtleties. This capability allows Whisper to provide a more accurate and nuanced
    interpretation of spoken language, thereby enhancing the quality of the generated
    text.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 保留重要听觉元素的能力是其与其他语音识别系统的关键区别。虽然其他系统可能忽视人类语言的细微差别，Whisper 旨在捕捉和解读这些微妙之处。这一能力使
    Whisper 能够提供更准确、细腻的口语语言解读，从而提升生成文本的质量。
- en: Enhancing agricultural insights with multimodal Osprey AI and Whisper
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多模态 Osprey AI 和 Whisper 提升农业洞察力
- en: In the rapidly evolving field of agrotechnology, integrating OpenAI’s Whisper
    and Osprey AI presents a novel approach to plant and crop analysis. This combination
    offers a transformative solution for farmers and agronomists, providing deeper
    insights into agricultural practices.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的农业技术领域，整合 OpenAI 的 Whisper 和 Osprey AI 提出了一个全新的植物和作物分析方法。这一组合为农民和农艺师提供了一种变革性的解决方案，使他们能够更深入地了解农业实践。
- en: 'Osprey AI is a cutting-edge **multimodal large language model** (**MLLM**)
    that’s adept at interpreting and synthesizing diverse data forms, including text,
    images, and audio. This technology is particularly effective in generating comprehensive
    narratives and insights from combined visual and textual information. It is an
    ideal tool for applications that require detailed analysis and contextual understanding.
    Let’s explore a scenario in agricultural settings where Osprey AI and Whisper
    significantly enhance field analysis:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: Osprey AI 是一个前沿的**多模态大语言模型**（**MLLM**），擅长解读和综合多种数据形式，包括文本、图像和音频。这项技术在从结合的视觉和文本信息中生成全面的叙述和洞察方面尤为有效。它是需要详细分析和情境理解的应用的理想工具。让我们通过农业场景来探索
    Osprey AI 和 Whisper 如何显著提升田间分析：
- en: '- **Whisper’s application in the field**: Farmers or agronomists use a device
    integrated with Whisper to describe their observations while inspecting crops
    verbally. They might report issues such as “leaves on these tomato plants are
    showing yellow spots” or ask questions such as “What is the probable cause of
    wilted leaves in this row of corn?” Whisper efficiently converts these spoken
    inputs into accurate text.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '- **Whisper 在田间的应用**：农民或农艺师使用集成 Whisper 的设备口头描述他们在检查作物时的观察。他们可能会报告诸如“这些番茄植物的叶子出现黄色斑点”或询问类似“这一排玉米枯萎的叶子可能是什么原因？”等问题。Whisper
    高效地将这些口头输入转换为准确的文本。'
- en: '- **Integrating visual data with Osprey AI**: Concurrently, the device captures
    images of the plants in question. These images and the transcribed text from Whisper
    are fed into Osprey AI. Using MLLM capabilities, Osprey AI analyzes the combined
    data to understand the plants’ condition comprehensively.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '- **将视觉数据与 Osprey AI 相结合**：同时，设备会捕捉相关植物的图像。这些图像和来自 Whisper 的转录文本将输入到 Osprey
    AI。借助 MLLM 能力，Osprey AI 分析综合数据，全面了解植物的状况。'
- en: '- **Comprehensive crop analysis**: Osprey AI processes visual and textual data
    to identify potential issues, such as nutrient deficiencies, pest infestations,
    or diseases. For example, the yellow spots on tomato leaves mentioned by the farmer
    are analyzed in conjunction with the images. Osprey AI may conclude a diagnosis
    of a specific nutrient deficiency or disease, providing treatment recommendations.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '- **全面的作物分析**：Osprey AI 处理视觉和文本数据，以识别潜在问题，如营养缺乏、害虫侵扰或疾病。例如，农民提到的番茄叶子上的黄色斑点将与图像一起分析。Osprey
    AI 可能得出特定的营养缺乏症或疾病诊断，并提供治疗建议。'
- en: '- **Real-time feedback and guidance**: This integration offers farmers real-time
    feedback on crop health and actionable insights. It can suggest specific interventions,
    such as adjusting irrigation, applying particular fertilizers, or using targeted
    pest control methods tailored to the observed conditions.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '- **实时反馈与指导**：这一整合为农民提供了作物健康的实时反馈和可操作的洞察。它可以建议具体的干预措施，如调整灌溉、施用特定肥料或使用针对观察到的情况量身定制的害虫控制方法。'
- en: By leveraging Whisper and Osprey AI in agriculture, farmers gain access to a
    powerful tool that simplifies the process of monitoring and maintaining crop health
    and provides precise, data-driven recommendations for optimal crop management.
    This innovative approach marks a significant stride in precision agriculture,
    enabling more informed decisions that lead to healthier crops and higher yields.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将Whisper与Osprey AI结合应用于农业，农民可以获得一个强大的工具，简化作物健康监测和维护的过程，并提供精确、数据驱动的最佳作物管理建议。这一创新方法标志着精准农业的重大进步，使农民能够做出更明智的决策，从而提高作物健康和产量。
- en: Having explored Whisper’s advanced training processes and potential in diverse
    applications, let’s explore how its integration with other leading-edge technologies
    can further augment and expand Whisper’s capabilities, opening new horizons in
    our journey with this transformative tool.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了Whisper先进的训练过程和在多种应用中的潜力后，让我们进一步探讨它与其他前沿技术的集成如何进一步增强和拓展Whisper的能力，为我们与这一变革性工具的旅程开启新的视野。
- en: Learning how integration augments Whisper’s capabilities
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习集成如何增强Whisper的能力
- en: As we have seen, Whisper demonstrates remarkable prowess in speech recognition
    across diverse languages and tasks. However, integrating complementary AI technologies
    unlocks even more significant potential – augmenting Whisper’s capabilities and
    empowering innovative applications.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，Whisper在跨多种语言和任务的语音识别中展现了显著的能力。然而，集成互补的AI技术能够释放更大的潜力——增强Whisper的能力并推动创新应用的实现。
- en: This section will explore various integrations that *amplify* Whisper’s strengths.
    By understanding the technical synergies involved, you’ll gain skills to build
    systems that transcend Whisper’s transcription abilities alone. Let’s dive in!
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨各种集成方式，这些方式可以*增强*Whisper的优势。通过理解其中的技术协同效应，您将获得构建超越Whisper单一转录能力的系统的技能。让我们开始吧！
- en: Boosting performance with multi-encoder fusion
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过多编码器融合提升性能
- en: An impactful integration strategy combines multiple encoders focused on different
    modalities before joint processing. For example, fusing audio encoders such as
    Whisper and visual encoders like CLIP allows us to leverage speech and images
    to understand complex environments.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有影响力的集成策略将多个编码器结合在一起，这些编码器专注于不同的模态，之后进行联合处理。例如，融合语音编码器如Whisper和视觉编码器如CLIP，使我们能够利用语音和图像来理解复杂的环境。
- en: This architecture provides multiple perspectives on input scenarios before a
    consolidated decoding phase. Challenges such as identifying noise sources amid
    machinery or analyzing social group dynamics benefit from joint visual and auditory
    comprehension.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构在合并解码阶段之前提供了多角度的输入场景分析。识别机械噪声源或分析社会群体动态等挑战，得益于视觉和听觉的联合理解。
- en: 'The key lies in finding the proper fusion methodology to synergize different
    encodings most effectively:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于找到合适的融合方法，以最有效地协同不同的编码方式：
- en: '**Multistage cascading** pipelines the output of one encoder as input to another.
    This chains contextual understanding.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多阶段级联**将一个编码器的输出作为输入传递给另一个编码器。这种方式链式地构建了上下文理解。'
- en: '**Encoder concatenation** directly combines vector representations to retain
    modality specifics. Joint decoders then learn optimal mixing strategies.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器拼接**直接结合向量表示以保留模态特性。联合解码器随后学习最佳的混合策略。'
- en: '**Dual-encoder networks with shared weights** force common learned patterns
    across modalities. This transfers knowledge between encoders.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共享权重的双编码器网络**迫使跨模态的学习模式保持一致。这有助于在编码器之间传递知识。'
- en: So, by creatively fusing Whisper with visual AI such as CLIP, applications tap
    into the best of both sensory worlds!
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过创造性地将Whisper与视觉AI（如CLIP）融合，应用能够同时利用两种感知世界的最佳特点！
- en: Scaling NLP capabilities via speech chains
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过语音链条扩展NLP能力
- en: Whisper also interlinks powerfully with large language models such as GPT-4\.
    Consider conversational agents – while dialogue systems can intake text queries,
    adding Whisper as a speech frontend makes interactions more natural.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper还与大型语言模型如GPT-4强有力地互联。以对话代理为例——虽然对话系统可以接收文本查询，但加入Whisper作为语音前端可以让互动更加自然。
- en: But the benefits run deeper than hands-free operation. Whisper captures nuances
    such as pause lengths, interruptions, and confirmations lost in text. Propagating
    these speech dynamics into language models boosts contextual understanding and
    more organic agent responses!
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 但其好处不仅仅是免提操作。Whisper捕捉到了诸如停顿长度、打断和确认等在文本中丢失的细微差别。将这些语音动态传播到语言模型中，提升了上下文理解和更自然的代理响应！
- en: 'This speech-to-text-to-action pipeline is a force multiplier for NLP capabilities:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这个语音到文本再到行动的流程是NLP能力的倍增器：
- en: Multistep inference chains connect modalities
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步推理链连接不同模态
- en: Speech adds additional interaction signals beyond language alone
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音在语言之外增加了额外的交互信号
- en: More contextual comprehension enriches downstream processing
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更加丰富的上下文理解使下游处理更加完善
- en: Unlocking voice-based access to services via Whisper profoundly expands their
    accessibility and user experience.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Whisper解锁语音访问服务，极大地扩展了它们的可访问性和用户体验。
- en: Advancing creative applications via grounding
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过定位推动创意应用的发展
- en: Finally, interfaces between modalities spur creativity, too! In the context
    of NLP and ASR, the process of enhancing these systems by linking language to
    real-world knowledge or multimodal data is called **grounding**.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，不同模态之间的接口也能激发创造力！在NLP和ASR的上下文中，通过将语言与现实世界的知识或多模态数据相连接来增强这些系统的过程被称为**定位**。
- en: Grounding is about establishing mutual information required for successful communication
    and understanding between agents, whether humans or machines. In ASR, grounding
    can refer to integrating visual or other multimodal information to aid in recognizing
    and interpreting spoken language. For example, fine-grained grounding for multimodal
    speech recognition involves using visual information from different parts of an
    image to improve speech recognition related to those visual elements. This can
    help ASR systems recover a broader range of word types, including entities, adjectives,
    and verbs, by localizing relevant regions in an image corresponding to the spoken
    content. For instance, a **speech-scene graph grounding network** (**SGGNet^2**)
    has been proposed to robustly ground spoken utterances by leveraging the structure
    of a scene graph, which can be particularly useful in speech-guided navigation
    tasks ([https://arxiv.org/abs/2307.07468](https://arxiv.org/abs/2307.07468)).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 定位是指建立成功交流与理解所需的共同信息，无论是人与人之间，还是人机之间。在ASR中，定位可以指整合视觉或其他多模态信息来帮助识别和解释口语。例如，针对多模态语音识别的细粒度定位，涉及使用图像中不同部分的视觉信息来提高与这些视觉元素相关的语音识别。这可以帮助ASR系统恢复更广泛的单词类型，包括实体、形容词和动词，通过定位与口语内容相关的图像中的相关区域。例如，提出了一种**语音场景图定位网络**（**SGGNet^2**），通过利用场景图的结构稳健地对口语进行定位，这在语音引导的导航任务中尤为有效（[https://arxiv.org/abs/2307.07468](https://arxiv.org/abs/2307.07468)）。
- en: As we consider the promise of grounded language learning, the capabilities of
    models such as OpenAI’s Whisper come into focus. Whisper demonstrates astonishing
    accuracy in speech recognition across a breadth of domains, laying the foundation
    for more contextually aware applications. Now, let’s examine some examples of
    how integrating Whisper could significantly enhance interactive systems across
    industries.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑有根据的语言学习的前景时，像OpenAI的Whisper这样的模型的能力变得愈发重要。Whisper在各个领域的语音识别中展现了惊人的准确性，为更多具有上下文感知的应用奠定了基础。现在，让我们来看看一些例子，展示集成Whisper如何显著提升各行业的交互系统。
- en: Examining examples of applications that benefit from integration with Whisper
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查一些从Whisper集成中受益的应用示例
- en: We’ve explored powerful integrations that augment Whisper’s prowess – from visual
    grounding to creative narrations. But how might these technical opportunities
    manifest concretely as user-impacting capabilities? This closing section will
    overview promising applications to spark ideas that translate AI synergies into
    practical solutions.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了增强Whisper强大功能的强力集成——从视觉定位到创造性叙述。但这些技术机会如何在具体应用中转化为影响用户的能力呢？这一部分将概述一些有前景的应用，以激发将AI协同作用转化为实际解决方案的想法。
- en: Infusing virtual assistants with emotional intelligence
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为虚拟助手注入情感智能
- en: As we delve deeper into AI, one of the most promising applications of Whisper’s
    integration is the enhancement of virtual assistants’ emotional intelligence.
    Virtual assistants, such as Alexa, Siri, and Google Assistant, have become integral
    to our daily lives, assisting us in tasks ranging from setting reminders to controlling
    smart home devices. However, these assistants often stumble when conveying empathy
    and reading subtle social cues, making interactions feel robotic and impersonal.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们深入探索 AI，Whisper 集成最有前景的应用之一就是增强虚拟助手的情感智能。像 Alexa、Siri 和 Google Assistant
    这样的虚拟助手已经成为我们日常生活的一部分，帮助我们完成从设置提醒到控制智能家居设备等任务。然而，这些助手在传达同理心和解读细微的社交线索时常常出现困难，使得互动显得机械且缺乏个性。
- en: By integrating Whisper, we can unlock a new dimension of interaction for these
    virtual assistants. Whisper’s advanced speech recognition capabilities allow it
    to detect nuances in speech, such as pauses, sighs, laughter, and excited interruptions.
    This enables the assistant to react appropriately based on the conversational
    context, enhancing its relatability and likability.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集成 Whisper，我们可以为这些虚拟助手解锁一个全新的互动维度。Whisper 强大的语音识别能力使其能够捕捉到语音中的细微差别，如停顿、叹息、笑声和激动的插话。这使得虚拟助手能够根据对话的语境做出恰当的反应，从而增强其亲和力和可爱度。
- en: Imagine a virtual assistant that can engage users displaying frustration, celebrate
    good news shared excitedly, or know when to interrupt politely. This level of
    emotional skill intelligence can transform the user experience, making interactions
    feel more natural and engaging. It’s like having a conversation with a friend
    who understands your mood and responds accordingly, rather than a machine simply
    executing commands.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个虚拟助手能够与用户互动，展现出沮丧的情绪，庆祝兴奋的好消息，或是在适当的时候礼貌地打断对话。这种情感技能的智能可以改变用户体验，让互动更加自然和富有吸引力。就像是与一个了解你情绪并相应回应的朋友对话，而不是一个单纯执行命令的机器。
- en: Illustrating stories with dynamic imagery
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用动态图像讲故事
- en: Another exciting application of Whisper’s integration is in the realm of children’s
    learning apps. Traditionally, these apps display static illustrations alongside
    passages read aloud. But what if we could make these illustrations come alive,
    guided dynamically by Whisper’s speech encoding?
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 集成的另一个令人兴奋的应用是儿童学习应用领域。传统上，这些应用会展示静态插图，并配合朗读的文字。但如果我们能让这些插图生动起来，并通过
    Whisper 的语音编码动态引导呢？
- en: As young readers listen to fantastical tales and engaging educational concepts,
    associated imagery can be generated in real time to match the unfolding narrative
    context. This creates immersive environments representing people, places, and
    things mentioned alongside spoken audio. Imagine a child listening to a story
    about a brave knight fighting a dragon, and as the story unfolds, the images on
    the screen change to reflect the narrative. The knight charges, the dragon breathes
    fire, and the princess cheers – all in sync with the audio.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 当年轻读者听到充满幻想的故事和有趣的教育概念时，相关的图像可以实时生成，与 unfolding 的叙事情境相匹配。这会创造一个身临其境的环境，展示故事中提到的人物、地点和事物，并与音频同步。想象一个孩子在听关于勇敢骑士与龙战斗的故事，随着故事的发展，屏幕上的图像会变化以反映叙事内容。骑士冲锋，龙喷吐火焰，公主欢呼——这一切与音频完美同步。
- en: This dynamic imagery makes the learning experience more engaging and aids comprehension
    and retention. It’s a powerful way to bring stories to life and foster a love
    for learning in young minds.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 这种动态图像使得学习体验更加引人入胜，帮助理解和记忆。这是一种让故事栩栩如生、激发年轻人热爱学习的强大方式。
- en: Searching multimedia archives via voice
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过语音搜索多媒体档案
- en: The integration of Whisper also revolutionizes the way we search multimedia
    archives. Traditional content management systems struggle with speech data, focusing
    primarily on text search. However, leveraging Whisper unlocks voice-based information
    retrieval, even inside video and audio files.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 的集成还彻底改变了我们搜索多媒体档案的方式。传统的内容管理系统在处理语音数据时存在困难，主要集中在文本搜索上。然而，通过利用 Whisper，我们可以实现基于语音的信息检索，甚至是视频和音频文件内部的检索。
- en: Whether you’re searching corporate meeting records, video lectures, or radio
    archives, spoken queries powered by Whisper can rapidly pinpoint multimedia moments
    matching your search criteria. This voice-driven capability expands access and
    discoverability to rich, untapped audiovisual knowledge repositories.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是搜索公司会议记录、视频讲座还是广播档案，Whisper驱动的语音查询都能快速定位匹配搜索标准的多媒体时刻。这种语音驱动的能力扩展了对丰富、未开发的视听知识库的访问和发现。
- en: Imagine finding a specific moment in a long video meeting simply by saying,
    “*Find the part where we discussed the marketing strategy*,” or a student being
    able to locate a particular topic in a series of recorded lectures with a command
    such as, “*Show me the lecture where the professor explained quantum mechanics*.”
    This level of convenience and efficiency can save countless hours and make information
    retrieval a breeze.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，只需说“*找到我们讨论营销策略的那一部分*”，就能在长时间的视频会议中找到特定的时刻，或者学生能够通过命令“*展示教授讲解量子力学的那一课*”定位到一系列录制讲座中的某个特定主题。这种便利性和高效性能够节省无数小时，并使信息检索变得轻松。
- en: Summary
  id: totrans-422
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: As we unravel Whisper’s inner workings in this chapter, let’s consolidate the
    critical insights revealed during this exploration before proceeding to the customization
    pathways ahead.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，当我们解开Whisper的内部机制时，让我们在继续探索定制路径之前，巩固在本次探索过程中揭示的关键见解。
- en: We began by highlighting pioneering architectural advancements within Whisper’s
    transformer model backbone that upgrade speech recognition to new levels. Breakthrough
    encoder-decoder mechanics effectively extract signals across input speech to accurately
    generate transcriptions reflecting coherent meaning.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从突出Whisper变换器模型骨干中的开创性架构进展开始，这些进展将语音识别提升到新的高度。突破性的编码器-解码器机制有效地从输入语音中提取信号，准确生成反映一致意义的转录。
- en: Hierarchical transformers and time-restricted self-attention allow us to selectively
    focus on relevant utterance regions, striking a balance between detail and speed,
    which is crucial for conversational responsiveness. Extensive pretraining across
    90 languages develops versatile comprehension beyond template matching seen in
    previous ASR systems.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化变换器和时间限制的自注意力机制使我们能够选择性地聚焦于相关的发音区域，在细节和速度之间找到平衡，这对于对话响应性至关重要。通过在90种语言中进行广泛的预训练，Whisper发展出了超越以往ASR系统中模板匹配的多元化理解能力。
- en: These strategies translate manual efforts into maximal speech recognition gains,
    unlocking customization for industry terminology or noisy acoustic environments.
    We learned that modifying decoder sequence lengths, beam search widths, and context
    windows allows us to customize Whisper’s accuracy.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略将手动工作转化为最大的语音识别提升，解锁了行业术语或噪音环境的定制化。我们了解到，通过调整解码器序列长度、束搜索宽度和上下文窗口，我们可以定制Whisper的准确性。
- en: The next chapter will expand on how these strategies help transform speech recognition
    from mechanical transcription into flexible language understanding.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将扩展讨论这些策略如何帮助将语音识别从机械化转录转变为灵活的语言理解。
