- en: '<html:html><html:head><html:title>Prompt Engineering Guidelines and Best Practices</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-216">Prompt
    Engineering Guidelines and Best Practices</html:h1> <html:div id="_idContainer115"><html:p>In
    this chapter, we embark on an exploration of how the latest advancements in technology
    are reshaping our interaction with digital tools and applications. As the digital
    landscape evolves, the traditional interfaces we’ve relied upon for decades are
    being reimagined, paving the way for more intuitive and efficient forms of communication
    between humans and machines. At the heart of this transformation is the advent
    of <html:a id="_idIndexMarker1071"></html:a>conversational interfaces powered
    by <html:strong class="bold">natural language</html:strong> ( <html:strong class="bold">NL</html:strong>
    ). As a result, understanding how to write effective prompts to customize the
    behavior of our LlamaIndex components becomes a critical skill in building and
    improving <html:span class="No-Break">RAG applications.</html:span></html:p> <html:p>Therefore,
    in this chapter, we’re going to cover the following <html:span class="No-Break">main
    topics:</html:span></html:p> <html:ul><html:li>Why prompts are your <html:span
    class="No-Break">secret weapon</html:span></html:li> <html:li>Understanding how
    LlamaIndex <html:span class="No-Break">uses prompts</html:span></html:li> <html:li>Customizing
    <html:span class="No-Break">default prompts</html:span></html:li> <html:li>The
    golden rules of <html:span class="No-Break">prompt engineering</html:span></html:li></html:ul>
    <html:a id="_idTextAnchor216"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-217">Technical requirements</html:h1> <html:div id="_idContainer115"><html:p>All
    code samples from this chapter can be found in the <html:code class="literal">ch10</html:code>
    subfolder of the book’s GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor217"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Why
    prompts are your secret weapon</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-218">Why prompts are your secret weapon</html:h1> <html:div id="_idContainer115"><html:p>I
    was 6 years old when I started writing my first lines of code using a ZX Spectrum
    computer. At the time, in the mid-1980s, computers were still a new thing in the
    world, and not many people <html:a id="_idIndexMarker1072"></html:a>understood
    the extraordinary impact they were going to have on human society. Today, we all
    live in a reality dominated, and in many ways, driven by technology. The way we
    relate to technology has also changed fundamentally in the last 40 years. Almost
    all human activities have come to be touched to a greater or lesser extent by
    <html:span class="No-Break">technological progress.</html:span></html:p> <html:p>What
    hasn’t changed much is the way we interact with technology. With a few notable
    exceptions – such as the introduction of touch screens and voice interfaces –
    our interaction with technology has remained almost unchanged. We use, as we did
    40 years ago, rudimentary methods to get computers to perform the functions <html:span
    class="No-Break">we need.</html:span></html:p> <html:p class="callout-heading">Clarification</html:p>
    <html:p class="callout">When I say rudimentary, I’m not necessarily referring
    to the sophistication of the interface itself – although functionally, if we were
    to compare a modern-day keyboard or mouse, we would find that even here, the advances
    are not fantastic. I’m referring rather to another aspect that unfortunately continues
    to stagnate: the bandwidth that our current interfaces <html:span class="No-Break">can
    offer.</html:span></html:p> <html:p>The way we currently interact with our technology
    is long due <html:span class="No-Break">for replacement.</html:span></html:p>
    <html:p>Let’s go through a simple <html:span class="No-Break">rationale together:</html:span></html:p>
    <html:ul><html:li>The computing power offered by IT systems continues to grow
    at a rapid pace. Even if Moore’s law – see <html:span class="No-Break"><html:em
    class="italic">Figure 10</html:em></html:span> <html:em class="italic">.1</html:em>
    – is arguably no longer considered a valid benchmark, progress is far from slowing
    <html:span class="No-Break">down (</html:span> <html:a><html:span class="No-Break">https://en.wikipedia.org/wiki/Moore%27s_law</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:li> <html:li>We live in a world
    dominated almost entirely by applications. At the moment, applications are the
    layer between the user and the machine that makes our interaction with a computer
    possible – apps running on local systems, apps running on mobile devices, or apps
    running in the cloud. Each app offers a very specific set <html:span class="No-Break">of
    functionalities.</html:span></html:li> <html:li>Many apps are designed to run
    only on specific platforms and cannot be easily ported to other platforms. This
    means a different app for each <html:span class="No-Break">specific platform.</html:span></html:li>
    <html:li>Many applications overlap in terms of functionality. For a given task
    there are, in most cases, dozens of different applications that can perform it.
    So, there is a lot <html:span class="No-Break">of duplication.</html:span></html:li>
    <html:li>Our interaction with technology has remained broadly the same bandwidth
    as 40 years ago. We use almost the same types of interfaces – keyboard, mouse,
    touchscreen, gesture- or voice-based – to control <html:span class="No-Break">application
    logic.</html:span></html:li> <html:li>Almost every application <html:a id="_idIndexMarker1073"></html:a>comes
    with its own UI. There is a mandatory learning curve that users have to go through
    to learn how to operate each application. If we multiply this time by the number
    of applications that a typical user uses on a regular basis, we find that we actually
    spend a lot of time learning to use a tool effectively, and this eats into the
    actual time we spend using the tool to <html:span class="No-Break">be productive.</html:span></html:li>
    <html:li>The number of software applications – including both publicly available
    applications and those used privately by organizations – is already huge. There
    are already more than 1 billion applications in the world. That’s without taking
    into account the fact that an application very often exists in several different
    versions. And the number <html:span class="No-Break">is growing.</html:span></html:li>
    <html:li>From an evolutionary point of view, the capacity of the human brain has
    remained unchanged throughout this time. Neuroplasticity gives us a remarkable
    ability to learn and adapt to new technologies, but unfortunately, evolution itself
    cannot keep up with <html:span class="No-Break">technological progress.</html:span></html:li></html:ul>
    <html:p class="IMG---Caption" lang="en-US">Figure 10.1 – According to Moore’s
    law, the number of transistors roughly doubles every 2 years</html:p> <html:p>See
    where I’m aiming? This very specific way of interacting with technology, combined
    with the rapid <html:a id="_idIndexMarker1074"></html:a>evolution of technology,
    is slowly making us victims of our own success. On the one hand, we have managed
    to build a huge number of specialized tools capable of solving a huge number of
    problems. But now, we have a bigger problem: we have so many tools that organizing
    and using them efficiently has become an extremely complicated process. A new
    paradigm <html:span class="No-Break">is needed.</html:span></html:p> <html:p>Conversational
    <html:a id="_idIndexMarker1075"></html:a>interfaces, based on <html:strong class="bold">natural
    language processing</html:strong> ( <html:strong class="bold">NLP</html:strong>
    ), present themselves as a promising alternative to the current way of interacting
    with technology. They represent a natural evolution in the way we communicate
    with our devices. Instead of relying on complex visual interfaces and input methods
    that require effort and time to learn, conversational interfaces allow us to use
    NL – the most fundamental and intuitive form of <html:span class="No-Break">human
    communication.</html:span></html:p> <html:p>This is where a new core competency
    in this new paradigm <html:span class="No-Break">comes in.</html:span></html:p>
    <html:p class="callout-heading">Prompt engineering</html:p> <html:p class="callout">As
    human-machine interaction becomes increasingly dependent on NL, the ability to
    formulate <html:a id="_idIndexMarker1076"></html:a>effective prompts that guide
    <html:strong class="bold">artificial intelligence</html:strong> ( <html:strong
    class="bold">AI</html:strong> ) algorithms toward <html:a id="_idIndexMarker1077"></html:a>desired
    responses or actions becomes essential. This skill involves not only formulating
    prompts clearly but also anticipating how different formulations may influence
    the interpretation and execution of commands by <html:span class="No-Break">the
    AI.</html:span></html:p> <html:p>Conversational interfaces transform the interaction
    with technology into a dialogue where linguistic precision and understanding of
    algorithmic subtleties become key factors in achieving desired outcomes. The ability
    to interact directly and effectively with computer <html:a id="_idIndexMarker1078"></html:a>systems
    using NL can significantly reduce the barrier between humans and technology. It
    offers a pathway to democratizing access to technology, making it accessible to
    a wider range of users, regardless of their <html:span class="No-Break">technical
    expertise.</html:span></html:p> <html:p>There are already indications that the
    intensive use of prompts in our everyday interactions with LLMs can improve even
    our interpersonal communication skills, as shown, for example, by this study:
    Liu et al. (2023), <html:em class="italic">Improving Interpersonal Communication
    by Simulating Audiences with Language</html:em> <html:span class="No-Break"><html:em
    class="italic">Models</html:em></html:span> <html:span class="No-Break">(</html:span>
    <html:a><html:span class="No-Break">https://doi.org/10.48550/arXiv.2311.00687</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>Imagine computer systems
    that can replace the functionality of dozens or even hundreds of different applications
    but without the complexity of traditional interfaces. Language interaction: a
    form of technology where LLMs, augmented with RAG, take the place of applications
    and operating systems, giving us a universal and much simpler way to use computing
    power. Without getting too deep into the area of speculation, if I were to make
    a medium-to-long-term prediction, this is the direction I think we are heading
    in. In the short term, classical computing systems will continue to prevail. At
    first, conversational agent-based interfaces will gradually simplify user interaction
    with them, masking the complexity of the backend application layer. Then, as dedicated
    AI hardware becomes a commodity, a large part of the applications will be phased
    out of the ecosystem, and the functionality they provide will be taken over by
    <html:span class="No-Break">AI models.</html:span></html:p> <html:p>And I think
    this whole exposition justifies the title I have chosen for this section. Next,
    let’s discover together how prompts are used by LlamaIndex for <html:span class="No-Break">LLM
    interactions.</html:span></html:p> <html:a id="_idTextAnchor218"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Understanding
    how LlamaIndex uses prompts</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-219">Understanding how LlamaIndex uses prompts</html:h1> <html:div
    id="_idContainer115">from llama_index.core import SummaryIndex, SimpleDirectoryReader
    documents = SimpleDirectoryReader("files").load_data() summary_index = SummaryIndex.from_documents(documents)
    qe = summary_index.as_query_engine() prompts = qe.get_prompts() for k, p in prompts.items():
        print(f"Prompt Key: {k}")     print("Text:")     print(p.get_template())     print("\n")
    <html:p>In terms of mechanics, a RAG-based application follows exactly the same
    rules and principles of <html:a id="_idIndexMarker1079"></html:a>interaction that
    a simple user would use in a chat session with an LLM. A major difference comes
    from the fact that RAG is actually a <html:a id="_idIndexMarker1080"></html:a>kind
    of prompt engineer on steroids. Behind the scenes, for almost every indexing,
    retrieval, metadata extraction, or final response synthesis operation, the RAG
    framework programmatically produces prompts. These prompts are enriched with context
    and then sent to <html:span class="No-Break">the LLM.</html:span></html:p> <html:p>In
    LlamaIndex, for each type of operation that requires an LLM, there is a default
    prompt that is used as a template. Take <html:code class="literal">TitleExtractor</html:code>
    as an example. This is one of the metadata extractors that we already talked about
    in <html:a><html:span class="No-Break"><html:em class="italic">Chapter 4</html:em></html:span></html:a>
    , <html:em class="italic">Ingesting Data into Our RAG Workflow</html:em> . The
    <html:code class="literal">TitleExtractor</html:code> class uses two predefined
    prompt templates to get titles from text nodes inside documents. It does this
    in <html:span class="No-Break">two steps:</html:span></html:p> <html:ol><html:li>It
    gets potential titles from individual text Nodes using the <html:code class="literal">node_template</html:code>
    argument, which creates prompts to generate <html:span class="No-Break">appropriate
    titles</html:span></html:li> <html:li>Combines the individual Node titles into
    one overall comprehensive title for the whole Document using the <html:span class="No-Break"><html:code
    class="literal">combine_template</html:code></html:span> <html:span class="No-Break">prompt</html:span></html:li></html:ol>
    <html:p>The default values for the <html:code class="literal">TitleExtractor</html:code>
    prompts are stored in <html:span class="No-Break">two constants:</html:span></html:p>
    <html:p>Looking at these two default templates used by <html:code class="literal">TitleExtractor</html:code>
    , we can easily understand how they work. Each template contains a <html:em class="italic">fixed</html:em>
    text part and a <html:em class="italic">dynamic</html:em> part, designated by
    <html:code class="literal">{context_str}</html:code> or other variables. That
    is where LlamaIndex will actually inject the text content of our Nodes during
    execution, as seen in <html:span class="No-Break"><html:em class="italic">Figure
    10</html:em></html:span> <html:span class="No-Break"><html:em class="italic">.2</html:em></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 10.2 – How prompts are built by injecting variables into a
    prompt template</html:p> <html:p>The prompt templates used by metadata extractors
    such as <html:code class="literal">TitleExtractor</html:code> are defined <html:a
    id="_idIndexMarker1081"></html:a>directly within the <html:code class="literal">metadata_extractors.py</html:code>
    module. The relative path of this module in the LlamaIndex <html:a id="_idIndexMarker1082"></html:a>GitHub
    repository is <html:code class="literal">llama-index-core/llama_index/core/extractors/metadata_extractors.py</html:code>
    . However, this is just an exception as the vast majority of the default templates
    are defined in two other key modules: <html:code class="literal">llama-index-core/llama_index/core/prompts/default_prompts.py</html:code>
    <html:span class="No-Break">and</html:span> <html:span class="No-Break"><html:code
    class="literal">llama-index-core/llama_index/core/prompts/chat_prompts.py</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Because a RAG workflow
    built with LlamaIndex can have so many different components that rely on LLM interactions
    and not all prompt templates can be easily located within the code base, the framework
    provides a simple method to identify the templates used by a specific component.
    That method is called <html:code class="literal">get_prompts()</html:code> and
    can be used with agents, retrievers, query engines, response synthesizers, and
    many other RAG components. Here is a simple example of how we can use it to obtain
    a list of prompt templates used by a query engine built on top <html:span class="No-Break">of</html:span>
    <html:span class="No-Break"><html:code class="literal">SummaryIndex</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:p>The first part of the
    code should be very straightforward at this point. We import <html:code class="literal">SummaryIndex</html:code>
    and <html:code class="literal">SimpleDirectoryReader</html:code> and then ingest
    the two sample files that should have been cloned from our GitHub repository.
    Once the files have been ingested as Documents, we build an index and a query
    engine from that index. In this example, we won’t run any queries because we don’t
    need to. We just want to see the prompts. Therefore, the next step retrieves a
    dictionary containing the default prompts used within the <html:span class="No-Break">query
    engine:</html:span></html:p> <html:p>The dictionary returned by the <html:code
    class="literal">get_prompts()</html:code> method maps keys, which identify the
    different <html:a id="_idIndexMarker1083"></html:a>prompt types used by the query
    engine, to values <html:a id="_idIndexMarker1084"></html:a>that are the actual
    prompt templates. The last part of the code is responsible for iterating and displaying
    the keys and their <html:span class="No-Break">corresponding templates:</html:span></html:p>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 10</html:em></html:span>
    <html:em class="italic">.3</html:em> shows the results after running <html:span
    class="No-Break">this sample:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 10.3 – The two prompt templates used by the SummaryIndex query
    engine</html:p> <html:p>Examining the output, we’ll see the two templates used
    by the query engine: <html:code class="literal">text_qa_template</html:code> and
    <html:code class="literal">refine_template</html:code> . You’ll notice that both
    keys begin with the text <html:code class="literal">response_synthesizer:</html:code>
    . This indicates the exact component of the query engine that actually uses the
    prompts – in our case, the response synthesizer. Following the same logic, we
    can use the <html:code class="literal">get_prompts()</html:code> method on many
    other types of RAG components in order to understand prompts used under <html:span
    class="No-Break">the hood.</html:span></html:p> <html:p class="callout-heading">Pro
    tip</html:p> <html:p class="callout">An alternative option to inspect the underlying
    prompts would be to use an advanced tracing method – such as the one using the
    Arize AI Phoenix framework, presented in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 9</html:em></html:span></html:a> , <html:em class="italic">Customizing
    and Deploying Our LlamaIndex Project</html:em> . Phoenix provides a visual representation
    of the execution flow, making it easier to understand how and when different prompts
    are used, in addition to displaying the final prompts with the inserted context.
    One caveat of using that method, though, is that instead of getting the original
    prompt templates, we’ll see the final prompts – also including any context already
    inserted in <html:span class="No-Break">the prompt.</html:span></html:p> <html:p>Now
    that we <html:a id="_idIndexMarker1085"></html:a>have a reliable technique for
    inspecting prompts, the next <html:a id="_idIndexMarker1086"></html:a>step explores
    ways in which can customize them. Building on the title extractor and query engine
    examples, in the next section, we’ll explore how to customize prompts used by
    various <html:span class="No-Break">RAG components.</html:span></html:p> <html:a
    id="_idTextAnchor219"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Customizing
    default prompts</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-220">Customizing default prompts</html:h1> <html:div id="_idContainer115">Context
    information is below. --------------------- {context_str} ---------------------
    Given the context information from llama_index.core import SummaryIndex, SimpleDirectoryReader
    from llama_index.core import PromptTemplate documents = SimpleDirectoryReader("files").load_data()
    summary_index = SummaryIndex.from_documents(documents) qe = summary_index.as_query_engine()
    print(qe.query("Who burned Rome?")) print("------------------------") new_qa_template
    = ( "Context information is below." "---------------------" "{context_str}" "---------------------"
    "Given the context information " " template = PromptTemplate(new_qa_template)
    qe. Update_prompts(     {"response_synthesizer: text_qa_template": template} )
    print(qe.query("Who burned Rome?")) from llama_index.core import SimpleDirectoryReader
    from llama_index.core.node_parser import SentenceSplitter from llama_index.core.extractors
    import TitleExtractor reader = SimpleDirectoryReader(''files'') documents = reader.load_data()
    parser = SentenceSplitter() nodes = parser.get_nodes_from_documents(documents)
    title_extractor = TitleExtractor(summaries=["self"]) meta = title_extractor.extract(nodes)
    print("\nFirst title: " +meta[0][''document_title'']) print("Second title: " +meta[1][''document_title''])
    First title: "The Enduring Influence of Ancient Rome: Architecture, Engineering,
    Conquest, and Legacy" Second title: "The Enduring Bond: Dogs as Loyal Companions
    - Exploring the Unbreakable Connection Between Humans and Man''s Best Friend"
    combine_template = (     "{context_str}. Based on the above candidate titles "
        "and content, what is the comprehensive title for "     "this document? Keep
    it under 6 words. Title: " ) title_extractor = TitleExtractor(     summaries=["self"],
        combine_template=combine_template ) meta = title_extractor.extract(nodes)
    print("\nFirst title: "+meta[0][''document_title'']) print("Second title: "+meta[1][''document_title'']
    First title: "Roman Legacy: Architecture, Engineering, Conquest" Second title:
    "Man''s Best Friend: The Enduring Bond" <html:p>While the default prompts provided
    by LlamaIndex are designed to work well in most scenarios, there may <html:a id="_idIndexMarker1087"></html:a>be
    instances where customization is necessary or desirable. For example, you might
    want to adjust prompts to do <html:span class="No-Break">the following:</html:span></html:p>
    <html:ul><html:li>Incorporate domain-specific knowledge <html:span class="No-Break">or
    terminology</html:span></html:li> <html:li>Adapt prompts to a particular writing
    style <html:span class="No-Break">or tone</html:span></html:li> <html:li>Modify
    prompts to prioritize certain types of information <html:span class="No-Break">or
    outputs</html:span></html:li> <html:li>Experiment with different prompt structures
    to optimize performance <html:span class="No-Break">or quality</html:span></html:li></html:ul>
    <html:p>By customizing prompts, we can fine-tune the interaction between the RAG
    components and the language model, potentially leading to improved accuracy, relevance,
    and overall effectiveness of <html:span class="No-Break">our application.</html:span></html:p>
    <html:p>The good news is that we can modify the behavior of various LlamaIndex
    components by supplying our own custom prompt templates. The not-so-good news
    is that contrary to common expectations, writing a good prompt template is not
    a trivial task. One would have to consider many intricacies such as accuracy,
    relevance, query formulation, prompt size, output formatting, and others. Because
    of the involved complexity, the recommended approach for customization is to start
    with the default prompts and use them as a foundation for making any desired modifications.
    Changes should be incremental and ideally followed by rigorous evaluation against
    a diverse set of edge cases. We will have <html:a id="_idIndexMarker1088"></html:a>a
    more detailed discussion about general principles and best practices for writing
    prompts in the next section. For now, let us focus on the methods used for <html:span
    class="No-Break">prompt customization.</html:span></html:p> <html:p>In LlamaIndex,
    every RAG component that exposes the <html:code class="literal">get_prompts()</html:code>
    method also provides an equivalent for modifying these prompt templates – the
    <html:code class="literal">update_prompts()</html:code> method. So, this is the
    easiest way to change a particular prompt template. Let’s take our example from
    the previous section and experiment with a different prompt. This time, we will
    adapt the <html:code class="literal">text_qa_template</html:code> template to
    also rely on the LLM’s own knowledge when answering the query. The default <html:code
    class="literal">text_qa_template</html:code> template would normally look <html:span
    class="No-Break">like this:</html:span></html:p> <html:p>In the following example,
    we’ll make a very subtle change to this template and see how that will affect
    the behavior of our query engine. Let’s have a look at <html:span class="No-Break">the
    code:</html:span></html:p> <html:p>So far, the code is identical to the previous
    example, with only one additional import that I will explain in a few moments.
    This time, though, we’ll first run a query using the default template. We’ll use
    this response as a <html:span class="No-Break">reference later:</html:span></html:p>
    <html:p>It’s now time <html:a id="_idIndexMarker1089"></html:a>to change the <html:code
    class="literal">prompt_template</html:code> template. We first define a string
    containing the <html:span class="No-Break">new version:</html:span></html:p> <html:p>If
    you carefully compare the new version with the original template, you’ll notice
    a subtle but very important change. In this new version, I’m instructing the model
    to apply not just the knowledge provided in the retrieved context but also use
    its own knowledge on the matter. It’s time to make use of that new import we added
    at the beginning of our code. Because the <html:code class="literal">update_prompts()</html:code>
    method requires the prompts to be in the <html:code class="literal">BasePromptTemplate</html:code>
    format, we must first make sure that our new prompt is structured <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>We’re now ready to rerun <html:span class="No-Break">the
    query:</html:span></html:p> <html:p>Let’s have a look at the final output shown
    in <html:span class="No-Break"><html:em class="italic">Figure 10</html:em></html:span>
    <html:span class="No-Break"><html:em class="italic">.4</html:em></html:span> <html:span
    class="No-Break">:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    10.4 – The query output before and after updating the prompt templates</html:p>
    <html:p>As you can see <html:a id="_idIndexMarker1090"></html:a>in the output,
    that slight modification in the <html:code class="literal">text_qa_template</html:code>
    template of the query engine completely changed its behavior. In a similar fashion,
    instead of changing the answering approach, we could have instructed the LLM to
    answer in a certain linguistic style, speak in rhymes, or anything else we might
    need. I think the value this feature provides for a RAG application is pretty
    clear <html:span class="No-Break">by now.</html:span></html:p> <html:p>Unfortunately,
    not all LlamaIndex components support the <html:code class="literal">update_prompts()</html:code>
    method. Take, for example, the <html:code class="literal">TitleExtractor</html:code>
    metadata extractor that I mentioned in the previous section. Although metadata
    extractors do not support the <html:code class="literal">update_prompts()</html:code>
    method, the good news is that we can still change their underlying prompt templates
    by using arguments. In particular, the two templates used by <html:code class="literal">TitleExtractor</html:code>
    can be customized with the <html:code class="literal">node_template</html:code>
    and <html:code class="literal">combine_template</html:code> arguments. Let’s have
    a look at <html:span class="No-Break">an example:</html:span></html:p> <html:p>The
    first part of the example is responsible for ingesting our sample files as Documents
    and then chunking them into individual Nodes. Let’s extract the titles, first
    by using the default prompt templates that we saw in the <html:span class="No-Break">previous
    section:</html:span></html:p> <html:p>The output <html:a id="_idIndexMarker1091"></html:a>so
    far should be something similar <html:span class="No-Break">to this:</html:span></html:p>
    <html:p>Next, let’s define a custom prompt template and pass it as an argument
    to <html:code class="literal">TitleExtractor</html:code> for the <html:span class="No-Break">second
    run:</html:span></html:p> <html:p>Because we’ve added an extra instruction in
    this custom prompt, the extractor should now generate <html:a id="_idIndexMarker1092"></html:a>shorter
    titles. The output for the second run should be something along the lines of <html:span
    class="No-Break">the following:</html:span></html:p> <html:p>After seeing the
    basic mechanics of prompt customization, it’s time to move on to more <html:span
    class="No-Break">advanced methods.</html:span></html:p> <html:a id="_idTextAnchor220"></html:a><html:h2
    id="_idParaDest-221">Using advanced prompting techniques in LlamaIndex</html:h2>
    <html:p>LlamaIndex offers several advanced prompting techniques that enable you
    to create more <html:a id="_idIndexMarker1093"></html:a>customized and expressive
    prompts, reuse existing prompts, and express certain operations <html:a id="_idIndexMarker1094"></html:a>more
    concisely. These <html:a id="_idIndexMarker1095"></html:a>techniques include partial
    formatting, prompt template variable mappings, and prompt function mappings. <html:em
    class="italic">Table 10.1</html:em> breaks down the purpose and potential use
    cases for <html:span class="No-Break">each method:</html:span></html:p> <html:table
    class="No-Table-Style _idGenTablePara-1" id="table001-4"><html:tbody><html:tr
    class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span class="No-Break"><html:strong
    class="bold">Method</html:strong></html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p><html:span
    class="No-Break"><html:strong class="bold">Purpose</html:strong></html:span></html:p></html:td></html:tr>
    <html:tr class="No-Table-Style"><html:td class="No-Table-Style"><html:p><html:span
    class="No-Break">Partial formatting</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>Allows
    you to partially format a prompt by filling in some variables but leaving others
    to be filled in later. This is useful because it allows you to format variables
    as they become available, rather than maintaining all the required prompt variables
    until the end. The method is particularly useful in a multi-step RAG scenario
    that gradually builds the prompt by gathering different <html:span class="No-Break">user
    inputs.</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p>Prompt template <html:span class="No-Break">variable
    mappings</html:span></html:p></html:td> <html:td class="No-Table-Style"><html:p>They
    let you specify a mapping between some <html:em class="italic">expected</html:em>
    prompt keys and the keys actually used in your template, enabling you to reuse
    existing string templates without modifying the template variables. It is similar
    to creating an <html:em class="italic">alias</html:em> for <html:span class="No-Break">template
    keys.</html:span></html:p></html:td></html:tr> <html:tr class="No-Table-Style"><html:td
    class="No-Table-Style"><html:p>Prompt <html:span class="No-Break">function mappings</html:span></html:p></html:td>
    <html:td class="No-Table-Style"><html:p>This feature allows you to dynamically
    inject certain values, depending on other values or conditions, during query time
    by passing functions as template variables instead of <html:span class="No-Break">fixed
    values.</html:span></html:p></html:td></html:tr></html:tbody></html:table> <html:p
    class="IMG---Caption" lang="en-US">Table 10.1 – An overview of the more advanced
    prompting techniques provided by LlamaIndex</html:p> <html:p>You’ll <html:a id="_idIndexMarker1096"></html:a>find
    detailed <html:a id="_idIndexMarker1097"></html:a>code examples for all three
    <html:a id="_idIndexMarker1098"></html:a>methods in the official LlamaIndex <html:a
    id="_idIndexMarker1099"></html:a>documentation <html:span class="No-Break">here:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/prompts/advanced_prompts.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Having all these new
    cool gadgets in our knowledge inventory, we can now refine and tailor the dialogue
    between our application and the LLM, allowing us to customize the behavior of
    almost any RAG component <html:span class="No-Break">of LlamaIndex.</html:span></html:p>
    <html:p>For the final section of this chapter, we move our focus to an important
    aspect of maximizing our RAG setup’s potential: the art and science of <html:span
    class="No-Break">prompt engineering.</html:span></html:p> <html:a id="_idTextAnchor221"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>The
    golden rules of prompt engineering</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-222">The golden rules of prompt engineering</html:h1>
    <html:div id="_idContainer115">Classify the following reviews as positive or negative
    sentiment: <The food was delicious and the service was excellent!> // Positive
    <I waited over an hour and my meal arrived cold.> // Negative <The ambiance was
    nice but the dishes were overpriced.> // Output: There are 15 students in a class.
    8 students have dogs as pets. If 3 more students get a dog, how many of them would
    have a dog as a pet then? Step 1) Initially there are 15 students and 8 have dogs
    Step 2) 3 more students will get dogs soon Step 3) So the final number is the
    initial 8 students with dogs plus the 3 new students = 8 + 3 = 11 Therefore, the
    number of students that would have a dog as a pet is 11. A factory makes 100 items
    daily. On Tuesday, they boost production by 40% for a special order. However,
    to adjust inventory, they cut Thursday''s output by 20% from Tuesday''s high.
    Then, expecting a sales increase, Friday''s output rises by 10% over the day before.
    Calculate the production numbers for Tuesday, Thursday, and Friday. Let''s simulate
    a verbal conversation between three experts who tackle a complex puzzle. Each
    expert outlines one step in their thought process before exchanging insights with
    the others, without adding any unnecessary remarks. As they progress, any expert
    who identifies a flaw in their reasoning exits the discussion. The process continues
    until a solution is found or all available options have been exhausted. The problem
    they need to solve is: "Using only numbers 3, 3, 7, 7 and basic arithmetic operations,
    is it possible to obtain the value 25?" <html:p>This section is not intended to
    serve as a definitive guide to prompt engineering. In fact, the field is an <html:a
    id="_idIndexMarker1100"></html:a>ever-expanding one. Since many LLMs are demonstrating
    emerging capabilities that were not initially anticipated, it is only natural
    that our methods of interacting with these linguistic experts will also be refined
    over time. In other words, as LLMs evolve to better model and understand human
    nature, we in turn learn new ways of interacting with them. In this section, I
    aim to present some of the most commonly used techniques in prompt engineering,
    as well as the basic principles that govern the field. As stated in the previous
    section, writing a good prompt requires a fine balance between several parameters.
    Here are some of the most important aspects to consider when building prompts
    for a <html:span class="No-Break">RAG application.</html:span></html:p> <html:a
    id="_idTextAnchor222"></html:a><html:h2 id="_idParaDest-223">Accuracy and clarity
    in expression</html:h2> <html:p>The prompt should be clear and precise, avoiding
    ambiguity. The more clearly you state what you <html:a id="_idIndexMarker1101"></html:a>need,
    the more likely you are to get a relevant response. It’s important to articulate
    the question or task in a way that leaves little room for misinterpretation. Make
    no assumptions about the model’s ability to understand your message. These assumptions
    are usually biased and tend to produce hallucinations <html:span class="No-Break">in
    return.</html:span></html:p> <html:a id="_idTextAnchor223"></html:a><html:h2 id="_idParaDest-224">Directiveness</html:h2>
    <html:p>How directive the prompt is can significantly impact the response. A prompt
    can range from open-ended – encouraging creative or broad responses – to highly
    specific – requesting <html:a id="_idIndexMarker1102"></html:a>a very particular
    type of answer. The level of directiveness should match the intended outcome.
    Given that we’re actually building prompt templates that mix a static part with
    dynamically retrieved content, consider exceptional scenarios and edge cases in
    which the model might misunderstand the prompt. Use clear instructions or commands
    (for example, <html:code class="literal">Summarize</html:code> , <html:code class="literal">Analyze</html:code>
    , and <html:code class="literal">Explain</html:code> ) to guide the model on the
    desired task. Our prompts must be broad enough to accommodate varied inputs yet
    detailed enough to direct the <html:span class="No-Break">model effectively.</html:span></html:p>
    <html:a id="_idTextAnchor224"></html:a><html:h2 id="_idParaDest-225">Context quality</html:h2>
    <html:p>This is a major pain point for building an effective RAG system. Both
    the quality and structure <html:a id="_idIndexMarker1103"></html:a>of our proprietary
    knowledge base as well as the ability to retrieve the most relevant context from
    it are very important aspects. <html:em class="italic">Garbage in, garbage out</html:em>
    may be regarded as a general rule applicable to this subject. Try to remove any
    inconsistencies in the data, special characters that might derail the LLM, duplicate
    data, and even grammatical errors in the text. These types of quality issues will
    unfortunately affect both the retrieval and the final response synthesis. Experiment
    with different retrieval strategies, such as the ones discussed in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 – Context Retrieval</html:em>
    . Try different values for <html:code class="literal">similarity_top_k</html:code>
    , <html:code class="literal">chunk_size</html:code> , and <html:code class="literal">chunk_overlap</html:code>
    , as discussed in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    4</html:em></html:span></html:a> , <html:em class="italic">Ingesting Data into
    Our RAG Workflow</html:em> . Employ re-rankers and Node postprocessors to increase
    the context quality, as we did in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 7</html:em></html:span></html:a> , <html:em class="italic">Querying
    Our Data, Part 2 – Postprocessing and</html:em> <html:span class="No-Break"><html:em
    class="italic">Response Synthesis</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor225"></html:a><html:h2 id="_idParaDest-226">Context quantity</html:h2>
    <html:p>There’s a balance between being concise and offering sufficient detail.
    A prompt should be brief enough to maintain focus but detailed enough to convey
    the specific requirements <html:a id="_idIndexMarker1104"></html:a>of the task
    or question. Too little context may result in answers that lack depth or relevance,
    while too much may confuse <html:a id="_idIndexMarker1105"></html:a>the model
    or lead <html:span class="No-Break">it off-topic.</html:span></html:p> <html:p>In
    RAG scenarios, as the amount of context provided in a prompt increase, it’s important
    to consider the potential impact on the alignment and accuracy of generated responses.
    While providing more context can be beneficial in many cases, as it gives the
    language model a broader understanding of the task at hand, there are also risks
    associated with excessively <html:span class="No-Break">long prompts.</html:span></html:p>
    <html:p>For example, when a prompt becomes too long, there is a higher chance
    of introducing irrelevant or contradictory information. This can lead to misalignment
    between the intended task and the model’s understanding of it. The model may give
    too much attention to tangential details or lose focus on the core objective.
    Maintaining a clear and concise prompt helps ensure that the model stays aligned
    with the <html:span class="No-Break">desired output.</html:span></html:p> <html:p>Also,
    as the context grows, the model has to process and consider a larger amount of
    information. This <html:a id="_idIndexMarker1106"></html:a>increased <html:strong
    class="bold">cognitive load</html:strong> can lead to a decrease in accuracy.
    The model may struggle to identify the most relevant pieces of information or
    may give undue importance to less significant details. Additionally, longer prompts
    are more likely to contain ambiguities or inconsistencies, which can further degrade
    the accuracy of <html:span class="No-Break">the responses.</html:span></html:p>
    <html:p class="callout-heading">Cognitive load in the context of LLMs</html:p>
    <html:p class="callout">Cognitive load refers to the amount of processing effort
    and resources required by the language model to process, understand, and generate
    a response based on the provided context. In the case of RAG systems, the cognitive
    load is directly related to the quantity and complexity of the information present
    in <html:span class="No-Break">the prompt.</html:span></html:p> <html:p>Implementing
    Node postprocessors such as <html:code class="literal">SimilarityPostprocessor</html:code>
    or <html:code class="literal">SentenceEmbeddingOptimizer</html:code> can partially
    mitigate this issue by filtering less relevant Nodes or shortening their content,
    and therefore reducing the final prompt submitted to the LLM. We covered these
    methods in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    7</html:em></html:span></html:a> , <html:em class="italic">Querying Our Data,
    Part 2 – Postprocessing and Response Synthesis</html:em> . Moreover, if the retrieved
    context is inherently long, consider breaking it down into smaller, more <html:span
    class="No-Break">manageable chunks.</html:span></html:p> <html:h3>Context ordering</html:h3>
    <html:p>The overall effectiveness of our RAG pipeline does not rely just on the
    quantity and quality of context. Especially when dealing with longer context,
    most LLMs may perform differently <html:a id="_idIndexMarker1107"></html:a>when
    trying to extract the key information from that context, depending on where exactly
    that key information is placed. A good approach is to structure the prompt hierarchically,
    with the most critical information at the beginning or at the end. This ensures
    that the model prioritizes the core instructions and context. That’s where tools
    such as Node re-rankers or the <html:code class="literal">LongContextReorder</html:code>
    postprocessor may <html:span class="No-Break">become useful.</html:span></html:p>
    <html:p class="callout-heading">Side note</html:p> <html:p class="callout">There’s
    an increasingly popular RAG evaluation technique called the <html:em class="italic">needle
    in a haystack test</html:em> , in which researchers gauge the model’s ability
    to notice and recall a very specific piece of information from a larger context
    provided to the LLM. This specific information looks unsuspecting and is usually
    seamlessly blended into the overall context. In many ways, this method is similar
    to testing a human’s ability to pay attention to a certain text and then recall
    key information in <html:span class="No-Break">that text.</html:span></html:p>
    <html:a id="_idTextAnchor226"></html:a><html:h2 id="_idParaDest-227">Required
    output format</html:h2> <html:p>In most cases, when building RAG workflows, we
    need LLMs to generate structured or semi-structured outputs. In almost all scenarios,
    we need the output to be predictable in terms of <html:a id="_idIndexMarker1108"></html:a>format,
    size, or language. Sometimes, providing a few <html:a id="_idIndexMarker1109"></html:a>examples
    in our prompt may lead to better responses, but that’s not a silver bullet for
    all scenarios. That’s were using output parsers and Pydantic programs becomes
    really important. We talked about these topics in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 7</html:em></html:span></html:a> , <html:em class="italic">Querying
    Our Data, Part 2 – Postprocessing and</html:em> <html:span class="No-Break"><html:em
    class="italic">Response Synthesis</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor227"></html:a><html:h2 id="_idParaDest-228">Inference
    cost</html:h2> <html:p>In most cases, we’ll be running our applications within
    very specific cost constraints. Ignoring <html:a id="_idIndexMarker1110"></html:a>token
    usage would be a clear mistake. So, make <html:a id="_idIndexMarker1111"></html:a>sure
    you’re doing cost estimations, and always keep track of token usage. In addition,
    you could use tools such as <html:code class="literal">LongLLMLinguaPostprocessor</html:code>
    for prompt compression. We talked about this Node postprocessor in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 7</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 2 – Postprocessing and Response
    Synthesis</html:em> . Prompt compression techniques have the potential to improve
    not only cost efficiency but also the quality of the final response by eliminating
    redundant information from our context and keeping just <html:span class="No-Break">key
    information.</html:span></html:p> <html:a id="_idTextAnchor228"></html:a><html:h2
    id="_idParaDest-229">Overall system latency</html:h2> <html:p>While this parameter
    depends on many factors, bloated, inefficient, or ambiguous prompts can <html:a
    id="_idIndexMarker1112"></html:a>also negatively affect system latency. It’s just
    like talking to a real person. The longer and less efficient the query, the more
    <html:a id="_idIndexMarker1113"></html:a>processing will be required from the
    model in order to best understand the actual intent behind the query. Longer processing
    times will negatively impact the overall <html:span class="No-Break">user experience.</html:span></html:p>
    <html:p>Prompt engineering is a continuous process of experimentation and iteration.
    Regularly evaluate the performance of your prompts and refine them based on the
    results. Remember – this is a long game, and the rules are being constantly re-written.
    Try to keep your knowledge up to date with the latest advancements and techniques
    in prompt engineering, as the field is <html:span class="No-Break">rapidly evolving.</html:span></html:p>
    <html:a id="_idTextAnchor229"></html:a><html:h2 id="_idParaDest-230">Choosing
    the right LLM for the task</html:h2> <html:p>In the world of AI, not all LLMs
    are equal. In <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    9</html:em></html:span></html:a> , <html:em class="italic">Customizing and Deploying
    Our LlamaIndex Project</html:em> , we already saw how easy is to customize different
    components <html:a id="_idIndexMarker1114"></html:a>of our RAG pipeline, including
    the underlying LLM. But there are actually many options available, so which one
    should we <html:a id="_idIndexMarker1115"></html:a>select for the job? Choosing
    the <html:em class="italic">wrong</html:em> LLM for a particular task will likely
    cancel many of the efforts we invested in crafting the actual prompts. It’s pretty
    much like trying to get an answer from the wrong person. If you’re persuasive
    enough, chances are you’ll get an answer at some point. However, that may not
    be the answer you were <html:span class="No-Break">looking for.</html:span></html:p>
    <html:p>That’s why understanding the different flavors of LLMs and knowing which
    one qualifies for a given task is essential. Several key characteristics should
    be useful for our model selection. Let’s look at <html:span class="No-Break">these
    next.</html:span></html:p> <html:h3>Model architecture</html:h3> <html:p>Models
    can have different underlying architectures, and these may determine their inherent
    <html:a id="_idIndexMarker1116"></html:a>capabilities. For example, encoder-only
    models are specialized in encoding and classifying input text, useful for categorizing
    text into <html:a id="_idIndexMarker1117"></html:a>defined categories, such as
    with <html:strong class="bold">Bidirectional Encoder Representations from Transformers</html:strong>
    ( <html:strong class="bold">BERT</html:strong> ), which <html:a id="_idIndexMarker1118"></html:a>excels
    in <html:strong class="bold">next sentence prediction</html:strong> ( <html:strong
    class="bold">NSP</html:strong> ) <html:span class="No-Break">tasks (</html:span>
    <html:a><html:span class="No-Break">https://en.wikipedia.org/wiki/BERT_(language_model)</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>Encoder-decoder models
    are capable of both understanding input text and generating responses, making
    them ideal for text generation and comprehension tasks, such as translation <html:a
    id="_idIndexMarker1119"></html:a>and summarizing articles. One example that fits
    in this category is <html:strong class="bold">Bidirectional and Auto-Regressive
    Transformer</html:strong> ( <html:span class="No-Break"><html:strong class="bold">BART</html:strong></html:span>
    <html:span class="No-Break">) (</html:span> <html:a><html:span class="No-Break">https://huggingface.co/docs/transformers/en/model_doc/bart</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>Decoder-only models
    can decode or generate subsequent words or tokens from a given prompt and <html:a
    id="_idIndexMarker1120"></html:a>are primarily used for text generation. Models
    such as <html:strong class="bold">Generative Pre-trained Transformer</html:strong>
    ( <html:strong class="bold">GPT</html:strong> ), Mistral, Claude, and LLaMa are
    superstars in <html:span class="No-Break">this domain.</html:span></html:p> <html:p>There
    are <html:a id="_idIndexMarker1121"></html:a>also more exotic architectures such
    as <html:strong class="bold">Mixture-of-Experts</html:strong> ( <html:strong class="bold">MoE</html:strong>
    ), which essentially leverage a <html:em class="italic">sparse MoE</html:em> framework
    to offer dynamic, token-specific processing – see Shazeer et al. (2017), <html:em
    class="italic">Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
    Layer</html:em> ( <html:a>https://doi.org/10.48550/arXiv.1701.06538</html:a> ).
    This approach can significantly enhance performance across a range of domains,
    including mathematics, code generation, and multilingual <html:a id="_idIndexMarker1122"></html:a>tasks,
    as demonstrated by <html:span class="No-Break"><html:strong class="bold">Mixtral
    8x7B</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:h3>Model size</html:h3> <html:p>Model size is <html:a id="_idIndexMarker1123"></html:a>another
    critical factor to consider when selecting an LLM, as it directly impacts both
    the potential computational cost and the model’s capabilities. The number of parameters
    within an LLM, ranging from weights to biases adjusted during training, serves
    as a proxy for understanding the model’s complexity and, by extension, its operational
    expense. Larger models, such as GPT-4 with its estimated 1.76 trillion parameters,
    offer profound capabilities but come with higher costs <html:a id="_idIndexMarker1124"></html:a>and
    requirements for computational resources. On the other hand, medium-sized models,
    typically under 10 billion parameters, strike a balance between affordability
    and performance, making them suitable for a wide array of applications without
    breaking <html:span class="No-Break">the bank.</html:span></html:p> <html:h3>Inference
    speed</html:h3> <html:p>That’s also a key parameter as it determines how quickly
    a model can process input and generate output. While larger models may offer enhanced
    performance in terms of output quality <html:a id="_idIndexMarker1125"></html:a>and
    depth, their inference speed tends to be slower due to the sheer volume of computations
    required. It’s important to note that inference speed is influenced by various
    factors beyond just the number of parameters, including the efficiency of the
    model architecture and the computational infrastructure used. Techniques to reduce
    inference time, such as model pruning, quantization, and leveraging specialized
    hardware, can significantly improve the usability of LLMs in <html:span class="No-Break">real-world
    applications.</html:span></html:p> <html:p>To make things even more complex, apart
    from these characteristics, LLMs can be specialized for various tasks or domains,
    enhancing their performance in specific scenarios. This specialization arises
    from the type of data and the training objectives used to fine-tune the model.
    Let’s look at some common <html:span class="No-Break">specializations next.</html:span></html:p>
    <html:h3>Chat models</html:h3> <html:p>Chat models are optimized for conversational
    interactions. They are designed to engage users in dialogue, providing responses
    that mimic human-like conversation. These models are <html:a id="_idIndexMarker1126"></html:a>adept
    at back-and-forth exchanges and can maintain context over a series <html:span
    class="No-Break">of interactions.</html:span></html:p> <html:p>They are the ideal
    choice for building chatbots or virtual assistants where the interaction is more
    casual or conversational. These models are used in applications requiring natural,
    engaging dialogue with users, such as customer service bots, entertainment applications,
    or virtual companions. As a particular characteristic, they tend to be more open-ended
    in their responses, aiming to generate replies that are engaging, contextually
    relevant, and sometimes <html:span class="No-Break">even entertaining.</html:span></html:p>
    <html:h3>Instruct models</html:h3> <html:p>Instruct models are fine-tuned to understand
    and execute specific instructions or queries. They prioritize executing the given
    task based on the instruction over engaging in a dialogue. That makes them suitable
    for scenarios where the user needs the model to perform a <html:a id="_idIndexMarker1127"></html:a>particular
    task, such as summarizing a document, generating code based on a prompt, or providing
    detailed explanations. These models are preferred in educational tools, productivity
    applications, and anywhere a direct, clear response to a query is needed, such
    as in the intricate workflow of a <html:span class="No-Break">RAG application.</html:span></html:p>
    <html:p>They are more focused on accuracy and relevance to the task at hand rather
    than maintaining a conversational tone. Their responses are tailored toward fulfilling
    the user’s request as efficiently and effectively <html:span class="No-Break">as
    possible.</html:span></html:p> <html:h3>Codex models</html:h3> <html:p>These models
    are optimized for understanding and generating code. They have been trained in
    a <html:a id="_idIndexMarker1128"></html:a>vast corpus of programming languages
    and can assist with coding tasks, debug code, explain code snippets, and even
    generate entire programs based on a description. This makes them the perfect candidates
    for integrating into development environments, coding education tools, and anywhere
    automated coding assistance <html:span class="No-Break">is beneficial.</html:span></html:p>
    <html:h3>Summarization models</html:h3> <html:p>Specialized in condensing long
    texts into shorter summaries while retaining key information <html:a id="_idIndexMarker1129"></html:a>and
    context. These models focus on capturing the essence of the content and presenting
    it concisely. They are useful for news aggregation services, research, content
    creation, and any scenario where quick insights from long documents <html:span
    class="No-Break">are needed.</html:span></html:p> <html:h3>Translation models</html:h3>
    <html:p>As the name implies these models are designed to translate text from one
    language to another. They have <html:a id="_idIndexMarker1130"></html:a>been trained
    on large multilingual datasets to understand and translate between languages with
    high accuracy, and they are best suited for global communication platforms, content
    localization, and educational tools aimed at <html:span class="No-Break">language
    learners.</html:span></html:p> <html:h3>Question-answering models</html:h3> <html:p>Fine-tuned
    to understand questions posed in NL and provide accurate answers by referencing
    <html:a id="_idIndexMarker1131"></html:a>provided texts or their vast training
    data, these models are key in building intelligent search engines, educational
    aids, and interactive <html:span class="No-Break">knowledge bases.</html:span></html:p>
    <html:p>And the list could probably go on with other types of models, fine-tuned
    for specific domains or applications. Also, keep in mind that because these different
    specializations tend to enhance or diminish certain capabilities of the model,
    our carefully crafted prompts may yield inconsistent results. For one model, a
    prompt may lead to near-perfect responses, while for another it could barely hit
    an <html:span class="No-Break">average mark.</html:span></html:p> <html:p>When
    choosing your LLM, it’s essential to weigh the trade-offs between all these characteristics
    and the specific requirements of your RAG application. Understanding these aspects
    helps in selecting a model that not only fits within your budget but also meets
    your performance and speed expectations. Whether you’re deploying an LLM for real-time
    applications requiring quick responses or complex tasks demanding deep understanding
    and generation capabilities, the chosen model will have a profound impact on the
    outcomes of your LlamaIndex application. But keep in mind that you’re never limited
    to using a single model for your entire RAG logic. As LlamaIndex gives you endless
    possibilities for customization, working with a suite of different models can
    also be an option. You just have to experiment and evaluate until you find the
    ideal mix and purpose for <html:span class="No-Break">each one.</html:span></html:p>
    <html:a id="_idTextAnchor230"></html:a><html:h2 id="_idParaDest-231">Common methods
    used for creating effective prompts</html:h2> <html:p>While simple prompts can
    be useful for many tasks, more advanced techniques are often required <html:a
    id="_idIndexMarker1132"></html:a>for complex <html:a id="_idIndexMarker1133"></html:a>reasoning
    or multi-step processes. While definitely not exhaustive, this section covers
    several powerful prompting techniques that can significantly enhance the performance
    of language models in our RAG applications. Since there’s already an abundance
    of study materials, free courses, and plenty of examples available on the web,
    in case you’re not yet familiar with these methods, take this list as a mere starting
    point for your future <html:span class="No-Break">learning path.</html:span></html:p>
    <html:h3>Few-shot prompting, also known as k-shot prompting</html:h3> <html:p>As
    described in the paper by Brown et al. (2020), <html:em class="italic">Language
    Models are Few-Shot Learners</html:em> ( <html:a>https://doi.org/10.48550/arXiv.2005.14165</html:a>
    ), for complex tasks involving LLMs, few-shot <html:a id="_idIndexMarker1134"></html:a>prompting
    with demonstrations <html:a id="_idIndexMarker1135"></html:a>can enable in-context
    learning and improve performance. This <html:a id="_idIndexMarker1136"></html:a>method
    relies on providing a few examples of the task, along with the expected output,
    to condition the model. You can experiment with different numbers of examples
    (for example, one-shot, three-shot, and five-shot) to find the optimal balance,
    hence the <html:em class="italic">k-shot</html:em> <html:span class="No-Break">alternative
    name.</html:span></html:p> <html:p class="callout-heading">What about zero-shot
    prompting?</html:p> <html:p class="callout">For reference, <html:em class="italic">zero-shot
    prompting</html:em> involves presenting a model with a question without any <html:a
    id="_idIndexMarker1137"></html:a>preceding contextual question/answer pairs. This
    approach is more challenging for the model compared to one-shot or few-shot prompting,
    due to the absence <html:span class="No-Break">of context.</html:span></html:p>
    <html:p>When using few-shot prompting, keep in mind that the format you use for
    the examples and the distribution of the input text are important factors that
    can affect performance. While the few-shot prompting method increases the probability
    of a correct answer for simpler tasks, it may still struggle with more complex
    reasoning scenarios. Here’s a practical prompt example using <html:span class="No-Break">this
    technique:</html:span></html:p> <html:p>Providing the model with a few examples
    in this style enables in-context learning and improves performance on the task
    without <html:span class="No-Break">requiring fine-tuning.</html:span></html:p>
    <html:h3>Chain-of-Thought (CoT) prompting</html:h3> <html:p>First introduced in
    the paper by Wei et al. (2023), <html:em class="italic">Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models</html:em> ( <html:a>https://doi.org/10.48550/arXiv.2201.11903</html:a>
    ), this method provides impressive results for LLM tasks requiring reasoning or
    multi-step processes. We can use CoT prompting to encourage the model to break
    down the problem and show its thought process. We can include <html:a id="_idIndexMarker1138"></html:a>examples
    in our prompts, demonstrating <html:a id="_idIndexMarker1139"></html:a>the step-by-step
    reasoning process in the prompt. Here is a practical <html:span class="No-Break">prompt
    example:</html:span></html:p> <html:p>The first part of the prompt demonstrates
    the reasoning process, guiding the LLM to better answer the second part – which
    represents the <html:span class="No-Break">actual task.</html:span></html:p> <html:h3>Self-consistency</html:h3>
    <html:p>Self-consistency <html:a id="_idIndexMarker1140"></html:a>aims to improve
    the performance <html:a id="_idIndexMarker1141"></html:a>of CoT prompting by sampling
    multiple, diverse reasoning paths and using the generations to select the most
    consistent answer. First introduced in the paper by Wang et al. (2023), <html:em
    class="italic">Self-Consistency Improves Chain of Thought Reasoning in Language
    Models</html:em> ( <html:a>https://doi.org/10.48550/arXiv.2203.11171</html:a>
    ), the self-consistency method helps boost performance on tasks involving arithmetic
    and commonsense reasoning by replacing the more traditional CoT prompting. Self-consistency
    involves providing few-shot CoT examples, generating multiple reasoning paths,
    and then selecting the most consistent answer based on <html:span class="No-Break">these
    paths.</html:span></html:p> <html:p>This approach <html:a id="_idIndexMarker1142"></html:a>acknowledges
    that language models, like humans, may <html:a id="_idIndexMarker1143"></html:a>sometimes
    make mistakes or take incorrect reasoning steps. However, by leveraging the diversity
    of reasoning paths and selecting the most consistent answer, self-consistency
    can potentially provide better answers than <html:span class="No-Break">CoT prompting.</html:span></html:p>
    <html:h3>Tree of Thoughts (ToT) prompting</html:h3> <html:p>ToT is a framework
    that generalizes over CoT prompting and encourages the exploration of thoughts
    <html:a id="_idIndexMarker1144"></html:a>that serve as <html:a id="_idIndexMarker1145"></html:a>intermediate
    steps for general problem-solving with language models. Under the hood, it maintains
    a <html:em class="italic">tree of thoughts</html:em> , where thoughts represent
    coherent language sequences that serve as intermediate steps toward solving a
    problem. The language model’s ability to generate and evaluate thoughts is combined
    with specialized search algorithms to enable systematic exploration of thoughts.
    ToT prompting involves prompting the language model to evaluate intermediate thoughts
    as <html:em class="italic">sure</html:em> / <html:em class="italic">maybe</html:em>
    / <html:em class="italic">impossible</html:em> with regard to reaching the desired
    solution and then using search algorithms to explore the most promising paths.
    The method was presented for the first time in the following papers: Yao et al.
    (2023), <html:em class="italic">Tree of Thoughts: Deliberate Problem Solving with
    Large Language Models</html:em> ( <html:a>https://doi.org/10.48550/arXiv.2305.10601</html:a>
    ), and Long et al. (2023), <html:em class="italic">Large Language Model Guided</html:em>
    <html:span class="No-Break"><html:em class="italic">Tree-of-Thought</html:em></html:span>
    <html:span class="No-Break">(</html:span> <html:a><html:span class="No-Break">https://doi.org/10.48550/arXiv.2305.08291</html:span></html:a>
    <html:span class="No-Break">).</html:span></html:p> <html:p>Here’s a <html:span
    class="No-Break">sample prompt:</html:span></html:p> <html:h3>Prompt chaining</html:h3>
    <html:p>This method relies on breaking down complex tasks into subtasks and using
    a chain of prompts, where <html:a id="_idIndexMarker1146"></html:a>each prompt’s
    output serves <html:a id="_idIndexMarker1147"></html:a>as an input for the next.
    Similar to the approach I used for the PITS application in the <html:code class="literal">training_material_builder.py</html:code>
    module, prompt chaining can improve the reliability, transparency, and controllability
    of the application. By default, in RAG applications, we use separate prompts for
    retrieving relevant information and generating a final output based on the <html:span
    class="No-Break">retrieved context.</html:span></html:p> <html:p>By following
    these golden rules and methods, you can develop more effective and reliable RAG
    applications using LlamaIndex and leverage the full potential <html:span class="No-Break">of
    LLMs.</html:span></html:p> <html:a id="_idTextAnchor231"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-232">Summary</html:h1>
    <html:div id="_idContainer115"><html:p>This chapter explored the importance of
    prompt engineering in building effective RAG applications with LlamaIndex. We
    learned how to inspect and customize the default prompts used by <html:span class="No-Break">various
    components.</html:span></html:p> <html:p>The chapter provided an overview of key
    principles and best practices for crafting high-quality prompts, as well as advanced
    prompting techniques. Additionally, it emphasized the significance of choosing
    the right language model for the task at hand and understanding their different
    architectures, capabilities, <html:span class="No-Break">and trade-offs.</html:span></html:p>
    <html:p>Finally, we talked about some simple yet powerful prompting methods, such
    as few-shot prompting, CoT prompting, self-consistency, ToT, and prompt chaining
    to enhance the reasoning and problem-solving abilities of language models. Mastering
    prompt engineering is crucial for unlocking the full potential of LLMs in <html:span
    class="No-Break">RAG applications.</html:span></html:p> <html:p>As we prepare
    to wrap up our journey, I invite you to join me in the final chapter of this book,
    where I will do my best to equip you with some additional learning tools and provide
    you with a bit of guidance on your future <html:span class="No-Break">learning
    path.</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
