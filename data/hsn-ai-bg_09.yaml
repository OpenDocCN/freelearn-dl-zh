- en: Deep Learning for Intelligent Agents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能代理的深度学习
- en: Intelligent Assistants are one of the most visible forms of **Artificial Intelligence** (**AI**)
    that we see in our daily lives. Siri, Alexa, and other systems have come to be
    commonplace in day-to-day life in the 21st century. This chapter will commence
    our section of chapters that dive deeply into the application of **Artificial
    Neural Networks** (**ANNs**)for creating AI systems. In this chapter, we will
    cover one new topic, word embeddings, and then proceed to focus on the application
    of **Recurrent Neural Networks **(**RNNs**) and generative networks to natural
    language processing tasks. While an entire book could have been written about
    deep learning for natural language processing, as is already the case, we'll touch
    ...
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 智能助手是我们日常生活中最显眼的**人工智能**（**AI**）形式之一。Siri、Alexa和其他系统已经成为21世纪日常生活中的常见事物。本章将开始深入探讨**人工神经网络**（**ANNs**）在创建AI系统中的应用。在本章中，我们将介绍一个新话题，词嵌入，然后继续聚焦于**递归神经网络**（**RNNs**）和生成网络在自然语言处理任务中的应用。尽管整本书可以专门写关于自然语言处理的深度学习内容（实际上也已经有了相关书籍），我们会简要介绍…
- en: Technical requirements
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using Python 3 with a few standard python packages
    that you''ve seen before:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python 3和一些你之前见过的标准Python包：
- en: Numpy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Numpy
- en: TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: A GPU-enabled computer, or an AWS account for cloud computing, as described
    in [Chapter 3](69346214-320e-487f-b4cf-bd5c469dc75e.xhtml), *Platforms and Other
    Essentials*
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台支持GPU的计算机，或一个用于云计算的AWS账户，详细信息请参见[第3章](69346214-320e-487f-b4cf-bd5c469dc75e.xhtml)，*平台与其他必需工具*
- en: Word embeddings
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: So far, in our discussion of AI and deep learning, we've focused a lot on how
    rooted this field is in fundamental mathematical principles; so what do we do
    when we are faced with an unstructured source data such as text? In the previous
    chapters, we've talked about how we can convert images to numbers via convolutions,
    so how do we do the same thing with text? In modern AI systems, we use a technique
    called **word embedding**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，在我们对AI和深度学习的讨论中，我们重点讨论了该领域在基本数学原理中的根基；那么，当我们面临像文本这样未结构化的数据源时，我们该怎么办呢？在前几章中，我们讨论了如何通过卷积将图像转换为数字，那么我们如何用文本做同样的事情呢？在现代AI系统中，我们使用一种叫做**词嵌入**的技术。
- en: Word embedding is not a class of predictive models itself, but a means of pre-processing
    text so that it can be an input to a predictive model, or as an exploratory technique
    for data mining. It's a means by which we convert words and sentences into vectors
    of numbers, themselves called **word embeddings**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入本身并不是一种预测模型，而是处理文本的手段，使其能够作为预测模型的输入，或作为数据挖掘中的一种探索性技术。它是将单词和句子转换为数字向量的方式，这些数字向量被称为**词嵌入**。
- en: Word2vec
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec
- en: The Word2vec algorithm, invented by Tomas Mikolav while he was at Google in
    2013, was one of the first modern embedding methods. It is a shallow, two-layer
    neural network that follows a similar intuition to the autoencoder in that network
    and is trained to perform a certain task without being actually used to perform
    that task. In the case of the Word2vec algorithm, that task is learning the representations
    of natural language. You can think of this algorithm as a context algorithm –
    everything that it knows is from learning the contexts of words within sentences.
    It works off something called the **distributional hypothesis**, which tells us
    that the context for each word is found from its neighboring words. For instance,
    think about a corpus vector with 500 dimensions. Each word in the corpus is represented
    by a distribution of weights across every single one of those elements. It's not
    a one-to-one mapping; the embedding of each word is dependent upon every other
    word in the corpus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec算法由Tomas Mikolav在2013年于Google发明，是最早的现代嵌入方法之一。它是一个浅层的、两层的神经网络，其直觉与自编码器类似，目的是执行某个特定任务，但并不直接用于执行该任务。在Word2vec算法的情况下，这个任务就是学习自然语言的表示。你可以将这个算法看作一个上下文算法——它所知道的一切都来源于学习单词在句子中的上下文。它基于所谓的**分布假设**，该假设告诉我们每个单词的上下文是通过它相邻的单词来定义的。例如，假设有一个500维的语料库向量，语料库中的每个单词都会通过权重分布在这些元素上进行表示。这不是一种一对一的映射；每个单词的嵌入是依赖于语料库中其他所有单词的。
- en: In its basic form, Word2vec has a structure similar to many of the feedforward
    neural networks that we've already seen – it has an **input layer**, a **hidden
    layer**, and an **output layer**, all parameterized by a matrix **W**. It iterates
    through an input corpus word by word and develops vectors for that word. The algorithm
    actually contains two distinct variations, the **CBOW** (**continuous bag of words**) model
    and the **skip-gram model****, **which handle the creation of word vectors differently. Architecturally,
    the skip-gram model and the CBOW model are essentially reversed versions of one
    another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在其基本形式中，Word2vec 具有与我们已经看到的许多前馈神经网络类似的结构——它有一个**输入层**、一个**隐藏层**和一个**输出层**，所有这些都由矩阵**W**进行参数化。它逐字遍历输入语料库，并为每个单词开发向量。实际上，该算法包含两种不同的变体，**CBOW**（**连续词袋模型**）和
    **skip-gram 模型**，它们以不同的方式处理词向量的生成。从架构上讲，skip-gram 模型和 CBOW 模型本质上是彼此的倒置版本。
- en: 'The skip-gram model is shown in the following diagram:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型如以下图所示：
- en: '![](img/e2b4386b-f828-4423-8204-767d1ccc04f2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2b4386b-f828-4423-8204-767d1ccc04f2.png)'
- en: 'The mirror image of the skip-gram model is the CBOW model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型的镜像是 CBOW 模型：
- en: '![](img/f3678806-81d0-4a61-8ced-3871d1de361d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3678806-81d0-4a61-8ced-3871d1de361d.png)'
- en: In the skip-gram model, the network looks at sequences of words and tries to
    predict the likelihood of a certain combination of words occurring. The skip-gram
    method predicts the context given a particular word. The model inputs a singular
    letter, *w,* and outputs a vector of words, *w[1], w[2]... w[n]*. Breaking down
    this process, the preceding model takes in a one-hot encoded representation of
    an input word during training. The matrix between the input layer and the hidden
    layer of the network represents the vocabulary that the network is building, the
    rows of which will become our vectors. Each row in the matrix corresponds to one
    word in the vocabulary. The matrix gets updated row by row with new embeddings
    as new data flows through the network. Again, recall how we are not actually interested
    in what comes out of the network; we are interested in the embeddings that are
    created in the matrix *W*. This method works well with small amounts of training
    data and is good at embedding rare words. In the CBOW method, the input to the
    model is *w[1], w[2]* ... *w[n]*, the words surrounding the current word that
    the model is embedding. CBOW predicts the word given the context. TheCBOW method
    is faster than the skip-gram method and is better at embedding frequent words,
    but it requires a great deal of data given that it relies on context as input.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 skip-gram 模型中，网络查看词语序列，并尝试预测某一特定词语组合发生的可能性。skip-gram 方法在给定一个特定单词的情况下预测上下文。该模型输入一个单一的字母，*w*，并输出一个词向量，*w[1],
    w[2]... w[n]*。分解这个过程，前述模型在训练过程中接收一个输入单词的 one-hot 编码表示。输入层与隐藏层之间的矩阵代表了网络正在构建的词汇表，其行将成为我们的向量。矩阵中的每一行对应词汇表中的一个单词。随着新数据流经网络，矩阵按行逐步更新，添加新的嵌入。再次提醒，我们实际上并不关心网络输出的结果；我们关心的是在矩阵
    *W* 中创建的嵌入。该方法在训练数据较少时表现良好，并且适合嵌入稀有单词。在 CBOW 方法中，模型的输入是 *w[1], w[2]* ... *w[n]*，即当前单词周围的上下文单词。CBOW
    根据上下文预测单词。CBOW 方法比 skip-gram 方法速度更快，适合嵌入频繁出现的单词，但它需要大量的数据，因为它依赖于上下文作为输入。
- en: 'To illustrate this further, take this simple sentence:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步说明这一点，考虑这个简单的句子：
- en: '*The dog jumped over the fence*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*The dog jumped over the fence*'
- en: 'The skip-gram model parses the sentence by focusing on a subject, breaking
    the subject into chunks called **grams**, each time skipping as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型通过关注一个主题来解析句子，将该主题分解为称为**gram**的块，每次跳过如下：
- en: '{*The dog jumped*, *The dog over*, *dog jumped over*, *dog jumped the* ....
    and so on}'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '{*The dog jumped*, *The dog over*, *dog jumped over*, *dog jumped the* ....
    等等}'
- en: 'On the other hand, under the CBOW method, the grams would iteratively move
    through the context of the sentence as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在 CBOW 方法下，gram 会依次遍历句子的上下文，如下所示：
- en: '{*The dog jumped*, *dog jumped over*, *jumped over the*, *over the fence*}'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '{*The dog jumped*, *dog jumped over*, *jumped over the*, *over the fence*}'
- en: Training Word2vec models
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 Word2vec 模型
- en: As Word2vec models are neural networks themselves, we train them just like a
    standard feedforward network with a loss function and stochastic gradient descent.
    During the training process, the algorithm scans over the input corpus and takes
    batches of it as input. After each batch, a loss is calculated. When optimizing,
    we want to minimize our loss as we would with a standard feedforward neural network.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Word2vec模型本身是神经网络，我们像训练标准的前馈神经网络一样训练它们，使用损失函数和随机梯度下降。在训练过程中，算法扫描输入语料库，并将其批量输入。每处理完一个批次，就会计算一次损失。在优化时，我们希望像训练标准的前馈神经网络一样最小化损失。
- en: 'Let''s walk through how we would create and train a Word2vec model in TensorFlow:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何在TensorFlow中创建和训练一个Word2vec模型：
- en: First, let's start with our imports. We'll use our standard `tensorflow` and
    `numpy` imports and the Python library itertools, as well as two utility functions
    from the machine learning package `scikit-learn`. The following code block shows
    ...
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从导入开始。我们将使用标准的`tensorflow`和`numpy`导入，以及Python库itertools，还会使用机器学习包`scikit-learn`中的两个实用函数。以下代码块展示了...
- en: GloVe
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe
- en: '** Globalized Vectors** (**GloVe**)was developed by the Stanford NLP group
    in 2014 as a probabilistic follow-up to Word2Vec. GloVe was designed to preserve
    the analogies framework used by Word2vec, but instead uses dimensionality reduction
    techniques that would preserve key statistical information about the words themselves.
    Unlike Word2vec, which learns by streaming sentences, GloVe learns embeddings
    by constructing a rich co-occurrence matrix. The co-occurrence matrix is a global
    store of semantic information, and is key to the GloVe algorithm. The creators
    of GloVe developed it on the principle that co-occurrence ratios between two words
    in a contextare closely related to meaning.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**全球化向量** (**GloVe**) 是斯坦福NLP小组在2014年开发的，作为Word2Vec的概率性后续版本。GloVe旨在保留Word2vec使用的类比框架，但采用维度约简技术来保留关于单词本身的关键统计信息。与通过流式句子学习的Word2vec不同，GloVe通过构建丰富的共现矩阵来学习词嵌入。共现矩阵是一个全球语义信息存储库，是GloVe算法的关键。GloVe的创造者基于这样的原则开发了它：在上下文中，两个词之间的共现比率与其意义密切相关。'
- en: 'So how does it work, and how is it different from Word2vec? GloVe creates a
    word embedding by means of the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它是如何工作的，和Word2vec有什么不同呢？GloVe通过以下方式创建词嵌入：
- en: Iterating over a sentence, word by word
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐字迭代句子
- en: For each word, the algorithm looks at its context
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个词，算法会查看它的上下文。
- en: Given the word and its context, GloVe creates a new entry in the co-occurrence
    matrix
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定词汇及其上下文，GloVe会在共现矩阵中创建一个新条目。
- en: GloVe then reduces the dimensions of the co-occurrence matrix to create embeddings
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GloVe随后通过降低共现矩阵的维度来创建词嵌入。
- en: After creating the embedding, GloVe calculates its new loss function based on
    the accuracy of that embedding
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建词嵌入后，GloVe会根据该嵌入的准确性计算新的损失函数。
- en: 'Let''s walk through the GloVe algorithm alongside a Python implementation to
    see how it all pans out. We''re going to be using the Cornell movie lines dataset,
    which contains over 200,000 fictional movie script lines. Later on, we''ll use
    the embeddings we generated from this dataset in our intelligent agent. First,
    let''s write a function to import the data:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Python实现一步步走过GloVe算法，看看它是如何进行的。我们将使用康奈尔电影台词数据集，它包含超过200,000行虚构的电影剧本台词。稍后，我们将在智能体中使用从该数据集生成的词嵌入。首先，让我们写一个函数来导入数据：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then use this function to actually load the movie lines:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用这个函数来加载电影台词：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s get back to GloVe. Unlike Word2vec, GloVe parses over a sentence
    word by word, focusing on local context by using a fixed contextwindow size. In
    word embedding, the window size represents the extent to which and what an algorithm
    will focus on in order to provide context to a word''s meaning. There are two
    forms of context window sizes in GloVe – symmetric and asymmetric. For example,
    take a look at the following sentence:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到GloVe。与Word2vec不同，GloVe逐字解析句子，通过使用固定的上下文窗口大小来关注局部上下文。在词嵌入中，窗口大小表示算法关注的范围及其为词义提供上下文的方式。GloVe中有两种上下文窗口大小——对称和不对称。例如，看看以下句子：
- en: '*The horse ran fast across the finish line in the race. *'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*马在比赛的终点线上快速跑过。*'
- en: With a symmetric window size, the algorithm would look at words on either side
    of the subject. If GloVe was looking at the word *finish* with a window size of
    2 in the preceding example, the context would be *across the* and *line in*. Asymmetric
    windows look only at the preceding words, so the same window size of 2 would capture
    *across* *the*, but not *line in*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在对称窗口大小的情况下，算法会查看主题两侧的单词。如果 GloVe 在前面的示例中查看单词 *finish*，窗口大小为 2，那么上下文将是 *across
    the* 和 *line in*。非对称窗口仅查看前面的单词，因此相同的窗口大小 2 将捕获 *across* 和 *the*，但不会捕获 *line in*。
- en: 'Let''s go ahead and initialize our GloVe class and variables:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续初始化我们的 GloVe 类和变量：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We end up with a co-occurrence matrix that can tell us how often certain words
    occur together given a certain window size:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个共现矩阵，可以告诉我们在给定窗口大小的情况下，某些单词一起出现的频率：
- en: '![](img/b7e013cd-ab7d-48e2-8408-4036ffde9dc0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7e013cd-ab7d-48e2-8408-4036ffde9dc0.png)'
- en: 'While this table looks simple, it contains global statistical properties about
    the co-occurrence of the words within. From it, we can calculate the probabilities
    of certain words occurring together:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这张表看起来很简单，但它包含了关于单词共现的全局统计特性。我们可以从中计算出某些单词一起出现的概率：
- en: '![](img/96b6ca1b-db98-48c2-8f14-3fab4807e402.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96b6ca1b-db98-48c2-8f14-3fab4807e402.png)'
- en: As the co-occurrence matrix is combinatorial in size (it can become large very
    quickly), it leads to extremely large matrices of co-occurrence information. How
    do we remedy this? We can factorize the matrix to create a lower-dimensional matrix
    where each row contains a vector representation of a given word. This performs
    a form of dimensionality reduction on the co-occurrence matrix. We then pre-process
    the matrix by normalizing and log—smoothing the occurrence information. GloVe
    will learn vectors so that their differences predict occurrence ratios. All of
    this will maintain rich global statistical properties while still preserving the
    analogies that make Word2vec so desirable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于共现矩阵的大小是组合性的（它会迅速变得非常大），这导致了极其庞大的共现信息矩阵。我们该如何解决这个问题？我们可以对矩阵进行分解，创建一个低维矩阵，其中每一行包含给定单词的向量表示。这对共现矩阵执行一种降维操作。然后我们通过标准化和对数平滑处理该矩阵中的发生信息。GloVe
    将学习向量，使它们的差异预测发生比率。所有这些操作会保持丰富的全局统计特性，同时仍然保留使得 Word2vec 如此受欢迎的类比关系。
- en: 'GloVe is trained by minimizing a reconstruction loss that helps the model find
    the lower-dimensional representations that can explain the highest amount of variance
    in the original data. It utilizes a least squares loss function that seeks to
    minimize the difference between the dot product of two embeddings of a word and
    the log of their co-occurrence count:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 通过最小化重建损失来训练，帮助模型找到能够解释原始数据中最大方差的低维表示。它使用最小二乘损失函数，旨在最小化单词的两个嵌入的点积与它们的共现计数的对数之间的差异：
- en: '![](img/423d6543-fc84-4df4-9bb2-da38dea74340.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/423d6543-fc84-4df4-9bb2-da38dea74340.png)'
- en: Let's break this down; *w*[i] is a word vector and b[i] is a bias factor for
    a specific word *i*, while w[*j*] and b[*j*] are the word vector and bias factor
    for the context vector. *X[ij ]*is the count from the co-occurrence matrix of
    how many times *i* and *j* occur together. *f* is a weighting function for both
    rare and frequent co-occurrences so that they do not skew the results. In all,
    this loss function looks at the weight co-occurrences of a word and its neighboring
    context words, and multiplies that by the right term, which computes a combination
    of the word, its contexts, biases, and co-occurrences.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解析一下；*w*[i] 是一个单词向量，b[i] 是特定单词 *i* 的偏置因子，而 w[*j*] 和 b[*j*] 是上下文向量的单词向量和偏置因子。*X[ij]*
    是共现矩阵中记录 *i* 和 *j* 一起出现次数的计数。*f* 是一个加权函数，用于处理稀有和频繁的共现，以避免它们对结果产生偏差。总的来说，这个损失函数查看一个单词及其邻近上下文单词的加权共现，然后将其乘以右侧项，这个项计算了单词、其上下文、偏置和共现的组合。
- en: 'Let''s go ahead and initialize GloVe''s graph in TensorFlow to proceed with
    the training process:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续初始化 GloVe 的图形，并在 TensorFlow 中进行训练过程：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we''ll write two functions to prepare the batches of data for the GloVe
    model, just as we did with Word2vec. Remember that all of this is still contained
    within our GloVe class:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写两个函数，为 GloVe 模型准备数据批次，和我们在 Word2vec 中做的一样。记住，这一切仍然包含在我们的 GloVe 类中：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we''re going to understand different properties. For those of you that
    have used Java before, you are familiar with the concept of getters and setters.
    These methods enable the changes that can happen to a variable to be controlled.
    The `@property` decorator is Python''s response to these, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将了解不同的属性。对于那些之前使用过 Java 的朋友来说，你们一定熟悉 getter 和 setter 的概念。这些方法可以控制变量的变化。`@property`
    装饰器是 Python 对这些概念的回应，具体如下：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, the `foo` function is replaced by a new function, `property(foo)`, which
    is an object with special properties called **descriptors**. Now, let''s return
    to Word2vec:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`foo` 函数被一个新的函数 `property(foo)` 替代，这个新函数是一个具有特殊属性的对象，称为**描述符**。现在，让我们回到
    Word2vec：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We''ll also create a function for the `ContextWindow` that tells GloVe which
    words to focus on:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将为 `ContextWindow` 创建一个函数，告诉 GloVe 哪些词语需要关注：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Lastly, we''ll write our function for training purposes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写一个用于训练的函数：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can run our GloVe model with the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下内容运行我们的 GloVe 模型：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: GloVe's idea for dense matrices of co-occurrence information isn't new; it comes
    from a more traditional technique called **latent semantic analysis** (**LDA**)
    that learns embedding by decomposing bag-of-words term document matrices using
    a mathematical technique called **singular value decomposition** (**SVD**).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 关于共现信息的密集矩阵的想法并不新鲜；它来源于一种更传统的技术，叫做**潜在语义分析**（**LDA**），该技术通过使用一种叫做**奇异值分解**（**SVD**）的数学方法，分解词袋模型的文档矩阵来学习嵌入。
- en: Constructing a basic agent
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基本代理
- en: The simplest way to construct an artificial assistant with TensorFlow is to
    use a **sequence-to-sequence** (**Seq2Seq**) model, which we learned in the chapter
    on RNNs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 构建人工助手的最简单方法是使用**序列到序列**（**Seq2Seq**）模型，这个我们在 RNN 章节中已经学过。
- en: '![](img/75eb64b2-8221-4770-be93-dc9377127796.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75eb64b2-8221-4770-be93-dc9377127796.png)'
- en: 'While originally developed for neural machine translation, we can adjust this
    model to act as an intelligent chatbot for our own purposes. We''ll create the
    *brain* behind our assistant as a Python class called `IntelligentAssistant`.
    Then, we''ll create the training and chatting functions for our assistant:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最初是为神经机器翻译开发的，但我们可以调整这个模型使其作为智能聊天机器人，以便实现我们自己的目的。我们将创建作为助手“大脑”的 Python 类，命名为
    `IntelligentAssistant`。然后，我们将创建助手的训练和聊天函数：
- en: First, let's start with our standard imports and initialize our variables. Take
    special note of the `mask` variable here; `masks` are placeholders that allow
    ...
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从标准导入开始，并初始化我们的变量。请特别注意这里的 `mask` 变量；`masks` 是占位符，允许……
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this section, we learned how to create novel, state-of-the-art intelligent
    assistants by using word embeddings and ANNs. Word embedding techniques are the
    cornerstone of AI applications for natural language. They allow us to encode natural
    language as mathematics that we can feed into downstream models and tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们学习了如何通过使用词嵌入和人工神经网络（ANN）来创建新型的、最先进的智能助手。词嵌入技术是自然语言人工智能应用的基石。它们使我们能够将自然语言编码为数学模型，然后将其输入到下游模型和任务中。
- en: Intelligent agents take these word embeddings and reason over them. They utilize
    two RNNs, an encoder and a decoder, in what is called a Seq2Seq model. If you
    cast your mind back to the chapter on recurrent neural networks, the first RNN
    in the Seq2Seq model encodes the input into a compressed representation, while
    the second network draws from that compressed representation to deliver sentences.
    In this way, an intelligent agent learns to respond to a user based on a representation
    of what it learned during the training process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 智能代理利用这些词嵌入进行推理。它们使用两个 RNN，一个编码器和一个解码器，构成所谓的 Seq2Seq 模型。如果你回想一下 RNN 章节中的内容，Seq2Seq
    模型中的第一个 RNN 将输入编码为压缩表示，而第二个网络则从该压缩表示中获取信息来生成句子。通过这种方式，智能代理学会根据训练过程中学到的表示来回应用户。
- en: In the next chapter, we'll look into how we can create intelligent agents that
    can play board games.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章节中，我们将探讨如何创建能够玩棋类游戏的智能代理。
