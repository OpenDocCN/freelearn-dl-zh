- en: Chapter 2. Finding and Working with Words
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：查找和使用单词
- en: 'In this chapter, we cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下内容：
- en: Introduction to tokenizer factories – finding words in a character stream
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器工厂简介——在字符流中查找单词
- en: Combining tokenizers – lowercase tokenizer
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合分词器——小写分词器
- en: Combining tokenizers – stop word tokenizers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合分词器——停用词分词器
- en: Using Lucene/Solr tokenizers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Lucene/Solr分词器
- en: Using Lucene/Solr tokenizers with LingPipe
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LingPipe与Lucene/Solr分词器
- en: Evaluating tokenizers with unit tests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单元测试评估分词器
- en: Modifying tokenizer factories
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改分词器工厂
- en: Finding words for languages without white spaces
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为没有空格的语言查找单词
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: An important part of building NLP systems is to work with the appropriate unit
    for processing. This chapter addresses the abstraction layer associated with the
    word level of processing. This is called tokenization, which amounts to grouping
    adjacent characters into meaningful chunks in support of classification, entity
    finding, and the rest of NLP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 构建NLP系统的一个重要部分是与适当的处理单元合作。本章讨论与处理单词级别相关的抽象层。这被称为分词，其目的是将相邻字符分组为有意义的块，以支持分类、实体识别以及其他NLP任务。
- en: LingPipe provides a broad range of tokenizer needs, which are not covered in
    this book. Look at the Javadoc for tokenizers that do stemming, Soundex (tokens
    based on what English words sound like), and more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe提供了一系列的分词需求，这些需求在本书中没有涵盖。请查看分词器的Javadoc，这些分词器可以进行词干提取、Soundex（基于英语单词发音的标记）等。
- en: Introduction to tokenizer factories – finding words in a character stream
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器工厂简介——在字符流中查找单词
- en: LingPipe tokenizers are built on a common pattern of a base tokenizer that can
    be used on its own, or can be as the source for subsequent filtering tokenizers.
    Filtering tokenizers manipulate the tokens/white spaces provided by the base tokenizer.
    This recipe covers our most commonly used tokenizer, `IndoEuropeanTokenizerFactory`,
    which is good for languages that use the Indo-European style of punctuation and
    word separators—examples include English, Spanish, and French. As always, the
    Javadoc has useful information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe分词器基于一个通用模式，即一个可以单独使用或作为后续过滤分词器源的基部分词器。过滤分词器操作基部分词器提供的标记/空白。本食谱涵盖了我们的最常用分词器`IndoEuropeanTokenizerFactory`，它适用于使用印欧风格标点符号和单词分隔符的语言——例如英语、西班牙语和法语。一如既往，Javadoc提供了有用的信息。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`IndoEuropeanTokenizerFactory` creates tokenizers with built-in support for
    alpha-numerics, numbers, and other common constructs in Indo-European languages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndoEuropeanTokenizerFactory`创建具有内置对印欧语言中的字母数字、数字和其他常见结构的支持的分词器。'
- en: The tokenization rules are roughly based on those used in MUC-6 but are necessarily
    more fine grained, because the MUC tokenizers are based on lexical and semantic
    information, such as whether a string is an abbreviation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分词规则大致基于MUC-6中使用的规则，但必然更加精细，因为MUC分词器基于词汇和语义信息，例如一个字符串是否为缩写。
- en: MUC-6 refers to the Message Understanding Conference that originated the idea
    of government-sponsored competitions between contractors in 1995\. The informal
    term was *Bake off*, in reference to the Pillsbury Bake-Off that started in 1949,
    and one of the authors was a participant as postdoc in MUC-6\. MUC drove much
    of the innovation in the evaluation of NLP systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MUC-6指的是1995年发起的政府赞助的承包商之间竞争的会议，该会议起源于1995年。非正式术语是*Bake off*，指的是1949年开始的Pillsbury
    Bake-Off，其中一位作者作为MUC-6的后博士后参与者。MUC推动了NLP系统评估的大部分创新。
- en: LingPipe tokenizers are built using the LingPipe `TokenizerFactory` interface,
    which provides a way of invoking different types of tokenizers using the same
    interface. This is very useful in creating filtered tokenizers, which are constructed
    as a chain of tokenizers and modify their output in some way. A `TokenizerFactory`
    instance might be created either as a basic tokenizer, which takes simple parameters
    in its construction, or as a filtered tokenizer, which takes other tokenizer factory
    objects as parameters. In either case, an instance of `TokenizerFactory` has a
    single `tokenize()` method, which takes input as a character array, a start index,
    and the number of characters to process and outputs a `Tokenizer` object. The
    `Tokenizer` object represents the state of tokenizing a particular slice of string
    and provides a stream of tokens. While `TokenizerFactory` is thread safe and/or
    serializable, tokenizer instances are typically neither thread safe nor serializable.
    The `Tokenizer` object provides methods to iterate over the tokens in the string
    and to provide token positions of the tokens in the underlying text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 标记化器是使用 LingPipe `TokenizerFactory` 接口构建的，该接口提供了一种使用相同接口调用不同类型标记化器的方法。这在创建过滤标记化器时非常有用，过滤标记化器作为标记化器的链构建，并以某种方式修改其输出。`TokenizerFactory`
    实例可以是基本标记化器，它在构建时接受简单参数，或者作为过滤标记化器，它接受其他标记化器工厂对象作为参数。在两种情况下，`TokenizerFactory`
    实例都有一个单一的 `tokenize()` 方法，该方法接受输入作为字符数组、起始索引和要处理的字符数，并输出一个 `Tokenizer` 对象。`Tokenizer`
    对象表示对特定字符串片段进行标记化的状态，并提供标记流。虽然 `TokenizerFactory` 是线程安全且/或可序列化的，但标记化器实例通常既不是线程安全的也不是可序列化的。`Tokenizer`
    对象提供方法来遍历字符串中的标记，并提供标记在底层文本中的位置。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Download the JAR file and source for the book if you have not already done so.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请下载该书的 JAR 文件和源代码。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'It is all pretty simple. The following are the steps to get started with tokenization:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很简单。以下是与标记化开始相关的步骤：
- en: 'Go to the `cookbook` directory and invoke the following class:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 `cookbook` 目录并调用以下类：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will lead us to a command prompt, which asks us to type in some text:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将带我们到一个命令提示符，提示我们输入一些文本：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we type a sentence such as: `It''s no use growing older if you only learn
    new ways of misbehaving yourself`, we will get the following output:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们输入一个句子，例如：`It's no use growing older if you only learn new ways of misbehaving
    yourself`，我们将得到以下输出：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Examine the output and note what the tokens and white spaces are. The text is
    from the short story, *The Stampeding of Lady Bastable*, by Saki.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查输出并注意标记和空白。文本来自萨基的短篇小说《巴斯特布尔女士的狂奔》。
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The code is so simple that it can be included in its entirety as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单，可以完整地包含如下：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This recipe starts with the creation of `TokenizerFactory tokFactory` in the
    first statement of the `main()` method. Note that a singleton `IndoEuropeanTokenizerFactory.INSTANCE`
    is used. The factory will produce tokenizers for a given string, which is evident
    in the line, `Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),
    0, input.length())`. The entered string is converted to a character array with
    `input.toCharArray()` as the first argument to the `tokenizer` method and the
    start and finish offsets provided into the created character array.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方从 `main()` 方法的第一个语句中创建 `TokenizerFactory tokFactory` 开始。请注意，使用了单例 `IndoEuropeanTokenizerFactory.INSTANCE`。该工厂将为给定的字符串生成标记化器，这在
    `Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length())`
    这一行中很明显。输入的字符串通过 `input.toCharArray()` 转换为字符数组，作为 `tokenizer` 方法的第一个参数，并将起始和结束偏移量提供给创建的字符数组。
- en: 'The resulting `tokenizer` provides tokens for the provided slice of character
    array, and the white spaces and tokens are printed out in the `while` loop. Calling
    the `tokenizer.nextToken()` method does a few things:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的 `tokenizer` 为提供的字符数组片段提供标记，空白和标记在 `while` 循环中打印出来。调用 `tokenizer.nextToken()`
    方法执行以下操作：
- en: The method returns the next token or null if there is no next token. The null
    then ends the loop; otherwise, the loop continues.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法返回下一个标记或 null（如果没有下一个标记）。null 结束循环；否则，循环继续。
- en: The method also increments the corresponding white space. There is always a
    white space with a token, but it might be the empty string.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法还增加相应的空白。总是有一个与标记一起的空白，但它可能是空字符串。
- en: '`IndoEuropeanTokenizerFactory` assumes a fairly standard abstraction over characters
    that break down as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndoEuropeanTokenizerFactory`假设字符分解的相当标准的抽象，分解如下：'
- en: Characters from the beginning of the `char` array to the first token are ignored
    and not reported as white space
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`char`数组的开始到第一个标记之间的字符被忽略，并且不会作为空白字符报告
- en: Characters from the end of the last token to the end of the `char` array are
    reported as the next white space
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上一个标记的末尾字符到`char`数组末尾之间的字符被报告为下一个空白字符
- en: White spaces can be the empty string because of two adjoining tokens—note the
    apostrophe in the output and corresponding white spaces
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于两个相邻的标记，空白可以是空字符串——注意输出中的撇号和相应的空白
- en: This means that it is not possible to reconstruct the original string necessarily
    if the input does not start with a token. Fortunately, tokenizers are easily modified
    for customized needs. We will see this later in the chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着如果输入不以标记开始，则不一定能够重建原始字符串。幸运的是，分词器可以很容易地修改以满足定制需求。我们将在本章后面看到这一点。
- en: There's more…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: Tokenization can be arbitrarily complex. The LingPipe tokenizers are intended
    to cover most common uses, but you might need to create your own tokenizer to
    have fine-grained control, for example, Victoria's Secret with "Victoria's" as
    the token. Consult the source for `IndoEuropeanTokenizerFactory` if such customization
    is needed, to see how arbitrary tokenization is done here.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分词可以是任意复杂的。LingPipe分词器旨在覆盖大多数常见用途，但你可能需要创建自己的分词器以实现细粒度控制，例如，将“Victoria's Secret”中的“Victoria's”作为一个标记。如果需要此类自定义，请参考`IndoEuropeanTokenizerFactory`的源代码，以了解这里是如何进行任意分词的。
- en: Combining tokenizers – lowercase tokenizer
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合分词器 – 小写分词器
- en: We mentioned in the previous recipe that LingPipe tokenizers can be basic or
    filtered. Basic tokenizers, such as the Indo-European tokenizer, don't need much
    in terms of parameterization, none at all as a matter of fact. However, filtered
    tokenizers need a tokenizer as a parameter. What we're doing with filtered tokenizers
    is invoking multiple tokenizers where a base tokenizer is usually modified by
    a filter to produce a different tokenizer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们提到LingPipe分词器可以是基本的或过滤的。基本分词器，如印欧分词器，不需要太多的参数化，实际上根本不需要。然而，过滤分词器需要一个分词器作为参数。我们使用过滤分词器所做的就是在多个分词器中调用，其中基本分词器通常通过过滤器修改以产生不同的分词器。
- en: LingPipe provides several basic tokenizers, such as `IndoEuropeanTokenizerFactory`
    or `CharacterTokenizerFactory`. A complete list can be found in the Javadoc for
    LingPipe. In this section, we'll show you how to combine an Indo-European tokenizer
    with a lowercase tokenizer. This is a fairly common process that many search engines
    implement for Indo-European languages.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe提供了几个基本分词器，例如`IndoEuropeanTokenizerFactory`或`CharacterTokenizerFactory`。完整的列表可以在LingPipe的Javadoc中找到。在本节中，我们将向您展示如何将一个印欧分词器与一个小写分词器结合使用。这是一个相当常见的流程，许多搜索引擎为印欧语言实现此流程。
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要下载书籍的JAR文件，并设置Java和Eclipse，以便可以运行示例。
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'This works just the same way as the previous recipe. Perform the following
    steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这与前面的配方工作方式相同。执行以下步骤：
- en: 'Invoke the `RunLowerCaseTokenizerFactory` class from the command line:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`RunLowerCaseTokenizerFactory`类：
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, in the command prompt, let''s use the following example:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在命令提示符下，让我们使用以下示例：
- en: '[PRE5]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: You can see in the preceding output that all the tokens are converted to lowercase,
    including the word `UPPERCASE`, which was typed in uppercase. As this example
    uses an Indo-European tokenizer as its base tokenizer, you can see that the number
    4.5 is retained as `4.5` instead of being broken up into 4 and 5.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在前面的输出中看到，所有标记都被转换为小写，包括以大写输入的单词`UPPERCASE`。由于此示例使用印欧分词器作为其基本分词器，您可以看到数字4.5被保留为`4.5`，而不是被拆分为4和5。
- en: 'The way we put tokenizers together is very simple:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们组合分词器的方式非常简单：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we created a tokenizer that returns case and white space normalized tokens
    produced using an Indo-European tokenizer. The tokenizer created from the tokenizer
    factory is a filtered tokenizer that starts with the Indo-European base tokenizer,
    which is then modified by `LowerCaseTokenizer` to produce the lowercase tokenizer.
    This is then once again modified by `WhiteSpaceNormTokenizerFactory` to produce
    a lowercase, white space-normalized Indo-European tokenizer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个分词器，它返回使用印欧语分词器生成的归一化大小写和空白符标记符。从分词器工厂创建的分词器是一个过滤分词器，它以印欧语基础分词器开始，然后通过`LowerCaseTokenizer`修改以生成小写分词器。然后，它再次通过`WhiteSpaceNormTokenizerFactory`修改，以生成小写、空白符归一化的印欧语分词器。
- en: Case normalization is applied where the case of words doesn't matter much; for
    example, search engines often store case-normalized words in their indexes. Now,
    we will use case-normalized tokens in the upcoming examples on classifiers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用了大小写归一化，因为单词的大小写并不重要；例如，搜索引擎通常在它们的索引中存储大小写归一化的单词。现在，我们将使用大小写归一化的标记符在即将到来的关于分类器的示例中。
- en: See also
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more details on how filtered tokenizers are built, see the Javadoc for the
    abstract class, `ModifiedTokenizerFactory.`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关如何构建过滤分词器的更多详细信息，请参阅抽象类`ModifiedTokenizerFactory`的Javadoc。
- en: Combining tokenizers – stop word tokenizers
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合分词器 - 停用词分词器
- en: Similarly to the way in which we put together a lowercase and white space normalized
    tokenizer, we can use a filtered tokenizer to create a tokenizer that filters
    out stop words. Once again, using search engines as our example, we can remove
    commonly occurring words from our input set so as to normalize the text. The stop
    words that are typically removed convey very little information by themselves,
    although they might convey information in context.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们组合小写和空白符归一化分词器的方式，我们可以使用过滤分词器创建一个过滤掉停用词的分词器。再次以搜索引擎为例，我们可以从我们的输入集中移除常见单词，以便归一化文本。通常被移除的停用词本身传达的信息很少，尽管它们可能在上下文中传达信息。
- en: The input is tokenized using whatever base tokenizer is set up, and then, the
    resulting tokens are filtered out by the stop tokenizer to produce a token stream
    that is free of the stop words specified when the stop tokenizer is initialized.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入使用设置的任何基础分词器进行分词，然后，通过停用词分词器过滤掉结果标记符，以生成一个在初始化停用词分词器时指定的停用词免于出现的标记符流。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要下载书籍的JAR文件，并设置Java和Eclipse，以便你可以运行示例。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'As we did earlier, we will go through the steps of interacting with the tokenizer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所做的那样，我们将通过与分词器交互的步骤：
- en: 'Invoke the `RunStopTokenizerFactory` class from the command line:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`RunStopTokenizerFactory`类：
- en: '[PRE7]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, in the prompt, let''s use the following example:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在提示中，让我们使用以下示例：
- en: '[PRE8]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we lose adjacency information. In the input, we have `fox is jumping`,
    but the tokens came out as `fox` followed by `jumping`, because `is` was filtered.
    This can be a problem for token-based processes that need accurate adjacency information.
    In the *Foreground- or background-driven interesting phrase detection* recipe
    of [Chapter 4](part0051_split_000.html#page "Chapter 4. Tagging Words and Tokens"),
    *Tagging Words and Tokens*, we will show a length-based filtering tokenizer that
    preserves adjacency.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们丢失了相邻信息。在输入中，我们有`fox is jumping`，但分词结果却是`fox`后面跟着`jumping`，因为`is`被过滤掉了。这可能对需要准确相邻信息的基于分词的过程造成问题。在第4章（part0051_split_000.html#page
    "Chapter 4. Tagging Words and Tokens"）的*前景或背景驱动的有趣短语检测*配方中，我们将展示一个基于长度的过滤分词器，它可以保留相邻信息。
- en: How it works...
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The stop words used in this `StopTokenizerFactory` filter are just a very short
    list of words, `is`, `of`, `the`, and `to`. Obviously, this list can be much longer
    if required. As you saw in the preceding output, the words `the` and `is` have
    been removed from the tokenized output. This is done with a very simple step:
    we instantiate `StopTokenizerFactory` in `src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java`.
    The relevant code is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`StopTokenizerFactory`过滤器中使用的停用词只是一个非常短的单词列表，`is`、`of`、`the`和`to`。显然，如果需要，这个列表可以更长。正如你在前面的输出中看到的那样，单词`the`和`is`已经被从分词输出中移除。这是通过一个非常简单的步骤完成的：我们在`src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java`中实例化`StopTokenizerFactory`。相关的代码是：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we're using `LowerCaseTokenizerFactory` as one of the filters in the tokenizer
    factory, we can get away with the stop words that contain only lowercase words.
    If we want to preserve the case of the input tokens and continue to remove the
    stop words, we will need to add uppercase or mixed-case versions as well.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在分词器工厂中使用 `LowerCaseTokenizerFactory` 作为其中一个过滤器，我们可以忽略只包含小写单词的停用词。如果我们想保留输入标记的大小写并继续移除停用词，我们需要添加大写或混合大小写的版本。
- en: See also
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The complete list of filtered tokenizers provided by LingPipe can be found on
    the Javadoc page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LingPipe 提供的过滤分词器的完整列表可以在 Javadoc 页面 [http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html)
    上找到。
- en: Using Lucene/Solr tokenizers
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Lucene/Solr 分词器
- en: The very popular search engine, Lucene, includes many analysis modules, which
    provide general purpose tokenizers as well as language-specific tokenizers from
    Arabic to Thai. As of Lucene 4, most of these different analyzers can be found
    in separate JAR files. We will cover Lucene tokenizers, because they can be used
    as LingPipe tokenizers, as you will see in the next recipe.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非常流行的搜索引擎 Lucene 包含许多分析模块，它们提供通用分词器以及从阿拉伯语到泰语的语言特定分词器。截至 Lucene 4，这些不同的分析器大多可以在单独的
    JAR 文件中找到。我们将介绍 Lucene 分词器，因为它们可以用作 LingPipe 分词器，您将在下一个配方中看到。
- en: Much like the LingPipe tokenizers, Lucene tokenizers also can be split into
    basic tokenizers and filtered tokenizers. Basic tokenizers take a reader as input,
    and filtered tokenizers take other tokenizers as input. We will look at an example
    of using a standard Lucene analyzer along with a lowercase-filtered tokenizer.
    A Lucene analyzer essentially maps a field to a token stream. So, if you have
    an existing Lucene index, you can use the analyzer with the field name instead
    of the raw tokenizer, as we will show in the later part of this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LingPipe 分词器类似，Lucene 分词器也可以分为基本分词器和过滤分词器。基本分词器以读取器作为输入，而过滤分词器以其他分词器作为输入。我们将查看一个使用标准
    Lucene 分析器和 lowercase-filtered 分词器的示例。Lucene 分析器本质上将字段映射到标记流。因此，如果您有一个现有的 Lucene
    索引，您可以使用分析器而不是原始分词器，正如我们将在本章的后续部分展示的那样。
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example. Some of the Lucene analyzers used in the
    examples are part of the `lib` directory. However, if you'd like to experiment
    with other language analyzers, download them from the Apache Lucene website at
    [https://lucene.apache.org](https://lucene.apache.org).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要下载书籍的 JAR 文件，并设置 Java 和 Eclipse，以便运行示例。示例中使用的某些 Lucene 分析器是 `lib` 目录的一部分。但是，如果您想尝试其他语言分析器，请从
    Apache Lucene 网站下载它们：[https://lucene.apache.org](https://lucene.apache.org)。
- en: How to do it...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Remember that we are not using a LingPipe tokenizer in this recipe but introducing
    the Lucene tokenizer classes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在这个配方中我们没有使用 LingPipe 分词器，而是介绍了 Lucene 分词器类：
- en: 'Invoke the `RunLuceneTokenizer` class from the command line:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用 `RunLuceneTokenizer` 类：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, in the prompt, let''s use the following example:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在提示中，让我们使用以下示例：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let''s review the following code to see how the Lucene tokenizers differ in
    invocation from the previous examples—the relevant part of the code from `src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java`
    is:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾以下代码，看看 Lucene 分词器在调用上与前面的示例有何不同——来自 `src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java`
    的相关代码部分是：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'All the input is now wrapped up, and it is time to construct the actual tokenizer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有输入都已封装，现在是构建实际分词器的时候了：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The input text is used to construct `StandardTokenizer` with Lucene's versioning
    system supplied—this produces an instance of `TokenStream`. Then, we used `LowerCaseFilter`
    to create the final filtered `tokenStream` with the base `tokenStream` as an argument.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本用于使用 Lucene 的版本控制系统构建 `StandardTokenizer`，这产生了一个 `TokenStream` 实例。然后，我们使用
    `LowerCaseFilter` 创建最终的过滤 `tokenStream`，其中将基本 `tokenStream` 作为参数。
- en: 'In Lucene, we need to attach the attributes we''re interested in from the token
    stream; this is done by the `addAttribute` method:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Lucene 中，我们需要从标记流中附加我们感兴趣的属性；这是通过 `addAttribute` 方法完成的：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note that in Lucene 4, once the tokenizer has been instantiated, the `reset()`
    method must be called before using the tokenizer:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 Lucene 4 中，一旦分词器被实例化，在使用分词器之前必须调用 `reset()` 方法：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `tokenStream` is wrapped up with the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenStream` 被以下内容包装：'
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: See also
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: An excellent introduction to Lucene is in *Text Processing with Java*, *Mitzi
    Morris*, *Colloquial Media Corporation*, where the guts of what we explained earlier
    are made clearer than what we can provide in a recipe.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *Text Processing with Java*，*Mitzi Morris*，*Colloquial Media Corporation*
    中，对 Lucene 的一个很好的介绍，其中我们之前解释的内容比在配方中提供的更清晰。
- en: Using Lucene/Solr tokenizers with LingPipe
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LingPipe 与 Lucene/Solr 分词器
- en: We can use these Lucene tokenizers with LingPipe; this is useful because Lucene
    has such a rich set of them. We are going to show how to wrap a Lucene `TokenStream`
    into a LingPipe `TokenizerFactory` by extending the `Tokenizer` abstract class.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些 Lucene 分词器与 LingPipe 一起使用；这很有用，因为 Lucene 有如此丰富的分词器集。我们将展示如何通过扩展 `Tokenizer`
    抽象类将 Lucene `TokenStream` 包装到 LingPipe `TokenizerFactory` 中。
- en: How to do it...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: How to do it...
- en: 'We will shake things up a bit and have a recipe that is not interactive. Perform
    the following steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做一些不同的尝试，并有一个非交互式的配方。执行以下步骤：
- en: 'Invoke the `LuceneAnalyzerTokenizerFactory` class from the command line:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用 `LuceneAnalyzerTokenizerFactory` 类：
- en: '[PRE19]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `main()` method in the class specifies the input:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类中的 `main()` 方法指定了输入：
- en: '[PRE20]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding snippet creates a Lucene `StandardAnalyzer` and uses it to construct
    a LingPipe `TokenizerFactory`. The output is as follows—the `StandardAnalyzer`
    filters stop words, so the token `are` is filtered:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码片段创建了一个 Lucene `StandardAnalyzer` 并使用它来构建一个 LingPipe `TokenizerFactory`。输出如下——`StandardAnalyzer`
    过滤掉停用词，所以标记 `are` 被过滤掉：
- en: '[PRE21]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The white spaces report as `default` because the implementation does not accurately
    provide white spaces but goes with a default. We will discuss this limitation
    in the *How it works…* section.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空白字符报告为 `default`，因为实现没有准确提供空白字符，而是使用默认值。我们将在 *How it works…* 部分讨论这个限制。
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: How it works...
- en: 'Let''s take a look at the `LuceneAnalyzerTokenizerFactory` class. This class
    implements the LingPipe `TokenizerFactory` interface by wrapping a Lucene analyzer.
    We will start with the class definition from `src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 `LuceneAnalyzerTokenizerFactory` 类。这个类通过包装 Lucene 分析器实现了 LingPipe `TokenizerFactory`
    接口。我们将从 `src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java`
    中的类定义开始：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The constructor stores the analyzer and the name of the field as private variables.
    As this class implements the `TokenizerFactory` interface, we need to implement
    the `tokenizer()` method:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数将分析器和字段的名称存储为私有变量。由于这个类实现了 `TokenizerFactory` 接口，我们需要实现 `tokenizer()` 方法：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `tokenizer()` method creates a new character-array reader and passes it
    to the Lucene analyzer to convert it to a `TokenStream`. An instance of `LuceneTokenStreamTokenizer`
    is created based on the token stream. `LuceneTokenStreamTokenizer` is a nested
    static class that extends LingPipe''s `Tokenizer` class:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenizer()` 方法创建一个新的字符数组读取器，并将其传递给 Lucene 分析器以将其转换为 `TokenStream`。基于标记流创建了一个
    `LuceneTokenStreamTokenizer` 实例。`LuceneTokenStreamTokenizer` 是一个嵌套的静态类，它扩展了 LingPipe
    的 `Tokenizer` 类：'
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The constructor stores `TokenStream` and attaches the term and the offset attributes.
    In the previous recipe, we saw that the term and the offset attributes contain
    the token string, and the token start and end offsets into the input text. The
    token offsets are also initialized to `-1` before any tokens are found:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数存储 `TokenStream` 并附加术语和偏移量属性。在先前的配方中，我们看到了术语和偏移量属性包含标记字符串，以及输入文本中的标记起始和结束偏移量。在找到任何标记之前，标记偏移量也被初始化为
    `-1`：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will implement the `nextToken()` method and use the `incrementToken()` method
    of the token stream to retrieve any tokens from the token stream. We will set
    the token start and end offsets using `OffsetAttribute`. If the token stream is
    finished or the `incrementToken()` method throws an I/O exception, we will end
    and close the `TokenStream`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现 `nextToken()` 方法，并使用标记流的 `incrementToken()` 方法从标记流中检索任何标记。我们将使用 `OffsetAttribute`
    设置标记的起始和结束偏移量。如果标记流已结束或 `incrementToken()` 方法抛出 I/O 异常，我们将结束并关闭 `TokenStream`。
- en: 'The `nextWhitespace()` method has some limitations, because `offsetAttribute`
    is focused on the current token where LingPipe tokenizers quantize the input into
    the next token and next offset. A general solution here will be quite challenging,
    because there might not be any well-defined white spaces between tokens—think
    character ngrams. So, the `default` string is supplied just to make it clear.
    The method is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`nextWhitespace()` 方法有一些限制，因为 `offsetAttribute` 专注于当前标记，LingPipe 分词器将输入量化为下一个标记和下一个偏移量。在这里找到一个通用的解决方案将非常具有挑战性，因为标记之间可能没有明确定义的空白——想想字符
    n-gram。因此，提供了 `default` 字符串以使其更清晰。该方法如下：'
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The code also covers how to serialize the tokenizer, but we will not cover this
    in the recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码还涵盖了如何序列化分词器，但我们将不在菜谱中涵盖这一点。
- en: Evaluating tokenizers with unit tests
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单元测试评估分词器
- en: 'We will not evaluate Indo-European tokenizers like the other components of
    LingPipe with measures such as precision and recall. Instead, we will develop
    them with unit tests, because our tokenizers are heuristically constructed and
    expected to perform perfectly on example data—if a tokenizer fails to tokenize
    a known case, then it is a bug, not a reduction in performance. Why is this? There
    are a few reasons:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会像 LingPipe 的其他组件那样，用精确度和召回率等指标来评估印欧语系分词器。相反，我们将通过单元测试来开发它们，因为我们的分词器是启发式构建的，并预期在示例数据上表现完美——如果一个分词器未能分词一个已知案例，那么它是一个错误，而不是性能的降低。为什么是这样？有几个原因：
- en: Many tokenizers are very "mechanistic" and are amenable to the rigidity of the
    unit test framework. For example, the `RegExTokenizerFactory` is obviously a candidate
    to unit test rather than an evaluation harness.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多分词器非常“机械”，并且易于单元测试框架的刚性。例如，`RegExTokenizerFactory` 显然是单元测试的候选，而不是评估工具。
- en: The heuristic rules that drive most tokenizers are very general, and there is
    no issue of over-fitting training data at the expense of a deployed system. If
    you have a known bad case, you can just go and fix the tokenizer and add a unit
    test.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推动大多数分词器的启发式规则非常通用，并且不存在以部署系统为代价过度拟合训练数据的问题。如果你有一个已知的坏案例，你只需去修复分词器并添加一个单元测试即可。
- en: 'Tokens and white spaces are assumed to be semantically neutral, which means
    that tokens don''t change depending on context. This is not totally true with
    our Indo-European tokenizer, because it treats `.` differently if it is part of
    a decimal or at the end of a sentence, for example, `3.14 is pi.`:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记和空白被认为是语义中性的，这意味着标记不会根据上下文而改变。但我们的印欧语系分词器并非完全如此，因为它会根据上下文不同对待 `.`，例如，如果是十进制的一部分或在句子末尾，例如
    `3.14 是 pi.`：
- en: '[PRE27]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: It might be appropriate to use an evaluation metric for statistics-based tokenizers;
    this is discussed in the *Finding words for languages without white spaces* recipe
    in this chapter. See the *Evaluation of sentence detection* recipe in [Chapter
    5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text – Chunking"),
    *Finding Spans in Text – Chunking*, for appropriate span-based evaluation techniques.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于统计的分词器，可能需要使用评估指标；这在本章的 *为没有空格的语言寻找单词* 菜谱中有讨论。参见 [第 5 章](part0061_split_000.html#page
    "第 5 章. 在文本中寻找跨度 – 分块") 的 *句子检测评估* 菜谱，*在文本中寻找跨度 – 分块*，以了解适当的基于跨度的评估技术。
- en: How to do it...
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will forgo running the code step and just get right into the source to put
    together a tokenizer evaluator. The source is in `src/com/lingpipe/chapter2/TestTokenizerFactory.java`.
    Perform the following steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过运行代码步骤，直接进入源代码来构建一个分词器评估器。源代码位于 `src/com/lingpipe/chapter2/TestTokenizerFactory.java`。执行以下步骤：
- en: 'The following code sets up a base tokenizer factory with a regular expression—look
    at the Javadoc for the class if you are not clear about what is being constructed:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码设置了一个基于正则表达式的基分词工厂——如果你不清楚正在构建的内容，请查看该类的 Javadoc：
- en: '[PRE28]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `checkTokens` method takes `TokenizerFactory`, an array of `String` that
    is the desired tokenization, and `String` that is to be tokenized. It follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkTokens` 方法接受 `TokenizerFactory`，一个表示所需分词的 `String` 数组，以及要分词的 `String`。它遵循以下步骤：'
- en: '[PRE29]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The method is quite intolerant of errors, because it exits the program if the
    token arrays are not of the same length or if any of the tokens are not equal.
    A proper unit test framework such as JUnit will be a better framework, but that
    is beyond the scope of the book. You can look at the LingPipe unit tests in `lingpipe.4.1.0`/`src/com/aliasi/test`
    for how JUnit is used.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法对错误非常敏感，因为如果令牌数组长度不同或任何令牌不相等，程序就会退出。JUnit这样的适当单元测试框架将是一个更好的框架，但这超出了本书的范围。您可以查看`lingpipe.4.1.0`/`src/com/aliasi/test`中的LingPipe单元测试，了解JUnit的使用方法。
- en: The `checkTokensAndWhiteSpaces()` method checks white spaces as well as tokens.
    It follows the same basic ideas of `checkTokens()`, so we leave it unexplained.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkTokensAndWhiteSpaces()`方法检查空白字符以及令牌。它遵循与`checkTokens()`相同的基本思想，所以我们不再解释。'
- en: Modifying tokenizer factories
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改分词器工厂
- en: In this recipe, we will describe a tokenizer that modifies the tokens in the
    token stream. We will extend the `ModifyTokenTokenizerFactory` class to return
    text that is rotated by 13 places in the English alphabet, also known as rot-13\.
    Rot-13 is a very simple substitution cipher, which replaces a letter with the
    letter that follows after 13 places. For example, the letter `a` will be replaced
    by the letter `n`, and the letter `z` will be replaced by the letter `m`. This
    is a reciprocal cypher, which means that applying the same cypher twice recovers
    the original text.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将描述一个修改令牌流中令牌的分词器。我们将扩展`ModifyTokenTokenizerFactory`类以返回在英文字母表中旋转了13个位置的文本，也称为rot-13。Rot-13是一个非常简单的替换密码，用字母表中13个位置后的字母替换字母。例如，字母`a`将被替换为字母`n`，字母`z`将被替换为字母`m`。这是一个互逆密码，这意味着应用相同的密码两次可以恢复原始文本。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will invoke the `Rot13TokenizerFactory` class from the command line:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从命令行调用`Rot13TokenizerFactory`类：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You can see that the input text, which was mixed case and in normal English,
    has been transformed into its Rot-13 equivalent. You can see that the second time
    around, we passed the Rot-13 modified text as input and got the original text
    back, except that it was all lowercase.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，输入文本，原本是混合大小写且为正常英语，已经被转换为其Rot-13等价形式。您可以看到，第二次，我们将经过Rot-13修改后的文本作为输入，并得到了原始文本，除了它全部都是小写字母。
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`Rot13TokenizerFactory` extends the `ModifyTokenTokenizerFactory` class. We
    will override the `modifyToken()` method, which operates a token at a time and,
    in this case, converts the token to its Rot-13 equivalent. There is a similar
    `modifyWhiteSpace` (String) method, which modifies the white spaces if required:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Rot13TokenizerFactory`扩展了`ModifyTokenTokenizerFactory`类。我们将重写`modifyToken()`方法，该方法一次操作一个令牌，在这种情况下，将令牌转换为它的Rot-13等价形式。还有一个类似的`modifyWhiteSpace`（String）方法，如果需要，会修改空白字符：'
- en: '[PRE31]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The start and end offsets of the tokens themselves remain the same as that of
    the underlying tokenizer. Here, we will use an Indo-European tokenizer as our
    base tokenizer. Filter it once through `LowerCaseTokenizer` and then through `Rot13Tokenizer`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌本身的起始和结束偏移量与底层分词器的相同。在这里，我们将使用一个印欧语系分词器作为我们的基础分词器。首先通过`LowerCaseTokenizer`过滤一次，然后通过`Rot13Tokenizer`过滤。
- en: 'The `rot13` method is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`rot13`方法如下：'
- en: '[PRE32]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finding words for languages without white spaces
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为没有空格的语言寻找单词
- en: Languages such as Chinese do not have word boundaries. For example, 木卫三是围绕木星运转的一颗卫星，公转周期约为7天
    from Wikipedia is a sentence in Chinese that translates roughly into "Ganymede
    is running around Jupiter's moons, orbital period of about seven days" as done
    by the machine translation service at [https://translate.google.com](https://translate.google.com).
    Notice the absence of white spaces.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，像中文这样的语言没有单词边界。例如，木卫三是围绕木星运转的一颗卫星，公转周期约为7天，来自维基百科的这句话在中文中大致翻译为“甘尼德在木星的卫星周围运行，轨道周期约为七天”，这是由[https://translate.google.com](https://translate.google.com)上的机器翻译服务完成的。注意空格的缺失。
- en: Finding tokens in this sort of data requires a very different approach that
    is based on character-language models and our spell-checking class. This recipe
    encodes finding words by treating untokenized text as *misspelled* text, where
    the *correction* inserts a space to delimit tokens. Of course, there is nothing
    misspelled about Chinese, Japanese, Vietnamese, and other non-word delimiting
    orthographies, but we have encoded it in our spelling-correction class.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这类数据中查找标记需要一种非常不同的方法，该方法基于字符语言模型和我们的拼写检查类。这个配方通过将未标记的文本视为*拼写错误*的文本来编码查找单词，其中*更正*插入空格以分隔标记。当然，中文、日语、越南语和其他非单词分隔的语系并没有拼写错误，但我们已经在拼写纠正类中对其进行了编码。
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will approximate non-word delimiting orthographies with de-white spaced English.
    This is sufficient to understand the recipe and can be easily modified to the
    actual language when needed. Get a 100,000 or so words of English and get them
    to the disk in UTF-8 encoding. The reason for fixing the encoding is that the
    input is assumed to be UTF-8—you can change it by changing the encoding and recompiling
    the recipe.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用去空格的英语来近似非单词分隔的语系。这足以理解配方，并且可以很容易地修改为实际所需的语言。获取大约10万个英语单词，并将它们以UTF-8编码的方式存储到磁盘上。固定编码的原因是输入假定是UTF-8——你可以通过更改编码并重新编译配方来更改它。
- en: We used *A Connecticut Yankee in King Arthur's Court* by Mark Twain, downloaded
    from Project Gutenberg ([http://www.gutenberg.org/](http://www.gutenberg.org/)).
    Project Gutenberg is an excellent source of texts that are in the public domain,
    and Mark Twain is fine writer—we highly recommend the book. Place your selected
    text in the cookbook directory or work with our default.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了马克·吐温的《亚瑟王宫廷中的康涅狄格州扬基》，从Project Gutenberg下载（[http://www.gutenberg.org/](http://www.gutenberg.org/)）。Project
    Gutenberg是公共领域文本的绝佳来源，马克·吐温是一位优秀的作家——我们强烈推荐这本书。将你的选定文本放在食谱目录中或使用我们的默认设置。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will run a program, play with it a bit, and explain what it does and how
    it does it, using the following steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行一个程序，稍作玩耍，并使用以下步骤解释它做什么以及它是如何做到的：
- en: 'Type the following command:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令：
- en: '[PRE33]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The following is the output:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下面的输出是：
- en: '[PRE34]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You might not get the perfect output. How good is Mark Twain at recovering
    proper white space from the Java program that generated it? Let''s find out:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能不会得到完美的输出。马克·吐温从生成它的Java程序中恢复正确空白的能力如何？让我们来看看：
- en: '[PRE35]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding way was not very good, but we are not being very fair; let''s
    use the concatenated source of LingPipe as training data:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的方法并不很好，但我们也不是非常公平；让我们使用LingPipe的连接源作为训练数据：
- en: '[PRE36]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This is the perfect space insertion.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就是完美的空格插入。
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'For all the fun and games, there is very little code involved. The cool thing
    is that we are building on the character-language models from [Chapter 1](part0014_split_000.html#page
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. The source is in `src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有很多乐趣和游戏，但涉及的代码非常少。酷的地方在于我们正在基于[第1章](part0014_split_000.html#page "Chapter 1. Simple
    Classifiers")中的字符语言模型，*简单分类器*。源代码位于`src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java`：
- en: '[PRE37]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `main()` method starts up with the creation of `NgramProcessLM`. Next up,
    we will access a class for edit distance that is designed to only add spaces to
    a character stream. That's it. `Editdistance` is typically a fairly crude measure
    of string similarity that scores how many edits need to happen to to `string1`
    to make it the same as `string2`. A lot of information on this is Javadoc `com.aliasi.spell`.
    For example, `com.aliasi.spell.EditDistance` has an excellent discussion of the
    basics.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()`方法通过创建`NgramProcessLM`来启动。接下来，我们将访问一个用于编辑距离的类，该类旨在仅向字符流中添加空格。就是这样。`Editdistance`通常是一个相当粗略的字符串相似度度量，它评估需要对`string1`进行多少编辑才能使其与`string2`相同。关于这方面的许多信息可以在Javadoc
    `com.aliasi.spell`中找到。例如，`com.aliasi.spell.EditDistance`对基础知识进行了很好的讨论。'
- en: Note
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `EditDistance` class implements the standard notion of edit distance, with
    or without transposition. The distance without transposition is known as the Levenshtein
    distance, and with transposition, it is known as the Damerau-Levenstein distance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`EditDistance`类实现了带有或不带有转置的标准编辑距离概念。不带转置的距离被称为Levenshtein距离，带有转置则称为Damerau-Levenstein距离。'
- en: Read the Javadoc with LingPipe; it has a lot of useful of information that we
    don't have space for in this book.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LingPipe阅读Javadoc；它包含许多有用的信息，但这些信息在这本书中我们没有足够的空间来介绍。
- en: 'So far we configured and constructed a `TrainSpellChecker` class. The next
    step is to naturally train it:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经配置和构建了一个 `TrainSpellChecker` 类。下一步是自然地对其进行训练：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We slurped up a text file, assuming it is UTF-8; if not, correct the character
    encoding and recompile. Then, we replaced all the multiple white spaces with a
    single one. This might not be the best move if multiple white spaces have meaning.
    This is followed by training, just like we trained language models in [Chapter
    1](part0014_split_000.html#page "Chapter 1. Simple Classifiers"), *Simple Classifiers*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设文本文件是 UTF-8 编码，将其全部读取；如果不是，则更正字符编码并重新编译。然后，我们将所有多个空白字符替换为单个空白字符。如果多个空白字符具有意义，这可能不是最好的选择。接下来是训练，就像我们在
    [第 1 章](part0014_split_000.html#page "第 1 章。简单分类器") 中训练语言模型一样，*简单分类器*。
- en: 'Next up, we will compile and configure the spell checker:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译和配置拼写检查器：
- en: '[PRE39]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The next interesting line compiles `spellChecker`, which translates all the
    counts in the underlying language model to precomputed probabilities, which is
    much faster. The compilation step can write to a disk, so it can be used later
    without training; however, visit the Javadoc for `AbstractExternalizable` on how
    to do this. The next lines configure `CompiledSpellChecker` to only consider the
    edits that insert characters and to check for the exact string matches, but it
    forbids deletions, substitutions, and transpositions. Finally, only one insert
    is allowed. It should be clear that we are using a very limited portion of the
    capabilities of `CompiledSpellChecker`, but this is exactly what is called for—insert
    a space or don't.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个有趣的步骤是编译 `spellChecker`，它将底层语言模型中的所有计数转换为预计算的概率，这要快得多。编译步骤可以写入磁盘，因此可以在不进行训练的情况下稍后使用；然而，请参阅
    `AbstractExternalizable` 的 Javadoc 了解如何进行此操作。接下来的几行配置 `CompiledSpellChecker` 以仅考虑插入字符的编辑，并检查精确字符串匹配，但禁止删除、替换和转置。最后，只允许一个插入。应该很清楚，我们正在使用
    `CompiledSpellChecker` 的非常有限的功能，但这正是所需要的——插入空格或不插入。
- en: 'Last up is our standard I/O routine:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是我们的标准 I/O 例程：
- en: '[PRE40]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The mechanics of the `CompiledSpellChecker` and `WeightedEditDistance` classes
    are better described in either the Javadoc or the *Using edit distance and language
    models for spelling correction* recipe in [Chapter 6](part0075_split_000.html#page
    "Chapter 6. String Comparison and Clustering"), *String Comparison and Clustering*.
    However, the basic idea is that the string entered is compared to the language
    model just trained, resulting in a score that shows how good a fit this string
    is to the model. This string is going to be one huge word without any white spaces—but
    note that there is no tokenizer at work here, so the spell checker starts inserting
    spaces and reassessing the score of the resulting sequence. It keeps these sequences
    where insertion of spaces increases the score of the sequence.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 和 `WeightedEditDistance` 类的机制在 Javadoc 或 [第 6 章](part0075_split_000.html#page
    "第 6 章。字符串比较和聚类") 的 *使用编辑距离和语言模型进行拼写校正* 菜单中描述得更好，*字符串比较和聚类*。然而，基本思想是将输入的字符串与刚刚训练的语言模型进行比较，得到一个分数，显示该字符串与模型匹配得有多好。这个字符串将是一个没有空白字符的巨大单词——但请注意，这里没有分词器在工作，因此拼写检查器开始插入空格并重新评估结果的分数。它保留这些序列，其中插入空格会增加序列的分数。'
- en: Remember that the language model was trained on text with white spaces. The
    spell checker tries to insert a space everywhere it can and keeps a set of "best
    so far" insertions of white spaces. In the end, it returns the best scoring series
    of edits.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，语言模型是在带有空白字符的文本上训练的。拼写检查器试图在可能的地方插入空格，并保留一组“迄今为止最佳”的空白字符插入。最后，它返回最佳得分的编辑序列。
- en: Note that to complete the tokenizer, the appropriate `TokenizerFactory` needs
    to be applied to the white space-modified text, but this is left as an exercise
    for the reader.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了完成分词器，需要将适当的 `TokenizerFactory` 应用到经过空白字符修改的文本上，但这被留作读者的练习。
- en: There's more...
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`CompiledSpellChecker` allows for an *n*-best output as well; this allows for
    multiple possible analyses of the text. In a high-coverage/recall situation such
    as a research search engine, it might serve to allow the application of multiple
    tokenizations. Also, the edit costs can be manipulated by extending the `WeightedEditDistance`
    class directly to tune the system.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 允许输出 *n*-best 结果；这允许对文本进行多种可能的解析。在像研究搜索引擎这样的高覆盖率/召回率情况下，可能需要允许应用多种分词。此外，可以通过直接扩展
    `WeightedEditDistance` 类来调整编辑成本。'
- en: See also
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: It will be unhelpful to not actually provide non-English resources for this
    recipe. We built and evaluated a Chinese tokenizer using resources available on
    the web for research use. Our tutorial on Chinese word segmentation covers this
    in detail. You can find the Chinese word segmentation tutorial at [http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不提供非英语资源，那么这份食谱将不会有所帮助。我们构建并评估了一个中文分词器，使用了网络上可用的资源进行科研。我们的关于中文分词教程详细介绍了这一点。您可以在[http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html)找到中文分词教程。
