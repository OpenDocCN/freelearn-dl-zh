- en: '*Chapter 4*: Training Models with Text Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：使用文本数据训练模型'
- en: In [*Chapter 3*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083), *Training
    Models with Tabular Data*, you went through a series of recipes that demonstrated
    how to use the facilities of fastai to train deep learning models on tabular data.
    In this chapter, we will examine how to take advantage of the fastai framework
    to train deep learning models on text datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083)，*使用表格数据训练模型*中，你通过一系列实例展示了如何利用fastai的功能在表格数据上训练深度学习模型。在本章中，我们将研究如何利用fastai框架在文本数据集上训练深度学习模型。
- en: To explore deep learning with text data in fastai, we will start by taking a
    pre-trained **language model** (that is, a model that, when given a phrase, predicts
    what words come next) and fine-tuning it with the IMDb curated dataset. We will
    then use the resulting fine-tuned language model to create a **text classifier
    model** for the movie review use case represented by the IMDb dataset. The text
    classifier predicts the class of a phrase; in the movie review use case, it predicts
    whether a given phrase is **positive** or **negative**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在fastai中探索文本数据的深度学习，我们将从一个预训练的**语言模型**开始（即给定一个短语，预测接下来会出现哪些词），并使用IMDb精心挑选的数据集对其进行微调。接着，我们将使用微调后的语言模型创建一个**文本分类器模型**，用于电影评论用例，这个用例由IMDb数据集表示。该文本分类器用于预测一个短语的类别；在电影评论用例中，它预测给定短语是**积极的**还是**消极的**。
- en: 'Finally, we apply the same approach to a standalone (that is, non-curated)
    text dataset of Covid-related tweets. First, we will fine-tune the existing language
    model on the Covid tweets dataset. Then, we will use the fine-tuned language model
    to train a text classifier that predicts the class of a phrase according to the
    categories defined in the Covid tweets dataset: **extremely negative**, **negative**,
    **neutral**, **positive**, and **extremely positive**.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将相同的方法应用于一个独立的（即非精心挑选的）关于Covid相关推文的文本数据集。首先，我们将在Covid推文数据集上对现有的语言模型进行微调。然后，我们将使用微调后的语言模型训练一个文本分类器，该分类器根据Covid推文数据集定义的类别来预测短语的类别：**极其消极**、**消极**、**中立**、**积极**和**极其积极**。
- en: The approach to training deep learning models on text datasets that is used
    in this chapter, also known as **ULMFiT**, was initially described by the creators
    of fastai in their paper entitled *Universal Language Model Fine-Tuning for Text
    Classification* [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146).
    This approach introduced the concept of **transfer learning** to **natural language
    processing** (**NLP**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中使用的在文本数据集上训练深度学习模型的方法，也叫**ULMFiT**，最初由fastai的创始人在他们的论文《*通用语言模型微调用于文本分类*》[https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)中描述。该方法将**迁移学习**的概念引入了**自然语言处理**（**NLP**）领域。
- en: Transfer learning is taking a model that has been trained on a large, general-purpose
    dataset and making it applicable to a specific use case by fine-tuning the large
    model with a smaller dataset that is specific to the use case.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是指将一个在大型通用数据集上训练的模型，通过在特定用例的小型数据集上微调，使其适用于特定的用例。
- en: 'The ULMFiT approach to transfer learning for NLP can be summarized as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ULMFiT的迁移学习方法可以总结如下：
- en: Start with a large language model trained on a large text dataset.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个在大型文本数据集上训练的大型语言模型开始。
- en: Fine-tune this language model on a text dataset that is related to a specific
    use case.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在与特定用例相关的文本数据集上微调该语言模型。
- en: Use the fine-tuned language model to create a text classifier for the specific
    use case.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用微调后的语言模型为特定用例创建文本分类器。
- en: 'In this chapter, the large model is referred to as `AWD_LSTM`, and it has been
    trained on a big corpus taken from Wikipedia articles. We will fine-tune this
    large language model on datasets for two specific use cases: IMDb for movie reviews,
    and Covid tweets for social media posts regarding the Covid 19 pandemic. We then
    use each of the resulting fine-tuned language models to train text classifiers
    for each use case.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，大型模型被称为`AWD_LSTM`，它是在一个来自维基百科文章的大型语料库上训练的。我们将根据两个特定用例的数据集对该大型语言模型进行微调：IMDb电影评论数据集和Covid推文数据集，用于社交媒体关于Covid-19疫情的帖子。然后，我们将使用每个微调后的语言模型来训练每个用例的文本分类器。
- en: 'Here are the recipes that will be covered in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章中将要介绍的实例：
- en: Training a deep learning language model with a curated IMDb text dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用精心挑选的IMDb文本数据集训练深度学习语言模型
- en: Training a deep learning classification model with a curated text dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用精心挑选的文本数据集训练深度学习分类模型
- en: Training a deep learning language model with a standalone text dataset
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用独立文本数据集训练深度学习语言模型
- en: Training a deep learning text classifier with a standalone text dataset
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用独立文本数据集训练深度学习文本分类器
- en: Test your knowledge
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试你的知识
- en: Technical requirements
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Ensure that you have completed the setup sections from [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, and have a working `ch4` folder. This folder contains
    the code samples described in this chapter.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已经完成了[**第1章**](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019)的设置部分，*快速入门fastai*，并且有一个有效的`ch4`文件夹。该文件夹包含本章中描述的代码示例。
- en: Some of the examples in this chapter will take over an hour to run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些示例可能需要一个多小时才能运行。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Do not use Colab for these examples. With Colab you cannot control the GPU that
    you get for a session and these examples may run for many hours. For the examples
    in this chapter, use a for-pay GPU-enabled Gradient environment to ensure they
    complete in a reasonable time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要使用Colab来运行这些示例。在Colab中，你无法控制会话使用的GPU，而这些示例可能需要很长时间才能运行。对于本章中的示例，请使用付费的GPU支持的Gradient环境，以确保它们能在合理时间内完成。
- en: Training a deep learning language model with a curated IMDb text dataset
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用精心挑选的IMDb文本数据集训练深度学习语言模型
- en: In this section, you will go through the process of training a language model
    on a curated text dataset using fastai. We take a pre-existing language model
    that is packaged with fastai and fine-tune it with one of the curated text datasets,
    IMDb, that contains text samples for the movie review use case. The result will
    be a language model with the broad language capability of the pre-existing language
    model, along with the use case-specific details of the IMDb dataset. This recipe
    illustrates one of the breakthroughs made by the team that created fastai, that
    is, transfer learning applied to NLP.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解如何使用fastai在精心挑选的文本数据集上训练语言模型。我们将使用一个与fastai一起打包的现有语言模型，并通过精心挑选的文本数据集IMDb对其进行微调，该数据集包含电影评论用例的文本样本。最终结果将是一个具有现有语言模型的广泛语言能力，同时具备IMDb数据集的特定用例细节。本食谱展示了fastai团队所取得的突破之一，即将迁移学习应用于自然语言处理（NLP）。
- en: Getting ready
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'For the recipes so far in this book, we have recommended using the Gradient
    environment. You can use Gradient for this recipe and the instructions below include
    several workarounds to make the recipe work on Gradient. In particular, the pre-trained
    `AWD_LSTM` model will not be available in Gradient if its initial setup gets interrupted
    and the directory for the `IMDB` model is not writeable. If the setup of the pre-trained
    `AWD_LSTM` model gets interrupted, follow these steps:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们推荐使用Gradient环境。你可以在此食谱中使用Gradient，下面的说明包括几个解决方法，帮助你在Gradient中运行该食谱。特别地，如果预训练的`AWD_LSTM`模型的初始设置被中断，且`IMDB`模型目录不可写，Gradient中将无法使用该模型。如果预训练的`AWD_LSTM`模型设置被中断，请按照以下步骤操作：
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Colab中，运行`text_model_training.ipynb`笔记本中的单元格，直到并包括学习者定义和训练单元格。完成后，将`/root/.fastai/models/wt103-fwd`目录的内容复制到你的Drive环境中的一个文件夹里。
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你在上一步复制的文件上传到Gradient环境中的`/storage/models/wt103-fwd`目录。
- en: With these steps, you should now be able to run the notebook for this recipe
    (and other recipes that make use of `AWD_LSTM`) in Gradient.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你现在应该能够在Gradient中运行本食谱的笔记本（以及其他使用`AWD_LSTM`的食谱）。
- en: I am grateful for the opportunity to include the IMDB dataset featured in this
    chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我感谢能有机会在本章中使用IMDB数据集。
- en: Dataset citation
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher
    Potts. (2011) *Learning Word Vectors for Sentiment Analysis* ([https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf))
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher
    Potts. (2011) *学习情感分析的词向量* ([https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf))
- en: How to do it…
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作步骤…
- en: 'In this section, you will be running through the `text_model_training.ipynb`
    notebook to train a language model using the IMDb curated dataset. Once you have
    the notebook open in Colab, complete the following steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将通过运行`text_model_training.ipynb`笔记本来使用IMDb精心策划的数据集训练语言模型。一旦您在Colab中打开笔记本，请完成以下步骤：
- en: Run the cells in the notebook up to the `Training a language model` cell.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中直到`Training a language model`单元格的单元格。
- en: 'Run the following cell to define a `path` object associated with the IMDb curated
    dataset:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以定义与IMDb精心策划的数据集相关联的`path`对象：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can see the directory structure for this dataset in the output of the `tree
    -d` command run in the `imdb` directory (`/storage/data/imdb` in Gradient). Note
    that the labels for the dataset (whether a review is positive or negative) are
    encoded by the directory in which the text sample is located. For example, the
    negative review text samples in the training dataset are contained in the `train/neg`
    directory.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以在`imdb`目录中运行`tree -d`命令的输出中看到此数据集的目录结构（在Gradient中为`/storage/data/imdb`）。请注意数据集的标签（评论是积极的还是消极的）是由文本样本所在的目录编码的。例如，训练数据集中负面评论文本样本包含在`train/neg`目录中。
- en: 'The following is the output of the `tree -d` command run in the `imdb` directory:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是在`imdb`目录中运行`tree -d`命令的输出：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以定义`TextDataLoaders`对象：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`TextDataLoaders`对象定义的参数：
- en: 'a) `path`: The `path` object (associated with the IMDb curated dataset) that
    you defined earlier in the notebook.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) `path`: 您在笔记本中先前定义的与IMDb精心策划的数据集相关联的`path`对象。'
- en: 'b) `valid`: Identifies the folder in the dataset''s directory structure that
    will be used to assess the performance of the model: `imdb/test`.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) `valid`: 标识将用于评估模型性能的数据集目录结构中的文件夹：`imdb/test`。'
- en: 'c) `is_lm`: Set to `True` to indicate that this object will be used for a language
    model (as opposed to a text classifier).'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'c) `is_lm`: 设置为`True`以指示此对象将用于语言模型（而不是文本分类器）。'
- en: 'd) `bs`: Specifies the batch size.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'd) `bs`: 指定批量大小。'
- en: Note
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: When you are training a language model with a large dataset such as IMDb, adjusting
    the `bs` value to be lower than the default batch size of `64` will be essential
    for avoiding memory errors, and that is why it is set to `16` in this `TextDataLoaders`
    definition.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当您训练大型数据集（如IMDb）的语言模型时，将`bs`值调整为低于默认批量大小`64`将是避免内存错误的关键，因此在此`TextDataLoaders`定义中将其设置为`16`。
- en: 'Run the following cell to show a couple of items from a sample batch:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以显示样本批次中的几个项目：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `max_n` argument specifies the number of sample batch items to show.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`max_n`参数指定要显示的样本批次项数。'
- en: 'Note the output of this cell. The `text` column shows the original text. The
    `text_` column shows the same text shifted one token ahead, that is, it starts
    one word after the original text and ends one word past the original text. Given
    a sample such as one of the entries in the `text` column, the language model will
    predict the next word, as shown in the `text_` column. We can see the output of
    `show_batch()` in the following screenshot:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意此单元格的输出。`text`列显示原始文本。`text_`列显示相同文本向前移动一个标记，即从原始文本的下一个词开始，直到原始文本的下一个词。例如，给定`text`列中的一个样本，语言模型将预测下一个单词，如`text_`列中所示。我们可以在以下截图中看到`show_batch()`的输出：
- en: '![Figure 4.1 – Output of show_batch()'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.1 – show_batch()的输出'
- en: '](img/B16216_4_1.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_1.jpg)'
- en: Figure 4.1 – Output of show_batch()
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.1 – show_batch()的输出
- en: 'Run the following cell to define and train the deep learning model:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以定义和训练深度学习模型：
- en: '[PRE4]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the arguments for the definition of the `language_model_learner` object:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`language_model_learner`对象定义的参数：
- en: 'a) `dls`: The `TextDataLoaders` object that is defined previously in this notebook.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) `dls`: 在此笔记本中先前定义的`TextDataLoaders`对象。'
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia. If you are running this notebook on Colab, you can find the files that
    make up this model in the `/root/.fastai/models/wt103-fwd` directory after you
    have run this cell.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) `AWD_LSTM`: 用作此模型基础的预训练模型。这是在Wikipedia上使用fastai训练的预训练语言模型。如果您在Colab上运行此笔记本，在运行此单元格后，您可以在`/root/.fastai/models/wt103-fwd`目录中找到构成此模型的文件。'
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `metrics`：模型需要优化的性能指标，在此案例中为准确度。
- en: 'Here are the arguments for the `fine_tune` statement:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`fine_tune`语句的参数：
- en: a) The epoch number (first argument) specifies the number of epochs, that is,
    the number of times the algorithm goes through the full training data during the
    training process.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 纪元数（第一个参数）指定了训练过程中算法遍历完整训练数据的次数。
- en: b) The learning rate (second argument) specifies the learning rate for the training
    process. The learning rate is the rate at which the algorithm moves toward learning
    optimal parameters.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 学习率（第二个参数）指定了训练过程中的学习率。学习率是算法在学习最优参数时的步长。
- en: Note
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: a) Depending on your environment, it may take more than an hour for this cell
    to run to completion. I strongly recommend that you use a for-pay GPU-enabled
    Gradient environment for this example and specify at least 3 hours for the instance
    to ensure that it completes in a reasonable time and that the instance doesn't
    shut down while this cell is running.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 根据你的环境，运行该单元格可能需要超过一小时才能完成。强烈建议你使用收费的GPU支持的Gradient环境，并为实例指定至少3小时，以确保其在合理的时间内完成并且实例在该单元格运行时不会关闭。
- en: 'b) The `language_model_learner` definition includes a call to `to_fp16()` to
    specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process and to prevent memory
    errors. Refer to the *There''s more…* section for more details.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `language_model_learner`定义中包括了调用`to_fp16()`来指定混合精度训练（简要说明：[https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16)），以减少训练过程中的内存消耗并防止内存错误。更多细节请参考*更多内容…*部分。
- en: 'The output of the `fine_tune` statement shows the accuracy of the model and
    the time taken to complete the fine-tuning, as shown in the following screenshot:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`fine_tune`语句的输出显示了模型的准确率以及完成微调所需的时间，如下方截图所示：'
- en: '![Figure 4.2 – Output of the fine_tune statement'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.2 – `fine_tune`语句的输出'
- en: '](img/B16216_4_2.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_2.jpg)'
- en: Figure 4.2 – Output of the fine_tune statement
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.2 – `fine_tune`语句的输出
- en: 'Run the following cell to exercise the language model you just trained:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以执行你刚刚训练的语言模型：
- en: '[PRE5]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here are the arguments for this invocation of the language model:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是该语言模型调用的参数：
- en: a) The input text sample `"what comes next"` (first argument) is the phrase
    that the model will complete. The language model will predict what words should
    follow this phrase.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 输入文本样本 `"what comes next"`（第一个参数）是模型将完成的短语。语言模型将预测接下来应该跟随哪些词语。
- en: 'b) `n_words`: This is the number of words that the language model is supposed
    to predict to complete the input phrase.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `n_words`：这是语言模型应预测的单词数，用来完成输入的短语。
- en: 'The following screenshot shows an example of what the model''s prediction could
    look like:'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下截图展示了模型预测结果可能的样子：
- en: '![Figure 4.3 – The language model completes a phrase'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.3 – 语言模型完成一个短语'
- en: '](img/B16216_4_3.jpg)'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_3.jpg)'
- en: Figure 4.3 – The language model completes a phrase
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.3 – 语言模型完成一个短语
- en: 'Run the following cell to save the model. You can update the cell to specify
    the directory and filename to which to save the model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存模型。你可以更新单元格来指定保存模型的目录和文件名：
- en: '[PRE6]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Run the following cell to save the current path value:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存当前路径值：
- en: '[PRE7]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is that the default location for the model is not writeable
    on Gradient so you need to change the path value to a directory where you have
    write access:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以为学习器对象路径指定一个新值。这样做的原因是，默认情况下，模型的位置在Gradient上不可写，因此你需要更改路径值为你具有写入权限的目录：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Run the following cell to save the encoder subset of the model. This is the
    model minus the final layer. You will use this in the *Training a deep learning
    classification model with a curated text dataset* section when you train a text
    classifier:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存模型的编码器子集。这个模型去除了最后一层。在你训练文本分类器时，你将在*使用精心策划的文本数据集训练深度学习分类模型*部分使用这个模型：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Congratulations! You have successfully applied transfer learning to train a
    language model on the curated IMDb dataset. Note that the idea of applying transfer
    learning to NLP like this was only described for the first time in 2018\. Now,
    thanks to the fastai framework, with just a few lines of code, you can take advantage
    of a technique that didn't exist scarcely 4 years ago!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功地将迁移学习应用于在精心挑选的IMDb数据集上训练语言模型。需要注意的是，像这样将迁移学习应用于NLP的想法，直到2018年才首次被提出。现在，感谢fastai框架，只需几行代码，你就能利用一种在四年前几乎不存在的技术！
- en: How it works…
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this section, you have seen a simple example of how to train a language model
    with fastai deep learning using a curated text dataset. The language model is
    created by taking a model (`AWD_LSTM`) that has been pre-trained with the massive
    wiki dataset and then fine-tuning it using the IMDb dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你已经看到了如何使用经过精心挑选的文本数据集训练语言模型的简单示例。语言模型通过使用在庞大的维基数据集上预训练的`AWD_LSTM`模型，并使用IMDb数据集进行微调来创建。
- en: By taking advantage of transfer learning in this way, we end up with a language
    model that combines a good degree of capability on general-purpose English (thanks
    to the model pre-trained on the wiki dataset) as well as the capability to produce
    text that is specific to the use case of movie reviews (thanks to the IMDb dataset).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式利用迁移学习，我们最终得到了一个语言模型，它不仅具备了在通用英语方面的较强能力（得益于在维基数据集上预训练的模型），还能生成与电影评论用例相关的文本（得益于IMDb数据集）。
- en: It's worthwhile to look a bit closer at the model in this recipe. A deeply detailed
    description of the model is beyond the scope of this book, so we will just focus
    on some highlights here.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 值得稍微深入了解一下本食谱中的模型。关于该模型的详细描述超出了本书的范围，因此我们将重点关注其中的一些要点。
- en: 'As shown in the recipe, the model is defined as a `language_model_learner`
    object (documentation here: [https://docs.fast.ai/text.learner.html#language_model_learner](https://docs.fast.ai/text.learner.html#language_model_learner)).
    This object is a specialization of the fastai `learner` object which you first
    saw in the *Understanding the world in four applications: tables, text, recommender
    systems* section, and in the images of [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019)*,
    Getting Started with fastai*.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如食谱所示，模型被定义为`language_model_learner`对象（文档链接：[https://docs.fast.ai/text.learner.html#language_model_learner](https://docs.fast.ai/text.learner.html#language_model_learner)）。这个对象是fastai
    `learner`对象的一个特化版本，你第一次看到这个对象是在*理解世界的四种应用：表格、文本、推荐系统*部分，以及[*第1章*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019)*，快速上手fastai*中的图片中。
- en: 'The model in the recipe is based on the predefined `AWD_LSTM` model (documentation
    here: [https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM](https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM)).
    For this model, the output of `learn.summary()` shows only the high-level structure,
    including LSTM layers (fundamental to traditional NLP deep learning models) and
    dropout layers (used to reduce overfitting). Similarly, the output of `learn.model`
    for this model starts with layers for encoding (that is, transforming the input
    data to an intermediate representation used within the model) and ends with layers
    for decoding (that is, transforming the internal representation back to words).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中的模型基于预定义的`AWD_LSTM`模型（文档链接：[https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM](https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM)）。对于这个模型，`learn.summary()`的输出仅显示高级结构，包括LSTM层（传统NLP深度学习模型的基础）和dropout层（用于减少过拟合）。类似地，该模型的`learn.model`输出从编码层开始（即将输入数据转换为模型内部使用的中间表示），以解码层结束（即将内部表示转换回单词）。
- en: There's more…
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: In this chapter, you will be working with some very large datasets, which means
    that you may need to take some extra steps to ensure that you don't run out of
    memory while you are preparing the datasets and training the model. Here, we'll
    describe some steps you can take to ensure that you train your fastai deep learning
    models on text datasets without running out of memory. We'll also go into more
    detail about how to save encoders in Gradient.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用一些非常大的数据集，这意味着你可能需要采取额外的步骤，以确保在准备数据集和训练模型时不会耗尽内存。在这里，我们将描述一些步骤，帮助你确保在处理文本数据集时，不会因为内存不足而无法训练你的fastai深度学习模型。我们还会详细讨论如何在Gradient中保存编码器。
- en: What happens if you run out of memory?
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如果内存不足会发生什么？
- en: 'If you are trying to train a large model, you may get an out of memory message
    such as the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在尝试训练一个大型模型，可能会遇到类似以下的内存不足提示：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This message is saying that you have run out of memory on the GPU for your
    environment. What can you do if you encounter such a memory error? There are three
    steps you can take to get around this kind of memory error:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 该消息表示你的环境中的GPU内存已满。如果遇到这种内存错误，你可以采取哪些措施呢？有三种方法可以帮助你解决此类内存错误：
- en: Explicitly set the batch size.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显式设置批量大小。
- en: Use mixed-precision training.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合精度训练。
- en: Ensure that you have only one notebook active at a time.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保一次只激活一个笔记本。
- en: 'Memory error mitigation #1: Explicitly set the batch size'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '内存错误缓解方法 #1：显式设置批量大小'
- en: 'To explicitly set the batch size, you can restart the kernel for your notebook
    and then update the definition of the `TextDataLoaders` object to set the `bs`
    parameter, as shown here:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要显式设置批量大小，你可以重新启动笔记本的内核，然后更新`TextDataLoaders`对象的定义来设置`bs`参数，如下所示：
- en: '[PRE11]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Setting the `bs` parameter explicitly specifies a batch size (the number of
    items on which the average loss is calculated) other than the default of 64\.
    By explicitly setting the batch size to a smaller value than the default, you
    limit the amount of memory consumed by each training epoch (that is, a complete
    iteration through the training data). When you set the value of `bs` explicitly
    like this, ensure that the value you set for the `bs` parameter is a multiple
    of 8.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`bs`参数可以显式指定批量大小（即用于计算平均损失的数据项数量），而不是默认值64。通过显式将批量大小设置为比默认值小的值，你限制了每个训练周期（即完全遍历训练数据）消耗的内存量。当你像这样显式设置`bs`参数的值时，确保所设置的`bs`参数值是8的倍数。
- en: 'Memory error mitigation #2: Mixed-precision training'
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '内存错误缓解方法 #2：混合精度训练'
- en: 'Another technique that you can use to control memory consumption is using `to_fp16()`
    function to the definition of the learner object, as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以用来控制内存消耗的技巧是将`to_fp16()`函数应用于学习者对象的定义，如下所示：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By specifying this call to `to_fp16()`, you allow the model to be trained using
    floating-point numbers that are less precise and are therefore expressed with
    less memory. The result is that the model training process consumes less memory.
    Refer to the fastai documentation for more details: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定此`to_fp16()`调用，你可以让模型使用精度较低的浮点数进行训练，从而节省内存。结果是，模型训练过程消耗的内存更少。有关更多详细信息，请参考
    fastai 文档：[https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16)。
- en: 'Memory error mitigation #3: Stick to a single active notebook'
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '内存错误缓解方法 #3：坚持使用单个活跃的笔记本'
- en: Finally, another approach that you can take to prevent running out of memory
    is to run a single notebook at a time. On Gradient, for example, if you have multiple
    notebooks active at the same time, then you can exhaust your available memory.
    If you take the steps of setting a smaller batch size in the `TextDataLoaders`
    object and specifying `to_fp16()` in the learner object and still get memory errors,
    shut down the kernel of all the notebooks except the one you are currently working
    on.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一种防止内存不足的方法是一次运行一个笔记本。例如，在 Gradient 中，如果你同时激活多个笔记本，可能会耗尽可用内存。如果你已经在`TextDataLoaders`对象中设置了较小的批量大小，并在学习者对象中指定了`to_fp16()`，但仍然出现内存错误，那么可以关闭除当前正在使用的笔记本之外的所有笔记本的内核。
- en: 'In JuptyerLab in Gradient, you can shut down the kernel for a notebook by right-clicking
    on the notebook in the navigation pane and selecting **Shut Down Kernel** from
    the menu, as shown in the following screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Gradient 的 JupyterLab 中，你可以通过右键点击导航面板中的笔记本，然后从菜单中选择**关闭内核**来关闭一个笔记本的内核，如下图所示：
- en: '![Figure 4.4 – Shutting down a kernel in Gradient JupyterLab'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4 – 在 Gradient JupyterLab 中关闭内核'
- en: '](img/B16216_4_4.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_4.jpg)'
- en: Figure 4.4 – Shutting down a kernel in Gradient JupyterLab
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 在 Gradient JupyterLab 中关闭内核
- en: Workaround to allow you to save encoders
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决方法允许你保存编码器
- en: In addition to the memory tips we've just reviewed, there is one more tip you
    need to know for text models if you are using Gradient. On Gradient, you may run
    into a situation where you are not able to save and retrieve interim objects in
    the directory where fastai wants to save them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们刚才回顾的内存优化技巧之外，如果你在使用 Gradient 时，还需要知道一个针对文本模型的额外技巧。在 Gradient 中，你可能会遇到无法在
    fastai 希望保存的目录中保存和检索临时对象的情况。
- en: 'For example, you need to save the encoder from the language model and then
    load that encoder when you train the text classifier. However, fastai forces you
    to save the encoder in the path for the dataset. The `save_encoder` function only
    lets you specify the unqualified file name, not the directory in which to save
    the encoder, as you can see in the following call to `save_encoder`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你需要保存语言模型的编码器，并在训练文本分类器时加载该编码器。然而，fastai 强制你将编码器保存在数据集的路径中。`save_encoder`
    函数只允许你指定不带路径的文件名，而不是指定保存编码器的目录，正如你在下面的 `save_encoder` 调用中看到的那样：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At the same time, in Gradient, the directory for the IMDb dataset, `/storage/data/imdb`,
    is read-only. So, how can you save an encoder if the directory where it must be
    saved is not writeable? You can work around this problem by temporarily updating
    the learner''s `path` object, saving the encoder in the directory indicated by
    this temporary `path` value, and then setting the `path` object back to its original
    value, as shown here:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，在 Gradient 中，IMDb 数据集的目录 `/storage/data/imdb` 是只读的。那么，如果必须保存的目录不可写，如何保存编码器呢？你可以通过临时更新学习者的
    `path` 对象，先将编码器保存在该临时 `path` 值指示的目录中，然后将 `path` 对象设置回原始值来解决这个问题，如下所示：
- en: 'Save the path value for your model:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型的路径值：
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Change the path value for your model to a directory that you have write access
    to, for example:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的路径值更改为你有写权限的目录，例如：
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Save the model.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存模型。
- en: 'Change the path back to the original value:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将路径恢复为原始值：
- en: '[PRE16]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Training a deep learning classification model with a curated text dataset
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用精心整理的文本数据集训练深度学习分类模型
- en: In the previous section, we trained a language model using the curated text
    IMDb dataset. The model in the previous section predicted the next set of words
    that would follow a given set of words. In this section, we will take the language
    model that was fine-tuned on the IMDb dataset and use it to train a text classification
    model that classifies text samples that are specific to the movie review use case.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们使用整理过的 IMDb 文本数据集训练了一个语言模型。前一部分中的模型预测给定一组单词后，接下来的一组单词。在本节中，我们将使用在 IMDb
    数据集上微调的语言模型，来训练一个文本分类模型，该模型用于分类特定于电影评论的文本样本。
- en: Getting ready
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe makes use of the encoder that you trained in the previous section,
    so ensure that you have followed the steps in the recipe in that section, in particular,
    that you have saved the encoder from the trained language model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方使用了你在前一部分中训练的编码器，因此请确保你已经按照该部分中的配方步骤进行操作，特别是已经保存了从训练语言模型中得到的编码器。
- en: 'As mentioned in the previous section, you need to take some additional steps
    before you can run recipes in Gradient that use the language model pre-trained
    on the Wikipedia corpus. To ensure that you have access to the pre-trained language
    model that you need to use in this recipe, complete the following steps if the
    setup of AWD_LSTM was interrupted:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一部分所述，在 Gradient 中运行使用了在 Wikipedia 语料库上预训练的语言模型的配方之前，你需要执行一些额外的步骤。为了确保你可以使用在本配方中所需的预训练语言模型，如果
    AWD_LSTM 的设置被中断，请完成以下步骤。
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Colab 中，运行 `text_model_training.ipynb` 笔记本中的单元格，直到并包括学习者定义和训练单元格。完成后，将 `/root/.fastai/models/wt103-fwd`
    目录中的内容复制到你的 Drive 环境中的一个文件夹。
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你在上一步复制的文件上传到 Gradient 环境中的 `/storage/models/wt103-fwd` 目录。
- en: With these steps, you will be able to run the notebook for this recipe (and
    other recipes that make use of `AWD_LSTM`) in Gradient.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这些步骤，你就可以在 Gradient 中运行此配方的笔记本（以及其他使用 `AWD_LSTM` 的配方）。
- en: How to do it…
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In this section, you will be running through the `text_classifier_model.ipynb`
    notebook to train a text classifier deep learning model using the `IMDb` curated
    dataset. Once you have the notebook open in Gradient, complete these steps:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将通过运行`text_classifier_model.ipynb`笔记本，使用`IMDb`整理的数据集训练一个文本分类深度学习模型。打开笔记本后，完成以下步骤：
- en: Run the cells in the notebook up to the `Define the text classifier` cell.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的单元，直到`定义文本分类器`单元。
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元来定义一个`TextDataLoaders`对象：
- en: '[PRE17]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是`TextDataLoaders`对象定义的参数：
- en: 'a) `path`: Defines the path of the dataset used to define the `TextDataLoaders`
    object'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `path`：定义用于定义`TextDataLoaders`对象的数据集路径
- en: 'b) `valid`: Identifies the folder in the dataset''s directory structure that
    will be used to assess the performance of the model: `imdb/test`'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `valid`：标识数据集目录结构中用于评估模型性能的文件夹：`imdb/test`
- en: 'Run the following cell to see a sample of entries from a batch:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元查看一个批次中的样本条目：
- en: '[PRE18]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output of `show_batch()` shows text samples along with the class (indicated
    in the `category` column). fastai knows that the class is encoded in the directory
    where the text sample is and correctly renders it in `show_batch()`, as seen in
    the following screenshot:![Figure 4.5 – Output of show_batch()
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`show_batch()`的输出展示了文本样本及其类别（在`category`列中表示）。fastai知道类别是编码在文本样本所在的目录中的，并在`show_batch()`中正确显示，如下图所示：![图4.5
    – `show_batch()`输出'
- en: '](img/B16216_4_5.jpg)'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_5.jpg)'
- en: Figure 4.5 – Output of show_batch()
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.5 – `show_batch()`输出
- en: 'Run the following cell to define the text classifier model:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元来定义文本分类模型：
- en: '[PRE19]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here are the arguments for the definition of the `text_classifier_learner`
    object:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是`text_classifier_learner`对象定义的参数：
- en: 'a) `dls_clas`: This is the `TextDataLoaders` object defined in the previous
    cell.'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `dls_clas`：这是在前一个单元中定义的`TextDataLoaders`对象。
- en: 'b) `AWD_LSTM`: This is the pre-trained model to use as a basis for this model.
    If you run this notebook in Colab, you can find the files that make up this model
    in the `/root/.fastai/models/wt103-fwd` directory after you have run this cell.'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `AWD_LSTM`：这是作为该模型基础使用的预训练模型。如果你在Colab中运行此笔记本，你可以在运行此单元后，找到组成该模型的文件，路径是`/root/.fastai/models/wt103-fwd`。
- en: 'c) `metrics`: This is the performance metric to be optimized for the model,
    in this case, accuracy.'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `metrics`：这是要优化的模型性能指标，在此案例中为准确率。
- en: 'You need to load the encoder that you saved as part of the recipe in the previous
    section. The first step is to set the path for the `learn_clas` object so that
    it is the path in which the encoder is saved by running the following cell. Ensure
    that the directory specified is the directory where you saved the encoder in the
    previous recipe:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要加载在上一节中作为配方保存的编码器。第一步是为`learn_clas`对象设置路径，使其指向保存编码器的路径，运行以下单元即可。确保指定的目录是你在上一节保存编码器的目录：
- en: '[PRE20]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Run the following cell to load the encoder that you saved in the recipe in
    the *Training a deep learning language model with a curated text dataset* section
    to the `learn_clas` object:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元来加载你在*使用整理文本数据集训练深度学习语言模型*一节中保存的编码器到`learn_clas`对象：
- en: '[PRE21]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Run the following cell to train the model:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元来训练模型：
- en: '[PRE22]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here are the arguments for `fit_one_cycle`:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是`fit_one_cycle`的参数：
- en: a) The argument epoch count (first argument) specifies that the training is
    run for `5` epochs.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 参数epoch计数（第一个参数）指定训练运行`5`个epoch。
- en: b) The argument learning rate (second argument) specifies that the learning
    rate is equal to `0.02`.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 参数学习率（第二个参数）指定学习率为`0.02`。
- en: 'The output of this cell shows the results of the training, including the accuracy
    and the time taken for each epoch, as shown in the following screenshot:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该单元的输出显示了训练结果，包括每个epoch的准确度和所需时间，如下图所示：
- en: '![Figure 4.6 – Results of training the text classification model'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.6 – 文本分类模型训练结果'
- en: '](img/B16216_4_6.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_6.jpg)'
- en: Figure 4.6 – Results of training the text classification model
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.6 – 文本分类模型训练结果
- en: 'Run the cells to get predictions on text strings that you expect to be negative
    and positive and observe whether the trained model makes the expected predictions,
    as shown in the following screenshot:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行这些单元，对你期望为负面和正面的文本字符串进行预测，并观察训练好的模型是否做出了预期的预测，如以下截图所示：
- en: '![Figure 4.7 – Using the text classifier to get predictions on text strings'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 – 使用文本分类器对文本字符串进行预测'
- en: '](img/B16216_4_7.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_7.jpg)'
- en: Figure 4.7 – Using the text classifier to get predictions on text strings
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 使用文本分类器对文本字符串进行预测
- en: Congratulations! You have taken the language model that was fine-tuned on the
    IMDb dataset and used it to train a text classification model that classifies
    text samples that are specific to the movie review use case.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你！你已经将经过IMDb数据集微调的语言模型用于训练文本分类模型，该模型对特定于电影评论的文本样本进行分类。
- en: How it works…
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'You can see the power of fastai by contrasting the code in this section, which
    defines a text classifier, with the code in the previous section, which defines
    a language model. There are only three differences in total:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过对比本节定义文本分类器的代码与前一节定义语言模型的代码，看到fastai的强大。总共只有三点差异：
- en: 'In the definition of the `TextDataLoaders` object, the following applies:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`TextDataLoaders`对象的定义中，以下内容适用：
- en: a) The language model has the `is_lm` argument to indicate that the model is
    a language model.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 语言模型具有`is_lm`参数，用于指示该模型是一个语言模型。
- en: b) The text classifier has the `label_col` argument to indicate which column
    in the dataset contains the category that is being predicted by the model. In
    the case of the text classifier defined in this section, the label for the dataset
    is encoded in the directory structure of the dataset rather than as a column in
    the dataset, so this parameter is not needed in the definition of the `TextDataLoaders`
    object.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 文本分类器具有`label_col`参数，用于指示数据集中的哪一列包含模型正在预测的类别。在本节定义的文本分类器中，数据集的标签是通过数据集的目录结构进行编码的，而不是数据集中的一列，因此在定义`TextDataLoaders`对象时不需要该参数。
- en: 'In the definition of the model, the following applies:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型定义中，以下内容适用：
- en: a) The language model defines a `language_model_learner` object.
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 语言模型定义了一个`language_model_learner`对象。
- en: b) The text classifier defines a `text_classifier_learner` object.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 文本分类器定义了一个`text_classifier_learner`对象。
- en: 'In getting a prediction from the model, the following applies:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在从模型获取预测时，以下内容适用：
- en: a) The language model takes two arguments for its call to `learn.predict()`,
    the string on which to make the prediction, and the number of words to predict.
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 语言模型在调用`learn.predict()`时有两个参数，第一个是用来进行预测的字符串，第二个是要预测的单词数量。
- en: b) The text classifier takes one argument for its call to `learn.predict()`,
    the string whose class the model will predict.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 文本分类器在调用`learn.predict()`时有一个参数，即模型将预测其类别的字符串。
- en: With just these three differences, fastai takes care of all the underlying differences
    between a language model and a text classifier.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 仅通过这三点差异，fastai就处理了语言模型和文本分类器之间的所有底层差异。
- en: There's more…
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'If you are using the Gradient environment and you are using a notebook with
    a cost, you will want to control how long the notebook is active to avoid paying
    for more time than you need. You can select the duration of your session when
    you start it up by selecting an hour value from the **Auto-Shutdown** menu, as
    shown in the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Gradient环境，并且正在使用带有费用的笔记本，你可能需要控制笔记本的活跃时间，以避免支付超过所需的时间。你可以在启动时通过从**自动关机**菜单中选择小时数来选择会话持续时间，如下截图所示：
- en: '![Figure 4.8 – Selecting a duration for a Gradient notebook session'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 – 为Gradient笔记本会话选择持续时间'
- en: '](img/B16216_4_8.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_8.jpg)'
- en: Figure 4.8 – Selecting a duration for a Gradient notebook session
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 – 为Gradient笔记本会话选择持续时间
- en: Suppose that you end up selecting more time than you need and you are done with
    your session before the auto-shutdown time limit is reached. Should you explicitly
    shut down the session?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你选择了比实际需要更长的时间，并且在自动关机时间限制到达之前就完成了会话。那么，你是否应该显式地关闭会话？
- en: 'My experience has been that if you try to stop the instance in the Gradient
    notebook interface by selecting the **Stop Instance** button in the **Instance**
    view (as shown in *Figure 4.9*), you will risk putting your instance into a state
    where you cannot start it again easily:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我的经验是，如果你尝试在Gradient笔记本界面中通过选择**停止实例**按钮（如*图4.9*所示）来停止实例，你将面临将实例置于无法轻松重新启动的状态的风险：
- en: '![Figure 4.9 – Stop Instance button in the Instance view in Gradient'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 – 在Gradient中的实例视图中停止实例按钮'
- en: '](img/B16216_4_9.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_9.jpg)'
- en: Figure 4.9 – Stop Instance button in the Instance view in Gradient
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 在Gradient中的实例视图中的“停止实例”按钮
- en: If you select **Stop Instance** and your instance gets into a state where you
    cannot start it again, then you will have to open a ticket with *Paperspace* support
    to fix your instance. After this happened to me a couple of times, I stopped using
    the **Stop Instance** button and just let the instance time out when I was finished
    working with it. You will save yourself time by never explicitly stopping your
    Gradient instance and instead just letting it time out when you are done with
    a session.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择**停止实例**并且您的实例进入无法重新启动的状态，那么您必须联系*Paperspace*支持团队来修复您的实例。在这种情况发生了几次之后，我停止使用**停止实例**按钮，而是等实例在我完成工作后超时自动停止。通过不显式停止您的Gradient实例，而是让其在会话结束时超时，您将节省更多时间。
- en: Training a deep learning language model with a standalone text dataset
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用独立文本数据集训练深度学习语言模型
- en: 'In the previous sections, we trained a language model and a text classifier
    using the curated text dataset IMDb. In this section and the next section, we
    will train a language model and a text classifier using a standalone text dataset,
    the *Kaggle Coronavirus tweets NLP – Text Classification* dataset described here:
    [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
    This dataset includes a selection of tweets related to the Covid-19 pandemic,
    along with categorization for the tweets according to the following five categories:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用精选的文本数据集IMDb训练了一个语言模型和文本分类器。在这一节和接下来的章节中，我们将使用一个独立的文本数据集——*Kaggle新冠推文NLP
    – 文本分类*数据集，进行语言模型和文本分类器的训练。该数据集可以在这里找到：[https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)。该数据集包含与新冠疫情相关的一些推文，并且对这些推文进行了五个类别的分类：
- en: Extremely negative
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极度负面
- en: Negative
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负面
- en: Neutral
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中立
- en: Positive
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正面
- en: Extremely positive
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极度正面
- en: The goal of the language model trained on this dataset is to predict the subsequent
    words in a Covid-related tweet given a starting phrase. The goal of the text classification
    model trained on this dataset, as described in the *Training a deep learning text
    classifier with a standalone text dataset* section, is to predict which of the
    five categories a phrase belongs in.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集上训练的语言模型的目标是根据一个起始短语预测新冠相关推文中的后续单词。在这个数据集上训练的文本分类模型的目标，如*使用独立文本数据集训练深度学习文本分类器*一节中所述，是预测一个短语属于五个类别中的哪一个。
- en: Getting ready
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As mentioned in previous sections in this chapter, you need to take some additional
    steps before you can run this recipe in Gradient to ensure that you have access
    to the pre-trained language model that you will use in this recipe. If you have
    not done so already, follow these steps to prepare your Gradient environment if
    the setup of AWD_LSTM was interrupted:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，在您能够在Gradient中运行此食谱之前，您需要采取一些额外的步骤，以确保您可以访问将在此食谱中使用的预训练语言模型。如果您尚未执行这些步骤，请按照以下步骤准备Gradient环境（如果AWD_LSTM的设置中断了）：
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Colab中，运行`text_model_training.ipynb`笔记本的单元格，直到包括学习者定义和训练单元格为止。完成后，将`/root/.fastai/models/wt103-fwd`目录中的内容复制到您的Drive环境中的文件夹中。
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您在上一步中复制的文件上传到Gradient环境中的`/storage/models/wt103-fwd`目录。
- en: With these steps, you should now be able to run the notebook for this recipe
    (and other recipes that make use of `AWD_LSTM`) in Gradient.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，您现在应该能够在Gradient中运行此食谱（以及其他使用`AWD_LSTM`的食谱）。
- en: 'Ensure that you have uploaded the files that make up the standalone Covid-related
    tweets dataset to your Gradient environment by following these steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您已将构成独立新冠相关推文数据集的文件上传到您的Gradient环境中，可以按照以下步骤操作：
- en: Download the `archive.zip` file from [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)下载`archive.zip`文件。
- en: Unzip the downloaded `archive.zip` file to extract the `Corona_NLP_test.csv`
    and `Corona_NLP_train.csv` files.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解压下载的`archive.zip`文件，提取`Corona_NLP_test.csv`和`Corona_NLP_train.csv`文件。
- en: 'From the terminal in your Gradient environment, make `/storage/archive` your
    current directory:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Gradient环境的终端中，将`/storage/archive`设置为当前目录：
- en: '[PRE23]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create the `/storage/archive/covid_tweets` directory:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`/storage/archive/covid_tweets`目录：
- en: '[PRE24]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Make `/storage/archive/covid_tweets` your current directory:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`/storage/archive/covid_tweets`设置为当前目录：
- en: '[PRE25]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create `test` and `train` directories in `/storage/archive/covid_tweets`:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`/storage/archive/covid_tweets`中创建`test`和`train`目录：
- en: '[PRE26]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Upload the files you extracted in *step 2* (`Corona_NLP_test.csv` and `Corona_NLP_train.csv`)
    to `/storage/archive/covid_tweets`. You can use the upload button in JupyterLab
    in Gradient to do the upload, but you need to do it in several steps:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您在*步骤2*中解压的文件（`Corona_NLP_test.csv`和`Corona_NLP_train.csv`）上传到`/storage/archive/covid_tweets`。您可以使用Gradient中的JupyterLab的上传按钮进行上传，但需要分几步完成：
- en: 'a) From the terminal in your Gradient environment, make `/notebooks` your current
    directory:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 在Gradient环境的终端中，将`/notebooks`设置为当前目录：
- en: '[PRE27]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'temp your current folder, select the upload button (see *Figure 4.10*), and
    select the Corona_NLP_test.csv and Corona_NLP_train.csv files from your local
    system folder where you extracted them in *step 2*:'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在临时文件夹中，选择上传按钮（见*图4.10*），并从本地系统文件夹中选择在*步骤2*中解压的`Corona_NLP_test.csv`和`Corona_NLP_train.csv`文件：
- en: '[PRE28]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 4.10 – Upload button in JupyterLab'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.10 – JupyterLab中的上传按钮'
- en: '](img/B16216_4_10.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_10.jpg)'
- en: Figure 4.10 – Upload button in JupyterLab
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – JupyterLab中的上传按钮
- en: 'd) From the terminal in your Gradient environment, copy the `Corona_NLP_test.csv` file
    into the `/storage/archive/covid_tweets/test` directory:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: d) 在Gradient环境的终端中，将`Corona_NLP_test.csv`文件复制到`/storage/archive/covid_tweets/test`目录：
- en: '[PRE29]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'e) Copy the `Corona_NLP_train.csv` file into the `/storage/archive/covid_tweets/train`
    directory:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: e) 将`Corona_NLP_train.csv`文件复制到`/storage/archive/covid_tweets/train`目录：
- en: '[PRE30]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once you have completed the steps to upload the files that make up the Covid-related
    tweets dataset, you should have the following directory structure in the `/storage/archive/covid_tweets`
    directory in your Gradient environment:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成上传构成Covid相关推文数据集的文件步骤，您应该在Gradient环境中的`/storage/archive/covid_tweets`目录下拥有以下目录结构：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With these preparation steps, you have brought the files that make up the dataset
    into the correct location in your Gradient environment to be used by a fastai
    model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些准备步骤，您已将构成数据集的文件放入Gradient环境中的正确位置，以供fastai模型使用。
- en: I am grateful for the opportunity to include the Covid tweets dataset in this
    book and I would like to thank the curators of this dataset and Kaggle for making
    the dataset available.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我感谢有机会在本书中包含Covid推文数据集，并感谢该数据集的策展人以及Kaggle使该数据集得以公开。
- en: Dataset citation
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集引用
- en: Aman Miglani (2020). *Coronavirus tweets NLP - Text Classification* ([https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification))
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Aman Miglani (2020). *Coronavirus tweets NLP - Text Classification* ([https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification))
- en: How to do it…
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In this section, you will be running through the `text_standalone_dataset_lm.ipynb`
    notebook to train a language model using the Covid-related tweets standalone dataset.
    Once you have the notebook open in Gradient, complete these steps:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将运行`text_standalone_dataset_lm.ipynb`笔记本，使用与Covid相关的推文独立数据集训练语言模型。打开笔记本后，完成以下步骤：
- en: Run the cells in the notebook up to the `Ingest the dataset` cell.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的单元格直到`Ingest the dataset`单元格。
- en: 'Run the following cell to define a path object for the dataset:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义数据集的路径对象：
- en: '[PRE32]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The argument for this path definition is the name of the root of the directory
    hierarchy in your Gradient environment into which you copied the CSV files for
    the dataset.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该路径定义的参数是您在Gradient环境中复制了数据集CSV文件的目录层次结构根目录的名称。
- en: 'Run the following cell to define a `df_train` dataframe to contain the contents
    of the `Corona_NLP_train.csv` file:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义一个`df_train`数据框，用于包含`Corona_NLP_train.csv`文件的内容：
- en: '[PRE33]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here are the arguments for the definition of the dataframe:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是定义数据框架的参数：
- en: a) The `path/'train/Corona_NLP_train.csv'` argument specifies the partially
    qualified filename for the training portion of the dataset.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `path/'train/Corona_NLP_train.csv'`参数指定了数据集训练部分的部分文件名。
- en: b) The `encoding = "ISO-8859-1"` argument specifies the encoding to use for
    the file. This encoding is selected to ensure that the content of the CSV file
    can be ingested into a dataframe without any errors.
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `encoding = "ISO-8859-1"`参数指定用于文件的编码。此编码选择是为了确保CSV文件的内容可以无误地导入到数据框中。
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以定义`TextDataLoaders`对象：
- en: '[PRE34]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`TextDataLoaders`对象定义的参数：
- en: 'a) `df_train`: The dataframe that you created in the previous step.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `df_train`：你在上一步中创建的数据框。
- en: 'b) `path`: The path object for the dataset.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `path`：数据集的路径对象。
- en: 'c) `text_col`: The column in the dataframe containing the text that will be
    used to train the model. For this dataset, the `OriginalTweet` column contains
    the text used to train the model.'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `text_col`：数据框中包含用于训练模型的文本的列。在这个数据集里，`OriginalTweet`列包含了用于训练模型的文本。
- en: 'd) `is_lm`: An indicator that this model is a language model.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `is_lm`：指示该模型是语言模型的标志。
- en: 'Run the following cell to define and train the deep learning model with a `language_model_learner`
    object:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以定义并训练带有`language_model_learner`对象的深度学习模型：
- en: '[PRE35]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The definition of the `language_model_learner` object includes the call to
    `to_fp16()` to specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process.'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`language_model_learner`对象的定义包括调用`to_fp16()`来指定混合精度训练（总结见：[https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16)），以减少训练过程中的内存消耗。'
- en: 'Here are the arguments for the definition of the `language_model_learner` object:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`language_model_learner`对象定义的参数：
- en: 'a) `dls`: The `TextDataLoaders` object that you defined in the previous step.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `dls`：你在上一步中定义的`TextDataLoaders`对象。
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `AWD_LSTM`：用作此模型基础的预训练模型。这是与fastai结合使用的预训练语言模型，使用维基百科进行训练。
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `metrics`：模型优化的性能指标，在本例中为准确率。
- en: 'Here are the arguments for the `fine_tune` statement:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是`fine_tune`语句的参数：
- en: a) The epoch count argument (first argument) specifies the number of epochs
    for the training process.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) epoch计数参数（第一个参数）指定训练过程中的迭代次数。
- en: b) The learning rate argument (second argument) specifies the learning rate
    for the training process.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 学习率参数（第二个参数）指定训练过程中的学习率。
- en: 'The results of the training process, as shown in *Figure 4.11*, are displayed
    once the `fine_tune` statement has been run:'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程的结果，如*图4.11*所示，在执行`fine_tune`语句后显示：
- en: '![Figure 4.11 – Results of the training process'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.11 – 训练过程的结果'
- en: '](img/B16216_4_11.jpg)'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_11.jpg)'
- en: Figure 4.11 – Results of the training process
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.11 – 训练过程的结果
- en: 'Run the following cell to exercise the trained language model:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以运行训练好的语言模型：
- en: '[PRE36]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The results are displayed as follows:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下所示：
- en: '![Figure 4.12 – Prediction of a language model trained on a standalone text
    dataset'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.12 – 在独立文本数据集上训练的语言模型的预测'
- en: '](img/B16216_4_12.jpg)'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_12.jpg)'
- en: Figure 4.12 – Prediction of a language model trained on a standalone text dataset
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.12 – 在独立文本数据集上训练的语言模型的预测
- en: 'Run the following cell to save the model. You can update the cell to specify
    the directory and filename to which to save the model:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存模型。你可以更新单元格来指定保存模型的目录和文件名：
- en: '[PRE37]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Run the following cell to save the current path value:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存当前路径值：
- en: '[PRE38]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is that the default location for the model is not writeable
    on Gradient, so you need to change the path value to a directory where you have
    write access:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以为学习器对象路径分配新值。这样做的原因是，模型的默认位置在Gradient上不可写，因此你需要将路径值更改为你有写入权限的目录：
- en: '[PRE39]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Run the following cell to save the encoder subset of the language model. This
    is the model minus the final layer. You will use this encoder in the next recipe
    when you train a text classifier on the Covid-related tweets standalone dataset:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格以保存语言模型的编码器子集。这是去除最后一层的模型。你将在下一部分中使用这个编码器，当你在Covid相关推文的独立数据集上训练文本分类器时将用到它：
- en: '[PRE40]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Congratulations! You used fastai to do transfer learning on top of an existing
    model with the Covid-related tweets standalone dataset to create a language model
    fine-tuned on that dataset. In the next section, you will use the encoder that
    you saved in the last step to fine-tune a text classifier trained on the standalone
    dataset.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你使用fastai在现有模型的基础上进行了迁移学习，并结合Covid相关推文独立数据集创建了一个经过微调的语言模型。在下一部分中，你将使用上一步保存的编码器来微调一个基于独立数据集训练的文本分类器。
- en: How it works…
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this section, you have seen a simple example of how to train a language model
    with fastai using a standalone text dataset. The language model is created by
    taking an existing model (`AWD_LSTM`) that has been trained with the massive Wikipedia
    dataset and then fine-tuning it using the standalone Covid-related tweets dataset.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你已经看到了一个简单的例子，展示了如何使用fastai和独立文本数据集训练语言模型。语言模型的创建是通过使用一个已经在海量Wikipedia数据集上训练的现有模型（`AWD_LSTM`），然后使用独立的Covid相关推文数据集对其进行微调。
- en: By taking advantage of transfer learning in this way, we end up with a language
    model that combines a good degree of capability in terms of general-purpose English
    (thanks to the model pre-trained on the wiki dataset) as well as the capability
    to produce text that is specific to the use case of social media related to the
    Covid-19 pandemic (thanks to the Covid tweets dataset). By following the recipe
    in this section, you can take advantage of fastai to apply this approach (transfer
    learning for NLP) on other text datasets for other use cases.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式利用迁移学习，我们得到了一个语言模型，既具备了良好的通用英语能力（得益于在Wikipedia数据集上预训练的模型），也具备了生成与Covid-19大流行相关的社交媒体内容的能力（得益于Covid推文数据集）。通过遵循本节中的教程，你可以利用fastai在其他文本数据集和用例中应用这种方法（NLP的迁移学习）。
- en: Training a deep learning text classifier with a standalone text dataset
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用独立文本数据集训练深度学习文本分类器
- en: 'In the *Training a deep learning language model with a standalone text dataset*
    section, we trained a language model using the standalone text dataset: the Kaggle
    Coronavirus tweets NLP – Text Classification dataset described here: [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
    In this section, we will use this language model to create a text classifier trained
    with the Covid-related tweets dataset.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用独立文本数据集训练深度学习语言模型*部分，我们使用独立文本数据集训练了一个语言模型：Kaggle上的“Coronavirus tweets NLP
    – Text Classification”数据集，详细描述请见这里：[https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)。在这一部分，我们将使用这个语言模型创建一个基于与Covid相关的推文数据集训练的文本分类器。
- en: Getting ready
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe makes use of the encoder that you trained in the *Training a deep
    learning language model with a standalone text dataset* section, so ensure that
    you have followed the steps in the recipe in that section. In particular, ensure
    that you have saved the encoder from the language model you trained in the previous
    section.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这个教程使用了你在*使用独立文本数据集训练深度学习语言模型*部分中训练的编码器，因此请确保你已按照该部分中的教程步骤进行操作，特别是要确保你已经保存了上部分训练的语言模型中的编码器。
- en: 'Also, make sure you have followed all the steps from the *Getting ready* sub-section
    of the previous section to ensure the following:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请确保你已按照上一部分的*准备工作*子节中的所有步骤，确保以下几点：
- en: That you have access to the `AWD_LSTM` model in your Gradient environment
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已在Gradient环境中访问到`AWD_LSTM`模型。
- en: That you have uploaded the files (`Corona_NLP_test.csv` and `Corona_NLP_train.csv`)
    that make up the standalone Covid-related tweets dataset to your Gradient environment
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经将包含独立Covid相关推文数据集的文件（`Corona_NLP_test.csv`和`Corona_NLP_train.csv`）上传到你的Gradient环境中。
- en: How to do it…
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this section, you will be running through the `text_standalone_dataset_classifier.ipynb`
    notebook to train a text classifier deep learning model using the Covid-related
    tweets dataset. Once you have the notebook open in Gradient, perform the following
    steps:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将通过 `text_standalone_dataset_classifier.ipynb` 笔记本来训练一个文本分类深度学习模型，使用的是与
    Covid 相关的推文数据集。打开笔记本后，按以下步骤操作：
- en: Run the cells in the notebook up to the `Ingest the dataset` cell.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的单元格，直到 `Ingest the dataset` 单元格。
- en: 'Run the following cell to define a path object for the dataset. Note that the
    argument is the name of the root directory in your Gradient environment into which
    you copied the CSV files for the dataset:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义数据集的路径对象。注意，参数是你在 Gradient 环境中复制CSV文件所在的根目录的名称：
- en: '[PRE41]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Run the following cell to define a dataframe to contain the contents of the
    `Corona_NLP_train.csv` file (the training portion of the Covid-related tweets
    dataset):'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义一个数据框，包含 `Corona_NLP_train.csv` 文件的内容（与 Covid 相关的推文数据集的训练部分）：
- en: '[PRE42]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here are the arguments for the definition of the dataframe:'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是定义数据框的参数：
- en: a) The `path/'train/Corona_NLP_train.csv'` argument specifies the partially
    qualified filename for the training portion of the dataset.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `path/'train/Corona_NLP_train.csv'` 参数指定了数据集训练部分的部分限定文件名。
- en: b) The `encoding = "ISO-8859-1"` argument specifies the encoding to use for
    the file. This encoding is selected to ensure that the content of the CSV file
    can be ingested into a dataframe without any errors.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `encoding = "ISO-8859-1"` 参数指定了用于文件的编码格式。选择此编码格式是为了确保CSV文件的内容能够无误地导入到数据框中。
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义 `TextDataLoaders` 对象：
- en: '[PRE43]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是定义 `TextDataLoaders` 对象的参数：
- en: 'a) `df_train`: The dataframe that you created in the previous step.'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `df_train`：你在前一步中创建的数据框。
- en: 'b) `path`: The path object for the dataset.'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `path`：数据集的路径对象。
- en: 'c) `text_col`: The column in the dataframe containing the text that will be
    used to train the model. For this dataset, the `OriginalTweet` column contains
    the text used to train the model.'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `text_col`：数据框中包含将用于训练模型的文本的列。对于该数据集，`OriginalTweet` 列包含用于训练模型的文本。
- en: 'd) `label_col`: The column in the dataframe containing the labels that the
    text classifier will predict.'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `label_col`：数据框中包含文本分类器将预测的标签的列。
- en: 'Run the following cell to see a batch from the `TextDataLoaders` object that
    you defined in the previous step:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来查看你在前一步中定义的 `TextDataLoaders` 对象中的一个批次：
- en: '[PRE44]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output of this statement, the `text` and `category` columns, will be as
    follows:'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该语句的输出，`text` 和 `category` 列，将如下所示：
- en: '![Figure 4.13 – Batch from the Covid tweets dataset'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.13 – 来自 Covid 推文数据集的批次'
- en: '](img/B16216_4_13.jpg)'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_13.jpg)'
- en: Figure 4.13 – Batch from the Covid tweets dataset
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.13 – 来自 Covid 推文数据集的批次
- en: 'Run the following cell to define the `text_classifier_learner` object for the
    text classifier model:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来定义 `text_classifier_learner` 对象，以用于文本分类模型：
- en: '[PRE45]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'The definition of the `text_classifier_learner` object includes the call to
    `to_fp16()` to specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`text_classifier_learner` 对象的定义包括调用 `to_fp16()` 来指定混合精度训练（此处总结：[https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16)），以减少训练过程中内存的消耗。'
- en: 'Here are the arguments for the definition of the `text_classifier_learner`
    object:'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是定义 `text_classifier_learner` 对象的参数：
- en: 'a) `dls`: The `TextDataLoaders` object that you defined in the previous step.'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `dls`：你在前一步中定义的 `TextDataLoaders` 对象。
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia.'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `AWD_LSTM`：用作此模型基础的预训练模型。它是与 fastai 集成的预训练语言模型，已使用维基百科进行训练。
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `metrics`：要优化的模型性能指标，在此案例中为准确度。
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is to set the path to match the directory where you saved
    the encoder in the recipe in the previous section:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格，为学习器对象路径分配一个新值。这样做的原因是将路径设置为与你在前一部分保存编码器的目录匹配：
- en: '[PRE46]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Run the following cell to load the encoder that you saved in the recipe in
    the *Training a deep learning language model with a standalone text dataset* section
    to the `learn_clas` object:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码单元，将你在*训练深度学习语言模型与独立文本数据集*部分保存的编码器加载到`learn_clas`对象中：
- en: '[PRE47]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Run the following cell to reset the value of the learner object path:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下代码单元，重置学习器对象路径的值：
- en: '[PRE48]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Run the following cell to train the model:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下单元格来训练模型：
- en: '[PRE49]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Here are the arguments for `fit_one_cycle`:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是`fit_one_cycle`的参数：
- en: a) The epoch count argument (first argument) specifies that the training is
    run for 1 epoch.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 纪元计数参数（第一个参数）指定训练运行1个纪元。
- en: b) The learning rate argument (second argument) specifies that the learning
    rate is equal to 0.02.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 学习率参数（第二个参数）指定学习率为0.02。
- en: 'The output of this cell (as shown in *Figure 4.14*) shows the results of the
    training:'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该单元格的输出（如*图4.14*所示）展示了训练的结果：
- en: '![Figure 4.14 – Output of the text classifier training'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.14 – 文本分类器训练的输出'
- en: '](img/B16216_4_14.jpg)'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16216_4_14.jpg)'
- en: Figure 4.14 – Output of the text classifier training
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.14 – 文本分类器训练的输出
- en: 'Run the cells to get predictions on text strings that you expect to be negative
    and positive and observe whether the trained model makes the expected predictions
    (as shown in *Figure 4.15*):'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元格，对你预期为负面和正面的文本字符串进行预测，观察训练好的模型是否做出了预期的预测（如*图4.15*所示）：
- en: '![Figure 4.15 – Using the text classifier to get predictions on text strings'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15 – 使用文本分类器对文本字符串进行预测'
- en: '](img/B16216_4_15.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_15.jpg)'
- en: Figure 4.15 – Using the text classifier to get predictions on text strings
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 使用文本分类器对文本字符串进行预测
- en: Congratulations! You have taken advantage of the facilities of fastai to train
    a text classifier on a standalone dataset using transfer learning.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经利用fastai的功能，在独立数据集上使用迁移学习训练了文本分类器。
- en: How it works…
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The code for the text classifier model for the standalone Covid-related tweets
    dataset has some differences from the code for the text classifier model for the
    curated IMDb text dataset. Let's examine some of these differences.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 针对独立的Covid相关推文数据集，文本分类器模型的代码与针对精心策划的IMDb文本数据集的文本分类器模型代码有一些不同。让我们来看一下其中的一些差异。
- en: 'For the IMDb dataset, the `TextDataLoaders` definition does not include a `label_col`
    parameter:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IMDb数据集，`TextDataLoaders`定义不包括`label_col`参数：
- en: '[PRE50]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'By contrast, the `TextDataLoaders` definition for the standalone dataset includes
    both `text_col` and `label_col` parameters:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，独立数据集的`TextDataLoaders`定义包括`text_col`和`label_col`参数：
- en: '[PRE51]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'What''s the reason for these differences? First, for the `IMDb` dataset, we
    use the `from_folder` variation of `TextDataLoaders` because the dataset is organized
    as a collection of individual text files whose class is encoded by the directory
    that the file is in. Here is the directory structure of the IMDb dataset:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会有这些差异？首先，对于`IMDb`数据集，我们使用`TextDataLoaders`的`from_folder`变体，因为数据集是按单独的文本文件组织的，而每个文件所属的类别由该文件所在的目录编码。以下是IMDb数据集的目录结构：
- en: '[PRE52]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Consider one file from the IMDB dataset, `train/pos/9971_10.txt`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑IMDb数据集中的一个文件，`train/pos/9971_10.txt`：
- en: '*This film was Excellent, I thought that the original one was quiet mediocre.
    This one however got all the ingredients, a factory 1970 Hemi Challenger with
    4 speed transmission that really shows that Mother Mopar knew how to build the
    best muscle cars! I was in Chrysler heaven every time Kowalski floored that big
    block Hemi, and he sure did that a lot :)*'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '*这部电影非常棒，我觉得原版还算平庸。不过这部作品却具备了所有要素，一辆1970年的Hemi Challenger，配备四速变速器，真的展示了母亲Mopar如何打造出最棒的肌肉车！每次Kowalski踩下那大块Hemi时，我仿佛置身于克莱斯勒的天堂，而他确实经常这么做
    :)*'
- en: How does fastai know the class of this review when we train the text classifier
    model? It knows because this file is in the `/pos` directory. Thanks to the flexibility
    of fastai, we simply have to pass the `path` value to the definition of the `TextDataLoaders`
    object and the fastai framework figures out the category of each text sample in
    the dataset.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练文本分类模型时，fastai如何知道这篇评论的类别？它知道，因为这个文件位于`/pos`目录中。得益于fastai的灵活性，我们只需将`path`值传递给`TextDataLoaders`对象的定义，fastai框架会自动识别数据集中每个文本样本的类别。
- en: 'Now, let''s consider the standalone Covid-related tweets dataset. This dataset
    is packaged as CSV files that look like this:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下独立的与Covid相关的推文数据集。这个数据集被打包为CSV文件，格式如下：
- en: '![Figure 4.16 – Sample from the Covid-related tweets dataset'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.16 —— 来自与Covid相关的推文数据集的样本'
- en: '](img/B16216_4_16.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16216_4_16.jpg)'
- en: Figure 4.16 – Sample from the Covid-related tweets dataset
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 —— 来自与Covid相关的推文数据集的样本
- en: 'Unlike the IMDb dataset, the Covid-related tweets dataset is encapsulated in
    just two files (one for the training dataset and one for the test dataset). The
    columns of these files have the information that fastai needs to train the model:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 与IMDb数据集不同，Covid相关的推文数据集只包含两个文件（一个用于训练数据集，一个用于测试数据集）。这些文件的列包含fastai训练模型所需的信息：
- en: The text of the sample – in the `OriginalTweet` column
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本的文本 —— 在`OriginalTweet`列中
- en: The class (also known as the label) of the sample – in the `Sentiment` column
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本的类别（也称为标签）——在`Sentiment`列中
- en: 'To tell fastai how to interpret this dataset, we need to explicitly tell it
    which column of the dataset the text is in and which column the class (or label)
    is in the definition of the `TextDataLoaders` object, as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为了告诉fastai如何解读这个数据集，我们需要在`TextDataLoaders`对象的定义中明确告诉它文本所在的列和类别（或标签）所在的列，方法如下：
- en: '[PRE53]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The `IMDb` dataset is made up of thousands of individual text files spread across
    a complex set of directories that encode the class of each text file. By contrast,
    the Covid-related tweets dataset is made up of two CSV files that have the text
    samples and their classes as columns. Despite the differences in the organization
    of these two datasets, fastai can ingest them and prepare them to train a deep
    learning model with just a few tweaks to the definition of the `TextDataLoaders`
    object. fastai's ability to easily ingest datasets in a variety of different formats
    isn't just useful for text datasets; it is useful for all kinds of datasets. As
    you will see in [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152),
    *Training Models with Visual Data*, we really benefit from this ability when we
    deal with image datasets, which have many different kinds of organization.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '`IMDb`数据集由成千上万的单独文本文件组成，这些文件分布在一个复杂的目录结构中，目录编码了每个文本文件的类别。相比之下，Covid相关的推文数据集由两个CSV文件组成，其中列包含文本样本及其类别。尽管这两个数据集的组织方式不同，fastai仍然能够加载它们并通过对`TextDataLoaders`对象定义进行少许调整，准备好用于训练深度学习模型。fastai能够轻松加载各种格式的数据集，不仅对文本数据集有用，对所有种类的数据集都很有帮助。正如你将在[*第6章*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152)《使用视觉数据训练模型》中看到的那样，处理图像数据集时，我们从这一能力中受益匪浅，因为图像数据集有许多不同的组织方式。'
- en: Test your knowledge
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试你的知识
- en: Now that you have worked through a number of extended examples of training fastai
    deep learning models with text datasets, you can try some variations to practice
    what you've learned.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经完成了多个使用文本数据集训练fastai深度学习模型的扩展示例，你可以尝试一些变化来练习你所学到的内容。
- en: Getting ready
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensure that you have followed the *Getting ready* steps from the *Training a
    deep learning text classifier with a standalone text dataset* section to prepare
    your Gradient environment and upload the Covid-related tweets dataset.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你已按照*使用独立文本数据集训练深度学习文本分类器*部分中的*准备工作*步骤准备好你的Gradient环境并上传与Covid相关的推文数据集。
- en: How to do it…
  id: totrans-376
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'You can follow the steps in this section to try some variations on the models
    that you trained with the Covid-related tweets dataset:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照本节中的步骤，尝试一些使用Covid相关推文数据集训练的模型的变化：
- en: 'Make a copy of the `text_standalone_dataset_lm.ipynb` notebook that you worked
    through in the *Training a deep learning language model with a standalone text
    dataset* recipe. Give your new copy of the notebook the following name: `text_standalone_dataset_lm_combo.ipynb`.'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制你在*使用独立文本数据集训练深度学习语言模型*部分中操作过的`text_standalone_dataset_lm.ipynb`笔记本。给你的新副本命名为：`text_standalone_dataset_lm_combo.ipynb`。
- en: 'In your new notebook, in addition to creating a dataframe for the train CSV
    `Corona_NLP_train.csv` file, create a dataframe for the test CSV `Corona_NLP_test.csv`
    file by adding a cell to the notebook that looks like this:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的新笔记本中，除了为训练用的 CSV 文件 `Corona_NLP_train.csv` 创建一个数据框（dataframe），还需要通过在笔记本中添加一个类似于下面的单元格来为测试用的
    CSV 文件 `Corona_NLP_test.csv` 创建一个数据框：
- en: '[PRE54]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Use the pandas `concat` function to combine the two dataframes into a new dataframe
    called `df_combo`:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 的 `concat` 函数将两个数据框合并成一个新的数据框，命名为 `df_combo`：
- en: '[PRE55]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now update the remainder of your new notebook to use `df_combo` instead of `df_train`
    and run the whole notebook to train a new language model. Do you notice any difference
    in the performance of the model?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在更新你新笔记本的其余部分，使用 `df_combo` 代替 `df_train`，并运行整个笔记本来训练一个新的语言模型。你注意到模型性能有什么不同吗？
- en: In most model training situations, you need to ensure that you don't use the
    test dataset to train the model. Can you think of why you could get away with
    using the test set to train a language model like this?
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数模型训练场景中，你需要确保不要使用测试数据集来训练模型。你能想出为什么可以用测试集来训练这样的语言模型吗？
- en: Congratulations! You have completed your review of training fastai deep learning
    models on text datasets using fastai.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了使用 fastai 在文本数据集上训练 fastai 深度学习模型的复习。
