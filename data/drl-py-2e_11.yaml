- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Actor-Critic Methods – A2C and A3C
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家方法——A2C 和 A3C
- en: So far, we have covered two types of methods for learning the optimal policy.
    One is the value-based method, and the other is the policy-based method. In the
    value-based method, we use the Q function to extract the optimal policy. In the
    policy-based method, we compute the optimal policy without using the Q function.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了两种学习最优策略的方法。一种是基于价值的方法，另一种是基于策略的方法。在基于价值的方法中，我们使用Q函数来提取最优策略。在基于策略的方法中，我们无需使用Q函数就能计算出最优策略。
- en: In this chapter, we will learn about another interesting method called the actor-critic
    method for finding the optimal policy. The actor-critic method makes use of both
    the value-based and policy-based methods. We will begin the chapter by understanding
    what the actor-critic method is and how it makes use of value-based and policy-based
    methods. We will acquire a basic understanding of actor-critic methods, and then
    we will learn about them in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一种有趣的方法，称为演员-评论家方法，用于寻找最优策略。演员-评论家方法结合了基于价值和基于策略的方法。我们将从了解演员-评论家方法是什么以及它如何结合基于价值和基于策略的方法开始。我们将获得演员-评论家方法的基本理解，之后会详细学习它们。
- en: Moving on, we will also learn how actor-critic differs from the policy gradient
    with baseline method, and we will learn the algorithm of the actor-critic method
    in detail. Next, we will understand what **Advantage Actor-Critic** (**A2C**)
    is, and how it makes use of the advantage function.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还将了解演员-评论家方法与带基线的策略梯度方法的不同，并将详细学习演员-评论家方法的算法。接下来，我们将理解**优势演员-评论家**（**A2C**）是什么，以及它如何利用优势函数。
- en: At the end of the chapter, we will learn about one of the most popularly used
    actor-critic algorithms, called **Asynchronous Advantage Actor-Critic** (**A3C**).
    We will understand what A3C is and the details of how it works along with its
    architecture.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，我们将学习一种最常用的演员-评论家算法，称为**异步优势演员-评论家**（**A3C**）。我们将理解A3C是什么，以及它的工作原理和架构。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of the actor-critic method
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论家方法概述
- en: Understanding the actor-critic method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解演员-评论家方法
- en: The actor-critic method algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 演员-评论家方法算法
- en: Advantage actor-critic (A2C)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优势演员-评论家（A2C）
- en: Asynchronous advantage actor-critic (A3C)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步优势演员-评论家（A3C）
- en: The architecture of asynchronous advantage actor-critic (A3C)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步优势演员-评论家（A3C）的架构
- en: Mountain car climbing using A3C
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用A3C进行山地车攀爬
- en: Let's begin the chapter by getting a basic understanding of the actor-critic
    method.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过首先获得演员-评论家方法的基本理解，来开始本章的内容。
- en: Overview of the actor-critic method
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家方法概述
- en: The actor-critic method is one of the most popular algorithms in deep reinforcement
    learning. Several modern deep reinforcement learning algorithms are designed based
    on actor-critic methods. The actor-critic method lies at the intersection of value-based
    and policy-based methods. That is, it takes advantage of both value-based and policy-based
    methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法是深度强化学习中最受欢迎的算法之一。许多现代深度强化学习算法都是基于演员-评论家方法设计的。演员-评论家方法位于基于价值方法和基于策略方法的交集处。也就是说，它结合了基于价值和基于策略的方法。
- en: In this section, without going into further detail, first, let's acquire a basic
    understanding of how the actor-critic method works and then, in the next section,
    we will get into more detail and understand the math behind the actor-critic method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先不深入细节，简单了解演员-评论家方法如何工作，接下来我们将深入探讨，并理解演员-评论家方法背后的数学原理。
- en: 'Actor-critic, as the name suggests, consists of two types of network—the actor
    network and the critic network. The role of the actor network is to find an optimal
    policy, while the role of the critic network is to evaluate the policy produced
    by the actor network. So, we can think of the critic network as a feedback network
    that evaluates and guides the actor network in finding the optimal policy, as
    *Figure 11.1* shows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，演员-评论家方法由两种网络组成——演员网络和评论家网络。演员网络的作用是找到最优策略，而评论家网络的作用是评估演员网络生成的策略。因此，我们可以将评论家网络视为一个反馈网络，它评估并引导演员网络找到最优策略，正如*图
    11.1*所示：
- en: '![](img/B15558_11_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_01.png)'
- en: 'Figure 11.1: The actor-critic network'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1：演员-评论家网络
- en: Okay, so what actually are the actor and critic networks? How do they work together
    and improve the policy? The actor network is basically the policy network, and
    it finds the optimal policy using a policy gradient method. The critic network
    is basically the value network, and it estimates the state value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么，演员和评论员网络到底是什么？它们如何协同工作并改进策略？演员网络基本上就是策略网络，它通过策略梯度方法找到最优策略。评论员网络基本上就是价值网络，它估计状态值。
- en: Thus, using its state value, the critic network evaluates the action produced
    by the actor network and sends its feedback to the actor. Based on the critic's
    feedback, the actor network then updates its parameter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，评论员网络利用其状态值评估由演员网络产生的动作，并将其反馈发送给演员。根据评论员的反馈，演员网络然后更新其参数。
- en: Thus, in the actor-critic method, we use two networks—the actor network (policy
    network), which computes the policy, and the critic network (value network), which
    evaluates the policy produced by the actor network by computing the value function
    (state values). Isn't this similar to something we just learned in the previous
    chapter?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在演员-评论员方法中，我们使用两个网络——演员网络（策略网络），它计算策略，和评论员网络（价值网络），它通过计算价值函数（状态值）来评估演员网络产生的策略。这不就是我们在上一章刚学到的东西吗？
- en: Yes! If you recall, it is similar to the policy gradient method with the baseline
    (REINFORCE with baseline) we learned in the previous chapter. Similar to REINFORCE
    with baseline, here also, we have an actor (policy network) and a critic (value
    network) network. However, actor-critic is NOT the same as REINFORCE with baseline.
    In the REINFORCE with baseline method, we learned that we use a value network
    as the baseline and it helps to reduce the variance in the gradient updates. In
    the actor-critic method as well, we use the critic to reduce variance in the gradient
    updates of the actor, but it also helps to improve the policy iteratively in an
    online fashion. The distinction between these two will be made clear in the next section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！如果你还记得，它类似于我们在上一章中学习的带基准的策略梯度方法（带基准的REINFORCE）。与带基准的REINFORCE类似，在这里，我们也有一个演员（策略网络）和一个评论员（价值网络）。然而，演员-评论员并不等同于带基准的REINFORCE。在带基准的REINFORCE方法中，我们了解到，我们使用价值网络作为基准，它有助于减少梯度更新中的方差。而在演员-评论员方法中，我们也使用评论员来减少演员的梯度更新中的方差，但它还帮助以在线方式迭代地改进策略。这两者之间的区别将在下一节中进一步阐明。
- en: Now that we have a basic understanding of the actor-critic method, in the next
    section, we will learn how the actor-critic method works in detail.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对演员-评论员方法有了基本的了解，在下一节中，我们将详细学习演员-评论员方法的工作原理。
- en: Understanding the actor-critic method
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解演员-评论员方法
- en: In the REINFORCE with baseline method, we learned that we have two networks—policy
    and value networks. The policy network finds the optimal policy, while the value
    network acts as a baseline and corrects the variance in the gradient updates.
    Similar to REINFORCE with baseline, the actor-critic method also consists of a
    policy network, known as the actor network, and the value network, known as the
    critic network.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在带基准的REINFORCE方法中，我们学习到，我们有两个网络——策略网络和价值网络。策略网络找到最优策略，而价值网络作为基准，修正梯度更新中的方差。类似于带基准的REINFORCE，演员-评论员方法也由策略网络（称为演员网络）和价值网络（称为评论员网络）组成。
- en: The fundamental difference between the REINFORCE with baseline method and the
    actor-critic method is that in the REINFORCE with baseline method, we update the
    parameter of the network at the end of an episode. But in the actor-critic method,
    we update the parameter of the network at every step of the episode. But why do
    we have to do this? What is the use of updating the network parameter at every
    step of the episode? Let's explore this in further detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 带基准的REINFORCE方法和演员-评论员方法的根本区别在于，在带基准的REINFORCE方法中，我们在每一集的结束时更新网络的参数。但在演员-评论员方法中，我们在每一步都更新网络的参数。但为什么我们必须这样做？在每一步更新网络参数有什么用呢？让我们进一步探讨这个问题。
- en: We can think of the REINFORCE with baseline method being similar to the **Monte
    Carlo** (**MC**) method, which we covered in *Chapter 4*, *Monte Carlo Methods*,
    and the actor-critic method being similar to the TD learning method, which we
    covered in *Chapter 5*, *Understanding Temporal Difference Learning*. So, first,
    let's recap these two methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为带基线的REINFORCE方法类似于我们在*第4章*“蒙特卡洛方法”中讨论的**蒙特卡洛**（**MC**）方法，而演员-评论员方法类似于我们在*第5章*“理解时序差分学习”中讨论的TD学习方法。因此，首先，让我们回顾一下这两种方法。
- en: In the MC method, to compute the value of a state, we generate some *N* trajectories
    and compute the value of a state as an average return of a state across the *N*
    trajectories. We learned that when the trajectory is too long, then the MC method
    will take us a lot of time to compute the value of the state and is also unsuitable
    for non-episodic tasks. So, we resorted to the TD learning method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在MC方法中，计算一个状态的值时，我们生成*N*条轨迹，并计算状态的值作为*N*条轨迹中该状态的平均回报。我们了解到，当轨迹过长时，MC方法会花费大量时间计算状态值，并且不适用于非序列任务。因此，我们转向了TD学习方法。
- en: In the TD learning method, we learned that instead of waiting until the end
    of the episode to compute the value of the state, we can make use of bootstrapping
    and estimate the value of the state as the sum of the immediate reward and the
    discounted value of the next state.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在TD学习方法中，我们了解到，我们可以利用引导法，而不是等到每集结束才计算状态值，估计状态值为即时奖励与下一个状态折扣值之和。
- en: Now, let's see how the MC and TD methods relate to the REINFORCE with baseline
    and actor-critic methods, respectively.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看MC和TD方法分别如何与带基线的REINFORCE方法和演员-评论员方法相关联。
- en: 'First, let''s recall what we learned in the REINFORCE with baseline method.
    In the REINFORCE with baseline method, we generate *N* number of trajectories
    using the policy ![](img/B15558_10_111.png) and compute the gradient as:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下我们在带基线的REINFORCE方法中学到的内容。在带基线的REINFORCE方法中，我们使用策略![](img/B15558_10_111.png)生成*N*条轨迹，并计算梯度如下：
- en: '![](img/B15558_10_163.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_163.png)'
- en: 'As we can observe, in order to compute the gradients, we need a complete trajectory.
    That is, as the following equation shows, in order to compute the gradient, we
    need to compute the return of the trajectory. We know that the return is the sum
    of rewards of the trajectory, so in order to compute the return (reward-to-go),
    first, we need a complete trajectory generated using the policy ![](img/B15558_10_120.png).
    So, we generate several trajectories using the policy ![](img/B15558_10_111.png)
    and then we compute the gradient:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，为了计算梯度，我们需要一个完整的轨迹。也就是说，正如以下方程所示，为了计算梯度，我们需要计算轨迹的回报。我们知道回报是轨迹中所有奖励的总和，因此为了计算回报（奖励-目标），首先，我们需要一个使用策略![](img/B15558_10_120.png)生成的完整轨迹。因此，我们使用策略![](img/B15558_10_111.png)生成多个轨迹，然后计算梯度：
- en: '![](img/B15558_11_10.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_10.png)'
- en: 'After computing the gradients, we update the parameter as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度后，我们更新参数如下：
- en: '![](img/B15558_11_005.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_005.png)'
- en: 'Instead of generating the complete trajectory and then computing the return,
    can we make use of bootstrapping, as we learned in TD learning? Yes! In the actor-critic
    method, we approximate the return by just taking the immediate reward and the
    discounted value of the next state as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像在TD学习中学到的那样，利用引导法，而不是生成完整轨迹后再计算回报吗？是的！在演员-评论员方法中，我们通过仅使用即时奖励和下一个状态的折扣值来近似回报，公式为：
- en: '![](img/B15558_11_006.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_006.png)'
- en: Where *r* is the immediate reward and ![](img/B15558_05_006.png) is the discounted
    value of the next state.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*r*是即时奖励，![](img/B15558_05_006.png)是下一个状态的折扣值。
- en: 'So, we can rewrite the policy gradient by replacing the return *R* by the bootstrap
    estimate, ![](img/B15558_05_008.png), as shown here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过将回报*R*替换为引导估计![](img/B15558_05_008.png)来重写策略梯度，如下所示：
- en: '![](img/B15558_11_009.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_009.png)'
- en: Now, we don't have to wait till the end of the episode to compute the return.
    Instead, we bootstrap, compute the gradient, and update the network parameter
    at every step of the episode.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不必等到每一集的结束才能计算回报。相反，我们采用引导法，在每个步骤中计算梯度并更新网络参数。
- en: 'The difference between how we compute the gradient and update the parameter
    of the policy network in REINFORCE with baseline and the actor-critic method is
    shown in *Figure 11.2*. As we can observe in REINFORCE with baseline, first we
    generate complete episodes (trajectories), and then we update the parameter of
    the network. Whereas, in the actor-critic method, we update the parameter of the
    network at every step of the episode:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在带基线的 REINFORCE 和演员-评论方法中计算梯度并更新策略网络参数的方式的区别如 *图 11.2* 所示。正如我们在带基线的 REINFORCE
    中看到的那样，首先生成完整的回合（轨迹），然后更新网络参数。而在演员-评论方法中，我们在每个回合的每一步更新网络参数：
- en: '![](img/B15558_11_02.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_02.png)'
- en: 'Figure 11.2: The difference between the REINFORCE with baseline and actor-critic
    methods'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：带基线的 REINFORCE 方法与演员-评论方法的区别
- en: 'Okay, what about the critic network (value network)? How can we update the
    parameter of the critic network? Similar to the actor network, we update the parameter
    of the critic network at every step of the episode. The loss of the critic network
    is the TD error, which is the difference between the target value of the state
    and the value of the state predicted by the network. The target value of the state
    can be computed as the sum of reward and the discounted value of the next state
    value. Thus, the loss of the critic network is expressed as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么评论网络（值网络）呢？我们如何更新评论网络的参数？与演员网络类似，我们在每个回合的每一步更新评论网络的参数。评论网络的损失是 TD 误差，即状态的目标值与网络预测的状态值之间的差异。状态的目标值可以通过奖励与下一个状态值的折现值之和来计算。因此，评论网络的损失表示为：
- en: '![](img/B15558_11_010.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_010.png)'
- en: Where ![](img/B15558_11_011.png) is the target value of the state and ![](img/B15558_11_012.png)
    is the predicted value of the state.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_11_011.png) 是状态的目标值，！[](img/B15558_11_012.png) 是网络预测的状态值。
- en: 'After computing the loss of the critic network, we compute gradients ![](img/B15558_11_014.png)
    and update the parameter ![](img/B15558_11_043.png) of the critic network at every
    step of the episode using gradient descent:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 计算评论网络的损失后，我们计算梯度！[](img/B15558_11_014.png)，并在每个回合的每一步使用梯度下降更新评论网络的参数！[](img/B15558_11_043.png)：
- en: '![](img/B15558_11_013.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_013.png)'
- en: Now that we have learned how the actor (policy network) and critic (value network)
    work in the actor-critic method; let's look at the algorithm of the actor-critic
    method in the next section for more clarity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了演员（策略网络）和评论（值网络）在演员-评论方法中的工作原理；接下来让我们看看演员-评论方法的算法，以便更清楚地理解。
- en: The actor-critic algorithm
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演员-评论算法
- en: 'The steps for the actor-critic algorithm are:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论算法的步骤如下：
- en: Initialize the actor network parameter ![](img/B15558_09_008.png) and the critic
    network parameter ![](img/B15558_11_016.png)
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化演员网络参数！[](img/B15558_09_008.png) 和评论网络参数！[](img/B15558_11_016.png)
- en: For *N* number of episodes, repeat *step 3*
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *N* 个回合，重复 *步骤 3*
- en: 'For each step in the episode, that is, for *t* = 0,. . . ., *T*-1:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每一步骤，即对于 *t* = 0, . . . , *T*-1：
- en: Select an action using the policy, ![](img/B15558_11_017.png)
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略选择动作，！[](img/B15558_11_017.png)
- en: Take the action *a*[t] in the state *s*[t], observe the reward *r*, and move
    to the next state ![](img/B15558_11_018.png)
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在状态 *s*[t] 中采取动作 *a*[t]，观察奖励 *r*，并转移到下一个状态！[](img/B15558_11_018.png)
- en: Compute the policy gradients:![](img/B15558_11_009.png)
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度：![](img/B15558_11_009.png)
- en: Update the actor network parameter ![](img/B15558_09_054.png) using gradient
    ascent:![](img/B15558_11_005.png)
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升更新演员网络参数！[](img/B15558_09_054.png)：![](img/B15558_11_005.png)
- en: Compute the loss of the critic network:![](img/B15558_11_010.png)
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算评论网络的损失：![](img/B15558_11_010.png)
- en: Compute gradients ![](img/B15558_11_014.png) and update the critic network parameter
    ![](img/B15558_11_023.png) using gradient descent:![](img/B15558_11_013.png)
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度！[](img/B15558_11_014.png) 并使用梯度下降更新评论网络参数！[](img/B15558_11_023.png)：![](img/B15558_11_013.png)
- en: As we can observe from the preceding algorithm, the actor network (policy network)
    parameter is being updated at every step of the episode. So, in each step of the
    episode, we select an action based on the updated policy while the critic network
    (value network) parameter is also getting updated at every step, and thus the
    critic also improves at evaluating the actor network at every step of the episode.
    While with the REINFORCE with baseline method, we only update the parameter of
    the network after generating the complete episodes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的算法中我们可以观察到，演员网络（策略网络）的参数在每个步骤都会被更新。因此，在每个步骤中，我们都会根据更新后的策略选择一个动作，而评论员网络（价值网络）的参数也会在每个步骤中更新，因此评论员在每个步骤中都会提高对演员网络的评估。而在REINFORCE与基准方法中，我们只会在生成完整的轨迹后才更新网络的参数。
- en: One more important difference we should note down between the REINFORCE with
    baseline and the actor-critic method is that, in the REINFORCE with baseline we
    use the full return of the trajectory whereas in the actor-critic method we use
    the bootstrapped return.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该注意到，REINFORCE与基准方法和演员-评论员方法之间的一个重要区别是，在REINFORCE与基准方法中，我们使用轨迹的完整回报，而在演员-评论员方法中，我们使用引导回报。
- en: The actor-critic algorithm we just learned is often referred to as the **Advantage
    Actor-Critic** (**A2C**). In the next section, we will look into the advantage
    function and learn why our algorithm is called the advantage actor-critic in more
    detail.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习的演员-评论员算法通常被称为**优势演员-评论员**（**A2C**）。在下一节中，我们将深入探讨优势函数，并更详细地了解为什么我们的算法被称为优势演员-评论员。
- en: Advantage actor-critic (A2C)
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势演员-评论员（A2C）
- en: 'Before moving on, first, let''s recall the advantage function. The advantage
    function is defined as the difference between the Q function and the value function.
    We can express the advantage function as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，首先让我们回顾一下优势函数。优势函数被定义为Q函数与价值函数之间的差异。我们可以将优势函数表示为：
- en: '![](img/B15558_11_025.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_025.png)'
- en: The advantage function tells us, in state *s*, how good action *a* is compared
    to the average actions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数告诉我们，在状态*s*下，动作*a*相对于平均动作的优劣。
- en: In A2C, we compute the policy gradient with the advantage function. So, first,
    let's see how to compute the advantage function. We know that the advantage function
    is the difference between the Q function and the value function, that is, *Q*(*s*,
    *a*) – *V*(*s*), so we can use two function approximators (neural networks), one
    for estimating the Q function and the other for estimating the value function.
    Then, we can subtract the values of these two networks to get the advantage value.
    But this will definitely not be an optimal method and, computationally, it will
    be expensive.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在A2C中，我们使用优势函数计算策略梯度。因此，首先让我们看看如何计算优势函数。我们知道，优势函数是Q函数与价值函数之间的差异，即*Q*(*s*, *a*)
    – *V*(*s*)，因此我们可以使用两个函数逼近器（神经网络），一个用于估计Q函数，另一个用于估计价值函数。然后，我们可以通过减去这两个网络的值来得到优势值。但这肯定不是一种最优方法，并且在计算上会很昂贵。
- en: 'So, we can approximate the Q value as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将Q值逼近为：
- en: '![](img/B15558_11_026.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_026.png)'
- en: But how can we approximate the Q value like this? Do you recall in *Chapter
    3*, *The Bellman Equation and Dynamic Programming*, in *The relationship between
    the value and Q functions* section, we learned we could derive the Q function
    from the value function? Using that identify, we can approximate the Q function
    as the sum of the immediate reward and the discounted value of the next state.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何像这样逼近Q值呢？你还记得在*第3章*中，*贝尔曼方程和动态规划*部分，在*价值函数与Q函数的关系*这一节中，我们学习过如何从价值函数推导出Q函数吗？利用这一公式，我们可以将Q函数逼近为即时奖励和下一个状态的折扣价值之和。
- en: 'Substituting the preceding Q value in the advantage function, equation (1),
    we can write the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面提到的Q值代入优势函数中的方程（1），我们可以写出如下公式：
- en: '![](img/B15558_11_027.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_027.png)'
- en: 'Thus, now we have the advantage function. We learned that in A2C, we compute
    the policy gradient with the advantage function. So, we can write this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经有了优势函数。我们了解到，在A2C中，我们使用优势函数计算策略梯度。因此，我们可以写出如下公式：
- en: '![](img/B15558_11_028.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_028.png)'
- en: 'Expanding the advantage function, we can write the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 展开优势函数，我们可以写出如下公式：
- en: '![](img/B15558_11_029.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_029.png)'
- en: 'As we can observe, our policy gradient is now computed using the advantage
    function:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们的策略梯度现在是使用优势函数来计算的：
- en: '![](img/B15558_11_11.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_11.png)'
- en: Now, check the preceding equation with how we computed the gradient in the previous
    section. We can observe that both are essentially the same. Thus, the A2C method
    is the same as what we learned in the previous section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查前面的公式与我们在上一部分中计算梯度的方法。我们可以观察到两者本质上是相同的。因此，A2C方法与我们在前一部分学到的内容是一样的。
- en: Asynchronous advantage actor-critic (A3C)
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步优势演员-评论员（A3C）
- en: Asynchronous advantage actor-critic, hereinafter referred to as A3C, is one
    of the popular actor-critic algorithms. The main idea behind the asynchronous
    advantage actor-critic method is that it uses several agents for learning in parallel
    and aggregates their overall experience.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 异步优势演员-评论员，以下简称A3C，是一种流行的演员-评论员算法。异步优势演员-评论员方法背后的主要思想是，它使用多个代理并行学习，并汇总它们的整体经验。
- en: In A3C, we will have two types of networks, one is a global network (global
    agent), and the other is the worker network (worker agent). We will have many
    worker agents, each worker agent uses a different exploration policy, and they
    learn in their own copy of the environment and collect experience. Then, the experience
    obtained from these worker agents is aggregated and sent to the global agent.
    The global agent aggregates the learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在A3C中，我们将有两种类型的网络，一种是全局网络（全局代理），另一种是工作网络（工作代理）。我们将有多个工作代理，每个工作代理使用不同的探索策略，并在它们自己的环境副本中进行学习并收集经验。然后，从这些工作代理获得的经验将被汇总并发送到全局代理。全局代理汇总这些学习。
- en: Now that we have a very basic idea of how A3C works, let's go into more detail.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对A3C的工作原理有了一个基本的了解，让我们深入细节。
- en: The three As
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三个A
- en: Before diving in, let's first learn what the three A's in A3C signify.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨之前，我们先了解一下A3C中的三个A代表什么。
- en: '**Asynchronous**: Asynchronous implies the way A3C works. That is, instead
    of having a single agent that tries to learn the optimal policy, here, we have
    multiple agents that interact with the environment. Since we have multiple agents
    interacting with the environment at the same time, we provide copies of the environment
    to every agent so that each agent can then interact with their own copy of the
    environment. So, all these multiple agents are called worker agents and we have
    a separate agent called the global agent. All the worker agents report to the
    global agent asynchronously and the global agent aggregates the learning.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**异步**：异步意味着A3C的工作方式。也就是说，A3C并不是只有一个代理尝试学习最优策略，而是有多个代理与环境进行交互。由于我们有多个代理同时与环境进行交互，因此我们为每个代理提供环境副本，这样每个代理就可以与它们自己的环境副本进行交互。因此，这些多个代理被称为工作代理，我们有一个单独的代理称为全局代理。所有工作代理异步地向全局代理报告，全局代理汇总这些学习。'
- en: '**Advantage**:We have already learned what an advantage function is in the
    previous section. An advantage function can be defined as the difference between
    the Q function and the value function.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**优势**：我们已经在前一部分学过什么是优势函数。优势函数可以定义为Q函数与价值函数之间的差异。'
- en: '**Actor-critic**: Each of the worker networks (worker agents) and the global
    network (global agent) basically follow an actor-critic architecture. That is,
    each of the agents consists of an actor network for estimating the policy and
    the critic network for evaluating the policy produced by the actor network.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**演员-评论员**：每个工作网络（工作代理）和全局网络（全局代理）基本上遵循演员-评论员架构。也就是说，每个代理由一个演员网络用于估计策略和一个评论员网络用于评估由演员网络生成的策略组成。'
- en: Now, let us move on to the architecture of A3C and understand how A3C works
    in detail.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续了解A3C的架构，并详细理解A3C是如何工作的。
- en: The architecture of A3C
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A3C的架构
- en: 'Now, let''s understand the architecture of A3C. The architecture of A3C is
    shown in the following figure:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解A3C的架构。A3C的架构如下图所示：
- en: '![](img/B15558_11_03.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_03.png)'
- en: 'Figure 11.3: The architecture of A3C'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3：A3C的架构
- en: As we can observe from the preceding figure, we have multiple worker agents
    and each worker agent interacts with their own copies of the environment and collects
    experience. We can also observe that each worker agent follows an actor-critic
    architecture. So, the worker agents compute the actor network loss (policy loss)
    and critic network loss (value loss).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从前面的图中看到的，我们有多个工作代理，每个工作代理与它们自己的环境副本进行交互并收集经验。我们还可以看到，每个工作代理遵循演员-评论员架构。因此，工作代理计算演员网络损失（策略损失）和评论员网络损失（价值损失）。
- en: 'In the previous section, we learned that our actor network is updated by computing
    the policy gradient:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们学到了如何通过计算策略梯度来更新我们的演员网络：
- en: '![](img/B15558_11_030.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_030.png)'
- en: 'Thus, the actor loss is basically the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，演员损失基本上是以下内容：
- en: '![](img/B15558_11_045.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_045.png)'
- en: 'As we can observe, actor loss is the product of log probability of the action
    and the TD error. Now, we add a new term to our actor loss called the entropy
    (measure of randomness) of the policy and redefine the actor loss as:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，演员损失是动作的对数概率与 TD（时间差分）误差的乘积。现在，我们在演员损失中加入一个新项，即策略的熵（随机性度量），并将演员损失重新定义为：
- en: '![](img/B15558_11_031.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_031.png)'
- en: Where ![](img/B15558_11_032.png) denotes the entropy of the policy. Adding the
    entropy of the policy promotes sufficient exploration, and the parameter ![](img/B15558_06_030.png)
    is used to control the significance of the entropy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_11_032.png) 表示策略的熵。加入策略的熵可以促进足够的探索，参数 ![](img/B15558_06_030.png)
    用于控制熵的显著性。
- en: The critic loss is just the mean squared TD error.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 评论家损失仅仅是均方 TD 误差。
- en: After computing the losses of the actor and critic networks, worker agents compute
    the gradients of the loss and then they send those gradients to the global agent.
    That is, the worker agents compute the gradients and their gradients are asynchronously
    accumulated to the global agent. The global agent updates their parameters using
    the asynchronously received gradients from the worker agents. Then, the global
    agent sends the updated parameter periodically to the worker agents, so now the
    worker agents will get updated.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完演员和评论家网络的损失后，工作代理计算损失的梯度，然后将这些梯度发送给全局代理。也就是说，工作代理计算梯度，并将其梯度异步地累积到全局代理中。全局代理使用从工作代理异步接收到的梯度更新其参数。然后，全局代理周期性地将更新后的参数发送给工作代理，这样工作代理就会得到更新。
- en: In this way, each worker agent computes loss, calculates gradients, and sends
    those gradients to the global agent asynchronously. Thus, the global agent parameter
    is updated by gradients received from the worker agents. Then, the global agent
    sends the updated parameter to the worker agents periodically.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，每个工作代理计算损失、计算梯度，并异步地将这些梯度发送给全局代理。然后，全局代理通过从工作代理接收到的梯度更新其参数。接着，全局代理周期性地将更新后的参数发送给工作代理。
- en: Since we have many worker agents interacting with their own copies of the environment
    and aggregating the information to the global network, there will be low to no
    correlation between the experiences.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有许多工作代理与它们自己的环境副本进行交互，并将信息汇聚到全局网络，因此经验之间几乎没有相关性。
- en: 'The steps involved in A3C are:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 涉及的步骤如下：
- en: The worker agent interacts with their own copies of the environment.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 工作代理与它们自己的环境副本进行交互。
- en: Each worker follows a different policy and collects the experience.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个工作代理遵循不同的策略并收集经验。
- en: Next, the worker agents compute the losses of the actor and critic networks.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，工作代理计算演员和评论家网络的损失。
- en: After computing the loss, they calculate gradients of the loss, and send those gradients
    to the global agent asynchronously.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算完损失后，他们计算损失的梯度，并异步地将这些梯度发送给全局代理。
- en: The global agent updates their parameters with the gradients received from the
    worker agents.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全局代理使用从工作代理接收到的梯度更新其参数。
- en: Now, the updated parameter from the global agent will be sent to the worker agents
    periodically.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，全局代理更新后的参数将周期性地发送给工作代理。
- en: We repeat the preceding steps for several iterations to find the optimal policy.
    To get a clear understanding of how A3C works, in the next section, we will learn
    how to implement it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复上述步骤进行若干次迭代，以找到最优策略。为了更清晰地了解 A3C 是如何工作的，在下一节中，我们将学习如何实现它。
- en: Mountain car climbing using A3C
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 A3C 攀爬山地车
- en: 'Let''s implement the A3C algorithm for the mountain car climbing task. In the
    mountain car climbing environment, a car is placed between two mountains and the goal
    of the agent is to drive up the mountain on the right. But the problem is, the agent
    can''t drive up the mountain in one pass. So, the agent has to drive back and forth
    to build momentum to drive up the mountain on the right. A high reward will be
    assigned if our agent spends less energy while driving up. *Figure 11.4* shows
    the mountain car environment:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为山地车爬坡任务实现A3C算法。在山地车爬坡环境中，一辆车被放置在两座山之间，代理的目标是驱车爬上右侧的山。但是问题是，代理不能一次性爬上山。所以，代理必须来回驾驶以积累动能，才能爬上右侧的山。如果代理在上山时消耗较少的能量，则会分配更高的奖励。*图11.4*展示了山地车环境：
- en: '![](img/B15558_11_04.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_04.png)'
- en: 'Figure 11.4: The mountain car environment'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4：山地车环境
- en: The code used in this section is adapted from the open source implementation
    of A3C ([https://github.com/stefanbo92/A3C-Continuous](https://github.com/stefanbo92/A3C-Continuous))
    provided by Stefan Boschenriedter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用的代码改编自Stefan Boschenriedter提供的A3C开源实现（[https://github.com/stefanbo92/A3C-Continuous](https://github.com/stefanbo92/A3C-Continuous)）。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE0]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Creating the mountain car environment
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建山地车环境
- en: 'Let''s create a mountain car environment using Gym. Note that our mountain
    car environment is a continuous environment, meaning that our action space is
    continuous:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Gym创建一个山地车环境。请注意，我们的山地车环境是连续环境，意味着我们的动作空间是连续的：
- en: '[PRE1]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Get the state shape of the environment:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的状态形状：
- en: '[PRE2]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the action shape of the environment:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 获取环境的动作形状：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that we created the continuous mountain car environment, and thus our
    action space consists of continuous values. So, we get the bounds of our action
    space:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们创建了连续的山地车环境，因此我们的动作空间由连续值组成。所以，我们获取动作空间的边界：
- en: '[PRE4]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the variables
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义变量
- en: Now, let's define some of the important variables.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一些重要的变量。
- en: 'Define the number of workers as the number of CPUs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 定义工作者的数量为CPU的数量：
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the number of episodes:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 定义回合数：
- en: '[PRE6]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the number of time steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 定义时间步长的数量：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the global network (global agent) scope:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 定义全球网络（全球代理）作用域：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the time step at which we want to update the global network:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们希望更新全球网络的时间步长：
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the discount factor, ![](img/B15558_03_190.png):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 定义折扣因子， ![](img/B15558_03_190.png)：
- en: '[PRE10]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the beta value:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 定义beta值：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the directory where we want to store the logs:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 定义我们希望存储日志的目录：
- en: '[PRE12]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Defining the actor-critic class
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义演员-评论家类
- en: We learned that in A3C, both the global and worker agents follow the actor-critic
    architecture. So, let's define the class called `ActorCritic`, where we will implement
    the actor-critic algorithm. For a clear understanding, let's look into the code
    line by line. You can also access the complete code from the book's GitHub repository.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在A3C中，全球网络和工作代理都遵循演员-评论家架构。所以，让我们定义一个名为`ActorCritic`的类，在其中实现演员-评论家算法。为了清晰理解，让我们逐行查看代码。你也可以从本书的GitHub仓库中获取完整的代码。
- en: '[PRE13]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Defining the init method:'
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义初始化方法：
- en: 'First, let''s define the init method:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义初始化方法：
- en: '[PRE14]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Initialize the TensorFlow session:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化TensorFlow会话：
- en: '[PRE15]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the actor network optimizer as RMS prop:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将演员网络优化器定义为RMS prop：
- en: '[PRE16]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the critic network optimizer as RMS prop:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 将评论家网络优化器定义为RMS prop：
- en: '[PRE17]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If the scope is the global network (global agent):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果作用域是全球网络（全球代理）：
- en: '[PRE18]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define the placeholder for the state:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Build the global network (global agent) and get the actor and critic parameters:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 构建全球网络（全球代理），并获取演员和评论家参数：
- en: '[PRE20]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If the network is not the global network, then:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络不是全球网络，则：
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Define the placeholder for the state:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 定义状态的占位符：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We learned that our environment is the continuous environment, so our actor
    network (policy network) returns the mean and variance of the action and then
    we build the action distribution out of this mean and variance and select the
    action based on this action distribution.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到我们的环境是连续环境，因此我们的演员网络（策略网络）返回动作的均值和方差，然后我们根据这些均值和方差构建动作分布，并基于该分布选择动作。
- en: 'Define the placeholder to obtain the action distribution:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 定义占位符以获取动作分布：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the placeholder for the target value:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 定义目标值的占位符：
- en: '[PRE24]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Build the worker network (worker agent) and get the mean and variance of the
    action, the value of the state, and the actor and critic network parameters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 构建工作网络（工作代理），并获取动作的均值和方差、状态的值以及演员和评论家网络的参数：
- en: '[PRE25]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Compute the TD error, which is the difference between the target value of the
    state and its predicted value:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 计算时间差误差（TD误差），它是状态的目标值与预测值之间的差：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, let''s define the critic network loss:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义评论家网络的损失：
- en: '[PRE27]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a normal distribution based on the mean and variance of the action:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 基于动作的均值和方差创建正态分布：
- en: '[PRE28]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, let''s define the actor network loss. We learned that the loss of the
    actor network is defined as:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义演员网络的损失。我们了解到，演员网络的损失定义为：
- en: '![](img/B15558_11_041.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_041.png)'
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Compute the log probability of the action:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 计算动作的对数概率：
- en: '[PRE30]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Define the entropy of the policy:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 定义策略的熵：
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Compute the actor network loss:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 计算演员网络损失：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Select the action based on the normal distribution:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 基于正态分布选择动作：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Compute the gradients of the actor and critic network losses of the worker
    agent (local agent):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 计算工作代理（本地代理）演员和评论家网络损失的梯度：
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let''s perform the sync operation:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们执行同步操作：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After computing the gradients of the losses of the actor and critic networks,
    the worker agents send (push) those gradients to the global agent:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完演员网络和评论家网络的损失的梯度后，工作代理将这些梯度发送（推送）到全局代理：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The global agent updates their parameters with the gradients received from
    the worker agents (local agents). Then, the worker agents pull the updated parameters
    from the global agent:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 全局代理使用从工作代理（本地代理）接收到的梯度更新他们的参数。然后，工作代理从全局代理拉取更新后的参数：
- en: '[PRE37]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Building the network
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建网络
- en: 'Now, let''s define the function for building the actor-critic network:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义构建演员-评论家网络的函数：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Initialize the weight:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化权重：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Define the actor network, which returns the mean and variance of the action:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 定义演员网络，它返回动作的均值和方差：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define the critic network, which returns the value of the state:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 定义评论家网络，它返回状态的值：
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Get the parameters of the actor and critic networks:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 获取演员和评论家网络的参数：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Return the mean and variance of the action produced by the actor network, the
    value of the state computed by the critic network, and the parameters of the actor
    and critic networks:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 返回由演员网络生成的动作的均值和方差，评论家网络计算的状态值，以及演员和评论家网络的参数：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Updating the global network
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新全局网络
- en: 'Let''s define a function called `update_global` to update the parameters of
    the global network with the gradients of loss computed by the worker networks,
    that is, the push operation:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`update_global`的函数，用于通过工作网络计算的损失梯度更新全局网络的参数，即推送操作：
- en: '[PRE44]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Updating the worker network
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新工作网络
- en: 'We also define a function called `pull_from_global` to update the parameters
    of the worker networks by pulling from the global network, that is, the pull operation:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个名为`pull_from_global`的函数，用于通过从全局网络拉取来更新工作网络的参数，即拉取操作：
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Selecting the action
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择动作
- en: 'Define a function called `select_action` to select the action:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`select_action`的函数来选择动作：
- en: '[PRE46]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Defining the worker class
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义工作类
- en: 'Let''s define the class called `Worker`, where we will implement the worker
    agent:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`Worker`的类，在这里我们将实现工作代理：
- en: '[PRE47]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Defining the init method
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义初始化方法
- en: 'First, let''s define the `init` method:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义`init`方法：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We learned that each worker agent works with their own copies of the environment.
    So, let''s create a mountain car environment:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，每个工作代理与他们自己的环境副本一起工作。所以，让我们创建一个山地车环境：
- en: '[PRE49]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define the name of the worker:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 定义工作代理的名称：
- en: '[PRE50]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Create an object for our `ActorCritic` class:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 创建我们的`ActorCritic`类的对象：
- en: '[PRE51]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Initialize a TensorFlow session:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 TensorFlow 会话：
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Define a function called `work` for the worker to learn:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为`work`的函数供工作代理学习：
- en: '[PRE53]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Initialize the time step:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化时间步长：
- en: '[PRE54]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Initialize a list to store the states, actions, and rewards:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个列表来存储状态、动作和奖励：
- en: '[PRE55]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'When the global episodes are less than the number of episodes and the coordinator
    is active:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当全局回合数小于回合总数且协调器处于激活状态时：
- en: '[PRE56]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境来初始化状态：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Initialize the return:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化返回值：
- en: '[PRE58]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For each step in the environment:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 对于环境中的每一步：
- en: '[PRE59]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Render the environment of only the worker 0:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 仅渲染工作代理0的环境：
- en: '[PRE60]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Select the action:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 选择动作：
- en: '[PRE61]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Perform the selected action:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所选动作：
- en: '[PRE62]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Set `done` to `True` if we have reached the final step of the episode `else`
    set to `False`:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已经到达回合的最后一步，则将 `done` 设置为 `True`，否则设置为 `False`：
- en: '[PRE63]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Update the return:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 更新回报：
- en: '[PRE64]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Store the state, action, and reward in the lists:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态、动作和奖励存储到列表中：
- en: '[PRE65]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, let''s update the global network. If `done` is `True`, then set the value
    of the next state to `0` `else` compute the value of the next state:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更新全局网络。如果 `done` 为 `True`，则将下一个状态的值设置为 `0`，否则计算下一个状态的值：
- en: '[PRE66]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Compute the target value, which is ![](img/B15558_11_042.png):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 计算目标值，公式为 ![](img/B15558_11_042.png)：
- en: '[PRE67]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Reverse the target value:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 反转目标值：
- en: '[PRE68]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Stack the state, action, and target value:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠状态、动作和目标值：
- en: '[PRE69]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Define the feed dictionary:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 定义馈送字典：
- en: '[PRE70]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Update the global network:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 更新全局网络：
- en: '[PRE71]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Empty the lists:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 清空列表：
- en: '[PRE72]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Update the worker network by pulling the parameters from the global network:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从全局网络拉取参数来更新工作网络：
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Update the state to the next state and increment the total step:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态更新到下一个状态，并增加总步数：
- en: '[PRE74]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Update the global rewards:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 更新全局奖励：
- en: '[PRE75]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Training the network
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: 'Now, let''s start training the network. Initialize the global rewards list
    and also initialize the global episodes counter:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练网络。初始化全局奖励列表，并初始化全局回合计数器：
- en: '[PRE76]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Start the TensorFlow session:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话：
- en: '[PRE77]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Create a global agent:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 创建全局代理：
- en: '[PRE78]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Create *n* number of worker agents:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 *n* 个工作代理：
- en: '[PRE79]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Create the TensorFlow coordinator:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 TensorFlow 协调器：
- en: '[PRE80]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有 TensorFlow 变量：
- en: '[PRE81]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Store the TensorFlow computational graph in the log directory:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 将 TensorFlow 计算图存储在日志目录中：
- en: '[PRE82]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, run the worker threads:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行工作线程：
- en: '[PRE83]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: For a better understanding of the A3C architecture, let's take a look at the
    computational graph of A3C in the next section.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 A3C 架构，让我们在下一节中查看 A3C 的计算图。
- en: Visualizing the computational graph
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化计算图
- en: 'As we can observe, we have four worker agents (worker networks) and one global
    agent (global network):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，我们有四个工作代理（工作网络）和一个全局代理（全局网络）：
- en: '![](img/B15558_11_05.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_05.png)'
- en: 'Figure 11.5: A computation graph of A3C'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：A3C 的计算图
- en: 'Let''s take a look at the architecture of the worker agent. As we can observe,
    our worker agents follow the actor-critic architecture:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看工作代理的架构。正如我们所观察到的，工作代理遵循演员-评论家架构：
- en: '![](img/B15558_11_06.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_06.png)'
- en: 'Figure 11.6: A computation graph of A3C with the W_0 node expanded'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：A3C 的计算图，展示了展开的 W_0 节点
- en: 'Now, let''s examine the sync node. As *Figure 11.7* shows, we have two operations
    in the sync node, called *push* and *pull*:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查同步节点。正如 *图 11.7* 所示，我们在同步节点中有两个操作，分别是 *推送* 和 *拉取*：
- en: '![](img/B15558_11_07.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_07.png)'
- en: 'Figure 11.7: A computation graph of A3C showing the push and pull operations
    of the sync node'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7：A3C 的计算图，展示了同步节点的推送和拉取操作
- en: 'After computing the gradients of the losses of the actor and critic networks,
    the worker agent pushes those gradients to the global agent:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了演员和评论家网络的损失的梯度之后，工作代理将这些梯度推送到全局代理：
- en: '![](img/B15558_11_08.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_08.png)'
- en: 'Figure 11.8: A computation graph of A3C—worker agents push their gradients
    to the global agent'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：A3C 的计算图——工作代理将其梯度推送到全局代理
- en: 'The global agent updates their parameters with the gradients received from
    the worker agents. Then, the worker agents pull the updated parameters from the
    global agent:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 全局代理通过从工作代理接收到的梯度来更新其参数。然后，工作代理从全局代理拉取更新后的参数：
- en: '![](img/B15558_11_09.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_11_09.png)'
- en: 'Figure 11.9: A computation graph of A3C – worker agents pull updated parameters
    from the global agent'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9：A3C 的计算图——工作代理从全局代理拉取更新后的参数
- en: Now that we have learned how A3C works, in the next section, let's revisit the
    A2C method.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 A3C 的工作原理，在接下来的章节中，让我们回顾一下 A2C 方法。
- en: A2C revisited
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 再次审视 A2C
- en: We can design our A2C algorithm with many worker agents, just like the A3C algorithm.
    However, unlike A3C, A2C is a synchronous algorithm, meaning that in A2C, we can
    have multiple worker agents, each interacting with their own copies of the environment,
    and all the worker agents perform synchronous updates, unlike A3C, where the worker
    agents perform asynchronous updates.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像 A3C 算法一样设计具有多个工作代理的 A2C 算法。然而，不同于 A3C，A2C 是一个同步算法，意味着在 A2C 中，我们可以有多个工作代理，每个工作代理与它们自己的环境副本进行交互，且所有工作代理执行同步更新，而
    A3C 中工作代理执行的是异步更新。
- en: That is, in A2C, each worker agent interacts with the environment, computes
    losses, and calculates gradients. However, it won't send those gradients to the
    global network independently. Instead, it waits for all other worker agents to
    finish their work and then updates the weights to the global network in a synchronous
    fashion. Performing synchronous weight updates reduces the inconsistency introduced
    by A3C.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，在A2C中，每个工作代理与环境互动，计算损失并计算梯度。然而，它不会独立地将这些梯度发送到全局网络。相反，它会等待所有其他工作代理完成工作，然后以同步的方式将权重更新到全局网络。进行同步的权重更新可以减少A3C引入的不一致性。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started the chapter by understanding what the actor-critic method is. We
    learned that in the actor-critic method, the actor computes the optimal policy,
    and the critic evaluates the policy computed by the actor network by estimating
    the value function. Next, we learned how the actor-critic method differs from
    the policy gradient method with the baseline.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过理解演员-评论员方法是什么开始了本章的学习。我们了解到，在演员-评论员方法中，演员计算最优策略，而评论员通过估计价值函数来评估演员网络计算的策略。接下来，我们学习了演员-评论员方法与带基线的策略梯度方法的不同之处。
- en: We learned that in the policy gradient method with the baseline, first, we generate
    complete episodes (trajectories), and then we update the parameter of the network.
    Whereas, in the actor-critic method, we update the parameter of the network at
    every step of the episode. Moving forward, we learned what the advantage actor-critic
    algorithm is and how it uses the advantage function in the gradient update.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在带基线的策略梯度方法中，首先生成完整的回合（轨迹），然后更新网络的参数。而在演员-评论员方法中，我们在每一步回合中更新网络的参数。接下来，我们了解了优势演员-评论员算法是什么，以及它如何在梯度更新中使用优势函数。
- en: At the end of the chapter, we learned about another interesting actor-critic
    algorithm, called asynchronous advantage actor-critic method. We learned that
    A3C consists of several worker agents and one global agent. All the worker agents
    send their gradients to the global agent asynchronously and then the global agent
    updates their parameters with gradients received from the worker agents. After
    updating the parameters, the global agent sends the updated parameters to the
    worker agents periodically.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，我们了解了另一种有趣的演员-评论员算法，称为异步优势演员-评论员方法。我们了解到，A3C由多个工作代理和一个全局代理组成。所有工作代理异步地将它们的梯度发送到全局代理，然后全局代理使用从工作代理接收到的梯度更新它们的参数。更新参数后，全局代理会定期将更新后的参数发送给工作代理。
- en: Hence, in this chapter, we learned about two interesting actor-critic algorithms
    – A2C and A3C. In the next chapter, we will understand several state-of-the-art
    actor-critic algorithms, including DDPG, TD3, and SAC.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们了解了两种有趣的演员-评论员算法——A2C和A3C。在下一章中，我们将了解几种最先进的演员-评论员算法，包括DDPG、TD3和SAC。
- en: Questions
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s assess our understanding of the actor-critic method by answering the
    following questions:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们对演员-评论员方法的理解：
- en: What is the actor-critic method?
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员-评论员方法是什么？
- en: What is the role of the actor and critic networks?
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员网络和评论员网络的作用是什么？
- en: How does the actor-critic method differ from the policy gradient with the baseline
    method?
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员-评论员方法与基线策略梯度方法有何不同？
- en: What is the gradient update equation of the actor network?
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员网络的梯度更新方程是什么？
- en: How does A2C work?
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A2C是如何工作的？
- en: What does *asynchronous* mean in A3C?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*异步*在A3C中意味着什么？'
- en: How does A2C differ from A3C?
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A2C与A3C有何不同？
- en: Further reading
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'To learn more, refer to the following paper:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多，请参考以下论文：
- en: '*Asynchronous Methods for Deep Reinforcement Learning*, by Volodymyr Mnih et
    al.: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度强化学习的异步方法*，Volodymyr Mnih等著：[https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
