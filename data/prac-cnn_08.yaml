- en: 'GAN: Generating New Images with CNN'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN：使用CNN生成新图像
- en: 'Generally, a neural network needs labeled examples to learn effectively. Unsupervised
    learning approaches to learn from unlabeled data have not worked very well. A **generative
    adversarial network**, or simply a **GAN**, is part of an unsupervised learning
    approach but based on differentiable generator networks. GANs were first invented
    by Ian Goodfellow and others in 2014\. Since then they have become extremely popular.
    This is based on game theory and has two players or networks: a generator network
    and b) a discriminator network, both competing against each other. This dual network
    game theory-based approach vastly improved the process of learning from unlabeled
    data. The generator network produces fake data and passes it to a discriminator.
    The discriminator network also sees real data and predicts whether the data it
    receives is fake or real. So, the generator is trained so that it can easily produce
    data that is very close to real data in order to fool the discriminator network.
    The discriminator network is trained to classify which data is real and which
    data is fake. So, eventually, a generator network learns to produce data that
    is very, very close to real data. GANs are going to be widely popular in the music
    and arts domains.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络需要带标签的示例才能有效学习。无监督学习方法从未标注的数据中学习的效果并不好。**生成对抗网络**，简称**GAN**，是一种无监督学习方法，但基于可微分的生成器网络。GAN最初由Ian
    Goodfellow等人于2014年发明。从那时起，它们变得非常流行。这是基于博弈论的，有两个参与者或网络：生成器网络和判别器网络，它们相互竞争。这种基于双网络的博弈论方法大大改善了从未标注数据中学习的过程。生成器网络生成伪造数据并传递给判别器。判别器网络也看到真实数据并预测它收到的数据是假的还是真的。因此，生成器被训练成可以轻松生成非常接近真实数据的数据，从而欺骗判别器网络。判别器网络被训练成分类哪些数据是真实的，哪些数据是假的。所以，最终，生成器网络学会生成非常非常接近真实数据的数据。GAN将在音乐和艺术领域广泛流行。
- en: According to Goodfellow, "*You can think of generative models as giving Artificial
    Intelligence a form of imagination*."
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Goodfellow的说法，"*你可以把生成模型看作是赋予人工智能一种想象力的形式*。"
- en: 'The following are a couple of examples of GANs:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些GAN的示例：
- en: Pix2pix
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pix2pix
- en: CycleGAN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN
- en: Pix2pix - Image-to-Image translation GAN
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pix2pix - 图像到图像翻译GAN
- en: 'This network uses a **conditional generative adversarial network** (**cGAN**)
    to learn mapping from the input and output of an image. Some of the examples that
    can be done from the original paper are as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络使用**条件生成对抗网络**（**cGAN**）来学习图像的输入和输出之间的映射。以下是原始论文中可以完成的一些示例：
- en: '![](img/5bd742bd-4360-4fb0-acaf-595d92808a92.jpeg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd742bd-4360-4fb0-acaf-595d92808a92.jpeg)'
- en: Pix2pix examples of cGANs
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Pix2pix的cGAN示例
- en: In the handbags example, the network learns how to color a black and white image.
    Here, the training dataset has the input image in black and white and the target
    image is the color version.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在手袋示例中，网络学习如何为黑白图像上色。在这里，训练数据集中的输入图像是黑白的，目标图像是彩色版。
- en: CycleGAN
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CycleGAN
- en: 'CycleGAN is also an image-to-image translator but without input/output pairs.
    For example, to generate photos from paintings, convert a horse image into a zebra
    image:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN也是一种图像到图像翻译器，但没有输入/输出对。例如，从画作中生成照片，将马的图像转换成斑马图像：
- en: '![](img/60b9b3b9-a03d-4a53-acbf-49563705690e.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60b9b3b9-a03d-4a53-acbf-49563705690e.jpeg)'
- en: In a discriminator network, use of dropout is important. Otherwise, it may produce
    a poor result.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在判别器网络中，使用dropout非常重要。否则，它可能会产生较差的结果。
- en: The generator network takes random noise as input and produces a realistic image
    as output. Running a generator network for different kinds of random noise produces
    different types of realistic images. The second network, which is known as the **discriminator
    network**, is very similar to a regular neural net classifier. This network is
    trained on real images, although training a GAN is quite different from a supervised
    training method. In supervised training, each image is labeled first before being
    shown to the model. For example, if the input is a dog image, we tell the model
    this is a dog. In case of a generative model, we show the model a lot of images
    and ask it to make more such similar images from the same probability distribution.
    Actually, the second discriminator network helps the generator network to achieve
    this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络以随机噪声作为输入，并产生一个真实感的图像作为输出。对不同类型的随机噪声运行生成器网络会产生不同种类的真实图像。第二个网络，称为**判别器网络**，与常规的神经网络分类器非常相似。该网络在真实图像上进行训练，尽管训练GAN与监督训练方法有很大不同。在监督训练中，每个图像在显示给模型之前都会先被标注。例如，如果输入是一张狗的图像，我们会告诉模型这是狗。而在生成模型中，我们会向模型展示大量图像，并要求它从相同的概率分布中生成更多类似的图像。实际上，第二个判别器网络帮助生成器网络实现这一目标。
- en: 'The discriminator outputs the probability that the image is real or fake from
    the generator network. In other words, it tries to assign a probability close
    to 1 for a real image and a probability close to 0 for fake images. Meanwhile,
    the generator does the opposite. It is trained to output images that will have
    a probability close to 1 by the discriminator. Over time, the generator produces
    more realistic images and fools the discriminator:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器输出图像是真实的还是生成器生成的假的概率。换句话说，它试图给真实图像分配一个接近1的概率，而给假的图像分配一个接近0的概率。与此同时，生成器则做相反的事情。它被训练成输出能被判别器判定为接近1的图像。随着时间的推移，生成器会生成更真实的图像，从而欺骗判别器：
- en: '![](img/31c3ec0c-b8e9-407f-9e05-9764f2b33178.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31c3ec0c-b8e9-407f-9e05-9764f2b33178.png)'
- en: Training a GAN model
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练GAN模型
- en: 'Most machine learning models explained in earlier chapters are based on optimization,
    that is, we minimize the cost function over its parameter space. GANs are different
    because of two networks: the generator G and the discriminator D. Each has its
    own cost. An easy way to visualize GAN is the cost of the discriminator is the
    negative of the cost of the generator. In GAN, we can define a value function
    that the generator has to minimize and the discriminator has to maximize. The
    training process for a generative model is quite different from the supervised
    training method. GAN is sensitive to the initial weights. So we need to use batch
    normalization. Batch normalization makes the model stable, besides improving performance.
    Here, we train two models, the generative model and the discriminative model,
    simultaneously. Generative model G captures data distribution and discriminative
    model D estimates the probability of a sample that came from training data rather
    than G.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中解释的大多数机器学习模型都是基于优化的，也就是说，我们在其参数空间中最小化代价函数。生成对抗网络（GAN）则不同，因为它包含了两个网络：生成器G和判别器D。每个网络都有自己的代价函数。一个简单的方式来理解GAN是，判别器的代价函数是生成器代价函数的负值。在GAN中，我们可以定义一个值函数，生成器需要最小化，而判别器需要最大化。生成模型的训练过程与监督训练方法大不相同。GAN对初始权重非常敏感，因此我们需要使用批量归一化（batch
    normalization）。批量归一化不仅能提高性能，还能使模型更加稳定。在这里，我们同时训练两个模型：生成模型和判别模型。生成模型G捕捉数据分布，而判别模型D估计一个样本来自训练数据的概率，而不是来自G。
- en: GAN – code example
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN – 代码示例
- en: 'In the following example, we build and train a GAN model using an MNIST dataset
    and using TensorFlow. Here, we will use a special version of the ReLU activation
    function known as **Leaky ReLU**. The output is a new type of handwritten digit:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用MNIST数据集并利用TensorFlow构建和训练一个GAN模型。这里，我们将使用一种特殊版本的ReLU激活函数，称为**Leaky
    ReLU**。输出是一个新的手写数字类型：
- en: Leaky ReLU is a variation of the ReLU activation function given by the formula *f(x) = max(α∗x, x**)*.
    So the output for the negative value for *x* is *alpha * x *and the output for
    positive *x* is *x*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU是ReLU激活函数的一种变体，其公式为*f(x) = max(α∗x, x**)*。因此，*x*为负值时，输出为*alpha * x*，而*x*为正值时，输出为*x*。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to build this network, we need two inputs, one for the generator and
    one for the discriminator. In the following code, we create placeholders for `real_input`
    for the discriminator and `z_input` for the generator, with the input sizes as
    `dim_real` and `dim_z`, respectively:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建这个网络，我们需要两个输入，一个是生成器的输入，一个是判别器的输入。在下面的代码中，我们为判别器创建`real_input`的占位符，为生成器创建`z_input`的占位符，输入尺寸分别为`dim_real`和`dim_z`：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, input `z` is a random vector to the generator which turns this vector
    into an image. Then we add a hidden layer, which is a leaky ReLU layer, to allow
    gradients to flow backwards. Leaky ReLU is just like a normal ReLU (for negative
    values emitting zero) except that there is a small non-zero output for negative
    input values. The generator performs better with the `tanh``sigmoid` function.
    Generator output is `tanh` output. So, we''ll have to rescale the MNIST images
    to be between -1 and 1, instead of 0 and 1\. With this knowledge, we can build
    the generator network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，输入`z`是一个随机向量传入生成器，生成器将这个向量转化为图像。然后我们添加一个隐藏层，这是一个带有泄漏的ReLU层，以允许梯度向后传播。泄漏ReLU就像普通的ReLU（对负值输出零）一样，除了对于负输入值，输出有一个小的非零值。生成器使用`tanh`和`sigmoid`函数表现更好。生成器的输出是`tanh`输出。因此，我们必须将MNIST图像重新缩放到-1到1之间，而不是0到1之间。通过这些知识，我们可以构建生成器网络：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The discriminator network is the same as the generator except that output layer
    is a `sigmoid` function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络与生成器相同，只是输出层使用的是`sigmoid`函数：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To build the network, use the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建网络，使用以下代码：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We want to share weights between real and fake data, so we need to reuse the
    variables:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在真实数据和假数据之间共享权重，因此需要重用变量：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Calculating loss
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算损失
- en: 'For the discriminator, the total loss is the sum of the losses for real and
    fake images. The losses will be sigmoid cross-entropyies, which we can get using
    the TensorFlow `tf.nn.sigmoid_cross_entropy_with_logits`. Then we compute the
    mean for all the images in the batch. So the losses will look like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器，总损失是对真实图像和假图像损失的总和。损失将是sigmoid交叉熵损失，我们可以使用TensorFlow的`tf.nn.sigmoid_cross_entropy_with_logits`得到。然后我们计算批次中所有图像的均值。因此，损失将如下所示：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To help the discriminator generalize better, the `labels` can be reduced a bit
    from 1.0 to 0.9, by for example, using the parameter `smooth`*. *This is known
    as **label smoothing**, and is typically used with classifiers to improve performance. The
    discriminator loss for the fake data is similar. The `logits` are `d_logits_fake`,
    which we got from passing the generator output to the discriminator. These fake
    `logits` are used with `labels` of all zeros. Remember that we want the discriminator
    to output 1 for real images and 0 for fake images, so we need to set up the losses
    to reflect that.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助判别器更好地泛化，可以通过例如使用`平滑`*参数，将`labels`从1.0稍微减少到0.9。* 这被称为**标签平滑**，通常与分类器一起使用以提高性能。假的数据的判别器损失类似。`logits`是`d_logits_fake`，它是通过将生成器输出传递给判别器得到的。这些假的`logits`与全为零的`labels`一起使用。记住，我们希望判别器对真实图像输出1，对假图像输出0，因此我们需要设置损失函数来反映这一点。
- en: 'Finally, the generator losses are using `d_logits_fake`*, *the fake image `logits`.
    But now the `labels` are all 1s. The generator is trying to fool the discriminator,
    so it wants the discriminator to output ones for fake images:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，生成器的损失使用的是`d_logits_fake`*，*即假的图像`logits`。但现在`labels`全为1。生成器试图欺骗判别器，因此它希望判别器对假的图像输出1：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Adding the optimizer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加优化器
- en: 'We need to update the generator and discriminator variables separately. So,
    first get all the variables of the graph and then, as we explained earlier, we
    can get only generator variables from the generator scope and, similarly, discriminator
    variables from the discriminator scope:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要分别更新生成器和判别器的变量。因此，首先获取图中的所有变量，然后如前所述，我们可以仅从生成器作用域获取生成器变量，类似地从判别器作用域获取判别器变量：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To train the network, use:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练网络，使用：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the model is trained and saved, you can visualize the generated digits
    (the code is not here, but it can be downloaded).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练并保存后，你可以可视化生成的数字（代码不在此处，但可以下载）。
- en: Semi-supervised learning and GAN
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习与GAN
- en: 'So for, we have seen how GAN can be used to generate realistic images. In this
    section, we will see how GAN can be used for classification tasks where we have
    less labeled data but still want to improve the accuracy of the classifier. Here
    we will also use the same **Street View House Number** or **SVHN** dataset to
    classify images. As previously, here we also have two networks, the generator
    G and discriminator D. In this case, the discriminator is trained to become a
    classifier. Another change is that the output of the discriminator goes to a softmax
    function instead of a `sigmoid` function, as seen earlier. The softmax function
    returns the probability distribution over labels:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到 GAN 如何用于生成逼真的图像。在本节中，我们将看到 GAN 如何用于分类任务，尤其是在标签数据较少的情况下，但仍希望提高分类器的准确性。这里我们仍然使用相同的
    **街景房屋号码**（**SVHN**）数据集来对图像进行分类。如前所述，我们这里也有两个网络，生成器 G 和判别器 D。在这种情况下，判别器被训练成一个分类器。另一个变化是，判别器的输出将传递给
    softmax 函数，而不是早期看到的 `sigmoid` 函数。softmax 函数返回标签的概率分布：
- en: '![](img/8f617d78-0c02-4e22-9e9c-1e99923915dc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f617d78-0c02-4e22-9e9c-1e99923915dc.png)'
- en: 'Now we model the network as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将网络建模为：
- en: '*total cost = cost of labeled data + cost of unlabeled data*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*总成本 = 有标签数据的成本 + 无标签数据的成本*'
- en: 'To get the cost of labeled data, we can use the `cross_entropy` function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取有标签数据的成本，我们可以使用 `cross_entropy` 函数：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we can calculate the sum of all classes:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以计算所有类别的总和：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Normal classifiers work on labeled data. However, semi-supervised GAN-based
    classifiers work on labeled data, real unlabeled data, and fake images. This works
    very well, that is, there are less classification errors even though we have less
    labeled data in the training process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 正常的分类器作用于有标签数据。然而，基于 GAN 的半监督分类器作用于有标签数据、真实未标注数据和假图像。这种方法非常有效，即使我们在训练过程中有较少的标注数据，分类错误也较少。
- en: Feature matching
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征匹配
- en: The idea of feature matching is to add an extra variable to the cost function
    of the generator in order to penalize the difference between absolute errors in
    the test data and training data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特征匹配的思想是，在生成器的成本函数中添加一个额外的变量，以惩罚测试数据和训练数据中的绝对误差之间的差异。
- en: Semi-supervised classification using a GAN example
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 示例进行半监督分类
- en: In this section, we explain how to use GAN to build a classifier with the semi-supervised
    learning approach.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释如何使用 GAN 来构建一个采用半监督学习方法的分类器。
- en: In supervised learning, we have a training set of inputs `X` and class labels `y`.
    We train a model that takes `X` as input and gives `y` as output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有一个包含输入 `X` 和类别标签 `y` 的训练集。我们训练一个模型，该模型以 `X` 作为输入并输出 `y`。
- en: In semi-supervised learning, our goal is still to train a model that takes `X` as
    input and generates `y` as output. However, not all of our training examples have
    a label `y`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习中，我们的目标仍然是训练一个模型，该模型以 `X` 作为输入并生成 `y` 作为输出。然而，并非所有的训练示例都有标签 `y`。
- en: 'We use the SVHN dataset. We''ll turn the GAN discriminator into an 11 class
    discriminator (0 to 9 and one label for the fake image). It will recognize the
    10 different classes of real SVHN digits, as well as an eleventh class of fake
    images that come from the generator. The discriminator will get to train on real
    labeled images, real unlabeled images, and fake images. By drawing on three sources
    of data instead of just one, it will generalize to the test set much better than
    a traditional classifier trained on only one source of data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 SVHN 数据集。我们将 GAN 判别器转变为一个 11 类判别器（0 到 9 以及一个假图像标签）。它将识别真实 SVHN 数字的 10 个不同类别，以及来自生成器的第
    11 类假图像。判别器将能在真实标注图像、真实未标注图像和假图像上进行训练。通过利用三种数据来源，而不仅仅是单一来源，它将在测试集上表现得比传统的仅在单一数据源上训练的分类器更好：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Add the generator:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 添加生成器：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Add the discriminator:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 添加判别器：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Calculate the loss:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Add the optimizers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 添加优化器：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Build the network model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 构建网络模型：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Train and persist the model:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 训练并保存模型：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Deep convolutional GAN
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积 GAN
- en: '**Deep convolutional GAN**, also called **DCGAN**, is used to generate color
    images. Here we use a convolutional layer in the generator and discriminator.
    We''ll also need to use batch normalization to get the GAN to train appropriately.
    We will discuss batch normalization in detail in the performance improvement of
    deep neural networks chapter. We''ll be training GAN on the SVHN dataset; a small
    example is shown in the following figure. After training, the generator will be
    able to create images that are nearly identical to these images. You can download
    the code for this example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度卷积生成对抗网络**，也称为 **DCGAN**，用于生成彩色图像。在这里，我们在生成器和判别器中使用了卷积层。我们还需要使用批量归一化来确保
    GAN 能够正常训练。我们将在《深度神经网络性能提升》章节中详细讨论批量归一化。我们将在 SVHN 数据集上训练 GAN；以下是一个小示例。训练后，生成器将能够创建几乎与这些图像相同的图像。你可以下载这个示例的代码：'
- en: '![](img/ca8ff0f4-19fc-465c-9b3e-aab5e4b9f1d7.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca8ff0f4-19fc-465c-9b3e-aab5e4b9f1d7.png)'
- en: Google Street View house numbers view
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Google 街景房屋号码视图
- en: Batch normalization
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化
- en: Batch normalization is a technique for improving the performance and stability
    of neural networks. The idea is to normalize the layer inputs so that they have
    a mean of zero and variance of 1\. Batch normalization was introduced in Sergey
    Ioffe's and Christian Szegedy's 2015 paper, *Batch Normalization is Necessary
    to Make DCGANs Work*. The idea is that instead of just normalizing the inputs
    to the network, we normalize the inputs to layers within the network. It's called
    **batch** **normalization** because during training, we normalize each layer's
    input by using the mean and variance of the values in the current mini-batch.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化是一种提高神经网络性能和稳定性的技术。其思想是对层输入进行归一化，使其均值为零，方差为 1。批量归一化最早由 Sergey Ioffe 和 Christian
    Szegedy 于 2015 年在论文 *Batch Normalization is Necessary to Make DCGANs Work* 中提出。其思路是，与其仅对网络输入进行归一化，不如对网络中各个层的输入进行归一化。之所以称之为
    **批量** **归一化**，是因为在训练过程中，我们使用当前小批量中的均值和方差来对每一层的输入进行归一化。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have seen how the GAN model truly displays the power of
    CNN. We learned how to train our own generative model and saw a practical example
    of GAN that can generate photos from paintings and turn horses into zebras.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看到了 GAN 模型如何真正展示 CNN 的强大功能。我们学习了如何训练自己的生成模型，并看到了一个实际的 GAN 示例，它能够将画作转化为照片，将马变成斑马。
- en: We understood how GAN differs from other discriminative models and learned why
    generative models are preferred.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解了 GAN 与其他判别模型的区别，并学会了为什么生成模型更受青睐。
- en: In the next chapter, we will learn about deep learning software comparison from
    scratch.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从头开始学习深度学习软件的比较。
