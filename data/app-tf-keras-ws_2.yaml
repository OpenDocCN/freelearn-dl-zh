- en: '2\. Real-World Deep Learning: Predicting the Price of Bitcoin'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 实际深度学习：预测比特币价格
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter will help you to prepare data for a deep learning model, choose
    the right model architecture, use Keras—the default API of TensorFlow 2.0, and
    make predictions with the trained model. By the end of this chapter, you will
    have prepared a model to make predictions which we will explore in the upcoming
    chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助您为深度学习模型准备数据，选择合适的模型架构，使用Keras——TensorFlow 2.0的默认API，并用训练好的模型进行预测。到本章结束时，您将准备好一个模型以进行预测，我们将在接下来的章节中探索它。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Building on fundamental concepts from *Chapter 1*, *Introduction to Neural Networks
    and Deep Learning*, let's now move on to a real-world scenario and identify whether
    we can build a deep learning model that predicts Bitcoin prices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第1章*，*神经网络和深度学习简介*的基本概念基础上，现在让我们移步到一个实际情景，并确定我们是否能够构建一个预测比特币价格的深度学习模型。
- en: We will learn the principles of preparing data for a deep learning model, and
    how to choose the right model architecture. We will use Keras—the default API
    of TensorFlow 2.0 and make predictions with the trained model. We will conclude
    this chapter by putting all these components together and building a bare bones,
    yet complete, first version of a deep learning application.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习为深度学习模型准备数据的原则，以及如何选择合适的模型架构。我们将使用Keras——TensorFlow 2.0的默认API，并用训练好的模型进行预测。我们将通过将所有这些组件放在一起并构建一个简陋但完整的深度学习应用的第一个版本来结束本章。
- en: Deep learning is a field that is undergoing intense research activity. Among
    other things, researchers are devoted to inventing new neural network architectures
    that can either tackle new problems or increase the performance of previously
    implemented architectures.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个正在经历激烈研究活动的领域。研究人员致力于发明新的神经网络架构，可以解决新问题或提高先前实现的架构的性能。
- en: In this chapter, we will study both old and new architectures. Older architectures
    have been used to solve a large array of problems and are generally considered
    the right choice when starting a new project. Newer architectures have shown great
    success in specific problems but are harder to generalize. The latter are interesting
    as references of what to explore next but are hardly a good choice when starting
    a project.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究旧和新的架构。旧的架构已经被用来解决各种问题，并且通常被认为是开始新项目时的正确选择。新的架构在特定问题上显示了巨大的成功，但很难一般化。后者作为下一步探索的参考是有趣的，但在开始项目时很难是一个好选择。
- en: The following topic discusses the details of these architectures and how to
    determine the best one for a particular problem statement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下主题讨论了这些架构的细节以及如何确定特定问题陈述的最佳架构。
- en: Choosing the Right Model Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适的模型架构
- en: 'Considering the available architecture possibilities, there are two popular
    architectures that are often used as starting points for several applications:
    **Convolutional Neural Networks** (**CNNs**) and **Recurrent Neural Networks**
    (**RNNs**). These are foundational networks and should be considered starting
    points for most projects.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到可用的架构可能性，有两种流行的架构通常被用作多个应用的起点：**卷积神经网络**（**CNNs**）和**循环神经网络**（**RNNs**）。这些是基础网络，应该被视为大多数项目的起点。
- en: 'We also include descriptions of another three networks, due to their relevance
    in the field: **Long Short-Term Memory** (**LSTM**) networks (an RNN variant);
    **Generative Adversarial Networks** (**GANs**); and **Deep Reinforcement Learning**
    (**DRL**). These latter architectures have shown great success in solving contemporary
    problems, however, they are slightly difficult to use. The next section will cover
    the use of different types of architecture in different problems.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了其他三种网络的描述，因为它们在领域中的相关性：**长短期记忆**（**LSTM**）网络（一种RNN变体）；**生成对抗网络**（**GANs**）；以及**深度强化学习**（**DRL**）。这些后者在解决当代问题上取得了巨大成功，但使用起来略显困难。接下来的部分将涵盖在不同问题中使用不同类型架构的情况。
- en: Convolutional Neural Networks (CNNs)
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）
- en: CNNs have gained notoriety for working with problems that have a grid-like structure.
    They were originally created to classify images, but have been used in several
    other areas, ranging from speech recognition to self-driving vehicles.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs因处理具有网格结构的问题而声名鹊起。最初它们被创建用于分类图像，但已被用于多个其他领域，从语音识别到自动驾驶车辆。
- en: 'A CNN''s essential insight is to use closely related data as an element of
    the training process, instead of only individual data inputs. This idea is particularly
    effective in the context of images, where a pixel located at the center is related
    to the ones located to its right and left. The name **convolution** is given to
    the mathematical representation of the following process:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的核心理念是将紧密相关的数据作为训练过程的元素，而不仅仅是单独的数据输入。在图像处理的背景下，这一思想尤为有效，因为位于中心的像素与其右侧和左侧的像素相关联。**卷积**（convolution）这一名称被用来表示以下过程的数学形式：
- en: '![Figure 2.1: Illustration of the convolution process'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1：卷积过程的示意图'
- en: '](img/B15911_02_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_01.jpg)'
- en: 'Figure 2.1: Illustration of the convolution process'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：卷积过程的示意图
- en: Note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'Image source: Volodymyr Mnih, *et al.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：Volodymyr Mnih，*等人*
- en: 'You can find this image at: [https://packt.live/3fivWLB](https://packt.live/3fivWLB)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接找到这张图像：[https://packt.live/3fivWLB](https://packt.live/3fivWLB)
- en: For more information about deep reinforcement learning, refer to *Human-level
    Control through Deep Reinforcement Learning*. *February 2015*, *Nature*, available
    at [https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多有关深度强化学习的信息，请参考*通过深度强化学习实现人类级别的控制*。*2015年2月*，*Nature*，可在[https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)获取。
- en: Recurrent Neural Networks (RNNs)
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）
- en: A CNN works with a set of inputs that keeps altering the weights and biases
    of the network's respective layers and nodes. A known limitation of this approach
    is that its architecture ignores the sequence of these inputs when determining
    the changes to the network's weights and biases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: CNN使用一组输入，这些输入不断调整网络相应层和节点的权重和偏置。这种方法的已知局限性在于，当确定网络权重和偏置的变化时，它的架构忽略了这些输入的顺序。
- en: RNNs were created precisely to address that problem. They are designed to work
    with sequential data. This means that at every epoch, layers can be influenced
    by the output of previous layers. The memory of previous observations in each
    sequence plays an important role in the evaluation of posterior observations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的出现正是为了解决这一问题。它们被设计用于处理顺序数据。这意味着在每一个时期（epoch），层次结构可以受到前一层输出的影响。在每个序列中的前期观测记忆在后期观测的评估中起着重要作用。
- en: RNNs have had successful applications in speech recognition due to the sequential
    nature of that problem. Also, they are used for translation problems. Google Translate's
    current algorithm—Transformer, uses an RNN to translate text from one language
    to another. In late 2018, Google introduced another algorithm based on the Transformer
    algorithm called **Bidirectional Encoder Representations from Transformers** (**BERT**),
    which is currently state of the art in **Natural Language Processing** (**NLP**).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于语音识别问题的顺序性，RNN（循环神经网络）在该领域得到了成功应用。此外，RNN也被用于翻译问题。Google Translate当前的算法——Transformer，使用RNN将文本从一种语言翻译到另一种语言。在2018年底，Google推出了另一种基于Transformer算法的算法，称为**双向编码器表示（Bidirectional
    Encoder Representations from Transformers）**（**BERT**），它目前是**自然语言处理**（**NLP**）领域的最先进技术。
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: 'For more information RNN applications, refer to the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于RNN应用的信息，请参考以下内容：
- en: '*Transformer: A Novel Neural Network Architecture for Language Understanding*,
    *Jakob Uszkoreit*, *Google Research Blog*, *August 2017*, available at [https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Transformer：一种用于语言理解的全新神经网络架构*，*Jakob Uszkoreit*，*Google Research Blog*，*2017年8月*，可在[https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)获取。'
- en: '*BERT: Open Sourcing BERT: State-of-the-Art Pre-Training for Natural Language
    Processing*, available at [https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*BERT：开源BERT：自然语言处理的最先进预训练技术*，可在[https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)获取。'
- en: 'The following diagram illustrates how words in English are linked to words
    in French, based on where they appear in a sentence. RNNs are very popular in
    language translation problems:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了英语单词与法语单词如何根据它们在句子中的位置关联。RNN在语言翻译问题中非常受欢迎：
- en: '![Figure 2.2: Illustration from distill.pub linking words in English and French'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2：distill.pub中展示的英语和法语单词关联示意图'
- en: '](img/B15911_02_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_02.jpg)'
- en: 'Figure 2.2: Illustration from distill.pub linking words in English and French'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：distill.pub中的插图，展示了英语和法语单词的关联
- en: Note
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Image source: [https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)
- en: Long Short-Term Memory (LSTM) Networks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络
- en: '**LSTM** networks are RNN variants created to address the vanishing gradient
    problem. This problem is caused by memory components that are too distant from
    the current step that receive lower weights due to their distance. LSTMs are a
    variant of RNNs that contain a memory component called a **forget gate**. This
    component can be used to evaluate how both recent and old elements affect the
    weights and biases, depending on where the observation is placed in a sequence.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**网络是RNN的变体，用于解决梯度消失问题。该问题是由于与当前步骤相距过远的记忆组件所导致，它们由于距离较远而接收到较低的权重。LSTM是RNN的一种变体，包含一个叫做**遗忘门**的记忆组件。该组件可以用来评估最近的和较早的元素如何影响权重和偏置，具体取决于观察值在序列中的位置。'
- en: Note
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The LSTM architecture was first introduced by Sepp Hochreiter and Jürgen Schmidhuber
    in 1997\. Current implementations have had several modifications. For a detailed
    mathematical explanation of how each component of an LSTM works, refer to the
    article *Understanding LSTM Networks*, by Christopher Olah, August 2015, available
    at [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM架构最早由Sepp Hochreiter和Jürgen Schmidhuber于1997年提出。目前的实现已做了若干修改。如需详细了解LSTM每个组件的数学原理，请参考Christopher
    Olah的文章*理解LSTM网络*，2015年8月，文章可通过以下链接获取：[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: Generative Adversarial Networks
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**Generative Adversarial Networks** (**GANs**) were invented in 2014 by Ian
    Goodfellow and his colleagues at the University of Montreal. GANs work based on
    the approach that, instead of having one neural network that optimizes weights
    and biases with the objective to minimize its errors, there should be two neural
    networks that compete against each other for that purpose.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）由Ian Goodfellow及其同事于2014年在蒙特利尔大学发明。GAN的工作原理是，通过一个神经网络优化权重和偏置，以最小化其误差，而不是仅有一个神经网络优化权重和偏置，应该有两个神经网络互相竞争完成这一目标。'
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on GANs, refer to *Generative Adversarial Networks*, *Ian
    Goodfellow, et al.*, *arXiv*. *June 10, 2014*, available at [https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解有关GAN的更多信息，请参考*生成对抗网络*，*Ian Goodfellow等人*，*arXiv*，*2014年6月10日*，可通过以下链接获取：[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)。
- en: 'GANs generate new data (*fake* data) and a network that evaluates the likelihood
    of the data generated by the first network being *real* or *fake*. They compete
    because both learn: one learns how to better generate *fake* data, and the other
    learns how to distinguish whether the data presented is real. They iterate on
    every epoch until convergence. That is the point when the network that evaluates
    generated data cannot distinguish between *fake* and *real* data any further.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GANs生成新的数据（*伪*数据）和一个评估第一个网络生成的数据是否为*真实*或*伪*的网络。它们相互竞争，因为它们都在学习：一个学习如何更好地生成*伪*数据，另一个学习如何区分数据是否真实。它们在每个迭代周期中反复训练，直到收敛。那时，评估生成数据的网络无法再区分*伪*数据和*真实*数据。
- en: 'GANs have been successfully used in fields where data has a clear topological
    structure. Originally, GANs were used to create synthetic images of objects, people''s
    faces, and animals that were similar to real images of those things. You will
    see in the following image, used in the **StarGAN** project, that the expressions
    on the face change:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GANs已在数据具有明确拓扑结构的领域成功应用。最初，GAN被用来生成与真实图像相似的物体、人物面孔和动物的合成图像。你将在接下来的图像中看到，在**StarGAN**项目中，面部表情发生了变化：
- en: '![Figure 2.3: Changes in people''s faces based on emotion, using GAN algorithms'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：基于情感的面部变化，使用GAN算法'
- en: '](img/B15911_02_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_03.jpg)'
- en: 'Figure 2.3: Changes in people''s faces based on emotion, using GAN algorithms'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：基于情感的面部变化，使用GAN算法
- en: This domain of image creation is where GANs are used the most frequently, but
    applications in other domains occasionally appear in research papers.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像创建领域中GAN使用最为频繁的地方，但在其他领域也偶尔能在研究论文中看到其应用。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Image source: StarGAN project, available at [https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：StarGAN项目， 可在[https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN)查阅。
- en: Deep Reinforcement Learning (DRL)
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）
- en: The original DRL architecture was championed by DeepMind, a Google-owned artificial
    intelligence research organization based in the UK. The key idea of DRL networks
    is that they are unsupervised in nature and that they learn from trial and error,
    only optimizing for a reward function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的深度强化学习（DRL）架构由DeepMind提出，DeepMind是一个总部位于英国的Google拥有的人工智能研究机构。DRL网络的核心思想是它们本质上是无监督的，通过试验和错误来学习，只优化奖励函数。
- en: That is, unlike other networks (which use a supervised approach to optimize
    incorrect predictions as compared to what are known to be correct ones), DRL networks
    do not know of a correct way of approaching a problem. They are simply given the
    rules of a system and are then rewarded every time they perform a function correctly.
    This process, which takes a very large number of iterations, eventually trains
    networks to excel in several tasks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，DRL网络与其他网络不同（后者使用监督方法来优化与已知正确结果相比的错误预测），DRL网络并不知道解决问题的正确方法。它们只是被给定一个系统的规则，然后每当它们正确执行一个功能时，都会得到奖励。这个过程需要大量的迭代，最终训练网络在多个任务上表现出色。
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about DRL, refer to *Human-Level Control through Deep
    Reinforcement Learning*, *Volodymyr Mnih et al.*, *February 2015*, *Nature*, available
    at: [https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多关于DRL的信息，请参阅*Human-Level Control through Deep Reinforcement Learning*，*Volodymyr
    Mnih等*，*2015年2月*，*Nature*，可在以下网址查阅：[https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)。
- en: DRL models gained popularity after DeepMind created AlphaGo—a system that plays
    the game Go better than professional players. DeepMind also created DRL networks
    that learn how to play video games at a superhuman level, entirely on their own.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: DRL模型在DeepMind创建了AlphaGo（一个比职业玩家更擅长围棋的系统）之后获得了广泛关注。DeepMind还创建了DRL网络，使其能够独立地以超人类水平学习玩视频游戏。
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For more information about DQN, look up the DQN that was created by DeepMind
    to beat Atari games. The algorithm uses a DRL solution to continuously increase
    its reward.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于DQN的信息，可以查阅DeepMind创建的DQN，它能够击败Atari游戏。该算法使用DRL解决方案不断提高奖励。
- en: 'Image source: [https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/)。
- en: 'Here''s a summary of neural network architectures and their applications:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是神经网络架构及其应用的总结：
- en: '![Figure 2.4: Different neural network architectures, data structures, and
    their successful applications'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4：不同的神经网络架构、数据结构及其成功应用'
- en: '](img/B15911_02_04.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_04.jpg)'
- en: 'Figure 2.4: Different neural network architectures, data structures, and their
    successful applications'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：不同的神经网络架构、数据结构及其成功应用
- en: Data Normalization
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据归一化
- en: Before building a deep learning model, data normalization is an important step.
    Data normalization is a common practice in machine learning systems. For neural
    networks, researchers have proposed that normalization is an essential technique
    for training RNNs (and LSTMs), mainly because it decreases the network's training
    time and increases the network's overall performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建深度学习模型之前，数据归一化是一个重要的步骤。数据归一化是机器学习系统中的常见做法。对于神经网络，研究人员提出归一化是训练RNN（和LSTM）时的一个关键技术，主要是因为它可以减少网络的训练时间，并提高网络的整体性能。
- en: Note
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, refer to *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*, *Sergey Ioffe et al.*, *arXiv*,
    March 2015, available at [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '欲了解更多信息，请参阅*Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift*，*Sergey Ioffe等*，*arXiv*，2015年3月， 可在[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)查阅。'
- en: 'Which normalization technique works best depends on the data and the problem
    at hand. A few commonly used techniques are listed here:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种归一化技术最有效取决于数据和所处理的问题。以下是一些常用的技术：
- en: Z-Score
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Z-Score
- en: 'When data is normally distributed (that is, Gaussian), you can compute the
    distance between each observation as a standard deviation from its mean. This
    normalization is useful when identifying how distant the data points are from
    more likely occurrences in the distribution. The Z-score is defined by the following
    formula:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据呈正态分布（即，高斯分布）时，你可以计算每个观察值与其均值的标准差距离。这种归一化方法在识别数据点距离分布中更可能出现的情况有多远时非常有用。Z分数的定义如下公式：
- en: '![Figure 2.5: Formula for Z-Score'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5: Z分数公式'
- en: '](img/B15911_02_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_05.jpg)'
- en: 'Figure 2.5: Formula for Z-Score'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.5: Z分数公式'
- en: Here, *x*i is the *i*th observation, ![1](img/B15911_02_Formula_01.png) is the
    mean, and ![2](img/B15911_02_Formula_02.png) is the standard deviation of the
    series.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*x*i是第*i*个观察值，![1](img/B15911_02_Formula_01.png)是均值，![2](img/B15911_02_Formula_02.png)是序列的标准差。
- en: Note
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, refer to the standard score article (*Z-Score: Definition,
    Formula, and Calculation*), available at [https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参阅标准分数文章（*Z分数：定义、公式与计算*），该文章可在[https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/)上找到。
- en: Point-Relative Normalization
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 点相对归一化
- en: 'This normalization computes the difference in a given observation in relation
    to the first observation of the series. This kind of normalization is useful for
    identifying trends in relation to a starting point. The point-relative normalization
    is defined by:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化方法计算给定观察值与序列第一个观察值之间的差异。这种归一化方法有助于识别与起始点相关的趋势。点相对归一化的定义为：
- en: '![Figure 2.6: Formula for point-relative normalization'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6: 点相对归一化公式'
- en: '](img/B15911_02_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_06.jpg)'
- en: 'Figure 2.6: Formula for point-relative normalization'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.6: 点相对归一化公式'
- en: Here, *o*i is the *i*th observation, and *o*o is the first observation of the
    series.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*o*i是第*i*个观察值，*o*o是序列的第一个观察值。
- en: Note
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on making predictions, watch *How to Predict Stock Prices
    Easily – Intro to Deep Learning #7*, *Siraj Raval*, available on YouTube at [https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '欲了解更多关于预测的信息，请观看*如何轻松预测股票价格 – 深度学习入门 #7*，*Siraj Raval*，可以在YouTube上找到，链接：[https://www.youtube.com/watch?v=ftMq5ps503w](https://www.youtube.com/watch?v=ftMq5ps503w)。'
- en: Maximum and Minimum Normalization
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大值和最小值归一化
- en: 'This type of normalization computes the distance between a given observation
    and the maximum and minimum values of the series. This is useful when working
    with series in which the maximum and minimum values are not outliers and are important
    for future predictions. This normalization technique can be applied with the following
    formula:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这种归一化方法计算给定观察值与序列中最大值和最小值之间的距离。当处理最大值和最小值不是异常值且对未来预测非常重要的序列时，这种方法非常有用。这种归一化技术可以使用以下公式应用：
- en: '![Figure 2.7: Formula for calculating normalization'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7: 归一化计算公式'
- en: '](img/B15911_02_07.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_07.jpg)'
- en: 'Figure 2.7: Formula for calculating normalization'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.7: 归一化计算公式'
- en: Here, *O*i is the *i*th observation, *O* represents a vector with all *O* values,
    and the functions *min (O)* and *max (O)* represent the minimum and maximum values
    of the series, respectively.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*O*i是第*i*个观察值，*O*代表包含所有*O*值的向量，*min(O)*和*max(O)*函数分别代表序列的最小值和最大值。
- en: During *Exercise* *2.01*, *Exploring the Bitcoin Dataset and Preparing Data
    for a Model*, we will prepare available Bitcoin data to be used in our LSTM model.
    That includes selecting variables of interest, selecting a relevant period, and
    applying the preceding point-relative normalization technique.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习* *2.01*，*探索比特币数据集并为模型准备数据*中，我们将准备可用的比特币数据，以便用于我们的LSTM模型。这包括选择感兴趣的变量，选择相关的时间段，并应用前述的点相对归一化技术。
- en: Structuring Your Problem
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构化你的问题
- en: 'Compared to researchers, practitioners spend much less time determining which
    architecture to choose when starting a new deep learning project. Acquiring data
    that represents a given problem correctly is the most important factor to consider
    when developing these systems, followed by an understanding of the dataset''s
    inherent biases and limitations. When starting to develop a deep learning system,
    consider the following questions for reflection:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与研究人员相比，实践者在开始一个新的深度学习项目时，花在选择架构上的时间要少得多。获取能够正确代表给定问题的数据是开发这些系统时最重要的因素，其次是理解数据集的固有偏差和限制。当开始开发深度学习系统时，可以考虑以下反思问题：
- en: Do I have the right data? This is the hardest challenge when training a deep
    learning model. First, define your problem with mathematical rules. Use precise
    definitions and organize the problem into either categories (classification problems)
    or a continuous scale (regression problems). Now, how can you collect data pertaining
    to those metrics?
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否拥有正确的数据？这是训练深度学习模型时最具挑战性的问题。首先，用数学规则定义你的问题。使用精确的定义，并将问题组织为类别（分类问题）或连续的量表（回归问题）。现在，如何收集与这些度量标准相关的数据？
- en: Do I have enough data? Typically, deep learning algorithms have shown to perform
    much better on large datasets than on smaller ones. Knowing how much data is necessary
    to train a high-performance algorithm depends on the kind of problem you are trying
    to address, but aim to collect as much data as you can.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我有足够的数据吗？通常，深度学习算法在大数据集上的表现远优于小数据集。了解训练一个高性能算法所需的数据量取决于你试图解决的问题类型，但应尽可能收集尽可能多的数据。
- en: Can I use a pre-trained model? If you are working on a problem that is a subset
    of a more general application, but within the same domain. Consider using a pre-trained
    model. Pre-trained models can give you a head start on tackling the specific patterns
    of your problem, instead of the more general characteristics of the domain at
    large. A good place to start is the official TensorFlow repository ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能使用预训练模型吗？如果你正在解决的问题是更一般应用的一个子集，但仍然属于相同领域，考虑使用预训练模型。预训练模型可以帮助你迅速识别问题的特定模式，而不是广泛领域中的一般特征。一个好的起点是官方的TensorFlow代码库（[https://github.com/tensorflow/models](https://github.com/tensorflow/models)）。
- en: 'When you structure your problem with such questions, you will have a sequential
    approach to any new deep learning project. The following is a representative flow
    chart of these questions and tasks:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当你用这些问题来构建你的问题时，你将对任何新的深度学习项目采用一种顺序化的方法。以下是这些问题和任务的代表性流程图：
- en: '![Figure 2.8: Decision tree of key reflection questions to be asked at the
    beginning of a deep learning project'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：深度学习项目开始时需要问的关键反思问题的决策树'
- en: '](img/B15911_02_08.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_08.jpg)'
- en: 'Figure 2.8: Decision tree of key reflection questions to be asked at the beginning
    of a deep learning project'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：深度学习项目开始时需要问的关键反思问题的决策树
- en: In certain circumstances, the data may simply not be available. Depending on
    the case, it may be possible to use a series of techniques to effectively create
    more data from your input data. This process is known as **data augmentation**
    and can be applied successfully when working with image recognition problems.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，数据可能根本无法获取。根据具体情况，可能可以使用一系列技术有效地从输入数据中生成更多数据。这个过程被称为**数据增强**，在图像识别问题中应用得非常成功。
- en: Note
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A good reference is the article *Classifying plankton with deep neural networks*,
    available at [https://benanne.github.io/2015/03/17/plankton.html](https://benanne.github.io/2015/03/17/plankton.html).
    The authors show a series of techniques for augmenting a small set of image data
    in order to increase the number of training samples the model has.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的参考是文章 *使用深度神经网络分类浮游生物*，可以在[https://benanne.github.io/2015/03/17/plankton.html](https://benanne.github.io/2015/03/17/plankton.html)找到。作者展示了一系列增强少量图像数据的技术，以增加模型的训练样本数量。
- en: Once the problem is well-structured, you will be able to start preparing the
    model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦问题结构清晰，你就可以开始准备模型。
- en: Jupyter Notebook
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: We will be using Jupyter Notebook to code in this section. Jupyter Notebooks
    provide Python sessions via a web browser that allows you to work with data interactively.
    They are a popular tool for exploring datasets. They will be used in exercises
    throughout this book.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Jupyter Notebook 进行编码。Jupyter Notebook 提供通过 web 浏览器进行 Python 会话的功能，让您可以交互式地处理数据。它们是探索数据集的热门工具，将在本书的练习中使用。
- en: 'Exercise 2.01: Exploring the Bitcoin Dataset and Preparing Data for a Model'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：探索比特币数据集并为模型准备数据
- en: In this exercise, we will prepare the data and then pass it to the model. The
    prepared data will then be useful in making predictions as we move ahead in the
    chapter. Before preparing the data, we will do some visual analysis on it, such
    as looking at when the value of Bitcoin was at its highest and when the decline
    started.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将准备数据，然后将其传递给模型。准备好的数据将在本章的后续部分有助于进行预测。在准备数据之前，我们将对其进行一些可视化分析，例如查看比特币的价值何时达到最高，以及下降何时开始。
- en: Note
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will be using a public dataset originally retrieved from the Yahoo Finance
    website ([https://finance.yahoo.com/quote/BTC-USD/history/](https://finance.yahoo.com/quote/BTC-USD/history/)).
    The dataset has been slightly modified, as it has been provided alongside this
    chapter, and will be used throughout the rest of this book.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个原本从 Yahoo Finance 网站获取的公共数据集 ([https://finance.yahoo.com/quote/BTC-USD/history/](https://finance.yahoo.com/quote/BTC-USD/history/))。该数据集经过轻微修改，并且与本章一起提供，之后会在本书的其余部分使用。
- en: 'The dataset can be downloaded from: [https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以从以下网址下载：[https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)。
- en: 'The following are the steps to complete this exercise:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此练习的步骤：
- en: 'Using your Terminal, navigate to the `Chapter02/Exercise2.01` directory. Activate
    the environment created in the previous chapter and execute the following command
    to start a Jupyter Notebook instance:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的终端，导航到 `Chapter02/Exercise2.01` 目录。激活在上一章创建的环境，并执行以下命令以启动 Jupyter Notebook
    实例：
- en: '[PRE0]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This should automatically open the Jupyter lab server in your browser. From
    there you can start a Jupyter Notebook.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该会自动在您的浏览器中打开 Jupyter Lab 服务器。从那里您可以启动 Jupyter Notebook。
- en: 'You should see the following output or similar:'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该会看到以下输出或类似内容：
- en: '![Figure 2.9: Terminal image after starting a Jupyter lab instance'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.9: 启动 Jupyter Lab 实例后的终端图片'
- en: '](img/B15911_02_09.jpg)'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_09.jpg)'
- en: 'Figure 2.9: Terminal image after starting a Jupyter lab instance'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.9: 启动 Jupyter Lab 实例后的终端图片'
- en: 'Select the `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb` file. This is a Jupyter
    Notebook file that will open in a new browser tab. The application will automatically
    start a new Python interactive session for you:![Figure 2.10: Landing page of
    your Jupyter Notebook instance'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '选择 `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb` 文件。这是一个 Jupyter Notebook
    文件，将在新的浏览器标签页中打开。应用程序将自动为您启动一个新的 Python 交互式会话：![图 2.10: Jupyter Notebook 实例的登录页面'
- en: '](img/B15911_02_10.jpg)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_10.jpg)'
- en: 'Figure 2.10: Landing page of your Jupyter Notebook instance'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.10: Jupyter Notebook 实例的登录页面'
- en: 'Click the Jupyter Notebook file:![Figure 2.11: Image of Jupyter Notebook'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '点击 Jupyter Notebook 文件：![图 2.11: Jupyter Notebook 图片'
- en: '](img/B15911_02_11.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_11.jpg)'
- en: 'Figure 2.11: Image of Jupyter Notebook'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.11: Jupyter Notebook 图片'
- en: 'Opening our Jupyter Notebook, consider the Bitcoin data made available with
    this chapter. The dataset `data/bitcoin_historical_prices.csv` ([https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r))
    contains the details of Bitcoin prices since early 2013\. It contains eight variables,
    two of which (`date` and `week`) describe a time period of the data. These can
    be used as indices—and six others (`open`, `high`, `low`, `close`, `volume`, and
    `market_capitalization`) can be used to understand changes in the price and value
    of Bitcoin over time:![Figure 2.12: Available variables (that is, columns) in
    the Bitcoin historical prices dataset'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '打开我们的 Jupyter Notebook，考虑本章提供的比特币数据集。数据集 `data/bitcoin_historical_prices.csv`
    ([https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)) 包含了自2013年初以来的比特币价格详情。它包含八个变量，其中两个（`date`
    和 `week`）描述数据的时间周期。它们可以作为索引使用——其余六个（`open`、`high`、`low`、`close`、`volume` 和 `market_capitalization`）可以用来理解比特币价格和价值随时间的变化：![图
    2.12: 比特币历史价格数据集中的可用变量（即列）'
- en: '](img/B15911_02_12.jpg)'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_12.jpg)'
- en: 'Figure 2.12: Available variables (that is, columns) in the Bitcoin historical
    prices dataset'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.12: 比特币历史价格数据集中的可用变量（即列）'
- en: 'Using the open Jupyter Notebook instance, consider the time series of two of
    those variables: `close` and `volume`. Start with those time series to explore
    price fluctuation patterns, that is, how the price of Bitcoin varied at different
    times in the historical data.'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用打开的Jupyter Notebook实例，考虑这两个变量的时间序列：`close`和`volume`。从这些时间序列开始，探索价格波动模式，也就是比特币在历史数据中不同时间段的价格变化情况。
- en: 'Navigate to the open instance of the Jupyter Notebook, `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb`.
    Now, execute all cells under the `Introduction` header. This will import the required
    libraries and import the dataset into memory:![Figure 2.13: Output from the first
    cells of the notebook time-series plot'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到打开的Jupyter Notebook实例，`Exercise2.01_Exploring_Bitcoin_Dataset.ipynb`。现在，执行`Introduction`标题下的所有单元格。这将导入所需的库，并将数据集导入到内存中：![图
    2.13：笔记本中第一个单元格的输出时间序列图
- en: of the closing price for Bitcoin from the close variable](img/B15911_02_13.jpg)
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比特币收盘价时间序列图（来自close变量）](img/B15911_02_14.jpg)
- en: 'Figure 2.13: Output from the first cells of the notebook time-series plot of
    the closing price for Bitcoin from the close variable'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.13：笔记本中第一个单元格的输出，比特币收盘价时间序列图（来自close变量）
- en: 'After the dataset has been imported into memory, move to the `Exploration`
    section. You will find a snippet of code that generates a time series plot for
    the `close` variable:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据集加载到内存后，转到`探索`部分。你会找到一段代码，可以生成`close`变量的时间序列图：
- en: '[PRE1]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output looks like:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.14: Time series plot of the closing price for Bitcoin from the close
    variable'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.14：比特币收盘价时间序列图（来自close变量）'
- en: '](img/B15911_02_14.jpg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_15.jpg)'
- en: 'Figure 2.14: Time series plot of the closing price for Bitcoin from the close
    variable'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.14：比特币收盘价时间序列图（来自close变量）
- en: 'Reproduce this plot but using the `volume` variable in a new cell below this
    one. You will have most certainly noticed that price variables surge in 2017 and
    then the downfall starts:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的一个新单元格中，使用`volume`变量重新生成该图。你肯定已经注意到，比特币价格变量在2017年暴涨，随后开始下跌：
- en: '[PRE2]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 2.15: The total daily volume of Bitcoin coins'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.15：比特币每日交易量总和](img/B15911_02_13.jpg)'
- en: '](img/B15911_02_15.jpg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_15.jpg)'
- en: 'Figure 2.15: The total daily volume of Bitcoin coins'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.15：比特币每日交易量总和
- en: '*Figure 2.15* shows that since 2017, Bitcoin transactions have significantly
    increased in the market. The total daily volume varies much more than daily closing
    prices.'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*图 2.15* 显示自2017年以来，比特币交易在市场中显著增加。每日交易量的变化比每日收盘价的变化大得多。'
- en: Execute the remaining cells in the Exploration section to explore the range
    from 2017 to 2018.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行探索部分的其余单元格，探索2017年至2018年的范围。
- en: Fluctuations in Bitcoin prices have been increasingly commonplace in recent
    years. While those periods could be used by a neural network to understand certain
    patterns, we will be excluding older observations, given that we are interested
    in predicting future prices for not-too-distant periods. Filter the data after
    2016 only. Navigate to the `Preparing Dataset for a Model` section. Use the pandas
    API to filter the data. Pandas provides an intuitive API for performing this operation.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 近年来，比特币价格波动已变得越来越常见。虽然这些时期可以被神经网络用于理解某些模式，但我们将排除较旧的观测数据，因为我们关注的是预测未来不久的价格。因此，只过滤2016年后的数据。导航到`为模型准备数据集`部分，使用pandas
    API来过滤数据。Pandas提供了一个直观的API来执行此操作。
- en: 'Extract recent data and save it into a variable:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取最新数据并将其保存到一个变量中：
- en: '[PRE3]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `bitcoin_recent` variable now has a copy of our original Bitcoin dataset,
    but filtered to the observations that are newer or equal to January 4, 2016.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`bitcoin_recent`变量现在包含我们原始的比特币数据集副本，但仅保留2016年1月4日及以后的数据。'
- en: Normalize the data using the point-relative normalization technique described
    in the *Data Normalization* section in the Jupyter Notebook. You will only normalize
    two variables—`close` and `volume`—because those are the variables that we are
    working to predict.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用*数据归一化*部分中描述的点相对归一化技术对数据进行归一化。你只需归一化两个变量——`close`和`volume`——因为这两个是我们要预测的变量。
- en: Run the next cell in the notebook to ensure that we only keep the close and
    volume variables.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的下一个单元格，以确保我们只保留close和volume变量。
- en: In the same directory containing this chapter, we have placed a script called
    `normalizations.py`. That script contains the three normalization techniques described
    in this chapter. We import that script into our Jupyter Notebook and apply the
    functions to our series.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在包含此章节的同一目录中，我们放置了一个名为`normalizations.py`的脚本。该脚本包含了本章描述的三种归一化技术。我们将该脚本导入到Jupyter
    Notebook中，并将函数应用于我们的数据系列。
- en: 'Navigate to the `Preparing Dataset for a Model` section. Now, use the `iso_week`
    variable to group daily observations from a given week using the pandas `groupby()`
    method. We can now apply the normalization function, `normalizations.point_relative_normalization()`,
    directly to the series within that week. We can store the normalization output
    as a new variable in the same pandas DataFrame using the following code:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`Preparing Dataset for a Model`部分。现在，使用`iso_week`变量，利用pandas的`groupby()`方法将每日观测按周分组。我们现在可以直接对该周的数据系列应用归一化函数`normalizations.point_relative_normalization()`。我们可以使用以下代码将归一化后的输出作为新变量存储在同一个pandas
    DataFrame中：
- en: '[PRE4]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `close_point_relative_normalization` variable now contains the normalized
    data for the `close` variable:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`close_point_relative_normalization`变量现在包含了`close`变量的归一化数据：'
- en: '[PRE5]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will result in the following output:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导致以下输出：
- en: '![Figure 2.16: Image of the Jupyter Notebook focusing on the section where
    the normalization function is applied'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.16：Jupyter Notebook的图像，聚焦于应用归一化函数的部分'
- en: '](img/B15911_02_16.jpg)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_16.jpg)'
- en: 'Figure 2.16: Image of the Jupyter Notebook focusing on the section where the
    normalization function is applied'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.16：Jupyter Notebook的图像，聚焦于应用归一化函数的部分
- en: 'Do the same with the `volume` variable (`volume_point_relative_normalization`).
    The normalized `close` variable contains an interesting variance pattern every
    week. We will be using that variable to train our LSTM model:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`volume`变量（`volume_point_relative_normalization`）执行相同操作。归一化后的`close`变量每周都包含一个有趣的方差模式。我们将使用这个变量来训练我们的LSTM模型：
- en: '[PRE6]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Your output should be as follows.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出应如下所示。
- en: '![Figure 2.17: Plot that displays the series from the normalized variable'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.17：显示归一化变量系列的图表'
- en: '](img/B15911_02_17.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_17.jpg)'
- en: 'Figure 2.17: Plot that displays the series from the normalized variable'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.17：显示归一化变量系列的图表
- en: 'In order to evaluate how well the model performs, you need to test its accuracy
    versus some other data. Do this by creating two datasets: a training set and a
    test set. You will use 90 percent of the dataset to train the LSTM model and 10
    percent to evaluate its performance. Given that the data is continuous and in
    the form of a time series, use the last 10 percent of available weeks as a test
    set and the first 90 percent as a training set:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估模型的表现，你需要测试其与其他数据的准确性。通过创建两个数据集来实现这一点：一个训练集和一个测试集。你将使用数据集的90％来训练LSTM模型，10％来评估模型的表现。由于数据是连续的并且以时间序列的形式存在，因此使用最后10％的可用周作为测试集，前90％作为训练集：
- en: '[PRE7]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This will display the following output:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '![Figure 2.18: Output of the test set weeks'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.18：测试集周的输出'
- en: '](img/B15911_02_18.jpg)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_18.jpg)'
- en: 'Figure 2.18: Output of the test set weeks'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.18：测试集周的输出
- en: '![Figure 2.19: Using weeks to create a training set'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.19：使用周数创建训练集'
- en: '](img/B15911_02_19.jpg)'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15911_02_19.jpg)'
- en: 'Figure 2.19: Using weeks to create a training set'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.19：使用周数创建训练集
- en: 'Create the separate datasets for each operation:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个操作创建单独的数据集：
- en: '[PRE8]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, navigate to the `Storing Output` section and save the filtered variable
    to disk, as follows:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，导航到`Storing Output`部分，并将筛选后的变量保存到磁盘，如下所示：
- en: '[PRE9]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3ehbgCi](https://packt.live/3ehbgCi).
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3ehbgCi](https://packt.live/3ehbgCi)。
- en: You can also run this example online at [https://packt.live/2ZdGq9s](https://packt.live/2ZdGq9s).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是[https://packt.live/2ZdGq9s](https://packt.live/2ZdGq9s)。你必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we explored the Bitcoin dataset and prepared it for a deep
    learning model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们探索了比特币数据集，并为深度学习模型做好了准备。
- en: We learned that in 2017, the price of Bitcoin skyrocketed. This phenomenon took
    a long time to take place and may have been influenced by a number of external
    factors that this data alone doesn't explain (for instance, the emergence of other
    cryptocurrencies). After the great surge of 2017, we saw a great fall in the value
    of Bitcoin in 2018.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，2017 年比特币的价格暴涨。这一现象发生得很缓慢，可能受到了许多外部因素的影响，而这些数据本身无法完全解释（例如，其他加密货币的出现）。在
    2017 年的暴涨后，我们看到比特币在 2018 年的价值大幅下跌。
- en: We also used the point-relative normalization technique to process the Bitcoin
    dataset in weekly chunks. We do this to train an LSTM network to learn the weekly
    patterns of Bitcoin price changes so that it can predict a full week into the
    future.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了点相对归一化技术，将比特币数据集按周进行处理。我们这样做是为了训练一个 LSTM 网络，学习比特币价格变化的周度模式，从而预测未来一整周的价格。
- en: However, Bitcoin statistics show significant fluctuations on a weekly basis.
    Can we predict the price of Bitcoin in the future? What will the price be seven
    days from now? We will build a deep learning model to explore these questions
    in our next section using Keras.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，比特币的统计数据显示其价格在周度基础上波动较大。我们能预测比特币未来的价格吗？七天后的价格会是多少？我们将在下一节中使用 Keras 构建一个深度学习模型来探索这些问题。
- en: Using Keras as a TensorFlow Interface
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 作为 TensorFlow 接口
- en: We are using Keras because it simplifies the TensorFlow interface into general
    abstractions and, in TensorFlow 2.0, this is the default API in this version.
    In the backend, the computations are still performed in TensorFlow, but we spend
    less time worrying about individual components, such as variables and operations,
    and spend more time building the network as a computational unit. Keras makes
    it easy to experiment with different architectures and hyperparameters, moving
    more quickly toward a performant solution.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Keras 是因为它将 TensorFlow 接口简化为通用抽象，并且在 TensorFlow 2.0 中，这是该版本的默认 API。在后台，计算仍然在
    TensorFlow 中执行，但我们花费更少的时间关注各个组件，如变量和操作，而更多的时间用来构建作为计算单元的网络。Keras 使得我们可以轻松地实验不同的架构和超参数，更快速地朝着高效的解决方案迈进。
- en: As of TensorFlow 2.0.0, Keras is now officially distributed with TensorFlow
    as `tf.keras`. This suggests that Keras is now tightly integrated with TensorFlow
    and will likely continue to be developed as an open source tool for a long period
    of time. Components are an integral part when building models. Let's deep dive
    into this concept now.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 从 TensorFlow 2.0.0 开始，Keras 已正式与 TensorFlow 一起作为 `tf.keras` 进行分发。这意味着 Keras
    现在与 TensorFlow 紧密集成，并且可能会继续作为一个开源工具长时间开发。组件是构建模型时的一个重要组成部分。让我们现在深入了解这个概念。
- en: Model Components
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型组件
- en: As we saw in *Chapter 1*, *Introduction to Neural Networks and Deep Learning*,
    LSTM networks also have input, hidden, and output layers. Each hidden layer has
    an activation function that evaluates that layer's associated weights and biases.
    As expected, the network moves data sequentially from one layer to another and
    evaluates the results by the output at every iteration (that is, an epoch).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第1章*《神经网络与深度学习简介》中所看到的，LSTM 网络同样具有输入层、隐藏层和输出层。每个隐藏层都有一个激活函数，用于评估该层相关的权重和偏差。正如预期，网络将数据从一个层顺序地传递到另一个层，并在每次迭代（即一个
    epoch）时通过输出评估结果。
- en: 'Keras provides intuitive classes that represent each one of the components
    listed in the following table:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了直观的类，表示下表中列出的每个组件：
- en: '![Figure 2.20: Description of key components from the Keras API'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.20：Keras API 关键组件的描述'
- en: '](img/B15911_02_20.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_20.jpg)'
- en: 'Figure 2.20: Description of key components from the Keras API'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.20：Keras API 关键组件的描述
- en: We will be using these components to build a deep learning model.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这些组件来构建深度学习模型。
- en: Keras' `keras.models.Sequential()` component represents a whole sequential neural
    network. This Python class can be instantiated on its own and have other components
    added to it subsequently.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的 `keras.models.Sequential()` 组件表示一个完整的顺序神经网络。这个 Python 类可以单独实例化，并可以后续添加其他组件。
- en: 'We are interested in building an LSTM network because those networks perform
    well with sequential data—and a time series is a kind of sequential data. Using
    Keras, the complete LSTM network would be implemented as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以对构建 LSTM 网络感兴趣，是因为这些网络在处理序列数据时表现优异，而时间序列正是一种序列数据。使用 Keras，完整的 LSTM 网络将如下实现：
- en: '[PRE10]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This implementation will be further optimized in *Chapter 3*, *Real-World Deep
    Learning with TensorFlow and Keras: Evaluating the Bitcoin Model*.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现将在*第 3 章*，*使用 TensorFlow 和 Keras 进行现实世界深度学习：评估比特币模型*中进一步优化。
- en: 'Keras abstraction allows you to focus on the key elements that make a deep
    learning system more performant: determining the right sequence of components,
    how many layers and nodes to include, and which activation function to use. All
    of these choices are determined by either the order in which components are added
    to the instantiated `keras.models``.Sequential()` class or by parameters passed
    to each component instantiation (that is, `Activation("linear")`). The final `model.compile()`
    step builds the neural network using TensorFlow components.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 抽象使你能够专注于让深度学习系统更高效的关键元素：确定正确的组件顺序、包括多少层和节点、以及使用哪种激活函数。所有这些选择都是通过将组件添加到实例化的
    `keras.models.Sequential()` 类中的顺序，或通过传递给每个组件实例化的参数（例如，`Activation("linear")`）来确定的。最后的
    `model.compile()` 步骤使用 TensorFlow 组件构建神经网络。
- en: 'After the network is built, we train our network using the `model.fit()` method.
    This will yield a trained model that can be used to make predictions:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络构建完成后，我们使用 `model.fit()` 方法来训练网络。这将生成一个经过训练的模型，可以用于进行预测：
- en: '[PRE11]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The `X_train` and `Y_train` variables are, respectively, a set used for training
    and a smaller set used for evaluating the loss function (that is, testing how
    well the network predicts data). Finally, we can make predictions using the `model.predict()`
    method:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train` 和 `Y_train` 变量分别是用于训练的集合和用于评估损失函数（即测试网络预测数据的准确性）的小集合。最后，我们可以使用 `model.predict()`
    方法进行预测：'
- en: '[PRE12]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding steps cover the Keras paradigm for working with neural networks.
    Despite the fact that different architectures can be dealt with in very different
    ways, Keras simplifies the interface for working with different architectures
    by using three components – `Network Architecture`, `Fit`, and `Predict`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤涵盖了 Keras 在处理神经网络时的范式。尽管不同的架构可以通过非常不同的方式来处理，Keras 通过使用三个组件——`Network Architecture`、`Fit`
    和 `Predict`——简化了与不同架构的交互：
- en: '![Figure 2.21: The Keras neural network paradigm'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.21：Keras 神经网络范式'
- en: '](img/B15911_02_21.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_21.jpg)'
- en: 'Figure 2.21: The Keras neural network paradigm'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：Keras 神经网络范式
- en: 'The Keras neural network diagram comprises the following three steps:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 神经网络图包括以下三个步骤：
- en: A neural network architecture
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络架构
- en: Training a neural network (or **Fit**)
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络（或 **拟合**）
- en: Making predictions
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行预测
- en: Keras allows much greater control within each of these steps. However, its focus
    is to make it as easy as possible for users to create neural networks in as little
    time as possible. That means that we can start with a simple model, and then add
    complexity to each one of the preceding steps to make that initial model perform
    better.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 在这些步骤中提供了更大的控制权。然而，它的重点是尽可能简化用户创建神经网络的过程，尽量在最短时间内完成。这意味着我们可以从一个简单的模型开始，然后在每个前面的步骤中增加复杂性，以使初始模型表现得更好。
- en: 'We will take advantage of that paradigm during our upcoming exercise and chapters.
    In the next exercise, we will create the simplest LSTM network possible. Then,
    in *Chapter 3*, *Real-World Deep Learning: Evaluating the Bitcoin Model*, we will
    continuously evaluate and alter that network to make it more robust and performant.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的练习和章节中利用这一范式。在下一个练习中，我们将创建一个最简单的 LSTM 网络。然后，在*第 3 章*，*现实世界中的深度学习：评估比特币模型*中，我们将不断评估和调整该网络，使其更加稳健和高效。
- en: 'Exercise 2.02: Creating a TensorFlow Model Using Keras'
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：使用 Keras 创建一个 TensorFlow 模型
- en: 'In this notebook, we design and compile a deep learning model using Keras as
    an interface to TensorFlow. We will continue to modify this model in our next
    chapters and exercises by experimenting with different optimization techniques.
    However, the essential components of the model are designed entirely in this notebook:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 Notebook 中，我们使用 Keras 作为 TensorFlow 接口来设计和编译一个深度学习模型。我们将在接下来的章节和练习中继续修改这个模型，通过尝试不同的优化技术来优化它。然而，模型的基本组件完全是在这个
    Notebook 中设计的：
- en: 'Open a new Jupyter Notebook and import the following libraries:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并导入以下库：
- en: '[PRE13]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our dataset contains daily observations and each observation influences a future
    observation. Also, we are interested in predicting a week—that is, 7 days—of Bitcoin
    prices in the future:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的数据集包含每日的观测值，每个观测值会影响未来的观测值。此外，我们的目标是预测未来一周——即 7 天——的比特币价格：
- en: '[PRE14]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We have calculated `number_of_observations` based on available weeks in our
    dataset. Given that we will be using last week to test the LSTM network on every
    epoch, we will use 208 – 21 – 1\. You''ll get:'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已根据数据集中可用的周数计算了`number_of_observations`。由于我们将在每个epoch使用上周的数据来测试LSTM网络，因此我们将使用208
    – 21 – 1。你将得到：
- en: '[PRE15]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Build the LSTM model using Keras. We have the batch size as one because we
    are passing the whole data in a single iteration. If data is big, then we can
    pass the data with multiple batches, That''s why we used batch_input_shape:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras构建LSTM模型。我们将批处理大小设为1，因为我们将整个数据一次性传递。如果数据量很大，我们可以通过多个批次来传递数据，这就是为什么我们使用`batch_input_shape`的原因：
- en: '[PRE16]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This should return a compiled Keras model that can be trained and stored in
    disk.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这应该返回一个已经编译的Keras模型，可以进行训练并存储到磁盘中。
- en: 'Let''s store the model on the output of the model to a disk:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将模型存储到模型输出的磁盘上：
- en: '[PRE17]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that the `bitcoin_lstm_v0.h5` model hasn''t been trained yet. When saving
    a model without prior training, you effectively only save the architecture of
    the model. That same model can later be loaded by using Keras'' `load_model()`
    function, as follows:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，`bitcoin_lstm_v0.h5`模型尚未训练。当保存一个未经训练的模型时，你实际上只保存了模型的架构。该模型稍后可以通过使用Keras的`load_model()`函数加载，如下所示：
- en: '[PRE18]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/38KQI3Y](https://packt.live/38KQI3Y).
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参考[https://packt.live/38KQI3Y](https://packt.live/38KQI3Y)。
- en: You can also run this example online at [https://packt.live/3fhEL89](https://packt.live/3fhEL89).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3fhEL89](https://packt.live/3fhEL89)上在线运行此示例。你必须执行整个Notebook才能获得预期的结果。
- en: This concludes the creation of our Keras model, which we can now use to make
    predictions.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们Keras模型的创建完成，现在我们可以使用它进行预测。
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You may encounter the following warning when loading the Keras library:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你在加载Keras库时可能会遇到以下警告：
- en: '`Using TensorFlow backend`'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`使用TensorFlow后端`'
- en: Keras can be configured to use another backend instead of TensorFlow (that is,
    Theano). In order to avoid this message, you can create a file called `keras.json`
    and configure its backend there. The correct configuration of that file depends
    on your system. Hence, it is recommended that you visit Keras' official documentation
    on the topic at [https://keras.io/backend/](https://keras.io/backend/).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Keras可以配置为使用TensorFlow以外的其他后端（即Theano）。为了避免出现此消息，你可以创建一个名为`keras.json`的文件，并在那里配置其后端。该文件的正确配置取决于你的系统。因此，建议你访问Keras的官方文档[https://keras.io/backend/](https://keras.io/backend/)。
- en: In this section, we have learned how to build a deep learning model using Keras—an
    interface for TensorFlow. We studied core components of Keras and used those components
    to build the first version of our Bitcoin price-predicting system based on an
    LSTM model.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用Keras（TensorFlow的接口）构建一个深度学习模型。我们研究了Keras的核心组件，并使用这些组件构建了基于LSTM模型的比特币价格预测系统的第一个版本。
- en: In our next section, we will discuss how to put all the components from this
    chapter together into a (nearly complete) deep learning system. That system will
    yield our very first predictions, serving as a starting point for future improvements.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，我们将讨论如何将本章中的所有组件整合成一个（几乎完整的）深度学习系统。该系统将产生我们第一个预测，作为未来改进的起点。
- en: From Data Preparation to Modeling
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据准备到建模
- en: This section focuses on the implementation aspects of a deep learning system.
    We will use the Bitcoin data from the *Choosing the Right Model Architecture*
    section, and the Keras knowledge from the preceding section, *Using Keras as a
    TensorFlow Interface*, to put both of these components together. This section
    concludes the chapter by building a system that reads data from a disk and feeds
    it into a model as a single piece of software.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本节专注于深度学习系统的实现方面。我们将使用*选择正确的模型架构*部分中的比特币数据，以及前一部分*将Keras作为TensorFlow接口使用*中的Keras知识，将这两个组件结合在一起。本节通过构建一个从磁盘读取数据并将其作为一个整体输入到模型中的系统来结束本章。
- en: Training a Neural Network
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Neural networks can take long periods of time to train. Many factors affect
    how long that process may take. Among them, three factors are commonly considered
    the most important:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可能需要很长时间才能训练。影响训练时间的因素有很多。其中，三个因素通常被认为是最重要的：
- en: The network's architecture
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的架构
- en: How many layers and neurons the network has
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络的层数和神经元数
- en: How much data there is to be used in the training process
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中要使用多少数据
- en: Other factors may also greatly impact how long a network takes to train, but
    most of the optimization that a neural network can have when addressing a business
    problem comes from exploring those three.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其他因素也可能极大地影响网络训练的时间，但在解决业务问题时，神经网络可以进行大多数优化。
- en: We will be using the normalized data from our previous section. Recall that
    we have stored the training data in a file called `train_dataset.csv`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前一节中的标准化数据。请记住，我们已将训练数据存储在名为`train_dataset.csv`的文件中。
- en: 'Note:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: 'You can download the training data by visiting this link: [https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r).'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过访问此链接下载训练数据：[https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)。
- en: 'We will load that dataset into memory using the `pandas` library for easy exploration:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`pandas`库将该数据集加载到内存中，以便轻松探索：
- en: '[PRE19]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure you change the path (highlighted) based on where you have downloaded
    or saved the CSV file.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 确保根据您下载或保存CSV文件的位置更改路径（已突出显示）。
- en: 'You will see the output in a tabular form as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到以下表格形式的输出：
- en: '![Figure 2.22: Table showing the first five rows of the training dataset'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.22：显示训练数据集前五行的表格'
- en: '](img/B15911_02_22.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_22.jpg)'
- en: 'Figure 2.22: Table showing the first five rows of the training dataset'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22：显示训练数据集前五行的表格
- en: We will be using the series from the `close_point_relative_normalization` variable,
    which is a normalized series of the Bitcoin closing prices—from the `close` variable—since
    the beginning of 2016.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`close_point_relative_normalization` 变量的系列，这是比特币收盘价从2016年初以来的标准化系列——自`close`变量开始。
- en: 'The `close_point_relative_normalization` variable has been normalized on a
    weekly basis. Each observation from the week''s period is made relative to the
    difference from the closing prices on the first day of the period. This normalization
    step is important and will help our network train faster:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`close_point_relative_normalization` 变量已按周进行了标准化。每周期内的每个观察值都相对于该期的第一天的收盘价差异进行了。这一标准化步骤很重要，将有助于我们的网络更快地训练：'
- en: '![Figure 2.23: Plot that displays the series from the normalized variable.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.23：显示来自标准化变量的系列的图表。'
- en: '](img/B15911_02_23.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_23.jpg)'
- en: 'Figure 2.23: Plot that displays the series from the normalized variable.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23：显示来自标准化变量的系列的图表。
- en: This variable will be used to train our LSTM model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 此变量将用于训练我们的LSTM模型。
- en: Reshaping Time Series Data
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重塑时间序列数据
- en: Neural networks typically work with vectors and tensors—both mathematical objects
    that organize data in a number of dimensions. Each neural network implemented
    in Keras will have either a vector or a tensor that is organized according to
    a specification as input.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通常使用向量和张量——这两种数学对象来组织数据在多个维度上。Keras中实现的每个神经网络都将有一个按照规定组织的向量或张量作为输入。
- en: At first, understanding how to reshape the data into the format expected by
    a given layer can be confusing. To avoid confusion, it is advisable to start with
    a network with as few components as possible, and then add components gradually.
    Keras' official documentation (under the `Layers` section) is essential for learning
    about the requirements for each kind of layer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，理解如何将数据重塑为给定层期望的格式可能会令人困惑。为了避免混淆，建议从尽可能少的组件开始，逐渐添加组件。Keras的官方文档（位于`Layers`部分）对了解每种层的要求至关重要。
- en: Note
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The Keras official documentation is available at [https://keras.io/layers/core/](https://keras.io/layers/core/).
    That link takes you directly to the `Layers` section.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的官方文档可在[https://keras.io/layers/core/](https://keras.io/layers/core/)找到。该链接直接进入了`Layers`部分。
- en: '**NumPy** is a popular Python library used for performing numerical computations.
    It is used by the deep learning community to manipulate vectors and tensors and
    prepare them for deep learning systems.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy** 是一个流行的Python库，用于进行数值计算。它被深度学习社区用于操作向量和张量，并为深度学习系统准备数据。'
- en: In particular, the `numpy.reshape()` method is very important when adapting
    data for deep learning models. That model allows for the manipulation of NumPy
    arrays, which are Python objects analogous to vectors and tensors.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，`numpy.reshape()` 方法在为深度学习模型调整数据时非常重要。该模型允许操作NumPy数组，这些数组是Python对象，类似于向量和张量。
- en: We'll now organize the prices from the `close_point_relative_normalization`
    variable using the weeks after 2016\. We will create distinct groups containing
    7 observations each (one for each day of the week) for a total of 208 complete
    weeks. We do that because we are interested in predicting the prices of a week's
    worth of trading.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将根据 2016 年之后的周次对 `close_point_relative_normalization` 变量中的价格进行组织。我们将创建不同的组，每组包含
    7 个观察值（每周一天），总共 208 周完整数据。这样做是因为我们有兴趣预测一周内的交易价格。
- en: Note
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We use the ISO standard to determine the beginning and the end of a week. Other
    kinds of organizations are entirely possible. This one is simple and intuitive
    to follow, but there is room for improvement.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ISO 标准来确定一周的开始和结束。其他类型的组织方式完全是可能的。这种方法简单直观，但仍有改进的空间。
- en: 'LSTM networks work with three-dimensional tensors. Each one of those dimensions
    represents an important property for the network. These dimensions are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络处理三维张量。每一个维度代表了网络的一个重要特性。这些维度如下：
- en: '**Period length**: The period length, that is, how many observations there
    are for a period'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期长度**：周期长度，即一个周期内有多少个观察值'
- en: '**Number of periods**: How many periods are available in the dataset'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**周期数**：数据集中可用的周期数量'
- en: '**Number of features**: The number of features available in the dataset'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征数**：数据集中可用的特征数量'
- en: Our data from the `close_point_relative_normalization` variable is currently
    a one-dimensional vector. We need to reshape it to match those three dimensions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来自 `close_point_relative_normalization` 变量的数据目前是一个一维向量。我们需要将其重塑为匹配这三个维度。
- en: We will be using a period of a week. So, our period length is 7 days (period
    length = 7). We have 208 complete weeks available in our data. We will be using
    the very last of those weeks to test our model against during its training period.
    That leaves us with 187 distinct weeks. Finally, we will be using a single feature
    in this network (number of features = 1); we will include more features in future
    versions.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一周的周期。因此，我们的周期长度为 7 天（周期长度 = 7）。我们有 208 周完整的周数据可用。在训练期间，我们将使用这些周中的最后一周来测试我们的模型。这使得我们剩下
    187 个不同的周。最后，我们将在该网络中使用单个特征（特征数 = 1）；未来版本中我们会加入更多特征。
- en: 'To reshape the data to match those dimensions, we will be using a combination
    of base Python properties and the `reshape()` method from the `numpy` library.
    First, we create the 186 distinct week groups with 7 days, each using pure Python:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整数据以匹配这些维度，我们将使用基本的 Python 属性和 `numpy` 库中的 `reshape()` 方法的组合。首先，我们使用纯 Python
    创建 186 个包含 7 天的不同周组：
- en: '[PRE20]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This piece of code creates distinct week groups. The resulting variable data
    is a variable that contains all the right dimensions.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了不同的周组。生成的变量数据是一个包含所有正确维度的变量。
- en: Note
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Each Keras layer will expect its input to be organized in specific ways. However,
    Keras will reshape data accordingly, in most cases. Always refer to the Keras
    documentation on layers ([https://keras.io/layers/core/](https://keras.io/layers/core/))
    before adding a new layer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Keras 层将期望其输入以特定的方式组织。然而，在大多数情况下，Keras 会自动调整数据的形状。添加新层之前，请始终参考 Keras 的层文档（[https://keras.io/layers/core/](https://keras.io/layers/core/)）。
- en: 'The Keras LSTM layer expects these dimensions to be organized in a specific
    order: the number of features, the number of observations, and the period length.
    Reshape the dataset to match that format:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Keras LSTM 层期望这些维度按特定顺序组织：特征数、观察数和周期长度。将数据集重塑为匹配该格式：
- en: '[PRE21]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding snippet also selects the very last week of our set as a validation
    set (via `data[-1]`). We will be attempting to predict the very last week in our
    dataset by using the preceding 76 weeks. The next step is to use those variables
    to fit our model:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段还选择了我们数据集中最后一周作为验证集（通过 `data[-1]`）。我们将尝试使用前 76 周的数据预测数据集中的最后一周。下一步是使用这些变量来拟合我们的模型：
- en: '[PRE22]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'LSTMs are computationally expensive models. They may take up to 5 minutes to
    train with our dataset on a modern computer. Most of that time is spent at the
    beginning of the computation when the algorithm creates the full computation graph.
    The process gains speed after it starts training:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是计算量大的模型。在现代计算机上，它们可能需要多达 5 分钟的时间来训练我们的数据集。大部分时间都花费在计算的开始阶段，此时算法会创建完整的计算图。开始训练后，过程会变得更快：
- en: '![Figure 2.24: Graph that shows the results of the loss function evaluated
    at each epoch'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.24：显示每个时期评估的损失函数结果的图表'
- en: '](img/B15911_02_24.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_24.jpg)'
- en: 'Figure 2.24: Graph that shows the results of the loss function evaluated at
    each epoch'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.24：显示每个时期评估的损失函数结果的图表
- en: Note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This compares what the model predicted at each epoch, and then compares that
    with the real data using a technique called mean-squared error. This plot shows
    those results.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这比较了模型在每个时期预测的内容，然后使用称为均方误差的技术将其与真实数据进行比较。该图显示了这些结果。
- en: At a glance, our network seems to perform very well; it starts with a very small
    error rate that continuously decreases. Now that we have lowered the error rate,
    let's move on to make some predictions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，我们的网络表现似乎非常好；它从一个非常小的错误率开始，持续下降。现在我们已经降低了错误率，让我们继续进行一些预测。
- en: Making Predictions
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进行预测
- en: After our network has been trained, we can proceed to make predictions. We will
    be making predictions for a future week beyond our time period.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络训练完毕后，我们可以开始进行预测。我们将为未来一个超出我们时间范围的周进行预测。
- en: 'Once we have trained our model with the `model.fit()` method, making predictions
    is simple:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使用 `model.fit()` 方法训练了我们的模型，进行预测就很简单了：
- en: '[PRE23]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We use the same data for making predictions as the data used for training (the
    `X_train` variable). If we have more data available, we can use that instead—given
    that we reshape it to the format the LSTM requires.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与训练数据相同的数据进行预测（变量 `X_train`）。如果有更多数据可用，我们可以使用那些数据——前提是将其调整为 LSTM 所需的格式。
- en: Overfitting
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过拟合
- en: 'When a neural network overfits a validation set, this means that it learns
    patterns present in the training set but is unable to generalize it to unseen
    data (for instance, the test set). In the next chapter, we will learn how to avoid
    overfitting and create a system for both evaluating our network and increasing
    its performance:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络在验证集上过拟合时，这意味着它学习了训练集中存在的模式，但无法将其推广到未见过的数据（例如测试集）。在下一章中，我们将学习如何避免过拟合，并创建一个既能评估网络又能提升其性能的系统：
- en: '![Figure 2.25: Graph showing the weekly performance of Bitcoin'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.25：显示比特币每周表现的图表'
- en: '](img/B15911_02_25.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_25.jpg)'
- en: 'Figure 2.25: Graph showing the weekly performance of Bitcoin'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.25：显示比特币每周表现的图表
- en: In the plot shown above, the horizontal axis represents the week number and
    the vertical axis represents the predicted performance of Bitcoin. Now that we
    have explored the data, prepared a model, and learned how to make predictions,
    let's put this knowledge into practice.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图中，水平轴表示周数，垂直轴表示比特币的预测表现。现在我们已经探索了数据、准备了模型并学会了如何进行预测，让我们将这些知识付诸实践。
- en: 'Activity 2.01: Assembling a Deep Learning System'
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：组装一个深度学习系统
- en: 'In this activity, we''ll bring together all the essential pieces for building
    a basic deep learning system – the data, a model, and predictions:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将集合所有构建基本深度学习系统的关键部分——数据、模型和预测：
- en: Start a Jupyter Notebook.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Jupyter Notebook。
- en: Load the training dataset into memory.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据集加载到内存中。
- en: Inspect the training set to see whether it is in the form period length, number
    of periods, or number of features.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查训练集，看看它是否以周期长度、周期数或特征数量的形式呈现。
- en: Convert the training set if it is not in the required format.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不在所需格式中，将训练集转换。
- en: Load the previously trained model.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载先前训练过的模型。
- en: Train the model using your training dataset.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的训练数据集训练模型。
- en: Make a prediction on the training set.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对训练集进行预测。
- en: Denormalize the values and save the model.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对值进行反归一化并保存模型。
- en: 'The final output will look as follows with the horizontal axis representing
    the number of days and the vertical axis represents the price of Bitcoin:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出将如下所示，水平轴表示天数，垂直轴表示比特币的价格：
- en: '6'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '6'
- en: '![Figure 2.26: Expected output'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.26：预期输出'
- en: '](img/B15911_02_26.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15911_02_26.jpg)'
- en: 'Figure 2.26: Expected output'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.26：预期输出
- en: Note
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 136.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第 136 页找到。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have assembled a complete deep learning system, from data
    to prediction. The model created in this activity requires a number of improvements
    before it can be considered useful. However, it serves as a great starting point
    from which we will continuously improve.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经组装了一个完整的深度学习系统，从数据到预测。这个活动中创建的模型在被认为有用之前需要进行多项改进。然而，它作为我们不断改进的一个很好的起点。
- en: The next chapter will explore techniques for measuring the performance of our
    model and will continue to make modifications until we reach a model that is both
    useful and robust.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将探讨衡量我们模型性能的技术，并继续进行修改，直到我们得到一个既有用又稳健的模型。
