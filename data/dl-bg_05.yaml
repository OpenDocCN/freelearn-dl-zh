- en: Learning from Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中学习
- en: Data preparation takes a great deal of time for complex datasets, as we saw
    in the previous chapter. However, time spent on data preparation is time well
    invested... this I can guarantee! In the same way, investing time in understanding
    the basic theory of learning from data is super important for any person that
    wants to join the field of deep learning. Understanding the fundamentals of learning
    theory will pay off whenever you read new algorithms or evaluate your own models.
    It will also make your life much easier when you get to the later chapters in
    this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备对于复杂数据集来说花费大量时间，正如我们在上一章中所见。然而，花时间准备数据是值得的……这一点我可以保证！同样，花时间理解从数据中学习的基本理论对于任何想进入深度学习领域的人来说都非常重要。理解学习理论的基础，将在你阅读新算法或评估自己模型时带来回报。当你阅读本书后面的章节时，这也会让你的学习过程变得更加轻松。
- en: More specifically, this chapter introduces the most elementary concepts around
    the theory of deep learning, including measuring performance on regression and
    classification as well as the identification of overfitting. It also offers some
    warnings about the sensibility of—and the need to optimize—model hyperparameters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本章介绍了深度学习理论中最基础的概念，包括回归和分类的性能衡量，以及过拟合的识别。它还提供了一些关于模型超参数的合理性以及优化需求的警告。
- en: 'The outline of this chapter is as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构如下：
- en: Learning for a purpose
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有目的的学习
- en: Measuring success and error
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衡量成功与错误
- en: Identifying overfitting and generalization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别过拟合和泛化
- en: The art behind learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习的艺术
- en: Ethical implications of training deep learning algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习算法训练的伦理影响
- en: Learning for a purpose
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有目的的学习
- en: In [Chapter 3](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit),
    *Preparing Data*, we discussed how to prepare data for two major types of problems: **regression**
    and **classification**. In this section, we will cover the technical differences
    between classification and regression in more detail. These differences are important
    because they will limit the type of machine learning algorithms you can use to
    solve your problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit)《准备数据》中，我们讨论了如何为两种主要问题类型准备数据：**回归**和**分类**。在这一节中，我们将更详细地讨论分类和回归的技术差异。这些差异很重要，因为它们会限制你可以使用的机器学习算法类型，以解决你的问题。
- en: Classification
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类
- en: 'How do you know whether your problem is classification? The answer depends
    on two major factors: the **problem** you are trying to solve and the **data**
    you have to solve your problem. There might be other factors, for sure, but these
    two are by far the most significant.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如何判断你的问题是否属于分类问题？答案取决于两个主要因素：你要解决的**问题**和你用于解决问题的**数据**。当然可能还有其他因素，但这两个因素无疑是最为重要的。
- en: 'If your purpose is to make a model that, given some input, will determine whether
    the response or output of the model is to distinguish between two or more distinct
    categories, then you have a classification problem. Here is a non-exhaustive list
    of examples of classification problems:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的目标是建立一个模型，给定某些输入，模型将确定输出是否属于两个或更多的不同类别，那么你遇到的是一个分类问题。以下是分类问题的一些非详尽例子：
- en: 'Given an image, indicate what number it contains (distinguish between 10 categories:
    0-9 digits).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张图片，标出其包含的数字（区分10个类别：0-9数字）。
- en: 'Given an image, indicate whether it contains a cat or not (distinguish between
    two categories: yes or no).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张图片，确定其中是否包含猫（区分两个类别：是或否）。
- en: 'Given a sequence of readings about temperature, determine the season (distinguish
    between four categories: the four seasons).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一系列温度读数，确定其季节（区分四个类别：四季）。
- en: 'Given the text of a tweet, determine the sentiment (distinguish between two
    categories: positive or negative).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一条推文的文本，确定其情感（区分两个类别：正面或负面）。
- en: 'Given an image of a person, determine the age group (distinguish between five
    categories: <18, 18-25, 26-35, 35-50, >50).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张人的图片，确定其年龄段（区分五个类别：<18，18-25，26-35，35-50，>50）。
- en: 'Given an image of a dog, determine its breed (distinguish between 120 categories:
    those breeds that are internationally recognized).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张狗的图片，确定其品种（区分120个类别：国际公认的犬种）。
- en: 'Given an entire document, determine whether it has been tampered with (distinguish between
    categories: authentic or altered).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定整个文档，确定它是否被篡改（区分类别：真实或被更改）。
- en: 'Given satellite readings of a spectroradiometer, determine whether the geolocation
    matches the spectral signature of vegetation or not (distinguish between two categories:
    yes or no).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据光谱辐射计的卫星读数，确定地理位置是否与植被的光谱特征匹配（区分两个类别：是或否）。
- en: As you can see from the examples in the list, there are different types of data
    for different types of problems. The data that we are seeing in these examples
    is known as **labeled data**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在列表中的示例中所见，不同类型的问题具有不同类型的数据。在这些示例中看到的数据称为**标记数据**。
- en: Unlabeled data is very common but is rarely used for classification problems
    without some type of processing that allows the matching of data samples to a
    category. For example, unsupervised clustering can be used on unlabeled data to
    assign the data to specific clusters (such as groups or categories); at which
    point, the data technically becomes "labeled data."
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 未标记数据非常常见，但在没有某种允许将数据样本匹配到类别的处理的情况下，很少用于分类问题。例如，可以对未标记数据使用无监督聚类，将数据分配给特定的聚类（例如组或类别），此时，数据在技术上成为“标记数据”。
- en: 'The other important thing to notice from the list is that we can categorize
    the classification problems into two major groups:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中另一个需要注意的重要事项是，我们可以将分类问题分为两大类：
- en: '**Binary classification**: For classification between any two classes only'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：仅用于任意两个类别之间的分类'
- en: '**Multi-class classification**: For classification between more than just two
    classes'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类分类**：用于对超过两个类别进行分类'
- en: This distinction may seem arbitrary but it is not; in fact, the type of classification
    will limit the type of learning algorithm you can use and the performance you
    can expect. To understand this a little better, let's discuss each classification
    separately.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种区别可能看起来是任意的，但实际上并非如此；事实上，分类的类型将限制您可以使用的学习算法类型和您可以期望的性能。为了更好地理解这一点，让我们分别讨论每种分类。
- en: Binary classification
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类
- en: This type of classification is usually regarded as a much simpler problem than
    multiple classes. In fact, if we can solve the binary classification problem,
    we could, technically, solve the problem of multiple classes by deciding on a
    strategy to break down the problem into several binary classification problems
    (*Lorena, A. C.* et al., *2008*).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分类通常被认为比多类别问题简单得多。实际上，如果我们能解决二元分类问题，我们可以通过决定将问题分解为几个二元分类问题的策略，从技术上讲解决多类问题（*Lorena,
    A. C.* 等，*2008*）。
- en: 'One of the reasons why this is considered a simpler problem is because of the
    algorithmic and mathematical foundations behind binary classification learning
    algorithms. Let''s say that we have a binary classification problem, such as the
    Cleveland dataset explained in [Chapter 3](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit), *Preparing
    Data*. This dataset consists of 13 medical observations for each patient—we can
    call that ![](img/4ae6e047-6db7-4cea-89a7-d59b4577f989.png). For each of these
    patient records, there is an associated label that indicates whether the patient
    has some type of heart disease (+1) or not (-1)—we will call that ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png).
    So, an entire dataset, ![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png), with *N *samples
    can be defined as a set of data and labels:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这被认为是一个较简单的问题之一，是因为二元分类学习算法背后的算法和数学基础。假设我们有一个二元分类问题，比如在[第三章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit)，《准备数据》中解释的克利夫兰数据集。该数据集包含每个患者的13个医学观察结果
    — 我们可以称之为 ![](img/4ae6e047-6db7-4cea-89a7-d59b4577f989.png)。对于每个患者记录，都有一个相关的标签，指示患者是否患有某种心脏疾病（+1）或没有（-1）
    — 我们将称之为 ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png)。因此，一个完整的数据集 ![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png)，包含 *N *个样本，可以被定义为一组数据和标签：
- en: '![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png)'
- en: 'Then, as discussed in [Chapter 1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit), *Introduction
    to Machine Learning*, the whole point of learning is to use an algorithm that
    will find a way to map input data, **x**, to label the *y* correctly for all samples
    in [![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png)] and to be able to further
    do so (hopefully) for samples outside of the known dataset, ![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png).
    Using a perceptron and a corresponding **Perceptron Learning Algorithm** (**PLA**),
    what we want is to find the parameters [![](img/72310c26-8d2e-4d7b-8b0e-2694ac960fbf.png)] that
    can satisfy the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit)所讨论，*机器学习简介*，学习的核心目的是使用一种算法，它能够找到将输入数据**x**映射到标签*y*的方式，并正确地处理所有样本在[![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png)]中的情况，并能够进一步（希望）对已知数据集之外的样本进行处理，![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png)。使用感知机和相应的**感知机学习算法**（**PLA**），我们的目标是找到能够满足以下条件的参数[![](img/72310c26-8d2e-4d7b-8b0e-2694ac960fbf.png)]：
- en: '![](img/a8cba13d-2fa4-48ee-907a-23d24d12ba66.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8cba13d-2fa4-48ee-907a-23d24d12ba66.png)'
- en: For all samples, *i* = 1, 2, ..., *N.* However, as we discussed in [Chapter
    1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit),
    *Introduction to Machine Learning*, the equation cannot be satisfied if the data
    is non-linearly separable. In that case, we can obtain an approximation, or a
    prediction, that is not necessarily the desired outcome; we will call such a prediction ![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有样本，*i* = 1, 2, ..., *N*。然而，正如我们在[第1章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit)中讨论的，*机器学习简介*，如果数据是非线性可分的，方程式将无法满足。在这种情况下，我们可以得到一个近似值或预测结果，这不一定是期望的结果；我们将称这样的预测为![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png)。
- en: The whole point of a learning algorithm, then, becomes to reduce the differences
    between the desired target label, ![](img/67f2f764-f8f9-4e62-abfd-5c9dddfe4114.png),
    and the prediction, ![](img/dda3ec0e-9026-4af0-abb5-71bf7486e83c.png). In an ideal
    world, we want ![](img/46139ee6-41db-420e-a2e9-95cc864cda3a.png) for all cases
    of *i* = 1, 2, ..., *N.* In cases of *i *where ![](img/7f1182f2-7155-4ec8-aca1-50a4033f4c85.png),
    the learning algorithm must make adjustments (that is, train itself) to avoid
    making such mistakes in the future by finding new parameters ![](img/2be5be4c-924d-462b-b31e-0dd3771de027.png) that
    are hopefully better.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，学习算法的核心目标便是减少期望目标标签![](img/67f2f764-f8f9-4e62-abfd-5c9dddfe4114.png)与预测结果![](img/dda3ec0e-9026-4af0-abb5-71bf7486e83c.png)之间的差异。在理想的情况下，我们希望所有*i*
    = 1, 2, ..., *N*时都能实现![](img/46139ee6-41db-420e-a2e9-95cc864cda3a.png)。对于某些*i*的情况，其中![](img/7f1182f2-7155-4ec8-aca1-50a4033f4c85.png)，学习算法必须进行调整（即自我训练），通过寻找新的参数![](img/2be5be4c-924d-462b-b31e-0dd3771de027.png)，希望能够避免在未来犯下类似的错误。
- en: 'The science behind such algorithms varies from model to model, but the ultimate
    goals are usually the same:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此类算法背后的科学因模型而异，但最终目标通常是相同的：
- en: Reduce the number of errors, ![](img/dc0b01b2-c4f3-499b-8a4d-302e1d8fa01b.png),
    in every learning iteration.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少每次学习迭代中的错误数量，![](img/dc0b01b2-c4f3-499b-8a4d-302e1d8fa01b.png)。
- en: Learn the model parameters in as few iterations (steps) as possible.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能少的迭代（步骤）学习模型参数。
- en: Learn the model parameters as fast as possible.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能快地学习模型参数。
- en: Since most datasets deal with non-separable problems, the PLA is disregarded
    in favor of other algorithms that will converge faster and in fewer iterations.
    Many learning algorithms like this learn to adjust the parameters ![](img/10030282-7398-4858-8911-a2f401ae5315.png) by
    taking specific steps to reduce the error, ![](img/fa1abe60-b158-4da2-b57c-73f2a10d40cf.png),
    based on derivatives with respect to the variability of the error and the choice
    of parameters. So, the most successful algorithms (in deep learning, at least)
    are those based on some type of gradient descent strategy (Hochreiter, S., et.al.
    2001).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数数据集处理的是不可分问题，PLA通常会被忽略，转而使用其他能够更快收敛且迭代次数较少的算法。许多学习算法通过采取特定的步骤来调整参数![](img/10030282-7398-4858-8911-a2f401ae5315.png)，以减少错误![](img/fa1abe60-b158-4da2-b57c-73f2a10d40cf.png)，这些步骤是基于错误的变化性和参数选择的导数。因此，最成功的算法（至少在深度学习中）是那些基于某种梯度下降策略的算法（Hochreiter,
    S.等，2001）。
- en: Now, let's go over the most basic iterative gradient strategy. Say that we want
    to learn the parameters ![](img/293a24a9-8122-473a-b0e9-3f3c1f5f62e0.png) given
    the dataset, ![](img/66b34a6d-fe6c-42c0-b839-75629dd44781.png). We will have to
    make a small adjustment to the problem formulation to make things a little easier.
    What we want is for ![](img/4e82d5bd-8068-4b93-8586-84592a77fc05.png) to be implied
    in the expression ![](img/4e4b289f-920a-4351-9622-14e68b8cacce.png). The only
    way this could work is if we set ![](img/f1ff6cd9-db78-495b-b8ed-23cf890c12cb.png)and ![](img/357ebb91-cfb5-4c22-b82d-49ddbb38b5f2.png).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下最基本的迭代梯度策略。假设我们想要通过给定的数据集，![](img/66b34a6d-fe6c-42c0-b839-75629dd44781.png)，来学习参数 ![](img/293a24a9-8122-473a-b0e9-3f3c1f5f62e0.png)。为了让事情变得稍微简单一些，我们需要对问题的表述进行一些小调整。我们希望 ![](img/4e82d5bd-8068-4b93-8586-84592a77fc05.png) 能够在表达式 ![](img/4e4b289f-920a-4351-9622-14e68b8cacce.png) 中隐含出来。唯一能够实现这一点的方式是，如果我们设置 ![](img/f1ff6cd9-db78-495b-b8ed-23cf890c12cb.png) 和 ![](img/357ebb91-cfb5-4c22-b82d-49ddbb38b5f2.png)。
- en: 'With this simplification, we can simply search for **w**, which implies a search
    for **b** as well. Gradient descent with a fixed *learning rate* is as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个简化，我们可以直接搜索**w**，这也意味着同时搜索**b**。使用固定的*学习率*进行梯度下降如下所示：
- en: Initialize the weights to zero ( ![](img/1f4e3960-c78b-45c9-90e2-3921c8452c3a.png)) and
    the iteration counter to zero (![](img/6ce01007-2b63-4bc4-8db8-4f1d0f7c33a1.png)).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重初始化为零（ ![](img/1f4e3960-c78b-45c9-90e2-3921c8452c3a.png)）并将迭代计数器初始化为零（![](img/6ce01007-2b63-4bc4-8db8-4f1d0f7c33a1.png)）。
- en: 'When ![](img/429a3278-fffb-4640-8730-c1c103a64135.png), do the following:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 ![](img/429a3278-fffb-4640-8730-c1c103a64135.png)时，执行以下操作：
- en: Calculate the gradient with respect to ![](img/23cda490-47ff-4f78-b5fc-195f6d6b2d0a.png) and
    store it in ![](img/99c1192e-4889-40ad-9e1b-34a8ad598d5f.png).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相对于 ![](img/23cda490-47ff-4f78-b5fc-195f6d6b2d0a.png) 的梯度，并将其存储在 ![](img/99c1192e-4889-40ad-9e1b-34a8ad598d5f.png)中。
- en: Update ![](img/8074e322-e8f8-4e22-828c-06357c3902b8.png) so that it looks like
    this: ![](img/1dfaa02c-ed8e-4bd1-aa7a-4ebd9003ccc8.png).
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/8074e322-e8f8-4e22-828c-06357c3902b8.png)，使其看起来像这样：![](img/1dfaa02c-ed8e-4bd1-aa7a-4ebd9003ccc8.png)。
- en: Increase the iteration counter and repeat.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加迭代计数器并重复。
- en: 'There are a couple of things that need to be explained here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个需要解释的地方：
- en: The gradient calculation, ![](img/d6ee9c14-1d4a-4ce7-92eb-66e6f7639c98.png),
    is not trivial. For some specific machine learning models, it can be determined
    analytically; but in most cases, it must be determined numerically by using some
    of the latest algorithms.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度计算，![](img/d6ee9c14-1d4a-4ce7-92eb-66e6f7639c98.png)，并非易事。对于某些特定的机器学习模型，它可以通过解析方式确定；但在大多数情况下，必须通过使用一些最新的算法来数值求解。
- en: We still need to define how the error,![](img/497c3550-a1d6-4dae-bbf2-c38fce7e4fe8.png),
    is calculated; but this will be covered in the next section of this chapter.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仍然需要定义如何计算误差，![](img/497c3550-a1d6-4dae-bbf2-c38fce7e4fe8.png)；但这将在本章的下一部分中讲解。
- en: A learning rate, ![](img/1553a1e5-b84e-433e-8a44-39ec9c0a72ae.png), needs to
    be specified as well, which is a problem in itself.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要指定一个学习率，![](img/1553a1e5-b84e-433e-8a44-39ec9c0a72ae.png)，这本身就是一个问题。
- en: One way of looking at this last issue is that in order to find the parameter, ![](img/de1605e4-2c5a-4802-919c-f70cf201ca77.png),
    that minimizes the error, we need parameter ![](img/68663c10-bdf9-4cd2-8f73-e5c381771b62.png).
    Now, we could, when applying gradient descent, think about finding the ![](img/10ee9ff7-c459-40a1-ae7e-e9f5d4fdfbe5.png) parameter, but
    we will then fall into an infinite cycle. We will not go into more detail about
    gradient descent and its learning rate since, nowadays, algorithms for gradient
    descent often include automatic calculations of it or adaptive ways of adjusting
    it (Ruder, S. 2016).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个最后问题的一种方式是：为了找到最小化误差的参数 ![](img/de1605e4-2c5a-4802-919c-f70cf201ca77.png)，我们需要参数 ![](img/68663c10-bdf9-4cd2-8f73-e5c381771b62.png)。现在，在应用梯度下降时，我们可以考虑找到 ![](img/10ee9ff7-c459-40a1-ae7e-e9f5d4fdfbe5.png) 参数，但那样我们就会陷入一个无限循环。由于现如今梯度下降的算法通常会自动计算学习率或使用自适应方法来调整它（Ruder,
    S. 2016），我们不再详细讨论梯度下降及其学习率的问题。
- en: Multi-class classification
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多类分类
- en: Classifying into multiple categories can have an important effect on the performance
    of learning algorithms. In a general sense, the performance of a model will decrease
    with the number of classes it is required to recognize. The exception is if you
    have plenty of data and access to lots of computing power because if you do, you
    can overcome the limitations of poor datasets that have class imbalance problems
    and you can estimate massive gradients and make large calculations and updates
    to the model. Computing power may not be a limitation in the future, but at the
    moment it is.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分类为多个类别会对学习算法的性能产生重要影响。一般来说，模型的性能会随着需要识别的类别数量的增加而下降。例外情况是，如果你有大量的数据和强大的计算能力，因为这样你可以克服数据集中的类别不平衡问题，估计巨大的梯度，并进行大规模的计算和模型更新。计算能力未来可能不再是限制因素，但目前仍然是。
- en: The multiple classes problem can be solved by using strategies such as **one
    versus one** or **one versus all**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别问题可以通过使用如**一对一**或**一对多**等策略来解决。
- en: 'In one versus all, you essentially have an expert binary classifier that is
    really good at recognizing one pattern from all the others and the implementation
    strategy is typically cascaded. An example is shown here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对多（one versus all）中，你实际上有一个专家二分类器，擅长从所有其他模式中识别出一个模式，而其实现策略通常是级联的。下面是一个示例：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is a graphical explanation of this strategy. Suppose we have two-dimensional
    data that tells us something about the four seasons of the year, as shown:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是该策略的图示。假设我们有二维数据，能够告诉我们一年四季的一些信息，如下所示：
- en: '![](img/6ea29b49-ff32-4be0-a0cc-40d19d1896f6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ea29b49-ff32-4be0-a0cc-40d19d1896f6.png)'
- en: Figure 4.1 - Randomized two-dimensional data that could tell us something about
    the four seasons of the year
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 - 随机化的二维数据，可能告诉我们一年四季的一些信息
- en: 'In this case of randomized two-dimensional data, we have four categories corresponding
    to the seasons of the year. Binary classification will not work directly. However,
    we could train expert binary classifiers that specialize in *one* specific category
    *versus all* the rest. If we train one binary classifier to determine whether
    data points belong to the Summer category, using a simple perceptron, we could
    get the separating hyperplane shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种随机化的二维数据情况下，我们有四个类别，对应于四季。二分类直接使用是行不通的。然而，我们可以训练专家型二分类器，专门处理*一个*特定类别*与所有其他类别*的区别。如果我们训练一个二分类器来判断数据点是否属于夏季类别，使用一个简单的感知器，我们可以得到如下所示的分隔超平面：
- en: '![](img/ac4f8026-284f-43eb-89d9-1c25844ee256.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac4f8026-284f-43eb-89d9-1c25844ee256.png)'
- en: 'Figure 4.2: A PLA that is an expert in distinguishing from the Summer season
    data versus all the rest of the other seasons'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：一个专家型PLA，它擅长将夏季数据与其他所有季节数据区分开来
- en: Similarly, we can train the rest of the experts until we have enough to test
    our entire hypothesis; that is, until we are able to distinguish all the classes
    from each other.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以训练其余的专家，直到我们拥有足够的专家来测试整个假设；也就是说，直到我们能够将所有类别相互区分开来。
- en: Another alternative is to use classifiers that can handle multiple outputs;
    for example, decision trees or ensemble methods. But in the case of deep learning
    and neural networks, this refers to networks that can have multiple neurons in
    the output layer, such as the one depicted in *Figure 1.6* and *Figure 1.9* in [Chapter
    1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit), *Introduction
    to Machine Learning*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用能够处理多输出的分类器；例如，决策树或集成方法。但在深度学习和神经网络的情况下，这指的是可以在输出层有多个神经元的网络，例如在[第1章](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit)中展示的*图
    1.6*和*图 1.9*，*机器学习导论*。
- en: 'The mathematical formulation of a multi-output neural network only changes
    slightly from a single-output one in that the output is no longer within a binary
    set of values, such as ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png), but
    is now a vector of one-hot encoded values, such as ![](img/3476b823-1083-4257-a7e0-71949ade683a.png).
    In this case, |*C*| denotes the size of the set *C*, which contains all the different
    class labels. For the previous example, *C *would contain the following: *C* =
    {''*Summer*'', ''*Fall*'', ''*Winter*'', ''*Spring*''}. Here is what each one-hot
    encoding would look like:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 多输出神经网络的数学公式与单输出神经网络仅有细微差异，输出不再是一个二值集合，如！[](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png)，而是一个独热编码值的向量，如！[](img/3476b823-1083-4257-a7e0-71949ade683a.png)。在这种情况下，|*C*|表示集合*C*的大小，其中包含所有不同的类别标签。对于前面的例子，*C*将包含以下内容：*C*
    = {'*夏季*', '*秋季*', '*冬季*', '*春季*'}。以下是每个独热编码的样子：
- en: '**Summer**: ![](img/383c2835-581b-49ae-b2d7-745c43d5ed09.png)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**夏季**：![](img/383c2835-581b-49ae-b2d7-745c43d5ed09.png)'
- en: '**Fall**: ![](img/08a54e98-f381-4fd1-8834-8ae7c8c6a885.png)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秋季**：![](img/08a54e98-f381-4fd1-8834-8ae7c8c6a885.png)'
- en: '**Winter**: ![](img/1688ddb7-c3c5-48eb-9514-54e26474bb93.png)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冬季**：![](img/1688ddb7-c3c5-48eb-9514-54e26474bb93.png)'
- en: '**Spring**: ![](img/6f45ab24-f3c6-45f1-b841-636832c00cf0.png)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**春季**：![](img/6f45ab24-f3c6-45f1-b841-636832c00cf0.png)'
- en: 'Every element in the target vector will correspond to the desired output of
    the four neurons. We should also point out that the dataset definition should now reflect
    that both the sample input data and the labels are vectors:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目标向量中的每个元素将对应四个神经元的期望输出。我们还应指出，数据集定义现在应该反映样本输入数据和标签都是向量：
- en: '![](img/556fdbd7-1f5b-4934-8441-d7a4d5a260d4.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/556fdbd7-1f5b-4934-8441-d7a4d5a260d4.png)'
- en: Another way of dealing with the problem of multiple-class classification is
    by using **regression**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 处理多类别分类问题的另一种方法是使用**回归**。
- en: Regression
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: Previously, we specified that for binary classification, the target variable
    could take on a set of binary values; for example, ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png).
    We also said that for multiple classification, we could modify the target variable
    to be a vector whose size depends on the number of classes, ![](img/a5246959-0c32-4040-aa86-b99a914f8c09.png).
    Well, regression problems deal with cases where the target variable is any real
    value, ![](img/8dbf0dcb-9e08-48c6-888e-9e62c7d77f96.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们指定了，对于二分类问题，目标变量可以取一组二值，如！[](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png)。我们还提到，对于多分类问题，可以修改目标变量，使其成为一个向量，向量的大小取决于类别的数量，如！[](img/a5246959-0c32-4040-aa86-b99a914f8c09.png)。那么，回归问题处理的情况是目标变量是任何实数值，如！[](img/8dbf0dcb-9e08-48c6-888e-9e62c7d77f96.png)。
- en: 'The implications here are very interesting because with a regression model
    and algorithm, we could *technically* do binary classification since the set of
    real numbers contains any binary set of numbers:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的含义非常有趣，因为使用回归模型和算法，我们可以*严格来说*进行二分类，因为实数集包含任何二值数字集合：
- en: '![](img/aeaca13b-8358-4c7e-8bf1-e50fae3e3f72.png).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/aeaca13b-8358-4c7e-8bf1-e50fae3e3f72.png)。'
- en: 'Further, if we change *C* = {''*Summer*'', ''*Fall*'', ''*Winter*'', ''*Spring*''}
    to a numerical representation instead, such as C = {0,1,2,3}, then *technically*,
    we would again use regression due to the same property:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们将*C* = {'*夏季*', '*秋季*', '*冬季*', '*春季*'}更改为数值表示形式，例如C = {0,1,2,3}，那么*严格来说*，我们又会使用回归，因为它具有相同的特性：
- en: '![](img/c06b709f-7343-4fe6-9e3b-712fac9177ae.png).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/c06b709f-7343-4fe6-9e3b-712fac9177ae.png)。'
- en: Although regression models can solve classification problems, it is recommended
    that you use models that are specialized in classification specifically and leave
    the regression models only for regression tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管回归模型可以解决分类问题，但建议使用专门针对分类的模型，并仅将回归模型用于回归任务。
- en: 'Even if regression models can be used for classification (Tan, X., et.al. 2012),
    they are ideal for when the target variable is a real number. Here is a sample
    list of regression problems:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即使回归模型可以用于分类（Tan, X., et.al. 2012），它们也非常适合目标变量是实数的情况。以下是一些回归问题的示例列表：
- en: When given an image, indicate how many people are in it (the output can be any
    integer >=0).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张图片时，指出其中有多少人（输出可以是任何大于等于0的整数）。
- en: When given an image, indicate the probability of it containing a cat (the output
    can be any real number between 0 and 1).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一张图片时，指出其中含有猫的概率（输出可以是0到1之间的任何实数）。
- en: When given a sequence of readings about temperature, determine what the temperature
    actually feels like (the output can be any integer whose range depends on the
    units).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一系列关于温度的读数，判断实际的体感温度（输出可以是任何整数，范围取决于单位）。
- en: When given the text of a tweet, determine the probability of it being offensive (the
    output can be any real number between 0 and 1).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一条推文的文本，判断其是否具有攻击性（输出可以是0到1之间的任何实数）。
- en: When given an image of a person, determine their age (the output can be any
    positive integer, usually less than 100).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个人的图片，判断他们的年龄（输出可以是任何正整数，通常小于100）。
- en: When given an entire document, determine the probable compression rate (the
    output can be any real number between 0 and 1).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一整篇文档，判断可能的压缩率（输出可以是0到1之间的任何实数）。
- en: When given satellite readings of a spectroradiometer, determine the corresponding
    infrared value (the output can be any real number).
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定卫星光谱辐射仪的读数，判断对应的红外值（输出可以是任何实数）。
- en: When given the headlines of some major newspapers, determine the price of oil
    (the output can be any real number >=0).
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一些主要报纸的头条，判断油价（输出可以是任何大于等于0的实数）。
- en: As you can see from this list, there are many possibilities due to the fact
    that the range of real numbers encompasses all integers and all positive and negative
    numbers, and even if the range is too broad for specific applications, the regression
    model can be scaled up or down to meet the range specifications.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个列表中可以看到，由于实数范围涵盖了所有整数以及所有正负数，可能性非常多，即使这个范围对于特定应用来说过于宽泛，回归模型也可以根据范围要求进行扩展或缩小。
- en: To explain the potential of regression models, let's start with a basic **linear
    regression** model and in later chapters, we will cover more complex regression
    models based on deep learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明回归模型的潜力，让我们从一个基本的**线性回归**模型开始，后续章节中我们将介绍基于深度学习的更复杂的回归模型。
- en: 'The linear regression model tries to solve the following problem:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型尝试解决以下问题：
- en: '![](img/970566f0-b7ac-4868-81dc-6b18e0b0fc2b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/970566f0-b7ac-4868-81dc-6b18e0b0fc2b.png)'
- en: 'The problem is solved for *i* = 1, 2, ..., *N.* We could, however, use the
    same trick as before and include the calculation of *b *in the same equation.
    So, we can say that we are trying to solve the following problem:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题的求解适用于*i* = 1, 2, ..., *N*。然而，我们也可以像之前一样使用相同的技巧，将*b*的计算包含在同一个方程中。所以，我们可以说我们正在尝试解决以下问题：
- en: '![](img/c802c25e-40e3-46c3-92bc-22fc0113e952.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c802c25e-40e3-46c3-92bc-22fc0113e952.png)'
- en: Once again, we are trying to learn the parameters, ![](img/d59f4bf8-7872-4734-8050-abd4ec68ecab.png),
    that yield ![](img/562075f4-c6a5-4f29-a6e4-84f8c4f329c3.png) for all cases of *i.*
    In the case of linear regression, the prediction, ![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png),
    should ideally be equal to the true target value, ![](img/2a654437-6c4b-4da0-a2ad-8d800b7847ac.png),
    if the input data, ![](img/5a5aa15c-23cf-45b3-801f-b4d8ca5449b8.png), somehow
    describes a perfect straight line. But because this is very unlikely, there has
    to be a way of learning the parameters, ![](img/d1d74b08-42ed-48ee-b896-c39e9f170034.png),
    even if ![](img/95a975fe-111d-454d-b981-b65ab5053683.png). To achieve this, the
    linear regression learning algorithm begins by describing a low penalty for small
    mistakes and a larger penalty for big mistakes. This does make sense, right? It
    is very intuitive.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们正在尝试学习参数，![](img/d59f4bf8-7872-4734-8050-abd4ec68ecab.png)，从而得到所有*i*情况下的![](img/562075f4-c6a5-4f29-a6e4-84f8c4f329c3.png)。在线性回归的情况下，预测值，![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png)，应该理想情况下等于真实目标值，![](img/2a654437-6c4b-4da0-a2ad-8d800b7847ac.png)，如果输入数据，![](img/5a5aa15c-23cf-45b3-801f-b4d8ca5449b8.png)，某种程度上能描述一条完美的直线。但由于这非常不可能，所以必须有一种方法来学习参数，![](img/d1d74b08-42ed-48ee-b896-c39e9f170034.png)，即使是这样，![](img/95a975fe-111d-454d-b981-b65ab5053683.png)。为了实现这一点，线性回归学习算法首先会对小错误给予低惩罚，对大错误给予较大惩罚。这是有道理的，对吧？这非常直观。
- en: 'A natural way of penalizing mistakes in proportion to their size is by squaring
    the difference between the prediction and the target. Here is an example of when
    the difference is small:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚预测误差的自然方式是通过将预测值与目标值之间的差异平方。以下是差异较小时的一个例子：
- en: '![](img/3f4c1827-d243-4724-b234-61abbeae95c8.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f4c1827-d243-4724-b234-61abbeae95c8.png)'
- en: 'Here is an example of when the difference is large:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个当差异较大时的例子：
- en: '![](img/879e3b84-a279-419f-bb57-1d71b48c3f14.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/879e3b84-a279-419f-bb57-1d71b48c3f14.png)'
- en: In both of these examples, the desired target value is `1`. In the first case,
    the predicted value of `0.98` is very close to the target and the squared difference
    is `0.0004`, which is small compared to the second case. The second prediction
    is off by `14.8`, which yields a squared difference of `219.4`. This seems reasonable
    and intuitive for building up a learning algorithm; that is, one that penalizes
    mistakes in proportion to how big or small they are.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中，期望的目标值是`1`。在第一种情况下，预测值`0.98`非常接近目标，平方差是`0.0004`，相对于第二种情况来说很小。第二个预测值偏离了`14.8`，这导致平方差为`219.4`。这对于构建学习算法来说是合理且直观的；即，惩罚错误的程度与其大小成正比。
- en: 'We can formally define the overall average error in function of the choice
    of parameters **w** as the averaged sum of all squared errors, which is also known
    as the **mean squared error (MSE)**:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以正式定义总的平均误差，作为参数**w**的选择的函数，即所有平方误差的平均和，这也被称为**均方误差（MSE）**：
- en: '![](img/6b88425f-07b8-4cf1-8c8c-5e15ad59dcc4.png).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/6b88425f-07b8-4cf1-8c8c-5e15ad59dcc4.png)。'
- en: 'If we define the prediction in terms of the current choice of ![](img/1d7cd898-6ee9-4952-9d43-b441c2412c88.png) as ![](img/2aa011c3-c882-4e20-a5b0-99e710545353.png),
    then we can rewrite the error function as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将预测定义为当前选择的![](img/1d7cd898-6ee9-4952-9d43-b441c2412c88.png)，即![](img/2aa011c3-c882-4e20-a5b0-99e710545353.png)，那么我们可以将误差函数重写如下：
- en: '![](img/b5a94efc-3f8c-47bc-8b0e-67bbf7851798.png).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b5a94efc-3f8c-47bc-8b0e-67bbf7851798.png)。'
- en: 'This can be simplified in terms of the ![](img/3f0e57b4-ce39-452d-ad17-78002c082199.png)-norm
    (also known as the Euclidean norm, ![](img/da7143d1-c3d3-4a85-aef9-2dd8b3aff0b3.png))
    by first defining a matrix of data ![](img/15a5e1a8-a00b-478c-bd88-29ffbcd6374e.png),
    whose elements are data vector ![](img/a0383757-b717-41b7-8e94-64b4fc666f00.png),
    and a vector of corresponding targets, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过![](img/3f0e57b4-ce39-452d-ad17-78002c082199.png)-范数（也称为欧几里得范数，![](img/da7143d1-c3d3-4a85-aef9-2dd8b3aff0b3.png)）来简化，首先定义一个数据矩阵![](img/15a5e1a8-a00b-478c-bd88-29ffbcd6374e.png)，其元素为数据向量![](img/a0383757-b717-41b7-8e94-64b4fc666f00.png)，以及一个相应的目标向量，如下所示：
- en: '![](img/bdd9cc28-d833-4f38-b123-f42e24649ef5.png).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/bdd9cc28-d833-4f38-b123-f42e24649ef5.png)。'
- en: 'The simplification of the error is then as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的简化形式如下：
- en: '![](img/b8f14b75-d5fd-42ed-8486-9a0c2d91df10.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8f14b75-d5fd-42ed-8486-9a0c2d91df10.png)'
- en: 'This can then be expanded into the following important equation:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以展开成以下重要方程：
- en: '![](img/56c796c0-55f6-447a-b116-0a44e5b47bcf.png).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/56c796c0-55f6-447a-b116-0a44e5b47bcf.png)。'
- en: 'This is important because it facilitates the calculation of the derivative
    of the error, ![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png), which is necessary
    for adjusting the parameters, ![](img/04db5ca2-e2be-4220-8c6d-e87905762461.png),
    in the direction of the derivative and in proportion to the error. Now, following
    the basic properties of linear algebra, we can say that the derivative of the
    error (which is called a gradient since it yields a matrix) is the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，因为它有助于计算误差的导数，![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png)，这是调整参数![](img/04db5ca2-e2be-4220-8c6d-e87905762461.png)所必需的，调整方向与误差的导数成比例。现在，按照线性代数的基本性质，我们可以说误差的导数（由于它会产生一个矩阵，因此被称为梯度）如下：
- en: '![](img/ffea4ee4-25f6-4e04-be3f-e846f88d33f2.png).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/ffea4ee4-25f6-4e04-be3f-e846f88d33f2.png)。'
- en: 'Because we want to find the parameters that yield the smallest error, we can
    set the gradient to `0` and solve for ![](img/c50dc76b-19c9-43bb-818a-a8a9f7c6ee3d.png).By
    setting the gradient to `0` and ignoring constant values, we arrive at the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望找到能够最小化误差的参数，我们可以将梯度设置为`0`，然后解出![](img/c50dc76b-19c9-43bb-818a-a8a9f7c6ee3d.png)。通过将梯度设置为`0`并忽略常数值，我们得出如下结果：
- en: '![](img/5eb81dc2-15b3-4822-a5ae-a533d265eb0c.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5eb81dc2-15b3-4822-a5ae-a533d265eb0c.png)'
- en: '![](img/7e5c5343-31fc-429b-8511-bc9a1c84fd1c.png).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7e5c5343-31fc-429b-8511-bc9a1c84fd1c.png)。'
- en: 'These are called **normal**** equations**(Krejn, S. G. E. 1982). Then, if we
    simply use the term ![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png), we arrive
    at the definition of a **pseudo-inverse** (Golub, G., and Kahan, W. 1965). The
    beauty of this is that we do not need to calculate the gradient iteratively to
    choose the best parameters, ![](img/0c2d89de-6794-45ef-9e8d-537d24518945.png). As
    a matter of fact, because the gradient is analytic and direct, we can calculate ![](img/b59769c0-1de8-4212-a587-0e18479a6b73.png)in
    one shot, as explained in this linear regression algorithm:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被称为**正规方程**（Krejn, S. G. E. 1982）。然后，如果我们简单地使用术语![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png)，我们就得到了**伪逆**的定义（Golub,
    G. 和 Kahan, W. 1965）。这一方法的美妙之处在于，我们不需要通过迭代计算梯度来选择最佳参数，![](img/0c2d89de-6794-45ef-9e8d-537d24518945.png)。事实上，由于梯度是解析的和直接的，我们可以一次性计算![](img/b59769c0-1de8-4212-a587-0e18479a6b73.png)，正如在这个线性回归算法中所解释的那样：
- en: From ![](img/f94f1b86-f92e-4f64-99e2-2b8cd4d9a7e0.png), construct the pair, ![](img/29abe296-f275-4635-a806-55a85636b84d.png).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从![](img/f94f1b86-f92e-4f64-99e2-2b8cd4d9a7e0.png)构建对，![](img/29abe296-f275-4635-a806-55a85636b84d.png)。
- en: Estimate the pseudo-inverse ![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png).
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 估算伪逆![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png)。
- en: Calculate and return ![](img/1905d8ba-cdea-4768-af8c-e0528aec4962.png).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并返回![](img/1905d8ba-cdea-4768-af8c-e0528aec4962.png)。
- en: 'To show this graphically, let''s say that we have a system that sends a signal
    that follows a linear function; however, the signal, when it is transmitted, becomes
    contaminated with normal noise with a `0` mean and unit variance and we are only
    able to observe the noisy data, as shown:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地展示这一点，假设我们有一个系统，它发送一个遵循线性函数的信号；然而，当信号被传输时，它会被常规噪声污染，噪声具有`0`均值和单位方差，并且我们只能观察到被噪声污染的数据，如下所示：
- en: '![](img/0aaa35da-96e2-4872-9d97-880766124f97.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0aaa35da-96e2-4872-9d97-880766124f97.png)'
- en: Figure 4.3 - Data readings that are contaminated with random noise
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 - 被随机噪声污染的数据读取
- en: 'If, say, a hacker reads this data and runs linear regression to attempt to
    determine the true function that produced this data before it was contaminated,
    then the data hacker would obtain the solution shown here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 比如说，一个黑客读取了这些数据，并运行线性回归尝试确定在数据被污染之前产生这些数据的真实函数，那么这个数据黑客将得到这里显示的解：
- en: '![](img/cb38549f-33ef-47a1-a783-777e0249d647.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb38549f-33ef-47a1-a783-777e0249d647.png)'
- en: Figure 4.4 - A linear regression solution to the problem of finding the true
    function given noisy data readings
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 - 针对给定噪声数据读取问题的线性回归解法
- en: 'Clearly, as the previous figure shows, the linear regression solution is very
    close to the true original linear function. In this particular example, a high
    degree of closeness can be observed since the data was contaminated with noise
    that follows a pattern of **white noise**; however, for different types of noise,
    the model may not perform as well as in this example. Furthermore, most regression
    problems are not linear at all; in fact, the most interesting regression problems
    are highly non-linear. Nonetheless, the basic learning principle is the same:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，正如前面的图所示，线性回归解法非常接近真实的原始线性函数。在这个特定的例子中，可以观察到较高的接近度，因为数据被噪声污染，而这种噪声符合**白噪声**的模式；然而，对于不同类型的噪声，模型的表现可能不如这个例子中的效果。此外，大多数回归问题根本不是线性的；实际上，最有趣的回归问题是高度非线性的。尽管如此，基本的学习原理是相同的：
- en: Reduce the number of errors,![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png),
    in every learning iteration (or directly in one shot, such as in linear regression).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次学习迭代中（或者直接一次性得到结果，如线性回归中所示），减少错误的数量，![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png)。
- en: Learn the model parameters in as few iterations (steps) as possible.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尽可能少的迭代（步骤）中学习模型参数。
- en: Learn the model parameters as fast as possible.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽快学习模型参数。
- en: The other major component that guides the learning process is the way the success
    or error is calculated with respect to a choice of parameters, ![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png).
    In the case of the PLA, it simply found a mistake and adjusted with respect to
    it. For multiple classes, this was through a process of gradient descent over
    some measure of error and in linear regression, this was through direct gradient
    calculation using the MSE. But now, let's dive deeper into other types of error
    measures and successes that can be quantitative and qualitative.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 引导学习过程的另一个重要组成部分是成功或错误的计算方式，这与选择的参数有关，![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png)。在PLA的情况下，它简单地找到了错误并进行了调整。对于多类问题，这是通过在某些误差度量上进行梯度下降的过程；而在线性回归中，这是通过直接使用均方误差（MSE）进行梯度计算。但现在，让我们深入探讨其他可以量化和定性分析的误差度量和成功标准。
- en: Measuring success and error
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衡量成功与错误
- en: 'There is a wide variety of performance metrics that people use in deep learning
    models, such as accuracy, balanced error rate, mean squared error, and many others.
    To keep things organized, we will divide them into three groups: for binary classification,
    for multiple classes, and for regression.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型中，使用的性能度量标准种类繁多，如准确率、平衡误差率、均方误差等。为了保持条理，我们将它们分为三类：二分类、多分类和回归。
- en: Binary classification
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二分类
- en: 'There is one essential tool used when analyzing and measuring the success of
    our models. It is known as a **c****onfusion matrix**. A confusion matrix is not
    only helpful in visually displaying how a model makes predictions, but we can
    also retrieve other interesting information from it. The following diagram shows
    a template of a confusion matrix:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析和衡量模型成功与否时，有一个重要的工具被广泛使用，它被称为**混淆矩阵**。混淆矩阵不仅在直观展示模型如何进行预测时非常有用，我们还可以从中提取其他有趣的信息。下图展示了一个混淆矩阵的模板：
- en: '![](img/e700456f-66b7-4e67-91cc-b5cfa91d005d.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e700456f-66b7-4e67-91cc-b5cfa91d005d.png)'
- en: Figure 4.5 - A confusion matrix and the performance metrics derived from it
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 - 混淆矩阵及其派生的性能度量标准
- en: A confusion matrix and all the metrics derived from it are a very important
    way of conveying how good your models are. You should bookmark this page and come
    back to it whenever you need it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵及其派生的所有度量标准是传达模型好坏的重要方式。你应该将此页面加入书签，并在需要时随时返回查看。
- en: In the preceding confusion matrix, you will notice that it has two columns in
    the vertical axis that indicate the true target values, while in the horizontal
    axis, it indicates the predicted value. The intersection of rows and columns indicates
    the relationship of what should have been predicted against what was actually
    predicted. Every entry in the matrix has a special meaning and can lead to other
    meaningful composite performance metrics.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的混淆矩阵中，你会注意到它在垂直轴上有两列，表示真实的目标值，而在水平轴上，表示预测值。行与列的交点表示实际预测值与应预测值之间的关系。矩阵中的每个条目都有特殊的含义，并且可以推导出其他有意义的综合性能度量标准。
- en: 'Here is the list of metrics and what they mean:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是度量标准及其含义：
- en: '| **Acronym** | **Description** | **Interpretation** |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| **缩写** | **描述** | **解释** |'
- en: '| TP | *True Positive* | This is when a data point was of the positive class
    and was correctly predicted to be of the positive class. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| TP | *真阳性* | 这是指一个数据点属于正类，并且被正确预测为正类。 |'
- en: '| TN | *True Negative* | This is when a data point was of the negative class
    and was correctly predicted to be of the negative class. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| TN | *真阴性* | 这是指一个数据点属于负类，并且被正确预测为负类。 |'
- en: '| FP | *False Positive* | This is when a data point was of the negative class
    and was incorrectly predicted to be of the positive class. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| FP | *假阳性* | 这是指一个数据点属于负类，但被错误地预测为正类。 |'
- en: '| FN | *False Negative* | This is when a data point was of the positive class
    and was incorrectly predicted to be of the negative class. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| FN | *假阴性* | 这是指一个数据点属于正类，但被错误地预测为负类。 |'
- en: '| PPV | *Positive Predictive Value* or *Precision* | This is the proportion
    of positive values that are predicted correctly out of all the values predicted
    to be positive. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| PPV | *正预测值* 或 *精度* | 这是指所有预测为正类的值中，正确预测为正类的比例。 |'
- en: '| NPV | *Negative Predictive Value* | This is the proportion of negative values
    that are predicted correctly out of all the values that are predicted to be negative.
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| NPV | *负预测值* | 这是所有预测为负的值中，正确预测为负的比例。 |'
- en: '| FDR | *False Discovery Rate* | This is the proportion of incorrect predictions
    as false positives out of all the values that are predicted to be positive. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| FDR | *假发现率* | 这是所有预测为正的值中，错误预测为正的比例。 |'
- en: '| FOR | *False Omission Rate* | This is the proportion of incorrect predictions
    as false negatives out of all the values that are predicted to be negative. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| FOR | *假阴性率* | 这是所有预测为负的值中，错误预测为负的比例。 |'
- en: '| TPR | *True Positive Rate,* *Sensitivity*, *Recall*, *Hit Rate* | This is
    the proportion of predicted positives that are actually positives out of all that
    should be positives. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| TPR | *真正率*，*灵敏度*，*召回率*，*命中率* | 这是所有应为正类的值中，正确预测为正类的比例。 |'
- en: '| FPR | *False Positive Rate *or *Fall-Out* | This is the proportion of predicted
    positives that are actually negatives out of all that should be negatives. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| FPR | *假阳性率*或*假警报率* | 这是所有预测为正的值中，错误预测为负的比例。 |'
- en: '| TNR | **True Negative Rate**, *Specificity,* or *Selectivity* | This is the
    proportion of predicted negatives that are actually negatives out of all that
    should be negatives. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| TNR | **真阴性率**，*特异度*，或*选择性* | 这是所有应为负类的值中，正确预测为负类的比例。 |'
- en: '| FNR | **False Negative Rate**or *Miss Rate* | This is the proportion of predicted
    negatives that are actually positives out of all that should be positives. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| FNR | **假阴性率**或*漏检率* | 这是所有应为正类的值中，错误预测为负类的比例。 |'
- en: Some of these can be a little bit obscure to understand; however, you don't
    have to memorize them now, you can always come back to this table.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些中的一些可能会比较难以理解；不过，你现在不需要记住它们，随时可以回来查看这个表格。
- en: 'There are other metrics that are a little bit complicated to calculate, such
    as the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他指标，它们的计算稍微复杂一些，例如：
- en: '| **Acronym** | **Description** | **Interpretation** |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **缩写** | **描述** | **解释** |'
- en: '| ACC | *Accuracy* | This is the rate of correctly predicting the positives
    and the negatives out of all the samples. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ACC | *准确率* | 这是正确预测正类和负类的比例，基于所有样本。 |'
- en: '| *F*[1] | *F*[1]*-Score* | This is the average of the precisionand sensitivity.
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| *F*[1] | *F*[1]*-得分* | 这是精确度和灵敏度的平均值。 |'
- en: '| MCC | *Matthews Correlation Coefficient* | This is the correlation between
    the desired and the predicted classes. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| MCC | *马修斯相关系数* | 这是期望类别和预测类别之间的相关性。 |'
- en: '| BER | *Balanced Error Rate* | This is the average error rate for cases where
    there is a class imbalance. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| BER | *平衡错误率* | 这是在类别不平衡情况下的平均错误率。 |'
- en: I included, in this list of *complicated*calculations, acronyms such as **ACC**
    and **BER**, which are acronyms that have a very intuitive meaning. The main issue
    is, however, that these will vary when we have multiple classes. So, their calculation
    will be slightly different in multiple classes. The rest of the metrics remain
    exclusive (as defined) to binary classification.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份包含*复杂*计算的列表中，我加入了如**ACC**和**BER**这样的缩写，它们有非常直观的含义。然而，主要问题是当我们处理多类时，它们会有所变化。因此，在多类情况下，它们的计算会稍有不同。其余的指标依然是针对二分类的（如定义）。
- en: 'Before we discuss metrics for multiple classes, here are the formulas for calculating
    the previous metrics:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论多类指标之前，以下是计算之前这些指标的公式：
- en: '![](img/5276ec41-4004-4150-8622-dcf51e6d52c6.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5276ec41-4004-4150-8622-dcf51e6d52c6.png)'
- en: '![](img/40867f27-e549-47ff-95e9-55fe50bf85ad.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40867f27-e549-47ff-95e9-55fe50bf85ad.png)'
- en: '![](img/29c0a645-a95a-43e3-9a0a-6e7b4f2d3152.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29c0a645-a95a-43e3-9a0a-6e7b4f2d3152.png)'
- en: '![](img/17310bd8-85d1-4b80-8976-ec16f0bdd58b.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17310bd8-85d1-4b80-8976-ec16f0bdd58b.png)'
- en: In a general sense, you want **ACC**, **F[1]**, and **MCC** to be high and **BER**
    to be low.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般意义上，你希望**ACC**、**F[1]**和**MCC**尽量高，而**BER**尽量低。
- en: Multiple classes
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类
- en: When we go beyond simple binary classification, we often deal with multiple
    classes, such as *C* = {'*Summer*', '*Fall*', '*Winter*', '*Spring*'} or *C* =
    {0,1,2,3}. This can limit, to a certain point, the way we measure error or success.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们超越简单的二分类时，常常涉及多类问题，例如*C* = {'*夏季*'，'*秋季*'，'*冬季*'，'*春季*'}或*C* = {0,1,2,3}。这会在一定程度上限制我们衡量错误或成功的方式。
- en: 'Consider the confusion matrix for multiple classes shown here:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑多类的混淆矩阵，如下所示：
- en: '![](img/8b825d44-a56e-451e-8e4f-213c04487453.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b825d44-a56e-451e-8e4f-213c04487453.png)'
- en: Figure 4.6 - A confusion matrix for multiple classes
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 - 多类别的混淆矩阵
- en: 'From the following diagram, it is evident that the notion of true positive
    or negative has disappeared since we no longer have just positive and negative
    classes, but also sets of finite classes:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下图表可以明显看出，由于我们不再只有正负类别，而是有一组有限的类别，真正的正类或负类的概念已经消失：
- en: '![](img/f9d6ecb6-50a5-4079-84ed-85028b1436ab.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9d6ecb6-50a5-4079-84ed-85028b1436ab.png)'
- en: Individual classes, ![](img/16d50594-4373-4352-bfa9-e7c0c228c469.png), can be
    strings or numbers, as long as they follow the rules of sets. That is, the set
    of classes, ![](img/35755034-de0c-4834-9eb3-e8d7445dc5dd.png), must be finite
    and unique.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 各个类别，![](img/16d50594-4373-4352-bfa9-e7c0c228c469.png)，可以是字符串或数字，只要它们符合集合的规则。也就是说，类别的集合，![](img/35755034-de0c-4834-9eb3-e8d7445dc5dd.png)，必须是有限且唯一的。
- en: 'To measure ACC here, we will count all the elements in the main diagonal of
    the confusion matrix and divide it by the total number of samples:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 要在此测量ACC，我们将统计混淆矩阵主对角线上的所有元素，并将其除以样本的总数：
- en: '![](img/cb518092-12fe-4555-986f-95df3fd0102d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb518092-12fe-4555-986f-95df3fd0102d.png)'
- en: 'In this equation, ![](img/7e28c26a-52a3-4d61-ba59-d9c1bd3b9356.png) denotes
    the confusion matrix and ![](img/130d0fd7-ce83-47e6-b8c3-df493c208c66.png) denotes
    the trace operation; that is, the sum of the elements in the main diagonal of
    a square matrix. Consequently, the total error is `1-ACC`, but in the case of
    class imbalance, the error metric or plain accuracy may be deceiving. For this,
    we must use the BER metric, which for multiple classes can be defined as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![](img/7e28c26a-52a3-4d61-ba59-d9c1bd3b9356.png)表示混淆矩阵，![](img/130d0fd7-ce83-47e6-b8c3-df493c208c66.png)表示迹运算；即，方阵主对角线元素的和。因此，总误差为`1-ACC`，但在类别不平衡的情况下，误差指标或简单的准确率可能会产生误导。为此，我们必须使用BER指标，对于多个类别，定义如下：
- en: '![](img/e9878abc-5d61-4822-8244-c05f31a27b88.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9878abc-5d61-4822-8244-c05f31a27b88.png)'
- en: In this new formula for BER, ![](img/69426ee5-6e9a-46cf-a421-8d20e26857e7.png) refers
    to the element in the *j*th row and *i*th column of the confusion matrix, ![](img/308bc727-30e0-4f7f-b86d-9ca99ac39cc7.png).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新的BER公式中，![](img/69426ee5-6e9a-46cf-a421-8d20e26857e7.png)表示混淆矩阵中*第j*行和*第i*列的元素，![](img/308bc727-30e0-4f7f-b86d-9ca99ac39cc7.png)。
- en: Some machine learning schools of thought use the rows of the confusion matrix
    to denote true labels and the columns to denote the predicted labels. The theory
    behind the analysis is the same and the interpretation is, too. Don't be alarmed
    that `sklearn` uses the flipped approach; this is irrelevant and you should not
    have any problems with following any discussions about this.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一些机器学习学派使用混淆矩阵的行来表示真实标签，列来表示预测标签。分析背后的理论和解释都是相同的。不要对`sklearn`使用翻转的方法感到惊慌；这与此无关，你应该不会遇到任何理解上的问题。
- en: 'As an example, consider the dataset that was shown earlier in *Figure 4.1*.
    If we run a five-layered neural network classifier, we could obtain decision boundaries
    like this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，考虑之前在*图 4.1*中展示的数据集。如果我们运行一个五层的神经网络分类器，可能会得到像这样的决策边界：
- en: '![](img/170655fa-57cc-4652-bd30-d34d624664c8.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/170655fa-57cc-4652-bd30-d34d624664c8.png)'
- en: Figure 4.7 - Classification regions for a sample two-dimensional dataset with
    a five-layer neural net
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 - 使用五层神经网络对一个二维数据集进行分类的区域
- en: Clearly, the dataset is not perfectly separable by a non-linear hyperplane;
    there are some data points that cross the boundaries for each class. In the previous
    graph, we can see that only the *Summer* class has no points that are incorrectly
    classified based on the classification boundaries.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，该数据集不能通过一个非线性超平面完美分开；每个类别的边界都有一些数据点交叉。在前面的图表中，我们可以看到，只有*夏季*类别没有数据点被错误分类。
- en: 'However, this is more evident if we actually calculate and display the confusion
    matrix, shown here:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们实际计算并展示混淆矩阵，以下结果会更为明显：
- en: '![](img/dc71ff7b-05ae-410b-9ec5-b66bf34c08b0.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc71ff7b-05ae-410b-9ec5-b66bf34c08b0.png)'
- en: Figure 4.8 - A confusion matrix obtained from training errors on the sample
    two-dimensional dataset
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 - 从训练误差中获得的混淆矩阵，基于示例二维数据集
- en: 'In this case, the accuracy can be calculated as ACC=(25+23+22+24)/100, which
    yields an ACC of 0.94, which seems nice, and an error rate of 1-ACC = 0.06\. This
    particular example has a slight class imbalance. Here are the samples for each
    class:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，准确率可以计算为 ACC=(25+23+22+24)/100，得到的 ACC 为 0.94，看起来不错，错误率为 1-ACC = 0.06。这个例子有轻微的类别不平衡。这里是每个类别的样本数：
- en: 'Summer: 25'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏季：25
- en: 'Fall: 25'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秋季：25
- en: 'Winter: 24'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冬季：24
- en: 'Spring: 26'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 春季：26
- en: The Winter group has fewer examples than the rest and the Spring group has more
    examples than the rest. While this is a very small class imbalance, it can be
    enough to yield a deceivingly low error rate. We must now calculate the balanced
    error rate, BER.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 冬季组的样本数比其他组少，春季组的样本数比其他组多。虽然这是一个非常小的类别不平衡，但它可能足以导致一个误导性较低的错误率。现在我们必须计算平衡错误率
    BER。
- en: 'BER can be calculated as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: BER 可以按如下方式计算：
- en: '![](img/585f5334-7957-47d0-b576-3aec44b83eac.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/585f5334-7957-47d0-b576-3aec44b83eac.png)'
- en: '![](img/366a761b-958b-4dea-bf40-3ca41bcaaa2a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/366a761b-958b-4dea-bf40-3ca41bcaaa2a.png)'
- en: Here, the difference between the error rate and BER is a 0.01% under-estimation
    of the error. However, for classes that are highly imbalanced, the gap can be
    much larger and it is our responsibility to measure carefully and report the appropriate
    error measure, BER.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，错误率与 BER 之间的差异是 0.01% 的低估。然而，对于类别严重不平衡的情况，差距可能会大得多，因此我们有责任仔细测量并报告适当的错误度量，BER。
- en: Another interesting fact about BER is that it intuitively is the counterpart
    of a balanced accuracy; this means that if we remove the `1–` term in the BER
    equation, we are left with the balanced accuracy. Further, if we examine the terms
    in the numerator, we can see that the fractions on it lead to class-specific accuracies;
    for example, the first class, Summer, has a 100% accuracy, the second, Fall, has
    a 92% accuracy, and so on.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 BER 另一个有趣的事实是，它直观上是平衡准确率的对应物；这意味着如果我们去掉 BER 方程中的 `1–` 项，就剩下平衡准确率。此外，如果我们检查分子中的项，就可以看到其上的分数导致了每个类别的准确率；例如，第一个类别，夏季，准确率为
    100%，第二个，秋季，准确率为 92%，依此类推。
- en: 'In Python, the `sklearn` library has a class that can determine the confusion
    matrix automatically, given the true and predicted labels. The class is called `confusion_matrix`
    and it belongs to the `metrics` super class and we can use it as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，`sklearn` 库有一个类，可以根据真实标签和预测标签自动确定混淆矩阵。这个类叫做 `confusion_matrix`，它属于
    `metrics` 超类，我们可以通过以下方式使用它：
- en: '[PRE1]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If `y` contains the true labels, and `y_pred` contains the predicted labels,
    then the preceding instructions will output something like this:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `y` 包含真实标签，`y_pred` 包含预测标签，那么前述操作将输出类似这样的结果：
- en: '[PRE2]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can calculate BER by simply doing this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地做以下操作来计算 BER：
- en: '[PRE3]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will output the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE4]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Alternatively, `sklearn` has a built-in function to calculate the balanced
    accuracy score in the same super class as the confusion matrix. The class is called `balanced_accuracy_score`
    and we can produce BER by doing the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，`sklearn` 有一个内置函数，可以在与混淆矩阵相同的超级类中计算平衡准确率得分。这个类叫做 `balanced_accuracy_score`，我们可以通过以下方式计算
    BER：
- en: '[PRE5]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We get the following output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '[PRE6]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's now discuss the metrics for regression.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们讨论回归的指标。
- en: Regression
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归
- en: 'The most popular metric is **MSE**, which we discussed earlier in this chapter
    when explaining how linear regression works. However, we explained it as a function
    of the choice of hyperparameters. Here, we will redefine it in a general sense
    as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的指标是**MSE**，我们在本章前面解释线性回归如何工作时已讨论过该指标。然而，我们将它作为超参数选择的函数进行了解释。在这里，我们将其重新定义为一般意义上的如下：
- en: '![](img/ce4606dc-3c6f-42b1-9172-4094989cc388.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce4606dc-3c6f-42b1-9172-4094989cc388.png)'
- en: 'Another metric that is very similar to MSE is **mean absolute error** (**MAE**).
    While MSE penalizes big mistakes more (quadratically) and small errors much less,
    MAE penalizes everything in direct proportion to the absolute difference between
    what should be and what was predicted. This is a formal definition of MAE:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与 MSE 非常相似的指标是**均值绝对误差**（**MAE**）。虽然 MSE 对大错误的惩罚更严重（平方惩罚），而对小错误的惩罚较轻，MAE
    则直接按实际值和预测值之间的绝对差异惩罚所有误差。这是 MAE 的正式定义：
- en: '![](img/240ba3a7-8194-473f-9e0f-5bda05e4c582.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/240ba3a7-8194-473f-9e0f-5bda05e4c582.png)'
- en: 'Finally, out of the other measures for regression, the popular choice in deep
    learning is the ***R*² score**,also known as the **coefficient of determination**.
    This metric represents the proportion of variance, which is explained by the independent
    variables in the model. It measures how likely the model is to perform well on
    unseen data that follows the same statistical distribution as the training data.
    This is its definition:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在其他回归评估指标中，深度学习中常用的指标是***R*² 得分**，也叫做**决定系数**。这个指标表示模型中独立变量所解释的方差比例。它衡量模型在面对与训练数据具有相同统计分布的未见数据时，表现良好的可能性。这是它的定义：
- en: '![](img/0f2c6d77-a573-4d0a-b3c6-b582c4f166c7.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f2c6d77-a573-4d0a-b3c6-b582c4f166c7.png)'
- en: 'The sample mean is defined as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 样本均值定义如下：
- en: '![](img/7de549fd-f078-464a-904d-b20a8675fead.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7de549fd-f078-464a-904d-b20a8675fead.png)'
- en: 'Scikit-learn has classes available for each one of these metrics, indicated
    in the following table:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 提供了每个这些指标的类，见下表：
- en: '| **Regression metric** | **Scikit-learn class** |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| **回归指标** | **Scikit-learn 类** |'
- en: '| *R*² score | `sklearn.metrics.r2_score` |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| *R*² 得分 | `sklearn.metrics.r2_score` |'
- en: '| MAE | `sklearn.metrics.mean_absolute_error` |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| MAE | `sklearn.metrics.mean_absolute_error` |'
- en: '| MSE | `sklearn.metrics.mean_squared_error` |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| MSE | `sklearn.metrics.mean_squared_error` |'
- en: All of these classes take the true labels and predicted labels as input arguments.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些类都将真实标签和预测标签作为输入参数。
- en: 'As an example, if we take the data and linear regression model shown in *Figure
    4.3* and *Figure 4.4* as input, we can determine the three error metrics, as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，如果我们将*图 4.3* 和 *图 4.4* 中展示的线性回归模型及其数据作为输入，我们可以确定三个误差指标，如下所示：
- en: '[PRE7]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE8]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following graph shows the sample data used, along with the performance
    obtained. Clearly, the performance using the three performance metrics is good:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用的样本数据及其获得的性能。显然，使用这三种性能指标的表现很好：
- en: '![](img/d22cb47d-a379-462d-bf76-0acbf7ddda8e.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d22cb47d-a379-462d-bf76-0acbf7ddda8e.png)'
- en: Figure 4.9 - Error metrics over a linear regression model on data contaminated
    with white noise
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 - 在含有白噪声的线性回归模型上的误差指标
- en: In general, you always want to have a determination coefficient that is as close
    to `1` as possible and all your errors (MSE and MAE) as close to `0` as possible.
    But while all of these are good metrics to report on our models, we need to be
    careful to report these metrics over **unseen validation** or **test data**. This
    is so that we accurately measure the generalization ability of the model and identify
    overfitting in our models before it becomes a catastrophic error.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你总是希望确定系数尽可能接近 `1`，并且所有的误差（MSE 和 MAE）尽可能接近 `0`。尽管这些都是很好的模型评估指标，但我们需要注意的是，要在**未见的验证**或**测试数据**上报告这些指标。这样做可以准确衡量模型的泛化能力，并在模型发生灾难性错误之前识别出过拟合。
- en: Identifying overfitting and generalization
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别过拟合和泛化
- en: Often, when we are in a controlled machine learning setting, we are given a
    dataset that we can use for training and a different set that we can use for testing.
    The idea is that you only run the learning algorithm on the **training** data,
    but when it comes to seeing how good your model is, you feed your model the **test**
    data and observe the output. It is typical for competitions and hackathons to
    give out the test data but withhold the labels associated with it because the
    winner will be selected based on how well the model performs on the test data
    and you don't want them to cheat by looking at the labels of the test data and
    making adjustments. If this is the case, we can use a **validation** dataset,
    which we can create by ourselves by separating a portion of the training data
    to be the validation data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在受控的机器学习环境中，我们会得到一个可以用于训练的数据集和一个可以用于测试的不同数据集。其思路是：你只在**训练**数据上运行学习算法，但当你想了解模型的表现时，你将**测试**数据输入到模型中并观察输出。在竞赛和黑客马拉松中，通常会提供测试数据，但不会提供与之相关的标签，因为获胜者将根据模型在测试数据上的表现来选择，而不希望他们通过查看测试数据的标签并做出调整来作弊。如果是这种情况，我们可以使用**验证**数据集，我们可以通过从训练数据中分离一部分数据来创建验证数据集。
- en: The whole point of having separate sets, namely a validation or test dataset,
    is to measure the performance on this data, knowing that our model was not trained
    with it. A model's ability to perform equally, or close to equally, well on unseen
    validation or test data is known as **generalization.**
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有单独数据集的全部意义，即验证集或测试集，是为了评估在这些数据上的表现，因为我们知道我们的模型并没有用它来训练。一个模型能够在未见过的验证集或测试集上表现得同样好，或者接近同样好的能力，称为**泛化**。
- en: Generalization is the ultimate goal of most learning algorithms; all of us professionals
    and practitioners of deep learning dream of achieving great generalization in
    all of our models. Similarly, our greatest nightmare is **overfitting**.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化是大多数学习算法的最终目标；我们所有深度学习的专业人员和从业者都梦想在我们的所有模型中实现出色的泛化。同样，我们最大的噩梦是**过拟合**。
- en: Overfitting is the opposite of generalization. It occurs when our models perform
    extremely well on the training data but when presented with validation or test
    data, the performance decreases significantly. This indicates that our model almost
    memorized the intricacies of the training data and missed the big picture generalities
    of the sample space that lead to good models.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是泛化的对立面。当我们的模型在训练数据上表现非常好，但在面对验证集或测试集时，表现显著下降时，就发生了过拟合。这表明我们的模型几乎记住了训练数据的细节，而忽略了样本空间中的一般性规律，这些规律有助于产生良好的模型。
- en: 'In this and further chapters, we will follow these rules with respect to data
    splits:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章及后续章节中，我们将遵循以下数据拆分规则：
- en: If we are given test data (with labels), we will train on the training set and
    report the performance based on the test set.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们提供了带标签的测试数据，我们将基于训练集进行训练，并根据测试集报告性能。
- en: If we are not given test data (or if we have test data with no labels), we will
    split the training set, creating a validation set that we can report performance
    on using a cross-validation strategy.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们没有提供测试数据（或如果我们有无标签的测试数据），我们将拆分训练集，创建一个验证集，通过交叉验证策略报告性能。
- en: Let's discuss each scenario separately.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们单独讨论每种情况。
- en: If we have test data
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果我们有测试数据
- en: 'To begin this discussion, let''s say that we have a deep learning model with
    a set of hyper parameters, ![](img/fb401be2-c787-431a-ac8a-47d74004dcb3.png),
    which could be the weights of the model, the number of neurons, layers, the learning
    rate, the drop-out rate, and so on. Then, we can say that a model, ![](img/d790c8ca-2267-4437-b9df-2eb6ca27611c.png),
    (with parameters ![](img/cb15d854-ebcd-426f-9120-961cdcc51165.png)) that is trained
    with training data, ![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png), can have
    a training accuracy as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始讨论，假设我们有一个深度学习模型，并且有一组超参数，![](img/fb401be2-c787-431a-ac8a-47d74004dcb3.png)，这些超参数可能是模型的权重、神经元数量、层数、学习率、丢弃率等等。然后，我们可以说，一个用训练数据训练的模型，![](img/d790c8ca-2267-4437-b9df-2eb6ca27611c.png)，（具有超参数![](img/cb15d854-ebcd-426f-9120-961cdcc51165.png)）可以得到如下的训练准确率：
- en: '![](img/43cdd752-074f-4433-a75a-6c958eb9dbda.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43cdd752-074f-4433-a75a-6c958eb9dbda.png)'
- en: 'This is the training accuracy of a trained model on the training data. Consequently,
    if we are given labeled test data, ![](img/b2cc358b-36e4-4460-a96e-6f6234315997.png), with *M*
    data points, we can simply estimate the **test accuracy** by calculating the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个训练模型在训练数据上的训练准确率。因此，如果我们有带标签的测试数据，![](img/b2cc358b-36e4-4460-a96e-6f6234315997.png)，有*M*个数据点，我们可以通过计算以下内容来简单地估算**测试准确率**：
- en: '![](img/b3b5f613-b3a4-40fb-9e26-a7097dd0ea7b.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3b5f613-b3a4-40fb-9e26-a7097dd0ea7b.png)'
- en: 'One important property when reporting test accuracy usually holds true in most
    cases—all test accuracy is usually less than the training accuracy plus some noise
    caused by a poor selection of parameters:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告测试准确率时，有一个重要的特性在大多数情况下通常是成立的——所有的测试准确率通常小于训练准确率加上一些由于参数选择不当造成的噪声：
- en: '![](img/1e080952-7eb2-4e89-9893-848d25b5e9ae.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e080952-7eb2-4e89-9893-848d25b5e9ae.png)'
- en: This usually implies that if your test accuracy is significantly larger than
    your training accuracy, then there could be something wrong with the trained model.
    Also, we could consider the possibility that the test data is drastically different
    from the training data in terms of its statistical distribution and the multidimensional
    manifold that describes it.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常意味着，如果你的测试准确率明显高于训练准确率，那么可能存在训练模型的问题。此外，我们还可以考虑测试数据与训练数据在统计分布和描述它的多维流形上有显著差异的可能性。
- en: In summary, reporting performance on the test set is very important if we have
    test data that was properly chosen. Nonetheless, it would be completely normal
    for the performance to be less than it was in training. However, if it is significantly
    lower, there could be a problem of overfitting and if it is significantly greater,
    then there could be a problem with the code, the model, and even the choice of
    test data. The problem of overfitting can be solved by choosing better parameters, ![](img/cee96f36-9658-4621-9c99-9eadf7d7d500.png),
    or by choosing a different model, ![](img/d01418e4-a07a-41a4-83dd-3c65bde9d857.png),
    which is discussed in the next section.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果我们有正确选择的测试数据，那么在测试集上报告性能是非常重要的。然而，性能低于训练时的表现是完全正常的。不过，如果性能显著较低，可能存在过拟合问题；如果性能显著较高，则可能是代码、模型或测试数据选择有问题。过拟合问题可以通过选择更好的参数
    ![](img/cee96f36-9658-4621-9c99-9eadf7d7d500.png)，或选择不同的模型 ![](img/d01418e4-a07a-41a4-83dd-3c65bde9d857.png)
    来解决，这在下一节中有讨论。
- en: Now, let's briefly discuss a case where we don't have test data or we have test
    data with no labels.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要讨论一种情况，即我们没有测试数据，或者我们有没有标签的测试数据。
- en: No test data? No problem – cross-validate
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 没有测试数据？没问题——交叉验证
- en: Cross-validation is a technique that allows us to split the training data, ![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png), into
    smaller groups for training purposes. The most important point to remember is
    that the splits are ideally made of an equal number of samples overall and that
    we want to rotate the choice of groups for training and validation sets.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种技术，它允许我们将训练数据 ![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png) 分割成更小的组用于训练。需要记住的最重要一点是，分割应该是理想情况下具有相等数量的样本，并且我们要轮流选择用于训练和验证集的组。
- en: Let's discuss the famous cross-validation strategy known as ***k*-fold cross-validation** (Kohavi,
    R. 1995). The idea here is to divide the training data into *k* groups, which
    are (ideally) equally large, then select *k*-1 groups for training the model and
    measure the performance of the group that was left out. Then, change the groups
    each time until all the groups have been selected for testing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论著名的交叉验证策略——***k*-折交叉验证**（Kohavi, R. 1995）。这里的想法是将训练数据划分为 *k* 组，这些组（理想情况下）大小相等，然后选择
    *k*-1 组用于训练模型，并衡量被排除的组的性能。接着，每次更换组，直到所有组都被选中用于测试。
- en: 'In the previous sections, we discussed measuring performance using the standard
    accuracy, ACC, but we could use any performance metric. To show this, we will
    now calculate the MSE. This is how the *k*-fold cross-validation algorithm will
    look:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了使用标准准确度（ACC）来衡量性能，但我们也可以使用任何性能指标。为了展示这一点，我们现在将计算均方误差（MSE）。这就是 *k*-折交叉验证算法的样子：
- en: Input the dataset, ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png), the model, ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png),
    the parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png), and the number
    of folds, ![](img/1d85f939-85aa-4c66-8a53-c5451f35ba70.png).
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入数据集 ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png)，模型 ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png)，参数
    ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)，以及折数 ![](img/1d85f939-85aa-4c66-8a53-c5451f35ba70.png)。
- en: Divide the set of indices, ![](img/7976dcab-c59d-4f7d-adc1-46aa541730e0.png), into ![](img/7853dbb6-4144-4bdc-b6e9-f536bad0862d.png)groups
    (ideally equal in size), ![](img/1dab9af3-28c9-4288-b612-2871cba7b80e.png), such
    that ![](img/09103fa9-8eb1-4d3c-bda9-34629dee8f42.png).
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将索引集 ![](img/7976dcab-c59d-4f7d-adc1-46aa541730e0.png) 划分为 ![](img/7853dbb6-4144-4bdc-b6e9-f536bad0862d.png)
    组（理想情况下大小相等），![](img/1dab9af3-28c9-4288-b612-2871cba7b80e.png)，使得 ![](img/09103fa9-8eb1-4d3c-bda9-34629dee8f42.png)。
- en: 'For each case of ![](img/3ae133ea-6268-4366-b1b1-8a00537970d7.png), do the
    following:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个 ![](img/3ae133ea-6268-4366-b1b1-8a00537970d7.png) 情况，执行以下操作：
- en: Select the indices for training as [![](img/84dc0f45-f878-48b2-ade1-11e12122e33d.png)] and
    form the training set, [![](img/5bedbb94-b2d2-43fa-b526-2eda18f88551.png)].
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择训练集的索引，如 [![](img/84dc0f45-f878-48b2-ade1-11e12122e33d.png)]，并形成训练集，[![](img/5bedbb94-b2d2-43fa-b526-2eda18f88551.png)]。
- en: Select the indices for validation as ![](img/5e3df28e-3838-4ca1-b407-8f00962e9d85.png) and
    form the validation set, ![](img/107f4c15-a2f6-4f29-8a47-2393165addff.png).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择验证集的索引，如 ![](img/5e3df28e-3838-4ca1-b407-8f00962e9d85.png)，并形成验证集，![](img/107f4c15-a2f6-4f29-8a47-2393165addff.png)。
- en: Train the model with a choice of parameters over the training set: ![](img/9b069e01-510c-4bad-b188-0df9797af76a.png).
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用在训练集上选择的参数训练模型：![](img/9b069e01-510c-4bad-b188-0df9797af76a.png)。
- en: 'Compute the error of the model, [![](img/40685b40-6076-4fba-b045-11a6d39face0.png)], on
    the validation set : [![](img/cb27f873-4b41-48ee-b205-215589f73273.png)]'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算模型在验证集上的误差，[![](img/40685b40-6076-4fba-b045-11a6d39face0.png)]，[![](img/cb27f873-4b41-48ee-b205-215589f73273.png)]
- en: Return ![](img/c8557d04-4efd-4275-845a-ca9183a4be72.png) for all cases of ![](img/355f6774-f073-4a73-9a57-c7df89a2cbbe.png).
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回![](img/c8557d04-4efd-4275-845a-ca9183a4be72.png)对于所有![](img/355f6774-f073-4a73-9a57-c7df89a2cbbe.png)的情况。
- en: 'With this, we can calculate the cross-validation error (MSE) given by the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们可以计算交叉验证误差（MSE），公式如下：
- en: '![](img/f2e94af6-ec29-4bc0-9c9c-8cb59ba9ed2e.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2e94af6-ec29-4bc0-9c9c-8cb59ba9ed2e.png)'
- en: 'We can also calculate its corresponding standard deviation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算相应的标准差：
- en: '![](img/bba147e0-2d91-4e2c-bcb8-fb4db4c08f9a.png).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/bba147e0-2d91-4e2c-bcb8-fb4db4c08f9a.png)。'
- en: It is usually a good idea to look at the standard deviation of our performance
    metric—regardless of the choice—since it gives an idea of how consistent our performance
    on the validation sets is. Ideally, we would like to have a cross-validated MSE
    of `0`, ![](img/5f2ba80e-d9bd-4d27-9965-a87fa2cb272a.png), and a standard deviation
    of `1`, ![](img/51bcebac-4507-44fd-bae4-111feca2dbd9.png).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，观察我们性能度量的标准差是一个好主意——无论我们选择哪种方法——因为它能反映我们在验证集上的表现一致性。理想情况下，我们希望交叉验证的MSE为`0`，![](img/5f2ba80e-d9bd-4d27-9965-a87fa2cb272a.png)，标准差为`1`，![](img/51bcebac-4507-44fd-bae4-111feca2dbd9.png)。
- en: To explain this, we can use the regression example of the sample data contaminated
    by white noise, shown in *Figure 4.3* and *Figure 4.4*. To keep things simple
    for this example, we will use a total of 100 samples, *N*=100, and we will use
    3 folds. We will use scikit-learn's `KFold` class inside the `model_selection`
    super class and we will obtain the cross-validated MSE and its standard deviation.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释这一点，我们可以使用受白噪声污染的样本数据回归示例，见*图 4.3*和*图 4.4*。为了简化这个例子，我们将使用总共100个样本，*N*=100，并且使用3折交叉验证。我们将使用
    scikit-learn 的`KFold`类，在`model_selection`父类中，并获得交叉验证后的MSE及其标准差。
- en: 'To do this, we can use the following code and include other metrics as well:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们可以使用以下代码，并加入其他度量标准：
- en: '[PRE9]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result of this code will return something as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的结果会返回如下内容：
- en: '[PRE10]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'These results are cross-validated and give a clearer picture of the generalization
    abilities of the model. For comparison purposes, see the results shown in *Figure
    4.9*. You will notice that the results are very consistent between the performance
    measured before using the whole set in *Figure 4.9* and now, using only about
    66% of the data (since we split it into three groups) for training and about 33%
    for testing, as shown:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果是交叉验证后的，能更清晰地展示模型的泛化能力。为了比较目的，参考*图 4.9*中展示的结果。你会注意到，现在仅使用约66%的数据（因为我们将数据分成了三组）进行训练，约33%的数据用于测试，得到的结果与之前在*图
    4.9*中测得的性能结果非常一致，如下所示：
- en: '![](img/352f369c-e46e-403f-80db-daee8d4fc3d2.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/352f369c-e46e-403f-80db-daee8d4fc3d2.png)'
- en: Figure 4.10 - Cross-validated performance metrics with standard deviation in
    parenthesis
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 - 交叉验证性能度量，括号中为标准差
- en: The previous graph shows the linear regression solution found for every split
    of the data as well as the true original function; you can see that the solutions
    found are fairly close to the true model, yielding a good performance, as measured
    by ***R*²**, **MAE**, and **MSE**.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表展示了每次数据拆分所找到的线性回归解以及真实的原始函数；你可以看到，找到的解与真实模型非常接近，从而得到了良好的性能，这通过***R*²**、**MAE**和**MSE**来衡量。
- en: '**Exercise**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**'
- en: Go ahead and change the number of folds, progressively increasing it, and document
    your observations. What happens to the cross-validated performances? Do they stay
    the same, increase, or decrease? What happens to the standard deviations of the
    cross-validated performances? Do they stay the same, increase, or decrease? What
    do you think this means?
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续改变折数，逐步增加，并记录你的观察结果。交叉验证的性能有什么变化？它们是保持不变、增加还是减少？交叉验证性能的标准差发生了什么变化？它们是保持不变、增加还是减少？你认为这意味着什么？
- en: Usually, cross-validation is used on a dataset, ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png),
    with a model, ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png), trained on parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png).
    However, one of the greatest challenges in learning algorithms is finding the
    best set of parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png), that
    can yield the best (test or cross-validated) performance. Many machine learning
    scientists believe choosing the set of parameters can be **automated** with some
    algorithms and others believe this is an **art** (Bergstra, J. S., et.al. 2011).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，交叉验证用于数据集 ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png)，并用模型 ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png)
    在参数 ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png) 上进行训练。然而，学习算法中最大的一项挑战就是找到能够产生最佳（测试或交叉验证）性能的最佳参数集
    ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)。许多机器学习科学家认为，选择参数集的过程可以通过一些算法**自动化**，而另一些人则认为这是一种**艺术**（Bergstra,
    J. S., 等，2011年）。
- en: The art behind learning
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习背后的艺术
- en: For those of us who have spent decades studying machine learning, experience
    informs the way we choose parameters for our learning algorithms. But for those
    who are new to it, this is a skill that needs to be developed and this skill comes
    after learning how learning algorithms work. Once you have finished this book,
    I believe you will have enough knowledge to choose your parameters wisely. In
    the meantime, we can discuss some ideas for finding parameters automatically using
    standard and novel algorithms here.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们这些花费数十年研究机器学习的人来说，经验影响着我们为学习算法选择参数的方式。但对于那些初学者来说，这是需要培养的技能，而这种技能是在理解学习算法如何工作的基础上发展起来的。一旦你读完这本书，我相信你会掌握足够的知识，能够明智地选择参数。与此同时，我们可以在这里讨论一些使用标准和新颖算法自动寻找参数的思路。
- en: 'Before we go any further, we need to make a distinction at this point and define
    two major sets of parameters that are important in learning algorithms. These
    are as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要在此时做出区分，并定义在学习算法中非常重要的两大类参数。具体如下：
- en: '**Model parameters:** These are parameters that represent the solution that
    the model represents. For example, in perceptron and linear regression, this would
    be vector ![](img/5c1ca364-6e24-407a-8b15-dd412d3dda71.png)and scalar ![](img/f8b8288d-5510-4840-9753-7b375f040e5a.png),
    while for a deep neural network, this would be a matrix of weights, ![](img/74c8f74e-3b92-4f54-b1c7-85e2dcc4318e.png), and
    a vector of biases, ![](img/9fe4b0b9-c94e-4df7-bb9b-9efa507ee211.png). For a convolutional
    network, this would be filter sets.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数：** 这些是表示模型所代表的解决方案的参数。例如，在感知机和线性回归中，这可能是向量 ![](img/5c1ca364-6e24-407a-8b15-dd412d3dda71.png)
    和标量 ![](img/f8b8288d-5510-4840-9753-7b375f040e5a.png)，而对于深度神经网络来说，这可能是权重矩阵 ![](img/74c8f74e-3b92-4f54-b1c7-85e2dcc4318e.png)
    和偏置向量 ![](img/9fe4b0b9-c94e-4df7-bb9b-9efa507ee211.png)。对于卷积网络，这可能是滤波器集。'
- en: '**Hyperparameters:** These are parameters needed by the model to guide the
    learning process to search for a solution (model parameters) and are usually represented
    as ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png). For example, in the PLA,
    a hyperparameter would be the maximum number of iterations; in a deep neural network,
    it would be the number of layers, the number of neurons, the activation function
    for the neurons, and the learning rate; and for a **convolutional neural network**
    (**CNN**), it would be the number of filters, the size of filters, the stride,
    the pooling size, and so on.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数：** 这些是模型所需的参数，用来引导学习过程以寻找解决方案（模型参数），通常表示为 ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)。例如，在感知机中，超参数可能是最大迭代次数；在深度神经网络中，超参数可能包括层数、神经元的数量、神经元的激活函数以及学习率；对于**卷积神经网络**（**CNN**），超参数可能包括滤波器的数量、滤波器的大小、步幅、池化大小等。'
- en: Put in other words, the model parameters are determined, in part, by the choice
    of hyperparameters. Usually, unless there is a numerical anomaly, all learning
    algorithms will consistently find solutions (model parameters) for the same set
    of hyperparameters. So, one of the main tasks when learning is finding the best
    set of hyperparameters that will give us the best solutions.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型参数在一定程度上是由超参数的选择决定的。通常，除非出现数值异常，否则所有学习算法都会为相同的超参数集一致地找到解决方案（模型参数）。因此，学习过程中的主要任务之一就是找到最佳的超参数集，以便为我们提供最好的解决方案。
- en: 'To observe the effects of altering the hyperparameters of a model, let''s once
    more consider the four-class classification problem of the seasons, shown earlier
    in *Figure 4.7*. We will assume that we are using a fully connected network, such
    as the one described in [*Chapter 1*](e3181710-1bb7-4069-825a-a235355bc116.xhtml),
    *Introduction to Machine Learning*, and the hyperparameter we want to determine
    is the best number of layers. Just for didactic purposes, let''s say that the
    number of neurons in each layer will increase exponentially in each layer, as
    shown:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察改变模型超参数的影响，我们再次考虑之前展示的四类季节分类问题，参见*图4.7*。我们假设使用的是全连接网络，如[《第1章》](e3181710-1bb7-4069-825a-a235355bc116.xhtml)《机器学习导论》中所述，我们希望确定的超参数是最适合的层数。为了教学目的，假设每层的神经元数量会按指数增加，如下所示：
- en: '| **Layer** | **Neurons in each layer** |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| **层数** | **每层的神经元数量** |'
- en: '| 1 | (8) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (8) |'
- en: '| 2 | (16, 8) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 2 | (16, 8) |'
- en: '| 3 | (32, 16, 8) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 3 | (32, 16, 8) |'
- en: '| 4 | (64, 32, 16, 8) |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 4 | (64, 32, 16, 8) |'
- en: '| 5 | (128, 64, 32, 16, 8) |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 5 | (128, 64, 32, 16, 8) |'
- en: '| 6 | (256, 128, 64, 32, 16, 8) |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 6 | (256, 128, 64, 32, 16, 8) |'
- en: '| 7 | (512, 256, 128, 64, 32, 16, 8) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 7 | (512, 256, 128, 64, 32, 16, 8) |'
- en: In the previous configuration, the first number in the brackets corresponds
    to the number of neurons closest to the input layer, while the last number in
    the brackets corresponds to the number of neurons closest to the output layer
    (which consists of 4 neurons, one per class).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的配置中，括号中的第一个数字对应最接近输入层的神经元数量，而括号中的最后一个数字对应最接近输出层的神经元数量（输出层由4个神经元组成，每个类别一个）。
- en: 'So, the number of layers represents ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png),
    in this example. If we loop through each configuration and determine the cross-validated
    BER, we can determine which architecture yields the best performance; that is,
    we are optimizing ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png) for performance.
    The results obtained will look as follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，层数代表 ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)，在这个例子中。如果我们遍历每个配置并确定交叉验证后的BER，我们可以确定哪个架构表现最佳；也就是说，我们正在为性能优化 ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)。得到的结果如下所示：
- en: '| **Layers – [![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)]** | **1**
    | **2** | **3** | **4** | **5** | **6** | **7** |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **层数 – [![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)]** | **1** | **2**
    | **3** | **4** | **5** | **6** | **7** |'
- en: '| **BER** | 0.275 | 0.104 | 0.100 | 0.096 | 0.067 | 0.079 | 0.088 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **BER** | 0.275 | 0.104 | 0.100 | 0.096 | 0.067 | 0.079 | 0.088 |'
- en: '| **Standard deviation** | 0.22 | 0.10 | 0.08 | 0.10 | 0.05 | 0.04 | 0.08 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **标准差** | 0.22 | 0.10 | 0.08 | 0.10 | 0.05 | 0.04 | 0.08 |'
- en: 'From the results, we can easily determine that the best architecture is one
    with five layers since it has the lowest BER and the second smallest standard
    deviation. We could, indeed, gather all the data at each split for each configuration
    and produce the box plot shown here:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以很容易地确定，最佳架构是五层的，因为它具有最低的BER和第二小的标准差。事实上，我们可以收集每个配置在每个分割点的数据，并生成这里展示的箱线图：
- en: '![](img/3c159f1a-3d99-48ee-8517-ac6a33da553e.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c159f1a-3d99-48ee-8517-ac6a33da553e.png)'
- en: Figure 4.11 - A box plot of the cross-validated data optimizing the number of
    layers
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 - 优化层数的交叉验证数据箱线图
- en: This box plot illustrates a couple of important points. First, that there is
    a clear tendency of the model to reduce the BER as the number of layers increases
    up to `5`, then increases after that. This is very common in machine learning
    and it is known as the **overfitting curve**, which is usually a *u *shape (or
    *n *shape, for performance metrics that are better on higher values). The lowest
    point, in this case, would indicate the best set of hyperparameters (at `5`);
    anything to the left of that represents **underfitting** and anything to the right
    represents **overfitting**. The second thing that the box plot shows is that even
    if several models have a similar BER, we will choose the one that shows less variability
    and most consistency.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这个箱线图展示了几个重要的点。首先，模型在增加层数到`5`时，BER明显下降，之后又开始上升。这在机器学习中非常常见，被称为**过拟合曲线**，通常呈*u*形（或者对于高值表现较好的性能指标来说是*n*形）。在这种情况下，最低点表示最优的超参数（在`5`时）；该点左侧表示**欠拟合**，右侧则表示**过拟合**。第二个要点是，即使多个模型具有相似的BER，我们也会选择那个波动性较小且最稳定的模型。
- en: 'To illustrate the differences between underfitting, good fitting, and overfitting,
    we will show the decision boundaries produced by the worst underfit, the best
    fit, and the worst overfit. In this case, the worst underfit is one layer, the
    best fit is five layers, and the worst overfit is seven layers. Their respective
    decision boundaries are shown in *Figure 4.12*, *Figure 4.13*, and *Figure 4.14*, respectively:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明欠拟合、良拟合和过拟合之间的差异，我们将展示由最差的欠拟合、最好的拟合和最差的过拟合产生的决策边界。在这种情况下，最差的欠拟合是一个隐层，最好的拟合是五个隐层，最差的过拟合是七个隐层。它们各自的决策边界分别展示在
    *图 4.12*、*图 4.13* 和 *图 4.14* 中：
- en: '![](img/7e730ac9-4d3a-4868-9313-a783b113408f.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e730ac9-4d3a-4868-9313-a783b113408f.png)'
- en: Figure 4.12 - Classification boundaries for a one-hidden-layer network that
    is underfitting
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 - 一个单隐层网络的分类边界，出现了欠拟合
- en: 'In the preceding graph, we can see that the underfit is clear since there are
    decision boundaries that prevent many datapoints from being classified correctly:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到欠拟合的情况非常明显，因为存在决策边界，这些边界阻止了许多数据点被正确分类：
- en: '![](img/5e2f7982-3a60-4d8d-941a-0dc2d14e0b42.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e2f7982-3a60-4d8d-941a-0dc2d14e0b42.png)'
- en: Figure 4.13 - Classification boundaries for a five-hidden-layer network that
    has a relatively good fit
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 - 一个五隐层网络的分类边界，具有相对较好的拟合
- en: 'Similarly, the previous graph shows the decision boundaries, but compared to
    *Figure 4.12*, these boundaries seem to provide a nicer separation of the data
    points for the different groups—a good fit:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，前面的图展示了决策边界，但与 *图 4.12* 相比，这些边界似乎为不同组的数据显示了更好的分离——这是一个良好的拟合：
- en: '![](img/60fedb99-d5fc-4dfb-b89c-53f1318e7d43.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60fedb99-d5fc-4dfb-b89c-53f1318e7d43.png)'
- en: Figure 4.14 - Classification boundaries for a seven-hidden-layer network that
    is overfitting
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 - 一个七隐层网络的分类边界，出现了过拟合
- en: If you look closely, *Figure 4.12* shows that some regions are designated very
    poorly, while in *figure 4.14*, the network architecture is trying *too hard* to
    classify all the examples perfectly, to the point where the outlier in the *Fall* class
    (the yellow points) that goes into the region of the *Winter* class (the blue
    points) has its own little region, which may have negative effects down the road.
    The classes in *Figure 4.13* seem to be robust against some of the outliers and
    have well-defined regions, for the most part.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，*图 4.12* 显示某些区域被划分得非常差，而在 *图 4.14* 中，网络架构试图 *过于努力* 地完美分类所有样本，以至于 *Fall*
    类别（黄色点）的离群值进入了 *Winter* 类别（蓝色点）的区域，并且它有了自己的小区域，这可能会在后续产生负面影响。*图 4.13* 中的类别似乎对一些离群值具有鲁棒性，并且大多数情况下具有明确的区域。
- en: As we progress through this book, we will deal with more complex sets of hyperparameters.
    Here we just dealt with one, but the theory is the same. This method of looking
    at the best set of hyperparameters is known as an exhaustive search. However,
    there are other ways of looking at parameters, such as performing a **grid search.**
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续深入本书，我们将处理更多复杂的超参数集。这里我们只处理了一个，但理论是相同的。通过这种方式寻找最佳超参数集的方法被称为穷举搜索。然而，也有其他查看参数的方法，比如执行**网格搜索**。
- en: Suppose that you do not have a fixed way of knowing the number of neurons in
    each layer (as opposed to the earlier example); you only know that you would like
    to have something between `4` and `1024` neurons and something between `1` and
    `100` layers to allow deep or shallow models. In that case, you cannot do an exhaustive
    search; it would take too much time! Here, grid searchis used as a solution that
    will sample the search space in—usually—equally-spaced regions.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你没有固定的方法来确定每个层的神经元数量（与之前的例子不同）；你只知道你希望神经元数量在 `4` 到 `1024` 之间，层数在 `1` 到 `100`
    之间，以便允许深层或浅层模型。在这种情况下，你无法进行穷举搜索，因为那样需要太多时间！此时，网格搜索作为一种解决方案，用于在通常是等间距的区域内对搜索空间进行采样。
- en: For example, grid search can look at a number of neurons in the `[4, 1024]` range on
    10 equally spaced values—`4`, `117`, `230`, `344`, `457`, `570`, `684`, `797`,
    `910`, and `1024`—and the number of layers that is in the `[1,100]` range on 10
    equally spaced values—`1`, `12`, `23`, `34`, `45`, `56`, `67`, `78`, `89`, and
    `100`. Rather than looking at 1020*100=102,000 searches, it will look at 10*10=100,
    instead.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，网格搜索可以查看在 `[4, 1024]` 范围内的多个神经元数量，使用 10 个等间距的值——`4`、`117`、`230`、`344`、`457`、`570`、`684`、`797`、`910`
    和 `1024`——以及在 `[1, 100]` 范围内的层数，使用 10 个等间距的值——`1`、`12`、`23`、`34`、`45`、`56`、`67`、`78`、`89`
    和 `100`。与查看 1020*100=102,000 次搜索不同，网格搜索只需查看 10*10=100 次搜索。
- en: In `sklearn`, there is a class, `GridSearchCV`, that can return the best models
    and hyperparameters in cross-validation; it is part of the `model_selection` super
    class. The same class group has another class, called `RandomizedSearchCV`, which
    contains a methodology based on randomly searching the space. This is called **random
    search.**
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sklearn`中，有一个类，`GridSearchCV`，它可以在交叉验证中返回最佳模型和超参数；它是`model_selection`超类的一部分。同一类中还有另一个类，叫做`RandomizedSearchCV`，其方法基于在搜索空间中进行随机搜索。这就叫做**随机搜索**。
- en: In **random search**, the premise is that it will look within the `[4, 1024]` range and
    the `[1,100]` range for neurons and layers, respectively, by randomly drawing
    numbers uniformly until it reaches a maximum limit of total iterations.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在**随机搜索**中，前提是它将在`[4, 1024]`范围内以及`[1,100]`范围内随机抽取数字，分别用于神经元和层数，直到达到最大迭代次数的限制。
- en: Typically, if you know the range and distribution of the parameter search space,
    try a **grid search** approach on the space you believe is likely to have a better
    cross-validated performance. However, if you know very little or nothing about
    the parameter search space, use a **random search** approach. In practice, both
    of these methods work well.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你知道参数搜索空间的范围和分布，尝试在你认为可能具有更好交叉验证性能的空间上进行**网格搜索**。然而，如果你对参数搜索空间知之甚少或完全不了解，可以使用**随机搜索**方法。实际上，这两种方法都很有效。
- en: 'There are other, more sophisticated methods that work well but whose implementation
    in Python is not yet standard, so we will not cover them in detail here. However,
    you should know about them:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些更复杂的方法，它们效果很好，但在Python中的实现尚未标准化，因此我们在此不会详细介绍。不过，你应该了解这些方法：
- en: Bayesian hyperparameter optimization (Feurer, M., et.al. 2015)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯超参数优化（Feurer, M., 等，2015年）
- en: Evolution theory-based hyperparameter optimization (Loshchilov, I., et.al. 2016)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于进化理论的超参数优化（Loshchilov, I., 等，2016年）
- en: Gradient-based hyperparameter optimization (Maclaurin, D., et.al. 2015)
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于梯度的超参数优化（Maclaurin, D., 等，2015年）
- en: Least squares-based hyperparameter optimization (Rivas-Perea, P., et.al. 2014)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于最小二乘法的超参数优化（Rivas-Perea, P., 等，2014年）
- en: Ethical implications of training deep learning algorithms
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习算法训练的伦理影响
- en: There are a few things that can be said about the ethical implications of training
    deep learning models. There is potential harm whenever you are handling data that
    represents human perceptions. But also, data about humans and human interaction
    has to be rigorously protected and examined carefully before creating a model
    that will generalize based on such data. Such thoughts are organized in the following
    sections.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练深度学习模型的伦理影响，有一些要点可以提及。在处理代表人类感知的数据时，总是存在潜在的危害。但同样，关于人类和人类互动的数据必须严格保护，并且在创建基于此类数据的模型之前需要仔细审查。这些思考在接下来的部分中有详细展开。
- en: Reporting using the appropriate performance measures
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用适当的性能度量来报告
- en: Avoid faking good performance by picking the one performance metric that makes
    your model look good. It is not uncommon to read articles and reports of multi-class
    classification models that are trained over clear, class-imbalanced datasets but
    report the standard accuracy. Most likely, these models will report a high standard
    of accuracy since the models will be biased toward the over-sampled class and
    against the under-sampled groups. So, these types of models must report the balanced
    accuracy or the balanced error rate.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 避免通过选择唯一的性能指标来伪造良好的性能，这种做法让你的模型看起来很优秀。阅读文章和报告时，不乏有多分类模型在明显类别不平衡的数据集上训练，但仅报告标准的准确度。这些模型很可能会报告较高的准确率，因为模型会偏向过采样类别，而对欠采样类别产生偏见。因此，这些模型必须报告平衡准确度或平衡错误率。
- en: Similarly, for other types of classification and regression problems, you must
    report the appropriate performance metric. When in doubt, report as many performance
    metrics as you can. Nobody has ever complained about someone reporting model performance
    using too many metrics.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于其他类型的分类和回归问题，你必须报告适当的性能指标。如果不确定，报告尽可能多的性能指标是明智的。没有人会抱怨有人使用过多的指标来报告模型性能。
- en: The consequences of not reporting the appropriate metrics go from having biased
    models that go undetected and are deployed into production systems with disastrous
    consequences to having misleading information that can be detrimental to our understanding
    of specific problems and how models perform. We must recall that what we do may
    affect others and we need to be vigilant.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 不报告适当的度量标准可能带来的后果有很多，从拥有未被发现的偏差模型，并将其部署到生产系统中引发灾难性后果，到提供误导性信息，可能会对我们理解特定问题以及模型性能产生不利影响。我们必须记住，我们所做的可能会影响他人，因此需要保持警惕。
- en: Being careful with outliers and verifying them
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小心处理异常值并验证它们
- en: Outliers are usually seen as bad things to work around during the learning process
    and I agree. Models should be robust against outliers, unless they are not really
    outliers. If we have some data and we don't know anything about it, it is a safe
    assumption to interpret outliers as anomalies.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值通常被视为在学习过程中需要避免的坏东西，我同意这个观点。除非它们不是真正的异常值，否则模型应该对异常值具有鲁棒性。如果我们拥有一些数据，但对它一无所知，合理的假设是将异常值解读为异常现象。
- en: However, if we know anything about the data (because we collected it, were given
    all the information about it, or know the sensors that produced it), then we can
    verify that outliers are really outliers. We must verify that they were the product
    of human error when typing data or produced by a faulty sensor, data conversion
    error, or some other artifact because if an outlier is not the product of any
    of these reasons, there is no reasonable basis for us to assume that it is an
    outlier. In fact, data like this gives us important information about situations
    that may not occur frequently but will eventually happen again and the model needs
    to respond properly.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们对数据有所了解（因为我们收集了数据、获得了所有相关信息，或者知道产生数据的传感器），我们就可以验证异常值是否真的是异常值。我们必须验证它们是否是由于人工输入错误、传感器故障、数据转换错误或其他人为因素造成的。如果异常值不是这些原因的产物，那么我们没有合理的依据认为它是异常值。事实上，这样的数据给我们提供了关于可能不常发生，但最终会再次发生的情况的重要信息，模型需要做出正确的响应。
- en: 'Consider the data shown in the following figure. If we arbitrarily decide to
    ignore outliers without verification (such as in the top diagram), it may be that
    they are in fact not really outliers and the model will create a narrow decision
    space that ignores the outliers. The consequence, in this example, is that one
    point will be incorrectly classified as belonging to another group, while another
    point might be left out of the majority group:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑下图所示的数据。如果我们在没有验证的情况下随意决定忽略异常值（如顶部的图示所示），那么这些异常值实际上可能并不是真正的异常值，模型将会创建一个狭窄的决策空间，忽略这些异常值。在这个例子中，结果是一个点将被错误地分类为属于另一个群体，而另一个点可能会被排除在主要群体之外：
- en: '![](img/ad524a4e-52df-40bc-a71f-6c80e34dd76c.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad524a4e-52df-40bc-a71f-6c80e34dd76c.png)'
- en: Figure 4.15 - Differences in the learned space of my models. The top diagram
    shows the ignoring outliers outcome. The bottom diagram shows the including outliers
    outcome
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 - 我的模型在学习空间中的差异。顶部的图示展示了忽略异常值的结果。底部的图示展示了包括异常值的结果
- en: However, if we verify the data and discover that the outliers are completely
    valid input, the models might learn a better decision space that could potentially
    include the outliers. Nonetheless, this can yield a secondary problem where a
    point is classified as belonging to two different groups with different degrees
    of membership. While this is a problem, it is a much smaller risk than incorrectly
    classifying something. It is better to have, say, 60% certainty that a point belongs
    to one class and 40% certainty that it belongs to the other class, rather than
    classifying it incorrectly with 100% certainty.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们验证数据并发现异常值是完全有效的输入，那么模型可能会学习到一个更好的决策空间，这个空间可能包括异常值。然而，这可能会导致一个次要问题，即一个点被分类为属于两个不同的群体，并且具有不同程度的隶属关系。虽然这是一个问题，但它的风险远小于错误地将某些东西分类。比起以100%的确定性错误分类，最好是有60%的确定性认为一个点属于一个类别，40%的确定性认为它属于另一个类别。
- en: If you think about it, models that were built by ignoring outliers and then deployed
    into government systems can cause discrimination problems. They may show bias
    against minority or protected population groups. If deployed into incoming school
    student selection, it could lead to the rejection of exceptional students. If
    deployed into DNA classification systems, it could incorrectly ignore the similarity
    of two very close DNA groups. Therefore, always verify outliers if you can.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一下，忽略离群值构建的模型，若被部署到政府系统中，可能会导致歧视问题。它们可能对少数群体或受保护群体存在偏见。如果部署到学校学生选拔系统中，可能会导致优秀学生的被拒绝。如果部署到DNA分类系统中，可能会错误地忽略两个非常相近的DNA群体的相似性。因此，始终验证离群值，如果可能的话。
- en: Weight classes with undersampled groups
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于下采样的群体，使用加权类别。
- en: 'If you have a class imbalance, as in *Figure 4.15*, I recommend you try to
    balance the classes by getting more data rather than reducing it. If this is not
    an option, look into algorithms that allow you to weight some classes differently,
    so as to even out the imbalance. Here are a couple of the most common techniques:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到类不平衡的问题，如*图 4.15*所示，我建议你通过获取更多数据来平衡类，而不是减少数据。如果这不可行，可以考虑使用允许你为某些类别赋予不同权重的算法，从而平衡类之间的不均衡。以下是几种最常见的技术：
- en: On small datasets, use `sklearn` and the `class_weight` option. When training
    a model, it penalizes mistakes based on the provided weight for that class. There
    are a couple of automatic alternatives that you can look into that will also help,
    such as `class_weight="auto"` and `class_weight="balanced"`.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在小型数据集上，使用`sklearn`和`class_weight`选项。在训练模型时，它会根据提供的类权重惩罚错误。你还可以尝试一些自动化的替代方法，它们也会有所帮助，例如`class_weight="auto"`和`class_weight="balanced"`。
- en: On large datasets where batch training is used, use Keras and the `BalancedBatchGenerator`
    class. This will prepare a selection of samples (batches) that is consistently
    balanced each time, thereby guiding the learning algorithm to consider all groups
    equally. The class is part of `imblearn.keras`.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用批处理训练的大型数据集上，使用Keras和`BalancedBatchGenerator`类。每次准备的样本（批次）都会保持一致的平衡，从而引导学习算法平等地考虑所有群体。该类是`imblearn.keras`的一部分。
- en: You should try to use these strategies every time you want to have a model that
    is not biased toward a majority group. The ethical implications of this are similar
    to the previous points already mentioned. But above all, we must protect life
    and treat people with respect; all people have an equal, infinite worth.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 每次当你想要构建一个不偏向多数群体的模型时，你应该尝试使用这些策略。这些策略的伦理意义与前述已提到的点相似。但最重要的是，我们必须保护生命，并尊重他人；每个人都有平等且无限的价值。
- en: Summary
  id: totrans-348
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this basic-level chapter, we discussed the basics of learning algorithms
    and their purpose. Then, we studied the most basic way of measuring success and
    failure through performance analysis using accuracies, errors, and other statistical
    devices. We also studied the problem of overfitting and the super important concept
    of generalization, which is its counterpart. Then, we discussed the art behind
    the proper selection of hyperparameters and strategies for their automated search.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一基础章节中，我们讨论了学习算法的基础知识及其目的。接着，我们研究了通过准确率、误差和其他统计工具来衡量成功与失败的最基本方法。我们还探讨了过拟合问题以及其对应概念——泛化，这一概念非常重要。随后，我们讨论了超参数选择的艺术及其自动化搜索策略。
- en: After reading this chapter, you are now able to explain the technical differences
    between classification and regression and how to calculate different performance
    metrics, such as ACC, BER, MSE, and others, as appropriate for different tasks.
    Now, you are capable of detecting overfitting by using train, validation, and
    test datasets under cross-validation strategies, you can experiment with and observe
    the effects of altering the hyperparameters of a learning model. You are also
    ready to think critically about the precautions and devices necessary to prevent
    human harm caused by deep learning algorithms.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你现在可以解释分类和回归的技术差异，并能够计算不同的性能指标，如ACC、BER、MSE等，以适应不同的任务。现在，你可以通过交叉验证策略使用训练集、验证集和测试集来检测过拟合，能够尝试和观察调整学习模型超参数的效果。你也准备好批判性地思考预防深度学习算法对人类造成伤害所需的预防措施和设备。
- en: The next chapter is [*Chapter 5*](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml), *Training
    a Single Neuron,* which revises and expands the concept of a neuron, which was
    introduced in [*Chapter 1*](e3181710-1bb7-4069-825a-a235355bc116.xhtml), *Introduction
    to Machine Learning*, and shows its implementation in Python using different datasets
    to analyze the potential effects of different data; that is, linear and non-linearly
    separable data. However, before we go there, please try to quiz yourself using
    the following questions.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章是 [*第5章*](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml)，*训练单个神经元*，它修订并扩展了神经元的概念，神经元概念最初在
    [*第1章*](e3181710-1bb7-4069-825a-a235355bc116.xhtml)，*机器学习简介* 中介绍，并展示了其在 Python
    中的实现，使用不同的数据集分析不同数据可能产生的潜在影响；即，线性和非线性可分数据。然而，在进入这一部分之前，请尝试使用以下问题进行自我测验。
- en: Questions and answers
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**When you did the exercise on cross-validation, what happened to the standard
    deviation and what does that mean?**'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**当你做交叉验证练习时，标准差发生了什么变化，这意味着什么？**'
- en: The standard deviation stabilizes and reduces on more folds. This means that
    the performance measurements are more reliable; it is an accurate measure of generalization
    or overfitting.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 随着折数增加，标准差趋于稳定并减小。这意味着性能度量更加可靠；它是对泛化能力或过拟合的准确度量。
- en: '**What is the difference between hyperparameters and model parameters?**'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**超参数和模型参数有什么区别？**'
- en: Model parameters are numerical solutions to a learning algorithm; hyperparameters
    are what the model needs to know in order to find a solution effectively.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数是学习算法的数值解；超参数是模型需要了解的内容，以便有效地找到解决方案。
- en: '**Is a grid search faster than a randomized search for hyperparameters?**'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**网格搜索比随机搜索在超参数调优上更快吗？**'
- en: It depends. If the choice of hyperparameters affects the computational complexity
    of the learning algorithm, then both could behave differently. However, in similar
    search spaces and in the amortized case, both should finish at about the same
    time.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于。如果超参数的选择影响学习算法的计算复杂性，那么两者可能会表现得不同。然而，在类似的搜索空间和摊销的情况下，两者应该大致在相同时间完成。
- en: '**Can I use a regression-based learning algorithm for a classification problem?**'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我可以将基于回归的学习算法用于分类问题吗？**'
- en: Yes, as long as the labels, categories, or groups are mapped to a number in
    the set of real numbers.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 可以，只要标签、类别或组映射到实数集合中的一个数字。
- en: '**Can I use a classification-based learning algorithm for a regression problem?**'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**我可以将基于分类的学习算法用于回归问题吗？**'
- en: No.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 不是。
- en: '**Is the concept of a loss function the same as an error metric?**'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**损失函数的概念是否与误差度量相同？**'
- en: Yes and no. Yes, in the sense that a loss function will measure performance;
    however, the performance may not necessarily be with respect to the accuracy of
    classifying or regressing the data; it may be with respect to something else,
    such as the quality of groups or distances in information-theoretic spaces. For
    example, linear regression is based on the MSE algorithm as a loss function to
    minimize, while the loss function of the K-means algorithm is the sum of the squared
    distances of the data to their means, which it aims to minimize, but this does
    not necessarily mean it is an error. In the latter case, it is arguably meant
    as a cluster quality measure.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，但也不完全是。是的，从损失函数将衡量性能的角度来看；然而，性能不一定是关于分类或回归数据的准确性；它可能与其他方面有关，比如在信息论空间中的组或距离的质量。例如，线性回归基于最小化的
    MSE 算法作为损失函数，而 K-means 算法的损失函数是数据到其均值的平方距离之和，它的目标是最小化这个值，但这并不一定意味着它是一个误差。在后一种情况下，它可以说是作为一种聚类质量度量。
- en: References
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Lorena, A. C., De Carvalho, A. C., & Gama, J. M. (2008), A review on the combination
    of binary classifiers in multiclass problems, *Artificial Intelligence Review*,
    30(1-4), 19
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lorena, A. C., De Carvalho, A. C., & Gama, J. M. (2008)，在多类问题中组合二分类器的综述，*人工智能评论*，30(1-4)，19
- en: 'Hochreiter, S., Younger, A. S., & Conwell, P. R. (2001, August), Learning to
    learn using gradient descent, in *International Conference on Artificial Neural
    Networks* (pp. 87-94), Springer: Berlin, Heidelberg'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., Younger, A. S., & Conwell, P. R. (2001年8月)，使用梯度下降进行学习的研究，发表于
    *国际人工神经网络会议*（第87-94页），Springer：柏林，海德堡
- en: Ruder, S. (2016), An overview of gradient descent optimization algorithms, *arXiv*
    *preprint* arXiv:1609.04747
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruder, S. (2016), 梯度下降优化算法概述，*arXiv* *预印本* arXiv:1609.04747
- en: 'Tan, X., Zhang, Y., Tang, S., Shao, J., Wu, F., & Zhuang, Y. (2012, October),
    Logistic tensor regression for classification, in *International Conference on
    Intelligent Science and Intelligent Data Engineering* (pp. 573-581), Springer:
    Berlin, Heidelberg'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan, X., Zhang, Y., Tang, S., Shao, J., Wu, F., & Zhuang, Y. (2012年10月), 用于分类的逻辑张量回归，在*智能科学与智能数据工程国际会议*（第573-581页），Springer:
    柏林，海德堡'
- en: 'Krejn, S. G. E. (1982), *Linear Equations in Banach Spaces,* Birkhäuser: Boston'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Krejn, S. G. E. (1982), *巴拿赫空间中的线性方程*，Birkhäuser: 波士顿'
- en: 'Golub, G., & Kahan, W. (1965), Calculating the singular values and pseudo-inverse
    of a matrix, *Journal of the Society for Industrial and Applied Mathematics*,
    Series B: Numerical Analysis, 2(2), (pp. 205-224)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golub, G., & Kahan, W. (1965), 计算矩阵的奇异值和伪逆，*工业与应用数学学会期刊*，B卷：数值分析，2(2)，（第205-224页）
- en: Kohavi, R. (1995, August), A study of cross-validation and bootstrap for accuracy
    estimation and model selection, in *IJCAI*, 14(2), (pp. 1137-1145)
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohavi, R. (1995年8月), 交叉验证与自助法在准确性估计和模型选择中的研究，*IJCAI*，14(2)，（第1137-1145页）
- en: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011), Algorithms for
    hyper-parameter optimization, in *Advances in Neural Information Processing Systems,*
    (pp. 2546-2554)
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011), 超参数优化算法，在*神经信息处理系统进展*中，（第2546-2554页）
- en: Feurer, M., Springenberg, J. T., & Hutter, F. (2015, February), Initializing
    Bayesian hyperparameter optimization via meta-learning, in *Twenty-Ninth AAAI
    Conference on Artificial Intelligence*
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feurer, M., Springenberg, J. T., & Hutter, F. (2015年2月), 通过元学习初始化贝叶斯超参数优化，在*第二十九届人工智能美国协会会议*中
- en: Loshchilov, I., & Hutter, F. (2016), CMA-ES for hyperparameter optimization
    of deep neural networks, *arXiv preprint* arXiv:1604.07269
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov, I., & Hutter, F. (2016), CMA-ES用于深度神经网络的超参数优化，*arXiv预印本* arXiv:1604.07269
- en: Maclaurin, D., Duvenaud, D., & Adams, R. (2015, June), Gradient-based hyperparameter
    optimization through reversible learning, in *International Conference on Machine
    Learning* (pp. 2113-2122)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maclaurin, D., Duvenaud, D., & Adams, R. (2015年6月), 通过可逆学习进行基于梯度的超参数优化，在*国际机器学习会议*（第2113-2122页）
- en: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014), A nonlinear least squares
    quasi-Newton strategy for LP-SVR hyper-parameters selection, *International Journal
    of Machine Learning and Cybernetics*, 5(4), (pp.579-597)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014), 一种用于LP-SVR超参数选择的非线性最小二乘准牛顿策略，*机器学习与控制论国际期刊*，5(4)，（第579-597页）
