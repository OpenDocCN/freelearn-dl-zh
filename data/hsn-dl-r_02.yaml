- en: Machine Learning Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: Welcome to *Hands-On Deep Learning with R*! This book will take you through
    all of the steps that are necessary to code deep learning models using the R statistical
    programming language. It begins with simple examples as the first step for those
    just getting started, along with a review of the foundational elements of deep
    learning for those with more experience. As you progress through this book, you
    will learn how to code increasingly complex deep learning solutions for a wide
    variety of tasks. However, regardless of the complexity, each chapter will carefully
    detail each step. This is so that all topics and concepts can be fully comprehended
    and the reason for every line of code is completely explained.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到*R语言深度学习实战*！本书将带领你完成使用R统计编程语言编写深度学习模型的所有必要步骤。它从简单的示例开始，为刚刚入门的人提供第一步，并回顾了深度学习的基础元素，以便有更多经验的人复习。在你逐步深入本书时，你将学习如何编写越来越复杂的深度学习解决方案，适用于各种任务。无论任务复杂度如何，每一章都将详细说明每一步。这样所有的主题和概念都能被充分理解，并且每一行代码的原因都能得到完全解释。
- en: In this chapter, we will go through a quick overview of the machine learning
    process as it will form a base for the subsequent chapters of this book. We will
    look at processing a dataset to review techniques such as handling outliers and
    missing values. We will learn how to model data to brush up on the process of
    predicting an outcome and evaluating the results, and we will also review the
    most suitable metrics for various problems. We will look at improving a model
    using parameter tuning, feature engineering, and ensembling, and we will learn when
    to use different machine learning algorithms based on the task to solve.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将快速概述机器学习过程，因为它将为本书后续章节奠定基础。我们将学习如何处理数据集，以复习处理离群值和缺失值等技术。我们将学习如何对数据进行建模，以回顾预测结果的过程并评估结果，还将复习针对各种问题最合适的评估指标。我们将探讨如何通过参数调整、特征工程和集成方法来改进模型，并学习如何根据任务选择不同的机器学习算法。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Preparing data for modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据以进行建模
- en: Training a model on prepared data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在准备好的数据上训练模型
- en: Evaluating model results
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型结果
- en: Improving model results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进模型结果
- en: Reviewing different algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾不同的算法
- en: An overview of machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: All deep learning is machine learning, but not all machine learning is deep
    learning. Throughout this book, we will focus on processes and techniques that
    are specific to deep learning in R. However, all the core principles of machine
    learning are essential to understand before we can move on to explore deep learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的深度学习都是机器学习，但并非所有机器学习都是深度学习。本书将重点介绍与R中深度学习相关的过程和技术。然而，理解机器学习的所有核心原理是非常重要的，只有这样我们才能继续探索深度学习。
- en: Deep learning is marked as a special subset of machine learning based on the
    use of neural networks that mimic brain activity behavior. The learning is referred
    to as being deep because, during the modeling process, the data is manipulated
    by a number of hidden layers. In this type of modeling, specific information is
    gathered from each layer. For example, one layer may find the edges of images
    while another finds particular hues.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被视为机器学习的一个特殊子集，基于使用模拟大脑活动的神经网络。学习被称为“深度”是因为在建模过程中，数据通过多个隐藏层进行处理。在这种建模方式中，每个层都会收集特定的信息。例如，一个层可能会找到图像的边缘，而另一个层则可能会找到特定的色调。
- en: 'Notable applications for this type of machine learning include the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该类型机器学习的显著应用包括以下几个方面：
- en: Image recognition (including facial recognition)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别（包括人脸识别）
- en: Signal detection
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号检测
- en: Recommendation systems
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Document summarization
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档摘要
- en: Topic modeling
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: Forecasting
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Solving games
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决游戏
- en: Moving an object through space, for example, self-driving cars
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在空间中移动物体，例如自动驾驶汽车
- en: All of these topics will be covered throughout the course of this book. All
    of these topics implement deep learning and neural networks, which are primarily
    used for classification and regression.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的整个过程中将涵盖所有这些主题。所有这些主题都实现了深度学习和神经网络，主要用于分类和回归。
- en: Preparing data for modeling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据以进行建模
- en: 'One of the benefits of deep learning is that it largely removes the need for
    feature engineering, which you may be used to with machine learning. That being
    said, the data still needs to be prepared before we begin modeling. Let''s review
    the following goals to prepare data for modeling:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个好处是，它大大减少了对特征工程的需求，这可能是你在机器学习中常见的。不过，数据在建模之前仍然需要准备。让我们回顾以下目标，以便为建模做好数据准备：
- en: Remove no-information and extremely low-information variables
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除无信息和极低信息变量
- en: Identify dates and extract date parts
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别日期并提取日期部分
- en: Handle missing values
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Handle outliers
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理离群值
- en: In this chapter, we will be investigating air quality data using data provided
    by the London Air Quality Network. Specifically, we will look at readings for
    nitrogen dioxide in the area of Tower Hamlets (Mile End Road) during 2018\. This
    is a very small dataset with only a few features and approximately 35,000 observations.
    We are using a limited dataset here so that all of our code, even our modeling,
    runs quickly. That said, the dataset fits well for the process that we will explore.
    It requires some, but not an inordinate amount of, initial cleaning and preparation.
    In addition to this, it is suitable to use for decision tree-based modeling, which
    will be a useful form of machine learning to review as we start to apply deep
    learning models in future chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用伦敦空气质量网络提供的数据，研究空气质量数据。具体来说，我们将查看2018年Tower Hamlets（Mile End Road）地区的二氧化氮读数。这是一个非常小的数据集，只有几个特征和大约35,000个观测值。我们使用一个有限的数据集，以确保我们的所有代码，甚至包括建模，都能快速运行。也就是说，这个数据集非常适合我们要探索的过程。它需要一些初步的清理和准备，但不需要过多的工作。除此之外，这个数据集适合用于基于决策树的建模，这也是我们在未来章节中开始应用深度学习模型时，审查的一个有用的机器学习方法。
- en: 'Our first step will be to do some cursory data exploration to see what data
    cleaning and preparation steps will be necessary. R has some really helpful convenience
    packages for this type of exploratory data analysis. Let''s do a quick review
    by looking at some important areas of exploratory data analysis using the following
    series of code blocks:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是进行一些初步的数据探索，以便了解哪些数据清理和准备步骤是必要的。R有一些非常有用的便捷包，可以用于这种类型的探索性数据分析。让我们通过查看以下代码块中的一些重要领域，快速回顾一下探索性数据分析：
- en: 'We will start by loading our data and libraries. To do this, we will use the
    base R `library()` function to load all of the libraries that we will need. If
    there are any libraries listed that you do not have installed, use the `install.packages()`
    function to install these libraries. We will also use the `read_csv()` function
    from the `readr` package to load in the data. We load libraries and data using
    the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载数据和库开始。为此，我们将使用基础R中的`library()`函数加载我们所需的所有库。如果列表中有你没有安装的库，可以使用`install.packages()`函数来安装它们。我们还将使用`readr`包中的`read_csv()`函数来加载数据。我们将使用以下代码来加载库和数据：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Packages are included before the functions are called; therefore, this gives
    us an understanding of where and why each package is being used. As shown in the
    preceding code block, the following packages will be used in this chapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 包会在调用函数之前包含；因此，这能帮助我们理解每个包的使用位置和原因。如前面的代码块所示，本章将使用以下包：
- en: '`tidyverse`: This suite of packages will be used extensively. In this case,
    the `dplyr` package is used in this chapter for data wrangling, for instance,
    looking at aggregate values or adding and removing columns and rows.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyverse`：这一系列的包将会广泛使用。在本章中，`dplyr`包用于数据清理，例如，查看汇总值或添加和删除列和行。'
- en: '`lubridate`: This will be used to easily extract details from a column holding
    values with a date data type.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lubridate`：这个包将用于轻松提取包含日期数据类型的列中的详细信息。'
- en: '`xgboost`: This will be the model that we will use for our data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost`：这是我们将用于数据建模的模型。'
- en: '`Metrics`: This will be used to evaluate our model.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Metrics`：这个包将用于评估我们的模型。'
- en: '`DataExplorer`: This will be used for generating exploratory data analysis
    plots.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer`：这个包将用于生成探索性数据分析的图表。'
- en: '`caret`: This will be used when tuning our model as it provides a convenient
    method for performing a grid search of hyperparameters.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret`：当我们调整模型时，将使用此包，它提供了一种方便的网格搜索超参数的方法。'
- en: 'Next, we will view the structure of the data using the `str` function, which
    provides details on the data object class and dimensions and column-specific details
    on the data type, along with some sample values, as shown in the following code:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`str`函数查看数据的结构，该函数提供有关数据对象类别和维度的详细信息，以及每列数据类型的详细信息，并显示一些示例值，如下代码所示：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After running the code, we will see the following printed to our console:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们将看到控制台输出如下内容：
- en: '![](img/2e73256f-5d6a-4543-a8ae-7865d1cd07be.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e73256f-5d6a-4543-a8ae-7865d1cd07be.png)'
- en: 'All of this data is from the same site, with readings for the same species
    of pollutants, and we can conclude that the unit of measure will likely be consistent
    throughout as well. If this is the case, we can remove these columns as they provide
    no informational value. Even if we did not know the first variable would always
    be the same, we can start to see this pattern from the results of the structure
    (`str`) function. We can confirm that this is the case, though, by running the
    following code, using the `group_by` and `summarise` functions from the `dplyr`
    package:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些数据来自同一站点，且污染物的物种读数相同，我们可以得出单位度量在整个数据中可能保持一致的结论。如果是这种情况，我们可以删除这些列，因为它们没有提供任何信息价值。即使我们不知道第一个变量总是相同的，通过`str`函数的结果，我们也可以开始看出这个模式。我们可以通过运行以下代码来确认这一点，使用`dplyr`包中的`group_by`和`summarise`函数：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After running the preceding code, we will see the following printed to our
    console:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们将看到控制台输出如下内容：
- en: '![](img/7e62e5f7-96fc-4110-adbf-c4835663da13.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e62e5f7-96fc-4110-adbf-c4835663da13.png)'
- en: We have confirmed that the `Site`, `Species`, and `Units` values are always
    the same, so we can remove them from the data as they will provide no information.
    We can also see that the actual reading values are stored as character strings
    and we have dates stored that way as well. In its current form, the `date` field
    exhibits a characteristic known as high cardinality, which is to say that there
    are a large number of unique values. When we see this, we will usually want to
    act on these types of columns so that they have fewer distinct values. In this
    case, the technique is clear because we know that this should be a date value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认`Site`、`Species`和`Units`的值始终相同，因此可以将它们从数据中删除，因为它们不会提供任何有用信息。我们还可以看到，实际的读数值是以字符字符串的形式存储的，日期也以相同的方式存储。当前形式下，`date`字段表现出一种特征，称为高基数性，即存在大量唯一的值。当我们看到这种情况时，通常会希望对这些类型的列进行处理，以减少其独特值的数量。在这种情况下，方法是显而易见的，因为我们知道这应该是一个日期值。
- en: 'In the code that follows, we will use the `dplyr` `select` function to remove
    the columns that we don''t want to keep. We will use the `dplyr` `mutate` function
    along with functions from the `lubridate` package to transform the variables we
    have identified. After transforming the data, we can remove the old character
    string date column and the full date column as we will use the atomized date values
    going forward. We remove the columns we don''t need and break the converted date
    field into its component parts using the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将使用`dplyr`的`select`函数删除不需要保留的列。我们将使用`dplyr`的`mutate`函数，并结合`lubridate`包中的函数来转换我们已识别的变量。转换数据后，我们可以删除旧的字符字符串日期列和完整日期列，因为我们将使用原子化的日期值。我们删除不需要的列，并使用以下代码将转换后的日期字段拆分为其组成部分：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before running the preceding code, we should note that the dataframe in our
    Environment pane looks like the following screenshot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码之前，我们应该注意到我们的环境面板中的`dataframe`如下所示：
- en: '![](img/90893f34-2a4b-47f5-ac2c-b07650f18196.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90893f34-2a4b-47f5-ac2c-b07650f18196.png)'
- en: 'After running the code, we can note the differences to the dataframe, which
    should now look like the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们可以注意到`dataframe`的变化，现在应该如下所示：
- en: '![](img/37ede6ab-693c-42d1-827a-39365609dfb9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37ede6ab-693c-42d1-827a-39365609dfb9.png)'
- en: As you can see, the columns that contained only one value have been removed,
    and the data column now occupies five columns with one for each of the date and
    time parts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，只包含一个值的列已被删除，数据列现在占据了五个列，每个列对应一个日期和时间部分。
- en: 'Next, we will use the `DataExplorer` package to explore any missing values.
    There are numerous ways in which we summarize the number and proportion of missing
    values in a given data object. Of these, the `plot_missing()` function offers
    a count and percentage of missing values along with a visualization—all from one
    function call. We plot missing values using the following line of code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`DataExplorer`包来检查是否存在缺失值。有多种方法可以总结数据对象中缺失值的数量和比例。其中，`plot_missing()`函数可以一次性提供缺失值的计数、百分比及其可视化效果。我们通过以下代码行来绘制缺失值：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running this code, a plot is produced. In your **Viewer** pane, you should
    see the following plot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，会生成一个图表。在你的**查看器**面板中，你应该能看到如下图表：
- en: '![](img/bbf2f7c5-bd39-40b0-862a-bd86a8823db1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbf2f7c5-bd39-40b0-862a-bd86a8823db1.png)'
- en: As you can see, there are no missing values among the independent variables.
    However, among the target variable class, there are around 1,500 missing values,
    accounting for 3.89% of the column.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，独立变量中没有缺失值。然而，在目标变量类别中，约有1,500个缺失值，占该列的3.89%。
- en: 'Since we have missing values, we should consider whether any action should
    be taken. In this case, we will simply remove these rows as there are not many
    of them, and the portion of the data with no missing values will still be representative
    of the entire dataset. While, in this case, the values are simply removed, there
    are a number of options available to handle missing values. A summary of possible
    actions is presented later on in this chapter. To remove the rows with missing
    values, we run the following line of code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于存在缺失值，我们应该考虑是否需要采取任何措施。在这种情况下，我们将简单地删除这些行，因为它们并不多，并且没有缺失值的数据部分仍然能够代表整个数据集。虽然在此案例中我们仅删除了缺失值，但处理缺失值的方式有很多种选择。本章稍后将介绍可能的处理方法。为了删除含有缺失值的行，我们运行以下代码：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will also run a check on our discrete variable as well just to see the distribution
    of values among the categories present in this column. Again, the `DataExplorer`
    package offers a convenient function for this, which will provide a plot for the
    discrete values present noting the frequency of each. We generate this plot with
    the `plot_bar` function using the following line of code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将检查我们的离散变量，以查看该列中各类别的值分布。同样，`DataExplorer`包提供了一个方便的函数，可以为离散值生成图表，显示每个值的频率。我们通过以下代码行使用`plot_bar`函数生成此图表：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the preceding function, we are able to view the following visualization,
    which clearly shows that there are more **Ratified** results than **Provisional**
    results. From reading the documentation for this dataset, the **Ratified** results
    are verified and can be trusted to be accurate, while the **Provisional** results
    may not be as accurate. Let''s take a look at the output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述函数后，我们能够看到以下可视化结果，清晰地显示了**批准**结果比**临时**结果更多。从查阅该数据集的文档中，**批准**结果是经过验证的，可以信赖其准确性，而**临时**结果可能不如其准确。让我们来看一下输出：
- en: '![](img/19b0d38a-2dac-4413-bb72-2d78982c2ec2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19b0d38a-2dac-4413-bb72-2d78982c2ec2.png)'
- en: 'We created a plot in order to quickly see the distribution of values among
    the discrete terms in the **Provisional** or **Ratified** column. We can also
    create a table using the following code to get more specific details:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个图表，以便快速查看**临时**或**批准**列中离散项的值分布。我们还可以使用以下代码创建一个表格，以获取更具体的详细信息：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code uses the `group_by` function to group rows based on the
    values present in the `Provisional or Ratified` column. It then uses the `summarise`
    function, along with setting the `count` argument equal to `n()`, to calculate
    the number of rows containing each of the discrete values from the `Provisional
    or Ratified` column. Running the preceding code will print the output to your
    console, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用`group_by`函数按`临时或批准`列中的值对行进行分组。接着，它使用`summarise`函数，并将`count`参数设置为`n()`，以计算包含每个离散值的行数。运行上述代码后，输出将显示在控制台中，如下所示：
- en: '![](img/dc87a317-c018-4d13-a281-87aa35e51c16.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc87a317-c018-4d13-a281-87aa35e51c16.png)'
- en: 'Since there are very few values marked as `Provisional`, we will just remove
    these rows. To do so, we will first use the `filter` function, which is used to
    remove rows based on a particular condition. In this case, we will filter the
    data so that only rows with an `''R''` value in the `Provisional or Ratified`
    column will remain, as shown here:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于标记为`Provisional`的值非常少，我们将删除这些行。为此，我们将首先使用`filter`函数，它用于根据特定条件删除行。在这种情况下，我们将过滤数据，使得`Provisional
    or Ratified`列中只有值为`'R'`的行保留下来，如下所示：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will remove the `Provisional or Ratified` column since it only holds
    one unique value. To do this, we will use the `select()` function, which is used
    to remove columns in a similar way to how a filter is used to remove rows. Here,
    the `select()` function is called and the argument passed to the function is ``
    -`Provisional or Ratified` ``, which will remove this column. Alternatively, all
    of the other column names, aside from `Provisional or Ratified`, could be passed
    in as an argument. The `select()` function works by either using the columns to
    include or the columns to exclude. In this case, it is faster to note the column
    to exclude, which is why this choice was made. Please refer to the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将删除`Provisional or Ratified`列，因为它只包含一个唯一值。为此，我们将使用`select()`函数，删除列的方式与删除行的过滤器类似。在这里，调用`select()`函数，传递给函数的参数是`-Provisional
    or Ratified`，这样就会删除这一列。或者，可以将所有其他列名（除了`Provisional or Ratified`）作为参数传递给它。`select()`函数通过指定要包含或排除的列来工作。在这种情况下，指定要排除的列更为高效，这就是为什么做出这个选择的原因。请参考以下代码：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Earlier, we noted that all of our data is from the year 2018\. Now that our
    date data is broken up into component parts, we should only see one value in the
    `reading_year` column. One way to test whether this is the case is to use the
    `range()` function. We check for the minimum and maximum values in the `reading_year`
    column by running the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 早些时候，我们提到所有数据都来自2018年。现在我们的日期数据已经被拆分成了各个组成部分，我们应该只在`reading_year`列中看到一个值。检验这一点的一个方法是使用`range()`函数。我们通过运行以下代码来检查`reading_year`列中的最小值和最大值：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the preceding code will result in values being printed to our console.
    Your console should look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码将导致值被打印到控制台。您的控制台应该如下所示：
- en: '![](img/066e67dd-a241-4afe-9579-76d2c8b912aa.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/066e67dd-a241-4afe-9579-76d2c8b912aa.png)'
- en: 'We can see from the results of our call to the `range()` function that, in
    fact, the `reading_year` column only includes one value. With this being the case,
    we can use the `select()` function to remove the `reading_year` column with the
    help of the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们调用`range()`函数的结果中，我们可以看到，实际上`reading_year`列只包含一个值。既然如此，我们可以使用`select()`函数在以下代码的帮助下删除`reading_year`列：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can do a histogram check of continuous variables to look for outliers.
    To accomplish this, we will once again use the `DataExplorer` package. This time,
    we will use the `plot_histogram` function to visualize the continuous values for
    all of the columns with these types of values, as shown in the following code
    block:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以对连续变量进行直方图检查，以寻找异常值。为了实现这一点，我们将再次使用`DataExplorer`包。这次，我们将使用`plot_histogram`函数来可视化所有具有这些类型值的列中的连续值，如下所示的代码块：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After running the preceding code, five plots are generated displaying the frequency
    for continuous values among the five columns that have these types of values.
    Your **Viewer** pane should look like the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，会生成五个图表，显示这五个包含连续值的列的频率。您的**查看器**面板应该如下所示：
- en: '![](img/918b3b3b-8119-4bc4-8bd0-7e88e503861b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/918b3b3b-8119-4bc4-8bd0-7e88e503861b.png)'
- en: In the preceding output, we do see that our independent variable is slightly
    right-skewed, and, as a result, we could take some action on the outlier values.
    If there was a more dramatic skew, then we could apply a log transformation. As
    there are not many, we could also remove these values if we thought they were
    noisy. However, for now, we will just leave them in the data. If, in the end,
    our model is performing poorly, then it would be worthwhile to perform some outlier
    treatment to see whether this improves performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们确实看到我们的自变量稍微偏向右侧，因此，我们可以对异常值采取一些处理措施。如果存在更剧烈的偏斜，我们可以应用对数变换。由于异常值不多，我们也可以删除这些值，如果我们认为它们是噪声的话。然而，目前我们将这些值保留在数据中。如果最终我们的模型表现不佳，那么进行一些异常值处理，看看是否能改善性能，将是值得的。
- en: 'Lastly, let''s do a correlation check. We will again use the `DataExplorer`
    package, and this time we will use the `plot_correlation()` function. To generate
    a correlation plot, we run the following line of code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们进行一次相关性检查。我们将再次使用`DataExplorer`包，这次我们将使用`plot_correlation()`函数。为了生成相关图，我们运行以下代码：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the preceding line of code will generate a plot. The plot uses blue
    and red colors to denote negative and positive correlations, respectively. These
    colors will not be present in the diagram in this book; however, you can still
    see the correlation values. After running the preceding code, you will see a plot
    in your Viewer pane, which looks like the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面一行代码将生成一个图表。该图表使用蓝色和红色分别表示负相关和正相关。这些颜色在本书中的图表中不会出现；然而，你仍然可以看到相关值。在运行前面的代码后，你将在查看器面板中看到一个图表，如下所示：
- en: '![](img/f41a9729-df2e-493a-961b-f943ad256b53.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f41a9729-df2e-493a-961b-f943ad256b53.png)'
- en: From this plot, we can see that the pollutant value does have some correlation
    with the `reading_hour` feature, and less correlation with the `reading_day` and
    `reading_month` features, suggesting that there is a higher trend throughout the
    day for when this pollutant is being produced than in the week, month, or year.
    The main objective of this plot is to look for independent variables that are
    highly correlated as it might suggest that these variables are conveying the same
    information, and, in that case, we may want to remove one or combine them in some
    way.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们可以看到污染物值确实与`reading_hour`特征有一定的相关性，而与`reading_day`和`reading_month`特征的相关性较小，这表明该污染物的产生在一天内有较高的趋势，而不是在一周、一个月或一年内。这个图表的主要目的是寻找高度相关的独立变量，因为这可能意味着这些变量传达了相同的信息，在这种情况下，我们可能希望删除其中一个或以某种方式将它们合并。
- en: We now have a dataset that is properly preprocessed and ready for modeling.
    The correlation plot shows that the variables are not significantly correlated.
    Of course, this is not surprising as the remaining variables simply describe discrete
    moments in time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个已经适当预处理并准备好建模的数据集。相关图显示这些变量之间没有显著的相关性。当然，这并不令人惊讶，因为剩余的变量只是描述时间中的离散时刻。
- en: The date value was converted from a string that provided no informational value
    to a date value further split into date parts represented as numeric data. All
    columns that contained only one value were removed as they provided no information.
    The few rows marked as provisional values were removed since there were not many
    of these, and, in the description of the data, there were warnings about the validity
    of pollutant measures marked this way. Lastly, rows containing null values for
    the predictor variable were removed. This was done because there were not many
    of these; however, there are other tactics we could have taken in this situation.
    We have noted them in the following section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 日期值已从没有信息价值的字符串转换为日期值，进一步分割为以数字数据表示的日期部分。所有仅包含一个值的列都已被删除，因为它们没有提供任何信息。标记为临时值的少数行被删除，因为这些行不多，而且在数据描述中有关于这些污染物测量有效性的警告。最后，包含预测变量为空值的行被删除。之所以这样做，是因为这种行的数量不多；然而，我们在这种情况下本可以采取其他策略，我们将在下节中提到它们。
- en: Handling missing values
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'In the preprocessing work that we just completed, we decided to remove missing
    values. This is an option when there are very few cases that contain missing values
    and, in this example, this was true. However, other situations may require different
    approaches to handling missing values. Here are some common options in addition
    to deleting rows and columns:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚完成的预处理工作中，我们决定删除缺失值。当缺失值的情况非常少时，这是一个可行的选择，在这个例子中确实如此。然而，其他情况下可能需要不同的方法来处理缺失值。除了删除行和列外，以下是一些常见的其他选项：
- en: '**Imputation with a measure of centrality** (**mean**/**median**/**mode**):
    Use one of the measures of centrality to fill in the missing values. This can
    work well if you have normally distributed numeric data. Modal imputation can
    also be used on non-numeric data by selecting the most frequent value to replace
    the missing values.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用集中趋势度量进行插补**（**均值**/**中位数**/**众数**）：使用一种集中趋势度量来填充缺失值。如果你的数据是正态分布的数值数据，这种方法效果较好。对于非数值数据，也可以使用众数插补，通过选择最频繁的值来替换缺失值。'
- en: '**Tweak for the missing values**: You can use the known values to impute the
    missing values. Examples of this approach include using regression with linear
    data or the **k-nearest neighbor** (**KNN**) algorithm to assign a value based
    on similarity to known values in the feature space.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整缺失值**：你可以使用已知值来填补缺失值。此方法的例子包括使用回归分析处理线性数据或使用**k近邻**（**KNN**）算法，根据特征空间中与已知值的相似度来分配一个值。'
- en: '**Replace it with a constant value**: The missing value can also be replaced
    with a constant value outside the range of values present or not already present
    in the categorical data. The advantage here is that it will become clear later
    on whether these missing values have any informational value, as they will be
    clearly set to the side. This is in contrast to imputing with a measure of centrality
    where the final result will be some missing values now containing the imputed
    value, while some equal values will have actually already been present in the
    data. In this case, it becomes difficult to know which values were missing values
    and which were the values already present in the data.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用常数值替换**：缺失值也可以用一个常数值替换，该常数值不在已有数据的值范围内，也未出现在分类数据中。这样做的好处是，后来会更清楚这些缺失值是否具有信息价值，因为它们会被明确地放置在一边。与用集中趋势度量填补缺失值的方式不同，集中趋势填补后的最终结果会使一些缺失值包含填补后的值，而有些相同的值实际上已经存在于数据中。在这种情况下，便难以区分哪些是缺失值，哪些是原本已存在的数据值。'
- en: Training a model on prepared data
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个模型来拟合准备好的数据
- en: Now that the data is ready, we will split it into train and test sets and run
    a simple model. The objective at this point is not to try to achieve the best
    performance, but rather to get some type of a benchmark result to use in the future
    as we try to improve our model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据准备好了，我们将其划分为训练集和测试集，并运行一个简单的模型。此时的目标不是尽力取得最佳性能，而是得到一种基准结果，以便未来在尝试提升模型时使用。
- en: Train and test data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据和测试数据
- en: 'When we build predictive models, we need to create two separate sets of data
    with the help of the following segments. One is used by the model to learn the
    task and the other is used to test how well the model learned the task. Here are
    the types of data that we will look at:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建预测模型时，我们需要通过以下几个部分创建两个独立的数据集。一个用来让模型学习任务，另一个用来测试模型是否学会了这个任务。以下是我们将要查看的数据类型：
- en: '**Train data**: The segment of the data used to fit the model. The model has
    access to the explainer variables or independent variables, which are the selected columns,
    to describe a record in your data, as well as the target variable or dependent
    variable. That is the value we are trying to predict during the training process
    using this dataset. This segment should usually be between 50% and 80% of your
    total data.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：用于拟合模型的数据部分。模型可以访问解释变量或独立变量（即被选择的列，用来描述数据记录），以及目标变量或依赖变量。我们在训练过程中尝试预测的就是这个目标变量。这个数据部分通常应该占你总数据的50%到80%。'
- en: '**Test data**: The segment of the data used to evaluate the model results.
    The model should never have access to this data during the learning process and
    should never see the target variable. This dataset is used to test what the model
    has learned about the dependent variables. After fitting our model during the
    training phase, we now use this model to predict values on the test set. During
    this phase, only we have the correct answers; the independent variable, that is,
    the model, never has access to these values. After the model makes its predictions,
    we can evaluate how well the model performed by comparing the predicted values
    to the actual correct values.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据**：用于评估模型结果的数据部分。在学习过程中，模型永远不能接触到这些数据，也永远不应看到目标变量。这个数据集用于测试模型在依赖变量上的学习情况。在训练阶段拟合模型后，我们现在使用这个模型来预测测试集中的值。在这一阶段，只有我们拥有正确答案；而独立变量，即模型，永远无法接触到这些值。模型做出预测后，我们可以通过将预测值与实际正确值进行比较来评估模型的表现。'
- en: '**Validation data**: Validation data is a portion of the training dataset that
    the model uses to refine hyperparameters. As the varying values are selected for
    the hyperparameters, the model makes checks against the validation set and uses
    the results gathered during this process to select the values for the hyperparameters
    that produce the best performing models.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据**：验证数据是训练数据集的一部分，模型利用它来调整超参数。随着超参数的不同值被选定，模型会对验证集进行检查，并使用在此过程中收集的结果来选择产生最佳性能模型的超参数值。'
- en: '**C****ross-validation**: One potential issue that can arise when we use only
    one train and test set is that the model will learn about specific descriptive
    features that are particular to this segment of the data. What the model learns
    may not generalize well when applied to other data in the future. This is known
    as overfitting. To mitigate this problem, we can use a process known as cross-validation.
    In a simple example, we can do an 80/20 split of the data where 20% is held for
    test data and we can model and test on this split. We can then create a separate
    80/20 split and do the same modeling and testing. We can repeat this process 5
    times with 5 different test sets—each composed of a different fifth of the data.
    This exact type of cross-validation is known as 5-fold cross-validation. After
    all of the iterations are completed, we can check whether the results are consistent
    for each. And, if so, we can feel more confident that our model is not overfitting
    and use this to generalize on more data.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：当我们只使用一个训练集和测试集时，可能会出现一个潜在问题，即模型会学习到一些特定的描述性特征，这些特征是该数据段特有的。模型所学到的内容在未来应用于其他数据时可能无法很好地推广。这种情况被称为过拟合。为了减轻这个问题，我们可以使用一种叫做交叉验证的过程。在一个简单的例子中，我们可以将数据按80/20的比例划分，其中20%作为测试数据，模型在这个划分上进行建模和测试。然后，我们可以创建一个独立的80/20划分，进行相同的建模和测试。我们可以重复这个过程5次，使用5个不同的测试集——每个测试集由数据的五分之一组成。这种精确的交叉验证方法被称为5折交叉验证。在所有迭代完成后，我们可以检查每次迭代的结果是否一致。如果一致，我们可以更有信心地认为我们的模型没有过拟合，并可以利用这个模型在更多数据上进行推广。'
- en: Choosing an algorithm
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择算法
- en: For this task, we will use `xgboost`, which is a very popular implementation
    of the gradient tree boosting algorithm. The reason this works so well is that
    each model iteration learns from the results of the previous model. This model
    uses boosting for iterative learning in contrast to bagging. Both of these ensembling
    techniques can be used to compensate for a known weakness in tree-based learners,
    which has to do with overfitting to the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用`xgboost`，这是一个非常流行的梯度树提升算法的实现。它之所以效果如此好，是因为每次模型迭代都会从前一个模型的结果中学习。与bagging相比，该模型使用提升方法进行迭代学习。这两种集成技术都可以用来弥补基于树的学习者在训练数据上过拟合的已知弱点。
- en: One simple difference between bagging and boosting is that, with bagging, full
    trees are grown and then the results are averaged, while, with boosting, each
    iteration of the tree model learns from the model before it. This is an important
    concept, as this idea of an algorithm that incorporates an additive function with
    information gained after modeling on the residuals of the previous model will
    be used in deep learning as we move forward.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: bagging和boosting之间的一个简单区别是，在bagging中，会生长完整的树，然后将结果进行平均，而在boosting中，每次树模型的迭代都会从前一个模型中学习。这是一个重要的概念，因为这个结合了加法函数的算法思想，在对前一个模型残差建模后获得信息，将在深度学习中得到应用。
- en: 'Here, we will explore the power of this type of machine learning algorithm
    on this simple example. Additionally, we will pay particular attention to how
    what we learn here is relevant for more complex examples in the subsequent chapters.
    We will use the following code to train an `xgboost` model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探索这种类型的机器学习算法在这个简单例子中的强大功能。此外，我们还将特别关注我们在这里学到的内容如何与后续章节中更复杂的例子相关。我们将使用以下代码来训练一个`xgboost`模型：
- en: 'We start the process of fitting a model to our data by partitioning our data
    into train and test sets, using the following code:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下代码开始将模型拟合到数据的过程，首先将数据划分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we started with the `set.seed()` function. This is because
    aspects of this modeling process involve pseudorandomness, and setting the seed
    ensures that the same values are used for these elements every time, so we can
    consistently produce the same results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们从 `set.seed()` 函数开始。这是因为这个建模过程的一些部分涉及伪随机性，设置种子可以确保每次使用相同的值，从而我们能始终如一地产生相同的结果。
- en: We split the data into training data, which we will model on, and a test set
    to check whether our predictions are accurate. We do this by getting a random
    sample of row index values, which we store in the vector labeled `partition`,
    and use them to subset our data. In addition, after partitioning, we split out
    the target variable and store this in a vector.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分为训练数据（用于建模）和测试集（用于检查预测是否准确）。我们通过获取随机的行索引值样本，存储在名为 `partition` 的向量中，并用这些索引来子集化数据。此外，在分区后，我们将目标变量分离出来，并将其存储在一个向量中。
- en: Then, we convert our data into a dense matrix so that it is in the proper format
    to be passed to the `xgboost` algorithm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据转换为稠密矩阵，以便将其传递给 `xgboost` 算法时符合正确的格式。
- en: 'Following this, we will create a list of parameters. Some of these values are
    required for the analysis that we will be doing and others are just starting values
    chosen arbitrarily. For those values, later on, we will look at ways to more scientifically
    choose them. We prepare our initial parameter list using the following code:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个参数列表。该列表中的某些值是进行分析所需的，其他值则是随意选择的初始值。对于这些值，稍后我们将探讨更科学地选择它们的方法。我们使用以下代码准备初始参数列表：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, the required values in this list include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，以下是该列表中需要的值：
- en: '`objective = "reg:linear"`: This is used to define the task objective as linear
    regression. Here, we are conducting a regression task seeking the value of nitrogen
    dioxide.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective = "reg:linear"`：用于将任务目标定义为线性回归。在这里，我们正在进行一个回归任务，目标是预测二氧化氮的值。'
- en: '`booster = "gbtree"`: This tells us that we will use gradient tree boosting
    to choose the best model for predicting results.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`booster = "gbtree"`：这告诉我们将使用梯度提升树来选择最适合预测结果的模型。'
- en: '`eval_metric = "rmse"`: This tells us that we will use the **Root Mean Squared
    Error** (**RMSE**) to evaluate the success of our model. Later on, we will look
    at why this is the most appropriate choice, along with some of the other options
    we can use here for other tasks.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_metric = "rmse"`：这告诉我们将使用**均方根误差**（**RMSE**）来评估模型的成功。稍后我们将探讨为什么这是最合适的选择，并讨论我们在其他任务中可以使用的一些其他选项。'
- en: 'Next, these are the variables where we are arbitrarily choosing starting values:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，这些是我们任意选择初始值的变量：
- en: '`eta=0.1`: This is used to define the learning rate. To begin, it makes sense
    to use a larger number; however, as we move forward, we will want to use a smaller
    learning rate and additional rounds to improve performance.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta=0.1`：用于定义学习率。起初，使用较大的数值是有意义的；但是，随着进展，我们将希望使用较小的学习率并增加轮次以提高性能。'
- en: '`subsample=0.8`: This tells us that, for each tree, we will use 80% of the
    rows from the data.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample=0.8`：这告诉我们，对于每棵树，我们将使用数据中 80% 的行。'
- en: '`col_subsample=0.75`: This tells us that, for each tree, we will use 75% of
    the columns from the data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col_subsample=0.75`：这告诉我们，对于每棵树，我们将使用数据中的 75% 列。'
- en: 'These are the parameters that do not impact the model and only impact how we
    review the results of the model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是不会影响模型本身，仅影响我们查看模型结果的参数：
- en: '`print_every_n = 10`: This is used to state that the evaluation scores should
    be printed after every 10 rounds.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print_every_n = 10`：用于指定每经过 10 轮后打印一次评估分数。'
- en: '`verbose = TRUE`: This is used to denote that the evaluation scores should
    be printed to the console so that they can be seen by the end user during the
    model run.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose = TRUE`：用于表示评估分数应打印到控制台，以便在模型运行时，最终用户能够看到这些分数。'
- en: 'Now that we have the parameters defined, we will run the model using the following
    code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了参数，我们将使用以下代码运行模型：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When we run the model, we bring in the list of parameters that we previously
    defined, that is, the model should be run against the training dataset and we
    choose to run the model for 100 rounds. This means that we will grow 100 trees.
    This is, again, an arbitrary value. Later, we will look at ways to discover the
    optimal number of rounds. Then, we predict the test dataset using the model that
    we just defined. For our train dataset, the algorithm knows the correct value
    and uses this to adjust the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行模型时，我们会带入之前定义的参数列表，也就是说，模型应该在训练数据集上运行，我们选择运行模型100轮。这意味着我们将生成100棵树。这同样是一个任意值。稍后我们将探讨如何找到最优的轮次。接着，我们使用刚刚定义的模型来预测测试数据集。对于训练数据集，算法知道正确的值，并用它来调整模型。
- en: 'In the next section, we will apply the model to the test data where the model
    no longer has access to the correct values, and, without this knowledge, the model
    uses the independent variables to make predictions for the target variable. Please
    refer to the following code block:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把模型应用于测试数据，其中模型不再访问正确的值，没有这些知识，模型将使用自变量来预测目标变量。请参阅以下代码块：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When we run the preceding code, we take the data from the dense matrix, labeled
    `dtest`, and run it through our model labeled `xgb`. The model takes the tree
    splits that were calculated during training and applies them to the new data to
    make predictions. The predictions are stored in a vector called `pred`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行前面的代码时，我们从密集矩阵中提取数据，标记为`dtest`，并通过我们标记为`xgb`的模型运行它。模型应用在训练过程中计算出的树分割，并将其应用于新数据来做出预测。预测结果将存储在一个名为`pred`的向量中。
- en: 'Lastly, we will use the RMSE function to check model performance. For this
    evaluation metric, the closer it is to zero, the better, as this is a measure
    of the difference between true values and predicted values. To evaluate the performance
    of our model, we run the following line of code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用RMSE函数检查模型的表现。对于这个评估指标，越接近零越好，因为它衡量的是真实值与预测值之间的差异。为了评估我们模型的表现，我们运行以下代码：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running the preceding code, we will see a value printed to our console.
    Your console should look like the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们将看到一个值被打印到控制台。你的控制台应该显示如下输出：
- en: '![](img/18404f69-a22b-4d99-b795-01f4b55398fb.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18404f69-a22b-4d99-b795-01f4b55398fb.png)'
- en: From our quick and simple model, we have achieved an RMSE score of 0.054\. Soon,
    we will make some changes to the model parameters to try to improve our score.
    Before that, let's take a quick look at all of the different evaluation metrics
    that we can use in addition to RMSE while also taking a deep dive into explaining
    how RMSE works.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的快速而简单的模型来看，我们已经获得了0.054的RMSE分数。很快，我们将对模型参数进行一些调整，以尝试提高分数。在此之前，让我们快速看一下除了RMSE之外，我们可以使用的所有不同评估指标，并深入解释RMSE是如何工作的。
- en: Evaluating model results
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型结果
- en: We only know whether a model is successful if we can measure it, and it is worthwhile
    taking a moment to remember which metrics to use in which scenarios. Take, for
    example, a credit card fraud dataset where there is a large imbalance in the target
    variable because there will only be a, relatively, few cases of fraud among many
    non-fraudulent cases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有在能够衡量模型时，才知道模型是否成功，值得花点时间记住在不同场景下使用哪些评估指标。举个例子，假设有一个信用卡欺诈数据集，其中目标变量存在很大的不平衡，因为在许多非欺诈案例中，只有相对较少的欺诈案件。
- en: If we use a metric that just measures the percentage of the target variable
    that we predict successfully, then we will not be evaluating our model in a very
    helpful way. In this case, to keep the math simple, let's imagine we have 10,000
    cases and only 10 of them are fraudulent accounts. If we predict that all cases
    are not fraudulent, then we will have 99.9% accuracy. This is very accurate, but
    it is not very helpful. Here is a review of the different metrics and when to
    use them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用一个仅仅衡量目标变量预测成功百分比的指标，那么我们就不会以一种非常有帮助的方式评估我们的模型。在这种情况下，为了简化数学计算，假设我们有10,000个案例，其中只有10个是欺诈账户。如果我们预测所有案件都不是欺诈，那么我们的准确率将是99.9%。这个准确率很高，但其实用性不大。以下是各种评估指标的回顾以及何时使用它们。
- en: Machine learning metrics
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习评估指标
- en: 'Choosing the wrong metric will make it very difficult to evaluate performance
    and, as a result, improve our model. Therefore, it is very important to choose
    the right metric. Let''s take a look at the following machine learning metrics:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 选择错误的指标会使得评估模型表现变得非常困难，从而也很难提升我们的模型。因此，选择正确的指标非常重要。让我们来看看以下的机器学习指标：
- en: '**Accuracy**: The simplest evaluation metric is accuracy. Accuracy measures
    the difference between the predicted value and the actual value. This metric is
    easy to interpret and communicate; however, as we mentioned earlier, it doesn''t
    measure performance well when used to evaluate a highly unbalanced target variable,
    for example.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：最简单的评估指标是准确率。准确率衡量预测值与实际值之间的差异。这个指标易于解释和沟通；然而，正如我们之前提到的，当用来评估一个高度不平衡的目标变量时，它并不能很好地反映模型表现。'
- en: '**Confusion Matrix**: The confusion matrix provides a convenient way to display
    classification accuracy along with Type I and Type II errors. The combined view
    of these four related metrics can be especially informative in deciding where
    to focus our efforts during the tuning process. It can also help mark cases where
    other metrics may be more helpful. When there are too many values in the majority
    class, then a metric that is designed for use with class imbalances, such as log-loss,
    should be employed.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：混淆矩阵提供了一种方便的方式来展示分类准确性以及I型错误和II型错误。结合这四个相关指标的视图，在调优过程中决定集中精力的地方特别有帮助。它还可以帮助标出其他指标可能更有用的情况。当多数类别的值过多时，应该使用设计用于处理类别不平衡的指标，例如对数损失。'
- en: '**Mean Absolute Error** (**MAE**): This metric takes the difference between
    the predicted value and the actual value and calculates the mean value of these
    errors. This metric is simple to interpret and is useful when there is no need
    to apply an additional penalty to large errors. If an error that is three times
    larger than another error is three times as bad, then this is a good metric to
    use. However, there are many cases where an error that is three times larger than
    another error is much more than three times as bad, and this results in adding
    an additional penalty, which can be accomplished with the next metric.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）：该指标计算预测值与实际值之间的差异，并求出这些误差的平均值。这个指标简单易懂，适用于在不需要对大误差施加额外惩罚的情况。如果一个误差是另一个误差的三倍大，而三倍大的误差被认为是三倍糟糕的，那么这个指标非常适用。然而，很多情况下，三倍大的误差远不止是三倍糟糕，这时就需要加上额外的惩罚，而这一点可以通过下一个指标来实现。'
- en: '**RMSE**: This metric takes the square of the error for every prediction, the
    difference between the predicted value and the actual value, sums these squared
    errors, and then takes the square root of the sums. In this case, if the squaring
    has even a few highly inaccurate predictions, it will result in a sizable penalty
    and a higher value on this error metric. We can see how this would help in our
    preceding example and why we have chosen to use RMSE. This metric is used for
    regression.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE**：该指标对每个预测的误差进行平方处理，即预测值与实际值之间的差异，然后将这些平方误差相加，最后对和进行平方根运算。在这种情况下，如果平方误差中有一些预测非常不准确，它将导致较大的惩罚，从而使这个误差指标的值更高。我们可以看到这在前面的例子中如何发挥作用，也可以理解我们为什么选择使用RMSE。这个指标用于回归问题。'
- en: '**Area Under the Curve** (**AUC**): The AUC refers to the *Area under the Receiver-Operator
    Curve*. In this model, your target variable needs to be a value expressing the
    confidence or probability that a row belongs to the positive or negative target
    condition. To make this more concrete, AUC can be used when your task is to predict
    how likely someone is to make a given purchase. Clearly, from this explanation,
    we can see that AUC is a metric for classification.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曲线下面积**（**AUC**）：AUC指的是*接收者操作特征曲线下面积*。在这个模型中，你的目标变量需要表示一个值，表达某一行属于正目标条件或负目标条件的置信度或概率。为了让这个概念更加具体，AUC可以用于预测某人进行某次购买的可能性。从这个解释中，我们可以清楚地看出AUC是一个分类问题的指标。'
- en: '**Logarithmic Loss** (**Log-Loss**): The log-loss evaluation metric rewards
    confident predictions more than AUC and penalizes neutral predictions. This is
    important when we have an imbalanced target dataset and finding the minority class
    is critical. Having an extra penalty on incorrect guesses helps us get to the
    model that better predicts these minority class members correctly. Log-loss is
    also better for multiclass models where the target variable is not binary.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对数损失**（**Log-Loss**）：对数损失评估指标相比AUC更加奖励自信的预测，同时惩罚中立的预测。当目标数据集不平衡，且发现少数类非常关键时，这一点尤为重要。对错误预测施加额外的惩罚有助于我们找到一个能够更好地正确预测少数类成员的模型。对数损失在多类别模型中也表现更好，尤其是当目标变量不是二元时。'
- en: Improving model results
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型结果
- en: Since we have a regression problem, we now know why we chose RMSE, and we have
    a baseline metric of performance, we can begin to work on improving our model.
    Every model will have its own different way of improving results; however, we
    can generalize slightly. Feature engineering helps to improve model performance;
    however, since this type of work is less important with deep learning, we will
    not focus on that here. Also, we have already used feature engineering to generate
    our date and time parts. In addition, we can run our model for longer at a slower
    learning rate and we can tune hyperparameters. In order to find the best values
    using this type of model improvement method, we will use a technique called **grid
    search** to look at a range of values for a number of different fields.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有一个回归问题，现在我们明白为什么选择RMSE，并且有了一个性能基准指标，我们可以开始着手改进模型了。每个模型都有自己独特的改进方法；不过我们可以稍微概括一下。特征工程有助于提升模型性能；然而，由于深度学习对这类工作要求较低，我们在这里不再过多关注。此外，我们已经通过特征工程生成了日期和时间部分。我们还可以通过使用较慢的学习率运行模型更长时间，并且调优超参数。为了使用这种模型改进方法找到最佳的参数值，我们将使用一种叫做**网格搜索**（grid
    search）的方法，查看多个字段的不同值范围。
- en: Let's search for the optimal number of rounds. Using the cross-validation version
    of `xgboost` through the R interface, we can train our model again on our default
    hyperparameter settings. This time, instead of choosing 100 rounds, we will use
    the functionality within `xgboost` to determine the optimal number of trees to
    grow. Using cross-validation, the model can evaluate the error rate at the end
    of every round, and we will use the `early_stopping_rounds` feature so that the
    model stops growing additional trees after a given number of attempts, when it
    no longer continues to decrease the error rate.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来寻找最佳的轮次数。通过R接口使用`xgboost`的交叉验证版本，我们可以根据默认的超参数设置再次训练我们的模型。这一次，我们将不再选择100轮，而是利用`xgboost`中的功能来确定最佳的树木数量。通过交叉验证，模型可以在每轮结束时评估误差率，并且我们将使用`early_stopping_rounds`功能，使模型在误差率不再下降时，停止增加树木的数量。
- en: 'Let''s take a look at the following code, which determines the number of rounds
    or number of trees that produces the lowest error rate given the default setting:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下下面的代码，它确定了在默认设置下，哪一个轮次或树木数量能够产生最低的误差率：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After we run the preceding code, we will see model performance metrics printing
    to the console as the model runs. The beginning of this report on the console
    should look like the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码后，我们将看到模型性能指标在控制台上打印出来，随着模型的运行，报告的开头应该像以下截图一样：
- en: '![](img/a3f1eb80-0aef-4aca-ab6c-b7fe03e2b5dc.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3f1eb80-0aef-4aca-ab6c-b7fe03e2b5dc.png)'
- en: Here, we can see that our model performance is improving rapidly, and we have
    confirmation that our model will stop training when the model hasn't improved
    for 25 rounds.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到模型性能正在迅速提升，并且得到了确认，当模型在25轮内没有任何改进时，训练会停止。
- en: 'When your model reaches the optimal number of runs, your console should look
    like the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的模型达到最佳的运行次数时，控制台应该显示如下截图：
- en: '![](img/405145eb-3455-465a-90b3-7f44a11164e9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/405145eb-3455-465a-90b3-7f44a11164e9.png)'
- en: Here, we can see that the model performance is improving more slowly and that
    the model has stopped because it was no longer improving. The report that is printed
    out to the console also identifies the best performing round.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到模型的表现提升较慢，且模型已经停止，因为不再有改善。控制台打印出的报告也标明了表现最好的轮次。
- en: Breaking down everything that is happening in the preceding example, we again
    train an `xgboost` model on the same data with the same parameters as we did previously.
    However, we set the number of rounds to be much higher. In order to find the best
    iteration, we need enough rounds so that the model doesn't stop growing trees
    before finding the optimal number of trees. In this case, the best iteration occurs
    at around 3,205; so, if we had set the number of rounds at 1,000, for example,
    the modeling process would have completed after growing 1,000 trees. However,
    we still would not know the number of rounds that produces the lowest error rate,
    which is why we set the number of rounds to be so high.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析前面的示例中发生的一切时，我们再次在相同的数据和相同的参数下训练`xgboost`模型，就像我们之前做的一样。然而，我们将轮次设置得更高。为了找到最佳迭代次数，我们需要足够多的轮次，这样模型在找到最佳的树木数量之前不会停止生长树木。在这种情况下，最佳的迭代发生在大约3,205次；所以，如果我们将轮次设置为1,000次，例如，建模过程将在生长1,000棵树后完成。然而，我们仍然无法知道产生最低错误率的轮次数，这就是为什么我们将轮次设置得如此之高的原因。
- en: 'The following is the list of settings being used in the preceding code. Let''s
    take a look at the purpose of using each of these settings:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码中使用的设置列表。让我们来看看使用这些设置的目的：
- en: '`nfold`: This is the number of folds or how many partitions to make in the
    data for cross-validation. Here, we use `5`, which utilizes the alternating 80/20
    split referenced earlier.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nfold`：这是折叠数或进行交叉验证时将数据划分成多少个部分。这里我们使用`5`，这利用了前面提到的交替80/20拆分。'
- en: '`showsd`: This shows the standard deviation in order to note the variation
    among the results from the different combinations of folds. This is important
    to ensure the model works well on all sets of data and will generalize well when
    used on future data.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`showsd`：这个选项显示标准差，以显示不同折叠组合之间结果的变化。这一点很重要，因为它能确保模型在所有数据集上都表现良好，并且在未来的数据上具有良好的泛化能力。'
- en: '`stratified`: This ensures that each fold of data contains the same proportion
    of the target class.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stratified`：这确保每个折叠的数据包含相同比例的目标类。'
- en: '`print_every_n`: This tells us how often to print the results of the cross-validation
    to the console.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`print_every_n`：这个选项告诉我们每隔多少次打印一次交叉验证的结果到控制台。'
- en: '`early_stopping_rounds`: This is a value that decides when the model should
    stop growing trees. You can use this value to check whether performance has improved
    during the given number of rounds. The process will stop when the model no longer
    improves while growing trees to the limit of rounds set.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`：这是一个决定模型何时停止生长树木的值。你可以使用这个值来检查在给定轮次数内性能是否有所改善。当模型在达到设定的轮次限制后不再改善时，过程将停止。'
- en: '`maximize`: This notes whether the evaluation metric is one where improvement
    involves maximizing the score or minimizing the score.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maximize`：这个选项标明评估指标是否是一个通过最大化分数或最小化分数来改进的指标。'
- en: 'Now, let''s do a grid search on select hyperparameters. As the name implies,
    a grid search will model for all defined hyperparameter value combinations. Using
    this technique, we can adjust a few settings that will control how the model grows
    trees and then evaluate which settings provide the best performance. A complete
    list of all of the tunable hyperparameters for `xgboost` is included with the
    package documentation. For this example, we will focus on the following three
    hyperparameters:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对选定的超参数进行网格搜索。顾名思义，网格搜索会为所有定义的超参数值组合建模。使用这种技术，我们可以调整一些控制模型如何生长树木的设置，然后评估哪些设置提供了最佳的性能。`xgboost`的所有可调超参数的完整列表包含在软件包文档中。对于这个示例，我们将重点关注以下三个超参数：
- en: '`max_depth`: This is the maximum depth of the tree. Since we only have 4 features,
    we will try depths of `2`, `3`, and `4`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这是树的最大深度。由于我们只有4个特征，我们将尝试`2`、`3`和`4`的深度。'
- en: '`gamma`: This is the minimum loss reduction needed to continue creating splits.
    Setting this level higher will create shallower trees as nodes that contribute
    less to reducing the error rate are not further divided. We will try values of
    `0`, `0.5`, and `1`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：这是继续创建拆分所需的最小损失减少。将此级别设置得更高会创建更浅的树，因为对减少错误率贡献较小的节点不会进一步拆分。我们将尝试`0`、`0.5`和`1`的值。'
- en: '`min_child_weight`: This is the number of instances needed to grow a node.
    If there are fewer instances than the threshold, then the tree will discontinue
    partitioning from this node. The higher this number, the more shallow trees that
    will be grown. For this example, we will try values of `1`, `3`, and `5`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_child_weight`：这是增长一个节点所需的实例数量。如果实例数少于此阈值，则树将停止从此节点分裂。这个数字越大，生成的树就越浅。在本例中，我们将尝试`1`、`3`和`5`这几个值。'
- en: 'We will now go through all the code required to perform a grid search to tune
    our parameters to the optimal values in order to improve model performance:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将演示执行网格搜索所需的所有代码，以调整我们的参数到最佳值，从而提高模型性能：
- en: 'Our first step will be to define our search grid by assigning the vector of
    values, mentioned previously, to their respective hyperparameter within the parameter
    grid. We define the values we will try for our hyperparameters by running the
    following code:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的第一步将是通过将之前提到的值向量分配给参数网格中的各个超参数来定义我们的搜索网格。我们通过运行以下代码来定义我们将尝试的超参数值：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As shown in the preceding code, for expediency, we will set the number of rounds
    to 500\. Though, you could use the rounds found by searching for the best iteration
    previously in a real-world situation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，为了提高效率，我们将设置轮次为500。尽管在实际情况下，您可以使用之前通过搜索最佳迭代得到的轮次。
- en: When including `eta` in your grid search, also remember to include `nrounds`
    as you will need more rounds as the learning rate decreases.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在将`eta`包含到网格搜索中时，记得同时包含`nrounds`，因为随着学习率的降低，您将需要更多的迭代轮次。
- en: 'After this, we will use the `trainControl` function within `caret` to define
    how we want to handle this parameter search. For this, we will list the code and
    then walk through the settings selected. There are many additional settings for
    `trainControl`; however, we are focusing on a select few for this chapter. We
    set how we will train our model using the following code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在`caret`中使用`trainControl`函数来定义如何处理此参数搜索。为此，我们将列出代码并逐步解释所选择的设置。`trainControl`有很多其他设置，然而我们这一章只专注于其中几个。我们使用以下代码来设置模型训练方式：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `method` and `number` parameters for this function simply define our cross-validation
    strategy, which will be 5-fold again, as used previously. We will use a grid search
    to go through all possible combinations among the parameter settings defined in
    the last section of code. The next two parameters are used to save the resample
    and prediction details for the best iteration after modeling on all combinations.
    Lastly, we set `verboseIter` to `TRUE` to print iteration details to the console,
    and `allowParallel` is set to `TRUE` to use parallel processing to increase computational
    speed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的`method`和`number`参数仅定义我们的交叉验证策略，这里将使用5折交叉验证，和之前使用的一样。我们将使用网格搜索遍历在上一节代码中定义的所有超参数组合。接下来的两个参数用于在对所有组合建模后，保存最佳迭代的重采样和预测细节。最后，我们将`verboseIter`设置为`TRUE`，以便将迭代详情打印到控制台，并将`allowParallel`设置为`TRUE`，以使用并行处理提高计算速度。
- en: 'With the grid search values in place and the search strategy defined, we now
    run the model again using every possible hyperparameter combination. We then train
    our model while employing a grid search of our hyperparameters using the settings
    from the first two steps:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在设置好网格搜索的值并定义搜索策略后，我们将再次运行模型，使用每一种可能的超参数组合。然后，我们在训练模型时会使用前两步设置中的网格搜索超参数：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After running this code, we will see a report printing to our console that
    is similar to when we ran our model the first time. Your console should look like
    the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们将在控制台看到一个类似于第一次运行模型时的报告。您的控制台应类似于以下截图：
- en: '![](img/0fd3e9e5-7cb7-4d1d-86a1-906cc08b8020.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0fd3e9e5-7cb7-4d1d-86a1-906cc08b8020.png)'
- en: The report shows the current fold and current parameter setting as the model
    goes through all combinations on all five different splits of the data. In the
    preceding screenshot, we see a test of all `min_child_weight` options holding
    everything else constant on the fifth split of the data. In the end, we see the
    best tuning parameters being selected.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 报告显示了当前的折叠和当前的参数设置，当模型遍历所有五个不同数据划分的所有组合时。在前面的截图中，我们看到在数据的第五次划分上，对所有`min_child_weight`选项进行测试，同时保持其他条件不变。最终，我们看到选择了最佳的调优参数。
- en: In this situation, the best depth is using all of the features, which is not
    surprising given the lack of features. The best minimum child weight is `1`, which
    means that even sparsely populated nodes still hold important information for
    our model. The best gamma value is `0`, which is the default value. As this value
    rises, it places a slight constraint on the error rate improvement needed by each
    node before splitting. In this case, after our grid search, we are largely left
    with the default values; yet, we can see the process by which we would choose
    alternatives to these defaults if they helped to improve model performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最好的深度是使用所有特征，这并不奇怪，考虑到特征的缺乏。最佳的最小子节点权重是`1`，这意味着即使稀疏的节点仍然包含了对我们模型重要的信息。最佳的gamma值是`0`，这是默认值。随着这个值的增加，它对每个节点在分裂之前需要改进的误差率施加了轻微的约束。在这种情况下，经过我们的网格搜索，我们基本上仍然使用默认值；然而，我们可以看到选择这些默认值之外的替代方案的过程，如果它们有助于改进模型性能的话。
- en: 'Now that we know the best hyperparameter settings, we can plug them back into
    the model that we ran before and see whether there is any improvement. We train
    our model using the parameters and iteration count that we found optimizes performance
    by running the following code:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们知道最佳的超参数设置，可以将它们重新插入之前运行的模型中，看看是否有任何改进。我们使用找到的参数和迭代次数训练我们的模型，优化性能，运行以下代码：
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When we run the preceding code, we train our model and make predictions, like
    we did earlier, and we also run the line of code to calculate the RMSE value.
    When we run this line, we will see a value printed to our console. Your console
    should look like the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行上述代码时，我们训练我们的模型并进行预测，就像我们之前做的那样，我们还运行了一行代码来计算RMSE值。当我们运行这行代码时，我们将在控制台看到一个打印的值。您的控制台应该看起来像以下屏幕截图：
- en: '![](img/5d8f1590-72d6-450f-8c12-05069c79dbc6.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5d8f1590-72d6-450f-8c12-05069c79dbc6.png)'
- en: From the results of calculating the error rate, we can see that the score has
    changed from 0.054 to 0.022, which is an improvement from our first attempt. Using
    this data, with limited features, we may think that a time series model would
    have been a better choice; however, with only 1 year of data, a time series approach
    wouldn't catch any late seasonality effects that are not already present. This
    type of modeling creates a map for future years and shows that missing data can
    be predicted by simply using the date and time values for known data. This means
    that we can estimate NO2 values for future years. After collecting several years
    of data, a time series approach could then be used to make predictions that take
    into account year over year trends, in addition to the seasonality information
    captured here.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算错误率的结果来看，得分从0.054提高到了0.022，这比我们第一次尝试有所改进。利用这些数据，即使特征有限，我们可能会认为时间序列模型是更好的选择；然而，只有1年的数据，时间序列方法不会捕捉到尚未存在的季节性影响。这种建模方式为未来几年创建了一张地图，并显示缺失数据可以通过简单使用日期和时间值来预测已知数据。这意味着我们可以估算未来几年的NO2值。在收集了几年的数据之后，可以使用时间序列方法进行预测，考虑到逐年趋势和此处捕获的季节性信息。
- en: We used `xgboost`, which is a popular tree boosting algorithm, to predict pollution
    levels in an area of London. Earlier, we walked through creating a simple model
    to establish a benchmark. We then looked at how we should measure performance.
    Then, we took steps that improved performance. While we selected `xgboost` for
    this task, there are other machine learning algorithms that we could choose from,
    which we will review next.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`xgboost`，这是一种流行的树提升算法，用于预测伦敦某一地区的污染水平。早些时候，我们通过创建一个简单的模型来建立基准。然后，我们看了如何衡量性能。接着，我们采取了提高性能的步骤。虽然我们为这项任务选择了`xgboost`，但还有其他机器学习算法可供选择，我们将在接下来进行审查。
- en: Reviewing different algorithms
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查不同的算法
- en: We have raced through machine learning relatively quickly, as we wanted to focus
    on the underlying concepts that will follow along with us as we head into deep
    learning. As such, we cannot offer a comprehensive explanation of all machine
    learning techniques; however, we will quickly review the different algorithm types
    here, as this will be helpful to remember going forward.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相对快速地介绍了机器学习，因为我们希望专注于随着我们进入深度学习而跟随的基本概念。因此，我们不能提供对所有机器学习技术的全面解释；然而，我们将快速回顾这里的不同算法类型，这将有助于未来记忆。
- en: 'We''ll do a quick review of the following machine learning algorithms:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速审查以下几种机器学习算法：
- en: '**Decision Trees**: A decision tree is a simple model that makes up the base
    learners of many more complex algorithms. A decision tree simply splits a dataset
    at a given variable and notes the proportion of the target class that exists in
    the splits. For example, if we were to predict who is more likely to enjoy playing
    with baby toys, then a split on age would likely show that the split of the data
    containing just those under the age of 3 has a high percentage of true results
    in the `target` variable, that is, a high proportion that does enjoy this type
    of activity, while those who are older would likely not enjoy this.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**：决策树是一种简单的模型，它构成了许多更复杂算法的基本学习器。决策树简单地在给定的变量上将数据集进行划分，并记录每个分支中目标类别的比例。例如，如果我们要预测谁更可能喜欢玩婴儿玩具，那么基于年龄的划分可能会显示，包含3岁以下的数据显示出在`target`变量中高比例的正确结果，也就是说，喜欢这种活动的人占较大比例，而年龄较大的则可能不太喜欢。'
- en: '**Random Forests**: Random forests are similar to `xgboost`, which was used
    in this brief machine learning overview. A notable distinction between random
    forests and `xgboost` is that random forests build full decision trees. This makes
    up the set of base learners. The results from these simple base learner models
    are then averaged together to arrive at predictions that are better than any base
    learner. This technique is known as bagging. In contrast, `xgboost` uses boosting,
    which includes what it has learned from building previous base learners as it
    applies additional decision trees to the data. While both are useful and powerful
    ways to ensemble results and improve performance, we have chosen to focus on `xgboost`
    in this example because this idea of carrying forward information learned from
    the previous iteration is also present in deep learning.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**：随机森林类似于`xgboost`，后者在本篇机器学习概述中有提到。随机森林与`xgboost`的显著区别在于，随机森林构建完整的决策树，这些树组成了基本学习器的集合。然后将这些简单的基本学习器模型的结果进行平均，得出比任何基本学习器更好的预测结果。这种技术被称为bagging。相比之下，`xgboost`使用提升法（boosting），它会在构建先前基本学习器时学到的知识的基础上，应用额外的决策树来处理数据。尽管这两种方法都是有用且强大的集成结果和提升性能的方式，我们选择在本例中聚焦于`xgboost`，因为从前一次迭代中汲取的信息这一思想也同样体现在深度学习中。'
- en: '**Logistic Regression and Support Vector Machines** (**SVM**):SVMs separate
    features in *n*-dimensional space with a line that is the farthest from the two
    closest points in that space. This boundary is then applied to the test data and
    points on one side are classified one way, while points on the other are classified
    as a member of the other class. This is similar to logistic regression, with the
    main difference being that logistic regression evaluates all data points, while
    SVM just includes the points nearest to the line used to split the data. In addition,
    logistic regression works better when there are fewer explainer variables, and
    SVM works better when the dataset contains a larger number of dimensions. SVM
    will seek to find a line that divides all features, while logistic regression
    will use a combination of best-fitting, but not perfect, lines to estimate the
    probability that a data point belongs to a particular member of the target variable.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归与支持向量机**（**SVM**）：SVM通过一条距离该空间中最近的两个点最远的直线来划分特征。然后将这个边界应用于测试数据，一侧的点按一种方式分类，而另一侧的点则按另一种方式分类。这与逻辑回归类似，主要的区别在于，逻辑回归会评估所有数据点，而SVM只包括离用于划分数据的直线最近的点。此外，当解释变量较少时，逻辑回归效果较好，而当数据集的维度较大时，SVM效果更佳。SVM会寻找一条分隔所有特征的直线，而逻辑回归则会使用一系列拟合度最好的直线来估计数据点属于目标变量中特定类别的概率。'
- en: '**KNN** **and k-means**: These are two ways to create clusters within our data.
    KNN is a supervised learning technique. Using this method, the model plots the
    points on a *k*-dimensional feature space. When new points are introduced during
    the training process, the model identifies the class for the nearest neighbors
    to the new point and assigns this class to this record. By contrast, *k*-means
    is an unsupervised learning technique that finds centroids in the feature space
    such that *k* clusters can be created where each point is classified based on
    the minimum distance to a given centroid.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KNN** **和k-means**：这两种方法用于在数据中创建聚类。KNN是一种监督学习技术。通过这种方法，模型将数据点绘制在*k*维特征空间中。当在训练过程中引入新点时，模型会识别出新点的最近邻，并将该类分配给这个记录。相对而言，*k*-means是一种无监督学习技术，它在特征空间中找到质心，使得可以创建*k*个聚类，每个点根据与给定质心的最小距离来进行分类。'
- en: '**GBM and LightGBM**: Aside from `xgboost`, GBM and LightGBM also provide a
    means to generate predictions using a boosting mechanism for improving model performance
    between iterations. **Gradient Boosting Machines** (**GBM**) is the precursor
    to `xgboost` and LightGBM. It largely operates the same way by using a boosting
    ensemble technique on decision tree base learners; however, it is more primitive
    in its approach. GBM grows full trees using all features, while `xgboost` and
    LightGBM have different ways to reduce the number of splits that take place, which
    speed up how fast trees can be grown.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GBM和LightGBM**：除了`xgboost`，GBM和LightGBM也提供了一种使用提升机制生成预测的方法，以提高迭代之间的模型性能。**梯度提升机**（**GBM**）是`xgboost`和LightGBM的前身。它大体上以相同的方式运作，通过在决策树基学习器上使用提升集成技术；然而，它在方法上较为原始。GBM使用所有特征生长完整的树，而`xgboost`和LightGBM则有不同的方式来减少分裂的数量，从而加快树的生长速度。'
- en: The biggest difference between `xgboost` and LightGBM is that, where `xgboost`
    grows a new level for every tree after computing the feature splits, LightGBM
    will just grow the level below the most predictive leaf. This leaf-wise splitting
    offers superior speed advantages over the level-wise splitting used by `xgboost`.
    Also, with the focus on single leaf splits, the model can better find the values
    that minimize error compared with splitting into entire levels, which can lead
    to better performance. LightGBM may overtake `xgboost` as the go-to model for
    practitioners; however, for now, `xgboost` is still more widely used, which is
    why it was selected for this brief overview.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`和LightGBM之间的最大区别在于，`xgboost`在计算特征划分后，会为每棵树生长一个新层级，而LightGBM则仅会在最具预测性的叶子下方生长新层级。这种按叶子划分的方式相比于`xgboost`使用的按层级划分，提供了更快的速度优势。此外，专注于单一叶子的划分，模型能够更好地找到最小化误差的值，相较于划分整个层级，这能带来更好的性能。LightGBM可能会超越`xgboost`，成为从业者的首选模型；然而，目前`xgboost`仍然被更广泛地使用，这也是为何在此简要概述中选择了它。'
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we referred to a raw dataset, explored the data, and took the
    necessary preprocessing steps to get the data ready for modeling. We performed
    data type transformations to convert numbers and dates being stored as character
    strings into numeric and date value columns, respectively. In addition, we performed
    some feature engineering by breaking up the date value into its component parts.
    After completing preprocessing, we modeled our data. We followed an approach that
    included creating a baseline model and then tuning hyperparameters to improve
    our initial score. We used early stopping rounds and grid searches to identify
    hyperparameter values that produced the best results. After modifying our model-based
    results from our tuning procedures, we noticed much better performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个原始数据集，探索了数据，并采取了必要的数据预处理步骤，为建模做好准备。我们进行了数据类型转换，将以字符字符串形式存储的数字和日期分别转换为数值型和日期型列。此外，我们通过将日期值拆分成其组成部分，进行了特征工程。在完成预处理后，我们对数据进行了建模。我们采用了包括创建基准模型，并调优超参数以提高初始得分的方法。我们使用了早停轮次和网格搜索，找到了能够产生最佳结果的超参数值。在根据调优过程修改了模型的结果后，我们注意到性能有了显著提升。
- en: All of the aspects of machine learning that were discussed in this chapter will
    be used in the subsequent chapters too. We will need to get our data ready for
    modeling, and we will need to know how we can improve model performance by adjusting
    its settings. In addition, we have been focusing on a decision tree ensembling
    model in `xgboost` because our work with neural networks in upcoming chapters
    will be similar. We will need to consider efficiency and performance just as we
    did with `xgboost`, by adjusting how trees are grown.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的所有机器学习方面的内容将在后续章节中继续使用。我们需要为建模准备数据，并且需要了解如何通过调整设置来提高模型的性能。此外，我们一直专注于`xgboost`中的决策树集成模型，因为在接下来的章节中我们将进行与神经网络相关的工作，内容类似。我们将像在`xgboost`中一样，考虑效率和性能，通过调整树的生长方式来实现。
- en: This review of machine learning provides the foundation for stepping into deep
    learning. We begin, in the next chapter, with installing and exploring the packages
    used.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这段机器学习的回顾为进入深度学习奠定了基础。在下一章中，我们将开始安装和探索所用的包。
