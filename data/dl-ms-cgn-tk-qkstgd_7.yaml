- en: Deploying Models to Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境
- en: In the previous chapters of this book, we've worked on our skills for developing,
    testing, and using various deep learning models. We haven't talked much about
    the role of deep learning within the broader context of software engineering.
    In this last chapter, we will use the time to talk about continuous delivery,
    and the role of machine learning within this context. We will then look at how
    you can deploy models to production with a continuous delivery mindset. Finally,
    we will look at Azure Machine Learning service to properly manage the models you
    develop.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前几章中，我们已经在开发、测试和使用各种深度学习模型方面提高了技能。我们没有过多讨论深度学习在软件工程更广泛背景中的作用。在这一章中，我们将利用这段时间讨论持续交付，以及机器学习在这一背景中的作用。然后，我们将探讨如何以持续交付的思维方式将模型部署到生产环境。最后，我们将讨论如何使用
    Azure 机器学习服务来有效管理你开发的模型。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: Using machine learning in a DevOps environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 DevOps 环境中使用机器学习
- en: Storing models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储模型
- en: Using Azure Machine Learning service to manage models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Azure 机器学习服务来管理模型
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We assume that you have a recent version of Anaconda installed on your computer,
    and have followed the steps in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, to install CNTK on your computer. The sample code
    for this chapter can be found in our GitHub repository at: [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设你已在电脑上安装了最新版的 Anaconda，并按照[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)中的步骤，*开始使用
    CNTK*，将 CNTK 安装在你的电脑上。本章的示例代码可以在我们的 GitHub 仓库中找到： [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch7)。
- en: 'In this chapter, we''ll work on a few examples stored in Jupyter notebooks.
    To access the sample code, run the following commands inside an Anaconda prompt
    in the directory where you''ve downloaded the code:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理几个存储在 Jupyter 笔记本中的示例。要访问示例代码，请在你下载代码的目录中，打开 Anaconda 提示符并运行以下命令：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This chapter also contains a C# code sample to demonstrate how to load models
    in the open source ONNX format. If you want to run the C# code you will need to
    have .NET Core 2.2 installed on your machine. You can download the latest version
    of .NET core from: [https://dotnet.microsoft.com/download](https://dotnet.microsoft.com/download).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还包含一个 C# 代码示例，用于演示如何加载开源的 ONNX 格式模型。如果你想运行 C# 代码，你需要在机器上安装 .NET Core 2.2。你可以从以下网址下载最新版本的
    .NET Core：[https://dotnet.microsoft.com/download](https://dotnet.microsoft.com/download)。
- en: 'Check out the following video to see the code in action:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码的实际效果：
- en: '[http://bit.ly/2U8YkZf](http://bit.ly/2U8YkZf)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2U8YkZf](http://bit.ly/2U8YkZf)'
- en: Using machine learning in a DevOps environment
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 DevOps 环境中使用机器学习
- en: Most modern software development happens in an agile fashion, in an environment
    where developers and IT-pros work on the same project. The software we're building
    often is deployed to production through continuous integration and continuous
    deployment pipelines. How are we going to integrate machine learning in this modern
    environment? And does it mean we have to change a lot when we start building AI
    solutions? These are some of the frequently asked questions you can run into when
    you introduce AI and machine learning to the workflow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代软件开发都以敏捷的方式进行，在一个开发者和 IT 专业人员共同参与的环境中进行。我们所构建的软件通常通过持续集成和持续部署管道部署到生产环境中。我们如何在这种现代环境中集成机器学习？这是否意味着当我们开始构建
    AI 解决方案时，我们必须做出很多改变？这些是当你将 AI 和机器学习引入工作流程时，常见的一些问题。
- en: Luckily, you don't have to change your whole build environment or deployment
    tool stack to integrate machine learning into your software. Most of the things
    that we'll talk about will fit right into your existing environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你不需要改变整个构建环境或部署工具栈，就可以将机器学习集成到你的软件中。我们将讨论的大部分内容都可以很好地融入你现有的环境中。
- en: 'Let''s take a look at a typical continuous delivery scenario that you may encounter
    in a regular agile software project:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个典型的持续交付场景，这是你在常规敏捷软件项目中可能会遇到的：
- en: '![](img/31eff2da-92c2-470c-8e45-8b98931548be.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31eff2da-92c2-470c-8e45-8b98931548be.png)'
- en: This overview will look familiar if you've worked in a DevOps environment before.
    It starts with source control, which is connected to a continuous integration
    pipeline. The continuous integration pipeline produces artifacts that can be deployed
    to production. These artifacts are typically stored somewhere for backup and rollback
    purposes. This artifact repository is connected to a release pipeline that deploys
    the software to a test, acceptance, and, finally, a production environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾在DevOps环境中工作过，这个概述会让你感觉很熟悉。它从源代码管理开始，连接到持续集成管道。持续集成管道会生成可以部署到生产环境的工件。这些工件通常会被存储在某个地方，以便备份和回滚。这些工件仓库与一个发布管道相连接，发布管道将软件部署到测试、验收，最后到生产环境。
- en: 'You don''t need to change much of this standard setup to integrate machine
    learning into it. There are, however, a few key things that are important to get
    right when you start to use machine learning. Let''s focus on four stages and
    explore how to extend the standard continuous delivery setup:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要改变太多的标准设置就能将机器学习集成到其中。然而，开始使用机器学习时，有几个关键点是必须正确处理的。我们将重点讨论四个阶段，并探索如何扩展标准的持续交付设置：
- en: How to keep track of the data you use for machine learning.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何跟踪你用于机器学习的数据。
- en: Training models in a continuous integration pipeline.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在持续集成管道中训练模型。
- en: Deploying models to production.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境。
- en: Gathering feedback on production.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集生产反馈
- en: Keeping track of your data
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪你的数据
- en: 'Let''s start where it all begins with machine learning: the data with which
    you are going to train your models. It''s difficult to get good data for machine
    learning. Almost 80% of your effort will be on data management and data processing.
    It would be really sad if you had to redo all your work every time you want to
    train a model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从机器学习的起点开始：用于训练模型的数据。获取好的机器学习数据是非常困难的。几乎80%的工作将会是数据管理和数据处理。如果每次训练模型时都不得不重做所有工作，那会非常令人沮丧。
- en: That's why it is important to have some form of data management in place. This
    can be a central server where you store datasets that you know are good to use
    for training models. It could also be a data warehouse, if you have more than
    a few gigabytes of data. Some companies choose to use tools such as Hadoop or
    Azure Data Lake to manage their data. Whatever you use, the most important thing
    is to keep your dataset clean and in a format that is ready to use for training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么拥有某种形式的数据管理系统非常重要的原因。这可以是一个中央服务器，用于存储你知道适合用来训练模型的数据集。如果你有超过几GB的数据，它也可以是一个数据仓库。一些公司选择使用像Hadoop或Azure
    Data Lake这样的工具来管理他们的数据。无论你使用什么，最重要的是保持数据集的干净，并且以适合训练的格式存储。
- en: To create a data pipeline for your solution you can use traditional **Extract**
    **Transform** **Load** (**ETL**) tools, such as SQL server integration services,
    or you can build custom scripts in Python and execute them as part of a dedicated
    continuous integration pipeline in Jenkins, Azure DevOps, or Team Foundation Server.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要为你的解决方案创建数据管道，你可以使用传统的**提取** **转换** **加载**（**ETL**）工具，如SQL Server集成服务，或者你可以在Python中编写自定义脚本，并将其作为Jenkins、Azure
    DevOps或Team Foundation Server中专用持续集成管道的一部分执行。
- en: The data pipeline will be your tool to gather data from various business sources,
    and process it so that you get a dataset that is of sufficient quality to be stored
    as the master dataset for your model. It's important to note here that, although
    you can reuse datasets across different models, it is best not to start out with
    this goal in mind. You will quickly find that your master dataset will turn dirty
    and unmanageable when you try to use the dataset across too many usage scenarios.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道将是你从各种业务来源收集数据的工具，并处理它，以获得足够质量的数据集，作为你模型的主数据集存储。需要注意的是，虽然你可以在不同的模型间重用数据集，但最好不要一开始就以此为目标。你会很快发现，当你尝试将数据集用于太多不同的使用场景时，主数据集会变得脏乱且难以管理。
- en: Training models in a continuous integration pipeline
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在持续集成管道中训练模型
- en: Once you have a basic data pipeline running, it's time to look at integrating
    the training of AI models in your continuous integration environment. Up until
    now, we've only used Python notebooks to create our models. Sadly, Python notebooks
    don't deploy well to production. You can't automatically run them during a build.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了基本的数据管道，接下来就是将AI模型的训练集成到持续集成环境中的时候了。到目前为止，我们只使用了Python笔记本来创建我们的模型。可惜的是，Python笔记本在生产环境中并不好部署。你不能在构建过程中自动运行它们。
- en: In a continuous delivery environment, you can still use Python notebooks to
    perform initial experiments in order to discover patterns in the data and to build
    an initial version of your model. Once you have a candidate model, you will have
    to move your code away from a notebook and into a proper Python program.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续交付环境中，你仍然可以使用 Python 笔记本进行初步实验，以便发现数据中的模式并构建模型的初始版本。一旦你有了候选模型，就必须将代码从笔记本迁移到一个正式的
    Python 程序中。
- en: You can run your Python training code as part of a continuous integration pipeline.
    For example, if you're using Azure DevOps, Team Foundation Server, or Jenkins,
    you already have all the tools to run your training code as a continuous integration
    pipeline.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将 Python 训练代码作为持续集成管道的一部分来运行。例如，如果你使用 Azure DevOps、Team Foundation Server
    或 Jenkins，你已经拥有了运行训练代码作为持续集成管道的所有工具。
- en: We recommend running the training code as a separate pipeline from the rest
    of your software. Training a deep learning model often takes a very long time,
    and you don't want to lock your build infrastructure on that. Often, you will
    see people build training pipelines for their machine learning models using dedicated
    virtual machines, or even dedicated hardware, because of the amount of computation
    power it takes to train a model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议将训练代码作为与其他软件独立的管道运行。训练深度学习模型通常需要很长时间，你不希望将构建基础设施锁定在这上面。通常，你会看到人们为他们的机器学习模型构建训练管道，使用专用的虚拟机，甚至专用硬件，因为训练模型需要大量的计算能力。
- en: The continuous integration pipeline will produce a model based on a dataset
    you produced using your data pipeline. Just like code, you should also version
    your models and the settings you've used to train them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 持续集成管道将基于你通过数据管道生成的数据集生成模型。就像代码一样，你也应该为你的模型和用于训练它们的设置进行版本控制。
- en: Keeping track of your models and the settings you used to train them is important,
    as this allows you to experiment with different versions of the same model in
    production and gather feedback. Keeping a backup of your trained models also helps
    to get back in production fast after a disaster, such as a crashed production
    server.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪你的模型和用于训练它们的设置非常重要，因为这可以让你在生产环境中尝试同一模型的不同版本，并收集反馈。保持已训练模型的备份还可以帮助你在灾难发生后迅速恢复生产，例如生产服务器崩溃。
- en: Since models are binary files and can get quite large, it's best you treat your
    models as binary artifacts, much like NuGet packages in .NET, or Maven artifacts
    in Java.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型是二进制文件，且可能非常大，最好将模型视为二进制工件，就像 .NET 中的 NuGet 包或 Java 中的 Maven 工件一样。
- en: Tools like Nexus or Artifactory are great for storing models. Publishing your
    models in Nexus or Artifactory takes only a few lines of code, and will save you
    up to hundreds of hours of work retraining your model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Nexus 或 Artifactory 这样的工具非常适合存储模型。在 Nexus 或 Artifactory 中发布你的模型只需要几行代码，并且能节省你数百小时的重新训练模型的工作。
- en: Deploying models to production
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境
- en: Once you have a model, you need to be able to deploy it to production. If you've
    stored your models in a repository, such as Artifactory or Nexus, this becomes
    easier. You can create dedicated release pipelines in the same way that you would
    create a continuous integration pipeline. In Azure DevOps and Team Foundation
    Server, there's a dedicated feature for this. In Jenkins, you can use a separate
    pipeline to deploy models to a server.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了模型，你需要能够将其部署到生产环境。如果你将模型存储在诸如 Artifactory 或 Nexus 的仓库中，这将变得更加容易。你可以像创建持续集成管道一样创建专门的发布管道。在
    Azure DevOps 和 Team Foundation Server 中，有一个专用的功能来实现这一点。在 Jenkins 中，你可以使用单独的管道将模型部署到服务器。
- en: In the release pipeline, you can download your model from the artifact repository
    and deploy it to production. There are two main deployment methods for machine
    learning models. Either you can deploy it as an extra file with your application,
    or you can deploy it as a dedicated service component.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布管道中，你可以从工件仓库中下载模型并将其部署到生产环境。有两种主要的机器学习模型部署方法：你可以将其作为应用程序的额外文件进行部署，或者将其作为一个专用的服务组件进行部署。
- en: If you're deploying your model as part of an application, you will typically
    store just the model in your artifact repository. The model now becomes an extra
    artifact that needs to be downloaded in an existing release pipeline that deploys
    your solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将模型作为应用程序的一部分进行部署，通常只会将模型存储在你的工件仓库中。现在，模型变成了一个额外的工件，需要在现有的发布管道中下载，并部署到你的解决方案中。
- en: If you're deploying a dedicated service component for your model, you will typically
    store the model, the scripts that use the model to make a prediction, and other
    files needed by the model, in the artifact repository and deploy that to production.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为你的模型部署一个专用的服务组件，你通常会将模型、使用该模型进行预测的脚本以及模型所需的其他文件存储在工件仓库中，并将其部署到生产环境中。
- en: Gathering feedback on your models
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集模型反馈
- en: There's one last point that is important to keep in mind when working with deep
    learning or machine learning models in production. You've trained the models with
    a certain dataset. You hope this dataset contains a good representation of what
    is really happening in your production environment. But it doesn't have to be
    that way, because the world changes around you as you build your models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中使用深度学习或机器学习模型时，有一个最后需要记住的重要点。你用某个数据集训练了这些模型，你希望这个数据集能很好地代表生产环境中真实发生的情况。但实际情况可能并非如此，因为随着你构建模型，周围的世界也在变化。
- en: That's why it is important to ask for feedback from your users and update your
    model accordingly. Although not officially part of a continuous deployment environment,
    it's still an important aspect to set up correctly if you want to be successful
    with your machine learning solution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么向用户征求反馈并根据反馈更新模型非常重要的原因。尽管这不是持续部署环境的正式组成部分，但如果你希望你的机器学习解决方案成功运行，正确设置这一点仍然是非常重要的。
- en: Setting up a feedback loop doesn't have to be very complicated. For example,
    when you're classifying transactions for fraud detection, you can set up a feedback
    loop by asking an employee to validate the outcome of the model. You can then
    store the validation result of the employee with the input that was classified.
    By doing this, you make sure your model doesn't falsely accuse customers of fraud,
    and it helps you gather new observations to improve your model. Later, when you
    want to improve the model, you can use the newly gathered observations to extend
    your training set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 设置反馈循环并不需要非常复杂。例如，当你为欺诈检测分类交易时，你可以通过让员工验证模型的输出结果来设置反馈循环。然后，你可以将员工的验证结果与被分类的输入一起存储。通过这样做，你确保模型不会错误地指控客户欺诈，同时帮助你收集新的观察数据以改进模型。稍后，当你想要改进模型时，你可以使用新收集的观察数据来扩展你的训练集。
- en: Storing your models
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储模型
- en: In order to be able to deploy your models to production, you need to be able
    to store a trained model on disk. CNTK offers two ways to store models on disk.
    You can either store checkpoints to continue training at a later time, or you
    can store a portable version of your model. Each of these storage methods has
    its own use.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够将你的模型部署到生产环境中，你需要能够将训练好的模型存储到磁盘上。CNTK提供了两种在磁盘上存储模型的方法。你可以存储检查点以便稍后继续训练，或者你可以存储一个便携版的模型。这两种存储方法各有其用途。
- en: Storing model checkpoints to continue training at a later point
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储模型检查点以便稍后继续训练
- en: Some models take a long time to train, sometimes up to weeks at a time. You
    don't want to lose all your progress when your machine crashes during training,
    or if there's a power outage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型训练时间较长，有时甚至需要几周时间。你不希望在训练过程中机器崩溃或者停电时丢失所有进度。
- en: 'This is where checkpointing becomes useful. You can create a checkpoint during
    training using a `CheckpointConfig` object. You can add this additional callback
    to your training code by modifying the callbacks list as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这时，检查点功能就变得非常有用。你可以在训练过程中使用`CheckpointConfig`对象创建一个检查点。你可以通过以下方式修改回调列表，添加此额外的回调到你的训练代码中：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Follow the given steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, create a new `CheckpointConfig` and provide a filename for the checkpointed
    model file, the number of minibatches before a new checkpoint should be created
    as the `frequency` and set the `preserve_all` setting to `False`.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个新的`CheckpointConfig`，并为检查点模型文件提供文件名，设置在创建新检查点之前的小批量数量作为`frequency`，并将`preserve_all`设置为`False`。
- en: Next, use the train method on the `loss` and provide the `checkpoint_config`
    in the `callbacks` keyword argument to use checkpointing.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`loss`上的train方法，并在`callbacks`关键字参数中提供`checkpoint_config`以使用检查点功能。
- en: When you use checkpointing during training, you will start to see additional
    files on disk named `solar.dnn`, and `solar.dnn.ckp`. The `solar.dnn` file contains
    the trained model stored in a binary format. The `solar.dnn.ckp` file contains
    the checkpoint information for the minibatch source used during training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在训练过程中使用检查点时，你会在磁盘上看到额外的文件，名为 `solar.dnn` 和 `solar.dnn.ckp`。`solar.dnn` 文件包含以二进制格式存储的训练模型。`solar.dnn.ckp`
    文件包含在训练过程中使用的小批量源的检查点信息。
- en: The most recent checkpoint is automatically restored for you when you set the
    restore parameter of the `CheckpointConfig` object to `True`. This makes it easy
    to integrate checkpointing in your training code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将 `CheckpointConfig` 对象的 `restore` 参数设置为 `True` 时，最近的检查点会自动恢复给你。这使得在训练代码中集成检查点变得非常简单。
- en: Having a checkpointed model is not only useful in case you run into a computer
    problem during training. A checkpoint is also useful if you want to continue training
    after you've gathered additional data from production. You can simply restore
    the latest checkpoint and start feeding new samples into the model from there.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个检查点模型不仅在训练过程中遇到计算机问题时很有用。如果你在从生产环境收集到额外数据后希望继续训练，检查点也会派上用场。你只需要恢复最新的检查点，然后从那里开始将新的样本输入到模型中。
- en: Storing portable models for use in other applications
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储可移植的模型以供其他应用使用
- en: Although you can use a checkpointed model in production, it's not very smart
    to do so. Checkpointed models are stored in a format that is only understood by
    CNTK. For now, it's fine to use the binary format, since CNTK is around and the
    model format will remain compatible for quite a long time. But, as with all software,
    CNTK isn't made to last for an eternity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以在生产环境中使用检查点模型，但这样做并不聪明。检查点模型以 CNTK 只能理解的格式存储。现在，使用二进制格式是可以的，因为 CNTK 仍然存在，且模型格式将在相当长一段时间内保持兼容。但和所有软件一样，CNTK
    并不是为了永恒存在而设计的。
- en: That's exactly why ONNX was invented. ONNX is the open neural network exchange
    format. When you use ONNX, you store your model in a protobuf compatible format
    that is understood by many other frameworks. There's even a native ONNX runtime
    available for Java and C#, which allows you to use models created in CNTK from
    your .NET or Java application.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 ONNX 被发明的原因。ONNX 是开放的神经网络交换格式。当你使用 ONNX 时，你将模型存储为 protobuf 兼容的格式，这种格式被许多其他框架所理解。甚至还有针对
    Java 和 C# 的原生 ONNX 运行时，这使得你可以在 .NET 或 Java 应用程序中使用 CNTK 创建的模型。
- en: ONNX is supported by a number of large companies, such as Facebook, Intel, NVIDIA,
    Microsoft, AMD, IBM, and Hewlett-Packard. Some of these companies offer converters
    for ONNX, while others even support running ONNX models directly on their hardware
    without using additional software. NVIDIA has a number of chips available now
    that can read ONNX files directly and execute these models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 得到了许多大型公司的支持，如 Facebook、Intel、NVIDIA、Microsoft、AMD、IBM 和惠普。这些公司中的一些提供了
    ONNX 转换器，而另一些甚至支持在其硬件上直接运行 ONNX 模型，而无需使用额外的软件。NVIDIA 目前有多款芯片可以直接读取 ONNX 文件并执行这些模型。
- en: As an example, we'll first explore how to store a model in the ONNX format and
    use C# to load it from disk again to make predictions. First, we'll look at how
    to save a model in the ONNX format and after that we'll explore how to load ONNX
    models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，我们将首先探索如何将模型存储为 ONNX 格式，并使用 C# 从磁盘加载它来进行预测。首先，我们将看看如何保存一个模型为 ONNX 格式，之后再探索如何加载
    ONNX 模型。
- en: Storing a model in ONNX format
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储 ONNX 格式的模型
- en: 'To store a model in the ONNX format you can use the `save` method on the `model`
    function. When you don''t provide any additional parameters, it will store the
    model in the same format as is used for checkpointing. You can, however, provide
    an additional parameter to specify the model format as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要将模型存储为 ONNX 格式，你可以在 `model` 函数上使用 `save` 方法。当你不提供额外参数时，它将以用于检查点存储的相同格式存储模型。不过，你可以提供额外的参数来指定模型格式，如下所示：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the given steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, import the `ModelFormat` enumeration from the `cntk` package.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 `cntk` 包中导入 `ModelFormat` 枚举。
- en: Next, invoke the `save` method on the trained model with the output filename
    and specify `ModelFormat.ONNX` as the `format` keyword argument.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在训练好的模型上调用 `save` 方法，指定输出文件名，并将 `ModelFormat.ONNX` 作为 `format` 关键字参数。
- en: Using ONNX models in C#
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 C# 中使用 ONNX 模型
- en: Once the model is stored on disk, we can use C# to load and use it. CNTK version
    2.6 includes a pretty complete API for C#, which you can use for training and
    evaluating models.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型存储在磁盘上，我们可以使用 C# 加载并使用它。CNTK 版本 2.6 包含了一个相当完整的 C# API，你可以用它来训练和评估模型。
- en: To use a CNTK model in C# you need to use a library called `CNTK.GPU` or `CNTK.CPUOnly`,
    which can be retrieved from NuGet, a package manager for .NET. The CPU-only version
    of CNTK includes a version of the CNTK binaries that have been compiled to run
    models on the CPU, while the GPU version can use the GPU as well as the CPU.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 C# 中使用 CNTK 模型，你需要使用一个名为`CNTK.GPU`或`CNTK.CPUOnly`的库，它们可以通过 NuGet（.NET 的包管理器）获取。CNTK
    的 CPU-only 版本包含了已编译的 CNTK 二进制文件，用于在 CPU 上运行模型，而 GPU 版本则既可以使用 GPU，也可以使用 CPU。
- en: 'Loading a CNTK model in C# is done by using the following snippet of code:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 C# 加载 CNTK 模型是通过以下代码片段实现的：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Follow the given steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, create a device descriptor so the model is executed against the CPU.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个设备描述符，以便模型在 CPU 上执行。
- en: Next, use the `Function.Load` method to load the previously stored model. Provide
    the `deviceDescriptor` and use the `ModelFormat.ONNX` to load the file as ONNX
    model.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用 `Function.Load` 方法加载先前存储的模型。提供 `deviceDescriptor`，并使用 `ModelFormat.ONNX`
    将文件加载为 ONNX 模型。
- en: 'Now that we have loaded the model, let''s make a prediction with it. For, this
    we need to write another fragment of code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了模型，接下来让我们用它进行预测。为此，我们需要编写另一个代码片段：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Follow the given steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: Create a new method `Predict` that accepts the input features for the model.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的 `Predict` 方法，该方法接受模型的输入特征。
- en: Within the `Predict` method, store the input and output variable of the model
    in two separate variables for easy access.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `Predict` 方法中，将模型的输入和输出变量存储在两个独立的变量中，方便访问。
- en: Next, create a dictionary to map data to the input and output variables of the
    model.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个字典，将数据映射到模型的输入和输出变量。
- en: Then, create a new batch, containing one sample with the input features for
    the model.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的批次，包含一个样本，作为模型的输入特征。
- en: Add a new entry to the input mapping to map the batch to the input variable.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向输入映射添加一个新条目，将批次映射到输入变量。
- en: Next, add a new entry to the output mapping for the output variable.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，向输出映射添加一个新条目，映射到输出变量。
- en: Now, invoke the `Evaluate` method on the loaded model with the input, output
    mapping, and a device descriptor.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用输入、输出映射和设备描述符在加载的模型上调用`Evaluate`方法。
- en: Finally, extract the output variable from the output mapping and retrieve the
    data.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，从输出映射中提取输出变量并检索数据。
- en: The sample code for this chapter includes a basic C# project in .NET core that
    demonstrates the use of CNTK from a .NET Core project. You can find the sample
    code in the `csharp-client` folder in the code examples directory for this chapter.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例代码包含一个基本的 .NET Core C# 项目，演示了如何在 .NET Core 项目中使用 CNTK。你可以在本章的代码示例目录中的 `csharp-client`
    文件夹找到示例代码。
- en: Working with models stored in the ONNX format makes it possible to use Python
    for training models and C# or another language to run models on production. This
    is especially useful since the runtime performance of a language like C# is much
    better than that of Python.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用存储为 ONNX 格式的模型，可以让你用 Python 训练模型，使用 C# 或其他语言在生产环境中运行模型。这尤其有用，因为像 C# 这样的语言的运行时性能通常比
    Python 更好。
- en: In the next section we'll look at using Azure Machine Learning service to manage
    the process of training and storing models so we have a much more structured way
    of working with models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何使用 Azure 机器学习服务来管理训练和存储模型的过程，从而让我们能够更加有条理地处理模型。
- en: Using Azure Machine Learning service to manage models
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Azure 机器学习服务来管理模型
- en: While you can completely hand-build a continuous integration pipeline, it's
    still quite a bit of work. You need to get dedicated hardware to run deep learning
    training jobs, and that can bring up the costs. There are great alternatives available
    in the cloud. Google has a TensorFlow serving offer. Microsoft offers Azure Machine
    Learning service as a way to manage models. Both are great tools that we can highly
    recommend.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以完全手动构建一个持续集成管道，但这仍然是相当费力的工作。你需要购买专用硬件来运行深度学习训练任务，这可能会带来更高的成本。不过，云端有很好的替代方案。Google
    提供了 TensorFlow 服务，Microsoft 提供了 Azure 机器学习服务来管理模型。两者都是非常出色的工具，我们强烈推荐。
- en: 'Let''s take a look at Azure Machine Learning service to get a sense of what
    it can do for you when you want to set up a complete machine learning pipeline:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Azure 机器学习服务，当你希望设置一个完整的机器学习管道时，它可以为你做些什么：
- en: '![](img/931612d7-2dae-4252-b2ba-0ea9d6bf7d19.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/931612d7-2dae-4252-b2ba-0ea9d6bf7d19.png)'
- en: Azure Machine Learning service is a cloud service that offers a complete solution
    for every phase of your machine learning project. It has the concept of experiments,
    and runs that allow you to manage experiments. It features a model registry that
    allows you to store trained models and Docker images for those models. You can
    use the Azure Machine Learning service tools to deploy these models to production
    in a matter of minutes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习服务是一个云服务，提供每个机器学习项目阶段的完整解决方案。它具有实验和运行的概念，允许你管理实验。它还提供模型注册功能，可以存储已训练的模型和这些模型的
    Docker 镜像。你可以使用 Azure 机器学习服务工具将这些模型快速部署到生产环境中。
- en: Deploying Azure Machine Learning service
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Azure 机器学习服务
- en: 'In order to use this service, you need to have an active account on Azure.
    You can use a trial account by going to: [https://azure.microsoft.com/en-gb/free/](https://azure.microsoft.com/en-gb/free/),
    if you haven''t got an account yet. This will give you a free account for 12 months
    with 150 dollars, worth of credits to explore the different Azure services.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此服务，你需要在 Azure 上拥有一个有效账户。如果你还没有账户，可以访问：[https://azure.microsoft.com/en-gb/free/](https://azure.microsoft.com/en-gb/free/)，使用试用账户。这将为你提供一个免费账户，有效期为
    12 个月，附带价值 150 美元的信用额度，可以探索各种 Azure 服务。
- en: There are many ways in which you can deploy Azure Machine Learning service.
    You can create a new instance through the portal, but you can also use the cloud
    shell to create an instance of the service. Let's take a look at how you can create
    a new Azure Machine Learning service instance through the portal.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 Azure 机器学习服务有很多种方式。你可以通过门户创建一个新实例，也可以使用云 Shell 创建服务实例。让我们看看如何通过门户创建一个新的 Azure
    机器学习服务实例。
- en: 'With your favorite browser, navigate to the URL at: [https://portal.azure.com/](https://portal.azure.com/).
    Log in with your credentials, and you will be greeted with a portal that shows
    you all your available Azure resources and a dashboard resembling the following
    screenshot:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你最喜欢的浏览器，导航到以下 URL：[https://portal.azure.com/](https://portal.azure.com/)。使用你的凭据登录，你将看到一个门户，展示所有可用的
    Azure 资源和一个类似于以下截图的仪表板：
- en: '![](img/d6be4d9a-532b-43d3-873e-7f298c5fd304.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6be4d9a-532b-43d3-873e-7f298c5fd304.png)'
- en: Azure resources and a dashboard
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 资源和仪表板
- en: 'From this portal you can create new Azure resources, such as the Azure Machine
    Learning Workspace. Click the large + button at the top left of the screen to
    get started. This will show the following page, allowing you to create a new resource:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此门户，你可以创建新的 Azure 资源，例如 Azure 机器学习工作区。在屏幕左上角点击大号的 + 按钮开始操作。这将显示以下页面，允许你创建一个新的资源：
- en: '![](img/d473560b-00c5-4ba2-88ef-2cb881565369.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d473560b-00c5-4ba2-88ef-2cb881565369.png)'
- en: Creating a new resource
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建新资源
- en: 'You can search for different types of resources in this search bar. Search
    for Azure Machine Learning and select the Azure Machine Learning Workspace resource
    type from the list. This will show the following details panel that allows you
    to start the creation wizard:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此搜索框中搜索不同类型的资源。搜索 Azure 机器学习并从列表中选择 Azure 机器学习工作区资源类型。这将显示以下详细信息面板，允许你启动创建向导：
- en: '![](img/ea5b88f6-7e13-4d81-8726-59f65d9c68c3.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea5b88f6-7e13-4d81-8726-59f65d9c68c3.png)'
- en: Starting the creation wizard
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 开始创建向导
- en: 'This details panel will explain what the resource does, and point towards the
    documentation and other important information about this resource, such as the
    pricing details. To create a new instance of this resource type, click the create
    button. This will start the wizard to create a new instance of the Azure Machine
    Learning workspace as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该详细信息面板将解释该资源的功能，并指向文档及有关此资源的其他重要信息，例如定价详情。要创建此资源类型的新实例，请点击创建按钮。这将启动创建 Azure
    机器学习工作区实例的向导，如下所示：
- en: '![](img/c36e7609-d67b-4cd8-8700-c6d3da70200d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c36e7609-d67b-4cd8-8700-c6d3da70200d.png)'
- en: Creating a new instance of the Azure Machine Learning workspace
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的 Azure 机器学习工作区实例
- en: In the creation wizard, you can configure the name of the workspace, the resource
    group it belongs to, and the data center it should create. Azure resources are
    created as part of resource groups. These resource groups help you organize things,
    and keep related infrastructure together in one place. If you want to remove a
    set of resources, you can just delete the resource group instead of every resource
    separately. This is especially useful if you want to remove everything after you're
    done testing the machine learning workspace.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建向导中，您可以配置工作区的名称、它所属的资源组以及它应该创建的数据中心。Azure 资源作为资源组的一部分创建。这些资源组有助于组织您的资源，并将相关的基础设施集中在一个地方。如果您想删除一组资源，可以直接删除资源组，而不需要单独删除每个资源。如果您完成机器学习工作区的测试后想要删除所有内容，这个功能尤其有用。
- en: It's a good idea to use a dedicated resource group for the machine learning
    workspace, since it will contain more than one resource. Mixing this with other
    resources will make it harder to clean up after you're done or need to move resources
    for some reason.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用专用的资源组来创建机器学习工作区是一个好主意，因为它将包含多个资源。如果与其他资源混合使用，将使得在完成后清理资源或需要移动资源时变得更加困难。
- en: Once you have clicked the create button at the bottom of the screen, the machine
    learning workspace is created. This will take a few minutes. In the background,
    the Azure Resource Manager will create a number of resources based on the selection
    in the creation wizard. You will receive a notification in the portal when the
    deployment is completed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦点击屏幕底部的创建按钮，机器学习工作区就会被创建。这需要几分钟时间。在后台，Azure 资源管理器将根据创建向导中的选择创建多个资源。部署完成后，您将在门户中收到通知。
- en: 'When the machine learning workspace is created, you can navigate to the workspace
    through the portal by first going to the Resource groups on the portal in the
    navigation bar on the left of the screen. Next, click the resource group you just
    created to get an overview of the machine learning workspace and related resources,
    as shown in the following screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 创建机器学习工作区后，您可以通过门户进行导航。首先，在屏幕左侧的导航栏中进入资源组。接下来，点击您刚刚创建的资源组，以查看机器学习工作区及其相关资源的概况，如下图所示：
- en: '![](img/d9c88065-5451-4366-9750-fb73570dbb4b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9c88065-5451-4366-9750-fb73570dbb4b.png)'
- en: Getting an overview of the machine learning workspace and related resources
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 了解机器学习工作区和相关资源概况
- en: There's the workspace itself, with a dashboard that allows you to explore experiments
    and manage some aspects of your machine learning solution. The workspace also
    includes a Docker registry to store models as Docker images, together with the
    scripts needed to make a prediction using a model. When you check out the workspace
    on Azure Portal, you'll also find a storage account that you can use to store
    datasets and data generated by your experiments.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 工作区本身包括一个仪表板，您可以通过它来探索实验并管理机器学习解决方案的某些方面。工作区还包括一个 Docker 注册表，用于存储模型作为 Docker
    镜像，以及用于使用模型进行预测所需的脚本。当您在 Azure 门户查看工作区时，您还会找到一个存储帐户，您可以使用它来存储数据集和实验生成的数据。
- en: One of the nice things that's included in an Azure Machine Learning service
    environment is an Application Insights instance. You can use Application Insights
    to monitor your models in production and gather valuable feedback to improve your
    models later on. This is included by default, so you don't have to manually create
    a monitoring solution for your machine learning solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习服务环境中的一个亮点是包含了一个应用程序洞察（Application Insights）实例。您可以使用应用程序洞察来监控生产环境中的模型，并收集宝贵的反馈以改进模型。这是默认包含的，因此您不需要为机器学习解决方案手动创建监控解决方案。
- en: Exploring the machine learning workspace
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索机器学习工作区
- en: 'The Azure Machine Learning workspace contains a number of elements. Let''s
    explore them to get a feel of what''s available to you when you start working
    with it:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习工作区包含多个元素。让我们来探索一下这些元素，以便了解当您开始使用工作区时可以使用哪些功能：
- en: '![](img/560a062f-5309-48c3-a1b7-0f4bdef7b38e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/560a062f-5309-48c3-a1b7-0f4bdef7b38e.png)'
- en: Machine learning workspace
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作区
- en: To get to the machine learning workspace, click the resource groups item in
    the navigation bar on the left of the screen. Select the resource group containing
    the machine learning workspace item and click on machine learning workspace. It
    will have the name you've configured in the creation wizard earlier.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要进入机器学习工作区，点击屏幕左侧导航栏中的资源组项目。选择包含机器学习工作区项目的资源组，并点击机器学习工作区。它将显示你在创建向导中之前配置的名称。
- en: In the workspace, there's a dedicated section for experiments. This section
    will provide access to experiments that you've run in the workspace, as well as
    details about the runs executed as part of the experiments.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作区中，有一个专门用于实验的部分。这个部分将提供对你在工作区中运行的实验以及作为实验一部分执行的运行的详细信息。
- en: Another useful element of the machine learning workspace is the models section.
    When you've trained a model, you can store it in the model registry so you can
    deploy it to production at a later time. A model automatically connects to the
    experiment run that produced it, so you can always trace back what code was used
    to produce a model, and which settings were used to train it.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作区的另一个有用部分是模型部分。当你训练了一个模型时，你可以将它存储在模型注册表中，以便以后将其部署到生产环境。模型会自动连接到生成它的实验运行，因此你总是可以追溯到使用了什么代码来生成模型，以及训练模型时使用了哪些设置。
- en: Below the model section is the images section. This section shows you the Docker
    images created from your models. You can package models in Docker images together
    with a scoring script to make deployment to production easier and more predictable.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 模型部分下方是镜像部分。这个部分显示了你从模型创建的 Docker 镜像。你可以将模型与评分脚本一起打包成 Docker 镜像，以便更轻松、可预测地部署到生产环境。
- en: Finally, there's the deployment sections that contain all the deployments based
    on the images. You can use Azure Machine Learning service to deploy models to
    production using single container instances, a virtual machine, or even a Kubernetes
    cluster, should you need to scale your model deployment.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，部署部分包含了所有基于镜像的部署。你可以使用 Azure 机器学习服务将模型部署到生产环境，使用单个容器实例、虚拟机，或者如果需要扩展模型部署，还可以使用
    Kubernetes 集群。
- en: Azure Machine Learning service also offers a technique that allows you to build
    a pipeline to prepare data, train a model, and deploy it to production. This feature
    can be useful should you want to build a single process that contains both preprocessing
    steps and training steps. It's especially powerful in cases where you need to
    execute many steps to obtain a trained model. For now, we'll limit ourselves to
    running basic experiments and deploying the resulting model to a production Docker
    container instance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习服务还提供了一种技术，允许你构建一个管道，用于准备数据、训练模型并将其部署到生产环境。如果你想构建一个包含预处理步骤和训练步骤的单一过程，这个功能将非常有用。特别是在需要执行多个步骤才能获得训练模型的情况下，它尤其强大。现在，我们将限制自己进行基本的实验并将结果模型部署到生产
    Docker 容器实例中。
- en: Running your first experiment
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行你的第一个实验
- en: Now that you have a workspace, let's take a look at how to use it from a Python
    notebook. We'll modify some deep learning code so we save the trained model to
    the Azure Machine Learning service workspace as the output of an experiment, and
    track metrics for the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了工作区，我们来看看如何在 Python 笔记本中使用它。我们将修改一些深度学习代码，以便将训练后的模型作为实验的输出保存到 Azure 机器学习服务的工作区，并跟踪模型的指标。
- en: 'First, we need to install the `azureml` package as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要安装`azureml`包，方法如下：
- en: '[PRE5]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `azureml` package contains the necessary components to run experiments.
    In order for it to work, you''ll need to create a file called `config.json` in
    the root of your machine learning project. If you''re working with the sample
    code for this chapter, you can modify the `config.json` file in the `azure-ml-service`
    folder. It contains the following content:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`azureml`包包含了运行实验所需的组件。为了使其工作，你需要在机器学习项目的根目录下创建一个名为`config.json`的文件。如果你正在使用本章的示例代码，你可以修改`azure-ml-service`文件夹中的`config.json`文件。它包含以下内容：'
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This file contains the workspace your Python code will work with, the resource
    group that contains the workspace you're working with, and the subscription that
    the workspace was created in. The workspace name should match the name you've
    chosen in the wizard to create the workspace earlier. The resource group should
    match the one that contains the workspace. Finally, you will need to find the
    subscription ID.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件包含了你的Python代码将使用的工作区、包含该工作区的资源组，以及创建工作区的订阅。工作区名称应与之前在向导中创建工作区时选择的名称匹配。资源组应与包含该工作区的资源组匹配。最后，你需要找到订阅ID。
- en: 'When you navigate to Resource groups for the machine learning workspace on
    the portal, you''ll see the Subscription ID at the top of the resource group details
    panel, as shown in the following screenshot:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在门户中导航到机器学习工作区的资源组时，你会看到资源组详情面板顶部显示了订阅ID，如下图所示：
- en: '![](img/e46ec086-f4c5-4d0b-b3f5-dffbfabc7860.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e46ec086-f4c5-4d0b-b3f5-dffbfabc7860.png)'
- en: Subscription ID at the top of the resource group details panel
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 资源组详情面板顶部的订阅ID
- en: 'When you hover over the value for the Subscription ID, the portal will show
    a button to copy the value to your clipboard. Paste this value into the subscriptionId
    field of the config file and save it. You can now connect to your workspace from
    any Python notebook or Python program by using the following small snippet of
    code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将鼠标悬停在订阅ID的值上时，门户会显示一个按钮，允许你将该值复制到剪贴板。将此值粘贴到配置文件中的subscriptionId字段，并保存。你现在可以通过以下代码片段从任何Python笔记本或Python程序连接到你的工作区：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Follow the given steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, we create a new workspace based on the configuration file we just created.
    This connects to the workspace in Azure. Once you're connected, you can create
    a new experiment with a name of your choice and connect it to the workspace.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们基于刚才创建的配置文件创建一个新的工作区。这将连接到Azure中的工作区。一旦连接成功，你可以创建一个新的实验，并为它选择一个名称，然后将其连接到工作区。
- en: Next, create a new experiment and connect it to the workspace.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个新的实验并将其连接到工作区。
- en: An experiment in Azure Machine Learning service can be used to keep track of
    an architecture you're testing with CNTK. For example, you could create an experiment
    for a convolutional neural network, and a second experiment to try solving the
    same problem with a recurrent neural network.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习服务中，实验可用于跟踪你正在使用CNTK测试的架构。例如，你可以为卷积神经网络创建一个实验，然后再创建一个实验来尝试使用递归神经网络解决相同的问题。
- en: 'Let''s explore how to track metrics and other output from experiments. We''ll
    use the iris flower classification model from previous chapters and extend the
    training logic to track metrics as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何跟踪实验中的度量和其他输出。我们将使用前几章的鸢尾花分类模型，并扩展训练逻辑以跟踪度量，具体如下：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Follow the given steps:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the `default_options` and `input_variable` function.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`default_options`和`input_variable`函数。
- en: Next, import the layer types for the model from the `cntk.layers` module.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从`cntk.layers`模块导入模型所需的层类型。
- en: After that, import the `log_softmax` and `sigmoid` activation function from
    the `cntk.ops` module.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从`cntk.ops`模块导入`log_softmax`和`sigmoid`激活函数。
- en: Create a new `Sequential` layer set.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`Sequential`层集。
- en: Add a new `Dense` layer to the `Sequential` layer set with 4 neurons and the
    `sigmoid` activation function.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向`Sequential`层集添加一个新的`Dense`层，包含4个神经元和`sigmoid`激活函数。
- en: Add another `Dense` layer with 3 outputs and a `log_softmax` activation function.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加另一个具有3个输出的`Dense`层，并使用`log_softmax`激活函数。
- en: Create a new `input_variable` with size 4.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`input_variable`，大小为4。
- en: Invoke the model with the `features` variable to complete the model.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`features`变量调用模型以完成模型。
- en: 'To train the model, we''re going to use a manual minibatch loop. First, we''ll
    have to load and preprocess the iris dataset so that it matches the format that
    our model expects, as demonstrated in the following code snippet:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们将使用手动的小批量循环。首先，我们需要加载并预处理鸢尾花数据集，以便它与我们的模型所期望的格式匹配，如下方的代码片段所示：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Follow the given steps:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: Import the `pandas` and `numpy` modules to load the CSV file containing the
    training samples.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`numpy`模块以加载包含训练样本的CSV文件。
- en: Use the read_csv function to load the input file containing the training data.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用read_csv函数加载包含训练数据的输入文件。
- en: Next, extract the first 4 columns as the input features
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，提取前4列作为输入特征。
- en: Finally, extract the species column as the labels
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，提取物种列作为标签。
- en: 'The labels are stored as a string, so we''ll have to convert those to a set
    of one-hot vectors in order to match the model as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是以字符串形式存储的，因此我们需要将它们转换为一组独热向量，以便与模型匹配，具体如下：
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Follow the given steps:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: Create a mapping from labels to their numeric representation.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个标签到其数值表示的映射。
- en: Next, define a new utility function `one_hot` to encode a numeric value to a
    one-hot vector.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个新的工具函数`one_hot`，将数字值编码为独热向量。
- en: Finally, use a python list comprehension, to iterate over the values in the
    labels collection and turn them into one-hot encoded vectors.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用Python列表推导式遍历标签集合中的值，并将它们转换为独热编码向量。
- en: 'We need to execute one more step to prepare the dataset for training. In order
    to be able to verify that the model did indeed get optimized correctly, we want
    to create a hold-out set, against which we will run a test:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行一步操作来准备数据集进行训练。为了能够验证模型是否已正确优化，我们希望创建一个保留集，并在该集上进行测试：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the `train_test_split` method, create a small hold-out set containing
    20% of training samples. Use the `stratify` keyword and provide the labels to
    balance the split.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`train_test_split`方法，创建一个包含20%训练样本的小型保留集。使用`stratify`关键字并提供标签，以平衡拆分。
- en: 'Once we have the data prepared, we can focus on training the model. First,
    we''ll need to set up a `loss` function, `learner`, and `trainer` as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了数据，就可以专注于训练模型。首先，我们需要设置一个`loss`函数、`learner`和`trainer`，如下所示：
- en: '[PRE12]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Import the cross_entropy_with_softmax function from the `cntk.losses` module.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`cntk.losses`模块导入cross_entropy_with_softmax函数。
- en: Next, import the classificatin_error function from the `cnkt.metrics` module.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从`cnkt.metrics`模块导入classification_error函数。
- en: Then, import the `sgd` learner from the `cntk.learners` module.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从`cntk.learners`模块导入`sgd`学习器。
- en: Create a new `input_variable` with shape 3 to store the labels
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`input_variable`，形状为3，用于存储标签。
- en: Next, create a new instance of the cross_entropy_with_softmax loss and provide
    it the model variable `z` and the `label` variable.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个新的cross_entropy_with_softmax损失实例，并为其提供模型变量`z`和`label`变量。
- en: Then, create a new metric using the classification_error function and provide
    it the network and `label` variable.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用classification_error函数创建一个新的指标，并为其提供网络和`label`变量。
- en: Now, initialize the `sgd` learner with the parameters of the network and set
    its learning rate to 0.001.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用网络的参数初始化`sgd`学习器，并将其学习率设置为0.001。
- en: Finally, initialize the `Trainer` with the network, `loss`, `metric` , and `learner`.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用网络、`loss`、`metric`和`learner`初始化`Trainer`。
- en: 'Normally, we could just use the `train` method on the `loss` function to optimize
    the parameters in our model. This time, however, we want to have control over
    the training process so we can inject logic to track metrics in the Azure Machine
    Learning workspace, as demonstrated in the following code snippet:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以直接在`loss`函数上使用`train`方法来优化模型中的参数。然而，这一次，我们希望对训练过程进行控制，以便能够注入逻辑以跟踪Azure机器学习工作区中的指标，如以下代码片段所示：
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Follow the given steps:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: To start a new run, invoke the `start_logging` method on the experiment. This
    will create a new `run`. Within the scope of the run, we can execute the training
    logic.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始一个新的训练，调用实验的`start_logging`方法。这将创建一个新的`run`。在该运行的范围内，我们可以执行训练逻辑。
- en: Create a new for-loop to train for 10 epochs.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的for循环进行10个训练周期。
- en: Within the for-loop, call the `train_minibatch` method on the `trainer` to train
    the model. Provide it a mapping between the input variables and the data to train
    with.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在for循环内，调用`trainer`上的`train_minibatch`方法进行模型训练。为其提供输入变量与用于训练的数据之间的映射。
- en: After this, log the `average_loss` metric for the run using the `previous_minibatch_loss_average`
    value from the trainer object.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此之后，使用来自训练器对象的`previous_minibatch_loss_average`值记录`average_loss`指标。
- en: In addition to the average loss, log the average metric in the run using the
    `previous_minibatch_evaluation_average` property on the trainer object.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了平均损失外，还使用训练器对象的`previous_minibatch_evaluation_average`属性在运行中记录平均指标。
- en: Once we have trained the model, we can execute a test against the test set using
    the `test_minibatch` method. This method returns the output of the `metric` function
    that we created earlier. We will log this to the machine learning workspace as
    well.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练完模型，就可以使用`test_minibatch`方法在测试集上执行测试。该方法返回我们之前创建的`metric`函数的输出。我们也会将其记录到机器学习工作区中。
- en: A run allows us to keep track of data related to a single training session for
    a model. We can log metrics using the `log` method on the `run` object. This method
    accepts the name of the metric and a value for the metric. You can use this method
    to record things such as the output of the `loss` function to monitor how your
    model is converging to an optimal set of parameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 运行允许我们跟踪与模型单次训练会话相关的数据。我们可以使用 `run` 对象上的 `log` 方法记录度量数据。该方法接受度量的名称和度量的值。你可以使用此方法记录诸如
    `loss` 函数的输出，以监控模型如何收敛到最佳参数集。
- en: Other things can also be logged, such as the number of epochs used to train
    the model, the random seed used in the program, and other useful settings that
    you may need in order to reproduce the experiment at a later time.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以记录其他内容，例如用于训练模型的 epoch 数量、程序中使用的随机种子以及其他有用的设置，以便日后复现实验。
- en: Metrics recorded during a run automatically show up on the portal when you navigate
    to the experiment in the machine learning workspace under the experiments tab
    as is shown in the image below.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作区的实验标签下，导航到实验时，运行期间记录的度量数据会自动显示在门户上，如下图所示。
- en: '![](img/2cbb7274-466a-4831-943b-ab5c0d1ad1e0.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cbb7274-466a-4831-943b-ab5c0d1ad1e0.png)'
- en: Navigating to the experiment in the machine learning workspace under the experiments
    tab
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作区的实验标签下导航到实验
- en: 'Aside from the `log` method, there''s an `upload_file` method to upload files
    generating during training, as demonstrated in the following code snippet. You
    can use this method to store model files that you''ve saved after training is
    completed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `log` 方法，还有一个 `upload_file` 方法用于上传训练过程中生成的文件，示例如下代码片段。你可以使用这个方法存储训练完成后保存的模型文件：
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `upload_file` method needs a name for the file, as it can be found in the
    workspace and a local path where the source file can be found. Please be aware
    of the location of the file. Due to a limitation in the Azure Machine Learning
    workspace, it will only pick up files from the outputs folder. This limitation
    will likely be lifted in the future.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '`upload_file` 方法需要文件的名称（在工作区中可以找到）和文件的本地路径。请注意文件的位置。由于 Azure 机器学习工作区的限制，它只会从输出文件夹中提取文件。这个限制未来可能会被解除。'
- en: Make sure you execute the `upload_file` method within the scope of the run,
    so that the AzureML library links the model to your experiment run as to make
    it traceable.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在运行的作用域内执行 `upload_file` 方法，这样 AzureML 库就会将模型链接到你的实验运行，从而使其可追溯。
- en: 'After you''ve uploaded the file to the workspace, you can find it in the portal
    under the outputs section of a run. To get to the run details, open up the machine
    learning workspace in Azure Portal, navigate to the experiment, and then select
    the run you want to see the details for as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在将文件上传到工作区后，你可以在门户中通过运行的输出部分找到它。要查看运行详情，请打开 Azure Portal 中的机器学习工作区，导航到实验，然后选择你想查看详情的运行，如下所示：
- en: '![](img/663a6299-7df9-4aac-b503-0005bb600bc0.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/663a6299-7df9-4aac-b503-0005bb600bc0.png)'
- en: Selecting the run
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 选择运行
- en: 'Finally, when you''re done with the run and want to publish the model, you
    can register it in the model registry as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当你完成运行并希望发布模型时，可以按照以下步骤将其注册到模型注册表：
- en: '[PRE15]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `register_model` method stores the model in the model registry so you can
    deploy it to production. When the model was previously stored in the registry,
    it will automatically be stored as a new version. Now you can always go back to
    a previous version should you need to, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`register_model` 方法将模型存储在模型注册表中，以便你可以将其部署到生产环境。当模型先前存储在注册表中时，它将自动作为新版本存储。现在你可以随时回到之前的版本，如下所示：'
- en: '![](img/1747e79d-4e26-473d-abbf-941254425c2f.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1747e79d-4e26-473d-abbf-941254425c2f.png)'
- en: Model stored as a new version
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为新版本存储的模型
- en: You can find the model in the model registry in your workspace by going to the
    Machine Learning workspace on Azure Portal and clicking on the Models item in
    the bar in the navigation menu of the workspace.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过前往 Azure Portal 上的机器学习工作区，并在工作区导航菜单中的模型项下找到模型注册表。
- en: Models are automatically related to experiment runs, so you can always find
    the settings that you used to train the model. This is important, as it increases
    the chance that you can reproduce the results, should you need to.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 模型会自动与实验运行相关联，因此你总能找到用于训练模型的设置。这一点很重要，因为它增加了你能够复现结果的可能性，如果你需要这样做的话。
- en: 'We''ve limited ourselves to running experiments locally. You can use Azure
    Machine Learning to run experiments on dedicated hardware, should you want to.
    You can read more about this on the Azure Machine Learning documentation website
    at: [https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实验限制在本地运行。如果你愿意，可以使用Azure Machine Learning在专用硬件上运行实验。你可以在Azure Machine Learning文档网站上阅读更多相关信息：[https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets)。
- en: Once you have completed a run for an experiment you can deploy the trained model
    to production. In the next section we'll explore how to do this.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成实验运行，你就可以将训练好的模型部署到生产环境。在下一部分，我们将探讨如何执行此操作。
- en: Deploying your model to production
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境
- en: The final interesting piece of Azure Machine Learning is the deployment tooling
    that is included with it. The deployment tooling allows you to take a model from
    the model registry and deploy it to production.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning的一个有趣功能是其附带的部署工具。该部署工具允许你从模型注册表中提取模型，并将其部署到生产环境中。
- en: 'Before you can deploy a model to production, you need to have an image that
    includes the model and a scoring script. The image is a Docker image that includes
    a web server, which will invoke the scoring script when a request is made against
    it. The scoring script accepts input in the form of a JSON payload, and uses it
    to make a prediction using the model. The scoring script for our iris classification
    model looks like this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在将模型部署到生产环境之前，你需要有一个包含模型和评分脚本的镜像。该镜像是一个包含Web服务器的Docker镜像，当收到请求时，它会调用评分脚本。评分脚本接受JSON格式的输入，并利用它通过模型进行预测。我们针对鸢尾花分类模型的评分脚本如下所示：
- en: '[PRE16]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Follow the given steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定步骤操作：
- en: First, import the components needed to build the script.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入构建脚本所需的组件。
- en: Next, define a global model variable that will contain the loaded model.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，定义一个全局模型变量，用于存储加载的模型。
- en: After that, define the init function to initialize the model in the script.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，定义init函数来初始化脚本中的模型。
- en: Within the init function, retrieve the path for the model using the `Model.get_model_path`
    function. This automatically locates the model file in the Docker image.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在init函数中，使用`Model.get_model_path`函数检索模型的路径。这将自动定位Docker镜像中的模型文件。
- en: Next, load the model by initializing a new instance of the `onnxruntime.InferenceSession`
    class.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过初始化`onnxruntime.InferenceSession`类的新实例来加载模型。
- en: Define another function, `run` that accepts a single parameter `raw_data`.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义另一个函数`run`，该函数接受一个参数`raw_data`。
- en: Within the `run` function, convert the contents of `raw_data` variable from
    JSON to a Python array.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`run`函数中，将`raw_data`变量的内容从JSON转换为Python数组。
- en: Next, convert the `data` array into a Numpy array so we can use it to make a
    prediction.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，将`data`数组转换为Numpy数组，这样我们就可以用它来进行预测。
- en: After that, use the `run` method on the loaded model and feed it the input features.
    Include a dictionary that tells the ONNX runtime how to map the input data to
    the input variable of the model.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用加载的模型的`run`方法，并将输入特征提供给它。包括一个字典，告诉ONNX运行时如何将输入数据映射到模型的输入变量。
- en: The model returns an array of outputs with 1 element for the output of the model.
    This output contains one row of data. Select the first element from the output
    array and the first row from the selected output variable and store it in the
    `prediction` variable.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型返回一个包含1个元素的输出数组，用于模型的输出。该输出包含一行数据。从输出数组中选择第一个元素，再从选中的输出变量中选择第一行，并将其存储在`prediction`变量中。
- en: Finally, return the predicted output as a JSON object.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，返回预测结果作为JSON对象。
- en: Azure Machine Learning service will automatically include any model files that
    you registered for a particular model when you create a container image. So, `get_model_path`
    will also work inside deployed images and resolve to a directory in the container
    that hosts the model and scoring script.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning服务将自动包含你为特定模型注册的任何模型文件，当你创建容器镜像时。因此，`get_model_path`也可以在已部署的镜像中使用，并解析为容器中托管模型和评分脚本的目录。
- en: 'Now that we have a scoring script, let''s create an image and deploy the image
    as a web service in the cloud. To deploy a web service, you can explicitly create
    an image. Or, you can let Azure Machine Learning service create one based on the
    configuration you provided, as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了评分脚本，接下来让我们创建一个镜像，并将该镜像部署为云中的Web服务。要部署Web服务，你可以明确地创建一个镜像，或者你可以让Azure机器学习服务根据你提供的配置自动创建一个，方法如下：
- en: '[PRE17]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Follow the given steps:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, import the ContainerImage class from the `azureml.core.image` module.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`azureml.core.image`模块导入ContainerImage类。
- en: Next, create a new image configuration using the `ContainerImage.image_configuration`
    method. Provide it with the score.py as the `execution_script` argument, the python
    `runtime` and finally provide conda_env.yml as the `conda_file` for the image.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`ContainerImage.image_configuration`方法创建一个新的镜像配置。为其提供score.py作为`execution_script`参数，Python
    `runtime`，并最终提供conda_env.yml作为镜像的`conda_file`。
- en: 'We configure the container image to use Python as the runtime. We''re also
    configuring a special environment file for Anaconda so that we can configure custom
    modules like CNTK as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将容器镜像配置为使用Python作为运行时。我们还配置了一个特殊的Anaconda环境文件，以便可以像以下这样配置CNTK等自定义模块：
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Follow the given steps:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, give the environment a name. This optional, but can be useful when you
    create an environment from this file locally for testing.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为环境命名。这个步骤是可选的，但在你从此文件本地创建环境进行测试时会很有用。
- en: Next, Provide the python version 3.6.2 for your scoring script.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为你的评分脚本提供Python版本3.6.2。
- en: Finally, add a pip dependency to the list with a sublist containing `azureml-default`
    and `onnxruntime`.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将一个包含`azureml-default`和`onnxruntime`的子列表添加到pip依赖列表中。
- en: The `azureml-default` package contains everything you need to work with experiments
    and models in the docker container image. It includes standard packages like Numpy
    and Pandas as well for easier installation. The `onnxruntime` package is required
    so we can load the model inside the scoring script that we're using.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`azureml-default`包包含了在docker容器镜像中处理实验和模型所需的所有内容。它还包括像Numpy和Pandas这样的标准包，便于安装。`onnxruntime`包是必须的，因为我们需要在使用的评分脚本中加载模型。'
- en: 'One more step is needed to deploy the trained model as a web service. We''ll
    need to set up a web service configuration and deploy the model as a service.
    Machine Learning service supports deploying to virtual machines, Kubernetes clusters,
    and Azure Container Instances, which are basic Docker containers running in the
    cloud. This is how to deploy the model to an Azure Container Instance:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 部署已训练的模型作为Web服务还需要一步操作。我们需要设置Web服务配置并将模型作为服务部署。机器学习服务支持部署到虚拟机、Kubernetes集群和Azure容器实例，这些都是在云中运行的基本Docker容器。以下是如何将模型部署到Azure容器实例：
- en: '[PRE19]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Follow the given steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, Import the AciWebservice and Webservice classes from the azureml.core.webservice
    module.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`azureml.core.webservice`模块导入AciWebservice和Webservice类。
- en: Then, create a new `AciWebservice` configuration using the deploy_configuration
    method on the AziWebservice class. Provide it with a set of resource limits for
    the software. One CPU and 1GB of memory.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用AziWebservice类上的`deploy_configuration`方法创建一个新的`AciWebservice`配置。为其提供一组资源限制，包括1个CPU和1GB内存。
- en: Once you have a configuration for the web service, deploy the model the registered
    model to production by calling `deploy_from_model` with the workspace to deploy
    from, a service name and the models that you want to deploy. Provide the image
    configuration you created earlier.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你为Web服务配置完成后，调用`deploy_from_model`，使用要部署的工作区、服务名称以及要部署的模型，将已注册的模型部署到生产环境。提供你之前创建的镜像配置。
- en: Once the container image is created, it will get deployed as a container instance
    on Azure. This will create a new resource in the resource group for your machine
    learning workspace.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦容器镜像创建完成，它将作为容器实例部署到Azure。这将为你的机器学习工作区在资源组中创建一个新资源。
- en: 'Once the new service is started, you will see a new deployment on Azure Portal
    in your machine learning workspace, as seen in the following screenshot:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦新服务启动，你将在Azure门户的机器学习工作区中看到新的部署，如下图所示：
- en: '![](img/8c048b5c-208c-4e6a-917c-b4043faf8d70.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c048b5c-208c-4e6a-917c-b4043faf8d70.png)'
- en: New deployment on Azure Portal in your machine learning workspace
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的机器学习工作区的Azure门户上进行新部署
- en: The deployment includes a scoring URL that you can invoke from your application
    to use the model. Because you're using REST to invoke the model, you're isolated
    from the fact that it runs CNTK underneath the covers. You also have something
    that can be used from any programming language you can possibly think of, as long
    as it can execute HTTP requests.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 部署包括一个评分 URL，您可以从应用程序中调用该 URL 来使用模型。由于您使用的是 REST 来调用模型，因此您与它在后台运行 CNTK 的事实是隔离的。您还可以使用任何您能想到的编程语言，只要它能够执行
    HTTP 请求。
- en: 'For example, in Python, we can use the `requests` package as a basic REST client
    to make predictions using the service you just created. Let''s start by installing
    the `requests` module first, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 Python 中，我们可以使用 `requests` 包作为基本的 REST 客户端，通过您刚创建的服务进行预测。首先，让我们安装 `requests`
    模块，如下所示：
- en: '[PRE20]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With the `requests` package installed, we can write a small script to execute
    a request against the deployed service as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了 `requests` 包后，我们可以编写一个小脚本，通过以下方式对已部署的服务执行请求：
- en: '[PRE21]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Follow the given steps:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the requests and json package.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入 requests 和 json 包。
- en: Next, create a new variable for the service_url and fill it with the URL for
    the webservice.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为 service_url 创建一个新变量，并用 Web 服务的 URL 填充它。
- en: Then, create another variable, to store the data you want to make a prediction
    for.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建另一个变量来存储您想要进行预测的数据。
- en: After that, use the requests.post function to post the data to the deployed
    service and store the response.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，使用 requests.post 函数将数据发布到已部署的服务并存储响应。
- en: Finally, read the JSON data returned in the response to obtain the predicted
    values.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，读取响应中返回的 JSON 数据以获取预测值。
- en: 'The service_url can be obtained by performing the following steps:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下步骤获取 service_url：
- en: First, navigate to the resource group that contains the machine learning workspace.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导航到包含机器学习工作区的资源组。
- en: Then, select the workspace and choose the Deployments section on the left of
    the details panel.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择工作区并在详细信息面板的左侧选择部署部分。
- en: Select the deployment you want to view the details of and copy the URL from
    the details page.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您想查看详细信息的部署，并从详细信息页面复制 URL。
- en: '![](img/98ea4d64-d656-4c2d-a74e-7a76b9ace6fc.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98ea4d64-d656-4c2d-a74e-7a76b9ace6fc.png)'
- en: Selecting the deployment
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 选择部署
- en: 'When you run the script you just created, you''ll receive a response with the
    predicted classes for the input sample. The output will look similar to this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行刚创建的脚本时，您将收到包含输入样本预测类别的响应。输出将类似于以下内容：
- en: '[PRE22]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Summary
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've looked at what it takes to bring deep learning and machine
    learning models to production. We've explored some of the basic principles that
    will help you to be successful with deep learning in a continuous delivery environment.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了将深度学习和机器学习模型投入生产所需的条件。我们探讨了一些基本原理，这些原理将帮助您在持续交付环境中成功实施深度学习。
- en: We've taken a look at exporting models to ONNX to make it easier to deploy your
    trained models to production and keep them running for years, thanks to the portable
    nature of the ONNX format. We then explored how you can use the CNTK API in other
    languages, such as C#, to make predictions.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经查看了将模型导出到 ONNX 的方法，借助 ONNX 格式的可移植性，使得将训练好的模型部署到生产环境并保持多年运行变得更加容易。接着，我们探讨了如何使用
    CNTK API 在其他语言中，如 C#，来进行预测。
- en: Finally, we've looked at using Azure Machine Learning service to level-up your
    DevOps experience with experiment management, model management, and deployment
    tools. Although you don't need a tool like this to get started, it really helps
    to have something like Azure Machine Learning service in your arsenal when you're
    planning on running a bigger project on production.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了如何使用 Azure 机器学习服务，通过实验管理、模型管理和部署工具提升您的 DevOps 体验。尽管您不需要像这样的工具就能入门，但在您计划将更大的项目投入生产时，拥有像
    Azure 机器学习服务这样的工具，确实能够为您提供很大帮助。
- en: With this chapter, we've reached the end of this book. In the first chapter,
    we started exploring CNTK. We then looked at how to build models, feed them with
    data, and measure their performance. With the basics covered, we explored two
    interesting use cases looking at images and time series data. Finally, we ended
    with taking models to production. You should now have enough information get started
    with building your own models with CNTK!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 本章标志着本书的结束。在第一章中，我们开始探索了CNTK。接着，我们学习了如何构建模型、为模型提供数据并评估其性能。基础内容讲解完毕后，我们研究了两个有趣的应用案例，分别涉及图像和时间序列数据。最后，我们介绍了如何将模型投入生产。现在，你应该已经掌握了足够的信息，可以开始使用CNTK构建自己的模型了！
