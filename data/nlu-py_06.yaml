- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Exploring and Visualizing Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化数据
- en: Exploring and visualizing data are essential steps in the process of developing
    a **natural language understanding** (**NLU**) application. In this chapter, we
    will explore techniques for **data exploration**, such as visualizing word frequencies,
    and techniques for visualizing document similarity. We will also introduce several
    important visualization tools, such as Matplotlib, Seaborn, and WordCloud, that
    enable us to graphically represent data and identify patterns and relationships
    within our datasets. By combining these techniques, we can gain valuable perspectives
    into our data, make informed decisions about the next steps in our NLU processing,
    and ultimately, improve the accuracy and effectiveness of our analyses. Whether
    you’re a data scientist or a developer, data exploration and visualization are
    essential skills for extracting actionable insights from text data in preparation
    for further NLU processing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 探索和可视化数据是开发**自然语言理解**（**NLU**）应用过程中的关键步骤。在本章中，我们将探讨**数据探索**的技术，如可视化词频以及可视化文档相似性的技术。我们还将介绍几个重要的可视化工具，如Matplotlib、Seaborn和WordCloud，这些工具使我们能够以图形方式呈现数据并识别数据集内的模式和关系。通过结合这些技术，我们可以深入了解数据，为NLU处理的下一步做出明智的决策，从而提高分析的准确性和效果。无论您是数据科学家还是开发人员，数据探索和可视化都是从文本数据中提取可操作洞见的关键技能，为进一步的NLU处理做好准备。
- en: In this chapter, we will cover several topics related to the initial exploration
    of data, especially visual exploration, or **visualization**. We will start by
    reviewing a few reasons why getting a visual perspective of our data can be helpful.
    This will be followed by an introduction to a sample dataset of movie reviews
    that we will be using to illustrate our techniques. Then, we will look at techniques
    for data exploration, including summary statistics, visualizations of word frequencies
    in a dataset, and measuring document similarity. We will follow with a few general
    tips on developing visualizations and conclude with some ideas about using information
    from visualizations to make decisions about further processing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖与数据的初始探索相关的几个主题，特别是可视化探索或**可视化**。我们将首先回顾几个查看数据视角的原因，以便帮助。接下来，我们将介绍一个电影评论样本数据集，用来说明我们的技术。然后，我们将探讨数据探索的技术，包括总结统计信息、可视化数据集中的词频以及测量文档相似性。最后，我们将给出一些开发可视化的一般提示，并结论关于如何使用可视化信息来做出关于进一步处理的决策的一些想法。
- en: 'The topics we will cover are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要涵盖的主题如下：
- en: Why visualize?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要可视化？
- en: Data exploration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据探索
- en: General considerations for developing visualizations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发可视化的一般考虑因素
- en: Using information from visualization to make decisions about processing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可视化信息来做出关于处理的决策
- en: Why visualize?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要可视化？
- en: Visualizing data means displaying data in a graphical format such as a chart
    or graph. This is almost always a useful precursor to training a **natural language
    processing** (**NLP**) system to perform a specific task because it is typically
    very difficult to see patterns in large amounts of text data. It is often much
    easier to see overall patterns in data visually. These patterns might be very
    helpful in making decisions about the most applicable text-processing techniques.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据意味着以图表或图形等图形格式显示数据。这几乎总是训练**自然语言处理**（**NLP**）系统执行特定任务的有用前提，因为通常很难在大量文本数据中看到模式。通过视觉方式通常更容易看到数据的总体模式。这些模式可能对决定最适用的文本处理技术非常有帮助。
- en: Visualization can also be useful in understanding the results of NLP analysis
    and deciding what the next steps might be. Because looking at the results of NLP
    analysis is not an initial exploratory step, we will postpone this topic until
    [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226) and [*Chapter 14*](B19005_14.xhtml#_idTextAnchor248).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化在理解自然语言处理分析结果并决定下一步操作时也很有用。由于查看自然语言处理分析结果并非初始的探索步骤，我们将把这个话题推迟到[*第13章*](B19005_13.xhtml#_idTextAnchor226)和[*第14章*](B19005_14.xhtml#_idTextAnchor248)。
- en: In order to explore visualization, in this chapter, we will be working with
    a dataset of text documents. The text documents will illustrate a binary classification
    problem, which will be described in the next section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索可视化，在本章中，我们将使用一组文本文档数据集。文本文档将说明一个二元分类问题，该问题将在下一节中描述。
- en: Text document dataset – Sentence Polarity Dataset
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本文档数据集 – 句子极性数据集
- en: The Sentence Polarity Dataset is a commonly used dataset that consists of movie
    reviews originally from the **Internet Movie Database** (**IMDb**). The reviews
    have been categorized in terms of positive and negative reviews. The task that
    is most often used with these reviews is classifying the reviews into positive
    and negative. The data was collected by a team at Cornell University. More information
    about the dataset and the data itself can be found at [https://www.cs.cornell.edu/people/pabo/movie-review-data/](https://www.cs.cornell.edu/people/pabo/movie-review-data/).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 句子极性数据集是一个常用的数据集，包含最初来自**互联网电影数据库**（**IMDb**）的电影评论。这些评论已根据正面和负面进行分类。最常见的任务是将这些评论分类为正面和负面。数据是由康奈尔大学的一个团队收集的。有关该数据集及其数据的更多信息，请访问[https://www.cs.cornell.edu/people/pabo/movie-review-data/](https://www.cs.cornell.edu/people/pabo/movie-review-data/)。
- en: '**Natural Language Toolkit** (**NLTK**) comes with this dataset as one of its
    built-in corpora. There are 1,000 positive reviews and 1,000 negative reviews.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言工具包**（**NLTK**）将这个数据集作为其内置语料库之一。该数据集包含1,000条正面评论和1,000条负面评论。'
- en: 'Here’s an example of a positive review from this dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个数据集中的一个正面评论示例：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s an example of a negative review:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个负面评论的示例：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can look at the thousands of examples in our dataset, but it will be hard
    to see any large-scale patterns by looking at individual examples; there is just
    too much data for us to separate the big picture from the details. The next sections
    will cover tools that we can use to identify these large-scale patterns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看数据集中成千上万的示例，但仅通过查看单个示例很难看出任何大规模的模式；数据量太大，我们无法从细节中分辨出整体图景。接下来的章节将介绍一些可以帮助我们识别这些大规模模式的工具。
- en: Data exploration
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: '**Data exploration**, which is sometimes also called **exploratory data analysis**
    (**EDA**), is the process of taking a first look at your data to see what kinds
    of patterns there are to get an overall perspective on the full dataset. These
    patterns and overall perspective will help us identify the most appropriate processing
    approaches. Because some NLU techniques are very computationally intensive, we
    want to ensure that we don’t waste a lot of time applying a technique that is
    inappropriate for a particular dataset. Data exploration can help us narrow down
    the options for techniques at the very beginning of our project. Visualization
    is a great help in data exploration because it is a quick way to get the big picture
    of patterns in the data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据探索**，有时也叫做**探索性数据分析**（**EDA**），是对数据进行初步查看的过程，目的是找出数据中有哪些模式，从而对整个数据集有一个全面的认识。这些模式和总体视角将帮助我们识别最合适的处理方法。由于一些自然语言理解（**NLU**）技术计算开销很大，我们希望确保不会浪费大量时间应用不适合特定数据集的技术。数据探索有助于我们在项目初期就缩小技术选择的范围。可视化在数据探索中非常有帮助，因为它是一种快速了解数据模式全貌的方式。'
- en: The most basic kind of information about a corpus that we would want to explore
    includes information such as the number of words, the number of distinct words,
    the average length of documents, and the number of documents in each category.
    We can start by looking at the frequency distributions of words. We will cover
    several different ways of visualizing word frequencies in our datasets and then
    look at some measurements of document similarity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要探索的语料库中最基本的信息包括词数、不同词汇的数量、文档的平均长度以及每个类别中的文档数量等信息。我们可以从查看单词的频率分布开始。我们将介绍几种不同的可视化单词频率的方法，然后查看一些文档相似度的度量。
- en: Frequency distributions
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 频率分布
- en: 'Frequency distributions show how many items of a specific type occur in some
    context. In our case, the context is a dataset, and we will be looking at the
    frequencies of *words* and then *ngrams* (sequences of words) that occur in our
    dataset and subsets of the dataset. We’ll start by defining word frequency distributions
    and doing some preprocessing. We’ll then visualize the word frequencies with Matplotlib
    tools and WordCloud, and then apply the same techniques to ngrams. Finally, we’ll
    cover some techniques for visualizing document similarity: bag of words (**BoW**)
    and k-means clustering.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 频率分布显示特定类型的项目在某个上下文中出现的频率。在我们的案例中，上下文是数据集，我们将查看数据集中出现的*单词*的频率，然后是*n-gram*（单词序列）的频率。我们将从定义单词频率分布并进行一些预处理开始。然后，我们将使用Matplotlib工具和WordCloud可视化单词频率，并将相同的技术应用于n-gram。最后，我们将介绍一些可视化文档相似度的技术：词袋模型（**BoW**）和k均值聚类。
- en: Word frequency distributions
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词频率分布
- en: 'The words and their frequencies that occur in a corpus are often very informative.
    It’s a good idea to take a look at this information before getting too far into
    an NLP project. Let’s take a look at computing this information using NLTK. We
    start by importing NLTK and the movie reviews dataset, as shown in *Figure 6**.1*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中出现的单词及其频率通常包含很多有用的信息。在深入进行自然语言处理项目之前，查看这些信息是一个好主意。让我们通过使用 NLTK 来计算这些信息。我们首先导入
    NLTK 和电影评论数据集，如*图 6.1*所示：
- en: "![Figure 6.1 – Importing \uFEFFNLTK and the movie reviews dataset](img/B19005_06_01.jpg)"
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 导入 NLTK 和电影评论数据集](img/B19005_06_01.jpg)'
- en: Figure 6.1 – Importing NLTK and the movie reviews dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 导入 NLTK 和电影评论数据集
- en: '*Figure 6**.2* shows code that we can use to collect the words in the `movie_reviews`
    dataset into a list and take a look at some of the words in the following steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.2* 显示了我们可以用来将 `movie_reviews` 数据集中的单词收集到列表中的代码，并在接下来的步骤中查看一些单词：'
- en: To collect the words in the corpus into a Python list, NLTK provides a helpful
    function, `words()`, which we use here.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将语料库中的单词收集到一个 Python 列表中，NLTK 提供了一个有用的函数`words()`，我们在这里使用它。
- en: Find out the total number of words in the dataset by using the normal Python
    `len()` function for finding the size of a list, with the list of words as a parameter.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 Python 的 `len()` 函数来找出数据集中的总单词数，传入的参数是单词列表。
- en: 'Print the list of words:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印单词列表：
- en: "![Figure 6.2\uFEFF –  Making a list of the words in the corpus, counting them,\
    \ and displaying the first few](img/B19005_06_02.jpg)"
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 制作语料库中的单词列表，统计它们，并显示前几个单词](img/B19005_06_02.jpg)'
- en: Figure 6.2 – Making a list of the words in the corpus, counting them, and displaying
    the first few
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 制作语料库中的单词列表，统计它们，并显示前几个单词
- en: 'The result of counting the words and printing the first few words is shown
    here:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 统计单词并打印出前几个单词的结果如下所示：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We won’t see more words than the first few (`'plot'`, `':'`, `'two'`, and so
    on) because there are too many to show. By printing the length of the list, we
    can see that there are 1,583,820 individual words in the dataset, which is certainly
    too many to list. What we can also notice is that this list of words includes
    punctuation, specifically `:`. Since punctuation is so common, looking at it is
    unlikely to yield any insights into differences between documents. In addition,
    including punctuation will also make it harder to see the words that actually
    distinguish categories of documents. So, let’s remove punctuation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会看到比前几个单词更多的单词（`'plot'`、`':'`、`'two'`等），因为有太多单词无法全部显示。通过打印列表的长度，我们可以看到数据集中有
    1,583,820 个单词，显然这个数量太多，无法一一列出。我们还可以注意到，这个单词列表中包含标点符号，特别是 `:`。由于标点符号非常常见，查看它们不太可能为我们提供关于文档之间差异的洞察。此外，包含标点符号也会使得我们难以看到实际上区分文档类别的单词。因此，我们需要去除标点符号。
- en: '*Figure 6**.3* shows a way to remove punctuation with the following steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.3* 显示了通过以下步骤去除标点符号的方法：'
- en: Initialize an empty list where we will store the new list of words after removing
    punctuation (`words_no_punct`).
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空列表，用于存储去除标点符号后的新单词列表（`words_no_punct`）。
- en: Iterate through the list of words (`corpus_words`).
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历单词列表（`corpus_words`）。
- en: Keep only the alphanumeric words. This code uses the Python string function,
    `string.isalnum()`, at line 4 to detect alphanumeric words, which are added to
    the `words_no_punct` list that we previously initialized.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只保留字母数字单词。此代码在第4行使用了 Python 字符串函数 `string.isalnum()` 来检测字母数字单词，这些单词会被添加到我们之前初始化的
    `words_no_punct` 列表中。
- en: Once we remove the punctuation, we might be interested in the frequencies of
    occurrence of the non-punctuation words. Which are the most common words in our
    corpus? NLTK provides a useful `FreqDist()` function (frequency distribution)
    that computes word frequencies. This function is applied at line 6 in *Figure
    6**.3*.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们去除了标点符号，可能会对非标点符号单词的出现频率感兴趣。我们语料库中最常见的单词是什么？NLTK 提供了一个有用的`FreqDist()`函数（频率分布），可以计算单词的出现频率。这个函数在*图
    6.3*的第6行中应用。
- en: We can then see the most common words in the frequency distribution using the
    NLTK `most_common()` method, which takes as a parameter how many words we want
    to see, shown in line 8.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 NLTK 的 `most_common()` 方法查看频率分布中最常见的单词，该方法的参数是我们想查看的单词数量，显示在第8行。
- en: 'Finally, we print the 50 most common words:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出最常见的50个单词：
- en: "![Figure 6.3\uFEFF –  The most common 50 words in the movie review database\
    \ and their frequencies](img/B19005_06_03.jpg)"
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: "![图 6.3\uFEFF – 电影评论数据库中最常见的50个单词及其频率](img/B19005_06_03.jpg)"
- en: Figure 6.3 – The most common 50 words in the movie review database and their
    frequencies
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 电影评论数据库中最常见的50个单词及其频率
- en: In *Figure 6**.3*, we can easily see that `the` is the most common word, with
    76,529 occurrences in the dataset, which is not surprising. However, it’s harder
    to see the frequencies of the less common words. It’s not easy to see, for example,
    the tenth most common word, and how much more common it is than the eleventh most
    common word. This is where we bring in our visualization tools.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6**.3*中，我们可以很容易地看到`the`是最常见的单词，在数据集中出现了76,529次，这并不令人惊讶。然而，要看到不太常见单词的频率就更难了。举个例子，看到第十个最常见的单词，以及它与第十一个最常见单词相比有多少差距，这并不容易。这就是我们引入可视化工具的原因。
- en: 'The frequencies that we computed in *Figure 6**.3* can be plotted with the
    `plot()` function for frequency distributions. This function takes two parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 6**.3*中计算的频率可以通过`plot()`函数绘制频率分布图。该函数有两个参数：
- en: The first parameter is the number of words we want to see – in this case, it
    is `50`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个参数是我们希望查看的单词数量——在这个例子中是`50`。
- en: The `cumulative` parameter determines whether we just want to see the frequency
    of each of our 50 words (`cumulative=False`). If we want to see the cumulative
    frequencies for all the words, we would set this parameter to `True`.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`cumulative`参数决定了我们是只想查看50个单词的频率（`cumulative=False`），还是查看所有单词的累积频率。如果我们想查看所有单词的累积频率，我们需要将此参数设置为`True`。'
- en: 'To plot the frequency distribution, we can simply add a call to `plot()` as
    follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要绘制频率分布，我们可以简单地添加`plot()`调用，代码如下：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The result is shown in *Figure 6**.4*, a plot of the frequencies of each word,
    with the most frequent words first:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如*图 6**.4*所示，是每个单词频率的图示，最常见的单词排在最前面：
- en: '![Figure 6.4 – Frequency distribution of words in the movie review corpus](img/B19005_06_04.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 电影评论语料库中单词的频率分布](img/B19005_06_04.jpg)'
- en: Figure 6.4 – Frequency distribution of words in the movie review corpus
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 电影评论语料库中单词的频率分布
- en: Try `freq.plot()` with different numbers of words and with `cumulative = True`.
    What happens when you ask for more and more words? You will see that looking at
    rarer and rarer words (by increasing the value of the first argument to larger
    numbers) does not provide much insight.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用不同数量的单词和`cumulative = True`来调用`freq.plot()`。当你要求查看越来越多的单词时，会发生什么呢？你会发现，查看越来越稀有的单词（通过增加第一个参数的数值）并不会提供太多见解。
- en: While the graph in *Figure 6**.4* gives us a much clearer idea of the frequencies
    of the different words in the dataset, it also tells us something more important
    about the frequent words. Most of them, such as `the`, `a`, and `and`, are not
    very informative. Just like punctuation, they are not likely to help us distinguish
    the categories (positive and negative reviews) that we’re interested in because
    they occur very frequently in most documents. These kinds of words are called
    **stopwords** and are usually removed from natural language data before NLP precisely
    because they don’t add much information.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*图 6**.4*中的图表给我们提供了关于数据集中不同单词频率的更清晰概念，但它还告诉我们关于频繁单词更重要的事情。大多数这些单词，比如`the`、`a`和`and`，并不含有太多信息。就像标点符号一样，它们很可能不会帮助我们区分我们感兴趣的类别（正面和负面评论），因为它们在大多数文档中频繁出现。这些类型的单词被称为**停用词**，通常会在自然语言数据中被去除，尤其是在自然语言处理（NLP）中，因为它们并没有提供太多信息。
- en: 'NLTK provides a list of common stopwords for different languages. Let’s take
    a look at some of the English stopwords:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK提供了不同语言的常见停用词列表。让我们看一下其中一些英语停用词：
- en: '![Figure 6.5 – Examples of 50 English stopwords](img/B19005_06_05.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 50个英语停用词的示例](img/B19005_06_05.jpg)'
- en: Figure 6.5 – Examples of 50 English stopwords
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 50个英语停用词的示例
- en: '*Figure 6**.5* shows the code we can use to examine English stopwords, using
    the following steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.5*显示了我们可以使用的代码，通过以下步骤检查英语停用词：'
- en: Import `nltk` and the `stopwords` package.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`nltk`和`stopwords`包。
- en: Make the set of stopwords into a Python list so that we can do list operations
    on it.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将停用词集合转换为Python列表，以便我们对其进行列表操作。
- en: Find out how long the list is with the Python `len()` function and print the
    value (179 items).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Python的`len()`函数查看列表的长度并打印值（179项）。
- en: Print the first `50` stopwords.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印前`50`个停用词。
- en: 'Stopwords are available for other languages in NLTK as well. We can see the
    list by running the code in *Figure 6**.6*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 中也为其他语言提供了停用词。我们可以通过运行*图 6.6*中的代码来查看这些停用词列表：
- en: '![Figure 6.6 – NLTK languages with stopwords](img/B19005_06_06.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – NLTK 含停用词的语言](img/B19005_06_06.jpg)'
- en: Figure 6.6 – NLTK languages with stopwords
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – NLTK 含停用词的语言
- en: 'The code has the following three steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 代码包含以下三个步骤：
- en: Collect the names of the languages with NLTK stopwords. The stopwords are contained
    in files, one file per language, so getting the files is a way to get the names
    of the languages.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集包含 NLTK 停用词的语言名称。停用词存储在每个语言对应的文件中，因此获取这些文件就能获得语言名称。
- en: Print the length of the list of languages.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印语言列表的长度。
- en: Print the names of the languages.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印语言的名称。
- en: 'Once we have the list of stopwords, it is easy to remove them from a corpus.
    After removing the punctuation, as we saw in *Figure 6**.3*, we can just iterate
    through the list of words in the corpus, removing the words that are in the stopwords
    list, as shown here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了停用词的列表，就可以轻松地将它们从语料库中移除。在移除标点符号后，如我们在*图 6.3*中所见，我们只需遍历语料库中的单词列表，移除停用词列表中的单词，如下所示：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As an aside, it would clearly be more efficient to combine removing punctuation
    and stopwords with one iteration through the entire list of words, checking each
    word to see whether it is either punctuation or a stopword, and removing it if
    it is either one. We show these steps separately for clarity, but you may want
    to combine these two steps in your own projects. Just as we did in the case of
    removing punctuation, we can use the NLTK frequency distribution function to see
    which words other than stopwords are most common, and display them, as shown in
    *Figure 6**.7*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，将移除标点符号和停用词的操作合并为一次遍历整个单词列表会显得更加高效，在遍历过程中检查每个单词是否是标点符号或停用词，如果是，就将其移除。我们单独展示这些步骤是为了清晰起见，但在你自己的项目中，你可能想将这两个步骤合并。就像我们在移除标点符号时所做的那样，我们可以使用
    NLTK 的频率分布函数，查看除了停用词之外，哪些单词最为常见，并将其显示出来，如*图 6.7*所示：
- en: '![Figure 6.7 – Most common words, after removing punctuation and stopwords](img/B19005_06_07.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 移除标点符号和停用词后的最常见单词](img/B19005_06_07.jpg)'
- en: Figure 6.7 – Most common words, after removing punctuation and stopwords
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 移除标点符号和停用词后的最常见单词
- en: We can see in *Figure 6**.7* that removing stopwords gives us a much clearer
    picture of the words in the movie review corpus. The most common word is now `film`,
    and the third most common word is `movie`. This is what we would expect from a
    corpus of movie reviews. A similar review corpus (for example, a corpus of product
    reviews) would be expected to show a different distribution of frequent words.
    Even looking at the simple information provided by frequency distributions can
    sometimes be enough to do some NLP tasks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 6.7*中看到，移除停用词后，我们对电影评论语料库中的单词有了更清晰的认识。现在最常见的单词是 `film`，第三常见的单词是 `movie`。这正是我们从电影评论语料库中所期望的。类似的评论语料库（例如产品评论语料库）可能会显示出不同的频率分布。即便仅仅通过频率分布提供的简单信息，有时也足以完成一些
    NLP 任务。
- en: For example, in authorship studies, we’re interested in trying to attribute
    a document whose author is unknown to a known author. The word frequencies in
    the document whose author we don’t know can be compared to those in the documents
    with a known author. Another example would be domain classification, where we
    want to know whether the document is a movie review or a product review.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在作者身份研究中，我们希望将一篇作者未知的文档归属于一个已知的作者。我们可以将该文档中单词的频率与已知作者的文档中的单词频率进行比较。另一个例子是领域分类，我们想知道该文档是电影评论还是产品评论。
- en: Visualizing data with Matplotlib, Seaborn, and pandas
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Matplotlib、Seaborn 和 pandas 可视化数据
- en: While NLTK has some basic plotting functions, there are some other, more powerful
    Python plotting packages that are used very often for plotting all kinds of data,
    including NLP data. We’ll look at some of these in this section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 NLTK 提供了一些基本的绘图功能，但还有一些其他更强大的 Python 绘图库，它们被广泛用于绘制各种数据，包括自然语言处理（NLP）数据。在本节中，我们将探讨其中一些。
- en: Matplotlib ([https://matplotlib.org/](https://matplotlib.org/)) is very popular.
    Matplotlib can create a variety of visualizations, including animations and interactive
    visualizations. We’ll use it here to create another visualization of the data
    that we plotted in *Figure 6**.4*. Seaborn ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    is built on Matplotlib and is a higher-level interface for producing attractive
    visualizations. Both of these packages frequently make use of another Python data
    package, pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)), which
    is very useful for handling tabular data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib ([https://matplotlib.org/](https://matplotlib.org/)) 是一个非常流行的工具。Matplotlib
    可以创建多种可视化，包括动画和交互式可视化。我们将在这里使用它来创建另一个我们在 *图 6.4* 中绘制的数据的可视化。Seaborn ([https://seaborn.pydata.org/](https://seaborn.pydata.org/))
    基于 Matplotlib 构建，是一个更高层次的接口，用于生成美观的可视化。这两个包通常还会使用另一个 Python 数据包 pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/))，它非常适合处理表格数据。
- en: For our example, we’ll use the same data that we plotted in *Figure 6**.7*,
    but we’ll use Matplotlib, Seaborn, and pandas to create our visualization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用与 *图 6.7* 中相同的数据，但我们将使用 Matplotlib、Seaborn 和 pandas 来创建可视化。
- en: '*Figure 6**.8* shows the code for this example:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.8* 显示了此示例的代码：'
- en: '![Figure 6.8 – Python code for collecting the 25 most common words, after removing
    punctuation and stopwords](img/B19005_06_08.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 收集前 25 个常见词的 Python 代码，移除标点符号和停用词后](img/B19005_06_08.jpg)'
- en: Figure 6.8 – Python code for collecting the 25 most common words, after removing
    punctuation and stopwords
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 收集前 25 个常见词的 Python 代码，移除标点符号和停用词后
- en: '*Figure 6**.8* shows the preparatory code for plotting the data, with the following
    steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.8* 显示了绘制数据的准备代码，包括以下步骤：'
- en: Import the NLTK, pandas, Seaborn, and Matplotlib libraries.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 NLTK、pandas、Seaborn 和 Matplotlib 库。
- en: Set the frequency cutoff to `25` (this can be whatever number we’re interested
    in).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将频率截止设置为 `25`（这可以是我们感兴趣的任何数字）。
- en: Compute a frequency distribution of the corpus words without stopwords (line
    7).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算没有停用词的语料库单词的频率分布（第 7 行）。
- en: We start by importing NLTK, pandas, Seaborn, and Matplotlib. We set the frequency
    cutoff to `25`, so that we will plot only the 25 most common words, and we get
    the frequency distribution of our data at line 7.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入 NLTK、pandas、Seaborn 和 Matplotlib。我们将频率截止设置为 `25`，因此我们只会绘制前 25 个最常见的单词，并且在第
    7 行获得我们的数据的频率分布。
- en: 'The resulting plot is shown in *Figure 6**.9*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在 *图 6.9* 中：
- en: '![Figure 6.9 – Most common words, after removing punctuation and stopwords,
    displayed using Matplotlib, Seaborn, and pandas libraries](img/B19005_06_09.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 在移除标点符号和停用词后，使用 Matplotlib、Seaborn 和 pandas 库显示的最常见单词](img/B19005_06_09.jpg)'
- en: Figure 6.9 – Most common words, after removing punctuation and stopwords, displayed
    using Matplotlib, Seaborn, and pandas libraries
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 在移除标点符号和停用词后，使用 Matplotlib、Seaborn 和 pandas 库显示的最常见单词
- en: '*Figure 6**.7* and *Figure 6**.9* show the same data. For example, the most
    common word in each figure is `film`, followed by `one`, and then `movie`. Why
    would you pick one visualization tool over the other? One difference is that it
    seems to be easier to see the information for a specific word in *Figure 6**.9*,
    which is probably because of the bar graph format, where every word is associated
    with a specific bar. In contrast, in *Figure 6**.7*, it is a little harder to
    see the frequencies of individual words because this isn’t very clear from the
    line.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.7* 和 *图 6.9* 显示的是相同的数据。例如，每个图中最常见的单词是 `film`，接着是 `one`，然后是 `movie`。为什么要选择一种可视化工具而非另一种呢？其中一个区别是，在
    *图 6.9* 中，似乎更容易看到某个特定单词的信息，这可能是由于条形图格式，每个单词都有对应的条形图。而在 *图 6.7* 中，由于线条并不清晰，因此稍微难以看到每个单词的频率。'
- en: On the other hand, the overall frequency distribution is easier to see in *Figure
    6**.7* because of the continuous line. So, choosing one type of visualization
    over the other is really a matter of the kind of information that you want to
    emphasize. Of course, there is also no reason to limit yourself to one visualization
    if you want to see your data in different ways.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于连续的线条，*图 6.7* 中的整体频率分布更容易看到。因此，选择一种可视化方式而非另一种，实际上取决于你想要强调的信息类型。当然，如果你希望以不同的方式查看数据，完全没有理由只局限于一种可视化方式。
- en: As a side note, the pattern in both figures, where we have very high frequencies
    for the most common words, falling off rapidly for less common words, is very
    common in natural language data. This pattern illustrates a concept called **Zipf’s
    law** (for more information about this concept, see [https://en.wikipedia.org/wiki/Zipf%27s_law](https://en.wikipedia.org/wiki/Zipf%27s_law)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，在两个图中的模式，最常见的单词具有非常高的频率，较不常见的单词频率急剧下降，这种模式在自然语言数据中非常常见。这个模式说明了一个叫做**齐夫定律**的概念（关于这个概念的更多信息，见[https://en.wikipedia.org/wiki/Zipf%27s_law](https://en.wikipedia.org/wiki/Zipf%27s_law)）。
- en: Looking at another frequency visualization technique – word clouds
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看另一种频率可视化技术——词云
- en: A **word cloud** is another way of visualizing the relative frequencies of words.
    A word cloud displays the words in a dataset in different font sizes, with the
    more frequent words shown in larger fonts. Word clouds are a good way to make
    frequent words pop out visually.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**词云**是另一种可视化单词相对频率的方法。词云以不同的字体大小显示数据集中的单词，频率较高的单词以较大的字体显示。词云是一个很好的方法，可以让常见单词在视觉上更加突出。'
- en: 'The code in *Figure 6**.10* shows how to import the `WordCloud` package and
    create a word cloud from the movie review data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.10*中的代码展示了如何导入`WordCloud`包并从电影评论数据创建词云：'
- en: '![Figure 6.10 – Python code for most common words, shown as a word cloud](img/B19005_06_10.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 显示最常见单词的 Python 代码，作为词云呈现](img/B19005_06_10.jpg)'
- en: Figure 6.10 – Python code for most common words, shown as a word cloud
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 显示最常见单词的 Python 代码，作为词云呈现
- en: 'The code shows the following steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 代码展示了以下步骤：
- en: We import a new library, `WordCloud`, at line 2\. For this display, we will
    select only the most *common 200 words* (with a frequency cutoff of `200` at line
    6).
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第2行导入了一个新的库`WordCloud`。对于此显示，我们将仅选择最*常见的200个单词*（在第6行设置了频率阈值`200`）。
- en: We create a frequency distribution at line 7.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第7行创建了一个频率分布。
- en: The frequency distribution is converted to a pandas Series at line 8.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频率分布在第8行被转换为pandas Series。
- en: To reduce the number of very short words, we include only words that are longer
    than two letters at line 10.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少非常短单词的数量，我们在第10行只包括长度大于两个字母的单词。
- en: The code in line 11 generates the word cloud. The `colormap` parameter specifies
    one of the many Matplotlib color maps (color maps are documented at [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html)).
    You can experiment with different color schemes to find the ones that you prefer.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第11行的代码生成了词云。`colormap`参数指定了许多Matplotlib颜色映射之一（颜色映射的文档在[https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html)）。你可以尝试不同的配色方案，找到你喜欢的样式。
- en: Lines 12-14 format the plot area.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第12-14行格式化图表区域。
- en: Finally, the plot is displayed at line 15.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，图表在第15行显示。
- en: '*Figure 6**.11* shows the word cloud that results from the code in *Figure
    6**.10*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.11*展示了来自*图 6.10*中代码生成的词云：'
- en: '![Figure 6.11 – Word cloud for word frequencies in movie reviews](img/B19005_06_11New.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – 电影评论中单词频率的词云](img/B19005_06_11New.jpg)'
- en: Figure 6.11 – Word cloud for word frequencies in movie reviews
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – 电影评论中单词频率的词云
- en: As we saw in *Figure 6**.7* and *Figure 6**.9*, the most common words are `film`,
    `one`, and `movie`. However, the word cloud visualization makes the most common
    words stand out in a way that the graphs do not. On the other hand, the less frequent
    words are difficult to differentiate in the word cloud. For example, it’s hard
    to tell whether `good` is more frequent than `story` from the word cloud. This
    is another example showing that you should consider what information you want
    to get from a visualization before selecting one.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在*图 6.7*和*图 6.9*中看到的，最常见的单词是`film`、`one`和`movie`。然而，词云可视化使得最常见的单词以图表无法比拟的方式脱颖而出。另一方面，词云中频率较低的单词很难区分。例如，从词云中很难判断`good`是否比`story`更常见。这又是一个例子，说明在选择可视化方式之前，你应该考虑希望从中获取什么信息。
- en: The next section will show you how to dig into the data a little more deeply
    to get some insight into subsets of the data such as positive and negative reviews.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将展示如何更深入地挖掘数据，获取一些关于数据子集（如积极和消极评论）的洞见。
- en: Positive versus negative movie reviews
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 积极与消极的电影评论
- en: We might want to look at differences between the different classes (positive
    and negative) of movie reviews, or, more generally, any different categories.
    For example, are the frequent words in positive and negative reviews different?
    This preliminary look at the properties of the positive and negative reviews can
    give us some insight into how the classes differ, which, in turn, might help us
    select the approaches that will enable a trained system to differentiate the categories
    automatically.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要查看电影评论的不同类别（正面和负面）之间的差异，或者更一般地，查看任何不同类别之间的差异。例如，正面和负面评论中的常见词是否不同？对正面和负面评论属性的初步分析可以让我们更好地理解这些类别的差异，这反过来可能帮助我们选择合适的方法，使训练好的系统能够自动区分这些类别。
- en: We can use any of the visualizations based on frequency distributions for this,
    including the line graph in *Figure 6**.7*, the bar graph in *Figure 6**.9*, or
    the word cloud visualization in *Figure 6**.11*. We’ll use the word cloud in this
    example because it is a good way of seeing the differences in word frequencies
    between the two frequency distributions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用任何基于频率分布的可视化方式，包括*图 6**.7*中的折线图，*图 6**.9*中的条形图，或*图 6**.11*中的词云可视化。在这个例子中，我们将使用词云，因为它是查看两种频率分布之间单词频率差异的好方法。
- en: 'Looking at the code in *Figure 6**.12*, we start by importing the libraries
    we’ll need:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*图 6**.12*中的代码，我们首先导入所需的库：
- en: '![Figure 6.12 – Importing libraries for computing word clouds](img/B19005_06_12.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 导入用于计算词云的库](img/B19005_06_12.jpg)'
- en: Figure 6.12 – Importing libraries for computing word clouds
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 导入用于计算词云的库
- en: 'The next thing we’ll do is create two functions that will make it easier to
    perform similar operations on different parts of the corpus:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将创建两个函数，方便我们对语料库的不同部分执行类似的操作：
- en: '![Figure 6.13 – Functions for computing word clouds](img/B19005_06_13.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 计算词云的函数](img/B19005_06_13.jpg)'
- en: Figure 6.13 – Functions for computing word clouds
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 计算词云的函数
- en: 'The first function in *Figure 6**.13*, `clean_corpus()`, removes the punctuation
    and stopwords from a corpus. The second one, `plot_freq_dist()`, plots the word
    cloud from the frequency distribution. Now we’re ready to create the word clouds:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.13*中的第一个函数`clean_corpus()`会从语料库中移除标点符号和停用词。第二个函数`plot_freq_dist()`会从频率分布中绘制词云。现在我们准备好创建词云了：'
- en: "![Figure 6.14 – The code for displaying \uFEFFword frequencies for positive\
    \ and negative reviews in a word cloud](img/B19005_06_14New.jpg)"
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 显示正面和负面评论词频的代码](img/B19005_06_14New.jpg)'
- en: Figure 6.14 – The code for displaying word frequencies for positive and negative
    reviews in a word cloud
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 显示正面和负面评论词频的代码
- en: 'We create the word cloud with the following steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下步骤创建词云：
- en: Now that we’ve defined the functions, we separate the corpus into positive and
    negative reviews. This is done by the code shown in lines 28 and 29.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了函数，将语料库分为正面和负面评论。这是在第28行和第29行的代码中完成的。
- en: Instead of asking for all the words in the corpus, as we saw in *Figure 6**.2*,
    we ask for words in specific categories. In this example, the categories are positive
    (`pos`) and negative (`neg`).
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们在*图 6**.2*中看到的要求获取语料库中所有单词不同，我们请求获取特定类别的单词。在这个例子中，类别是正面（`pos`）和负面（`neg`）。
- en: We remove the stopwords and punctuation in each set of words in lines 28 and
    29.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第28行和第29行的每一组词中移除停用词和标点符号。
- en: We then create frequency distributions for the positive and negative words in
    lines 30 and 31\. Finally, we plot the word cloud.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在第30行和第31行为正面和负面词创建频率分布。最后，我们绘制词云。
- en: '*Figure 6**.15* shows the word cloud for the word frequencies in the positive
    reviews:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.15* 显示了正面评价的词云：'
- en: '![Figure 6.15 – Displaying word frequencies for positive reviews in a word
    cloud](img/B19005_06_15New.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 在词云中显示正面评价的词频](img/B19005_06_15New.jpg)'
- en: Figure 6.15 – Displaying word frequencies for positive reviews in a word cloud
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 在词云中显示正面评价的词频
- en: '*Figure 6**.16* shows the word frequencies for negative reviews in a word cloud:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.16* 显示了在词云中显示负面评价的词频：'
- en: '![Figure 6.16 – Displaying word frequencies for negative reviews in a word
    cloud](img/B19005_06_16New.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 在词云中显示负面评价的词频](img/B19005_06_16New.jpg)'
- en: Figure 6.16 – Displaying word frequencies for negative reviews in a word cloud
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 在词云中显示负面评价的词频
- en: Comparing *Figures 6.15* and *6.16* with the original word cloud in *Figure
    6**.11*, we can see that the words `film`, `one`, and `movie` are the most frequent
    words in positive and negative reviews, as well as being the most frequent overall
    words, so they would not be very useful in distinguishing positive and negative
    reviews.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 比较*图 6.15*和*图 6.16*与*图 6.11*中的原始词云，我们可以看到，`film`、`one` 和 `movie` 是正面和负面评论中最常出现的词，也是总体最常出现的词，因此它们在区分正负面评论方面不会很有用。
- en: The word `good` is larger (and therefore more frequent) in the positive review
    word cloud than in the negative review word cloud, which is just what we would
    expect. However, `good` does occur fairly frequently in the negative reviews,
    so it’s not a definite indication of a positive review by any means. Other differences
    are less expected – for example, `story` is more common in the positive reviews,
    although it does occur in the negative reviews. This comparison shows that the
    problem of distinguishing between positive and negative reviews is unlikely to
    be solved by simple keyword-spotting techniques.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 词语`good`在正面评论词云中的大小（因此也更为频繁）大于在负面评论词云中的出现频率，这正是我们所期望的。然而，`good`在负面评论中也出现得相当频繁，因此它绝不是判断正面评论的明确标志。其他的差异则不太符合预期——例如，`story`在正面评论中更常见，尽管它也会出现在负面评论中。这一比较表明，仅通过简单的关键词识别技术，区分正负面评论的问题可能无法得到解决。
- en: The techniques we’ll be looking at in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173)
    and [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184), will be more suitable for
    addressing the problem of classifying texts into different categories. However,
    we can see that this initial exploration with word clouds was very useful in eliminating
    a simple keyword-based approach from the set of ideas we are considering.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第9章*](B19005_09.xhtml#_idTextAnchor173)和[*第10章*](B19005_10.xhtml#_idTextAnchor184)中探讨的技术，将更适合解决将文本分类到不同类别的问题。然而，我们可以看到，这一开始的词云探索对于排除基于简单关键词的方法非常有用。
- en: In the next section, we’ll look at other frequencies in the corpus.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看语料库中的其他频率。
- en: Looking at other frequency measures
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看其他频率度量
- en: So far we’ve only looked at word frequencies, but we can look at the frequency
    of any other text property we can measure. For example, we can look at the frequencies
    of different characters or different parts of speech. You can try extending the
    code that was presented earlier in this chapter, in the previous section, *Frequency
    distributions*, to count some of these other properties of the movie review texts.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了词频，但我们可以查看任何其他可以度量的文本特征的频率。例如，我们可以查看不同字符或不同词性出现的频率。你可以尝试扩展本章前一部分*词频分布*中展示的代码，去统计电影评论文本中的一些其他特征。
- en: One important property of language is that words don’t just occur in isolation
    – they occur in specific combinations and orders. The meanings of words can change
    dramatically depending on their contexts. For example, *not a good movie* has
    a very different meaning (in fact, the opposite meaning) from *a good movie*.
    There are a number of techniques for taking word context into account, which we
    will explore in *Chapters 8* to *11*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 语言的一个重要特性是，单词不仅仅是孤立地出现——它们会以特定的组合和顺序出现。单词的意思可以根据它们的上下文发生显著变化。例如，*not a good
    movie* 与 *a good movie* 的含义截然不同（实际上是相反的含义）。有许多技术可以考虑单词的上下文，我们将在*第8章*到*第11章*中探讨这些技术。
- en: 'However, here we’ll just describe one very simple technique – looking at words
    that occur next to each other. Where two words occur together, this is called
    a `ngrams()`, which takes the desired value of `n` as an argument. *Figure 6**.17*
    shows code for counting and displaying ngrams in the movie review corpus:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这里我们将仅描述一种非常简单的技术——观察彼此相邻的单词。当两个单词一起出现时，称之为`ngrams()`，它接受一个`n`值作为参数。*图 6.17*展示了计算和显示电影评论语料库中n-gram的代码：
- en: '![Figure 6.17 – Computing bigrams in the movie review data](img/B19005_06_17.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – 计算电影评论数据中的二元组](img/B19005_06_17.jpg)'
- en: Figure 6.17 – Computing bigrams in the movie review data
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – 计算电影评论数据中的二元组
- en: 'The code shows the following initialization steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 代码显示了以下初始化步骤：
- en: We start by importing `nltk`, the `ngrams` function, and the movie review corpus.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入`nltk`、`ngrams`函数以及电影评论语料库。
- en: We set the frequency cutoff to `25`, but as in previous examples, this can be
    any number that we think will be interesting.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将频率截止值设置为`25`，但和之前的例子一样，这个值可以是我们认为有趣的任何数字。
- en: We collect all the words in the corpus at line 8 (if we wanted just the words
    from the positive reviews or just the words from the negative reviews, we could
    get those by setting `categories = 'neg'` or `categories = 'pos'`, as we saw in
    *Figure 6**.14*).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第8行收集了语料库中的所有单词（如果我们只想要正面评论中的单词或负面评论中的单词，我们可以通过设置 `categories = 'neg'` 或
    `categories = 'pos'` 来获取这些单词，正如我们在*图 6.14*中看到的）。
- en: Finally, we remove punctuation and stopwords at line 11, using the `clean_corpus`
    function defined in *Figure 6**.13*.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在第11行使用在*图 6.13*中定义的 `clean_corpus` 函数，移除标点符号和停用词。
- en: 'In *Figure 6**.18*, we collect the bigrams:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.18*中，我们收集了二元组：
- en: '![Figure 6.18 – Computing bigrams in the movie review data](img/B19005_06_18.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 计算电影评论数据中的二元组](img/B19005_06_18.jpg)'
- en: Figure 6.18 – Computing bigrams in the movie review data
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 计算电影评论数据中的二元组
- en: 'The bigrams are collected using the following steps:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 二元组的收集步骤如下：
- en: The `ngrams()` function is used at line 14, with an argument, `2`, indicating
    that we want bigrams (or pairs of two adjacent words). Any number can be used
    here, but very large numbers are not likely to be very useful. This is because
    as the value of `n` increases, there will be fewer and fewer ngrams with that
    value. At some point, there won’t be enough examples of a particular ngram to
    provide any information about patterns in the data. Bigrams or trigrams are usually
    common enough in the corpus to be helpful in identifying patterns.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ngrams()`函数在第14行使用，并传入参数`2`，表示我们想要二元组（即两个相邻单词的组合）。这里可以使用任何数字，但非常大的数字可能不会非常有用。这是因为随着`n`值的增加，符合该值的
    ngram 会越来越少。到某个程度时，某个特定的 ngram 可能没有足够的示例来提供关于数据模式的信息。二元组或三元组通常在语料库中足够常见，因此有助于识别模式。'
- en: In lines 21-23, we loop through the list of bigrams, creating a string from
    each pair.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第21至23行，我们遍历二元组列表，从每一对中创建一个字符串。
- en: In *Figure 6.19*, we make a frequency distribution of the bigrams and display
    it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.19*中，我们制作了二元组的频率分布并展示了它。
- en: '![Figure 6.19 – Displaying bigrams in the movie review data](img/B19005_06_19.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 显示电影评论数据中的二元组](img/B19005_06_19.jpg)'
- en: Figure 6.19 – Displaying bigrams in the movie review data
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 显示电影评论数据中的二元组
- en: We use the familiar NLTK `FreqDist()` function on our list of bigrams and convert
    it to a pandas Series in line 29.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第29行使用熟悉的 NLTK `FreqDist()` 函数对二元组列表进行处理，并将其转换为 pandas Series。
- en: The rest of the code sets up a bar graph plot in Seaborn and Matplotlib, and
    finally, displays it at line 39.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩下的代码设置了 Seaborn 和 Matplotlib 中的条形图，并在第39行显示了它。
- en: Since we’re using a frequency cutoff of `25`, we’ll only be looking at the most
    common 25 bigrams. You may want to experiment with larger and smaller frequency
    cutoffs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的频率截止值为`25`，因此我们只会查看最常见的25个二元组。你可能想尝试不同的更大或更小的频率截止值。
- en: '*Figure 6**.20* shows the results of the plot that we computed in *Figure 6**.19*.
    Because bigrams, in general, will be longer than single words and take more room
    to display, the *x* and *y* axes have been swapped around so that the counts of
    bigrams are displayed on the *x* axis and the individual bigrams are displayed
    on the *y* axis:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.20* 显示了我们在*图 6.19*中计算的图表结果。由于二元组通常比单个词更长，占用更多空间，因此我们交换了 *x* 和 *y* 轴，使得二元组的计数显示在
    *x* 轴上，而单个二元组显示在 *y* 轴上：'
- en: '![Figure 6.20 – Bigrams in the movie review data, ordered by frequency](img/B19005_06_20.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.20 – 按频率排序的电影评论数据中的二元组](img/B19005_06_20.jpg)'
- en: Figure 6.20 – Bigrams in the movie review data, ordered by frequency
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20 – 按频率排序的电影评论数据中的二元组
- en: '*Figure 6**.20* reveals several interesting facts about the corpus. For example,
    the most common bigram, `special effects`, occurs about 400 times in the corpus,
    compared to the most common single word, `film`, which occurs nearly 8,000 times.
    This is to be expected because two words have to occur together to count as a
    bigram. We also see that many of the bigrams are `New York` and `Star Trek` are
    examples in this list. Other bigrams are not idioms but just common phrases, such
    as `real life` and `one day`. In this list, all of the bigrams are very reasonable,
    and it is not surprising to see any of them in a corpus of movie reviews.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.20*揭示了语料库中一些有趣的事实。例如，最常见的双字组`special effects`在语料库中出现约400次，而最常见的单词`film`则出现了将近8000次。这是可以预期的，因为两个单词必须一起出现才能算作一个双字组。我们还看到，许多双字组例如`New
    York`和`Star Trek`出现在这个列表中。其他的双字组并非习语，而只是常见短语，如`real life`和`one day`。在这个列表中，所有的双字组都非常合理，在电影评论的语料库中看到它们并不令人惊讶。'
- en: As an exercise, try comparing the bigrams in the positive and negative movie
    reviews. In looking at single-word frequencies, we saw that the most common words
    were the same in both positive and negative reviews. Is that also the case with
    the most common bigrams?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试比较正面和负面电影评论中的双字组。在查看单词频率时，我们发现正面和负面评论中最常见的词汇是相同的。那么，最常见的双字组是否也是如此呢？
- en: This section covered some ways to get insight into our datasets by simple measurements
    such as counting words and bigrams. There are also some useful exploratory techniques
    for measuring and visualizing the similarities among documents in a dataset. We
    will cover these in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一些通过简单的测量方法（如计算单词和双字组）来洞察我们数据集的方式。还有一些有用的探索性技术，可以用来衡量和可视化数据集中文档之间的相似性。我们将在下一节中介绍这些技术。
- en: Measuring the similarities among documents
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量文档之间的相似性
- en: So far, we’ve been looking at visualizing the frequencies of various properties
    of the corpus, such as words and bigrams, with tools such as line charts, bar
    graphs, and word clouds. It is also very informative to visualize document similarity
    – that is, how similar documents in a dataset are to each other. There are many
    ways to measure document similarity, which we will discuss in detail in later
    chapters, especially [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), [*Chapter
    10*](B19005_10.xhtml#_idTextAnchor184), [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193),
    and [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217). We will start here with
    an introductory look at two basic techniques.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用诸如折线图、条形图和词云等工具来可视化语料库中各种属性的频率，如单词和双字组。可视化文档相似性也非常有意义——也就是说，数据集中的文档彼此之间的相似程度。有很多方法可以衡量文档的相似性，我们将在后续章节中详细讨论，尤其是[*第9章*](B19005_09.xhtml#_idTextAnchor173)、[*第10章*](B19005_10.xhtml#_idTextAnchor184)、[*第11章*](B19005_11.xhtml#_idTextAnchor193)和[*第12章*](B19005_12.xhtml#_idTextAnchor217)。我们将在这里简单介绍两种基本的技术。
- en: BoW and k-means clustering
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BoW 和 k-means 聚类
- en: For now, we will use a very simple idea, called **bag of words** (**BoW**).
    The idea behind BoW is that documents that are more similar to each other will
    contain more of the same words. For each document in a corpus and for each word
    in the corpus, we look at whether or not that word occurs in that document. The
    more words that any two documents have in common, the more similar to each other
    they are. This is a very simple measurement, but it will give us a basic document
    similarity metric that we can use to illustrate visualizations of document similarity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用一个非常简单的概念，叫做**词袋模型**（**BoW**）。BoW的核心思想是，相似的文档会包含更多相同的单词。对于语料库中的每个文档和每个单词，我们会查看该单词是否出现在该文档中。任何两个文档中共同出现的单词越多，它们就越相似。这是一个非常简单的度量方法，但它能为我们提供一个基本的文档相似性度量，便于我们展示文档相似性的可视化效果。
- en: 'The code in *Figure 6**.21* computes the BoW for the movie review corpus. You
    do not need to be concerned with the details of this code at this time, since
    it’s just a way of getting a similarity measure for the corpus. We will use this
    similarity metric (that is, BoW) to see how similar any two documents are to each
    other. The BoW metric has the advantage that it is easy to understand and compute.
    Although it is not the state-of-the-art way of measuring document similarity,
    it can be useful as a quick first step:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.21*中的代码计算了电影评论语料库的 BoW。此时你不需要关注代码的细节，因为它只是获取语料库相似度度量的一种方式。我们将使用这个相似度度量（即
    BoW）来查看任意两篇文档之间的相似度。BoW 指标的优点是易于理解和计算。虽然它不是衡量文档相似度的最先进方法，但作为一个快速的第一步，它依然有用：'
- en: "![Figure 6.21 – Setting up the B\uFEFFoW computation](img/B19005_06_21.jpg)"
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – 设置 BoW 计算](img/B19005_06_21.jpg)'
- en: Figure 6.21 – Setting up the BoW computation
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – 设置 BoW 计算
- en: '*Figure 6**.21* shows the process of getting the most frequent 1,000 words
    from the corpus and making them into a list. The number of words we want to keep
    in the list is somewhat arbitrary – a very long list will slow down later processing
    and will also start to include rare words that don’t provide much information.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.21* 显示了从语料库中获取最常见的 1,000 个单词并将它们制作成列表的过程。我们希望保留的单词数量有些随意——一个非常长的列表会减慢后续处理速度，并且也开始包含一些不会提供太多信息的稀有单词。'
- en: 'The next steps, shown in *Figure 6**.22*, are to define a function to collect
    the words in a document and then make a list of the documents:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，如*图 6**.22*所示，是定义一个函数来收集文档中的单词，然后列出文档：
- en: '![Figure 6.22 – Collecting the words that occur in a document](img/B19005_06_22.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.22 – 收集出现在文档中的单词](img/B19005_06_22.jpg)'
- en: Figure 6.22 – Collecting the words that occur in a document
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22 – 收集出现在文档中的单词
- en: 'The `document_features()` function in *Figure 6**.22* iterates through the
    given document creating a Python dictionary with the words as keys and 1s and
    0s as the values, depending on whether or not the word occurred in the document.
    Then, we create a list of the features for each document and display the result
    in *Figure 6**.23*:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.22* 中的 `document_features()` 函数通过遍历给定的文档，创建一个 Python 字典，其中单词作为键，1 和
    0 作为值，取决于该单词是否出现在文档中。然后，我们为每个文档创建一个特征列表，并在*图 6**.23*中显示结果：'
- en: "![Figure 6.23 – Computing the full feature set for all documents and displaying\
    \ the resulting B\uFEFFoW](img/B19005_06_23.jpg)"
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.23 – 计算所有文档的完整特征集并显示结果 BoW](img/B19005_06_23.jpg)'
- en: Figure 6.23 – Computing the full feature set for all documents and displaying
    the resulting BoW
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – 计算所有文档的完整特征集并显示结果 BoW
- en: Although the list of features for each document includes its category, we don’t
    need the category in order to display the BoW, so we remove it at lines 34-39.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个文档的特征列表包括其类别，但我们在显示 BoW 时并不需要类别，因此我们在第 34-39 行删除了它。
- en: 'We can see the first 10 documents in the resulting BoW itself in *Figure 6**.24*:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图 6**.24*中看到结果 BoW 本身的前 10 个文档：
- en: "![Figure 6.24 – B\uFEFFoW for the movie review corpus](img/B19005_06_24.jpg)"
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.24 – 电影评论语料库的 BoW](img/B19005_06_24.jpg)'
- en: Figure 6.24 – BoW for the movie review corpus
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24 – 电影评论语料库的 BoW
- en: In *Figure 6**.24*, each of the 10 rows of 0s and 1s represents one document.
    There is one column for each word in the corpus. There are 1,000 columns, but
    this is too many to display, so we only see the first few and last few columns.
    The words are sorted in order of frequency, and we can see that the most frequent
    words (`film`, `one`, and `movie`) are the same words that we found to be the
    most frequent (except for stopwords) in our earlier explorations of word frequencies.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6**.24*中，0 和 1 的 10 行每一行代表一个文档。每个单词在语料库中都有一个对应的列。共有 1,000 列，但由于太多无法显示，我们只能看到前几列和后几列。单词按照频率排序，我们可以看到最常见的单词（`film`、`one`
    和 `movie`）与我们之前在单词频率分析中找到的最常见单词（去除停用词）一致。
- en: Each document is represented by a row in the BoW. This list of numbers in these
    rows is a **vector**, which is a very important concept in NLP, which we will
    be seeing much more of in later chapters. Vectors are used to represent text documents
    or other text-based information as numbers. Representing words as numbers opens
    up many opportunities for analyzing and comparing documents, which are difficult
    to do when the documents are in text form. Clearly, BoW loses a lot of information
    compared to the original text representation – we no longer can tell which words
    are even close to each other in the text, for example – but in many cases, the
    simplicity of using BoW outweighs the disadvantage of losing some of the information
    in the text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档在BoW中由一行表示。这些行中的数字列表是一个**向量**，这是自然语言处理（NLP）中的一个非常重要的概念，我们将在后续章节中看到更多的应用。向量用于将文本文档或其他基于文本的信息表示为数字。将单词表示为数字为分析和比较文档打开了许多机会，而这些操作在文本文档的形式下是非常困难的。显然，相比于原始的文本表示，BoW丢失了很多信息——例如我们无法知道文本中的哪些词语彼此接近——但在许多情况下，使用BoW的简便性超过了丢失一些文本信息的缺点。
- en: One very interesting way that we can use vectors in general, including BoW vectors,
    is to try to capture document similarities, which can be a very helpful first
    step in exploring a dataset. If we can tell which documents are similar to which
    other documents, we can put them into categories based on their similarity. However,
    just looking at the document vectors in the rows of *Figure 6**.24* does not provide
    much insight, because it’s not easy to see any patterns. We need some tools for
    visualizing document similarity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量来捕捉文档相似性是一个非常有趣的方式，包括BoW向量。这是探索数据集的第一步，并且非常有用。如果我们能知道哪些文档与其他文档相似，就可以根据相似性将它们分类。然而，仅仅看*图6.24*中的文档向量并不能提供太多的洞见，因为很难看出任何模式。我们需要一些可视化文档相似性的工具。
- en: A good technique for visualizing document similarities is k-means clustering.
    `k` value refers to the number of clusters we want to have and is selected by
    the developer. In our case, we will start with `2` as the value of `k` since we
    have 2 known classes, corresponding to the sets of positive and negative reviews.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一种很好的可视化文档相似性的方法是k-means聚类。`k`值指的是我们想要的聚类数量，由开发者选择。在我们的例子中，由于我们有2个已知类别，对应于正面和负面的评论集，所以我们将从`2`开始作为`k`的值。
- en: '*Figure 6**.25* shows the code for computing and displaying the results of
    k-means clustering on the BoW we computed in *Figure 6**.23*. We do not have to
    go into detail about the code in *Figure 6**.25* because it will be covered in
    detail in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217). However, we can note
    that this code uses another important Python machine learning library, `sklearn`,
    which is used to compute the clusters:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.25*展示了计算并显示我们在*图6.23*中计算的BoW的k-means聚类结果的代码。我们不必深入讨论*图6.25*中的代码，因为它将在[*第12章*](B19005_12.xhtml#_idTextAnchor217)中详细介绍。然而，我们可以注意到，这段代码使用了另一个重要的Python机器学习库`sklearn`，用于计算聚类：'
- en: '![Figure 6.25 – Setting up for k-means clustering](img/B19005_06_25.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 设置k-means聚类](img/B19005_06_25.jpg)'
- en: Figure 6.25 – Setting up for k-means clustering
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 设置k-means聚类
- en: The first step is to import the libraries we’ll need and set the number of clusters
    we want (`true_k`).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是导入我们需要的库，并设置我们希望的聚类数量（`true_k`）。
- en: The next part of the code, shown in *Figure 6**.26*, computes the k-means clusters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的下一部分，如*图6.26*所示，计算k-means聚类。
- en: "![Figure 6.26 – K-means clustering for visualizing document similarities based\
    \ on the B\uFEFFoW](img/B19005_06_26.jpg)"
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图6.26 – 基于BoW可视化文档相似性的k-means聚类](img/B19005_06_26.jpg)'
- en: Figure 6.26 – K-means clustering for visualizing document similarities based
    on the BoW
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – 基于BoW可视化文档相似性的k-means聚类
- en: 'The steps for computing the clusters as shown in *Figure 6**.26* are as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 计算聚类的步骤如*图6.26*所示如下：
- en: Reduce the dimensions to `2` for display.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将维度降至`2`以进行显示。
- en: Initialize a `kmeans` object (line 13).
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个`kmeans`对象（第13行）。
- en: Compute the result (line 18).
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算结果（第18行）。
- en: Get the labels from the result.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取结果中的标签。
- en: 'The final step is plotting the clusters:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是绘制聚类图：
- en: '![Figure 6.27 – Plotting k-means clustering](img/B19005_06_27.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27 – 绘制k-means聚类图](img/B19005_06_27.jpg)'
- en: Figure 6.27 – Plotting k-means clustering
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 – 绘制k-means聚类图
- en: '*Figure 6**.27* plots the clusters by iterating through the clusters and printing
    the items in each cluster in the same color.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.27*通过迭代各个聚类，并以相同的颜色打印每个聚类中的项目，来绘制聚类图。'
- en: 'The results of using k-means clustering on the movie review BoW can be seen
    in *Figure 6**.28*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用K-means聚类对电影评论的BoW进行处理的结果，可以在*图 6**.28*中看到：
- en: '![Figure 6.28 – K-means clustering for the movie corpus with two classes](img/B19005_06_28.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.28 – 对电影语料库进行二类K-means聚类](img/B19005_06_28.jpg)'
- en: Figure 6.28 – K-means clustering for the movie corpus with two classes
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.28 – 对电影语料库进行二类K-means聚类
- en: There are two major clusters in *Figure 6**.28*, one above and one below the
    0 point on the *y* axis. The colors are defined based on the Matplotlib `Accent`
    color map at line 21\. (More information about the many color maps available in
    Matplotlib can be found at [https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html).)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6**.28*中，有两个主要的聚类，一个位于*y*轴的0点上方，另一个位于下方。颜色是根据第21行的Matplotlib `Accent`颜色映射定义的。（关于Matplotlib中可用的多种颜色映射的更多信息，可以参考[https://matplotlib.org/stable/tutorials/colors/colormaps.html](https://matplotlib.org/stable/tutorials/colors/colormaps.html)。）
- en: 'Each dot in *Figure 6**.28* represents one document. Because the clusters are
    clearly separated, we can have some confidence that the similarity metric (BoW)
    reflects some real difference between the two classes of documents. However, that
    does not mean that the most insightful number of classes for this data is necessarily
    two. It is always worth checking out other numbers of classes; that is, other
    values of `k`. This can easily be done by changing the value of `true_k` at line
    7 in *Figure 6**.25*. For example, if we change the value of `true_k` to `3`,
    and thereby specify that the data should be divided into three classes, we’ll
    get a chart like the one in *Figure 6**.29*:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6**.28*中的每个点代表一个文档。由于这些聚类明显分离，我们可以有一定信心认为相似性度量（BoW）反映了两个文档类别之间的某些真实差异。然而，这并不意味着该数据的最有意义的类别数一定是两个。检查其他类别数始终是值得的，也就是说，检查`k`的其他值。通过更改*图
    6**.25*中第7行的`true_k`值，可以轻松做到这一点。例如，如果我们将`true_k`的值更改为`3`，从而指定数据应该分成三个类别，那么我们将得到一个类似*图
    6**.29*中的图表：'
- en: "![Figure 6.29\uFEFF – K-means clustering for the movie corpus with three classes](img/B19005_06_29.jpg)"
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: "![图 6.29\uFEFF – 对电影语料库进行三类K-means聚类](img/B19005_06_29.jpg)"
- en: Figure 6.29 – K-means clustering for the movie corpus with three classes
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.29 – 对电影语料库进行三类K-means聚类
- en: There are definitely three clear classes in *Figure 6**.29*, although they aren’t
    as nicely separated as the classes in *Figure 6**.28*. This could mean that the
    two classes of positive and negative reviews don’t tell the whole story. Perhaps
    there actually should be a third class of *neutral* reviews? We could investigate
    this by looking at the documents in the three clusters, although we won’t go through
    that exercise here.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6**.29*中，确实有三个明显的类别，尽管它们的分离度不如*图 6**.28*中的类别那么明显。这可能意味着积极和消极评论这两个类别并没有讲述完整的故事。也许实际上应该有第三类*中立*评论？我们可以通过查看三个聚类中的文档来调查这一点，尽管我们这里不会进行这个操作。
- en: By comparing the results in *Figures 6.28* and *6.29*, we can see that initial
    data exploration can be very useful in deciding how many classes in which to divide
    the dataset at the outset. We will take up this topic in much greater detail in
    [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较*图 6.28*和*图 6.29*中的结果，我们可以看到，初步的数据探索对于决定如何将数据集分成多少类别非常有用。我们将在[*第12章*](B19005_12.xhtml#_idTextAnchor217)中详细讨论这个话题。
- en: We have so far seen a number of ways of visualizing the information in a dataset,
    including word and bigram frequencies, as well as some introductory visualizations
    of document similarity.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了一些可视化数据集信息的方法，包括词频和双词频，以及一些文档相似性的初步可视化。
- en: Let’s now consider some points about the overall process of visualization.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一些关于可视化总体过程的要点。
- en: General considerations for developing visualizations
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发可视化时的一般考虑事项
- en: Stepping back a bit from the specific techniques we have reviewed so far, we
    will next discuss some general considerations about visualizations. Specifically,
    in the next sections, we will talk about what to measure, followed by how to represent
    these measurements and the relationships among the measurements. Because the most
    common visualizations are based on representing information in the *XY* plane
    in two dimensions, we will mainly focus on visualizations in this format, starting
    with selecting among measurements.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们迄今为止回顾的具体技术中稍微退一步，我们接下来将讨论一些关于可视化的总体考虑。具体来说，在接下来的几节中，我们将讨论测量什么，接着是如何表示这些测量及它们之间的关系。由于最常见的可视化是基于在*XY*平面中以二维形式表示信息，我们将主要聚焦于这种格式的可视化，首先从测量选择开始。
- en: Selecting among measurements
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在测量之间进行选择
- en: Nearly all NLP begins with measuring some property or properties of texts we
    are analyzing. The goal of this section is to help you understand the different
    kinds of text measurements that are available in NLP projects.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的自然语言处理（NLP）都从测量我们分析的文本的某些属性开始。本节的目标是帮助你理解在NLP项目中可用的不同文本测量方法。
- en: So far, we’ve primarily focused on measurements involving words. Words are a
    natural property to measure because they are easy to count accurately – in other
    words, counting words is a **robust** measurement. In addition, words intuitively
    represent a natural aspect of language. However, just looking at words can lead
    to missing important properties of the meanings of texts, such as those that depend
    on considering the order of words and their relationship to other words in the
    text.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要关注涉及单词的测量。单词是一个自然的测量属性，因为它们容易准确计数——换句话说，计数单词是一种**稳健**的测量方法。此外，单词直观地代表了语言的一个自然方面。然而，单纯只看单词可能会忽视文本含义中的重要属性，比如那些依赖于考虑单词顺序及其与文本中其他单词关系的属性。
- en: To try to capture richer information that does take into account the orders
    of words and their relationships, we can measure other properties of texts. We
    can, for example, count characters, syntactic constructions, parts of speech,
    ngrams (sequences of words), mentions of named entities, and word lemmas.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉更丰富的信息，这些信息考虑到单词的顺序及其关系，我们可以测量文本的其他属性。例如，我们可以统计字符、句法结构、词性、n-grams（词组序列）、命名实体的提及以及词的词根形式。
- en: As an example, we could look at whether pronouns are more common in positive
    movie reviews than in negative movie reviews. However, the downside of looking
    at these richer properties is that, unlike counting words, measurements of richer
    properties are less robust. That means they are more likely to include errors
    that would make the measurements less accurate. For example, if we’re counting
    verbs by using the results of part of speech tagging, an incorrect verb tag that
    should be a noun tag would make the verb count incorrect.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我们可以研究代词在正面电影评论中是否比负面电影评论中更常见。然而，关注这些更丰富属性的缺点是，和计数单词不同，测量更丰富属性的稳健性较差。这意味着它们更容易出现错误，从而使得测量不够准确。例如，如果我们通过词性标注的结果来统计动词，一个本应为名词的错误动词标注会使得动词计数不准确。
- en: For these reasons, deciding what to measure is not always going to be a cut-and-dry
    decision. Some measurements, such as those that are based on words or characters,
    are very robust but exclude information that could be important. Other measurements,
    such as counting parts of speech, are less robust but more informative. Consequently,
    we can’t provide hard and fast rules for deciding what to measure. However, one
    rule of thumb is to start with the simplest and most robust approaches, such as
    counting words, to see whether your application is working well with those techniques.
    If it is, you can stick with the simplest approaches. If not, you can try using
    richer information. You should also keep in mind that you aren’t limited to only
    one measurement.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些原因，决定测量什么并不总是一个简单明了的决定。有些测量方法，如基于单词或字符的测量，虽然非常稳健，但却排除了可能重要的信息。其他测量方法，如统计词性，虽然稳健性差，但信息量更大。因此，我们无法为选择测量方法提供硬性规则。不过，一个经验法则是，从最简单且最稳健的方法开始，比如计数单词，看看你的应用在这些技术下是否能正常工作。如果可以，你就可以坚持使用这些简单的方法。如果不行，你可以尝试使用更丰富的信息。你还应记住，你并不只限于使用一种测量方法。
- en: Once you have selected what you want to measure, there are other general considerations
    that have to do with visualizing your measurements. In the next sections, we will
    talk about representing variables on the axes of the *XY* plane, different kinds
    of scales, and dimensionality reduction.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了要测量的内容，还有其他与可视化你的测量有关的常见考虑因素。在接下来的章节中，我们将讨论如何在*XY*平面上表示变量、不同种类的尺度以及降维方法。
- en: Insights from representing independent and dependent variables
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从表示独立变量和依赖变量中获得的见解
- en: 'Most measurement involves measuring one `good` and we can tell that it’s negative
    if it contains `bad`. We can test this hypothesis by looking at the counts for
    `good` and `bad` (the dependent variable) for positive and negative reviews (the
    independent variable), as shown in *Figure 6**.30*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数测量涉及测量一个`good`，如果它包含`bad`，我们可以判断它是负面的。我们可以通过查看正面和负面评论中的`good`和`bad`的计数（依赖变量）来验证这个假设，正如在*图6.30*中所示：
- en: '![Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews](img/B19005_06_30.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图6.30 – 正面和负面评论中“good”和“bad”的计数](img/B19005_06_30.jpg)'
- en: Figure 6.30 – Counts of “good” and “bad” in positive and negative reviews
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30 – 正面和负面评论中“good”和“bad”的计数
- en: The columns on the *x* axis represent positive reviews containing the word `good`,
    negative reviews containing the word `good`, positive reviews containing the word
    `bad`, and negative reviews containing the word `bad`, respectively. Clearly,
    the hypothesis that `good` signals a positive review and `bad` signals a negative
    review is wrong. In fact, `good` occurs more often than `bad` in negative reviews.
    This kind of exploration and visualization can help us rule out lines of investigation
    at the outset of our project that are unlikely to be fruitful. In general, bar
    charts such as the one in *Figure 6**.30* are a good way to represent categorical
    independent variables or data that occurs in distinct classes.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '*x*轴上的列分别代表包含`good`的正面评论、包含`good`的负面评论、包含`bad`的正面评论以及包含`bad`的负面评论。显然，假设`good`表示正面评论，而`bad`表示负面评论是错误的。实际上，`good`在负面评论中出现的频率高于`bad`。这种探索和可视化有助于我们在项目开始时排除那些不太可能有成果的研究方向。一般来说，像*图6.30*这样的条形图是表示分类独立变量或数据的好方法，这些数据出现在不同的类别中。'
- en: 'After looking at displaying the values of dependent and independent variables
    in graphs like *Figure 6**.30*, we can turn to another general consideration:
    linear versus log scales.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看了如*图6.30*中依赖变量和独立变量的值在图形中的展示后，我们可以转向另一个常见的考虑因素：线性尺度与对数尺度。
- en: Log scales and linear scales
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对数尺度与线性尺度
- en: 'Sometimes you will see a pattern, as in *Figures 6.4* and *6.7*, where the
    first few items on the *x* axis have extremely high values and where the values
    of the other items drop off quickly. This makes it hard to see what’s going on
    in the part of the graph on the right where the items have lower values. If we
    see this pattern, it suggests that a **log scale** may be a better choice for
    the *y* axis than the **linear scale** in *Figure 6**.2*. We can see a log scale
    for the data from *Figure 6**.7* in *Figure 6**.31*:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有时你会看到一个模式，如*图6.4*和*图6.7*所示，其中* x*轴的前几个项具有非常高的值，而其他项的值迅速下降。这使得我们很难看到图形右侧的部分，那里项的值较低。如果我们看到这种模式，那么它暗示了**对数尺度**可能比*图6.2*中的**线性尺度**更适合用在*
    y*轴上。我们可以在*图6.7*的数据显示中看到对数尺度，见*图6.31*：
- en: "![Figure 6.31 – Log scale visualization for the data in \uFEFFFigure 6.4](img/B19005_06_31.jpg)"
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图6.31 – *图6.4*中数据的对数尺度可视化](img/B19005_06_31.jpg)'
- en: Figure 6.31 – Log scale visualization for the data in Figure 6.4
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31 – *图6.4*中数据的对数尺度可视化
- en: '*Figure 6**.31* shows a log scale plot of the data in *Figure 6**.7*. The *y-a*xis
    values are the counts of the 50 most common words in equally spaced powers of
    10, so each value on the *y* axis is 10 times greater than the previous one. Compared
    with *Figure 6**.7*, we can see that the line representing the counts of frequencies
    is much flatter, especially toward the right. In the log display, it is easier
    to compare the frequencies of these less common, but still very frequent, words.
    Because even the least frequent word in this chart, `she`, still occurs over 3,000
    times in the dataset, the line does not go below 10 3.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.31*展示了*图6.7*中数据的对数尺度图。*y*轴的值是按等距10的幂次排列的50个最常见单词的计数，因此*y*轴上的每个值都是前一个值的10倍。与*图6.7*相比，我们可以看到表示频率计数的线更加平坦，特别是在右侧。在对数显示中，更容易比较这些较不常见但仍然非常频繁的单词的频率。因为即使在这个图表中最不常见的单词`she`，在数据集中也出现了超过3,000次，所以这条线没有低于10³。'
- en: You should keep the option of displaying your data in a log scale in mind if
    you see a pattern like the one in *Figure 6**.7*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到像*图6.7*中的模式，你应该考虑将数据以对数尺度显示的选项。
- en: So far, we’ve been looking at data in two dimensions that can easily be displayed
    on paper or a flat screen. What happens if we have more than two dimensions? Let’s
    now consider higher dimensional data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在查看可以轻松显示在纸面或平面屏幕上的二维数据。如果我们有超过两个维度的情况会怎样呢？让我们现在考虑更高维度的数据。
- en: Dimensions and dimensionality reduction
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度与降维
- en: The examples we’ve seen up to this point have been largely two-dimensional diagrams
    with *x* and *y* axes. However, many NLP techniques result in data that is of
    much higher dimensionality. While two dimensions are easy to plot and visualize,
    higher dimensional data can be harder to understand. Three-dimensional data can
    be displayed by adding a *z* axis and rotating the graph so that the *x* and *z*
    axes are at 45-degree angles on the display. A fourth or time dimension can be
    added by animating an on-screen graph and showing the changes over time. However,
    dimensions beyond four are really impossible for humans to visualize, so there
    is no way to display them in a meaningful way.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看到的示例大多是带有*x*和*y*轴的二维图表。然而，许多NLP技术会产生维度更高的数据。虽然二维数据容易绘制和可视化，但更高维度的数据可能更难理解。三维数据可以通过添加*z*轴并旋转图形使*x*和*z*轴在显示屏上呈45度角来显示。通过给屏幕图形加上动画，并显示随时间变化的情况，可以增加第四维或时间维度。然而，四维以上的维度对于人类来说几乎不可能可视化，因此无法以有意义的方式展示它们。
- en: However, in many cases, it turns out that some of the higher dimensions in NLP
    results can be removed without seriously impacting either the visualization or
    the information produced by NLP. This is called **dimensionality reduction**.
    Dimensionality reduction techniques are used to remove the less important dimensions
    from the data so that it can be displayed more meaningfully. We will look at dimensionality
    reduction in more detail in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，事实证明可以去除NLP结果中的一些高维度，而不会严重影响NLP所生成的可视化效果或信息。这就是所谓的**降维**。降维技术用于从数据中去除不太重要的维度，以便能够更有意义地展示数据。我们将在[*第12章*](B19005_12.xhtml#_idTextAnchor217)中更详细地讨论降维。
- en: Note that dimensionality reduction was done in *Figures 6.28* and *6.29* in
    order to display the results on a two-dimensional page.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*图6.28*和*图6.29*中的降维是为了将结果显示在二维页面上。
- en: The final topic in this chapter addresses some things we can learn from visualizations
    and includes some suggestions for using visualization to decide how to proceed
    with the next phases of an NLP project.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最后一个主题讨论了我们可以从可视化中学到的一些内容，并包括一些使用可视化来决定如何继续进行NLP项目下阶段的建议。
- en: Using information from visualization to make decisions about processing
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用可视化中的信息来做出处理决策
- en: This section includes guidance about how visualization can help us make decisions
    about processing. For example, in making a decision about whether to remove punctuation
    and stopwords, exploring word frequency visualizations such as frequency distribution
    and word clouds can tell us whether very common words are obscuring patterns in
    the data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包括了如何利用可视化来帮助我们做出处理决策的指导。例如，在决定是否删除标点符号和停用词时，探索单词频率的可视化方法，如频率分布图和词云，可以告诉我们是否有非常常见的单词掩盖了数据中的模式。
- en: Looking at frequency distributions of words for different categories of data
    can help rule out simple keyword-based classification techniques.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 查看不同数据类别的词汇频率分布可以帮助排除基于简单关键词的分类技术。
- en: Frequencies of different kinds of items, such as words and bigrams, can yield
    different insights. It can also be worth exploring the frequencies of other kinds
    of items, such as parts of speech or syntactic categories such as noun phrases.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型项的频率，例如词汇和二元组，能够提供不同的见解。还可以探索其他类型项的频率，比如词性或句法类别（如名词短语）。
- en: Displaying document similarities with clustering can provide insight into the
    most meaningful number of classes that you would want to use in dividing a dataset.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用聚类展示文档相似性可以帮助我们洞察出将数据集划分为最有意义的类别所需要的类别数。
- en: The final section summarizes the information that we learned in this chapter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一节总结了我们在本章中学到的信息。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about some techniques for the initial exploration
    of text datasets. We started out by exploring data by looking at the frequency
    distributions of words and bigrams. We then discussed different visualization
    approaches, including word clouds, bar graphs, line graphs, and clusters. In addition
    to visualizations based on words, we also learned about clustering techniques
    for visualizing similarities among documents. Finally, we concluded with some
    general considerations for developing visualizations and summarized what can be
    learned from visualizing text data in various ways. The next chapter will cover
    how to select approaches for analyzing NLU data and two kinds of representations
    for text data – symbolic representations and numerical representations.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了一些用于初步探索文本数据集的技术。我们首先通过查看词汇和二元组的频率分布来探索数据。接着讨论了不同的可视化方法，包括词云、条形图、折线图和聚类图。除了基于词汇的可视化方法，我们还了解了用于展示文档相似性的聚类技术。最后，我们总结了开发可视化时的一些考虑因素，并总结了通过不同方式可视化文本数据所能获得的见解。下一章将介绍如何选择分析自然语言理解（NLU）数据的方法，以及两种文本数据表示方式——符号表示和数值表示。
