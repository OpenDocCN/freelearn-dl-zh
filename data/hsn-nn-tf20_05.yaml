- en: TensorFlow Graph Architecture
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 图形架构
- en: The most concise and complete explanation of what TensorFlow is can be found
    on the project home page ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    and it highlights every important part of the library. TensorFlow is an open source
    software library for high-performance numerical computation. Its flexible architecture
    allows easy deployment of computation across a variety of platforms (CPUs, GPUs,
    and TPUs), from desktops to clusters of servers, to mobile and edge devices. Originally
    developed by researchers and engineers from the Google Brain team within Google's
    AI organization, it comes with strong support for machine learning and deep learning,
    and the flexible numerical computation core is used across many other scientific
    domains.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的最简洁和完整的解释可以在项目主页上找到（[https://www.tensorflow.org/](https://www.tensorflow.org/)），它突出了库的每个重要部分。TensorFlow
    是一个用于高性能数值计算的开源软件库。其灵活的架构允许在多种平台（CPU、GPU 和 TPU）上轻松部署计算，从桌面到服务器集群，再到移动设备和边缘设备。最初由
    Google AI 组织内的 Google Brain 团队的研究人员和工程师开发，TensorFlow 对机器学习和深度学习提供强大支持，其灵活的数值计算核心在许多其他科学领域中都有应用。
- en: 'TensorFlow''s strengths and most important features can be summarized in the
    following three points:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的优势和最重要的特点可以总结为以下三点：
- en: '**High-performance numerical computation library**: TensorFlow can be used
    in many different applications just by importing it. It is written in C++ and
    it offers bindings for several languages. The most complete, high-level, and widely
    used binding is the Python one. TensorFlow is a high-performance computational
    library that can be used in several domains (not only machine learning!) to execute
    numerical computation efficiently.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高性能数值计算库**：TensorFlow 可以通过导入它在许多不同的应用中使用。它用 C++ 编写，并为多种语言提供绑定。最完整、高级和广泛使用的绑定是
    Python 绑定。TensorFlow 是一个高性能的计算库，可以在多个领域（不仅仅是机器学习！）中高效执行数值计算。'
- en: '**Flexible architecture**: TensorFlow has been designed to work on different
    hardware (GPUs, CPUs, and TPUs) and different network architectures; its abstraction
    level is so high that (almost) the same code can train a model on a single computer
    or a cluster of machines in a data center.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的架构**：TensorFlow 被设计为可以在不同的硬件（GPU、CPU 和 TPU）和不同的网络架构上工作；其抽象级别非常高，几乎可以用相同的代码在单台计算机或数据中心的机群中训练模型。'
- en: '**Production-oriented**: TensorFlow has been developed by the Google Brain
    team as a tool for developing and serving machine learning models at scale. It
    was designed with the idea of simplifying the whole design-to-production pipeline;
    the library already comes with several APIs ready to be used in a production environment.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面向生产**：TensorFlow 由 Google Brain 团队开发，作为在规模上开发和提供机器学习模型的工具。它的设计理念是简化整个设计到生产流程；该库已经准备好在生产环境中使用几个
    API。'
- en: TensorFlow, thus, is a numerical computational library—keep that in mind. You
    can use it to perform any mathematical operation it offers, leveraging the power
    of all the hardware you have at your disposal, without doing anything ML-related.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，TensorFlow 是一个数值计算库，请牢记这一点。您可以使用它执行它提供的任何数学操作，利用您手头所有硬件的能力，而无需进行任何与 ML 相关的操作。
- en: 'In this chapter, you''ll learn everything you need to know about the TensorFlow
    architecture: what TensorFlow is, how to set up your environment to test both
    versions 1.x and 2.0 to see the differences, and you will learn a lot about how
    a computational graph is built; in the process, you will also learn how to use
    TensorBoard to visualize graphs.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习关于 TensorFlow 架构的所有必要知识：TensorFlow 是什么，如何设置您的环境来测试 1.x 和 2.0 两个版本以查看差异，您将了解到如何构建计算图；在此过程中，您还将学习如何使用
    TensorBoard 可视化图形。
- en: In this chapter, you'll (finally!) start reading some code. Please don't just
    read the code and the related explanations; write all the code you read and try
    to execute it. Follow the instructions on how to set up the two virtual environments
    we need and get your hands dirty with the code. At the end of this chapter, you'll
    be familiar with the fundamentals of TensorFlow that are valid for every TensorFlow
    version.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您（终于！）将开始阅读一些代码。请不要只是阅读代码和相关说明；请编写您阅读的所有代码并尝试执行它们。按照设置我们需要的两个虚拟环境的说明操作，并通过编码深入了解
    TensorFlow 的基础知识，这对于每个 TensorFlow 版本都是适用的。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Environment setup
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境设置
- en: Dataflow graphs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流图
- en: Model definition and training
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型定义与训练
- en: Interacting with the graph using Python
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python与图形交互
- en: Environment setup
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境设置
- en: 'In order to understand the structure of TensorFlow, all the examples presented
    in this chapter will use the latest TensorFlow 1.x release: 1.15; however, we
    will also set up everything needed to run TensorFlow 2.0 since we are going to
    use it in the next chapter, [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml),
    *TensorFlow 2.0 Architecture.*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解TensorFlow的结构，本章中展示的所有示例将使用最新的TensorFlow 1.x版本：1.15；然而，我们也将设置运行TensorFlow
    2.0所需的所有内容，因为我们将在下一章[第4章](655b734e-1636-4e11-b944-a71fafacb977.xhtml)，*TensorFlow
    2.0架构*中使用它。
- en: All the examples presented in this book specify the version of TensorFlow to
    use when running it. Being a library, we can just install it specifying the version
    we need. Of course, having two different versions of the same library installed
    on one system would be a mistake. In order to be able to switch between versions,
    we are going to use two different *Python virtual environments.*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中展示的所有示例都指定了在运行时使用的TensorFlow版本。作为一个库，我们只需指定所需的版本来安装它。当然，在一台系统上安装两个不同版本的同一个库是错误的。为了能够在版本之间切换，我们将使用两个不同的*Python虚拟环境*。
- en: 'An explanation of what a **virtual environment** (**virtualenv**) is and why
    it perfectly fits our needs follows here, from the official introduction to virtual
    environments ([https://docs.Python.org/3/tutorial/venv.html#introduction](https://docs.Python.org/3/tutorial/venv.html#introduction)):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是关于**虚拟环境**（**virtualenv**）是什么以及为什么它完全符合我们需求的解释，来自虚拟环境的官方介绍([https://docs.Python.org/3/tutorial/venv.html#introduction](https://docs.Python.org/3/tutorial/venv.html#introduction))：
- en: Python applications will often use packages and modules that don't come as part
    of the standard library. Applications will sometimes need a specific version of
    a library, because the application may require that a particular bug has been
    fixed or the application may be written using an obsolete version of the library's
    interface.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Python应用程序通常会使用一些不包含在标准库中的包和模块。应用程序有时需要某个库的特定版本，因为该应用程序可能需要修复特定的bug，或者该应用程序可能是使用库的过时版本编写的。
- en: This means it may not be possible for one Python installation to meet the requirements
    of every application. If application A needs version 1.0 of a particular module,
    but application B needs version 2.0, then the requirements are in conflict and
    installing either version 1.0 or 2.0 will leave one application unable to run.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着一个Python安装可能无法满足每个应用程序的要求。如果应用程序A需要特定模块的版本1.0，但应用程序B需要版本2.0，那么这些需求就发生了冲突，安装版本1.0或2.0都会导致某个应用程序无法运行。
- en: The solution to this problem is to create a virtual environment, a self-contained
    directory tree that contains a Python installation for a particular version of
    Python, plus a number of additional packages.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是创建一个虚拟环境，这是一个自包含的目录树，包含特定版本Python的安装以及一些附加包。
- en: Different applications can then use different virtual environments. To resolve
    the earlier example of conflicting requirements, application A can have its own
    virtual environment with version 1.0 installed, while application B has another
    virtual environment with version 2.0\. If application B requires a library to
    be upgraded to version 3.0, this will not affect application A's environment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的应用程序可以使用不同的虚拟环境。为了解决前面提到的冲突需求问题，应用程序A可以拥有自己的虚拟环境，安装版本1.0，而应用程序B则有另一个虚拟环境，安装版本2.0。如果应用程序B需要将某个库升级到版本3.0，这将不会影响应用程序A的环境。
- en: In order to create virtual environments in the easiest way, we use `pipenv`: the
    definitive tool for `virtualenv` creation and management; follow the installation
    guide at [https://github.com/pypa/pipenv](https://github.com/pypa/pipenv). Being
    a cross-platform tool, using Windows, Mac, or Linux makes no difference. Having
    installed `pipenv`, we just need to create these two separate virtual environments
    for the two different TensorFlow versions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以最简单的方式创建虚拟环境，我们使用`pipenv`：用于创建和管理`virtualenv`的终极工具；请参阅[https://github.com/pypa/pipenv](https://github.com/pypa/pipenv)上的安装指南。作为跨平台工具，使用Windows、Mac或Linux没有区别。安装了`pipenv`后，我们只需为两个不同的TensorFlow版本创建这两个独立的虚拟环境。
- en: We'll install TensorFlow without GPU support because `tensorflow-gpu` depends
    on CUDA and a recent NVIDIA GPU is required to use the GPU acceleration provided
    by the `CUDA` package. If you own a recent NVIDIA GPU, you can install the `tensorflow-gpu` package,
    but you have to take care to install the version of CUDA required by the TensorFlow
    package you are installing (TensorFlow 2.0 and TensorFlow 1.15 require CUDA 10).
    Moreover, you have to ensure that both the `tensorflow-gpu` packages installed
    in the `virtualenvs` depend on the same CUDA version (CUDA 10); otherwise, one
    installation will work and the other won't. However, if you stick with versions
    2.0 and 1.15 of TensorFlow, both are compiled with CUDA 10 support, hence, installing
    them in their GPU version and having CUDA 10 installed on your system should work
    perfectly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将安装不带 GPU 支持的 TensorFlow，因为 `tensorflow-gpu` 依赖于 CUDA 和最近的 NVIDIA GPU 才能使用
    `CUDA` 包提供的 GPU 加速。如果你拥有最近的 NVIDIA GPU，可以安装 `tensorflow-gpu` 包，但必须确保安装了 TensorFlow
    包所需的 CUDA 版本（TensorFlow 2.0 和 TensorFlow 1.15 需要 CUDA 10）。此外，你必须确保在 `virtualenvs`
    中安装的 `tensorflow-gpu` 包依赖于相同的 CUDA 版本（CUDA 10）；否则，其中一个安装将正常工作，另一个则不会。然而，如果你坚持使用
    TensorFlow 的 2.0 和 1.15 版本，它们都编译了对 CUDA 10 的支持，因此，在系统上安装 CUDA 10 并在其 GPU 版本中安装它们应该可以完美工作。
- en: TensorFlow 1.x environment
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 1.x 环境
- en: 'Create a folder, `tf1`, step inside it, and run the following commands to create
    an environment, activate it, and install TensorFlow using `pip`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 `tf1` 的文件夹，进入其中，并运行以下命令来创建一个环境，激活它，并使用 `pip` 安装 TensorFlow：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using Python 3.7 is not strictly mandatory; TensorFlow comes with support for
    Python 3.5, 3.6, and 3.7. Hence, if you are using a distribution/operating system
    that ships an older Python version, such as Python 3.5, you just have to change
    the Python version in the `pipenv` command.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 3.7 并非强制要求；TensorFlow 支持 Python 3.5、3.6 和 3.7。因此，如果你使用的发行版/操作系统安装了较旧的
    Python 版本，比如 Python 3.5，你只需在 `pipenv` 命令中更改 Python 版本。
- en: So far, so good. Right now, you are in an environment that uses Python 3.7 and
    has `tensorflow==1.15` installed. In order to create a new environment for TensorFlow
    2.0, we have to first exit from the `pipenv shell` created for us, which we're
    currently using. As a general rule, to switch from one `virtualenv` to another,
    we activate it using `pipenv shell` and deactivate it, exiting the session from
    the shell, by typing `exit`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 目前为止，一切顺利。现在，你处于一个使用 Python 3.7 并安装了 `tensorflow==1.15` 的环境中。为了创建 TensorFlow
    2.0 的新环境，我们必须首先退出当前正在使用的 `pipenv shell`。一般来说，要从一个 `virtualenv` 切换到另一个，我们使用 `pipenv
    shell` 激活它，并通过输入 `exit` 退出 shell 会话。
- en: Thus, before creating the second virtual environment, just close the currently
    running shell by typing `exit`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在创建第二个虚拟环境之前，只需通过输入 `exit` 关闭当前运行的 shell。
- en: TensorFlow 2.0 environment
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 环境
- en: 'In the same manner as with the TensorFlow 1.x environment, create a folder, `tf2`,
    step inside it, and run the following commands:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TensorFlow 1.x 环境相同的方式，创建一个名为 `tf2` 的文件夹，进入其中，并运行以下命令：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the rest of the book, whether the TensorFlow 1.x or 2.0 environment should
    be used is indicated by the `(tf1)` or `(tf2)` symbol before the code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分指出了是否应该使用 TensorFlow 1.x 或 2.0 环境，代码前使用 `(tf1)` 或 `(tf2)` 符号表示。
- en: 'We can now start digging inside the TensorFlow structure, analyzing, and describing
    something that was explicit in TensorFlow 1.x and hidden in TensorFlow 2.0 (but
    still present!): the data flow graph. Since the analysis that follows looks at
    the details of how a graph is built and how various low-level operations can be
    used to build graphs, almost every code snippet uses the TensorFlow 1.x environment.
    If you are interested in version 2.0 only because you already know and use TensorFlow
    1.x, you can skip this section; although, reading it is also recommended for the
    experienced user.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始深入分析 TensorFlow 结构，并描述在 TensorFlow 1.x 中显式显示但在 TensorFlow 2.0 中隐藏（但仍然存在的）数据流图。由于接下来的分析关注如何构建图以及如何使用各种低级操作来构建图的细节，几乎每个代码片段都使用
    TensorFlow 1.x 环境。如果你仅因为已经了解并使用 TensorFlow 1.x 所以对版本 2.0 感兴趣，可以跳过此部分；但建议有经验的用户也阅读一下。
- en: It is possible to use only the `tensorflow 2.0` environment and replace every
    call to the `tensorflow` package, `tf`, using the compatibility module present
    in TensorFlow 2; therefore, to have a single `(tf2)` environment, you must replace
    every `tf.` with `tf.compat.v1.` and disable eager execution by adding the `tf.compat.v1.disable_eager_execution()` line
    just after importing the TensorFlow package.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 只使用`tensorflow 2.0`环境并替换对`tensorflow`包的每次调用，`tf`，可以使用TensorFlow 2中的兼容模块；因此，要拥有一个单一的`(tf2)`环境，必须将每个`tf.`替换为`tf.compat.v1.`，并通过在导入TensorFlow包之后添加`tf.compat.v1.disable_eager_execution()`行来禁用即时执行。
- en: Now that we have our environment setup complete, let's move on to dataflow graphs
    and learn how to start working on some practical code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了环境设置，接下来让我们深入了解数据流图，学习如何开始编写一些实际的代码。
- en: Dataflow graphs
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流图
- en: In order to be a highly efficient, flexible, and production-ready library, TensorFlow
    uses dataflow graphs to represent computation in terms of the relationships between
    individual operations. Dataflow is a programming model widely used in parallel
    computing and, in a dataflow graph, the nodes represent units of computation while
    the edges represent the data consumed or produced by a computation unit.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成为一个高效、灵活且适合生产的库，TensorFlow使用数据流图以操作之间的关系来表示计算。数据流是并行计算中广泛使用的一种编程模型，在数据流图中，节点表示计算单元，而边表示计算单元消耗或产生的数据。
- en: As seen in the previous chapter, [Chapter 2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml),
    *Neural Networks and Deep Learning, *representing computation using graphs comes
    with the advantage of being able to run the forward and backward passes required
    to train a parametric machine learning model via gradient descent, applying the
    chain rule to compute the gradient as a local process to every node; however,
    this is not the only advantage of using graphs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所示，[第二章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)，*神经网络与深度学习*，通过图表示计算的方式，具有能够通过梯度下降运行训练参数化机器学习模型所需的前向和后向传播的优点，应用链式法则将梯度计算作为一个局部过程应用到每个节点；然而，使用图的优势不仅仅是这个。
- en: 'Reducing the abstraction level and thinking about the implementation details
    of representing computation using graphs brings the following advantages:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 降低抽象层次，思考使用图表示计算的实现细节，带来了以下优点：
- en: '**Parallelism**: Using nodes to represent operations and edges that represent
    their dependencies, TensorFlow is able to identify operations that can be executed
    in parallel.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性**：通过使用节点表示操作，边表示它们的依赖关系，TensorFlow能够识别可以并行执行的操作。'
- en: '**Computation optimization**: Being a graph, a well-known data structure, it
    is possible to analyze it with the aim of optimizing execution speed. For example,
    it is possible to detect unused nodes in the graph and remove them, hence optimizing
    it for size; it is also possible to detect redundant operations or sub-optimal
    graphs and replace them with the best alternatives.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算优化**：作为一个图，一个众所周知的数据结构，它可以被分析以优化执行速度。例如，可以检测到图中未使用的节点并将其删除，从而优化图的大小；还可以检测到冗余操作或次优图，并将其替换为最佳替代方案。'
- en: '**Portability**: A graph is a language-neutral and platform-neutral representation
    of computation. TensorFlow uses **Protocol Buffers** (**Protobuf**), which is
    a simple language-neutral, platform-neutral, and extensible mechanism for serializing
    structured data to store graphs. This, in practice, means that a model defined
    in Python using TensorFlow can be saved in its language-neutral representation
    (Protobuf) and then used inside another program written in another language.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：图是一种与语言无关、与平台无关的计算表示。TensorFlow使用**协议缓冲区**（**Protobuf**），它是一种简单的、与语言无关、与平台无关且可扩展的机制，用于序列化结构化数据以存储图。这实际上意味着，使用Python定义的TensorFlow模型可以以其语言无关的表示（Protobuf）保存，并且可以在另一个用其他语言编写的程序中使用。'
- en: '**Distributed execution**: Every graph''s node can be placed on an independent
    device and on a different machine. TensorFlow will take care of the communication
    between the nodes and ensure that the execution of a graph is correct. Moreover,
    TensorFlow itself is able to partition a graph across multiple devices, knowing
    that certain operations perform better on certain devices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式执行**：每个图的节点可以放置在独立的设备上，甚至在不同的机器上。TensorFlow将负责节点之间的通信，并确保图的执行正确无误。此外，TensorFlow本身能够将图分配到多个设备上，知道某些操作在特定设备上表现更好。'
- en: 'Let''s describe our first dataflow graph to compute a product and a sum between
    matrices and a vector; save the graphical representation and use TensorBoard to
    visualize it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述第一个数据流图，用于计算矩阵和向量之间的乘积与和，并保存图形表示，然后使用TensorBoard来可视化它：
- en: '`(tf1)`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In these few lines, there are a lot of peculiarities of TensorFlow and its way
    of building a computational graph. This graph represents the matrix product between
    the constant tensor identified by the `A` Python variable and the constant tensor
    identified by the `x` Python variable and the sum of the resulting matrix with
    the tensor identified by the `b` Python variable.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这几行代码中，包含了TensorFlow构建计算图的一些特性。该计算图表示常量张量`A`（由Python变量`A`标识）与常量张量`x`（由Python变量`x`标识）之间的矩阵乘法，以及与标识为`b`的张量相加的结果。
- en: The result of the computation is represented by the `y` Python variable, also
    known as the output of the `tf.add` node named `result` in the graph.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 计算的结果由`y` Python变量表示，也就是在计算图中名为`result`的`tf.add`节点的输出。
- en: 'Please note the separation between the concept of a Python variable and a node
    in the graph: we''re using Python only to describe the graph; the name of the
    Python variable means nothing in the graph definition.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意Python变量与计算图中的节点之间的区别：我们仅使用Python来描述计算图；Python变量的名称在计算图定义中并没有意义。
- en: Moreover, we created `tf.summary.SummaryWriter` to save a graphical representation
    of the graph we've built. The `writer` object has been created, specifying the
    path in which to store the representation (`log/matmul`) and a `tf.Graph` object
    obtained using the `tf.get_default_graph` function call that returns the default
    graph since at least one graph is always present in any TensorFlow application.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们创建了`tf.summary.SummaryWriter`来保存我们构建的计算图的图形表示。`writer`对象已经创建，指定了存储图形表示的路径（`log/matmul`）以及通过`tf.get_default_graph`函数调用获得的`tf.Graph`对象，该函数返回默认的计算图，因为在任何TensorFlow应用中，至少会有一个图存在。
- en: You can now visualize the graph using TensorBoard, the data visualization tool
    that comes free with TensorFlow. TensorBoard works by reading the log files placed
    in the specified `--logdir` and creates a web server so we're able to visualize
    our graph by using a browser.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用TensorFlow自带的免费数据可视化工具TensorBoard来可视化计算图。TensorBoard通过读取指定`--logdir`路径中的日志文件，并创建一个Web服务器，使我们能够通过浏览器可视化我们的计算图。
- en: 'To execute TensorBoard and visualize the graph, just type the command that
    follows and open a web browser at the address indicated by TensorBoard itself:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行TensorBoard并可视化计算图，只需输入以下命令，并在TensorBoard指定的地址打开网页浏览器：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot shows the built graph, as seen in TensorBoard, and
    the detail of the node result. The screenshot allows an understanding of how TensorFlow
    represents the nodes and which features every node has:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在TensorBoard中看到的构建好的计算图，以及节点结果的详细信息。该截图有助于理解TensorFlow是如何表示节点以及每个节点具有哪些特征的：
- en: '![](img/ca188909-c582-473a-9fbe-16c999310c42.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca188909-c582-473a-9fbe-16c999310c42.png)'
- en: The computational graph that describes the operation y = Ax +b. The result node
    is highlighted in red and its details are shown in the right-hand column.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是描述操作y = Ax + b的计算图。结果节点用红色高亮显示，右侧栏展示了该节点的详细信息。
- en: Please note that **we are just describing the graph**—the calls to the TensorFlow
    API are just adding operations (nodes) and connections (edges) among them; there
    is **no computation** performed in this phase. In TensorFlow 1.x, the following
    approach needs to be followed—static graph definition and execution, while this
    is no longer mandatory in 2.0.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，**我们只是描述了计算图**——对TensorFlow API的调用仅仅是添加操作（节点）和它们之间的连接（边）；在这个阶段**并没有执行计算**。在TensorFlow
    1.x中，必须遵循以下方法——静态图定义和执行，而在2.0版本中，这已经不再是强制要求。
- en: Since the computational graph is the fundamental building block of the framework
    (in every version), it is mandatory to understand it in depth, since even after
    transitioning to 2.0, having an understanding of what's going on under the hood
    makes the difference (and it helps a lot with debugging!).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算图是框架的基本构建块（在每个版本中都是如此），因此必须深入理解它，因为即使在过渡到2.0版本后，了解底层发生的事情仍然至关重要（这对调试也大有帮助！）。
- en: The main structure – tf.Graph
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结构 – tf.Graph
- en: 'As stated in the previous section, there''s no relation between the Python
    variables'' name and the names of the nodes. Always keep in mind that TensorFlow
    is a C++ library and we''re using Python to build a graph in an easy way. Python
    simplifies the graph description phase since it even creates a graph without the
    need to explicitly define it; in fact, there are two different ways to define
    a graph:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一部分所述，Python 变量的名称与节点名称之间没有关系。始终记住，TensorFlow 是一个 C++ 库，我们使用 Python 来简化图的构建。Python
    简化了图的描述阶段，因为它甚至可以在不显式定义图的情况下创建一个图；实际上，定义图有两种不同的方式：
- en: '**Implicit**: Just define a graph using the `tf.*` methods. If a graph is not
    explicitly defined, TensorFlow always defines a default `tf.Graph`, accessible
    by calling `tf.get_default_graph`. The implicit definition limits the expressive
    power of a TensorFlow application since it is constrained to using a single graph.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐式**：只需使用 `tf.*` 方法定义一个图。如果图没有显式定义，TensorFlow 总是定义一个默认的 `tf.Graph`，可以通过调用
    `tf.get_default_graph` 来访问。隐式定义限制了 TensorFlow 应用程序的表达能力，因为它只能使用一个图。'
- en: '**Explicit**: It is possible to explicitly define a computational graph and
    thus have more than one graph per application. This option has more expressive
    power, but is usually not needed since applications that need more than one graph
    are not common.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式**：可以显式定义计算图，从而每个应用程序可以有多个图。这个选项具有更强的表达能力，但通常不需要，因为需要多个图的应用程序并不常见。'
- en: In order to explicitly define a graph, TensorFlow allows the creation of `tf.Graph`
    objects that, through the `as_default` method, create a context manager; every
    operation defined inside the context is placed inside the associated graph. In
    practice, a `tf.Graph` object defines a namespace for the `tf.Operation` objects
    it contains.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了显式定义一个图，TensorFlow 允许创建 `tf.Graph` 对象，通过 `as_default` 方法创建一个上下文管理器；在该上下文内定义的每个操作都将被放置在关联的图中。实际上，`tf.Graph`
    对象为其包含的 `tf.Operation` 对象定义了一个命名空间。
- en: The second peculiarity of the `tf.Graph` structure is its **graph c****ollections**.
    Every `tf.Graph` uses the collection mechanism to store metadata associated with
    the graph structure. A collection is uniquely identified by a key and its content
    is a list of objects/operations. The user does not usually need to worry about the
    existence of a collection since they are used by TensorFlow itself to correctly
    define a graph.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Graph` 结构的第二个特点是其 **图集合**。每个 `tf.Graph` 使用集合机制来存储与图结构相关的元数据。一个集合通过键唯一标识，其内容是一个对象/操作的列表。用户通常不需要关心集合的存在，因为它们由
    TensorFlow 本身使用，以正确地定义图。'
- en: 'For example, when defining a parametric machine learning model, the graph must
    know which `tf.Variable` objects are the variables to update during the learning
    phase and which other variables are not part of the model but are something else
    (such as moving the mean/variance computed during the training process—these are
    variables but not trainable). In this case, when, as we will see in the following
    section, a `tf.Variable` is created, it is added by default to two collections:
    the global variable and trainable variable collections.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在定义一个参数化的机器学习模型时，图必须知道哪些 `tf.Variable` 对象是学习过程中需要更新的变量，以及哪些变量不是模型的一部分，而是其他的东西（例如在训练过程中计算的均值/方差——这些是变量，但不可训练）。在这种情况下，正如我们将在下一部分看到的，当创建一个
    `tf.Variable` 时，它默认会被添加到两个集合中：全局变量集合和可训练变量集合。
- en: Graph definition – from tf.Operation to tf.Tensor
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图定义——从 tf.Operation 到 tf.Tensor
- en: A dataflow graph is the representation of a computation where the nodes represent
    units of computation, and the edges represent the data consumed or produced by
    the computation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流图是计算的表示，其中节点表示计算单元，边表示计算消耗或生成的数据。
- en: In the context of `tf.Graph`, every API call defines `tf.Operation` (node) that
    can have multiple inputs and outputs `tf.Tensor` (edges). For instance, referring
    to our main example, when calling `tf.constant([[1, 2], [3, 4]], dtype=tf.float32)`,
    a new node (`tf.Operation`) named `Const` is added to the default `tf.Graph` inherited
    from the context. This node returns a `tf.Tensor` (edge) named `Const:0`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `tf.Graph` 的上下文中，每个 API 调用定义一个 `tf.Operation`（节点），它可以有多个输入和输出 `tf.Tensor`（边）。例如，参照我们的主要示例，当调用
    `tf.constant([[1, 2], [3, 4]], dtype=tf.float32)` 时，一个名为 `Const` 的新节点（`tf.Operation`）会被添加到从上下文继承的默认
    `tf.Graph` 中。这个节点返回一个名为 `Const:0` 的 `tf.Tensor`（边）。
- en: Since each node in a graph is unique, if there is already a node named *Const*
    in the graph (that is the default name given to all the constants), TensorFlow
    will make it unique by appending the suffix '_1', '_2', and so on to the name.
    If a name is not provided, as in our example, TensorFlow gives a default name
    to each operation added and adds the suffix to make them unique in this case too.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图中的每个节点都是唯一的，如果图中已有一个名为 *Const* 的节点（这是默认赋给所有常量的名称），TensorFlow 会通过附加后缀 '_1'、'_2'
    等来确保名称的唯一性。如果未提供名称，正如我们的例子中所示，TensorFlow 会为每个添加的操作提供一个默认名称，并同样添加后缀以保证它们的唯一性。
- en: The output `tf.Tensor` has the same name as the associated `tf.Operation`, with
    the addition of the *:ID* suffix. The *ID* is a progressive number that indicates
    how many outputs the operation produces. In the case of `tf.constant`, the output
    is just a single tensor, therefore *ID=0*; but there can be operations with more
    than one output, and in this case, the suffixes *:0, :1,* and so on are added
    to the `tf.Tensor` name generated by the operation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的 `tf.Tensor` 与关联的 `tf.Operation` 具有相同的名称，并附加了 *:ID* 后缀。*ID* 是一个递增的数字，表示操作生成的输出数量。对于
    `tf.constant`，输出只有一个张量，因此 *ID=0*；但也可以有多个输出的操作，在这种情况下，后缀 *:0、:1* 等将被添加到操作生成的 `tf.Tensor`
    名称中。
- en: It is also possible to add a name scope prefix to all operations created within
    a context—a context defined by the `tf.name_scope` call. The default name scope
    prefix is a `/` delimited list of names of all the active `tf.name_scope` context
    managers. In order to guarantee the uniqueness of the operations defined within
    the scopes and the uniqueness of the scopes themselves, the same suffix appending
    rule used for `tf.Operation` holds.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以为在上下文中创建的所有操作添加名称作用域前缀——上下文由 `tf.name_scope` 调用定义。默认的名称作用域前缀是一个由 `/` 分隔的活动
    `tf.name_scope` 上下文管理器的名称列表。为了保证作用域内定义的操作和作用域本身的唯一性，`tf.Operation` 使用的相同后缀追加规则也适用。
- en: 'The following code snippet shows how our baseline example can be wrapped into
    a separate graph, how a second independent graph can be created in the same Python
    script, and how we can change the node names, adding a prefix, using `tf.name_scope`.
    First, we import the TensorFlow library:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何将我们的基准示例包装到一个独立的图中，如何在同一 Python 脚本中创建第二个独立的图，并展示了如何使用 `tf.name_scope`
    更改节点名称并添加前缀。首先，我们导入 TensorFlow 库：
- en: '`(tf1)`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we define two `tf.Graph` objects (the scoping system allows you to use
    multiple graphs easily):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了两个 `tf.Graph` 对象（作用域系统使得轻松使用多个图成为可能）：
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Then, we define two summary writers. We need to use two different `tf.summary.FileWriter`
    objects to log two separate graphs.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义两个摘要写入器。我们需要使用两个不同的 `tf.summary.FileWriter` 对象来记录两个独立的图。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Run the example and use TensorBoard to visualize the two graphs, using the left-hand
    column on TensorBoard to switch between "runs."
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 运行示例并使用 TensorBoard 可视化这两个图，使用 TensorBoard 左侧的列切换不同的“运行”。
- en: Nodes with the same name, `x` in the example, can live together in the same
    graph, but they have to be under different scopes. In fact, being under different
    scopes makes the nodes completely independent and completely different objects.
    The node name, in fact, is not only the parameter `name` passed to the operation
    definition, but its full path, complete with all of the prefixes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，具有相同名称 `x` 的节点可以共存于同一个图中，但必须位于不同的作用域下。实际上，位于不同作用域下使得这些节点完全独立，并且是完全不同的对象。节点名称实际上不仅仅是传递给操作定义的
    `name` 参数，而是其完整路径，包括所有的前缀。
- en: 'In fact, running the script, the output is as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，运行脚本时，输出如下：
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we can see, the full names are different and we also have other information
    about the tensors produced. In general, every tensor has a name, a type, a rank,
    and a shape:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，完整的名称不同，我们还可以看到有关生成张量的其他信息。通常，每个张量都有一个名称、类型、秩和形状：
- en: The **name** uniquely identifies the tensor in the computational graphs. Using
    `tf.name_scope`, we can prefix tensor names, thus changing their full path. We
    can also specify the name using the `name` attribute of every `tf.*` API call.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名称** 唯一标识计算图中的张量。通过使用 `tf.name_scope`，我们可以为张量名称添加前缀，从而改变其完整路径。我们还可以通过每个 `tf.*`
    API 调用的 `name` 属性来指定名称。'
- en: The **type** is the data type of the tensor; for example, `tf.float32`, `tf.int8`,
    and so on.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类型**是张量的数据类型，例如 `tf.float32`、`tf.int8` 等。'
- en: The **rank**, in the TensorFlow world (this is different from the strictly mathematical
    definition), is just the number of dimensions of a tensor; for example, a scalar
    has rank 0, a vector has rank 1, a matrix has rank 2, and so on.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**秩**在TensorFlow中（这与严格的数学定义不同）仅指张量的维度数量；例如，标量的秩为0，向量的秩为1，矩阵的秩为2，依此类推。'
- en: The **shape** is the number of elements in each dimension; for example, a scalar
    has rank 0 and an empty shape of `()`, a vector has rank 1 and a shape of `(D0)`,
    a matrix has rank 2 and a shape of `(D0, D1)`, and so on.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**形状**是每个维度中的元素数量；例如，标量的秩为0，形状为`()`，向量的秩为1，形状为`(D0)`，矩阵的秩为2，形状为`(D0, D1)`，依此类推。'
- en: Sometimes, it is possible to see a shape with a dimension of `-1`. This is a
    particular syntax that tells TensorFlow to infer from the other, well-defined,
    dimensions of the tensor which value should be placed in that position. Usually,
    a negative shape is used in the `tf.reshape` operation, which is able to change
    the shape of a tensor if the requested one is compatible with the number of elements
    of the tensor.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，可能会看到一个维度为`-1`的形状。这是一种特殊语法，告诉TensorFlow从其他已定义的维度推断出应该放置在该位置的值。通常，负形状用于`tf.reshape`操作，它能够改变张量的形状，只要请求的形状与张量的元素数量兼容。
- en: When defining a tensor, instead, it is possible to see one or more dimensions
    with the value of `None`. In this case, the full shape definition is delegated
    to the execution phase, since using `None` instructs TensorFlow to expect a value
    in that position known only at runtime.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义张量时，可能会看到一个或多个维度的值为`None`。在这种情况下，完整的形状定义会推迟到执行阶段，因为使用`None`指示TensorFlow在运行时才会知道该位置的值。
- en: Being a C++ library, TensorFlow is strictly statically typed. This means that
    the type of every operation/tensor must be known at graph definition time. Moreover,
    this also means that it is not possible to execute an operation among incompatible
    types.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个C++库，TensorFlow是严格静态类型的。这意味着每个操作/张量的类型必须在图定义时就已知。而且，这也意味着不可能在不兼容的类型之间执行操作。
- en: Looking closely at the baseline example, it is possible to see that both matrix
    multiplication and addition operations are performed on tensors with the same
    type, `tf.float32`. The tensors identified by the Python variables `A` and `b`
    have been defined, making the type clear in the operation definition, while tensor `x`
    has the same `tf.float32` type; but in this case, it has been inferred by the
    Python bindings, which are able to look inside the constant value and infer the
    type to use when creating the operation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察基准示例，可以看到矩阵乘法和加法操作都是在具有相同类型`tf.float32`的张量上进行的。由Python变量`A`和`b`标识的张量已经定义，确保了操作定义时的类型明确，而张量`x`也具有相同的`tf.float32`类型；但在这种情况下，它是由Python绑定推断出来的，Python绑定能够查看常量值并推断出在创建操作时应使用的类型。
- en: Another peculiarity of Python bindings is their simplification in the definition
    of some common mathematical operations using operator overloading. The most common
    mathematical operations have their counterpart as `tf.Operation`; therefore, using
    operator overloading to simplify the graph definition is natural.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Python绑定的另一个特点是，它们简化了一些常见数学操作的定义，采用了操作符重载。最常见的数学操作都有对应的`tf.Operation`；因此，使用操作符重载来简化图定义是很自然的。
- en: 'The following table shows the available operators overloaded in the TensorFlow
    Python API:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了TensorFlow Python API中重载的可用操作符：
- en: '| **Python operator** | **Operation name** |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **Python 操作符** | **操作名称** |'
- en: '| `__neg__` | unary `-` |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `__neg__` | 一元 `-` |'
- en: '| `__abs__` | `abs()` |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `__abs__` | `abs()` |'
- en: '| `__invert__` | unary `~` |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `__invert__` | 一元 `~` |'
- en: '| `__add__` | binary `+` |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `__add__` | 二进制 `+` |'
- en: '| `__sub__` | binary `-` |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `__sub__` | 二进制 `-` |'
- en: '| `__mul__` | binary elementwise `*` |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `__mul__` | 二进制逐元素 `*` |'
- en: '| `__floordiv__` | binary `//` |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `__floordiv__` | 二进制 `//` |'
- en: '| `__truediv__` | binary `/` |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `__truediv__` | 二进制 `/` |'
- en: '| `__mod__` | binary `%` |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `__mod__` | 二进制 `%` |'
- en: '| `__pow__` | binary `**` |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `__pow__` | 二进制 `**` |'
- en: '| `__and__` | binary `&` |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `__and__` | 二进制 `&` |'
- en: '| `__or__` | binary `&#124;` |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `__or__` | 二进制 `&#124;` |'
- en: '| `__xor__` | binary `^` |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `__xor__` | 二进制 `^` |'
- en: '| `__le__` | binary `<` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `__le__` | 二进制 `<` |'
- en: '| `__lt__` | binary `<=` |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `__lt__` | 二进制 `<=` |'
- en: '| `__gt__` | binary `>` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `__gt__` | 二进制 `>` |'
- en: '| `__ge__` | binary `<=` |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `__ge__` | 二进制 `<=` |'
- en: '| `__matmul__` | binary `@` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `__matmul__` | 二进制 `@` |'
- en: 'Operator overloading allows a faster graph definition and is completely equivalent
    to their `tf.*` API call (for example, using `__add__` is the same as using the `tf.add`
    function). There is only one case in which it is beneficial to use the TensorFlow
    API call instead of the associated operator overload: when a name for the operation
    is needed. Usually, when defining a graph, we''re interested in giving meaningful
    names only to the input and output nodes, while any other node can just be automatically
    named by TensorFlow.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符重载允许更快速的图形定义，且与它们的 `tf.*` API 调用完全等效（例如，使用 `__add__` 与使用 `tf.add` 函数是一样的）。只有在需要为操作指定名称时，使用
    TensorFlow API 调用才有优势。通常，在定义图形时，我们只关心给输入和输出节点赋予有意义的名称，而其他节点可以由 TensorFlow 自动命名。
- en: 'Using overloaded operators, we can''t specify the node name and thus the output
    tensor''s name. In fact, in the baseline example, we defined the addition operation
    using the `tf.add` method, because we wanted to give the output tensor a meaningful
    name (result). In practice, these two lines are equivalent:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用重载运算符时，我们无法指定节点名称，因此也无法指定输出张量的名称。事实上，在基准示例中，我们使用 `tf.add` 方法定义加法操作，是因为我们想给输出张量一个有意义的名称（result）。实际上，这两行代码是等效的：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned at the beginning of this section, TensorFlow itself can place specific
    nodes on different devices better suited to the operation execution. The framework
    is so flexible that it allows the user to manually place operations on different
    local and remote devices just using the `tf.device` context manager.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节开头所述，TensorFlow 本身可以将特定的节点放置在更适合执行操作的设备上。该框架非常灵活，允许用户只使用 `tf.device` 上下文管理器将操作手动放置在不同的本地和远程设备上。
- en: Graph placement – tf.device
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图形放置 – tf.device
- en: '`tf.device` creates a context manager that matches a device. The function allows
    the user to request that all operations created within the context it creates
    are placed on the same device. The devices identified by `tf.device` are more
    than physical devices; in fact, it is capable of identifying devices such as remote
    servers, remote devices, remote workers, and different types of physical devices
    (GPUs, CPUs, and TPUs). It is required to follow a device specification to correctly
    instruct the framework to use the desired device. A device specification has the
    following form:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.device` 创建一个上下文管理器，匹配一个设备。该函数允许用户请求在其创建的上下文中创建的所有操作都放置在相同的设备上。由 `tf.device`
    标识的设备不仅仅是物理设备；事实上，它能够识别远程服务器、远程设备、远程工作者以及不同类型的物理设备（GPU、CPU 和 TPU）。必须遵循设备规格来正确指示框架使用所需设备。设备规格的形式如下：'
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Broken down as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 详细说明如下：
- en: '`<JOB_NAME>` is an alpha-numeric string that does not start with a number'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<JOB_NAME>` 是一个字母数字字符串，且不能以数字开头'
- en: '`<DEVICE_TYPE>` is a registered device type (such as GPU or CPU)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<DEVICE_TYPE>` 是已注册的设备类型（例如 GPU 或 CPU）'
- en: '`<TASK_INDEX>` is a non-negative integer representing the index of the task
    in the job named `<JOB_NAME>`'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<TASK_INDEX>` 是一个非负整数，表示名为 `<JOB_NAME>` 的任务索引'
- en: '`<DEVICE_NAME>` is a non-negative integer representing the index of the device;
    for example, `/GPU:0` is the first GPU'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<DEVICE_NAME>` 是一个非负整数，表示设备的索引；例如，`/GPU:0` 是第一个 GPU'
- en: There is no need to specify every part of a device specification. For example,
    when running a single-machine configuration with a single GPU, you might use `tf.device`
    to pin some operations to the CPU and GPU.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要指定设备规格的每一部分。例如，当运行单机配置并且只有一张 GPU 时，你可以使用 `tf.device` 将某些操作固定到 CPU 和 GPU。
- en: We can thus extend our baseline example to place the operations on the device
    we choose. Thus, it is possible to place the matrix multiplication on the GPU,
    since it is hardware optimized for this kind of operation, while keeping all the
    other operations on the CPU.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以扩展我们的基准示例，将操作放置在我们选择的设备上。因此，可以将矩阵乘法放置在 GPU 上，因为它对这种操作进行了硬件优化，同时将其他所有操作保持在
    CPU 上。
- en: 'Please note that since this is only a graph description, there''s no need to
    physically have a GPU or to use the `tensorflow-gpu` package. First, we import
    the TensorFlow library:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于这只是图形描述，因此无需实际拥有 GPU 或使用 `tensorflow-gpu` 包。首先，我们导入 TensorFlow 库：
- en: '`(tf1)`'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, use the context manager to place operations on different devices, first,
    on the first CPU of the local machine:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用上下文管理器将操作放置在不同的设备上，首先是在本地机器的第一个 CPU 上：
- en: '[PRE11]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, on the first GPU of the local machine:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在本地机器的第一个 GPU 上：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'When the device is not forced by a scope, TensorFlow decides which device is
    better to place the operation on:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当设备未通过作用域强制指定时，TensorFlow 会决定哪个设备更适合放置操作：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we define the summary writer:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义总结写入器：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If we look at the generated graph, we''ll see that it is identical to the one
    generated by the baseline example, with two main differences:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看生成的图，我们会看到它与基准示例生成的图完全相同，有两个主要的区别：
- en: Instead of having a meaningful name for the output tensor, we have just the
    default one
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出张量没有有意义的名称，而是使用默认名称
- en: Clicking on the matrix multiplication node, it is possible to see (in TensorBoard)
    that this operation must be executed in the first GPU of the local machine
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点击矩阵乘法节点后，可以在 TensorBoard 中看到该操作必须在本地机器的第一个 GPU 上执行。
- en: 'The `matmul` node is placed on the first GPU of the local machine, while any
    other operation is executed in the CPU. TensorFlow takes care of communication
    among different devices in a transparent manner:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`matmul` 节点被放置在本地机器的第一个 GPU 上，而任何其他操作则在 CPU 上执行。TensorFlow 会以透明的方式处理不同设备之间的通信：'
- en: '![](img/4982764b-9b4f-4813-af3f-ed57cec12b46.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4982764b-9b4f-4813-af3f-ed57cec12b46.png)'
- en: Please also note that even though we have defined constant operations that produce
    constant tensors, their values are not visible among the attributes of the node
    nor among the input/output properties.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管我们已定义产生常量张量的常量操作，但它们的值并未显示在节点的属性中，也未显示在输入/输出属性中。
- en: 'When using the static-graph and session execution parading, the execution is
    completely separated from the graph definition. This is no longer true in eager
    execution, but since, in this chapter, the focus is on the TensorFlow architecture,
    it is worth also focusing on the execution part using `tf.Session`: in TensorFlow
    2.0, the session is still present, but hidden, as we will see in the next chapter,
    [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml), *TensorFlow 2.0 Architecture*.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用静态图和会话执行的模式下，执行与图的定义是完全分离的。然而在 eager 执行模式下，这种情况不再成立，但由于本章的重点是 TensorFlow
    架构，因此也值得关注使用`tf.Session`的执行部分：在 TensorFlow 2.0 中，会话仍然存在，但被隐藏了，正如我们将在下一章[第4章](655b734e-1636-4e11-b944-a71fafacb977.xhtml)，*TensorFlow
    2.0 架构*中看到的那样。
- en: Graph execution – tf.Session
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图执行 - tf.Session
- en: '`tf.Session` is a class that TensorFlow provides to represent a connection
    between the Python program and the C++ runtime.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Session` 是 TensorFlow 提供的一个类，用于表示 Python 程序和 C++ 运行时之间的连接。'
- en: The `tf.Session` object is the only object able to communicate directly with
    the hardware (through the C++ runtime), placing operations on the specified devices,
    using the local and distributed TensorFlow runtime, with the goal of concretely
    building the defined graph. The `tf.Session` object is highly optimized and, once
    correctly built, caches `tf.Graph` in order to speed up its execution.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Session` 对象是唯一能够直接与硬件（通过 C++ 运行时）进行通信的对象，它将操作放置在指定的设备上，使用本地和分布式的 TensorFlow
    运行时，目的是具体构建定义好的图。`tf.Session` 对象经过高度优化，一旦正确构建，它会缓存 `tf.Graph`，以加速执行。'
- en: 'Being the owner of physical resources, the `tf.Session` object must be used
    as a file descriptor to do the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作为物理资源的所有者，`tf.Session` 对象必须像文件描述符一样使用，以执行以下操作：
- en: Acquire the resources by creating a `tf.Session` (the equivalent of the `open` operating
    system call)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建 `tf.Session` 获取资源（相当于操作系统的 `open` 调用）
- en: Use the resources (the equivalent of using the `read/write` operation on the
    file descriptor)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用资源（相当于对文件描述符执行 `read/write` 操作）
- en: Release the resources with `tf.Session.close` (the equivalent of the `close`
    call)
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `tf.Session.close` 释放资源（相当于 `close` 调用）
- en: Typically, instead of manually defining a session and taking care of its creation
    and destruction, a session is used through a context manager that automatically
    closes the session at the block exit.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们通过上下文管理器使用会话，而不是手动定义会话并处理其创建和销毁，这样会话会在代码块退出时自动关闭。
- en: The constructor of `tf.Session` is fairly complex and highly customizable since
    it is used to configure and create the execution of the computational graph.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Session` 的构造函数相当复杂且高度可定制，因为它用于配置和创建计算图的执行。'
- en: 'In the simplest and most common scenario, we just want to use the current local
    hardware to execute the previously described computational graph as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单且最常见的场景中，我们只是希望使用当前本地硬件执行先前描述的计算图，具体如下：
- en: '`(tf1)`'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'There are more complex scenarios in which we wouldn''t want to use the local
    execution engine, but use a remote TensorFlow server that gives access to all
    the devices it controls. This is possible by specifying the `target` parameter
    of `tf.Session` just by using the URL (`grpc://`) of the server:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些更复杂的场景，在这些场景中，我们可能不希望使用本地执行引擎，而是使用远程的TensorFlow服务器，该服务器可以访问它控制的所有设备。通过仅使用服务器的URL（`grpc://`），可以通过`tf.Session`的`target`参数来实现这一点。
- en: '`(tf1)`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE16]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: By default, the `tf.Session` will capture and use the default `tf.Graph` object,
    but when working with multiple graphs, it is possible to specify which graph to
    use by using the `graph` parameter. It's easy to understand why working with multiple
    graphs is unusual, since even the `tf.Session` object is able to work with only
    a single graph at a time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`tf.Session`将捕获并使用默认的`tf.Graph`对象，但在使用多个图时，可以通过`graph`参数指定要使用的图。理解为什么使用多个图不常见是很容易的，因为即使是`tf.Session`对象也只能一次处理一个图。
- en: 'The third and last parameter of the `tf.Session` object is the hardware/network
    configuration specified through the `config` parameter. The configuration is specified
    through the `tf.ConfigProto` object, which is able to control the behavior of
    the session. The `tf.ConfigProto` object is fairly complex and rich with options,
    the most common and widely used being the following two (all the others are options
    used in distributed, complex environments):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Session`对象的第三个参数是通过`config`参数指定的硬件/网络配置。该配置通过`tf.ConfigProto`对象指定，`tf.ConfigProto`对象能够控制会话的行为。`tf.ConfigProto`对象相当复杂，选项丰富，其中最常见和广泛使用的两个选项如下（其他选项通常用于分布式和复杂的环境）：'
- en: '`allow_soft_placement`: If set to `True`, it enables a soft device placement.
    Not every operation can be placed indifferently on the CPU and GPU, because the
    GPU implementation of the operation may be missing, for example, and using this
    option allows TensorFlow to ignore the device specification made via `tf.device`
    and place the operation on the correct device when an unsupported device is specified
    at graph definition time.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`allow_soft_placement`：如果设置为`True`，则启用软设备放置。并不是每个操作都能随意地放置在CPU和GPU上，因为某些操作在GPU上的实现可能缺失，使用此选项可以让TensorFlow忽略通过`tf.device`指定的设备，并在图定义时当指定了不支持的设备时，将操作放置在正确的设备上。'
- en: '`gpu_options.allow_growth`: If set to `True`, it changes the TensorFlow GPU
    memory allocator; the default allocator allocates all the available GPU memory
    as soon as the `tf.Session` is created, while the allocator used when `allow_growth`
    is `True` gradually increases the amount of memory allocated. The default allocator
    works in this way because, in production environments, the physical resources
    are completely dedicated to the `tf.Session` execution, while, in a standard research
    environment, the resources are usually shared (the GPU is a resource that can
    be used by other processes while the TensorFlow `tf.Session` is in execution).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpu_options.allow_growth`：如果设置为`True`，则改变TensorFlow的GPU内存分配器；默认的分配器会在创建`tf.Session`时分配所有可用的GPU内存，而当`allow_growth`设置为`True`时，分配器会逐步增加分配的内存量。默认的分配器采用这种方式，因为在生产环境中，物理资源完全用于`tf.Session`的执行，而在标准研究环境中，资源通常是共享的（GPU是可以被其他进程使用的资源，尽管TensorFlow的`tf.Session`正在执行）。'
- en: 'The baseline example can now be extended to not only define a graph, but to
    proceed on to an effective construction and the execution of it:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基础示例现在可以扩展，不仅定义一个图，还可以继续有效地构建并执行它：
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The first `sess.run` call evaluates the three `tf.Tensor` objects, `A, x, b`,
    and returns their values as `numpy` arrays.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`sess.run`调用会评估三个`tf.Tensor`对象，`A, x, b`，并返回它们的值作为`numpy`数组。
- en: 'The second call, `sess.run(y)`, works in the following way:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次调用，`sess.run(y)`，按照以下方式工作：
- en: '`y` is an output node of an operation: backtrack to its inputs'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`y`是一个操作的输出节点：回溯到它的输入。'
- en: Recursively backtrack through every node until all the nodes without a parent
    are found
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 递归回溯每个节点，直到找到所有没有父节点的节点。
- en: Evaluate the input; in this case, the `A, x, b` tensors
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估输入；在此情况下，为`A, x, b`张量。
- en: 'Follow the dependency graph: the multiplication operation must be executed
    before the addition of its result with `b`'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟随依赖图：乘法操作必须在将其结果与`b`相加之前执行。
- en: Execute the matrix multiplication
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行矩阵乘法。
- en: Execute the addition
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行加法操作。
- en: The addition is the entry point of the graph resolution (Python variable `y`)
    and the computation ends.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 加法是图解析的入口点（Python变量`y`），并且计算结束。
- en: 'The first print call, therefore, produces the following output:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一个打印调用会产生以下输出：
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The third `sess.run` call shows how it is possible to inject into the computational
    graph values from the outside, as `numpy` arrays, overwriting a node. The `feed_dict`
    parameter allows you to do this: usually, inputs are passed to the graph using
    the `feed_dict` parameter and through the overwriting of the `tf.placeholder`
    operation created exactly for this purpose.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个 `sess.run` 调用展示了如何将外部的值作为 `numpy` 数组注入到计算图中，从而覆盖节点。 `feed_dict` 参数允许你这样做：通常，输入是通过 `feed_dict` 参数传递给图的，并通过覆盖为此目的而创建的 `tf.placeholder` 操作。
- en: '`tf.placeholder` is just a placeholder created with the aim of throwing an
    error when values from the outside are not injected inside the graph. However,
    the `feed_dict` parameter is more than just a way to feed the placeholders. In
    fact, the preceding example shows how it can be used to overwrite any node. The
    result produced by the overwriting of the node identified by the Python variable, `b`,
    with a `numpy` array that must be compatible, in terms of both type and shape,
    with the overwritten variable, is as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.placeholder` 只是一个占位符，目的是当外部值没有注入到图中时抛出错误。然而，`feed_dict` 参数不仅仅是传递占位符的方式。事实上，前面的示例展示了它如何被用来覆盖任何节点。通过将节点由
    Python 变量 `b` 指定的变量通过 `numpy` 数组覆盖，且该数组在类型和形状上必须与被覆盖的变量兼容，得到的结果如下：'
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The baseline example has been updated in order to show the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 基准示例已经更新，以展示以下内容：
- en: How to build a graph
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个图
- en: How to save a graphical representation of the graph
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何保存图的图形表示
- en: How to create a session and execute the defined graph
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建一个会话并执行已定义的图
- en: 'So far, we have used graphs with constant values and used the `feed_dict` parameter
    of the `sess.run` call to overwrite a node parameter. However, since TensorFlow
    is designed to solve complex problems, the concept of `tf.Variable` has been introduced:
    every parametric machine learning model can be defined and trained with TensorFlow.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用了具有常量值的图，并通过 `sess.run` 调用的 `feed_dict` 参数覆盖了节点参数。然而，由于 TensorFlow
    被设计用来解决复杂问题，因此引入了 `tf.Variable` 的概念：每个参数化的机器学习模型都可以用 TensorFlow 来定义和训练。
- en: Variables in static graphs
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态图中的变量
- en: A variable is an object that maintains a state in the graph across multiple
    calls to `sess.run`. A variable is added to `tf.Graph` by constructing an instance
    of the `tf.Variable` class.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 变量是一个对象，它在多个 `sess.run` 调用中保持图的状态。通过构造 `tf.Variable` 类的实例，变量会被添加到 `tf.Graph`
    中。
- en: A variable is completely defined by the pair (type, shape), and variables created
    by calling `tf.Variable` can be used as input for other nodes in the graph; in
    fact, the `tf.Tensor` and `tf.Variable` objects can be used in the same manner
    when building a graph.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 变量由（类型，形状）对完全定义，通过调用 `tf.Variable` 创建的变量可以作为图中其他节点的输入；实际上，`tf.Tensor` 和 `tf.Variable`
    对象在构建图时可以以相同的方式使用。
- en: 'Variables have more attributes with respect to tensors: a variable object must
    be initialized and thus have its initializer; a variable is, by default, added
    to the global variables and trainable variable graph collections. If a variable
    is set as non-trainable, it can be used by the graph to store the state, but the
    optimizers will ignore it when performing the learning process.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 变量相对于张量具有更多的属性：变量对象必须初始化，因此必须有其初始化器；变量默认会添加到全局变量和可训练变量的图集合中。如果将变量设置为 非可训练，它可以被图用来存储状态，但优化器在执行学习过程时会忽略它。
- en: 'There are two ways of declaring a variable in a graph: `tf.Variable` and `tf.get_variable`.
    Using `tf.Variable` is easier but less powerful—the second way is more complex
    to use, but has more expressive power.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 声明图中变量有两种方式：`tf.Variable` 和 `tf.get_variable`。使用 `tf.Variable` 更简单，但功能较弱——第二种方法使用起来更复杂，但具有更强的表达能力。
- en: tf.Variable
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.Variable
- en: 'Creating a variable by calling `tf.Variable` will always create a new variable
    and it always requires an initial value to be specified. In the following lines,
    the creation of a variable named `W` with shape `(5, 5, size_in, size_out)` and
    a variable, `B`, with shape `(size_out)` is shown:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `tf.Variable` 创建变量将始终创建一个新的变量，并且始终需要指定初始值。以下几行展示了如何创建一个名为 `W` 的变量，形状为 `(5,
    5, size_in, size_out)`，以及一个名为 `B` 的变量，形状为 `(size_out)`：
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `w` initial value is generated by the `tf.truncated_normal` operation, which
    samples from a normal distribution with 0 mean and a standard deviation of 0.1
    the `5 x 5 x size_in x size_out` (total number) values required to initialize
    the tensor, while `b` is initialized using the constant value of 0.1 generated
    by the `tf.constant` operation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`w`的初始值通过`tf.truncated_normal`操作生成，该操作从均值为0、标准差为0.1的正态分布中采样，生成初始化张量所需的`5 x
    5 x size_in x size_out`（总数）值，而`b`则使用通过`tf.constant`操作生成的常数值0.1进行初始化。'
- en: 'Since each call to `tf.Variable` creates a new variable in the graph, it is
    the perfect candidate for the creation of layers: every layer (for example, a
    convolutional layer/a fully connected layer) definition requires the creation
    of a new variable. For instance, the following lines of code show the definition
    of two functions that can be used to define a convolutional neural network and/or
    a fully connected neural network:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每次调用`tf.Variable`都会在图中创建一个新的变量，它是创建层的完美候选者：每个层（例如卷积层/全连接层）定义都需要创建一个新的变量。例如，以下代码行展示了两个可以用来定义卷积神经网络和/或全连接神经网络的函数定义：
- en: '`(tf1)`'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: 'The first function creates a 2D convolutional layer (with a 5 x 5 kernel) followed
    by a max-pool operation to halve the output''s spatial extent:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数创建了一个2D卷积层（使用5 x 5的卷积核），并随后进行最大池化操作，将输出的空间维度减半：
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The second function defines a fully connected layer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数定义了一个全连接层：
- en: '`(tf1)`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Both functions also use the `tf.summary` module to log the histograms of the
    weight, bias, and activation values, which can change during training.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 两个函数还使用了`tf.summary`模块来记录权重、偏置和激活值的直方图，这些值在训练过程中可能会发生变化。
- en: 'The call to a `tf.summary` method automatically adds the summaries to a global
    collection that is used by `tf.Saver` and `tf.SummaryWriter` objects to log every
    summary value in the TensorBoard log directory:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`tf.summary`方法会自动将摘要添加到一个全局集合中，该集合由`tf.Saver`和`tf.SummaryWriter`对象使用，以将每个摘要值记录到TensorBoard日志目录中：
- en: '`(tf1)`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: A layer definition made in this way is perfect for the most common scenarios
    in which a user wants to define a deep learning model composed by a stack of several
    layers and train it given a single input that flows from the first to the last
    layer.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式定义的层非常适合在用户希望定义一个由多个层堆叠组成的深度学习模型，并根据一个从第一层到最后一层的输入进行训练的最常见场景。
- en: What if, instead, the training phase is not standard and there is the need to
    share the variable's values among different inputs?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练阶段不是标准的，而是需要在不同的输入之间共享变量的值，怎么办？
- en: We need to use the TensorFlow feature called **variable sharing**, which is not
    possible using a layer definition made with `tf.Variable`, so we have to instead
    use the most powerful method, `tf.get_variable`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用TensorFlow的**变量共享**功能，这在使用`tf.Variable`创建的层定义中是不可行的，因此我们必须改用最强大的方法——`tf.get_variable`。
- en: tf.get_variable
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: tf.get_variable
- en: Like `tf.Variable`, `tf.get_variable` also allows the definition and creation
    of new variables. The main difference is that its behavior changes if the variable
    has already been defined.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与`tf.Variable`类似，`tf.get_variable`也允许定义和创建新变量。主要的区别在于，如果变量已经定义，它的行为会发生变化。
- en: '`tf.get_variable` is always used together with `tf.variable_scope` since it
    enables the variable sharing capabilities of `tf.get_variable` through its `reuse`
    parameter. The following example clarifies the concept:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.get_variable`总是与`tf.variable_scope`一起使用，因为它通过`reuse`参数启用`tf.get_variable`的变量共享功能。以下示例阐明了这一概念：'
- en: '`(tf1)`'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE24]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the preceding example, the Python variables `a` and `c` point to the same
    graph variable, named `scope/v:0`. Hence, a layer that uses `tf.get_variable` to
    define variables can be used in conjunction with `tf.variable_scope` to define
    or reuse the layer's variables. This is extremely useful and powerful when training
    generative models using adversarial training, as we will see in [Chapter 9](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml), *Generative
    Adversarial Networks*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，Python变量`a`和`c`指向相同的图变量，名为`scope/v:0`。因此，使用`tf.get_variable`定义变量的层可以与`tf.variable_scope`结合使用，以定义或重用该层的变量。当使用对抗训练训练生成模型时，这非常有用且强大，我们将在[第9章](66948c53-131c-43ef-a7fc-3d242d1e0664.xhtml)中详细讨论*生成对抗网络*。
- en: 'Different from `tf.Variable`, in this case, we can''t pass an initial value
    in a raw way (passing the value directly as input to the `call `method ); we always
    have to explicitly use an initializer. The previously defined layer can be written
    using `tf.get_variable` (and this is the recommended way to define variables)
    as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `tf.Variable` 不同，在这种情况下，我们不能直接传递初始值（将值直接作为输入传递给 `call` 方法）；我们必须始终显式使用初始化器。之前定义的层可以通过
    `tf.get_variable` 编写（这是定义变量的推荐方式），如下所示：
- en: '`(tf1)`'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE25]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Invoking `conv2D` or `fc` defines the variables needed to define a layer in
    the current scope; hence, to define two convolutional layers without having naming
    conflicts, `tf.variable_scope` must be used:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `conv2D` 或 `fc` 会定义当前作用域内所需的变量，因此，为了定义两个卷积层而不发生命名冲突，必须使用 `tf.variable_scope`：
- en: '[PRE26]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Manually defining layers is a good exercise, and knowing that TensorFlow has
    all the primitives required to define every ML layer is something every ML practitioner
    should know. However, manually defining every single layer is tedious and repetitive
    (we need fully connected, convolutional, dropout, and batch normalization layers
    in almost every project), and, for this reason, TensorFlow already comes with
    a module named `tf.layers`, which contains all the most common and widely used
    layers, defined using `tf.get_variable` under the hood, and therefore, layers
    can be used in conjunction with `tf.variable_scope` to share their variables.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 手动定义层是一个很好的练习，了解 TensorFlow 提供了定义每个机器学习层所需的所有原语是每个机器学习从业者应该知道的事情。然而，手动定义每一层是乏味且重复的（我们几乎每个项目都需要全连接层、卷积层、dropout
    层和批量归一化层），因此，TensorFlow 已经提供了一个名为 `tf.layers` 的模块，其中包含所有最常用且广泛使用的层，这些层在底层使用 `tf.get_variable`
    定义，因此，可以与 `tf.variable_scope` 一起使用，以共享它们的变量。
- en: Model definition and training
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型定义与训练
- en: 'Disclaimer: the layer module has been completely removed in TensorFlow 2.0,
    and the layer definition using `tf.keras.layers` is the new standard; however,
    an overview of `tf.layers` is still worth reading because it shows how reasoning
    layer by layer to define deep models is the natural way to proceed and it also
    gives us an idea of the reasons behind the migration from `tf.layers` to `tf.keras.layers`.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 免责声明：在 TensorFlow 2.0 中，层模块已被完全移除，使用 `tf.keras.layers` 定义层是新的标准；然而，`tf.layers`
    的概述仍然值得阅读，因为它展示了按层逐步推理定义深度模型是自然的做法，并且也让我们了解了从 `tf.layers` 到 `tf.keras.layers`
    迁移背后的原因。
- en: Defining models with tf.layers
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf.layers 定义模型
- en: 'As shown in the previous section, TensorFlow provides all the primitive features
    to define a neural network layer: the user should take care when defining the
    variables, the operation nodes, the activation functions, and the logging, and
    define a proper interface to handle all cases (adding, or not, the bias term,
    adding regularization to the layer parameters, and so on).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一节所示，TensorFlow 提供了定义神经网络层所需的所有原始功能：用户在定义变量、操作节点、激活函数和日志记录时需要小心，并定义一个适当的接口来处理所有情况（例如，添加或不添加偏置项、为层参数添加正则化等）。
- en: The `tf.layers` module in TensorFlow 1.x and the `tf.keras.layers` module in
    TensorFlow 2.0 provide an excellent API to define machine learning models in a
    convenient and powerful way. Every layer in `tf.layers`, defines variables using `tf.get_variable`,
    and therefore, each layer defined in this way can use the variable-sharing features
    provided by `tf.variable_scope`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 1.x 中的 `tf.layers` 模块和 TensorFlow 2.0 中的 `tf.keras.layers` 模块提供了一个出色的
    API，以便以方便且强大的方式定义机器学习模型。每个 `tf.layers` 中的层都使用 `tf.get_variable` 来定义变量，因此，用这种方式定义的每个层都可以使用
    `tf.variable_scope` 提供的变量共享功能。
- en: 'The previously manually defined 2D convolution and fully connected layers are
    clearly present and well-defined in `tf.layers` and using them to define a LeNet-like
    CNN is easy, as shown. First, we define a convolutional neural network for classification:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 之前手动定义的 2D 卷积层和全连接层在 `tf.layers` 中得到了清晰的呈现，并且使用它们来定义类似 LeNet 的卷积神经网络非常容易，如所示。首先，我们定义一个用于分类的卷积神经网络：
- en: '`(tf1)`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE27]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we flatten the data to a 1D vector so that we can use a fully connected
    layer. Please note how the new shape is computed and the negative dimension in
    the batch size position:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将数据展平为一个 1D 向量，以便使用全连接层。请注意新形状的计算方式，以及批量大小位置的负维度：
- en: '[PRE28]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Being high-level wrappers on primitive TensorFlow operations, there is no need
    to detail what every layer does in this book since it is pretty clear from the
    layer names themselves and from the documentation. The reader is invited to become
    familiar with the official TensorFlow documentation, and, in particular, to try
    to define their own classification model using layers: [https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)[.](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些是 TensorFlow 原始操作的高级封装，本书无需详细说明每一层的功能，因为从层的名称和文档中已经能清楚地了解。读者被邀请熟悉官方的 TensorFlow
    文档，特别是尝试使用层来定义自己的分类模型：[https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)[.](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/layers)
- en: Using the baseline example and replacing the graph with this CNN definition,
    using TensorFlow, it is possible to see how every layer has its own scope, how
    the layers are connected among them, and, as shown in the second diagram, by double-clicking
    on a layer, it is possible to see its contents to understand how it is implemented
    without having to look at the code.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基线示例并将图替换为此 CNN 定义，使用 TensorFlow 可以看到每一层都有自己的作用域，各层之间是如何连接的，并且正如第二张图所示，通过双击某一层，可以查看其内容，了解其实现方式，而无需查看代码。
- en: 'The following diagram shows the architecture of the defined LeNet-like CNN.
    The whole architecture is placed under the *cnn* scope; the input node is a placeholder.
    It is possible to visualize how the layers are connected and how TensorFlow added
    the `_1` suffix to blocks with the same name to avoid conflicts:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了定义的类似 LeNet 的 CNN 架构。整个架构位于*cnn*作用域下，输入节点是一个占位符。可以直观地看到各层如何连接，以及 TensorFlow
    如何通过在相同名称的模块后添加`_1`后缀来避免命名冲突：
- en: '![](img/c6aedc60-0542-4150-8377-b04cad633ac1.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6aedc60-0542-4150-8377-b04cad633ac1.png)'
- en: 'Double-clicking on the `conv2d` block allows you to analyze how the various
    components defined by the layers are connected to each other. Please note how,
    different from our layer implementation, the TensorFlow developers used an operation
    named `BiasAdd` to add the bias and not the raw `Add` operation. The behavior
    is the same, but the semantics are clearer:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 双击 `conv2d` 块可以分析各层定义的不同组件是如何相互连接的。请注意，与我们的层实现不同，TensorFlow 开发者使用了名为`BiasAdd`的操作来添加偏置，而不是直接使用原始的`Add`操作。虽然行为相同，但语义更为清晰：
- en: '![](img/40e6d1cd-926e-4334-bc2c-1f8b77660c67.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40e6d1cd-926e-4334-bc2c-1f8b77660c67.png)'
- en: As an exercise, you can try to extend the baseline by defining a CNN like the
    one just presented to visualize and understand the layer structure.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，你可以尝试通过定义一个类似刚刚展示的 CNN 来扩展基线，进而可视化并理解层的结构。
- en: We always have to keep in mind that TensorFlow 1.x follows the graph definition
    and session execution approach. This means that even the training phase should
    be described within the same `tf.Graph` object before being executed.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须始终记住，TensorFlow 1.x 遵循图定义和会话执行的方法。这意味着即使是训练阶段，也应该在同一个`tf.Graph`对象中描述，然后才能执行。
- en: Automatic differentiation – losses and optimizers
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动微分 – 损失函数和优化器
- en: TensorFlow uses automatic differentiation—a differentiator is an object that
    contains all the rules required to build a new graph that takes the derivative
    of each node it traverses. The `tf.train` module in TensorFlow 1.x contains the
    most widely used type of differentiator, called optimizers here. In the module,
    among the other optimizers, it is possible to find the ADAM optimizer as `tf.train.AdamOptimizer` and
    the standard gradient descent optimizer as `tf.train.GradientDescentOptimizer`.
    Each optimizer is an object that implements a common interface. The interface
    standardizes how to use an optimizer to train a model. Performing a mini-batch
    gradient descent step is just a matter of executing a train operation in a Python
    loop; that is, an operation returned by the `.minimize` method of every optimizer.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 使用自动微分——微分器是一个包含所有必要规则的对象，用于构建一个新的图，该图对每个经过的节点计算其导数。TensorFlow 1.x
    中的 `tf.train` 模块包含最常用的微分器类型，这些微分器在这里被称为优化器。在该模块中，除了其他优化器，还可以找到 ADAM 优化器（`tf.train.AdamOptimizer`）和标准的梯度下降优化器（`tf.train.GradientDescentOptimizer`）。每个优化器都是实现了通用接口的对象。该接口标准化了如何使用优化器来训练模型。执行一个小批量梯度下降步骤仅仅是执行
    Python 循环中的训练操作；也就是说，执行每个优化器的`.minimize`方法返回的操作。
- en: 'As you will know from the theory presented in the previous chapter, [Chapter
    2](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml), *Neural Networks and Deep Learning, *to
    train a classifier using cross-entropy loss, it is necessary to one-hot encode
    the labels. TensorFlow has a module, `tf.losses`, that contains the most commonly
    used loss functions that are also capable of performing the one-hot encoding of
    labels by themselves. Moreover, every loss function expects the `logits` tensor
    as input; that is, the linear output of the model without the application of the
    softmax/sigmoid activation function. The name of the `logits` tensor is a TensorFlow
    design choice: it is called in this way even if no sigmoidal transformation has
    been applied to it (a better choice would be naming this parameter `unscaled_logits`).'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从前一章中呈现的理论中所了解的，[第2章](ad05a948-1703-460a-afaf-2bf1fcdfba5a.xhtml)，*神经网络与深度学习*，要使用交叉熵损失训练分类器，必须对标签进行独热编码。TensorFlow
    有一个模块，`tf.losses`，其中包含了最常用的损失函数，它们也能够自行执行标签的独热编码。此外，每个损失函数都期望接收 `logits` 张量作为输入；即模型的线性输出，未应用
    softmax/sigmoid 激活函数。`logits` 张量的名称是 TensorFlow 的设计选择：即使没有对其应用 sigmoidal 转换，它仍然被这样称呼（一个更好的选择是将这个参数命名为
    `unscaled_logits`）。
- en: The reason for this choice is to let the user focus on the network design without
    having to worry about the numerical instability problems that could arise when
    computing certain loss functions; in fact, every loss defined in the `tf.losses`
    module is numerically stable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这种选择的原因是让用户专注于网络设计，而不必担心计算某些损失函数时可能出现的数值不稳定问题；实际上，`tf.losses` 模块中定义的每个损失函数在数值上都是稳定的。
- en: In order to have a complete understanding of the topic and to show that an optimizer
    just builds a graph connected to the previous one (it only adds nodes in practice),
    it is possible to mix the baseline example that logs the graph together with the
    example that defines the network with its loss function and an optimizer.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整理解该主题，并展示优化器只是构建一个与前一个图连接的图（实际上它只添加节点），可以将记录图的基准示例与定义网络、损失函数和优化器的示例结合起来。
- en: 'Thus, the previous example can be modified as follows. To define the input
    placeholder for the labels, we can define the loss function (`tf.losses.sparse_softmax_cross_entropy`)
    and instantiate the ADAM optimizer to minimize it:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以按如下方式修改前面的示例。为了定义标签的输入占位符，我们可以定义损失函数（`tf.losses.sparse_softmax_cross_entropy`），并实例化
    ADAM 优化器以最小化它：
- en: '[PRE29]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'TensorBoard allows us to visualize the graph built as shown:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 允许我们可视化构建的图，如下所示：
- en: '![](img/b8f0e238-3feb-4ad1-9816-bf5f0c3d6fc2.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8f0e238-3feb-4ad1-9816-bf5f0c3d6fc2.png)'
- en: The preceding diagram shows the structure of the graph when a loss function
    is defined, and the `.minimize` method is invoked.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了在定义损失函数并调用 `.minimize` 方法时图的结构。
- en: 'The ADAM optimizer is a separate block that only has inputs—the model (*cnn*),
    the gradients, and the nontrainable *beta1* and *beta2* parameters used by ADAM
    (and specified in its constructor, left at their default values in this case).
    The gradients, as you will know from the theory, are computed with respect to
    the model parameters to minimize the loss function: the graph on the left perfectly
    describes this construction. The gradient block created by the minimize method
    invocation is a named scope, and as such, it can be analyzed by double-clicking
    on it just like any other block in TensorBoard.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ADAM 优化器是一个单独的块，它只有输入——模型（*cnn*）、梯度，以及 ADAM 使用的不可训练的 *beta1* 和 *beta2* 参数（在此情况下，它们在构造函数中指定并保持默认值）。如你从理论中所知，梯度是相对于模型参数计算的，以最小化损失函数：左侧的图完全描述了这个构造。由
    minimize 方法调用创建的梯度块是一个命名范围，因此，可以通过双击它来分析，就像分析 TensorBoard 中的任何其他块一样。
- en: 'The following graph shows the gradient block expanded: it contains a mirrored
    structure of the graph used for the forward model. Every block the optimizer uses
    to optimize the parameters is an input of the gradient block. The gradients are
    the input of the optimizer (ADAM):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了扩展后的梯度块：它包含一个与用于前向模型的图的镜像结构。优化器用来优化参数的每个块都是梯度块的输入。梯度是优化器（ADAM）的输入：
- en: '![](img/932f5a8d-ca86-41f6-96c6-51aa85c5dba8.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/932f5a8d-ca86-41f6-96c6-51aa85c5dba8.png)'
- en: Following on from the theory, the differentiator (optimizer) created a new graph
    that mirrors the original graph; this second graph, inside the gradient block,
    performs the gradient calculation. The optimizer uses the gradients produced to
    apply the variables update rule it defines and implements the learning process.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 继理论之后，区分器（优化器）创建了一个新的图形，它与原始图形相似；这个第二个图形在梯度块内部执行梯度计算。优化器利用生成的梯度来应用它定义的变量更新规则并实施学习过程。
- en: 'A brief recap on what we have seen so far for the static-graph and session
    execution is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 关于静态图和会话执行，我们到目前为止所看到的内容的简要回顾如下：
- en: Define the model inputs using placeholders or other optimized methods, as shown
    in later chapters
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用占位符或其他优化方法定义模型输入，如后续章节所示
- en: Define the model as a function of the input
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型定义为输入的函数
- en: Define the loss function as a function of the model output
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失函数定义为模型输出的函数
- en: Define the optimizer and invoke the `.minimize` method to define the gradient
    computation graph
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义优化器并调用`.minimize`方法以定义梯度计算图
- en: 'These four steps allow us to define a simple training loop and train our model.
    However, we''re skipping some important parts:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个步骤使我们能够定义一个简单的训练循环并训练我们的模型。然而，我们跳过了一些重要部分：
- en: Model performance measurement on training and validation sets
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练集和验证集上的模型性能评估
- en: Saving the model parameters
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存模型参数
- en: Model selection
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型选择
- en: 'Moreover, since the input is defined using placeholders, we have to take care
    of everything related to the input: splitting the dataset, creating the mini-batches,
    keeping track of the training epochs, and so on.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于输入是通过占位符定义的，我们必须处理与输入相关的所有事情：拆分数据集、创建小批量、跟踪训练周期等。
- en: TensorFlow 2.0 with `tfds` (TensorFlow Datasets) simplified and standardized
    the input pipeline definition, as we will see in later chapters; however, having
    a clear idea about what happens under the hood is always an advantage, therefore,
    it's a good exercise for the reader to continue with the following low-level use
    of placeholders in order to have a better understanding of the problems `tfds` solves.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 与 `tfds`（TensorFlow 数据集）简化并标准化了输入管道的定义，正如我们将在后续章节中看到的那样；然而，清楚地了解底层发生的事情总是有优势的，因此，继续使用占位符的低级实现对读者来说是一个很好的练习，这有助于更好地理解
    `tfds` 解决了哪些问题。
- en: So far, you should have a clear understanding of the operations that must be
    executed in a computational graph, and you should have understood that Python
    is used only to build a graph and to do non-learning related operations (hence,
    it's not Python that performs the variable update, but it is the execution within
    a session of a Python variable that represents the training operation that triggers
    all the required operations to make the model learn).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该清楚地了解在计算图中必须执行的操作，并且应该明白 Python 仅用于构建图形和执行与学习无关的操作（因此，并不是 Python 执行变量更新，而是表示训练操作的
    Python 变量在会话中执行，从而触发所有必需的操作使得模型能够学习）。
- en: In the next section, the previous CNN example is extended, adding all the functionality
    on the Python-side that is required to perform model selection (saving the model,
    measuring performance, making the training loop, and feeding the input placeholders).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，扩展前面的 CNN 示例，增加在 Python 端执行模型选择所需的所有功能（保存模型、衡量性能、建立训练循环并喂入输入占位符）。
- en: Interacting with the graph using Python
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 与图形交互
- en: Python is the language of choice to train a TensorFlow model; however, after
    defining a computational graph in Python, there are no constraints regarding using
    it with another language to execute the learning operations defined.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是训练 TensorFlow 模型的首选语言；然而，在 Python 中定义了计算图之后，使用其他语言来执行已定义的学习操作并没有限制。
- en: Always keep in mind that we use Python to define a graph and this definition
    can be exported in a portable and language-agnostic representation (Protobuf)—this
    representation can then be used in any other language to create a concrete graph
    and using it within a session.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 始终牢记，我们使用 Python 来定义一个图形，这个定义可以导出为便携式且与语言无关的表示（Protobuf）——该表示可以在任何其他语言中使用，用来创建具体的图形并在会话中使用。
- en: The TensorFlow Python API is complete and easy to use. Therefore, we can extend
    the previous example to measure the accuracy (defining the accuracy measurement
    operation in the graph) and use this metric to perform model selection.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的Python API非常完整且易于使用。因此，我们可以扩展前面的示例来衡量准确度（在图中定义准确度度量操作），并使用该指标进行模型选择。
- en: Selecting the best model means storing the model parameters at the end of each
    epoch and moving the parameters that produced the highest metric value in a different
    folder. To do this, we have to define the input pipeline in Python and use the
    Python interpreter to interact with the graph.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳模型意味着在每个训练周期结束时存储模型参数，并将产生最高指标值的参数移至不同的文件夹。为此，我们必须在Python中定义输入管道，并使用Python解释器与图进行交互。
- en: Feeding placeholders
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供占位符
- en: As mentioned in the previous section, placeholders are the easiest to use but
    are also the least performant and most error-prone way to build a data input pipeline.
    In later chapters, a better, highly efficient solution will be presented. This
    highly efficient solution has the complete input pipeline completely defined inside
    the graph. However, the placeholder solution is not only the easiest but also
    the only one that can be used in certain scenarios (for example, when training
    reinforcement learning agents, input via a placeholder is the preferred solution).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，占位符是最容易使用的方式，但也是性能最差且最容易出错的构建数据输入管道的方式。在后续章节中，将介绍一种更好且高效的解决方案。这个高效的解决方案在图中完全定义了整个输入管道。然而，占位符解决方案不仅是最简单的，而且在某些场景下（例如，在训练强化学习代理时，使用占位符进行输入是首选解决方案）也是唯一可用的。
- en: In [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What is Machine
    Learning?*, the Fashion-MNIST dataset was described, and we're now going to use
    it as the input dataset for our model—the previously defined CNN will be used
    to classify the fashion items.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml)《什么是机器学习？》中，描述了Fashion-MNIST数据集，我们现在将把它作为模型的输入数据集——之前定义的CNN将用于分类时尚物品。
- en: 'Fortunately, we don''t have to worry about the dataset download and processing
    part, since TensorFlow, in its `keras` module, already has a function that downloads
    and processes the dataset for us to have the training images and the test images
    together with their labels in the expected form (28 x 28 images):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要担心数据集下载和处理部分，因为TensorFlow在其`keras`模块中已经有一个函数，能够下载和处理数据集，以便我们获得训练图像和测试图像及其标签，并且它们的形式符合预期（28
    x 28图像）：
- en: '`(tf1)`'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE30]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`train_x` and `test_x` contain the whole dataset—training a model using a single
    batch containing the complete dataset is not tractable on a standard computer;
    therefore, using Python, we have to take care when splitting the dataset and building
    mini-batches to make the training process affordable.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_x`和`test_x`包含整个数据集——使用包含完整数据集的单个批次来训练模型在标准计算机上不可行；因此，使用Python时，我们必须在划分数据集和构建小批量时小心，以便让训练过程变得可负担。'
- en: 'Let''s say we want to train the model for 10 epochs using batches of 32 elements
    each; it is easy to compute the number of batches needed to train the model for
    an epoch and then run a training loop that iterates over the batches:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要训练模型10个周期，每个周期使用32个元素的批次；计算训练一个周期所需的批次数量非常容易，然后运行一个训练循环，遍历这些批次：
- en: '`(tf1)`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE31]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Of course, since we need to perform model selection, we need to first define
    an operation that computes the accuracy as a function of the model and the input
    and then use the `tf.summary.SummaryWriter` object to write the train and validation
    accuracy on the same graph.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，由于我们需要进行模型选择，我们首先需要定义一个操作，该操作根据模型和输入计算准确度，然后使用`tf.summary.SummaryWriter`对象将训练和验证的准确度写入同一个图中。
- en: Writing summaries
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写摘要
- en: The baseline example already uses a `tf.summary.SummaryWriter` object to write
    the graph in the log directory and make it appear in the graph section of TensorBoard.
    However, `SummaryWriter` can be used to write not only the graph but also a histogram,
    scalar values, distributions, log images, and many other data types.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 基线示例已经使用了`tf.summary.SummaryWriter`对象，将图写入日志目录，并使其出现在TensorBoard的图形部分。然而，`SummaryWriter`不仅可以用来写入图，还可以写入直方图、标量值、分布、日志图像和许多其他数据类型。
- en: The `tf.summary` package is filled with easy-to-use methods to log any data.
    For instance, we are interested in logging the loss value; the loss value is a
    scalar and, therefore, `tf.summary.scalar` is the method to use. The package is
    well-documented, and you should take the time to explore it: [https://www.tensorflow.org/versions/r1.15/api_docs/python/tf](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf)[.](https://www.tensorflow.org/versions/r1.15/api_docs/Python/tf)
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.summary`包包含了易于使用的方法来记录任何数据。例如，我们对记录损失值感兴趣；损失值是一个标量，因此`tf.summary.scalar`是我们要使用的方法。该包文档齐全，你应该花时间去探索它：[https://www.tensorflow.org/versions/r1.15/api_docs/python/tf](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf)[.](https://www.tensorflow.org/versions/r1.15/api_docs/Python/tf)'
- en: To extend the previous example, we can define the accuracy operation as a function
    of the input placeholders. In this way, we can run the same operation, changing
    the input when needed. For instance, we could be interested in measuring both
    the training and validation accuracy at the end of each training epoch.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展前面的示例，我们可以将准确度操作定义为输入占位符的函数。通过这种方式，我们可以在需要时更改输入，运行相同的操作。例如，我们可能希望在每个训练周期结束时，测量训练准确度和验证准确度。
- en: 'The same reasoning applies to the loss value: defining the loss as a function
    of the model and the model as a function of a placeholder, we are able to measure
    how the loss changes on the training and validation input just by changing the
    input:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的推理也适用于损失值：将损失定义为模型的函数，模型又是占位符的函数，我们可以通过更改输入来衡量损失在训练和验证输入上的变化：
- en: '`(tf1)`'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE32]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A single `tf.train.FileWriter` object is associated with a unique path on the
    disk, called **run**. A run represents a different configuration of the current
    experiment. For example, the default run is usually the training phase. At this
    phase, hence at this run, the metrics attached (loss, accuracy, logs of images,
    and so on) are measured during the training phase, on the training set.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 单个`tf.train.FileWriter`对象与磁盘上的一个唯一路径相关联，称为**run**。一个run表示当前实验的不同配置。例如，默认的run通常是训练阶段。在这个阶段，因此在这个run中，附加的度量（损失、准确率、图像日志等）是在训练阶段，对训练集进行度量的。
- en: A different run can be created by creating a new `tf.train.FileWriter` with
    a different path associated with it, but with the same root of the other (training) `FileWriter`.
    In this way, using TensorBoard, we can visualize different curves on the same
    graph; for example, visualizing the validation accuracy and the training accuracy
    on the same plot. This feature is of extreme importance when analyzing the behavior
    of an experiment and when you are interested in comparing different experiments
    at a glance.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过创建一个新的`tf.train.FileWriter`，并与之关联一个不同的路径来创建一个不同的run，但它与另一个（训练）`FileWriter`的根路径相同。通过这种方式，使用TensorBoard，我们可以在同一图表上可视化不同的曲线；例如，在同一图表上可视化验证准确度和训练准确度。当分析实验行为时，或当你希望一目了然地比较不同实验时，这个特性非常重要。
- en: 'Hence, since we want to visualize the training and the validation curves on
    the same plot, we can create two different writers:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然我们希望在同一个图表中可视化训练曲线和验证曲线，我们可以创建两个不同的写入器：
- en: '[PRE33]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The first one is the train phase writer; the second, the validation phase one.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是训练阶段的写入器；第二个是验证阶段的写入器。
- en: 'Now, potentially, we could measure the validation accuracy and the training
    accuracy, just by running the `accuracy` tensor, changing the input placeholder
    values accordingly; this means that we are already able to perform model selection:
    the model with the highest validation accuracy is the one to select.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，理论上，我们可以通过运行`accuracy`张量来测量验证准确率和训练准确率，并相应地更改输入占位符的值；这意味着我们已经能够执行模型选择：选择验证准确率最高的模型。
- en: To save the model parameters, a `tf.Saver` object is required.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存模型参数，需要一个`tf.Saver`对象。
- en: Saving model parameters and model selection
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存模型参数和模型选择
- en: Saving model parameters is important since it's the only way to continue to
    train a model after an interruption, and the only way to checkpoint a model status
    for any reason—training finished, the model reached the best validation performance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 保存模型参数很重要，因为这是在中断后继续训练模型的唯一方式，也是保存模型状态的唯一方法，原因可以是训练结束，或者模型达到了最佳验证性能。
- en: '`tf.Saver` is the object the TensorFlow Python API provides to save the current
    model variables. Please note that the `tf.Saver` object saves the variables only
    and not the graph structure!'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Saver` 是 TensorFlow Python API 提供的对象，用于保存当前模型的变量。请注意，`tf.Saver` 对象只保存变量，而不保存图结构！'
- en: To save both the graph structure and variables, a `SavedModel` object is required;
    however, since the `SavedModel` object is more connected with putting a trained
    model into production, its definition and usage are demanded to the paragraph
    dedicated to the production.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存图结构和变量，必须使用 `SavedModel` 对象；然而，由于 `SavedModel` 对象与将训练好的模型投入生产更为相关，它的定义和使用将涉及到专门介绍生产的段落。
- en: The `tf.Saver` object saves the list of the trainable variables plus any other
    nontrainable variables specified in its constructor. Once created, the object
    provides the `save` method, which accepts the path used to store the variables.
    A single `Saver` object can be used to create several checkpoints and thus save
    the model that reached the top performance on the validation metric in a different
    path, in order to perform model selection.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.Saver` 对象保存可训练变量列表以及其构造函数中指定的任何其他不可训练变量。创建后，该对象提供了 `save` 方法，接受用于存储变量的路径。单个
    `Saver` 对象可以用来创建多个检查点，从而在不同路径中保存在验证指标上达到最佳性能的模型，以进行模型选择。'
- en: Moreover, the `Saver` object offers the `restore` method, which can be used
    to populate the variables of the previously defined graph, before starting to
    train them, to restart an interrupted training phase. Eventually, it is possible
    to specify the list of the variables to restore from the checkpoint in the restore
    call, making it possible to use pre-trained layers and fine-tune them. The `tf.Saver`
    is the main object involved when doing transfer learning and fine-tuning a model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`Saver` 对象提供了 `restore` 方法，可以用来填充先前定义图的变量，在开始训练之前，重新启动一个中断的训练阶段。最终，可以在恢复调用中指定要从检查点恢复的变量列表，从而可以使用预训练的层并对其进行微调。`tf.Saver`
    是进行迁移学习和微调模型时的主要对象。
- en: The previous example can thus be extended to perform logging of the measured
    training/validation accuracy in TensorBoard (in the code, the accuracy is measured
    on a batch of 128 elements at the end of each epoch), the training/validation
    loss, and to perform model selection using the measured validation accuracy and
    a new saver.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的示例可以扩展以在 TensorBoard 中执行测量的训练/验证准确度的记录（在代码中，准确度是基于每个 epoch 结束时 128 个元素的批次进行测量的），训练/验证损失，并使用测量的验证准确度和新的保存器进行模型选择。
- en: 'You are invited to analyze and run the complete example to completely understand
    how every presented object works in detail. For any additional tests, always keep
    the TensorFlow API reference and documentation open and try everything:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们邀请你分析并运行完整的示例，深入了解每个展示对象如何详细工作。对于任何额外的测试，始终保持 TensorFlow API 参考和文档处于打开状态，并尝试所有内容：
- en: '`(tf1)`'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`(tf1)`'
- en: '[PRE34]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The result, as seen in TensorBoard, is shown in the following two screenshots.
    The first one shows that, by using two different writers, it is possible to write
    two different curves on the same plot; while the second one shows the graph tab:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如下两张截图所示，在 TensorBoard 中查看的结果。第一张展示了通过使用两个不同的写入器，可以在同一图上绘制两条不同的曲线；第二张展示了图表选项卡：
- en: '![](img/45b985d8-952d-479c-bf17-40455e27ab0d.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45b985d8-952d-479c-bf17-40455e27ab0d.png)'
- en: Using two `SummaryWriter`, it's possible to draw different curves on the same
    plot. The graph on top is the validation graph; the one on the bottom is the loss
    graph. Orange is the color of the training run, while blue is validation.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两个 `SummaryWriter`，可以在同一图表上绘制不同的曲线。顶部的图表是验证图；底部的是损失图。橙色表示训练运行，而蓝色表示验证。
- en: '![](img/76b43e22-a414-4379-b2a2-6768af93ee5e.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76b43e22-a414-4379-b2a2-6768af93ee5e.png)'
- en: The resulting graph—please note how proper use of the variable scopes makes
    the graph easy to read and understand
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图——请注意，如何正确使用变量作用域使得图表易于阅读和理解。
- en: It is worth noting that, even if trained for only a few epochs, the model defined
    already reaches notable performance, although it should be clear from the accuracy
    plot that it suffers from overfitting.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，即使只训练了几个 epoch，已定义的模型已经达到了显著的性能，尽管从准确度图中可以明显看出它存在过拟合问题。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we analyzed how TensorFlow works under the hood—the separation
    between the graph definition phase and its execution within a session, how to
    use the Python API to interact with a graph, and how to define a model and measure
    the metrics during training.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们分析了 TensorFlow 的底层工作原理——图定义阶段与会话中的执行之间的分离，如何使用 Python API 与图进行交互，以及如何定义模型并在训练期间度量指标。
- en: It's worth noting that this chapter analyzed how TensorFlow works in its static
    graph version, which is no longer the default in TensorFlow 2.0; however, the
    graph is still present and even when used in eager mode, every API call produces
    operations that can be executed inside a graph to speed up execution. As will
    be shown in the next chapter, TensorFlow 2.0 still allows models to be defined
    in static graph mode, especially when defining models using the Estimator API.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本章分析了 TensorFlow 在其静态图版本中的工作原理，但该版本不再是 TensorFlow 2.0 的默认版本；然而，图仍然存在，即使在使用急切执行模式时，每个
    API 调用也会产生可以在图中执行的操作，从而加速执行。正如下一章所示，TensorFlow 2.0 仍允许在静态图模式下定义模型，特别是在使用 Estimator
    API 定义模型时。
- en: Having knowledge of graph representation is of fundamental importance, and having
    at least an intuitive idea about the advantages that representing computation
    using dataflow graphs brings should make it clear why TensorFlow scales so well,
    even in huge, complex environments such as Google data centers.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 了解图表示法至关重要，并且至少对表示计算的优势有一个直观的理解，能够让你清楚为什么 TensorFlow 在巨大复杂的环境中，如 Google 数据中心，能够如此高效地扩展。
- en: The exercise section is incredibly important—it asks you to solve problems not
    introduced in the previous sections because this is the only way to become familiar
    with the TensorFlow documentation and code base. Keep track of the time it takes
    you to solve every exercise and try to figure out the solution by yourself with
    only the help of the TensorFlow documentation and some Stack Overflow questions!
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 练习部分非常重要——它要求你解决前面部分没有介绍的问题，因为这是熟悉 TensorFlow 文档和代码库的唯一途径。记录下每个练习你解决所花的时间，并尽量依靠
    TensorFlow 文档和一些 Stack Overflow 问题自行找出解决方案！
- en: 'In the next chapter, [Chapter 4](655b734e-1636-4e11-b944-a71fafacb977.xhtml), *TensorFlow
    2.0 Architecture**,* you''ll deep dive into the TensorFlow 2.0 world: eager mode;
    automatic graph conversion; a better, cleaner code base; and a Keras-based approach.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第 4 章](655b734e-1636-4e11-b944-a71fafacb977.xhtml)，*TensorFlow 2.0 架构*，你将深入探索
    TensorFlow 2.0 世界：急切执行模式；自动图转换；更好、更清晰的代码库；以及基于 Keras 的方法。
- en: Exercises
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Why is it possible to assess that the model suffers from overfitting only by
    looking at the graph?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么仅通过查看图就能评估模型是否存在过拟合？
- en: Extend the baseline example to place the matrix multiplication operation on
    a remote device at IP 192.168.1.12; visualize the result on TensorBoard.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展基线示例，将矩阵乘法操作放置在远程设备 IP 地址为 192.168.1.12 的设备上；并在 TensorBoard 上可视化结果。
- en: Is it necessary to have a remote device to place an operation on?
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否需要远程设备来放置操作？
- en: 'Extend the CNN architecture defined in the `define_cnn` method: add a batch
    normalization layer (from `tf.layers`) between the output of the convolutional
    layer and its activation function.'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩展 `define_cnn` 方法中定义的 CNN 架构：在卷积层的输出与激活函数之间添加一个批量归一化层（来自 `tf.layers`）。
- en: 'Try to train the model with the extended CNN architecture: the batch normalization
    layer adds two update operations that must be executed before running the training
    operation. Become familiar with the `tf.control_dependencies` method to force
    the execution of the operations contained inside the collection `tf.GraphKeys.UPDATE_OPS`,
    to be executed before the train operation (look at the documentation of `tf.control_dependencies` and `tf.get_collection`!).'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用扩展的 CNN 架构训练模型：批量归一化层添加了两个更新操作，这些操作必须在运行训练操作之前执行。熟悉 `tf.control_dependencies`
    方法，以强制执行 `tf.GraphKeys.UPDATE_OPS` 集合中的操作在训练操作之前执行（查看 `tf.control_dependencies`
    和 `tf.get_collection` 的文档！）。
- en: Log the training and validation images in TensorBoard.
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorBoard 中记录训练和验证图像。
- en: Has the model selection in the last example been performed correctly ? Probably
    not. Extend the Python script to measure the accuracy on the complete dataset
    and not just a batch.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一个示例中的模型选择是否正确执行了？可能没有。扩展 Python 脚本以在完整数据集上而不仅仅是一个批次上测量准确性。
- en: Replace the accuracy measurement performed manually with the accuracy operation
    provided in the `tf.metrics` package.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`tf.metrics`包中提供的准确率操作替换手动执行的准确率测量。
- en: 'Process the fashion-MNIST dataset and make it a binary dataset: all the items
    with a label different from 0 are now labeled as 1\. The dataset is unbalanced
    now. Which metric should you use to measure the model performance and perform
    model selection? Give reasons for your answer (see [Chapter 1](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml), *What
    is Machine Learning?*) and implement the metric manually.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 处理fashion-MNIST数据集，并将其转换为二进制数据集：所有标签不为0的项现在都标记为1。数据集现在是不平衡的。你应该使用什么度量标准来衡量模型性能并执行模型选择？请给出理由（参见[第1章](0dff1bba-f231-45fa-9a89-b4f127309579.xhtml)，*什么是机器学习？*），并手动实现该度量标准。
- en: Replace the manually implemented metric using the same metric defined in the `tf.metrics` package.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`tf.metrics`包中定义的相同度量标准，替换手动实现的度量标准。
