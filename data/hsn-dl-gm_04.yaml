- en: GAN for Games
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GAN在游戏中的应用
- en: Thus far, in our deep learning exploration, we have trained all our networks
    using a technique called **supervised training**. This training technique works
    well for when you have taken the time to identify and label your data. All of
    our previous example exercises used supervised training, because it is the simplest
    form of teaching. However, supervised learning tends to be the most cumbersome
    and tedious method, largely because it requires some amount of data labeling or
    identification before training. There have been attempts to use this form of training
    for machine learning or deep learning in gaming and simulation, but they have
    proven to be unsuccessful.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们的深度学习探索中，我们所有的网络训练都采用了一种叫做**监督训练**的技术。当你花时间标记和识别数据时，这种训练方法非常有效。我们之前的所有示例练习都使用了监督训练，因为它是最简单的教学方法。然而，监督学习往往是最繁琐和冗长的方法，主要因为它在训练前需要一定的标签或数据识别。在机器学习或深度学习在游戏和仿真中的应用尝试中，尽管有人尝试使用这种训练方式，但结果都证明是失败的。
- en: This is why, for most of this book, we will look at other forms of training,
    starting with a form of unsupervised training called a **generative adversarial
    network** (**GAN**). GANs are able to train themselves using, in essence, a two-player
    game. This makes them an ideal next step in our learning and a perfect way to
    actually start generating content for games.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在本书的大部分内容中，我们将探讨其他形式的训练，首先从一种无监督训练方法开始，称为**生成对抗网络**（**GAN**）。GAN通过本质上是一个双人游戏的方式进行自我训练，这使得它们成为我们学习的理想下一步，并且是实际开始为游戏生成内容的完美方法。
- en: 'In this chapter, we explore GANs and their use in developing game content.
    Along the way, we will learn more fundamentals of deep learning techniques. In
    this chapter, we will cover the following content:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索生成对抗网络（GAN）及其在游戏内容开发中的应用。在这个过程中，我们还将学习更多深度学习技术的基础知识。本章将涵盖以下内容：
- en: Introducing GANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍GAN
- en: Coding a GAN in Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Keras中编写GAN代码
- en: Wasserstein GAN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein GAN
- en: GAN for creating textures
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN用于创建纹理
- en: Generating music with a GAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GAN生成音乐
- en: Exercises
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习
- en: GANs are notoriously hard to train and build successfully. Therefore, it is
    recommended you take your time with this chapter and go through the exercises
    a couple of times if you need to. The techniques we learn to make effective GANs
    will provide you with a better overall understanding of training networks and
    the many other options available. We also still need to cover many fundamental
    concepts about training networks, so please work through this chapter thoroughly.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: GAN以其训练和构建的难度著称。因此，建议你花时间仔细阅读本章内容，并在需要时多做几次练习。我们学习的制作有效GAN的技术将帮助你更好地理解训练网络的整体概念以及其他可用的选项。同时，我们仍然需要涵盖许多关于训练网络的基础概念，所以请认真完成本章的内容。
- en: Introducing GANs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍GAN
- en: 'The concept of GANs is typically introduced using the analogy of a two-player
    game. In this game, there is typically an art expert and an art forger. The goal
    of the art forger or counterfeiter is to make a convincing-enough fake to fool
    the art expert and thus win the game. An example of how this was first portrayed
    as a neural network is as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的概念通常通过一个双人游戏类比来介绍。在这个游戏中，通常有一位艺术专家和一位艺术伪造者。艺术伪造者的目标是制作出足够逼真的假画来欺骗艺术专家，从而赢得游戏。以下是最早通过神经网络展示这一过程的示例：
- en: '![](img/5736d34f-71e3-4723-acce-24916b356499.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5736d34f-71e3-4723-acce-24916b356499.png)'
- en: GAN by Ian and others
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Ian及其他人提出的GAN
- en: In the preceding diagram, the Generator takes the place of the art forger, the
    one trying to best the art expert, shown as the Discriminator. The Generator uses
    random noise as a source to generate an image, with a goal that the image is convincing
    enough to fool the Discriminator. The Discriminator is trained on both real and
    fake images, and all it does is classify the image as real or fake. The Generator
    is then trained to build a convincing-enough fake that will fool the Discriminator.
    While this concept seems simple enough as a way of self-training a network, in
    the last few years, the implementation of this adversarial technique has proven
    exceptional in many areas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，生成器（Generator）扮演着艺术伪造者的角色，试图超越被称为判别器（Discriminator）的艺术专家。生成器使用随机噪声作为来源来生成图像，目标是让图像足够逼真，以至于能够欺骗判别器。判别器在真实和虚假图像上进行训练，任务就是将图像分类为真实或虚假。然后，生成器被训练去制作足够逼真的假图像来欺骗判别器。虽然这个概念作为一种自我训练网络的方式看起来简单，但在过去几年里，这种对抗技术的实现已在许多领域表现出卓越的效果。
- en: 'GANs were first developed by Ian Goodfellow and others at the University of
    Montreal in 2014\. In only a few short years, this technique has exploded into
    many wide and varied applications, from generating images and text to animating
    static images, all in a very short time. The following is a short summary of some
    of the more impressive GAN improvements/implementations currently turning heads
    in the deep learning community:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 最早由伊恩·古德费洛（Ian Goodfellow）及其团队在蒙特利尔大学于 2014 年开发。仅仅几年时间，这项技术已经迅速扩展到众多广泛而多样的应用领域，从生成图像和文本到为静态图像添加动画，几乎在短短几年内就完成了突破。以下是目前在深度学习领域引起关注的几项令人印象深刻的
    GAN 改进/实现的简短总结：
- en: '**Deep convolutional GANs** (**DCGANs**): These were the first major improvement
    to the standard architecture we just covered. We will explore this as our first
    form of GAN in the next section of this chapter.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积 GAN**（**DCGAN**）：这是我们刚才讨论的标准架构的首次重大改进。我们将在本章的下一部分中，探讨它作为我们学习的第一个 GAN
    形式。'
- en: '**Adversarial Autoencoder GAN**: This variation of an autoencoder uses the
    adversarial GAN technique to isolate attributes or properties of your data. It
    has interesting applications for determining latent relationships in data, such
    as being able to tell the difference in style versus content for a set of handwritten
    digits, for instance.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗自编码器 GAN**：这种自编码器变体利用对抗性 GAN 技术来隔离数据的属性或特征。它在发现数据中的潜在关系方面具有有趣的应用，例如能够区分手写数字的风格与内容之间的差异。'
- en: '**Auxiliary Classifier GAN**: This is another enhanced GAN that relates to
    conditioned or conditional GANs. It has been shown to synthesize higher-resolution
    images and is certainly worth exploring more in gaming.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**辅助分类器 GAN**：这是另一种增强型 GAN，与条件 GAN 相关。它已被证明能够合成更高分辨率的图像，尤其在游戏领域非常值得进一步探索。'
- en: '**CycleGAN**: This is a variation that is impressive in that it allows the
    translation of style from one image to another. There are plenty of examples of
    this form of GAN being used to style a picture as if Van Gogh painted it, to swapping
    celebrity faces. If this chapter piques your interest in GANs and you want to
    explore this form, check out this post: [https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CycleGAN**：这是一个变体，其令人印象深刻之处在于它允许将一种图像的风格转换为另一种图像。许多使用这种形式的 GAN 示例都很常见，例如将一张图片的风格转换成梵高的画风，或者交换名人的面孔。如果本章激发了你对
    GAN 的兴趣，并且你想进一步探索这一形式，可以查看这篇文章：[https://hardikbansal.github.io/CycleGANBlog/](https://hardikbansal.github.io/CycleGANBlog/)。'
- en: '**Conditional GANS**: These use a form of semi-supervised learning. This means
    that the training data is labeled but with meta data or attributes. So, instead
    of labeling a handwritten digit from the MNIST data set as a 9, you may instead
    label the writing style (cursive or print). Then, this new form of conditioned
    GAN can learn not only the digits, but also whether they are cursive or print.
    This form of GAN has shown some interesting applications and it is one we will
    explore further when we speak to specific applications in gaming.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件 GAN**：这些 GAN 使用一种半监督学习的形式。这意味着训练数据被标记，但带有元数据或属性。例如，不是将 MNIST 数据集中的手写数字标记为“9”，而是标记其书写风格（草书或印刷体）。然后，这种新的条件
    GAN 形式不仅可以学习数字，还可以学习它们是草书还是印刷体。这种 GAN 形式已经展现出一些有趣的应用，并且我们将在探讨游戏领域的具体应用时进一步讨论。'
- en: '**DiscoGAN**: This is yet another form of GAN showing fun results, from swapping
    celebrity hairstyles to genders. This GAN extracts features or domains and allows
    you to transfer them to other images or data spaces. This GAN has numerous applications
    in gaming and is certainly worth exploring further for the interested reader.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DiscoGAN**：这又是一种 GAN，展示了有趣的结果，从交换名人发型到性别转换。这种 GAN 提取特征或领域，并允许将它们转移到其他图像或数据空间。这在游戏中有很多应用，值得对感兴趣的读者进一步探索。'
- en: '**DualGAN**: This uses dual GANs to train two generators against two discriminators
    in order to transfer images or data to other styles. This would be very useful
    as a way of restyling multiple assets and would work nicely for generating different
    forms of art content for games.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DualGAN**：这使用双重 GAN，通过训练两个生成器与两个判别器进行对抗，以将图像或数据转换为其他风格。这对于重新设计多个资产非常有用，尤其是在为游戏生成不同风格的艺术内容时表现出色。'
- en: '**Least squares GAN** (**LSGAN**): This uses a different form of calculating
    loss and has been shown to be more effective than the DCGAN.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小二乘 GAN**（**LSGAN**）：这种 GAN 使用不同的损失计算方式，并且已被证明比 DCGAN 更有效。'
- en: '**pix2pixGAN**: This is an extension to conditional GANs that allows it to
    transfer or generate multiple features from one image to another. This allows
    for images of the sketch of an object to return an actual 3D-rendered image of
    the same object or vice versa. While this is a very powerful GAN, it still is
    very much research-driven and may not be ready for use in games. Perhaps you will
    just have to wait six months or a year.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pix2pixGAN**：这是对条件GAN的扩展，使其能够从一张图片转换或生成多种特征到另一张图片。这允许将物体的草图转换为该物体的真实3D渲染图像，反之亦然。虽然这是一个非常强大的GAN，但它仍然是研究驱动的，可能还不适合用于游戏开发。或许你得再等六个月或一年。'
- en: '**InfoGANs**: These types of GANs are, as of yet, used extensively to explore
    features or information about the training data. They can be used to identify
    the rotation of a digit in the MNIST dataset, for instance. Also, they are often
    used as a way of identifying attributes for conditioned GAN training.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InfoGANs**：这些类型的GAN迄今为止被广泛用于探索训练数据中的特征或信息。例如，它们可以用来识别MNIST数据集中数字的旋转方向。此外，它们通常被用作识别条件GAN训练属性的一种方式。'
- en: '**Stacked or SGAN**: This is a form of GAN that breaks itself into layers where
    each layer is a generator and discriminator battling it out. This makes the overall
    GAN easier to train but also requires you to understand each stage or layer in
    some detail. If you are just starting, this is not the GAN for you, but as you
    build more complex networks, revisit this model again.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stacked或SGAN**：这是一种将自身分解为多个层次的GAN，每个层都是一个生成器和判别器相互对抗。这使得整个GAN更容易训练，但也要求你理解每个阶段或层的细节。如果你是刚开始学习，这可能不是适合你的GAN，但随着你构建更复杂的网络，稍后可以再次回顾这个模型。'
- en: '**Wasserstein GANs**: This is a state-of-the-art GAN, and it will also get
    attention in its own section in this chapter. The calculation of loss is the improvement
    in this form of GAN.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wasserstein GANs**：这是一种最先进的GAN，它将在本章的专门章节中获得关注。损失的计算是这种形式的GAN的改进之处。'
- en: '**WassGANs**: This uses the Wasserstein distance to determine loss, which dramatically
    helps with model convergence.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WassGANs**：这使用Wasserstein距离来确定损失，从而显著帮助模型的收敛。'
- en: We will explore further instances of specific GAN implementations as we work
    through this chapter. Here, we will look at how to generate game textures and
    music with a GAN. For now, though, let's move on to the next section and learn
    how to code a GAN in Keras.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探索具体GAN实现的其他实例。在这里，我们将学习如何使用GAN生成游戏纹理和音乐。暂时，我们先跳到下一部分，学习如何在Keras中编写GAN。
- en: Coding a GAN in Keras
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中编写一个GAN
- en: 'Of course, the best way to learn is by doing, so let''s jump in and start coding
    our first GAN. In this example, we will be building the basic DCGAN and then modifying
    it later for our purposes. Open up `Chapter_3_2.py` and follow these steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最好的学习方式是通过实践，所以让我们跳进来，开始编写第一个GAN。在这个例子中，我们将构建基础的DCGAN，并稍后根据我们的需求进行修改。打开`Chapter_3_2.py`，并按照以下步骤进行：
- en: This code was originally pulled from [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN),
    which is the best representation of GANs in Keras anywhere, and is all thanks
    to Erik Linder-Norén. Great job, and thanks for the hard work, Erik.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码最初来自[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)，它是Keras中最好的GAN表示，感谢Erik
    Linder-Norén的辛勤工作。做得好，感谢你的努力，Erik。
- en: An alternate listing a vanilla GAN has been added as `Chapter_3_1.py` for your
    learning pleasure.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个备用的普通GAN列表已被添加为`Chapter_3_1.py`，供你学习使用。
- en: 'We start by importing libraries:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: There are a few highlighted new types introduced in the preceding code: `Reshape`,
    `BatchNormalization`, `ZeroPadding2D`, `LeakyReLU`, `Model`, and `Adam`. We will
    explore each of these types in more detail next.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的代码中引入了一些新的重要类型：`Reshape`、`BatchNormalization`、`ZeroPadding2D`、`LeakyReLU`、`Model`和`Adam`。我们将更详细地探讨这些类型。
- en: 'Most of our previous examples worked with basic scripts. We are now at a point
    where we want types (classes) of our own built for further use later. That means
    we now start by defining our class like so:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们之前的大多数示例使用的是基本的脚本。现在我们进入了一个阶段，需要为将来的进一步使用创建自定义的类型（类）。这意味着我们现在开始像这样定义我们的类：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, we create a new class (type) called `DCGAN` for our implementation of a
    deep convolutional GAN.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们创建了一个新的类（类型），命名为`DCGAN`，用于实现深度卷积GAN。
- en: 'Next, we would normally define our `init` function by Python convention. However,
    for our purposes, let''s first look at the `generator` function:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通常会按照Python的约定定义`init`函数。然而，为了我们的目的，让我们先看看`generator`函数：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `build_generator` function builds the art-forger model, which means it takes
    that sample set of noise and tries to convert it into an image the discriminator
    will believe is real. In this form, it uses the principle of convolution to make
    it more efficient, except, in this case, it generates a feature map of noise that
    it then turns into a real image. Essentially, the generator is doing the opposite
    of recognizing an image, but instead trying to generate an image based on feature
    maps.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`build_generator`函数构建了伪造艺术模型，意味着它接受那一组噪声样本，并尝试将其转换为判别器认为是真的图像。在这个过程中，它利用卷积原理提高效率，然而在这种情况下，它生成了一张噪声特征图，然后将其转化为一张真实图像。从本质上讲，生成器做的是与识别图像相反的工作，它并不是识别图像，而是尝试基于特征图生成图像。'
- en: In the preceding block of code, note how the input starts with `128, 7x7` feature
    maps of noise then uses a `Reshape` layer to turn it into the proper image layout
    we want to create. It then up-samples (the reverse of pooling or down-sampling)
    the feature map into 2x size (14 x 14), training another layer of convolution
    followed by more up-sampling (2x to 28 x 28) until the correct image size (28x28
    for the MNIST) is generated. We also see the use of a new layer type called `BatchNormalization`,
    which we will cover in more detail shortly.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的代码块中，注意输入是如何以`128, 7x7`的噪声特征图开始，然后使用`Reshape`层将其转换为我们想要创建的正确图像布局。接着，它通过上采样（即池化或下采样的逆过程）将特征图放大到2倍大小（14
    x 14），并训练另一个卷积层，之后继续进行更多的上采样（2倍至28 x 28），直到生成正确的图像尺寸（MNIST的28x28）。我们还看到了一个新的层类型`BatchNormalization`的使用，稍后我们会详细讨论它。
- en: 'Next, we will build the `build_discriminator` function like so:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将像这样构建`build_discriminator`函数：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time, the discriminator is testing the image inputs and determining whether
    they are fake. It uses convolution to identify features, but in this example it
    uses `ZeroPadding2D` to place a buffer of zeros around the images in order to
    help identification. The opposite form of this layer would be `Cropping2D`, which
    crops an image. Note how this model does not use down-sampling or pooling with
    the convolution. We will explore the other new special layers `LeakyReLU` and
    `BatchNormalization` in the coming sections. Note how we have not used any pooling
    layers in our convolution. This is done to increase the spatial dimensionality
    through the fractionally strided convolutions. See how inside the convolution
    layers we are using an odd kernel and stride size.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这次，判别器正在测试图像输入，并判断它们是否为伪造图像。它使用卷积来识别特征，但在这个示例中，它使用`ZeroPadding2D`将一层零填充放置在图像周围，以帮助识别。该层的相反形式是`Cropping2D`，它会裁剪图像。注意，模型没有在卷积中使用下采样或池化层。我们将在接下来的部分中探讨其他新的特殊层`LeakyReLU`和`BatchNormalization`。注意，我们在卷积中没有使用任何池化层。这是为了通过分数步幅卷积增加空间维度。看看在卷积层内部我们是如何使用奇数大小的卷积核和步幅的。
- en: 'We will now circle back and define the `init` function like so:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将回过头来像这样定义`init`函数：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This initialization code sets up the sizes for our input images (28 x 28 x 1,
    one channel for grayscale). It then sets up an `Adam` optimizer, something else
    we will review in another section on optimizers. After this, it builds the `discriminator`
    and then the `generator`. Then it combines the two models or sub networks (`generator`
    and `discriminator`) together. This allows the networks to work in tandem and
    optimize training across an entire network. Again, this is a concept we will look
    at more closely under optimizers.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段初始化代码设置了我们输入图像的大小（28 x 28 x 1，表示一个通道的灰度图像）。然后设置一个`Adam`优化器，这是我们将在优化器章节中进一步回顾的内容。之后，它构建了`discriminator`，然后是`generator`。接着，它将这两个模型或子网络（`generator`和`discriminator`）组合在一起，使得网络能够协同工作，并在整个网络上优化训练。这个概念我们将在优化器部分更详细地讨论。
- en: Before we get too deep, take some time to run this example. This sample can
    take an extensive amount of time to run, so return to the book after it starts
    and keep it running.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们深入之前，花点时间运行这个示例。这个示例可能需要相当长的时间来运行，所以启动后可以回到书本，保持运行状态。
- en: 'As the sample runs, you will be able to see the generated output get placed
    into a folder called `images` within the same folder as your running Python file.
    Go ahead and watch as every 50 epochs a new image is saved, which is shown in
    the following diagram:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在样本运行过程中，你将能够看到生成的输出被保存到与运行的Python文件同一个文件夹下的`images`文件夹中。可以观察到，每经过50次迭代，都会保存一张新图像，如下图所示：
- en: '![](img/dfa6688e-16d8-4d0f-b177-6f2ea058dd59.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfa6688e-16d8-4d0f-b177-6f2ea058dd59.png)'
- en: Example of output generated from a GAN
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GAN生成的输出示例
- en: The preceding shows the results after 3,900 epochs or so. When you start training,
    it will take a while to get results this good.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了大约经过3900次迭代后的结果。当你开始训练时，会需要一些时间才能获得如此好的结果。
- en: That covers the basics of setting up the models, except all the work that is
    in the training, which we will cover in the next section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了模型设置的基础知识，除了训练过程中所有的工作，这部分将在下一节中讲解。
- en: Training a GAN
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个GAN
- en: 'Training a GAN requires a fair bit more attention to detail and an understanding
    of more advanced optimization techniques. We will walk through each section of
    this function in detail in order to understand the intricacies of training. Let''s
    open up `Chapter_3_1.py` and look at the `train` function and follow these steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个GAN需要更多的细节关注以及对更高级优化技术的理解。我们将详细讲解该函数的每个部分，以便理解训练过程的复杂性。让我们打开`Chapter_3_1.py`，查看`train`函数并按照以下步骤进行操作：
- en: 'At the start of the `train` function, you will see the following code:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`train`函数的开头，你会看到以下代码：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data is first loaded from the MNIST training set and then rescaled to the
    range of `-1` to `1`. We do this in order to better center that data around 0
    and to accommodate our activation function, `tanh`. If you go back to the generator
    function, you will see that the bottom activation is `tanh`.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据首先从MNIST训练集加载，然后重新缩放到`-1`到`1`的范围。我们这样做是为了更好地将数据围绕0进行中心化，并且适配我们的激活函数`tanh`。如果你回去查看生成器函数，你会看到底部的激活函数是`tanh`。
- en: 'Next, we build a `for` loop to iterate through the epochs like so:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个`for`循环来迭代整个训练周期，代码如下：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we randomly select half of the *real* training images, using this code:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码随机选择一半的*真实*训练图像：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After that, we sample `noise` and generate a set of forged images with the
    following code:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们采样`noise`并使用以下代码生成一组伪造图像：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, half of the images are real and the other half are faked by our `generator`.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，图像中一半是真实的，另一半是我们`generator`生成的伪造图像。
- en: 'Next, the `discriminator` is trained against the images generating a loss for
    incorrectly predicted fakes and correctly identified real images as shown:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，`discriminator`会对图像进行训练，产生错误预测的伪造图像损失和正确识别的真实图像损失，如下所示：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Remember, this block of code is running across a set or batch. This is why we
    use the `numpy np.add` function to add the `d_loss_real`, and `d_loss_fake`. `numpy`
    is a library we will often use to work on sets or tensors of data.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住，这段代码是针对一个批次的数据运行的。这就是为什么我们使用`numpy np.add`函数来将`d_loss_real`和`d_loss_fake`相加的原因。`numpy`是我们常用的一个库，用于处理数据集或张量。
- en: 'Finally, we train the generator using the following code:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用以下代码训练生成器：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note how the `g_loss` is calculated based on training the combined model. As
    you may recall, the combined model takes the input from real and fake images and
    backpropagates the training back through the entire model. This allows us to train
    both the `generator` and `discriminator` together as a combined model. An example
    of how this looks is shown next, but just note that the image sizes are a little
    different than ours:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，`g_loss`是如何基于训练合并模型来计算的。正如你可能记得的，合并模型会将真实和伪造图像的输入传入，并将训练的误差反向传播到整个模型中。这使得我们能够将`generator`和`discriminator`一起训练，作为一个合并模型。接下来展示了这一过程的一个示例，但请注意图像大小与我们不同：
- en: '![](img/c85f204f-7196-444a-be1c-53b285b79cd8.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c85f204f-7196-444a-be1c-53b285b79cd8.png)'
- en: Layer architecture diagram of DCGAN
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN的层架构图
- en: Now that we have a better understanding of the architecture, we need to go back
    and understand some details about the new layer types and the optimization of
    the combined model. We will look at how we can optimize a joined model such as
    our GAN in the next section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对架构有了更好的理解，我们需要回过头来理解一些关于新层类型和合并模型优化的细节。我们将在下一节中探讨如何优化像我们的GAN这样的联合模型。
- en: Optimizers
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器
- en: An **optimizer** is really nothing more than another way to train the backpropagation
    of error through a network. As we learned back in [Chapter 1](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml)*,
    Deep Learning for Games*, the base algorithm we use for backpropagation is the
    gradient descent and the more advanced **stochastic gradient descent** (**SGD**).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器**实际上只是另一种通过网络训练误差反向传播的方式。正如我们在[第一章](108dd4cb-0332-4f3b-963b-fbfb49f2c8f0.xhtml)*《深度学习与游戏》*中学到的那样，我们用于反向传播的基本算法是梯度下降法，以及更高级的**随机梯度下降法**（**SGD**）。'
- en: 'SGD works by altering the evaluation of the gradient by randomly picking the
    batch order during each training iteration. While SGD works well for most cases,
    it does not perform well in a GAN, due to a problem known as the **vanishing **/ **exploding
    gradient**, which happens when trying to train multiple, but combined, networks.
    Remember, we are directly feeding the results of our generator into the discriminator.
    Instead, we look to more advanced optimizers. A graph showing the performance
    of the typical best optimizers is shown in the following diagram:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SGD通过在每次训练迭代中随机选择批次顺序来改变梯度的评估。尽管SGD在大多数情况下表现良好，但它在生成对抗网络（GAN）中表现不佳，因为它存在一个被称为**梯度消失**/
    **梯度爆炸**的问题，这通常出现在训练多个但组合的网络时。记住，我们将生成器的结果直接输入判别器中。为此，我们转向更高级的优化器。以下图示展示了典型最佳优化器的性能：
- en: '![](img/ce2b1eaf-c1e9-4018-ac13-b8208344a68e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce2b1eaf-c1e9-4018-ac13-b8208344a68e.png)'
- en: Performance comparison of various optimizers
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 各种优化器的性能比较
- en: 'All of the methods in the graph have their origin in SGD, but you can clearly
    see the winner in this instance is **Adam**. There are cases where this is not
    the case, but the current favorite optimizer is Adam. It is something we have
    used extensively before, as you may have noticed, and you will likely continue
    using it in the future. However, let''s take a look at each of the optimizers
    in a little more detail, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的所有方法都源自SGD，但你可以清楚地看到，在这个实例中，**Adam**是赢家。当然，也有例外情况，但目前最受欢迎的优化器是Adam。你可能已经注意到，我们以前广泛使用过它，并且未来你可能会继续使用它。不过，接下来我们会更详细地了解每个优化器，如下所示：
- en: '**SGD**: This is one of the first models we looked at and it will often be
    our baseline to train against.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SGD**：这是我们最早研究的模型之一，通常它将是我们用来作为训练基准的模型。'
- en: '**SGD with Nesterov**: The problem SGD often faces is that wobble effect we
    saw in network loss, in one of the earlier training examples. Remember, during
    training, our network loss would fluctuate between two values, almost as if it
    was a ball going up and down a hill. In essence, that is exactly what is happening,
    but we can correct that by introducing a term we call **momentum**. An example
    of the effect momentum has on training is shown in the following diagram:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有Nesterov的SGD**：SGD常面临的问题是我们在早期训练示例中看到的网络损失中的晃动效应。记住，在训练过程中，我们的网络损失会在两个值之间波动，几乎就像一个球在山坡上下滚动。实质上，这正是发生的情况，但我们可以通过引入一个我们称之为**动量**的项来纠正这一点。以下图示展示了动量对训练的影响：'
- en: '![](img/9ab302ca-8dd7-4e12-b02e-da2befc259a0.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ab302ca-8dd7-4e12-b02e-da2befc259a0.png)'
- en: SGD with and without momentum
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 带有和不带动量的SGD
- en: So, now, instead of just letting the ball blindly roll around, we control its
    speed. We give it a push to get over some of those annoying bumps or wobbles,
    and more efficiently get to the lowest point.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在，我们不再只是让球盲目地滚动，而是控制它的速度。我们给它一点推力，让它越过那些恼人的颠簸或晃动，更高效地到达最低点。
- en: As you may recall from studying the math of backpropagation, we control the
    gradient in SGD to train the network to minimize error or loss. By introducing
    momentum, we try to control the gradient to be more efficient by approximating
    what the values should be. The **Nesterov technique**, or it may just be referred
    to as **Momentum**, uses an accelerated momentum term to further optimize the
    gradient.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得，从反向传播的数学中我们控制SGD中的梯度，以训练网络最小化误差或损失。通过引入动量，我们试图通过近似值来更高效地控制梯度。**Nesterov技术**，或者可以称之为**动量**，使用加速的动量项来进一步优化梯度。
- en: '**AdaGrad**: This method optimizes the individual training parameters based
    on the frequency of the updates, which makes it ideal for working with smaller
    datasets. The other main benefit is that it allows you to not have to tune the
    learning rate. However, a big weakness with this method is squared gradients causing
    the learning rate to become so small that the network stops learning.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaGrad**：这种方法根据更新的频率优化个别训练参数，因此非常适合处理较小的数据集。另一个主要优点是它不需要调节学习率。然而，这种方法的一个大缺点是平方梯度导致学习率变得过小，从而使得网络停止学习。'
- en: '**AdaDelta**: This method is an extension to AdaGrad, which deals with the
    squared gradients and vanishing learning rate. It does this by fixing the learning
    rate window to a particular minimum.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaDelta**：这种方法是AdaGrad的扩展，用于处理平方梯度和消失的学习率。它通过将学习率窗口固定为一个特定的最小值来解决这一问题。'
- en: '**RMSProp**: Developed by Geoff Hinton, the grandfather of deep learning, this
    is a technique to manage the vanishing learning rate problem in AdaGrad. As you
    can see in the graph, it is on par with AdaDelta for the sample shown.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSProp**：由深度学习的祖师爷Geoff Hinton开发，这是一种用于解决AdaGrad中消失学习率问题的技术。如图所示，它与AdaDelta的表现相当。'
- en: '**Adaptive Moment Estimation** (**Adam**): This is another technique that attempts
    to control that gradient using a more controlled version of Momentum. It is often
    described as Momentum plus RMSProp, since it combines the best of both techniques.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应矩估计（Adam）**：这是一种尝试通过更控制版本的动量来控制梯度的技术。它通常被描述为动量加上RMSProp，因为它结合了两者的优点。'
- en: '**AdaMax**: This method is not shown on the performance graph but is worth
    mentioning. It is an extension to Adam that generalizes each iteration of an update
    applied to the momentum.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaMax**：这种方法未在性能图表中显示，但值得一提。它是Adam的扩展，对每次更新迭代应用于动量进行了推广。'
- en: '**Nadam**: This is another method not on the graph; it is a combination of
    Nesterov-accelerated Momentum and Adam. The vanilla Adam just uses a Momentum
    term that is not accelerated.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nadam**：这是一种未出现在图表中的方法，它是Nesterov加速动量和Adam的结合。普通的Adam仅使用了一个未加速的动量项。'
- en: '**AMSGrad**: This is a variation of Adam that works best when Adam is shown
    to be unable to converge or wobble. This is caused by the algorithm failing to
    adapt learning rates and is fixed by taking a maximum rather than an average of
    previously squared gradients. The difference is subtle and tends to prefer smaller
    datasets. Keep this option in the back of your mind as a possible future tool.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AMSGrad**：这是Adam的一种变种，在Adam无法收敛或出现震荡时效果最佳。这是由于算法未能适应学习率，通过取之前平方梯度的最大值而非平均值来修正这一问题。这个区别较为微妙，且更倾向于较小的数据集。在使用时可以将这个选项作为未来可能的工具记在心中。'
- en: That completes our short overview of optimizers; be sure to refer to the exercises
    at the end of the chapter for ways you can explore them further. In the next section,
    we build our own GAN that can generate textures we can use in games.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们对优化器的简要概述；务必参考章节末尾的练习，以进一步探索它们。在下一节中，我们将构建自己的GAN，生成可以在游戏中使用的纹理。
- en: Wasserstein GAN
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Wasserstein GAN
- en: As you can most certainly appreciate by now, GANs have wide and varied applications,
    several of which apply very well to games. One such application is the generation
    of textures or texture variations. We often want slight variations in textures
    to give our game worlds a more convincing look. This is and can be done with **shaders**,
    but for performance reasons, it is often best to create **static assets**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在可以非常清楚地理解的，GAN有着广泛而多样的应用，其中许多在游戏中应用得非常好。其中一个应用是生成纹理或纹理变化。我们经常需要对纹理做些微小变化，以便给我们的游戏世界增添更具说服力的外观。这可以通过**着色器**来完成，但出于性能考虑，通常最好创建**静态资源**。
- en: Therefore, in this section, we will build a GAN project that allows us to generate
    textures or height maps. You could also extend this concept using any of the other
    cool GANs we briefly touched on earlier. We will be using a default implementation
    of the Wasserstein GAN by Erik Linder-Norén and converting it for our purposes.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们将构建一个GAN项目，允许我们生成纹理或高度图。你也可以使用我们之前简要提到的其他一些酷炫的GAN扩展这个概念。我们将使用Erik
    Linder-Norén的Wasserstein GAN的默认实现，并将其转换为我们的用途。
- en: 'One of the major hurdles you will face when first approaching deep learning
    problems is shaping data to the form you need. In the original sample, Erik used
    the MNIST dataset, but we will convert the sample to use the CIFAR100 dataset.
    The CIFAR100 dataset is a set of color images classified by type, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次接触深度学习问题时，你将面临的一个主要障碍是如何将数据调整为所需的格式。在原始示例中，Erik 使用了 MNIST 数据集，但我们将把示例转换为使用
    CIFAR100 数据集。CIFAR100 数据集是一组按类别分类的彩色图像，如下所示：
- en: '![](img/87786072-1218-45e0-868c-b71599064740.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87786072-1218-45e0-868c-b71599064740.png)'
- en: CIFAR 100 dataset
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR 100 数据集
- en: 'For now, though, let''s open up `Chapter_3_wgan.py` and follow these steps:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开 `Chapter_3_wgan.py` 并按照以下步骤操作：
- en: 'Open the Python file and review the code. Most of the code will look the same
    as the DCGAN we already looked at. However, there are a few key differences we
    want to look at, as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Python 文件并查看代码。大部分代码与我们之前查看的 DCGAN 相同。然而，我们想要查看几个关键的不同点，具体如下：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The Wasserstein GAN uses a distance function in order to determine the cost
    or loss for each training iteration. Along with this, this form of GAN uses multiple
    critics rather than a single discriminator to determine cost or loss. Training
    multiple critics together improves performance and handles the vanishing gradient
    problem we often see plaguing GANs. An example of a different form of GAN training
    is as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wasserstein GAN 使用一种距离函数来确定每次训练迭代的成本或损失。除此之外，这种形式的 GAN 使用多个评判器而不是单一的判别器来确定成本或损失。训练多个评判器可以提高性能，并解决我们通常在
    GAN 中看到的梯度消失问题。另一种 GAN 训练方式的示例如下：
- en: '![](img/b1e15842-e119-44ed-9f30-a52a0aab0e32.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b1e15842-e119-44ed-9f30-a52a0aab0e32.png)'
- en: Training performance across GAN implementations ([https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf))
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 实现的训练性能对比 ([https://arxiv.org/pdf/1701.07875.pdf](https://arxiv.org/pdf/1701.07875.pdf))
- en: A WGAN overcomes the gradient problem by managing cost through a distance function
    that determines the cost of moving, rather than a difference in error values.
    A linear cost function could be as simple as the number of moves a character needs
    to take in order to spell a word correctly. For example, the word *SOPT* would
    have a cost of 2, since the *T* character needs to move two places to spell *STOP*
    correctly. The word *OTPS* has a distance cost of *3 (S) + 1 (T) = 4* to spell
    *STOP* correctly.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WGAN 通过管理成本来克服梯度问题，采用距离函数来确定移动的成本，而不是依赖错误值的差异。一个线性成本函数可能像字符拼写一个单词所需的移动次数那样简单。例如，单词
    *SOPT* 的成本为 2，因为 *T* 字符需要移动两次才能正确拼写成 *STOP*。单词 *OTPS* 的距离成本为 *3 (S) + 1 (T) =
    4*，才能正确拼写成 *STOP*。
- en: The Wasserstein distance function essentially determines the cost of transforming
    one probability distribution to another. As you can imagine, the math to understand
    this can be quite complex, so we will defer that to the more interested reader.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wasserstein 距离函数本质上确定了将一个概率分布转换为另一个概率分布的成本。如你所想，理解这些数学可能相当复杂，因此我们将这一部分留给对数学更感兴趣的读者。
- en: Run the example. This sample can take a significant time to run, so be patient.
    Also, this sample has been shown to have trouble training on some GPU hardware.
    If you find this to be the case, just disable the use of GPU.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例。这个示例可能需要较长时间才能运行，所以请耐心等待。此外，已知这个示例在某些 GPU 硬件上可能无法顺利训练。如果你遇到这种情况，只需禁用 GPU
    的使用即可。
- en: As the sample runs, open the `images` folder from the same folder as the Python
    file and watch the training images generate.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当示例运行时，打开与 Python 文件相同文件夹中的 `images` 文件夹，查看训练图像的生成过程。
- en: Run the sample for as long as you feel the need to in order to understand how
    it works. This sample can take several hours even on advanced hardware. When you
    are done, move on to the next section, and we will see how to modify this sample
    for generating textures.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 运行示例，直到你理解它是如何工作的为止。即使在高级硬件上，这个示例也可能需要几个小时。完成后，继续进行下一部分，我们将看到如何修改这个示例以生成纹理。
- en: Generating textures with a GAN
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GAN 生成纹理
- en: 'One of the things so rarely covered in advanced deep learning books is the
    specifics of shaping data to input into a network. Along with shaping data is
    the need to alter the internals of a network to accommodate the new data. The
    final version of this example is `Chapter_3_3.py`, but for this exercise, start
    with the `Chapter_3_wgan.py` file and follow these steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的高级书籍中，很少涉及如何调整数据以输入到网络中的具体细节。除了数据调整外，还需要修改网络内部结构以适应新的数据。这个示例的最终版本是 `Chapter_3_3.py`，但本练习从
    `Chapter_3_wgan.py` 文件开始，并按以下步骤操作：
- en: 'We will start by changing the training set of data from MNIST to CIFAR by swapping
    out the imports like so:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过交换导入语句来将训练数据集从MNIST切换到CIFAR，代码如下：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At the start of the class, we will change the image size parameters from 28
    x 28 grayscale to 32 x 32 color like so:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在类的开始，我们将把图像尺寸的参数从28 x 28的灰度图像更改为32 x 32的彩色图像，代码如下：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, move down to the `train` function and alter the code as follows:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，移动到`train`函数并按如下方式修改代码：
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This code loads the images from the CIFAR100 dataset and sorts through them
    by label. Labels are stored in the `y` variable, and the code loops through all
    the downloaded images and isolates those to one specific set. In this case, we
    are using the label `33`, which corresponds to forest images. There are 100 categories
    in the CIFAR100, and we are selecting one category that holds 500 images. Feel
    free to try to generate other textures from other categories.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码加载CIFAR100数据集中的图像，并按标签进行分类。标签存储在`y`变量中，代码会遍历所有下载的图像，并将它们隔离到特定的集合中。在这个例子中，我们使用标签`33`，对应的是森林图像。CIFAR100中有100个类别，我们选择其中一个类别，这个类别包含500张图像。你也可以尝试生成其他类别的纹理。
- en: The rest of the code is fairly straightforward, except for the `np.reshape`
    call where we reshape the data into a list of 500 images `32x32` pixels by three
    channels. You may also want to note that we do not need to expand the axis to
    three as we did before. This is because our image is already scaled to three channels.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其余的代码相当简单，除了`np.reshape`调用部分，在那里我们将数据重塑为包含500张`32x32`像素并且有三个通道的图像。你可能还需要注意，我们不再像之前那样需要扩展轴到三个通道，因为我们的图像已经是三通道的。
- en: 'We now need to go back to the generator and critic models and alter that code
    slightly. First, we will change the generator like so:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要返回生成器和判别器模型，并稍微修改代码。首先，我们将按如下方式修改生成器：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The boldface code denotes the changes. All we are doing for this model is converting
    the `7x7` original feature map to `8x8`. Recall that the original full image size
    is `28x28`. Our convolution starts with a `7x7` feature map, doubled twice, which
    equals `28x28`. Since our new image size is `32x32`, we need to convert our network
    to start with `8x8` feature maps, which doubled twice equals `32x32`, the same
    size as the CIFAR100 images. Fortunately, we can leave the critic model as it
    is.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粗体代码表示所做的更改。我们对这个模型所做的所有操作就是将`7x7`的原始特征图转换为`8x8`。回想一下，原始图像的完整尺寸是`28x28`。我们的卷积从`7x7`的特征图开始，经过两次放大，得到`28x28`的图像。由于我们的新图像尺寸是`32x32`，我们需要将网络调整为从`8x8`的特征图开始，经过两次放大得到`32x32`，与CIFAR100图像的尺寸相同。幸运的是，我们可以保持判别器模型不变。
- en: 'Next, we add a new function to save samples of the original CIFAR images, and
    this is shown here:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个新函数来保存原始CIFAR图像的样本，代码如下：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `save_images` function outputs a sampling of the original images and is
    called by the following code in the `train` function:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`save_images`函数输出原始图像样本，并通过以下代码在`train`函数中调用：'
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The new code is in boldface and just outputs what a sampling of the originals
    looks like, as follows:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新代码是粗体部分，输出的是原始图像的一个样本，如下所示：
- en: '![](img/ecd433e1-66ac-49a0-817c-0eb87d7531ae.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecd433e1-66ac-49a0-817c-0eb87d7531ae.png)'
- en: Example of the original images
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像示例
- en: Run the sample and observe the output in the `images` folder again labeled `cifar`,
    showing the result of training. Again, this sample can take some time to run,
    so read on to the next section.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例并再次查看`images`文件夹中的输出，文件夹名称为`cifar`，展示训练结果。再次提醒，这个示例可能需要一些时间来运行，因此请继续阅读下一部分。
- en: As the sample runs, you can observe how the GAN is training to match the images.
    The benefit here is that you can generate various textures easily using a variety
    of techniques. You can use these as textures or height maps in Unity or another
    game engine. Before we finish up this section, let's jump into some normalization
    and other parameters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在样本运行时，你可以观察到GAN是如何训练以匹配这些图像的。这里的好处是，你可以通过多种技术轻松生成不同的纹理。你可以将这些纹理或高度图用作Unity或其他游戏引擎中的素材。在完成本节之前，我们先来讨论一些归一化和其他参数。
- en: Batch normalization
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '**Batch normalization**, as its name suggests, normalizes the distribution
    of weights in a layer around some mean of 0\. This allows for the network to use
    a higher learning while still avoiding a vanishing or exploding gradient problem.
    It is due to the weights being normalized, which allows for fewer shifts or training
    wobble, as we have seen before.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**，顾名思义，它会将一层中权重的分布标准化，使其围绕均值0。这样可以让网络使用更高的学习率，同时避免梯度消失或爆炸的问题。这是因为权重被标准化，从而减少了训练过程中的偏移或震荡，正如我们之前看到的那样。'
- en: 'By normalizing the weights in a layer, we allow for the network to use a higher
    learning rate and thus train faster. Also, we can avoid or reduce the need to
    use `DropOut`. You will see that we use the standard term, shown here, to normalize
    the layers:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准化一层中的权重，我们使网络能够使用更高的学习率，从而加速训练。此外，我们还可以避免或减少使用`DropOut`的需要。你会看到我们使用标准术语来标准化这些层，如下所示：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Recall from our discussions of optimizers that momentum controls how quickly
    or slowly we want to decrease the training gradient. In this case, momentum refers
    to the amount of change of the mean or center of the normalized distribution.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们对优化器的讨论中回忆一下，动量控制着我们希望训练梯度减少的快慢。在这里，动量指的是标准化分布的均值或中心变化的程度。
- en: In the next section, we look at another special layer called LeakyReLU.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论另一种特殊的层——LeakyReLU。
- en: Leaky and other ReLUs
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Leaky和其他ReLU
- en: '**LeakyReLU** adds an activation layer that allows for negative values to have
    a small slope, rather than just 0, as in the case of the standard ReLU activation
    function. The standard ReLU encourages sparsity in the network by only allowing
    neurons with positive activation to fire. However, this also creates a dead neuron
    state, where parts of the network essentially die off or become untrainable. To
    overcome this issue, we introduce a leaky form of ReLU activation called LeakyReLU.
    An example of how this activation works is shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**LeakyReLU**添加了一个激活层，允许负值有一个小的斜率，而不是像标准ReLU激活函数那样为0。标准ReLU通过只允许正激活的神经元激活，鼓励网络的稀疏性。然而，这也会导致死神经元的状态，网络的某些部分实际上会“死亡”或变得无法训练。为了解决这个问题，我们引入了一种名为LeakyReLU的ReLU激活形式。以下是这种激活方式的示例：'
- en: '![](img/0377b7b9-e422-4d78-9c9e-c8dc7c37d9b0.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0377b7b9-e422-4d78-9c9e-c8dc7c37d9b0.png)'
- en: Example of a leaky and parametric ReLU
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky和参数化ReLU的示例
- en: Pictured in the preceding diagram is **Parametric ReLU**, which is similar to
    Leaky, but it allows the network to train the parameter itself. This allows the
    network to adjust on its own, but it will take longer to train.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中是**参数化ReLU**，它类似于Leaky，但允许网络自行训练参数。这使得网络能够自我调整，但训练时间会更长。
- en: 'The other ReLU variants you can use are summarized here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的其他ReLU变体总结如下：
- en: '**Exponential Linear** (**ELU, SELU**): These forms of ReLU activate as shown
    in the diagram as follows:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数线性** (**ELU, SELU**): 这些ReLU形式的激活如图所示：'
- en: '![](img/cea2fa2e-b196-457a-9603-d4cf88bcff72.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cea2fa2e-b196-457a-9603-d4cf88bcff72.png)'
- en: ELU and SELU
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ELU和SELU
- en: '**Concatenated ReLU** (**CReLU**): This joins the regular and leaky form together
    to provide a new function that produces two output values. For positive values,
    it generates *[0,x],* while for negative values, it returns *[x,0]*. One thing
    to note about this layer is the doubling of output, since two values are generated
    per neuron.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连接ReLU** (**CReLU**): 这一层将常规ReLU和LeakyReLU结合在一起，提供一种新的功能，生成两个输出值。对于正值，它产生*
    [0,x]*，而对于负值，它返回 *[x,0]*。需要注意的是，这一层的输出翻倍，因为每个神经元会生成两个值。'
- en: '**ReLU-6**: The value of 6 is arbitrary but allows for the network to train
    sparse neurons. Sparsity is of value because it encourages the network to learn
    or build stronger weights or bonds. The human brain has been shown to function
    in a sparse state, with only a few activated neurons at a time. You will often
    hear the myth that we only use 10% of our brain at a time at most. This may very
    well be true, but the reasons for this are more mathematical than us being able
    to use our entire brain. We do use our entire brain, just not all of it at the
    same time. Stronger individual weights, encouraged by sparsity, allow for the
    network to make better/stronger decisions. Fewer weights also encourage less overfitting
    or memorization of data. This can often happen in deep networks with thousands
    of neurons.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU-6**：6的值是任意的，但它允许网络训练稀疏的神经元。稀疏性具有价值，因为它鼓励网络学习或建立更强的权重或连接。已经有研究表明，人脑在稀疏状态下工作，通常只有少数几个神经元同时被激活。你经常会听到一个神话，说我们大脑最多一次只使用10%。这可能是真的，但其中的原因更多的是数学上的问题，而不是我们无法使用大脑的全部功能。我们确实使用大脑的全部，只不过不是同时使用所有部分。稀疏性鼓励的更强的单个权重，使网络能够做出更好、更强的决策。更少的权重也能减少过拟合或数据记忆的情况。这种情况常发生在具有成千上万神经元的深度网络中。'
- en: Regularization is another technique we will often use to trim or reduce unneeded
    or weights and create sparse networks. We will have a few opportunities to look
    at regularization and sparsity later in the coming chapters.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是我们经常使用的另一种技术，用于修剪或减少不需要的权重，创建稀疏网络。在接下来的章节中，我们将有机会更深入地了解正则化和稀疏性。
- en: In the next section, we use what we have learned to build a working music GAN
    that can generate game music.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将利用所学知识构建一个能生成游戏音乐的工作型音乐GAN。
- en: A GAN for creating music
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建音乐的GAN
- en: In our final grand example of this chapter, we are going to look at generating
    music with GANs for games. Music generation is not especially difficult, but it
    does allow us to see a whole variation of a GAN that uses LSTM layers to identify
    sequences and patterns in music. Then it attempts to build that music back from
    random noise to a passable sequence of notes and melodies. This sample becomes
    ethereal when you listen to those generated notes and realize the tune originates
    from a computer brain.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最终示例中，我们将研究使用GAN为游戏生成音乐。音乐生成本身并不特别困难，但它让我们看到了使用LSTM层的GAN变体，该变体能够识别音乐中的序列和模式。然后，它尝试将随机噪音重建成可接受的音符序列和旋律。当你听到那些生成的音符并意识到这段旋律来自计算机的大脑时，这个示例会显得非常虚幻。
- en: The origins of this sample are pulled from GitHub, [https://github.com/megis7/musegen](https://github.com/megis7/musegen),
    and developed by Michalis Megisoglou. The reason we look at these code examples
    is so that we can see the best of what others have produced and learn from those.
    In some cases, these samples are close to the original, and others not so much.
    We did have to tweak a few things. Michalis also produced a nice GitHub README
    on the code he built for his implementation of **museGAN**, music generation with
    GAN. If you are interested in building on this example further, be sure to check
    out the GitHub site as well. There are a few implementations of museGAN available
    using various libraries; one of them is TensorFlow.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例的来源来自GitHub，[https://github.com/megis7/musegen](https://github.com/megis7/musegen)，并由Michalis
    Megisoglou开发。我们查看这些代码示例的原因是为了看到别人最优秀的作品，并从中学习。在某些情况下，这些示例接近原始版本，而在其他情况下则不完全相同。我们确实做了一些调整。Michalis还在GitHub上发布了他为实现**museGAN**（基于GAN的音乐生成）编写的代码的详细README。如果你有兴趣进一步扩展这个示例，务必查看GitHub网站。不同的库有几个museGAN的实现，其中之一是TensorFlow。
- en: We use Keras in this example in order to make this example easier to understand.
    If you are serious about using TensorFlow, then be sure to take a look at the
    TensorFlow version of museGAN as well.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们使用Keras，目的是使示例更易于理解。如果你对使用TensorFlow非常认真，那么一定要查看museGAN的TensorFlow版本。
- en: 'This example trains the discriminator and generator separately, which means
    it needs to have the discriminator trained first. For our first run, we will run
    this example with the author''s previously generated models, but we still need
    some setup; let''s follow these steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例分别训练判别器和生成器，这意味着需要先训练判别器。对于我们的第一次运行，我们将使用作者之前生成的模型来运行这个示例，但我们仍然需要一些设置；让我们按照以下步骤进行：
- en: 'We first need to install a couple of dependencies. Open an Anaconda or Python
    window as an admin and run the following commands:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要安装一些依赖项。以管理员身份打开Anaconda或Python窗口，并运行以下命令：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`Music21` is a Python library for loading MIDI files. **MIDI** is a music interchange
    format used to describe, as you might have guessed, music/notes. The original
    models were trained on a collection of MIDI files that describe 300 chorales of
    Bach''s music. You can locate the project by navigating to the `musegen` folder
    and running the script.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Music21`是一个用于加载MIDI文件的Python库。**MIDI**是一种音乐交换格式，用于描述音乐/音符，正如你所猜测的那样。原始模型是通过一组描述巴赫300首合唱音乐的MIDI文件进行训练的。你可以通过导航到`musegen`文件夹并运行脚本来找到该项目。'
- en: 'Navigate to the project folder and execute the script that runs the previously
    trained models like so:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到项目文件夹，并执行运行先前训练的模型的脚本，如下所示：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This will load the previously saved modelsand use those models to train the
    generator and generate music. You could, of course, train this GAN on other MIDI
    files of your choosing later as needed. There are plenty of free sources for MIDI
    files from classical music, to TV theme music, games, and modern pop. We use the
    author's original models in this example, but the possibilities are endless.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将加载先前保存的模型，并使用这些模型来训练生成器并生成音乐。当然，您稍后可以根据需要使用您选择的其他MIDI文件训练这个GAN。对于MIDI文件，有许多免费的来源，包括古典音乐、电视主题音乐、游戏和现代流行音乐。我们在这个例子中使用的是作者的原始模型，但可能性是无穷无尽的。
- en: 'Loading the music files and training can take a really long time, as training
    typically does. So, take this opportunity to look at the code. Open up the `musegen.py`
    file located in the project folder. Take a look at around line 39, as follows:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载音乐文件和训练可能会非常耗时，因为训练通常需要较长时间。所以，趁此机会查看一下代码。打开项目文件夹中的`musegen.py`文件。查看大约第39行，如下所示：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This section of code loads the previously trained model from an `hdf5` or hierarchical
    data file. The preceding code sets up a number of variables that define the notes
    to a vocabulary we will use to generate new notes.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一段代码加载了从`hdf5`或分层数据文件中训练的模型。前面的代码设置了多个变量，用于定义我们将在生成新音符时使用的音符词汇。
- en: 'Locate the `notegenerator.py` file located in the same project folder. Take
    a look at the creation of the model code, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到项目文件夹中名为`notegenerator.py`的文件。查看模型创建的代码，如下所示：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note how we have changed from using `Conv2D` layers to `LSTM` layers, since
    we have gone from image recognition to sequence or note pattern recognition. We
    have also gone from using more straightforward layers to a complex time-distributed
    architecture. Also, the author used a concept known as **variational auto encoding**
    in order to determine the distribution of notes in a sequence. This network is
    the most complex we have looked at so far, and there is a lot going on here. Don't
    fret too much about this example, except to see how the code flows. We will take
    a closer look at more of these type of advanced time- distributed networks in
    [Chapter 4](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml)*, Building a Deep Learning
    Gaming Chatbot*[.](http://Chapter_4)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意我们如何从使用`Conv2D`层改为使用`LSTM`层，因为我们已经从图像识别转向了序列或音符模式识别。我们还从使用更直接的层次结构转向了复杂的时间分布架构。此外，作者使用了一种称为**变分自编码**的概念，用于确定序列中音符的分布。这个网络是我们迄今为止看到的最复杂的，内容非常丰富。对于这个例子，不必过于担心，只需看看代码的流向。我们将在[第4章](a8e699ff-c668-4601-842d-4c6e06c47a61.xhtml)*《构建深度学习游戏聊天机器人》*中详细探讨更多这种类型的高级时间分布网络[.](http://Chapter_4)
- en: Let the sample run and generate some music samples into the `samples/note-generator`
    folder. As we get into more complex problems, our training time will go from hours
    to days for very complex problems or more. It is possible that you could easily
    generate a network that you would not have the computing power to train in a reasonable
    time.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让示例运行并生成一些音乐样本到`samples/note-generator`文件夹。随着我们进入更复杂的问题，训练时间将从几个小时变成几天，甚至更长。很可能你会轻松生成一个网络，但却没有足够的计算能力在合理时间内完成训练。
- en: Open the folder and double-click on one of the sample files to listen to the
    generated MIDI file. Remember, this music was just generated by a computer brain.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文件夹，双击其中一个示例文件来听听生成的MIDI文件。记住，这段音乐是由计算机“大脑”生成的。
- en: There is a lot of code that we did not cover in this example. So, be sure to
    go back and go through the `musegen.py` file to get a better understanding of
    the flow and types of layers used to build the network generator. In the next
    section, we explore how to train this GAN.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们没有涵盖很多代码。所以，请务必返回并查看`musegen.py`文件，以更好地理解用于构建网络生成器的流程和层类型。在下一部分，我们将探讨如何训练这个GAN。
- en: Training the music GAN
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练音乐GAN
- en: 'Before we get into training this network, we will look at the overall architecture
    as depicted in the author''s original GitHub source:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练这个网络之前，我们将查看作者原始GitHub源码中展示的整体架构：
- en: '![](img/e630248f-63d9-43d6-9d64-616a450543f6.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e630248f-63d9-43d6-9d64-616a450543f6.png)'
- en: Overview of museGAN network architecture
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: museGAN网络架构概述
- en: The networks are almost identical until you look closer and see the subtle differences
    in the LSTM layers. Note how one set uses double the units as the other model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络几乎完全相同，直到你仔细观察并发现LSTM层的细微差异。注意，某一模型使用的单元数是另一个模型的两倍。
- en: 'We can generate music models by running the following command at the Python
    or Anaconda prompt:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在Python或Anaconda提示符下运行以下命令来生成音乐模型：
- en: '[PRE23]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This script loads the sample data and generates the models we use in the `musegen.py`
    file later when we create original music. Open up the `note-generator.py` file
    with the main parts shown here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本加载示例数据并生成我们在稍后创建原创音乐时会在`musegen.py`文件中使用的模型。打开`note-generator.py`文件，主要部分如下所示：
- en: The code was modified from the original to make it more Windows-compatible and
    cross-platform. Again, this is certainly not a criticism of the author's excellent
    work.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 代码已经从原始版本进行了修改，以使其更加兼容Windows并支持跨平台。再次强调，这绝不是对作者出色工作的批评。
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This code uses the Music21 library to read the MIDI notes and other music forms
    from the corpus of music you can use for your own testing. This training dataset
    is an excellent way to generate other sources of music and is composed of the
    following: [http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html](http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码使用Music21库来读取MIDI音符和其他音乐形式，来自您可以用于自己测试的音乐语料库。这个训练数据集是生成其他音乐来源的一个很好的方式，包含以下内容：[http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html](http://web.mit.edu/music21/doc/moduleReference/moduleCorpus.html)。
- en: 'You can further modify this example by modifying the contents or adding additional
    configuration options in the `config.py` file as shown:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过修改`config.py`文件中的内容或添加额外的配置选项来进一步修改此示例，文件示例如下：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The previous sample is great for exploring the generation of music. A more practical
    and potentially useful example will be introduced in the next section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个示例非常适合探索音乐生成。一个更实用且潜在有用的示例将在下一部分介绍。
- en: Generating music via an alternative GAN
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过另一种GAN生成音乐
- en: Another example of music generation is also included in the `Chapter_3` source
    folder, called **Classical-Piano-Composer**, with the source located at [https://github.com/Skuldur/Classical-Piano-Composer](https://github.com/Skuldur/Classical-Piano-Composer),
    developed by Sigurður Skúli. This example uses a full set of Final Fantasy MIDI
    files as source inspiration for the music generation and is a great practical
    example for generating your own music.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个音乐生成示例也包含在`Chapter_3`源文件夹中，名为**Classical-Piano-Composer**，其源代码位于[https://github.com/Skuldur/Classical-Piano-Composer](https://github.com/Skuldur/Classical-Piano-Composer)，由Sigurður
    Skúli开发。这个示例使用了完整的《最终幻想》MIDI文件作为音乐生成的灵感来源，是一个生成自己音乐的极好实用示例。
- en: 'In order to run this sample, you need to run the `lstm.py` first using the
    following command from the `Classical-Piano-Composer` project folder:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行这个示例，您需要先运行`lstm.py`，并使用以下命令从`Classical-Piano-Composer`项目文件夹中执行：
- en: '[PRE26]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This sample can take a substantial time to train, so be sure to open the file
    and read through what it does.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例可能需要相当长的时间来训练，所以请确保打开文件并阅读它的功能。
- en: 'After the models are trained, you can run the generator by running the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您可以通过运行以下命令来运行生成器：
- en: '[PRE27]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This script loads the trained model and generates the music. It does this by
    encoding the MIDI notes into network input in terms of sequences or sets of notes.
    What we are doing here is breaking up the music files into short sequences, or
    a music snapshot if you will. You can control the length of these sequences by
    adjusting the `sequences_length` property in the code file.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本加载了训练好的模型并生成音乐。它通过将MIDI音符编码为网络输入，按序列或音符集的形式进行处理。我们在这里做的就是将音乐文件分解成短序列，或者如果你愿意，可以称之为音乐快照。你可以通过调整代码文件中的`sequences_length`属性来控制这些序列的长度。
- en: The great thing about this second example is the ability to download your own
    MIDI files and put them in the appropriate input folder for training. It is also
    interesting to see how both projects use a similar three-layer LSTM structure
    but vary quite widely in other forms of execution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第二个示例的一个优点是可以下载你自己的MIDI文件并将其放入适当的输入文件夹进行训练。更有趣的是，两个项目都使用了类似的三层LSTM结构，但在其他执行方式上差异较大。
- en: If you want to learn more about audio or music development for games and especially
    for Unity, check out the book *Game Audio Development with Unity 5.x*, by Micheal
    Lanham. This book can show you many more techniques for working with audio and
    music in games.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解游戏中的音频或音乐开发，尤其是在Unity中的开发，可以阅读Micheal Lanham的书籍《*Game Audio Development
    with Unity 5.x*》。这本书可以向你展示更多在游戏中处理音频和音乐的技巧。
- en: Both music samples can take some time to train and then generate music, but
    it is certainly worth the effort to run through both examples and understand how
    they work. GANs have innovated the way we think of training neural networks and
    what type of output they are able to produce. As such, they certainly have a place
    in generating content for games.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个音乐样本的训练和生成音乐可能需要一些时间，但毫无疑问，通过运行这两个示例并理解它们的工作原理，绝对值得付出努力。GAN技术革新了我们对神经网络训练的理解，并改变了它们能够产生的输出类型。因此，它们在生成游戏内容方面无疑具有重要地位。
- en: Exercises
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Take some time to reinforce your learning by undertaking the following exercises:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 花些时间通过进行以下练习来巩固你的学习：
- en: What type of GAN would you use to transfer styles on an image?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会使用哪种类型的GAN来在图像上转移风格？
- en: What type of GAN would you use to isolate or extract the style?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会使用哪种类型的GAN来隔离或提取风格？
- en: Modify the number of critics used in the Wasserstein GAN example and see the
    effect it has on training.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改Wasserstein GAN示例中使用的评论者数量，看看它对训练的影响。
- en: Modify the first GAN, the DCGAN, to improve training performance using any technique
    you learned in this chapter. How did you increase training performance?
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改第一个GAN，即DCGAN，使用你在本章中学到的任何技巧来提高训练性能。你是如何提高训练性能的？
- en: Modify the BatchNormalization momentum parameter and see what effect it has
    on training.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改BatchNormalization动量参数，看看它对训练的影响。
- en: Modify a few of the samples by changing the activation from LeakyReLU to another
    advanced form of activation.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改一些样本，将激活函数从LeakyReLU更改为另一种更高级的激活形式。
- en: Modify the Wasserstein GAN example to use your own textures. There is a sample
    data loader available in the downloaded code sample for the chapter.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改Wasserstein GAN示例，使用你自己的纹理。在章节下载的代码示例中有一个示例数据加载器可供使用。
- en: Download one of the other reference GANs from [https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)
    and modify that to use your own dataset.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/eriklindernoren/Keras-GAN](https://github.com/eriklindernoren/Keras-GAN)下载其他参考GAN之一，并修改它以使用你自己的数据集。
- en: Alter the first music generation GAN to use a different corpus.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改第一个音乐生成GAN，使用不同的语料库。
- en: Use your own MIDI files to train the second music generation GAN example.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你自己的MIDI文件来训练第二个音乐生成GAN示例。
- en: (BONUS) Which music GAN generated better music? Is it what you expected?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （附加题）哪个音乐GAN生成的音乐更好？它是你预期的吗？
- en: You certainly don't have to work through all these exercises, but give a few
    a try. Putting this knowledge to practice right away can substantially improve
    your understanding of the material. Practice does make perfect, after all.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你当然不必完成所有这些练习，但可以尝试做几个。立刻将这些知识应用到实践中，能够大大提高你对材料的理解。毕竟，实践才能完美。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at generative adversarial networks, or GANs, as a
    way to build DNNs that can generate unique content based on copying or extracting
    features from other content. This also allowed us to explore unsupervised training,
    a method of training that requires no previous data classification or labeling.
    In the previous chapter, we used supervised training. We started with looking
    at the many variations of GANs currently making an impression in the DL community.
    Then we coded up a deep convolutional GAN in Keras, followed by the state-of-the-art
    Wasserstein GAN. From there, we looked at how to generate game textures or height
    maps using sample images. We finished the chapter off by looking at two music-generating
    GANs that can generate original MIDI music from sampled music.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了生成对抗网络（GAN），它是一种构建深度神经网络（DNN）的方法，可以通过复制或提取其他内容的特征来生成独特的内容。这也使我们能够探索无监督学习，这是一种无需先前数据分类或标记的训练方法。在上一章中，我们使用了监督学习。我们从研究当前在深度学习社区产生影响的各种GAN变种开始。然后，我们用Keras编写了一个深度卷积GAN，接着介绍了最先进的Wasserstein
    GAN。随后，我们探讨了如何利用样本图像生成游戏纹理或高度图。最后，我们通过研究两个能够从样本音乐生成原创MIDI音乐的音乐生成GAN，结束了本章的内容。
- en: For the final sample, we looked at music generation with GANs that relied heavily
    on RNNs (LSTM). We will continue our exploration of RNNs when we look at how to
    build DL chatbots for games.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后的示例中，我们研究了依赖于RNN（LSTM）的生成对抗网络（GAN）在音乐生成中的应用。我们将在接下来的章节中继续探讨RNN，重点讲解如何为游戏构建深度学习聊天机器人。
