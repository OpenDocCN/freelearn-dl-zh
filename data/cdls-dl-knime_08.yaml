- en: '*Chapter 6:* Recurrent Neural Networks for Demand Prediction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章:* 用于需求预测的递归神经网络'
- en: 'We have gathered some experience, by now, with fully connected feedforward
    neural networks in two variants: implementing a classification task by assigning
    an input sample to a class in a set of predefined classes or trying to reproduce
    the shape of an input vector via an autoencoder architecture. In both cases, the
    output response depends only on the values of the current input vector. At time
    ![](img/Formula_B16391_06_001.png), the output response, ![](img/Formula_B16391_06_002.png),
    depends on, and only on, the input vector, ![](img/Formula_B16391_06_003.png),
    at time ![](img/Formula_B16391_06_004.png). The network has no memory of what
    came before ![](img/Formula_B16391_06_005.png) and produces ![](img/Formula_B16391_06_006.png)
    only based on input ![](img/Formula_B16391_06_007.png).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经通过两种变体的全连接前馈神经网络积累了一些经验：通过将输入样本分配到一组预定义的类中来实现分类任务，或者尝试通过自编码器架构重建输入向量的形状。在这两种情况下，输出响应仅依赖于当前输入向量的值。在时刻![](img/Formula_B16391_06_001.png)，输出响应，![](img/Formula_B16391_06_002.png)，依赖于且仅依赖于时刻![](img/Formula_B16391_06_004.png)的输入向量，![](img/Formula_B16391_06_003.png)。网络没有对![](img/Formula_B16391_06_005.png)之前的任何记忆，并且仅根据输入![](img/Formula_B16391_06_007.png)生成![](img/Formula_B16391_06_006.png)。
- en: With **Recurrent Neural Networks** (**RNNs**), we introduce the time component
    ![](img/Formula_B16391_06_008.png). We are going to discover networks where the
    output response, ![](img/Formula_B16391_06_009.png), at time ![](img/Formula_B16391_06_001.png)
    depends on the current input sample, ![](img/Formula_B16391_06_011.png), as well
    as on previous input samples, ![](img/Formula_B16391_06_012.png), ![](img/Formula_B16391_06_013.png),
    … ![](img/Formula_B16391_06_014.png), where the memory of the network of the past
    ![](img/Formula_B16391_06_015.png) samples depends on the network architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**递归神经网络**（**RNNs**），我们引入了时间组件![](img/Formula_B16391_06_008.png)。我们将发现网络的输出响应，![](img/Formula_B16391_06_009.png)，在时刻![](img/Formula_B16391_06_001.png)依赖于当前输入样本，![](img/Formula_B16391_06_011.png)，以及之前的输入样本，![](img/Formula_B16391_06_012.png)，![](img/Formula_B16391_06_013.png)，……![](img/Formula_B16391_06_014.png)，网络对过去![](img/Formula_B16391_06_015.png)样本的记忆依赖于网络架构。
- en: 'We will first introduce the general concept of RNNs, and then the specific
    concept of **Long Short-Term Memory** (**LSTM**) in the realm of a classic time
    series analysis task: **demand prediction**. Then, we will show how to feed the
    network with not only static vectors, ![](img/Formula_B16391_06_016.png), but
    also sequences of vectors, such as ![](img/Formula_B16391_06_017.png), ![](img/Formula_B16391_06_018.png),
    ![](img/Formula_B16391_06_019.png), … ![](img/Formula_B16391_06_020.png), spanning
    ![](img/Formula_B16391_06_021.png) samples of the past input signal. These sequences
    of input vectors (tensors) built on the training set are used to train and evaluate
    a practical implementation of an LSTM-based RNN.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍RNN的一般概念，然后在经典时间序列分析任务**需求预测**的领域中，介绍**长短期记忆**（**LSTM**）的具体概念。接着，我们将展示如何将网络输入不仅仅是静态向量，![](img/Formula_B16391_06_016.png)，还包括向量序列，例如
    ![](img/Formula_B16391_06_017.png)，![](img/Formula_B16391_06_018.png)，![](img/Formula_B16391_06_019.png)，……![](img/Formula_B16391_06_020.png)，这些序列跨越了![](img/Formula_B16391_06_021.png)个过去输入信号的样本。这些基于训练集构建的输入向量（张量）序列将用于训练和评估基于LSTM的RNN的实际实现。
- en: 'In summary, this chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章将涵盖以下主题：
- en: Introducing RNNs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入RNN
- en: The Demand Prediction Problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需求预测问题
- en: Data Preparation – Creating the Past
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备——创造过去
- en: Building, Training, and Deploying an LSTM-Based RNN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建、训练和部署基于LSTM的RNN
- en: Introducing RNNs
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入RNN
- en: Let's start with an overview of RNNs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从RNN的概述开始。
- en: '**RNNs** are a family of neural networks that cannot be constrained in the
    feedforward architecture.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNNs**是无法局限于前馈架构的神经网络家族。'
- en: Important note
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: RNNs are obtained by introducing auto or backward connections – that is, recurrent
    connections – into feedforward neural networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在前馈神经网络中引入自连接或反向连接——即递归连接——来获得RNN。
- en: When introducing a recurrent connection, we introduce the concept of time. This
    allows RNNs to take context into account; that is, to remember inputs from the
    past by capturing the dynamic of the signal.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在引入递归连接时，我们引入了时间的概念。这使得RNN可以考虑上下文；即通过捕捉信号的动态来记住过去的输入。
- en: Introducing recurrent connections changes the nature of the neural network from
    static to dynamic and is therefore suitable for analyzing time series. Indeed,
    RNNs are often used to create solutions to problems involving time-ordered sequences,
    such as time series analysis, language modeling, free text generation, automatic
    machine translation, speech recognition, image captioning, and other similar problems
    investigating the time evolution of a given signal.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 引入递归连接改变了神经网络的性质，从静态变为动态，因此适用于分析时间序列。事实上，RNNs 常用于解决涉及时间顺序序列的问题，例如时间序列分析、语言建模、自由文本生成、自动机器翻译、语音识别、图像描述等问题，这些问题都在研究给定信号的时间演化。
- en: Recurrent Neural Networks
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: 'As stated in the previous section, the introduction of an auto or backward
    connection into a feedforward network transforms it into an RNN. For example,
    let''s consider the simple feedforward network depicted in *Figure 6.1*, looking
    at its detailed representation on the left and its compact representation on the
    right:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，引入自连接或反向连接到前馈网络中会将其转变为 RNN。例如，考虑 *图 6.1* 中所示的简单前馈网络，查看其左侧的详细表示和右侧的紧凑表示：
- en: '![Figure 6.1 – A simple fully connected feedforward network on the left and
    its more compact matrix-based representation on the right](img/B16391_06_001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 左侧是简单的全连接前馈网络，右侧是其更紧凑的基于矩阵的表示](img/B16391_06_001.jpg)'
- en: Figure 6.1 – A simple fully connected feedforward network on the left and its
    more compact matrix-based representation on the right
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – 左侧是简单的全连接前馈网络，右侧是其更紧凑的基于矩阵的表示
- en: The compact representation of the network in *Figure 6.1* includes one multi-dimensional
    input, ![](img/Formula_B16391_06_022.png), one possibly multi-dimensional output,
    ![](img/Formula_B16391_06_023.png), one hidden layer represented by the box containing
    the neuron icons, and the two weight matrixes from the input to the hidden layer,
    ![](img/Formula_B16391_06_024.png), and from the hidden to the output layer, ![](img/Formula_B16391_06_025.png).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.1* 中网络的紧凑表示包括一个多维输入 ![](img/Formula_B16391_06_022.png)、一个可能是多维的输出 ![](img/Formula_B16391_06_023.png)、一个由包含神经元图标的框表示的隐藏层，以及从输入到隐藏层的两个权重矩阵
    ![](img/Formula_B16391_06_024.png)，以及从隐藏层到输出层的权重矩阵 ![](img/Formula_B16391_06_025.png)。'
- en: 'Let''s now introduce to this network a recurrent connection, feeding the output
    vector, ![](img/Formula_B16391_06_026.png), back into the input layer in addition
    to the original input vector, ![](img/Formula_B16391_06_027.png) (*Figure 6.2*).
    This simple change to the network architecture changes the network behavior. Before,
    the function implemented by the network was just ![](img/Formula_B16391_06_028.png),
    where ![](img/Formula_B16391_06_029.png) is the current time when the input sample,
    ![](img/Formula_B16391_06_030.png), is presented to the network. Now, the function
    implemented by the recurrent network assumes the shape ![](img/Formula_B16391_06_031.png);
    that is, the current output depends on the current input, as well as on the output
    produced in the previous step for the previous input sample. We have introduced
    the concept of time:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们向这个网络引入一个递归连接，将输出向量 ![](img/Formula_B16391_06_026.png) 反馈到输入层，除了原始输入向量 ![](img/Formula_B16391_06_027.png)
    之外（*图 6.2*）。这个对网络架构的简单修改改变了网络的行为。之前，网络实现的函数仅仅是 ![](img/Formula_B16391_06_028.png)，其中
    ![](img/Formula_B16391_06_029.png) 是输入样本 ![](img/Formula_B16391_06_030.png) 被呈现给网络时的当前时间。现在，递归网络实现的函数变为
    ![](img/Formula_B16391_06_031.png)；也就是说，当前的输出不仅依赖于当前的输入，还依赖于上一步为上一个输入样本产生的输出。我们引入了时间的概念：
- en: '![Figure 6.2 – Adding a recurrent connection to the feedforward network](img/B16391_06_002.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – 向前馈网络添加递归连接](img/B16391_06_002.jpg)'
- en: Figure 6.2 – Adding a recurrent connection to the feedforward network
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 向前馈网络添加递归连接
- en: Thanks to these recurrent connections, the output of RNNs also contains a bit
    of the history of the input signal. We then say that they have memory. How far
    in the past the memory span goes depends on the recurrent architecture and the
    paradigms contained in it. For this reason, RNNs are more suitable than feedforward
    networks for analyzing sequential data, because they can also process information
    from the past. Past input information is metabolized via the output feedback into
    the input layer through the recurrent connection.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了这些循环连接，RNN的输出也包含了一些输入信号的历史信息。因此我们说它们具有记忆功能。记忆跨度能追溯到多远取决于递归架构和其中包含的范式。因此，RNN比前馈网络更适合分析序列数据，因为它们也能处理来自过去的信息。过去的输入信息通过输出反馈转化到输入层，经过循环连接。
- en: The problem now becomes how to train a network where the output depends on the
    previous output(s) as well. As you can imagine, a number of algorithms have been
    proposed over the years. The simplest one, and therefore the most commonly adopted,
    is **Back Propagation Through Time** (**BPTT**) (Goodfellow I, Bengio Y., Courville
    A., *Deep Learning*, MIT Press, (2016)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 问题现在变成了如何训练一个网络，使得其输出不仅依赖于当前输入，还依赖于先前的输出。正如你可以想象的那样，多年来已经提出了许多算法。最简单的算法，也是最常被采用的，就是**反向传播通过时间**（**BPTT**）（Goodfellow
    I, Bengio Y., Courville A., *深度学习*，MIT出版社，(2016)）。
- en: 'BPTT is based on the concept of *unrolling* the network over time. To understand
    the concept of *unrolling*, let''s take a few glimpses at the network at different
    times, ![](img/Formula_B16391_06_032.png), during training:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT的基础是*展开*网络的概念。为了理解*展开*的概念，让我们在训练过程中查看网络在不同时间点的情况，![](img/Formula_B16391_06_032.png)：
- en: At time ![](img/Formula_B16391_06_033.png), we have the original feedforward
    network, with weight matrixes ![](img/Formula_B16391_06_034.png) and ![](img/Formula_B16391_06_035.png),
    input ![](img/Formula_B16391_06_036.png) and ![](img/Formula_B16391_06_037.png),
    and output ![](img/Formula_B16391_06_038.png).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时刻![](img/Formula_B16391_06_033.png)，我们有原始的前馈网络，具有权重矩阵![](img/Formula_B16391_06_034.png)和![](img/Formula_B16391_06_035.png)，输入为![](img/Formula_B16391_06_036.png)和![](img/Formula_B16391_06_037.png)，输出为![](img/Formula_B16391_06_038.png)。
- en: At time ![](img/Formula_B16391_06_039.png), we again have the original feedforward
    network, with weight matrixes ![](img/Formula_B16391_06_040.png) and ![](img/Formula_B16391_06_041.png),
    but this time with input ![](img/Formula_B16391_06_042.png) and ![](img/Formula_B16391_06_043.png)
    and output ![](img/Formula_B16391_06_044.png).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时刻![](img/Formula_B16391_06_039.png)，我们再次得到了原始的前馈网络，具有权重矩阵![](img/Formula_B16391_06_040.png)和![](img/Formula_B16391_06_041.png)，但这次的输入是![](img/Formula_B16391_06_042.png)和![](img/Formula_B16391_06_043.png)，输出是![](img/Formula_B16391_06_044.png)。
- en: At time ![](img/Formula_B16391_06_045.png), again, we have the original feedforward
    network, with weight matrixes ![](img/Formula_B16391_06_046.png) and ![](img/Formula_B16391_06_047.png),
    but this time with input ![](img/Formula_B16391_06_048.png) and ![](img/Formula_B16391_06_049.png)
    and output ![](img/Formula_B16391_06_050.png).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在时刻![](img/Formula_B16391_06_045.png)，我们再次得到了原始的前馈网络，具有权重矩阵![](img/Formula_B16391_06_046.png)和![](img/Formula_B16391_06_047.png)，但这次的输入是![](img/Formula_B16391_06_048.png)和![](img/Formula_B16391_06_049.png)，输出是![](img/Formula_B16391_06_050.png)。
- en: This continues for the ![](img/Formula_B16391_06_021.png) samples in the input
    sequence.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这对于输入序列中的![](img/Formula_B16391_06_021.png)样本是持续进行的。
- en: Practically, we can copy the same original feedforward network with static weight
    matrixes ![](img/Formula_B16391_06_052.png) and ![](img/Formula_B16391_06_053.png)
    ![](img/Formula_B16391_06_054.png) times, which is as many ![](img/Formula_B16391_06_055.png)
    samples as in the input sequence (*Figure 6.3*). Each copy of the original network
    at time ![](img/Formula_B16391_06_056.png) will have the current input vector,
    ![](img/Formula_B16391_06_057.png), and the previous output vector, ![](img/Formula_B16391_06_058.png),
    as input. More generically, at each time ![](img/Formula_B16391_06_059.png), the
    network copy will produce an output, ![](img/Formula_B16391_06_060.png), and a
    related state, ![](img/Formula_B16391_06_061.png). The state, ![](img/Formula_B16391_06_062.png),
    is the network memory and feeds the next copy of the static network, while ![](img/Formula_B16391_06_063.png)
    is the dedicated output of each network copy. In some recurrent architectures,
    ![](img/Formula_B16391_06_064.png) and ![](img/Formula_B16391_06_065.png) are
    identical.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以将原始前馈网络复制多次，使用静态权重矩阵![](img/Formula_B16391_06_052.png) 和 ![](img/Formula_B16391_06_053.png)
    ![](img/Formula_B16391_06_054.png)，复制的次数与输入序列中的样本数相同(*图6.3*)。在时间![](img/Formula_B16391_06_056.png)时，原始网络的每个副本将接收当前输入向量![](img/Formula_B16391_06_057.png)和前一个输出向量![](img/Formula_B16391_06_058.png)作为输入。更一般地，在每个时间![](img/Formula_B16391_06_059.png)时，网络副本将产生一个输出![](img/Formula_B16391_06_060.png)和一个相关的状态![](img/Formula_B16391_06_061.png)。状态![](img/Formula_B16391_06_062.png)是网络的记忆，并馈送到下一个静态网络副本，而![](img/Formula_B16391_06_063.png)是每个网络副本的专用输出。在某些递归架构中，![](img/Formula_B16391_06_064.png)和![](img/Formula_B16391_06_065.png)是相同的。
- en: 'Let''s summarize. We have a recurrent network with the following features:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下。我们有一个递归网络，具有以下特点：
- en: Fed by input tensor ![](img/Formula_B16391_06_066.png), of ![](img/Formula_B16391_06_067.png)
    size, consisting of a sequence of ![](img/Formula_B16391_06_068.png) ![](img/Formula_B16391_06_069.png)-dimensional
    vectors
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由输入张量![](img/Formula_B16391_06_066.png)提供，大小为![](img/Formula_B16391_06_067.png)，由一系列![](img/Formula_B16391_06_068.png)
    ![](img/Formula_B16391_06_069.png)维向量组成
- en: Producing an output tensor, ![](img/Formula_B16391_06_070.png), of ![](img/Formula_B16391_06_071.png)
    size, consisting of a sequence of ![](img/Formula_B16391_06_072.png) ![](img/Formula_B16391_06_073.png)
    -dimensional vectors
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 产生一个输出张量，![](img/Formula_B16391_06_070.png)，大小为![](img/Formula_B16391_06_071.png)，由一系列![](img/Formula_B16391_06_072.png)
    ![](img/Formula_B16391_06_073.png)维向量组成
- en: Producing a state tensor, ![](img/Formula_B16391_06_074.png), related to output
    tensor ![](img/Formula_B16391_06_075.png), used as the network memory
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个状态张量，![](img/Formula_B16391_06_074.png)，与输出张量![](img/Formula_B16391_06_075.png)相关，用作网络记忆
- en: Important note
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: This recurrent network can also be just a sub-network that is a hidden unit
    in a bigger neural architecture. In this case, it is fed by the outputs of previous
    layers, and its output forms the input to the next layers in the bigger network.
    Then, ![](img/Formula_B16391_06_076.png) is not the output of the whole network,
    but just the output of this recurrent unit – that is, an intermediate hidden state
    of the full network.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个递归网络也可以只是一个子网络，作为更大神经架构中的一个隐藏单元。在这种情况下，它由前一层的输出作为输入，并且它的输出构成了更大网络中下一层的输入。此时，![](img/Formula_B16391_06_076.png)不是整个网络的输出，而只是这个递归单元的输出——即整个网络的中间隐藏状态。
- en: 'In *Figure 6.3*, we propose the unrollment over four time steps of the simple
    recurrent network in *Figure 6.2*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.3*中，我们提出了对*图6.2*中简单递归网络的四个时间步展开：
- en: '![Figure 6.3 – Unrolling a recurrent network though time](img/B16391_06_003.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 随时间展开递归网络](img/B16391_06_003.jpg)'
- en: Figure 6.3 – Unrolling a recurrent network though time
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 随时间展开递归网络
- en: At this point, we have transformed the recurrent sub-network into a sequence
    of ![](img/Formula_B16391_03_173.png) copies of the original feedforward network
    – that is, into a much larger static feedforward network. As large as it might
    be, we do already know how to train fully connected feedforward networks with
    the backpropagation algorithm. So, the backpropagation algorithm has been adapted
    to include the unrolling process and to train the resulting feedforward network.
    This is the basic BPTT algorithm. Many variations of the BPTT algorithm have also
    been proposed over the years.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将循环子网络转化为 ![](img/Formula_B16391_03_173.png) 个原始前馈网络的副本——也就是一个更大的静态前馈网络。尽管它可能非常庞大，但我们已经知道如何使用反向传播算法训练完全连接的前馈网络。因此，反向传播算法已经适应了展开过程，并用来训练得到的前馈网络。这就是基本的
    BPTT 算法。多年来，也提出了许多 BPTT 算法的变种。
- en: We will now dive into the details of the simplest recurrent network, the one
    made of just one layer of recurrent units.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入了解最简单的循环网络，它由一个仅包含循环单元的单层网络组成。
- en: Recurrent Neural Units
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经单元
- en: 'The simplest recurrent neural unit consists of a network with just one single
    hidden layer, with activation function ![](img/Formula_B16391_06_078.png), with
    an auto connection. Using the same unrolling-over-time process, we can represent
    this unit as ![](img/Formula_B16391_03_252.png) copies of a feedforward network
    with one hidden layer of just one unit (*Figure 6.4*):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的循环神经单元由一个仅包含单一隐藏层的网络组成，激活函数为 ![](img/Formula_B16391_06_078.png)，并带有自连接。通过相同的时间展开过程，我们可以将这个单元表示为
    ![](img/Formula_B16391_03_252.png) 个具有一个隐藏层且仅有一个单元的前馈网络的副本（*图 6.4*）：
- en: '![Figure 6.4 – The simplest recurrent neural unit](img/B16391_06_004.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 最简单的循环神经单元](img/B16391_06_004.jpg)'
- en: Figure 6.4 – The simplest recurrent neural unit
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 最简单的循环神经单元
- en: In this case, the output, ![](img/Formula_B16391_06_080.png), is also the state
    of the network, which is fed back into the input – that is, into the input of
    the next copy of the unrolled network at time ![](img/Formula_B16391_06_081.png).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输出 ![](img/Formula_B16391_06_080.png) 也是网络的状态，它会反馈到输入中——也就是反馈到时间 ![](img/Formula_B16391_06_081.png)
    时展开网络的下一个副本的输入中。
- en: This simple recurrent unit already shows some memory, in the sense that the
    current output also depends on previously presented samples at the input layer.
    However, its architecture is a bit too simple to show a considerable memory span.
    Of course, it depends on the task to solve how long of a memory span is needed.
    A classic example is sentence completion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的循环单元已经展现出一些记忆功能，因为当前的输出也依赖于先前输入层呈现的样本。然而，它的架构过于简单，无法展示出显著的记忆跨度。当然，所需的记忆跨度取决于要解决的任务。一个经典的例子就是句子补全。
- en: To complete a sentence, you need to know the topic of the sentence, and to know
    the topic, you need to know the previous words in the sentence. For example, analyzing
    the sentence *Cars drive on the …*, we realize that the topic is *cars* and then
    the only logical answer would be *road*. To complete this sentence, we need a
    memory of just four words. Let's now take a more complex sentence, such as *I
    love the beach. My favorite sound is the crashing of the …*. Here, many answers
    are possible, including *cars*, *glass*, or *waves*. To understand which is the
    logical answer, we need to go back in the sentence to the word *beach*, which
    is nine words backward. The memory span needed to analyze this sentence is more
    than double the memory span needed to analyze the previous sentence. This short
    example shows that sometimes a longer memory span is needed to give the correct
    answer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成一个句子，你需要知道句子的主题，而要知道主题，你需要了解句子中的前几个词。例如，分析句子 *Cars drive on the …*，我们意识到主题是
    *cars*，然后唯一合逻辑的答案是 *road*。为了完成这个句子，我们只需要记住四个词。现在，我们来看看一个更复杂的句子，比如 *I love the
    beach. My favorite sound is the crashing of the …*。在这种情况下，可能的答案有很多，包括 *cars*、*glass*
    或 *waves*。为了理解哪个是合逻辑的答案，我们需要回溯到句子中的 *beach* 这个词，回溯的距离是九个词。分析这个句子所需的记忆跨度是分析前一个句子所需记忆跨度的两倍多。这个简短的例子表明，有时需要更长的记忆跨度才能给出正确答案。
- en: The simple recurrent neural unit provides some memory, but often not enough
    to solve most required tasks. We need something more powerful that can crawl backward
    farther in the past than just what the simple recurrent unit can do. This is exactly
    why LSTM units were introduced.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的循环神经单元提供了一些记忆，但通常不足以解决大多数需要的任务。我们需要更强大的东西，能够回溯更远的过去，而不仅仅是简单循环单元能做到的。这正是为什么引入了
    LSTM 单元。
- en: Long Short-Term Memory
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: LSTM was introduced for the first time in 1997 (Hochreiter, Sepp and Schmidhuber,
    Jürgen (1997), *Long Short-term Memory. Neural computation*, 9\. 1735-80\. 10.1162/
    neco.1997.9.8.1735, https://www.researchgate.net/publication/13853244_Long_Short-term_Memory).
    It is a more complex type of recurrent unit, using an additional hidden vector,
    the cell state or memory state, ![](img/Formula_B16391_06_082.png), and the concept
    of gates.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 首次于 1997 年提出（Hochreiter, Sepp 和 Schmidhuber, Jürgen（1997），*长短期记忆。神经计算*，9.
    1735-80。10.1162/neco.1997.9.8.1735，https://www.researchgate.net/publication/13853244_Long_Short-term_Memory）。它是一种更复杂的递归单元，使用了一个额外的隐藏向量——细胞状态或记忆状态，![](img/Formula_B16391_06_082.png)，以及门的概念。
- en: '*Figure 6.5* shows the structure of an unrolled LSTM unit (C. Olah, *Understanding
    LSTM Networks*, 2015, [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.5* 显示了展开的 LSTM 单元的结构（C. Olah，*理解 LSTM 网络*，2015，[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)）：'
- en: '![Figure 6.5 – LSTM layer](img/B16391_06_005.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – LSTM 层](img/B16391_06_005.jpg)'
- en: Figure 6.5 – LSTM layer
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – LSTM 层
- en: As you can see, the different copies of the unit are connected by two hidden
    vectors. The one on the top is the cell state vector, ![](img/Formula_B16391_06_083.png),
    used to make information travel through the different unit copies. The second
    one on the bottom is the output vector of the unit.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，不同副本的单元通过两个隐藏向量相连。顶部的那个是细胞状态向量，![](img/Formula_B16391_06_083.png)，用于使信息在不同单元副本之间流动。底部的那个是单元的输出向量。
- en: Next, we have the gates, three in total. Gates can open or close (or partially
    open/close) and, in this way, they make decisions on what to store or delete from
    a hidden vector. A gate consists of a sigmoid function and a pointwise multiplication.
    Indeed, the sigmoid function takes values in [0,1]. Specifically, ![](img/Formula_B16391_06_084.png)
    removes the input (forgets it), while ![](img/Formula_B16391_06_085.png) lets
    the input pass unaltered (remembers it). In between ![](img/Formula_B16391_06_086.png)
    and ![](img/Formula_B16391_06_087.png), a variety of nuances of remembering and
    forgetting are possible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有三个门。门可以开启或关闭（或者部分开启/关闭），通过这种方式，它们决定了从隐藏向量中存储或删除什么内容。一个门由一个 Sigmoid 函数和逐元素相乘组成。实际上，Sigmoid
    函数的输出值范围是 [0,1]。具体来说，![](img/Formula_B16391_06_084.png) 删除输入（即遗忘），而 ![](img/Formula_B16391_06_085.png)
    让输入不变（即记住）。在 ![](img/Formula_B16391_06_086.png) 和 ![](img/Formula_B16391_06_087.png)
    之间，可能会出现各种记忆和遗忘的细微差别。
- en: The weights of these sigmoid layers, which implement the gates, are adjusted
    via the learning process. That is, the gates learn when to allow data to enter,
    leave, or be deleted through the iterative process of making guesses, backpropagating
    error, and adjusting weights via gradient descent. The training algorithm for
    LSTM layers is again an adaptation of the backpropagation algorithm.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 Sigmoid 层的权重（用于实现门）通过学习过程进行调整。也就是说，门通过反复猜测、反向传播误差，并通过梯度下降调整权重的迭代过程来学习何时允许数据进入、离开或被删除。LSTM
    层的训练算法再次是反向传播算法的一个变体。
- en: 'An LSTM layer contains three gates: a forget gate, an input gate, and an output
    gate (*Figure 6.5*). Let''s have a closer look at these gates.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 LSTM 层包含三个门：遗忘门、输入门和输出门（*图 6.5*）。我们来仔细看看这些门。
- en: The Forget Gate
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 遗忘门
- en: 'The first gate from the left, the **forget gate**, filters the components from
    the cell state vector. Based on the values in the current input vector, ![](img/Formula_B16391_06_088.png)
    and in the output vector of the previous unit, ![](img/Formula_B16391_06_089.png),
    the gate produces a forget or remember decision, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最左边的第一个门，**遗忘门**，过滤掉细胞状态向量中的成分。基于当前输入向量中的值，![](img/Formula_B16391_06_088.png)
    和上一单元的输出向量中的值，![](img/Formula_B16391_06_089.png)，该门做出遗忘或记住的决策，如下所示：
- en: '![](img/Formula_B16391_06_090.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_090.png)'
- en: Here, ![](img/Formula_B16391_06_091.png) is the weight matrix of the forget
    gate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_06_091.png) 是遗忘门的权重矩阵。
- en: The vector of decision ![](img/Formula_B16391_06_092.png) is then pointwise
    multiplied by the hidden cell state vector, ![](img/Formula_B16391_06_093.png),
    to decide what to remember (![](img/Formula_B16391_06_094.png)) and what to forget
    (![](img/Formula_B16391_06_095.png)) from the previous state.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 决策向量 ![](img/Formula_B16391_06_092.png) 然后与隐藏的细胞状态向量 ![](img/Formula_B16391_06_093.png)
    逐元素相乘，决定从上一状态中记住什么内容（![](img/Formula_B16391_06_094.png)）以及忘记什么内容（![](img/Formula_B16391_06_095.png)）。
- en: The question now is why do we want to forget? If LSTM units have been introduced
    to obtain a longer memory, why should we need to forget something? Take, for example,
    analyzing a document in a text corpus; you might need to forget all knowledge
    about the previous document since the two documents are probably unrelated. Therefore,
    with each new document, the memory should be reset to 0.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，现在的问题是，为什么我们要忘记呢？如果引入了 LSTM 单元来获得更长的记忆，为什么还需要忘记某些内容呢？举个例子，分析文本语料库中的一篇文档时；你可能需要忘记之前文档中的所有知识，因为这两篇文档可能没有关联。因此，对于每个新文档，记忆应该重置为
    0。
- en: Even within the same text, if you move to the next sentence and the subject
    of the text changes, and with the new subject a new gender appears, then you might
    want to forget the gender of the previous subject, to be ready to incorporate
    the new one and to adjust the corresponding part of speech accordingly.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在同一文本中，如果你移动到下一个句子并且文本的主语发生变化，并且随着新主语出现新的性别，那么你可能希望忘记之前主语的性别，为了准备接受新的性别，并相应地调整词性。
- en: The Input Gate
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入门
- en: 'The goal of the input gate is more straightforward: it keeps the input information
    that is new and useful. Here, again, a sigmoid gate lets input components pass
    completely (![](img/Formula_B16391_06_096.png)), blocks them completely (![](img/Formula_B16391_06_097.png)),
    or something in between depending on their importance to the final, current, and
    future outputs.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门的目标更为直接：它保留那些新的和有用的输入信息。这里，同样，sigmoid 门完全通过输入成分（![](img/Formula_B16391_06_096.png)），完全阻止它们（![](img/Formula_B16391_06_097.png)），或者根据它们对最终当前和未来输出的重要性做出某种调整。
- en: 'This decision is implemented again as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策再次通过以下方式实现：
- en: '![](img/Formula_B16391_06_098.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_098.png)'
- en: This is done with a new set of weights, ![](img/Formula_B16391_06_099.png),
    of course.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然是使用新的一组权重，![](img/Formula_B16391_06_099.png)，完成的。
- en: The input gate doesn't operate on the previous cell state, ![](img/Formula_B16391_06_100.png),
    directly. Instead, a new cell state candidate, ![](img/Formula_B16391_06_101.png),
    is created, based on the values in the current input vector, ![](img/Formula_B16391_06_102.png),
    and in the output vector of the previous unit, ![](img/Formula_B16391_06_103.png),
    using a tanh layer.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门不会直接作用于先前的单元状态，![](img/Formula_B16391_06_100.png)。相反，基于当前输入向量中的值，![](img/Formula_B16391_06_102.png)，以及前一个单元的输出向量，![](img/Formula_B16391_06_103.png)，通过一个
    tanh 层创建一个新的单元状态候选，![](img/Formula_B16391_06_101.png)。
- en: 'This looks as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![](img/Formula_B16391_06_104.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_104.png)'
- en: Again, this is with another set of weights, ![](img/Formula_B16391_06_105.png).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 再次使用另一组权重，![](img/Formula_B16391_06_105.png)。
- en: 'The input gate now decides which information of the cell candidate state vector,
    ![](img/Formula_B16391_06_106.png), should be added to the cell state vector,
    ![](img/Formula_B16391_06_107.png). Therefore, the candidate state, ![](img/Formula_B16391_06_108.png),
    is multiplied pointwise by the output of the sigmoid layer of the input gate,
    ![](img/Formula_B16391_06_109.png), and then added to the filtered cell state
    vector, ![](img/Formula_B16391_06_110.png) The final state, ![](img/Formula_B16391_06_111.png),
    then results in the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入门现在决定哪些信息应添加到单元候选状态向量，![](img/Formula_B16391_06_106.png)，应添加到单元状态向量，![](img/Formula_B16391_06_107.png)。因此，候选状态，![](img/Formula_B16391_06_108.png)，与输入门
    sigmoid 层的输出，![](img/Formula_B16391_06_109.png)，逐点相乘，然后添加到过滤后的单元状态向量，![](img/Formula_B16391_06_110.png)。最终状态，![](img/Formula_B16391_06_111.png)，就会得到如下结果：
- en: '![](img/Formula_B16391_06_112.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_112.png)'
- en: What have we done here? We have added new content to the previous cell state
    vector, ![](img/Formula_B16391_06_113.png). Let's suppose we want to look at a
    new sentence in the text where ![](img/Formula_B16391_06_114.png) is a subject
    with a different gender. In the forget gate, we forgot about the gender previously
    stored in the cell state vector. Now, we need to fill in the void and push the
    new gender into memory – that is, into the new cell state vector.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了什么？我们将新的内容添加到了之前的单元状态向量中，![](img/Formula_B16391_06_113.png)。假设我们要查看文本中的一个新句子，其中
    ![](img/Formula_B16391_06_114.png) 是一个性别不同的主语。在遗忘门中，我们忘记了先前存储在单元状态向量中的性别。现在，我们需要填补空缺并将新的性别推入记忆中——也就是推入新的单元状态向量。
- en: The Output Gate
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出门
- en: Finally, the output gate! We have the new cell state, ![](img/Formula_B16391_06_115.png),
    to pass to the next copy of the unit; we just need to output something for this
    current time, ![](img/Formula_B16391_06_116.png).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出门！我们有新的单元状态，![](img/Formula_B16391_06_115.png)，将其传递到下一个单元副本；我们只需要输出当前时刻的某些内容，![](img/Formula_B16391_06_116.png)。
- en: 'Again, like all other gates, the output gate applies a sigmoid function to
    all components of the input vector, ![](img/Formula_B16391_06_117.png), and of
    the previous output vector, ![](img/Formula_B16391_06_118.png), in order to decide
    what to block and what to pass from the newly created state vector, ![](img/Formula_B16391_06_119.png),
    into the final output vector, ![](img/Formula_B16391_06_120.png). All decisions,
    ![](img/Formula_B16391_06_121.png), are then pointwise multiplied by the newly
    created state vector, ![](img/Formula_B16391_06_122.png), previously normalized
    through a ![](img/Formula_B16391_06_123.png) function to fall in ![](img/Formula_B16391_06_124.png):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，像所有其他门一样，输出门对输入向量的所有成分应用一个Sigmoid函数，![](img/Formula_B16391_06_117.png)，以及先前的输出向量，![](img/Formula_B16391_06_118.png)，以决定从新创建的状态向量，![](img/Formula_B16391_06_119.png)，传递到最终输出向量，![](img/Formula_B16391_06_120.png)时，应该阻止什么，传递什么。所有决策，![](img/Formula_B16391_06_121.png)，然后与新创建的状态向量，![](img/Formula_B16391_06_122.png)，逐点相乘，该状态向量通过![](img/Formula_B16391_06_123.png)函数进行标准化，以确保其在![](img/Formula_B16391_06_124.png)范围内：
- en: '![](img/Formula_B16391_06_125.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_125.png)'
- en: '![](img/Formula_B16391_06_126.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_126.png)'
- en: This is with a new set of weights, ![](img/Formula_B16391_06_127.png), for this
    output gate.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个新权重集，![](img/Formula_B16391_06_127.png)，用于这个输出门。
- en: In this case, the output vector, ![](img/Formula_B16391_06_128.png), and the
    state vector, ![](img/Formula_B16391_06_129.png), produced by the LSTM recurrent
    unit are different, ![](img/Formula_B16391_06_130.png) being a filtered version
    of ![](img/Formula_B16391_06_131.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由LSTM递归单元生成的输出向量，![](img/Formula_B16391_06_128.png)，和状态向量，![](img/Formula_B16391_06_129.png)，是不同的，![](img/Formula_B16391_06_130.png)，是![](img/Formula_B16391_06_131.png)的过滤版本。
- en: Why do we need a different output from the unit cell state? Well, sometimes
    the output needs to be something different from the memory. For example, while
    the cell state is supposed to carry the memory of the gender to the next unit
    copy, the output might be required to produce the number, plural or singular,
    of the subject rather than its gender.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要单元状态与输出不同的结果呢？嗯，有时输出需要与记忆不同。例如，当单元状态应该将性别记忆传递给下一个单元副本时，输出可能需要产生主语的数量（复数或单数），而不是它的性别。
- en: LSTM layers are a very powerful recurrent architecture, capable of keeping the
    memory of a large number of previous inputs. These layers thus fit – and are often
    used to solve – problems involving ordered sequences of data. If the ordered sequences
    of data are sorted based on time, then we talk about time series. Indeed, LSTM-based
    RNNs have been applied often and successfully to time series analysis problems.
    A classic task to solve in time series analysis is demand prediction. In the next
    section, we will explore an application of LSTM-based neural networks to solve
    a demand prediction problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM层是一种非常强大的递归结构，能够保持大量前期输入的记忆。因此，这些层非常适合——并且常常用于解决——涉及有序数据序列的问题。如果有序数据序列是按时间排序的，那么我们就称之为时间序列。实际上，基于LSTM的RNN已被广泛应用并成功解决时间序列分析问题。时间序列分析中的经典任务之一是需求预测。在接下来的部分中，我们将探讨将基于LSTM的神经网络应用于需求预测问题的一个实例。
- en: The Demand Prediction Problem
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求预测问题
- en: Let's continue then by exploring a demand prediction problem and how it can
    be treated as a time series analysis problem.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们继续，探讨一个需求预测问题，以及它如何被视为时间序列分析问题。
- en: Demand prediction is a task related to the need to make estimates about the
    future. We all agree that knowing what lies ahead in the future makes life much
    easier. This is true for life events as well as, for example, the prices of washing
    machines and refrigerators, or demand for electrical energy in an entire city.
    Knowing how many bottles of olive oil customers will want tomorrow or next week
    allows for better restocking plans in retail stores. Knowing of a likely increase
    in the demand for gas or diesel allows a trucking company to better plan its finances.
    There are countless examples where this kind of knowledge of the future can be
    of help.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 需求预测是一个与未来估计需求相关的任务。我们都同意，了解未来的情况会让生活变得更加轻松。这对于生活事件同样适用，比如洗衣机和冰箱的价格，或者整个城市的电力需求。知道客户明天或下周将需要多少瓶橄榄油，可以让零售商制定更好的补货计划。知道可能会增加对汽油或柴油的需求，可以帮助卡车公司更好地规划其财务。未来需求的这种知识在无数的实例中都能提供帮助。
- en: Demand Prediction
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求预测
- en: '**Demand prediction**, or demand forecasting, is a big branch of data science.
    Its goal is to make estimations about future demand using historical data and
    possibly other external information. Demand prediction can refer to any kind of
    numbers: visitors to a restaurant, generated kW/h, school new registrations, beer
    bottles, diaper packages, home appliances, fashion clothing and accessories, and
    so on. Demand forecasting may be used in production planning, inventory management,
    and at times in assessing future capacity requirements, or in making decisions
    on whether to enter a new market.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**需求预测**，或称需求预报，是数据科学的一个重要分支。它的目标是通过使用历史数据和可能的其他外部信息，来对未来需求进行估计。需求预测可以指任何类型的数字：餐馆的访客数、生成的千瓦时、学校的新注册人数、啤酒瓶、纸尿裤包、家电、时尚服装和配饰等等。需求预测可用于生产计划、库存管理，有时还用于评估未来的产能需求，或者在决策是否进入新市场时提供参考。'
- en: Demand prediction techniques are usually based on time series analysis. Previous
    values of demand for a given product, goods, or service are stored and sorted
    over time to form a time series. When past values in the time series are used
    to predict future values in the same time series, we are talking about autoregressive
    analysis techniques. When past values from other external time series are also
    used to predict future values in the time series, then we are talking about multi-regression
    analysis techniques.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 需求预测技术通常基于时间序列分析。某一特定产品、商品或服务的历史需求值被存储并按时间排序形成时间序列。当利用时间序列中的历史值来预测同一时间序列中的未来值时，我们称之为自回归分析技术。当来自其他外部时间序列的历史值也被用来预测时间序列中的未来值时，我们则称之为多元回归分析技术。
- en: '**Time series analysis** is a field of data science with a lot of tradition,
    as it already offers a wide range of classical techniques. Traditional forecasting
    techniques stem from statistics and their top techniques are found in the **Autoregressive
    Integrated Moving Average** (**ARIMA**) model and its variations. These techniques
    require the assumption of a number of statistical hypotheses, are hard to verify,
    and are often not realistic. On the other hand, they are satisfied with a relatively
    small amount of past data.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间序列分析**是数据科学的一个传统领域，它已经提供了多种经典技术。传统的预测技术源自统计学，它们的顶尖技术可以在**自回归积分滑动平均**（**ARIMA**）模型及其变种中找到。这些技术要求假设一些统计假设，难以验证，而且通常不现实。另一方面，它们对于过去数据的要求较少。'
- en: Recently, with the growing popularity of **machine learning** algorithms, a
    few data-based regression techniques have also been applied to demand prediction
    problems. The advantages of these machine learning techniques consist of the absence
    of required statistical hypotheses and less overhead in data transformation. The
    disadvantages consist of the need for a larger amount of data. Also, notice that
    in the case of time series where all required statistical hypotheses are verified,
    traditional methods tend to perform better.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着**机器学习**算法的日益普及，一些基于数据的回归技术也被应用于需求预测问题。这些机器学习技术的优点是无需统计假设，且数据转换的开销较小。缺点是需要大量的数据。此外，需要注意的是，在所有必需的统计假设都得到验证的时间序列情况下，传统方法通常表现更好。
- en: Let's try to predict the next ![](img/Formula_B16391_06_132.png) values in the
    time series based on the past ![](img/Formula_B16391_06_133.png) values. When
    using a machine learning model for time series analysis, such as, for example,
    linear regression or a regression tree, we need to supply the vector of the past
    ![](img/Formula_B16391_06_021.png) samples as input to train the model to predict
    the next ![](img/Formula_B16391_06_132.png) values. While this strategy is commonly
    implemented and yields satisfactory results, it is still a static approach to
    time series analysis – **static** in the sense that each output response depends
    only on the corresponding input vector. The order of presentation of input samples
    to the model does not influence the response. There is no concept of an input
    sequence, but just of an input vector.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试根据过去的 ![](img/Formula_B16391_06_133.png) 值预测时间序列中下一个 ![](img/Formula_B16391_06_132.png)
    值。当使用机器学习模型进行时间序列分析时，例如线性回归或回归树，我们需要提供过去 ![](img/Formula_B16391_06_021.png) 个样本的向量作为输入，以训练模型预测下一个
    ![](img/Formula_B16391_06_132.png) 值。虽然这种策略已经被广泛实施并取得了令人满意的结果，但它仍然是时间序列分析的静态方法——**静态**指的是每个输出响应仅依赖于对应的输入向量。输入样本的呈现顺序不会影响响应。没有输入序列的概念，只有输入向量的概念。
- en: Tip
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: KNIME Analytics Platform offers a few nodes and standard components to deal
    with time series analysis. The key node here is the `EXAMPLES/00_Components/Time
    Series` folder in the `statsmodels` Python module in the background. Because of
    that, they require the installation of the KNIME Python integration ([https://www.knime.com/blog/setting-up-the-knime-python-extension-revisited-for-python-30-and-20](https://www.knime.com/blog/setting-up-the-knime-python-extension-revisited-for-python-30-and-20)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: KNIME Analytics Platform 提供了几个节点和标准组件来处理时间序列分析。这里的关键节点是后台 `statsmodels` Python
    模块中的 `EXAMPLES/00_Components/Time Series` 文件夹。因此，它们需要安装 KNIME Python 集成（[https://www.knime.com/blog/setting-up-the-knime-python-extension-revisited-for-python-30-and-20](https://www.knime.com/blog/setting-up-the-knime-python-extension-revisited-for-python-30-and-20)）。
- en: 'In *Figure 6.6*, you can see the list of available components for time series
    analysis tasks within KNIME Analytics Platform:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 6.6* 中，你可以看到 KNIME Analytics Platform 中用于时间序列分析任务的可用组件列表：
- en: '![Figure 6.6 – The EXAMPLES/00_Components/Time Series folder contains components
    dedicated to time series analysis](img/B16391_06_006.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – EXAMPLES/00_Components/Time Series 文件夹包含专门用于时间序列分析的组件](img/B16391_06_006.jpg)'
- en: Figure 6.6 – The EXAMPLES/00_Components/Time Series folder contains components
    dedicated to time series analysis
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – EXAMPLES/00_Components/Time Series 文件夹包含专门用于时间序列分析的组件
- en: All things considered, these machine learning-based strategies, using regression
    models, do not fully exploit the sequential structure of the data, where the fact
    that ![](img/Formula_B16391_06_136.png) comes after ![](img/Formula_B16391_06_137.png)
    carries some additional information. This is where RNNs, and particularly LSTMs,
    might offer an edge on the other machine learning algorithms, thanks to their
    internal **memory**.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 综合考虑，这些基于机器学习的策略，使用回归模型，并没有充分利用数据的顺序结构，其中！[](img/Formula_B16391_06_136.png)紧接着！[](img/Formula_B16391_06_137.png)带来了一些额外信息。正是在这里，RNN（递归神经网络），特别是
    LSTM（长短时记忆网络），由于其内部的**记忆**，可能会在其他机器学习算法中提供优势。
- en: 'Let''s now introduce the case study for this chapter: predicting energy demand
    in **kilowatts** (**kW**) needed by the hour.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们引入本章的案例研究：预测每小时所需的 **千瓦**（**kW**）电力需求。
- en: Predicting Energy Demand
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电力需求预测
- en: As an example of demand prediction, we want to tackle the problem of electrical
    energy prediction – that is, of predicting the number of kW needed in the next
    hour by an average household consumer.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 作为需求预测的一个例子，我们想解决电力预测问题——即预测一个普通家庭消费者下一小时所需的千瓦数（kW）。
- en: One of the hardest problems in the energy industry is matching supply and demand.
    On the one hand, over-production of energy can be a waste of resources; on the
    other hand, under-production can leave people without the basic commodities of
    modern life. The prediction of electrical energy demand at each point in time
    is therefore a very important topic in data science.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 能源行业最棘手的问题之一是供需匹配。一方面，能源过度生产可能浪费资源；另一方面，生产不足可能导致人们无法获得现代生活所需的基本商品。因此，准确预测每一时刻的电力需求是数据科学中的一个重要课题。
- en: For this reason, a couple of years ago energy companies started to monitor the
    electricity consumption of each household, store, or other entity, by means of
    smart meters. A pilot project was launched in 2009 by the Irish **Commission for
    Energy Regulation** (**CER**).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，几年前，能源公司开始通过智能电表监控每个家庭、商店或其他实体的电力消耗。爱尔兰**能源监管委员会**（**CER**）在2009年启动了一个试点项目。
- en: The Smart Metering Electricity **Customer Behaviour Trials** (**CBTs**) took
    place between 2009 and 2010 with over 5,000 Irish homes and businesses participating.
    The purpose of the trials was to assess the impact on consumers' electricity consumption,
    in order to inform the cost-benefit analysis for a national rollout. Electric
    Ireland residential and business customers and Bord Gáis Energy business customers
    who participated in the trials had an electricity smart meter installed in their
    homes or on their premises and agreed to take part in the research to help establish
    how smart metering can help shape energy usage behaviors across a variety of demographics,
    lifestyles, and home sizes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 智能电表电力**客户行为试验**（**CBTs**）于2009年到2010年之间进行，超过5,000个爱尔兰家庭和企业参与。试验的目的是评估智能电表对消费者电力消耗的影响，以便为全国推广的成本效益分析提供依据。参与试验的爱尔兰电力住宅和商业客户，以及博尔吉能源的商业客户，均在其家中或场所安装了智能电表，并同意参与研究，帮助确定智能电表如何有助于塑造不同人口群体、生活方式和家庭规模的能源使用行为。
- en: The original dataset contains over 5,000 time series, each one measuring the
    electricity usage for each installed smart meter for a bit over a year. All original
    time series have been aligned and standardized to report energy measures by the
    hour.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据集包含超过5,000个时间序列，每个时间序列测量每个安装的智能电表在略超过一年的时间内的电力使用情况。所有原始时间序列已被对齐并标准化，按小时报告能源数据。
- en: 'The final goal is to predict energy demand across all users. At this point,
    we have a dilemma: should we train one model for each time series and sum up all
    predictions to get the demand in the next hour or should we train one single model
    on all time series to get the global demand for the next hour?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是预测所有用户的能源需求。目前，我们面临一个困境：是为每个时间序列训练一个模型，然后将所有预测结果相加以得到下一个小时的需求，还是为所有时间序列训练一个单一模型，以获得下一个小时的全球需求？
- en: Training one model on a single time series is easier and probably more accurate.
    However, training 5,000 models (and probably more in real life) can pose a few
    technical problems. Training one single model on all time series might not be
    that accurate. As expected, a compromise solution was implemented. Smart energy
    meters have been clustered based on energy usage profile, and the average time
    series of hourly energy usage for each cluster has been calculated. The goal now
    is to calculate the energy demand in the next hour for each clustered time series,
    weight it by the cluster size, and then sum up all contributions to find the final
    total energy demand for the next hour.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在单一时间序列上训练一个模型较为简单，且可能更为准确。然而，训练5,000个模型（在实际情况中可能更多）可能会遇到一些技术问题。在所有时间序列上训练一个单一模型可能不那么准确。正如预期的那样，实施了一种折衷方案。智能电表已根据能源使用概况进行聚类，并计算出每个聚类的平均小时能源使用时间序列。现在的目标是计算每个聚类时间序列在下一个小时的能源需求，对其按聚类大小加权，然后将所有贡献加总，以得出下一个小时的总能源需求。
- en: Thirty smart meter clusters have been detected based on the energy used on business
    days versus the weekend, at different times over the 24 hours, and the average
    hourly consumption.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 已经根据工作日与周末的能源使用差异、24小时内的不同时间段以及平均小时消耗，检测到30个智能电表聚类。
- en: 'More details on this data preparation procedure can be found in the *Data Chef
    ETL Battles. What can be prepared with today''s data? Ingredient Theme: Energy
    Consumption Time Series* blog post, available at https://www.knime.com/blog/EnergyConsumptionTimeSeries,
    and in the *Big Data, Smart Energy, and Predictive Analytics* whitepaper, available
    at [https://files.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf](https://files.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此数据准备过程的更多细节，可以参考*数据厨师ETL战斗：今天的数据可以做什么？主题：能源消耗时间序列*的博客，网址为 [https://www.knime.com/blog/EnergyConsumptionTimeSeries](https://www.knime.com/blog/EnergyConsumptionTimeSeries)，以及*大数据、智能能源与预测分析*的白皮书，网址为
    [https://files.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf](https://files.knime.com/sites/default/files/inline-images/knime_bigdata_energy_timeseries_whitepaper.pdf)。
- en: 'The final dataset contains 30 time series of average energy usage by the 30
    clusters. Each time series shows the electrical profile of a given cluster of
    smart meters: from stores (high energy consumption from 9 a.m. to 5 p.m. on business
    days) to nightly business customers (high energy consumption from 9 p.m. to 6
    a.m. every day), from family households (high energy consumption from 7 a.m. to
    9 a.m. and then again from 6 p.m. to 10 p.m. every business day) to other unclear
    entities (using energy across 24 hours on all 7 days of the week). For example,
    cluster 26 refers to stores (*Figure 6.7*). Here, electrical energy is used mainly
    between 9 a.m. and 5 p.m. on all business days:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的数据集包含30个簇的平均能耗的30条时间序列。每个时间序列显示了一个给定簇的智能电表的电力使用情况：从商店（在工作日从早上9点到下午5点有较高的能耗）到夜间商业用户（每天晚上9点到第二天早上6点有较高的能耗），从家庭住户（在工作日早上7点到9点以及下午6点到10点之间有较高的能耗）到其他不明确的实体（在一周7天的24小时内均有能耗）。例如，第26簇指的是商店（*图6.7*）。在这里，电能主要在所有工作日的早上9点到下午5点之间使用：
- en: '![Figure 6.7 – Plot of energy usage by the hour for cluster 26](img/B16391_06_007.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 第26簇按小时划分的能耗图](img/B16391_06_007.jpg)'
- en: Figure 6.7 – Plot of energy usage by the hour for cluster 26
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 第26簇按小时划分的能耗图
- en: 'On the opposite side, cluster 13 includes a number of restaurants (*Figure
    6.8*), where the energy usage is pushed to the evening, mainly from 6 p.m. to
    midnight, every day of the week:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，第13簇包括许多餐厅（*图6.8*），其能耗主要集中在晚上，从每周的每天6点到午夜。
- en: '![Figure 6.8 – Plot of energy usage by the hour for cluster 13](img/B16391_06_008.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 第13簇按小时划分的能耗图](img/B16391_06_008.jpg)'
- en: Figure 6.8 – Plot of energy usage by the hour for cluster 13
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 第13簇按小时划分的能耗图
- en: Notice that cluster 26 is the poster child for time series analysis, with a
    clear seasonality on the 24 hours in a day and the 7 days of the week series.
    In this chapter, we will continue with an autoregressive analysis of cluster 26's
    time series. The goal will be to predict the average energy usage in the next
    hour, based on the average energy usage in the past ![](img/Formula_B16391_06_138.png)
    hours, for cluster 26.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第26簇是时间序列分析的典型例子，具有明显的季节性特征，包括一天24小时和一周7天的周期性变化。在本章中，我们将继续对第26簇的时间序列进行自回归分析。我们的目标是基于过去的平均能耗来预测下一小时的平均能耗！[](img/Formula_B16391_06_138.png)，并且这个预测将适用于第26簇。
- en: Now that we have a set of time series describing the usage of electrical energy
    by the hour for clusters of users, we will try to perform some predictions of
    future usage for each cluster. Let's focus first on the data preparation for this
    time series problem.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了一组描述用户簇按小时使用电能的时间序列数据，我们将尝试对每个簇的未来使用情况进行预测。首先，让我们聚焦于这个时间序列问题的数据准备工作。
- en: Data Preparation – Creating the Past
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备 – 创建过去的数据
- en: 'Let''s now implement in practice a demand prediction application, using the
    time series for cluster 26\. Again, we will have two separate workflows: one to
    train the LSTM-based RNN and one to deploy it in production. Both applications
    will include a data preparation phase, which must be exactly the same for both.
    In this section, we will go through this data preparation phase.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实际实现一个需求预测应用，使用第26簇的时间序列数据。我们将有两个独立的工作流程：一个用于训练基于LSTM的RNN，另一个用于将其部署到生产环境中。两个应用程序都会包括一个数据准备阶段，且该阶段在两者中必须完全相同。在本节中，我们将介绍这个数据准备阶段。
- en: 'Dealing with **time series**, the **data preparation** steps are slightly different
    from what is implemented in other classification or clustering applications. Let''s
    go through these steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 处理**时间序列**时，**数据准备**步骤与其他分类或聚类应用中的实现略有不同。让我们逐步了解这些步骤：
- en: '**Data loading**: Read from the file the time series of the average hourly
    used energy for the 30 identified clusters and the corresponding times.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加载**：从文件中读取30个已识别簇的每小时平均能耗的时间序列及相应的时间。'
- en: '**Date and time standardization**: Time is usually read as a string from the
    file. To make sure that it is processed appropriately, it is best practice to
    transform it into a **Date&Time** object. A number of nodes are available to deal
    with Date&Time objects in an appropriate and easy way, but especially in a standardized
    way.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日期和时间标准化**：时间通常是以字符串的形式从文件中读取的。为了确保它能够被适当地处理，最好的做法是将其转换为**日期和时间**对象。有许多节点可以以适当且简单的方式处理日期和时间对象，尤其是以标准化的方式。'
- en: '**Timestamp alignment**: Once the time series has been loaded, we need to make
    sure that its sampling has been consistent with no time holes. Possible time holes
    need to be filled with missing values. We also need to make sure that the data
    of the time series has been time-sorted.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间戳对齐**：一旦时间序列被加载，我们需要确保它的采样是一致的，没有时间空缺。可能的时间空缺需要用缺失值填充。我们还需要确保时间序列的数据是按时间排序的。'
- en: '**Partitioning**: Here, we need to create a training set to train the network
    and a test set to evaluate its performance. Differently from classification problems,
    here we need to respect the time order so as not to mix the past and future of
    the time series in the same set. Past samples should be reserved for the training
    set and future samples for the test set.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分割**：在这里，我们需要创建一个训练集来训练网络，以及一个测试集来评估其性能。与分类问题不同，这里我们需要遵循时间顺序，以避免在同一数据集中混淆时间序列的过去和未来。过去的样本应保留用于训练集，未来的样本用于测试集。'
- en: '**Missing value imputation**: Missing value imputation for time series is also
    different from missing value imputation in a static dataset. Since what comes
    after depends on what was there before, most techniques of missing value imputation
    for time series are based on previous and/or the following sample values.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失值填补**：时间序列的缺失值填补与静态数据集的缺失值填补也不同。由于后续的内容依赖于之前的内容，大多数时间序列的缺失值填补技术都是基于先前和/或后续样本的值。'
- en: '**Creating the input vector of past samples**: Once the time series is ready
    for analysis, we need to build the tensors to feed the network. The tensors must
    consist of ![](img/Formula_B16391_06_021.png) past samples that the network will
    use to predict the value for the next sample in time. So, we need to produce sequences
    of ![](img/Formula_B16391_03_173.png) past ![](img/Formula_B16391_06_141.png)-dimensional
    vectors (the ![](img/Formula_B16391_06_142.png) past samples) for all training
    and test records.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建过去样本的输入向量**：一旦时间序列准备好进行分析，我们需要构建张量以供网络使用。这些张量必须包含![](img/Formula_B16391_06_021.png)过去样本，网络将使用这些样本预测下一个时间点的值。因此，我们需要为所有训练和测试记录生成![](img/Formula_B16391_03_173.png)过去的![](img/Formula_B16391_06_141.png)维向量（这些![](img/Formula_B16391_06_142.png)过去的样本）。'
- en: '**Creating the list to feed the network**: Finally, the input tensors of past
    samples must be transformed into a list of values, as this is the input format
    required by the network.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建供网络使用的列表**：最后，过去样本的输入张量必须转换为值的列表，因为这是网络所需的输入格式。'
- en: Let's start with data loading.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据加载开始。
- en: Data Loading and Standardization
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载和标准化
- en: 'The dataset is read from a **CSV** file via the **File Reader** node: 30 time
    series and one date column. The date column is imported by the File Reader node
    as a string and must be converted into a Date&Time object to make sure it is processed
    – for example, sorted – appropriately in the next steps. **Date&Time** is the
    internal standard object to represent date and time entities in KNIME Analytics
    Platform. In order to convert a string into a Date&Time object, we use the **String
    to Date&Time** node:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是通过**CSV**文件通过**文件读取器**节点读取的：30个时间序列和一个日期列。日期列由文件读取器节点作为字符串导入，必须转换为Date&Time对象，以确保它在接下来的步骤中被适当地处理——例如，排序。**Date&Time**是KNIME
    Analytics平台中表示日期和时间实体的内部标准对象。为了将字符串转换为Date&Time对象，我们使用**字符串到日期和时间**节点：
- en: '![Figure 6.9 – The String to Date&Time node and its configuration window](img/B16391_06_009.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 字符串到日期和时间节点及其配置窗口](img/B16391_06_009.jpg)'
- en: Figure 6.9 – The String to Date&Time node and its configuration window
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 字符串到日期和时间节点及其配置窗口
- en: In the configuration window (*Figure 6.9*), you must select the string input
    columns containing the date and/or time information and define the date/time format.
    You can do this manually, by providing a string format – for example, as `dd.mm.yyyy`,
    where `dd` indicates the day, `mm` the month, and `yyyy` the year.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置窗口（*图 6.9*）中，您必须选择包含日期和/或时间信息的字符串输入列，并定义日期/时间格式。您可以手动进行此操作，提供一个字符串格式——例如，`dd.mm.yyyy`，其中`dd`表示天，`mm`表示月，`yyyy`表示年。
- en: 'For example, if you have date format of `day(2).month(2).year(4)`, you can
    manually add the option `dd.MM.yyyy`, if this is not available in the **Date format**
    options. When manually adding the date/time type, you must select the appropriate
    **New type** option: **Date or Time** or **Date&time**.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的日期格式是`day(2).month(2).year(4)`，你可以手动添加选项`dd.MM.yyyy`，如果该格式在**日期格式**选项中不可用。当手动添加日期/时间类型时，必须选择合适的**新类型**选项：**日期或时间**或**日期&时间**。
- en: Alternatively, you can provide the date/time format automatically, by pressing
    the **Guess data type and format** button. With this last option, KNIME Analytics
    Platform will parse your string to find out the date/time format. It works most
    of the time! If it does not, you can always revert to manually entering the date/time
    format.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以通过点击**自动猜测数据类型和格式**按钮来自动提供日期/时间格式。选择此选项时，KNIME分析平台会解析你的字符串以找出日期/时间格式。它通常能正常工作！如果无法识别格式，你总是可以手动输入日期/时间格式。
- en: Tip
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: In the node description of the **String to Data&Time** node, you can find an
    overview of possible placeholders in the format structures. The most important
    ones are **y** for year, **M** for month in year, **d** for day of month, **H**
    for hour of day (between 0 and 23), **m** for minute of hour, and **s** for second
    of minute. Many more placeholders are supported – for example, **W** for week
    of month or **D** for day of year.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在**字符串到日期&时间**节点的节点描述中，你可以找到格式结构中可能使用的占位符概述。最重要的占位符包括**y**表示年份，**M**表示年份中的月份，**d**表示月份中的日期，**H**表示一天中的小时（0到23之间），**m**表示小时中的分钟，**s**表示分钟中的秒。还支持更多占位符——例如，**W**表示月份中的周数，**D**表示一年中的天数。
- en: The String to Date&Time node is just one of the many nodes that deals with Date&Time
    objects, all contained in the **Other Data Types/Time Series** folder in the **Node
    Repository** panel. Some nodes manipulate Date&Time objects, such as, for example,
    to calculate a time difference or produce a time shift; other nodes are used to
    convert Date&Time objects from one format to another.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: “字符串到日期&时间”节点只是众多处理日期&时间对象的节点之一，所有这些节点都位于**其他数据类型/时间序列**文件夹中，位于**节点库**面板内。一些节点用于操作日期&时间对象，例如计算时间差或生成时间偏移；而其他节点则用于将日期&时间对象从一种格式转换为另一种格式。
- en: After that, the Column Filter node is inserted to isolate the time series for
    cluster 26 only. The only required standardization here was about the date conversion
    from a string to a Date&Time object. We can now move on to data cleaning.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，插入“列过滤器”节点，以便仅隔离集群26的时间序列。这里唯一需要标准化的是将日期从字符串转换为日期和时间对象。我们现在可以继续进行数据清理了。
- en: Data Cleaning and Partitioning
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清理与分区
- en: The **Timestamp Alignment** component is inserted to check for time holes in
    the time series. This component checks whether the selected timestamp column is
    uniformly sampled in the selected time scale. Missing values will be inserted
    at skipped sampling times. In this case, it checks whether the **rowID** column,
    containing the timestamps, has missing sampling times considering an hourly sampling
    rate.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间戳对齐**组件被插入以检查时间序列中的时间空缺。此组件检查所选的时间戳列是否在选定的时间尺度上均匀采样。缺失的值将在跳过的采样时间点插入。在这种情况下，它检查包含时间戳的**rowID**列是否在按小时采样时存在缺失的采样时间。'
- en: The Timestamp Alignment component is part of the time series-dedicated component
    set available in `EXAMPLES/00_Components/Time Series`. To create an instance in
    your workflow, just drag and drop it into the workflow editor or double-click
    it.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳对齐组件是`EXAMPLES/00_Components/Time Series`文件夹中专门针对时间序列的组件集合的一部分。要在工作流中创建一个实例，只需将其拖放到工作流编辑器中或双击它。
- en: 'After that, we partition the data into a training set and test set, to train
    the LSTM-based RNN and evaluate it. We have not provided an additional validation
    set here to evaluate the network performance throughout the training process.
    We decided to keep things simple and just provide a training set to the Keras
    Network Learner node and a test set to measure the error on the time series prediction
    task:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将数据划分为训练集和测试集，用于训练基于LSTM的RNN并进行评估。这里我们没有提供额外的验证集来评估网络在训练过程中的表现。我们决定保持简单，只向Keras网络学习节点提供训练集，测试集用于测量时间序列预测任务的误差：
- en: '![Figure 6.10 – The Partitioning node and its configuration window. Notice
    the Take from top data extraction mode for time series analysis](img/B16391_06_010.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 分区节点及其配置窗口。注意用于时间序列分析的从顶部提取数据模式](img/B16391_06_010.jpg)'
- en: Figure 6.10 – The Partitioning node and its configuration window. Notice the
    Take from top data extraction mode for time series analysis
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 分区节点及其配置窗口。注意用于时间序列分析的从顶部提取数据模式
- en: 'To separate the input dataset into training and test sets, we use again a **Partitioning**
    node. Here, we decided to implement an 80%–20% split: 80% of the input data will
    be directed toward training and 20% toward testing. In addition, we set the extraction
    procedure to **Take from top** (*Figure 6.10*). In a time series analysis problem,
    we want to keep the intrinsic time order of the data: we use the past to train
    the network and the future to test it. When using the **Take from top** data extraction
    option, the top percentage of the data is designated to the top output port, while
    the remaining at the bottom to the lower output port. If the data is time-sorted
    from past to future, then this data extraction modality preserves the time order
    of the data.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将输入数据集划分为训练集和测试集，我们再次使用**分区**节点。在这里，我们决定实现80%–20%的划分：80%的输入数据将用于训练，20%用于测试。此外，我们将提取程序设置为**从顶部提取**（*图6.10*）。在时间序列分析问题中，我们希望保持数据的固有时间顺序：我们使用过去的数据来训练网络，用未来的数据来测试网络。当使用**从顶部提取**的数据提取选项时，数据的顶部百分比将被分配到顶部输出端口，而底部剩余部分将分配到底部输出端口。如果数据按照时间顺序从过去到未来排列，则这种数据提取模式能够保持数据的时间顺序。
- en: Important note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In a time series analysis problem, partitioning should use the **Take from top**
    data extraction modality, in order to preserve the time order of the data and
    use the past for training and the future for testing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列分析问题中，分区应该使用**从顶部提取**数据提取模式，以便保持数据的时间顺序，并用过去的数据进行训练，未来的数据进行测试。
- en: As for every dataset, the operation for missing value imputation is an important
    one; first, because neural networks cannot deal with missing values and second,
    because choosing the right missing value imputation technique can affect your
    final results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个数据集，缺失值插补操作都非常重要；首先，因为神经网络无法处理缺失值，其次，因为选择正确的缺失值插补技术可能会影响最终结果。
- en: Important note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Missing value imputation must be implemented after the Timestamp Alignment component
    since this component, by definition, creates missing values.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 缺失值插补必须在时间戳对齐组件之后实现，因为该组件定义上会创建缺失值。
- en: 'In [*Chapter 4*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101), *Building
    and Training a Feedforward Network*, we already introduced the **Missing Value**
    node and its different strategies to impute missing values. Some of these strategies
    are especially useful when it comes to sequential data, as they take the previous
    and/or following values in a time series into account. Possible strategies are
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101)《构建和训练前馈网络》中，我们已经介绍了**缺失值**节点及其不同的缺失值插补策略。这些策略中的一些在处理序列数据时特别有用，因为它们考虑了时间序列中的前后值。可能的策略如下：
- en: '**Average/linear interpolation**, replacing the missing value with the average
    value of previous and next sample'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均/线性插值**，用前后样本的平均值替代缺失值'
- en: '**Moving average**, replacing the missing value with the mean value of the
    sample window'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移动平均**，用样本窗口的均值替代缺失值'
- en: '**Next**, replacing the missing value with the value of the next sample'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下一个**，用下一个样本的值替代缺失值'
- en: '**Previous**, replacing the missing value with the value of the previous sample'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前一个**，用前一个样本的值替代缺失值'
- en: 'We went for linear interpolation between the previous and next values to impute
    missing values in the time series (*Figure 6.11*):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了在前后值之间进行线性插值来插补时间序列中的缺失值（*图6.11*）：
- en: '![Figure 6.11 – The Missing Value node and its configuration window](img/B16391_06_011.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 缺失值节点及其配置窗口](img/B16391_06_011.jpg)'
- en: Figure 6.11 – The Missing Value node and its configuration window
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 缺失值节点及其配置窗口
- en: The formula to use for missing value imputation is calculated in the **Missing
    Value** node on the training data, applied to the test data with the **Missing
    Value (Apply)** node, and saved to a file through the Model Writer node. The pure
    application on the test set of the formula defined on the training set prevents
    the test data from interfering with the implementation of any transformation required
    for model training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 用于缺失值插补的公式是在训练数据上的**Missing Value**节点计算的，应用于测试数据时使用**Missing Value (Apply)**节点，并通过模型写入节点保存到文件。仅在测试集上应用定义在训练集上的公式，能够防止测试数据干扰任何模型训练所需的转换实现。
- en: Let's focus next on the creation of input tensors for the neural network.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注神经网络输入张量的创建。
- en: Creating the Input Tensors
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建输入张量
- en: We have read the data, converted the date cells into Date&Time objects, isolated
    the time series for cluster 26, assigned missing values to missing sampling steps,
    partitioned the data into 80% for the training set and 20% for the test set, applied
    a linear interpolation between previous and next value for missing value imputation.
    The data is ready, it is now time to create the input tensors for the neural network.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经读取了数据，将日期单元格转换为 Date&Time 对象，隔离了集群 26 的时间序列，为缺失的采样步骤分配了缺失值，将数据划分为 80% 的训练集和
    20% 的测试集，使用线性插值法填补缺失值。数据已经准备好，现在是时候为神经网络创建输入张量了。
- en: Important note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: A key node to create vectors of past samples that is so often needed in time
    series analysis is the Lag Column node.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在时间序列分析中经常需要创建过去样本向量的关键节点是 Lag Column 节点。
- en: '*Figure 6.12* shows the Lag Column node and its configuration window:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.12* 显示了 Lag Column 节点及其配置窗口：'
- en: '![Figure 6.12 – The Lag Column node and its configuration window](img/B16391_06_012.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – Lag Column 节点及其配置窗口](img/B16391_06_012.jpg)'
- en: Figure 6.12 – The Lag Column node and its configuration window
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – Lag Column 节点及其配置窗口
- en: The **Lag Column** node makes copies of the selected column and shifts them
    down a number, ![](img/Formula_B16391_06_143.png), of cells, where ![](img/Formula_B16391_06_144.png)
    cells, where ![](img/Formula_B16391_06_145.png) is the lag interval and ![](img/Formula_B16391_06_146.png)
    is the **Lag** setting in the configuration window.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lag Column** 节点会生成所选列的副本，并将它们下移一个数量的单元格，![](img/Formula_B16391_06_143.png)，其中
    ![](img/Formula_B16391_06_144.png) 是 Lag 间隔，![](img/Formula_B16391_06_146.png)
    是配置窗口中的**Lag**设置。'
- en: The Lag Column node is a very simple yet very powerful node that comes in handy
    in a lot of situations. If the input column is time-sorted, then shifting down
    the cells corresponds to moving them into the past or the future, depending on
    the time order.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Lag Column 节点是一个非常简单但非常强大的节点，在很多情况下都非常有用。如果输入列已经按时间排序，那么下移单元格的操作就相当于将数据移动到过去或未来，具体取决于时间顺序。
- en: 'In *Figure 6.13*, we explain this concept:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.13*中，我们解释了这个概念：
- en: '![Figure 6.13 – The Lag Column node takes snapshots of the same column at different
    times, as defined by the Lag and Lag interval settings](img/B16391_06_013.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – Lag Column 节点根据 Lag 和 Lag 间隔设置，在不同时间拍摄同一列的快照](img/B16391_06_013.jpg)'
- en: Figure 6.13 – The Lag Column node takes snapshots of the same column at different
    times, as defined by the Lag and Lag interval settings
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – Lag Column 节点根据 Lag 和 Lag 间隔设置，在不同时间拍摄同一列的快照
- en: Considering Lag = 4 and Lag Interval = 2, the Lag Column node produces four
    copies of the selected column, each copy moving backward with a step of 2\. That
    is, besides the selected column at current time *t*, we will also have four snapshots
    of the same column at time *t*-2, *t*-4, *t*-6, and *t*-8 (*Figure 6.13*).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 Lag = 4 和 Lag 间隔 = 2，Lag Column 节点会生成四个所选列的副本，每个副本按 2 的步长向后移动。也就是说，除了当前时间
    *t* 的所选列外，我们还将拥有四个快照，分别是在时间 *t*-2、*t*-4、*t*-6 和 *t*-8 时的同一列（*图 6.13*）。
- en: For our demand prediction problem, we used the values for the average energy
    used by cluster 26 in the immediate 200 past hours to predict the average energy
    need at the current hour. That is, we built an input vector with the 200 immediate
    past samples, using a Lag Column node with Lag=200 and Lag Interval=1 (*Figure
    6.12*).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的需求预测问题，我们使用了过去 200 小时内集群 26 平均能耗的值来预测当前小时的平均能耗。也就是说，我们构建了一个包含过去 200 个小时样本的输入向量，使用了
    Lag Column 节点，Lag=200 和 Lag 间隔=1（*图 6.12*）。
- en: For space reasons, we then transformed the vector of cells into a collection
    of cells using the **Column Aggregator** node, as it is one of the possible formats
    to feed the neural network via the Keras Network Learner node. The Column Aggregator
    node is another way to produce **lists** of data cells. The node groups the selected
    columns per row and aggregates their cells using the selected aggregation method.
    In this case, the **List** aggregation method was selected and applied to the
    200 past values of cluster 26, as created via the Lag Column node.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 出于空间考虑，我们将单元格向量转换为单元格集合，使用**Column Aggregator**节点，因为它是通过Keras Network Learner节点将数据馈送到神经网络的可能格式之一。Column
    Aggregator节点是另一种生成**列表**数据单元格的方式。该节点按行对选定列进行分组，并使用选定的聚合方法对其单元格进行聚合。在本例中，选择并应用了**List**聚合方法，将其应用于通过Lag
    Column节点创建的群集26的200个过去值。
- en: 'The workflow snippet, implementing data preparation part to feed the upcoming
    RNN for the demand prediction problem, is shown in *Figure 6.14*:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 实现数据准备部分，以便为即将到来的RNN进行需求预测任务的工作流片段，如*图6.14*所示：
- en: '![Figure 6.14 – Data preparation for demand prediction: date and time standardization,
    time alignment, missing value imputation, creating the input vector of past samples,
    and partitioning](img/B16391_06_014.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14 – 需求预测的数据准备：日期和时间标准化、时间对齐、缺失值填补、创建过去样本的输入向量以及数据分割](img/B16391_06_014.jpg)'
- en: 'Figure 6.14 – Data preparation for demand prediction: date and time standardization,
    time alignment, missing value imputation, creating the input vector of past samples,
    and partitioning'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 需求预测的数据准备：日期和时间标准化、时间对齐、缺失值填补、创建过去样本的输入向量以及数据分割
- en: The data is ready. Let's now build, train, and test the LSTM-based RNN to predict
    the average demand of electrical energy for cluster 26 at the current hour given
    the average energy used in the previous 200 hours by the same cluster 26.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已经准备好。现在让我们构建、训练并测试基于LSTM的RNN，以预测群集26在当前小时的电能平均需求，给定群集26在过去200小时的平均能耗。
- en: Building, Training, and Deploying an LSTM-Based RNN
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建、训练和部署基于LSTM的RNN
- en: 'Let''s proceed with the next step: building a simple LSTM-based RNN for demand
    prediction. First, we will train the network, then we will test it, and finally,
    we will deploy it. In this case study, we used no validation set for the network
    and we performed no optimization on the static hyperparameters of the network,
    such as, for example, the size of the LSTM layer.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续下一步：构建一个简单的基于LSTM的RNN进行需求预测。首先，我们将训练网络，然后测试它，最后部署它。在这个案例研究中，我们没有为网络使用验证集，并且没有对网络的静态超参数进行优化，例如LSTM层的大小。
- en: A relatively simple network is already achieving good error measures on the
    test set for our demand prediction task, and therefore, we decided to focus this
    section on how to test a model for time series prediction rather than on how to
    optimize the static parameters of a neural network. We looked at the optimization
    loop in [*Chapter 5*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152), *Autoencoder
    for Fraud Detection*. In general, this optimization loop can also be applied to
    optimize network hyperparameters. Let's begin by building an LSTM-based RNN.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对简单的网络已经在我们的需求预测任务的测试集上取得了良好的误差度量，因此，我们决定将本节重点放在如何测试时间序列预测模型，而不是如何优化神经网络的静态参数。我们查看了[*第5章*](B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152)，*用于欺诈检测的自编码器*中的优化循环。通常，这个优化循环也可以应用于优化网络的超参数。现在，让我们开始构建基于LSTM的RNN。
- en: Building the LSTM-Based RNN
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基于LSTM的RNN
- en: 'For this case study, we went for the simplest possible LSTM-based RNN: an RNN
    with just one hidden LSTM layer. So, the final network consists of the following:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们选择了最简单的基于LSTM的RNN：一个只有一个隐藏LSTM层的RNN。因此，最终的网络包括以下内容：
- en: One input layer accepting tensors of 200 past vectors – each past vector being
    just the previous sample, that is, with size 1 – obtained through a Keras Input
    Layer node with Shape = 200, 1.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入层，接收200个过去的向量的张量——每个过去的向量仅为前一个样本，即大小为1——通过Keras输入层节点获取，形状为200, 1。
- en: One hidden layer with 100 LSTM units, accepting the previous tensor as the only
    input, through the **Keras LSTM Layer** node
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有100个LSTM单元的隐藏层，通过**Keras LSTM Layer**节点接受先前的张量作为唯一输入
- en: A classic dense layer as output with just one neuron producing the predicted
    value for the next sample in the time series, obtained through the Keras Dense
    Layer node with the ReLU activation function.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个经典的全连接层作为输出，只有一个神经元生成预测的下一个时间序列样本的值，该值是通过使用带有ReLU激活函数的Keras全连接层节点得到的。
- en: 'The nodes used to build this neural architecture are shown in *Figure 6.15*:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 用于构建此神经网络架构的节点如*图6.15*所示：
- en: '![Figure 6.15 – Building a very basic, very simple LSTM-based RNN](img/B16391_06_015.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – 构建一个非常基础、非常简单的基于LSTM的RNN](img/B16391_06_015.jpg)'
- en: Figure 6.15 – Building a very basic, very simple LSTM-based RNN
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 构建一个非常基础、非常简单的基于LSTM的RNN
- en: Important note
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The size of the input tensor was [200,1], which is a sequence of 200 1-sized
    vectors. If the length of the input sequence is not known, we can use *?* to indicate
    unknown sequence length. The NLP case studies in the next chapter will show you
    some examples of this.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输入张量的大小为[200,1]，这是一个包含200个大小为1的向量的序列。如果输入序列的长度未知，我们可以使用*?*表示未知的序列长度。下一章的NLP案例研究将向你展示一些相关的示例。
- en: We have already described the Keras Input Layer node and the Keras Dense Layer
    node in previous chapters. Let's explore, in this section, just the Keras LSTM
    Layer node.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中已经描述了Keras输入层节点和Keras全连接层节点。在这一节中，我们将专门探讨Keras LSTM层节点。
- en: Important note
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Until now, we have used the term vector when we've talked about the input, the
    cell state, and the output. A tensor is a more generalized form, representing
    a vector stretching along *k*-dimensions. A rank 0 tensor is equal to a scalar
    value, a rank 1 tensor is equal to a vector, and a rank 2 tensor is equal to a
    matrix.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们谈到输入、细胞状态和输出时使用了“向量”这个术语。张量是一种更广义的形式，表示沿着*k*维度拉伸的向量。秩为0的张量等于标量值，秩为1的张量等于向量，秩为2的张量等于矩阵。
- en: 'Notice that the Keras LSTM Layer node accepts up to three input tensors: one
    with the input values of the sequence and two to initialize the hidden state tensors,
    ![](img/Formula_B16391_06_147.png) and ![](img/Formula_B16391_06_148.png).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras LSTM层节点最多接受三个输入张量：一个是序列的输入值，两个用于初始化隐藏状态张量，![](img/Formula_B16391_06_147.png)和![](img/Formula_B16391_06_148.png)。
- en: If the previous neural layer produces more than one tensor as output, in the
    configuration window of the current LSTM layer, via a drop-down menu, you can
    select which tensor should be used as input or to initialize the hidden states.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前一层神经网络输出多个张量，在当前LSTM层的配置窗口中，你可以通过下拉菜单选择哪个张量作为输入或用于初始化隐藏状态。
- en: 'We will explore more complex neural architectures in the next chapters. Here,
    we have limited our architecture to the simplest classic LSTM layer configuration,
    accepting just one input tensor from the input layer. The one input tensor accepted
    as input can be seen in the configuration window of the LSTM Layer node in *Figure
    6.16*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中探讨更复杂的神经网络架构。在这里，我们将架构限制为最简单的经典LSTM层配置，只接受来自输入层的一个输入张量。可以在*图6.16*的LSTM层节点配置窗口中看到该输入张量：
- en: '![Figure 6.16 – The Keras LSTM Layer node and its configuration window](img/B16391_06_016.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – Keras LSTM层节点及其配置窗口](img/B16391_06_016.jpg)'
- en: Figure 6.16 – The Keras LSTM Layer node and its configuration window
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – Keras LSTM层节点及其配置窗口
- en: For the LSTM layer, we can set two activation functions, called **Activation**
    and **Recurrent activation**. The **Recurrent activation** function is used by
    the gates to filter the input components. The function selected as **Activation**
    is used to create the candidates for the cell state, ![](img/Formula_B16391_06_149.png),
    and to normalize the new cell state, ![](img/Formula_B16391_06_150.png), before
    applying the output gate. This means that for the standard LSTM unit, which we
    introduced in this chapter, the setting for **Activation** is the tanh function
    and for **Recurrent activation** the sigmoid function.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LSTM层，我们可以设置两个激活函数，分别称为**激活**和**递归激活**。**递归激活**函数由门控机制用来过滤输入的成分。选定的**激活**函数用于创建细胞状态的候选值，![](img/Formula_B16391_06_149.png)，并在应用输出门之前对新的细胞状态进行标准化，![](img/Formula_B16391_06_150.png)。这意味着，对于我们在本章介绍的标准LSTM单元，**激活**的设置为tanh函数，**递归激活**的设置为sigmoid函数。
- en: We set the layer to add biases to the different layers of the LSTM unit but
    decided to not use the dropout.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了该层为LSTM单元的不同层添加偏置，但决定不使用dropout。
- en: The **Implementation** and **Unroll** setting options don't have any impact
    on the results but can improve the performance depending on your hardware and
    the sequence length. When activating the **Unroll** checkbox, the network will
    be unrolled before training, which can speed up the learning process, but it is
    memory-expensive and only suitable for short input sequences. If unchecked, a
    so-called symbolic loop is used in the TensorFlow backend.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**实现**和**展开**设置选项不会影响结果，但可以根据硬件和序列长度提高性能。当激活**展开**复选框时，网络将在训练之前被展开，这可以加速学习过程，但它占用大量内存，只适合短输入序列。如果未选中，则会在TensorFlow后端使用所谓的符号循环。'
- en: You can choose whether to return the intermediate output tensors ![](img/Formula_B16391_06_151.png)
    as a full sequence or just the last output tensor, ![](img/Formula_B16391_06_152.png)
    (the **Return sequences** option). In addition, you can also output the hidden
    cell state tensor as output (the **Return state** option). In the energy demand
    prediction case study, only the final output tensor of the LSTM unit is used to
    feed the next dense layer with the ReLU activation function. Therefore, the two
    checkboxes are not activated.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择是否将中间输出张量 ![](img/Formula_B16391_06_151.png) 作为完整序列返回，还是只返回最后一个输出张量 ![](img/Formula_B16391_06_152.png)（**返回序列**选项）。此外，你还可以选择将隐藏单元状态张量作为输出（**返回状态**选项）。在能源需求预测的案例研究中，仅使用LSTM单元的最终输出张量来馈送下一层具有ReLU激活函数的全连接层。因此，这两个复选框没有被激活。
- en: The other three tabs in the node configuration window set the regularization
    terms, initialization strategies, and constraints on the learning algorithm. We
    set no regularizations and no constraints in this layer. Let's train this network.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 节点配置窗口中的其他三个标签设置了正则化项、初始化策略和学习算法的约束条件。在这一层中，我们没有设置正则化和约束条件。现在开始训练这个网络。
- en: Training the LSTM-Based RNN
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练基于LSTM的RNN
- en: 'The Keras Network Learner node then follows to train this LSTM-based RNN on
    the training set. We know about this node already. Let''s summarize the specs
    used in its configuration window for this case study here:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Keras网络学习器节点继续在训练集上训练这个基于LSTM的RNN。我们已经了解过这个节点。这里总结一下该案例研究中其配置窗口的规格：
- en: The input tensor is accepted with conversion to **From Collection of Number
    (double)**.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入张量通过转换为**从数字集合（双精度）**来接收。
- en: The output vector is produced with conversion to **Number (double)**.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出向量通过转换为**数字（双精度）**生成。
- en: The loss function is set to **Mean Squared Error** (**MSE**) in the **Target**
    tab.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数在**目标**标签页中设置为**均方误差**（**MSE**）。
- en: The number of epochs is set to `50`, the training batch size to `256`, and the
    training algorithm to **Adam** – an optimized version of backpropagation – in
    the **Options** tab.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的迭代次数设置为`50`，训练批次大小设置为`256`，训练算法设置为**Adam**——一种反向传播的优化版本——在**选项**标签页中。
- en: The learning rate is set to be `0.001` with no learning rate decay.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率设置为`0.001`，没有学习率衰减。
- en: 'For this network, with just one neuron in the output layer, the MSE loss function
    on a training batch takes on a simpler form and becomes the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个网络，输出层只有一个神经元时，MSE损失函数在训练批次中的形式变得更简单，变为以下形式：
- en: '![](img/Formula_B16391_06_153.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_153.png)'
- en: Here, ![](img/Formula_B16391_06_154.png) is the batch size, ![](img/Formula_B16391_06_155.png)
    is the output value for training sample ![](img/Formula_B16391_06_156.png), and
    ![](img/Formula_B16391_06_157.png) is the corresponding target answer.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_06_154.png)是批次大小，![](img/Formula_B16391_06_155.png)是训练样本![](img/Formula_B16391_06_156.png)的输出值，而![](img/Formula_B16391_06_157.png)是对应的目标答案。
- en: 'Since we are talking about number prediction and MSE as the loss function,
    the plot in the **Loss** tab of the **Learning Monitor** view is the one to take
    into account to evaluate the learning process. Since we are trying to predict
    exact numbers, the accuracy is not meaningful in this case. *Figure 6.17* shows
    the **Learning Monitor** view of the Keras Network Learner node for this demand
    prediction example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们讨论的是数字预测，并且使用均方误差（MSE）作为损失函数，**学习监控器**视图中的**损失**标签页中的图表是用来评估学习过程的。由于我们尝试预测精确的数字，因此在这种情况下，准确度并不具备实际意义。*图6.17*展示了此需求预测示例中Keras网络学习器节点的**学习监控器**视图：
- en: '![Figure 6.17 – Plot of the MSE loss function over training epochs in the Loss
    tab of'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.17 – MSE损失函数在训练周期中的变化图，显示在损失标签页中'
- en: the Learning Monitor view](img/B16391_06_017.jpg)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习监控器视图**](img/B16391_06_017.jpg)'
- en: Figure 6.17 – Plot of the MSE loss function over training epochs in the Loss
    tab of the Learning Monitor view
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 在学习监视器视图的损失标签中，训练轮次期间MSE损失函数的绘图
- en: The screenshot in *Figure 6.17* shows that after just a few batch training iterations,
    we reach an acceptable prediction error, at least on the training set. After training,
    the network should be applied to the test set, using the **Keras Network Executor**
    node, and saved for deployment as a Keras file using the **Keras Network Writer**
    node.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.17*中的截图显示，经过几次批量训练迭代后，我们在训练集上至少达到了一个可接受的预测误差。训练完成后，应将网络应用于测试集，使用**Keras网络执行器**节点，并使用**Keras网络写入器**节点将其保存为Keras文件以供部署。'
- en: Let's now apply the trained LSTM network to the test set.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将训练好的LSTM网络应用到测试集上。
- en: Testing the LSTM-Based RNN
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试基于LSTM的RNN
- en: In theory, to test the performance of the network, we just need to apply the
    network to the input tensors in the test set. This is easily done with a **Keras
    Network Executor** node.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，要测试网络的性能，我们只需将网络应用于测试集中的输入张量。这可以通过**Keras网络执行器**节点轻松完成。
- en: '*Figure 6.18* shows the inside of the **In-sample testing** component:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.18*展示了**样本内测试**组件内部：'
- en: '![Figure 6.18 – Inside of the In-sample testing component](img/B16391_06_018.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 样本内测试组件内部](img/B16391_06_018.jpg)'
- en: Figure 6.18 – Inside of the In-sample testing component
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 样本内测试组件内部
- en: The In-sample testing component selects the number of input sequences to test
    on (the **Row Filter** node), then passes them through the **Keras Network Executor**
    node, and joins the predictions with the corresponding target answers.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 样本内测试组件选择要测试的输入序列的数量（**行过滤器**节点），然后通过**Keras网络执行器**节点处理它们，并将预测结果与相应的目标答案合并。
- en: After that, and outside of the **In-sample testing** component, the **Numeric
    Scorer** node calculates some error metrics and the **Line Plot (Plotly)** node
    shows the original time series and the reconstructed time series (final workflow
    in *Figure* *6.25*). The numeric error metrics quantify the error, while the line
    plot gives a visual idea of how faithful the predictions are. Predictions generated
    with this approach are called **in-sample** predictions.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，在**样本内测试**组件之外，**数值评分器**节点计算一些误差度量，**线图（Plotly）**节点展示原始时间序列和重构后的时间序列（最终工作流程见*图*
    *6.25*）。数值误差度量量化了误差，而线图则直观地展示了预测结果的忠实度。采用这种方法生成的预测称为**样本内**预测。
- en: 'The Numeric Scorer node calculates six error metrics (*Figure 6.19*): R2, **Mean
    Absolute Error** (**MAE**), MSE, **Root Mean Squared Error** (**RMSE**), **Mean
    Signed Difference** (**MSD**), and **Mean Absolute Percentage Error** (**MAPE**).
    The corresponding formulas are shown here:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 数值评分器节点计算六个误差度量（*图6.19*）：R2，**平均绝对误差**（**MAE**），MSE，**均方根误差**（**RMSE**），**平均符号差异**（**MSD**）和**平均绝对百分比误差**（**MAPE**）。对应的公式如下所示：
- en: '![](img/Formula_B16391_06_158.jpg)![](img/Formula_B16391_06_159.jpg)![](img/Formula_B16391_06_160.jpg)![](img/Formula_B16391_06_161.jpg)![](img/Formula_B16391_06_162.jpg)![](img/Formula_B16391_06_163.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B16391_06_158.jpg)![](img/Formula_B16391_06_159.jpg)![](img/Formula_B16391_06_160.jpg)![](img/Formula_B16391_06_161.jpg)![](img/Formula_B16391_06_162.jpg)![](img/Formula_B16391_06_163.jpg)'
- en: 'Here, ![](img/Formula_B16391_06_164.png) is the number of predictions from
    the test set, ![](img/Formula_B16391_06_165.png) is the output value for the test
    sample ![](img/Formula_B16391_06_166.png), and ![](img/Formula_B16391_06_167.png)
    is the corresponding target answer. We chose to apply the network on a test set
    of 600 tensors, generated the corresponding predictions, and calculated the error
    metrics. This is the result we got:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B16391_06_164.png)是来自测试集的预测数量，![](img/Formula_B16391_06_165.png)是测试样本的输出值
    ![](img/Formula_B16391_06_166.png)，而![](img/Formula_B16391_06_167.png)是对应的目标答案。我们选择在一个包含600个张量的测试集上应用该网络，生成相应的预测结果，并计算误差度量。以下是我们得到的结果：
- en: '![Figure 6.19 – Error measures between in-sample predicted 600 values and the
    corresponding target values](img/B16391_06_019.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 样本内预测的600个值与对应目标值之间的误差度量](img/B16391_06_019.jpg)'
- en: Figure 6.19 – Error measures between in-sample predicted 600 values and the
    corresponding target values
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 样本内预测的600个值与对应目标值之间的误差度量
- en: 'Each metric has its pros and cons. Commonly adopted errors for time series
    predictions are MAPE, MAE, or MSE. MAPE, for example, shows just 9% error on the
    next 600 values of the predicted time series, which is a really good result. The
    plot in *Figure 6.20* proves it:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 每个度量标准都有其优缺点。常用的时间序列预测误差包括MAPE、MAE或MSE。例如，MAPE在预测的时间序列的接下来的600个值上显示出仅9%的误差，这是一个非常好的结果。*图6.20*中的图表证明了这一点：
- en: '![](img/B16391_06_020.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16391_06_020.jpg)'
- en: Figure 6.20 – The next 600 in-sample predicted values against the next 600 target
    values in the time series
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 时间序列中接下来的600个样本预测值与目标值的比较
- en: This is an easy test. For each value to predict, we feed the network with the
    previous history of real values. This is a luxury situation that we cannot always
    afford. Often, we predict the next 600 values, one by one, based just on past
    predicted values. That is, once we have trained the network, we trigger the next
    prediction with the first 200 real past values in the test set. After that, however,
    we predict the next value based on the latest 199 real values plus the currently
    predicted one; then again based on the latest 198 real values plus the previously
    predicted one and the currently predicted one, and so on. This is a suboptimal,
    yet more realistic, situation. Predictions generated with this approach are called
    **out-sample** predictions and this kind of testing is called out-sample testing.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的测试。对于每一个要预测的值，我们将历史真实值输入网络。这是一个奢侈的情况，我们并不总是能承担。通常，我们仅根据过去预测的值预测接下来的600个值，一次一个。也就是说，一旦我们训练好网络，我们就用测试集中的前200个真实过去值触发下一次预测。然而，在此之后，我们将基于最新的199个真实值和当前预测值预测下一个值；然后再基于最新的198个真实值、之前预测的值以及当前预测的值进行预测，依此类推。这是一个次优的，但更现实的情况。使用这种方法生成的预测称为**外样本**预测，这种测试方式被称为外样本测试。
- en: To implement out-sample testing, we need to implement the loop that feeds the
    current prediction back into the vector of past samples. This loop has been implemented
    in the deployment workflow as well. Let's have a look at the details of this implementation.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现外样本测试，我们需要实现一个将当前预测反馈到过去样本向量中的循环。这个循环也在部署工作流中得到了实现。让我们来看一下这个实现的细节。
- en: Building a Deployment Loop
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建部署循环
- en: 'To implement out-sample testing, we need to implement the loop described in
    the previous section, where the currently predicted value becomes part of the
    tensor of past values for the next prediction. This is done in the component named
    **Deployment Loop** (*Figure 6.21*), which is also inside the out-sample testing
    component in the final workflow (*Figure 6.25*):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现外样本测试，我们需要实现上一节中描述的循环，其中当前预测值成为下一个预测的过去值张量的一部分。这是在名为**部署循环**的组件中完成的（*图6.21*），该组件也位于最终工作流中的外样本测试组件内（*图6.25*）：
- en: '![Figure 6.21 – The deployment loop. Notice the recursive loop to pass back'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.21 – 部署循环。注意递归循环将新输入序列传回每次迭代](img/B16391_06_021.jpg)'
- en: the new input sequence at each iteration](img/B16391_06_021.jpg)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中的新输入序列](img/B16391_06_021.jpg)
- en: Figure 6.21 – The deployment loop. Notice the recursive loop to pass back the
    new input sequence at each iteration
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – 部署循环。注意递归循环将新输入序列传回每次迭代
- en: Here, a `no_preds`, created in the `no_preds=600`).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`no_preds` 被创建为`no_preds=600`）。
- en: 'The Integer Configuration node belongs to a special group of configuration
    nodes, so its configuration window transfers into the configuration window of
    the component that contains it. As a consequence, the **Deployment Loop** component
    has a configuration setting for the number of predictions to create with the recursive
    loop, as shown in *Figure 6.22*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Integer Configuration节点属于一个特殊的配置节点组，因此其配置窗口会转移到包含它的组件的配置窗口中。因此，**部署循环**组件具有一个配置设置，用于设置递归循环中要生成的预测次数，如*图6.22*所示：
- en: '![Figure 6.22 – The configuration window of the Deployment Loop component](img/B16391_06_022.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – 部署循环组件的配置窗口](img/B16391_06_022.jpg)'
- en: Figure 6.22 – The configuration window of the Deployment Loop component
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – 部署循环组件的配置窗口
- en: Important note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The recursive loop is one of the few loops in KNIME Analytics Platform that
    allows you to pass the results back to be consumed in the next iteration.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 递归循环是KNIME分析平台中少数几个允许将结果传回并在下一次迭代中使用的循环之一。
- en: 'The **Deployment Loop** component uses two more new important nodes:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**部署循环**组件使用了两个新的重要节点：'
- en: '**The Keras to TensorFlow Network Converter node**: The Keras to TensorFlow
    Converter node converts a Keras deep learning model with a TensorFlow backend
    into a TensorFlow model. TensorFlow models are executed using the TensorFlow Java
    API, which is usually faster than the Python kernel available via the Keras Python
    API. If we use the Keras Network Executor node within the recursive loop, a Python
    kernel must be started at each iteration, which slows down the network execution.
    A TensorFlow model makes the network execution much faster.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Keras 到 TensorFlow 网络转换节点**：Keras 到 TensorFlow 转换节点将一个带有 TensorFlow 后端的 Keras
    深度学习模型转换为 TensorFlow 模型。TensorFlow 模型使用 TensorFlow Java API 执行，这通常比通过 Keras Python
    API 提供的 Python 内核要更快。如果在递归循环中使用 Keras 网络执行节点，那么每次迭代都必须启动一个 Python 内核，这会导致网络执行变慢。使用
    TensorFlow 模型能显著加快网络执行速度。'
- en: '**The TensorFlow Network Executor node**: The configuration window of the TensorFlow
    Network Executor node is similar to the configuration window of the Keras Network
    Executor node, the only difference being the backend engine, which in this case
    is TensorFlow.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 网络执行节点**：TensorFlow 网络执行节点的配置窗口与 Keras 网络执行节点的配置窗口类似，唯一的区别是后端引擎，在这种情况下是
    TensorFlow。'
- en: For out-sample testing, the deployment loop is triggered with the first tensor
    in the test set and from there it generates 600 predictions autonomously. In the
    out-sample testing component, these predictions are then joined with the target
    values and outside of the out-sample testing component, the Numeric Error node
    calculates the selected error metrics.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于外样本测试，部署循环通过测试集中的第一个张量触发，从那里开始自动生成 600 个预测。在外样本测试组件中，这些预测会与目标值结合，而在外样本测试组件之外，数值误差节点会计算选定的误差指标。
- en: 'Obviously, for out-sample testing, the error values become larger (*Figure
    6.23*), since the prediction error is influenced by the prediction errors in the
    previous steps. MAPE, for example, reaches 18%, which is practically double the
    result from in-sample testing:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于外样本测试，误差值变大了（*图 6.23*），因为预测误差受到前几步预测误差的影响。例如，MAPE 达到 18%，几乎是样本内测试结果的两倍：
- en: '![Figure 6.23 – Error measures between the out-sample predicted 600 values
    and'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.23 – 外样本预测的 600 个值与相应目标值之间的误差度量'
- en: the corresponding target values](img/B16391_06_023.jpg)
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的目标值](img/B16391_06_023.jpg)
- en: Figure 6.23 – Error measures between the out-sample predicted 600 values and
    the corresponding target values
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23 – 外样本预测的 600 个值与相应目标值之间的误差度量
- en: 'In *Figure 6.24*, we can see the prediction error when visualizing the predicted
    time series and comparing it with the original time series for the first 600 out-sample
    predictions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 6.24*中，我们可以看到在可视化预测的时间序列并将其与原始时间序列进行比较时，前 600 个外样本预测的预测误差：
- en: '![Figure 6.24 – The next 600 out-sample predicted values (orange) against the
    next 600 target values (blue) in the time series](img/B16391_06_024.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.24 – 时间序列中接下来的 600 个外样本预测值（橙色）与接下来的 600 个目标值（蓝色）对比](img/B16391_06_024.jpg)'
- en: Figure 6.24 – The next 600 out-sample predicted values (orange) against the
    next 600 target values (blue) in the time series
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24 – 时间序列中接下来的 600 个外样本预测值（橙色）与接下来的 600 个目标值（蓝色）对比
- en: There, we can see that the first predictions are quite correct, but they start
    deteriorating the further we move from the onset of the test set. This effect
    is, of course, not present for in-sample predictions. Indeed, the error values
    on the first out-sample predictions are comparable to the error values for the
    corresponding in-sample predictions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在那里，我们可以看到第一次预测相当准确，但随着我们远离测试集的起点，预测开始变差。当然，这种效果在样本内预测中是不存在的。实际上，第一次外样本预测的误差值与相应样本内预测的误差值相当。
- en: We have performed here a pretty crude time series prediction since we have not
    taken into account the seasonality prediction as a separate problem. We have somehow
    let the network manage the whole prediction by itself, without splitting seasonality
    and residuals. Our results are satisfactory for this use case. However, for more
    complex use cases, the seasonality index could be calculated, the seasonality
    subtracted, and predictions performed only on the residual values of the time
    series. Hopefully, this would be an easier problem and would lead to more accurate
    predictions. Nevertheless, we are satisfied with the prediction error, especially
    considering that the network had to manage the prediction of the seasonality as
    well.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里进行了一个相对粗略的时间序列预测，因为我们没有将季节性预测作为一个独立的问题来考虑。我们某种程度上让网络自行管理整个预测过程，而没有将季节性和残差分开。对于这个用例，结果是令人满意的。然而，对于更复杂的用例，可以计算季节性指数，扣除季节性部分，并仅对时间序列的残差值进行预测。希望这样的问题会更简单，并且会带来更准确的预测。不过，考虑到网络还需要处理季节性预测，我们对预测误差还是感到满意的。
- en: 'The final workflow, building, training, and in-sample testing the network,
    is shown in *Figure 6.25*:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的工作流，包括构建、训练和样本内测试网络，如*图 6.25*所示：
- en: '![Figure 6.25 – The final workflow to prepare the data and build, train, and
    test the LSTM-based network on a time series prediction problem](img/B16391_06_025.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.25 – 为时间序列预测问题准备数据、构建、训练并测试基于LSTM的网络的最终工作流](img/B16391_06_025.jpg)'
- en: Figure 6.25 – The final workflow to prepare the data and build, train, and test
    the LSTM-based network on a time series prediction problem
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25 – 为时间序列预测问题准备数据、构建、训练并测试基于LSTM的网络的最终工作流
- en: This workflow is available in the book's GitHub space. Let's now move on to
    the deployment workflow.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工作流可以在本书的GitHub空间中找到。接下来，我们将讨论部署工作流。
- en: Deploying the LSTM-Based RNN
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署LSTM基于RNN的模型
- en: Deployment at this point is easy. We read the deployment data, for example,
    from a `.table` file; then, we apply the same data preparation steps as for the
    training and test data. We isolate the first input sequence with 200 past samples;
    we apply the deployment loop to generate ![](img/Formula_B16391_06_168.png) new
    samples (here, we went for ![](img/Formula_B16391_06_169.png)); we apply the trained
    LSTM-based RNN inside the deployment loop; and finally, we visualize the predictions
    with a Line Plot (Plotly) node. Notice that this time there are no predictions
    versus target values, since the deployment data is real-world data and not lab
    data, and as such does not have any target values to be compared to.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，部署非常简单。我们读取部署数据，例如从`.table`文件中；然后，按照与训练数据和测试数据相同的步骤进行数据准备。我们隔离出前200个过去的样本作为输入序列；我们应用部署循环生成
    ![](img/Formula_B16391_06_168.png) 新样本（这里我们选择了 ![](img/Formula_B16391_06_169.png)）；我们在部署循环中应用训练好的基于LSTM的RNN；最后，我们通过一个线图（Plotly）节点来可视化预测结果。请注意，这次没有预测与目标值的对比，因为部署数据是真实世界数据，而非实验室数据，因此没有目标值可以进行对比。
- en: 'The deployment workflow is shown in *Figure 6.26* and is available on KNIME
    Hub at [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%206/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%206/):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 部署工作流如*图 6.26*所示，并可以在KNIME Hub上找到：[https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%206/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%206/)
- en: '![Figure 6.26 – The deployment workflow for a demand prediction problem](img/B16391_06_026.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.26 – 需求预测问题的部署工作流](img/B16391_06_026.jpg)'
- en: Figure 6.26 – The deployment workflow for a demand prediction problem
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.26 – 需求预测问题的部署工作流
- en: This is the deployment workflow, including data reading, the same data preparation
    as for the data in the training workflow, network reading, and a deployment loop
    to generate the predictions.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这是部署工作流，包括数据读取、与训练工作流中数据相同的数据准备、网络读取和部署循环生成预测。
- en: In this last section, we have learned how to apply the deployment loop to a
    deployment workflow to generate new predictions in real life.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们学会了如何将部署循环应用到部署工作流中，以生成现实生活中的新预测。
- en: Summary
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we introduced a new recurrent neural unit: the LSTM unit.
    We showed how it is built and trained, and how it can be applied to a time series
    analysis problem, such as demand prediction.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种新的递归神经单元：LSTM单元。我们展示了它是如何构建和训练的，以及如何将其应用于时间序列分析问题，如需求预测。
- en: As an example of a demand prediction problem, we tried to predict the average
    energy consumed by a cluster of users in the next hour, given the energy used
    in the previous 200 hours. We showed how to test in-sample and out-sample predictions
    and some numeric measures commonly used to quantify the prediction error. Demand
    prediction applied to energy consumption is just one of the many demand prediction
    use cases. The same approach learned here could be applied to predict the number
    of customers in a restaurant, the number of visitors to a web site, or the amount
    of a type of food required in a supermarket.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个需求预测问题的例子，我们尝试预测一个用户群体在接下来一小时内的平均能耗，前提是给定过去200小时的能耗数据。我们展示了如何进行样本内和样本外预测测试，以及一些常用的数值衡量方法来量化预测误差。需求预测应用于能耗仅仅是众多需求预测用例中的一个。这里学到的相同方法也可以应用于预测餐厅的顾客数量、网站的访客数量，或超市中某种食品的需求量。
- en: In this chapter, we also introduced a new loop in KNIME Analytics Platform,
    the recursive loop, and we mentioned a new visualization node, the Line Plot (Plotly)
    node.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还介绍了 KNIME Analytics Platform 中的一种新循环——递归循环，并提到了一个新的可视化节点——线性图（Plotly）节点。
- en: In the next chapter, we will continue with RNNs, focusing on different text-related
    applications.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续讲解 RNN，重点讨论不同的文本相关应用。
- en: Questions and Exercises
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与练习
- en: 'Check your level of understanding of the concepts explored in this chapter
    by answering the following questions:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题，检查你对本章所探讨概念的理解程度：
- en: Why are LSTM units suitable for time series analysis?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么 LSTM 单元适用于时间序列分析？
- en: a). Because they are faster than classic feedforward networks
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a). 因为它们比经典的前馈网络更快
- en: b). Because they can remember past input tensors
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b). 因为它们能记住过去的输入张量
- en: c). Because they use gates
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c). 因为它们使用门控
- en: d). Because they have hidden states
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d). 因为它们具有隐藏状态
- en: What is the data extraction option to use for partitioning in time series analysis?
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间序列分析中，进行数据分割时应使用哪种数据提取选项？
- en: a). Draw randomly
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a). 随机绘制
- en: b). Take from top
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b). 从顶部开始
- en: c). Stratified Sampling
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c). 分层抽样
- en: d). Linear Sampling
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d). 线性抽样
- en: What is a tensor?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是张量？
- en: a). A tensor is a two-dimensional vector.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a). 张量是一个二维向量。
- en: b). A tensor is a *k*-dimensional vector.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b). 张量是一个*k*维向量。
- en: c). A tensor is just a number.
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c). 张量仅仅是一个数字。
- en: d). A tensor is a sequence of numbers.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d). 张量是一个数字序列。
- en: What is the difference between in-sample and out-sample testing?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 样本内测试和样本外测试有什么区别？
- en: a). In-sample testing uses the real past values from the test set to make the
    predictions. Out-sample testing uses past prediction values to make new predictions.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a). 样本内测试使用来自测试集的真实过去值进行预测。样本外测试则使用过去的预测值来进行新的预测。
- en: b). In-sample testing is more realistic than out-sample testing.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b). 样本内测试比样本外测试更具现实性。
- en: c). In-sample testing is more complex than out-sample testing.
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c). 样本内测试比样本外测试更复杂。
- en: d). In-sample testing applies the trained network while out-sample testing uses
    rules.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d). 样本内测试应用经过训练的网络，而样本外测试使用规则。
