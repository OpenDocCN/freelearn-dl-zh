- en: '*Chapter 1*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*'
- en: Introduction to Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理简介
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够：
- en: Describe natural language processing and its applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述自然语言处理及其应用
- en: Explain different text preprocessing techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释不同的文本预处理技术
- en: Perform text preprocessing on text corpora
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本语料进行文本预处理
- en: Explain the functioning of Word2Vec and GloVe word embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释Word2Vec和GloVe词向量的工作原理
- en: Generate word embeddings using Word2Vec and GloVe
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Word2Vec和GloVe生成词向量
- en: Use the NLTK, Gensim, and Glove-Python libraries for text preprocessing and
    generating word embeddings
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NLTK、Gensim和Glove-Python库进行文本预处理和生成词向量
- en: This chapter aims to equip you with knowledge of the basics of natural language
    processing and experience with the various text preprocessing techniques used
    in Deep Learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在让你掌握自然语言处理的基础知识，并体验在深度学习中使用的各种文本预处理技术。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Welcome to deep learning for Natural Language Processing. This book guides you
    in understanding and optimizing deep learning techniques for the purpose of natural
    language processing, which furthers the reality of generalized artificial intelligence.
    You will journey through the concepts of natural language processing – its applications
    and implementations – and learn the ways of deep neural networks, along with utilizing
    them to enable machines to understand natural language.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到自然语言处理的深度学习之旅。本书将引导你理解并优化深度学习技术，用于自然语言处理，推动通用人工智能的现实化。你将深入了解自然语言处理的概念——它的应用和实现——并学习深度神经网络的工作方式，同时利用它们使机器理解自然语言。
- en: The Basics of Natural Language Processing
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理基础
- en: 'To understand what natural language processing is, let''s break the term into
    two:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解什么是自然语言处理，我们将这个术语分解成两个部分：
- en: Natural language is a form of written and spoken communication that has developed
    organically and naturally.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言是一种书面和口头沟通形式，它是自然和有机发展的。
- en: Processing means analyzing and making sense of input data with computers.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理是指用计算机分析和理解输入数据。
- en: '![Figure 1.1: Natural language processing'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1：自然语言处理'
- en: '](img/C13783_01_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_01.jpg)'
- en: 'Figure 1.1: Natural language processing'
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.1：自然语言处理
- en: Therefore, natural language processing is the machine-based processing of human
    communication. It aims to teach machines how to process and understand the language
    of humans, thereby allowing an easy channel of communication between human and
    machines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，自然语言处理是基于机器的人类沟通处理。它旨在教会机器如何处理和理解人类的语言，从而建立起人与机器之间的轻松沟通渠道。
- en: For example, the personal voice assistants found in our phones and smart speakers,
    such as Alexa and Siri, are a result of natural language processing. They have
    been created in such a manner that they are able to not only understand what we
    say to them but also to act upon what we say and respond with feedback. Natural
    language processing algorithms aid these technologies in communicating with humans.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们手机和智能音响中的个人语音助手，如Alexa和Siri，就是自然语言处理的产物。它们的设计使得它们不仅能理解我们对它们说的话，还能根据我们的指示做出反应并提供反馈。自然语言处理算法帮助这些技术与人类进行交流。
- en: The key thing to consider in the mentioned definition of natural language processing
    is that the communication needs to occur in the natural language of humans. We've
    been communicating with machines for decades now by creating programs to perform
    certain tasks and executing them. However, these programs are written in languages
    that are not natural languages, because they are not forms of spoken communication
    and they haven't developed naturally or organically. These languages, such as
    Java, Python, C, and C++, were created with machines in mind and the consideration
    always being, "what will the machine be able to understand and process easily?"
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理的上述定义中，需要考虑的关键点是沟通必须发生在人类的自然语言中。几十年来，我们通过编写程序来与机器沟通，执行特定的任务。然而，这些程序使用的语言并非自然语言，因为它们不是口语交流形式，也没有自然或有机发展。这些语言，如Java、Python、C和C++，是为机器设计的，且始终以“机器能够理解和轻松处理什么”为出发点。
- en: While Python is a more user-friendly language and so is easier for humans to
    learn and be able to write code in, the basic point remains the same – to communicate
    with a machine, humans must learn a language that the machine is able to understand.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Python是一种更易于使用的语言，因此对于人类来说更容易学习并编写代码，但基本的观点保持不变——为了与机器沟通，人类必须学习一种机器能够理解的语言。
- en: '![Figure 1.2: Venn diagram for natural language processing'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2：自然语言处理的维恩图'
- en: '](img/C13783_01_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_02.jpg)'
- en: 'Figure 1.2: Venn diagram for natural language processing'
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.2：自然语言处理的维恩图
- en: The purpose of natural language processing is the opposite of this. Rather than
    having humans conform to the ways of a machine and learn how to effectively communicate
    with them, natural language processing enables machines to conform to humans and
    learn their way of communication. This makes more sense since the aim of technology
    is to make our lives easier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的目的正好与此相反。与其让人类适应机器的方式，学习如何与机器有效沟通，不如让机器适应人类并学习人类的沟通方式。这更加合理，因为技术的目的是让我们的生活更轻松。
- en: To clarify this with an example, your first ever program was probably a piece
    of code that asked the machine to print 'hello world'. This was you conforming
    to the machine and asking it to execute a task in a language that it understood.
    Asking your voice assistant to say 'hello world' by voicing this command to it,
    and having it say 'hello world' back to you, is an example of the application
    of natural language processing, because you are communicating with a machine in
    your natural language (in this case, English). The machine is conforming to your
    form of communication, understanding what you're saying, processing what you're
    asking it to do, and then executing the task.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过一个例子来说明这一点，你第一次编写的程序可能是一段要求计算机打印“hello world”的代码。这是你在遵循机器的规则，并要求它执行你所理解的语言中的任务。当你对语音助手发出命令说“hello
    world”，并且它返回“hello world”时，这是自然语言处理的应用示例，因为你正在使用自然语言（在此情况下是英语）与计算机进行交流。计算机则遵循你的交流方式，理解你说的话，处理你要求它执行的任务，然后执行该任务。
- en: Importance of natural language processing
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理的重要性
- en: 'The following figure illustrates the various sections of the field of artificial
    intelligence:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了人工智能领域的各个部分：
- en: '![Fig 1.3: Artificial intelligence and some of its subfields'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.3：人工智能及其部分子领域'
- en: '](img/C13783_01_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_03.jpg)'
- en: 'Fig 1.3: Artificial intelligence and some of its subfields'
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.3：人工智能及其部分子领域
- en: Along with machine learning and deep learning, natural language processing is
    a subfield of artificial intelligence, and because it deals with natural language,
    it's actually at the intersection of artificial intelligence and linguistics.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理与机器学习和深度学习一起，是人工智能的一个子领域，并且由于它处理的是自然语言，实际上它处于人工智能和语言学的交汇点。
- en: As mentioned, natural language processing is what enables machines to understand
    the language of humans, thus allowing an efficient channel of communication between
    the two. However, there is another reason Natural language processing is necessary,
    and that is because, like machines, machine learning and deep learning models
    work best with numerical data. Numerical data is hard for humans to naturally
    produce; imagine us talking in numbers rather than words. So, natural language
    processing works with textual data and converts it into numerical data, enabling
    machine learning and deep learning models to be fitted on it. Thus, it exists
    to bridge the communication gap between humans and machines by taking the spoken
    and written forms of language from humans and converting them into data that can
    be understood by machines. Thanks to natural language processing, the machine
    is able to make sense of, answer questions based on, solve problems using, and
    communicate in a natural language, among other things.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自然语言处理使得机器能够理解人类的语言，从而在两者之间建立一个高效的沟通渠道。然而，自然语言处理之所以必要，还有另一个原因，那就是，像机器一样，机器学习和深度学习模型在处理数值数据时效果最佳。数值数据对于人类来说很难自然产生；想象一下我们用数字而不是单词交流。因此，自然语言处理处理文本数据并将其转换为数值数据，使得机器学习和深度学习模型能够在其上进行训练。因此，它的存在是为了弥合人类与机器之间的沟通鸿沟，将人类的口语和书面语言转化为机器能够理解的数据。多亏了自然语言处理，机器能够理解、回答基于数据的问题、使用数据解决问题，并用自然语言进行沟通等。
- en: Capabilities of Natural language processing
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理的能力
- en: 'Natural language processing has many real-world applications that benefit the
    lives of humans. These applications fall under three broad capabilities of natural
    language processing:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理在现实生活中有许多有益的应用，这些应用属于自然语言处理的三大主要能力：
- en: Speech Recognition
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音识别
- en: The machine is able to recognize a natural language in its spoken form and translate
    it into a textual form. An example of this is dictation on your smartphones –
    you can enable dictation and speak to your phone, and it will convert whatever
    you are saying into text.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器能够识别口语形式的自然语言并将其转化为文本形式。一个例子是智能手机上的语音输入——你可以启用语音输入，向手机说话，手机会将你说的内容转化为文本。
- en: Natural Language Understanding
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言理解
- en: The machine is able to understand a natural language in both its spoken and
    written form. If given a command, the machine is able to understand and execute
    it. An example of this would be saying 'Hey Siri, call home' to Siri on your iPhone
    for Siri to automatically call 'home' for you.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器能够理解自然语言的书面和口语形式。如果给出命令，机器能够理解并执行。一个例子是对着iPhone上的Siri说“嘿，Siri，打电话回家”，Siri会自动为你拨打“家”的电话。
- en: Natural Language Generation
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言生成
- en: The machine is able to generate natural language itself. An example of this
    is asking 'Siri, what time is it?' to Siri on your iPhone and Siri replying with
    the time – 'It's 2:08pm'.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器能够自主生成自然语言。一个例子是对iPhone上的Siri说“现在几点了？”，Siri会回答时间——“现在是下午2:08”。
- en: These three capabilities are used to accomplish and automate a lot of tasks.
    Let's take a look at some of the things natural language processing contributes
    to, and how.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种能力被用来完成和自动化许多任务。让我们来看看自然语言处理为哪些方面做出了贡献，以及如何做到的。
- en: Note
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: Textual data is known as corpora (plural) and a corpus (singular).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据被称为语料（复数），单个语料称为语料库。
- en: Applications of Natural Language Processing
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理的应用
- en: 'The following figure depicts the general application areas of natural language
    processing:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了自然语言处理的一般应用领域：
- en: '![Figure 1.4: Application areas of natural language processing'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.4：自然语言处理的应用领域'
- en: '](img/C13783_01_04.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_04.jpg)'
- en: 'Figure 1.4: Application areas of natural language processing'
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.4：自然语言处理的应用领域
- en: Automatic text summarization
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动文本摘要
- en: This involves processing corpora to provide a summary.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这涉及处理语料以提供总结。
- en: Translation
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译
- en: This entails translation tools that translate text to and from different languages,
    for example, Google Translate.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这包括翻译工具，能够将文本从一种语言翻译成另一种语言，例如谷歌翻译。
- en: Sentiment analysis
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: This is also known as emotional artificial intelligence or opinion mining, and
    it is the process of identifying, extracting, and quantifying emotions and affective
    states from corpora, both written and spoken. Sentiment analysis tools are used
    to process things such as customer reviews and social media posts to understand
    emotional responses to and opinions regarding particular things, such as the quality
    of food at a new restaurant.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这也被称为情感人工智能或意见挖掘，是从书面和口头的语料中识别、提取和量化情感与情绪状态的过程。情感分析工具用于处理如客户评论和社交媒体帖子等内容，旨在理解人们对特定事物（如新餐厅的食品质量）的情绪反应和意见。
- en: Information extraction
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息提取
- en: This is the process of identifying and extracting important terms from corpora,
    known as entities. Named entity recognition falls under this category and is a
    process that will be explained in the next chapter.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是从语料中识别和提取重要术语的过程，称为实体。命名实体识别属于这一类别，这一过程将在下一章中进行解释。
- en: Relationship extraction
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系提取
- en: Relationship extraction involves extracting semantic relationships from corpora.
    Semantic relationships occur between two or more entities (such as people, organizations,
    and things) and fall into one of the many semantic categories. For example, if
    a relationship extraction tool was given a paragraph about Sundar Pichai and how
    he is the CEO of Google, the tool would be able to produce "Sundar Pichai works
    for Google" as output, with Sundar Pichai and Google being the two entities, and
    'works for' being the semantic category that defines their relationship.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关系抽取涉及从语料库中提取语义关系。语义关系存在于两个或更多实体（如人、组织和事物）之间，并且属于多种语义类别中的一种。例如，如果一个关系抽取工具收到一段关于
    Sundar Pichai 及其作为谷歌首席执行官的描述，该工具能够输出“Sundar Pichai 为谷歌工作”，其中 Sundar Pichai 和谷歌是两个实体，而“为……工作”则是定义它们关系的语义类别。
- en: Chatbot
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人
- en: Chatbots are forms of artificial intelligence that are designed to converse
    with humans via speech and text. The majority of them mimic humans and make it
    feel as though you are speaking to another human being. Chatbots are being used
    in the health industry to help people who suffer from depression and anxiety.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聊天机器人是一种人工智能形式，旨在通过语音和文本与人类对话。它们大多数模仿人类，使你感觉像是在与另一个人交谈。聊天机器人在健康行业中被用于帮助那些遭受抑郁和焦虑的人。
- en: Social media analysis
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体分析
- en: Social media applications such as Twitter and Facebook have hashtags and trends
    that are tracked and monitored using natural language processing to understand
    what is being talked about around the world. Additionally, natural language processing
    aids the process of moderation by filtering out negative, offensive, and inappropriate
    comments and posts.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像 Twitter 和 Facebook 这样的社交媒体应用有话题标签和趋势，这些话题和趋势通过自然语言处理技术被跟踪和监控，以便了解全球范围内正在讨论的内容。此外，自然语言处理还协助内容审查过程，通过过滤负面、冒犯性和不当的评论和帖子，帮助维护社交平台的健康环境。
- en: Personal voice assistants
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个人语音助手
- en: Siri, Alexa, Google Assistant, and Cortana are all personal voice assistants
    that leverage natural language processing techniques to understand and respond
    to what we say.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Siri、Alexa、Google Assistant 和 Cortana 都是个人语音助手，利用自然语言处理技术来理解并回应我们说的话。
- en: Grammar checking
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法检查
- en: Grammar-checking software automatically checks and corrects your grammar, punctuation,
    and typing errors.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法检查软件会自动检查和修正你的语法、标点符号和打字错误。
- en: Text Preprocessing
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预处理
- en: When answering questions on a comprehension passage, the questions are specific
    to different parts of the passage, and so while some words and sentences are important
    to you, others are irrelevant. The trick is to identify key words from the questions
    and match them to the passage to find the correct answer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在回答理解性阅读题时，问题是针对文章的不同部分的，因此，某些单词和句子对你来说很重要，而其他的则无关紧要。技巧在于从问题中识别关键词，并将其与文章进行匹配，从而找到正确的答案。
- en: Text preprocessing works in a similar fashion – the machine doesn't need the
    irrelevant parts of the corpora; it just needs the important words and phrases
    required to execute the task at hand. Thus, text preprocessing techniques involve
    prepping the corpora for proper analysis and for the machine learning and deep
    learning models. Text preprocessing is basically telling the machine what it needs
    to take into consideration and what it can disregard.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理的工作方式类似——机器不需要语料库中的无关部分，它只需要执行任务所需的重要单词和短语。因此，文本预处理技术涉及为语料库准备适当的分析，并为机器学习和深度学习模型做好准备。文本预处理基本上是告诉机器它需要考虑什么，以及可以忽略什么。
- en: Each corpus requires different text preprocessing techniques depending on the
    task that needs to be executed, and once you've learned the different preprocessing
    techniques, you'll understand where to use what and why. The order in which the
    techniques have been explained is usually the order in which they are performed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每个语料库根据需要执行的任务不同，要求采用不同的文本预处理技术，一旦你掌握了不同的预处理技术，你就会理解在何时、为何使用特定的技术。技术解释的顺序通常也是它们执行的顺序。
- en: We will be using the **NLTK** Python library in the following exercises, but
    feel free to use different libraries while doing the activities. **NLTK** stands
    for **Natural Language Toolkit** and is the simplest and one of the most popular
    Python libraries for natural language processing, which is why we will be using
    it to understand the basic concepts of natural language processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中，我们将使用**NLTK** Python库，但在进行活动时也可以自由使用其他库。**NLTK**代表**自然语言工具包**，它是最简单且最流行的Python自然语言处理库之一，这也是我们使用它来理解自然语言处理基本概念的原因。
- en: Note
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For further information on NLTK, go to https://www.nltk.org/.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于NLTK的信息，请访问 https://www.nltk.org/。
- en: Text Preprocessing Techniques
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本预处理技术
- en: 'The following are the most popular text preprocessing techniques in natural
    language processing:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是自然语言处理中的最流行的文本预处理技术：
- en: Lowercasing/uppercasing
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小写/大写
- en: Noise removal
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声去除
- en: Text normalization
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本标准化
- en: Stemming
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词干提取
- en: Lemmatization
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词形还原
- en: Tokenization
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Removing stop words
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去除停用词
- en: Let's look at each technique one by one.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一查看每种技术。
- en: Lowercasing/Uppercasing
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小写/大写
- en: This is one of the most simple and effective preprocessing techniques that people
    often forget to use. It either converts all the existing uppercase characters
    into lowercase ones so that the entire corpus is in lowercase, or it converts
    all the lowercase characters present in the corpus into uppercase ones so that
    the entire corpus is in uppercase.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单且最有效的预处理技术之一，但人们常常忘记使用它。它可以将所有现有的大写字母转换为小写字母，使整个语料库都是小写的，或者将语料库中所有的小写字母转换为大写字母，使整个语料库都是大写的。
- en: This method is especially useful when the size of the corpus isn't too large
    and the task involves identifying terms or outputs that could be recognized differently
    due to the case of the characters, since a machine inherently processes uppercase
    and lowercase letters as separate entities – 'A' is different from 'a.' This kind
    of variation in the input capitalization could result in incorrect output or no
    output at all.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法尤其适用于当语料库的大小不太大且任务涉及识别由于字符大小写不同而可能被识别为不同的术语或输出时，因为机器本身会将大写字母和小写字母视为独立的实体——'A'与'a'是不同的。输入中的这种大小写变体可能导致输出不正确或完全没有输出。
- en: An example of this would be a corpus that contains both 'India' and 'india.'
    Without applying lowercasing, the machine would recognize these as two separate
    terms, when in reality they're both different forms of the same word and correspond
    to the same country. After lowercasing, there would exist only one instance of
    the term "India," which would be 'india,' simplifying the task of finding all
    the places where India has been mentioned in the corpus.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是，一个语料库中包含了'India'和'india'。如果没有进行小写转换，机器会将它们识别为两个不同的术语，而实际上它们只是同一个单词的不同形式，表示的是同一个国家。经过小写转换后，语料库中只会存在一个"India"的实例，即'india'，这简化了查找语料库中所有提到印度的地方的任务。
- en: Note
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: All exercises and activities will be primarily developed on Jupyter Notebook.
    You will need to have Python 3.6 and NLTK installed on your system.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所有练习和活动将主要在 Jupyter Notebook 上开发。你需要在系统上安装 Python 3.6 和 NLTK。
- en: Exercises 1 – 6 can be done within the same Jupyter notebook.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1 – 6 可以在同一个 Jupyter 笔记本中完成。
- en: 'Exercise 1: Performing Lowercasing on a Sentence'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 1：对句子进行小写转换
- en: 'In this exercise, we will take an input sentence with both uppercase and lowercase
    characters and convert them all into lowercase characters. The following steps
    will help you with the solution:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将输入一个包含大写和小写字母的句子，并将其全部转换为小写字母。以下步骤将帮助你完成该解决方案：
- en: Open **cmd** or another terminal depending on your operating system.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**cmd**或根据你的操作系统打开其他终端。
- en: 'Navigate to the desired path and use the following command to initiate a `Jupyter`
    notebook:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到所需路径并使用以下命令启动`Jupyter`笔记本：
- en: '[PRE0]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Store an input sentence in an '`s = "The cities I like most in India are Mumbai,
    Bangalore, Dharamsala and Allahabad."`
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入句子存储在`'s = "The cities I like most in India are Mumbai, Bangalore, Dharamsala
    and Allahabad."`中
- en: 'Apply the `lower()` function to convert the capital letters into lowercase
    characters and then print the new string, as shown:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用`lower()`函数将大写字母转换为小写字母，然后打印出新的字符串，如下所示：
- en: '[PRE1]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Expected output:**'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![](img/C13783_01_05.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13783_01_05.jpg)'
- en: 'Figure 1.5: Output for lowercasing with mixed casing in a sentence'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.5：包含混合大小写的句子的小写处理输出
- en: 'Create an array of words with capitalized characters, as shown:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含大写字符的单词数组，如下所示：
- en: '[PRE2]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using list comprehension, apply the `lower()` function on each element of the
    `words` array and then print the new array, as follows:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列表推导，对`words`数组中的每个元素应用`lower()`函数，然后打印新数组，如下所示：
- en: '[PRE3]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Expected output:**'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.6: Output for lowercasing with mixed casing of words'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.6：带混合大小写单词的小写化输出'
- en: '](img/C13783_01_06.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_06.jpg)'
- en: 'Figure 1.6: Output for lowercasing with mixed casing of words'
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.6：带混合大小写单词的小写化输出
- en: Noise Removal
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 噪声移除
- en: Noise is a very general term and can mean different things with respect to different
    corpora and different tasks. What is considered noise for one task may be what
    is considered important for another, and thus this is a very domain-specific preprocessing
    technique. For example, when analyzing tweets, hashtags might be important to
    recognize trends and understand what's being spoken about around the globe, but
    hashtags may not be important when analyzing a news article, and so hashtags would
    be considered noise in the latter's case.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声是一个非常通用的术语，在不同的语料库和任务中可能意味着不同的东西。对于一个任务来说被视为噪声的内容，可能在另一个任务中被认为是重要的，因此这是一个非常领域特定的预处理技术。例如，在分析推文时，标签可能对识别趋势和理解全球讨论的内容非常重要，但在分析新闻文章时，标签可能并不重要，因此在后者的情况下，标签就被视为噪声。
- en: Noise doesn't include only words, but can also include symbols, punctuation
    marks, HTML markup (**<**,**>**, *****, **?**,**.**), numbers, whitespaces, stop
    words, particular terms, particular regular expressions, non-ASCII characters
    (**\W**|**\d+**), and parse terms.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声不仅仅包括单词，还可以包括符号、标点符号、HTML标记（**<**,**>**,*****, **?**,**.**）、数字、空格、停用词、特定术语、特定的正则表达式、非ASCII字符（**\W**|**\d+**）以及解析术语。
- en: 'Removing noise is crucial so that only the important parts of the corpora are
    fed into the models, ensuring accurate results. It also helps by bringing words
    into their root or standard form. Consider the following example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 移除噪声至关重要，这样只有语料库中重要的部分才会被输入到模型中，从而确保准确的结果。它还通过将单词转化为根形态或标准形式来帮助分析。考虑以下示例：
- en: '![Figure 1.7: Output for noise removal'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7：噪声移除输出'
- en: '](img/C13783_01_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_07.jpg)'
- en: 'Figure 1.7: Output for noise removal'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.7：噪声移除输出
- en: After removing all the symbols and punctuation marks, all the instances of sleepy
    correspond to the one form of the word, enabling more efficient prediction and
    analysis of the corpus.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 删除所有符号和标点符号后，所有“sleepy”的实例都对应于单一的单词形式，从而提高了语料库的预测和分析效率。
- en: 'Exercise 2: Removing Noise from Words'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2：移除单词中的噪声
- en: In this exercise, we will take an input array containing words with noise attached
    (such as punctuation marks and HTML markup) and convert these words into their
    clean, noise-free forms. To do this, we will need to make use of Python's regular
    expression library. This library has several functions that allow us to filter
    through input data and remove the unnecessary parts, which is exactly what the
    process of noise removal aims to do.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将输入一个包含噪声的单词数组（如标点符号和HTML标记），并将这些单词转换为清洁、无噪声的形式。为此，我们需要使用Python的正则表达式库。该库提供了多个函数，允许我们筛选输入数据并移除不必要的部分，这正是噪声移除过程的目标。
- en: Note
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about '**re**,' click on https://docs.python.org/3/library/re.html.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于‘**re**’的信息，请点击 https://docs.python.org/3/library/re.html。
- en: 'In the same `Jupyter` notebook, import the regular expression library, as shown:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的`Jupyter`笔记本中，导入正则表达式库，如下所示：
- en: '[PRE4]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create a function called ''`clean_words`'', which will contain methods to remove
    different types of noise from the words, as follows:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`'clean_words'`的函数，该函数包含移除不同类型噪声的方法，如下所示：
- en: '[PRE5]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create an array of raw words with noise, as demonstrated:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含噪声的原始单词数组，如下所示：
- en: '[PRE6]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Apply the `clean_words()` function on the words in the raw array and then print
    the array of clean words, as shown:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对原始数组中的单词应用`clean_words()`函数，然后打印清理后的单词数组，如下所示：
- en: '[PRE7]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Expected output:**'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![](img/C13783_01_08.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C13783_01_08.jpg)'
- en: 'Figure 1.8: Output for noise removal'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.8：噪声移除输出
- en: Text Normalization
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本归一化
- en: This is the process of converting a raw corpus into a canonical and standard
    form, which is basically to ensure that the textual input is guaranteed to be
    consistent before it is analyzed, processed, and operated upon.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将原始语料库转换为规范和标准形式的过程，基本上是确保文本输入在分析、处理和操作之前保持一致。
- en: Examples of text normalization would be mapping an abbreviation to its full
    form, converting several spellings of the same word to one spelling of the word,
    and so on.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标准化的示例包括将缩写映射到其完整形式，将同一单词的不同拼写转换为单一拼写形式，等等。
- en: 'The following are examples for canonical forms of incorrect spellings and abbreviations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些不正确拼写和缩写的标准形式示例：
- en: '![Figure 1.9: Canonical form for incorrect spellings'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.9：不正确拼写的标准形式'
- en: '](img/C13783_01_09.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_09.jpg)'
- en: 'Figure 1.9: Canonical form for incorrect spellings'
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.9：不正确拼写的标准形式
- en: '![Figure 1.10: Canonical form for abbreviations'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.10：缩写的标准形式'
- en: '](img/C13783_01_10.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_10.jpg)'
- en: 'Figure 1.10: Canonical form for abbreviations'
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.10：缩写的标准形式
- en: There is no standard way to go about normalization since it is very dependent
    on the corpus and the task at hand. The most common way to go about it is with
    dictionary mapping, which involves manually creating a dictionary that maps all
    the various forms of one word to that one word, and then replaces each of those
    words with one standard form of the word.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于标准化的方法非常依赖于语料库和具体任务，因此没有统一的标准方式。最常见的做法是使用字典映射法，即手动创建一个字典，将一个单词的各种形式映射到该单词的标准形式，然后将这些单词替换为该单词的标准形式。
- en: Stemming
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: Stemming is performed on a corpus to reduce words to their stem or root form.
    The reason for saying "stem or root form" is that the process of stemming doesn't
    always reduce the word to its root but sometimes just to its canonical form.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是在语料库上进行的，以将单词简化为它们的词干或词根形式。之所以说“词干或词根形式”，是因为词干提取的过程并不总是将单词还原为词根形式，有时只是简化为它的标准形式。
- en: 'The words that undergo stemming are known as inflected words. These words are
    in a form that is different from the root form of the word, to imply an attribute
    such as the number or gender. For example, "journalists" is the plural form of
    "journalist." Thus, stemming would cut off the ''**s**'', bringing "journalists"
    to its root form:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 经历词干提取的单词称为屈折词。这些单词与词根形式不同，表示某种属性，例如数目或性别。例如，“journalists”是“journalist”的复数形式。因此，词干提取会去掉“**s**”，将“journalists”还原为它的词根形式：
- en: '![Figure 1.11: Output for stemming'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.11：词干提取的输出'
- en: '](img/C13783_01_11.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_11.jpg)'
- en: 'Figure 1.11: Output for stemming'
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.11：词干提取的输出
- en: Stemming is beneficial when building search applications due to the fact that
    when searching for something in particular, you might also want to find instances
    of that thing even if they're spelled differently. For example, if you're searching
    for exercises in this book, you might also want 'Exercise' to show up in your
    search.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取在构建搜索应用时非常有用，因为在搜索特定内容时，您可能还希望找到拼写不同的该内容的实例。例如，如果您在搜索本书中的练习，您可能还希望搜索到“Exercise”。
- en: However, stemming doesn't always provide the desired stem, since it works by
    chopping off the ends of the words. It's possible for the stemmer to reduce 'troubling'
    to 'troubl' instead of 'trouble' and this won't really help in problem solving,
    and so stemming isn't a method that's used too often. When it is used, Porter's
    stemming algorithm is the most common algorithm for stemming.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，词干提取并不总是提供所需的词干，因为它是通过去掉单词末尾的部分来工作的。词干提取器有可能将“troubling”还原为“troubl”而不是“trouble”，这对解决问题没有太大帮助，因此词干提取并不是一种常用的方法。当使用时，Porter
    的词干提取算法是最常见的算法。
- en: 'Exercise 3: Performing Stemming on Words'
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3：对单词进行词干提取
- en: In this exercise, we will take an input array containing various forms of one
    word and convert these words into their stem forms.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将处理一个包含同一单词不同形式的输入数组，并将这些单词转换为它们的词干形式。
- en: 'In the same `Jupyter` notebook, import the `nltk` and `pandas` libraries as
    well as `Porter Stemmer`, as shown:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个`Jupyter`笔记本中，导入`nltk`和`pandas`库以及`Porter Stemmer`，如示例所示：
- en: '[PRE8]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create an instance of `stemmer`, as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`stemmer`实例，如下所示：
- en: '[PRE9]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create an array of different forms of the same word, as shown:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含同一单词不同形式的数组，如下所示：
- en: '[PRE10]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Apply the stemmer to each of the words in the `words` array and store them
    in a new array, as given:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将词干提取器应用于`words`数组中的每个单词，并将它们存储在一个新数组中，如所示：
- en: '[PRE11]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Print the raw words and their stems in the form of a DataFrame, as shown:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始单词及其词干以数据框的形式打印，如下所示：
- en: '[PRE12]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Expected output:**'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.12: Output of stemming'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12：词干提取的输出'
- en: '](img/C13783_01_12.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_12.jpg)'
- en: 'Figure 1.12: Output of stemming'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.12：词干提取的输出
- en: Lemmatization
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词形还原
- en: Lemmatization is a process that is like stemming – its purpose is to reduce
    a word to its root form. What makes it different is that it doesn't just chop
    the ends of words off to obtain this root form, but instead follows a process,
    abides by rules, and often uses WordNet for mappings to return words to their
    root forms. (WordNet is an English language database that consists of words and
    their definitions along with synonyms and antonyms. It is considered to be an
    amalgamation of a dictionary and a thesaurus.) For example, lemmatization is capable
    of transforming the word 'better' into its root form 'good', since 'better' is
    just the comparative form of 'good."
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是一个类似于词干提取的过程——它的目的是将单词还原为其根本形式。与词干提取不同的是，词形还原不仅仅是通过剪切单词的结尾来获得根本形式，而是遵循一个过程，遵循规则，并且通常使用WordNet进行映射，将单词还原为其根本形式。（WordNet是一个英语语言数据库，包含单词及其定义，此外还包含同义词和反义词。它被认为是词典和同义词词典的融合。）例如，词形还原能够将单词“better”转换为其根本形式“good”，因为“better”只是“good”的比较级形式。
- en: While this quality of lemmatization makes it highly appealing and more efficient
    when compared with stemming, the drawback is that since lemmatization follows
    such an organized procedure, it takes a lot more time than stemming does. Hence,
    lemmatization is not recommended when you're working with a large corpus.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词形还原的这一特性使得它在与词干提取相比时更具吸引力和效率，但缺点是，由于词形还原遵循如此有序的程序，因此它比词干提取需要更多时间。因此，当你处理大规模语料库时，不推荐使用词形还原。
- en: 'Exercise 4: Performing Lemmatization on Words'
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4：对单词进行词形还原
- en: In this exercise, we will take an input array containing various forms of one
    word and convert these words into their root form.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将接收一个包含单词不同形式的输入数组，并将这些单词转换为其根本形式。
- en: 'In the same Jupyter notebook as the previous exercise, import `WordNetLemmatizer`
    and download `WordNet`, as shown:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在与前一个练习相同的`Jupyter`笔记本中，导入`WordNetLemmatizer`并下载`WordNet`，如所示：
- en: '[PRE13]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create an instance of `lemmatizer`, as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`lemmatizer`实例，如下所示：
- en: '[PRE14]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create an array of different forms of the same word, as demonstrated:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含同一单词不同形式的数组，如示范所示：
- en: '[PRE15]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Apply `lemmatizer` to each of the words in the `words` array and store them
    in a new array, as follows. The `word` parameter provides the lemmatize function
    with the word it is supposed to lemmatize. The `pos` parameter is the part of
    speech you want the lemma to be. ''`v`'' stands for verb and thus the lemmatizer
    will reduce the word to its closest verb form:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`words`数组中的每个单词应用`lemmatizer`，并将结果存储在一个新数组中，如下所示。`word`参数向词形还原函数提供它应还原的单词。`pos`参数是你希望词形还原为的词性。'`v`'代表动词，因此词形还原器会将单词还原为其最接近的动词形式：
- en: '[PRE16]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Print the raw words and their root forms in the form of a DataFrame, as shown:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印原始单词及其根本形式，结果以数据框形式展示，如所示：
- en: '[PRE17]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Expected output:**'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.13: Output of lemmatization'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13：词形还原的输出'
- en: '](img/C13783_01_13.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_13.jpg)'
- en: 'Figure 1.13: Output of lemmatization'
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.13：词形还原的输出
- en: Tokenization
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: Tokenization is the process of breaking down a corpus into individual tokens.
    Tokens are the most commonly used words – thus, this process breaks down a corpus
    into individual words – but can also include punctuation marks and spaces, among
    other things.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是将语料库拆解为单个词元的过程。词元是最常用的单词——因此，这个过程将语料库拆解为单个单词——但也可以包括标点符号和空格等其他元素。
- en: This technique is one of the most important ones since it is a prerequisite
    for a lot of applications of natural language processing that we will be learning
    about in the next chapter, such as **Parts-of-Speech** (**PoS**) tagging. These
    algorithms take tokens as input and can't function with strings or paragraphs
    of text as input.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这一技术是最重要的技术之一，因为它是很多自然语言处理应用的前提，我们将在下一章学习这些应用，比如**词性标注**（**PoS**）。这些算法将词元作为输入，不能使用字符串或段落文本作为输入。
- en: Tokenization can be performed to obtain individual words as well as individual
    sentences as tokens. Let's try both of these out in the following exercises.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过分词将文本分解成单个单词或单个句子作为词元。让我们在接下来的练习中尝试这两种方法。
- en: 'Exercise 5: Tokenizing Words'
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5：词元化单词
- en: In this exercise, we will take an input sentence and produce individual words
    as tokens from it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将接收一个输入句子，并从中生成单个单词作为词元。
- en: 'In the same `Jupyter` notebook, import `nltk`:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个`Jupyter`笔记本中，导入`nltk`：
- en: '[PRE18]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From `nltk`, import `word_tokenize` and `punkt`, as shown:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`nltk`导入`word_tokenize`和`punkt`，如所示：
- en: '[PRE19]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Store words in a variable and apply `word_tokenize()` on it, then print the
    results, as follows:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单词存储在一个变量中，并对其应用 `word_tokenize()`，然后打印结果，如下所示：
- en: '[PRE20]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Expected output:**'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.14: Output for the tokenization of words'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14：单词分词的输出'
- en: '](img/C13783_01_14.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_14.jpg)'
- en: 'Figure 1.14: Output for the tokenization of words'
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.14：单词分词的输出
- en: As you can see, even the punctuation marks are tokenized and considered as individual
    tokens.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，甚至标点符号也被分词并视为独立的标记。
- en: Now let's see how we can tokenize sentences.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何分词句子。
- en: 'Exercise 6: Tokenizing Sentences'
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6：句子分词
- en: In this exercise, we will take an input sentence and produce individual words
    as tokens from it.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将从输入句子中生成单独的单词作为标记。
- en: 'In the same `Jupyter` notebook, import `sent_tokenize`, as shown:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个 `Jupyter` 笔记本中，按如下方式导入 `sent_tokenize`：
- en: '[PRE21]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Store two sentences in a variable (our sentence from the previous exercise
    was actually two sentences, so we can use the same one to see the difference between
    word and sentence tokenization) and apply `sent_tokenize()` on it, then print
    the results, as follows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个句子存储在一个变量中（我们之前的句子实际上是两个句子，所以可以使用同一个句子来查看单词分词和句子分词之间的区别），然后对其应用 `sent_tokenize()`，接着打印结果，如下所示：
- en: '[PRE22]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Expected output:**'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.15: Output for tokenizing sentences'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.15：句子分词的输出'
- en: '](img/C13783_01_15.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_15.jpg)'
- en: 'Figure 1.15: Output for tokenizing sentences'
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.15：句子分词的输出
- en: As you can see, the two sentences have formed two individual tokens.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两个句子已经形成了两个独立的标记。
- en: Additional Techniques
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他技术
- en: There are several ways to perform text preprocessing, including the usage of
    a variety of Python libraries such as **BeautifulSoup** to strip away HTML markup.
    The previous exercises serve the purpose of introducing some techniques to you.
    Depending on the task at hand, you may need to use just one or two or all of them,
    including the modifications made to them. For example, at the noise removal stage,
    you may find it necessary to remove words such as 'the,' 'and,' 'this,' and 'it.'
    So, you will need to create an array containing these words and pass the corpus
    through a **for** loop to store only the words that are not a part of that array,
    removing the noisy words from the corpus. Another way of doing this is given later
    in this chapter and is done after tokenization has been performed.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以进行文本预处理，包括使用各种 Python 库，如 **BeautifulSoup** 来剥离 HTML 标记。前面的练习是为了向你介绍一些技术。根据手头的任务，你可能只需要使用其中的一两个，或者全部使用它们，包括对它们所做的修改。例如，在噪声移除阶段，你可能觉得有必要移除像
    'the'、'and'、'this' 和 'it' 这样的词。因此，你需要创建一个包含这些词的数组，并通过 **for** 循环将语料库中的词汇传递，只保留那些不在该数组中的词，去除噪声词。另一种方法将在本章后面介绍，且是在完成分词后执行的。
- en: 'Exercise 7: Removing Stop Words'
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 7：移除停用词
- en: In this exercise, we will take an input sentence and remove the stop words from
    it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将从输入句子中移除停用词。
- en: 'Open a `Jupyter` notebook and download ''`stopwords`'' using the following
    line of code:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个 `Jupyter` 笔记本，并使用以下代码行下载 '`stopwords`'：
- en: '[PRE23]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Store a sentence in a variable, as shown:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个句子存储在一个变量中，如下所示：
- en: '[PRE24]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Import `stopwords` and create a set of the English stop words, as follows:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `stopwords` 并创建一组英语停用词，如下所示：
- en: '[PRE25]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Tokenize the sentence using `word_tokenize`, and then store those tokens that
    do not occur in `stop_words` in an array. Then, print that array:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `word_tokenize` 对句子进行分词，然后将那些在 `stop_words` 中没有出现的标记存储在一个数组中。接着，打印该数组：
- en: '[PRE26]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Expected output:**'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.16: Output after removing stopwords'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.16：移除停用词后的输出'
- en: '](img/C13783_01_16.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_16.jpg)'
- en: 'Figure 1.16: Output after removing stopwords'
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.16：移除停用词后的输出
- en: Additionally, you may need to convert numbers into their word forms. This is
    also a method you can add to the noise removal function. Furthermore, you might
    need to make use of the contractions library, which serves the purpose of expanding
    the existing contractions in the text. For example, the contractions library will
    convert 'you're' into 'you are,' and if this is necessary for your task, then
    it is recommended to install this library and use it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能需要将数字转换为其词语形式。这也是可以添加到噪声移除功能中的一种方法。此外，你可能需要使用缩写词库，它的作用是扩展文本中已有的缩写词。例如，缩写词库会将
    'you're' 转换为 'you are'，如果这对你的任务是必要的，那么建议安装并使用该库。
- en: Text preprocessing techniques go beyond the ones that have been discussed in
    this chapter and can include anything and everything that is required for a task
    or a corpus. In some instances, some words may be important, while in others they
    won't be.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理技术超出了本章所讨论的范围，可以包括任务或语料库中任何所需的内容和技术。在某些情况下，一些词语可能很重要，而在其他情况下则不重要。
- en: Word Embeddings
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入
- en: As mentioned in the earlier sections of this chapter, natural language processing
    prepares textual data for machine learning and deep learning models. The models
    perform most efficiently when provided with numerical data as input, and thus
    a key role of natural language processing is to transform preprocessed textual
    data into numerical data, which is a numerical representation of the textual data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面的部分所提到的，自然语言处理为机器学习和深度学习模型准备文本数据。当提供数值数据作为输入时，模型表现最为高效，因此自然语言处理的一个关键角色是将预处理后的文本数据转化为数值数据，这是文本数据的数值表示。
- en: 'This is what word embeddings are: they are numerical representations in the
    form of real-value vectors for text. Words that have similar meanings map to similar
    vectors and thus have similar representations. This aids the machine in learning
    the meaning and context of different words. Since word embeddings are vectors
    mapping to individual words, word embeddings can only be generated once tokenization
    has been performed on the corpus.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是词嵌入：它们是文本的数值表示，采用实值向量的形式。具有相似含义的词映射到相似的向量，因此具有相似的表示形式。这帮助机器学习不同单词的意义和上下文。由于词嵌入是映射到单个单词的向量，词嵌入只能在对语料库进行分词之后生成。
- en: '![Figure 1.17: Example for word embeddings'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.17：词嵌入示例'
- en: '](img/C13783_01_17.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_17.jpg)'
- en: 'Figure 1.17: Example for word embeddings'
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.17：词嵌入示例
- en: Word embeddings encompass a variety of techniques used to create a learned numerical
    representation and are the most popular way to represent a document's vocabulary.
    The beneficial aspect of word embeddings is that they are able to capture contextual,
    semantic, and syntactic similarities, and the relations of a word with other words,
    to effectively train the machine to comprehend natural language. This is the main
    aim of word embeddings – to form clusters of similar vectors that correspond to
    words with similar meanings.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入包括用于创建学习到的数值表示的各种技术，是表示文档词汇最流行的方式。词嵌入的优点在于，它们能够捕捉上下文、语义和句法的相似性，以及一个单词与其他单词的关系，从而有效地训练机器理解自然语言。这就是词嵌入的主要目的——形成类似向量的簇，这些簇对应于具有相似意义的词。
- en: The reason for using word embeddings is to make machines understand synonyms
    the same way we do. Consider an example of online restaurant reviews – they consist
    of adjectives describing food, ambience, and the overall experience. They are
    either positive or negative, and comprehending which reviews fall into which of
    these two categories is important. The automatic categorization of these reviews
    can provide a restaurant with quick insights as to what areas they need to improve
    on, what people liked about their restaurant, and so on.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用词嵌入的原因是为了让机器像我们一样理解同义词。以在线餐厅评论为例——这些评论由形容词描述食物、环境和整体体验。它们要么是积极的，要么是消极的，理解哪些评论属于这两类中的哪一类非常重要。自动分类这些评论可以为餐厅提供快速的见解，了解需要改进的地方，人们喜欢餐厅的哪些方面，等等。
- en: There exist a variety of adjectives that can be classified as positive, and
    the same goes with negative adjectives. Thus, not only does the machine need to
    be able to differentiate between negative and positive, it also needs to learn
    and understand that multiple words can relate to the same category because they
    ultimately mean the same thing. This is where word embeddings are helpful.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种可以归类为积极的形容词，消极形容词也是如此。因此，机器不仅需要能够区分负面和正面，还需要学习和理解多个单词可以归属于同一类别，因为它们最终意味着相同的东西。这就是词嵌入发挥作用的地方。
- en: 'Consider the example of restaurant reviews received on a food service application.
    The following two sentences are from two separate restaurant reviews:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以餐饮服务应用上的餐厅评论为例。以下两句话来自两条不同的餐厅评论：
- en: Sentence A – The food here was great.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子A – 这里的食物很好。
- en: Sentence B – The food here was good.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子B – 这里的食物很好。
- en: The machine needs to be able to comprehend that both these reviews are positive
    and mean a similar thing, despite the adjective in both sentences being different.
    This is done by creating word embeddings, because the two words 'good' and 'great'
    map to two separate but similar real-value vectors and, thus, can be clustered
    together.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 机器需要能够理解这两条评论都是积极的，并且它们意味着类似的内容，尽管这两句话中的形容词不同。这是通过创建词嵌入来实现的，因为两个词“good”和“great”分别映射到两个不同但相似的实际值向量，因此可以将它们聚集在一起。
- en: The Generation of Word Embeddings
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入的生成
- en: We've understood what word embeddings are and their importance; now we need
    to understand how they're generated. The process of transforming words into their
    real-value vectors is known as vectorization and is done by word embedding techniques.
    There are many word embedding techniques available, but in this chapter, we will
    be discussing the two main ones – Word2Vec and GloVe. Once word embeddings (vectors)
    have been created, they combine to form a vector space, which is an algebraic
    model consisting of vectors that follow the rules of vector addition and scalar
    multiplication. If you don't remember your linear algebra, this might be a good
    time to quickly review it.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了什么是词嵌入以及它们的重要性；现在我们需要理解它们是如何生成的。将词语转换为其实际值向量的过程被称为向量化，这是通过词嵌入技术完成的。市面上有许多词嵌入技术，但在本章中，我们将讨论两种主要技术——Word2Vec和GloVe。一旦词嵌入（向量）被创建，它们会结合成一个向量空间，这是一个代数模型，由遵循向量加法和标量乘法规则的向量组成。如果你不记得线性代数的内容，现在可能是快速复习的好时机。
- en: Word2Vec
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec
- en: As mentioned earlier, Word2Vec is one of the word embedding techniques used
    to generate vectors from words – something you can probably understand from the
    name itself.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Word2Vec 是一种词嵌入技术，用于从词语中生成向量——这从其名字上就能大致理解。
- en: Word2Vec is a shallow neural network – it has only two layers – and thus does
    not qualify as a deep learning model. The input is a text corpus, which it uses
    to generate vectors as the output. These vectors are known as feature vectors
    for the words present in the input corpus. It transforms a corpus into numerical
    data that can be understood by a deep neural network.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 是一个浅层神经网络——它只有两层——因此不算是深度学习模型。输入是一个文本语料库，机器使用它来生成向量作为输出。这些向量被称为输入语料库中单词的特征向量。它将语料库转换为深度神经网络能够理解的数值数据。
- en: The aim of Word2Vec is to understand the probability of two or more words occurring
    together and thus to group words with similar meanings together to form a cluster
    in a vector space. Like any other machine learning or deep learning model, Word2Vec
    becomes more and more efficient by learning from past data and past occurrences
    of words. Thus, if provided with enough data and context, it can accurately guess
    a word's meaning based on past occurrences and context, similar to how we understand
    language.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 的目标是理解两个或多个词语共同出现的概率，从而将具有相似意义的词语聚集在一起，形成向量空间中的一个簇。像其他机器学习或深度学习模型一样，Word2Vec
    通过从过去的数据和单词的出现中学习，变得越来越高效。因此，如果提供足够的数据和上下文，它可以根据过去的出现和上下文准确地猜测一个词的含义，类似于我们如何理解语言。
- en: For example, we are able to create a connection between the words 'boy' and
    'man', and 'girl' and 'woman,' once we have heard and read about them and understood
    what they mean. Likewise, Word2Vec can also form this connection and generate
    vectors for these words that lie close together in the same cluster so as to ensure
    that the machine is aware that these words mean similar things.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一旦我们听说并阅读过“boy”和“man”，“girl”和“woman”这些词，并理解它们的含义，我们就能建立起这些词之间的联系。同样，Word2Vec
    也可以建立这种联系，并为这些词生成向量，这些向量彼此靠近，位于同一个簇中，以确保机器能够意识到这些词意味着相似的东西。
- en: Once Word2Vec has been given a corpus, it produces a vocabulary wherein each
    word has a vector of its own attached to it, which is known as its neural word
    embedding, and simply put, this neural word embedding is a word written in numbers.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Word2Vec 接收到一个语料库，它会生成一个词汇表，其中每个词都有一个与之关联的向量，这被称为其神经词嵌入，简单来说，这个神经词嵌入就是用数字表示的词语。
- en: Functioning of Word2Vec
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec的工作原理
- en: 'Word2Vec trains a word against words that neighbor the word in the input corpus,
    and there are two methods of doing so:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec 训练一个词与其在输入语料库中邻近的词进行对比，有两种方法可以实现这一点：
- en: '*Continuous Bag of Words (CBOW)*:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续词袋模型（CBOW）*：'
- en: This method predicts the current word based on the context. Thus, it takes the
    word's surrounding words as input to produce the word as output, and it chooses
    this word based on the probability that this is indeed the word that is a part
    of the sentence.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法根据上下文预测当前单词。因此，它将单词的周围单词作为输入，输出该单词，并根据该单词是否确实是句子的一部分来选择该单词。
- en: For example, if the algorithm is provided with the words "the food was" and
    needs to predict the adjective after it, it is most likely to output the word
    "good" rather than output the word "delightful," since there would be more instances
    where the word "good" was used, and thus it has learned that "good" has a higher
    probability than "delightful." CBOW it said to be faster than skip-gram and has
    a higher accuracy with more frequent words.
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果算法提供了“the food was”这些单词并需要预测后面的形容词，它最有可能输出“good”而不是“delightful”，因为“good”出现的次数更多，因此它学到“good”的出现概率高于“delightful”。CBOW被认为比跳字法更快，并且在频繁出现的单词上准确性更高。
- en: '![Fig 1.18: The CBOW algorithm'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.18: CBOW算法'
- en: '](img/C13783_01_18.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_18.jpg)'
- en: 'Fig 1.18: The CBOW algorithm'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 1.18: CBOW算法'
- en: '*Skip-gram*'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*跳字法*'
- en: This method predicts the words surrounding a word by taking the word as input,
    understanding the meaning of the word, and assigning it to a context. For example,
    if the algorithm was given the word "delightful," it would have to understand
    its meaning and learn from past context to predict that the probability that the
    surrounding words are "the food was" is highest. Skip-gram is said to work best
    with a small corpus.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法通过将一个单词作为输入，理解该单词的含义并将其与上下文关联，来预测周围的单词。例如，如果算法给定单词“delightful”，它需要理解该词的含义，并从过去的上下文中学习，预测出周围的单词是“the
    food was”的概率最大。跳字法通常认为在小型语料库中表现最好。
- en: '![Fig 1.19: The skip-gram algorithm'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.19: 跳字法算法'
- en: '](img/C13783_01_19.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_19.jpg)'
- en: 'Fig 1.19: The skip-gram algorithm'
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 1.19: 跳字法算法'
- en: While both methods seem to be working in opposite manners, they are essentially
    predicting words based on the context of local (nearby) words; they are using
    a window of context to predict what word will come next. This window is a configurable
    parameter.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种方法看起来是相反的方式，但它们本质上是根据局部（附近）单词的上下文来预测单词；它们使用一个上下文窗口来预测接下来会出现哪个单词。这个窗口是一个可配置的参数。
- en: The decision of choosing which algorithm to use depends on the corpus at hand.
    CBOW works on the basis of probability and thus chooses the word that has the
    highest probability of occurring given a specific context. This means it will
    usually predict only common and frequent words since those have the highest probabilities,
    and rare and infrequent words will never be produced by CBOW. Skip-gram, on the
    other hand, predicts context, and thus when given a word, it will take it as a
    new observation rather than comparing it to an existing word with a similar meaning.
    Due to this, rare words will not be avoided or looked over. However, this also
    means that a lot of training data will be required for skip-gram to work efficiently.
    Thus, depending on the training data and corpus at hand, the decision to use either
    algorithm should be made.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用哪种算法的决定取决于手头的语料库。CBOW基于概率工作，因此会选择在特定上下文中出现概率最高的单词。这意味着它通常只会预测常见和频繁出现的单词，因为这些单词的概率最高，而罕见和不常见的单词则永远不会由CBOW生成。另一方面，跳字法预测上下文，因此，当给定一个单词时，它会将其视为一个新的观察，而不是将其与具有相似意义的现有单词进行比较。因此，罕见单词不会被跳过或忽视。然而，这也意味着跳字法需要大量的训练数据才能高效地工作。因此，根据训练数据和语料库的不同，应该决定使用哪种算法。
- en: Essentially, both algorithms, and thus the model as a whole, require an intense
    learning phase where they are trained over thousands and millions of words to
    better understand context and meaning. Based on this, they are able to assign
    vectors to words and thus aid the machine in learning and predicting natural language.
    To understand Word2Vec better, let's do an exercise using Gensim's Word2Vec model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这两种算法以及整个模型，都需要一个强度很高的学习阶段，在这个阶段它们会经过数千、数百万个单词的训练，以便更好地理解上下文和含义。基于此，它们能够为单词分配向量，从而帮助机器学习和预测自然语言。为了更好地理解Word2Vec，让我们通过Gensim的Word2Vec模型做一个练习。
- en: Gensim is an open source library for unsupervised topic modeling and natural
    language processing using statistical machine learning. Gensim's Word2Vec algorithm
    takes an input of sequences of sentences in the form of individual words (tokens).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Gensim是一个开源库，用于使用统计机器学习进行无监督主题建模和自然语言处理。Gensim的Word2Vec算法接收由单独的单词（令牌）组成的句子序列作为输入。
- en: Also, we can use the **min_count** parameter. It exists to ask you how many
    instances of a word should be there in a corpus for it to be important to you,
    and then takes that into consideration when generating word embeddings. In a real-life
    scenario, when dealing with millions of words, a word that occurs only once or
    twice may not be important at all and thus can be ignored. However, right now,
    we are training our model only on three sentences each with only 5-6 words in
    every sentence. Thus, **min_count** is set to 1 since a word is important to us
    even if it occurs only once.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用**min_count**参数。它的作用是问你一个单词在语料库中应该出现多少次才对你有意义，然后在生成词嵌入时考虑这一点。在实际场景中，当处理数百万个单词时，只出现一次或两次的单词可能完全不重要，因此可以忽略它们。然而，现在我们仅在三句话上训练模型，每句只有5到6个单词。因此，**min_count**设置为1，因为即使一个单词只出现一次，对我们来说它也是重要的。
- en: 'Exercise 8: Generating Word Embeddings Using Word2Vec'
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8：使用Word2Vec生成词嵌入
- en: In this exercise, we will be using Gensim's Word2Vec algorithm to generate word
    embeddings post tokenization.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用Gensim的Word2Vec算法，在分词后生成词嵌入。
- en: Note
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'You will need to have `gensim` installed on your system for the following exercise.
    You can use the following command to install it, if it is not already installed:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在系统上安装`gensim`才能进行以下练习。如果尚未安装，你可以使用以下命令进行安装：
- en: '`pip install –-upgrade gensim`'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install --upgrade gensim`'
- en: For further information, click on https://radimrehurek.com/gensim/models/word2vec.html.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请点击 https://radimrehurek.com/gensim/models/word2vec.html。
- en: 'The following steps will help you with the solution:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该解答：
- en: Open a new `Jupyter` notebook.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的`Jupyter`笔记本。
- en: 'Import the Word2Vec model from `gensim`, and import `word_tokenize` from `nltk`,
    as shown:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`gensim`导入Word2Vec模型，并从`nltk`导入`word_tokenize`，如所示：
- en: '[PRE27]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Store three strings with some common words into three separate variables, and
    then tokenize each sentence and store all the tokens in an array, as shown:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将三个包含常见单词的字符串存入三个独立的变量中，然后对每个句子进行分词，并将所有令牌存储在一个数组中，如所示：
- en: '[PRE28]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can print the array of sentences to view the tokens.
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以打印句子数组来查看令牌。
- en: 'Train the model, as follows:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，训练模型：
- en: '[PRE29]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Word2Vec's default value for `min_count` is 5\.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Word2Vec的`min_count`默认值为5\。
- en: 'Summarize the model, as demonstrated:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如所示，总结模型：
- en: '[PRE30]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Your output will look something like this:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出将会是如下所示：
- en: '![Figure 1.20: Output for model summary'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图1.20：模型摘要的输出'
- en: '](img/C13783_01_20.jpg)'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_01_20.jpg)'
- en: 'Figure 1.20: Output for model summary'
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.20：模型摘要的输出
- en: Vocab = 12 signifies that there are 12 different words present in the sentences
    that were input to the model.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Vocab = 12 表示输入模型的句子中有12个不同的单词。
- en: 'Let''s find out what words are present in the vocabulary by summarizing it,
    as shown:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过总结模型来找出词汇表中存在哪些单词，如下所示：
- en: '[PRE31]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Your output will look something like this:'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出将会是如下所示：
- en: '![Figure 1.21: Output for the vocabulary of the corpus'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.21：语料库词汇表的输出'
- en: '](img/C13783_01_21.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_21.jpg)'
- en: 'Figure 1.21: Output for the vocabulary of the corpus'
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.21：语料库词汇表的输出
- en: 'Let''s see what the vector (word embedding) for the word ''singer'' is:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下单词‘singer’的向量（词嵌入）：
- en: '[PRE32]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '**Expected output:**'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '**期望输出：**'
- en: '![Figure 1.22: Vector for the word ‘singer’'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.22：单词‘singer’的向量'
- en: '](img/C13783_01_22.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_22.jpg)'
- en: 'Figure 1.22: Vector for the word ''singer'''
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1.22：单词‘singer’的向量
- en: 'Our Word2Vec model has been trained on these three sentences, and thus its
    vocabulary only includes the words present in this sentence. If we were to find
    words that are similar to a particular input word from our Word2Vec model, we
    wouldn''t get words that actually make sense since the vocabulary is so small.
    Consider the following examples:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Word2Vec模型已经在这三句话上进行了训练，因此它的词汇表仅包含这些句子中出现的单词。如果我们试图从Word2Vec模型中找到与某个特定输入单词相似的单词，由于词汇表非常小，我们不会得到任何实际有意义的单词。考虑以下例子：
- en: '[PRE33]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The 'positive' refers to the depiction of only positive vector values in the
    output.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: “positive”指的是在输出中仅展示正向的向量值。
- en: 'The top six similar words to ''great'' would be:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 与‘great’最相似的六个单词是：
- en: '![Figure 1.23: Word vectors similar to the word ‘great’'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23：与“great”类似的词向量'
- en: '](img/C13783_01_23.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_23.jpg)'
- en: 'Figure 1.23: Word vectors similar to the word ''great'''
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.23：与“great”类似的词向量
- en: 'Similarly, for the word ''singer'', it could be as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于“singer”这个词，它可能是如下所示：
- en: '[PRE34]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 1.24: Word vector similar to word ‘singer’'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.24：与“singer”类似的词向量'
- en: '](img/C13783_01_24.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_24.jpg)'
- en: 'Figure 1.24: Word vector similar to word ''singer'''
  id: totrans-322
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.24：与“singer”类似的词向量
- en: We know that these words are not actually similar in meaning to our input words
    at all, and that also shows up in the correlation value beside them. However,
    they show up because these are the only words that exist in our vocabulary.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这些词与我们的输入词在意义上并不相似，这也体现在它们旁边的相关性值中。然而，它们会出现在这里，因为它们是我们词汇中唯一存在的词。
- en: Another important parameter of the `Gensim` Word2Vec model is the size parameter.
    Its default value is 100 and implies the size of the neural network layers that
    are being used to train the model. This corresponds to the amount of freedom the
    training algorithm has. A larger size requires more data but also leads to higher
    accuracy.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '`Gensim` Word2Vec 模型的另一个重要参数是大小参数。它的默认值是 100，表示用于训练模型的神经网络层的大小。这对应于训练算法的自由度。较大的大小需要更多的数据，但也会提高准确性。'
- en: Note
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on Gensim's Word2Vec model, click on
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Gensim Word2Vec 模型的更多信息，请点击
- en: https://rare-technologies.com/word2vec-tutorial/.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: https://rare-technologies.com/word2vec-tutorial/.
- en: GloVe
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GloVe
- en: GloVe, an abbreviation of "global vectors," is a word embedding technique that
    has been developed by Stanford. It is an unsupervised learning algorithm that
    builds on Word2Vec. While Word2Vec is quite successful in generating word embeddings,
    the issue with it is that is it has a small window through which it focuses on
    local words and local context to predict words. This means that it is unable to
    learn from the frequency of words present globally, that is, in the entire corpus.
    GloVe, as mentioned in its name, looks at all the words present in a corpus.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe，"global vectors" 的缩写，是一种由斯坦福大学开发的词嵌入技术。它是一种无监督学习算法，基于 Word2Vec。虽然 Word2Vec
    在生成词嵌入方面非常成功，但它的问题在于它有一个较小的窗口，通过这个窗口它集中关注局部词汇和局部上下文来预测词汇。这意味着它无法从全局词频中学习，也就是说，无法从整个语料库中学习。正如其名称所示，GloVe
    会查看语料库中出现的所有词汇。
- en: 'While Word2Vec is a predictive model as it learns vectors to improve its predictive
    abilities, GloVe is a count-based model. What this means is that GloVe learns
    its vectors by performing dimensionality reduction on a co-occurrence counts matrix.
    The connections that GloVe is able to make are along the lines of this:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Word2Vec 是一种预测模型，它通过学习向量来提高预测能力，但 GloVe 是一种基于计数的模型。这意味着 GloVe 通过对共现计数矩阵进行降维来学习向量。GloVe
    能够建立的连接如下所示：
- en: '*king – man + woman = queen*'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '*king – man + woman = queen*'
- en: This means it's able to understand that "king" and "queen" share a relationship
    that is similar to that between "man" and "woman".
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它能够理解“king”和“queen”之间的关系与“man”和“woman”之间的关系相似。
- en: These are complicated terms, so let's understand them one by one. All of these
    concepts come from statistics and linear algebra, so if you already know what's
    going on, you can skip to the activity!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是复杂的术语，所以我们一个个来理解。所有这些概念来自于统计学和线性代数，如果你已经了解这些内容，可以跳过活动部分！
- en: When dealing with a corpus, there exist algorithms to construct matrices based
    on term frequencies. Basically, these matrices contain words that occur in a document
    as rows, and the columns are either paragraphs or separate documents. The elements
    of the matrices represent the frequency with which the words occur in the documents.
    Naturally, with a large corpus, this matrix will be huge. Processing such a large
    matrix will take a lot of time and memory, thus we perform dimensionality reduction.
    This is the process of reducing the size of the matrix so it is possible to perform
    further operations on it.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理语料库时，存在根据词频构建矩阵的算法。基本上，这些矩阵包含文档中出现的词作为行，列则是段落或单独的文档。矩阵的元素表示词在文档中出现的频率。自然地，对于一个大型语料库，这个矩阵会非常庞大。处理这样一个大型矩阵将需要大量的时间和内存，因此我们进行降维处理。这是减少矩阵大小的过程，使得可以对其进行进一步操作。
- en: In the case of GloVe, the matrix is known as a co-occurrence counts matrix,
    which contains information on how many times a word has occurred in a particular
    context in a corpus. The rows are the words and the columns are the contexts.
    This matrix is then factorized in order to reduce the dimensions, and the new
    matrix has a vector representation for each word.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GloVe来说，矩阵被称为共现计数矩阵，包含单词在语料库中特定上下文中出现的次数信息。行是单词，列是上下文。然后，这个矩阵会被分解以减少维度，新的矩阵为每个单词提供了一个向量表示。
- en: GloVe also has pretrained words with vectors attached to them that can be used
    if the semantics match the corpus and task at hand. The following activity guides
    you through the process of implementing GloVe in Python, except that the code
    isn't directly given to you, so you'll have to do some thinking and maybe some
    googling. Try it out!
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe还具有预训练的单词，并附有向量，可以在语义匹配语料库和任务时使用。以下活动将引导您完成在Python中实现GloVe的过程，不过代码不会直接给出，所以您需要动脑筋，甚至可能需要一些谷歌搜索。试试看！
- en: 'Exercise 9: Generating Word Embeddings Using GloVe'
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 9：使用GloVe生成词嵌入
- en: In this exercise, we will be generating word embeddings using **Glove-Python**.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用**Glove-Python**生成词嵌入。
- en: Note
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To install Glove-Python on your platform, go to https://pypi.org/project/glove/#files.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 要在您的平台上安装Glove-Python，请访问 https://pypi.org/project/glove/#files。
- en: Download the Text8Corpus from http://mattmahoney.net/dc/text8.zip.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 从 http://mattmahoney.net/dc/text8.zip 下载Text8Corpus。
- en: Extract the file and store it with your Jupyter notebook.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 提取文件并将其与您的Jupyter笔记本一起存储。
- en: 'Import `itertools`:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`itertools`：
- en: '[PRE35]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We need a corpus to generate word embeddings for, and the `gensim.models.word2vec`
    library, luckily, has one called `Text8Corpus`. Import this along with two modules
    from the **Glove-Python** library:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要一个语料库来生成词嵌入，而幸运的是，`gensim.models.word2vec`库中有一个名为`Text8Corpus`的语料库。导入它和**Glove-Python**库中的两个模块：
- en: '[PRE36]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Convert the corpus into sentences in the form of a list using `itertools`:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`itertools`将语料库转换为列表形式的句子：
- en: '[PRE37]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Initiate the `Corpus()` model and fit it on to the sentences:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`Corpus()`模型并将其拟合到句子上：
- en: '[PRE38]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `window` parameter controls how many neighboring words are considered.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`window`参数控制考虑多少个相邻单词。'
- en: 'Now that we have prepared our corpus, we need to train the embeddings. Initiate
    the `Glove()` model:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了语料库，需要训练嵌入。启动`Glove()`模型：
- en: '[PRE39]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Generate a co-occurrence matrix based on the corpus and fit the `glove` model
    on to this matrix:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于语料库生成共现矩阵，并将`glove`模型拟合到此矩阵上：
- en: '[PRE40]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The model has been trained!
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型已训练完成！
- en: 'Add the dictionary of the corpus:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加语料库的字典：
- en: '[PRE41]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Use the following command to see which words are similar to your choice of
    word based on the word embeddings generated:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令查看根据生成的词嵌入，哪些单词与您选择的单词相似：
- en: '[PRE42]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Expected output:**'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.25: Output of word embeddings for ‘man’'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.25：''man''的词嵌入输出'
- en: '](img/C13783_01_25.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_25.jpg)'
- en: 'Figure 1.25: Output of word embeddings for ''man'''
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.25：'man'的词嵌入输出
- en: 'You can try this out for several different words to see which words neighbor
    them and are the most similar to them:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试对多个不同的单词进行此操作，看看哪些单词与它们相邻并且最相似：
- en: '[PRE43]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Expected output:**'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.26: Output of word embeddings for ‘queen’'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.26：''queen''的词嵌入输出'
- en: '](img/C13783_01_26.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_26.jpg)'
- en: 'Figure 1.26: Output of word embeddings for ''queen'''
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.26：'queen'的词嵌入输出
- en: Note
  id: totrans-371
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about GloVe, go to https://nlp.stanford.edu/projects/glove/.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于GloVe的信息，请访问 https://nlp.stanford.edu/projects/glove/。
- en: 'Activity 1: Generating Word Embeddings from a Corpus Using Word2Vec.'
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 1：使用Word2Vec从语料库生成词嵌入。
- en: You have been given the task of training a Word2Vec model on a particular corpus
    – the Text8Corpus, in this case – to determine which words are similar to each
    other. The following steps will help you with the solution.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 您的任务是基于特定语料库（此处为Text8Corpus）训练Word2Vec模型，确定哪些单词彼此相似。以下步骤将帮助您解决问题。
- en: Note
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the text corpus file at http://mattmahoney.net/dc/text8.zip.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在http://mattmahoney.net/dc/text8.zip找到文本语料库文件。
- en: Upload the text corpus from the link given previously.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从先前给出的链接上传文本语料库。
- en: Import `word2vec` from `gensim` models.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`gensim`模型中导入`word2vec`。
- en: Store the corpus in a variable.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将语料库存储在一个变量中。
- en: Fit the word2vec model on the corpus.
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在语料库上拟合word2vec模型。
- en: Find the most similar word to 'man'.
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到与“man”最相似的单词。
- en: '*''Father'' is to ''girl'', ''x'' is to "boy*." Find the top 3 words for x.'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*''Father'' 对 ''girl''，''x'' 对 ''boy''。* 找到x的前3个单词。'
- en: Note
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 296.
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 活动的解决方案可以在第296页找到。
- en: '**Expected Outputs:**'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 1.27: Output for similar word embeddings'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27：相似词嵌入的输出'
- en: '](img/C13783_01_27.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_01_27.jpg)'
- en: 'Figure 1.27: Output for similar word embeddings'
  id: totrans-388
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.27：相似词嵌入的输出
- en: 'Top three words for ''**x**'' could be:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '''**x**''的前三个词可能是：'
- en: '![Figure 1.28: Output for top three words for ‘x’](img/C13783_01_28.jpg)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.28：''x''的前三个词的输出](img/C13783_01_28.jpg)'
- en: 'Figure 1.28: Output for top three words for ''x'''
  id: totrans-391
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1.28：'x'的前三个词的输出
- en: Summary
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about how natural language processing enables humans
    and machines to communicate in natural human language. There are three broad applications
    of natural language processing, and these are speech recognition, natural language
    understanding, and natural language generation.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了自然语言处理如何使人类和机器能够使用自然语言进行交流。自然语言处理有三大应用领域，分别是语音识别、自然语言理解和自然语言生成。
- en: Language is a complicated thing, and so text is required to go through several
    phases before it can make sense to a machine. This process of filtering is known
    as text preprocessing and comprises various techniques that serve different purposes.
    They are all task- and corpora-dependent and prepare text for operations that
    will enable it to be input into machine learning and deep learning models.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是复杂的，因此文本需要经过多个阶段，才能让机器理解。这个过滤过程称为文本预处理，包含了多种技术，服务于不同的目的。它们都是任务和语料库依赖的，并为将文本输入到机器学习和深度学习模型中做好准备。
- en: Since machine learning and deep learning models work best with numerical data,
    it is necessary to transform preprocessed corpora into numerical form. This is
    where word embeddings come into the picture; they are real-value vector representations
    of words that aid models in predicting and understanding words. The two main algorithms
    used to generate word embeddings are Word2Vec and GloVe.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习和深度学习模型最适合处理数值数据，因此有必要将预处理过的语料库转换为数值形式。此时，词嵌入便登场了；它们是单词的实值向量表示，帮助模型预测和理解单词。生成词嵌入的两种主要算法是Word2Vec和GloVe。
- en: In the next chapter, we will be building on the algorithms used for natural
    language processing. The processes of POS tagging and named entity recognition
    will be introduced and explained.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将基于自然语言处理算法进行深入探讨。我们将介绍和解释词性标注和命名实体识别的过程。
