- en: TensorFlow in Action - Some Basic Examples
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 实践 - 一些基本示例
- en: ',In this chapter, we will explain the main computational concept behind TensorFlow,
    which is the computational graph model, and demonstrate how to get you on track
    by implementing linear regression and logistic regression.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解释 TensorFlow 背后的主要计算概念，即计算图模型，并展示如何通过实现线性回归和逻辑回归帮助你入门。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Capacity of a single neuron and activation functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个神经元的能力与激活函数
- en: Activation functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: Feed-forward neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: The need for a multilayer network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层网络的需求
- en: TensorFlow terminologies—recap
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 术语—回顾
- en: Linear regression model—building and training
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归模型—构建与训练
- en: Logistic regression model—building and training
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归模型—构建与训练
- en: We will start by explaining what a single neuron can actually do/model, and
    based on this, the need for a multilayer network will arise. Next up, we will
    do more elaboration of the main concepts and tools that are used/available within
    TensorFlow and how to use these tools to build up simple examples such as linear
    regression and logistic regression.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从解释单个神经元实际上可以做什么/建模开始，并基于此，提出多层网络的需求。接下来，我们将对在 TensorFlow 中使用/可用的主要概念和工具做更详细的阐述，并展示如何使用这些工具构建简单的示例，如线性回归和逻辑回归。
- en: Capacity of a single neuron
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单个神经元的能力
- en: A **neural network**  is a computational model that is mainly inspired by the
    way the biological neural networks of the human brain process the incoming information.
    Neural networks made a huge breakthrough in machine learning research (deep learning,
    specifically) and industrial applications, such as breakthrough results in computer
    vision, speech recognition, and text processing. In this chapter, we will try
    to develop an understanding of a particular type of neural network called the **multi-layer
    Perceptron**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络** 是一种计算模型，主要受到人类大脑生物神经网络处理传入信息方式的启发。神经网络在机器学习研究（特别是深度学习）和工业应用中取得了巨大突破，如计算机视觉、语音识别和文本处理等领域取得了突破性的成果。本章中，我们将尝试理解一种特定类型的神经网络，即
    **多层感知器**。'
- en: Biological motivation and connections
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物学动机与连接
- en: The basic computational unit of our brains is called a **neuron**, and we have
    approximately 86 billion neurons in our nervous system, which are connected with
    approximately ![](img/d63a8baf-bbe2-4bae-ad11-1657839aa8bf.png) to ![](img/c889ca1f-940a-4aa4-b33d-d9631a8aa4c0.png)
    synapses.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大脑的基本计算单元是**神经元**，我们神经系统中大约有 860 亿个神经元，这些神经元通过大约 ![](img/d63a8baf-bbe2-4bae-ad11-1657839aa8bf.png)
    到 ![](img/c889ca1f-940a-4aa4-b33d-d9631a8aa4c0.png) 的突触相连接。
- en: '*Figure 1* shows a biological neuron. *Figure 2* shows the corresponding mathematical
    model. In the drawing of the biological neuron, each neuron receives incoming
    signals from its dendrites and then produces output signals along its axon, where
    the axon gets split out and connects via synapses to other neurons.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 1* 显示了生物神经元，*图 2* 显示了对应的数学模型。在生物神经元的图示中，每个神经元通过树突接收传入信号，然后沿着轴突产生输出信号，轴突分支后通过突触连接到其他神经元。'
- en: In the corresponding mathematical computational model of a neuron, the signals
    that travel along the axons ![](img/613ebbfe-8820-436c-914e-9fcf1212b25f.png)
    interact with a multiplication operation ![](img/12164cb0-d15a-430c-94a1-ba0adabcfa27.png)
    with the dendrites of the other neuron in the system based on the synaptic strength
    at that synapse, which is represented by ![](img/b060c83c-50ec-4aa1-9eec-425f11521da3.png).
    The idea is that the synaptic weights/strength ![](img/9ed5a5b6-6d41-4348-8719-8ee0f4812468.png)
    gets learned by the network and they're the ones that control the influence of
    a specific neuron on another.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经元的对应数学计算模型中，沿轴突传播的信号 ![](img/613ebbfe-8820-436c-914e-9fcf1212b25f.png) 与树突的乘法操作
    ![](img/12164cb0-d15a-430c-94a1-ba0adabcfa27.png) 相互作用，该树突来自系统中另一个神经元，并根据该突触的突触强度进行交互，突触强度由
    ![](img/b060c83c-50ec-4aa1-9eec-425f11521da3.png) 表示。其核心思想是，突触权重/强度 ![](img/9ed5a5b6-6d41-4348-8719-8ee0f4812468.png)
    由网络学习，它们控制一个特定神经元对另一个神经元的影响。
- en: Also, in the basic computational model in *Figure 2*, the dendrites carry the
    signal to the main cell body where it sums them all. If the final result is above
    a certain threshold, the neuron can fire in the computational model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在*图 2*中，树突将信号传送到细胞体，细胞体将这些信号求和。如果最终结果超过某个阈值，神经元就会在计算模型中被激活。
- en: 'Also, it is worth mentioning that we need to control the frequency of the output
    spikes along the axon, so we use something called an **activation function**.
    Practically, a common choice of activation function is the sigmoid function σ,
    since it takes a real-valued input (the signal strength after the sum) and squashes
    it to be between 0 and 1\. We will see the details of these activation functions
    later in the following section:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，值得一提的是，我们需要控制通过轴突传递的输出脉冲频率，因此我们使用被称为**激活函数**的东西。实际上，一个常用的激活函数是 Sigmoid 函数
    σ，因为它接受一个实数值输入（求和后的信号强度）并将其压缩到 0 和 1 之间。我们将在接下来的部分中看到这些激活函数的详细信息：
- en: '![](img/903ae559-7cca-47b8-a502-97797b4a994c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/903ae559-7cca-47b8-a502-97797b4a994c.png)'
- en: 'Figure 1: Computational unit of the brain (http://cs231n.github.io/assets/nn1/neuron.png)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：大脑的计算单元（http://cs231n.github.io/assets/nn1/neuron.png）
- en: 'There is the corresponding basic mathematical model for the biological one:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生物学模型对应的基本数学模型：
- en: '![](img/c94c95ce-1f72-4a7b-9f55-c17a2f00ad10.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c94c95ce-1f72-4a7b-9f55-c17a2f00ad10.jpeg)'
- en: 'Figure 2: Mathematical modeling of the Brain''s computational unit (http://cs231n.github.io/assets/nn1/neuron_model.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：大脑计算单元的数学模型（http://cs231n.github.io/assets/nn1/neuron_model.jpeg）
- en: The basic unit of computation in a neural network is the neuron, often called
    a **node** or **unit**. It receives input from some other nodes or from an external
    source and computes an output. Each input has an associated **weight** (**w**),
    which is assigned on the basis of its importance relative to other inputs. The
    node applies a function *f* (we've defined it later) to the weighted sum of its
    inputs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的基本计算单元是神经元，通常称为**节点**或**单元**。它接收来自其他节点或外部来源的输入，并计算输出。每个输入都有一个相关的**权重**（**w**），该权重根据该输入相对于其他输入的重要性分配。节点将一个函数
    *f*（我们稍后会定义）应用于其输入的加权和。
- en: So, the basic computational unit of neural networks in general is called **neuron**/**node**/**unit.**
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络的一般基本计算单元称为**神经元**/**节点**/**单元**。
- en: This neuron receives its input from previous neurons or even an external source
    and then it does some processing on this input to produce a so-called activation.
    Each input to this neuron is associated with its own weight ![](img/c0332e36-48b3-4716-b43a-177ce42e409a.png),
    which represents the strength of this connection and hence the importance of this
    input.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个神经元接收来自前一个神经元或外部来源的输入，然后对该输入进行处理以产生所谓的激活。每个输入到这个神经元的信号都有自己的权重 ![](img/c0332e36-48b3-4716-b43a-177ce42e409a.png)，它表示连接的强度，从而也表示该输入的重要性。
- en: So, the final output of this basic building block of the neural network is a
    summed version of the inputs weighted by their importance *w*, and then the neuron
    passes the summed output through an activation function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络这个基本构建模块的最终输出是加权求和后的输入 *w*，然后神经元通过激活函数处理加和后的输出。
- en: '![](img/7cb94f65-1a2f-497f-9668-800a94d2ae6a.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cb94f65-1a2f-497f-9668-800a94d2ae6a.png)'
- en: 'Figure 3: A single neuron'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：单个神经元
- en: Activation functions
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'The output from the neuron is computed as shown in *Figure 3*, and passed through
    an activation function that introduces non-linearity to the output. This *f* is
    called an **activation function**. The main purposes of the activation functions
    are to:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的输出如*图 3*所示进行计算，并通过激活函数进行处理，从而在输出中引入非线性。这个 *f* 称为**激活函数**。激活函数的主要目的是：
- en: Introduce nonlinearity into the output of a neuron. This is important because
    most real-world data is nonlinear and we want neurons to learn these nonlinear
    representations.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在神经元的输出中引入非线性。这一点非常重要，因为大多数真实世界的数据是非线性的，我们希望神经元能够学习这些非线性表示。
- en: Squash the output to be in a specific range.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出压缩到特定范围内。
- en: Every activation function (or nonlinearity) takes a single number and performs
    a certain fixed mathematical operation on it. There are several activation functions
    you may encounter in practice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个激活函数（或非线性函数）接受一个数字并对其执行一定的固定数学操作。在实际中，你可能会遇到几种激活函数。
- en: So, we are going to briefly cover the most common activation functions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将简要介绍最常见的激活函数。
- en: Sigmoid
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Sigmoid
- en: 'Historically, the sigmoid activation function is widely used among researchers.
    This function accepts a real-valued input and squashes it to a range between 0
    and 1, as shown in the following figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，Sigmoid 激活函数在研究人员中广泛使用。该函数接受一个实数值输入，并将其压缩到 0 和 1 之间，如下图所示：
- en: '*σ(x) = 1 / (1 + exp(−x))*![](img/cb589e92-9a77-4fa2-b417-d69f20ea718a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ(x) = 1 / (1 + exp(−x))*![](img/cb589e92-9a77-4fa2-b417-d69f20ea718a.png)'
- en: 'Figure 4: Sigmoid activation function'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Sigmoid 激活函数
- en: Tanh
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tanh
- en: 'Tanh is another activation function that tolerates some negative values. Tanh
    accepts a real-valued input and squashes them to [-1, 1]:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 是另一种激活函数，能够容忍一些负值。Tanh 接受一个实值输入，并将其压缩到 [-1, 1] 之间：
- en: '*tanh(x) = 2σ(2x) − 1*![](img/feef9554-16ff-4f6b-9047-cdc9b679729f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*tanh(x) = 2σ(2x) − 1*![](img/feef9554-16ff-4f6b-9047-cdc9b679729f.png)'
- en: 'Figure 5: Tanh activation function'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Tanh 激活函数
- en: ReLU
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ReLU
- en: '**Rectified linear unit** (**ReLU**) does not tolerate negative values as it
    accepts a real-valued input and thresholds it at zero (replaces negative values
    with zero):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLU**）不容忍负值，因为它接受一个实值输入并将其在零处进行阈值处理（将负值替换为零）：'
- en: '*f(x) = max(0, x)*![](img/f1c98eda-2e60-4fb1-b43d-094110178204.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x) = max(0, x)*![](img/f1c98eda-2e60-4fb1-b43d-094110178204.jpg)'
- en: 'Figure 6: Relu activation function'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Relu 激活函数
- en: '**Importance of bias**: The main function of bias is to provide every node
    with a trainable constant value (in addition to the normal inputs that the node
    receives). See this link at [https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)
    to learn more about the role of bias in a neuron.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏置的重要性**：偏置的主要功能是为每个节点提供一个可训练的常量值（除了节点接收的正常输入之外）。请参见此链接 [https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)
    了解有关神经元中偏置作用的更多信息。'
- en: Feed-forward neural network
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络
- en: The feed-forward neural network was the first and simplest type of artificial
    neural network devised. It contains multiple neurons (nodes) arranged in layers.
    Nodes from adjacent layers have connections or edges between them. All these connections
    have weights associated with them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络是最早且最简单的人工神经网络类型。它包含多个神经元（节点），这些神经元按层排列。相邻层的节点之间有连接或边。这些连接都有与之关联的权重。
- en: 'An example of a feed-forward neural network is shown in *Figure 7*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前馈神经网络的示例如 *图7* 所示：
- en: '![](img/6b47331f-bfc0-4af2-8fd1-f08ddf2ec754.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b47331f-bfc0-4af2-8fd1-f08ddf2ec754.png)'
- en: 'Figure 7: An example feed-forward neural network'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一个示例前馈神经网络
- en: In a feed-forward network, the information moves in only one direction—forward—from
    the input nodes, through the hidden nodes (if any), and to the output nodes. There
    are no cycles or loops in the network (this property of feed-forward networks
    is different from recurrent neural networks, in which the connections between
    nodes form a cycle).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在前馈网络中，信息仅向一个方向流动——从输入节点，通过隐藏节点（如果有的话），然后到输出节点。网络中没有循环或回路（这种前馈网络的特性与循环神经网络不同，后者节点之间的连接会形成循环）。
- en: The need for multilayer networks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层网络的需求
- en: A **multi-layer perceptron** (**MLP**) contains one or more hidden layers (apart
    from one input and one output layer). While a single layer perceptron can learn only linear
    functions, a MLP can also learn non-linear functions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）包含一个或多个隐藏层（除了一个输入层和一个输出层）。虽然单层感知器只能学习线性函数，但 MLP 也可以学习非线性函数。'
- en: '*Figure 7* shows MLP with a single hidden layer. Note that all connections
    have weights associated with them, but only three weights (*w0*, *w1*, and *w2*)
    are shown in the figure.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7* 显示了一个具有单个隐藏层的 MLP。请注意，所有连接都有与之关联的权重，但图中仅显示了三个权重（*w0*、*w1* 和 *w2*）。'
- en: '**Input Layer**: The Input layer has three nodes. The bias node has a value
    of 1\. The other two nodes take X1 and X2 as external inputs (which are numerical
    values depending upon the input dataset). As discussed before, no computation,
    is performed in the **Input Layer**, so the outputs from nodes in the **Input
    Layer** are **1**, **X1**, and **X2** respectively, which are fed into the **Hidden
    Layer**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**：输入层有三个节点。偏置节点的值为 1。其他两个节点将 X1 和 X2 作为外部输入（这些数值取决于输入数据集）。如前所述，**输入层**中不执行计算，因此
    **输入层** 中节点的输出分别是 **1**、**X1** 和 **X2**，并将其送入 **隐藏层**。'
- en: '**Hidden Layer**: The **Hidden Layer** also has three nodes, with the bias
    node having an output of 1\. The output of the other two nodes in the **Hidden
    Layer** depends on the outputs from the **Input Layer** (**1**, **X1**, and **X2**)
    as well as the weights associated with the connections (edges). Remember that
    *f* refers to the activation function. These outputs are then fed to the nodes
    in the **Output Layer**.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**隐藏层：** **隐藏层**也有三个节点，其中偏置节点的输出为1。隐藏层中另外两个节点的输出依赖于来自**输入层**的输出（**1**，**X1**
    和 **X2**），以及与连接（边）相关的权重。记住，*f*指的是激活函数。这些输出随后被馈送到**输出层**中的节点。'
- en: '![](img/38718db0-2454-42d0-be89-545151286aba.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38718db0-2454-42d0-be89-545151286aba.png)'
- en: 'Figure 8: A multi-layer perceptron having one hidden layer'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：具有一个隐藏层的多层感知器
- en: '**Output Layer:** The **Output Layer** has two nodes; they take inputs from
    the **Hidden Layer** and perform similar computations as shown for the highlighted
    hidden node. The values calculated (**Y1** and **Y2**) as a result of these computations
    act as outputs of the multi-layer perceptron.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出层：** **输出层**有两个节点；它们从**隐藏层**接收输入，并执行类似于高亮显示的隐藏节点所示的计算。计算得出的值（**Y1** 和 **Y2**）作为多层感知器的输出。'
- en: Given a set of features *X = (x1, x2, …)* and a target *y*, a multi-layer perceptron
    can learn the relationship between the features and the target for either classification
    or regression.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组特征 *X = (x1, x2, …)* 和目标 *y*，多层感知器可以学习特征与目标之间的关系，无论是分类问题还是回归问题。
- en: 'Let''s take an example to understand multi-layer perceptrons better. Suppose
    we have the following student marks dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来更好地理解多层感知器。假设我们有以下学生成绩数据集：
- en: '**Table 1 – Sample student marks dataset**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**表1 – 示例学生成绩数据集**'
- en: '| **Hours studied** | **Mid term marks** | **Final term results** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **学习小时数** | **期中考试成绩** | **期末考试结果** |'
- en: '| 35 | 67 | Pass |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 35 | 67 | 通过 |'
- en: '| 12 | 75 | Fail |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 75 | 未通过 |'
- en: '| 16 | 89 | Pass |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 89 | 通过 |'
- en: '| 45 | 56 | Pass |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 45 | 56 | 通过 |'
- en: '| 10 | 90 | Fail |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 90 | 未通过 |'
- en: The two input columns show the number of hours the student has studied and the
    mid term marks obtained by the student. The **Final Result** column can have two
    values, **1** or **0**, indicating whether the student passed in the final term
    or not. For example, we can see that if the student studied 35 hours and had obtained
    67 marks in the mid term, he/she ended up passing the final term.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这两列输入数据表示学生学习的小时数和学生在期中考试中获得的成绩。**期末结果**列可以有两个值，**1** 或 **0**，表示学生是否通过期末考试。例如，我们可以看到，如果学生学习了35小时并且期中考试得了67分，他/她最终通过了期末考试。
- en: 'Now, suppose we want to predict whether a student studying 25 hours and having
    70 marks in the mid term will pass the final term:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想预测一个学生学习了25小时并且期中考试得了70分，他/她是否能通过期末考试：
- en: '**Table 2 – Sample student with unknown final term result**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**表2 – 示例学生期末考试结果未知**'
- en: '| **Hours studied** | **Mid term marks** | **Final term result** |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **学习小时数** | **期中考试成绩** | **期末考试结果** |'
- en: '| 26 | 70 | ? |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 70 | ? |'
- en: This is a binary classification problem, where a MLP can learn from the given
    examples (training data) and make an informed prediction given a new data point.
    We will soon see how a MLP learns such relationships.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个二分类问题，其中多层感知器可以从给定的示例（训练数据）中学习，并在给定新数据点时做出有根据的预测。我们很快就会看到多层感知器如何学习这些关系。
- en: Training our MLP – the backpropagation algorithm
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的MLP – 反向传播算法
- en: The process by which a multi-layer perceptron learns is called the **backpropagation**
    algorithm. I would recommend reading this Quora answer by Hemanth Kumar, [https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)
    (quoted later), which explains backpropagation clearly.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器学习的过程称为**反向传播**算法。我推荐阅读Hemanth Kumar在Quora上的这篇回答，[https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)（后面引用），该回答清晰地解释了反向传播。
- en: '"**Backward Propagation of Errors**, often abbreviated as BackProp is one of
    the several ways in which an artificial neural network (ANN) can be trained. It
    is a supervised training scheme, which means, it learns from labeled training
    data (there is a supervisor, to guide its learning).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '"**误差反向传播**，通常简称为BackProp，是人工神经网络（ANN）训练的几种方式之一。它是一种监督式训练方法，这意味着它从带标签的训练数据中学习（有一个监督者来引导其学习）。'
- en: To put in simple terms, BackProp is like "**learning from mistakes"**. The supervisor
    corrects the ANN whenever it makes mistakes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，BackProp就像是“**从错误中学习**”。每当ANN犯错时，监督者都会纠正它。
- en: An ANN consists of nodes in different layers; input layer, intermediate hidden
    layer(s) and the output layer. The connections between nodes of adjacent layers
    have "weights" associated with them. The goal of learning is to assign correct
    weights for these edges. Given an input vector, these weights determine what the
    output vector is.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ANN由不同层次的节点组成：输入层、隐藏层和输出层。相邻层之间节点的连接有与之关联的“权重”。学习的目标是为这些边分配正确的权重。给定一个输入向量，这些权重决定了输出向量的值。
- en: In supervised learning, the training set is labeled. This means, for some given
    inputs, we know the desired/expected output (label).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，训练集是标注的。这意味着对于某些给定的输入，我们知道期望/预期的输出（标签）。
- en: 'BackProp Algorithm:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法：
- en: Initially all the edge weights are randomly assigned. For every input in the
    training dataset, the ANN is activated and its output is observed. This output
    is compared with the desired output that we already know, and the error is "propagated"
    back to the previous layer. This error is noted and the weights are "adjusted"
    accordingly. This process is repeated until the output error is below a predetermined
    threshold.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，所有边的权重是随机分配的。对于训练数据集中的每个输入，激活人工神经网络（ANN）并观察其输出。将此输出与我们已知的期望输出进行比较，误差被“传播”回前一层。该误差被记录并相应地“调整”权重。这个过程会不断重复，直到输出误差低于预定的阈值。
- en: Once the above algorithm terminates, we have a "learned" ANN which, we consider
    is ready to work with "new" inputs. This ANN is said to have learned from several
    examples (labeled data) and from its mistakes (error propagation)."
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦上述算法终止，我们就得到了一个“学习过”的ANN，我们认为它已经准备好处理“新”输入。这个ANN被认为已经从多个示例（标注数据）以及它的错误（误差传播）中学习了。”
- en: —Hemanth Kumar.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: —Hemanth Kumar。
- en: Now that we have an idea of how backpropagation works, let's go back to our
    student marks dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了反向传播的工作原理，让我们回到学生成绩数据集。
- en: The MLP shown in *Figure 8* has two nodes in the input layer, which take the
    inputs hours studied and mid term marks. It also has a hidden layer with two nodes.
    The output layer has two nodes as well; the upper node outputs the probability
    of *pass* while the lower node outputs the probability of *fail*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 显示在*图 8*中的MLP有两个输入层节点，分别接收学习时长和期中成绩作为输入。它还拥有一个包含两个节点的隐藏层。输出层也有两个节点；上层节点输出*通过*的概率，而下层节点输出*失败*的概率。
- en: 'In classification applications, we widely use a softmax function ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    as the activation function in the output layer of the MLP to ensure that the outputs
    are probabilities and they add up to 1\. The softmax function takes a vector of
    arbitrary real-valued scores and squashes it to a vector of values between 0 and
    1 that sum up to 1\. So, in this case:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类应用中，我们广泛使用softmax函数 ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    作为MLP输出层的激活函数，以确保输出是概率，并且它们的和为1。softmax函数接受一个任意实数值的向量，并将其压缩成一个在0和1之间的值的向量，且它们的和为1。因此，在此情况下：
- en: '![](img/e46f34df-4448-4998-aba9-9ce537e465ee.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e46f34df-4448-4998-aba9-9ce537e465ee.png)'
- en: Step 1 – forward propagation
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1 – 前向传播
- en: All weights in the network are randomly initialized. Let's consider a specific
    hidden layer node and call it *V*. Assume that the weights of the connections
    from the inputs to that node are **w1**, **w2**, and **w3** (as shown).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 网络中的所有权重都是随机初始化的。我们考虑一个特定的隐藏层节点，并称其为*V*。假设从输入到该节点的连接权重为**w1**、**w2**和**w3**（如图所示）。
- en: 'The network then takes the first training samples as input (we know that for
    inputs 35 and 67, the probability of passing is 1):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，网络将第一个训练样本作为输入（我们知道，对于输入35和67，及格的概率是1）：
- en: Input to the network = [35, 67]
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络输入 = [35, 67]
- en: Desired output from the network (target) = [1, 0]
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络期望输出（目标） = [1, 0]
- en: 'Then, output *V* from the node in consideration, which can be calculated as
    follows (f is an activation function such as sigmoid):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，考虑节点的输出*V*，可以通过以下方式计算（f是激活函数，如sigmoid）：
- en: '*V = f (1*w1 + 35*w2 + 67*w3)*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*V = f (1*w1 + 35*w2 + 67*w3)*'
- en: Similarly, outputs from the other node in the hidden layer are also calculated.
    The outputs of the two nodes in the hidden layer act as inputs to the two nodes
    in the output layer. This enables us to calculate output probabilities from the
    two nodes in the output layer.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，来自隐藏层的另一个节点的输出也会被计算出来。隐藏层中两个节点的输出作为输入，传递给输出层的两个节点。这使我们能够计算输出层两个节点的输出概率。
- en: Suppose the output probabilities from the two nodes in the output layer are
    0.4 and 0.6, respectively (since the weights are randomly assigned, outputs will
    also be random). We can see that the calculated probabilities (0.4 and 0.6) are
    very far from the desired probabilities (1 and 0 respectively); hence the network
    is said to have an *incorrect output*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设输出层两个节点的输出概率分别是0.4和0.6（由于权重是随机分配的，输出也会是随机的）。我们可以看到，计算出来的概率（0.4和0.6）与期望的概率（分别是1和0）相差很远，因此可以说网络产生了*错误的输出*。
- en: Step 2 – backpropagation and weight updation
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2 – 反向传播与权重更新
- en: We calculate the total error at the output nodes and propagate these errors
    back through the network using backpropagation to calculate the gradients. Then,
    we use an optimization method such as gradient descent to adjust all weights in
    the network with an aim of reducing the error at the output layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算输出节点的总误差，并通过反向传播将这些误差传递回网络，计算梯度。然后，我们使用诸如梯度下降之类的优化方法来调整网络中所有的权重，目的是减少输出层的误差。
- en: Suppose that the new weights associated with the node in consideration are *w4*,
    *w5*, and *w6* (after backpropagation and adjusting weights).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设考虑的节点的新权重是*w4*、*w5*和*w6*（经过反向传播并调整权重后）。
- en: If we now feed the same sample as an input to the network, the network should
    perform better than the initial run since the weights have now been optimized
    to minimize the error in prediction. The errors at the output nodes now reduce
    to [0.2, -0.2] as compared to [0.6, -0.4] earlier. This means that our network
    has learned to correctly classify our first training sample.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在将相同的样本作为输入喂入网络，由于权重已经被优化以最小化预测误差，网络的表现应该比初始运行更好。输出节点的误差现在减少到[0.2, -0.2]，而之前是[0.6,
    -0.4]。这意味着我们的网络已经学会正确地分类我们的第一个训练样本。
- en: We repeat this process with all other training samples in our dataset. Then,
    our network is said to have learned those examples.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据集中的所有其他训练样本重复这个过程。然后，我们可以说我们的网络已经学习了这些示例。
- en: If we now want to predict whether a student studying 25 hours and having 70
    marks in the mid term will pass the final term, we go through the forward propagation
    step and find the output probabilities for pass and fail.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在想预测一名学习了25小时并且期中考试得了70分的学生是否能通过期末考试，我们通过前向传播步骤，找到通过与不通过的输出概率。
- en: 'I have avoided mathematical equations and explanation of concepts such as gradient
    descent here and have rather tried to develop an intuition for the algorithm.
    For a more mathematically involved discussion of the backpropagation algorithm,
    refer to this link: [http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里避免了数学方程和梯度下降等概念的解释，而是尽量为算法建立直觉。关于反向传播算法的更深入的数学讨论，请参考这个链接：[http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html](http://home.agh.edu.pl/%7Evlsi/AI/backp_t_en/backprop.html)。
- en: TensorFlow terminologies – recap
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow术语回顾
- en: In this section, we will provide an overview of the TensorFlow library as well
    as the structure of a basic TensorFlow application. TensorFlow is an open source
    library for creating large-scale machine learning applications; it can model computations
    on a wide variety of hardware, ranging from android devices to heterogeneous multi-gpu
    systems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将概述TensorFlow库以及基本TensorFlow应用程序的结构。TensorFlow是一个开源库，用于创建大规模的机器学习应用程序；它可以在各种硬件上建模计算，从安卓设备到异构多GPU系统。
- en: TensorFlow uses a special structure in order to execute code on different devices
    such as CPUs and GPUs. Computations are defined as a graph and each graph is made
    up of operations, also known as **ops**, so whenever we work with TensorFlow,
    we define the series of operations in a graph.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 使用一种特殊的结构来在不同的设备上执行代码，如 CPU 和 GPU。计算被定义为一个图形，每个图形由操作组成，也称为**操作**，因此每当我们使用
    TensorFlow 时，我们都会在图形中定义一系列操作。
- en: To run these operations, we need to launch the graph into a session. The session
    translates the operations and passes them to a device for execution.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这些操作，我们需要将图形加载到一个会话中。会话会翻译这些操作并将它们传递给设备进行执行。
- en: For example, the following image represents a graph in TensorFlow. *W*, *x*,
    and *b* are tensors over the edges of this graph. *MatMul* is an operation over
    the tensors *W* and *x*; after that, *Add* is called and we add the result of
    the previous operator with *b*. The resultant tensors of each operation cross
    the next one until the end, where it's possible to get the desired result.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面的图像表示了一个 TensorFlow 图形。*W*、*x* 和 *b* 是图中边缘上的张量。*MatMul* 是对张量 *W* 和 *x*
    的操作；之后，调用 *Add*，并将前一个操作的结果与 *b* 相加。每个操作的结果张量会传递给下一个操作，直到最后，可以得到所需的结果。
- en: '![](img/e2f35a77-296a-4058-832f-d70cfdfbea53.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2f35a77-296a-4058-832f-d70cfdfbea53.png)'
- en: 'Figure 9: Sample TensorFlow computational graph'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：示例 TensorFlow 计算图
- en: 'In order to use TensorFlow, we need to import the library; we''ll give it the
    name `tf` so that we can access a module by writing `tf` dot and then the module''s
    name:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 TensorFlow，我们需要导入该库；我们将其命名为 `tf`，这样就可以通过写 `tf` 点号再加上模块名来访问模块：
- en: '[PRE0]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To create our first graph, we will start by using source operations, which do
    not require any input. These source operations or source ops will pass their information
    to other operations, which will actually run computations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的第一个图形，我们将从使用源操作开始，这些操作不需要任何输入。这些源操作或源操作将把它们的信息传递给其他操作，这些操作将实际执行计算。
- en: 'Let''s create two source operations that will output numbers. We will define
    them as `A` and `B`, which you can see in the following piece of code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个源操作，它们将输出数字。我们将它们定义为 `A` 和 `B`，你可以在下面的代码片段中看到：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After that, we''ll define a simple computational operation `tf.add()`, used
    to sum two elements. You can also use `C = A + B`, as shown in this code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将定义一个简单的计算操作 `tf.add()`，用来将两个元素相加。你也可以使用 `C = A + B`，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since graphs need to be executed in the context of a session, we need to create
    a session object:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图形需要在会话的上下文中执行，我们需要创建一个会话对象：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To watch the graph, let''s run the session to get the result from the previously
    defined `C` operation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看图形，让我们运行会话来获取之前定义的 `C` 操作的结果：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You're probably thinking that it was a lot of work just to add two numbers together,
    but it's extremely important that you understand the basic structure of TensorFlow.
    Once you do so, you can define any computations that you want; again, TensorFlow's
    structure allows it to handle computations on different devices (CPU or GPU),
    and even in clusters. If you want to learn more about this, you can run the method
    `tf.device()`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会觉得，仅仅是加两个数字就做了很多工作，但理解 TensorFlow 的基本结构是非常重要的。一旦你理解了它，你就可以定义任何你想要的计算；再次强调，TensorFlow
    的结构使它能够处理不同设备（CPU 或 GPU）甚至集群上的计算。如果你想了解更多，可以运行方法`tf.device()`。
- en: Also feel free to experiment with the structure of TensorFlow in order to get
    a better idea of how it works. If you want a list of all the mathematical operations
    that TensorFlow supports, you can check out the documentation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以随时实验 TensorFlow 的结构，以便更好地理解它是如何工作的。如果你想查看 TensorFlow 支持的所有数学操作，可以查阅文档。
- en: By now, you should understand the structure of TensorFlow and how to create
    a basic applications.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该已经理解了 TensorFlow 的结构以及如何创建基本的应用程序。
- en: Defining multidimensional arrays using TensorFlow
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 定义多维数组
- en: 'Now we will try to define such arrays using TensorFlow:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将尝试使用 TensorFlow 定义这些数组：
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that you understand these data structures, I encourage you to play with
    them using some previous functions to see how they will behave, according to their
    structure types:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了这些数据结构，我鼓励你使用一些之前的函数来尝试这些数据结构，看看它们如何根据结构类型表现：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With the regular symbol definition and also the `tensorflow` function, we were
    able to get an element-wise multiplication, also known as **Hadamard product**.
    But what if we want the regular matrix product? We need to use another TensorFlow
    function called `tf.matmul()`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用常规符号定义以及`tensorflow`函数，我们能够实现逐元素相乘，也叫做**哈达玛积**。但如果我们想要常规的矩阵乘法呢？我们需要使用另一个 TensorFlow
    函数，叫做`tf.matmul()`：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can also define this multiplication ourselves, but there is a function that
    already does that, so no need to reinvent the wheel!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以自己定义这个乘法，但已经有一个函数可以做这个，所以不需要重新发明轮子！
- en: Why tensors?
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用张量？
- en: The tensor structure helps us by giving us the freedom to shape the dataset
    the way we want.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 张量结构通过赋予我们自由来帮助我们按自己想要的方式构造数据集。
- en: This is particularly helpful when dealing with images, due to the nature of
    how information in images are encoded.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这在处理图像时特别有用，因为图像中信息的编码方式。
- en: Thinking about images, it's easy to understand that it has a height and width,
    so it would make sense to represent the information contained in it with a two-dimensional
    structure (a matrix)... until you remember that images have colors. To add information
    about the colors, we need another dimension, and that's when Tensors become particularly
    helpful.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 想到图像时，很容易理解它有高度和宽度，因此用二维结构（矩阵）表示其中包含的信息是有意义的……直到你记得图像有颜色。为了添加颜色信息，我们需要另一个维度，这就是张量特别有用的地方。
- en: 'Images are encoded into color channels; image data is represented in each color''s
    intensity in a color channel at a given point, the most common one being RGB (which
    means red, blue, and green). The information contained in an image is the intensity
    of each channel color in the width and height of the image, just like this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图像被编码为颜色通道；图像数据在每个颜色的强度在给定点的颜色通道中表示，最常见的是 RGB（即红色、蓝色和绿色）。图像中包含的信息是每个通道颜色在图像的宽度和高度中的强度，就像这样：
- en: '![](img/37850946-f7a2-47ba-8780-ae325f94f654.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37850946-f7a2-47ba-8780-ae325f94f654.png)'
- en: 'Figure 10: Different color channels for a specific image'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：特定图像的不同颜色通道
- en: So, the intensity of the red channel at each point with width and height can
    be represented in a matrix; the same goes for the blue and green channels. So,
    we end up having three matrices, and when these are combined, they form a tensor.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，红色通道在每个点上的强度（带宽和高度）可以用矩阵表示；蓝色和绿色通道也是如此。于是，我们最终得到三个矩阵，当这些矩阵结合在一起时，就形成了一个张量。
- en: Variables
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变量
- en: Now that we are more familiar with the structure of data, we will take a look
    at how TensorFlow handles variables.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更熟悉数据的结构了，我们将看看 TensorFlow 如何处理变量。
- en: To define variables, we use the command `tf.variable()`. To be able to use variables
    in a computation graph, it is necessary to initialize them before running the
    graph in a session. This is done by running `tf.global_variables_initializer()`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义变量，我们使用命令`tf.variable()`。为了能够在计算图中使用变量，有必要在会话中运行图之前初始化它们。这可以通过运行`tf.global_variables_initializer()`来完成。
- en: 'To update the value of a variable, we simply run an assign operation that assigns
    a value to the variable:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要更新变量的值，我们只需运行一个赋值操作，将一个值分配给变量：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s first create a simple counter, a variable that increases one unit at
    a time:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个简单的计数器，一个每次增加一个单位的变量：
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Variables must be initialized by running an initialization operation after
    having launched the graph. We first have to add the initialization operation to
    the graph:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 变量必须通过运行初始化操作来初始化，前提是图已启动。我们首先需要将初始化操作添加到图中：
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We then start a session to run the graph.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们启动一个会话来运行图。
- en: 'We first initialize the variables, then print the initial value of the state
    variable, and finally run the operation of updating the state variable and printing
    the result after each update:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先初始化变量，然后打印状态变量的初始值，最后运行更新状态变量的操作，并在每次更新后打印结果：
- en: '[PRE17]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Placeholders
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 占位符
- en: Now, we know how to manipulate variables inside TensorFlow, but what about feeding
    data outside of a TensorFlow model?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道如何在 TensorFlow 中操作变量，但如果要向 TensorFlow 模型外部提供数据怎么办？
- en: If you want to feed data to a TensorFlow model from outside a model, you will
    need to use placeholders.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从模型外部向 TensorFlow 模型提供数据，你需要使用占位符。
- en: So, what are these placeholders and what do they do? Placeholders can be seen
    as *holes* in your model, *holes* that you will pass the data to. You can create
    them using `tf.placeholder(datatype)`, where `datatype` specifies the type of
    data (integers, floating points, strings, and Booleans) along with its precision
    (8, 16, 32, and 64) bits.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些占位符是什么，它们有什么作用？占位符可以看作是模型中的*空洞*，*空洞*是你将数据传递给它的地方。你可以通过 `tf.placeholder(datatype)`
    创建它们，其中 `datatype` 指定数据的类型（整数、浮点数、字符串和布尔值）以及其精度（8、16、32 和 64 位）。
- en: 'The definition of each data type with the respective Python syntax is defined
    as:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每种数据类型的定义和相应的 Python 语法如下：
- en: '**Table 3 – Definition of different TensorFlow data types**'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3 – 不同 TensorFlow 数据类型的定义**'
- en: '| **Data type** | **Python type** | **Description** |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** | **Python 类型** | **描述** |'
- en: '| `DT_FLOAT` | `tf.float32` | 32-bits floating point. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `DT_FLOAT` | `tf.float32` | 32 位浮点数。 |'
- en: '| `DT_DOUBLE` | `tf.float64` | 64-bits floating point |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `DT_DOUBLE` | `tf.float64` | 64 位浮点数 |'
- en: '| `DT_INT8` | `tf.int8` | 8-bits signed integer. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT8` | `tf.int8` | 8 位带符号整数。 |'
- en: '| `DT_INT16` | `tf.int16` | 16-bits signed integer. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT16` | `tf.int16` | 16位带符号整数。 |'
- en: '| `DT_INT32` | `tf.int32` | 32-bits signed integer. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT32` | `tf.int32` | 32 位带符号整数。 |'
- en: '| `DT_INT64` | `tf.int64` | 64-bits signed integer. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| `DT_INT64` | `tf.int64` | 64 位带符号整数。 |'
- en: '| `DT_UINT8` | `tf.uint8` | 8-bits unsigned integer. |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| `DT_UINT8` | `tf.uint8` | 8 位无符号整数。 |'
- en: '| `DT_STRING` | `tf.string` | Variable length byte arrays. Each element of
    a Tensor is a byte array. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `DT_STRING` | `tf.string` | 可变长度的字节数组。每个张量的元素都是一个字节数组。 |'
- en: '| `DT_BOOL` | `tf.bool` | Boolean. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `DT_BOOL` | `tf.bool` | 布尔值。 |'
- en: '| `DT_COMPLEX64` | `tf.complex64` | Complex number made of two 32-bits floating
    points: real and imaginary parts. |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `DT_COMPLEX64` | `tf.complex64` | 由两个 32 位浮点数（实部和虚部）组成的复数。 |'
- en: '| `DT_COMPLEX128` | `tf.complex128` | Complex number made of two 64-bits floating
    points: real and imaginary parts. |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `DT_COMPLEX128` | `tf.complex128` | 由两个64位浮点数（实部和虚部）组成的复数。 |'
- en: '| `DT_QINT8` | `tf.qint8` | 8-bits signed integer used in quantized ops. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QINT8` | `tf.qint8` | 用于量化操作的 8 位带符号整数。 |'
- en: '| `DT_QINT32` | `tf.qint32` | 32-bits signed integer used in quantized ops.
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QINT32` | `tf.qint32` | 用于量化操作的 32 位带符号整数。 |'
- en: '| `DT_QUINT8` | `tf.quint8` | 8-bits unsigned integer used in quantized ops.
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| `DT_QUINT8` | `tf.quint8` | 用于量化操作的 8 位无符号整数。 |'
- en: 'So let''s create a placeholder:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们创建一个占位符：
- en: '[PRE19]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And define a simple multiplication operation:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个简单的乘法操作：
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now, we need to define and run the session, but since we created a *hole* in
    the model to pass the data, when we initialize the session. We are obliged to
    pass an argument with the data; otherwise we get an error.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义并运行会话，但由于我们在模型中创建了一个*空洞*来传递数据，因此在初始化会话时，我们必须传递一个带有数据的参数；否则会出现错误。
- en: 'To pass the data to the model, we call the session with an extra argument,
    `feed_dict`, in which we should pass a dictionary with each placeholder name followed
    by its respective data, just like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据传递给模型，我们调用会话时会传入一个额外的参数 `feed_dict`，在其中我们应该传递一个字典，字典的每个占位符名称后跟其对应的数据，就像这样：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since data in TensorFlow is passed in the form of multidimensional arrays,
    we can pass any kind of tensor through the placeholders to get the answer to the
    simple multiplication operation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 中的数据是以多维数组的形式传递的，我们可以通过占位符传递任何类型的张量，以获得简单的乘法操作的结果：
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Operations
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作
- en: Operations are nodes that represent mathematical operations over the tensors
    on a graph. These operations can be any kind of functions, like add and subtract
    tensors, or maybe an activation function.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 操作是表示图中张量的数学运算的节点。这些操作可以是任何类型的函数，比如加法、减法张量，或者可能是激活函数。
- en: '`tf.matmul`, `tf.add`, and  `tf.nn.sigmoid` are some of the operations in TensorFlow.
    These are like functions in Python, but operate directly over tensors and each
    one does a specific thing.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.matmul`、`tf.add` 和 `tf.nn.sigmoid` 是 TensorFlow 中的一些操作。这些类似于 Python 中的函数，但直接作用于张量，每个函数都有特定的功能。'
- en: Other operations can be easily found at: [https://www.tensorflow.org/api_guides/python/math_ops](https://www.tensorflow.org/api_guides/python/math_ops).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 其他操作可以在以下网址找到：[https://www.tensorflow.org/api_guides/python/math_ops](https://www.tensorflow.org/api_guides/python/math_ops)。
- en: 'Let''s play around with some of these operations:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来尝试一些操作：
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`tf.nn.sigmoid` is an activation function: it''s a little more complicated,
    but this function helps learning models to evaluate what kind of information is
    good or not good.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.nn.sigmoid` 是一个激活函数：它有点复杂，但这个函数有助于学习模型评估什么样的信息是有用的，什么是无用的。'
- en: Linear regression model – building and training
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归模型——构建与训练
- en: According to our explanation of linear regression in the [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml),
    *Data Modeling in Action - The Titanic Example* we are going to rely on this definition
    to build a simple linear regression model.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在[第2章](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml)《数据建模实践——泰坦尼克号示例》中的线性回归解释，*数据建模实践——泰坦尼克号示例*，我们将依赖这个定义来构建一个简单的线性回归模型。
- en: 'Let''s start off by importing the necessary packages for this implementation:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入实现所需的必要包：
- en: '[PRE27]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s define an independent variable:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个自变量：
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](img/8e33a510-0612-4242-8cc9-7229a1ae9265.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e33a510-0612-4242-8cc9-7229a1ae9265.png)'
- en: 'Figure 11: Visualization of the dependent variable versus the independent one'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：依赖变量与自变量的可视化
- en: Now, let's see how this gets interpreted into a TensorFlow code.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这如何转化为 TensorFlow 代码。
- en: Linear regression with TensorFlow
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行线性回归
- en: 'For the first part, we will generate random data points and define a linear
    relation; we''ll use TensorFlow to adjust and get the right parameters:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将生成随机数据点并定义线性关系；我们将使用 TensorFlow 来调整并获得正确的参数：
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The equation for the model used in this example is:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例中使用的模型方程是：
- en: '![](img/9bc580b1-9609-40b2-9c9c-f3cffa70bd0f.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bc580b1-9609-40b2-9c9c-f3cffa70bd0f.png)'
- en: 'Nothing special about this equation, it is just a model that we use to generate
    our data points. In fact, you can change the parameters to whatever you want,
    as you will do later. We add some Gaussian noise to the points to make it a bit
    more interesting:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程没有什么特别之处，它只是我们用来生成数据点的模型。事实上，你可以像稍后一样更改参数。我们添加了一些高斯噪声，使数据点看起来更有趣：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is a sample of data:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据的一个示例：
- en: '[PRE33]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'First, we initialize the variables ![](img/7beac794-06f0-4f0c-95d2-8f5344ad94e2.png)
    and ![](img/323cad53-6532-4e69-8234-50b2b8511bc7.png) with any random guess, and
    then we define the linear function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们用任何随机猜测初始化变量 ![](img/7beac794-06f0-4f0c-95d2-8f5344ad94e2.png) 和 ![](img/323cad53-6532-4e69-8234-50b2b8511bc7.png)，然后我们定义线性函数：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In a typical linear regression model, we minimize the squared error of the equation
    that we want to adjust minus the target values (the data that we have), so we
    define the equation to be minimized as a loss.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的线性回归模型中，我们最小化我们希望调整的方程的平方误差，减去目标值（即我们拥有的数据），因此我们将要最小化的方程定义为损失。
- en: 'To find loss''s value, we use `tf.reduce_mean()`. This function finds the mean
    of a multidimensional tensor, and the result can have a different dimension:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到损失值，我们使用 `tf.reduce_mean()`。这个函数计算多维张量的均值，结果可以具有不同的维度：
- en: '[PRE36]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, we define the optimizer method. Here, we will use a simple gradient descent
    with a learning rate of 0.5.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义优化器方法。在这里，我们将使用简单的梯度下降法，学习率为 0.5。
- en: Now, we will define the training method of our graph, but what method will we use
    for minimize the loss? It's `tf.train.GradientDescentOptimizer`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义图表的训练方法，但我们将使用什么方法来最小化损失呢？答案是 `tf.train.GradientDescentOptimizer`。
- en: 'The `.minimize()` function will minimize the error function of our optimizer,
    resulting in a better model:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '`.minimize()` 函数将最小化优化器的误差函数，从而得到一个更好的模型：'
- en: '[PRE37]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Don''t forget to initialize the variables before executing a graph:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了在执行图表之前初始化变量：
- en: '[PRE38]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we are ready to start the optimization and run the graph:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备开始优化并运行图表：
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s visualize the training process to fit the data points:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化训练过程，以适应数据点：
- en: '[PRE41]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/1840b114-b57e-4f87-b90b-6a8082c02b13.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1840b114-b57e-4f87-b90b-6a8082c02b13.png)'
- en: 'Figure 12: Visualization of the data points fitted by a regression line'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：回归线拟合数据点的可视化
- en: Logistic regression model – building and training
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型——构建与训练
- en: 'Also based on our explanation of logistic regression in [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml),
    *Data Modeling in Action - The Titanic Example*, we are going to implement the
    logistic regression algorithm in TensorFlow. So, briefly, logistic regression
    passes the input through the logistic/sigmoid but then treats the result as a
    probability:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 同样根据我们在[第2章](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml)《数据建模实践——泰坦尼克号示例》中的逻辑回归解释，*数据建模实践——泰坦尼克号示例*，我们将实现
    TensorFlow 中的逻辑回归算法。简而言之，逻辑回归将输入通过逻辑/ sigmoid 函数传递，然后将结果视为概率：
- en: '![](img/06e797f4-9584-46f6-8915-d1177426c2bb.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06e797f4-9584-46f6-8915-d1177426c2bb.png)'
- en: 'Figure 13: Discriminating between two linearly separable classes, 0''s and
    1''s'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：区分两个线性可分类别，0 和 1
- en: Utilizing logistic regression in TensorFlow
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用逻辑回归
- en: 'For us to utilize logistic regression in TensorFlow, we first need to import
    whatever libraries we are going to use. To do so, you can run this code cell:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 TensorFlow 中使用逻辑回归，我们首先需要导入我们将要使用的库。为此，你可以运行以下代码单元：
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we will load the dataset we are going to use. In this case, we are utilizing
    the iris dataset, which is inbuilt. So, there''s no need to do any preprocessing
    and we can jump right into manipulating it. We separate the dataset into *x*''s
    and *y*''s, and then into training *x*''s and *y*''s and testing *x*''s and *y*''s,
    (pseudo) randomly:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载我们要使用的数据集。在这种情况下，我们使用内置的鸢尾花数据集。因此，不需要进行任何预处理，我们可以直接开始操作它。我们将数据集分成 *x*
    和 *y*，然后再分成训练集的 *x* 和 *y* 以及测试集的 *x* 和 *y*，（伪）随机地：
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we define *x* and *y*. These placeholders will hold our iris data (both
    the features and label matrices) and help pass them along to different parts of
    the algorithm. You can consider placeholders as empty shells into which we insert
    our data. We also need to give them shapes that correspond to the shape of our
    data. Later, we will insert data into these placeholders by feeding the placeholders
    the data via a `feed_dict` (feed dictionary):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义了 *x* 和 *y*。这些占位符将存储我们的鸢尾花数据（包括特征和标签矩阵），并帮助将它们传递到算法的不同部分。你可以把占位符看作是空的壳子，我们将数据插入到这些壳子里。我们还需要给它们指定与数据形状相对应的形状。稍后，我们将通过
    `feed_dict`（数据字典）将数据插入到这些占位符中：
- en: Why use placeholders?
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么使用占位符？
- en: 'This feature of TensorFlow allows us to create an algorithm that accepts data
    and knows something about the shape of the data without knowing the amount of
    data going in. When we insert *batches* of data in training, we can easily adjust
    how many examples we train on in a single step without changing the entire algorithm:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的这一特性使得我们可以创建一个接受数据并且知道数据形状的算法，而不需要知道进入的数据量。在训练时，当我们插入 *batch* 数据时，我们可以轻松调整每次训练步骤中训练样本的数量，而无需改变整个算法：
- en: '[PRE45]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Set model weights and bias
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置模型的权重和偏置
- en: Much like linear regression, we need a shared variable weight matrix for logistic
    regression. We initialize both *W* and *b* as tensors full of zeros. Since we
    are going to learn *W* and *b*, their initial value doesn't matter too much. These
    variables are the objects that define the structure of our regression model, and
    we can save them after they've been trained so that we can reuse them later.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 和线性回归类似，我们需要一个共享的变量权重矩阵用于逻辑回归。我们将 *W* 和 *b* 都初始化为全零的张量。因为我们将要学习 *W* 和 *b*，所以它们的初始值并不重要。这些变量是定义我们回归模型结构的对象，我们可以在训练后保存它们，以便以后重用。
- en: We define two TensorFlow variables as our parameters. These variables will hold
    the weights and biases of our logistic regression and they will be continually
    updated during training.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个 TensorFlow 变量作为我们的参数。这些变量将存储我们逻辑回归的权重和偏置，并且在训练过程中会不断更新。
- en: 'Notice that *W* has a shape of [4, 3] because we want to multiply the 4-dimensional
    input vectors by it to produce 3-dimensional vectors of evidence for the difference
    classes. *b* has a shape of [3], so we can add it to the output. Moreover, unlike
    our placeholders (which are essentially empty shells waiting to be fed data),
    TensorFlow variables need to be initialized with values, say, with zeros:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*W* 的形状是 [4, 3]，因为我们希望将 4 维的输入向量与其相乘，以产生 3 维的证据向量来区分不同的类别。*b* 的形状是 [3]，因此我们可以将它加到输出中。此外，与我们的占位符（本质上是等待数据的空壳）不同，TensorFlow
    变量需要用值进行初始化，比如使用零初始化：
- en: '[PRE46]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Logistic regression model
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归模型
- en: 'We now define our operations in order to properly run the logistic regression.
    Logistic regression is typically thought of as a single equation:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义我们的操作，以便正确地运行逻辑回归。逻辑回归通常被视为一个单一的方程：
- en: '![](img/c5e9a90e-75b4-4675-878e-4be59ea25db9.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5e9a90e-75b4-4675-878e-4be59ea25db9.png)'
- en: 'However, for the sake of clarity, we can have it broken into its three main
    components:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了清晰起见，我们可以将其拆分为三个主要部分：
- en: A weight times features matrix multiplication operation
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个加权特征矩阵乘法操作
- en: A summation of the weighted features and a bias term
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对加权特征和偏置项的求和
- en: Finally, the application of a sigmoid function
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，应用 Sigmoid 函数
- en: 'As such, you will find these components defined as three separate operations:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您将会发现这些组件被定义为三个独立的操作：
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As we have seen previously, the function we are going to use is the logistic
    function, which is fed the input data after applying weights and bias. In TensorFlow,
    this function is implemented as the `nn.sigmoid` function. Effectively, it fits
    the weighted input with bias into a 0-100 percent curve, which is the probability
    function we want.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，我们将使用的函数是逻辑函数，在应用权重和偏差后将输入数据提供给它。在TensorFlow中，这个函数被实现为`nn.sigmoid`函数。有效地，它将带有偏差的加权输入拟合到0-100百分比曲线中，这是我们想要的概率函数。
- en: Training
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练
- en: The learning algorithm is how we search for the best weight vector (*w*). This
    search is an optimization problem looking for the hypothesis that optimizes an
    error/cost measure.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法是如何搜索最佳权重向量（*w*）的。这个搜索是一个优化问题，寻找能够优化错误/成本度量的假设。
- en: So, the cost or the loss function of the model is going to tell us our model
    is bad, and we need to minimize this function. There are different loss or cost
    criteria that you can follow. In this implementation, we are going to use **mean ****squared
    error** (**MSE**) as a loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型的成本或损失函数将告诉我们我们的模型不好，我们需要最小化这个函数。您可以遵循不同的损失或成本标准。在这个实现中，我们将使用**均方误差**（**MSE**）作为损失函数。
- en: To accomplish the task of minimizing the loss function, we are going to use
    the gradient descent algorithm.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成最小化损失函数的任务，我们将使用梯度下降算法。
- en: Cost function
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'Before defining our cost function, we need to define how long we are going
    to train and how we should define the learning rate:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义我们的成本函数之前，我们需要定义我们将要训练多长时间以及我们应该如何定义学习速率：
- en: '[PRE48]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, it's time to execute our computational graph through the session variable.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候通过会话变量执行我们的计算图了。
- en: 'So first off, we need to initialize our weights and biases with zeros or random
    values using `tf.initialize_all_variables()`. This initialization step will become
    a node in our computational graph, and when we put the graph into a session, the
    operation will run and create the variables:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用`tf.initialize_all_variables()`将我们的权重和偏差初始化为零或随机值。这个初始化步骤将成为我们计算图中的一个节点，当我们将图放入会话中时，操作将运行并创建变量：
- en: '[PRE49]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, it''s time to see how our trained model performs on the `iris` dataset,
    so let''s test our trained model against the test set:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候看看我们训练好的模型在`iris`数据集上的表现了，让我们将训练好的模型与测试集进行测试：
- en: '[PRE51]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Getting 0.9 accuracy on the test set is really good and you can try to get better
    results by changing the number of epochs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上获得0.9的准确率真的很好，您可以通过更改epochs的数量尝试获得更好的结果。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went through a basic explanation of neural networks and
    and the need for multi-layer neural networks. We also covered the TensorFlow computational
    graph model with some basic examples, such as linear regression and logistic regression.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对神经网络进行了基本解释，并讨论了多层神经网络的需求。我们还涵盖了TensorFlow的计算图模型，并举了一些基本的例子，如线性回归和逻辑回归。
- en: Next up, we will go through more advanced examples and demonstrate how TensorFlow
    can be used to build something like handwritten character recognition. We will
    also tackle the core idea of architecture engineering that has replaced feature
    engineering in traditional machine learning.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过更高级的例子，展示如何使用TensorFlow构建像手写字符识别之类的东西。我们还将解决传统机器学习中已经替代特征工程的核心架构工程思想。
