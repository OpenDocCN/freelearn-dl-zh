- en: Face Generation and Handling Missing Labels
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人脸生成与处理缺失标签
- en: The list of interesting applications that we can use GANs for is endless. In
    this chapter, we are going to demonstrate another promising application of GANs,
    which is face generation based on the CelebA database. We'll also demonstrate
    how to use GANs for semi-supervised learning setups where we've got a poorly labeled
    dataset with some missing labels.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用GAN的有趣应用无穷无尽。在本章中，我们将演示GAN的另一个有前途的应用——基于CelebA数据库的人脸生成。我们还将演示如何在半监督学习设置中使用GAN，其中我们有一个标签不全的数据集。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Face generation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人脸生成
- en: Semi-supervised learning with generative adversarial networks (GANs)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成对抗网络（GAN）的半监督学习
- en: Face generation
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人脸生成
- en: 'As we mentioned in the previous chapter, the Generator and Discriminator consist
    of a **Deconvolutional Network** (**DNN**: [https://www.quora.com/How-does-a-deconvolutional-neural-network-work](https://www.quora.com/How-does-a-deconvolutional-neural-network-work))
    and **Convolutional Neural Network** (**CNN**: [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一章所提到的，生成器和判别器由**反卷积网络**（**DNN**：[https://www.quora.com/How-does-a-deconvolutional-neural-network-work](https://www.quora.com/How-does-a-deconvolutional-neural-network-work)）和**卷积神经网络**（**CNN**：[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)）组成：
- en: CNN is a a type of neural network that encodes hundreds of pixels of an image
    into a vector of small dimensions (z), which is a summary of the image
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN是一种神经网络，它将图像的数百个像素编码成一个小维度的向量（z），该向量是图像的摘要。
- en: DNN is a network that learns some filters to recover the original image from
    z
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNN是一种网络，它学习一些滤波器，以从z中恢复原始图像。
- en: Also, the discriminator will output one or zero to indicate whether the input
    image is from the actual dataset or generated by the generator. On the other side,
    the generator will try to replicate images similar to the original dataset based
    on the latent space z, which might follow a Gaussian distribution. So, the goal
    of the discriminator is to correctly discriminate between the real images, and
    the goal of the generator is to learn the distribution of the original dataset
    and hence fool the discriminator so that it makes a wrong decision.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，判别器会输出1或0，表示输入的图像是来自真实数据集，还是由生成器生成。另一方面，生成器会尝试根据潜在空间z复制与原始数据集相似的图像，这些图像可能遵循高斯分布。因此，判别器的目标是正确区分真实图像，而生成器的目标是学习原始数据集的分布，从而欺骗判别器，使其做出错误的决策。
- en: In this section, we'll try to teach the generator to learn human face image
    distribution so that it can generate realistic faces.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将尝试教导生成器学习人脸图像的分布，以便它能够生成逼真的人脸。
- en: Generating human-like faces is crucial for most graphics companies, who are
    always looking for new faces for their applications, and it gives us a clue of
    how artificial intelligence is really close to achieving realism in generating
    artificial faces.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 生成类人面孔对于大多数图形公司来说至关重要，这些公司总是在为其应用程序寻找新的面孔，这也让我们看到人工智能在生成逼真人脸方面接近现实的程度。
- en: 'In this example, we''ll be using the CelebA dataset. The CelebFaces Attributes
    Dataset (CelebA)  is a large-scale face attributes dataset with about 200K celebrity
    images, each with 40 attribute annotations. There are lots of pose variations
    covered by the dataset, as well as background clutter, so CelebA is very diverse
    and well annotated. it includes:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将使用CelebA数据集。CelebFaces属性数据集（CelebA）是一个大规模的面部属性数据集，包含约20万张名人图像，每张图像有40个属性标注。数据集涵盖了大量的姿势变化，以及背景杂乱，因此CelebA非常多样且注释完备。它包括：
- en: 10,177 identities
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10,177个身份
- en: 202,599 face images
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 202,599张人脸图像
- en: Five landmark locations and 40 binary attribute annotations per image
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每张图像有五个地标位置和40个二元属性注释
- en: We can use this dataset for many computer vision applications other than face
    generation, such as face recognition and localization, or face attribute detection.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此数据集用于许多计算机视觉应用，除了人脸生成，还可以用于人脸识别、定位或人脸属性检测。
- en: 'This figure shows how the generator error, or learning human face distribution,
    gets close to realism during the training process:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了生成器误差，或者说学习人脸分布，在训练过程中如何逐渐接近现实：
- en: '![](img/26446f87-864b-4267-95b8-a4980fd70a11.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/26446f87-864b-4267-95b8-a4980fd70a11.png)'
- en: 'Figure 1: GANs for generating new faces from a celebrity images dataset'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用GAN从名人图像数据集中生成新面孔
- en: Getting the data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'In this section, we will define some helper functions that will help us to
    download the CelebA dataset. We''ll start off by importing their required packages
    for this implementation:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将定义一些辅助函数，帮助我们下载CelebA数据集。我们将通过导入实现所需的包开始：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next up, we are going to use the utils script to download the dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用utils脚本下载数据集：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Exploring the Data
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: The CelebA dataset contains over 200k annotated celebrity images. Since we are
    going to use GANs to generate similar images, it is worth looking at a bunch of
    images from the dataset and see how they look. In this section, we are going to
    define some helper functions for visualizing a bunch of images from the CelebA
    dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CelebA数据集包含超过20万张带注释的名人图像。由于我们将使用GAN生成类似的图像，因此值得看一些来自数据集的图像，看看它们的效果。在这一部分，我们将定义一些辅助函数，用于可视化CelebA数据集中的一组图像。
- en: 'Now, let''s use the `utils` script to display some images from the dataset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`utils`脚本从数据集中显示一些图像：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](img/1972d296-3172-438f-8dd0-4b700ba8c968.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1972d296-3172-438f-8dd0-4b700ba8c968.png)'
- en: 'Figure 2: Plotting a sample of images from CelebA dataset'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：从CelebA数据集中绘制一组图像
- en: The main focus of this computer vision task is to use GANs for generating images
    similar two the ones in the celebrity dataset, so we'll need to focus on the face
    part of the images. To focus on the face part of an image, we are going to remove
    the parts of the image that don't include a celebrity face.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算机视觉任务的主要目标是使用GAN生成类似于名人数据集中图像的图像，因此我们需要专注于图像的面部部分。为了聚焦于图像的面部部分，我们将去除不包含名人面孔的部分。
- en: Building the model
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Now, let''s start off by building the core of our implementation, which is
    the computational graph; it will mainly include the following components:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始构建我们实现的核心——计算图；它主要包括以下组件：
- en: Model inputs
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输入
- en: Discriminator
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器
- en: Generator
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器
- en: Model losses
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型损失
- en: Model optimizer
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型优化器
- en: Training the model
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Model inputs
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型输入
- en: In this section, we are going to implement a helper function that we'll define
    the model input placeholders which will responsible for feeding the data the the
    computational graph.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现一个辅助函数，定义模型的输入占位符，这些占位符将负责将数据输入到计算图中。
- en: 'The functions should be able to create three main placeholders:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数应该能够创建三个主要的占位符：
- en: Actual input images from the dataset which will have the dimensions of (batch
    size, input image width, input image height, number of channels)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自数据集的实际输入图像，尺寸为（批量大小，输入图像宽度，输入图像高度，通道数）
- en: The latent space Z, which will be used by the generator for generating fake
    images
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在空间Z，将被生成器用来生成假图像
- en: Learning rate placeholder
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率占位符
- en: 'The helper function will return a tuple of these three input placeholders.
    So, let''s go ahead and define this function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助函数将返回这三个输入占位符的元组。现在，让我们定义这个函数：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Discriminator
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器
- en: Next up, we need to implement the discriminator part of the network, which will
    be used to judge whether the incoming input is coming from the real dataset or
    generated by the generator. Again, we'll use the TensorFlow feature of `tf.variable_scope`
    to prefix some variables with discriminator so that we can retrieve and reuse
    them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要实现网络的判别器部分，用于判断输入是来自真实数据集还是由生成器生成的。我们将再次使用TensorFlow的`tf.variable_scope`功能为一些变量添加前缀“判别器”，以便我们能够检索和重用它们。
- en: 'So, let''s define the function which will return the binary output of the discriminator
    as well as the logit values:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们定义一个函数，返回判别器的二进制输出以及logit值：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Generator
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: Now, it's time to implement the second part of the network that will be trying
    to replicate the original input images using the latent space `z`. We'll be using
    `tf.variable_scope` for this function as well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，轮到实现网络的第二部分，它将尝试使用潜在空间`z`来复制原始输入图像。我们也将使用`tf.variable_scope`来实现这个功能。
- en: 'So, let''s define the function which will return a generated image by the generator:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们定义一个函数，返回生成器生成的图像：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Model losses
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型损失
- en: Now comes the tricky part, which we covered in the previous chapter, which is
    to calculate the losses of the discriminator and the generator.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是比较棘手的部分，我们在前一章中讲过，即计算判别器和生成器的损失。
- en: 'So, let''s define this function, which will make use of the `generator` and
    `discriminator` functions that were defined previously:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们定义这个函数，它将使用之前定义的`generator`和`discriminator`函数：
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Model optimizer
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化器
- en: 'Finally, before training our model, we need to implement the optimization criteria
    for this task. We will use the naming conventions that we used previously to retrieve
    the trainable parameters for the discriminator and the generator and train them:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在训练我们的模型之前，我们需要实现该任务的优化标准。我们将使用之前使用的命名约定来检索判别器和生成器的可训练参数并训练它们：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Training the model
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Now, it's time to train the model and see how the generator will be able to
    fool, to some extent, the discriminator, by generating images very close to the
    original CelebA dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候训练模型并观察生成器如何在一定程度上欺骗判别器，通过生成与原始CelebA数据集非常接近的图像。
- en: 'But first, let''s define a helper function that will display some generated
    images by the generator:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，让我们定义一个辅助函数，它将展示生成器生成的一些图像：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we will use the helper functions that we have defined before to build
    the model inputs, loss, and optimization criteria. We stack them together and
    start training our model based on the CelebA dataset:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用之前定义的辅助函数来构建模型输入、损失和优化标准。我们将它们堆叠在一起，并开始基于CelebA数据集训练我们的模型：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Kick off the training process, which might take some time depending on your
    host machine specs:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 启动训练过程，这可能会根据你的主机机器规格需要一些时间：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/2d5b7d29-193a-4b1d-898d-8f1a4345ca3e.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2d5b7d29-193a-4b1d-898d-8f1a4345ca3e.png)'
- en: 'Figure 3: Sample generated output at this point of training'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：此时训练的生成输出样本
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/5b21242d-61d1-45ba-90be-a766f2d984fd.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b21242d-61d1-45ba-90be-a766f2d984fd.png)'
- en: 'Figure 4: Sample generated output at this point of training'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：此时训练的生成输出样本
- en: 'After some time of training, you should get something like this:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间的训练后，你应该会得到类似这样的结果：
- en: '![](img/e78748d2-24a1-4507-9a53-c202659576b0.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e78748d2-24a1-4507-9a53-c202659576b0.png)'
- en: 'Figure 5: Sample generated output at this point of training'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：此时训练的生成输出样本
- en: Semi-supervised learning with Generative Adversarial Networks (GANs)
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成对抗网络（GAN）进行半监督学习
- en: With that in mind, semi-supervised learning is a technique in which both labeled
    and unlabeled data is used to train a classifier.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，半监督学习是一种技术，其中使用标注数据和未标注数据来训练分类器。
- en: This type of classifier takes a tiny portion of labeled data and a much larger
    amount of unlabeled data (from the same domain). The goal is to combine these
    sources of data to train a **Deep Convolution Neural Network** (**DCNN**) to learn
    an inferred function capable of mapping a new datapoint to its desirable outcome.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的分类器采用一小部分标注数据和大量未标注数据（来自同一领域）。目标是将这些数据源结合起来，训练一个**深度卷积神经网络**（**DCNN**），以学习一个推断函数，能够将新的数据点映射到其期望的结果。
- en: In this frontier, we present a GAN model to classify street view house numbers
    using a very small labeled training set. In fact, the model uses roughly 1.3%
    of the original SVHN training labels i.e. 1000 (one thousand) labeled examples.
    We use some of the techniques described in the paper *Improved Techniques for
    Training GANs from OpenAI* ([https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域，我们提出了一种GAN模型，用于使用非常小的标注训练集对街景房号进行分类。实际上，该模型使用了原始SVHN训练标签的大约1.3%，即1000个标注样本。我们使用了在论文*Improved
    Techniques for Training GANs from OpenAI*中描述的一些技术（[https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)）。
- en: Intuition
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直觉
- en: When building a GAN for generating images, we trained both   the generator and
    the discriminator at the same time. After training, we can discard the discriminator
    because we only used it for training the generator.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建生成图像的GAN时，我们同时训练生成器和判别器。训练后，我们可以丢弃判别器，因为我们只在训练生成器时使用了它。
- en: '![](img/7b223ba3-8c25-4477-9a17-f04fa287296b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b223ba3-8c25-4477-9a17-f04fa287296b.png)'
- en: 'Figure 6: Semi-supervised learning GAN architecture for an 11 class classification
    problem'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：用于11类分类问题的半监督学习GAN架构
- en: In semi-supervised learning, we need to transform the discriminator into a multi-class
    classifier. This new model has to be able to generalize well on the test set,
    even though we do not have many labeled examples for training. Additionally, this
    time, by the end of training, we can actually throw away the generator. Note that
    the roles changed. Now, the generator is only used for helping the discriminator
    during training. Putting it differently, the generator acts as a different source
    of information from which the discriminator gets raw, unlabeled training data.
    As we will see, this unlabelled data is key to improving the discriminator's performance.
    Also, for a regular image generation GAN, the discriminator has only one role.
    Compute the probability of whether its inputs are real or not — let's call it
    the GAN problem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习中，我们需要将判别器转变为一个多类分类器。这个新模型必须能够在测试集上很好地泛化，尽管我们没有很多带标签的训练样本。此外，这一次，训练结束时，我们实际上可以抛弃生成器。请注意，角色发生了变化。现在，生成器仅用于在训练过程中帮助判别器。换句话说，生成器充当了一个不同的信息来源，判别器从中获取未经标签的原始训练数据。正如我们将看到的，这些未经标签的数据对于提高判别器的性能至关重要。此外，对于一个常规的图像生成
    GAN，判别器只有一个角色：计算其输入是否真实的概率——我们将其称为 GAN 问题。
- en: However, to turn the discriminator into a semi-supervised classifier, besides
    the GAN problem, the discriminator also has to learn the probabilities of each
    of the original dataset classes. In other words, for each input image, the discriminator
    has to learn the probabilities of it being a one, two, three, and so on.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要将判别器转变为一个半监督分类器，除了 GAN 问题，判别器还必须学习原始数据集每个类别的概率。换句话说，对于每个输入图像，判别器必须学习它属于某个类别（如
    1、2、3 等）的概率。
- en: Recall that for an image generation GAN discriminator, we have a single sigmoid
    unit output. This value represents the probability of an input image being real
    (value close to 1), or fake (value near 0). In other words, from the discriminator's
    point of view, values close to 1 mean that the samples are likely to come from
    the training set. Likewise, value near 0 mean a higher chance that the samples
    come from the generator network. By using this probability, the discriminator
    is able to send a signal back to the generator. This signal allows the generator
    to adapt its parameters during training, making it possible to improve its capabilities
    of creating realistic images.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，对于图像生成 GAN 的判别器，我们有一个单一的 sigmoid 单元输出。这个值代表了输入图像是真实的概率（接近 1），还是假的概率（接近
    0）。换句话说，从判别器的角度来看，接近 1 的值意味着样本很可能来自训练集。同样，接近 0 的值则意味着样本更有可能来自生成器网络。通过使用这个概率，判别器能够将信号传回生成器。这个信号使生成器能够在训练过程中调整其参数，从而有可能提高生成真实图像的能力。
- en: We have to convert the discriminator (from the previous GAN) into an 11 class
    classifier. To do that, we can turn its sigmoid output into a softmax with 11
    class outputs, the first 10 for the individual class probabilities of the SVHN
    dataset (zero to nine), and the 11th class for all the fake images that come from
    the generator.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须将判别器（来自之前的 GAN）转换为一个 11 类分类器。为此，我们可以将其 sigmoid 输出转换为具有 11 个类别输出的 softmax，前
    10 个输出表示 SVHN 数据集各个类别的概率（0 至 9），第 11 类则表示所有来自生成器的假图像。
- en: Note that if we set the 11th class probability to 0, then the sum of the first
    10 probabilities represents the same probability computed using the sigmoid function.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们将第 11 类的概率设为 0，那么前 10 个概率的总和就等于使用 sigmoid 函数计算的相同概率。
- en: 'Finally, we need to set up the losses in such a way that the discriminator
    can do both:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要设置损失函数，使得判别器能够同时完成两项任务：
- en: Help the generator learn to produce realistic images. To do that, we have to
    instruct the discriminator to distinguish between real and fake samples.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帮助生成器学习生成真实图像。为了做到这一点，我们必须指示判别器区分真实样本和假样本。
- en: Use the generator’s images, along with the labeled and unlabeled training data,
    to help classify the dataset.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成器的图像以及带标签和不带标签的训练数据，帮助分类数据集。
- en: 'To summarize, the discriminator has three different sources of training data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，判别器有三种不同的训练数据来源：
- en: Real images with labels. These are image label pairs like in any regular supervised
    classification problem.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带标签的真实图像。这些是像任何常规监督分类问题一样的图像标签对。
- en: Real images without labels. For those, the classifier only learns that these
    images are real.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有标签的真实图像。对于这些图像，分类器只能学习到这些图像是“真实的”。
- en: Images from the generator. To use these ones, the discriminator learns to classify
    as fake.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自生成器的图像。为了使用这些图像，判别器学习将其分类为假。
- en: The combination of these different sources of data will make the classifier
    able to learn from a broader perspective. That, in turn, allows the model to perform
    inference much more precisely than it would be if only using the 1,000 labeled
    examples for training.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同数据源的结合将使分类器能够从更广泛的角度学习。反过来，这使得模型的推理性能比仅使用1,000个标注样本进行训练时更为精准。
- en: Data analysis and preprocessing
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分析和预处理
- en: 'For this task, we will be using SVHN dataset, which is an abbreviation for
    Street View House Numbers by Stanford ([http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)).
    So, let''s start the implementation by importing the required packages for this
    implementation:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用SVHN数据集，它是斯坦福大学的街景房屋号码（Street View House Numbers）数据集的缩写（[http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)）。所以，让我们通过导入实现所需的包开始实现：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next up, we are going to define a helper class to download the SVHN dataset
    (remember that you need to manually create the `input_data_dir` first):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个辅助类来下载SVHN数据集（记得你需要首先手动创建`input_data_dir`目录）：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s get a sense of what these images look like:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下这些图像的样子：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/a75e45c4-fe45-4bf0-b1a6-52d1ddbdeef6.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a75e45c4-fe45-4bf0-b1a6-52d1ddbdeef6.png)'
- en: 'Figure 7: Sample images from the SVHN dataset.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：来自SVHN数据集的样本图像。
- en: 'Next up, we need to scale our images to be between -1 and 1, and this will
    be necessary since we are going to use the `tanh()` function, which will squash
    the output values of the generator:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将图像缩放到-1到1之间，这对于使用`tanh()`函数是必要的，因为该函数将压缩生成器输出的值：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Building the model
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: In this section, we will build all the bits and pieces that are necessary for
    our test, so let's start off by defining the inputs that will be used to feed
    data to the computational graph.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建所有必要的组件以进行测试，因此我们首先定义将用于向计算图输入数据的输入。
- en: Model inputs
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型输入
- en: 'First off, we are going to define the model inputs function, which will create
    the model input placeholders to be used for feeding data to the computational
    model:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义模型输入函数，该函数将创建用于输入数据的模型占位符：
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Generator
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: 'In this section, we are going to implement the first core part of the GAN network.
    The architecture and implementation of this part will follow the original DCGAN
    paper:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现GAN网络的第一个核心部分。该部分的架构和实现将遵循原始的DCGAN论文：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Discriminator
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器
- en: Now, it's time to build the second core piece of the GAN network, which is the
    discriminator. In previous implementations, we said that the discriminator will
    produce a binary output that represents whether the input image is from the real
    dataset (1) or it's generated by the generator (0). The scenario is different
    here, so the discriminator will now be a multi-class classifier.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候构建GAN网络的第二个核心部分——判别器了。在之前的实现中，我们提到判别器会产生一个二元输出，表示输入图像是否来自真实数据集（1）还是由生成器生成（0）。在这里，情况有所不同，因此判别器将变为一个多类别分类器。
- en: 'Now, let''s go ahead and build up the discriminator part of the architecture:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续构建架构中的判别器部分：
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Instead of applying a fully connected layer at the end, we are going to perform
    so-called **global average pooling** (**GAP**), which takes the average over the
    spatial dimensions of a feature vector; this will produce a squashed tensor to
    only a single value:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再在最后应用全连接层，而是执行所谓的**全局平均池化**（**GAP**），该操作在特征向量的空间维度上取平均值；这将把张量压缩为一个单一的值：
- en: '[PRE27]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For example, suppose that after a stack of convolutions, we get an output tensor
    of shape:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设经过一系列卷积操作后，我们得到一个形状为的输出张量：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To apply global average pooling, we calculate the average value on the [8x8]
    tensor slice. This operation will result in a tensor which is the following shape:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用全局平均池化，我们计算[8x8]张量片的平均值。该操作将产生一个形状如下的张量：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'That can be reshaped to:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以重塑为：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After applying the global average pooling, we add a fully connected layer that
    will output the final logits. These have the shape of:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用全局平均池化后，我们添加一个全连接层，该层将输出最终的logits。这些logits的形状为：
- en: '`[BATCH_SIZE, NUM_CLASSES]`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`[BATCH_SIZE, NUM_CLASSES]`'
- en: 'which will represent the scores for each class. To get these scores for probability,
    we are going to use the `softmax` activation function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这将表示每个类别的得分。为了获得这些类别的概率得分，我们将使用`softmax`激活函数：
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And finally the discriminator function will look like this,
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，判别器函数将如下所示：
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Model losses
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型损失
- en: 'Now it''s time to define the model losses. First off, the discriminator loss
    will be divided into two parts:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候定义模型的损失函数了。首先，判别器的损失将分为两部分：
- en: One which will represent the GAN problem, which is the unsupervised loss
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将表示GAN问题的部分，即无监督损失
- en: The second one will compute the individual actual class probabilities, which
    is the supervised loss
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二部分将计算每个实际类别的概率，这就是监督损失
- en: For the discriminator's unsupervised loss, it has to discriminate between actual
    training images and the generated images by the generator.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器的无监督损失，它必须区分真实训练图像和生成器生成的图像。
- en: As for a regular GAN, half of the time, the discriminator will get unlabeled
    images from the training set as an input and the other half, fake, unlabeled images
    from the generator.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 和常规GAN一样，一半时间，判别器将从训练集获取未标记的图像作为输入，另一半时间，从生成器获取虚假未标记的图像。
- en: For the second part of the discriminator loss, which is the supervised loss,
    we need to build upon the logits from the discriminator. So, we will use the softmax
    cross entropy since it's a multi classification problem.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器损失的第二部分，即监督损失，我们需要基于判别器的logits来构建。因此，我们将使用softmax交叉熵，因为这是一个多分类问题。
- en: 'As mentioned in the *Enhanced Techniques for Training GANs* paper, we should
    use feature matching for the generator loss. As the authors describe:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 正如《*增强训练GAN的技术*》论文中提到的，我们应该使用特征匹配来计算生成器的损失。正如作者所描述的：
- en: '"Feature matching is the concept of penalizing the mean absolute error between
    the average value of some set of features on the training data and the average
    values of that set of features on the generated samples. To do that, we take some
    set of statistics (the moments) from two different sources and force them to be
    similar. First, we take the average of the features extracted from the discriminator
    when a real training minibatch is being processed. Second, we compute the moments
    in the same way, but now for when a minibatch composed of fake images that come
    from the generator was being analyzed by the discriminator. Finally, with these
    two sets of moments, the generator loss is the mean absolute difference between
    them. In other words, as the paper emphasizes: We train the generator to match
    the expected values of the features on an intermediate layer of the discriminator."'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: “特征匹配是通过惩罚训练数据集上一组特征的平均值与生成样本上该组特征的平均值之间的绝对误差来实现的。为此，我们从两个不同的来源提取一组统计数据（矩），并迫使它们相似。首先，我们取出从判别器中提取的特征的平均值，这些特征是在处理真实训练小批量数据时得到的。其次，我们以相同的方式计算矩，但这次是针对当判别器分析来自生成器的虚假图像小批量时的情况。最后，利用这两个矩的集合，生成器的损失是它们之间的平均绝对差。换句话说，正如论文强调的那样：我们训练生成器使其匹配判别器中间层特征的期望值。”
- en: 'And finally, the model loss function will look like this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型的损失函数将如下所示：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Model optimizer
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化器
- en: 'Now, let''s define the model optimizer, which is pretty much similar to the
    ones that we defined before:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义模型优化器，它与我们之前定义的非常相似：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Model training
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Finally, let''s go ahead and kick off the training process after putting it
    all together:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在将所有内容组合在一起后，让我们开始训练过程：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Don''t forget to create a directory called checkpoints:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了创建一个名为checkpoints的目录：
- en: '[PRE40]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, at `Epoch 24`, you should get something close to this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在`Epoch 24`时，你应该得到如下结果：
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/16442734-420c-4f3e-ad7e-f233a4280e9a.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16442734-420c-4f3e-ad7e-f233a4280e9a.png)'
- en: 'Figure 8: Sample images created by the generator network using the feature
    matching loss'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：使用特征匹配损失由生成器网络创建的示例图像
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![](img/4e8ab204-3482-4502-ad4e-2610c537b0e0.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8ab204-3482-4502-ad4e-2610c537b0e0.png)'
- en: 'Figure 9: Train versus Test accuracy over the training process'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：训练过程中训练与测试的准确率
- en: Although feature matching loss performs well on the task of semi-supervised
    learning, the images produced by the generator are not as good as the ones created
    in the previous chapter. But this implementation was mainly introduced to demonstrate
    how we can use GANs for semi-supervised learning setups.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管特征匹配损失在半监督学习任务中表现良好，但生成器生成的图像不如上一章中创建的图像那么好。不过，这个实现主要是为了展示我们如何将GAN应用于半监督学习设置。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: Finally, many researchers consider unsupervised learning as the missing link
    in general AI systems. To overcome these obstacles, attempts to solve established
    problems using less labeled data is key. In this scenario, GANs pose a real alternative
    for learning complicated tasks with less labeled samples. Yet, the performance
    gap between supervised and semi-supervised learning is still far from being equal.
    We can certainly expect this gap to become shorter as new approaches come into
    play.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，许多研究人员认为无监督学习是通用人工智能系统中的缺失环节。为了克服这些障碍，尝试通过使用更少标注数据来解决已知问题是关键。在这种情况下，GAN为使用较少标注样本学习复杂任务提供了真正的替代方案。然而，监督学习和半监督学习之间的性能差距仍然相当大。我们可以肯定地预期，随着新方法的出现，这一差距将逐渐缩小。
