- en: '*Chapter 4*: Image Classification and Regression Using AutoKeras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：使用 AutoKeras 进行图像分类与回归'
- en: In this chapter, we will focus on the use of AutoKeras applied to images. In
    [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, we got our first contact with **deep learning** (**DL**) applied
    to images, by creating two models (a classifier and a regressor) that recognized
    handwritten digits. We will now create more complex and powerful image recognizers,
    examine how they work, and see how to fine-tune them to improve their performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将重点介绍 AutoKeras 在图像上的应用。在[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中，*入门
    AutoKeras*，我们首次接触了应用于图像的**深度学习**（**DL**），通过创建两个模型（一个分类器和一个回归器），实现了对手写数字的识别。我们现在将创建更复杂、更强大的图像识别器，分析它们的工作原理，并学习如何微调它们以提高性能。
- en: After reading this chapter, you will be able to create your own image models
    and apply them, to solve a wide range of problems in the real world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，你将能够创建自己的图像模型，并将其应用于解决现实世界中的各种问题。
- en: As we discussed in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the most suitable models for recognizing images
    use a type of neural network called a **convolutional neural network** (**CNN**).
    For the two examples that we will see in this chapter, AutoKeras will also choose
    CNNs for the creation of its models. So, let's see in a little more detail what
    these types of neural networks are and how they work.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中讨论的那样，*入门 AutoKeras*，最适合图像识别的模型使用一种叫做**卷积神经网络**（**CNN**）的神经网络类型。对于我们在本章中看到的两个例子，AutoKeras
    也会选择 CNN 来创建其模型。所以，让我们更详细地了解这些类型的神经网络是什么，以及它们是如何工作的。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主要主题：
- en: Understanding CNNs—what are these neural networks and how do they work?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 CNN——这些神经网络是什么，它们是如何工作的？
- en: Creating a CIFAR-10 image classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 CIFAR-10 图像分类器
- en: Creating and fine-tuning a powerful image classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和微调一个强大的图像分类器
- en: Creating an image regressor to find out the age of people
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个图像回归器以找出人的年龄
- en: Creating and fine-tuning a powerful image regressor
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建和微调一个强大的图像回归器
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All coding examples in this book are available as Jupyter Notebooks that can
    be downloaded from the following link: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有编码示例都可以作为 Jupyter Notebooks 下载，链接如下：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras)。
- en: As code cells can be executed, each Notebook can be self-installable by adding
    a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook there is a code cell for environmental setup, which installs
    AutoKeras and its dependencies.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码单元可以执行，每个笔记本都可以通过添加所需的代码片段进行自我安装。因此，在每个笔记本的开头都有一个环境设置的代码单元，用于安装 AutoKeras
    及其依赖项。
- en: 'So, to run the coding examples, you only need a computer with Ubuntu/Linux
    as the operating system and you can install the Jupyter Notebook with this command
    line:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要运行编码示例，你只需要一台操作系统为 Ubuntu/Linux 的计算机，并且可以通过以下命令行安装 Jupyter Notebook：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can also run these notebooks using Google Colaboratory, in
    which case you will only need a web browser—see the *AutoKeras with Google Colaboratory*
    section in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting
    Started with AutoKeras*, for more details. Furthermore, in the *Installing AutoKeras*
    section, you will also find other installation options. Let's get started by understanding
    CNNs in detail.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以通过 Google Colaboratory 运行这些笔记本，在这种情况下，你只需要一个网页浏览器——有关更多详情，请参见[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)中关于*AutoKeras
    与 Google Colaboratory*的部分，*入门 AutoKeras*。此外，在*安装 AutoKeras*部分，你还可以找到其他安装选项。让我们通过详细了解
    CNN 来开始。
- en: Understanding CNNs
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 CNN
- en: A CNN is a type of neural network, inspired by the functioning of neurons in
    the visual cortex of a biological brain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种神经网络，灵感来源于生物大脑视觉皮层中神经元的工作原理。
- en: These types of networks perform very well in solving computer vision problems
    such as image classification, object detection, segmentation, and so on.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这类网络在解决计算机视觉问题（如图像分类、物体检测、分割等）方面表现非常出色。
- en: 'The following screenshot shows how a CNN recognizes a cat:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了 CNN 如何识别猫：
- en: '![Figure 4.1 – How a CNN recognizes a cat'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1 – CNN 如何识别猫](img/B16953_04_01.jpg)'
- en: '](img/B16953_04_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_01.jpg)'
- en: Figure 4.1 – How a CNN recognizes a cat
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – CNN 如何识别猫
- en: But why do these CNNs work so well, compared to a classical fully connected
    model? To answer this, let's dive into what the convolutional and pooling layers
    do.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么这些 CNN 比传统的全连接模型更有效呢？为了解答这个问题，让我们深入探讨卷积层和池化层的作用。
- en: Convolutional layer
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积层
- en: The key building block in a CNN is the convolutional layer, which uses a window
    (kernel) to scan an image and perform transformations on it to detect patterns.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 的关键构建块是卷积层，它使用一个窗口（卷积核）扫描图像，并对其进行变换以检测模式。
- en: A kernel is nothing more than a simple neural network fed by the pixel matrix
    of the scanned window that outputs a vector of numbers, which we will use as filters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 核心不过是一个简单的神经网络，由扫描窗口的像素矩阵提供输入，输出一个数字向量，我们将使用这些向量作为滤波器。
- en: Let's imagine a convolutional layer with many small square templates (called
    kernels) that go through an image and look for patterns. When the square of the
    input image matches the kernel pattern, the kernel returns a positive value; otherwise,
    it returns `0` or less.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个卷积层，其中有许多小的方形模板（称为卷积核），它们穿过图像并寻找模式。当输入图像的某个区域与卷积核模式匹配时，卷积核返回一个正值；否则，它返回
    `0` 或更小的值。
- en: 'The following screenshot shows how a convolutional layer processes an image:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了卷积层如何处理图像：
- en: '![Figure 4.2 – How a convolutional layer processes an image'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2 – 卷积层如何处理图像](img/B16953_04_02.jpg)'
- en: '](img/B16953_04_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_01.jpg)'
- en: Figure 4.2 – How a convolutional layer processes an image
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 卷积层如何处理图像
- en: Once we have the filters, we have to reduce their dimensions using a pooling
    operation, which is explained next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了滤波器，我们就必须使用池化操作来减少它们的维度，下一步将会解释这一过程。
- en: Pooling layer
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化层
- en: The function of the pooling layer is to progressively reduce the size of the
    input features matrix to reduce the number of parameters and calculations in the
    network. The most common form of pooling is max pooling, which performs downscaling
    by applying a maximum filter to non-overlapping subregions of the input features
    matrix.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的功能是逐步减小输入特征矩阵的大小，从而减少网络中的参数数量和计算量。最常见的池化方式是最大池化，它通过对输入特征矩阵的非重叠子区域应用最大值滤波器来执行下采样。
- en: 'The following screenshot provides an example of max pooling:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图提供了最大池化的示例：
- en: '![Figure 4.3 – Max pooling example](img/B16953_04_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 最大池化示例](img/B16953_04_03.jpg)'
- en: Figure 4.3 – Max pooling example
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 最大池化示例
- en: In the preceding screenshot, we can see an example of a max pooling operation
    on a features matrix. In the case of an image, this matrix would be made up of
    the pixel values of the image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们可以看到一个特征矩阵上进行最大池化操作的例子。对于图像来说，这个矩阵将由图像的像素值组成。
- en: Applying this operation reduces the computational cost by reducing the number
    of features to process, and it also helps prevent overfitting. Next, we will see
    how the convolutional and pooling layers are combined in a CNN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这一操作可以通过减少需要处理的特征数量，从而降低计算成本，同时有助于防止过拟合。接下来，我们将看到卷积层和池化层在 CNN 中是如何结合的。
- en: CNN structure
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNN 结构
- en: 'Usually, a CNN is made up of a series of convolutional layers, followed by
    a pooling layer (downscaling). This combination is repeated several times, as
    we can see in the following screenshot example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CNN 由一系列卷积层组成，之后是池化层（下采样）。这一组合会重复多次，正如我们在下面的屏幕截图示例中看到的那样：
- en: '![Figure 4.4 – Example of a CNN pipeline](img/B16953_04_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – CNN 管道示例](img/B16953_04_04.jpg)'
- en: Figure 4.4 – Example of a CNN pipeline
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – CNN 管道示例
- en: In this process, the first layer detects simple features such as the outlines
    of an image, and the second layer begins to detect higher-level features. In the
    intermediate layers, it is already capable of detecting more complex shapes, such
    as the nose or eyes. In the final layers, it is usually able to differentiate
    human faces.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，第一层检测简单的特征，比如图像的轮廓，第二层开始检测更高层次的特征。在中间层，它已经能够检测到更复杂的形状，比如鼻子或眼睛。在最后几层，它通常能够区分人脸。
- en: This seemingly simple repetition process is extremely powerful, detecting features
    of a slightly higher order than its predecessor at each step and generating astonishing
    predictions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个看似简单的重复过程非常强大，每一步都能检测出比前一步稍高阶的特征，并生成惊人的预测结果。
- en: Surpassing classical neural networks
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越经典神经网络
- en: A classical neural network uses fully connected (dense) layers as the main feature
    transformation operations, whereas a CNN uses convolution and pooling layers (Conv2D).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 经典神经网络使用全连接（密集）层作为主要的特征转换操作，而CNN则使用卷积层和池化层（Conv2D）。
- en: 'The main differences between a fully connected layer and a convolutional layer
    are outlined here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 完全连接层和卷积层之间的主要区别如下：
- en: Fully connected layers learn global patterns in their input feature space (for
    example, in the case of a digit from the **Modified National Institute of Standards
    and Technology** (**MNIST**) dataset, seen in the example from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, the input feature space would be all the pixels
    from the image).
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接层学习其输入特征空间的全局模式（例如，在**修改后的国家标准与技术研究所**（**MNIST**）数据集的数字示例中，见[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)，*AutoKeras入门*，输入特征空间将是图像的所有像素）。
- en: On the other hand, the convolution layers learn local patterns—in the case of
    images, patterns found in small two-dimensional windows that run through the image.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，卷积层学习局部模式——在图像的情况下，是通过小的二维窗口扫描图像时发现的模式。
- en: 'In the following screenshot, we can see how these little windows detect local
    patterns such as lines, edges, and so on:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们可以看到这些小窗口如何检测局部模式，如线条、边缘等：
- en: '![Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network](img/B16953_04_05.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 卷积网络提取模式的可视化表示](img/B16953_04_05.jpg)'
- en: Figure 4.5 – Visual representation of pattern extraction by a convolutional
    network
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 卷积网络提取模式的可视化表示
- en: The convolution operation performs a transformation of the input image through
    a window (2D matrix) that scans it, generating a new image with different features.
    Each of these generated images is called a **filter**, and each filter contains
    different patterns extracted from the original image (edges, axes, straight lines,
    and so on).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作通过一个扫描输入图像的窗口（2D矩阵）来执行图像转换，生成一张具有不同特征的新图像。每一张生成的图像称为**滤波器**，每个滤波器包含从原始图像中提取的不同模式（如边缘、轴线、直线等）。
- en: The set of filters created in each intermediate layer of a CNN is called a feature
    map, which is a matrix of numbers of dimensions *r x c x n*, where *r* and *c*
    are rows and columns and *n* is the number of filters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中每个中间层创建的滤波器集合被称为特征图，它是一个具有*r x c x n*维度的数字矩阵，其中*r*和*c*分别是行和列，*n*是滤波器的数量。
- en: Basically, these feature maps are the parameters that CNNs learn.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这些特征图是CNN学习的参数。
- en: As we were already able to see when viewing the architecture of the MNIST classifier
    from [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029), *Getting Started
    with AutoKeras*, CNNs stack several convolutional layers (Conv2D) combined with
    pooling layers (MaxPooling2D). The task of the latter consists of reducing the
    dimensions of the filters, keeping the most relevant values. This helps clean
    up noise and reduces training time for the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在查看[*第2章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)，*AutoKeras入门*中的MNIST分类器架构时所见，CNN堆叠了多个卷积层（Conv2D），并结合池化层（MaxPooling2D）。后者的任务是减少滤波器的维度，保留最相关的值。这有助于清除噪声并减少模型的训练时间。
- en: Now, it's time to implement some practical examples. Let's start with an image
    classifier for a well-known dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候实现一些实际示例了。让我们从一个著名数据集的图像分类器开始。
- en: Creating a CIFAR-10 image classifier
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个CIFAR-10图像分类器
- en: The model we are going to create will classify images from a dataset called
    `32x32` **red, green, blue** (**RGB**) colored images, classified into 10 different
    classes. It is a collection of images that is commonly used to train ML and computer
    vision algorithms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要创建的模型将对一个名为`32x32`的**红色、绿色、蓝色**（**RGB**）彩色图像数据集进行分类，分为10个不同类别。它是一个常用于训练机器学习和计算机视觉算法的图像集合。
- en: 'Here are the classes in the dataset:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集中的各个类别：
- en: '`airplane`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airplane`'
- en: '`automobile`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`automobile`'
- en: '`bird`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bird`'
- en: '`cat`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cat`'
- en: '`deer`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deer`'
- en: '`dog`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dog`'
- en: '`frog`'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frog`'
- en: '`horse`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horse`'
- en: '`ship`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ship`'
- en: '`truck`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truck`'
- en: 'In the next screenshot, you can see some random image samples found in the
    CIFAR-10 dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一张截图中，你可以看到CIFAR-10数据集中一些随机的图像样本：
- en: '![Figure 4.6 – CIFAR-10 image samples'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6 – CIFAR-10 图像样本'
- en: '](img/B16953_04_06.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_06.jpg)'
- en: Figure 4.6 – CIFAR-10 image samples
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – CIFAR-10 图像样本
- en: This a problem considered already solved. It is relatively easy to achieve a
    classification accuracy close to 80%. For better performance, we must use deep
    learning CNNs with which a classification precision greater than 90% can be achieved
    in the test dataset. Let's see how to implement it with AutoKeras.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个已经被认为解决了的问题。实现接近80%的分类准确率相对容易。为了获得更好的性能，我们必须使用深度学习CNN，通过它可以在测试数据集中实现超过90%的分类精度。让我们看看如何使用AutoKeras实现这一点。
- en: This is a classification task, so we can use the `ImageClassifier` class. This
    class generates and tests different models and hyperparameters, returning an optimal
    classifier to categorize each image with its corresponding class.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个分类任务，因此我们可以使用`ImageClassifier`类。该类生成并测试不同的模型和超参数，返回一个最佳分类器，将每个图像分类到相应的类别。
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The notebook with the complete source code can be found at [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完整源代码的笔记本可以在[https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb)找到。
- en: 'Let''s now have a look at the relevant cells of the notebook in detail, as
    follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细查看一下笔记本中的相关单元格，如下所示：
- en: '`pip` package manager:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` 包管理器：'
- en: '[PRE1]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`matplotlib`, a Python plotting library that we will use to plot some digit
    representations, and CIFAR-10, which contains the categorized images dataset.
    The code to import the packages is shown here:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`，我们将使用它来绘制一些数字表示图，并且CIFAR-10包含已分类的图像数据集。以下是导入这些包的代码：'
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Getting the CIFAR-10 dataset**: We have to first load the CIFAR-10 dataset
    in memory and have a quick look at the dataset shape, as follows:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取CIFAR-10数据集**：我们首先需要将CIFAR-10数据集加载到内存中，并快速查看数据集的形状，如下所示：'
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output of the preceding code:'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Although it is a well-known machine learning dataset, it is always important
    to ensure that the data is distributed evenly, to avoid surprises. This can be
    easily done using `numpy` functions, as shown in the following code block:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个广为人知的机器学习数据集，但始终确保数据分布均匀非常重要，以避免出现意外情况。可以通过使用`numpy`函数轻松实现这一点，如下代码块所示：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The samples are perfectly balanced, as you can see in the following screenshot:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如下截图所示，样本完美平衡：
- en: '![Figure 4.7 – Train and test dataset histograms'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 训练和测试数据集直方图'
- en: '](img/B16953_04_07.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_07.jpg)'
- en: Figure 4.7 – Train and test dataset histograms
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 训练和测试数据集直方图
- en: Now that we are sure that our dataset is correct, it's time to create our image
    classifier.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们确认我们的数据集是正确的，接下来是创建我们的图像分类器。
- en: Creating and fine-tuning a powerful image classifier
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并微调一个强大的图像分类器
- en: 'We will now use the AutoKeras `ImageClassifier` class to find the best classification
    model. Just for this little example, we set `max_trials` (the maximum number of
    different Keras models to try) to `2`, and we do not set the `epochs` parameter
    so that it will use an adaptive number of epochs automatically. For real use,
    it is recommended to set a large number of trials. The code is shown here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用AutoKeras的`ImageClassifier`类来找到最佳的分类模型。仅此示例，我们将`max_trials`（尝试的不同Keras模型的最大数量）设置为`2`，并且不设置`epochs`参数，以便它自动使用自适应的训练轮数。对于实际使用，建议设置更多的尝试次数。代码如下：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s run the training to search for the optimal classifier for the CIFAR-10
    training dataset, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行训练，搜索CIFAR-10训练数据集的最佳分类器，如下所示：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.8 – Notebook output of image classifier training'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – 图像分类器训练的笔记本输出'
- en: '](img/B16953_04_08.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_08.jpg)'
- en: Figure 4.8 – Notebook output of image classifier training
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 图像分类器训练的笔记本输出
- en: The previous output shows that the accuracy of the training dataset is increasing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示，训练数据集的准确率正在上升。
- en: As it has to process thousands of color images, the models that AutoKeras will
    generate will be more expensive to train, so this process will take hours, even
    using `max_trials = 5`). Increasing this number would give us a more accurate
    model, although it would also take longer to finish.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要处理成千上万的彩色图像，AutoKeras生成的模型在训练时会更为昂贵，因此这个过程将需要几个小时，即使使用`max_trials = 5`。增加这个数字将给我们一个更准确的模型，尽管它也会花费更长的时间才能完成。
- en: Improving the model performance
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升模型性能
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在更短时间内获得更高精度，我们可以使用AutoKeras的高级功能来微调我们的模型，允许你自定义搜索空间。
- en: By using `AutoModel` with `ImageBlock` instead of `ImageClassifier`, we can
    create high-level configurations, such as `block_type` for the type of neural
    network to look for. We can also perform data normalization or data augmentation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`AutoModel`与`ImageBlock`代替`ImageClassifier`，我们可以创建高级配置，例如用于指定神经网络类型的`block_type`。我们还可以执行数据标准化或数据增强。
- en: If we have knowledge of deep learning and have faced this problem before, we
    can design a suitable architecture such as an `EfficientNet`-based image classifier,
    for instance, which is a deep residual learning architecture for image recognition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有深度学习的知识，并且之前遇到过这个问题，我们可以设计一个合适的架构，例如基于`EfficientNet`的图像分类器，它是用于图像识别的深度残差学习架构。
- en: 'See the following example for more details:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下示例，了解更多细节：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code block, we have done the following with the settings:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们已经使用以下设置完成了操作：
- en: With `block_type = "efficient"`, AutoKeras will only explore `EfficientNet`
    architectures.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`block_type = "efficient"`，AutoKeras将只探索`EfficientNet`架构。
- en: Initializing `augment = True` means we want to do data augmentation, a technique
    to create new artificial images from the originals. Upon activating it, AutoKeras
    will perform a series of transformations in the original image, as translations,
    zooms, rotations, or flips.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化`augment = True`表示我们希望进行数据增强，这是一种通过原始图像生成新的人工图像的技术。在激活此功能后，AutoKeras将对原始图像执行一系列变换，如平移、缩放、旋转或翻转。
- en: You can also not specify these arguments, in which case these different options
    would be tuned automatically.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以不指定这些参数，在这种情况下，这些不同的选项会被自动调整。
- en: 'You can see more details about the `EfficientNet` function here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里看到关于`EfficientNet`函数的更多细节：
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
- en: '[https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/resnet/](https://keras.io/api/applications/resnet/)'
- en: Evaluating the model with the test set
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试集评估模型
- en: 'After training, it is time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can contrast the good results obtained
    from the training set with a dataset never seen before. To do this, we run the
    following code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，接下来就是使用保留的测试数据集来衡量模型的实际预测效果。通过这种方式，我们可以将从训练集获得的良好结果与从未见过的数据集进行对比。为此，我们运行以下代码：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see here that prediction accuracy has a margin to improve using our test
    dataset (84.4%), although it's a pretty decent score for just a few hours of training;
    but just increasing the trials, we have achieved 98% of precision training for
    the first model (`ImageClassifier`) and running during one day in Google Colaboratory.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用我们的测试数据集（84.4%）预测准确率仍有提升空间，尽管这是仅仅几个小时训练得到的相当不错的得分；但通过增加尝试次数，我们已经实现了首个模型（`ImageClassifier`）的98%精度，并且在Google
    Colaboratory中运行了一整天。
- en: 'Once we have created and trained our classifier model, let''s see how it predicts
    on a subset of test samples. To do this, we run the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建并训练了分类器模型，让我们看看它在一部分测试样本上的预测结果。为此，我们运行以下代码：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.9 – Samples with their predicted and true labels](img/B16953_04_09.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 带有预测标签和真实标签的样本](img/B16953_04_09.jpg)'
- en: Figure 4.9 – Samples with their predicted and true labels
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 带有预测标签和真实标签的样本
- en: We can see that all the predicted samples match their true value, so our classifier
    has predicted correctly. Now, let's take a look inside the classifier to understand
    how it is working.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有预测的样本都与其真实值匹配，因此我们的分类器预测正确。现在，让我们深入了解分类器，理解它是如何工作的。
- en: Visualizing the model
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'Now, we can see a little summary with the architecture of the best generated
    model found (the one with 98% accuracy), and we will explain the reason why its
    performance is so good. Run the following code to see the summary:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到一个小的总结，展示了最佳生成模型的架构（即准确度为98%的模型），我们将解释它为什么能有如此好的表现。运行以下代码查看总结：
- en: '[PRE12]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here is the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 4.10 – Best model architecture summary](img/B16953_04_10.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 最佳模型架构总结](img/B16953_04_10.jpg)'
- en: Figure 4.10 – Best model architecture summary
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 最佳模型架构总结
- en: The key layer here is the `efficientnetb7` layer, which implements a cutting-edge
    architecture created by Google. Today, `EfficientNet` models are the best choice
    for image classification because this is a recent architecture that not only focuses
    on improving accuracy but also on the efficiency of the models so that they achieve
    higher precision and better efficiency over existing convolutional network-based
    architectures, reducing parameter sizes and **floating-point operations per second**
    (**FLOPS**) by an order of magnitude. However, we didn't need to know anything
    about it because AutoKeras automatically chose it for us.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键层是`efficientnetb7`层，它实现了由 Google 创建的前沿架构。如今，`EfficientNet` 模型是图像分类的最佳选择，因为这是一个最近的架构，不仅专注于提高准确度，还关注模型的效率，使其在现有的卷积网络架构基础上实现更高的精度和更好的效率，减少参数大小和**每秒浮动点操作数**（**FLOPS**）数量级。然而，我们不需要了解任何关于它的内容，因为
    AutoKeras 已经自动为我们选择了它。
- en: 'Let''s see how the blocks are connected to each other in a more visual way,
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一种更直观的方式来看一下各个块是如何相互连接的，如下所示：
- en: '![Figure 4.11 – Best model architecture visualization](img/B16953_04_11.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 最佳模型架构可视化](img/B16953_04_11.jpg)'
- en: Figure 4.11 – Best model architecture visualization
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 最佳模型架构可视化
- en: As we explained in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*, each block represents a layer, and the output
    of each is connected to the input of the next, except the first block (whose input
    is the image) and the last block (whose output is the prediction). The blocks
    before the `efficientnetb7` layer are all data-preprocessing blocks and they are
    in charge of adapting the image to a suitable format for this `EfficientNet` block,
    as well as generating extra images through augmentation techniques.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [*第二章*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029)《AutoKeras 入门》中解释的那样，*每个块表示一个层，每个块的输出连接到下一个块的输入，除了第一个块（其输入是图像）和最后一个块（其输出是预测结果）。在`efficientnetb7`层之前的所有块都是数据预处理块，负责将图像转换为适合此`EfficientNet`块的格式，并通过数据增强技术生成额外的图像。*
- en: Now is the time to tackle a non-classification problem. In the following practical
    example, we are going to create a human-age predictor based on a set of celebrity
    data—a fun tool that could make anyone blush.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是处理非分类问题的时候了。在接下来的实际示例中，我们将基于一组名人数据创建一个人类年龄预测器——一个有趣的工具，可能会让任何人脸红。
- en: Creating an image regressor to find out the age of people
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个图像回归器来预测人们的年龄
- en: In this section, we will create a model that will find out the age of a person
    from an image of their face. For this, we will train the model with a dataset
    of faces extracted from images of celebrities in **Internet Movie Database** (**IMDb**).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个模型，该模型能够从人脸图像中找出一个人的年龄。为此，我们将使用从**互联网电影数据库**（**IMDb**）提取的名人面孔数据集来训练该模型。
- en: As we want to approximate the age, we will use an image regressor for this task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们要预测年龄，我们将使用图像回归器来完成这项任务。
- en: 'In the next screenshot, you can see some samples taken from this dataset of
    celebrity faces:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一张截图中，你可以看到一些从这个名人面孔数据集中提取的样本：
- en: '![Figure 4.12 – A few image samples from IMDb faces dataset](img/B16953_04_12.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 来自 IMDb 面孔数据集的几个图像样本](img/B16953_04_12.jpg)'
- en: Figure 4.12 – A few image samples from IMDb faces dataset
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 来自 IMDb 面孔数据集的几个图像样本
- en: 'This notebook with the complete source code can be found here: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这本包含完整源代码的笔记本可以在这里找到：[https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb)。
- en: 'We will now explain the relevant code cells of the notebook in detail, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细解释笔记本中的相关代码单元，具体如下：
- en: '`pip` package manager. The code is shown here:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip`包管理器。代码如下所示：'
- en: '[PRE13]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Importing needed packages**: We now load AutoKeras and some more used packages,
    such as matplotlib, a Python plotting library that we will use to plot some picture
    samples and the categories distribution. The code to do this is shown here:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导入所需的包**：现在我们加载AutoKeras和一些常用的包，如matplotlib，一个Python绘图库，我们将使用它来绘制一些图片样本和类别分布。执行此操作的代码如下所示：'
- en: '[PRE14]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Getting the IMDb faces dataset**: Before training, we have to download the
    IMDb cropped faces dataset that contains the images of each face, as well as metadata
    with the age tags.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取IMDb面部数据集**：在训练之前，我们必须下载包含每个人脸图像及带有年龄标签的元数据的IMDb裁剪面部数据集。'
- en: 'The following command lines are idempotent—they download and extract data only
    if it does not already exist:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下命令行是幂等的—它们只会在数据不存在时下载并解压数据：
- en: '[PRE15]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output of the preceding code:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE16]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`MatLab` file.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MatLab`文件。'
- en: b. The age is not in the params—it has to be calculated.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. 年龄不在参数中—它必须被计算出来。
- en: c. The images are not homogeneous—they have different dimensions and colors.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c. 图像不一致—它们的尺寸和颜色不同。
- en: 'To resolve these issues, we have created the following utility functions:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们创建了以下的工具函数：
- en: 'a. `imdb_meta_to_df(matlab_filename)`: This converts the IMDb MatLab file to
    a pandas DataFrame and calculates the age.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. `imdb_meta_to_df(matlab_filename)`：此函数将IMDb MatLab文件转换为Pandas DataFrame，并计算年龄。
- en: 'b. `normalize_dataset(df_train_set)`: This returns a tuple of normalized images
    (resized to `128x128` and converted to grayscale) and ages converted to integers.'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. `normalize_dataset(df_train_set)`：这会返回一个元组，包含归一化后的图像（调整为`128x128`并转换为灰度）和转为整数的年龄。
- en: In the notebook, you will find more details about how these functions are working.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，你将找到更多关于这些函数如何工作的细节。
- en: 'Let''s now see how to use them, as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下如何使用它们，具体如下：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the previous code snippet, we used the `imdb_meta_to_df` function to convert
    the `imdb` metadata information stored in a MatLab file to a Pandas DataFrame.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了`imdb_meta_to_df`函数，将存储在MatLab文件中的`imdb`元数据信息转换为Pandas DataFrame。
- en: 'The DataFrame contains a lot of images; to make the training faster, we will
    use only a part of them to create the datasets, as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame包含了大量的图像；为了加速训练，我们将只使用其中一部分图像来创建数据集，具体如下：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we create the final datasets with normalized images and ages, as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建最终的数据集，包含归一化的图像和年龄，具体如下：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once all the images are the same size (128×128) and the same color (grayscale)
    and we have the labels and the estimated age, we are ready to feed our model,
    but first we have to create it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有图片的尺寸（128×128）和颜色（灰度图）一致，并且我们有了标签和估计的年龄，我们就可以准备输入模型，但首先我们需要创建它。
- en: Creating and fine-tuning a powerful image regressor
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建和微调一个强大的图像回归器
- en: 'Because we want to predict age, and this is a scalar value, we are going to
    use AutoKeras `ImageRegressor` as an age predictor. We set `max_trials` (the maximum
    number of different Keras models to try) to `10`, and we do not set the `epochs`
    parameter so that it will use an adaptive number of epochs automatically. For
    real use, it is recommended to set a large number of trials. The code is shown
    here:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们要预测年龄，并且这是一个标量值，所以我们将使用AutoKeras的`ImageRegressor`作为年龄预测器。我们将`max_trials`（尝试的不同Keras模型的最大数量）设置为`10`，并且不设置`epochs`参数，这样它会自动使用适应性数量的epochs。对于实际使用，建议设置较大的试验次数。代码如下所示：
- en: '[PRE20]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s run the training model to search for the optimal regressor for the training
    dataset, as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行训练模型，寻找适合训练数据集的最佳回归器，具体如下：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here is the output of the preceding code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 4.13 – Notebook output of our age predictor training'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.13 – 我们的年龄预测器训练的笔记本输出'
- en: '](img/B16953_04_13.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_13.jpg)'
- en: Figure 4.13 – Notebook output of our age predictor training
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – 我们的年龄预测器训练的笔记本输出
- en: The previous output shows that the loss for the training dataset is decreasing.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示训练数据集的损失正在下降。
- en: This training process has taken 1 hour in Colaboratory. We have limited the
    search to 10 architectures (`max_trials = 10`) and restricted the number of images
    to 10,000\. Increasing these numbers would give us a more accurate model, although
    it would also take longer to finish.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这个训练过程在 Colaboratory 中花费了 1 小时。我们将搜索限制为 10 种架构（`max_trials = 10`），并将图像数量限制为
    10,000。增加这些数字将给我们一个更精确的模型，尽管也会花费更多时间。
- en: Improving the model performance
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升模型性能
- en: If we need more precision in less time, we can fine-tune our model using an
    advanced AutoKeras feature that allows you to customize your search space.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要在更短的时间内获得更高的精度，我们可以使用 AutoKeras 的高级功能微调我们的模型，从而自定义搜索空间。
- en: As we did earlier in the regressor example, we can use `AutoModel` with `ImageBlock`
    instead of `ImageRegressor` so that we can implement high-level configurations,
    such as define a specific architecture neural network to search using `block_type`.
    We can also perform data preprocessing operations, such as normalization or augmentation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在回归模型示例中所做的那样，我们可以使用 `AutoModel` 配合 `ImageBlock`，而不是使用 `ImageRegressor`，这样我们就可以实现更高级的配置，比如使用
    `block_type` 搜索特定架构的神经网络。我们还可以执行数据预处理操作，比如归一化或增强。
- en: As we did in the previous image classifier example, we can design a suitable
    architecture as an `EfficientNet`-based image regressor, for instance, which is
    a deep residual learning architecture for image recognition.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的图像分类器示例中所做的那样，我们可以设计一个合适的架构，例如基于 `EfficientNet` 的图像回归器，这是一种用于图像识别的深度残差学习架构。
- en: 'See the following example for more details:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下示例以获取更多细节：
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the previous code, we have done the following with the settings:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们已经做了以下设置：
- en: The `Normalization` block will transform all image values in the range between
    0 and 255 to floats between 0 and 1.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalization` 块会将所有图像值从 0 到 255 的范围转换为 0 到 1 之间的浮动值。'
- en: The shape has been set (60000, 28 * 28) with values between 0 and 1.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经设置了形状（60000, 28 * 28），值在 0 和 1 之间。
- en: With `ImageBlock(block_type="efficient"`, we are telling AutoKeras to only scan
    `EfficientNet` architectures.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ImageBlock(block_type="efficient"`，我们告诉 AutoKeras 只扫描 `EfficientNet` 架构。
- en: The `ImageAugmentation` block performs data augmentation, a technique to create
    new artificial images from the originals.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImageAugmentation` 块执行数据增强，这是通过原始图像生成新的人工图像的一种技术。'
- en: You can also not specify any of these arguments, in which case these different
    options would be tuned automatically.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以选择不指定这些参数，在这种情况下，系统将自动调整这些不同的选项。
- en: 'You can see more details about the `EfficientNet` function here:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里查看更多关于 `EfficientNet` 函数的细节：
- en: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/applications/efficientnet/](https://keras.io/api/applications/efficientnet/)'
- en: Evaluating the model with the test set
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用测试集评估模型
- en: 'After training, it''s time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can rule out that the good results
    obtained with the training set are due to overfitting. The code to do this is
    shown here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，是时候使用保留的测试数据集来衡量我们模型的实际预测效果了。这样，我们可以排除训练集获得的好结果是由于过拟合造成的。执行此操作的代码如下：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here is the output of the preceding code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '[PRE24]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This error still has a lot of margin to improve, but let''s have a look at
    how it''s predicting over a subset of test samples, as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个错误还有很大的提升空间，但让我们看看它是如何在一部分测试样本上进行预测的，如下所示：
- en: '[PRE25]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the output of the preceding code:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 4.14 – Samples with their predicted and true labels'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 – 样本及其预测标签与真实标签'
- en: '](img/B16953_04_14.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_14.jpg)'
- en: Figure 4.14 – Samples with their predicted and true labels
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 样本及其预测标签与真实标签
- en: We can see that some predicted samples are near to the real age of the person
    but others aren't, so investing in more training hours and fine-tuning will make
    it predict better. Let's take a look inside the classifier to understand how it
    is working.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到一些预测样本接近真实年龄，但另一些则不然，因此投入更多的训练时间和微调将使其预测得更好。让我们深入了解分类器，理解它是如何工作的。
- en: Visualizing the model
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化模型
- en: 'We can now see a little summary with the architecture of the best generated
    model found by running the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到通过运行以下代码找到的最佳生成模型的架构简要总结：
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the output of the preceding code:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是前面代码的输出：
- en: '![Figure 4.15 – Best model architecture summary'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15 – 最佳模型架构总结'
- en: '](img/B16953_04_15.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16953_04_15.jpg)'
- en: Figure 4.15 – Best model architecture summary
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 最佳模型架构概览
- en: 'The key layers here are the convolution and pooling blocks, as we explained
    at the beginning of this chapter. These layers learn local patterns from the image
    that help to perform the predictions. Here is a visual representation of this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键层是卷积和池化块，正如我们在本章开头所解释的那样。这些层从图像中学习局部模式，帮助进行预测。以下是这一点的可视化表示：
- en: '![Figure 4.16 – Best model architecture visualization](img/B16953_04_16.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图4.16 – 最佳模型架构可视化](img/B16953_04_16.jpg)'
- en: Figure 4.16 – Best model architecture visualization
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – 最佳模型架构可视化
- en: First, there are some data preprocessing blocks that normalize the images and
    do data augmentation; then, there are several stacked convolution and pooling
    blocks; then, a dropout block to do the regularization (a technique to reduce
    overfitting based on dropping random neurons while training, to reduce the correlation
    between the closest neurons); and finally, we see the regression block, to convert
    the output to a scalar (the age).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有一些数据预处理块用于规范化图像并进行数据增强；然后，接下来是几个堆叠的卷积和池化块；接着是一个dropout块用于正则化（这是一种减少过拟合的技术，通过在训练过程中随机丢弃神经元，减少相邻神经元之间的相关性）；最后，我们看到回归块，它将输出转换为一个标量（即年龄）。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概览
- en: In this chapter, we have learned how convolutional networks work, how to implement
    an image classifier, and how to fine-tune it to improve its accuracy. We have
    also learned how to implement an image regressor and fine-tune it to improve its
    performance.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了卷积网络的工作原理，如何实现图像分类器，以及如何微调它以提高准确性。我们还学习了如何实现图像回归器并微调它以提高性能。
- en: Now that we have learned how to work with images, we are ready to move on to
    the next chapter, where you will learn how to work with text by implementing classification
    and regression models using AutoKeras.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何处理图像，我们准备进入下一章，您将在那里学习如何通过使用AutoKeras实现分类和回归模型来处理文本。
