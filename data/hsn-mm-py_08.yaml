- en: 2D HMM for Image Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于图像处理的二维隐马尔可夫模型（2D HMM）
- en: 'In this chapter, we will introduce the application of HMM in the case of image
    segmentation. For image segmentation, we usually split up the given image into
    multiple blocks of equal size and then perform an estimation for each of these
    blocks. However, these algorithms usually ignore the contextual information from
    the neighboring blocks. To deal with that issue, 2D HMMs were introduced, which
    consider feature vectors to be dependent through an underlying 2D Markovian mesh.
    In this chapter, we will discuss how these 2D HMMs work and will derive parameter
    estimation algorithms for them. In this chapter, we will discuss the following
    topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍隐马尔可夫模型（HMM）在图像分割中的应用。在图像分割中，我们通常将给定的图像拆分成多个相同大小的块，然后对每个块进行估计。然而，这些算法通常忽略了来自邻近块的上下文信息。为了解决这个问题，引入了二维隐马尔可夫模型（2D
    HMM），它通过一个潜在的二维马尔可夫网格考虑特征向量之间的依赖关系。在本章中，我们将讨论二维隐马尔可夫模型是如何工作的，并推导出相应的参数估计算法。本章将讨论以下主题：
- en: Pseudo 2D HMMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假二维隐马尔可夫模型
- en: Introduction to 2D HMMs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维隐马尔可夫模型（2D HMM）简介
- en: Parameter learning in 2D HMMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维隐马尔可夫模型中的参数学习
- en: Applications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用
- en: Recap of 1D HMM
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一维隐马尔可夫模型回顾
- en: Let's recap how 1D HMMs work, which we discussed in the previous chapters of
    this book. We have seen that HMM is a just a process over Markov chains. At any
    point in time, an HMM is in one of the possible states, and the next state that
    the model will transition to depends on the current state and the transition probability
    of the model.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下前几章中讨论的如何使用一维隐马尔可夫模型（1D HMM）。我们看到，隐马尔可夫模型其实是马尔可夫链上的一个过程。在任何时刻，隐马尔可夫模型处于某个可能的状态，模型将转移到的下一个状态取决于当前状态以及模型的转移概率。
- en: Suppose that there are *M = {1, 2, ..., M}* possible states for HMM, and the
    transition probability of going from some state *i* to state *j* is given by *a[i,j]*. For
    such a model, if at time *t-1* the model is at state *i*, then at time *t* it
    would be in state *j* with a probability of *a[i,j]*. This probability is known
    as the **transition probability**. Also, we have defined the observed variable
    in the model, which only depends on the current state of our hidden variable.
    We can define the observed variable at time *t* as *u[t]*, so let's say the emission
    distribution for the state *i* for the variable *u[t]* is given by *b[i](u[t])*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于隐马尔可夫模型（HMM）有 *M = {1, 2, ..., M}* 个可能的状态，且从某个状态 *i* 转移到状态 *j* 的转移概率由 *a[i,j]*
    给出。对于这样的模型，如果在时刻 *t-1* 模型处于状态 *i*，那么在时刻 *t* 它将以概率 *a[i,j]* 进入状态 *j*。这个概率被称为**转移概率**。另外，我们已经定义了模型中的观察变量，它仅依赖于隐藏变量的当前状态。我们可以将时刻
    *t* 的观察变量定义为 *u[t]*，假设状态 *i* 对变量 *u[t]* 的发射分布为 *b[i](u[t])*。
- en: 'We also need to define the initial probability, *π[i]*, as the probability
    of being in state *i* at time *t = 1*. With all these given values, we can determine
    the likelihood of observing any given sequence, ![](img/3521f387-45e1-4ef5-9b5a-c4278bcc9e0c.png), as
    follows:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义初始概率 *π[i]*，即在时刻 *t = 1* 时处于状态 *i* 的概率。通过给定这些值，我们可以确定观察到任何给定序列的似然，![](img/3521f387-45e1-4ef5-9b5a-c4278bcc9e0c.png)，如下所示：
- en: '![](img/042c0543-12c7-41ed-80d4-b33dffcb2b12.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/042c0543-12c7-41ed-80d4-b33dffcb2b12.png)'
- en: 'In most situations, we assume the states to be a Gaussian mixture model; in
    which case, the previous equation can be generalized further, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，我们假设状态为高斯混合模型；在这种情况下，前面的方程可以进一步推广，如下所示：
- en: '![](img/65242904-b50a-4f5f-9b5f-3b7cec0002a2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65242904-b50a-4f5f-9b5f-3b7cec0002a2.png)'
- en: Here, *u[i]* is the mean, *∑[i]* is the covariance matrix, and *k* is the dimensionality
    of the observed variable *u[t]*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*u[i]* 是均值，*∑[i]* 是协方差矩阵，*k* 是观察变量 *u[t]* 的维度。
- en: 'Now we have our model defined, we can move on to the estimation method. Estimation
    is usually performed using the Baum-Welch algorithm that we saw in [Chapter 4](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml),
    *Parameter Inference using Maximum Likelihood*, which performs a maximum-likelihood
    estimation. Let *L[i](t)* be the conditional distribution of being in state *i* at
    time *t,* given the observations, and *H[i,j](t)* be the conditional probability
    of transitioning to state *j* from state *i* at time *t + 1,* again, given the
    observations. Then, we can re-estimate the mean, covariance, and transition probability
    as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型，可以继续进行估计方法。估计通常通过我们在[第 4 章](b3f2bff1-0fe7-4d54-8a9e-9911c77e7d62.xhtml)中看到的
    Baum-Welch 算法来进行，*最大似然参数推断*，该算法执行最大似然估计。令 *L[i](t)* 为在时刻 *t* 处于状态 *i* 的条件分布，给定观察结果，*H[i,j](t)*
    为从状态 *i* 在时刻 *t + 1* 转移到状态 *j* 的条件概率，仍然是给定观察结果。然后，我们可以重新估计均值、协方差和转移概率，如下所示：
- en: '![](img/bb175f4f-5958-4b48-8001-967193bace35.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb175f4f-5958-4b48-8001-967193bace35.png)'
- en: To compute the values of *L[i](t)* and *H[i,j](t),* we use the forward-backward
    algorithm. The forward algorithm gives us the probability, *α[i](t),* of observing
    the first *t* outcomes, ![](img/c6ef61da-9aa5-474e-9394-ba29bcd71567.png), and
    being in state *i* at time *t*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 *L[i](t)* 和 *H[i,j](t)* 的值，我们使用前向-后向算法。前向算法给出了观察到前 *t* 个结果的概率，*α[i](t)*，以及在时刻
    *t* 时处于状态 *i* 的概率。
- en: 'This probability can be evaluated using the following set of equations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率可以通过以下方程组进行评估：
- en: '![](img/ac5f17a1-566c-42f1-bda1-226ceed116ac.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac5f17a1-566c-42f1-bda1-226ceed116ac.png)'
- en: 'We also define the backward probability, *β[i](t),* as the conditional probability
    of having the observations *{u[r]}r=t + 1,...,T*, given that the model is in state
    *i* at time *t*. The conditional probability *β[i](t)* can be computed as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了后向概率，*β[i](t)*，作为给定模型在时刻 *t* 处于状态 *i* 时，观察到 *{u[r]}r=t + 1,...,T* 的条件概率。后向概率
    *β[i](t)* 可以通过以下方式计算：
- en: '![](img/f5182d06-65fc-4a97-835b-a47b2e57898b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5182d06-65fc-4a97-835b-a47b2e57898b.png)'
- en: 'With these values to hand, we can compute that *L[i](t)* and *H[i,j](t)* can
    be solved as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值后，我们可以计算出 *L[i](t)* 和 *H[i,j](t)* 可以通过以下方式求解：
- en: '![](img/0f83be13-899c-4c40-92dc-5896c50fa924.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f83be13-899c-4c40-92dc-5896c50fa924.png)'
- en: 'We can approximate this algorithm by assuming each observation to have resulted
    from the most likely hidden state. This allows us to simplify the Baum-Welch algorithm;
    this is commonly also known as the **Viterbi training algorithm**. Given the observed
    states, and assuming the state sequence to be ![](img/38ab31a3-1fcc-4aa2-8f9b-e47e836b3667.png) with
    the maximum conditional probability, this can be given as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过假设每个观察结果来自最可能的隐藏状态来近似该算法。这使得我们可以简化 Baum-Welch 算法；这种方法通常也被称为 **维特比训练算法**。给定观察到的状态，并假设状态序列为
    ![](img/38ab31a3-1fcc-4aa2-8f9b-e47e836b3667.png)，其具有最大条件概率，可以通过以下方式表示：
- en: '![](img/f80bb564-e3ab-40e5-bf45-04e98b052cd3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f80bb564-e3ab-40e5-bf45-04e98b052cd3.png)'
- en: 'Here, ![](img/b7a2aeba-0caf-497b-886c-33f9030cc85c.png) can be computed as
    follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b7a2aeba-0caf-497b-886c-33f9030cc85c.png) 可以通过以下方式计算：
- en: '![](img/351bc225-75ac-4ef2-8e3a-02286779feef.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/351bc225-75ac-4ef2-8e3a-02286779feef.png)'
- en: 'With these values, we can then compute the model parameters as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些值，我们可以计算模型参数，如下所示：
- en: '![](img/6e6edb61-4de7-4ae6-8240-9c2570a72d84.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e6edb61-4de7-4ae6-8240-9c2570a72d84.png)'
- en: Here, *I* is the indicator function, which returns 1 if the function's argument
    is true; otherwise, it returns 0.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*I* 是指示函数，当函数的参数为真时返回 1，否则返回 0。
- en: In this section, we have quickly reviewed the basic concepts of 1D HMMs when
    the states are parameterized using Gaussian mixture models, so we can now move
    on to 2D HMMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要回顾了当状态使用高斯混合模型参数化时的一维 HMM（隐马尔可夫模型）的基本概念，现在我们可以继续讨论二维 HMM。
- en: 2D HMMs
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二维 HMM
- en: A lot of work has been done regarding 2D HMMs, but the most recent work and
    well-received work has been done by Jia Li, Amir Najmi, and Robert Gray in their
    paper, *Image Classification by a Two Dimensional Hidden Markov Model.*This section
    has been written based on their work. We will start by giving the general algorithm
    they have introduced, and then, in further subsections, we will see how the algorithm
    works.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 关于二维 HMM，已经做了很多工作，但最近的、受欢迎的工作是由 Jia Li、Amir Najmi 和 Robert Gray 在他们的论文《*通过二维隐马尔可夫模型进行图像分类*》中完成的。本节内容基于他们的工作编写。我们将从介绍他们提出的通用算法开始，随后在接下来的小节中，我们将看到该算法的具体运作方式。
- en: Algorithm
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: 'The algorithm for image classification is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类的算法如下：
- en: 'Training:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练：
- en: Divide the training images into non-overlapping blocks with equal size and extract
    a feature vector for each block
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练图像划分为大小相等且不重叠的块，并为每个块提取特征向量
- en: Select the number of states for the 2D HMM
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择2D HMM的状态数
- en: Estimate the model parameters based on the feature vectors and the training
    labels
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据特征向量和训练标签估计模型参数
- en: 'Testing:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试：
- en: Similar to training, generate feature vectors for the testing images
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与训练类似，为测试图像生成特征向量
- en: Search for the set of classes with the maximum a posteriori probability, given
    the feature vectors, according to the training model
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据训练模型，给定特征向量，搜索具有最大后验概率的类别集合
- en: Assumptions for the 2D HMM model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2D HMM模型的假设
- en: In this section, we will quickly go through the assumptions for the 2D HMM model and
    the derivation of how these assumptions simplify our equations. For a more detailed
    derivation, please refer to the original paper.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将快速浏览2D HMM模型的假设以及这些假设如何简化我们的方程。更详细的推导请参阅原始论文。
- en: We start by dividing the image into smaller blocks, from which we evaluate the
    feature vectors, and, using these feature vectors, we classify the image. In the
    case of a 2D HMM, we make the assumption that the feature vectors are generated
    by a Markov model with a state change happening once every block. We also define
    the relationship between the blocks based on which block comes before or after
    which block. A block at position *(i', j')* is said to come before the block at
    position *(i, j)* if *i'* or *i' = i* and *j' < j*. Assuming that there are *M
    = {1, 2, ... M}* states, the state of any given block at position *(i, j)* is
    denoted by *S[i,j]*, the feature vector is denoted by *u[i,j]*, and the class
    is denoted by *c[i,j]*. Another point to keep in mind is that the order of the
    blocks has been introduced just to explain the assumptions of the model, and the
    algorithm doesn't consider any order of blocks while doing the classification.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从将图像划分为较小的块开始，从中评估特征向量，并使用这些特征向量对图像进行分类。在2D HMM的情况下，我们假设特征向量是由一个马尔可夫模型生成的，并且每次块的变化都会发生状态变化。我们还基于块的前后关系定义块之间的关系。若位置为
    *(i', j')* 的块出现在位置 *(i, j)* 之前，则满足 *i'* 或 *i' = i* 且 *j' < j*。假设有 *M = {1, 2,
    ... M}* 个状态，任何给定位置 *(i, j)* 的块的状态用 *S[i,j]* 表示，特征向量用 *u[i,j]* 表示，类别用 *c[i,j]*
    表示。另一个需要注意的点是，块的顺序只是为了说明模型的假设，算法在进行分类时并不考虑块的顺序。
- en: 'The first assumption made by the model is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型做出的第一个假设如下：
- en: '![](img/4c3eb9bf-477f-4321-8d16-2db25feaa3c7.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c3eb9bf-477f-4321-8d16-2db25feaa3c7.png)'
- en: This assumption states that knowing the current state is a sufficient statistic
    for estimating the transition probabilities, which means that *u* is conditionally
    independent. Also, according to the assumption, the state transition is a Markov
    process in two dimensions, and the probability of the system entering any particular
    state depends on the state of the model in both the horizontal and vertical directions
    in the previous time and observation instance. We also assume that there is one-to-one
    mapping from state to class, so that once we know the state, the class can be
    directly computed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该假设声明，知道当前状态足以估计转移概率，这意味着 *u* 是条件独立的。同时，根据该假设，状态转移是一个二维的马尔可夫过程，系统进入任何特定状态的概率取决于前一时间和观察实例中模型在水平和垂直方向上的状态。我们还假设状态与类别之间是1对1的映射，因此一旦知道状态，就可以直接计算类别。
- en: 'The second assumption is that the feature vectors are a Gaussian mixture distribution
    for each state. We know that any M-component Gaussian mixture can be split into
    M substates with single Gaussian distributions; therefore, for a block with state
    *s* and feature vector *u*, the probability of the distribution is given by the
    following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个假设是特征向量对于每个状态来说是一个高斯混合分布。我们知道，任何M成分的高斯混合可以分解为M个单一高斯分布的子状态；因此，对于状态 *s* 和特征向量
    *u* 的块，分布的概率由以下公式给出：
- en: '![](img/58b278e3-5a42-4ffa-bcf7-4a412c916567.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58b278e3-5a42-4ffa-bcf7-4a412c916567.png)'
- en: Here, *∑[s]* is the covariance matrix and *μ[s]* is the mean vector of the Gaussian
    distribution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*∑[s]* 是协方差矩阵，*μ[s]* 是高斯分布的均值向量。
- en: 'We can now use the Markovian assumptions to simplify the evaluation of the
    probability of the states. The probability of the states for all the blocks in
    the image is denoted by *P{s[i,j] : (i,j) ∈ N}*, where *N = {(i,j) : 0 ≤ i < w,
    0 ≤ j < z}*. But before we use the Markovian assumptions to efficiently expand
    the probability, we need to prove that, given the two previous assumptions, a
    rotated form of two-dimensional Markovian property holds for the image.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在可以使用马尔可夫假设来简化状态的概率评估。图像中所有块的状态概率用 *P{s[i,j] : (i,j) ∈ N}* 表示，其中 *N = {(i,j)
    : 0 ≤ i < w, 0 ≤ j < z}*。但在我们使用马尔可夫假设有效地展开概率之前，我们需要证明，基于前两个假设，图像满足二维马尔可夫属性的旋转形式。'
- en: 'We define a rotated relation of ![](img/3c09049a-47b5-4f1c-879e-b62c5ae7c047.png),
    denoted by ![](img/bf1768a6-d1d0-4807-8510-e7e986aa189e.png), which specifies ![](img/596b1745-78e3-4d41-be38-213c8116f383.png),
    if *j'' < j*, or *j'' = j* and *i'' < i*. We need to prove the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了 ![](img/3c09049a-47b5-4f1c-879e-b62c5ae7c047.png) 的旋转关系，记为 ![](img/bf1768a6-d1d0-4807-8510-e7e986aa189e.png)，它指定了 ![](img/596b1745-78e3-4d41-be38-213c8116f383.png)，如果
    *j' < j*，或 *j' = j* 且 *i' < i*。我们需要证明以下内容：
- en: '![](img/79ff27fe-7ec0-4bad-b171-b7e7136b11a2.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79ff27fe-7ec0-4bad-b171-b7e7136b11a2.png)'
- en: 'So, we use the previous definition of ![](img/8c06ffa1-e05e-495c-b5b2-86669d53cdd2.png) and
    introduce the following new notation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用前面定义的 ![](img/8c06ffa1-e05e-495c-b5b2-86669d53cdd2.png) 并引入以下新符号：
- en: '![](img/4d933c53-807c-49d6-977f-bd810ca54879.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d933c53-807c-49d6-977f-bd810ca54879.png)'
- en: 'From the preceding equations, we can also see this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的方程中，我们还可以看到这一点：
- en: '![](img/eeddc6c8-43d8-4b44-8e17-34f7ac4a7e4d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeddc6c8-43d8-4b44-8e17-34f7ac4a7e4d.png)'
- en: 'Now, to simplify notation, let''s introduce ![](img/b68105e2-f519-4505-ba80-a315a1478499.png) and ![](img/06f8b9c4-4aed-45be-8a0a-8eeb4f8e6ae9.png).
    From these we can do the following derivation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了简化符号，假设我们引入 ![](img/b68105e2-f519-4505-ba80-a315a1478499.png) 和 ![](img/06f8b9c4-4aed-45be-8a0a-8eeb4f8e6ae9.png)。从这些符号中，我们可以进行以下推导：
- en: '![](img/58cb7618-9a25-4eb7-b3d8-d5e79b4f6cb4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58cb7618-9a25-4eb7-b3d8-d5e79b4f6cb4.png)'
- en: 'Expanding the conditional probability, we get the following:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 展开条件概率，我们得到如下结果：
- en: '![](img/7250fd03-5f94-47b8-95ec-18a1dc043e5b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7250fd03-5f94-47b8-95ec-18a1dc043e5b.png)'
- en: 'Using the Markovian assumption, we get the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用马尔可夫假设，我们得到如下结果：
- en: '![](img/6fe48d95-9c3e-450e-8eba-734330c19647.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fe48d95-9c3e-450e-8eba-734330c19647.png)'
- en: 'And, finally, using the Markovian assumption and the assumption that the feature
    vector of a block is conditionally independent of other blocks given its state,
    we have the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用马尔可夫假设以及假设给定状态下，块的特征向量是条件独立的，我们得到以下结果：
- en: '![](img/8b01f2c6-4fda-49dc-ab90-eb64650b4f0e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b01f2c6-4fda-49dc-ab90-eb64650b4f0e.png)'
- en: Where *m = s[i-1,j]*, *n = s[i,j-1]*, and *l = s[i,j]*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m = s[i-1,j]*，*n = s[i,j-1]*，和 *l = s[i,j]*。
- en: 'If we replace ![](img/4c8ad46c-f557-4b7f-a6ea-4f8d6560d501.png) with ![](img/19fe49ce-a212-4450-a337-3b8006b7747c.png),
    and replace ![](img/7eed13f5-8e55-464b-9c0e-1d43f4ca917d.png) with ![](img/90b91eb4-b566-4bf1-827e-df853411dbfb.png) in
    the derivation, all the equations will still hold. This gives us the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 ![](img/4c8ad46c-f557-4b7f-a6ea-4f8d6560d501.png) 替换为 ![](img/19fe49ce-a212-4450-a337-3b8006b7747c.png)，并将 ![](img/7eed13f5-8e55-464b-9c0e-1d43f4ca917d.png) 替换为 ![](img/90b91eb4-b566-4bf1-827e-df853411dbfb.png) 在推导过程中，所有的方程仍然成立。这给我们带来了以下结果：
- en: '![](img/dea74b78-e673-4d7c-9ce0-ef8dce4f395c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dea74b78-e673-4d7c-9ce0-ef8dce4f395c.png)'
- en: 'Since the preceding equation implies the original Markovian assumption and
    its rotated version, we can show the equivalent end of the two assumptions as
    follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的方程包含了原始的马尔可夫假设及其旋转版本，我们可以如下展示这两个假设的等价性：
- en: '![](img/7c7cdb97-5dd2-4d69-be8d-98c616b266cc.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c7cdb97-5dd2-4d69-be8d-98c616b266cc.png)'
- en: 'Now we can simplify the expansion of *P{s[i,j] : (i,j) ∈ N}*, as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们可以简化 *P{s[i,j] : (i,j) ∈ N}* 的展开式，如下所示：'
- en: '![](img/a5b60120-a1df-4be4-ab1a-e8b9b72d6821.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5b60120-a1df-4be4-ab1a-e8b9b72d6821.png)'
- en: 'Here, *w* and *z* are the numbers of rows and columns in the image, respectively,
    and *T[i]* is the sequence of states for blocks on the diagonal *i*. Next, we
    need to prove that *P(T[i]|T[i-1],...,T[0]) = P(T[i]|T[i-1])*. Assuming *T[i]
    = {s[i,0], s[i-1,1], ..., s[0,i]}*, this means *T[i-1] = {s[i-1,0], s[i-2,1],
    ..., s[0,i-1]}* and hence we have the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*w* 和 *z* 分别是图像的行数和列数，*T[i]* 是对角线 *i* 上块的状态序列。接下来，我们需要证明 *P(T[i]|T[i-1],...,T[0])
    = P(T[i]|T[i-1])*。假设 *T[i] = {s[i,0], s[i-1,1], ..., s[0,i]}*，这意味着 *T[i-1] = {s[i-1,0],
    s[i-2,1], ..., s[0,i-1]}*，因此我们得到以下结果：
- en: '![](img/c6f12b4f-77fd-4c19-a1d3-a88d2728c541.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6f12b4f-77fd-4c19-a1d3-a88d2728c541.png)'
- en: 'Therefore, we can conclude:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以得出结论：
- en: '![](img/df39c7ac-122a-4ffa-9716-2eb4325006d1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df39c7ac-122a-4ffa-9716-2eb4325006d1.png)'
- en: 'Using this, we get the following simplified equation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此方法，我们得到以下简化的方程：
- en: '![](img/f3f98132-ebce-4291-8343-be8ab1f9a950.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3f98132-ebce-4291-8343-be8ab1f9a950.png)'
- en: Parameter estimation using EM
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EM进行参数估计
- en: Since we have the model ready, we now need to estimate the parameters of the
    model. We need to estimate the mean vectors* μ[m]*; the covariance matrices *∑[m]*;
    and the transition probabilities *a[m,n,l]*, where *m,n,l = 1,..., M*, and *M*
    is the total number of states. We will use the **expectation maximization** (**EM**)
    algorithm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经准备好模型，现在需要估计模型的参数。我们需要估计均值向量*μ[m]*；协方差矩阵*∑[m]*；以及转移概率*a[m,n,l]*，其中*m,n,l
    = 1,..., M*，*M*是状态的总数。我们将使用**期望最大化**（**EM**）算法。
- en: 'As we have seen in previous chapters, EM is an iterative algorithm that can
    learn the maximum likelihood estimates in the case of missing data; that is, when
    we have unobserved variables in our data. Let''s say that our unobserved variable *x* is
    in the sample space *x*, and the observed variable *y is* in the sample space
    *y*. If we postulate a family of distribution *f(x|Φ)*, with parameters *Φ ∈ Ω*,
    then the distribution over *y* is given as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，EM是一个迭代算法，可以在缺失数据的情况下学习最大似然估计；也就是说，当我们的数据中有未观察到的变量时。假设我们的未观察到的变量*x*位于样本空间*x*中，而观察到的变量*y*位于样本空间*y*中。如果我们假设一个分布族*f(x|Φ)*，其参数*Φ
    ∈ Ω*，那么*y*的分布如下所示：
- en: '![](img/e0df3b3b-5947-49b7-a79b-54219b731b50.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0df3b3b-5947-49b7-a79b-54219b731b50.png)'
- en: 'The EM algorithm will then try to find the value of ![](img/754282b8-979e-4614-b427-d7a6abe56fad.png) that
    would maximize the value of *g(y|Φ)* given the observed *y*. The EM iteration *Φ^((p)) → Φ^((p+1))* is
    defined as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: EM算法将尝试找到一个值！[](img/754282b8-979e-4614-b427-d7a6abe56fad.png)，该值会最大化给定观察到的*y*时，*g(y|Φ)*的值。EM迭代*Φ^((p))
    → Φ^((p+1))*定义如下：
- en: '**E-step**: Compute *Q*(*Φ|**Φ^((p))*) where *Q(Φ''|Φ)* is the expected value
    of *log f(x|Φ'')*'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**E步**：计算*Q*(*Φ|**Φ^((p))*)，其中*Q(Φ''|Φ)*是*log f(x|Φ'')*的期望值。'
- en: '**M-step**: Choose *Φ^((p+1))* to be a value of *Φ ∈ Ω* that maximizes *Q*(*Φ|**Φ^((p))*)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**M步**：选择*Φ^((p+1))*为*Φ ∈ Ω*中的一个值，以最大化*Q*(*Φ|**Φ^((p))*)。'
- en: 'Let''s now define the terms needed for 2D HMMs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义二维HMM所需的术语：
- en: The set of observed feature vectors for the entire image is ![](img/5baf8d28-1c1c-43e3-b639-1916db29e96b.png)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个图像的观察特征向量集是！[](img/5baf8d28-1c1c-43e3-b639-1916db29e96b.png)
- en: The set of states for the image is ![](img/36684bb8-35ff-45fc-8c9a-e5befbb557c7.png)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的状态集是！[](img/36684bb8-35ff-45fc-8c9a-e5befbb557c7.png)
- en: The set of classes for the image is ![](img/8fb5bae3-a5db-426e-b073-c21269e90748.png)
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像的类别集是！[](img/8fb5bae3-a5db-426e-b073-c21269e90748.png)
- en: The mapping from a state *s[i,j]* to its class is C(*s[i,j]*), and the set of
    classes mapped from states *s*, is denoted by *C(s)*
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从状态*s[i,j]*到其类别的映射为C(*s[i,j]*)，从状态*s*映射到类别的集合记作*C(s)*。
- en: 'Now let''s define the distribution over *x* for a 2D HMM, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个二维HMM中*x*的分布，如下所示：
- en: '![](img/c7bb7c16-b1fc-4f19-a125-347c08dc501e.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7bb7c16-b1fc-4f19-a125-347c08dc501e.png)'
- en: 'From this, we can compute the following *log f(x|Φ'')*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个，我们可以计算出以下*log f(x|Φ')*：
- en: '![](img/dfc8e4d5-0475-46bf-ad04-c6ad42bf944b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfc8e4d5-0475-46bf-ad04-c6ad42bf944b.png)'
- en: 'We now know that, given *y*, *x* can only have a finite number of values corresponding
    to states that are consistent with the value of *y*. Therefore, the distribution
    over *x* is given as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道，给定*y*，*x*只能有有限个与*y*的值一致的状态。因此，*x*的分布如下所示：
- en: '![](img/d343cd48-215c-40bb-bc1b-6b457581740f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d343cd48-215c-40bb-bc1b-6b457581740f.png)'
- en: 'Here, *α* is the normalizing constant and *I* is the indicator function. Now,
    for the M-step, we need to set the value of *Φ^((p+1))* to *Φ''*, which will maximize
    the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*α* 是归一化常数，*I* 是指示函数。现在，对于M步，我们需要将*Φ^((p+1))*的值设为*Φ'*，以最大化以下公式：
- en: '![](img/9f08f7a9-b0b0-47e3-827d-eedceb60bd93.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f08f7a9-b0b0-47e3-827d-eedceb60bd93.png)'
- en: 'Since the previous term has two parts with an addition between them, we can
    deal with each term separately, since we are trying to maximize the total. Consider
    the first term:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的项有两部分相加，我们可以分别处理每一项，因为我们正在尝试最大化总值。考虑第一项：
- en: '![](img/5e4cdb78-25e4-424a-9d1a-7d3f0dba5af2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e4cdb78-25e4-424a-9d1a-7d3f0dba5af2.png)'
- en: 'By defining ![](img/3e673b47-f5a0-4471-b1eb-e3e60933e5f0.png), the preceding
    equation can be reduced to this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定义！[](img/3e673b47-f5a0-4471-b1eb-e3e60933e5f0.png)，前面的方程可以简化为：
- en: '![](img/06a06dbc-eea3-4365-8b99-d67e16aca866.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06a06dbc-eea3-4365-8b99-d67e16aca866.png)'
- en: 'This term is concave in ![](img/16ae97f5-a5a0-4366-8677-b50ddae3bb22.png);
    therefore, using the Lagrangian multiplier and taking the derivative we get the
    following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 该项在![](img/16ae97f5-a5a0-4366-8677-b50ddae3bb22.png)中是凹函数；因此，使用拉格朗日乘数并对其求导，我们得到以下结果：
- en: '![](img/d976faaa-5f1f-4853-9b72-8ab8a8016870.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d976faaa-5f1f-4853-9b72-8ab8a8016870.png)'
- en: 'Coming back to maximizing the second term of ![](img/51b62008-0348-43ac-ac06-db11ff691a62.png):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回到最大化![](img/51b62008-0348-43ac-ac06-db11ff691a62.png)的第二项：
- en: '![](img/d04d67ed-25f4-426c-9100-f30ef17b88f9.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d04d67ed-25f4-426c-9100-f30ef17b88f9.png)'
- en: 'Again, to simplify the preceding equation, we define ![](img/0bdaf41a-7daa-4727-a14e-b46a5593304a.png),
    and the preceding term becomes this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 再次为了简化前面的方程，我们定义![](img/0bdaf41a-7daa-4727-a14e-b46a5593304a.png)，前面的项变成了如下形式：
- en: '![](img/264efbef-64f4-4c7c-a4b4-0928a4a1f95e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/264efbef-64f4-4c7c-a4b4-0928a4a1f95e.png)'
- en: 'In this case, our ML estimates for our Gaussian distribution are given by the
    following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的高斯分布的最大似然估计（ML估计）如下所示：
- en: '![](img/94e685d6-6ba3-4b77-abc9-2fae6b4ef39c.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94e685d6-6ba3-4b77-abc9-2fae6b4ef39c.png)'
- en: 'To summarize the whole derivation, we can write the EM algorithm in the following
    two steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结整个推导过程，我们可以将EM算法写成以下两个步骤：
- en: 'Given the model estimation *Φ^((p))*, the parameters are updated as follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定模型估计*Φ^((p))*, 参数更新如下所示：
- en: '![](img/31e11a3a-7e50-48eb-a615-c011fe7eadfe.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31e11a3a-7e50-48eb-a615-c011fe7eadfe.png)'
- en: 'Here, the term ![](img/426b5f7c-b95f-43fc-9c79-7e443ca525bf.png) can be computed
    as this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，项![](img/426b5f7c-b95f-43fc-9c79-7e443ca525bf.png)可以按以下方式计算：
- en: '![](img/854f7887-920d-4a60-97d7-49950b0712b7.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/854f7887-920d-4a60-97d7-49950b0712b7.png)'
- en: 'The transition probability is updated as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转移概率的更新如下所示：
- en: '![](img/e8e8ecf5-7202-414f-ab0d-a21ba88ebe43.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8e8ecf5-7202-414f-ab0d-a21ba88ebe43.png)'
- en: 'And, here, ![](img/4e68c128-e7df-4413-abe4-42050f931fdc.png) is calculated
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/4e68c128-e7df-4413-abe4-42050f931fdc.png)的计算方法如下：
- en: '![](img/b7250355-067f-4f1c-9d82-d2c4e2e13eff.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7250355-067f-4f1c-9d82-d2c4e2e13eff.png)'
- en: By applying the preceding two equations iteratively, our algorithm will converge
    to the maximum likelihood estimation of the parameters of the model.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迭代应用前面两个方程，我们的算法将收敛到模型参数的最大似然估计。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter  we started with a short recap of 1D HMMs which we introduced in
    the previous chapter. Later we introduced the concepts of 2D HMMs and derived
    the various assumptions that we make for 2D HMMs to simplify our computations
    and it can be applied in image processing tasks. We then introduce a generic EM
    algorithm for learning the parameters in the case of 2D-HMMs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们首先回顾了上一章介绍的1D HMMs（隐马尔可夫模型）。随后，我们介绍了2D HMMs（二维隐马尔可夫模型）的概念，并推导了为了简化计算，我们对2D
    HMMs所做的各种假设，这些假设可以应用于图像处理任务。接着，我们介绍了一种通用的EM算法，用于学习2D-HMMs中的参数。
- en: In the next chapter, we will look at another application of HMMs in the field
    of reinforcement learning and will introduce MDP.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨HMMs在强化学习领域中的另一种应用，并介绍MDP（马尔可夫决策过程）。
