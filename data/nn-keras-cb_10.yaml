- en: Text Analysis Using Word Vectors
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单词向量进行文本分析
- en: In the previous chapter, we learned about encoding an image or encoding users
    or movies for recommender systems, where the items that are similar have similar
    vectors. In this chapter, we will be discussing how to encode text data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何对图像、用户或电影进行编码以用于推荐系统，在这些系统中，相似的物品有相似的向量。在本章中，我们将讨论如何对文本数据进行编码。
- en: 'You will be learning about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习以下主题：
- en: Building a word vector from scratch in Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零开始在Python中构建单词向量
- en: Building a word vector using skip-gram and CBOW models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用skip-gram和CBOW模型构建单词向量
- en: Performing vector arithmetic using pre-trained word vectors
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的单词向量进行向量运算
- en: Creating a document vector
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建文档向量
- en: Building word vectors using fastText
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用fastText构建单词向量
- en: Building word vectors using GloVe
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GloVe构建单词向量
- en: Building sentiment classification using word vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单词向量构建情感分类
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the traditional approach of solving text-related problems, we would one-hot
    encode the word. However, if the dataset has thousands of unique words, the resulting
    one-hot-encoded vector would have thousands of dimensions, which is likely to
    result in computation issues. Additionally, similar words will not have similar
    vectors in this scenario. Word2Vec is an approach that helps us to achieve similar
    vectors for similar words.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的解决与文本相关的问题的方法中，我们会进行一-hot编码。然而，如果数据集包含成千上万个唯一的单词，那么得到的一-hot编码向量将有成千上万的维度，这可能会导致计算问题。此外，在这种情况下，相似的单词并不会拥有相似的向量。Word2Vec是一种方法，帮助我们为相似的单词获得相似的向量。
- en: To understand how Word2Vec is useful, let's explore the following problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Word2Vec如何有用，我们来探讨以下问题。
- en: 'Let''s say we have two input sentences:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个输入句子：
- en: '![](img/2ffc33e5-9913-45da-96d2-64b6c54b31b6.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ffc33e5-9913-45da-96d2-64b6c54b31b6.png)'
- en: 'Intuitively, we know that **enjoy** and **like** are similar words. However,
    in traditional text mining, when we one-hot encode the words, our output looks
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上看，我们知道**enjoy**（享受）和**like**（喜欢）是相似的单词。然而，在传统的文本挖掘中，当我们对这些单词进行一-hot编码时，输出如下所示：
- en: '![](img/06066bdf-95ff-41d5-85cb-e1789acc850e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06066bdf-95ff-41d5-85cb-e1789acc850e.png)'
- en: Notice that one-hot encoding results in each word being assigned a column. The
    major issue with one-hot encoding such as this is that the Eucledian distance
    between **I** and **enjoy** is the same as the Eucledian distance between **enjoy**
    and **like**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到一-hot编码会将每个单词分配到一个列中。这样的一-hot编码的主要问题是，**I**和**enjoy**之间的欧几里得距离与**enjoy**和**like**之间的欧几里得距离是相同的。
- en: However, intuitively, we know that the distance between **enjoy** and **like**
    should be lower than the distance between **I** and **enjoy**, as **enjoy** and
    **like** are similar to each other.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从直观上看，我们知道**enjoy**和**like**之间的距离应该比**I**和**enjoy**之间的距离要小，因为**enjoy**和**like**是彼此相似的。
- en: Building a word vector from scratch in Python
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始在Python中构建单词向量
- en: The principle based on which we'll build a word vector is *related words will
    have similar words surrounding them*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建单词向量的原理是*相关的单词周围会有相似的词*。
- en: 'For example: the words *queen* and *princess* will have similar words (related
    to a *kingdom*) around them more frequently. In a way, the context (surrounding
    words) of these words would be similar.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：*queen*（皇后）和*princess*（公主）这两个单词会更频繁地拥有与*kingdom*（王国）相关的词汇在它们周围。从某种程度上说，这些单词的上下文（周围的词汇）是相似的。
- en: Getting ready
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Our dataset (of two sentences) looks as follows when we take the surrounding
    words as input and the remaining (middle) word as output:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将周围的单词作为输入，中间的单词作为输出时，我们的数据集（包含两个句子）如下所示：
- en: '![](img/d5a3f1d0-5066-49a1-80a8-70c53b0979e8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5a3f1d0-5066-49a1-80a8-70c53b0979e8.png)'
- en: 'Notice that we are using the middle word as output and the remaining words
    as input. A vectorized form of this input and output looks as follows (recall
    the way in which we converted a sentence into a vector in the *Need for encoding
    in text analysis* section in [Chapter 9](18e82d39-d5b2-40fe-ae2f-df222c2e1ffe.xhtml), *Encoding
    Input*):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是使用中间词作为输出，其余的词作为输入。这个输入和输出的向量化形式如下所示（回想一下我们在[第9章](18e82d39-d5b2-40fe-ae2f-df222c2e1ffe.xhtml)的*文本分析中编码的必要性*部分中，*输入编码*部分，如何将一个句子转换成向量）：
- en: '![](img/17715c08-bfa2-4f12-a2dd-c8b30b67a2cc.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17715c08-bfa2-4f12-a2dd-c8b30b67a2cc.png)'
- en: Notice that the vectorized form of input in the first row is *{0, 1, 1, 1, 0}*,
    as the input word index is *{1, 2, 3}*, and the output is *{1, 0, 0, 0, 0}* as
    the output word's index is *{1}*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一行中输入的向量化形式是*{0, 1, 1, 1, 0}*，因为输入词的索引是*{1, 2, 3}*，输出是*{1, 0, 0, 0, 0}*，因为输出词的索引是*{1}*。
- en: 'In such a scenario, our hidden layer has three neurons associated with it.
    Our neural network would look as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的隐藏层有三个神经元与之相关联。我们的神经网络将如下所示：
- en: '![](img/536560c0-abd1-4c24-8e19-5e09c079e68f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/536560c0-abd1-4c24-8e19-5e09c079e68f.png)'
- en: 'The dimensions of each layer are as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的维度如下：
- en: '| **Layer** | **Shape of weights** | **Commentary** |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **层** | **权重形状** | **备注** |'
- en: '| Input layer | 1 x 5 | Each row is multiplied by five weights. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 输入层 | 1 x 5 | 每一行与五个权重相乘。 |'
- en: '| Hidden layer | 5 x 3 | There are five input weights each to the three neurons
    in the hidden layer. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层 | 5 x 3 | 每五个输入权重分别连接到隐藏层中的三个神经元。 |'
- en: '| Output of hidden layer | 1 x 3 | This is the matrix multiplication of the
    input and the hidden layer. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 隐藏层输出 | 1 x 3 | 这是输入和隐藏层的矩阵乘法。 |'
- en: '| Weights from hidden to output | 3 x 5 | Three output hidden units are mapped
    to five output columns (as there are five unique words). |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 从隐藏层到输出层的权重 | 3 x 5 | 三个输出隐藏单元被映射到五个输出列（因为有五个独特的词）。 |'
- en: '| Output layer | 1 x 5 | This is the matrix multiplication between the output
    of the hidden layer and the weights from the hidden to the output layer. |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 输出层 | 1 x 5 | 这是隐藏层输出与从隐藏层到输出层权重的矩阵乘法。 |'
- en: Note that we would not be applying activation on top of the hidden layer while
    building a word vector.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在构建词向量时，我们不会对隐藏层进行激活处理。
- en: The output layer's values are not restricted to a specific range. Hence, we
    pass them through the softmax function so that we arrive at the probability of
    words. Furthermore, we minimize the cross-entropy loss to arrive at the optimal
    weight values across the network. Now, the word vector of a given word is the
    hidden-layer unit values when the input is the one-hot encoded version of the
    word (not the input sentence).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的值没有限制在特定范围内。因此，我们通过softmax函数将其转换为词的概率。此外，我们最小化交叉熵损失，以便在整个网络中得到最优的权重值。现在，给定词的词向量就是当输入是该词的一热编码版本时，隐藏层单元的值（而不是输入句子）。
- en: How to do it...
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Now that we know how word vectors are generated, let''s code up the process
    of generating word vectors (the code file is available as `Word_vector_generation.ipynb`
    in GitHub):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何生成词向量，接下来我们在GitHub中编写生成词向量的过程（代码文件为`Word_vector_generation.ipynb`）：
- en: 'Define the sentences of interest:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义感兴趣的句子：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: From the preceding, we should expect the word vectors of `enjoy` and `like`
    to be similar, as the words around `enjoy` and `like` are exactly the same.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的内容可以预期，`enjoy`和`like`的词向量应该相似，因为它们周围的词是完全相同的。
- en: 'Let''s now create the one-hot encoded version of each sentence:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们创建每个句子的独热编码版本：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that vectorizer defines the parameters that convert a document into a vector
    format. Additionally, we pass in more parameters so that words such as `I` do
    not get filtered out in the `CountVectorizer`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，vectorizer定义了将文档转换为向量格式的参数。此外，我们传入更多的参数，以确保像`I`这样的词不会在`CountVectorizer`中过滤掉。
- en: Furthermore, we will fit our documents to the defined vectorizer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将把文档适配到定义的vectorizer中。
- en: 'Transform the documents into a vector format:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文档转换为向量格式：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Validate the transformations performed:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证所做的转换：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](img/5a5b6dc6-d127-48fa-b65e-75253bde3f66.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a5b6dc6-d127-48fa-b65e-75253bde3f66.png)'
- en: Note that `vocabulary_` returns the index of various words, and that converting
    the `toarray` vector returns the one-hot encoded version of sentences.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`vocabulary_`返回各种词的索引，并且转换后的`toarray`向量返回句子的独热编码版本。
- en: 'Create the input and the output dataset:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'From the preceding code, we have created the input and output datasets. Here
    is the input dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码，我们已经创建了输入和输出数据集。这里是输入数据集：
- en: '![](img/0e9852c5-b6c2-45ca-a41f-8ad7a8f4a1d2.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e9852c5-b6c2-45ca-a41f-8ad7a8f4a1d2.jpg)'
- en: 'And here is the output dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出数据集：
- en: '![](img/7269b7b9-b223-4ba1-b9cf-0103abd330da.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7269b7b9-b223-4ba1-b9cf-0103abd330da.png)'
- en: 'Transform the preceding input and output words into vectors:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前述的输入和输出词转换为向量：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the input array:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输入数组：
- en: '![](img/4ac45077-8dbc-41aa-b6b8-5ad81fbd5a88.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ac45077-8dbc-41aa-b6b8-5ad81fbd5a88.png)'
- en: 'Here is the output array:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出数组：
- en: '![](img/98d56a01-487d-4a36-8ae1-f040f3cf8a1e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98d56a01-487d-4a36-8ae1-f040f3cf8a1e.png)'
- en: 'Define the neural network model that maps the input and output vector with
    a hidden layer that has three units:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个神经网络模型，该模型通过一个包含三个单元的隐藏层来映射输入和输出向量：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Compile and fit the model:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并拟合模型：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Extract the word vectors by fetching the intermediate layer values where the
    inputs are the vectors of each individual word (not a sentence):'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提取中间层的值来提取词向量，输入为每个单独词的向量（而不是一个句子）：
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, we are extracting the output from the layer we are interested
    in: a layer named `dense_5` in the model we initialized.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们从我们感兴趣的层提取输出：在我们初始化的模型中，名为`dense_5`的层。
- en: 'In the code below, we are extracting the output of intermediate layer when
    we pass the one-hot-encoded version of the word as input:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们在传递词的一次性编码版本作为输入时提取中间层的输出：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The word vectors of individual words are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 单个词的词向量如下：
- en: '![](img/d0f17447-2ac1-4512-b2be-0c821dd11b4e.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0f17447-2ac1-4512-b2be-0c821dd11b4e.png)'
- en: Note that the words `enjoy` and `like` are more correlated to each other than
    others are and hence a better representation of word vectors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*享受*和*喜欢*之间的相关性比其他词更强，因此它们更好地表示词向量。
- en: The name could be different for the model you run, as we did not specify the
    layer name in our model build. Also, the layer name changes for every new run
    of model initialization when we do not explicitly specify the model name in the
    layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在构建模型时没有指定层名称，因此您运行的模型的名称可能不同。此外，在我们没有明确指定模型名称的情况下，每次初始化模型时，层名称都会发生变化。
- en: Measuring the similarity between word vectors
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量词向量之间的相似度
- en: 'The similarity between word vectors could be measured using multiple metrics—here
    are two of the more common ones:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量之间的相似度可以通过多种度量方法来测量——以下是两种常见的度量方法：
- en: Cosine similarity
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Eucledian distance
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'The cosine similarity between two different vectors, *A* and *B*, is calculated
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 两个不同向量，*A*和*B*之间的余弦相似度计算如下：
- en: '![](img/cf0c3808-dfe4-473c-be3a-b0fd82932a7e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf0c3808-dfe4-473c-be3a-b0fd82932a7e.png)'
- en: 'In the example in previous section, the cosine similarity between *enjoy* and
    *like* is calculated as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的例子中，*享受*和*喜欢*之间的余弦相似度计算如下：
- en: '*enjoy = (-1.43, -0.94, -2.49)*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*享受 = (-1.43, -0.94, -2.49)*'
- en: '*like     = (-1.43, -0.94, -2.66)*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*喜欢 = (-1.43, -0.94, -2.66)*'
- en: 'Here is the similarity between the *enjoy* and *like* vectors:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是*享受*和*喜欢*向量之间的相似度：
- en: '*(-1.43*-1.43 + -0.94*-0.94 +-2.49*-2.66)/ sqrt((-1.43)² + (-0.94)² + (-2.49)²)*
    sqrt((-1.43)^2 + (-0.94)^2 + (-2.66)^2) = 0.99*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*(-1.43*-1.43 + -0.94*-0.94 +-2.49*-2.66)/ sqrt((-1.43)² + (-0.94)² + (-2.49)²)*
    sqrt((-1.43)^2 + (-0.94)^2 + (-2.66)^2) = 0.99*'
- en: 'The Eucledian distance between two different vectors, *A* and *B*, is calculated
    as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 两个不同向量，*A*和*B*之间的欧几里得距离计算如下：
- en: '*distance = sqrt(A-B)^2*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*distance = sqrt(A-B)^2*'
- en: '*= sqrt((-1.43 - (-1.43))^2 + (-0.94 - (-0.94))^2 + (-2.49 - (-2.66))^2)*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*= sqrt((-1.43 - (-1.43))^2 + (-0.94 - (-0.94))^2 + (-2.49 - (-2.66))^2)*'
- en: '*= 0.03*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.03*'
- en: Building a word vector using the skip-gram and CBOW models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用skip-gram和CBOW模型构建词向量
- en: In the previous recipe, we built a word vector. In this recipe, we'll build
    skip-gram and CBOW models using the `gensim` library.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们构建了一个词向量。在本食谱中，我们将使用`gensim`库构建skip-gram和CBOW模型。
- en: Getting ready
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The method that we have adopted to build a word vector in this recipe is called
    a **continuous bag of words** (**CBOW**) model. The reason it is called as CBOW
    is explained as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本例中采用的方法来构建词向量称为**连续词袋模型**（**CBOW**）。之所以称为CBOW，解释如下：
- en: 'Let''s use this sentence as an example: *I enjoy playing TT*.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以这句话为例：*我享受玩TT*。
- en: 'Here''s how the CBOW model handles this sentence:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是CBOW模型处理此句子的方式：
- en: Fix a window of certain size—let's say 1.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 固定一个大小为1的窗口。
- en: By specifying the window size, we are specifying the number of words that will
    be considered to the right as well as to the left of the given word.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过指定窗口大小，我们确定了给定词的左右两边将被考虑的词的数量。
- en: 'Given the window size, the input and output vectors would look as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定窗口大小，输入和输出向量将如下所示：
- en: '| **Input words** | **Output word** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **输入词** | **输出词** |'
- en: '| *{I, playing}* | *{enjoy}* |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| *{我, 玩}* | *{享受}* |'
- en: '| *{enjoy,TT}* | *{playing}* |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| *{享受,TT}* | *{玩}* |'
- en: 'Another approach to building a word vector is the skip-gram model, where the
    preceding step is reversed, as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种构建词向量的方法是skip-gram模型，其中之前的步骤被反转，如下所示：
- en: '|  **Input words** | **Output word** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **输入词** | **输出词** |'
- en: '| *{enjoy}* | *{I, playing}* |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| *{enjoy}* | *{I, playing}* |'
- en: '| *{playing}* | *{enjoy, TT}* |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| *{playing}* | *{enjoy, TT}* |'
- en: The approach to arrive at the hidden layer values of a word remains the same
    as we discussed in previous section regardless of whether it is a skip-gram model
    or a CBOW model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是skip-gram模型还是CBOW模型，得到单词隐藏层值的方法都是相同的，正如我们在前面的部分讨论过的那样。
- en: How to do it
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现：
- en: 'Now that we understand the backend of how a word vector gets built, let''s
    build word vectors using the skip-gram and CBOW models. To build the model, we
    will be using the airline sentiment dataset, where tweet texts are given and the
    sentiments corresponding to the tweets are provided. To generate word vectors,
    we will be using the `gensim` package, as follows (the code file is available
    as `word2vec.ipynb` in GitHub):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了单词向量的构建后台工作，让我们使用skip-gram和CBOW模型来构建单词向量。为了构建模型，我们将使用航空公司情感数据集，其中给出了推文文本以及对应的情感。为了生成词向量，我们将使用`gensim`包，如下所示（代码文件在GitHub上可用，文件名为`word2vec.ipynb`）：
- en: 'Install the `gensim` package:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装`gensim`包：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Import the relevant packages:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Read the airline tweets sentiment dataset, which contains comments (text) related
    to airlines and their corresponding sentiment. The dataset can be obtained from [https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv](https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv):'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取航空公司推文情感数据集，其中包含与航空公司相关的评论（文本）及其相应的情感。数据集可以从[https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv](https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv)获取：
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'A sample of the dataset looks as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集样本如下所示：
- en: '![](img/7cdad718-62b5-41b1-8158-947dd9313b85.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cdad718-62b5-41b1-8158-947dd9313b85.png)'
- en: 'Preprocess the preceding text to do the following:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对前面的文本进行预处理，执行以下操作：
- en: Normalize every word to lower case.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个单词标准化为小写。
- en: Remove punctuation and retain only numbers and alphabets.
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号，仅保留数字和字母。
- en: 'Remove stop words:'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Split sentences into a list of tokens so that they can then be passed to `gensim`.
    The output of the first sentence should look as follows:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句子拆分为一个词汇表（tokens）的列表，这样它们就可以传递给`gensim`。第一句的输出应如下所示：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The above code splits the sentence by space and thus looks as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码通过空格分割句子，结果如下所示：
- en: '![](img/20e6f8c6-be19-41b0-a33e-6c2574611a1d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20e6f8c6-be19-41b0-a33e-6c2574611a1d.png)'
- en: 'We will loop through all the text we have and append it in a list, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历所有文本并将其附加到一个列表中，如下所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s inspect the first three lists within the list of lists:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看列表中的前三个子列表：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The list of lists of the first three sentences is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前三句话的词汇表如下所示：
- en: '![](img/b06668c0-b3d5-4475-afce-4f836f7cf292.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b06668c0-b3d5-4475-afce-4f836f7cf292.png)'
- en: 'Build the `Word2Vec` model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建`Word2Vec`模型：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Define the vector size, context window size to look into, and the minimum count
    of a word for it to be eligible to have a word vector, as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 定义词向量的维度、上下文窗口的大小以及词汇的最小计数要求，作为它有资格拥有词向量的标准，如下所示：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, `size` represents the size (dimension) of word vectors,
    window represents the context size of words that would be considered, `min_count`
    specifies the minimum frequency based on which a word is considered, `sg` represents
    whether skip-gram would be used (when `sg=1`) or CBOW (when `sg = 0`) would be
    used, and alpha represents the learning rate of the model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`size`表示词向量的大小（维度），`window`表示考虑的上下文词汇的大小，`min_count`指定词汇出现的最小频率，`sg`表示是否使用skip-gram（当`sg=1`时）或使用CBOW（当`sg=0`时），`alpha`表示模型的学习率。
- en: 'Once the model is defined, we will pass our list of lists to build a vocabulary,
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型定义完成，我们将传递我们的列表来构建词汇表，如下所示：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the vocabulary is built, the final words that would be left after filtering
    out the words that occur fewer than 30 times in the whole corpus can be found
    as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建好词汇表，过滤掉在整个语料库中出现次数少于30次的词语，剩余的最终词汇如下所示：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Train the model by specifying the total number of examples (lists) that need
    to be considered and the number of epochs to be run, as follows:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过指定需要考虑的示例（列表）总数以及运行的迭代次数（epochs）来训练模型，如下所示：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In the preceding code, `list_words` (the list of words) is the input, `total_examples`
    represents the total number of lists to be considered, and epochs is the number
    of epochs to be run.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`list_words`（单词列表）是输入，`total_examples`表示要考虑的列表总数，`epochs`是运行的训练轮数。
- en: 'Alternatively, you can also train the model by specifying the `iter` parameter
    in the `Word2Vec` method, as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以通过在`Word2Vec`方法中指定`iter`参数来训练模型，具体如下：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Extract the word vectors of a given word (`month`), as follows:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取给定词（`month`）的词向量，具体如下：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The word vector corresponding to the word "month" is as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对应"month"这个词的词向量如下：
- en: '![](img/074b3142-43d7-44e8-9b3d-0875022063eb.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/074b3142-43d7-44e8-9b3d-0875022063eb.png)'
- en: 'The similarity between two words can be calculated as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 两个词之间的相似度可以按以下方式计算：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The words that are most similar to a given word is calculated as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 给定词与最相似的词是通过以下方式计算的：
- en: '[PRE25]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The most similar words of the word `month` are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`month`这个词最相似的词如下：'
- en: '![](img/d45f1891-0a22-4ffe-b943-338221155ccd.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d45f1891-0a22-4ffe-b943-338221155ccd.png)'
- en: Note that, while these similarities look low and some of the most similar words
    do not look intuitive, it will be more realistic once we train on a huge dataset
    than the 11,000-tweet dataset that we have.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管这些相似度看起来较低，有些最相似的词也不直观，但一旦我们在比我们现有的11,000条推文数据集更大的数据集上进行训练，结果会更加真实。
- en: 'In the preceding scenario, let''s see the output of most similar words to the
    word "month", when we run the model for a few number of epochs:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述场景中，运行模型若干轮后，看看与"month"这个词最相似的词是什么：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The most similar words to the word "month" are as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与"month"最相似的词如下：
- en: '![](img/ccf6b20e-4013-4fc5-aea4-643a97f97cb9.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccf6b20e-4013-4fc5-aea4-643a97f97cb9.png)'
- en: We can see that if we have few epochs, the most similar words to the word `month`
    are not intuitive while the results are intuitive when there are many epochs,
    particularly as the weights are not fully optimized for few epochs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，如果训练轮数较少，与`month`最相似的词不太直观，而当训练轮数较多时，结果更具直观性，特别是因为在训练轮数少的情况下，权重没有完全优化。
- en: The same operations can be replicated for skip-gram by replacing the value of
    the `sg` parameter with `1`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`sg`参数的值设置为`1`，可以将相同的操作应用于skip-gram。
- en: Performing vector arithmetic using pre-trained word vectors
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的词向量进行向量运算
- en: In the previous section, one of the limitations that we saw is that the number
    of sentences is too small for us to build a model that is robust (we saw that
    the correlation of month and year is around 0.4 in the previous section, which
    is relatively low, as they belong to the same type of words).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们看到的一个限制是，句子的数量太少，无法构建一个强健的模型（我们在前一节中看到，month和year之间的相关性大约为0.4，比较低，因为它们属于相同类型的词）。
- en: To overcome this scenario, we will use the word vectors trained by Google. The
    pre-trained word vectors from Google include word vectors for a vocabulary of
    3,000,000 words and phrases that were trained on words from Google News dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这种情况，我们将使用由Google训练的词向量。Google提供的预训练词向量包括一个包含3,000,000个单词和短语的词汇表，这些词向量是在Google新闻数据集上的单词上训练的。
- en: How to do it...
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Download the pre-trained word vectors from Google News (the code file is available
    as `word2vec.ipynb` in GitHub):'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Google News下载预训练的词向量（代码文件可以在GitHub上作为`word2vec.ipynb`获取）：
- en: '[PRE27]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Unzip the downloaded file:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 解压下载的文件：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This command unzips the `bin` file, which is the saved version of the model.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令解压`bin`文件，该文件是模型的保存版本。
- en: 'Load the model:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载模型：
- en: '[PRE29]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Load the  most similar words to the given word, `month`:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载与给定词`month`最相似的词：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The words that are most similar to `month` are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与`month`最相似的词如下：
- en: '![](img/df73a48b-f995-4ad3-98a2-97506f9bad8d.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df73a48b-f995-4ad3-98a2-97506f9bad8d.png)'
- en: 'We will perform vector arithmetic; that is, we will try to answer the following analogy:
    woman is to man as what is to king? Check out the following code:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将执行向量运算；也就是说，我们将尝试回答以下类比问题：woman与man的关系，什么与king的关系最相似？请查看以下代码：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output of above arithmetic is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 上述算式的输出结果如下：
- en: '![](img/568bc06c-e395-4066-895d-74c8d070fd04.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/568bc06c-e395-4066-895d-74c8d070fd04.png)'
- en: In this scenario, the word vector of `woman` is subtracted from the word vector
    of `man` and added to the word vector of `king` – resulting in a vector that is
    closest to the word `queen`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，将`woman`的词向量从`man`的词向量中减去，并将其加到`king`的词向量中，从而得到一个最接近`queen`的词向量。
- en: Creating a document vector
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建文档向量
- en: To understand the reason for having a document vector, let's go through the
    following intuition.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解文档向量的存在原因，我们来梳理一下以下的直觉。
- en: The word *bank* is used in the context of finance and also in the context of
    a river. How do we identify whether the word *bank* in the given sentence or document
    is related to the topic of a river or the topic of finance?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 单词*bank*在金融和河流两个语境中都有使用。我们如何识别给定句子或文档中的*bank*是与河流主题相关，还是与金融主题相关呢？
- en: 'This problem could be solved by adding a document vector, which works in a
    similar way to word-vector generation but with the addition of a one-hot encoded
    version of the paragraph ID, as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过添加文档向量来解决，这与单词向量的生成方式类似，但在此基础上加入了段落ID的一热编码版本，如下所示：
- en: '![](img/d1f47a77-220b-459a-9c00-2e9d68fb81d9.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1f47a77-220b-459a-9c00-2e9d68fb81d9.png)'
- en: In the preceding scenario, the paragraph ID encompasses the delta that is not
    captured by just the words. For example, in the sentence *on the bank of river*
    where *on the bank of* is the input and *river* is the output, the words *on,
    the*, and *of* do not contribute to the prediction as they are frequently-occurring
    words, while the word *bank* confuses the output prediction to be either river
    or America. The document ID of this particular document/sentence will help to
    identify whether the document is related to the topic of rivers or to the topic
    of finance. This model is called the **Distributed Memory Model of Paragraph Vectors**
    (**PV-DM**).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述场景中，段落ID包含了那些仅通过单词无法捕捉到的差异。例如，在句子*on the bank of river*中，*on the bank of*是输入，*river*是输出，单词*on,
    the*和*of*由于是高频词，因此不会对预测产生贡献，而单词*bank*则使得输出预测变得模糊，可能是河流或是美国。这个特定文档/句子的文档ID将帮助识别该文档是与河流相关，还是与金融相关。这个模型称为**段落向量的分布式记忆模型**（**PV-DM**）。
- en: For example, if the number of documents is 100, the one-hot encoded version
    of the paragraph ID will be 100-dimensional. Similarly, if the number of unique
    words that meet the minimum frequency of a word is 1,000, the one-hot encoded
    version of the words is 1,000 in size. When the hidden-layer size (which is the
    word vector size) is 300, the total number of parameters would be 100 * 300 +
    1,000 * 300 = 330,000
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果文档数量为100，则段落ID的一热编码版本将是100维的。同样，如果符合最小频率的唯一单词数量为1,000，则这些单词的一热编码版本将是1,000维的。当隐藏层的大小（即单词向量大小）为300时，参数的总数将是100
    * 300 + 1,000 * 300 = 330,000。
- en: The document vector would be the value of the hidden layer when the one-hot
    encoded versions of all input words are 0 (that is, the effect of the words is
    neutralized and only the effect of the document/paragraph ID is considered).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 文档向量将是当所有输入单词的一热编码版本为0时的隐藏层值（即，单词的影响被中和，仅考虑文档/段落ID的影响）。
- en: 'Similar to the way in which input and output switch between the skip-gram and
    CBOW models, even for a document vector, the output and input can be switched
    as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于输入和输出在skip-gram和CBOW模型中相互转换的方式，即使是文档向量，输出和输入也可以按照如下方式交换：
- en: '![](img/4cc8cbac-3d82-4fe8-ad28-0b5ac55e1631.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cc8cbac-3d82-4fe8-ad28-0b5ac55e1631.png)'
- en: This representation of the model is called a **paragraph vector with a distributed
    bag of words** (**PVDBOW**).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的表示称为**段落向量与分布式词袋模型**（**PVDBOW**）。
- en: Getting ready
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中
- en: 'The strategy that we''ll adopt to build a document vector is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建文档向量的策略如下：
- en: Preprocess the input sentences to remove punctuation as well as lowercasing
    for all words, and remove the stop words (words that occur very frequently and
    do not add context to sentence, for example, *and* and *the*)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入句子进行预处理，去除标点符号，并将所有单词小写，同时移除停用词（如*and*和*the*等出现频率很高且不对句子提供上下文意义的词）。
- en: Tag each sentence with its sentence ID.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给每个句子标记上其句子ID。
- en: We are assigning an ID for each sentence.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为每个句子分配一个ID。
- en: Use the Doc2Vec method to extract vectors for document IDs as well as words.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Doc2Vec方法提取文档ID以及单词的向量。
- en: Train the Doc2Vec method over a high number of epochs, so that the model is
    trained.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在较多的训练周期中训练Doc2Vec方法，以便对模型进行训练。
- en: How to do it...
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'Now that we have the intuition of how a document vector gets generated and
    a strategy in place to build a document vector, let''s generate the document vectors
    of the airline tweets dataset (the code file is available as `word2vec.ipynb` in
    GitHub):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了文档向量是如何生成的，并且制定了构建文档向量的策略，接下来让我们生成航空公司推文数据集的文档向量（代码文件在 GitHub 上可用，名为`word2vec.ipynb`）：
- en: 'Import the relevant packages:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Preprocess the tweets'' text:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理推文文本：
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Create a dictionary of tagged documents where the document ID is generated
    along with the text (tweet):'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含标记文档的字典，其中文档 ID 会与文本（推文）一起生成：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The tagged document data looks as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 标记文档数据如下所示：
- en: '![](img/cfc4df1f-d5d2-4bbd-94a5-b9e08757d5c3.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfc4df1f-d5d2-4bbd-94a5-b9e08757d5c3.png)'
- en: In the preceding code, we are extracting a list of all the constituent words
    in a sentence (document).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们正在提取句子（文档）中所有组成词汇的列表。
- en: 'Initialize a model with parameters, as follows:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照如下方式初始化一个带有参数的模型：
- en: '[PRE35]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: In the preceding code snippet, `size` represents the vector size of the document,
    `alpha` represents the learning rate, `min_count` represents the minimum frequency
    for a word to be considered, and `dm = 1` represents the PV-DM
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，`size` 表示文档的向量大小，`alpha` 表示学习率，`min_count` 表示单词的最小频率，`dm = 1` 表示 PV-DM
- en: 'Build a vocabulary:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 构建词汇表：
- en: '[PRE36]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Train the model for a high number of epochs on the tagged data:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标记数据上训练模型，进行多次迭代：
- en: '[PRE37]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The training process would generate vectors for words as well as for the document/paragraph
    ID.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练过程会生成单词和文档/段落 ID 的向量。
- en: 'Word vectors can be fetched similarly to how we fetched them in the previous
    section, as follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量可以与上一节中提取的方式相似地获取，如下所示：
- en: '[PRE38]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Document vectors can be fetched as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按如下方式获取文档向量：
- en: '[PRE39]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The preceding code snippet generates snippets for the document vectors for the
    first document.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段生成了第一个文档的文档向量片段。
- en: 'Extract the most similar document to a given document ID:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取与给定文档 ID 最相似的文档：
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/45742cdd-3b4c-44a5-8459-8e4833b21286.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45742cdd-3b4c-44a5-8459-8e4833b21286.png)'
- en: In the preceding code snippet, we are extracting the document ID that is most
    similar to the document ID number 457, which is 827.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码片段中，我们正在提取与文档 ID 457 最相似的文档 ID，该 ID 为 827。
- en: 'Let''s look into the text of documents 457 and 827:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下文档 457 和 827 的文本：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![](img/87db2662-06c1-414d-a69d-25c9b5e0bacd.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87db2662-06c1-414d-a69d-25c9b5e0bacd.png)'
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/2c6c1158-93c8-4324-8de6-169ac90b85d4.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c6c1158-93c8-4324-8de6-169ac90b85d4.png)'
- en: If we inspect the vocabulary of the model, we would see that apart from the
    word `just`, all the other words occur between the two sentences—hence it is obvious
    that document ID `457` is most similar to document ID `827`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查模型的词汇表，我们会看到除了单词 `just`，所有其他单词都出现在两个句子之间——因此很明显，文档 ID `457` 与文档 ID `827`
    最相似。
- en: Building word vectors using fastText
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 fastText 构建词向量
- en: fastText is a library created by the Facebook Research Team for the efficient
    learning of word representations and sentence classification.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 是由 Facebook 研究团队创建的一个库，用于高效学习词表示和句子分类。
- en: fastText differs from word2vec in the sense that word2vec treats every single
    word as the smallest unit whose vector representation is to be found, but fastText
    assumes a word to be formed by a n-grams of character; for example, sunny is composed
    of *[sun, sunn, sunny]*,*[sunny, unny, nny]*, and so on, where we see a subset
    of the original word of size *n*, where *n* could range from *1* to the length
    of the original word.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: fastText 与 word2vec 的不同之处在于，word2vec 将每个单词视为最小的单位，其向量表示是要查找的，而 fastText 假设一个单词是由字符的
    n-gram 组成的；例如，sunny 由 *[sun, sunn, sunny]*，*[sunny, unny, nny]* 等组成，其中我们看到了原始单词的一个子集，大小为
    *n*，其中 *n* 可以从 *1* 到原始单词的长度。
- en: Another reason for the use of fastText would be that the words do not meet the
    minimum frequency cut-off in the skip-gram or CBOW models. For example, the word
    *appended* would not be very different than *append*. However, if *append* occurs
    frequently, and in the new sentence we have the word *appended* instead of *append*,
    we are not in a position to have a vector for *appended*. The n-gram consideration
    of fastText comes in handy in such a scenario.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 fastText 的另一个原因是，有些单词在 skip-gram 或 CBOW 模型中没有达到最小频率阈值。例如，单词 *appended* 与
    *append* 可能没有太大区别。然而，如果 *append* 出现频率较高，并且在新的句子中我们遇到的是 *appended* 而不是 *append*，那么我们就无法为
    *appended* 提供向量。在这种情况下，fastText 的 n-gram 考虑非常有用。
- en: Practically, fastText uses skip-gram/CBOW models, however, it augments the input
    dataset so that the unseen words are also taken into consideration.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，fastText 使用的是 skip-gram/CBOW 模型，但它增强了输入数据集，以便考虑到未见过的单词。
- en: Getting ready
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The strategy that we''ll adopt to extract word vectors using fastText is as
    follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的策略是使用 fastText 提取词向量，具体如下：
- en: Use the fastText method in the gensim library
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 gensim 库中的 fastText 方法
- en: Preprocess the input data
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理输入数据
- en: Break each input sentence into a list of lists
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个输入句子拆分成一个列表的列表
- en: Build a vocabulary on top of the input list of lists
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在输入的列表列表上构建词汇表
- en: Train the model with the preceding input data over multiple epochs
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前的输入数据训练模型，进行多次迭代
- en: Calculate the similarity between words
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算单词之间的相似度
- en: How to do it...
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In the following code, let''s look at how to generate word vectors using fastText
    (the code file is available as `word2vec.ipynb` in GitHub):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，让我们看看如何使用 fastText 生成词向量（代码文件在 GitHub 上的 `word2vec.ipynb` 中可用）：
- en: 'Import the relevant packages:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包：
- en: '[PRE43]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Preprocess and prepare the dataset into a list of lists, just like we did for
    the word2vec models:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理并将数据集准备为列表的列表，就像我们为 word2vec 模型所做的那样：
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In the preceding code, we are preprocessing the input text. Next, let''s convert
    the input text into a list of lists:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中，我们正在对输入文本进行预处理。接下来，让我们将输入文本转换成一个列表的列表：
- en: '[PRE45]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the model (specify the number of vectors per word) and build a vocabulary:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型（指定每个词的向量数）并构建词汇表：
- en: '[PRE46]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Train the model:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE47]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Check the word vectors of a word that is not present in the vocabulary of the
    model. For example, the word `first` is present in the vocabulary; however, the
    word `firstli` is not present in the vocabulary. In such a scenario, check the
    similarity between the word vectors for `first` and `firstli`:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型词汇表中不存在的单词的词向量。例如，单词 `first` 存在于词汇表中；然而，单词 `firstli` 不在词汇表中。在这种情况下，检查 `first`
    和 `firstli` 的词向量之间的相似度：
- en: '[PRE48]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output of the preceding code snippet is 0.97, which indicates a very high
    correlation between the two words.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 之前代码片段的输出是 0.97，表示这两个单词之间有很高的相关性。
- en: Thus, we can see that fastText word vectors help us to generate word vectors
    for words that are not present in the vocabulary.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到 fastText 词向量帮助我们生成词汇表中不存在的单词的词向量。
- en: The preceding method could also be leveraged to correct the spelling mistakes,
    if any, within our corpus of data, as the incorrectly-spelled words are likely
    to occur rarely, and the most similar word with the highest frequency is more
    likely to be the correctly-spelled version of the misspelled word.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法也可以用于修正数据集中可能存在的拼写错误，因为拼写错误的单词通常出现的频率较低，而与之最相似且频率最高的单词更可能是拼写正确的版本。
- en: 'Spelling corrections can be performed using vector arithmetic, as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写修正可以通过向量运算来执行，如下所示：
- en: '[PRE49]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note that in the preceding code, the positive words have a spelling mistake,
    while the negative word does not. The output of the code is `promise`. So this
    potentially corrects our spelling mistake.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在之前的代码中，正向词有拼写错误，而负向词没有。代码的输出是 `promise`，这意味着它可能会修正我们的拼写错误。
- en: 'Additionally, it can also be performed as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还可以通过以下方式执行：
- en: '[PRE50]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '*[(''experience'', 0.9027844071388245)]*'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '*[(''experience'', 0.9027844071388245)]*'
- en: However, note that this does not work when there are multiple spelling mistakes.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，当存在多个拼写错误时，这种方法不起作用。
- en: Building word vectors using GloVe
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 GloVe 构建词向量
- en: Similar to the way word2vec generates word vectors, **GloVe** (short for **Global
    Vectors for Word Representation**), also generates word vectors but using a different
    method. In this section, we will explore how GloVe works and then get into the
    implementation details of GloVe.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 word2vec 生成词向量的方式，**GloVe**（即**全局词向量表示**的缩写）也生成词向量，但采用的是不同的方法。在本节中，我们将探讨
    GloVe 的工作原理，然后详细介绍 GloVe 的实现细节。
- en: Getting ready
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'GloVe aims to achieve two goals:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 旨在实现两个目标：
- en: Creating word vectors that capture meaning in vector space
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建捕获意义的词向量
- en: Taking advantage of global count statistics instead of only local information
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用全局计数统计而非仅依赖局部信息
- en: 'GloVe learns word vectors by looking at the cooccurrence matrix of words and
    optimizing for a loss function. The working details of the GloVe algorithm can
    be understood from the following example:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe通过查看单词的共现矩阵并优化损失函数来学习词向量。GloVe算法的工作细节可以从以下示例中理解：
- en: 'Let''s consider a scenario where there are two sentences, as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个场景，其中有两个句子，如下所示：
- en: '| **Sentences** |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| **Sentences** |'
- en: '| This is test |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 这是测试 |'
- en: '| This is also a |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 这也是一个 |'
- en: 'Let''s try to build a word-cooccurrence matrix. There is a total of five unique
    words within our toy dataset of sentences, and from there the word-cooccurrence
    matrix looks as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个词共现矩阵。在我们的玩具数据集中有五个唯一单词，从而得到的词共现矩阵如下所示：
- en: '![](img/adf142cf-c7c0-4105-83bb-56260c16ab48.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adf142cf-c7c0-4105-83bb-56260c16ab48.png)'
- en: In the preceding table, the words *this* and *is* occur together in the two
    rows of the dataset and hence have a cooccurrence value of 2\. Similarly, the
    words *this* and *test* occur together only once in the dataset and hence have
    a cooccurrence value of 1.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，单词*this*和*is*在数据集的两行中共同出现，因此共现值为2。类似地，单词*this*和*test*仅在数据集中出现一次，因此共现值为1。
- en: However, in the preceding matrix, we have not taken the distance between the
    two words into consideration. The intuition for considering the distance between
    the two words is that the farther the cooccurring words are from each other, the
    less relevant they might be for the cooccurrence.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在前述矩阵中，我们没有考虑两个单词之间的距离。考虑两个单词之间距离的直觉是，共同出现的单词之间的距离越远，它们的相关性可能越低。
- en: We will introduce a new metric—*offset*, which penalizes for having a high distance
    between the given word and the cooccurring word. For example, *test* occurs at
    a distance of 2 from *this* in the first sentence, so we will divide the cooccurrence
    number by a value of 2.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将引入一个新的度量——*偏移量*，它惩罚给定单词与共现单词之间的高距离。例如，在第一句中，*test*与*this*的距离为2，因此我们将共现数除以2。
- en: 'The transformed cooccurrence matrix now looks as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的共现矩阵现在如下所示：
- en: '|  | **this** | **is** | **test** | **also** | **a** |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | **this** | **is** | **test** | **also** | **a** |'
- en: '| **this** | 0 | 2 | 0.5 | 0.5 | 0.33 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| **this** | 0 | 2 | 0.5 | 0.5 | 0.33 |'
- en: '| **is** | 0 | 0 | 1 | 1 | 0.5 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| **is** | 0 | 0 | 1 | 1 | 0.5 |'
- en: '| **test** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| **test** | 0 | 0 | 0 | 0 | 0 |'
- en: '| **also** | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **also** | 0 | 0 | 0 | 0 | 1 |'
- en: '| **a** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **a** | 0 | 0 | 0 | 0 | 0 |'
- en: 'Now that we have built the matrix, let''s bring in one additional parameter: the
    *context* of the words to be considered. For example, if the window size is 2,
    the cooccurrence value corresponding to the words *this* and *a* would be a value
    of 0 as the distance between the two words is greater than 2\. The transformed
    cooccurrence matrix when the context window size is 2 looks as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了矩阵，让我们引入一个额外的参数：*单词的上下文*。例如，如果窗口大小为2，则单词*this*和*a*之间的共现值将为0，因为两个单词之间的距离大于2。当上下文窗口大小为2时，转换后的共现矩阵如下所示：
- en: '|  | **this** | **is** | **test** | **also** | **a** |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | **this** | **is** | **test** | **also** | **a** |'
- en: '| **this** | 0 | 2 | 0.5 | 0.5 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **this** | 0 | 2 | 0.5 | 0.5 | 0 |'
- en: '| **is** | 0 | 0 | 1 | 1 | 0.5 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| **is** | 0 | 0 | 1 | 1 | 0.5 |'
- en: '| **test** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **test** | 0 | 0 | 0 | 0 | 0 |'
- en: '| **also** | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| **also** | 0 | 0 | 0 | 0 | 1 |'
- en: '| **a** | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| **a** | 0 | 0 | 0 | 0 | 0 |'
- en: 'Now that we have arrived at a modified cooccurrence matrix, we randomly initialize
    the word vectors of each word with a dimension of 2 in this instance. The randomly-initialized
    weights and bias values of each word, where each word has a vector size of 3,
    look as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了一个修改后的共现矩阵，我们随机初始化了每个单词的词向量，此示例中每个单词的维度为2。每个单词的随机初始化权重和偏置值如下所示：
- en: '| **Word** | **Weights 1** | **Weights 2** | **Weights 3** | **Bias** |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| **Word** | **Weights 1** | **Weights 2** | **Weights 3** | **Bias** |'
- en: '| **this** | -0.64 | 0.82 | -0.08 | 0.16 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| **this** | -0.64 | 0.82 | -0.08 | 0.16 |'
- en: '| **is** | -0.89 | -0.31 | 0.79 | -0.34 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| **is** | -0.89 | -0.31 | 0.79 | -0.34 |'
- en: '| **test** | -0.01 | 0.14 | 0.82 | -0.35 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| **test** | -0.01 | 0.14 | 0.82 | -0.35 |'
- en: '| **also** | -0.1 | -0.67 | 0.89 | 0.26 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| **also** | -0.1 | -0.67 | 0.89 | 0.26 |'
- en: '| **a** | -0.1 | -0.84 | 0.35 | 0.36 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| **a** | -0.1 | -0.84 | 0.35 | 0.36 |'
- en: 'Given that the preceding weights and biases are randomly initialized, we modify
    the weights to optimize the loss function. In order to do that, let''s define
    the loss function of interest, as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的权重和偏置是随机初始化的，我们修改权重以优化损失函数。为了做到这一点，我们定义感兴趣的损失函数如下：
- en: '![](img/7d0d0021-4a99-46aa-9ec9-6a8121574215.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d0d0021-4a99-46aa-9ec9-6a8121574215.png)'
- en: In the preceding equation, *w[i]* represents the word vector of the *i*^(th)
    word, and *w[j]* represents the word vector of *j*^(th) word; *b[i]* and *b[j]*
    are the biases that correspond to the *i*^(th) and *j*^(th) words, respectively.
    *X[ij]* represents the values in the final cooccurrence value that we defined
    earlier.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*w[i]*表示第*i*个单词的词向量，*w[j]*表示第*j*个单词的词向量；*b[i]*和*b[j]*分别是与第*i*个和第*j*个单词对应的偏置；*X[ij]*表示我们之前定义的最终共现值。
- en: For example, the value of *X[ij]* where *i* is the word *this* and *j* is the
    word *also* is 0.5
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当*i*是单词*this*，*j*是单词*also*时，*X[ij]*的值为0.5。
- en: 'When the value of *X[ij]* is 0, the value of f(*x[ij]*) is 0; otherwise, it
    is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 当*X[ij]*的值为0时，f(*x[ij]*)的值为0；否则，计算方式如下：
- en: '![](img/e3cdf189-d305-4b20-88c4-6c4ba834f4b1.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3cdf189-d305-4b20-88c4-6c4ba834f4b1.png)'
- en: In the preceding equation, alpha is empirically found to be 0.75, *x[max]* is
    100, and *x* is the value of *x[ij]*.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，alpha通过经验发现为0.75，*x[max]*为100，*x*是*x[ij]*的值。
- en: 'Now that the equation is defined, let''s apply that to our matrix, as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在公式已经定义，让我们将其应用到我们的矩阵中，如下所示：
- en: '![](img/d9c693ff-588a-47e5-b951-5503216d9586.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9c693ff-588a-47e5-b951-5503216d9586.png)'
- en: The first table represents the word-cooccurrence matrix and the randomly-initialized
    weights and biases.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张表表示单词共现矩阵和随机初始化的权重和偏置。
- en: The second table represents the loss-value calculation, where we calculate the
    overall weighted loss value.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 第二张表表示损失值计算，我们在其中计算整体加权损失值。
- en: We optimize the weights and biases until the overall weighted loss value is
    the least.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们优化权重和偏置，直到整体加权损失值最小。
- en: How to do it...
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Now that we know how word vectors are generated using GloVe, let''s implement
    the same in Python (the code file is available as `word2vec.ipynb` in GitHub):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了如何使用GloVe生成词向量，那么我们可以在Python中实现相同的功能（代码文件可以在GitHub上的`word2vec.ipynb`中找到）：
- en: 'Install GloVe:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装GloVe：
- en: '[PRE51]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Import the relevant packages:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关包：
- en: '[PRE52]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Preprocess the dataset the way we preprocessed in word2vec, skip-gram, and
    CBOW algorithms, as follows:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照我们在word2vec、skip-gram和CBOW算法中预处理数据集的方式进行预处理，如下所示：
- en: '[PRE53]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create a corpus and fit it with a vocabulary:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个语料库并为其配备词汇表：
- en: '[PRE54]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The dictionary of the corpus can be found as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库的字典可以通过以下方式找到：
- en: '[PRE55]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The unique words and their corresponding word IDs are obtained as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 得到唯一单词及其对应的单词ID如下：
- en: '![](img/a15b2447-73bd-42bd-97e8-5bb1d630f4e9.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a15b2447-73bd-42bd-97e8-5bb1d630f4e9.png)'
- en: The preceding screenshot represents the key values of the words and their corresponding
    index.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的截图表示单词的关键值及其对应的索引。
- en: 'The following code snippet gives us the cooccurrence matrix:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段给出了共现矩阵：
- en: '[PRE56]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](img/0248814c-3205-47f8-b94d-79788deb21df.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0248814c-3205-47f8-b94d-79788deb21df.png)'
- en: 'Let''s define the model parameters, that is, the number of dimensions of a
    vector, the learning rate, and the number of epochs to be run, as follows:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义模型参数，即向量的维度数量、学习率和要运行的epoch数，如下所示：
- en: '[PRE57]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Once the model is fit, the weights and biases of word vectors can be found
    as follows:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型拟合完成后，可以通过以下方式找到词向量的权重和偏置：
- en: '[PRE58]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The word vector for a given word can be determined as follows:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定单词的词向量可以通过以下方式确定：
- en: '[PRE59]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The most similar words for a given word can be determined as follows:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定单词的最相似单词可以通过以下方式确定：
- en: '[PRE60]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output of most similar words to "united" is as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 最相似的单词与“united”比较，输出如下：
- en: '![](img/f88b5cb8-7fc0-4a9f-97cc-13fdc15c6c0e.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f88b5cb8-7fc0-4a9f-97cc-13fdc15c6c0e.png)'
- en: Note that the words that are the most similar to the word `united` are the words
    that belong to other airlines.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与“united”最相似的单词是属于其他航空公司的单词。
- en: Building sentiment classification using word vectors
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用词向量构建情感分类
- en: In the previous sections, we learned how to generate word vectors using multiple
    models. In this section, we will learn how to build a sentiment classifier for
    a given sentence. We will continue using the airline sentiment tweet dataset for
    this exercise.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用多个模型生成词向量。在本节中，我们将学习如何构建一个情感分类器来处理给定句子。我们将继续使用航空公司情感推文数据集进行此练习。
- en: How to do it...
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Generate word vectors like the way we extracted in previous recipes (the code
    file is available as `word2vec.ipynb` in GitHub):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 生成词向量，方法与我们在前面的例子中提取的方式相同（代码文件可以在GitHub上的`word2vec.ipynb`找到）：
- en: 'Import the packages and download the dataset:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包并下载数据集：
- en: '[PRE61]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Preprocess the input text:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对输入文本进行预处理：
- en: '[PRE62]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Extract a list of lists across all the sentences in the dataset:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取数据集中所有句子的列表列表：
- en: '[PRE63]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Build a CBOW model, where the context window `size` is `5` and the vector length
    is 100:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个CBOW模型，其中上下文窗口`size`为`5`，向量长度为100：
- en: '[PRE64]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Specify the vocabulary to model and then train it:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定词汇表以构建模型，然后进行训练：
- en: '[PRE65]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Extract the average vector of a given tweet:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取给定推文的平均向量：
- en: '[PRE66]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: We are taking the average of the word vectors for all the words present in the
    input sentence. Additionally, there will be certain words that are not in the
    vocabulary (words that occur less frequently) and would result in an error if
    we try to extract their vectors. We've deployed `try` and `catch` errors for this
    specific scenario.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在对输入句子中所有单词的词向量取平均值。此外，会有一些不在词汇表中的单词（出现频率较低的单词），如果我们尝试提取它们的词向量，将会导致错误。我们为这个特定场景部署了`try`和`catch`错误处理机制。
- en: 'Preprocess features to convert them into an array, split the dataset, into
    train and test datasets and reshape the datasets so that they can be passed to
    model:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对特征进行预处理，将其转换为数组，分割数据集为训练集和测试集，并重塑数据集，以便可以传递给模型：
- en: '[PRE67]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Compile and build the neural network to predict the sentiment of a tweet:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并构建神经网络，以预测推文的情感：
- en: '[PRE68]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The summary of model defined above is as follows:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 上面定义的模型摘要如下：
- en: '![](img/f192b1ad-1bfb-48b4-aec9-fe09290cf0ad.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f192b1ad-1bfb-48b4-aec9-fe09290cf0ad.png)'
- en: 'In the preceding model, we have a 1,000-dimensional hidden layer that connects
    the 100 inputted average word vector values to the output, which has a value of
    1 (1 or 0 for a positive or negative sentiment, respectively):'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述模型中，我们有一个1000维的隐藏层，将100个输入的平均词向量值连接到输出层，输出值为1（1或0表示正面或负面情感）：
- en: '[PRE69]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We can see that the accuracy of our model is ~90% in predicting the sentiment
    of a tweet.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在预测推文情感时，我们的模型准确率约为90%。
- en: 'Plot the confusion matrix of predictions:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制预测结果的混淆矩阵：
- en: '[PRE70]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The output of confusion matrix is as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵的输出如下：
- en: '![](img/2c7b07d2-adcd-4ee5-bcaa-b8bfd5201169.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c7b07d2-adcd-4ee5-bcaa-b8bfd5201169.png)'
- en: From the above, we see that in 2,644 sentences, we predicted them to be positive
    and they are actually positive. 125 sentences were predicted to be negative and
    happened to be positive. 209 sentences were predicted to be positive and happened
    to be negative and finally, 485 sentences were predicted negative and were actually
    negative.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面可以看到，在2644个句子中，我们预测它们为正面情感，且它们确实是正面情感。125个句子被预测为负面情感，但实际为正面。209个句子被预测为正面情感，但实际上是负面情感，最后，485个句子被预测为负面情感，且实际为负面情感。
- en: There's more...
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'While we implemented the sentiment classification using the CBOW model and
    an average of all the word vectors that are present in the tweets, the other ways
    we could have proceeded are as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们使用CBOW模型和推文中所有词向量的平均值实现了情感分类，但我们本可以采用以下其他方法：
- en: Use the skip-gram model.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用skip-gram模型。
- en: Use the doc2vec model to build a model using document vectors.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用doc2vec模型通过文档向量构建模型。
- en: Use the fastText-model-based word vectors.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于fastText模型的词向量。
- en: Use the GloVe-based word vectors.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于GloVe的词向量。
- en: Use pre-trained models' word vector values.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型的词向量值。
- en: While these methods work in a similar fashion, one of the limitations of the
    preceding model is that it does not take word order into consideration. There
    are more sophisticated algorithms that solve the problem of word order, which
    will be discussed in the next chapter.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法工作原理相似，但上述模型的一个限制是它没有考虑单词的顺序。有更复杂的算法可以解决单词顺序的问题，这将在下一章讨论。
