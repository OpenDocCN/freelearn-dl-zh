- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exploring Supervised Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索监督深度学习
- en: '*Chapters 2* to *6* explored the core workhorse behind **deep learning** (**DL**)
    technology and included some minimal technical implementations for easy digestion.
    It is important to understand the intricacies of how different **neural networks**
    (**NNs**) work. One reason is that when things go wrong with any NN model, you
    can identify what the root cause is and mitigate it. Those chapters are also important
    to showcase how flexible DL architectures are to solve different types of real-world
    problems. But what are the problems exactly? Also, how should we train a DL model
    effectively in varying situations?'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二章*到*第六章*探讨了**深度学习**（**DL**）技术背后的核心工作原理，并包括了一些最基础的技术实现，便于理解。理解不同**神经网络**（**NNs**）工作原理的细节是很重要的。一个原因是，当任何神经网络模型出现问题时，你能够识别出根本原因并加以解决。这些章节也很重要，因为它们展示了深度学习架构如何灵活地解决各种现实问题。但这些问题到底是什么？我们应该如何在不同情况下有效训练深度学习模型？'
- en: 'In this chapter, we will attempt to answer the preceding two points specifically
    for supervised deep learning, but we will leave answering the same questions for
    unsupervised deep learning for the next chapter. This chapter will cover the following
    topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将尝试具体回答前面提到的两个问题，针对监督深度学习进行讨论，但针对无监督深度学习的问题会留到下一章。本章将涵盖以下主题：
- en: Exploring supervised use cases and problem types
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索监督学习的应用场景和问题类型
- en: Implementing neural network layers for foundational problem types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现神经网络层以解决基础问题类型
- en: Training supervised deep learning models effectively
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高效训练监督深度学习模型
- en: Exploring general techniques to realize and improve supervised deep learning-based
    solutions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索实现和改进基于监督深度学习的解决方案的一般技术
- en: Breaking down the multitask paradigm in supervised deep learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析监督深度学习中的多任务范式
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些Python编程语言中的实际实现。为了完成这些内容，你需要一台安装了以下库的计算机：
- en: '`pytorch`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`'
- en: '`catalyst==22.04`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`catalyst==22.04`'
- en: '`numpy`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`scikit-learn`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，链接：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8)。
- en: Exploring supervised use cases and problem types
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索监督学习的应用场景和问题类型
- en: 'Supervised learning requireslabeled data. Labels, targets, and ground truth
    all refer to the same thing. The provided labels essentially supervise the learning
    process of the **machine learning** (**ML**) model and provide the feedback needed
    for a DL model to generate gradients and update itself. Labels can exist in many
    different forms. They are **continuous numerical format**, **categorical format**,
    **text format**, **multiple categorical formats**, **image format**, **video format**,
    **audio format**, and **multiple target formats**. All of these are then categorized
    as either of the following supervised problem types:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习需要标签数据。标签、目标和真实值指的都是同一件事。提供的标签本质上监督着**机器学习**（**ML**）模型的学习过程，并为深度学习（DL）模型生成梯度和更新自身提供反馈。标签可以以多种形式存在，包括**连续数值格式**、**类别格式**、**文本格式**、**多类别格式**、**图像格式**、**视频格式**、**音频格式**以及**多目标格式**。所有这些都可以被归类为以下监督问题类型之一：
- en: '**Binary classification**: This is when the target has categorical data with
    only two unique values.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类**：这是指目标只有两个唯一值的分类数据。'
- en: '**Multiclassification**: This is when the target has categorical data with
    more than two unique values.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多分类**：这是指目标具有多个唯一值的分类数据。'
- en: '**Regression**: This is when the target has continuous numerical data.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：这是指目标具有连续的数值数据。'
- en: '**Multi-target/problem**:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多目标/问题**：'
- en: '**Multilabel**: This is when the target has more than one binary associated
    with a single data row.'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多标签**：这是指目标与单一数据行关联的多个二进制标签。'
- en: '**Multi-regression**: This is when the target has more than one regression
    target associated with a single data row.'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多回归**：这是指目标与单一数据行关联的多个回归目标。'
- en: '**Multiple problems**: Either multiple targets associated with a single data
    row in a single dataset or a chain of problems that is sequential in nature and
    multiple models are learned through different datasets.'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个问题**：每个数据行在单一数据集中关联多个目标，或者是一个序列性质的问题链，通过不同数据集学习多个模型。'
- en: '**Supervised representation learning**: This can be in many forms, and the
    main goal is to learn meaningful data representations given input data. The results
    of learned representation can subsequently be utilized for many purposes, including
    **transfer learning** (**TL**), and to realize recommendation systems.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督式表示学习**：这可以有多种形式，主要目标是根据输入数据学习有意义的数据表示。学习到的表示结果可以随后的多种用途，包括**迁移学习**（**TL**），以及实现推荐系统。'
- en: 'The definition of these problems may be hard to understand by itself. To create
    a better understanding of the different problems, we will check out an extensive
    set of use cases that can take advantage of DL technologies. Given the problems
    specified previously, the following table lists their use cases:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的定义可能单独理解起来比较困难。为了更好地理解不同的问题，我们将查看一组广泛的应用场景，这些应用场景可以利用深度学习技术。根据之前指定的问题，以下表格列出了它们的应用场景：
- en: '| **Problem Types** | **Supervised Deep Learning Model** **Use Cases** |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **问题类型** | **监督式深度学习模型** **应用场景** |'
- en: '| Binary classification |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 二分类 |'
- en: Gender prediction of babies with ultrasound imagery
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胎儿性别预测通过超声影像
- en: Semiconductor chip-quality rejection prediction in manufacturing with image
    data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半导体芯片质量拒绝预测，通过图像数据进行制造过程中的判断
- en: Using email data that can contain text, images, documents, audio, or any data
    to predict spam
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用包含文本、图像、文档、音频或任何数据的电子邮件数据来预测垃圾邮件
- en: '|'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multiclassification |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 多分类 |'
- en: Document data topic classification
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档数据主题分类
- en: Hate/toxic speech or text classification
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仇恨/有害言论或文本分类
- en: General image object classification
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般图像目标分类
- en: Sentiment prediction of text or speech data
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本或语音数据的情感预测
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Regression |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 回归 |'
- en: '**Click-through rate** (**CTR**) prediction of advertisements with image or
    text or both'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点击率**（**CTR**）预测，通过图像、文本或二者结合的方式进行广告预测'
- en: Human age prediction with a facial input image
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人类年龄预测通过面部输入图像
- en: Predicting GPS location from an image
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图像预测GPS位置
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multi-target |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 多目标 |'
- en: '**Text topic classification**: Multilabel, multiple topics can exist in a single
    text data row.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本主题分类**：多标签，单一文本数据行中可能包含多个主题。'
- en: '**Image object detection**: A multiple-target problem consisting of multiple
    regression targets and multiclass classification. First, with an image bounding
    box as multiple regression targets, a single *x* and *y* numerical coordinate,
    and its width and height as the two extra targets forms a rectangular-shaped bounding
    box. Next, the bounding box will be used to extract a cropped image for multiclassification
    purposes to predict the type of object.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像目标检测**：一个多目标问题，由多个回归目标和多类分类组成。首先，通过图像边界框作为多个回归目标，一个单一的*x*和*y*数值坐标，以及它的宽度和高度作为两个额外的目标，形成一个矩形形状的边界框。接下来，将使用这个边界框提取裁剪图像，用于多分类目的来预测物体的类型。'
- en: '**Image segmentation**: A kind of multilabel problem, where every pixel will
    serve as binary targets.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分割**：一种多标签问题，每个像素都会作为二进制目标。'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Supervised representation learning |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 监督式表示学习 |'
- en: Face feature representation for face recognition.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面部特征表示用于面部识别。
- en: Audio representation for speaker recognition using **K-Nearest** **Neighbors**
    (**KNN**).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**K-最近邻**（**KNN**）进行说话人识别的音频表示。
- en: Representing categories with their own representative feature vectors. This
    can be achieved with a method called **categorical embeddings,** which is an NN
    layer type that holds a feature vector for each category in a categorical feature
    column. It is learnable and serves as a lookup table. The method can reduce the
    feature dimensions of high-cardinality categorical data when compared to basic
    one-hot-encoding but still maintain around the same performance.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用方法**分类嵌入**（**categorical embeddings**）来表示类别，每个类别在分类特征列中都有一个代表性特征向量。这个方法是可学习的，并作为查找表。与基本的独热编码相比，这种方法能够减少高基数分类数据的特征维度，但仍能保持相似的性能。
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 8.1 – A table of DL problem use cases
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 – 深度学习问题应用场景表格
- en: Binary classification, multiclass classification, and regression problems are
    rather straightforward to approach. Multi-target types, however, pose complicated
    setups and require more architecting to be done depending on the nature of the
    problem. Multi-target tasks also can be straightforward, such as multilabel or
    multi-regression problems. This task falls into the bigger envelope of multitask
    solutions and will be discussed further in the *Exploring general techniques to
    realize and improve DL-based solutions* section of this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类、多类分类和回归问题的处理相对直接。然而，多目标类型的问题设置较为复杂，需根据问题的性质进行更多的架构设计。多目标任务也可以是直接的，例如多标签或多回归问题。该任务属于更广泛的多任务解决方案的范畴，将在本章的*探索实现和改进基于深度学习的解决方案*部分进一步讨论。
- en: Next, we will implement the basic NN layers of realizing the foundational problems,
    which include binary classification, multiclass classification, and regression
    problems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现实现基础问题的基本神经网络层，包括二分类、多类分类和回归问题。
- en: Implementing neural network layers for foundational problem types
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现基础问题类型的神经网络层
- en: In *Chapters 2* to *7*, although many types of NN layers were introduced, the
    core layers for the problem types were either not used or not explained. Here,
    we will go through each of them for clarity and intuition.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*到*第7章*中，尽管介绍了许多类型的神经网络层，但针对问题类型的核心层要么没有使用，要么没有解释。这里，我们将逐一讲解每一层，以便更清晰地理解和掌握直觉。
- en: Implementing the binary classification layer
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现二分类层
- en: Binary means two options for categorical data. Note that this does not necessarily
    mean a strict rule for the categories to be true or false nor positive or negative
    in the raw data. The two options can be in any format possible in terms of raw
    data, in strings, numbers, or symbols. However, note that NNs can always only
    produce numerical outputs. This means that the target itself has to be represented
    numerically, for which the optimal numbers are the binary values of zero and one.
    This means that the data column to be used as a target for training with only
    two unique values must go through preprocessing to map itself into zero or one.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类意味着类别数据有两个选项。请注意，这并不一定意味着原始数据中的类别严格表示为真或假，或者是正面或负面。两个选项可以是原始数据中的任何格式，可以是字符串、数字或符号。然而，请注意，神经网络始终只能产生数值输出。这意味着目标本身必须以数值形式表示，其中最优的数值是零和一的二进制值。这意味着，用作训练目标的数据列如果只有两个唯一值，必须经过预处理，将其映射为零或一。
- en: Generally, there are two ways to define binary outputs in NNs. The first is
    to use a linear layer with a size of one. The second method is to use a linear
    layer with a size of two. There is no significant difference between the two in
    terms of task-quality metrics, but method one takes slightly less space for storage
    and memory, so feel free to always use that version. The outputs from method 1
    will be constrained to values between `0` and `1` by using a sigmoid layer. For
    method 2, the outputs need to be passed into a softmax layer so that the probabilities
    for the two outputs will add up to one. Both methods usually can be optimized
    using **cross-entropy**. Cross-entropy is also known as **log loss**. Log loss
    measures the difference between predicted probabilities and true labels using
    a logarithmic scale. This scale penalizes incorrect predictions more heavily,
    emphasizing the importance of a model’s ability to assign high probabilities to
    the correct class and low probabilities to the incorrect class.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在神经网络中有两种方式来定义二进制输出。第一种是使用一个大小为一的线性层。第二种方法是使用一个大小为二的线性层。就任务质量指标而言，这两者之间没有显著区别，但方法一在存储和内存方面稍微占用更少的空间，因此可以随时使用这种版本。方法一的输出将通过使用sigmoid层约束在`0`到`1`之间。对于方法二，输出需要传入softmax层，这样两个输出的概率加起来为一。两种方法通常都可以通过**交叉熵**来优化。交叉熵也被称为**对数损失**。对数损失通过对数刻度衡量预测概率与真实标签之间的差异。该刻度对错误预测进行更严厉的惩罚，强调模型能够为正确类别分配高概率，为错误类别分配低概率的重要性。
- en: 'Translating the layer from method one into actual `pytorch` code will look
    like the following using the `nn` module from `torch`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将方法一中的层转换为实际的`pytorch`代码，将使用`torch`中的`nn`模块，代码如下所示：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we will implement the multiclass classification layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现多类分类层。
- en: Implementing the multiclass classification layer
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现多类分类层
- en: '`pytorch` code for a 100-class multiclass classification problem with 10 logits
    will look like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 针对一个有 10 个 logits 的 100 类多分类问题，`pytorch` 代码如下所示：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Another sub-problem of multiclass classification is when the classes are ordinal.
    This sub-problem and task is called ordinal classification. This means that the
    classes have an incremental relationship with each other. A plain multiclass classification
    layer strategy represents ordinal classes sub-optimally as the classes in the
    multiclass are considered to have an equal relationship with each other. A good
    strategy here to add the information of ordinal classes is to utilize a technique
    based on the multilabel classification task, which is a multiple-binary classification
    task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 多分类问题的另一个子问题是类别具有顺序关系时。这种子问题和任务被称为顺序分类。这意味着类别之间存在增量关系。一个普通的多分类层策略会将顺序类别表现为不理想，因为多分类中的类别被认为彼此之间具有平等关系。这里的一种好策略是使用基于多标签分类任务的技术，这是一个多二分类任务。
- en: 'Let’s say that we have five ordinal classes represented as numerical numbers
    from 1 to 5 for simplicity. In reality, this could be represented by any categorical
    data. In an NN, five binary classification heads would be used for this case where
    the classes will be assigned to the respective head in an ascending ordered manner.
    The raw predictions from this NN will be consumed in a way where the final predicted
    ordinal class will be derived from the position of the furthest consecutive positive
    binary prediction. Once there is a negative prediction, the rest of the prediction
    heads on the right will then be ignored. *Figure 8**.1* depicts this strategy
    by simulating the output predictions of the five binary classification heads:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有五个顺序类别，简化起见，将其表示为从 1 到 5 的数字。在实际应用中，这可以通过任何类别数据来表示。在神经网络中，这种情况将使用五个二分类头，其中类别将按升序依次分配到各个头。这个神经网络的原始预测将以一种方式被使用，最终预测的顺序类别将从最远连续的正二分类预测的位置中得出。一旦出现负预测，右侧其余的预测头将被忽略。*图
    8.1* 通过模拟五个二分类头的输出预测来描述这一策略：
- en: '![Figure 8.1 – Ordinal classification processing strategy using the output
    predictions of the five binary classification heads](img/B18187_08_001.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 使用五个二分类头的输出预测进行顺序分类处理策略](img/B18187_08_001.jpg)'
- en: Figure 8.1 – Ordinal classification processing strategy using the output predictions
    of the five binary classification heads
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 使用五个二分类头的输出预测进行顺序分类处理策略
- en: The learning process will be, as usual, using the cross-entropy loss for multiple
    binary targets. Additionally, the performance at every epoch can be monitored
    using robust metrics that don’t depend on probabilities such as recall or precision.
    The ordinal encoding method allows the model to learn that the targets have an
    ordinal relationship.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程将如常使用交叉熵损失来处理多个二进制目标。此外，可以使用不依赖于概率的强健指标（如召回率或精度）来监控每个 epoch 的性能。顺序编码方法使得模型能够学习目标之间的顺序关系。
- en: Next, we will dive into the implementation of a regression layer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨回归层的实现。
- en: Implementing a regression layer
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现回归层
- en: '`pytorch` code will look like the following using the `nn` module from `torch`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`pytorch` 代码将如下所示，使用来自 `torch` 的 `nn` 模块：'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`0` and `1`. Coupled with the scaling of target values, the bounds can then
    be enforced in the NN by using the sigmoid layer, which similarly scales activation
    values between `0` and `1`. During the inference stage, the predicted values can
    then be mapped into actual values by descaling values between `0` and `1` into
    the known minimum and maximum boundaries specified earlier.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`0` 和 `1`。结合目标值的缩放，边界可以通过使用 sigmoid 层在神经网络中强制执行，该层也会将激活值缩放到 `0` 和 `1` 之间。在推理阶段，预测值可以通过将
    `0` 和 `1` 之间的缩放值映射到先前指定的已知最小和最大边界，从而恢复为实际值。'
- en: The unbounded method allows for some form of generalization by allowing extrapolation,
    and the bounded method allows the addition of informed bias to the NN. Both methods
    have their own benefits and disadvantages, and thus the choice of approach needs
    to be evaluated on a case-by-case basis.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 无界方法通过允许外推来实现某种形式的泛化，而有界方法则允许在神经网络中加入已知的偏置。两种方法各有优缺点，因此选择哪种方法需要根据具体情况进行评估。
- en: Next, we will dive into the implementation of representation layers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨表示层的实现。
- en: Implementing representation layers
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现表示层
- en: Most methods focus on the interactions between architectures for different data
    modalities or the training methods that optimize the represented features. These
    are topics we will dive into further in the next topic after this. One key layer
    type that truly represents the representation layer is the embedding layer. Embeddings
    are a type of layer structure that maps categorical data types into learnable
    vectors. Through this layer, each category will be able to learn a representation
    that is able to perform well against the specified target. The method can be used
    for converting text word tokens into more representative features or plainly as
    a replacement for one-hot encoding. Categorical embeddings make it possible to
    automate the feature engineering process for categorical data types. One-hot encoding
    produces an encoding that enforces the same distance between all the categories
    to every other category. Categorical embedding, however, allows for the possibility
    of obtaining an appropriate distance based on its interactions with the target
    variable and with other data if any extra data exists.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法侧重于不同数据模态的架构之间的相互作用或优化所代表特征的训练方法。这些是我们将在接下来的话题中进一步深入讨论的主题。一个真正代表表示层的关键层类型是嵌入层。嵌入层是一种将分类数据类型映射为可学习向量的层结构。通过这一层，每个类别将能够学习到一个能够与指定目标很好地对应的表示。该方法可用于将文本单词标记转换为更具代表性的特征，或者简单地作为独热编码的替代。分类嵌入使得对分类数据类型进行特征工程处理变得可能。独热编码生成的编码强制所有类别之间的距离相同。然而，分类嵌入允许根据其与目标变量及其他数据（如果有额外数据存在）的交互来获得适当的距离。
- en: However, categorical embeddings are also not a silver bullet for all ML use
    cases, even if they are decoupled from an actual NN model after training and just
    act as a featurizer. They can sometimes perform better against one-hot-encoding
    in general and, vice versa, can happen other times to perform worse against one-hot-encoding.
    The method still remains a key method to experiment with for any dataset with
    categorical data as input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，分类嵌入并不是所有机器学习用例的灵丹妙药，即使它们在训练后与实际神经网络模型分离，只起到特征化作用。它们有时可以在总体上表现得比独热编码更好，反之亦然，有时可能会比独热编码表现得更差。这种方法仍然是对任何包含分类数据输入的数据集进行实验的关键方法。
- en: Training supervised deep learning models effectively
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效训练监督深度学习模型
- en: 'In [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*,
    it is emphasized that ML projects have a cyclical life cycle. In other words,
    a lot of iterative processes are carried out in the course of the project’s lifetime.
    To train supervised deep learning models effectively, there are a lot of general
    directions that should be taken based on different conditions, but the one that
    absolutely stands out across every problem is proper tooling. The tooling is more
    commonly known as `pytorch` or `keras` with `tensorflow`, ease of deployment,
    ease of model comparisons using different metrics, ease of model tuning, good
    visualization of model training monitoring, and, finally, good feedback about
    the progress (this can be sent through messages and notifications for alerts).
    If no advanced tools that truly simplify the entire process are at your disposal,
    you can focus on the important bits of making a model work well instead of dealing
    with infrastructure issues such as coordinating the saving of models into different
    folders like **DataRobot**, a paid-for tool, then open sourced tools such as MLflow,
    Kubeflow, or Metaflow will be the next-best alternative. Once the tool of choice
    is picked, carrying out training in DL models will be a breeze. We will be using
    MLflow as an example tool to demonstrate some effective methods for DL model training
    in the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B18187_01.xhtml#_idTextAnchor015)，*深度学习生命周期*中，强调了ML项目具有循环生命周期。换句话说，在项目生命周期中进行了大量迭代过程。为了有效训练监督学习深度模型，基于不同条件应采取许多通用方向，但绝对突出于各种问题的一点是适当的工具化。工具化更常见地称为`pytorch`或`keras`与`tensorflow`，部署的便利性，使用不同度量标准比较模型的便利性，模型调优的便利性，良好的模型训练监视可视化，以及关于进展的良好反馈（可以通过消息和通知发送警报）。如果没有真正简化整个流程的先进工具，您可以专注于使模型运行良好的重要部分，而不必处理基础设施问题，如协调将模型保存到不同文件夹中的**DataRobot**，一种付费工具，然后开源工具如MLflow、Kubeflow或Metaflow将成为下一个最佳替代方案。一旦选择了工具，进行DL模型训练将变得轻松。我们将使用MLflow作为示例工具，在以下步骤中展示DL模型训练的一些有效方法：
- en: Data preparation
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Configuring and tuning DL hyperparameters
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置和调优DL超参数
- en: Executing, visualizing, tracking, and comparing experiments
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行、可视化、跟踪和比较实验
- en: Additionally, we will explore some extra tips when building a model before ending
    this topic.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探讨在结束此主题之前构建模型时的一些额外提示。
- en: Let’s explore each in detail.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨每一个。
- en: Preparing the data for DL training
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为DL训练准备数据
- en: 'Data is the core of any ML model. Data ultimately determines the achievable
    model performance, the quality of the final trained model, and the validity of
    the final trained model. In [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep
    Learning Life Cycle*, we explored what it takes for a dataset setup to be DL-worthy,
    along with the qualities needed when acquiring data, coupled with **exploratory
    data analysis** (**EDA**) to verify causality and validity. The general idea there
    was to identify and add extra features and data modalities that have causal effects
    toward the desired target. In this section, we will cover more in-depth essential
    steps that convert the data into a DL trainable state listed here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是任何ML模型的核心。数据最终决定了可达到的模型性能、最终训练模型的质量以及最终训练模型的有效性。在[*第1章*](B18187_01.xhtml#_idTextAnchor015)，*深度学习生命周期*中，我们探讨了使数据集设置具备DL价值的条件，以及在获取数据时所需的特性，结合**探索性数据分析**（**EDA**）来验证因果关系和有效性。那里的一般想法是识别和添加对所需目标具有因果影响的额外特征和数据形式。在本节中，我们将更深入地讨论将数据转换为DL可训练状态的重要步骤，列在这里：
- en: Data partitioning
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据分区
- en: Data representation
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据表示
- en: Data augmentation
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据增强
- en: Partitioning the data for DL training
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为DL训练分区数据
- en: 'The first step we will cover is data partitioning. Having a good data-partitioning
    strategy for training, validating, and testing your model is essential for a good-performing
    model. The training partition will be the partition that will strictly be used
    for training. The validation partition will be the partition that will strictly
    be used for validating a model during training. In DL, a **validation partition**
    is often used as a guide to signal when to stop training or extract the best-performing
    weights using external data out of the training data. Since the validation data
    will affect the learning process of the model and add some bias that will cause
    overfitting toward the nature of the out-of-training validation data, a testing
    partition will be the final partition that will be used to verify the generalizability
    of the trained model. The testing partition is also known as the holdout partition.
    To be extra safe in preventing overfitting and to ensure the generalizability
    of the model, the validation partition can also be used exclusively to be validated
    only once after the model is trained instead of being used for validation at every
    epoch. This strategy, however, requires that a smaller internal validation partition
    is created from the original training partition. The following figure depicts
    the two different strategies:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍数据分区的步骤。对于训练、验证和测试模型来说，有一个良好的数据分区策略对于获得良好的性能模型至关重要。训练分区将严格用于训练。验证分区将在训练过程中严格用于验证模型。在深度学习中，**验证分区**通常被用作指导信号，用来指示何时停止训练或提取出使用外部数据训练的最佳性能权重。由于验证数据会影响模型的学习过程，并且会向着训练外验证数据的性质导致过拟合的方向发展，测试分区将是最终用于验证经过训练模型的泛化能力的分区。测试分区也被称为保留分区。为了更安全地防止过拟合，并确保模型的泛化能力，验证分区也可以专门用于在模型训练后仅验证一次，而不是在每个时期都用于验证。然而，这种策略要求从原始训练分区创建一个较小的内部验证分区。以下图展示了两种不同的策略：
- en: '![Figure 8.2 – Two different cross-validation data-partitioning strategies](img/B18187_08_002.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 两种不同的交叉验证数据分区策略](img/B18187_08_002.jpg)'
- en: Figure 8.2 – Two different cross-validation data-partitioning strategies
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 两种不同的交叉验证数据分区策略
- en: 'This process of partitioning the data is called **cross-validation**. The preceding
    figure shows simple cross-validation strategies with only a single partitioning
    setting. This might create issues where the model’s performance metrics reported
    are biased toward a specific resulting partitioning setting. The resulting partitioning
    may have some inherent distribution or nature that allowed results to perform
    particularly well or badly toward it. When that happens, having mismatched expectations
    of performance during the deployment stage will create more operational issues.
    To safely remove the possibility of such bias, **k-fold cross-validation** is
    typically used to report a more comprehensive validation score that could better
    reflect the performance of the model in the wild. To perform this partitioning
    method, a single testing set is removed from the original dataset, and validation
    scores are averaged across different ordered *k* cross-validation training and
    validation partitions. This is better visualized in *Figure 8**.3*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据分区的过程被称为**交叉验证**。前面的图展示了只有单个分区设置的简单交叉验证策略。这可能会导致模型的性能指标报告对特定的结果分区设置产生偏倚。结果的分区可能具有某种固有的分布或性质，使得结果在特定分区设置中表现特别好或特别差。当发生这种情况时，在部署阶段对性能期望不匹配会造成更多操作问题。为了安全地消除这种偏差可能性，通常使用**k
    折交叉验证**来报告更全面的验证分数，这些分数可以更好地反映模型在实际情况下的性能。为了执行这种分区方法，需要从原始数据集中移除单一测试集，并且在不同顺序的*k*交叉验证训练和验证分区上平均验证分数。这在*图
    8**.3*中更好地可视化：
- en: '![Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting
    bias](img/B18187_08_003.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – K 折交叉验证作为消除度量报告偏差的策略](img/B18187_08_003.jpg)'
- en: Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting
    bias
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – K 折交叉验证作为消除度量报告偏差的策略
- en: Finally, for testing performance reporting and deployment purposes, the model
    either gets retrained on the training and validation dataset combined or the model
    trained in the first fold is extracted for deployment purposes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试性能报告和部署目的，模型可以重新在训练和验证数据集合并或从第一折训练的模型中提取出用于部署目的。
- en: Recall that stratified partitioning is a recommended strategy to split your
    data into the three mentioned partitions. This means that the data will be approximately
    evenly separated into three partitions based on the label associated with the
    dataset. This ensures that no labels get left out in any partition, which could
    potentially cause misinformation. Take a simple case of binary classification
    where the dataset is randomly partitioned into three partitions of training, validation,
    and testing with prespecified sizes. Since each data row was randomly placed into
    one of the three partitions, there is a probability the partitions will only contain
    one label from the two binary labels. Since the validation or testing partition
    is usually assigned with smaller data sizes, they have more potential to face
    this issue. Let’s say the model is mistakenly trained to predict only a single
    label. If the label is exactly the label that exclusively exists in the validation
    and testing of ML learning practitioners, we will mistakenly think the model is
    doing extremely well, but in fact, it is useless. You never know when you will
    be unlucky when doing full random partitioning, so use stratified random partitioning
    whenever you can!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，分层划分是将数据划分为三个提到的部分的推荐策略。这意味着数据将根据与数据集关联的标签大致平均地分为三部分。这确保了任何一个分区中都不会遗漏标签，从而避免了可能导致误信息的情况。以二分类为例，假设数据集被随机划分为训练集、验证集和测试集三个部分，且每个部分的大小是预先指定的。由于每行数据是随机地分配到这三部分中的一个，因此存在分区中可能只包含二分类标签中的一个标签的概率。由于验证集或测试集通常被分配较小的数据量，它们面临这种问题的潜在概率更大。假设模型错误地只训练来预测单一标签。如果该标签正好是只在验证集和测试集里存在的标签，机器学习的从业者可能会误以为模型表现极好，实际上，它是无用的。在进行完全随机划分时，你永远不知道什么时候会不走运，所以只要可能，尽量使用分层随机划分！
- en: The data-partitioning strategy described here builds only a single model that
    will be utilized during inference mode in model deployment. This strategy is the
    standard option when the inference runtime of the final model setup is a concern
    and having a faster model is more important than having small improvements in
    the accuracy metrics. When it is okay to trade runtime for some accuracy performance,
    an alternative strategy called **k-fold cross-validation ensemble** can be used.
    This is a method that is widely advocated in many ML competitions, especially
    in the ones hosted on Kaggle. The method uses the *k*-fold cross-validation described
    previously but actually uses *k* models trained during cross-validation and performs
    an ensemble of the k model’s predictions. An ensembling method called blending
    aggregates predictions of models and almost always improves the accuracy metrics
    from a single model. This process can be thought of as a method that leverages
    the best ideas and expertise of each *k* model, making the final outcome better
    as an aggregate. This aggregate can be as simple as an average or median of the
    *k* predictions.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的数据划分策略只构建了一个模型，这个模型将在模型部署的推理模式下使用。当最终模型部署的推理运行时长是一个关注点时，这种策略是标准选择，尤其是在需要更快的模型，而不是稍微提高精度指标时。如果可以接受牺牲一些精度性能来换取运行时长，那么可以使用一种叫做**k折交叉验证集成**的替代策略。这是许多机器学习竞赛中广泛推荐的方法，尤其是在Kaggle上举办的竞赛中。该方法使用之前描述的*k*折交叉验证，但实际上使用在交叉验证过程中训练的*k*个模型，并对这*k*个模型的预测结果进行集成。一种叫做混合的方法将模型的预测结果聚合，几乎总是能提高单个模型的精度指标。这个过程可以被看作是一种利用每个*k*模型的最佳思路和专业知识的方法，从而使最终结果作为一个整体更好。这个整体可以是*k*个预测的平均值或中位数。
- en: A final tip before moving on to the next method is to always remember to make
    sure partitions match when comparing models between experiments. Misinformation
    often happens in the field when two models are separately developed using different
    data-partitioning strategies and data partitions. Even when one of the models
    achieves a significant performance advantage over the other, it does not mean
    anything and will not amount to any meaningful comparisons.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一个方法之前，有一个小贴士：在比较不同实验中的模型时，始终记得确保数据划分一致。误信息通常发生在当两个模型在不同的数据划分策略和数据划分下分别开发时。即使其中一个模型在性能上明显优于另一个，也并不意味着什么，这种比较没有任何意义。
- en: Next, we will dive into the data representation component for different data
    modalities.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨不同数据模态的数据表示组件。
- en: Representing different data modalities for training DL models
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为训练深度学习模型表示不同的数据模态
- en: So far, we have brushed over the utilization of numerical, categorical, text,
    audio, image, and video modalities. These are the most common modalities utilized
    across multiple industries. Representing different data modalities is a complicated
    topic as, in addition to the common modalities, there are actually a lot of rare
    data modalities out there. Examples of rare modalities are chemical formulas (a
    special structured form of textual data), document data (another special form
    of textual data with complex positional information), and graph data. In this
    section, we will only discuss the representation of the common unstructured modalities
    here to ensure the relevancy of content to our readers. Both numerical and categorical
    data are considered structured data and are have been covered properly in previous
    sections. Let’s now start with the text data modality.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已简要讨论了数值、分类、文本、音频、图像和视频模式的利用。这些是跨多个行业最常用的模式。表示不同数据模式是一个复杂的话题，因为除了常见模式外，实际上还有很多稀有的数据模式。例如，稀有模式包括化学公式（文本数据的特殊结构形式）、文档数据（另一种具有复杂位置信息的文本数据的特殊形式）和图数据。在本节中，我们仅讨论常见的非结构化模式的表示方式，以确保内容与读者的相关性。数值和分类数据被认为是结构化数据，已经在前面的章节中得到了充分的讨论。现在让我们开始讨论文本数据模式。
- en: Representing text data for supervised deep learning
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监督式深度学习的文本数据表示
- en: 'Text data representation in general has improved tremendously over the years.
    The following list shows a few relevant methods:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据的表示方式多年来得到了极大的改进。以下是一些相关方法的列表：
- en: '**Term frequency-inverse document frequency (TF-IDF) with N-grams**: The term
    here is implemented with N-grams. An N-gram is an adjacent sequence of *n* textual
    characters. N-grams are produced by a method called tokenization. The tokenization
    can be a representation as low level as single characters, or it can be a higher-level
    representation such as words. Once represented as N-grams, TF-IDF is computed
    using the following formula.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率（TF-IDF）与N-gram**：这里的术语是通过N-gram实现的。N-gram是相邻的*n*个文本字符序列。N-gram是通过一种叫做分词的方法生成的。分词可以是像单个字符这样低级的表示，也可以是更高级的表示，比如单词。一旦表示为N-gram，就可以使用以下公式计算TF-IDF。'
- en: TF-IDF = term frequency x inverse document frequency
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF = 词频 x 逆文档频率
- en: '**Term frequency** is simply the count array of a single row. **Inverse document
    frequency** is computed through the following formula:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频**只是单行的计数数组。**逆文档频率**通过以下公式计算：'
- en: IDF = log( number of text samples   ______________________________________    number
    of documents containing the term for each term )
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: IDF = log( 文本样本总数  ______________________________________    包含该术语的文档数 )
- en: The representation is an efficient and lightweight way to extract useful information
    from text, where words that are rare have higher values and more frequent words
    such as “the” and “and” will be suppressed. Outputs of TF-IDF can be directly
    fed into a simple **multilayer perceptron** (**MLP**) or any ML model to produce
    a predictive model. In simpler use cases, this representation will be enough to
    achieve a good metric performance. However, in more complex use cases that require
    the decoding of complex interactions that can happen with the different compositions
    of text and labels, it will underperform.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示法是一种高效且轻量级的从文本中提取有用信息的方式，其中稀有的词具有更高的值，而像“the”和“and”这样的常见词会被抑制。TF-IDF的输出可以直接输入到一个简单的**多层感知器**（**MLP**）或任何机器学习模型中，生成预测模型。在简单的用例中，这种表示法足以达到良好的性能指标。然而，在需要解码文本和标签的复杂交互的更复杂的用例中，它可能表现不佳。
- en: '**Word/token embeddings**: Word embeddings can be trained from scratch or pre-trained
    from a bigger dataset. Pre-trained embeddings can be pre-trained in either a supervised
    fashion or an unsupervised fashion, usually on a larger dataset. However, the
    embedding method suffers from the issue of token mismatch during the training,
    evaluation, and testing stages. This means that it is required to perform a lot
    of tinkering with the way the specific text token is preprocessed before looking
    up to the embeddings table. This occurrence is known as **out of vocabulary**
    (**OOV**) during the evaluation and testing stages. In the training stage, different
    variations of the same word will have their own meaning, which is inefficient
    in terms of learning and resource-space utilization. In practice, methods such
    as stemming, lemmatization, lowercasing, and known word-to-word replacements are
    applied to mitigate OOV, but the problem won’t be mitigated completely. These
    word embeddings can be paired with either **recurrent NNs** (**RNNs**) or transformers.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词/标记嵌入**：词嵌入可以从头开始训练，也可以从更大的数据集上进行预训练。预训练的嵌入可以通过监督学习或无监督学习的方式在较大的数据集上进行预训练。然而，嵌入方法在训练、评估和测试阶段存在标记不匹配的问题。这意味着在查找嵌入表之前，必须对特定文本标记的预处理方式进行大量调整。这个现象在评估和测试阶段被称为**词汇外**（**OOV**）。在训练阶段，同一个单词的不同变体会有各自的含义，这在学习和资源空间利用上是低效的。实际上，像词干提取、词形还原、转小写以及已知的词对词替换等方法通常会被用来缓解OOV问题，但问题不会完全解决。这些词嵌入可以与**递归神经网络**（**RNNs**）或变换器模型结合使用。'
- en: '**Subword-based tokenization**: This family of methods attempts to solve the
    token mismatch issue and the large vocabulary size of tokens. *Subword* might
    sound unintuitive, as we as humans use full words to perceive the meaning of text.
    This family of algorithms only performs subword tokenization when the word can’t
    be identified or is considered to be rare. For common words, they will remain
    full word tokens. Examples of such methods include **byte-pair encoding** (**BPE**),
    **WordPiece**, and **SentencePiece**. We will go through these methods briefly
    as a simple guide:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于子词的分词法**：这一类方法试图解决标记不匹配问题以及标记词汇表大小过大的问题。*子词*这个词可能听起来不直观，因为我们人类使用完整的词汇来感知文本的意义。这些算法只有在无法识别单词或单词被认为是稀有时，才会执行子词分词。对于常见单词，它们将保持为完整的词标记。这类方法的例子包括**字节对编码**（**BPE**）、**WordPiece**和**SentencePiece**。我们将简要介绍这些方法，作为简单的指南：'
- en: '**BPE tokenization**: BPE treats the text as characters and groups up the most
    common consecutive characters iteratively during training. The number of iterations
    determines when the training iterations to group up the most common characters
    should be stopped. This formulation allows for rare words to remain as subword
    tokens and common words to be grouped up into a single token. This representation
    is notably used by **generative pre-trained transformer** (**GPT**) models. However,
    this representation faces an issue where there will be multiple ways to encode
    a particular word.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BPE分词法**：BPE将文本视为字符，并在训练过程中迭代地将最常见的连续字符分组。迭代的次数决定了在训练过程中，何时停止将最常见的字符分组。此形式化方法允许稀有单词保持为子词标记，而常见单词则可以合并为一个单独的标记。这种表示方法特别用于**生成预训练变换器**（**GPT**）模型。然而，这种表示方法存在一个问题，就是同一个单词可能有多种编码方式。'
- en: '**WordPiece**: WordPiece improves upon BPE by utilizing a language model to
    choose the most likely pair of tokens to group up. This enforces a kind of intelligent
    choice when deciding the way to encode a particular word. This algorithm is utilized
    by **Bidirectional Encoder Representations from Transformers** (**BERT**) and
    **Efficiently Learning an Encoder that Classifies Token Replacements** **Accurately**
    (**ELECTRA**).'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WordPiece**：WordPiece在BPE的基础上进行了改进，通过使用语言模型来选择最可能的标记对进行分组。这在决定如何编码特定单词时，提供了一种智能选择方式。该算法被**双向编码器表示的变换器**（**BERT**）和**高效学习分类标记替换的编码器**（**ELECTRA**）所使用。'
- en: '**SentencePiece**: SentencePiece is a method that optimizes the tokens generated
    by base tokenizers such as BPE. It uses a couple of components, such as using
    a form of Unicode text conversion to ensure no language-dependent logic exists
    and using a method called subword regularization that performs a form of subword
    token group augmentation (probabilistically and randomly choose a single sample
    from the top-k predicted subword token to group up using language models) to solve
    multiple representation issues. This algorithm is used by XLNet and **A Lite BERT**
    (**ALBERT**) most notably.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SentencePiece**：SentencePiece是一种优化基础分词器（如BPE）生成的标记的方法。它使用几个组件，例如使用一种形式的Unicode文本转换来确保不存在依赖语言的逻辑，并使用称为子词正则化的方法，执行一种子词标记组增强形式（从预测的子词标记的前k个中以概率和随机方式选择单个样本来组合使用语言模型）。这个算法是XLNet和**A
    Lite BERT**（**ALBERT**）最显著使用的。'
- en: Text data are represented as tokens for DL. This also means that there will
    be strict limits to the number of tokens so that the NN model can be initialized
    with the right parameters. Pick a token size limit that is reasonable for your
    use case based on what’s needed to get a good model performance. Since the size
    limit will affect model size, be sure to make sure the model size doesn’t get
    so big that it overshoots your inference runtime requirements.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据被表示为DL的标记。这也意味着标记数量会有严格限制，以便NN模型能够以正确的参数初始化。根据需要获得良好的模型性能，选择一个合理的标记大小限制。由于大小限制会影响模型大小，请确保模型大小不会过大，超过推理运行时要求。
- en: In terms of missing text data, which can happen in real-world use cases with
    multimodal data, using an empty string is the most natural way to work as an imputation
    method. Under the hood, these models would usually use all zeros to represent
    the text array.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 关于缺失文本数据，在多模态数据的实际使用情况中可能会出现，使用空字符串作为插补方法是最自然的方式。在底层，这些模型通常会使用全零表示文本数组。
- en: Now that we’ve briefly covered supervised text data representations, let’s discover
    supervised audio data representations next.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简要介绍了监督文本数据表示，让我们接下来探索监督音频数据表示。
- en: Representing audio data for DL
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表示DL的音频数据
- en: Audio data is time-series data with one or two arrays of values where each value
    in the array represents a single piece of audio data at a specific timestamp.
    Audio data can be either represented as a simple normalized form from the original
    raw data, represented as something called a spectrogram, which is a two-dimensional
    data, or as **Mel-frequency cepstral** **coefficients** (**MFCCs**).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 音频数据是带有一个或两个值数组的时间序列数据，其中数组中的每个值表示特定时间戳处的单个音频数据片段。音频数据可以表示为从原始原始数据简单归一化形式表示的东西，表示为称为声谱图的二维数据，或作为**Mel频率倒谱系数**（**MFCCs**）。
- en: A spectrogram is the resulting output of a process called `wav2vec 2.0`, which
    is a type of transformer.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 声谱图是一个称为`wav2vec 2.0`的过程的结果，这是一种变压器类型。
- en: 'The STFT process has hyperparameters that can affect the resulting representation.
    The following list summarizes these hyperparameters and tips on how to set it
    up properly:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: STFT过程具有可以影响结果表示的超参数。以下列表总结了这些超参数以及如何正确设置它的技巧：
- en: '**Sampling rate**: This specifies the samples per second (Hertz/Hz) parameter
    that will be used before applying STFT. Audio data might be recorded in different
    sampling rates, and to build a model, this data will be required to be unified
    to a single sampling rate through resampling algorithms. The most typically used
    value is 16,000 Hz. As this value will affect the size and runtime of the model
    given a fixed time window a model is built to handle, be sure to only increase
    it if it’s necessary in terms of metric performance.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样率**：这指定了在应用STFT之前将使用的每秒样本数（赫兹/Hz）参数。音频数据可能以不同的采样率录制，并且为了构建模型，这些数据将需要通过重采样算法统一到单一的采样率。最常用的值是16,000
    Hz。由于这个值会影响模型在给定固定时间窗口下处理的大小和运行时间，请确保仅在指标表现需要时才增加它。'
- en: '**STFT window length**: Each window size will be responsible for the data for
    a fixed duration and specific position. Each window will produce a single value
    at the same time window for a range of frequencies. The typical value of this
    parameter is 4096 or 2048\. Configure this based on the prediction resolution
    you require for your use case if there is a strict requirement there. This parameter
    will also affect the model size.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**STFT 窗口长度**：每个窗口大小将负责特定时长和特定位置的数据。每个窗口将在同一时间窗口内为一系列频率生成一个单一的值。这个参数的典型值为 4096
    或 2048。根据你的使用案例中对预测分辨率的要求来配置这个值，如果有严格的需求的话。这个参数还会影响模型的大小。'
- en: '**Window stride**: This is similar to a convolutional layer filter stride and
    does not have many significant tuning methods. Using a small percentage of the
    window length, such as 10%, should be a good enough setting.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口步幅**：这与卷积层的滤波器步幅类似，并且没有太多显著的调节方法。使用窗口长度的一个小百分比，比如 10%，应该是一个足够好的设置。'
- en: '**Whether to use Mel scaling**: A **Mel scale** is a logarithmic transformation
    of the audio signal’s frequency. Fundamentally, it is a transformation to mimic
    how humans perceive audio. It makes higher-frequency changes matter less and lower-frequency
    changes matter more. Use this when it involves some form of human judgment to
    improve the metric performance.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**是否使用 Mel 频率尺度**：**Mel 频率尺度**是音频信号频率的对数变换。从根本上讲，它是一种模仿人类如何感知音频的变换。它使高频变化的影响变小，低频变化的影响变大。当涉及到某种形式的人工判断以提高度量性能时，应使用这种变换。'
- en: As for empty audio rows, imputing them with a single pre-generated random noise
    audio or using an array of zeros with the same length should work well in multimodal
    datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于空的音频行，可以使用预生成的单个随机噪音音频或使用与之长度相同的零数组来填充，这在多模态数据集中通常效果不错。
- en: Now that we’ve briefly covered supervised audio data representations, let’s
    discover supervised image and video data representations next.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们简要介绍了有监督的音频数据表示，接下来让我们探索有监督的图像和视频数据表示。
- en: Representing image and video data for DL
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 表示图像和视频数据以供深度学习使用
- en: Image data doesn’t require a lot of introduction here as we have gone through
    a few tutorials using them directly. The key is to perform some sort of normalization
    before feeding it to NN models such as CNNs, and the NN will extract great representations.
    Transformers have also been making tight competition with CNNs in image-based
    tasks and can be used to both extract representative features and predict directly
    on the task. One thing to note is that unless the resolution of the image is crucial
    in identifying certain patterns, it is usually much more effective as a model
    to utilize a smaller image resolution. One might not be able to visually certain
    patterns when the resolution of the image is smaller, but a computer would still
    be able to. The resolution of the image affects the runtime of the training, the
    runtime of the model in production, and sometimes the model size, as for transformers,
    so make sure this is done conservatively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据在这里不需要太多介绍，因为我们已经通过一些教程直接使用了它们。关键是要在将数据输入神经网络模型（如 CNN）之前进行某种归一化，神经网络将提取出很好的特征表示。变压器（Transformers）在图像任务中也与卷积神经网络（CNN）展开了激烈的竞争，并且可以用来提取代表性特征并直接进行任务预测。需要注意的一点是，除非图像的分辨率对于识别某些模式至关重要，否则通常使用较小的图像分辨率作为模型会更有效。尽管图像分辨率较小时，人工可能无法直观看到某些模式，但计算机仍然可以识别。图像分辨率会影响训练的运行时间、生产环境中模型的运行时间，有时也会影响模型的大小，特别是对于变压器模型，因此确保合理配置。
- en: Video data, however, is an extended form of image data where a number of images
    are aligned sequentially to form a video. This means that video data is a form
    of sequential data just like text without absolute timestamp information. Each
    sequential image is known as a **frame**. Video can have a variety of frame rates.
    Commonly, this would be in rates of 24, 30, or 48 **frames per second** (**FPS**)
    but can generally be any number. For **computer vision** (**CV**) use cases, make
    sure to set a low FPS so that the processing load can be reduced depending on
    the use case. For example, the use case of lip reading has lower FPS requirements
    than for asking a model to identify whether a person is running or not. For the
    frame resolution, the same guide for image resolution applies here. Once the video
    properties have been decided, representative features have to be learned and extracted.
    The current SoTA features are extracted through models similar to image-based
    use cases. Examples of such models are 3D CNNs and 3D transformers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，视频数据是图像数据的一种扩展形式，多个图像按顺序排列形成一个视频。这意味着视频数据是一种序列数据，就像文本一样，缺乏绝对的时间戳信息。每个连续的图像被称为**帧**。视频可以有多种帧率，通常为24、30或48
    **每秒帧数**（**FPS**），但一般可以是任何数字。对于**计算机视觉**（**CV**）的使用案例，确保设置较低的FPS，以便根据具体的使用场景减少处理负担。例如，唇读的使用案例需要的FPS比让模型判断一个人是否在跑步的需求要低。对于帧的分辨率，图像分辨率的相同指南也适用。一旦决定了视频属性，就必须学习并提取代表性特征。当前的最先进（SoTA）特征是通过类似于基于图像的使用案例的模型提取的。这样的模型包括3D卷积神经网络（CNN）和3D变压器（transformers）。
- en: There is, however, an intersection of these two data types, which are images
    extracted through video data. For this type of image data, it is possible to reduce
    the probability of predictive models making wrong predictions. ML models are not
    perfect predictors, so whenever there is a chance to reduce incorrect predictions
    such as false positives or false negatives without compromising the true predictions,
    do consider taking it. Consider using manual image processing techniques from
    the OpenCV library to perform any preliminary steps before a model takes the image
    as input. For example, the motion detection technique in OpenCV can be used as
    a preliminary condition checker before feeding the video array to the DL model.
    Since motion is required to identify most use cases of video data, it doesn’t
    make sense to predict anything if nothing is moving. This also reduces any false
    predictions that can happen from predicting on multiple unchanged video frames.
    The motion detector in OpenCV utilizes a simple change in pixel value without
    using a probabilistic model and thus is a far more reliable indicator of motion.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这两种数据类型之间是有交集的，那就是通过视频数据提取的图像。对于这种类型的图像数据，可以减少预测模型做出错误预测的概率。机器学习模型并不是完美的预测器，因此，每当有机会减少错误预测（如假阳性或假阴性）而不影响真实预测时，应该考虑采取这一机会。可以考虑使用来自OpenCV库的手动图像处理技术，在模型将图像作为输入之前执行任何初步步骤。例如，可以使用OpenCV中的运动检测技术作为喂给深度学习模型视频数组之前的初步条件检查器。由于大多数视频数据的使用案例需要运动，所以如果没有任何运动，预测任何东西就没有意义。这也减少了在多个未更改的视频帧上进行预测时可能出现的错误预测。OpenCV中的运动检测器通过简单的像素值变化来工作，而不使用概率模型，因此它是更可靠的运动指示器。
- en: Now that we’ve covered representing different data modalities, let’s move on
    to the topic of data augmentation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了不同数据模态的表示，接下来我们讨论数据增强的话题。
- en: Augmenting the data for training better DL models
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 增强数据以训练更好的深度学习模型
- en: 'Augmentation is widely used in DL to increase the generalization of the resulting
    trained model and increase the metric performance of the model. By accounting
    for the additional unique variations brought in by augmentation, the model would
    be able to attend to these unique variations on external data during the validation,
    testing, and inference stage. Naturally, this will also reduce any over-dependence
    on a specific pattern and any benefits that come with a sufficiently sized dataset.
    Augmentation increases the amount of training data and thus the variations of
    patterns in the training data. The process is usually done randomly and individually
    in every training iteration in memory. This makes sure there are no limitations
    to the additional data variations available for training and also removes the
    need for additional storage. However, you can’t just randomly use all the available
    types of augmentation known for a specific modality. The following list shows
    the different types of augmentation you can perform on your data:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在深度学习中被广泛使用，以提高训练模型的泛化能力并提升模型的度量性能。通过考虑增强带来的额外独特变异，模型将在验证、测试和推理阶段能够关注外部数据中的这些独特变异。自然，这也会减少模型对特定模式的过度依赖，并且消除足够大小数据集带来的好处。数据增强增加了训练数据的量，从而增加了训练数据中模式的变异性。这个过程通常在每次训练迭代时随机且独立地在内存中完成。这样可以确保训练数据的额外变异性没有限制，并且不需要额外的存储空间。然而，你不能仅仅随便使用所有已知的特定模态的数据增强类型。以下是你可以对数据进行的不同类型的数据增强：
- en: '**Image**: Image sharpening through **Contrast Limited Adaptive Histogram Equalization**
    (**CLAHE**), hue and saturation variation, color channel shuffling, contrast variation,
    brightness variation, horizontal/vertical flip, grayscale conversion, blurring,
    image masking, mixup (weighted combination of images and their labels), cutmix
    (mixup, but only by random patches from the original image), and more. Look into
    [https://github.com/albumentations-team/albumentations](https://github.com/albumentations-team/albumentations)
    for more than 70 augmentations!'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像**：通过**对比度限制自适应直方图均衡（CLAHE）**锐化图像，色相和饱和度变化，颜色通道打乱，对比度变化，亮度变化，水平/垂直翻转，灰度转换，模糊，图像遮罩，mixup（图像及其标签的加权组合），cutmix（mixup，但仅通过原始图像的随机补丁），等等。更多的增强方法可以查看[https://github.com/albumentations-team/albumentations](https://github.com/albumentations-team/albumentations)，其中有超过70种增强方法！'
- en: '**Text**: Synonym replacement, back translation (a process that translates
    a text into another language and then translates it back into the original language),
    and more.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本**：同义词替换，反向翻译（将文本翻译成另一种语言后，再翻译回原语言），等等。'
- en: '**Video**: Video mixup (same as image mixup but for videos), all the same augmentation
    for images.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频**：视频mixup（与图像mixup相同，但适用于视频），以及与图像相同的所有增强方法。'
- en: '`librosa` library.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`librosa` 库。'
- en: Choosing the type of augmentation to use requires some understanding of the
    expected environment that you will face when you deploy a model. Bad choices add
    noise to the model and might confuse the model during the training process, resulting
    in a degraded metric performance. Good choices revolve around estimating the variations
    that can realistically happen in the wild. Let’s take an example of a manufacturing
    use case where the goal is to deploy an image-based model that will predict product
    characteristics on a conveyor belt for sorting purposes using a camera sensor.
    If you can assume the camera will be fixed almost perfectly straight on the machine
    in all the setups, using image rotation augmentation likely wouldn’t be smart.
    Even if you want to use the augmentation, the rotation variation you should use
    should only be in the low end, such as below 10 degrees variation. Grayscale augmentation
    would also be unintuitive if the cameras do not provide grayscale images.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用何种增强类型需要对你在部署模型时将遇到的预期环境有一定的了解。错误的选择会为模型添加噪声，并可能在训练过程中让模型产生困惑，从而导致度量性能下降。好的选择则围绕估计在实际环境中可能出现的变异。举一个制造业应用的例子，目标是部署一个基于图像的模型，用于通过摄像头传感器预测生产线上的产品特征以进行分拣。如果假设摄像头在所有设置下几乎完美地固定直对机器，使用图像旋转增强可能就不太明智。即便你想使用该增强，旋转变化的范围也应仅限于较低的范围，比如小于10度的变化。如果摄像头不提供灰度图像，那么使用灰度增强也显得不太直观。
- en: This concludes the data preparation stage of training effective models. Next,
    we will dive into the model training stage of the workflow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着训练有效模型的数据准备阶段的结束。接下来，我们将深入探讨工作流程中的模型训练阶段。
- en: Configuring and tuning DL hyperparameters
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置和调优深度学习超参数
- en: Hyperparameter configuration and tuning play a crucial role in training DL models
    effectively. They control the learning process of the model and can significantly
    impact the model’s performance, generalization, and convergence. In this section,
    we will discuss some essential hyperparameters and their impact on training DL
    models.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数配置和调优在有效训练深度学习（DL）模型中起着至关重要的作用。它们控制着模型的学习过程，并能显著影响模型的性能、泛化能力和收敛性。本节将讨论一些重要的超参数及其对训练深度学习模型的影响。
- en: The most general set of impactful hyperparameters that need to be configured
    for training each NN model are its epochs, early stopping epochs, and learning
    rate. These three parameters are considered a set of parameters that together
    form a **learning schedule**. There have been a few notable learning schedules
    that focused on obtaining the best model with the least amount of time spent.
    However, they depend a lot on the initial estimation of the total number of epochs
    a model can converge with the method. Methods that depend on an estimation of
    the total number of epochs needed are fragile, and their configuration strategies
    are not easily transferable from one problem to another. Here, we will focus on
    using a validation dataset to track the number of epochs needed to achieve the
    best possible metric performance.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 训练每个神经网络（NN）模型时需要配置的最常见且有影响力的超参数是其训练轮次、早停轮次和学习率。这三个参数被视为一组参数，合在一起构成了**学习计划**。已有一些著名的学习计划，旨在以最少的时间获得最佳模型。然而，这些方法很大程度上依赖于对模型可以收敛的总轮次数的初步估计。依赖于对总轮次数估计的方法较为脆弱，其配置策略也不容易从一个问题迁移到另一个问题。在这里，我们将重点使用验证数据集来跟踪实现最佳度量性能所需的轮次数。
- en: Early stopping epochs is a parameter that controls how many epochs you want
    to keep training before you stop. This strategy means that the epochs’ hyperparameter
    can either be set to an infinite number or a very large number so that the best-performing
    model on the validation dataset can be found. Early stopping reduces the number
    of training epochs you need to spend dynamically based on the validation dataset.
    By saving the best-performing model weights on the validation dataset, when the
    model is stopped early, you will then be able to load the best-performing weights.
    The typical early stopping epoch is `10`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 早停轮次是一个控制你希望在停止之前继续训练多少轮的参数。该策略意味着，训练轮次的超参数可以设置为无限大或非常大的数字，以便找到在验证数据集上表现最好的模型。早停通过基于验证数据集动态减少所需的训练轮次。当模型提前停止时，通过保存验证数据集上表现最好的模型权重，你将能够加载这些最佳表现的权重。典型的早停轮次为`10`。
- en: As for the learning rate, there are two general directions that work consistently
    well in practice. One is to immediately start with a large learning rate such
    as `0.1` and to gradually decay the learning rate. The gradual decay of the learning
    rate can be through percentage reductions from validation score monitoring when
    it doesn’t improve after `3` to `5` epochs. The second method is to use a smaller
    learning rate as a warmup method in initializing a base weight for the NN before
    using method 1\. In the initial stage of learning, as models are initially in
    a randomized state, the learning process can be very unstable where the loss will
    seem to not follow a proper improvement trend. Using a warmup helps to initialize
    the foundation needed to make stable loss progressions. Note that if pre-trained
    weights are used to initialize the model, a warmup is usually not needed as the
    model will already be in a stable state, especially if the pre-trained weights
    are obtained from a similar dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 至于学习率，实际上有两个普遍有效的方向。一种方法是立即使用较大的学习率，比如`0.1`，并逐渐衰减学习率。学习率的逐渐衰减可以通过监控验证得分来实现，当验证得分在`3`到`5`轮后没有改善时，就减少一定比例的学习率。第二种方法是在使用第一种方法之前，先用较小的学习率作为暖身方法初始化神经网络（NN）的基本权重。在学习的初期，由于模型处于随机状态，学习过程可能非常不稳定，损失值似乎没有按照合理的改进趋势变化。使用暖身可以帮助初始化稳定的基础，从而使损失值稳定下降。需要注意的是，如果使用预训练权重来初始化模型，通常不需要暖身，因为模型已经处于稳定状态，特别是当预训练权重来自相似数据集时。
- en: Batch size is another crucial hyperparameter in the training of DL models, as
    it determines the number of training samples used in a single update of the model’s
    weights during the optimization process. The choice of batch size can significantly
    impact the model’s training speed, memory requirements, and convergence. Smaller
    batch sizes, such as 16 or 32, provide a more accurate estimate of the gradient,
    leading to more stable convergence, but may require more training iterations and
    can be slower due to less parallelism in computation. On the other hand, larger
    batch sizes, such as 128 or 256, increase the level of parallelism, speeding up
    the training process and reducing memory requirements, but may lead to a less
    accurate gradient estimate and potentially less stable convergence. In practice,
    it’s essential to experiment with different batch sizes to find the one that provides
    the best balance between training speed and convergence stability for your specific
    problem. Additionally, modern DL frameworks often support adaptive batch size
    techniques, which can automatically adjust the batch size during training to optimize
    the learning process.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小是深度学习模型训练中的另一个关键超参数，因为它决定了在优化过程中每次更新模型权重时使用的训练样本数量。批量大小的选择会显著影响模型的训练速度、内存需求和收敛性。较小的批量大小，如16或32，能提供更准确的梯度估计，从而实现更稳定的收敛，但可能需要更多的训练迭代，并且由于计算并行性较低，训练速度较慢。另一方面，较大的批量大小，如128或256，增加了并行性，能够加速训练过程并减少内存需求，但可能导致梯度估计不够准确，进而可能影响收敛的稳定性。实际上，实验不同的批量大小是非常重要的，以找到在训练速度和收敛稳定性之间提供最佳平衡的批量大小，针对你的具体问题。此外，现代深度学习框架通常支持自适应批量大小技术，可以在训练过程中自动调整批量大小，以优化学习过程。
- en: The foundational strategy discussed here is robust and can easily obtain the
    best-performing model most of the time while sacrificing some additional training
    time. It is worth noting that regularization methods, optimizers, and different
    activation functions have been covered in [*Chapter 2*](B18187_02.xhtml#_idTextAnchor040),
    *Designing Deep Learning Architectures*, and I encourage you to refer to that
    chapter for more information on those topics.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的基础策略是稳健的，大多数情况下能够轻松获得表现最佳的模型，尽管牺牲了一些额外的训练时间。值得注意的是，正则化方法、优化器和不同的激活函数已在[*第2章*](B18187_02.xhtml#_idTextAnchor040)，*深度学习架构设计*中讲解过，我鼓励你参考该章节以获取更多关于这些主题的信息。
- en: As much as there can be a manual strategy for the configuration of these hyperparameters,
    there will always be space to tune the hyperparameters further to optimize the
    metric performance of the model. Commonly, tuning can be executed through either
    grid search, random search, or tuning through more intelligent searching mechanisms.
    Grid search, otherwise known as brute-force searching, explores and validates
    all possible combinations of specified hyperparameter values to identify the optimal
    configuration for a given problem through cross-validation. For more intelligent
    tuning methods, refer back to [*Chapter 7*](B18187_07.xhtml#_idTextAnchor107),
    *Deep Neural Architecture Search*, for more insights on it. Additionally, as model
    evaluation metrics contribute to this hyperparameter-tuning process, we will explore
    more on this in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161), *Exploring Model*
    *Evaluation Methods*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管配置这些超参数可以有手动策略，但总会有进一步调优超参数的空间，以优化模型的指标表现。通常，调优可以通过网格搜索、随机搜索或更智能的搜索机制来执行。网格搜索，也称为暴力搜索，探索并验证所有指定超参数值的可能组合，通过交叉验证确定给定问题的最优配置。对于更智能的调优方法，请回到[*第7章*](B18187_07.xhtml#_idTextAnchor107)，*深度神经架构搜索*，获取更多的见解。此外，模型评估指标在这个超参数调优过程中起到重要作用，我们将在[*第10章*](B18187_10.xhtml#_idTextAnchor161)，*探索模型评估方法*中进一步探讨。
- en: The core of hyperparameter tuning depends on the process and workflow to iteratively
    execute, visualize, track, and compare modeling experiments, with each configuration
    being part of a modeling experiment. This brings us to the next topic, diving
    into the actual workflow of training DL models effectively.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调优的核心在于过程和工作流，能够迭代地执行、可视化、跟踪并比较建模实验，每个配置都是建模实验的一部分。这引出了下一个主题——有效训练深度学习模型的实际工作流。
- en: Executing, visualizing, tracking, and comparing experiments
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行、可视化、跟踪并比较实验
- en: The key to executing ML projects effectively is to iterate quickly between experiments.
    A lot of exploration is needed in any ML project, both in the initial stage of
    the project to gauge the viability of the use case, and in the later stage to
    improve the model’s performance. When this exploration process can be optimized,
    things meant to fail can fail quickly, and things that are viable can succeed
    quickly. Failure in ML projects is very common in practice. Once we acknowledge
    that and fail quickly, we can utilize the recovered time to tackle more valuable
    use cases. A good MLOps platform will help us execute, visualize, track, and compare
    experiments effectively and efficiently.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 高效执行ML项目的关键是快速在实验之间迭代。任何ML项目都需要大量的探索，无论是在项目的初期阶段，以评估用例的可行性，还是在后期阶段，以提高模型的性能。当这个探索过程能够得到优化时，本应失败的事情可以迅速失败，而有前景的事情可以迅速成功。在实际的ML项目中，失败是非常常见的。一旦我们承认这一点并快速失败，我们就可以利用恢复的时间来解决更有价值的用例。一个好的MLOps平台将帮助我们高效、有效地执行、可视化、追踪和比较实验。
- en: 'Let’s go through an example practically with the Iris dataset and an MLP using
    the MLflow MLOps platform. We will also be using the `catalyst` library, which
    is also considered to be an MLOps platform, albeit partially and mostly focused
    on providing common `pytorch` DL model training tools. Since `catalyst` provides
    most of the model versioning and model storing mechanisms, we will only utilize
    the tracking feature in MLflow. The steps for this example are as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际的例子来演示，使用Iris数据集和MLP模型，并使用MLflow MLOps平台。我们还将使用`catalyst`库，它也被认为是一个MLOps平台，尽管它只是部分地并主要集中于提供常见的`pytorch`深度学习模型训练工具。由于`catalyst`提供了大多数的模型版本控制和模型存储机制，我们将只利用MLflow中的追踪功能。这个例子的步骤如下：
- en: 'First, let’s import all the necessary libraries:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所有必要的库：
- en: '[PRE3]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, as we will be using the `pytorch`-based MLP, we will again set the random
    seed in `pytorch`:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，由于我们将使用基于`pytorch`的MLP，我们将在`pytorch`中再次设置随机种子：
- en: '[PRE4]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The dataset we will be using for the practical implementation here is the Iris
    dataset again. The dataset consists of the petal and sepal lengths of various
    flowers with three different iris types. We will now load this dataset:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在这里使用的实际实现数据集是Iris数据集。该数据集包含了不同种类的花卉的花瓣和萼片长度，以及三种不同的鸢尾花类型。现在让我们加载这个数据集：
- en: '[PRE5]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Scaling is a type of regularization method that can reduce memorization and
    reduce bias. Let’s perform a straightforward minimum and maximum scaling here:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放是一种正则化方法，可以减少记忆并减少偏差。让我们在这里执行简单的最小值和最大值缩放：
- en: '[PRE6]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To train a model, we need a proper cross-validation strategy to verify the
    validity and performance of the model. We will use 77% of the data for training
    and 33% for validation. Let’s prepare the data for cross-validation:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要训练一个模型，我们需要一个合适的交叉验证策略来验证模型的有效性和性能。我们将使用77%的数据用于训练，33%的数据用于验证。让我们为交叉验证准备数据：
- en: '[PRE7]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we will need to prepare the data loaders using the prepared data in `numpy`
    format for cross-validation:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用准备好的`numpy`格式数据为交叉验证准备数据加载器：
- en: '[PRE8]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since this is a multiclass problem of three classes, we will use the cross-entropy
    loss in `pytorch`:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个三类的多分类问题，我们将使用`pytorch`中的交叉熵损失函数：
- en: '[PRE9]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We will be using the `pytorch` high-level wrapper library called `catalyst`
    here. To train a model in `catalyst`, we have to define a model trainer class
    instance called a runner:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在这个项目中使用名为`catalyst`的`pytorch`高级封装库。在`catalyst`中训练一个模型时，我们必须定义一个名为runner的模型训练器类实例：
- en: '[PRE10]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will be using an MLP in this project with an MLP constructor class that
    allows us to specify the input data size, the hidden layer configuration, and
    the output data size. The hidden layer configuration is a list of layer sizes
    that simultaneously specifies the number of layers and the layer size at each
    layer. Let’s say that we want to randomly obtain 20 different hidden layer configurations
    and find out which performs the best on the validation partition. Let’s first
    define a method that generates the configuration randomly:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在本项目中使用MLP，并使用一个MLP构造类，它允许我们指定输入数据大小、隐藏层配置和输出数据大小。隐藏层配置是一个层大小的列表，它同时指定了层数和每一层的大小。假设我们想随机获得20种不同的隐藏层配置，并找出哪种配置在验证集上表现最好。首先，让我们定义一个方法，随机生成配置：
- en: '[PRE11]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let’s define a method that will allow us to train and evaluate the different
    MLP configurations:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个方法，允许我们训练和评估不同的MLP配置：
- en: '[PRE12]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The trial number here plainly differentiates the different experiments. Apart
    from layer configuration, we can also configure the epochs that we want to run.
    In this method, we will create an MLP model instance based on the layer configuration
    passed in:'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此处的试验编号清晰地区分了不同的实验。除了层配置外，我们还可以配置要运行的训练周期数。在这种方法中，我们将基于传入的层配置创建一个 MLP 模型实例：
- en: '[PRE13]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will use the `Adam` optimizer for gradient descent and set the checkpoint
    directory:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 `Adam` 优化器进行梯度下降，并设置检查点目录：
- en: '[PRE14]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will define the MLflow logger helper class available in `catalyst`
    to log experiments in MLflow format. In this setup, we log the mean and standard
    deviation of the training and validation log loss:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在 `catalyst` 中定义一个 MLflow 日志记录助手类，用于以 MLflow 格式记录实验数据。在这个设置中，我们记录训练和验证日志损失的均值和标准差：
- en: '[PRE15]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we will start the training process that trains for the specified number
    of epochs:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将开始训练过程，该过程将训练指定数量的训练周期：
- en: '[PRE16]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As the last code step, we will loop through each randomly generated layer configuration
    and perform the training and evaluation process:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最后的代码步骤，我们将循环遍历每个随机生成的层配置，并执行训练和评估过程：
- en: '[PRE17]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we need to start up the MLflow server service. We can do this by running
    the following command in the command line in the same directory as the directory
    that contains the introduced code:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要启动 MLflow 服务器服务。我们可以通过在与包含引入代码的目录相同的目录中运行以下命令来完成此操作：
- en: '[PRE18]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running this command, the same directory should contain the following
    file named `.catalyst`, which instructs `catalyst` to enable MLflow support. This
    file should have the following content:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此命令后，当前目录应该包含一个名为 `.catalyst` 的文件，该文件指示 `catalyst` 启用 MLflow 支持。该文件应包含以下内容：
- en: '[PRE19]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the command is executed, and by opening the HTTP website link, you should
    see the screen of MLflow, as shown in *Figure 8**.4*:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦命令执行并且通过打开 HTTP 网站链接，你应该能够看到 MLflow 的界面，如*图 8.4*所示：
- en: "![Figure 8.4 – M\uFEFFLflow interface](img/B18187_08_004.jpg)"
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – MLflow 界面](img/B18187_08_004.jpg)'
- en: Figure 8.4 – MLflow interface
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – MLflow 界面
- en: 'The interface shows a convenient way to visualize performance differences between
    different experiments while showing the utilized parameters. The numerical metric
    values can be sorted to obtain the best-performing model on the validation partition,
    as shown in the figure. The process of preparing data and training a model requires
    iterative comparisons to be made between different setups or experiments. Experiments
    can be compared more objectively with quantitative metrics with their experimentation
    parameters. When displayed visually automatically through code in an interface
    instead of plugging it manually into an Excel or Google sheet, this makes the
    process much more dependable and organized. Additionally, if you click into any
    of the experiments, you’ll be able to check out the loss curves at each epoch
    interactively, as shown in *Figure 8**.5*:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 该界面展示了一个方便的方式来可视化不同实验之间的性能差异，同时展示所使用的参数。数值度量值可以排序，从而获得在验证分区上表现最好的模型，如图所示。准备数据和训练模型的过程需要在不同设置或实验之间进行迭代比较。通过定量度量和实验参数，可以更客观地比较实验。当通过代码在界面中自动可视化展示，而不是手动插入到
    Excel 或 Google 表格时，这使得过程更加可靠和有序。此外，如果你点击任何一个实验，你将能够交互式查看每个训练周期的损失曲线，如*图 8.5*所示：
- en: "![Figure 8.5 – M\uFEFFLflow interface showing an example loss curve of the\
    \ best model](img/B18187_08_005.jpg)"
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.5 – MLflow 界面，显示最佳模型的示例损失曲线](img/B18187_08_005.jpg)'
- en: Figure 8.5 – MLflow interface showing an example loss curve of the best model
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 – MLflow 界面，显示最佳模型的示例损失曲线
- en: While ensuring speed of iteration, it is also required to organize and track
    all artifacts you generated for your model properly. This means that you need
    to version your model, your dataset, and any key components that affect the resulting
    model output. Artifacts can be model weights, metric performance reports, performance
    plots, embedding visualization, and loss visualization plots. This task can obviously
    be done manually through manual coding. However, organizing the artifacts of models’
    built-in experiments gets messy when the number of experiments goes up. Any custom
    files, graphs, and metrics can be tied into each of these experiment records and
    viewed in the MLflow interface. Experiments here can differ by using different
    models, different datasets, different featurization methods, different hyperparameters
    of a model, or a different sample size of the same dataset. Additionally, models
    can be stored directly in MLflow’s model registry, which allows MLflow to deploy
    the model directly.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在确保迭代速度的同时，还需要正确组织和跟踪你为模型生成的所有工件。这意味着你需要为你的模型、数据集以及任何影响最终模型输出的关键组件进行版本控制。工件可以是模型权重、指标性能报告、性能图、嵌入可视化和损失可视化图。显然，这项任务可以通过手动编码来完成。然而，当实验数量增加时，组织模型内置实验的工件会变得杂乱无章。任何自定义文件、图表和指标都可以绑定到每个实验记录中，并在MLflow界面中查看。这里的实验可以通过使用不同的模型、不同的数据集、不同的特征化方法、模型的不同超参数，或相同数据集的不同样本大小来区分。此外，模型还可以直接存储在MLflow的模型注册表中，这使得MLflow可以直接部署该模型。
- en: Exploring model-building tips
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索模型构建技巧
- en: 'This practical content serves as an example of how an MLOps platform such as
    MLflow can ease the process of building and choosing the right model programmatically
    and visually. As much as MLOps is great and helps in training models efficiently,
    there are a few things that an MLOps platform does not handle for you but are
    considered key components before a model can be properly utilized and have its
    predictions consumed. These components are listed next:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实际内容作为一个例子，展示了像MLflow这样的MLOps平台如何简化构建和选择正确模型的过程，既可以通过编程也可以通过可视化实现。尽管MLOps非常优秀并且有助于高效训练模型，但仍有一些事情是MLOps平台无法为你处理的，而这些事情在模型被正确利用并且预测结果被消费之前是关键的组成部分。以下是这些组件：
- en: '**Prediction consistency validation test**: This is a test that ensures the
    predictions made by the same trained model are consistent on the same data. A
    model’s predictions can’t be utilized if its logic is not deterministic. This
    will be discussed further in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161),
    *Exploring Model* *Evaluation Methods*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测一致性验证测试**：这是一个确保同一训练模型在相同数据上做出的预测是一致的测试。如果模型的预测逻辑不是确定性的，那么就不能利用该模型的预测。这个问题将在[*第10章*](B18187_10.xhtml#_idTextAnchor161)，“*探索模型*
    *评估方法*”中进一步讨论。'
- en: '`pytorch`, this can be done globally through the following code:'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch`，这可以通过以下代码全局实现：'
- en: '[PRE20]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In `tensorflow` and `keras`, this can be done globally through the following
    code:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`tensorflow`和`keras`中，这可以通过以下代码全局实现：
- en: '[PRE21]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This method automatically seeds both the `random` and `numpy` libraries. These
    global settings can help to set the random seed for layers that did not explicitly
    set random number generator seeds locally.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该方法会自动设置`random`和`numpy`库的种子。这些全局设置可以帮助为那些没有明确设置随机数生成器种子的层设置随机种子。
- en: One last piece of advice in experimentation is to make sure a baseline is created
    at the start of the project. A baseline is the simplest version of a solution
    possible. The solution can even be a non-DL model with simple features. Having
    a baseline can help ensure that any improvements or complications you add are
    justified by metric performance monitoring. Refrain from adding complications
    for the sake of them. Remember that the value of an ML project is not how complicated
    the process is but the results that can be extracted from it.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中的最后一条建议是确保在项目开始时创建基准线。基准线是可能的解决方案的最简单版本。这个解决方案甚至可以是一个非深度学习模型，具有简单的特征。拥有基准线可以帮助确保你添加的任何改进或复杂性都是通过指标性能监控来证明其合理性的。避免为了复杂而复杂。记住，机器学习项目的价值不在于过程的复杂性，而在于能够从中提取的结果。
- en: Next, we will dive into actual techniques that can be used to realize and improve
    a solution that utilizes DL.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨可以用于实现和改进利用深度学习的解决方案的实际技术。
- en: Exploring general techniques to realize and improve supervised deep learning
    based solutions
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索实现和改进监督式深度学习解决方案的通用技术
- en: Notice that earlier in the chapter we focused on use cases based on problem
    types and not the problems themselves. Solutions in turn solve and take care of
    the problem. DL and ML in general are great solvers of issues related to staffing
    difficulties and for the automation of mundane tasks. Furthermore, ML models in
    computers can process data much quicker than an average human can, allowing a
    much quicker response time and much more efficient scaling of any process. In
    many cases, ML models can help to increase the accuracy and efficiency of processes.
    Sometimes, they improve current processes, and other times, they make previously
    unachievable processes possible. However, a single DL model may or may not be
    enough to solve the problem. Let’s take an example of a solution that *can* be
    solved sufficiently with a single DL model.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本章前面，我们专注于基于问题类型的使用场景，而不是问题本身。解决方案则是解决并处理这些问题。深度学习和机器学习在解决人员配置困难和自动化日常任务方面非常有效。此外，计算机中的机器学习模型可以比普通人更快地处理数据，从而实现更快的响应时间和更高效的过程扩展。在许多情况下，机器学习模型能够提高过程的准确性和效率。有时，它们改善当前的流程，其他时候，它们使得过去无法实现的流程变为可能。然而，单一的深度学习模型可能不足以解决问题。让我们来看一个可以通过单一深度学习模型充分解决的例子。
- en: Consider the use case of using a DL model to predict the genders of babies with
    ultrasound imagery. Traditionally, a doctor would perform a visual-based gender
    analysis of the resulting ultrasound imagery of a baby in the mother’s womb in
    real time and offline before finally providing their prediction of the gender.
    Based on the amount of prior experience and knowledge, the doctor would have different
    levels of competency and accuracy in decoding the gender. Things might get more
    complicated when there are abnormalities in the baby. The probable underlying
    problem would be that experienced and capable doctors are scarce and expensive
    to hire. If we had a system that could decode the gender from the ultrasound imagery
    automatically, it would either be of good assistance to the judgment of real doctors
    or a replacement as a cheaper alternative. The same analogies can be applied to
    identifying diseases or symptoms in any advanced imaging results such as X-ray
    images.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑使用深度学习模型来预测婴儿性别的使用场景，模型通过超声波图像来进行预测。传统上，医生会基于视觉对母亲子宫中婴儿的超声波图像进行性别分析，既有实时的分析，也有离线的分析，最后给出性别预测。根据以往的经验和知识，医生在解读性别时会有不同的能力和准确性。当婴儿出现异常情况时，事情可能变得更加复杂。潜在的根本问题是，经验丰富且能力强的医生稀缺且聘用成本高。如果我们有一个系统可以自动从超声波图像中解码性别，这将能为医生的判断提供很好的辅助，或者作为一种更便宜的替代品，取而代之。同样的类比也可以应用于识别任何高级影像结果中的疾病或症状，例如X光图像。
- en: This example depicts a DL model as a component of a solution and a solution
    where a single DL model is enough to obtain the desired output. This is an example
    of staffing issues but not so much on the efficiency side. Note that for some
    use cases, it is required to explain in some form the reasons that drove the predictions
    that were made. In other words, you’d have to explain the decisions that were
    made by the model. To provide assistance to a doctor, pinpointing where and which
    types of patterns contributed to the decision would be more helpful than the decision
    itself, as doctors would be able to utilize the extra information to make their
    own decisions. This will be thoroughly introduced in [*Chapter 11*](B18187_11.xhtml#_idTextAnchor172),
    *Explaining Neural* *Network Predictions*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子将一个深度学习模型描绘为解决方案的一个组成部分，并展示了在某些情况下，单一的深度学习模型足以获得期望的输出。这是一个关于人员配置问题的例子，但在效率方面的讨论较少。请注意，对于某些使用场景，可能需要以某种形式解释驱动预测的原因。换句话说，你需要解释模型做出决策的理由。为了帮助医生，指出哪些模式及其类型对决策产生了影响，比单纯给出决策本身更有帮助，因为医生可以利用这些额外的信息做出自己的判断。这将在[*第11章*](B18187_11.xhtml#_idTextAnchor172)，*神经网络预测的解释*
    中详细介绍。
- en: Explanations aside, not all solutions to problems can be accomplished by a single
    ML or DL model alone. At times, DL models have to be coupled with general ML methods,
    and at others, multiple DL models have to be coupled together. In some special
    cases, intermediate data needs to be specially processed and prepared before feeding
    it to the next task in a pipeline. Creating and architecting logical pipelines
    are essential when dealing with such problems. Let’s take an example of a problem
    and solution that requires multiple datasets, multiple DL models, and constructing
    a task pipeline.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解释之外，并非所有问题的解决方案都能仅通过单一的机器学习（ML）或深度学习（DL）模型来完成。有时候，深度学习模型需要与一般的机器学习方法结合使用，而在其他情况下，需要将多个深度学习模型组合在一起。在某些特殊情况下，必须对中间数据进行特殊处理和准备，然后再将其输入到管道中的下一个任务。在处理此类问题时，创建和构建逻辑管道是至关重要的。让我们以一个需要多个数据集、多个深度学习模型以及构建任务管道的例子来说明问题和解决方案。
- en: 'Consider the problem of finding criminals who have just robbed a bank. By using
    CCTV cameras deployed in the city, you can use a face detection and recognition
    solution if you have identified the face of the criminals that did the robbery.
    The following figure shows an example solution task pipeline for this problem:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个问题：找到刚刚抢劫银行的罪犯。通过使用部署在城市中的 CCTV 摄像头，如果你已经识别出抢劫犯的面孔，可以使用人脸检测和识别解决方案。下图展示了该问题的解决方案任务管道示例：
- en: '![Figure 8.6 – Task pipeline of the solution for finding criminals through
    CCTV cameras](img/B18187_08_006.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 通过 CCTV 摄像头寻找罪犯的任务管道](img/B18187_08_006.jpg)'
- en: Figure 8.6 – Task pipeline of the solution for finding criminals through CCTV
    cameras
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 通过 CCTV 摄像头寻找罪犯的任务管道
- en: Face detection is an image-object-detection process where there is an image
    bounding box regressor and a binary classifier that predicts whether the bounding
    box is a face or not. The representative facial features extraction utilizes a
    DL model that can be trained using supervised representation learning methods
    that are trained against the goal of optimizing the discriminative effects of
    facial features against the facial features of different persons. Next, a separate
    task is needed to build the database of criminal facial features that will be
    passed into the KNN ML algorithm to find the matched facial ID based on queried
    facial features obtained from CCTV cameras deployed in the city. This solution
    shows the need to break a solution into multiple components in order to obtain
    the final result of finding the criminals.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 人脸检测是一个图像物体检测过程，其中包括一个图像边界框回归器和一个二分类器，用于预测该边界框是否为人脸。代表性的人脸特征提取利用深度学习模型，可以使用监督式表示学习方法进行训练，目的是优化人脸特征在不同个体间的区分效果。接下来，需要一个单独的任务来构建罪犯人脸特征的数据库，并将其传递给
    KNN 机器学习算法，以便根据从城市 CCTV 摄像头获得的查询人脸特征，找到匹配的人脸 ID。这个解决方案展示了将一个解决方案分解为多个组件的必要性，以最终实现寻找罪犯的目标。
- en: The preceding example is part of a larger paradigm called multitask learning
    and multitask problems. The multitask paradigm is a set of topics that allows
    for greater advancement in the ML space, not only for DL but definitely much more
    achievable through DL, due to its inherent flexibility. In the next topic, we
    will dive into the multitask paradigm.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例是更大范式的一部分，称为多任务学习和多任务问题。多任务范式是一系列主题，它不仅能够推动机器学习领域的更大发展，尤其是在深度学习中，由于其固有的灵活性，这一切变得更加可实现。在下一个主题中，我们将深入探讨多任务范式。
- en: Breaking down the multitask paradigm in supervised deep learning
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在监督式深度学习中分解多任务范式
- en: 'Multitask is a paradigm that covers a wide spectrum of tasks that involves
    the execution of ML models on multiple problems coupled with their respective
    datasets to achieve a goal. This paradigm is usually built based on two reasons:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务是一种涵盖广泛任务的范式，它涉及在多个问题上执行机器学习模型，并将相应的数据集与之结合，以实现目标。这个范式通常基于以下两个原因构建：
- en: To achieve better predictive performance and generalization.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了获得更好的预测性能和泛化能力。
- en: To break down complicated goals into smaller tasks that are directly solvable
    using separate ML models. This reiterates the point made in the previous topic.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的目标分解为可以通过单独的机器学习模型直接解决的小任务。这重申了前一个主题中的观点。
- en: Let’s dive into four multitask techniques, starting with multitask pipelines.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨四种多任务技术，从多任务管道开始。
- en: Multitask pipelines
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多任务管道
- en: This variation of multitask systems revolves around realizing solutions that
    can’t be directly solved by using a single ML model. Breaking down highly complicated
    tasks into smaller tasks can allow solutions to be made with multiple ML models
    handling different smaller tasks. These tasks can be sequential or parallel in
    their paths and generally form a **directed acyclic graph** (**DAG**)-like pipeline,
    similar to the example shown in *Figure 8**.6*.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多任务系统的变种围绕着实现那些无法通过单一ML模型直接解决的方案。将高度复杂的任务分解成较小的任务，可以通过多个ML模型处理不同的小任务来提供解决方案。这些任务可以是顺序的或并行的，并且通常形成一个**有向无环图**（**DAG**）样的管道，类似于*图8.6*中所示的例子。
- en: However, this does not mean that the tasks should exclusively be ML models.
    Problems for different industries and businesses can be in many forms, and being
    flexible in assigning components needed to produce a solution is key to deriving
    value from ML technology. For example, if human supervision is needed to accomplish
    a certain task after breaking down the larger task, do not hesitate to utilize
    it along with ML models to achieve value. Let’s go through another use case that
    utilizes multitask pipelines to create a solution, which is **recommendation systems**.
    First, we need to perform either supervised or unsupervised representation learning
    for feature extraction. Second, using the features extracted, create a database
    used to match extracted query features. Third, obtain the top-k closest data from
    the database and apply a regression model to predict the rank of the top-k data
    for fine-tuned e-tuned high-performance ranking.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不意味着任务必须仅限于机器学习（ML）模型。不同产业和业务的问题可以有多种形式，灵活地分配所需组件以产生解决方案是从ML技术中提取价值的关键。例如，如果在分解大任务后需要人工监督来完成某个任务，不要犹豫，利用人工监督与ML模型共同作用来实现价值。让我们通过另一个使用多任务管道来创建解决方案的用例——**推荐系统**，来进一步了解。首先，我们需要进行有监督或无监督的表示学习以提取特征。其次，利用提取的特征，创建一个用于匹配查询特征的数据库。第三，从数据库中获取排名前k的最接近数据，并应用回归模型来预测前k数据的排名，以进行高效的高性能排名微调。
- en: Next, we will discover another paradigm of multitasking, called **TL**.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索另一种多任务范式，称为**迁移学习**（TL）。
- en: TL
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**迁移学习**（TL）'
- en: 'TL is a technique that involves using what was learned from one task in another
    task. The core reasons can be one of the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**（TL）是一种技术，涉及将从一个任务中学到的知识应用到另一个任务中。其核心原因可以是以下之一：'
- en: Increasing the metric performance.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高度量性能。
- en: Decreasing the required number of epochs needed for the network to reach a state
    of convergence.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少网络达到收敛状态所需的epoch数量。
- en: Allowing more stable learning trajectories. In other cases, networks just take
    longer to converge when the initial learning process is unstable. However, in
    some other cases, networks cannot converge at all when networks don’t have a stable
    foundation to start learning. TL can help models that originally fail to learn
    anything reach convergence in the learning process.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许更稳定的学习轨迹。在其他情况下，当初始学习过程不稳定时，网络可能需要更长的时间才能收敛。然而，在一些情况下，当网络没有稳定的基础开始学习时，网络根本无法收敛。迁移学习可以帮助那些最初无法学习的模型在学习过程中达到收敛。
- en: Increasing generalization and reducing the probability of overfitting. When
    the second task involves only a small subset of variations from the actual data
    population, knowledge learned from a first task that covers a wider range of variations
    helps to prevent narrow oversights.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高泛化能力并减少过拟合的概率。当第二个任务仅涉及来自实际数据集的一小部分变化时，从第一个任务中学到的知识（该任务涵盖了更广泛的变化范围）有助于防止狭隘的忽视。
- en: Concretely, TL in DL is achieved by using the network parameters that were learned
    from the first task in the second task. The parameters involve all the weights
    and biases that are associated with a network. The parameters can be used as an
    initialization step for the same network instead of the usual randomly initialized
    parameters. These are known as **pre-trained weights**. The process of network
    learning with pre-trained weights is called **fine-tuning**. Additionally, the
    network parameters can also opt to be completely frozen and plainly act as a **featurizer**
    component that provides features for another SL algorithm.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，DL中的TL是通过在第二个任务中使用从第一个任务中学到的网络参数来实现的。这些参数涉及与网络相关的所有权重和偏置。可以将这些参数作为初始化步骤，用于同一个网络，而不是使用通常的随机初始化参数。这些被称为**预训练权重**。使用预训练权重进行网络学习的过程称为**微调**。此外，网络参数也可以选择完全冻结，并仅作为一个**特征提取器**组件，为另一个监督学习算法提供特征。
- en: 'There are a couple of automated strategies that focus on improving the results
    you can get with fine-tuning. However, these methods are not silver bullets and
    can take a lot of time to carry out. The practical strategy to achieve a better
    performance using TL is to choose the number of layers you want to train by gauging
    the transferability component of the two tasks. *Table 8.2* shows an easy way
    to decide on a TL strategy based on task similarity and dataset size of the second
    task:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种自动化策略，专注于提高微调后的结果。然而，这些方法并非万能，执行起来可能需要大量时间。通过TL实现更好性能的实际策略是，通过评估两个任务的迁移能力来选择要训练的层数。*表8.2*展示了根据任务相似性和第二个任务的数据集大小，决定TL策略的简易方法：
- en: '|  | **Dataset Size** |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | **数据集大小** |'
- en: '| **Small** | **Big** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| **小** | **大** |'
- en: '| Task similarity | Low | Train the entire network as usual. | Train the entire
    network as usual. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 任务相似性 | 低 | 按照常规训练整个网络。 | 按照常规训练整个网络。 |'
- en: '| High | Freeze all base network parameters, add an extra linear prediction
    layer, and only train this linear layer on the dataset. | Train the entire network
    as usual. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 冻结所有基础网络参数，增加一个额外的线性预测层，只在数据集上训练该线性层。 | 按照常规训练整个网络。 |'
- en: Table 8.2 – Deep TL (DTL) strategy guide
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2 – 深度TL（DTL）策略指南
- en: For clarity purposes, let’s say “big” is when the dataset has at least 10,000
    examples. For task transferability/similarity, human intuition is required to
    obtain an evaluation on a case-by-case basis. Here in the guide, we assume that
    a big dataset size means a dataset with large variations that represent the population
    adequately. A hidden component not presented in the preceding figure, however,
    is the size of the dataset of the first task. TL has the best impact when the
    task similarity is high, the second task dataset size is small, and, additionally,
    when the dataset size of the first task is big. The size of the first dataset
    usually also limits the range of NN sizes that can be used. Let’s say that the
    first dataset size is small; the best-performing models in this case are usually
    smaller-sized models. When TL is highly beneficial, even when the dataset size
    of the second dataset size is medium or big, the smaller models can still outperform
    bigger-sized models. An act of balancing is required in complex cases such as
    this to obtain the ideal model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，假设“大的”数据集是指数据集至少包含10,000个样本。对于任务的迁移能力/相似性，需要依靠人类直觉来进行逐个案例的评估。在本指南中，我们假设大的数据集意味着数据集具有足够的变化，能够充分代表总体。然而，在前面的图中没有展示的一个隐藏因素是第一个任务的数据集大小。当任务相似性较高、第二个任务的数据集较小、并且第一个任务的数据集较大时，TL会产生最佳效果。第一个数据集的大小通常也限制了可以使用的神经网络的规模。假设第一个数据集的大小较小；在这种情况下，表现最佳的通常是较小的模型。当TL非常有益时，即使第二个数据集的数据集大小为中等或较大，较小的模型仍然可以超越较大的模型。在这种复杂情况下，需要进行平衡才能获得理想的模型。
- en: 'One prominent issue in TL is called **catastrophic forgetting**. This is a
    phenomenon where the network performance regresses to earlier tasks as the network
    trains on new tasks. If the performance of the previous task is not of concern
    to you, this issue can be ignored. Practically, if it is required to maintain
    the performance of the previous task, you can follow these steps:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: TL中的一个显著问题被称为**灾难性遗忘**。这是一个现象，当网络在新的任务上训练时，其在早期任务上的性能会退步。如果你不关心之前任务的性能，这个问题可以忽略不计。实际上，如果需要保持之前任务的性能，你可以按照以下步骤操作：
- en: Use a unified metric that takes care of the performance of the first task and
    second task by additionally validating on the validation dataset of the first
    task.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用统一的指标，通过额外在第一个任务的验证数据集上进行验证，来处理第一个任务和第二个任务的性能。
- en: Combine the dataset from the first task and second task and train it as a single
    model. If the targets are not relevant to each other, use different fully connected
    layer prediction heads.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第一个任务和第二个任务的数据集结合起来，并作为单一模型进行训练。如果这些目标彼此不相关，则使用不同的全连接层预测头。
- en: Lastly, there is an additional popular technique for TL known as **knowledge
    distillation**. This method involves two models, where one pre-trained teacher
    model is used to distill its knowledge to a student model. Typically, the teacher
    model is a bigger model that has the capacity to learn more accurate information
    but is slower in runtime, and the student model is a smaller model that can be
    run at reasonable speeds during runtime. The method distills knowledge by using
    an additional similarity-based loss of a chosen layer output between the teacher
    and student model, which is typically the logit layer, on top of the base cross-entropy
    loss. This method encourages the student model to produce similar features to
    the teacher model. The technique is typically used to obtain a smaller model with
    better accuracy than if trained without knowledge distillation, so the deployment
    infrastructure can be cheaper. This technique will be practically introduced in
    [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    as a key technique to also mitigate bias.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一种在迁移学习（TL）中流行的技术，称为**知识蒸馏**。该方法涉及两个模型，其中一个预训练的教师模型用于将其知识蒸馏到学生模型中。通常，教师模型是一个较大的模型，能够学习更准确的信息，但运行速度较慢，而学生模型是一个较小的模型，在运行时能以合理的速度运行。该方法通过使用额外的基于相似性的损失来蒸馏知识，这通常是教师模型和学生模型之间选定层输出的损失，通常是对数层，在基础交叉熵损失的基础上进行计算。该方法鼓励学生模型生成与教师模型相似的特征。通常，这项技术用于获得一个比没有知识蒸馏时训练出的更小的、精度更高的模型，从而使部署基础设施可以更为经济。这项技术将在[*第13章*](B18187_13.xhtml#_idTextAnchor196)《探索偏见与公平》中作为关键技术进行实际介绍，以减轻偏见。
- en: Next, we will dive into another type of multitask execution, called multiple
    objective learning.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨另一种多任务执行方式，称为多目标学习。
- en: Multiple objective learning
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多目标学习
- en: 'Multiple objective learning is a type of multitasking process that involves
    training with simultaneously different goals. Different goals direct the learning
    trajectory of a network toward different paths. Multiple objective learning can
    be further broken down into the following options:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标学习是一种多任务处理过程，涉及同时训练多个不同的目标。不同的目标引导网络的学习轨迹走向不同的路径。多目标学习可以进一步细分为以下几种选项：
- en: Multiple losses on the same outputs.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相同输出上进行多重损失计算。
- en: 'Multiple targets, which are taken care of by separate NN prediction heads,
    each with their respective losses. This can be further broken down into the following
    categories:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个目标，由独立的神经网络预测头处理，每个预测头都有其各自的损失。这可以进一步细分为以下几类：
- en: Multiple targets with real impact and usage.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有实际影响和应用的多个目标。
- en: A single or multiple main targets and a single or multiple auxiliary targets.
    Auxiliary targets are paired with their own losses called auxiliary losses.
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个主要目标和一个或多个辅助目标。辅助目标与各自的损失配对，这些损失被称为辅助损失。
- en: Aside from option *2(A)*, the other options (that is, *1* and *2(B)* ) for multiple
    objective learning are mainly used to improve metric performance. Metrics can
    be as simple as accuracy or log loss, or more intricate, such as the degree of
    bias toward a minority class. A simple more straightforward example of multiple
    objective learning is the multilabel target type. Multilabel is where multiple
    labels can be associated with a single data row. This means that the setup will
    be a multiple binary classification target, which is the case with *2(A)*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选项 *2(A)*，其他选项（即 *1* 和 *2(B)*）主要用于提高指标性能。指标可以像准确率或对数损失这样简单，或更复杂，例如偏向少数类别的程度。一个更简单、直接的多目标学习示例是多标签目标类型。多标签是指一个数据行可以关联多个标签。这意味着设置将是多个二分类目标，这正是
    *2(A)* 的情况。
- en: Multiple targets and their associated losses mean there might be issues of conflicting
    gradients during the learning process. This phenomenon is more commonly known
    as **negative transfer**. A more extreme case of negative transfer is when gradients
    from the two losses cancel each other out when they have the same magnitude in
    exactly opposite directions. This will block the learning process of the model
    where the model will never converge. In reality, this issue can be at a lower
    scale and dampen the speed of convergence or, worse, introduce huge fluctuations
    that make it hard to learn anything. Unfortunately, there are no silver-bullet
    mitigation methods here other than to understand the background behind why a model
    learns poorly. Iterative experiments are usually required to figure out how to
    balance these losses properly to encourage a stable learning process.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 多个目标及其相关的损失意味着在学习过程中可能会出现梯度冲突问题。这种现象通常被称为**负迁移**。负迁移的一个极端情况是，当两个损失的梯度大小相同且方向完全相反时，它们会相互抵消。这会阻碍模型的学习过程，使模型永远无法收敛。实际上，这个问题可能表现为较低的规模，并减缓收敛速度，或者更糟糕的是，可能引入巨大的波动，导致模型无法学习到任何东西。不幸的是，除了理解模型学习不佳的背景原因外，没有什么“灵丹妙药”的缓解方法。通常需要反复实验，找出如何平衡这些损失，以鼓励稳定的学习过程。
- en: Next, we will dive into multimodal NN training.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨多模态神经网络的训练。
- en: Multimodal NN training
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态神经网络训练
- en: 'Multimodal NNs are a type of multitask system in the sense that networks responsible
    for different modalities learn in the same task in completely different paths.
    A common method of handling multimodality in NNs is to assign different neural
    blocks at the initial stage for different data modalities. Neural blocks contain
    the networks specific to each modality. The neural blocks for different modalities
    will then be merged using a series of intermediate fully connected layers and
    an output fully connected layer. This is depicted in *Figure 8**.7*:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态神经网络是一种多任务系统，因为负责不同模态的网络在同一任务中以完全不同的路径进行学习。处理多模态神经网络的常见方法是在初始阶段为不同的数据模态分配不同的神经网络块。神经网络块包含每个模态特定的网络。不同模态的神经网络块随后会通过一系列中间全连接层和输出全连接层进行合并。如*图
    8.7*所示：
- en: '![Figure 8.7 – Typical multimodal NN structure](img/B18187_08_007.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.7 – 典型的多模态神经网络结构](img/B18187_08_007.jpg)'
- en: Figure 8.7 – Typical multimodal NN structure
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7 – 典型的多模态神经网络结构
- en: 'The idea of leveraging multimodality is that the additional data input can
    allow for more comprehensive patterns to be identified and thus should improve
    the overall metric performance. In reality, this commonly will not be the case
    without careful handling of the training process. Different modalities exist in
    entirely different distributions and learn at different rates with different paths.
    A single global optimization strategy applied to all the data modalities will
    likely produce suboptimal results. A common and effective strategy is to do the
    following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 利用多模态的想法是，额外的数据输入可以帮助识别更全面的模式，从而应该提高整体指标的表现。实际上，如果没有仔细处理训练过程，这种情况通常不会发生。不同的模态存在于完全不同的分布中，并且以不同的速度和不同的路径进行学习。对所有数据模态应用单一的全局优化策略可能会产生次优的结果。一种常见且有效的策略是：
- en: Pretrain the individual modality neural block (unimodal) with a temporary prediction
    output layer until a certain degree of convergence
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练单独的模态神经网络块（单模态）并使用临时预测输出层，直到达到一定程度的收敛
- en: Remove the temporary prediction output layer and train the multimodal NN as
    usual with the pre-trained weights from the unimodal training process
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除临时预测输出层，并像往常一样使用单模态训练过程中的预训练权重来训练多模态神经网络（NN）
- en: Other than that, freezing the weights of the unimodal trained NN and only training
    the multimodal aggregation fully connected layer prediction head is also a sound
    strategy. Many more complex strategies exist to tackle this issue but are out
    of scope in this book.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，冻结单模态训练神经网络的权重，只训练多模态聚合的全连接层预测头也是一种有效策略。还有许多更复杂的策略来解决这个问题，但它们超出了本书的范围。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored supervised deep learning, including the types of
    problems it can be used to solve and the techniques for implementing and training
    DL models. Supervised deep learning involves training a model on labeled data
    to make predictions on new data. We also covered a variety of supervised learning
    use cases on different problem types, including binary classification, multiclassification,
    regression, and multitask and representation learning. The chapter also covered
    techniques for training DL models effectively, including regularization and hyperparameter
    tuning, and provided practical implementations in the Python programming language
    using popular DL frameworks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了有监督的深度学习，包括它可以解决的问题类型和实现及训练深度学习模型的技术。有监督的深度学习涉及在标记数据上训练模型，以对新数据进行预测。我们还介绍了多种有监督学习的应用案例，涵盖了不同问题类型，包括二分类、多分类、回归、以及多任务学习和表示学习。本章还涵盖了有效训练深度学习模型的技术，包括正则化和超参数调优，并提供了使用流行的深度学习框架在Python编程语言中的实际实现。
- en: Supervised deep learning can be used for a wide range of real-world applications
    in tasks such as image classification, **natural language processing** (**NLP**),
    and speech recognition. With the knowledge provided in this chapter, you should
    be able to identify supervised learning applications and train DL models effectively.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督的深度学习可以广泛应用于图像分类、**自然语言处理**（**NLP**）和语音识别等实际任务。通过本章提供的知识，你应该能够识别有监督学习的应用，并有效地训练深度学习模型。
- en: In the next chapter, we will explore **unsupervised learning** for DL.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨深度学习中的**无监督学习**。
