- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Harnessing Large Language Models for Graph Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用大型语言模型进行图学习
- en: Traditionally, **graph neural networks** ( **GNNs** ) have been the workhorse
    for graph learning tasks, achieving impressive results. However, recent research
    explores the exciting potential of **large language models** ( **LLMs** ) in this
    domain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，**图神经网络**（**GNNs**）一直是图学习任务的主力，取得了显著的成果。然而，最近的研究探索了**大型语言模型**（**LLMs**）在该领域的巨大潜力。
- en: In this chapter, we’ll delve into the intersection of LLMs and graph learning,
    exploring how these powerful language models can enhance graph-based tasks. We’ll
    begin with an overview of LLMs, followed by a discussion of the limitations of
    GNNs and the motivations for incorporating LLMs. Then, we’ll explore various approaches
    for utilizing LLMs in graph learning, the intersection of **retrieval-augmented
    generation** ( **RAG** ) with graphs, and explain the advantages and challenges
    associated with this integration.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨LLMs与图学习的交集，探讨这些强大的语言模型如何提升基于图的任务。我们将从LLMs的概述开始，随后讨论GNNs的局限性以及引入LLMs的动机。接下来，我们将探讨在图学习中使用LLMs的各种方法，**检索增强生成**（**RAG**）与图的交集，并解释这种整合的优势和挑战。
- en: 'In this chapter, we’ll explore the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨以下主题：
- en: Understanding LLMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLMs
- en: Textual data in graphs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中的文本数据
- en: LLMs for graph learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图学习中的LLMs
- en: Integrating RAG with graph learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将RAG与图学习结合
- en: Challenges in integrating LLMs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成LLMs的挑战
- en: Understanding LLMs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LLMs
- en: LLMs are a significant advancement in **artificial intelligence** ( **AI** ),
    particularly in **natural language processing** ( **NLP** ) and understanding.
    These models are designed to understand, generate, and interact with human language
    in a way that’s both meaningful and contextually relevant. The development and
    evolution of LLMs have been marked by a series of innovations that have expanded
    their capabilities and applications across various domains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是**人工智能**（**AI**）中的一项重大进展，特别是在**自然语言处理**（**NLP**）和理解方面。这些模型旨在以一种既有意义又具有上下文相关性的方式理解、生成和与人类语言互动。LLMs的发展和演化标志着一系列创新，这些创新扩大了其在各个领域的能力和应用。
- en: At their core, LLMs are trained on vast datasets of text from the internet,
    books, articles, and other sources of written language. This training involves
    analyzing patterns, structures, and the semantics of language, enabling these
    models to generate coherent, contextually appropriate text based on the input
    they receive. The training process relies on deep learning techniques, particularly
    neural networks, which allow the models to improve their language capabilities
    over time through exposure to more data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，LLMs是在互联网上的海量文本数据集、书籍、文章和其他书面语言来源上进行训练的。该训练过程涉及分析语言的模式、结构和语义，使得这些模型能够根据接收到的输入生成连贯且符合上下文的文本。训练过程依赖于深度学习技术，特别是神经网络，允许模型通过接触更多数据来不断改进其语言能力。
- en: One of the key characteristics of LLMs is their size, which is measured in the
    number of parameters they contain. Early models had millions of parameters, but
    the most advanced models today boast tens or even hundreds of billions of parameters.
    This increase in size has been correlated with a significant improvement in the
    models’ ability to understand and generate human-like text, making them more effective
    for a wide range of applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的一个关键特点是它们的规模，通常通过其包含的参数数量来衡量。早期的模型只有数百万个参数，但如今最先进的模型拥有数十亿甚至上百亿个参数。规模的增加与模型理解和生成类似人类文本的能力显著提高相关，这使得它们在广泛的应用中更为有效。
- en: LLMs have a wide array of applications, from simple tasks such as grammar correction
    and text completion to more complex ones such as writing articles, generating
    code, translating languages, and even creating poetry or prose. They’re also used
    in conversational agents, providing the backbone for chatbots and virtual assistants,
    which can engage in more natural and meaningful interactions with users.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs有广泛的应用，从简单的任务，如语法纠正和文本补全，到更复杂的任务，如写文章、生成代码、翻译语言，甚至创作诗歌或散文。它们还被用于对话代理，作为聊天机器人和虚拟助手的核心，能够与用户进行更自然和有意义的互动。
- en: 'The evolution of LLMs has been marked by significant milestones:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的发展经历了几个重要的里程碑：
- en: '**1990s** : The era of **statistical language models** ( **SLMs** ) began,
    utilizing **n-gram models** to predict the next word in a sequence based on a
    few preceding words. These models faced challenges with high-order predictions
    due to data sparsity issues.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1990年代** ：**统计语言模型**（**SLMs**）的时代开始，利用**n-gram模型**基于前面几个词预测序列中的下一个词。由于数据稀疏问题，这些模型在高阶预测方面面临挑战。'
- en: '**Early 2000s** : The introduction of **neural language models** ( **NLMs**
    ), which employed neural networks such as **multilayer perceptrons** ( **MLPs**
    ) and **recurrent neural networks** ( **RNNs** ), marked a shift toward understanding
    deeper linguistic relationships.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2000年代初** ：**神经语言模型**（**NLMs**）的引入，采用了如**多层感知机**（**MLPs**）和**递归神经网络**（**RNNs**）等神经网络，标志着向深入理解语言关系的转变。'
- en: '**2010s** : The development of word embeddings such as **Word2Vec** and **GloVe**
    , which represent words in continuous vector spaces, allowed models to capture
    semantic meaning and context.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2010年代** ：词嵌入技术的发展，如**Word2Vec**和**GloVe**，它们将词语表示为连续的向量空间，使得模型能够捕捉语义意义和上下文。'
- en: '**2017** : Transformer architecture was introduced, leading to a breakthrough
    in handling sequential data without the need for recurrent processing. This architecture
    is the foundation of many subsequent LLMs.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2017** ：变换器架构的引入，使得处理序列数据取得突破，无需递归处理。这一架构是许多后续LLM的基础。'
- en: '**2018** : The **Generative Pre-Trained Transformer** ( **GPT** ) was released
    by OpenAI, showcasing the power of transformers in language understanding and
    generation.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2018** ：**生成预训练变换器**（**GPT**）由OpenAI发布，展示了变换器在语言理解和生成方面的强大能力。'
- en: '**2019** : **Bidirectional Encoder Representations from Transformers** ( **BERT**
    ) by Google revolutionized the field by introducing a model trained to understand
    the context from both directions in a piece of text.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2019** ：**双向编码器表示法来自变换器**（**BERT**）由谷歌提出，革命性地引入了一个能够从文本中双向理解上下文的模型。'
- en: '**2020s** : Even larger models began to emerge, such as **GPT-3** , which demonstrated
    remarkable capabilities in generating human-like text, and models that can integrate
    LLMs with other AI fields were introduced, such as graph learning.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2020年代** ：更大规模的模型开始出现，如**GPT-3**，展现了生成类人文本的卓越能力，同时出现了能够将LLM与其他AI领域相结合的模型，如图学习。'
- en: '**2023 – 2024** : The landscape saw an explosion of powerful LLMs, starting
    with ChatGPT, followed by **GPT-4** , Claude from Anthropic, Gemini from Google,
    and Llama from Meta, indicating a trend toward more specialized and powerful language
    models.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2023 – 2024** ：LLM的强大进展引发了广泛关注，首先是ChatGPT，接着是**GPT-4**、Anthropic的Claude、谷歌的Gemini和Meta的Llama，这表明了更专业化、更强大的语言模型趋势。'
- en: The remarkable achievements of LLMs have sparked interest in harnessing their
    capabilities for tasks in graph machine learning. On the one hand, the extensive
    knowledge and logical prowess of LLMs offer promising prospects to improve upon
    conventional GNN models. On the other hand, the organized representations and
    concrete knowledge embedded within graphs hold the potential to mitigate some
    of the primary shortcomings of LLMs, including their tendency to generate misleading
    information and their challenges with interpretability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的显著成就激发了将其能力应用于图机器学习任务的兴趣。一方面，LLM的广泛知识和逻辑能力为改善传统的GNN模型提供了有前景的前景。另一方面，图中的有组织表示和具体知识有可能缓解LLM的一些主要缺点，包括它们产生误导性信息的倾向和解释性问题。
- en: Textual data in graphs
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图中的文本数据
- en: One of the fundamental hurdles in deploying GNNs lies in acquiring sophisticated
    feature representations for nodes and edges. This becomes particularly crucial
    when these elements are associated with complex textual attributes such as descriptions,
    titles, or abstracts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 部署GNN的一个基本障碍是获取节点和边的复杂特征表示。当这些元素与复杂的文本属性（如描述、标题或摘要）相关联时，这一点尤为重要。
- en: Traditional methods, such as the bag-of-words approach or the utilization of
    pre-trained word embedding models, have been the norm. However, these techniques
    typically fall short of grasping the subtle semantic intricacies inherent in the
    text. They tend to overlook the context and the interdependencies between words,
    leading to a loss of critical information that could be essential for the GNN
    to perform optimally.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法，如词袋模型或使用预训练词嵌入模型，一直是常见的做法。然而，这些技术通常无法把握文本中固有的微妙语义差异。它们往往忽视上下文及词语之间的相互依赖，导致丧失对于GNN进行最佳表现所需的关键信息。
- en: To overcome this challenge, there’s a growing need for more advanced methods
    that can understand and encode the richness of language into the graph structure.
    This is where LLMs come into play. With their deep understanding of language nuances
    and context, LLMs can generate embeddings that capture a broader spectrum of linguistic
    features.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这一挑战，越来越需要更先进的方法来理解和将语言的丰富性编码到图结构中。这正是LLM的优势所在。通过深刻理解语言的细微差别和上下文，LLM能够生成捕捉更广泛语言特征的嵌入。
- en: By integrating LLMs into the feature extraction process for GNNs, you can potentially
    encode richer, more informative representations that reflect the true semantic
    content of the textual attributes, thereby enhancing the GNN’s ability to perform
    tasks such as node classification, link prediction, and graph generation with
    greater accuracy and efficiency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将LLM整合到GNN的特征提取过程中，您可以潜在地编码更丰富、更具信息性的表示，这些表示能够反映文本属性的真实语义内容，从而提升GNN在执行如节点分类、链路预测和图生成等任务时的准确性和效率。
- en: LLMs have capabilities that extend beyond generating textual embeddings as features.
    LLMs are good at generating augmented information from original text attributes.
    They can be used to generate tags/labels and other useful metadata in an unsupervised/semi-supervised
    way.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的能力不仅仅局限于生成文本嵌入作为特征。LLM擅长从原始文本属性中生成增强信息。它们可以用来生成标签/标签以及其他有用的元数据，且可以以无监督/半监督的方式进行处理。
- en: A significant benefit of LLMs is their capacity to adapt to new tasks with minimal
    or no labeled data, owing to their extensive pre-training on broad text datasets.
    This ability for few-shot learning can help reduce the dependency of GNNs on extensive
    labeled datasets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的一个重要优势是它们能够在极少或没有标注数据的情况下适应新任务，这得益于它们在广泛文本数据集上的预训练。这种少量样本学习的能力有助于减少图神经网络（GNN）对大量标注数据集的依赖。
- en: One strategy involves employing LLMs to directly predict outcomes for graph-related
    tasks by framing the graph’s structure and the information of its nodes within
    natural language prompts. Techniques such as InstructGLM refine LLMs such as Llama
    and GPT-4 with well-crafted prompts that detail the graph’s topology, including
    aspects such as node connections and neighborhoods. These optimized LLMs are capable
    of making predictions for tasks such as **node classification** and **link prediction**
    without requiring any labeled examples at the inference stage.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一种策略是通过将图的结构和节点信息框架化为自然语言提示，直接利用LLM预测图相关任务的结果。像InstructGLM这样的技术通过精心设计的提示，优化像Llama和GPT-4这样的LLM，详细描述图的拓扑结构，包括节点连接和邻域等方面。这些优化后的LLM能够在推理阶段无需任何标注样本即可预测**节点分类**和**链路预测**等任务。
- en: Leveraging InstructGLM
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用InstructGLM
- en: '**InstructGLM** is a framework that leverages natural language to describe
    both graph structure and node features to a generative LLM, addressing graph-related
    problems through instruction-tuning. It’s a proposed instruction fine-tuned **graph
    language model** ( **GLM** ) that utilizes natural language instructions for graph
    machine learning, offering a powerful NLP interface for graph-related tasks.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**InstructGLM**是一个框架，利用自然语言描述图结构和节点特征，并通过指令调优来解决图相关问题。它是一个提议的指令微调**图语言模型**（**GLM**），利用自然语言指令进行图机器学习，为图相关任务提供强大的自然语言处理（NLP）接口。'
- en: The InstructGLM framework involves a multi-task, multi-prompt instructional
    tuning process to refine LLMs and integrate them with graphs effectively. This
    approach aims to reduce the reliance on labeled data by utilizing self-supervised
    learning on graphs and leveraging LLMs as text encoders to enhance performance
    and efficiency.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGLM框架涉及一个多任务、多提示的教学调优过程，用于优化大语言模型（LLM）并有效地将其与图结构结合。该方法旨在通过在图结构上利用自监督学习，并利用LLMs作为文本编码器，减少对标注数据的依赖，从而提升性能和效率。
- en: The InstructGLM technique employs linguistic cues that articulate the patterns
    of connections and the characteristics of nodes within a graph. These prompts
    serve as a teaching mechanism, guiding LLMs to comprehend the intricate architecture
    and inherent meaning of graphs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: InstructGLM技术采用语言线索，阐明图中连接模式和节点特征。这些提示作为教学机制，指导LLMs理解图的复杂结构和内在含义。
- en: 'As shown in *Figure 6* *.1* , the InstructGLM framework presents a sophisticated
    approach to multi-task, multi-prompt instructional tuning for LLMs:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6* *.1*所示，InstructGLM框架提出了一种复杂的多任务、多提示指令调优方法，适用于大型语言模型（LLMs）：
- en: '![Figure 6.1 – InstructGLM multi-task usage. Source: Ye et al., 2024 (https://arxiv.org/abs/2308.07134)](img/B22118_06_1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – InstructGLM 多任务应用。来源：Ye等，2024（https://arxiv.org/abs/2308.07134）](img/B22118_06_1.jpg)'
- en: 'Figure 6.1 – InstructGLM multi-task usage. Source: Ye et al., 2024 (https://arxiv.org/abs/2308.07134)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – InstructGLM 多任务应用。来源：Ye等，2024（https://arxiv.org/abs/2308.07134）
- en: 'This figure illustrates the core components of InstructGLM, showcasing different
    types of prompts and their applications:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该图展示了InstructGLM的核心组件，展示了不同类型的提示及其应用：
- en: '**1-hop prompt with meta node feature** : This prompt type categorizes central
    nodes based on their immediate connections, as shown in the blue box at the top
    left of the figure.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1跳提示与元节点特征**：这种提示类型根据节点的直接连接对中心节点进行分类，如图左上角蓝色框所示。'
- en: '**3-hop prompt with intermediate paths** : Depicted in the green box, this
    prompt explores connections up to three hops away, providing a broader context
    for node classification.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3跳提示与中间路径**：如绿色框所示，此提示探索最多三跳的连接，为节点分类提供更广泛的上下文。'
- en: '**Structure-free prompt** : The yellow box demonstrates how InstructGLM can
    categorize nodes based on their inherent features without relying on structural
    information.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无结构提示**：黄色框展示了InstructGLM如何基于节点的内在特征对其进行分类，而不依赖于结构信息。'
- en: '**2-hop prompt with meta node feature & intermediate nodes** : Illustrated
    in the pink box, this prompt type is used for link prediction tasks, considering
    connections up to two hops away.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2跳提示与元节点特征及中间节点**：如粉色框所示，这种提示类型用于链接预测任务，考虑最多两跳的连接。'
- en: '**1-hop prompt without meta node feature** : Another link prediction prompt,
    as shown in the orange box, this focuses on immediate connections without additional
    node information.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1跳提示无元节点特征**：另一种链接预测提示，如橙色框所示，该提示关注节点的直接连接，而不包含额外的节点信息。'
- en: '*Figure 6* *.1* also highlights the dual focus of InstructGLM on node classification
    and link prediction tasks, as indicated by the dotted line separating these two
    primary functions.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6* *.1* 同样强调了InstructGLM在节点分类和链接预测任务中的双重关注，正如由虚线分隔的这两个主要功能所示。'
- en: Although utilizing LLMs as opaque predictors has been effective, their accuracy
    diminishes for more intricate graph tasks where detailed modeling of the structure
    proves advantageous. Consequently, some methods combine LLMs with GNNs, where
    the GNN maps out the graph structure, and the LLM enriches this with a deeper
    semantic understanding of the nodes based on their textual descriptions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管将LLMs作为不透明预测器已经取得了有效的结果，但在处理更复杂的图任务时，它们的准确性下降，因为在这种任务中，结构的详细建模至关重要。因此，一些方法将LLMs与GNNs结合使用，其中GNN绘制图的结构，而LLM基于节点的文本描述，增强了对节点的深层语义理解。
- en: Next, let’s see how LLMs can help us with graph learning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看LLMs如何帮助我们进行图学习。
- en: LLMs for graph learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs在图学习中的应用
- en: Researchers have delved into various strategies for incorporating LLMs into
    the graph learning process. Each method presents distinct benefits and potential
    uses. Let’s look at some of the key functions that LLMs can fulfill.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经深入探讨了将LLMs融入图学习过程的各种策略。每种方法都有其独特的优势和潜在用途。让我们看看LLMs能发挥的几个关键功能。
- en: LLMs as enhancers
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs作为增强工具
- en: Traditional GNNs rely on the quality of initial node features, often with limited
    textual descriptions. LLMs, with their vast knowledge and language comprehension
    abilities, can bridge this gap. By enhancing these features, LLMs empower GNNs
    to capture intricate relationships and dynamics within the graph, leading to superior
    performance on tasks such as node classification or link prediction.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的图神经网络（GNNs）依赖于初始节点特征的质量，通常这些特征的文本描述有限。而LLMs凭借其广泛的知识和语言理解能力，能够弥补这一差距。通过增强这些特征，LLMs使得GNNs能够捕捉图中复杂的关系和动态，从而在节点分类或链接预测等任务中表现出色。
- en: 'There are two primary methods for harnessing LLMs as enhancers. The first is
    **feature-level enhancement** , which can be achieved in various ways using LLMs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种主要方法可以利用 LLM 作为增强工具。第一种是 **特征级增强**，可以通过多种方式使用 LLM 来实现：
- en: '**Synonyms and related concepts** : The LLM goes beyond the surface level of
    the text description by recognizing synonyms and semantically related concepts.
    This helps capture a broader range of information that might not be explicitly
    mentioned. For instance, if a product description mentions *waterproof* and *hiking
    boots* , the LLM can infer that the product is suitable for outdoor activities.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同义词与相关概念**：LLM 通过识别同义词和语义相关的概念，超越了文本描述的表面层次。这有助于捕捉可能没有明确提到的更广泛的信息。例如，如果产品描述提到
    *防水* 和 *登山靴*，LLM 可以推断该产品适合户外活动。'
- en: '**Implicit relationships** : LLMs can extract implicit relationships from the
    text. These relationships can be crucial for understanding the node’s context
    within the graph. For example, in a social network, the LLM might infer a friendship
    between two nodes based on their frequent interactions, even if the word *friend*
    is never explicitly mentioned.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐式关系**：LLMs 可以从文本中提取隐式关系。这些关系对于理解图中节点的上下文至关重要。例如，在社交网络中，LLM 可能会根据两个节点之间频繁的互动推断出它们之间的友谊，即使词语
    *friend* 从未明确提及。'
- en: '**External knowledge integration** : LLMs can access and integrate external
    knowledge bases to further enrich the node representation. This could involve
    linking product information to user reviews or connecting protein descriptions
    (in a biological network) to known protein-protein interactions.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部知识集成**：LLMs 可以访问并集成外部知识库，进一步丰富节点表示。这可能涉及将产品信息与用户评论关联，或将蛋白质描述（在生物网络中）与已知的蛋白质-蛋白质相互作用连接起来。'
- en: The other method is **text-level enhancement** , which can be employed to create
    richer, more informative textual descriptions for nodes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是 **文本级增强**，可以用于为节点创建更丰富、更具信息性的文本描述。
- en: 'This approach focuses on creating entirely new textual descriptions for the
    nodes, which is particularly beneficial when the original descriptions are limited
    or lack context. The LLM acts as a content generator, leveraging information about
    the node and its surrounding context within the graph to create a new, more informative
    description. This context might include the following aspects:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法侧重于为节点创建全新的文本描述，尤其在原始描述有限或缺乏上下文时特别有益。LLM 作为内容生成器，利用关于节点及其在图中的上下文信息，创造出新的、更具信息量的描述。这个上下文可能包括以下方面：
- en: Original text description
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始文本描述
- en: Labels of neighboring nodes
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻接节点的标签
- en: The structure of the graph (for example, the number of edges connected to the
    node)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的结构（例如，连接到节点的边的数量）
- en: The LLM utilizes this information to generate a rich textual description that
    captures the relationships with neighboring nodes, the overall network structure,
    and any relevant external knowledge. This newly created description becomes the
    node’s enhanced feature, providing the GNN with a more comprehensive understanding
    of the node’s role within the graph.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 利用这些信息生成丰富的文本描述，捕捉与邻接节点的关系、整体网络结构以及任何相关的外部知识。这个新生成的描述成为节点的增强特征，为 GNN 提供了对节点在图中作用的更全面理解。
- en: Benefits and challenges of LLM enhancement
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 增强的优势与挑战
- en: 'LLM-based enhancement offers several advantages for graph learning:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的增强为图学习提供了若干优势：
- en: '**Improved feature representation** : The enhanced node features capture a
    richer and more nuanced understanding of the node’s context within the graph.
    This allows GNNs to learn more complex relationships and patterns, leading to
    improved performance on tasks such as node classification, link prediction, and
    community detection.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改进的特征表示**：增强的节点特征捕捉了节点在图中上下文的更丰富、更细致的理解。这使得 GNN 能够学习更复杂的关系和模式，从而在节点分类、链接预测和社区检测等任务中提高性能。'
- en: '**Handling limited data** : LLMs can address situations where node descriptions
    are sparse or lack detail. By inferring relationships and leveraging external
    knowledge, they create more informative representations, mitigating the challenges
    associated with limited data availability.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理有限数据**：LLMs 可以解决节点描述稀缺或缺乏细节的情况。通过推断关系和利用外部知识，它们能够创建更具信息性的表示，从而缓解有限数据可用性带来的挑战。'
- en: '**Identifying implicit connections** : LLMs can go beyond the surface-level
    information in node features and identify subtle connections based on their understanding
    of language. This can be crucial for tasks such as uncovering hidden communities
    within a social network or predicting the existence of links between nodes in
    a biological network.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别隐含连接**：LLMs可以超越节点特征中的表层信息，基于它们对语言的理解识别出微妙的连接。这对于诸如揭示社交网络中隐藏社群或预测生物网络中节点之间存在链接等任务至关重要。'
- en: 'However, while LLM enhancement offers a compelling path forward, there are
    a few challenges to navigate:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然LLM增强提供了一条引人注目的前进道路，但也存在一些需要克服的挑战：
- en: '**The cost of computation** : Training and running LLMs can be computationally
    expensive, especially for massive graphs. Careful optimization strategies are
    needed to ensure scalability.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：训练和运行LLMs可能会耗费大量计算资源，尤其是在处理大规模图形时。需要谨慎优化策略以确保可伸缩性。'
- en: '**Data bias inheritance** : LLMs aren’t immune to biases present in their training
    data. It’s crucial to ensure the LLM that’s used for enhancement is trained on
    high-quality, unbiased data to prevent skewed results.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据偏倚继承**：LLMs并不免于其训练数据中存在的偏见。关键在于确保用于增强的LLM是在高质量、无偏见的数据上训练的，以防止结果偏斜。'
- en: '**The explainability enigma** : Understanding how LLMs generate enhanced features
    can be challenging. This lack of transparency can make it difficult to interpret
    the results of GNNs that utilize these features. Researchers are actively working
    on developing methods to make the enhancement process that’s implemented by LLMs
    more transparent.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性难题**：理解LLMs如何生成增强特征可能具有挑战性。这种透明度不足可能使得解释使用这些特征的GNN结果变得困难。研究人员正在积极开发方法，使LLMs实现的增强过程更加透明。'
- en: Real-world applications
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 真实世界应用
- en: Here are multiple real-world examples of how LLMs can enhance the graph learning
    process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有多个真实世界的例子，展示了LLMs如何增强图学习过程。
- en: '**Drug discovery and bioinformatics** : In drug discovery, predicting adverse
    **drug-drug interactions** ( **DDIs** ) is crucial for patient safety. Traditional
    methods often struggle due to the sheer volume of possible drug combinations and
    interactions. GNNs can model these relationships by representing drugs as *nodes*
    and known interactions as *edges* . When enhanced with LLMs, which process biomedical
    literature to extract information about drug mechanisms, side effects, and interactions,
    these models become significantly more powerful. The LLM-generated embeddings
    enrich the node and edge features in the GNN, leading to more accurate predictions
    of potential DDIs and ultimately safer medication management.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**药物发现与生物信息学**：在药物发现中，预测不良**药物-药物相互作用**（**DDIs**）对患者安全至关重要。传统方法通常因为可能的药物组合和相互作用数量庞大而难以应对。GNNs可以通过将药物表示为*节点*，已知相互作用表示为*边*来建模这些关系。当结合LLMs时，LLMs会处理生物医学文献以提取有关药物机制、副作用和相互作用的信息，使得这些模型显著增强。LLMs生成的嵌入丰富了GNN中的节点和边特征，从而更准确地预测潜在的DDIs，最终实现更安全的药物管理。'
- en: '**Social network analysis** : Detecting misinformation on social media is another
    area where LLMs can enhance GNNs. GNNs can model social networks with *nodes*
    representing users and *edges* representing interactions such as likes, shares,
    and comments. By processing the content of posts, an LLM can extract themes, sentiments,
    and potentially misleading information, which are then integrated into the graph.
    This enrichment enables the GNN to better identify clusters of misinformation
    and predict which users are most likely to spread false information, facilitating
    more effective interventions to maintain information integrity.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交网络分析**：LLMs可以增强GNNs在社交媒体上检测虚假信息的能力。GNNs可以模拟社交网络，*节点*代表用户，*边*代表互动，如点赞、分享和评论。通过处理帖子内容，LLMs可以提取主题、情感以及潜在误导信息，这些信息随后被整合到图中。这种丰富化使得GNN能够更好地识别虚假信息的群集，并预测哪些用户最有可能传播虚假信息，从而更有效地干预以维护信息的完整性。'
- en: '**Financial fraud detection** : Financial fraud detection involves identifying
    suspicious patterns among millions of transactions. GNNs can represent these transactions
    as graphs, with *nodes* as accounts and *edges* as transactions. An LLM can analyze
    transaction descriptions and notes to extract keywords and patterns indicative
    of fraud, enhancing the GNN’s node and edge features. This integration allows
    the GNN to detect fraudulent transactions more accurately by considering both
    transactional patterns and contextual information from textual descriptions, leading
    to more robust fraud detection systems.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融欺诈检测**：金融欺诈检测涉及在数百万笔交易中识别可疑模式。GNNs 可以将这些交易表示为图，其中 *节点* 代表账户，*边* 代表交易。LLM
    可以分析交易描述和备注，提取出欺诈的关键词和模式，增强 GNN 的节点和边特征。这一集成使得 GNN 能够通过考虑交易模式和文本描述中的上下文信息，更准确地检测欺诈交易，从而构建出更强大的欺诈检测系统。'
- en: '**Academic research and collaboration networks** : Identifying potential research
    collaborators is essential for advancing scientific discovery. GNNs can model
    academic networks, with *nodes* representing researchers and *edges* representing
    co-authorship or citation relationships. An LLM can analyze publication abstracts,
    keywords, and research interests, transforming these textual features into embeddings
    that are integrated into the graph. This enhancement enables the GNN to recommend
    potential collaborators by considering both structural relationships and semantic
    similarities in research interests, fostering more effective scientific collaborations.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学术研究和协作网络**：识别潜在的研究合作者对推动科学发现至关重要。图神经网络（GNNs）可以建模学术网络，其中 *节点* 代表研究人员，*边*
    代表共同署名或引用关系。LLM 可以分析论文摘要、关键词和研究兴趣，将这些文本特征转换为嵌入并集成到图中。这一增强功能使 GNN 能够通过考虑结构关系和研究兴趣的语义相似性来推荐潜在合作者，从而促进更有效的科学合作。'
- en: '**Knowledge graph construction** : Building comprehensive knowledge graphs
    involves integrating information from diverse and often unstructured sources.
    GNNs can model knowledge graphs with *nodes* representing entities and *edges*
    representing relationships. An LLM can extract entities and relationships from
    textual data sources, such as news articles, scientific literature, and web pages,
    and use these insights to augment the knowledge graph with additional nodes and
    edges. This enhancement allows the GNN to build more complete and accurate knowledge
    graphs by incorporating detailed and contextually rich information from a wide
    array of textual sources, facilitating better knowledge representation and discovery.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识图谱构建**：构建全面的知识图谱涉及从各种来源（通常是非结构化的）集成信息。GNNs 可以通过 *节点* 代表实体，*边* 代表关系来建模知识图谱。LLM
    可以从新闻文章、科研文献和网页等文本数据源中提取实体和关系，并利用这些信息来丰富知识图谱，增加额外的节点和边。这一增强功能使得 GNN 能够通过整合来自多种文本来源的详细和上下文丰富的信息，构建更完整、更准确的知识图谱，促进更好的知识表示和发现。'
- en: The integration of LLMs with GNNs provides a powerful approach to enhancing
    various applications by incorporating rich, contextual information from textual
    data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 与 GNN 的集成为通过结合文本数据中的丰富上下文信息来增强各种应用提供了一种强大的方法。
- en: LLMs as predictors
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 作为预测器
- en: With the advancement in language understanding, thanks to transformer-based
    LLMs, researchers are exploring how best to represent graph data in text for LLMs.
    A recent paper titled *Can Language Models Solve Graph Problems in Natural Language?*
    ( [https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3) )
    talks about an LLM constructing graphs from text descriptions, enhancing its graph
    comprehension. This paper demonstrates how prompting can help LLM understand graph
    structure and provide results using a set of instructions (algorithms) provided
    in the prompt.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言理解的进步，得益于基于 Transformer 的大型语言模型（LLMs），研究人员正在探索如何最佳地将图数据在文本中呈现给 LLMs。一篇名为
    *语言模型能否解决自然语言中的图问题？*（[https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3)）的最新论文讨论了如何通过
    LLM 从文本描述构建图，从而增强其图形理解能力。该论文展示了如何通过提示帮助 LLM 理解图结构，并通过提示中提供的一组指令（算法）给出结果。
- en: '*Figure 6* *.2* shows three distinct approaches to graph analysis through prompting
    techniques:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6* *.2* 展示了通过提示技术进行图分析的三种不同方法：'
- en: '![Figure 6.2 – Understanding graph structure using prompting. Source: Wang
    et al., 2024 (https://arxiv.org/html/2305.10037v3)](img/B22118_06_2.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 使用提示理解图结构。来源：Wang等，2024（[https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3)）](img/B22118_06_2.jpg)'
- en: 'Figure 6.2 – Understanding graph structure using prompting. Source: Wang et
    al., 2024 (https://arxiv.org/html/2305.10037v3)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 使用提示理解图结构。来源：Wang等，2024（[https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3)）
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The figure *Overview of Build-a-Graph prompting and Algorithmic prompting* (
    [https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3) ) by
    Wang et al. (2024) is licensed under CC BY 4.0.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图示*构建图提示与算法提示概述*（[https://arxiv.org/html/2305.10037v3](https://arxiv.org/html/2305.10037v3)）由Wang等（2024）发布，采用CC
    BY 4.0许可协议。
- en: 'This illustration demonstrates how LLMs can be guided to comprehend and solve
    graph-related problems using different prompting strategies:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此插图展示了如何通过不同的提示策略引导LLM理解和解决与图相关的问题：
- en: '**Standard prompting** : The first column presents the standard prompting method.
    Here, a simple undirected graph is depicted with nodes numbered from 0 to 4. The
    prompt provides context by describing the graph’s structure, including the weights
    of edges connecting various nodes. The question that’s posed is to find the shortest
    path from node 0 to node 2, demonstrating a basic graph traversal problem.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准提示**：第一列展示了标准提示方法。这里展示了一个简单的无向图，节点编号从0到4。该提示通过描述图的结构，包括连接各节点的边的权重，来提供上下文。提出的问题是找出从节点0到节点2的最短路径，展示了一个基本的图遍历问题。'
- en: '**Build-a-graph prompting** : The middle column introduces a more sophisticated
    approach called build-a-graph prompting. This method begins by instructing the
    LLM to construct the graph based on the given information. It then guides the
    model through the process of identifying all possible paths between nodes 0 and
    2 and calculating their total weights. This step-by-step approach helps the LLM
    to systematically analyze the graph and determine the shortest path.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建图提示**：中间一列介绍了一种更复杂的方法，称为构建图提示。该方法首先指示LLM基于给定的信息构建图，然后引导模型通过识别节点0和节点2之间的所有可能路径并计算其总权重的过程。这个逐步的方法帮助LLM系统地分析图并确定最短路径。'
- en: '**Algorithmic prompting** : The rightmost column showcases algorithmic prompting,
    which is the most advanced technique presented. It outlines a **depth-first search**
    ( **DFS** ) algorithm to find the shortest path between two nodes in an undirected
    graph. This method provides a detailed explanation of how to implement the algorithm,
    including tracking distances and backtracking to identify the optimal path. The
    prompt then applies this algorithm to the same graph problem, demonstrating how
    a more complex analytical approach can be used to solve graph traversal questions.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法提示**：最右边一列展示了算法提示，这是所展示的最先进的技术。它概述了一个**深度优先搜索**（**DFS**）算法，用于在无向图中找到两个节点之间的最短路径。该方法详细说明了如何实现该算法，包括跟踪距离和回溯以识别最优路径。然后，提示将这个算法应用于相同的图问题，演示了如何使用更复杂的分析方法来解决图遍历问题。'
- en: The **GPT4Graph** ( [https://arxiv.org/pdf/2305.15066](https://arxiv.org/pdf/2305.15066)
    ) study used graph markup languages and self-prompting to improve LLM understanding
    before generating the final output. The main strategy involves inputting graph
    data, ensuring the LLM understands it, and then querying it. However, this method
    struggles with scalability for large graphs due to context length limitations
    and requires new prompts for new tasks. To address these challenges and improve
    graph learning capabilities, let’s explore how to integrate RAG with graph learning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT4Graph**（[https://arxiv.org/pdf/2305.15066](https://arxiv.org/pdf/2305.15066)）研究使用图标记语言和自我提示来改善LLM在生成最终输出之前的理解。主要策略是输入图数据，确保LLM理解它，然后进行查询。然而，由于上下文长度的限制，这种方法在处理大型图时存在可扩展性问题，并且需要为新任务提供新的提示。为了应对这些挑战并提升图学习能力，我们来探讨如何将RAG与图学习结合。'
- en: Integrating RAG with graph learning
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将RAG与图学习结合
- en: 'RAG is an AI framework that combines the power of LLMs with external knowledge
    retrieval to produce more accurate, relevant, and up-to-date responses. Here’s
    how it works:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RAG是一个AI框架，它结合了LLM的强大功能和外部知识检索，以产生更准确、相关和最新的响应。其工作原理如下：
- en: '**Information retrieval** : When a query is received, RAG searches a knowledge
    base or database to find relevant information.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**信息检索**：当接收到查询时，RAG会搜索知识库或数据库以找到相关信息。'
- en: '**Context augmentation** : The retrieved information is then used to augment
    the input to the language model, providing it with additional context.'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**上下文增强**：检索到的信息随后被用来增强语言模型的输入，提供额外的上下文。'
- en: '**Generation** : The LLM uses this augmented input to generate a response that’s
    both fluent and grounded in the information that’s been retrieved.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成**：LLM利用这种增强的输入生成既流畅又基于检索信息的回答。'
- en: 'RAG offers several compelling perks:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: RAG提供了几个令人信服的好处：
- en: '**Improved accuracy** : By grounding responses in retrieved information, RAG
    reduces hallucinations and improves factual accuracy.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提高准确性**：通过将回答基于检索到的信息，RAG减少了幻觉现象，提升了事实准确性。'
- en: '**Up-to-date information** : The knowledge base can be updated regularly, allowing
    the system to access current information without having to retrain the entire
    model.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最新信息**：知识库可以定期更新，使系统能够访问当前信息，而无需重新训练整个模型。'
- en: '**Transparency** : RAG can provide sources for its information, increasing
    trustworthiness and allowing for fact-checking.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**：RAG可以提供信息来源，从而增加可信度并支持事实核查。'
- en: While traditional RAG approaches have proven effective, combining RAG with graph
    learning techniques offers even more powerful capabilities for information retrieval
    and generation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然传统的RAG方法已经证明有效，但将RAG与图谱学习技术结合，提供了更强大的信息检索和生成能力。
- en: Advantages of graph RAG (GRAG) approaches
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图谱RAG（GRAG）方法的优势
- en: 'Graph learning leverages the inherent structure and relationships within data,
    which can significantly enhance the context and relevance of the information that’s
    retrieved. The general benefits of integrating graphs with RAG are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图谱学习利用数据中固有的结构和关系，可以显著提高检索到的信息的上下文和相关性。将图谱与RAG（检索增强生成）结合的主要好处如下：
- en: '**Structural context** : Graphs capture complex relationships between entities,
    providing a richer context than flat text documents.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化上下文**：图谱捕捉实体之间的复杂关系，提供比平面文本文档更丰富的上下文。'
- en: '**Multi-hop reasoning** : Graph structures allow information to be retrieved
    across multiple connected entities, facilitating more complex reasoning tasks.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多跳推理**：图谱结构使得可以跨多个连接实体检索信息，从而促进更复杂的推理任务。'
- en: '**Improved relevance** : By considering both textual and topological information,
    graph-based retrieval can identify more pertinent information for the generation
    process.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改善相关性**：通过考虑文本信息和拓扑信息，基于图谱的检索能够识别出更相关的信息，用于生成过程。'
- en: In the following sections, we’ll explore the advantages of a few specific approaches.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨几种特定方法的优势。
- en: Knowledge graph RAG
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识图谱RAG
- en: '**Knowledge graph RAG** leverages structured knowledge graphs to enhance the
    retrieval and generation process. This approach offers several key advantages:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识图谱RAG**利用结构化的知识图谱来增强检索和生成过程。这种方法具有几个关键优势：'
- en: '**Precise entity and relationship retrieval** : By utilizing the structured
    nature of knowledge graphs, this method can retrieve not just relevant entities,
    but also the relationships between them. For example, a biomedical knowledge graph
    can retrieve not only a specific drug, but also its interactions with other drugs,
    its side effects, and its approved uses.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确的实体和关系检索**：通过利用知识图谱的结构化特性，该方法不仅能检索到相关实体，还能检索到它们之间的关系。例如，生物医学知识图谱不仅能检索到特定药物，还能检索到其与其他药物的相互作用、副作用以及批准用途。'
- en: '**Contextual enrichment** : The retrieved information includes the broader
    context of entities within the graph. This allows the LLM to understand the entity’s
    place in a larger network of information, leading to more nuanced and accurate
    responses.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文丰富**：检索到的信息包括图谱中实体的更广泛上下文。这使得大语言模型（LLM）能够理解实体在更大信息网络中的位置，从而提供更为细致和准确的回答。'
- en: '**Hierarchical information access** : Knowledge graphs often contain hierarchical
    relationships (for example, *is-a* and *part-of* ). This structure allows for
    more flexible retrieval, where the system can access both specific details and
    broader categories as needed.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次化信息访问**：知识图谱通常包含层次化关系（例如，*是* 和 *一部分*）。这种结构允许更灵活的检索，系统可以根据需要访问具体细节和更广泛的类别。'
- en: '**Multi-hop reasoning** : Knowledge graph RAG can facilitate multi-hop reasoning
    by retrieving paths of connected entities and relationships. This is particularly
    useful for complex queries that require information from multiple sources to be
    pieced together within the graph.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多跳推理**：知识图 RAG 可以通过检索连接实体和关系的路径来促进多跳推理。这对于需要从多个来源获取信息并在图中拼凑起来的复杂查询尤为有用。'
- en: GNNs
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GNNs
- en: 'As we have seen, GNNs are powerful tools for learning representations of graph-structured
    data. In the context of RAG, they offer several benefits:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，GNNs 是学习图结构数据表示的强大工具。在 RAG 的背景下，它们提供了几个优势：
- en: '**Learning graph embeddings** : GNNs can create dense vector representations
    (embeddings) of nodes, edges, and subgraphs. These embeddings capture both local
    and global structural information, allowing for more effective retrieval of relevant
    graph components.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习图嵌入**：GNNs 可以创建节点、边和子图的稠密向量表示（嵌入）。这些嵌入捕捉了局部和全局的结构信息，从而有效地检索相关的图组件。'
- en: '**Capturing graph topology** : Unlike traditional neural networks, GNNs explicitly
    model the relationships between entities in a graph. This allows them to capture
    complex topological features that are crucial for understanding the context and
    relevance of information in a graph.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**捕获图拓扑**：与传统神经网络不同，GNNs 明确建模图中实体之间的关系。这使它们能够捕捉图中理解信息的上下文和相关性的复杂拓扑特征。'
- en: '**Scalability** : GNNs can process large-scale graphs efficiently, making them
    suitable for real-world knowledge bases that often contain millions of entities
    and relationships.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性**：GNNs 能够高效处理大规模图，使其适用于现实世界中的知识库，这些知识库通常包含数百万个实体和关系。'
- en: '**Inductive learning** : Many GNN architectures support inductive learning,
    allowing them to generalize to unseen nodes or even entirely new graphs. This
    is particularly useful for dynamic knowledge bases that are constantly updated.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归纳学习**：许多 GNN 架构支持归纳学习，使它们能够推广到未见过的节点，甚至是全新的图。这对于不断更新的动态知识库特别有用。'
- en: GRAG
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GRAG
- en: '**Graph RAG** ( **GRAG** ) is an advanced technique that emphasizes the importance
    of subgraph structures throughout both the retrieval and generation processes:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 RAG**（**GRAG**）是一种先进的技术，它强调在检索和生成过程中子图结构的重要性：'
- en: '**Subgraph-aware retrieval** : Instead of retrieving individual entities or
    relationships, GRAG focuses on retrieving relevant subgraphs. This approach preserves
    the local structure around entities of interest, providing a richer context for
    the generation process.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**子图感知检索**：与检索单独的实体或关系不同，GRAG 专注于检索相关的子图。这种方法保留了感兴趣实体周围的局部结构，为生成过程提供了更丰富的上下文。'
- en: '**Topology-preserving generation** : GRAG maintains awareness of the graph
    topology during the generation process. This ensures that the generated text respects
    the structural relationships present in the retrieved subgraphs, leading to more
    coherent and factually consistent outputs.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拓扑保持生成**：GRAG 在生成过程中保持对图拓扑的意识。这确保生成的文本遵循在检索的子图中存在的结构关系，从而产生更连贯、事实一致的输出。'
- en: '**Soft pruning** : GRAG often employs a soft pruning mechanism to refine retrieved
    subgraphs. This process removes less relevant nodes and edges while maintaining
    the overall structure, helping to focus the LLM on the most pertinent information.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软剪枝**：GRAG 通常采用软剪枝机制来优化检索到的子图。这个过程移除不太相关的节点和边，同时保持整体结构，有助于将 LLM 的焦点集中在最相关的信息上。'
- en: '**Hierarchical text conversion** : GRAG typically includes methods for converting
    subgraphs into hierarchical text descriptions. This conversion preserves both
    the textual content and the structural information of the graph, allowing the
    LLM to work with a rich, structured input.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次化文本转换**：GRAG 通常包括将子图转换为层次化文本描述的方法。这种转换既保留了文本内容，又保留了图的结构信息，使得 LLM 可以处理丰富的结构化输入。'
- en: '**Multi-hop reasoning support** : By maintaining subgraph structures, GRAG
    naturally supports multi-hop reasoning tasks. The LLM can traverse the retrieved
    subgraph to connect distant pieces of information, enabling more complex inferencing.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多跳推理支持**：通过保持子图结构，GRAG 自然支持多跳推理任务。LLM 可以遍历检索到的子图，连接远距离的信息片段，从而实现更复杂的推理。'
- en: Let’s consider how GRAG might be applied in a customer support system for a
    tech company that utilizes a knowledge base structured as a knowledge graph. The
    knowledge graph contains interconnected information about products, error codes,
    troubleshooting steps, and user manuals.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑 GRAG 在一个技术公司客户支持系统中的应用，该系统使用结构化为知识图谱的知识库。该知识图谱包含有关产品、错误代码、故障排除步骤和用户手册的互联信息。
- en: 'Suppose a user submits the following query: *How do I resolve Error Code E101
    on my* *SmartHome Router?*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户提交了以下查询：*如何解决我的* *SmartHome 路由器上的错误代码 E101？*
- en: 'Traditional methods might retrieve isolated entities, like the product name
    or error code, which could lead to fragmented or incomplete answers. GRAG takes
    a unique approach to provide an accurate and context-rich response:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 传统方法可能会检索到孤立的实体，如产品名称或错误代码，这可能导致片段化或不完整的答案。GRAG 采用独特的方法，提供准确且富有上下文的回答：
- en: GRAG focuses on subgraph-aware retrieval, extracting a structured, coherent
    subgraph that relates to *Error Code E101* and the *SmartHome Router* . This subgraph
    can include details like the cause of the error (e.g., a network configuration
    conflict), the troubleshooting steps (e.g., updating firmware, resetting the router,
    or changing network settings), and related links to the router’s user manual or
    firmware update pages.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GRAG 专注于子图感知的检索，提取与*错误代码 E101*和*SmartHome 路由器*相关的结构化、连贯的子图。该子图可以包括错误原因（例如，网络配置冲突）、故障排除步骤（例如，更新固件、重置路由器或更改网络设置），以及与路由器用户手册或固件更新页面的相关链接。
- en: Once the subgraph is retrieved, GRAG applies a soft pruning mechanism to preserve
    the integrity of the local graph structure. Irrelevant or tangential information,
    such as troubleshooting steps for other devices or unrelated error codes, is removed.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦子图被检索到，GRAG 会应用软修剪机制，以保持本地图结构的完整性。与问题无关的或次要的信息，如其他设备的故障排除步骤或不相关的错误代码，将被移除。
- en: 'GRAG transforms the pruned subgraph into hierarchical text descriptions, which
    maintain the graph’s structure while converting the information into a form that
    the LLM can seamlessly process. Our subgraph is converted into a text hierarchy
    that organizes information as follows:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GRAG 将修剪后的子图转换为层次化的文本描述，保持图结构的同时，将信息转换为 LLM 可以顺利处理的形式。我们的子图被转换为文本层次结构，组织信息如下：
- en: An overview of *Error Code E101* and its cause (a network configuration conflict)
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*错误代码 E101* 及其原因概述（网络配置冲突）'
- en: Troubleshooting steps provided in a logical sequence
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按逻辑顺序提供的故障排除步骤
- en: Additional details where relevant, including reports from users linking E101
    to recent firmware updates
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相关的情况下，提供额外的细节，包括用户报告将 E101 错误与近期固件更新相关联的情况。
- en: 'GRAG employs topology-preserving generation to ensure that the relationships
    and structure encoded in the subgraph are reflected in the response. The system
    can now generate a detailed reply:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GRAG 采用拓扑保持生成方法，确保子图中编码的关系和结构反映在响应中。系统现在可以生成详细的回复：
- en: '*The Error Code E101 on your SmartHome Router typically occurs due to a network
    configuration conflict. To resolve it, verify if your router’s firmware is updated.
    If it is, reset the router by holding the reset button for 10 seconds. You may
    also need to update the network settings to avoid IP conflicts. Note that some
    users have reported encountering this error after recent firmware updates, so
    rolling back to a previous version might help if the* *issue persists.*'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*您的 SmartHome 路由器上的错误代码 E101 通常是由于网络配置冲突引起的。要解决此问题，请检查您的路由器固件是否已更新。如果已更新，请按住重置按钮
    10 秒钟重置路由器。您还可能需要更新网络设置，以避免 IP 冲突。请注意，一些用户在近期固件更新后报告遇到此错误，因此，如果问题仍然存在，回滚到之前的版本可能会有所帮助。*'
- en: As mentioned, a key strength of GRAG is its multi-hop reasoning support. For
    example, the system might connect *Error Code E101* to a network configuration
    conflict and, from there, to specific troubleshooting steps in the user manual.
    This enables the LLM to infer complex relationships and provide a comprehensive
    answer without missing valuable context.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，GRAG 的一大优势是其多跳推理支持。例如，系统可能会将*错误代码 E101*与网络配置冲突相连接，再从那里连接到用户手册中的具体故障排除步骤。这使得
    LLM 能够推断复杂的关系，并提供全面的答案而不遗漏有价值的上下文。
- en: These approaches offer unique strengths in leveraging graph structures for RAG.
    The choice between them often depends on the specific requirements of the application,
    the nature of the knowledge base, and the complexity of the queries being handled.
    Moreover, implementing your chosen approach carefully is crucial because of challenges
    such as managing large and noisy graphs, maintaining low latency, and adapting
    LLMs to handle structured graph inputs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在利用图结构进行RAG方面具有独特优势。选择哪种方法通常取决于应用的具体需求、知识库的性质以及所处理查询的复杂性。此外，由于存在诸如管理大型噪声图、保持低延迟以及使LLM能够处理结构化图输入等挑战，仔细实施所选方法至关重要。
- en: Challenges in integrating LLMs with graph learning
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将LLM与图学习相结合的挑战
- en: 'It’s worth noting that integrating LLMs with graph learning involves several
    challenges:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，将LLM与图学习相结合涉及若干挑战：
- en: '**Efficiency and scalability** : LLMs require significant computational resources,
    which poses deployment challenges in real-world applications, particularly on
    resource-constrained devices. Knowledge distillation, where an LLM (teacher model)
    transfers knowledge to a smaller, efficient GNN (student model), offers a promising
    solution.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率与可扩展性**：LLM需要大量的计算资源，这在实际应用中，尤其是在资源有限的设备上，带来了部署挑战。知识蒸馏（即将LLM（教师模型）的知识传递给更小、更高效的GNN（学生模型））提供了一种有前景的解决方案。'
- en: '**Data leakage and evaluation** : LLMs pre-trained on vast datasets risk data
    leakage, potentially inflating performance metrics. Mitigating this requires new
    datasets and careful test data sampling. Establishing fair evaluation benchmarks
    is also crucial for accurate performance assessment.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据泄露与评估**：LLM在大规模数据集上进行预训练，可能会面临数据泄露的风险，从而抬高性能指标。缓解这一问题需要新的数据集和仔细的测试数据抽样。建立公平的评估基准对于准确的性能评估也至关重要。'
- en: '**Transferability and explainability** : Enhancing LLMs’ ability to transfer
    knowledge across diverse graph domains and improving their explainability is vital.
    Techniques such as chain-of-thought prompting can leverage LLMs’ reasoning capabilities
    for better transparency.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移性与可解释性**：增强LLM在不同图领域之间迁移知识的能力，并提高其可解释性至关重要。诸如链式思维提示等技术可以利用LLM的推理能力，提供更好的透明度。'
- en: '**Multimodal integration** : Graphs often encompass multiple data types, including
    images, audio, and numeric data. Extending LLM integration to these multimodal
    settings represents an exciting research opportunity. With the rapid advancement
    in the quality of LLM generation, it’s going to play a critical role in augmenting
    intelligence over graph data and learning.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态集成**：图通常包含多种数据类型，包括图像、音频和数值数据。将LLM集成到这些多模态环境中是一个激动人心的研究机会。随着LLM生成质量的快速提升，它将在增强图数据和学习中的智能方面发挥至关重要的作用。'
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored integrating LLMs with graph learning, highlighting
    how LLMs can enhance traditional GNNs. We discussed the evolution of LLMs, their
    capabilities in processing textual data within graphs, and their potential to
    improve node representations and graph-related tasks. You learned about various
    approaches for utilizing LLMs in graph learning, including feature-level and text-level
    enhancements, as well as using LLMs as predictors through techniques such as InstructGLM.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了将LLM与图学习相结合，重点介绍了LLM如何增强传统的GNN。我们讨论了LLM的发展历程、其在图中处理文本数据的能力，以及其在改善节点表示和图相关任务中的潜力。你了解了多种在图学习中利用LLM的方法，包括特征层面和文本层面的增强，以及通过像InstructGLM这样的技术将LLM作为预测器使用。
- en: We also presented real-world applications in drug discovery, social network
    analysis, and financial fraud detection to illustrate the practical benefits of
    this integration. Furthermore, you became familiar with the challenges in combining
    LLMs with graph learning, such as computational costs, data bias, and explainability
    issues, while learning about the potential for future advancements in this field.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了在药物发现、社交网络分析和金融欺诈检测等领域的实际应用，以说明这种集成的实际效益。此外，你还了解了将LLM与图学习结合时面临的挑战，如计算成本、数据偏差和可解释性问题，同时学习了该领域未来可能的进展。
- en: In the next chapter, we’ll explore applying deep learning to graphs in more
    depth.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更深入地探讨将深度学习应用于图中的方法。
- en: 'Part 3: Practical Applications and Implementation'
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：实际应用与实施
- en: In this part of the book, you will dive into the practical implementation of
    graph deep learning across various domains. You will learn how to apply graph-based
    approaches to natural language processing, build recommendation systems, and leverage
    graph structures in computer vision applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的这一部分将带你深入探讨图深度学习在各个领域的实际应用。你将学习如何将基于图的方法应用于自然语言处理，构建推荐系统，并在计算机视觉应用中利用图结构。
- en: 'This part has the following chapters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B22118_07.xhtml#_idTextAnchor131) , *Graph Deep Learning in Practice*'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B22118_07.xhtml#_idTextAnchor131)，*图深度学习实践*'
- en: '[*Chapter 8*](B22118_08.xhtml#_idTextAnchor138) , *Graph Deep Learning for
    Natural Language Processing*'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B22118_08.xhtml#_idTextAnchor138)，*自然语言处理中的图深度学习*'
- en: '[*Chapter 9*](B22118_09.xhtml#_idTextAnchor156) , *Building Recommendation
    Systems Using Graph Deep Learning*'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B22118_09.xhtml#_idTextAnchor156)，*使用图深度学习构建推荐系统*'
- en: '[*Chapter 10*](B22118_10.xhtml#_idTextAnchor182) , *Graph Deep Learning for
    Computer Vision*'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B22118_10.xhtml#_idTextAnchor182)，*计算机视觉中的图深度学习*'
