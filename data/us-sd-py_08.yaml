- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Using Community-Shared LoRAs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用社区共享的LoRA
- en: To meet specific needs and generate higher fidelity images, we may need to fine-tune
    a pre-trained Stable Diffusion model, but the fine-tuning process is extremely
    slow without powerful GPUs. Even if you have all the hardware or resources on
    hand, the fine-tuned model is large, usually the same size as the original model
    file.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足特定需求并生成更高保真的图像，我们可能需要微调预训练的Stable Diffusion模型，但没有强大的GPU，微调过程会非常缓慢。即使你有所有硬件或资源，微调后的模型仍然很大，通常与原始模型文件大小相同。
- en: Fortunately, researchers from the Large Language Model (LLM) neighbor community
    developed an efficient fine-tuning method, **Low-Rank Adaptation** (**LoRA** —
    “Low” is why the “o” is in lowercase) [1]. With LoRA, the original checkpoint
    is frozen without any modification, and the tuned weight changes are stored in
    an independent file, which we usually call the LoRA file. Additionally, there
    are countless community-shared LoRAs on sites such as CIVITAI [4] and HuggingFace.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，来自大型语言模型（LLM）邻域社区的研究人员开发了一种高效的微调方法，**低秩适配**（**LoRA** —— “低”是为什么“o”是小写的原因）[1]。使用LoRA，原始检查点保持冻结状态，没有任何修改，而微调权重更改存储在一个独立的文件中，我们通常称之为LoRA文件。此外，在CIVITAI
    [4]和HuggingFace等网站上还有无数社区共享的LoRA。
- en: In this chapter, we are going to delve into the theory of LoRA, and then introduce
    the Python way to load up LoRA into a Stable Diffusion model. We will also dissect
    a LoRA model to understand the LoRA model structure internally and create a custom
    function to load up a Stable Diffusion V1.5 LoRA.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨LoRA的理论，然后介绍将LoRA加载到Stable Diffusion模型中的Python方法。我们还将剖析LoRA模型，以了解LoRA模型的结构，并创建一个自定义函数来加载Stable
    Diffusion V1.5 LoRA。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: How does LoRA work?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA是如何工作的？
- en: Using LoRA with Diffusers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Diffusers与LoRA
- en: Applying LoRA weight during loading
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加载过程中应用LoRA权重
- en: Diving into LoRA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解LoRA
- en: Making a function to load LoRA
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个用于加载LoRA的函数
- en: Why LoRA works
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么LoRA有效
- en: By the end of this chapter, we will be able to use any community LoRA programmatically
    and also understand how and why LoRA works in Stable Diffusion.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将能够程序化地使用任何社区LoRA，并了解LoRA在Stable Diffusion中是如何以及为什么工作的。
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: If you have `Diffusers` package running in your computer, you should be able
    to execute all code in this chapter as well as the code used to load LoRA with
    Diffusers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在你的计算机上运行了`Diffusers`包，你应该能够执行本章中的所有代码，以及用于使用Diffusers加载LoRA的代码。
- en: Diffusers use **PEFT** (**Parameter-Efficient Fine-Tuning**) [10] to manage
    the LoRA loading and offloading. PEFT is a library developed by Hugging Face that
    provides parameter-efficient ways to adapt large pre-trained models for specific
    downstream applications. The key idea behind PEFT is to fine-tune only a small
    fraction of a model’s parameters instead of fine-tuning all of them, resulting
    in significant savings in terms of computation and memory usage. This makes it
    possible to fine-tune very large models even on consumer hardware with limited
    resources. Turn to [*Chapter 21*](B21263_21.xhtml#_idTextAnchor405) for more about
    LoRA.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Diffusers使用**PEFT**（**参数高效微调**）[10]来管理LoRA的加载和卸载。PEFT是由Hugging Face开发的库，它提供了参数高效的适应大型预训练模型的方法，以适应特定的下游应用。PEFT背后的关键思想是仅微调模型参数的一小部分，而不是全部微调，从而在计算和内存使用方面节省了大量资源。这使得即使在资源有限的消费级硬件上也能微调非常大的模型。有关LoRA的更多信息，请参阅[*第21章*](B21263_21.xhtml#_idTextAnchor405)。
- en: 'We will need to install the PEFT package to enable Diffusers’ PEFT LoRA loading:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装PEFT包来启用Diffusers的PEFT LoRA加载：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also refer to [*Chapter 2*](B21263_02.xhtml#_idTextAnchor037), if you
    encounter other execution errors from the code.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在代码中遇到其他执行错误，也可以参考[*第2章*](B21263_02.xhtml#_idTextAnchor037)。
- en: How does LoRA work?
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA是如何工作的？
- en: LoRA is a technique for quickly fine-tuning diffusion models, first introduced
    by Microsoft researchers in a paper by Edward J. Hu et al [1]. It works by creating
    a small, low-rank model that is adapted for a specific concept. This small model
    can be merged with the main checkpoint model to generate images similar to the
    ones used to train LoRA.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA是一种快速微调扩散模型的技术，最初由微软研究人员在Edward J. Hu等人的论文中提出[1]。它通过创建一个针对特定概念进行适配的小型、低秩模型来实现。这个小模型可以与主检查点模型合并，以生成与用于训练LoRA的图像相似的图像。
- en: 'Let’s use W to denote the original UNet attention weights (Q,K,V), ΔW to denote
    the fine-tuned weights from LoRA, and W′ as the merged weights. The process of
    adding LoRA to a model can be expressed like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用W表示原始UNet注意力权重（Q,K,V），用ΔW表示LoRA的微调权重，用W′表示合并后的权重。将LoRA添加到模型的过程可以表示如下：
- en: W′= W + ΔW
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: W′= W + ΔW
- en: 'If we want to control the scale of LoRA weights, we denote the scale as α.
    Adding LoRA to a model can be expressed like this now:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想控制LoRA权重的比例，我们用α表示这个比例。现在，将LoRA添加到模型可以表示如下：
- en: W′= W + αΔW
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: W′= W + αΔW
- en: 'The range of α can be from `0` to `1.0` [2]. It should be fine if we set α
    slightly larger than `1.0`. The reason why LoRA is so small is that ΔW can be
    represented by two small matrices A and B, such that:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: α的范围可以从`0`到`1.0` [2]。如果我们把α设置得略大于`1.0`，应该没问题。LoRA之所以如此小，是因为ΔW可以用两个小的矩阵A和B来表示，使得：
- en: ΔW = A B T
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ΔW = A B T
- en: Where A ∈ ℝ n×d is an n × d matrix, and B ∈ ℝ m×d is an m × d matrix. The transpose
    of B denoted as B T is a d × m matrix.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中A ∈ ℝ n×d是一个n × d的矩阵，B ∈ ℝ m×d是一个m × d的矩阵。B的转置，记作B T，是一个d × m的矩阵。
- en: For example, if ΔW is a 6 × 8 matrix, there are a total of `48` weight numbers.
    Now, in the LoRA file, the 6 × 8 matrix can be represented by two matrices – one
    6 × 2 matrix, `12` numbers in total, and another 2 × 8 matrix, making it `16`
    numbers in total.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果ΔW是一个6 × 8的矩阵，总共有`48`个权重数。现在，在LoRA文件中，6 × 8的矩阵可以表示为两个矩阵——一个6 × 2的矩阵，总共`12`个数，另一个2
    × 8的矩阵，总共`16`个数。
- en: The total number of weights is reduced from `48` to `28`. This is why the LoRA
    file can be so small compared to the checkpoint model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 权重的总数从`48`减少到`28`。这就是为什么与检查点模型相比，LoRA文件可以如此小。
- en: Using LoRA with Diffusers
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Diffusers的LoRA
- en: With the contributions from the open source community, loading LoRA with Python
    has never been easier. In this section, we are going to cover the solutions to
    load a LoRA model with Diffusers.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开源社区的贡献，使用Python加载LoRA从未如此简单。在本节中，我们将介绍如何使用Diffusers加载LoRA模型。
- en: 'In the following steps, we will first load the base Stable Diffusion V1.5,
    generate an image without LoRA, and then load a LoRA model called `MoXinV1` into
    the base model. We will clearly see the difference with and without the LoRA model:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将首先加载基础Stable Diffusion V1.5，生成不带LoRA的图像，然后加载一个名为`MoXinV1`的LoRA模型到基础模型中。我们将清楚地看到带和不带LoRA模型之间的差异：
- en: '**Prepare a Stable Diffusion pipeline**: The following code will load up the
    Stable Diffusion pipeline and move the pipeline instance to VRAM:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准备Stable Diffusion管道**：以下代码将加载Stable Diffusion管道并将管道实例移动到VRAM：'
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Generate an image without LoRA**: Now, let’s generate an image without LoRA
    loaded. Here, I am going to use the Stable Diffusion default v1.5 model to generate
    “a branch of flower” in a “traditional Chinese ink painting” style:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成不带LoRA的图像**：现在，让我们生成一个不带LoRA加载的图像。这里，我将使用Stable Diffusion默认v1.5模型以“传统中国水墨画”风格生成“一枝花”：'
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code uses a non-cherry-picked generator with `default seed:1`.
    The result is shown in *Figure 8**.1*:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码使用了一个非精选的生成器，`默认种子：1`。结果如图*图8**.1*所示：
- en: '![Figure 8.1: A branch of flower without LoRA](img/B21263_08_01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1：不带LoRA的花枝](img/B21263_08_01.jpg)'
- en: 'Figure 8.1: A branch of flower without LoRA'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：未使用LoRA的花枝
- en: To be honest, the preceding image isn’t that good, and the “flower” is more
    like black ink dots.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 坦白说，前面的图像并不那么好，而且“花”更像是一团黑墨点。
- en: '**Generate an image with LoRA with default settings**: Next, let’s load up
    the LoRA model to the pipeline and see what the MoXin LoRA can do to help image
    generation. Loading LoRA with default settings is just one line of code:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用默认设置生成带LoRA的图像**：接下来，让我们将LoRA模型加载到管道中，看看MoXin LoRA能对图像生成带来什么帮助。使用默认设置加载LoRA只需一行代码：'
- en: '[PRE15]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Diffusers downloads the LoRA model file automatically if the model does not
    exist in your model cache.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果模型不存在于你的模型缓存中，Diffusers会自动下载LoRA模型文件。
- en: 'Now, run the inference again with the following code (the same code used in
    *step 2*):'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，再次运行以下代码进行推理（与*步骤2*中使用的相同代码）：
- en: '[PRE21]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We will have a new image with a better “flower” in the ink-painting style,
    as shown in *Figure 8**.2*:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到一个包含更好“花”的水墨画风格的新图像，如图*图8**.2*所示：
- en: '![Figure 8.2: A branch of flower with LoRA using the default settings](img/B21263_08_02.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2：使用默认设置下的带LoRA的花枝](img/B21263_08_02.jpg)'
- en: 'Figure 8.2: A branch of flower with LoRA using the default settings'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：使用默认设置下的LoRA的花枝
- en: This time, the “flower” is more like a flower and, overall, better than the
    one without applying LoRA. However, the code in this section loads LoRA without
    a “weight” applied to it. In the next section, we will load a LoRA model with
    an arbitrary weight (or α ).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，“花朵”更像是一朵花，总体上比没有应用 LoRA 的那朵花要好。然而，本节中的代码在加载 LoRA 时没有应用“权重”。在下一节中，我们将加载一个具有任意权重（或
    α）的 LoRA 模型。
- en: Applying a LoRA weight during loading
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在加载过程中应用 LoRA 权重
- en: In the *How does LoRA work?* section, we mentioned the α value used to define
    the portion of LoRA weight added to the main model. We can easily achieve this
    using Diffusers with PEFT [10].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *LoRA 如何工作？* 部分中，我们提到了用于定义添加到主模型中 LoRA 权重部分的 α 值。我们可以使用带有 PEFT [10] 的 Diffusers
    容易地实现这一点。
- en: What is PEFT? PEFT is a library developed by Hugging Face to efficiently adapt
    pre-trained models, such as **Large Language Models** (**LLMs**) and Stable Diffusion
    models, to new tasks without needing to fine-tune the whole model. PEFT is a broader
    concept representing a collection of methods aimed at efficiently fine-tuning
    LLMs. LoRA, conversely, is a specific technique that falls under the umbrella
    of PEFT.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是 PEFT？PEFT 是 Hugging Face 开发的一个库，用于高效地适应预训练模型，例如 **大型语言模型**（**LLMs**）和 Stable
    Diffusion 模型，而无需对整个模型进行微调。PEFT 是一个更广泛的概念，代表了一组旨在高效微调 LLMs 的方法。LoRA，相反，是 PEFT 范畴下的一种特定技术。
- en: Before the integration of PEFT, loading and managing LoRAs in Diffusers required
    a lot of custom code and hacking. To make it easier to manage multiple LoRAs with
    weight loading and offloading, Diffusers uses the PEFT library to help manage
    different adapters for inference. In PEFT, the fine-tuned parameters are called
    adapters, which is why you will see some parameters are named `adapters`. LoRA
    is one of the main adapter techniques; you can take LoRA and adapter as referring
    to the same thing through this chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成 PEFT 之前，Diffusers 中加载和管理 LoRAs 需要大量的自定义代码和破解。为了更轻松地管理具有加载和卸载权重的多个 LoRAs，Diffusers
    使用 PEFT 库来帮助管理推理的不同适配器。在 PEFT 中，微调的参数被称为适配器，这就是为什么你会看到一些参数被命名为 `adapters`。LoRA
    是主要的适配器技术之一；在本章中，你可以将 LoRA 和适配器视为同一事物。
- en: 'Loading a LoRA model with weight is simple, as shown in the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 加载具有权重的 LoRA 模型很简单，如下面的代码所示：
- en: '[PRE26]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the preceding code, we gave the LoRA weight as `0.5` to replace the default
    `1.0`. Now, you will see the generated image, as shown in *Figure 8**.3*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将 LoRA 权重设置为 `0.5` 以替换默认的 `1.0`。现在，你将看到如图 *图 8.3* 所示生成的图像。
- en: '![Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight](img/B21263_08_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3：通过应用 0.5 LoRA 权重添加的 LoRA 花枝](img/B21263_08_03.jpg)'
- en: 'Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：通过应用 0.5 LoRA 权重添加的 LoRA 花枝
- en: From *Figure 8**.3*, we can observe the difference after applying the `0.5`
    weight to the LoRA model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 8.3* 中，我们可以观察到应用 `0.5` 权重到 LoRA 模型后的差异。
- en: 'The PEFT-integrated Diffusers can also load another LoRA by reusing the same
    code we used to load the first LoRA model:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 PEFT 的 Diffusers 也可以通过重用我们用于加载第一个 LoRA 模型的相同代码来加载另一个 LoRA：
- en: '[PRE27]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, add the weight for the second LoRA model by calling the `set_adapters`
    function:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过调用 `set_adapters` 函数添加第二个 LoRA 模型的权重：
- en: '[PRE28]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will get a new image with style added from the second LoRA, as shown in
    *Figure 8**.4*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到一个新的图像，其中添加了来自第二个 LoRA 的样式，如图 *图 8.4* 所示：
- en: '![Figure 8.4: A branch of flower with two LoRA models](img/B21263_08_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4：具有两个 LoRA 模型的花枝](img/B21263_08_04.jpg)'
- en: 'Figure 8.4: A branch of flower with two LoRA models'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：具有两个 LoRA 模型的花枝
- en: We can also use the same code to load LoRA for Stable Diffusion XL pipelines.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用相同的代码为 Stable Diffusion XL 管道加载 LoRA。
- en: 'With PEFT, we don’t need to restart the pipeline to disable LoRA; we can disable
    all LoRAs with simply one line of code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PEFT，我们不需要重新启动管道来禁用 LoRA；我们可以通过一行代码简单地禁用所有 LoRAs：
- en: '[PRE29]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that the implementation of LoRA loading is somewhat different compared
    with other tools, such as A1111 Stable Diffusion WebUI. Using the same prompt,
    the same settings, and the same LoRA weight, you may get a different result.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LoRA 加载的实现与其他工具（如 A1111 Stable Diffusion WebUI）略有不同。使用相同的提示、相同的设置和相同的 LoRA
    权重，你可能会得到不同的结果。
- en: Don’t worry – in the next section, we are going to dive into the LoRA model
    internally and implement a solution to use LoRA that outputs the same result,
    with tools such as A1111 Stable Diffusion WebUI.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心——在下一节中，我们将深入探讨 LoRA 模型的内部结构，并使用 A1111 Stable Diffusion WebUI 等工具实现一个使用 LoRA
    输出相同结果的解决方案。
- en: Diving into the internal structure of LoRA
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨LoRA的内部结构
- en: Understanding how LoRA works internally will help us to implement our own LoRA-related
    features based on specific needs. In this section, we are going to dive into the
    internals of LoRA’s structure and its weights schema, and then manually load the
    LoRA model into the Stable Diffusion model step by step.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 理解LoRA内部工作原理将帮助我们根据具体需求实现自己的LoRA相关功能。在本节中，我们将深入探讨LoRA的结构和权重模式，然后逐步手动将LoRA模型加载到Stable
    Diffusion模型中。
- en: 'As we discussed at the beginning of the chapter, applying LoRA is as simple
    as the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本章开头所讨论的，应用LoRA就像以下这样简单：
- en: W′= W + αΔW
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: W′= W + αΔW
- en: 'And ΔW can be broken down into A and B:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ΔW可以分解为A和B：
- en: ΔW = A B T
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ΔW = A B T
- en: 'So, the overall idea of merging LoRA weights to the checkpoint model works
    like this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将LoRA权重合并到检查点模型的整体思路是这样的：
- en: Find the A and B weight matrix from the LoRA file.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从LoRA文件中找到A和B权重矩阵。
- en: Match the LoRA module layer name to the checkpoint module layer name so that
    we know which matrix to merge.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将LoRA模块层名与检查点模块层名匹配，以便我们知道要合并哪个矩阵。
- en: Produce ΔW = A B T.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成ΔW = A B T。
- en: Update the checkpoint model weights.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新检查点模型的权重。
- en: 'If you have prior experience training a LoRA model, you might be aware that
    a hyperparameter, `alpha`, can be set to a value greater than `1`, such as `4`.
    This is often done in conjunction with setting another parameter, `rank`, to `4`
    as well. However, α used in this context is typically less than 1\. The actual
    value of α is generally computed using the following formula:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前有训练LoRA模型的经验，你可能知道可以设置一个超参数`alpha`，其值大于`1`，例如`4`。这通常与将另一个参数`rank`也设置为`4`一起进行。然而，在此上下文中使用的α通常小于1。α的实际值通常使用以下公式计算：
- en: α =  alpha _ rank
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: α =  alpha _ rank
- en: During the training phase, setting both `alpha` and `rank` to `4` will yield
    an α value of `1`. This concept may seem confusing if not properly understood.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，将`alpha`和`rank`都设置为`4`将产生α值为`1`。如果不正确理解，这个概念可能会让人感到困惑。
- en: Next, let’s explore the internals of a LoRA model step by step.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们一步一步地探索LoRA模型的内部结构。
- en: Finding the A and B weight matrix from the LoRA file
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从LoRA文件中找到A和B权重矩阵
- en: 'Before start exploring the internals of a LoRA structure, you will need to
    download a LoRA file. You can download the `MoXinV1.safetensors` from the following
    URL: [https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors](https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始探索LoRA结构的内部结构之前，你需要下载一个LoRA文件。你可以从以下URL下载`MoXinV1.safetensors`：[https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors](https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors)。
- en: 'After setting up the LoRA file in the `.safetensors` format, load it using
    the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在`.safetensors`格式中设置好LoRA文件后，使用以下代码加载它：
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When LoRA weights are applied to the text encoder, the key names start with
    `lora_te_`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当LoRA权重应用于文本编码器时，键名以`lora_te_`开头：
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'When LoRA weights are applied to UNet, key names start with `lora_unet_`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当LoRA权重应用于UNet时，键名以`lora_unet_`开头：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is of the `string` type. Here are the meanings of the terms that
    appeared in the output LoRA weight keys:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是`string`类型。以下是输出LoRA权重键中出现过的术语的含义：
- en: The `lora_te_` prefix says that the weights are applied to the text encoder;
    `lora_unet_` says that the weights aim at updating the Stable Diffusion `unet`
    module layers.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_te_`前缀表示权重应用于文本编码器；`lora_unet_`表示权重旨在更新Stable Diffusion `unet`模块的层。'
- en: '`down_blocks_0_attentions_1_proj_in` is the layer name, which should exist
    in the checkpoint model `unet` modules too.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`down_blocks_0_attentions_1_proj_in`是层名，这个层名应该存在于检查点模型的`unet`模块中。'
- en: '`.alpha` is the trained weight set to denote how much of the LoRA weight will
    be applied to the main checkpoint model. It holds a float value that is denoted
    as α in W′= W + αΔW. Since the value will be replaced by user input, we can skip
    this value.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.alpha`是训练好的权重，用来表示将有多少LoRA权重应用到主检查点模型中。它持有表示α的浮点值，在W′= W + αΔW中。由于这个值将由用户输入替换，我们可以跳过这个值。'
- en: '`lora_down.weight` denotes the value of this layer that represents A.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_down.weight`表示代表A的这个层的值。'
- en: '`lora_up.weight` denotes the value of this layer that represents B.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lora_up.weight`表示代表B的这个层的值。'
- en: Note that `down` in `down_blocks` denotes the downside (the left side of UNet)
    of the `unet` model.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，`down`在`down_blocks`中表示`unet`模型的下方（UNet的左侧）。
- en: 'The following Python code will get the LoRA layer info and also have the model
    object handler:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码将获取LoRA层信息，并具有模型对象处理器：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`key` holds the LoRA module layer name, and `layer_infos` holds the checkpoint
    model layer name extracted from the LoRA layers. The reason we do this is that
    not all layers from the checkpoint model have LoRA weights to adjust, which is
    why we need to get the list of layers that will be updated.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`key`持有LoRA模块层名称，而`layer_infos`持有从LoRA层中提取的检查点模型层名称。我们这样做的原因是检查点模型中并非所有层都有LoRA权重进行调整，因此我们需要获取将要更新的层的列表。'
- en: Finding the corresponding checkpoint model layer name
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到相应的检查点模型层名称
- en: 'Print out the structure of the checkpoint model `unet` structure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出检查点模型`unet`结构：
- en: '[PRE34]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can see that the module is stored in a tree structure like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到模块是以这样的树状结构存储的：
- en: '[PRE35]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Each line is composed of a module name (`down_blocks`), and the module content
    can be `ModuleList` or a specific neural network layer, `Conv2d`. These are the
    components of the UNet. For now, applying LoRA to a specific UNet module isn''t
    required. However, it''s important to understand the UNet''s internal structure:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 每行由一个模块名称（`down_blocks`）组成，模块内容可以是`ModuleList`或特定的神经网络层，`Conv2d`。这些都是UNet的组成部分。目前，将LoRA应用于特定的UNet模块不是必需的。然而，了解UNet的内部结构很重要：
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The loop-through part is a bit tricky. When looking back to the checkpoint
    model structure, which is layered as a tree, we can’t simply use a `for` loop
    to loop through the list. Instead, we need to use a `while` loop to navigate every
    leaf of the tree. The overall process is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 循环部分有点棘手。当回顾检查点模型结构，它以分层的形式作为树时，我们不能简单地使用`for`循环来遍历列表。相反，我们需要使用`while`循环来导航树的每个叶子。整个过程如下：
- en: '`layer_infos.pop(0)` will return the first name of the list in the `string`
    type and remove it from the list such as `up` from the `layer_infos` list – `[''up'',
    ''blocks'', ''3'', ''attentions'', ''2'', ''transformer'', ''blocks'', ''0'',
    ''ff'', ''``net'', ''2'']`'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`layer_infos.pop(0)`将返回列表中的第一个名称，并将其从列表中移除，例如从`layer_infos`列表中移除`up` – `[''up'',
    ''blocks'', ''3'', ''attentions'', ''2'', ''transformer'', ''blocks'', ''0'',
    ''ff'', ''``net'', ''2'']`'
- en: Use `curr_layer.__getattr__(temp_name)` to check whether the layer exists or
    not. If it does not exist, an exception will be thrown, and the program will move
    to the `exception` section to continue outputting names from the `layer_infos`
    list and check again.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`curr_layer.__getattr__(temp_name)`来检查层是否存在。如果不存在，将抛出异常，程序将移动到`exception`部分继续输出`layer_infos`列表中的名称，并再次检查。
- en: If the layer is found but some names are still left in the `layer_infos` list,
    they will keep on popping out.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到了层，但`layer_infos`列表中仍有剩余的名称，它们将继续弹出。
- en: The names will continue to pop out until no exception is thrown out and we meet
    the `len(layer_infos) == 0` condition, which means that the layer is fully matched.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 名称将继续出现，直到没有抛出异常，并且我们遇到`len(layer_infos) == 0`条件，这意味着层已完全匹配。
- en: At this point, the `curr_layer` object points to the checkpoint model weight
    data and can be referenced in the next step.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，`curr_layer`对象指向检查点模型权重数据，可以在下一步中进行引用。
- en: Updating the checkpoint model weights
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新检查点模型权重
- en: 'For easier key value referencing, let’s make a `pair_keys = []` list, in which
    `pair_keys[0]` returns the A matrix and `pair_keys[1]` returns the B matrix:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于键值引用，让我们创建一个`pair_keys = []`列表，其中`pair_keys[0]`返回A矩阵，`pair_keys[1]`返回B矩阵：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we update the weights:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新权重：
- en: '[PRE38]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The `alpha * torch.mm(weight_up, weight_down)` code is the core code used to
    implement αA B T.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha * torch.mm(weight_up, weight_down)`代码是用于实现αA B T的核心代码。'
- en: And that’s it! Now, the pipeline’s text encoder and `unet` model weights are
    updated by LoRA. Next, let’s put all the parts together to create a full-featured
    function that can load a LoRA model into the Stable Diffusion pipeline.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在，管道的文本编码器和`unet`模型权重已经通过LoRA更新。接下来，让我们将所有部分组合起来，创建一个功能齐全的函数，可以将LoRA模型加载到Stable
    Diffusion管道中。
- en: Making a function to load LoRA
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个加载LoRA的函数
- en: 'Let’s add one more list to store keys that have been visited and put all the
    preceding code together into a function named `load_lora`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再添加一个列表来存储已访问的键，并将所有前面的代码组合到一个名为`load_lora`的函数中：
- en: '[PRE39]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'To use the function is easy; simply provide the `pipeline` object, the LoRA
    path, `lora_path`, and the LoRA weight number, `lora_weight`, like this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该函数很简单；只需提供`pipeline`对象、LoRA路径`lora_path`和LoRA权重编号`lora_weight`，如下所示：
- en: '[PRE40]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let’s try it out:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们试试看：
- en: '[PRE41]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It works, and works well; see the result shown in *Figure 8**.5*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实有效，效果很好；请参见*图8**.5*中所示的结果：
- en: '![Figure 8.5: A branch of flower using the custom LoRA loader](img/B21263_08_05.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5：使用自定义LoRA加载器的花朵分支](img/B21263_08_05.jpg)'
- en: 'Figure 8.5: A branch of flower using the custom LoRA loader'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：使用自定义LoRA加载器的花朵分支
- en: You might be wondering, “Why does a small LoRA file possess such formidable
    capabilities?” Let’s delve deeper into the reasons why a LoRA model is effective.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，“为什么一个小的LoRA文件具有如此强大的能力？”让我们深入探讨LoRA模型有效的原因。
- en: Why LoRA works
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么LoRA有效
- en: 'The paper *Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning* [8] by Armen et al. found that the pre-trained representations’
    intrinsic dimension is way lower than expected, stated by them as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Armen等人撰写的论文*内禀维度解释了语言模型微调的有效性* [8]发现，预训练表示的内禀维度远低于预期，他们如下所述：
- en: “We empirically show that common NLP tasks within the context of pre-trained
    representations have an intrinsic dimension several orders of magnitudes less
    than the full parameterization.”
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: “我们通过实验表明，在预训练表示的上下文中，常见的NLP任务具有比完整参数化低几个数量级的内禀维度。”
- en: The intrinsic dimension of a matrix is a concept used to determine the effective
    number of dimensions required to represent the important information contained
    within that matrix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的内禀维度是一个用于确定表示该矩阵中包含的重要信息所需的有效维数的概念。
- en: 'Let’s suppose we have a matrix, `M`, with five rows and three columns, like
    this:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个矩阵`M`，有五行三列，如下所示：
- en: '[PRE42]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Each row of this matrix represents a data point or a vector with three values.
    We can think of these vectors as points in a three-dimensional space. However,
    if we visualize these points, we might find that they lie approximately on a two-dimensional
    plane, rather than occupying the full three-dimensional space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的每一行代表一个包含三个值的数据点或向量。我们可以将这些向量视为三维空间中的点。然而，如果我们可视化这些点，我们可能会发现它们大约位于一个二维平面上，而不是占据整个三维空间。
- en: In this case, the intrinsic dimension of the matrix, `M`, would be `2`, indicating
    that the essential structure of the data can be captured effectively using two
    dimensions. The third dimension doesn’t provide much additional information.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，矩阵`M`的内禀维度将是`2`，这意味着可以使用两个维度有效地捕捉数据的本质结构。第三个维度没有提供太多额外的信息。
- en: A low intrinsic dimension matrix can be represented by two low-rank matrices
    because the data in the matrix can be compressed into a few key features. These
    features can then be represented by two smaller matrices, each of which has a
    rank that is equal to the intrinsic dimension of the original matrix.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一个低内禀维度的矩阵可以通过两个低秩矩阵来表示，因为矩阵中的数据可以被压缩到几个关键特征。然后，这些特征可以通过两个较小的矩阵来表示，每个矩阵的秩等于原始矩阵的内禀维度。
- en: 'The paper *LoRA: Low-Rank Adaptation of Large Language Models* [1] by Edward
    J. Hu et al goes a step further, introducing the concept of LoRA to leverage the
    low intrinsic dimension nature, boosting the fine-tuning process by breaking down
    the delta weights to two low-rank parts, ΔW = A B T.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Edward J. Hu等人撰写的论文*LoRA：大型语言模型的低秩自适应* [1]更进一步，引入了LoRA的概念，利用低内禀维度的特性，通过将权重差分解为两个低秩部分来加速微调过程，ΔW
    = A B T。
- en: The effectiveness of LoRA was soon discovered to extend beyond LLM models, also
    yielding good results with diffusion models. Simo Ryu published the LoRA [2] code
    and was the first one to try out LoRA training for Stable Diffusion. That was
    in July 2023 and there are now more than 40,000 LoRA models shared at [https://www.civitai.com](https://www.civitai.com).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 很快发现LoRA的有效性不仅限于LLM模型，还与扩散模型结合产生了良好的结果。Simo Ryu发布了LoRA [2]代码，并成为第一个尝试对Stable
    Diffusion进行LoRA训练的人。那是在2023年7月，现在在[https://www.civitai.com](https://www.civitai.com)上共享了超过40,000个LoRA模型。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how to enhance the Stable Diffusion model using
    LoRA, understood what LoRA is, and why it is good for fine-tuning and inference.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用LoRA增强Stable Diffusion模型，理解了LoRA是什么，以及为什么它对微调和推理有益。
- en: Then, we began loading LoRA using the experimental functions from the `Diffusers`
    package and provided LoRA weights through a custom implementation. We used simple
    code to quickly understand what LoRA can bring to the table.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始使用`Diffusers`包中的实验函数加载LoRA，并通过自定义实现提供LoRA权重。我们使用简单的代码快速了解LoRA能带来什么。
- en: Then, we dived into the internal structure of a LoRA model, walked through the
    detailed steps to extract LoRA weights, and understood how to merge those weights
    into the checkpoint model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们深入研究了LoRA模型的内部结构，详细介绍了提取LoRA权重的步骤，并了解了如何将这些权重合并到检查点模型中。
- en: Further, we implemented a function in Python that can load a LoRA safetensors
    file and perform weight merges.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们实现了一个Python函数，可以加载LoRA safetensors文件并执行权重合并。
- en: Finally, we briefly explored why LoRA works, based on the most recent papers
    from researchers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要探讨了LoRA为何有效，基于研究人员最新的论文。
- en: In the next chapter, we are going to explore another powerful technique – textual
    inversion – to teach a model new “words,” and then use the pre-trained “words”
    to add new concepts to the generated images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探索另一种强大的技术——文本反转——来教模型新的“单词”，然后使用预训练的“单词”向生成的图像添加新概念。
- en: References
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Edward J. et al, LoRA: Low-Rank Adaptation of Large Language Models: [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Edward J.等人，LoRA：大型语言模型的低秩自适应：[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
- en: 'Simo Ryu (cloneofsimo), `lora`: [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Simo Ryu (cloneofsimo), `lora`: [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)'
- en: '`kohya_lora_loader`: [https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44](https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44)'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kohya_lora_loader`: [https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44](https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44)'
- en: 'CIVITAI: [https://www.civitai.com](https://www.civitai.com)'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CIVITAI：[https://www.civitai.com](https://www.civitai.com)
- en: 'Rinon Gal et al, An Image is Worth One Word: Personalizing Text-to-Image Generation
    using Textual Inversion: [https://textual-inversion.github.io/](https://textual-inversion.github.io/)'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rinon Gal等人，一张图片胜过千言万语：使用文本反转个性化文本到图像生成：[https://textual-inversion.github.io/](https://textual-inversion.github.io/)
- en: 'Diffusers’ `lora_state_dict` function: [https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py)'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Diffusers的`lora_state_dict`函数：[https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py)
- en: 'Andrew Zhu, Improving Diffusers Package for High-Quality Image Generation:
    [https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4](https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4)'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew Zhu，改进Diffusers包以实现高质量图像生成：[https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4](https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4)
- en: 'Armen et al, Intrinsic Dimensionality Explains the Effectiveness of Language
    Model Fine-Tuning: [https://arxiv.org/abs/2012.13255](https://arxiv.org/abs/2012.13255)'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Armen等人，内在维度解释了语言模型微调的有效性：[https://arxiv.org/abs/2012.13255](https://arxiv.org/abs/2012.13255)
- en: 'Hugging Face, LoRA: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hugging Face, LoRA: [https://huggingface.co/docs/diffusers/training/lora](https://huggingface.co/docs/diffusers/training/lora)'
- en: 'Hugging Face, PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hugging Face, PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
