- en: 6\. LSTMs, GRUs, and Advanced RNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. LSTM、GRU和高级RNN
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will study and implement advanced models and variations
    of the plain **Recurrent Neural Network** (**RNN**) that overcome some of RNNs'
    practical drawbacks and are among the best performing deep learning models at
    the moment. We will start by understanding the drawbacks of plain RNNs and see
    how the novel idea of **Long Short-Term Memory** overcomes them. We will then
    see and implement a **Gated Recurrent Unit** based model. We will also work with
    bidirectional and stacked RNNs and explore attention-based models. By the end
    of this chapter, you will have built and assessed the performance of these models
    on a sentiment classification task, observing for yourself the trade-offs in choosing
    the different models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究并实现一些高级模型和普通**循环神经网络**（**RNN**）的变体，这些模型克服了RNN的一些实际缺点，并且目前是表现最好的深度学习模型之一。我们将从理解普通RNN的缺点开始，看看**长短期记忆网络**（**LSTM**）这一新颖的理念是如何克服这些缺点的。接着，我们将实现一个基于**门控循环单元**（**GRU**）的模型。我们还将研究双向RNN和堆叠RNN，并探索基于注意力机制的模型。到本章结束时，您将构建并评估这些模型在情感分类任务中的表现，并亲自观察选择不同模型时的权衡。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Let''s say you''re working with product reviews for a mobile phone and your
    task is to classify the sentiment in the reviews as being positive or negative.
    You encounter a review that says: *"The phone does not have a great camera, or
    an amazingly vivid display, or an excellent battery life, or great connectivity,
    or other great features that make it the best."* Now, when you read this, you
    can easily identify that the sentiment in the review is negative, despite the
    presence of many positive phrases such as *"excellent battery life"* and *"makes
    it the best"*. You understand that the presence of the term *"not"* right toward
    the beginning of the text negates everything else that comes after.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您正在处理一款手机的产品评论，任务是将评论中的情感分类为正面或负面。您遇到了一条评论说：“*这款手机没有很好的相机，也没有令人惊艳的显示效果，没有优秀的电池续航，没有良好的连接性，也没有其他让它成为最棒的特性*。”现在，当您读到这条评论时，您可以轻松识别出评论的情感是负面的，尽管其中有许多正面的词句，比如*“优秀的电池续航”*和*“让它成为最棒的”*。您理解到，文本开头的*“没有”*一词否定了之后的所有内容。
- en: Will the models we've created so far be able to identify the sentiment in such
    a case? Probably not, because if your models don't realize that the term *"not"*
    toward the beginning of the sentences is important and needs to be connected strongly
    to the output several terms later, they won't be able to identify the sentiment
    correctly. This, unfortunately, is a major drawback of plain RNNs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建的模型能否识别出这种情况中的情感？可能不能，因为如果您的模型没有意识到句子开头的*“没有”*一词是很重要的，并且需要与几词之后的输出强相关，那么它们就无法正确识别情感。这不幸的是，普通RNN的一个重大缺点。
- en: In the previous chapter, we looked at a couple of deep learning approaches for
    dealing with sequences, that is, one-dimensional convolutions and RNNs. We saw
    that RNNs are extremely powerful models that provide us with a great amount of
    flexibility to handle different sequence tasks. The plain RNNs that we saw have
    been subject to plenty of research. Now, we will look at some approaches that
    have been built on top of RNNs to create new, powerful models that overcome the
    drawbacks of RNNs. We will look at LSTM, GRUs, stacked and bidirectional LSTMs,
    and attention-based models. We will apply these models to a sentiment classification
    task, thereby bringing together the concepts discussed in *Chapter 4, Deep Learning
    for Text – Embeddings*, and *Chapter 5, Deep Learning for Sequences*, as well.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了几种用于处理序列的深度学习方法，即一维卷积和RNN。我们看到，RNN是极为强大的模型，能够为我们提供灵活性，处理各种序列任务。我们看到的普通RNN已经进行了大量研究。现在，我们将研究一些基于RNN的进阶方法，创建出新的、强大的模型，克服RNN的缺点。我们将研究LSTM、GRU、堆叠和双向LSTM，以及基于注意力机制的模型。我们将把这些模型应用于情感分类任务，结合*第4章：文本的深度学习—词嵌入*和*第5章：序列的深度学习*中讨论的概念。
- en: Long-Range Dependence/Influence
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期依赖/影响
- en: 'The sample mobile phone review we saw in the previous section was an example
    of a long-range dependence/influence – where a term/value in a sequence has an
    influence on the assessment of a lot of the subsequent terms/values. Consider
    the following example, where you need to fill in the blank with a missing country
    name: *"After a top German university granted her admission for her Masters in
    Dentistry, Hina was extremely excited to start this new phase of her career with
    international exposure and couldn''t wait till the end of the month to book her
    flight to ____."*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节看到的手机评论示例就是一个长期依赖/影响的例子——一个序列中的术语/值对后续许多术语/值的评估产生影响。考虑以下示例，你需要填补一个缺失的国家名称：“*在一所顶尖的德国大学为她的牙科硕士课程提供录取后，Hina
    非常兴奋，迫不及待地想要开始这一新的人生阶段，并且等不及要在月底前订机票去 ____。*”
- en: 'The correct answer, of course, is **Germany**, arriving at which would require
    you to understand the importance of the term "German", which appears at the beginning
    of the sentence, on the outcome at the end of the sentence. This is another example
    of long-range dependence. The following figure shows the long-range dependence
    of the answer, "*Germany*", on the term "*German*" appearing early in the sentence:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案当然是**德国**，要得出这个答案，你需要理解“German”一词的重要性，这个词出现在句子的开头，并影响句子结尾的结果。这是另一个长期依赖的例子。下图展示了答案“*Germany*”如何受到句子开头出现的“*German*”一词的长期依赖：
- en: '![Figure 6.1: Long-range dependence'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1：长期依赖'
- en: '](img/B15385_06_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_01.jpg)'
- en: 'Figure 6.1: Long-range dependence'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1：长期依赖
- en: To get the best outcome, we need to be able to handle long-range dependencies.
    In the context of deep learning models and RNNs, this would mean that learning
    (or the backpropagation of errors) needs to happen smoothly and effectively over
    many time steps. This is easier said than done, primarily because of the vanishing
    gradient problem.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳结果，我们需要能够处理长期依赖问题。在深度学习模型和 RNN 的背景下，这意味着学习（或误差反向传播）需要在多个时间步长上平稳且有效地进行。说起来容易做起来难，主要是因为消失梯度问题。
- en: The Vanishing Gradient Problem
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消失梯度问题
- en: One of the biggest challenges while training standard feedforward deep neural
    networks is the vanishing gradient problem (as discussed in *Chapter 2, Neural
    Networks*). As the model gets more and more layers, backpropagating the errors
    all the way back to the initial layers becomes increasingly difficult. Layers
    close to the output will be "learning"/updated at a good pace, but by the time
    the error propagates to the initial layers, its value will have diminished greatly
    and have little or no effect on the parameters for the initial layers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 训练标准前馈深度神经网络时面临的最大挑战之一是消失梯度问题（如*第二章，神经网络*所讨论的）。随着模型层数的增加，将误差从输出层反向传播到最初的层变得越来越困难。接近输出层的层将“学习”/更新得较快，但当误差传播到最初的层时，其值会大大减少，对初始层的参数几乎没有或完全没有影响。
- en: With RNNs, this problem is further compounded, as the parameters need to be
    updated not only along the depth but also for the time steps. If we have one hundred
    time steps in the inputs (which isn't uncommon, especially when working with text),
    the network needs to propagate the error (calculated at the 100th time step) all
    the way back to the first time step. For plain RNNs, this task can be a bit too
    much to handle. This is where RNN variants can come in useful.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RNN 来说，这个问题更为复杂，因为参数不仅需要沿深度更新，还需要在时间步长上更新。如果输入有一百个时间步长（这在处理文本时并不罕见），网络需要将误差（在第
    100 个时间步长计算出的）一直反向传播到第一个时间步长。对于普通的 RNN，这项任务可能有些难以应对。这时，RNN 的变体就显得尤为重要。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Another practical issue with training deep networks is the exploding gradient
    problem, where the gradient values get very high – too high to be represented
    by the system. This issue has a rather simple workaround called "**Gradient Clipping**",
    which means capping the values of the gradient.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度网络的另一个实际问题是梯度爆炸问题，其中梯度值变得非常高——高到系统无法表示的程度。这个问题有一个相当简单的解决方法，叫做“**梯度裁剪**”，即限制梯度值的范围。
- en: Sequence Models for Text Classification
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类的序列模型
- en: 'In *Chapter 5, Deep Learning for Sequences*, we learned that RNNs perform extremely
    well on sequence-modeling tasks and provide high performance on text-related tasks.
    In this chapter, we will use plain RNNs and variants of RNNs on a sentiment classification
    task: processing the input sequence and predicting whether the sentiment is positive
    or negative.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章，序列的深度学习*中，我们了解到RNN在序列建模任务中表现非常出色，并且在与文本相关的任务中提供了高性能。在本章中，我们将使用普通的RNN和RNN的变体来进行情感分类任务：处理输入序列并预测情感是正面还是负面。
- en: 'We''ll use the IMDb reviews dataset for this task. The dataset contains 50,000
    movie reviews, along with their sentiment – 25,000 highly polar movie reviews
    for training and 25,000 for testing. A few reasons for using this dataset are
    as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用IMDb评论数据集进行此任务。该数据集包含50,000条电影评论及其情感——25,000条极具极性（polar）的电影评论用于训练，25,000条用于测试。使用该数据集的原因如下：
- en: It is very conveniently available to load Keras (tokenized version) with a single
    command.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需一个命令，就可以方便地加载Keras（分词版本）。
- en: The dataset is commonly used for testing new approaches/models. This should
    help you compare your results with other approaches easily.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该数据集通常用于测试新方法/模型。这应该帮助你轻松地将自己的结果与其他方法进行比较。
- en: Longer sequences in the data (IMDb reviews can get very long) help us assess
    the differences between the variants of RNNs better.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中的较长序列（IMDb评论可能非常长）帮助我们更好地评估RNN变体之间的差异。
- en: Let's get started by building our first model using plain RNNs and then benchmark
    the future model performances against that of the plain RNN. Let's start with
    data preprocessing and formatting the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建第一个模型，使用普通的RNN，然后将未来的模型表现与普通RNN的表现进行基准比较。我们从数据预处理和模型格式化开始。
- en: Loading Data
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: Note
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Make sure that you work on all the exercises and example codes in this chapter
    in the same Jupyter Notebook. Note that the code in this section will load the
    dataset. To ensure all exercises and example codes that follow work, please ensure
    that you do not skip this section. You can access the complete code for the exercises
    at [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你在本章中的所有练习和示例代码都在同一个Jupyter Notebook中进行。请注意，本节的代码将加载数据集。为了确保后续的所有练习和示例代码都能正常工作，请确保不要跳过这一节。你可以通过[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)访问完整的练习代码。
- en: 'To begin, you need to start a new Jupyter Notebook and import the `imdb` module
    from the Keras datasets. Note that unless mentioned otherwise, the code and exercises
    for the rest of this chapter should continue in the same Jupyter Notebook:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要启动一个新的Jupyter Notebook，并从Keras数据集中导入`imdb`模块。请注意，除非另有说明，本章其余的代码和练习应继续在同一个Jupyter
    Notebook中进行：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the module imported, importing the dataset (tokenized and separated into
    train and test sets) is as easy as running `imdb.load_data`. The only parameter
    we need to provide is the vocabulary size we wish to use. Recall that the vocabulary
    size is the total number of unique terms we wish to consider for the modeling
    process. When we specify a vocabulary size, *V*, we work with the top *V* terms
    in the data. Here, we will specify a vocabulary size of 8,000 for our models (an
    arbitrary choice; you can modify this as desired) and load the data using the
    `load_data` method, as shown here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 导入该模块后，加载数据集（已分词并分为训练集和测试集）就像运行`imdb.load_data`一样简单。我们需要提供的唯一参数是我们希望使用的词汇表大小。回顾一下，词汇表大小是我们希望在建模过程中考虑的独特词汇数量。当我们指定词汇表大小*V*时，我们就会使用数据中排名前*V*的词汇。在这里，我们将为我们的模型指定一个8,000的词汇表大小（一个任意选择；你可以根据需要修改）并使用`load_data`方法加载数据，如下所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s inspect the `X_train` variable to see what we are working with. Let''s
    print the type of it and the type of constituting elements, and also have a look
    at one of the elements:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下`X_train`变量，看看我们正在处理什么。让我们打印出它的类型和构成元素的类型，并查看其中一个元素：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will see the following output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `X_train` variable is a `numpy` array – each element of the array is a list
    representing the text for a single review. The terms in the text are present as
    numerical tokens instead of raw tokens. This is a very convenient format.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train`变量是一个`numpy`数组——数组的每个元素都是表示单个评论文本的列表。文本中的词汇以数字令牌的形式出现，而不是原始令牌。这是一种非常方便的格式。'
- en: 'The next step is to define an upper limit on the length of the sequences that
    we''ll work with and limit all sequences to the defined maximum length. We''ll
    use `200` – an arbitrary choice, in this case – to quickly get started with the
    model-building process`200` steps so that the networks don''t get too heavy, and
    because `200` time steps are sufficient to demonstrate the different RNN approaches.
    Let''s define the `maxlen` variable:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义序列的最大长度，并将所有序列限制为这个最大长度。我们将使用`200`——在这种情况下是一个任意选择——来快速开始模型构建过程，以避免网络变得过于庞大，而且`200`个时间步长足以展示不同的RNN方法。让我们定义`maxlen`变量：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The next step is to get all our sequences to the same length using the `pad_sequences`
    utility from Keras.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用Keras的`pad_sequences`工具将所有序列调整为相同的长度。
- en: Note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*****Ideally, we would analyze the lengths of the sequences and identify one
    that covers most of the reviews. We''ll perform these steps in the activity at
    the end of the chapter, in which we''ll use ideas from not only the current chapter,
    but also from *Chapter 4, Deep Learning for Text – Embeddings*, and *Chapter 5,
    Deep Learning for Sequences*, bringing this all together in a single activity.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*****理想情况下，我们会分析序列的长度并找出一个覆盖大多数评论的长度。在本章结束时的活动中，我们将执行这些步骤，结合当前章节的内容，还将结合*第4章，文本的深度学习——嵌入*和*第5章，序列的深度学习*的概念，最终将这些内容整合到一个活动中。'
- en: Staging and Preprocessing Our Data
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据的分阶段处理与预处理
- en: 'The `pad_sequences` utility from the `sequences` module in Keras helps us in
    getting all the sequences to a specified length. If the input sequence is shorter
    than the specified length, the utility pads the sequence with a reserved token
    (indicating a blank/missing). If the input sequence is longer than the specified
    length, the utility truncates the sequence to limit it. In the following example,
    we will apply the `pad_sequences` utility to our test and train datasets:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中`sequences`模块的`pad_sequences`工具帮助我们将所有序列调整到指定长度。如果输入序列短于指定长度，该工具会用保留的标记（表示空白/缺失）对序列进行填充。如果输入序列长于指定长度，工具会截断序列以限制其长度。在以下示例中，我们将对测试集和训练集应用`pad_sequences`工具：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To understand the result of the steps, let''s see the output for a particular
    instance in the training data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些步骤的结果，我们来看一下训练数据中特定实例的输出：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The processed instance is as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 处理后的实例如下：
- en: '![Figure 6.2: Result of pad_sequences'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2：pad_sequences的结果'
- en: '](img/B15385_06_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_02.jpg)'
- en: 'Figure 6.2: Result of pad_sequences'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：pad_sequences的结果
- en: We can see that there are plenty of `0`s at the beginning of the result. As
    you may have inferred, this is the padding that's done by the `pad_sequence` utility
    because the input sequence was shorter than `200`. Padding at the beginning of
    the sequence is the default behavior of the utility. For a sequence that is less
    than the specified limit, the truncation, by default, is done from the left –
    that is, the last `200` terms would be retained. All instances in the output now
    have `200` terms. The dataset is now ready for modeling.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到结果的开头有很多`0`。正如你可能已经推测的，这些是`pad_sequences`工具进行的填充，因为输入序列短于`200`。默认情况下，填充会加在序列的开头。对于小于指定限制的序列，默认情况下会从左侧进行截断——也就是说，最后的`200`个术语将被保留。输出中的所有实例现在都有`200`个术语。数据集现在已经准备好进行建模。
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The default behavior of the utility is to pad the beginning of the sequence
    and truncate from the left. These can be important hyperparameters. If you believe
    that the first few terms are the most important for the prediction, you may want
    to truncate the last terms by specifying the "`truncating`" parameter as "`post`".
    Similarly, to have padding toward the end of the sequence, you can set "`padding`"
    to "`post`".
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具的默认行为是将填充添加到序列的开头，并从左侧截断。这些可能是重要的超参数。如果你认为序列的前几个术语对预测最为重要，可以通过将"`truncating`"参数设置为"`post`"来截断后面的术语。同样，若希望在序列的末尾进行填充，可以将"`padding`"设置为"`post`"。
- en: The Embedding Layer
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入层
- en: In *Chapter 4, Deep Learning for Text – Embeddings*, we discussed that we can't
    feed text directly into a neural network, and therefore need good representations.
    We discussed that embeddings (low-dimensional, dense vectors) are a great way
    of representing text. To pass the embeddings into the neural network's layers,
    we need to employ the embedding layer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章，深度学习文本——嵌入*中，我们讨论过不能将文本直接输入神经网络，因此需要良好的表示。我们讨论过嵌入（低维度、密集的向量）是表示文本的好方法。为了将嵌入传递到神经网络的层中，我们需要使用嵌入层。
- en: 'The functionality of the embedding layer is two-fold:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的功能是双重的：
- en: For any input term, perform a lookup and return its word embedding/vector
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于任何输入词项，进行查找并返回其词嵌入/向量
- en: During training, learn these word embeddings
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，学习这些词嵌入
- en: The part about looking up is straightforward – the word embeddings are stored
    as a matrix of the `V × D` dimensionality, where `V` is the vocabulary size (the
    number of unique terms considered) and `D` is the length/dimensionality of each
    vector. The following figure illustrates the embedding layer. The input term,
    "`life`", is passed to the embedding layer, which performs a lookup and returns
    the corresponding vector of length `D`. This vector, which is the representation
    for the term `life`, is fed to the hidden layer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 查找部分很简单——词嵌入以`V × D`的矩阵形式存储，其中`V`是词汇表的大小（考虑的唯一词项的数量），`D`是每个向量的长度/维度。下图展示了嵌入层。输入词项“`life`”被传递到嵌入层，嵌入层进行查找并返回相应长度为`D`的向量。这个表示“life”的向量被传递到隐藏层。
- en: 'What do we mean by learning these embeddings while training the predictive
    model? Aren''t word embeddings learned by using an algorithm such as `word2vec`,
    which tries to predict the center word based on context terms (remember the CBOW
    architecture we discussed in *Chapter 4, Deep Learning for Text – Embeddings*)?
    Well, yes and no:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，训练预测模型时，学习这些嵌入是什么意思呢？难道词嵌入不是通过使用像`word2vec`这样的算法学习的吗？它尝试基于上下文词汇预测中心词（记得我们在*第4章，深度学习文本——嵌入*中讨论的CBOW架构）？嗯，既是又不是：
- en: '![Figure 6.3: Embedding layer'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3：嵌入层'
- en: '](img/B15385_06_03.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_03.jpg)'
- en: 'Figure 6.3: Embedding layer'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：嵌入层
- en: The `word2vec` approach had the objective of learning a representation that
    captures the meaning of the term. Therefore, predicting the target word based
    on context was a perfect formulation for the objective. In our case, the objective
    is different – we wish to learn representations that help us best predict the
    sentiment in the text. It makes sense, then, to learn the representation that
    works explicitly toward our objective.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`word2vec`方法的目标是学习一个能够捕捉词义的表示。因此，基于上下文预测目标词是一个完美的目标设定。在我们的例子中，目标不同——我们希望学习有助于我们最好地预测文本情感的表示。因此，学习一个明确朝着我们目标工作的表示是有意义的。'
- en: The embedding layer is always the first layer in the model. You can follow it
    up with any architecture of your choice (RNNs, in our case). We randomly initialize
    the vectors, essentially the weights in the embedding layer. While the model trains,
    the weights are updated in a way that predicts the outcome in a better way. The
    weights learned, and therefore the word vectors, are then tuned to the task. This
    is a very useful step – why use generic representations when you can tune them
    to your task?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层始终是模型中的第一层。你可以根据需要选择任意架构（在我们的例子中是RNN）。我们会随机初始化嵌入层中的向量，实际上就是嵌入层中的权重。在训练过程中，权重会根据预测结果进行更新，以便更好地预测结果。学习到的权重，因此也学习到的词向量，随后会根据任务进行调整。这是一个非常有用的步骤——为什么要使用通用表示，而不是将其调整到你的任务上呢？
- en: 'The embedding layer in Keras has two main parameters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的嵌入层有两个主要参数：
- en: '`input_dim` : The number of unique terms in the vocabulary, that is, the vocabulary
    size'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_dim` ：词汇表中唯一词项的数量，即词汇表的大小'
- en: '`output_dim` : The dimension of the embedding/the length of the word vector'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_dim` : 嵌入的维度/词向量的长度'
- en: The `input_dim` parameter needs to be set to the vocabulary size being employed.
    The `output_dim` parameter specifies the length of the embedding vector for each
    term.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_dim` 参数需要设置为所使用词汇表的大小。`output_dim` 参数指定每个词项的嵌入向量的长度。'
- en: Note that the embedding layer in Keras also allows you to use your own specified
    weight matrix in the embedding layer. This means you can use pre-trained embeddings
    (such as `GloVe`, or even embeddings you trained in a different model) in the
    embedding layer. The `GloVe` model has been trained on billions of tokens and
    it could be useful to leverage this powerful general representation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Keras 中的嵌入层也允许你使用自定义的权重矩阵。这意味着你可以在嵌入层中使用预训练的词嵌入（例如 `GloVe`，或你在其他模型中训练的词嵌入）。`GloVe`
    模型已经在数十亿个词汇上进行了训练，因此利用这一强大的通用表示可能会非常有用。
- en: Note
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you use pre-trained embeddings, you also have the option to make them trainable
    in your model – essentially, use `GloVe` embeddings as a starting point and fine-tune
    them for your task. This is a great example of transfer learning for text.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用预训练的词嵌入，你还可以选择使其在模型中可训练——本质上，使用 `GloVe` 词嵌入作为起点，并对其进行微调，以适应你的任务。这是文本迁移学习的一个很好的例子。
- en: Building the Plain RNN Model
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建普通 RNN 模型
- en: 'In the next exercise, we will build our first model for the sentiment classification
    task using plain RNNs. The model architecture we''ll use is depicted in the following
    figure, which demonstrates how the model would process an input sentence "`Life
    is good`", with the term "`Life`" coming in at time step `T=0` and "`good`" appearing
    at time step `T=2`. The model will process the inputs one by one, using the embedding
    layer to look up the word embeddings that will be passed to the hidden layers.
    The classification will be done when the final term, "`good`", is processed at
    time step `T=2`. We''ll use Keras to build and train our models:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将使用普通的 RNN 构建我们的第一个情感分类模型。我们将使用的模型架构如下图所示，图中展示了模型如何处理输入句子 "`Life is
    good`"，其中单词 "`Life`" 出现在时间步 `T=0`，而 "`good`" 则出现在时间步 `T=2`。模型将逐个处理输入，使用嵌入层查找词嵌入，并将其传递到隐层。当最后一个单词
    "`good`" 在时间步 `T=2` 被处理时，分类操作将完成。我们将使用 Keras 来构建并训练模型：
- en: '![Figure 6.4: Architecture using an embedding layer and RNN'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4：使用嵌入层和 RNN 的架构'
- en: '](img/B15385_06_04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_04.jpg)'
- en: 'Figure 6.4: Architecture using an embedding layer and RNN'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：使用嵌入层和 RNN 的架构
- en: 'Exercise 6.01: Building and Training an RNN Model for Sentiment Classification'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.01：构建并训练一个用于情感分类的 RNN 模型
- en: 'In this exercise, we will build and train an RNN model for sentiment classification.
    Initially, we will define the architecture for the recurrent and prediction layers,
    and we will assess the model''s performance on the test data. We will add the
    embedding layer and some dropout and complete the model definition by adding the
    RNN layer, dropout, and a dense layer to finish. Then, we''ll check the accuracy
    of the predictions on the test data to assess how well the model generalizes.
    Follow these steps to complete this exercise:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建并训练一个用于情感分类的 RNN 模型。最初，我们将定义递归层和预测层的架构，并评估模型在测试数据上的表现。接着，我们将添加嵌入层和一些
    dropout，并通过添加 RNN 层、dropout 和一个全连接层来完成模型定义。然后，我们将检查模型在测试数据上的预测准确度，以评估模型的泛化能力。请按照以下步骤完成这个练习：
- en: 'Let''s begin by setting the seed for `numpy` and `tensorflow` random number
    generation, to get, to the best extent possible, reproducible results. We''ll
    import `numpy` and `tensorflow` and set the seed using the following commands:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先设置 `numpy` 和 `tensorflow` 的随机数生成种子，以尽可能地获得可复现的结果。我们将导入 `numpy` 和 `tensorflow`，并使用以下命令设置种子：
- en: '[PRE7]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Even though we have set the seeds for `numpy` and `tensorflow` to achieve reproducible
    results, there are a lot more causes for variation, owing to which you may get
    a result that's different from ours. This applies to all the models we'll use
    from now on. While the values you see may be different, the output you see should
    largely agree with ours. If the model's performance is very different, you may
    want to tweak the number of epochs – the reason for this being that the weights
    in neural networks are initialized randomly, so the starting points for you and
    us could be slightly different, and we may reach a similar position when training
    a different number of epochs.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管我们已经为 `numpy` 和 `tensorflow` 设置了种子，以实现可复现的结果，但仍有许多因素会导致结果的变化，因此你可能得到与我们不同的结果。这适用于我们今后使用的所有模型。虽然你看到的数值可能不同，但你看到的输出应该在很大程度上与我们的输出一致。如果模型的性能差异较大，你可能需要调整训练的轮次——这是因为神经网络中的权重是随机初始化的，所以你和我们的初始点可能会有所不同，而通过训练不同轮次，可能会达到类似的结果。
- en: 'Now, let''s continue by importing all the necessary packages and layers and
    initializing a sequential model named `model_rnn` using the following commands:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续通过导入所有必要的包和层，并使用以下命令初始化一个名为`model_rnn`的顺序模型：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we need to specify the embedding layer. The `input_dim` parameter needs
    to be set to the `vocab_size` variable. For the `output_dim` parameter, we''ll
    choose `32`. Recall from *Chapter 4, Deep Learning for Text – Embeddings*, that
    this is a hyperparameter and you may want to experiment with this to get better
    results. Let''s specify the embedding layer and use dropout (to minimize overfitting)
    using the following commands:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要指定嵌入层。`input_dim`参数需要设置为`vocab_size`变量。对于`output_dim`参数，我们选择`32`。回顾*第4章，文本的深度学习——嵌入*，这是一个超参数，你可能需要对其进行实验以获得更好的结果。让我们通过以下命令指定嵌入层并使用丢弃层（以减少过拟合）：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the dropout employed here is `SpatialDropout1D` – this version performs
    the same function as regular dropout layer, but instead of dropping individual
    elements, it drops entire one-dimensional feature maps (vectors, in our case).
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，这里使用的丢弃层是`SpatialDropout1D`——该版本执行与常规丢弃层相同的功能，但它不是丢弃单个元素，而是丢弃整个一维特征图（在我们的案例中是向量）。
- en: 'Add a `SimpleRNN` layer with `32` neurons to the model (chosen arbitrarily;
    another hyperparameter to tune):'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向模型中添加一个包含`32`个神经元的`SimpleRNN`层（该选择是随意的；另一个可调节的超参数）：
- en: '[PRE10]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, add a dropout layer with `40%` dropout (again, an arbitrary choice):'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，添加一个`40%`丢弃率的丢弃层（再次，这是一个随意的选择）：
- en: '[PRE11]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Add a dense layer with a `sigmoid` activation function to complete the model
    architecture. This is the output layer that makes the prediction:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个带有`sigmoid`激活函数的全连接层，以完成模型架构。这是输出层，用于进行预测：
- en: '[PRE12]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Compile the model and view the model summary:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译模型并查看模型总结：
- en: '[PRE13]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model summary is as follows:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型总结如下：
- en: '![Figure 6.5: Summary of the plain RNN model'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.5：简单RNN模型的总结'
- en: '](img/B15385_06_05.jpg)'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_05.jpg)'
- en: 'Figure 6.5: Summary of the plain RNN model'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.5：简单RNN模型的总结
- en: We can see that there are `258,113` parameters, most of which are present in
    the embedding layer. The reason for this is that the word embeddings are being
    learned during the training – so we're learning the embedding matrix, which is
    of dimensionality `vocab_size(8000) × output_dim(32)`.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到共有`258,113`个参数，其中大部分位于嵌入层。这是因为在训练过程中，词嵌入被学习——因此我们在学习嵌入矩阵，其维度为`vocab_size(8000)
    × output_dim(32)`。
- en: Let's proceed and train the model (with the hyperparameters that we've observed
    to provide the best result with this data and architecture).
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们继续训练模型（使用我们观察到的在此数据和架构中提供最佳结果的超参数）。
- en: 'Fit the model on the train data with a batch size of `128` for `10` epochs
    (both of these are hyperparameters that you can tune). Use a validation split
    of `0.2` – monitoring this will give us a sense of the model performance on unseen
    data:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用批量大小为`128`的训练数据对模型进行训练，训练`10`个epoch（这两个都是你可以调节的超参数）。使用`0.2`的验证集分割比例——监控这个过程将帮助我们了解模型在未见数据上的表现：
- en: '[PRE14]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The training output for the last five epochs will be as follows. Depending
    on your system configuration, this step could take more or less time than it did
    here for us:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后五个epoch的训练输出如下。根据你的系统配置，这一步可能需要的时间会长于或短于我们所用的时间：
- en: '![Figure 6.6: Training the plain RNN model – the final five epochs'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.6：训练简单RNN模型——最后五个epoch'
- en: '](img/B15385_06_06.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_06.jpg)'
- en: 'Figure 6.6: Training the plain RNN model – the final five epochs'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.6：训练简单RNN模型——最后五个epoch
- en: From the training output, we can see that the validation accuracy goes up to
    about 86%. Let's make predictions on the test set and check the performance of
    the model.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从训练输出中可以看到，验证准确率达到了大约86%。接下来，我们对测试集进行预测并检查模型的性能。
- en: 'Make predictions on the test data using the `predict_classes` method of the
    model and use the `accuracy_score` method from `sklearn`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的`predict_classes`方法对测试数据进行预测，并使用`sklearn`的`accuracy_score`方法：
- en: '[PRE15]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The accuracy of the test is as follows:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试的准确率如下：
- en: '[PRE16]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that the model does a decent job. We used a simple architecture with
    `32` neurons and used a vocabulary size of just `8000`. Tweaking these and other
    hyperparameters may get you better results and you are encouraged to do so.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，模型表现得相当不错。我们使用了一个简单的架构，`32`个神经元，词汇表大小仅为`8000`。调整这些和其他超参数可能会得到更好的结果，我们鼓励你这样做。
- en: Note
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)上在线运行此示例。您必须执行整个Notebook才能获得预期的结果。
- en: In this exercise, we have seen how to build an RNN-based model for text. We
    saw how an embedding layer can be used to derive word vectors for the task at
    hand. These word vectors are the representations for each incoming term, which
    are passed to the RNN layer. We have seen that even a simple architecture can
    give us good results. Now, let's discuss how this model can be used to make predictions
    on new, unseen reviews.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们已经了解了如何构建基于RNN的文本模型。我们看到了如何使用嵌入层为当前任务推导单词向量。这些单词向量是每个传入术语的表示，它们会被传递到RNN层。我们已经看到，甚至一个简单的架构也能给我们带来良好的结果。现在，让我们讨论一下如何使用该模型对新的、未见过的评论进行预测。
- en: Making Predictions on Unseen Data
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对未见数据进行预测
- en: Now that you've trained your model on some data and assessed its performance
    on the test data, the next thing is to learn how to use this model to predict
    the sentiment for new data. That is the purpose of the model, after all – being
    able to predict the sentiment for data previously unseen by the model. Essentially,
    for any new review in the form of raw text, we should be able to classify its
    sentiment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经在一些数据上训练了模型并评估了它在测试数据上的表现，接下来要学习的是如何使用此模型来预测新数据的情感。毕竟，这就是模型的目的——能够预测模型从未见过的数据的情感。实际上，对于任何以原始文本形式存在的新评论，我们都应该能够对其情感进行分类。
- en: The key step for this would be to create a process/pipeline that converts the
    raw text into a format the predictive model understands. This would mean that
    the new text would need to undergo exactly the same preprocessing steps that were
    performed on the text data that was used to train the model. The function for
    preprocessing needs to return formatted text for any input raw text. The complexity
    of this function depends on the steps performed on the train data. If tokenization
    was the only preprocessing step performed, then the function only needs to perform
    tokenization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 关键步骤是创建一个流程/管道，将原始文本转换为预测模型能够理解的格式。这意味着新文本需要经过与用于训练模型的文本数据相同的预处理步骤。预处理函数需要为任何输入的原始文本返回格式化后的文本。该函数的复杂性取决于对训练数据执行的步骤。如果分词是唯一的预处理步骤，那么该函数只需要执行分词操作。
- en: 'Our model (`model_rnn`) was trained on IMDb reviews that were tokenized, had
    their case lowered, had punctuation removed, had a defined vocabulary size, and
    were converted into a sequence of indices. Our function/pipeline for preparing
    data for the RNN model needs to perform the same steps. Let''s work toward creating
    our own function. To begin, let''s create a new variable called "`inp_review`"
    containing the text "*An excellent movie*" using the following code. This is the
    variable containing the raw review text:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型（`model_rnn`）是在IMDB评论数据集上训练的，这些评论数据已经经过分词、大小写转换、去除标点、定义词汇表大小并转换为索引序列。我们为RNN模型准备数据的函数/管道需要执行相同的步骤。让我们着手创建我们自己的函数。首先，让我们创建一个新变量，名为"`inp_review`"，它包含文本"*An
    excellent movie*"，使用以下代码。这是包含原始评论文本的变量：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The sentiment in the text is positive. If the model is working well enough,
    it should predict the sentiment as positive.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中的情感是积极的。如果模型工作得足够好，它应该将情感预测为积极的。
- en: 'First, we must tokenize this text into its constituent terms, normalize its
    case, and remove punctuation. To do so, we need to import the `text_to_word_sequence`
    utility from Keras using the following code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须将文本分词为其组成术语，规范化大小写并去除标点符号。为此，我们需要使用以下代码从Keras导入`text_to_word_sequence`工具：
- en: '[PRE18]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To check if it works as we expect, we can apply this to the `inp_review` variable,
    as shown in the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查它是否按预期工作，我们可以将其应用于`inp_review`变量，如下所示的代码：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The tokenized sentence will be as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 分词后的句子如下所示：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see that it works just as expected – the case has been normalized, the
    sentences have been tokenized, and punctuation has been removed from the input
    text. The next step would be to use a defined vocabulary for the data. This would
    require using the same vocabulary that was used by TensorFlow when we loaded the
    data. The vocabulary and the term-to-index mapping can be loaded using the `get_word_index`
    method from the `imdb` module (that we employed to load the code). The following
    code can be used to load the vocabulary into a dictionary named `word_map`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，它的效果正如预期——大小写已经被标准化，句子已经被分词，标点符号也已从输入文本中移除。下一步将是为数据使用一个定义好的词汇表。这将需要使用我们在加载数据时TensorFlow使用的相同词汇表。词汇表和术语到索引的映射可以通过`imdb`模块中的`get_word_index`方法加载（我们使用该方法加载代码）。以下代码可以用来将词汇表加载到名为`word_map`的字典中：
- en: '[PRE21]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This dictionary contains the mapping for about 88.6 K terms that were available
    in the raw reviews data. We loaded the data with a vocabulary size of `8000`,
    thereby using the first `8000` indices from the mapping. Let''s create our mapping
    with limited vocabulary so that we can use the same terms/indices that the training
    data used. We''ll limit the mapping to `8000` terms by sorting the `word_map`
    variable on the index and picking the first `8000` terms, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字典包含了大约88.6K个术语的映射，这些术语在原始评论数据中是可用的。我们加载了一个词汇表大小为`8000`的数据，从而使用了映射中的前`8000`个索引。让我们创建一个有限词汇表的映射，这样我们就可以使用与训练数据相同的术语/索引。我们将通过根据索引对`word_map`变量进行排序，并选择前`8000`个术语来限制映射，如下所示：
- en: '[PRE22]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The vocab map will be a dictionary containing the term for index mapping for
    the `8000` terms in the vocabulary. Using this mapping, we''ll convert the tokenized
    sentence into a sequence of term indices by performing a lookup for each term
    and returning the corresponding index. Using the following code, we''ll define
    a function that accepts raw text, applies the `text_to_word_sequence` utility
    to it, performs a lookup from `vocab_map`, and returns the corresponding sequence
    of integers:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇映射将是一个包含`8000`个术语词汇表索引映射的字典。使用这个映射，我们将通过查找每个术语并返回相应的索引，将分词后的句子转换为术语索引序列。通过以下代码，我们将定义一个接受原始文本、应用`text_to_word_sequence`工具、从`vocab_map`进行查找，并返回相应整数序列的函数：
- en: '[PRE23]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can apply this function to the `inp_review` variable, like so:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样将此函数应用于`inp_review`变量：
- en: '[PRE24]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is the sequence of term indices corresponding to the raw text. Note that
    the data is now in the same format as the IMDb data we loaded. This sequence of
    indices can be fed to the RNN model (using the `predict_classes` method) to classify
    the sentiment, as shown in the following code. If the model is working well enough,
    it should predict the sentiment as positive:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与原始文本对应的术语索引序列。请注意，数据现在与我们加载的IMDb数据格式相同。这个索引序列可以被输入到RNN模型中（使用`predict_classes`方法）以分类情感，如下所示。如果模型的表现足够好，它应该预测情感为正面：
- en: '[PRE26]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output prediction is `1` (positive), just as we expected:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出预测为`1`（正面），正如我们预期的那样：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s apply the function to another raw text review and supply it to the model
    for prediction. Let''s update the `inp_review` variable so that it contains the
    text "`Don''t watch this movie – poor acting, poor script, bad direction.`" The
    sentiment in the review is negative. We expect the model to classify it as such:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此函数应用于另一条原始文本评论，并将其提供给模型进行预测。我们将更新`inp_review`变量，使其包含文本"`Don't watch this
    movie – poor acting, poor script, bad direction.`" 这条评论的情感是负面的。我们希望模型将其分类为负面情感：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s apply our preprocessing function to the `inp_review` variable and make
    a prediction using the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将预处理函数应用于`inp_review`变量，并使用以下代码进行预测：
- en: '[PRE29]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The prediction is `0`, as shown here:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果为`0`，如图所示：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The predicted sentiment is negative, just as we would expect the model to behave.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的情感是负面的，这正是我们期望模型的行为。
- en: We applied this pipeline in the form of a function on a single review, but you
    can very easily apply this to a whole collection of reviews to make predictions
    using the model. You are now ready to classify the sentiment of any new review
    using the RNN model we trained.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以函数的形式将此流程应用于单条评论，但你可以很容易地将其应用于一整个评论集合，以使用模型进行预测。现在，你已经准备好使用我们训练的RNN模型来分类任何新的评论的情感。
- en: Note
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The pipeline we built here is specifically for this dataset and model. This
    is not a generic processing function that you can utilize for predictions from
    any model. The vocabulary used, the cleanup that was done, the patterns the model
    learned – these were all specific to this task and dataset. For any other model,
    you need to create your pipeline accordingly.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里构建的管道是专门针对这个数据集和模型的。这不是一个通用的处理功能，不能用于任何模型的预测。使用的词汇、进行的清理、模型学习到的模式——这些都是针对这个任务和数据集的。对于任何其他模型，你需要根据情况创建自己的管道。
- en: The higher-level approach can be employed to make processing pipelines for other
    models too. Depending on the data, the preprocessing steps, and setting up where
    the model will be deployed, the pipeline can vary. All these factors also affect
    the steps you may want to include in the model building process. Therefore, we
    encourage to you to start thinking about these aspects right away when you begin
    the whole modeling process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这种高级方法同样可以用于为其他模型构建处理管道。根据数据、预处理步骤以及模型部署的设置，管道可能会有所不同。所有这些因素也会影响你可能想要在模型构建过程中包括的步骤。因此，我们鼓励你在开始整个建模过程时就立即开始考虑这些方面。
- en: We saw how to make predictions on unseen data using the trained RNN model, thereby
    giving us an understanding of the end-to-end process. In the next section, we'll
    begin working with variants of RNNs. The implementation-related ideas we've discussed
    so far are applicable to all the subsequent models.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用训练好的RNN模型对未见过的数据进行预测，从而让我们理解了端到端的过程。在接下来的部分，我们将开始研究RNN的变种。到目前为止我们讨论的实现相关的思路适用于所有后续的模型。
- en: LSTMs, GRUs, and Other Variants
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM、GRU及其他变种
- en: The idea behind plain RNNs is very powerful and the architecture has shown tremendous
    promise. Due to this, researchers have experimented with the architecture of RNNs
    to find ways to overcome the one major drawback (the vanishing gradient problem)
    and exploit the power of RNNs. This led to the development of LSTMs and GRUs,
    which have now practically replaced RNNs. Indeed, these days, when we refer to
    RNNs, we usually refer to LSTMs, GRUs, or their variants.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 普通RNN的理念非常强大，并且架构已经展现出巨大的潜力。正因如此，研究人员对RNN架构进行了实验，寻找克服一个主要缺陷（梯度消失问题）并充分发挥RNN优势的方法。这导致了LSTM和GRU的出现，它们如今实际上已经取代了RNN。事实上，现在我们提到RNN时，通常指的是LSTM、GRU或它们的变种。
- en: This is because these variants are designed specifically to handle the vanishing
    gradient problem and learn long-range dependencies. Both approaches have outperformed
    plain RNNs significantly in most tasks around sequence modeling, and the difference
    is especially higher for long sequences. The paper titled *Learning Phrase Representations
    using RNN Encoder-Decoder for Statistical Machine Translation* (available at [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078))
    performs an empirical analysis of the performance of plain RNNs, LSTMs, and GRUs.
    How have these approaches overcome the drawbacks of plain RNNS? We'll understand
    this in the next section, where we'll discuss LSTMs in detail.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为这些变种专门设计来处理梯度消失问题并学习长距离依赖。两种方法在大多数序列建模任务中显著优于普通RNN，特别是对于长序列，差异更为明显。题为 *使用RNN编码器-解码器进行统计机器翻译学习短语表示*
    的论文（可在 [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078) 获取）对普通RNN、LSTM和GRU的性能进行了实证分析。这些方法是如何克服普通RNN缺陷的呢？我们将在下一节中详细讨论LSTM时理解这一点。
- en: LSTMs
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM
- en: Let's think about this for a moment. Knowing the architecture of the plain RNN,
    how can we tweak it, or what can be done differently to capture long-range influences?
    We can't add more layers; that would be counterproductive for sure, as every added
    layer would compound the problem. One idea (available at [https://pubmed.ncbi.nlm.nih.gov/9377276](https://pubmed.ncbi.nlm.nih.gov/9377276)),
    proposed in 1997 by Sepp Hochreiter and Jurgen Schmidhuber, is to use an explicit
    value (state) that does not pass through activations. If we had a cell (corresponding
    to a neuron for plain RNNs) value flowing freely and not through activations,
    this value could potentially help us model long-range dependence. This is the
    first key difference in an LSTM – an explicit cell state.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下这个问题。了解了普通 RNN 的架构后，我们如何调整它，或者说，能做什么改变以捕捉长期依赖关系呢？我们不能增加更多的层；这样做肯定会适得其反，因为每增加一层都会加重问题。一个想法（可以参考
    [https://pubmed.ncbi.nlm.nih.gov/9377276](https://pubmed.ncbi.nlm.nih.gov/9377276)），由
    Sepp Hochreiter 和 Jurgen Schmidhuber 于 1997 年提出，是使用一个显式的值（状态），它不经过激活函数。如果我们有一个自由流动且不经过激活函数的单元（对应于普通
    RNN 的神经元）值，这个值可能会帮助我们建模长期依赖性。这就是 LSTM 的第一个关键区别——一个显式的单元状态。
- en: The cell state can be thought of as a way to identify and store information
    over multiple time steps. Essentially, we are identifying some value as the long-term
    memory of the network that helps us predict the output better and taking care
    to retain this value as long as required.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 单元状态可以被视为一种在多个时间步中识别和存储信息的方式。本质上，我们将某个值识别为网络的长期记忆，它帮助我们更好地预测输出，并且我们会小心地保持这个值，直到需要为止。
- en: But how do we regulate the flow of this cell state? How do we decide when to
    update the value and by how much? For this, Hochreiter and Schmidhuber proposed
    the use of *gating mechanisms* as a way to regulate how and when to update the
    value of the cell state. This is the other key difference in an LSTM. The freely
    flowing cell state, together with the regulatory mechanisms, allow the LSTM to
    perform extremely well on longer sequences and provide it with all its predictive
    power.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何调节单元状态的流动呢？我们如何决定何时更新其值以及更新多少呢？为此，Hochreiter 和 Schmidhuber 提出了使用 *门控机制*
    来调节何时以及如何更新单元状态的值。这是 LSTM 的另一个关键区别。自由流动的单元状态与调节机制的结合，使得 LSTM 在处理长序列时表现极为出色，赋予了它强大的预测能力。
- en: Note
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: A detailed treatment of the inner workings of the LSTM and the associated math
    is beyond the scope of this book. For those interested in reading further, [https://packt.live/3gL42Ib](https://packt.live/3gL42Ib)
    is a good reference that provides a good visual understanding of LSTMs.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LSTM 的内部工作原理和相关的数学处理，本书无法做出详细的讨论。对于那些有兴趣深入阅读的读者，[https://packt.live/3gL42Ib](https://packt.live/3gL42Ib)
    是一个很好的参考资料，可以帮助你更好地理解 LSTM。
- en: 'Let''s understand the intuition behind the working of the LSTM. The following
    figure shows the internals of the LSTM cell. Apart from the usual outputs, that
    is, the hidden state, the LSTM cell also outputs a cell "state". The hidden state
    holds the short-term memory, while the cell state holds the long-term memory:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们理解一下 LSTM 工作原理背后的直觉。下图展示了 LSTM 单元的内部结构。除了通常的输出——隐藏状态，LSTM 单元还会输出一个单元“状态”。隐藏状态保存的是短期记忆，而单元状态则保存的是长期记忆：
- en: '![Figure 6.7: The LSTM cell'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7：LSTM 单元'
- en: '](img/B15385_06_07.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_07.jpg)'
- en: 'Figure 6.7: The LSTM cell'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：LSTM 单元
- en: This view of the internals can be intimidating, which is why we'll look at a
    more abstracted view, as can be seen in *Figure 6.8*. The first thing to notice
    is that the only operations that take place on the cell state are two linear operations
    – a multiplication and an addition. The cell state does not pass through any activation
    function. This is why we said that the cell state flows freely. This free-flow
    setup is also called a "Constant Error Carousel" – a moniker you *don't* need
    to remember.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对内部结构的看法可能会让人感到畏惧，因此我们将从一个更抽象的角度来理解，正如在*图 6.8*中所示。首先需要注意的是，对单元状态进行的唯一操作是两个线性操作——乘法和加法。单元状态并没有经过任何激活函数。这就是我们之前所说的单元状态自由流动的原因。这个自由流动的设置也叫做“常数误差旋转”（Constant
    Error Carousel）——这个名称你*不*需要记住。
- en: 'The output of the `FORGET` block is multiplied by the cell state. Because the
    output of this block is between `0` and `1` (modeled by the sigmoid activation),
    a multiplication of this with the cell state will regulate how much of the previous
    cell state is to be forgotten. If the `FORGET` block outputs `0`, the previous
    cell state is completely forgotten; while for output `1`, the cell state is completely
    retained. Note that the inputs to the `FORGET` gate are the output from the hidden
    layer from the previous time step (`h`t-1) and the new input at the present time
    step, `x`t (for a layer deep in the network, this could be the output from the
    previous hidden layer):'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`FORGET` 模块的输出与单元状态相乘。由于这个模块的输出值介于 `0` 和 `1` 之间（由 Sigmoid 激活函数建模），将其与单元状态相乘将调节前一个单元状态应忘记多少。如果
    `FORGET` 模块输出 `0`，则前一个单元状态会完全被遗忘；如果输出 `1`，则单元状态完全保留。需要注意的是，`FORGET` 门的输入包括来自前一时间步的隐层输出
    (`h`t-1`) 和当前时间步的新输入 `x`t（对于网络中的深层，可能是来自前一隐层的输出）：'
- en: '![Figure 6.8: Abstracted view of the LSTM cell'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8: LSTM 单元的抽象视图'
- en: '](img/B15385_06_08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_08.jpg)'
- en: 'Figure 6.8: Abstracted view of the LSTM cell'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.8: LSTM 单元的抽象视图'
- en: In the preceding figure, we can see that after the cell state is multiplied
    by the `FORGET` block's result, the next decision is how much to update the cell
    state by. This comes from the `UPDATE` block's output, which is added (note the
    plus sign) to the processed cell state. This way, the processed cell state is
    updated. That's all the operations that are performed on the previous cell state,
    `(C`t-1`)`, to give us the new cell state, `(C`t`)`, as an output. This is how
    the long-term memory of the cell is regulated. The cell also needs to update the
    hidden state. This operation takes place in the `OUTPUT` block and is pretty much
    the same as the update in a plain RNN. The only difference is that the explicit
    cell state is multiplied by the output from the sigmoid to form the final hidden
    state, `h`t.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到，在单元状态与 `FORGET` 模块的结果相乘之后，接下来的决策是要更新单元状态的多少。这一部分来自于 `UPDATE` 模块的输出，并将其加到（注意加号）处理后的单元状态上。这样，处理后的单元状态就会被更新。这就是对前一个单元状态
    `(C`t-1`)` 执行的所有操作，最终得出新的单元状态 `(C`t`)` 作为输出。这就是如何调节单元的长期记忆。单元还需要更新隐状态。这个操作发生在
    `OUTPUT` 模块，基本上与普通 RNN 中的更新相同。唯一的区别是，显式的单元状态会与 Sigmoid 输出相乘，形成最终的隐状态 `h`t`。
- en: 'Now that we understand the individual blocks/gates, let''s see them marked
    on the following detailed figure. This should clarify how these gating mechanisms
    come together to regulate the flow of information in an LSTM:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了各个独立的模块/门控机制，接下来让我们在以下详细图中标出它们。这将帮助我们理解这些门控机制如何共同作用，调控 LSTM 中信息的流动：
- en: '![Figure 6.9: The LSTM cell explained'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9: LSTM 单元解释'
- en: '](img/B15385_06_09.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_09.jpg)'
- en: 'Figure 6.9: The LSTM cell explained'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.9: LSTM 单元解释'
- en: 'To make this example more concrete, let''s take a look at the following figure
    and understand how the cell state is updated. We can assume the previous cell
    state, `(C`t-1`)`, was `5`. How much of this value should be propagated is decided
    by the output of the `FORGET` gate. The output value of the `FORGET` gate is multiplied
    by the previous cell state, `C`t-1\. In this case, the output of the forget block
    is `0.5`, resulting in `2.5` as the processed cell state being passed. This value
    (`2.5`) then encounters the addition from the `UPDATE` gate. Since the `UPDATE`
    gate output value of `-0.8`, the result of the addition is `1.7`. This is the
    final, updated cell state, `C`t, that is passed to the next time step:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个示例更具具体性，我们来看下图，了解单元状态是如何更新的。我们假设前一个单元状态 `(C`t-1`)` 为 `5`。这个值有多少应被传递，取决于
    `FORGET` 门的输出值。`FORGET` 门的输出值会与前一个单元状态 `C`t-1` 相乘。在这种情况下，`FORGET` 模块的输出值是 `0.5`，结果是传递
    `2.5` 作为处理后的单元状态。接着，这个值 (`2.5`) 会遇到来自 `UPDATE` 门的加法。由于 `UPDATE` 门的输出值为 `-0.8`，所以加法的结果是
    `1.7`。这就是最终更新的单元状态 `C`t`，它会传递给下一个时间步：
- en: '![Figure 6.10: LSTM cell state update example'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10: LSTM 单元状态更新示例'
- en: '](img/B15385_06_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_10.jpg)'
- en: 'Figure 6.10: LSTM cell state update example'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6.10: LSTM 单元状态更新示例'
- en: Parameters in an LSTM
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 中的参数
- en: LSTMs are built on plain RNNs. If you simplified the LSTM and removed all the
    gates, retaining only the tanh function for the hidden state update, you would
    have a plain RNN. The number of activations that the information – the new input
    data at time `t` and the previous hidden state at time `t-1` (`x`t and `h`t-1)
    – passes through in an LSTM is four times the number that it passes through in
    a plain RNN. The activations are applied once in the forget gate, twice in the
    update gate, and once in the output gate. The number of weights/parameters in
    an LSTM is, therefore, four times the number of parameters in a plain RNN.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是基于普通RNN构建的。如果你简化LSTM并移除所有门，仅保留`tanh`函数用于隐藏状态的更新，你将得到一个普通RNN。在LSTM中，信息——即时间`t`的新输入数据和时间`t-1`的先前隐藏状态（`x`t和`h`t-1）——通过的激活数量是普通RNN中通过的激活数量的四倍。激活分别在遗忘门中应用一次，在更新门中应用两次，在输出门中应用一次。因此，LSTM中的权重/参数数量是普通RNN的四倍。
- en: In *Chapter 5*, *Deep Learning For Sequences,* in the section titled *Parameters
    in an RNN*, we calculated the number of parameters in a plain RNN and saw that
    we already have a quite a few parameters to work with (`n`2 `+ nk + nm`, where
    `n` is the number of neurons in the hidden layer, `m` is the number of inputs,
    and `k` is the dimension of the output layer). With LSTMs, we saw that the number
    is four times this. Needless to say, we have a lot of parameters in an LSTM, and
    that isn't necessarily a good thing, especially when working with smaller datasets.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*深度学习与序列*，在标题为*循环神经网络（RNN）中的参数*的章节中，我们计算了一个普通RNN的参数数量，并发现我们已经有了相当多的参数可以使用（`n`²
    `+ nk + nm`，其中`n`是隐藏层神经元的数量，`m`是输入的数量，`k`是输出层的维度）。使用LSTM时，我们看到这个数字是普通RNN的四倍。无需多言，LSTM中有大量的参数，这不一定是件好事，尤其是在处理较小的数据集时。
- en: 'Exercise 6.02: LSTM-Based Sentiment Classification Model'
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.02：基于LSTM的情感分类模型
- en: 'In this exercise, we will build a simple LSTM-based model to predict sentiment
    on our data. We will continue with the same setup we used previously (that is,
    the number of cells, embedding dimensions, dropout, and so on). Thus, you must
    continue this exercise in the same Jupyter Notebook. Follow these steps to complete
    this exercise:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建一个简单的基于LSTM的模型来预测我们数据的情感。我们将继续使用之前的设置（即单元数、嵌入维度、dropout等）。因此，你必须在相同的Jupyter
    Notebook中继续这个练习。按照以下步骤完成此练习：
- en: 'Import the LSTM layer from Keras `layers`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Keras `layers`导入LSTM层：
- en: '[PRE31]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add a 40% spatial dropout:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化顺序模型，添加适当维度的嵌入层，并添加40%的空间dropout：
- en: '[PRE32]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add an LSTM layer with `32` cells:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含`32`单元的LSTM层：
- en: '[PRE33]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Add the dropout (`40%` dropout) and dense layers, compile the model, and print
    the model summary:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加dropout层（`40%`的dropout）和全连接层，编译模型并打印模型概述：
- en: '[PRE34]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The model summary is as follows:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型概述如下：
- en: '![Figure 6.11: Summary of the LSTM model'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.11：LSTM模型概述'
- en: '](img/B15385_06_11.jpg)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_11.jpg)'
- en: 'Figure 6.11: Summary of the LSTM model'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11：LSTM模型概述
- en: We can see from the model summary that the number of parameters in the LSTM
    layer is `8320`. A quick check can confirm that this is exactly four times the
    number of parameters in the plain RNN layer we saw in *Exercise 6.01,* *Building
    and Training an RNN Model for Sentiment Classification*, which is in line with
    our expectations. Next, let's fit the model on the training data.
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从模型概述中我们可以看到，LSTM层中的参数数量为`8320`。快速检查可以确认，这正是我们在*练习6.01*中看到的普通RNN层参数数量的四倍，*构建与训练用于情感分类的RNN模型*，这与我们的预期一致。接下来，让我们用训练数据来拟合模型。
- en: 'Fit on the training data for `5` epochs (this gives us the best result for
    the model) with a batch size of `128`:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上拟合`5`个周期（这为模型提供了最佳结果），批量大小为`128`：
- en: '[PRE35]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output from the training process is as follows:'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程的输出如下：
- en: '![Figure 6.12: LSTM training output'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.12：LSTM训练输出'
- en: '](img/B15385_06_12.jpg)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_12.jpg)'
- en: 'Figure 6.12: LSTM training output'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12：LSTM训练输出
- en: Notice that training the LSTM took much longer than it does with plain RNNs.
    Again, considering the architecture of the LSTM and the sheer number of parameters,
    this was expected. Also, note that the validation accuracy is significantly higher
    than that of the plain RNN. Let's check the performance on the test data in terms
    of the accuracy score.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，LSTM的训练时间比普通RNN要长得多。考虑到LSTM的架构和庞大的参数数量，这是可以预期的。另外，值得注意的是，验证集上的准确率明显高于普通RNN。接下来，让我们查看测试数据上的表现，看看准确率得分。
- en: 'Make predictions on the test set and print the accuracy score:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集进行预测并打印准确率得分：
- en: '[PRE36]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The accuracy is printed out as follows:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率如下所示：
- en: '[PRE37]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The accuracy we got (87%) is a significant improvement from the accuracy we
    got using plain RNNs (85.1%). It looks like the extra parameters and the extra
    predictive power from the cell state came in handy for our task.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们得到的准确率（87%）比使用普通RNN（85.1%）得到的准确率有了显著提高。看起来，额外的参数和来自单元状态的额外预测能力对于我们的任务非常有帮助。
- en: Note
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个例子，网址是[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)。你必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we saw how we can employ LSTMs for sentiment classification
    of text. The training time was significantly higher, and the number of parameters
    is higher too. But in the end, even this simple architecture (without any hyperparameter
    tuning) gave better results than the plain RNN. You are encouraged to tune the
    hyperparameters further to get the most out of the powerful LSTM architecture.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们展示了如何使用LSTM进行文本的情感分类。训练时间明显较长，参数的数量也更多。但最终，即使是这个简单的架构（没有进行任何超参数调优）也比普通的RNN得到了更好的结果。我们鼓励你进一步调优超参数，以充分发挥强大的LSTM架构的优势。
- en: LSTM versus Plain RNNs
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM与普通RNN的对比
- en: 'We saw that LSTMs are built on top of plain RNNs, with the primary goal of
    addressing the vanishing gradient problem to enable modeling long-range dependencies.
    Looking at the following figure tells us that a plain RNN passes only the hidden
    state (the short-term memory), whereas an LSTM passes the hidden state as well
    as the explicit cell state (the long-term memory), giving it more power. So, when
    the term "`good`" is being processed in the LSTM, the recurrent layer also passes
    the cell states holding the long-term memory:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，LSTM是在普通RNN的基础上构建的，主要目的是解决梯度消失问题，以便能够建模长范围的依赖关系。从下图可以看出，普通RNN仅传递隐藏状态（短期记忆），而LSTM同时传递隐藏状态和显式的单元状态（长期记忆），这赋予了它更多的能力。因此，当LSTM处理词语“`good`”时，递归层还会传递持有长期记忆的单元状态：
- en: '![Figure 6.13: Plain RNNs (left) and LSTMs (right)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.13：普通RNN（左）和LSTM（右）'
- en: '](img/B15385_06_13.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_13.jpg)'
- en: 'Figure 6.13: Plain RNNs (left) and LSTMs (right)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：普通RNN（左）和LSTM（右）
- en: In practice, does this mean that you always need an LSTM? The answer to this
    question, as with most questions in data science and especially deep learning,
    is, "it depends". To understand these considerations, we need to understand the
    benefits and drawbacks of LSTMs compared to plain RNNs.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是否意味着你总是需要使用LSTM？这个问题的答案，正如数据科学尤其是深度学习中的大多数问题一样，是“取决于情况”。为了理解这些考虑，我们需要了解LSTM与普通RNN相比的优缺点。
- en: '**Benefits of LSTMs:**'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM的优点：**'
- en: More powerful, as it uses more parameters and an explicit cell state
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更强大，因为它使用了更多的参数和一个显式的单元状态
- en: Models long-range dependencies better
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地建模长范围的依赖关系
- en: '**Drawbacks of LSTMs:**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM的缺点：**'
- en: Many more parameters
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多的参数
- en: Takes more time to train
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间更长
- en: More prone to overfitting
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更容易发生过拟合
- en: If you have long sequences to work with, LSTM would be a good choice. If you
    have a small dataset and the sequences you are dealing with are short (<10), then
    you're probably okay to use a plain RNN, owing to there being a lower number of
    parameters (although you could also try LSTMs, making sure to use regularization
    to avoid overfitting). A larger dataset with long sequences would probably extract
    the most out of powerful models such as LSTMs. Note that training LSTMs is computationally
    expensive and time-consuming, so if you have an extremely large dataset, training
    LSTMs may not be the most practical approach. Of course, all these statements
    should serve merely as guidance – the best approach would be what works best for
    your data and your task.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要处理长序列，LSTM会是一个不错的选择。如果你有一个小数据集，而且你处理的序列很短（<10），那么使用普通的RNN可能是可以的，因为它具有较少的参数（尽管你也可以尝试LSTM，确保使用正则化来避免过拟合）。一个拥有大量数据且包含长序列的数据集，可能会从像LSTM这样的强大模型中提取最大价值。请注意，训练LSTM计算开销大且耗时，因此如果你有一个非常大的数据集，训练LSTM可能不是最实际的方法。当然，所有这些陈述仅作为指导——最好的方法是根据你的数据和任务选择最合适的方式。
- en: Gated Recurrence Units
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控循环单元
- en: In the previous section, we saw that LSTMs have a lot of parameters and seem
    much more complex than the regular RNN. You may be wondering, are all these apparent
    complications really necessary? Can the LSTM be simplified a little without it
    losing significant predictive power? Researchers wondered the same for a while,
    and in 2014, Kyunghyun Cho and their team proposed the GRU as an alternative to
    LSTMs in their paper ([https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078))
    on machine translation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到LSTM有很多参数，看起来比普通的RNN复杂得多。你可能在想，这些明显的复杂性真的有必要吗？LSTM能不能在不失去显著预测能力的情况下简化一点？研究人员曾经也有同样的疑问，并且在2014年，Kyunghyun
    Cho和他们的团队在他们的论文中提出了GRU，作为LSTM在机器翻译中的替代方案（[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)）。
- en: GRUs are simplified forms of LSTMs and aim at reducing the number of parameters
    while retaining the power of the LSTM. In tasks around speech modeling and language
    modeling, GRUs provide the same performance as LSTMs, but with fewer parameters
    and faster training times.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: GRU是LSTM的简化形式，旨在减少参数数量，同时保留LSTM的强大功能。在语音建模和语言建模任务中，GRU提供与LSTM相同的性能，但具有更少的参数和更快的训练时间。
- en: 'One major simplification done in a GRU is the omission of the explicit cell
    state. This sounds counterintuitive considering that the freely flowing cell state
    was what gave the LSTM its power, right? What really gave LSTMs all that power
    was the freely flowing nature of the cell state and not the cell state itself?
    Indeed, if the cell state were also subject to activations, LSTMs probably wouldn''t
    have had the success they did:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在GRU中做出的一个重要简化是省略了显式的单元状态。这听起来有些违反直觉，因为自由流动的单元状态正是赋予LSTM其强大能力的因素，对吧？其实，赋予LSTM强大能力的并不是单元状态本身，而是单元状态的自由流动性？确实，如果单元状态也要经过激活函数，LSTM可能不会取得如此大的成功：
- en: '![Figure 6.14: Gated Recurrent Unit'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14：门控循环单元'
- en: '](img/B15385_06_14.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_14.jpg)'
- en: 'Figure 6.14: Gated Recurrent Unit'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：门控循环单元
- en: So, the freely flowing values is the key differentiating idea. GRUs retain this
    idea, by allowing the hidden state to flow freely. Let's look at the preceding
    figure to understand what this means. GRUs allow the hidden state to pass through
    freely. Another way to look at this is that GRUs effectively bring the idea of
    the cell state (as in LSTMs) to the hidden state.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，自由流动的值是关键的区分思想。GRU通过允许隐藏状态自由流动来保留这一思想。让我们看看前面的图，以理解这意味着什么。GRU允许隐藏状态自由地通过。换句话说，GRU实际上将单元状态（如LSTM中的单元状态）的思想带到了隐藏状态。
- en: We still need to regulate the flow of the hidden state, though, so we still
    have gates. GRUs combine the forget gate and update gate into a single update
    gate. To understand the motivation behind this, consider this – if we forget a
    cell state, and don't update it, what are we really doing? Maybe there is merit
    in having a single update operation. This is the second major difference in the
    architecture.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要调节隐藏状态的流动，因此我们仍然需要门控机制。GRU将遗忘门和更新门合并成一个更新门。为了理解背后的动机，请考虑这一点——如果我们忘记了一个单元状态，并且没有更新它，我们到底在做什么？也许拥有一个单一的更新操作是有意义的。这是架构中的第二个主要区别。
- en: As a result of these two changes, GRUs have the data pass through three activations
    instead of four, as in LSTMs, reducing the number of parameters. While GRUs still
    have three times the number of parameters of a plain RNN, these have 75% of the
    parameters of LSTMs, and that is a welcome change. We still have information flowing
    freely through the network and this should allow us to model long-range dependencies.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个变化，GRU 使数据经过了三个激活层，而不是 LSTM 中的四个激活层，从而减少了参数的数量。虽然 GRU 仍然拥有普通 RNN 的三倍参数，但它只有
    LSTM 的 75% 的参数，这是一项值得欢迎的改进。我们仍然可以让信息在网络中自由流动，这应该能帮助我们建模长程依赖。
- en: Let's see how a GRU-based model performs on our task of sentiment classification.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看基于 GRU 的模型在情感分类任务上的表现如何。
- en: 'Exercise 6.03: GRU-Based Sentiment Classification Model'
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.03：基于 GRU 的情感分类模型
- en: 'In this exercise, we will build a simple GRU-based model to predict sentiments
    in our data. We will continue with the same setup that we used previously (that
    is, the number of cells, embedding dimensions, dropout, and so on). Using GRUs
    instead of LSTMs in the model is as simple as replacing "`LSTM`" with "`GRU`"
    when adding the layer. Follow these steps to complete this exercise:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建一个简单的基于 GRU 的模型来预测数据中的情感。我们将继续使用之前的相同设置（即单元数、嵌入维度、丢弃率等）。在模型中使用 GRU
    代替 LSTM 就像在添加层时将 "`LSTM`" 替换为 "`GRU`" 一样简单。按照以下步骤完成本练习：
- en: 'Import the `GRU` layer from Keras `layers`:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras `layers` 中导入 `GRU` 层：
- en: '[PRE38]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add 40% spatial dropout:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化顺序模型，添加适当维度的嵌入层，并加入 40% 的空间丢弃层：
- en: '[PRE39]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Add a GRU layer with 32 cells. Set the `reset_after` parameter to `False` (this
    is a minor TensorFlow 2 implementation detail in order to maintain consistency
    with the implementation of plain RNNs and LSTMs):'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个具有 32 个单元的 GRU 层。将 `reset_after` 参数设置为 `False`（这是 TensorFlow 2 实现的一个小细节，为了与普通
    RNN 和 LSTM 的实现保持一致）：
- en: '[PRE40]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Add the dropout (40%) and dense layers, compile the model, and print the model
    summary:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加丢弃层（40%）和全连接层，编译模型，并打印模型摘要：
- en: '[PRE41]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The model summary is as follows:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![Figure 6.15: Summary of the GRU model'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.15：GRU 模型总结](img/B15385_06_15.jpg)'
- en: '](img/B15385_06_15.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_15.jpg)'
- en: 'Figure 6.15: Summary of the GRU model'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.15：GRU 模型总结
- en: From the summary of the GRU model, we can see that the number of parameters
    in the GRU layer is `6240`. You can check that this is exactly three times the
    number of parameters in the plain RNN layer we saw in *Exercise 6.01,* *Building
    and Training an RNN Model for Sentiment Classification*, and `0.75` times the
    parameters of the LSTM layer we saw in *Exercise 6.02,* *LSTM-Based Sentiment
    Classification Model* – again, this is in line with our expectations. Next, let's
    fit the model on the training data.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 GRU 模型的摘要中，我们可以看到 GRU 层的参数数量为 `6240`。你可以检查，这正好是我们在 *练习 6.01*，《构建与训练情感分类的 RNN
    模型》中看到的普通 RNN 层参数的三倍，也是我们在 *练习 6.02*，《基于 LSTM 的情感分类模型》中看到的 LSTM 层参数的 0.75 倍——这再次符合我们的预期。接下来，让我们在训练数据上拟合模型。
- en: 'Fit on the training data for four epochs (which gives us the best result):'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上训练四个 epoch（这给我们最佳结果）：
- en: '[PRE42]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The output from the training process is as follows:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程的输出如下所示：
- en: '![Figure 6.16: GRU training output'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.16：GRU 训练输出](img/B15385_06_16.jpg)'
- en: '](img/B15385_06_16.jpg)'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_16.jpg)'
- en: 'Figure 6.16: GRU training output'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.16：GRU 训练输出
- en: Notice that training the GRUs also took much longer than plain RNNs but was
    faster than LSTMs. The validation accuracy is better than the plain RNN and seems
    close to that of the LSTM. Let's see how the model fares on the test data.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，训练 GRU 的时间比普通 RNN 长，但比 LSTM 快。验证准确度优于普通 RNN，并且接近 LSTM 的水平。让我们看看模型在测试数据上的表现如何。
- en: 'Make predictions on the test set and print the accuracy score:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测并打印准确度评分：
- en: '[PRE43]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The accuracy is printed out as follows:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确度如下所示：
- en: '[PRE44]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can see that the accuracy of the GRU model (87.15%) is very similar to that
    of the LSTM (87%) and is higher than the plain RNN. This is an important point
    – GRUs are simplifications of LSTMs that aim to provide similar accuracy with
    fewer parameters. Our exercise here shows this is true.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，GRU 模型的准确度（87.15%）与 LSTM 模型（87%）非常相似，且高于普通 RNN。这是一个重要的点——GRU 是 LSTM 的简化版本，旨在以更少的参数提供类似的准确性。我们的练习表明这一点是正确的。
- en: Note
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参阅[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)。你必须执行整个Notebook才能得到预期结果。
- en: In this exercise, we saw how we can employ GRUs for the sentiment classification
    of text. The training time was slightly lower than the LSTM model and the number
    of parameters is lower. Even this simple architecture (without any hyperparameter
    tuning) gave better results than the plain RNN model and gave results similar
    to the LSTM model.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们看到如何使用GRU进行文本情感分类。训练时间稍低于LSTM模型，而且参数数量也更少。即使是这个简单的架构（没有任何超参数调整）也给出了比普通RNN模型更好的结果，并且与LSTM模型的结果相似。
- en: LSTM versus GRU
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LSTM 与 GRU
- en: So, which one should you choose? The LSTM has more parameters and an explicit
    cell state designed to store long-term memory. The GRU has fewer parameters, which
    means faster training, and also has a free-flowing cell state to allow it to model
    long-range dependencies.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该选择哪个呢？LSTM有更多的参数，并且设计了一个显式的单元状态来存储长期记忆。GRU参数较少，这意味着训练更快，同时它也具有自由流动的单元状态，使其能够建模长距离的依赖关系。
- en: An empirical evaluation (available at [https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555))
    by Junyoung Chung, Yoshua Bengio, and their team in 2014 on music-modeling and
    speech-modeling tasks showed that both LSTMs and GRUs are markedly superior to
    plain RNNs. They also found that GRUs are on par with LSTMs in terms of performance.
    They remarked that tuning hyperparameters such as layer size is probably more
    important than choosing between LSTM and GRU.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Junyoung Chung、Yoshua Bengio及其团队对音乐建模和语音建模任务进行的实证评估（可在[https://arxiv.org/abs/1412.3555](https://arxiv.org/abs/1412.3555)获取）表明，LSTM和GRU都明显优于普通的RNN。他们还发现，在性能方面，GRU与LSTM不分上下。他们指出，调整超参数，如层大小，可能比选择LSTM或GRU更为重要。
- en: In 2018, Gail Weiss, Yoav Goldberg, and their team demonstrated and concluded
    that LSTMs outperform GRUs in tasks that require unbounded counting, that is,
    those that need to handle sequences of an arbitrary length. The Google Brain team,
    in 2018, also showed that the performance of LSTMs is superior to GRUs when it
    comes to machine translation. This leads us to think that the extra power that
    LSTMs bring may be very useful in certain applications.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Gail Weiss、Yoav Goldberg及其团队展示并得出结论：LSTM在需要无限计数的任务中优于GRU，也就是说，处理任意长度序列的任务。2018年，Google
    Brain团队也展示了LSTM在机器翻译任务中的表现优于GRU。这让我们思考，LSTM所带来的额外优势可能在某些应用中非常有用。
- en: Bidirectional RNNs
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双向RNN
- en: The RNN models we've just looked at – LSTMs, GRUs – are powerful indeed and
    provide extremely good results when it comes to sequence-processing tasks. Now,
    let's discuss how to make them even more powerful, and the methods that yield
    the amazing successes in deep learning that you have been hearing about.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看的RNN模型——LSTM、GRU——确实很强大，并且在序列处理任务中提供了极好的结果。现在，让我们讨论如何让它们更强大，以及那些带来深度学习惊人成就的方法，你一定听说过。
- en: 'Let''s begin with the idea of bidirectional RNNs. The idea applies to all variants
    of RNNs, including, but not limited to, LSTMs and GRUs. Bidirectional RNNs process
    the sequence in both directions, allowing the network to have both backward and
    forward information about the sequence, providing it with a much richer context:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从双向RNN的概念开始。这个概念适用于所有类型的RNN，包括但不限于LSTM和GRU。双向RNN可以同时处理序列的正向和反向，使网络可以获得关于序列的前向和后向信息，提供更加丰富的上下文：
- en: '![Figure 6.17: Bidirectional LSTM'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.17：双向LSTM'
- en: '](img/B15385_06_17.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_17.jpg)'
- en: 'Figure 6.17: Bidirectional LSTM'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17：双向LSTM
- en: The bidirectional model essentially employs two RNNs in parallel – one as the
    "**forward layer**" and the other as the "**backward layer**". As shown in the
    preceding figure, the forward layer processes the sequence in the order of its
    elements. For the sentence, "*Life is good*", the forward layer will process the
    term "*Life*" first, followed by "*is*", followed by "*good*" – no different from
    the usual RNN layer. The backward layer reverses this order – it processes "good"
    first, followed by "is", followed by "Life". At each step, the states of the forward
    and the backward layers are concatenated to form the output.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 双向模型本质上是并行使用两个RNN——一个作为"**前向层**"，另一个作为"**后向层**"。如前图所示，前向层按元素顺序处理序列。对于句子"*Life
    is good*"，前向层首先处理"*Life*"，然后是"*is*"，接着是"*good*"——这与常规的RNN层没有区别。后向层则反转这个顺序——它首先处理"good"，然后是"is"，最后是"Life"。在每一步中，前向层和后向层的状态会被连接起来形成输出。
- en: What kind of tasks benefit the most from this architecture? Looking at both
    sides of the context helps resolve any ambiguity about the term at hand. When
    we read a statement such as "*The stars*", we're not sure as to what "*stars*"
    we're reading about – is it the stars in the sky or movie stars? But when we also
    see the terms coming later in the sequence and read "*The stars at the movie premiere*",
    we're confident that this sentence is about movie stars. The tasks that can benefit
    the most from such a setup are machine translation, parts-of-speech tagging, named
    entity recognition, and word prediction tasks, to list a few. Bidirectional RNNs
    show performance gains for general text classification tasks as well. Let's apply
    a bidirectional LSTM-based model to our sentiment classification task.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种任务最能从这种架构中受益？查看上下文的两个方面有助于解决当前词语的歧义。当我们阅读类似"*The stars*"的句子时，我们不确定这里的"*stars*"指的是什么——是天空中的星星，还是电影明星？但当我们看到后续的词语并阅读"*The
    stars at the movie premiere*"时，我们就能确定这句话是指电影明星。那些最能从这种设置中受益的任务包括机器翻译、词性标注、命名实体识别和词预测等。双向RNN在一般文本分类任务中也表现出了性能提升。让我们将基于双向LSTM的模型应用到情感分类任务中。
- en: 'Exercise 6.04: Bidirectional LSTM-Based Sentiment Classification Model'
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习6.04：基于双向LSTM的情感分类模型
- en: 'In this exercise, we will use bidirectional LSTMs to predict sentiment on our
    data. We''ll be using the bidirectional wrapper from Keras to create bidirectional
    layers on LSTMs (you could create a bidirectional GRU model by simply replacing
    `LSTM` with `GRU` in the wrapper). Follow these steps to complete this exercise:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用双向LSTM来预测我们数据的情感。我们将使用Keras的双向包装器来创建LSTM的双向层（你也可以通过简单地将`LSTM`替换为`GRU`来创建一个双向GRU模型）。请按照以下步骤完成这个练习：
- en: 'Import the `Bidirectional` layer from Keras `layers`. This layer is essentially
    a wrapper you can use around other RNNs:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Keras `layers`中导入`Bidirectional`层。这个层本质上是一个包装器，可以用于其他RNN：
- en: '[PRE45]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add a 40% spatial dropout:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化顺序模型，添加具有适当维度的嵌入层，并添加40%的空间丢弃：
- en: '[PRE46]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Add a `Bidirectional` wrapper to an LSTM layer with `32` cells:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Bidirectional`包装器添加到一个具有`32`单元的LSTM层：
- en: '[PRE47]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Add the dropout (40%) and dense layers, compile the model, and print the model
    summary:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加丢弃层（40%）和密集层，编译模型，并打印模型摘要：
- en: '[PRE48]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The summary is as follows:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结如下：
- en: '![Figure 6.18: Summary of the bidirectional LSTM model'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.18：双向LSTM模型总结'
- en: '](img/B15385_06_18.jpg)'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_18.jpg)'
- en: 'Figure 6.18: Summary of the bidirectional LSTM model'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.18：双向LSTM模型总结
- en: Note the parameters of the model shown in the preceding screenshot. Not surprisingly,
    the bidirectional LSTM layer has `16640` parameters – twice the number of parameters
    that the LSTM layer (`8320` parameters) had in *Exercise 6.02, LSTM-Based Sentiment
    Classification Model*. This is eight times the parameters of the plain RNN. Next,
    let's fit the model on the training data.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意前面截图中显示的模型参数。不出所料，双向LSTM层有`16640`个参数——是LSTM层（`8320`个参数）数量的两倍，正如*练习6.02，基于LSTM的情感分类模型*所展示的。这是普通RNN参数数量的八倍。接下来，让我们在训练数据上拟合该模型。
- en: 'Fit the training data for four epochs with a batch size of `128`:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`128`的批大小训练数据四个周期：
- en: '[PRE49]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output from training is as follows:'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练的输出如下：
- en: '![Figure 6.19: Bidirectional LSTM training output'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.19：双向LSTM训练输出'
- en: '](img/B15385_06_19.jpg)'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_19.jpg)'
- en: 'Figure 6.19: Bidirectional LSTM training output'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.19：双向LSTM训练输出
- en: Notice that, as we expect, training bidirectional LSTMs takes much longer than
    regular LSTMs, and several times longer than plain RNNs. The validation accuracy
    seems to be closer to the LSTM's accuracy.
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，正如我们预期的那样，训练双向 LSTM 比常规 LSTM 花费的时间要长得多，比普通 RNN 的时间还要长好几倍。验证准确度似乎更接近 LSTM
    的准确度。
- en: 'Make predictions on the test set and print the accuracy score:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测并打印准确度得分：
- en: '[PRE50]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The accuracy is as follows:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确度如下：
- en: '[PRE51]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The accuracy we received here (87.7%) is a slight improvement over the LSTM
    model's accuracy, which was 87%. Again, you can tune the hyperparameters even
    further to extract the most out of this powerful architecture. Note that we had
    twice the number of parameters compared to the LSTM model, and eight times the
    parameters of the plain RNN. Working with a large dataset may make the performance
    differences bigger.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里得到的准确度（87.7%）比 LSTM 模型的准确度略有提高，LSTM 的准确度为 87%。同样，你可以进一步调整超参数，以最大限度地发挥这种强大架构的优势。请注意，我们的参数数量是
    LSTM 模型的两倍，是普通 RNN 的八倍。使用大型数据集可能会使性能差异更加明显。
- en: Note
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)。你必须执行整个笔记本才能获得期望的结果。
- en: Stacked RNNs
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠 RNN
- en: Now, let's look at another approach we can follow to extract more power from
    RNNs. In all the models we've looked at in this chapter, we've used a single layer
    for the RNN layer (plain RNN, LSTM, or GRU). Going deeper, that is, adding more
    layers, has typically helped us for feedforward networks so that we can learn
    more complex patterns/features in the deeper layers. There is merit in trying
    this idea for recurrent networks. Indeed, stacked RNNs do seem to give us more
    predictive power.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们可以采取的另一种方法，以从 RNN 中提取更多的能力。在本章中我们查看的所有模型中，我们都使用了单层 RNN 层（普通 RNN、LSTM
    或 GRU）。深入学习，即添加更多层，通常有助于前馈网络，这样我们可以在更深的层中学习更复杂的模式/特征。尝试这种思路在递归网络中也有其价值。事实上，堆叠
    RNN 确实似乎为我们提供了更多的预测能力。
- en: 'The following figure illustrates a simple two-layer stacked LSTM model. Stacking
    RNNs simply means feeding the output of one RNN layer to another RNN layer. The
    RNN layers can output sequences (that is, output at each time step) and these
    can be fed, like any input sequence, into the subsequent RNN layer. In terms of
    implementation through code, stacking RNNs is as simple as returning sequences
    from one layer, and providing this as input to the next RNN layer, that is, the
    immediate next layer:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示例了一个简单的两层堆叠 LSTM 模型。堆叠 RNN 仅仅意味着将一个 RNN 层的输出馈送到另一个 RNN 层。RNN 层可以输出序列（即在每个时间步的输出），这些输出可以像任何输入序列一样传递给后续的
    RNN 层。在代码实现方面，堆叠 RNN 就是简单地从一层返回序列，并将其作为输入提供给下一个 RNN 层，即紧接着的下一层：
- en: '![Figure 6.20: Two-layer stacked RNN'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.20：两层堆叠 RNN'
- en: '](img/B15385_06_20.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_20.jpg)'
- en: 'Figure 6.20: Two-layer stacked RNN'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20：两层堆叠 RNN
- en: Let's see the stacked RNN (LSTM) in action by using it on our sentiment classification
    task.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过在情感分类任务中使用堆叠 RNN（LSTM）来看看它的实际效果。
- en: 'Exercise 6.05: Stacked LSTM-Based Sentiment Classification Model'
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 6.05：基于堆叠 LSTM 的情感分类模型
- en: 'In this exercise, we will "go deeper" into the RNN architecture by stacking
    two LSTM layers to predict sentiment in our data. We will continue with the same
    setup that we used in the previous exercises (the number of cells, embedding dimensions,
    dropout, and so on) for the other layers. Follow these steps to complete this
    exercise:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将通过堆叠两个 LSTM 层，进一步深入 RNN 架构，以预测我们数据的情感。我们将继续使用之前练习中的相同设置（单元数量、嵌入维度、丢弃率等）用于其他层。请按照以下步骤完成本练习：
- en: 'Instantiate the sequential model, add the embedding layer with the appropriate
    dimensions, and add 40% spatial dropout:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化顺序模型，添加具有适当维度的嵌入层，并添加 40% 的空间丢弃：
- en: '[PRE52]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Add an LSTM layer with `32` cells. Make sure to specify `return_sequences`
    as `True` in the LSTM layer. This will return the output of the LSTM at each time
    step, which can then be passed to the next LSTM layer:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个具有 `32` 个单元的 LSTM 层。确保在 LSTM 层中将 `return_sequences` 设置为 `True`。这样将返回 LSTM
    在每个时间步的输出，之后可以将其传递到下一个 LSTM 层：
- en: '[PRE53]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Add another LSTM layer with `32` cells. This time, you don''t need to return
    sequences. You can either specify the `return_sequences` option as `False` or
    skip it altogether (the default value is `False`):'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加一个具有`32`个单元的LSTM层。这次，你不需要返回序列。你可以将`return_sequences`选项设置为`False`，或者完全跳过该选项（默认值是`False`）：
- en: '[PRE54]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Add the dropout (50% dropout; this is higher since we''re building a more complex
    model) and dense layers, compile the model, and print the model summary:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加dropout（50%的dropout；这是因为我们正在构建一个更复杂的模型）和dense层，编译模型并打印模型摘要：
- en: '[PRE55]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The summary is as follows:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 汇总如下：
- en: '![Figure 6.21: Summary of the stacked LSTM model'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.21：堆叠LSTM模型概述'
- en: '](img/B15385_06_21.jpg)'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_21.jpg)'
- en: 'Figure 6.21: Summary of the stacked LSTM model'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.21：堆叠LSTM模型概述
- en: Note that the stacked LSTM model has the same number of parameters as the bidirectional
    model. Let's fit the model on the training data.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，堆叠LSTM模型的参数数量与双向模型相同。让我们在训练数据上拟合模型。
- en: 'Fit the model on the training data for four epochs:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上进行四个epoch的拟合：
- en: '[PRE56]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output from training is as follows:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练的输出如下：
- en: '![Figure 6.22: Stacked LSTM training output'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.22：堆叠LSTM训练输出'
- en: '](img/B15385_06_22.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_06_22.jpg)'
- en: 'Figure 6.22: Stacked LSTM training output'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.22：堆叠LSTM训练输出
- en: Training stacked LSTMs took less time than training bidirectional LSTMs. The
    validation accuracy seems to be close to that of the bidirectional LSTM model.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练堆叠LSTM花费的时间比训练双向LSTM少。验证准确率似乎接近双向LSTM模型的准确率。
- en: 'Make predictions on the test set and print the accuracy score:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测并打印准确率：
- en: '[PRE57]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The accuracy is printed out as follows:'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率如下所示：
- en: '[PRE58]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: The accuracy of `87.6%` is an improvement over the LSTM model (`87%`) and is
    practically the same as that of the bidirectional model (`87.7%`). This is a somewhat
    significant improvement over the performance of the regular LSTM model, considering
    that we're working with a rather small dataset. The larger your dataset is, the
    more you can benefit from these sophisticated architectures. Try tuning the hyperparameters
    in order to get the most out of this powerful architecture.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '`87.6%`的准确率比LSTM模型（`87%`）有所提高，且几乎与双向模型（`87.7%`）相同。考虑到我们使用的是相对较小的数据集，这对于常规LSTM模型的性能来说是一个相当显著的提升。数据集越大，越能从这些复杂的架构中获益。尝试调整超参数，以便最大限度地发挥这种强大架构的优势。'
- en: Note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31ZPO2g](https://packt.live/31ZPO2g).
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/31ZPO2g](https://packt.live/31ZPO2g)。
- en: You can also run this example online at [https://packt.live/2Oa2trm](https://packt.live/2Oa2trm).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2Oa2trm](https://packt.live/2Oa2trm)上在线运行此示例。你必须执行整个Notebook才能获得预期的结果。
- en: Summarizing All the Models
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汇总所有模型
- en: 'In this chapter, we''ve looked at different variants of RNNs – from plain RNNs
    to LSTMs to GRUs. We also looked at the bidirectional approach and the stacking
    approach to using RNNs. Now is a good time to take a holistic look at things and
    make a comparison between the models. Let''s look at the following table, which
    compares the five models in terms of parameters, training time, and performance
    (that is, the level of accuracy on our dataset):'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了不同变种的RNN——从普通RNN到LSTM再到GRU。我们还看了双向方法和堆叠方法在使用RNN时的应用。现在是时候全面地回顾一下，并对这些模型进行比较了。让我们看下表，它比较了五种模型在参数、训练时间和性能（即在我们的数据集上的准确性水平）方面的差异：
- en: '![Figure 6.23: Comparing the five models'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.23：比较五个模型'
- en: '](img/B15385_06_23.jpg)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_23.jpg)'
- en: 'Figure 6.23: Comparing the five models'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23：比较五个模型
- en: Note
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As mentioned earlier in the chapter, while working through the practical elements,
    you may have obtained values different from the ones shown above; however, the
    test accuracies you obtain should largely agree with ours. If the model's performance
    is very different, you may want to tweak the number of epochs.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所提到的，在实际操作过程中，你可能会得到与上面显示的不同的数值；然而，你获得的测试准确率应该与我们的结果大致相符。如果模型的表现差异较大，你可能需要调整epoch的数量。
- en: Plain RNNs are the lowest on parameters and have the lowest training times but
    have the lowest accuracy of all the models. This is in line with our expectations
    – we are dealing with sequences that are 200 characters in length, and we know
    not to expect much from plain RNNs, and that gated RNNs (LSTMs, GRUs) are more
    suitable. Indeed, LSTMs and GRUs do perform significantly better than plain RNNs.
    But the accuracy comes at the cost of significantly higher training times, and
    several times the parameters, making these models more prone to overfitting.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 普通的RNN在参数量和训练时间上是最低的，但其准确性也是所有模型中最低的。这与我们的预期一致——我们正在处理的是200字符长的序列，我们知道普通RNN不太可能表现好，而门控RNN（LSTM、GRU）更为适合。实际上，LSTM和GRU的表现明显优于普通RNN。但这种准确性是以显著更高的训练时间和几倍的参数量为代价的，使得这些模型更容易过拟合。
- en: The approaches of stacking and using bidirectional processing seem to provide
    an incremental benefit in terms of predictive power, but this is at the cost of
    significantly higher training times and several times the parameters. The stacked
    and bidirectional approaches gave us the highest accuracy, even on this small
    dataset.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠和双向处理的方式似乎在预测能力上提供了增量效益，但这需要以显著更长的训练时间和几倍的参数为代价。堆叠和双向的方式为我们提供了最高的准确性，即使是在这个小数据集上。
- en: While the performance results are specific to our dataset, the gradation in
    performance we see here is fairly common. The stacked and bidirectional models
    are present in many of the solutions today that provide state-of-the-art results
    in various tasks. With a larger dataset and when working with much longer sequences,
    we would expect the differences in model performances to be larger.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管性能结果特定于我们的数据集，但我们在这里看到的性能梯度是相当常见的。堆叠和双向模型在今天许多解决方案中都出现了，这些解决方案在各种任务中提供了最先进的结果。随着数据集的增大以及处理更长序列时，我们预计模型性能差异会更大。
- en: Attention Models
  id: totrans-364
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力模型
- en: Attention models were first introduced in late 2015 by Dzmitry Bahdanau, KyungHyun
    Cho, and Yoshua Bengio in their influential and seminal paper ([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473))
    that demonstrated the state-of-the-art results of English-to-French translation.
    Since then, this idea has been used for many sequence-processing tasks with great
    success, and attention models are becoming increasingly popular. While a detailed
    explanation and mathematical treatment is beyond the scope of this book, let's
    understand the intuition behind the idea that is considered by many big names
    in the field of deep learning as a significant development in our approach to
    sequence modeling.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模型最早由Dzmitry Bahdanau、KyungHyun Cho和Yoshua Bengio在2015年底的具有影响力的开创性论文中提出（[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)），该论文展示了英法翻译的最先进结果。从那时起，这一思想已被广泛应用于许多序列处理任务，并取得了巨大的成功，注意力模型也变得越来越流行。尽管本书不涉及详细的解释和数学处理，但我们可以理解这一思想的直觉，这一思想被许多深度学习领域的大佬视为我们对序列建模方法的重大进展。
- en: 'The intuition behind attention can be best understood using an example from
    the task it was developed for – translation. When a novice human translates a
    long sentence between languages, they don''t translate the entire sentence in
    one go. They break the original sentence down into smaller, manageable chunks,
    thereby generating a translation for each chunk sequentially. For each chunk,
    there would be a part that is the most important for the translation task, that
    is, where you need to pay the most attention:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力背后的直觉可以通过它最初为翻译任务开发的例子来最好地理解。当一个新手翻译长句子时，他们并不会一次性翻译整个句子，而是将原始句子拆分成更小、更易管理的部分，从而按顺序为每个部分生成翻译。对于每个部分，都有一部分是最重要的，需要特别关注的部分，即你需要集中注意力的地方：
- en: '![Figure 6.24: Idea of attention simplified'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.24：简化的注意力概念'
- en: '](img/B15385_06_24.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_24.jpg)'
- en: 'Figure 6.24: Idea of attention simplified'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24：简化的注意力概念
- en: The preceding figure shows a simple example where we're translating the sentence,
    "`Azra is moving to Berlin`", into French. The French translation is, "`Azra déménage
    à Berlin`". To get the first term in the French translation, "`Azra`", we need
    to pay attention primarily to the first term in the original sentence (underscored
    by a light gray line) and maybe a bit to the second (underscored by a dark gray
    line) – these terms get higher importance (weight). The remaining parts of the
    sentence aren't relevant. Similarly, to generate the term "`déménage`" in the
    output, we need to pay attention to the terms "`is`" and "`moving`". The importance
    of each term toward the output term is expressed as weights. This is known as
    "**alignment**".
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图展示了一个简单的例子，我们将句子“`Azra is moving to Berlin`”翻译成法语。法语翻译是“`Azra déménage à
    Berlin`”。为了得到法语翻译中的第一个词“`Azra`”，我们需要主要关注原句中的第一个词（由浅灰色线条标示），或许稍微关注第二个词（由深灰色线条标示）——这些词被赋予了更高的重要性（权重）。剩余的部分并不相关。类似地，为了生成输出中的词“`déménage`”，我们需要关注词“`is`”和“`moving`”。每个词对输出词的影响程度由权重表示。这就是所谓的**对齐**。
- en: These alignments can be seen in the following figure, which was sourced from
    the original paper ([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)).
    It beautifully demonstrates what the model identified as most important for each
    term in the output. A lighter color in a cell in the grid means a higher weight
    for the corresponding input term in the column. We can see that for the output
    term "`marin`", the model correctly identifies "`marine`" as the most important
    input term to pay attention to. Similarly, it has identified "`environment`" as
    the most important term for "`environnement`", "`known`" for "`connu`", and so
    on. Pretty neat, isn't it?
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了这些对齐关系，来源于原始论文（[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)）。它美妙地展示了模型识别出输出中每个词项最重要的部分。网格中单元格的浅色表示该列对应输入词项的权重较高。我们可以看到，对于输出词项"`marin`"，模型正确地识别出"`marine`"是最重要的输入词项。类似地，它识别出"`environment`"是"`environnement`"的最重要词项，"`known`"是"`connu`"的最重要词项，依此类推。挺酷的，不是吗？
- en: '![Figure 6.25: The alignment learned by the model'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.25：模型学习到的对齐'
- en: '](img/B15385_06_25.jpg)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_06_25.jpg)'
- en: 'Figure 6.25: The alignment learned by the model'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25：模型学习到的对齐
- en: While attention models were originally designed for translation tasks, the models
    have been employed on a variety of other tasks with good success. That being said,
    note that the attention models have a very high number of parameters. The models
    are typically employed on bidirectional LSTM layers and add additional weights
    for the importance values. A high number of parameters makes the model more prone
    to overfitting, which means they will need much larger datasets to utilize their
    power.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力模型最初是为翻译任务设计的，但这些模型已经成功地应用于许多其他任务。需要注意的是，注意力模型的参数非常多。这些模型通常应用于双向LSTM层，并为重要性值添加额外的权重。参数数量庞大使得模型更容易过拟合，这意味着它们需要更大的数据集才能发挥其强大的能力。
- en: More Variants of RNNs
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多RNN变种
- en: We've seen quite a few variations of RNNs in this chapter – covering all the
    prominent ones and the major upcoming (in terms of popularity) variations. Sequence
    modeling and its associated architectures are a hot area of research, and we see
    plenty of developments coming in every year. Many variants aim to make lighter
    models with fewer parameters that aren't as hardware hungry as current RNNs. **Clockwork
    RNNs** (**CWRNNs**) are a recent development and show great success. There are
    also **Hierarchal Attention Networks**, built on the idea of attention, but ultimately
    also propose that you shouldn't use RNNs as building blocks. There's a lot going
    on in this exciting area, so keep your eyes and ears open for the next big idea.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到了许多RNN的变种——涵盖了所有主要的变种以及一些未来可能会流行的变种。序列建模及其相关架构是一个热门的研究领域，每年都有很多新进展。许多变种旨在创建更轻量的模型，具有更少的参数，且不像当前的RNN那样对硬件要求高。**时钟型RNN**（**CWRNNs**）是最近的一项发展，并且取得了巨大成功。还有**层次化注意力网络**，该网络基于注意力机制的思想，但最终也提出不应将RNN作为构建模块。在这个激动人心的领域里，充满了各种新动向，所以一定要保持警觉，关注下一个重要的想法。
- en: 'Activity 6.01: Sentiment Analysis of Amazon Product Reviews'
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.01：亚马逊产品评论情感分析
- en: So far, we've looked at the variants of RNNs and used them to predict sentiment
    on movie reviews from the IMDb dataset. In this activity, we will build a sentiment
    classification model on Amazon product reviews. The data contains reviews for
    several categories of products. The original dataset, available at [https://snap.stanford.edu/data/web-Amazon.html](https://snap.stanford.edu/data/web-Amazon.html),
    is huge; therefore, we have sampled 50,000 reviews for this activity.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过RNN的变体，并用它们来预测IMDb数据集上的电影评论情感。在这个活动中，我们将构建一个基于亚马逊产品评论的情感分类模型。数据包含了多个类别产品的评论。原始数据集可以在[https://snap.stanford.edu/data/web-Amazon.html](https://snap.stanford.edu/data/web-Amazon.html)找到，它非常庞大，因此我们为本次活动采样了50,000条评论。
- en: Note
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The sampled dataset, which has been split into train and test sets, can be found
    at [https://packt.live/3iNTUjN](https://packt.live/3iNTUjN).
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 采样的数据集已经被分割为训练集和测试集，可以在[https://packt.live/3iNTUjN](https://packt.live/3iNTUjN)找到。
- en: This activity will bring together the concepts and methods we discussed in this
    chapter and those discussed in *Chapter 4, Deep Learning for Text – Embeddings*,
    and *Chapter 5, Deep Learning for Sequences*. You will begin by performing a detailed
    text cleanup and conduct preprocessing to get it ready for the deep learning model.
    You will also use embeddings to represent text. For the prediction part, you will
    employ stacked LSTMs (two layers) and two dense layers.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 本次活动将汇总我们在本章中讨论的概念和方法，以及*第4章，文本的深度学习——嵌入*和*第5章，序列的深度学习*中讨论的内容。你将从进行详细的文本清理和预处理开始，为深度学习模型做好准备。你还将使用嵌入来表示文本。在预测部分，你将使用堆叠LSTM（两层）和两个全连接层。
- en: 'For convenience (and awareness), you will also utilize the `Tokenizer` API
    from TensorFlow (Keras) to convert the cleaned-up text into the corresponding
    sequences. The `Tokenizer` combines the function of the tokenizer from `NLTK`
    with the `vectorizer` (`CountVectorizer`/ `TfIdfVectorizer`) by tokenizing the
    text first and then learning a vocabulary from a dataset. Let''s see it in action
    by creating some toy data using the following command:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便（并且有意识地处理），你还将使用TensorFlow（Keras）中的`Tokenizer` API将清理后的文本转换为相应的序列。`Tokenizer`结合了`NLTK`中的分词器功能和`vectorizer`（`CountVectorizer`
    / `TfIdfVectorizer`），它首先对文本进行分词，然后从数据集中学习词汇表。让我们通过以下命令创建一些玩具数据来实际操作：
- en: '[PRE59]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The `Tokenizer` can be imported, instantiated, and fit on the toy data using
    the following commands:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下命令导入`Tokenizer`，实例化它并在玩具数据上进行拟合：
- en: '[PRE60]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Once the vocabulary has been trained on the toy data (index learned for each
    term), we can convert the input text into a corresponding sequence of indices
    for the terms. Let''s convert the toy data into the corresponding sequences of
    indices using the `texts_to_sequences` method of the tokenizer:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦词汇表在玩具数据上进行训练（每个术语的索引已经学习），我们就可以将输入文本转换为相应的术语索引序列。让我们使用分词器的`texts_to_sequences`方法将玩具数据转换为相应的索引序列：
- en: '[PRE61]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We''ll get the following output:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE62]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now, the data format is the same as that of the IMDb dataset we've used throughout
    this chapter, and it can be processed in a similar fashion.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据格式与我们在本章中使用的IMDb数据集相同，可以以类似的方式进行处理。
- en: 'With this, you are now ready to get started. The following are the high-level
    steps you will need to follow to complete this activity:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，你现在准备好开始了。以下是你需要遵循的高层步骤，来完成本次活动：
- en: Read in the data files for the train and test sets (`Amazon_reviews_train.csv`
    and `Amazon_reviews_test.csv`). Examine the shapes of the datasets and print out
    the top five records from the train data.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取训练集和测试集的数据文件（`Amazon_reviews_train.csv` 和 `Amazon_reviews_test.csv`）。检查数据集的形状，并打印出训练数据中的前五条记录。
- en: 'For convenience when it comes to processing, separate the raw text and the
    labels for the train and test set. Print the first two reviews from the train
    text. You should have the following four variables: `train_raw` comprising the
    raw text for the train data, `train_labels` with labels for the train data, `test_raw`
    containing raw text for the test data, and `test_labels` with labels for the test
    data.'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了便于处理，请将训练集和测试集的原始文本和标签分开。打印出训练文本中的前两个评论。你应该有以下四个变量：`train_raw` 包含训练数据的原始文本，`train_labels`
    是训练数据的标签，`test_raw` 包含测试数据的原始文本，`test_labels` 是测试数据的标签。
- en: 'Normalize the case and tokenize the test and train texts using NLTK''s `word_tokenize`
    (after importing it, of course – hint: use list comprehension for cleaner code).
    Print the first review from the train data to check if the tokenization worked.
    Download `punkt` from NLTK if you haven''t used the tokenizer before.'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLTK的`word_tokenize`标准化大小写并对测试和训练文本进行分词（当然，在使用之前需要先导入——提示：使用列表推导式可以使代码更简洁）。打印训练数据中的第一条评论，检查分词是否成功。如果之前没有使用过分词器，请从NLTK下载`punkt`。
- en: Import `stopwords` (built in to NLTK) and punctuation from the string module.
    Define a function (`drop_stop`) to remove these tokens from any input tokenized
    sentence. Download `stopwords` from NLTK if you haven't used it before.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入NLTK内置的`stopwords`和字符串模块中的标点符号。定义一个函数（`drop_stop`）以从任何输入的分词句子中移除这些标记。如果之前没有使用过`stopwords`，请从NLTK下载它。
- en: Using the defined function (`drop_stop`), remove the redundant stop words from
    the train and the test texts. Print the first review of the processed train texts
    to check if the function worked.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用定义的函数（`drop_stop`），从训练和测试文本中移除冗余的停用词。打印处理后的训练文本中的第一条评论，检查函数是否生效。
- en: Using `Porter Stemmer` from NLTK, stem the tokens for the train and test data.
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLTK中的`Porter Stemmer`对训练和测试数据的标记进行词干提取。
- en: Create the strings for each of the train and text reviews. This will help us
    work with the utilities in Keras to create and pad the sequences. Create the `train_texts`
    and `test_texts` variables. Print the first review from the processed train data
    to confirm it.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练和测试评论创建字符串。这将帮助我们利用Keras中的工具创建和填充序列。创建`train_texts`和`test_texts`变量。打印经过处理的训练数据中的第一条评论以确认它。
- en: From the Keras preprocessing utilities for text (`keras.preprocessing.text`),
    import the `Tokenizer` module. Define a vocabulary size of `10000` and instantiate
    the tokenizer with this vocabulary.
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Keras的文本预处理工具（`keras.preprocessing.text`）中导入`Tokenizer`模块。定义词汇表大小为`10000`，并用此词汇表实例化分词器。
- en: Fit the tokenizer on the train texts. This works just like `CountVectorizer`
    did in *Chapter 4, Deep Learning for Text – Embeddings*, and trains the vocabulary.
    After fitting, use the `texts_to_sequences` method of the tokenizer on the train
    and test sets to create the sequences for them. Print the sequence for the first
    review in the train data.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练文本上拟合分词器。这个过程与*第4章 深度学习文本——词嵌入*中的`CountVectorizer`类似，训练词汇表。拟合后，使用分词器的`texts_to_sequences`方法对训练和测试集进行序列化。打印训练数据中第一条评论的序列。
- en: We need to find the optimal length of the sequences to process in the model.
    Get the length of the reviews from the train set into a list and plot the histogram
    of the lengths.
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要找到在模型中处理的最佳序列长度。获取训练集评论的长度，并绘制长度的直方图。
- en: The data is now in the same format as the IMDb data we used in the chapter.
    Using a sequence length of `100` (define the `maxlen = 100` variable), use the
    `pad_sequences` method from the `sequence` module in Keras' preprocessing utilities
    (`keras.preprocessing.sequence`) to limit the sequences to `100` for both the
    train and test data. Check the shape of the result for the train data.
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据现在与我们在本章中使用的IMDb数据格式相同。使用`100`的序列长度（定义`maxlen = 100`变量），使用Keras预处理工具中的`sequence`模块的`pad_sequences`方法（`keras.preprocessing.sequence`）将训练和测试数据的序列长度限制为`100`。检查训练数据的结果形状。
- en: To build the model, import all the necessary layers from Keras (`embedding`,
    `spatialdropout`, `LSTM`, `dropout`, and `dense`) and import the `Sequential`
    model. Initialize the `Sequential` model.
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了构建模型，从Keras中导入所有必要的层（`embedding`、`spatialdropout`、`LSTM`、`dropout`和`dense`），并导入`Sequential`模型。初始化`Sequential`模型。
- en: Add an embedding layer with `32` as the vector size (`output_dim`). Add a spatial
    dropout of `40%`.
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含`32`向量大小（`output_dim`）的嵌入层。添加一个`40%`的空间Dropout。
- en: Build a stacked LSTM model with `2` layers with `64` cells each. Add a dropout
    layer with `40%` dropout.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个堆叠的LSTM模型，包含`2`层，每层`64`个单元。添加一个`40%` Dropout层。
- en: Add a dense layer with `32` neurons with `relu` activation, then a `50%` dropout
    layer, followed by another dense layer of `32` neurons with `relu` activation,
    and follow this up with another dropout layer with `50%` dropout.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含`32`个神经元、`relu`激活函数的全连接层，然后是一个`50%`的Dropout层，接着是另一个包含`32`个神经元、`relu`激活函数的全连接层，再跟随一个`50%`的Dropout层。
- en: Add a final dense layer with a single neuron with `sigmoid` activation and compile
    the model. Print the model summary.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最后一个包含一个神经元的全连接层，使用`sigmoid`激活函数，并编译模型。打印模型摘要。
- en: Fit the model on the training data with a `20%` validation split and a batch
    size of `128`. Train for `5` epochs.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`20%`的验证集和`128`的批量大小，在训练数据上拟合模型，训练`5`个周期。
- en: Make a prediction on the test set using the `predict_classes` method of the
    model. Using the `accuracy_score` method from `scikit-learn`, calculate the accuracy
    on the test set. Also, print out the confusion matrix.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的`predict_classes`方法在测试集上进行预测。然后，使用`scikit-learn`中的`accuracy_score`方法计算测试集上的准确率，并打印出混淆矩阵。
- en: With the preceding parameters, you should get about `86%` accuracy. With some
    hyperparameter tuning, you should be able to get a significantly higher accuracy.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的参数，你应该能够得到大约`86%`的准确率。通过调整超参数，你应该能够获得显著更高的准确率。
- en: Note
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 416.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤，以及解决方案和额外的评论，已在第416页呈现。
- en: Summary
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by understanding the reasons for plain RNNs not
    being practical for very large sequences – the main culprit being the vanishing
    gradient problem, which makes modeling long-range dependencies impractical. We
    saw the LSTM as an update that performs extremely well for long sequences, but
    it is rather complicated and has a large number of parameters. GRU is an excellent
    alternative that is a simplification over LSTM and works well on smaller datasets.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先了解了为什么普通RNN对于非常长的序列不实用——主要原因是消失梯度问题，它使得建模长范围依赖关系变得不切实际。我们看到LSTM作为一种更新，它在处理长序列时表现非常好，但它相当复杂，并且有大量的参数。GRU是一个很好的替代方案，它是LSTM的简化版，并且在较小的数据集上表现良好。
- en: Then, we started looking at ways to extract more power from these RNNs by using
    bidirectional RNNs and stacked layers of RNNs. We also discussed attention mechanisms,
    a significant new approach that provides state-of-the-art results in translation
    but can also be employed on other sequence-processing tasks. All of these are
    extremely powerful models that have changed the way several tasks are performed
    and form the basis for models that produce state-of-the-art results. With active
    research in the area, we expect things to only get better as more novel variants
    and architectures are released.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始探索如何通过使用双向RNN和堆叠的RNN层来从这些RNN中提取更多的能力。我们还讨论了注意力机制，这是一种重要的新方法，在翻译中提供了最先进的结果，但也可以应用于其他序列处理任务。所有这些都是极其强大的模型，改变了许多任务的执行方式，并为产生最先进结果的模型奠定了基础。随着该领域的积极研究，我们预计随着更多新颖的变种和架构的发布，情况只会变得更好。
- en: Now that we've discussed a variety of powerful modeling approaches, in the next
    chapter, we will be ready to discuss a very interesting topic in the deep learning
    domain that enables AI to be creative – **Generative Adversarial Networks**.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了各种强大的建模方法，在下一章中，我们将准备好讨论深度学习领域中一个非常有趣的话题，它使得人工智能能够具有创造力——**生成对抗网络**。
