- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Multimodal Modular RAG for Drone Technology
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无人机技术的多模态模块化RAG
- en: We will take generative AI to the next level with modular RAG in this chapter.
    We will build a system that uses different components or modules to handle different
    types of data and tasks. For example, one module processes textual information
    using LLMs, as we have done until the last chapter, while another module manages
    image data, identifying and labeling objects within images. Imagine using this
    technology in drones, which have become crucial across various industries, offering
    enhanced capabilities for aerial photography, efficient agricultural monitoring,
    and effective search and rescue operations. They even use advanced computer vision
    technology and algorithms to analyze images and identify objects like pedestrians,
    cars, trucks, and more. We can then activate an LLM agent to retrieve, augment,
    and respond to a user’s question.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过模块化RAG将生成AI提升到新的水平。我们将构建一个使用不同组件或模块来处理不同类型数据和任务的系统。例如，一个模块使用LLM处理文本信息，正如我们在上一章所做的那样，而另一个模块管理图像数据，识别和标记图像中的对象。想象一下在无人机中使用这项技术，无人机已成为各个行业的关键，提供了增强的空中摄影、高效的农业监测和有效的搜救行动能力。它们甚至使用先进的计算机视觉技术和算法来分析图像并识别行人、汽车、卡车等对象。然后我们可以激活一个LLM代理来检索、增强并对用户的提问做出响应。
- en: In this chapter, we will build a multimodal modular RAG program to generate
    responses to queries about drone technology using text and image data from multiple
    sources. We will first define the main aspects of modular RAG, multimodal data,
    multisource retrieval, modular generation, and augmented output. We will then
    build a multimodal modular RAG-driven generative AI system in Python applied to
    drone technology with LlamaIndex, Deep Lake, and OpenAI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个多模态模块化RAG程序，使用来自多个来源的文本和图像数据来生成关于无人机技术的响应。我们首先定义模块化RAG的主要方面，包括多模态数据、多源检索、模块化生成和增强输出。然后，我们将使用LlamaIndex、Deep
    Lake和OpenAI在Python中构建一个多模态模块化RAG驱动的生成AI系统，应用于无人机技术。
- en: 'Our system will use two datasets: the first one containing textual information
    about drones that we built in the previous chapter and the second one containing
    drone images and labels from Activeloop. We will use Deep Lake to work with multimodal
    data, LlamaIndex for indexing and retrieval, and generative queries with OpenAI
    LLMs. We will add multimodal augmented outputs with text and images. Finally,
    we will build performance metrics for the text responses and introduce an image
    recognition metric with GPT-4o, OpenAI’s powerful **Multimodal LLM** (**MMLLM**).
    By the end of the chapter, you will know how to build a multimodal modular RAG
    workflow leveraging innovative multimodal and multisource functionalities.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统将使用两个数据集：第一个包含上一章中我们构建的关于无人机的文本信息，第二个包含来自Activeloop的无人机图像和标签。我们将使用Deep
    Lake来处理多模态数据，使用LlamaIndex进行索引和检索，以及使用OpenAI的LLM进行生成查询。我们将添加包含文本和图像的多模态增强输出。最后，我们将为文本响应构建性能指标，并引入GPT-4o，OpenAI强大的**多模态LLM**（**MMLLM**）的图像识别指标。到本章结束时，你将了解如何利用创新的多模态和多源功能构建一个多模态模块化RAG工作流程。
- en: 'This chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Multimodal modular RAG
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态模块化RAG
- en: Multisource retrieval
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多源检索
- en: OpenAI LLM-guided multimodal multisource retrieval
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI LLM引导的多模态多源检索
- en: Deep Lake multimodal datasets
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deep Lake多模态数据集
- en: Image metadata-based retrieval
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图像元数据的检索
- en: Augmented multimodal output
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强多模态输出
- en: Let’s begin by defining multimodal modular RAG.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义多模态模块化RAG。
- en: What is multimodal modular RAG?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是多模态模块化RAG？
- en: Multimodal data combines different forms of information, such as text, images,
    audio, and video, to enrich data analysis and interpretation. Meanwhile, a system
    is a modular RAG system when it utilizes distinct modules for handling different
    data types and tasks. Each module is specialized; for example, one module will
    focus on text and another on images, demonstrating a sophisticated integration
    capability that enhances response generation with retrieved multimodal data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据结合了不同形式的信息，如文本、图像、音频和视频，以丰富数据分析和理解。同时，当系统利用不同的模块来处理不同类型的数据和任务时，它就是一个模块化RAG系统。每个模块都是专业化的；例如，一个模块将专注于文本，另一个模块将专注于图像，展示了复杂的集成能力，增强了通过检索的多模态数据生成的响应。
- en: The program in this chapter will also be multisource through the two datasets
    we will use. We will use the LLM dataset on the drone technology built in the
    previous chapter. We will also use the Deep Lake multimodal VisDrone dataset,
    which contains thousands of labeled images captured by drones.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的程序也将通过我们使用的两个数据集实现多源。我们将使用前几章中构建的无人机技术LLM数据集。我们还将使用包含由无人机捕获的数千个标记图像的Deep
    Lake多模态VisDrone数据集。
- en: We have selected drones for our example since drones have become crucial across
    various industries, offering enhanced capabilities for aerial photography, efficient
    agricultural monitoring, and effective search and rescue operations. They also
    facilitate wildlife tracking, streamline commercial deliveries, and enable safer
    infrastructure inspections. Additionally, drones support environmental research,
    traffic management, and firefighting. They can enhance surveillance for law enforcement,
    revolutionizing multiple fields by improving accessibility, safety, and cost-efficiency.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择无人机作为示例，因为无人机在各个行业中已成为关键，提供了增强的空中摄影能力、高效的农业监测以及有效的搜救作业。它们还促进了野生动物追踪，简化了商业配送，并使基础设施检查更安全。此外，无人机支持环境研究、交通管理和消防。它们可以增强执法部门的监控，通过提高可访问性、安全性和成本效益来革新多个领域。
- en: '*Figure 4.1* contains the workflow we will implement in this chapter. It is
    based on the generative RAG ecosystem illustrated in *Figure 1.3* from *Chapter
    1*, *Why Retrieval-Augmented Generation?*. We added embedding and indexing functionality
    in the previous chapters, but this chapter will focus on retrieval and generation.
    The system we will build blurs the lines between retrieval and generation since
    the generator is intensively used for retrieving (seamless scoring and ranking)
    as well as generating in the chapter’s notebook.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.1*包含本章我们将实施的流程。它基于*第1章*，*为什么检索增强生成？*中*图1.3*所展示的生成RAG生态系统。我们在前几章中添加了嵌入和索引功能，但本章将专注于检索和生成。我们将构建的系统模糊了检索和生成之间的界限，因为生成器在检索（无缝评分和排名）以及本章笔记本中的生成方面都得到了大量使用。'
- en: '![A diagram of a multimodal modular rag system  Description automatically generated](img/B31169_04_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![多模态模块化RAG系统图  自动生成的描述](img/B31169_04_01.png)'
- en: 'Figure 4.1: A multimodal modular RAG system'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：多模态模块化RAG系统
- en: This chapter aims to build an educational modular RAG question-answering system
    focused on drone technology. You can rely on the functionality implemented in
    the notebooks of the preceding chapters, such as Deep Lake for vectors in *Chapter
    2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*, and indices with
    LlamaIndex in *Chapter 3*, *Building* *Index-based RAG with LlamaIndex, Deep Lake,
    and OpenAI*. If necessary, take your time to go back to the previous chapters
    and have a look.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在构建一个以无人机技术为重点的教育模块化RAG问答系统。你可以依赖前几章笔记本中实现的功能，例如*第2章*中的Deep Lake向量，*使用Deep
    Lake和OpenAI构建RAG嵌入向量存储*，以及*第3章*中的LlamaIndex索引，*使用LlamaIndex、Deep Lake和OpenAI构建基于索引的RAG*。如有必要，请花时间回顾前几章并查看。
- en: Let’s go through the multimodal, multisource, modular RAG ecosystem in this
    chapter, represented in *Figure 4.1*. We will use the titles and subsections in
    this chapter represented in italics. Also, each phase is preceded by its location
    in *Figure 4.1*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨多模态、多源、模块化RAG生态系统，如图*4.1*所示。我们将使用本章中用斜体表示的标题和副标题。此外，每个阶段都由*图4.1*中的位置先行。
- en: '**(D4)** *Loading the LLM dataset* created in *Chapter 3*, which contains textual
    data on drones.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(D4)** *加载*第3章中创建的*LLM数据集*，其中包含关于无人机的文本数据。'
- en: '**(D4)** *Initializing the LLM query engine* with a LlamaIndex vector store
    index using `VectorStoreIndex` and setting the created index for the query engine,
    which overlaps with **(G4)** as both a retriever and a generator with the OpenAI
    GPT model.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(D4)** 使用`VectorStoreIndex`初始化LLM查询引擎，并设置创建的索引用于查询引擎，这与**(G4)**重叠，因为两者都是OpenAI
    GPT模型下的检索器和生成器。'
- en: '**(G1)** Defining the *user input for multimodal modular RAG* for both the
    LLM query engine (for the textual dataset) and the multimodal query engine (for
    the `VisDrone` dataset).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**(G1)** 定义多模态模块化RAG的*用户输入*，适用于LLM查询引擎（用于文本数据集）和多模态查询引擎（用于`VisDrone`数据集）。'
- en: Once the textual dataset has been loaded, the query engine has been created,
    and the user input has been defined as a baseline query for the textual dataset
    and the multimodal dataset, the process continues by generating a response for
    the textual dataset created in *Chapter 2*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了文本数据集，创建了查询引擎，并将用户输入定义为文本数据集和多模态数据集的基线查询，过程将继续通过为 *第 2 章* 中创建的文本数据集生成响应来继续。
- en: While *querying the textual dataset*, **(G1)**, **(G2)**, and **(G4)** overlap
    in the same seamless LlamaIndex process that retrieves data and generates content.
    The response is saved as `llm_response` for the duration of the session.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在查询文本数据集 **（G1）**、**（G2）** 和 **（G4）** 时，它们在同一个无缝的 LlamaIndex 流程中重叠，该流程检索数据和生成内容。响应在会话期间被保存为
    `llm_response`。
- en: 'Now, the multimodal `VisDrone` dataset will be loaded into memory and queried:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，多模态 `VisDrone` 数据集将被加载到内存中并进行查询：
- en: '**(D4)** The multimodal process begins by *loading and visualizing the multimodal
    dataset*. The program then continues by *navigating the multimodal dataset structure*,
    *selecting an image*, and *adding bounding boxes*.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（D4）** 多模态过程首先通过 *加载和可视化多模态数据集* 开始。然后程序继续通过 *导航多模态数据集结构*、*选择一个图像* 和 *添加边界框*。'
- en: 'The same process as for the textual dataset is then applied to the `VisDrone`
    multimodal dataset:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将应用于文本数据集的相同过程应用于 `VisDrone` 多模态数据集：
- en: '**(D4)** *Building a multimodal query engine* with LlamaIndex by creating a
    vector store index based on `VisDrone` data using `VectorStoreIndex` and setting
    the created index for the query engine, which overlaps with **(G4)** as both a
    retriever and a generator with OpenAI GPT.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（D4）** 通过使用 `VectorStoreIndex` 基于数据集 `VisDrone` 创建向量存储索引，并设置创建的索引以供查询引擎使用，通过
    LlamaIndex 构建 *构建一个多模态查询引擎*，这与其重叠，作为 OpenAI GPT 的检索器和生成器 **（G4）**。'
- en: '**(G1)** The user input for the multimodal search engine is the same as the
    *user input for multimodal modular RAG* since it is used for both the LLM query
    engine (for the textual dataset) and the multimodal query engine (for the `VisDrone`
    dataset).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（G1）** 多模态搜索引擎的用户输入与 *多模态模块化 RAG* 的用户输入相同，因为它用于 LLM 查询引擎（用于文本数据集）和多模态查询引擎（用于
    `VisDrone` 数据集）。'
- en: 'The multimodal `VisDrone` dataset will now be loaded and indexed, and the query
    engine is ready. The purpose of **(G1)** user input is for the LlamaIndex query
    engine to retrieve relevant documents from VisDrone using an LLM—in this case,
    an OpenAI model. Then, the retrieval functions will trace the response back to
    its source in the multimodal dataset to find the image of the source nodes. We
    are, in fact, using the query engine to reach an image through its textual response:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态 `VisDrone` 数据集现在将被加载和索引，查询引擎已准备就绪。**（G1）** 用户输入的目的是让 LlamaIndex 查询引擎使用一个
    LLM（在这种情况下，是一个 OpenAI 模型）从 VisDrone 中检索相关文档。然后，检索函数将追踪响应回其多模态数据集中的来源，以找到源节点的图像。实际上，我们正在使用查询引擎通过其文本响应来获取图像：
- en: '**(G1)**, **(G2)**, and **(G4)** overlap in a seamless LlamaIndex query when
    running a query on the `VisDrone` multimodal dataset.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（G1）**、**（G2）** 和 **（G4）** 在对 `VisDrone` 多模态数据集进行查询时在无缝的 LlamaIndex 查询中重叠。'
- en: Processing the response **(G4)** to find the source node and retrieve its image
    leads us back to **(D4)** for image retrieval. This leads to selecting and processing
    the image of the source node.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理响应 **（G4）** 以找到源节点并检索其图像，使我们回到 **（D4）** 进行图像检索。这导致选择和处理源节点的图像。
- en: 'At this point, we now have the textual and the image response. We can then
    build a summary and apply an accuracy performance metric after having visualized
    the time elapsed for each phase as we built the program:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们现在有了文本和图像响应。然后我们可以构建一个摘要，并在可视化每个阶段所花费的时间后应用准确度性能指标：
- en: '**(G4)** We present a merged output with the LLM response and the augmented
    output with the image of the multimodal response in a *multimodal modular summary*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（G4）** 我们在一个 *多模态模块化摘要* 中呈现了 LLM 响应和带有多模态响应图像的增强输出。'
- en: '**(E)** Finally, we create an *LLM performance metric* and a *multimodal performance
    metric*. We then sum them up as a *multimodal modular RAG performance metric*.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**（E）** 最后，我们创建一个 *LLM 性能指标* 和一个 *多模态性能指标*。然后我们将它们作为一个 *多模态模块化 RAG 性能指标* 相加。'
- en: 'We can draw two conclusions from this multimodal modular RAG system:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个多模态模块化 RAG 系统中，我们可以得出两个结论：
- en: The system we are building in this chapter is one of the many ways RAG-driven
    generative AI can be designed in real-life projects. Each project will have its
    specific needs and architecture.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在本章中构建的系统是RAG驱动的生成式AI在现实项目中设计的许多方法之一。每个项目都会有其特定的需求和架构。
- en: The rapid evolution from generative AI to the complexity of RAG-driven generative
    AI requires the corresponding development of seamlessly integrated cross-platform
    components such as LlamaIndex, Deep Lake, and OpenAI in this chapter. These platforms
    are also integrated with many other frameworks, such as Pinecone and LangChain,
    which we will discuss in *Chapter 6*, *Scaling RAG Bank Customer Data with Pinecone*.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从生成式AI到由RAG驱动的生成式AI的快速演变，需要在本章中相应地开发无缝集成的跨平台组件，如LlamaIndex、Deep Lake和OpenAI。这些平台还与许多其他框架集成，例如Pinecone和LangChain，我们将在**第6章**，*使用Pinecone扩展RAG银行客户数据*中讨论。
- en: Now, let’s dive into Python and build the multimodal modular RAG program.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入Python，构建多模态模块化RAG程序。
- en: Building a multimodal modular RAG program for drone technology
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为无人机技术构建多模态模块化RAG程序
- en: 'In the following sections, we will build a multimodal modular RAG-driven generative
    system from scratch in Python, step by step. We will implement:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分中，我们将从头开始使用Python逐步构建一个多模态模块化RAG驱动的生成式系统。我们将实现：
- en: LlamaIndex-managed OpenAI LLMs to process and understand text about drones
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由LlamaIndex管理的OpenAI LLM处理和理解有关无人机的文本
- en: Deep Lake multimodal datasets containing images and labels of drone images taken
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深湖多模态数据集包含无人机图像及其标签
- en: Functions to display images and identify objects within them using bounding
    boxes
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数用于显示图像并在其中使用边界框识别对象
- en: A system that can answer questions about drone technology using both text and
    images
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可以使用文本和图像回答有关无人机技术问题的系统
- en: Performance metrics aimed at measuring the accuracy of the modular multimodal
    responses, including image analysis with GPT-4o
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对测量模块化多模态响应准确性的性能指标，包括使用GPT-4o进行图像分析
- en: Also, make sure you have created the LLM dataset in *Chapter 2* since we will
    be loading it in this section. However, you can read this chapter without running
    the notebook since it is self-contained with code and explanations. Now, let’s
    get to work!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请确保您已经在**第2章**中创建了LLM数据集，因为我们将在本节中加载它。然而，您可以在不运行笔记本的情况下阅读本章，因为它包含代码和解释，是自包含的。现在，让我们开始工作！
- en: Open the `Multimodal_Modular_RAG_Drones.ipynb` notebook in the GitHub repository
    for this chapter at [https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04](https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04).
    The packages installed are the same as those listed in the *Installing the environment*
    section of the previous chapter. Each of the following sections will guide you
    through building the multimodal modular notebook, starting with the LLM module.
    Let’s go through each section of the notebook step by step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 打开GitHub仓库中本章节的`Multimodal_Modular_RAG_Drones.ipynb`笔记本，网址为[https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04](https://github.com/Denis2054/RAG-Driven-Generative-AI/tree/main/Chapter04)。安装的包与上一章中**安装环境**部分列出的相同。以下每个部分都将指导您构建多模态模块化笔记本，从LLM模块开始。让我们一步一步地通过笔记本的每个部分。
- en: Loading the LLM dataset
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载LLM数据集
- en: 'We will load the drone dataset created in *Chapter 3*. Make sure to insert
    the path to your dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将加载在**第3章**中创建的无人机数据集。请确保插入您数据集的路径：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output will confirm that the dataset is loaded and will display the link
    to your dataset:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将确认数据集已加载，并将显示您的数据集链接：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The program now creates a dictionary to hold the data to load it into a pandas
    DataFrame to visualize it:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在创建一个字典来保存数据，以便将其加载到pandas DataFrame中以可视化：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output shows the text dataset with its structure: `embedding` (vectors),
    `id` (unique string identifier), `metadata` (in this case, the source of the data),
    and `text`, which contains the content:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了文本数据集的结构：`embedding`（向量）、`id`（唯一字符串标识符）、`metadata`（在这种情况下，数据的来源）和`text`，其中包含内容：
- en: '![](img/B31169_04_02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31169_04_02.png)'
- en: 'Figure 4.2: Output of the text dataset structure and content'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：文本数据集结构和内容的输出
- en: We will now initialize the LLM query engine.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将初始化LLM查询引擎。
- en: Initializing the LLM query engine
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化LLM查询引擎
- en: 'As in *Chapter 3*, *Building Indexed-Based RAG with LlamaIndex, Deep Lake,
    and OpenAI*, we will initialize a vector store index from the collection of drone
    documents (`documents_llm`) of the dataset (`ds`). The `GPTVectorStoreIndex.from_documents()`
    method creates an index that increases the retrieval speed of documents based
    on vector similarity:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如同*第3章*，*使用LlamaIndex、Deep Lake和OpenAI构建基于索引的RAG*，我们将从数据集（`ds`）的无人机文档集合（`documents_llm`）初始化一个向量存储索引。`GPTVectorStoreIndex.from_documents()`方法创建一个基于向量相似性提高文档检索速度的索引：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `as_query_engine()` method configures this index as a query engine with
    the specific parameters, as in *Chapter 3*, for similarity and retrieval depth,
    allowing the system to answer queries by finding the most relevant documents:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`as_query_engine()`方法配置此索引为具有特定参数的查询引擎，如同*第3章*中所述，用于相似性和检索深度，允许系统通过找到最相关的文档来回答查询：'
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, the program introduces the user input.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，程序引入了用户输入。
- en: User input for multimodal modular RAG
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模态模块化RAG的用户输入
- en: 'The goal of defining the user input in the context of the modular RAG system
    is to formulate a query that will effectively utilize both the text-based and
    image-based capabilities. This allows the system to generate a comprehensive and
    accurate response by leveraging multiple information sources:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在模块化RAG系统的上下文中定义用户输入的目的是制定一个查询，该查询将有效地利用基于文本和基于图像的能力。这允许系统通过利用多个信息源生成全面且准确响应：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this context, the user input is the *baseline*, the starting point, or a
    standard query used to assess the system’s capabilities. It will establish the
    initial frame of reference for how well the system can handle and respond to queries
    utilizing its available resources (e.g., text and image data from various datasets).
    In this example, the baseline is empirical and will serve to evaluate the system
    from that reference point.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，用户输入是**基准**、起点或用于评估系统能力的标准查询。它将建立系统利用其可用资源（例如，来自各种数据集的文本和图像数据）处理和响应查询的初始参考框架。在这个例子中，基准是经验性的，并将从该参考点评估系统。
- en: Querying the textual dataset
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询文本数据集
- en: 'We will run the vector query engine request as we did in *Chapter 3*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像*第3章*中那样运行向量查询引擎请求：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The execution time is satisfactory:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间令人满意：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output content is also satisfactory:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出内容也令人满意：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The program now loads the multimodal drone dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在加载多模态无人机数据集。
- en: Loading and visualizing the multimodal dataset
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和可视化多模态数据集
- en: 'We will use the existing pubic VisDrone dataset available on Deep Lake: [https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/](https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/).
    We will *not* create a vector store but simply load the existing dataset in memory:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Deep Lake上现有的公开VisDrone数据集：[https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/](https://datasets.activeloop.ai/docs/ml/datasets/visdrone-dataset/)。我们**不会**创建向量存储，而是简单地加载现有的数据集到内存中：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will display a link to the online dataset that you can explore with
    SQL, or natural language processing commands if you prefer, with the tools provided
    by Deep Lake:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将显示一个链接到在线数据集，您可以使用SQL或如果您更喜欢，使用Deep Lake提供的工具进行自然语言处理命令来探索：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s display the summary to explore the dataset in code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们显示摘要以在代码中探索数据集：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output provides useful information on the structure of the dataset:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了有关数据集结构的有用信息：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The structure contains images, boxes for the boundary boxes of the objects
    in the image, and labels describing the images and boundary boxes. Let’s visualize
    the dataset in code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结构包含图像、图像中对象的边界框框，以及描述图像和边界框的标签。让我们在代码中可视化数据集：
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output shows the images and their boundary boxes:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了图像及其边界框：
- en: '![](img/B31169_04_03.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片B31169_04_03.png](img/B31169_04_03.png)'
- en: 'Figure 4.3: Output showing boundary boxes'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：显示边界框的输出
- en: 'Now, let’s go further and display the content of the dataset in a pandas DataFrame
    to see what the images look like:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进一步显示数据集的内容在一个pandas DataFrame中，以查看图像的外观：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output in *Figure 4.4* shows the content of the dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.4*中的输出显示了数据集的内容：'
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_04_04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B31169_04_04.png)'
- en: 'Figure 4.4: Excerpt of the VisDrone dataset'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：VisDrone数据集的摘录
- en: 'There are 6,471 rows of images in the dataset and 3 columns:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有6,471行图像和3列：
- en: The `image` column contains the image. The format of the image in the dataset,
    as indicated by the byte sequence `b'\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00...'`,
    is JPEG. The bytes `b'\xff\xd8\xff\xe0'` specifically signify the start of a JPEG
    image file.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`列包含图像。数据集中图像的格式，如字节序列`b''\xff\xd8\xff\xe0\x00\x10JFIF\x00\x01\x01\x00...''`所示，是JPEG。字节`b''\xff\xd8\xff\xe0''`特别表示JPEG图像文件的开始。'
- en: The `boxes` column contains the coordinates and dimensions of bounding boxes
    in the image, which are normally in the format `[x, y, width, height]`.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boxes`列包含图像中边界框的坐标和尺寸，通常格式为`[x, y, width, height]`。'
- en: The `labels` column contains the label of each bounding box in the `boxes` column.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels`列包含`boxes`列中每个边界框的标签。'
- en: 'We can display the list of labels for the images:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示图像的标签列表：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output provides the list of labels, which defines the scope of the dataset:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 输出提供了标签列表，它定义了数据集的范围：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With that, we have successfully loaded the dataset and will now explore the
    multimodal dataset structure.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经成功加载了数据集，现在将探索多模态数据集结构。
- en: Navigating the multimodal dataset structure
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导航多模态数据集结构
- en: In this section, we will select an image and display it using the dataset’s
    image column. To this image, we will then add the bounding boxes of a label that
    we will choose. The program first selects an image.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将选择一张图像并使用数据集的图像列来显示它。然后，我们将添加我们选择的标签的边界框。程序首先选择一张图像。
- en: Selecting and displaying an image
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择和显示图像
- en: 'We will select the first image in the dataset:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将选择数据集中的第一张图像：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let’s display it with no bounding boxes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们不显示边界框来显示它：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The image displayed contains trucks, pedestrians, and other types of objects:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的图像包含卡车、行人和其他类型的物体：
- en: '![](img/B31169_04_05.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31169_04_05.png)'
- en: 'Figure 4.5: Output displaying objects'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：显示物体的输出
- en: Now that the image is displayed, the program will add bounding boxes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图像已显示，程序将添加边界框。
- en: Adding bounding boxes and saving the image
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加边界框并保存图像
- en: 'We have displayed the first image. The program will then fetch all the labels
    for the selected image:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经显示了第一张图像。然后程序将检索所选图像的所有标签：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output displays `value`, which contains the numerical indices of a label,
    and `text`, which contains the corresponding text labels of a label:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示`value`，它包含标签的数值索引，以及`text`，它包含相应的文本标签：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can display the values and the corresponding text in two columns:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在两列中显示值和相应的文本：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output gives us a clear representation of the content of the labels of
    an image:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 输出清楚地表示了图像标签的内容：
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can group the class names (labels in plain text) of the images:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按图像的类名字符串（标签）分组：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can now group and display all the labels that describe the image:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以分组并显示描述图像的所有标签：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see all the classes the image contains:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到图像中包含的所有类别：
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The number of label classes sometimes exceeds what a human eye can see in an
    image.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 标签类别的数量有时会超过人眼在图像中可以看到的数量。
- en: 'Let’s now add bounding boxes. We first create a function to add the bounding
    boxes, display them, and save the image:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们添加边界框。我们首先创建一个函数来添加边界框，显示它们，并保存图像：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can add the bounding boxes for a specific label. In this case, we selected
    the `"truck"` label:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为特定标签添加边界框。在这种情况下，我们选择了`"truck"`标签：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The image displayed now contains the bounding boxes for trucks:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在显示的图像包含了卡车的边界框：
- en: '![A truck with several trailers  Description automatically generated with medium
    confidence](img/B31169_04_06.png)Figure 4.6: Output displaying bounding boxes'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有几个拖车的卡车  描述由中等置信度自动生成](img/B31169_04_06.png)图4.6：显示边界框的输出'
- en: Let’s now activate a query engine to retrieve and obtain a response.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们激活一个查询引擎来检索并获得响应。
- en: Building a multimodal query engine
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建多模态查询引擎
- en: 'In this section, we will query the VisDrone dataset and retrieve an image that
    fits the user input we entered in the *User input for multimodal modular RAG*
    section of this notebook. To achieve this goal, we will:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查询VisDrone数据集并检索一个符合我们在本笔记本的“*用户输入的多模态模块RAG*”部分中输入的用户输入的图像。为了实现这个目标，我们将：
- en: Create a vector index for each row of the `df` DataFrame containing the images,
    boxing data, and labels of the VisDrone dataset.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为包含VisDrone数据集的图像、框数据和标签的`df` DataFrame的每一行创建一个向量索引。
- en: Create a query engine that will query the text data of the dataset, retrieve
    relevant image information, and provide a text response.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个查询引擎，该引擎将查询数据集的文本数据，检索相关图像信息，并提供文本响应。
- en: Parse the nodes of the response to find the keywords related to the user input.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析响应中的节点以找到与用户输入相关的关键词。
- en: Parse the nodes of the response to find the source image.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析响应中的节点以找到源图像。
- en: Add the bounding boxes of the source image to the image.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将源图像的边界框添加到图像中。
- en: Save the image.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存图像。
- en: Creating a vector index and query engine
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建向量索引和查询引擎
- en: The code first creates a document that will be processed to create a vector
    store index for the multimodal drone dataset. The `df` DataFrame we created in
    the *Loading and visualizing the multimodal dataset* section of the notebook on
    GitHub does not have unique indices or embeddings. We will create them in memory
    with LlamaIndex.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先创建一个文档，该文档将被处理以创建多模态无人机数据集的向量存储索引。我们在GitHub笔记本的*加载和可视化多模态数据集*部分创建的`df` DataFrame没有唯一的索引或嵌入。我们将使用LlamaIndex在内存中创建它们。
- en: 'The program first assigns a unique ID to the DataFrame:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先为DataFrame分配一个唯一的ID：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This line adds a new column to the `df` DataFrame called `doc_id`. It assigns
    unique identifiers to each row by converting the DataFrame’s row indices to strings.
    An empty list named `documents` is initialized, which we will use to create a
    vector index:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码向`df` DataFrame添加了一个名为`doc_id`的新列。它通过将DataFrame的行索引转换为字符串为每一行分配唯一标识符。初始化了一个名为`documents`的空列表，我们将使用它来创建向量索引：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, the `iterrows()` method iterates through each row of the DataFrame, generating
    a sequence of index and row pairs:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`iterrows()`方法遍历DataFrame的每一行，生成一系列索引和行对：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`documents` is appended with all the records in the dataset, and a DataFrame
    is created:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`documents`附加了数据集中的所有记录，并创建了一个DataFrame：'
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The documents are now ready to be indexed with `GPTVectorStoreIndex`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 文档现在准备好使用`GPTVectorStoreIndex`进行索引：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The dataset is then seamlessly equipped with indices that we can visualize
    in the index dictionary:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集随后无缝地配备了我们可以可视化在索引字典中的索引：
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output shows that an index has now been added to the dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示现在已将索引添加到数据集中：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We can now run a query on the multimodal dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在多模态数据集上运行查询。
- en: Running a query on the VisDrone multimodal dataset
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在VisDrone多模态数据集上运行查询
- en: 'We now set `vector_store_index` as the query engine, as we did in the *Vector
    store index query engine* section in *Chapter 3*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将`vector_store_index`设置为查询引擎，正如我们在第3章的*向量存储索引查询引擎*部分所做的那样：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can also run a query on the dataset of drone images, just as we did in *Chapter
    3* on an LLM dataset:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在像第3章中LLM数据集那样对无人机图像数据集运行查询：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The execution time is satisfactory:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间是令人满意的：
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will now examine the text response:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查文本响应：
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the output is logical and therefore satisfactory.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到输出是逻辑的，因此是令人满意的。
- en: Drones use various sensors such as cameras, LiDAR, and GPS to identify and track
    objects like trucks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机使用各种传感器，如摄像头、激光雷达和GPS，以识别和跟踪像卡车这样的物体。
- en: Processing the response
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理响应
- en: 'We will now parse the nodes in the response to find the unique words in the
    response and select one for this notebook:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将解析响应中的节点，以找到响应中的独特单词，并从中选择一个用于这个笔记本：
- en: '[PRE39]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We found a unique word (`''truck''`) and its unique index, which will lead
    us directly to the image of the source of the node that generated the response:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了一个独特的单词（`'truck'`）及其独特的索引，这将直接带我们到生成响应的节点源图像：
- en: '[PRE40]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We could select more words and design this function in many different ways depending
    on the specifications of each project.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择更多的单词，并根据每个项目的具体要求以多种不同的方式设计这个函数。
- en: We will now search for the image by going through the source nodes, just as
    we did for an LLM dataset in the *Query response and source* section of the previous
    chapter. Multimodal vector stores and querying frameworks are flexible. Once we
    learn how to perform retrievals on an LLM and a multimodal dataset, we are ready
    for anything that comes up!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过源节点搜索图像，就像我们在上一章的*查询响应和源*部分的LLM数据集上所做的那样。多模态向量存储和查询框架是灵活的。一旦我们学会了如何在LLM和多模态数据集上执行检索，我们就准备好应对任何可能出现的情况了！
- en: Let’s select and process the information related to an image.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择并处理与图像相关的信息。
- en: Selecting and processing the image of the source node
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择并处理源节点的图像
- en: 'Before running the image retrieval and displaying function, let’s first delete
    the image we displayed in the *Adding bounding boxes and saving the image* section
    of this notebook to make sure we are working on a new image:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行图像检索和显示功能之前，让我们首先删除我们在本笔记本的*添加边界框并保存图像*部分中显示的图像，以确保我们正在处理一张新图像：
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We are now ready to search for the source image, call the bounding box, and
    display and save the function we defined earlier:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以搜索源图像，调用边界框，并显示和保存我们之前定义的功能：
- en: '[PRE42]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The program now goes through the source nodes with the keyword `"truck"` search,
    applies the bounding boxes, and displays and saves the image:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在通过关键字 `"truck"` 进行源节点搜索，应用边界框，并显示和保存图像：
- en: '[PRE43]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is satisfactory:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 输出令人满意：
- en: '![An aerial view of a factory  Description automatically generated](img/B31169_04_07.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![工厂的空中视图  自动生成的描述](img/B31169_04_07.png)'
- en: 'Figure 4.7: Displayed satisfactory output'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：显示满意的结果
- en: Multimodal modular summary
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态模块化总结
- en: We have built a multimodal modular program step by step that we can now assemble
    in a summary. We will create a function to display the source image of the response
    to the user input, then print the user input and the LLM output, and display the
    image.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经逐步构建了一个多模态模块化程序，现在可以将其汇总。我们将创建一个函数来显示对用户输入的响应的源图像，然后打印用户输入和 LLM 输出，并显示图像。
- en: 'First, we create a function to display the source image saved by the multimodal
    retrieval engine:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个函数来显示多模态检索引擎保存的源图像：
- en: '[PRE44]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we can display the user input, the LLM response, and the multimodal response.
    The output first displays the textual responses (user input and LLM response):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以显示用户输入、LLM 响应和多模态响应。输出首先显示文本响应（用户输入和 LLM 响应）：
- en: '[PRE45]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, the image is displayed with the bounding boxes for trucks in this case:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，显示带有卡车边界框的图像：
- en: '![An aerial view of a factory  Description automatically generated](img/B31169_04_08.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![工厂的空中视图  自动生成的描述](img/B31169_04_08.png)'
- en: 'Figure 4.8: Output displaying boundary boxes'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：显示边界框的输出
- en: By adding an image to a classical LLM response, we augmented the output. Multimodal
    RAG output augmentation will enrich generative AI by adding information to both
    the input and output. However, as for all AI programs, designing a performance
    metric requires efficient image recognition functionality.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向经典的 LLM 响应中添加图像，我们增强了输出。多模态 RAG 输出增强通过向输入和输出添加信息来丰富生成式 AI。然而，对于所有 AI 程序，设计性能指标需要高效的图像识别功能。
- en: Performance metric
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能指标
- en: 'Measuring the performance of a multimodal modular RAG requires two types of
    measurements: text and image. Measuring text is straightforward. However, measuring
    images is quite a challenge. Analyzing the image of a multimodal response is quite
    different. We extracted a keyword from the multimodal query engine. We then parsed
    the response for a source image to display. However, we will need to build an
    innovative approach to evaluate the source image of the response. Let’s begin
    with the LLM performance.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 评估多模态模块化 RAG 的性能需要两种类型的测量：文本和图像。测量文本是直接的。然而，测量图像是一个相当大的挑战。分析多模态响应的图像相当不同。我们从多模态查询引擎中提取了一个关键词。然后，我们解析响应以显示源图像。然而，我们需要构建一种创新的方法来评估响应的源图像。让我们从
    LLM 性能开始。
- en: LLM performance metric
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 性能指标
- en: LlamaIndex seamlessly called an OpenAI model through its query engine, such
    as GPT-4, for example, and provided text content in its response. For text responses,
    we will use the same cosine similarity metric as in the *Evaluating the output
    with cosine similarity* section in *Chapter 2*, and the *Vector store index query
    engine* section in *Chapter 3*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: LlamaIndex 通过其查询引擎无缝调用了 OpenAI 模型，例如 GPT-4，并在其响应中提供了文本内容。对于文本响应，我们将使用与*第 2 章中*的*使用余弦相似度评估输出*部分以及*第
    3 章中*的*向量存储索引查询引擎*部分相同的余弦相似度指标。
- en: 'The evaluation function uses `sklearn` and `sentence_transformers` to evaluate
    the similarity between two texts—in this case, an input and an output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 评估函数使用 `sklearn` 和 `sentence_transformers` 来评估两段文本之间的相似性——在这种情况下，是一个输入和一个输出：
- en: '[PRE46]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can now calculate the similarity between our baseline user input and the
    initial LLM response obtained:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以计算我们的基线用户输入和初始 LLM 响应之间的相似性：
- en: '[PRE47]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output displays the user input, the text response, and the cosine similarity
    between the two texts:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了用户输入、文本响应以及两段文本之间的余弦相似度：
- en: '[PRE48]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The output is satisfactory. But we now need to design a way to measure the multimodal
    performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 输出令人满意。但现在我们需要设计一种方法来衡量多模态性能。
- en: Multimodal performance metric
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态性能指标
- en: To evaluate the image returned, we cannot simply rely on the labels in the dataset.
    For small datasets, we can manually check the image, but when a system scales,
    automation is required. In this section, we will use the computer vision features
    of GPT-4o to analyze an image, parse it to find the objects we are looking for,
    and provide a description of that image. Then, we will apply cosine similarity
    to the description provided by GPT-4o and the label it is supposed to contain.
    GPT-4o is a multimodal generative AI model.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估返回的图像，我们不能仅仅依赖于数据集中的标签。对于小型数据集，我们可以手动检查图像，但当系统扩展时，就需要自动化。在本节中，我们将使用 GPT-4o
    的计算机视觉功能来分析图像，解析它以找到我们正在寻找的对象，并描述该图像。然后，我们将对 GPT-4o 提供的描述和它应该包含的标签应用余弦相似度。GPT-4o
    是一个多模态生成 AI 模型。
- en: Let’s first encode the image to simplify data transmission to GPT-4o. Base64
    encoding converts binary data (like images) into ASCII characters, which are standard
    text characters. This transformation is crucial because it ensures that the image
    data can be transmitted over protocols (like HTTP) that are designed to handle
    text data smoothly. It also avoids issues related to binary data transmission,
    such as data corruption or interpretation errors.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先对图像进行编码，以简化数据传输到 GPT-4o。Base64 编码将二进制数据（如图像）转换为 ASCII 字符，这些字符是标准文本字符。这种转换至关重要，因为它确保图像数据可以通过设计用于平滑处理文本数据的协议（如
    HTTP）进行传输。它还避免了与二进制数据传输相关的问题，例如数据损坏或解释错误。
- en: 'The program encodes the source image using Python’s `base64` module:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 程序使用 Python 的 `base64` 模块对源图像进行编码：
- en: '[PRE49]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We now create an OpenAI client and set the model to `gpt-4o`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在创建一个 OpenAI 客户端并将模型设置为 `gpt-4o`：
- en: '[PRE50]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The unique word will be the result of the LLM query to the multimodal dataset
    we obtained by parsing the response:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一单词将是 LLM 查询到我们通过解析响应获得的多元数据集的结果：
- en: '[PRE51]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now submit the image to OpenAI GPT-4o:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将图像提交给 OpenAI GPT-4o：
- en: '[PRE52]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We instructed the `system` and `user` roles to analyze images looking for our
    target label, `u_word`—in this case, `truck`. We then submitted the source node
    image to the model. The output that describes the image is satisfactory:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指示 `system` 和 `user` 角色分析图像以寻找我们的目标标签 `u_word`——在这种情况下，`truck`。然后我们将源节点图像提交给模型。描述图像的输出是令人满意的：
- en: '[PRE53]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can now submit this response to the cosine similarity function by first
    adding an `"s"` to align with multiple trucks in a response:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过先添加一个 `"s"` 来与多个卡车在响应中进行对齐，然后将此响应提交给余弦相似度函数：
- en: '[PRE54]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output describes the image well but contains many other descriptions beyond
    the word “`truck`,” which limits its similarity to the input requested:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 输出很好地描述了图像，但除了单词“`truck`”之外，还包含许多其他描述，这限制了其与请求输入的相似性：
- en: '[PRE55]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: A human observer might approve the image and the LLM response. However, even
    if the score was very high, the issue would be the same. Complex images are challenging
    to analyze in detail and with precision, although progress is continually made.
    Let’s now calculate the overall performance of the system.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人类观察者可能会批准图像和 LLM 响应。然而，即使得分非常高，问题仍然相同。复杂图像在详细和精确分析方面具有挑战性，尽管不断取得进展。让我们现在计算系统的整体性能。
- en: Multimodal modular RAG performance metric
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态模块化 RAG 性能指标
- en: 'To obtain the overall performance of the system, we will divide the sum of
    the LLM response and the two multimodal response performances by `2`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得系统的整体性能，我们将 LLM 响应和两个多模态响应性能的总和除以 `2`：
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The result shows that although a human who observes the results may be satisfied,
    it remains difficult to automatically assess the relevance of a complex image:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，尽管观察结果的观察者可能感到满意，但自动评估复杂图像的相关性仍然很困难：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The metric can be improved because a human observer sees that the image is relevant.
    This explains why the top AI agents, such as ChatGPT, Gemini, and Bing Copilot,
    always have a feedback process that includes thumbs up and thumbs down.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标可以改进，因为人类观察者看到图像是相关的。这解释了为什么顶级 AI 代理，如 ChatGPT、Gemini 和 Bing Copilot，总是有一个包括点赞和踩的反馈过程。
- en: Let’s now sum up the chapter and gear up to explore how RAG can be improved
    even further with human feedback.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在总结本章内容，并准备好进一步探索如何通过人类反馈来进一步提高 RAG 的性能。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced us to the world of multimodal modular RAG, which uses
    distinct modules for different data types (text and image) and tasks. We leveraged
    the functionality of LlamaIndex, Deep Lake, and OpenAI, which we explored in the
    previous chapters. The Deep Lake VisDrone dataset further introduced us to drone
    technology for analyzing images and identifying objects. The dataset contained
    images, labels, and bounding box information. Working on drone technology involves
    multimodal data, encouraging us to develop skills that we can use across many
    domains, such as wildlife tracking, streamlining commercial deliveries, and making
    safer infrastructure inspections.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带我们进入了多模态模块化RAG的世界，它使用不同的模块来处理不同的数据类型（文本和图像）和任务。我们利用了LlamaIndex、Deep Lake和OpenAI的功能，这些我们在前面的章节中进行了探索。Deep
    Lake VisDrone数据集进一步介绍了我们如何使用无人机技术来分析图像和识别物体。该数据集包含图像、标签和边界框信息。在无人机技术方面的工作涉及多模态数据，这鼓励我们开发可以在许多领域使用的技能，例如野生动物追踪、简化商业配送和进行更安全的基础设施检查。
- en: We built a multimodal modular RAG-driven generative AI system. The first step
    was to define a baseline user query for both LLM and multimodal queries. We began
    by querying the Deep Lake textual dataset that we implemented in *Chapter 3*.
    LlamaIndex seamlessly ran a query engine to retrieve, augment, and generate a
    response. Then, we loaded the Deep Lake VisDrone dataset and indexed it in memory
    with LlamaIndex to create an indexed vector search retrieval pipeline. We queried
    it through LlamaIndex, which used an OpenAI model such as GPT-4 and parsed the
    text generated for a keyword. Finally, we searched the source nodes of the response
    to find the source image, display it, and merge the LLM and image responses into
    an augmented output. We applied cosine similarity to the text response. Evaluating
    the image was challenging, so we first ran image recognition with GPT-4o on the
    image retrieved to obtain a text to which we applied cosine similarity.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个多模态模块化RAG驱动的生成式AI系统。第一步是为LLM和多模态查询定义一个基线用户查询。我们首先查询了我们在*第3章*中实现的Deep
    Lake文本数据集。LlamaIndex无缝运行查询引擎以检索、增强和生成响应。然后，我们加载了Deep Lake VisDrone数据集，并使用LlamaIndex将其在内存中索引以创建索引向量搜索检索管道。我们通过LlamaIndex查询它，它使用OpenAI模型（如GPT-4）解析为关键词生成的文本。最后，我们搜索响应的源节点以找到源图像，显示它，并将LLM和图像响应合并为增强输出。我们对文本响应应用余弦相似度。评估图像具有挑战性，所以我们首先在检索到的图像上运行GPT-4o进行图像识别，以获得应用于余弦相似度的文本。
- en: The journey into multimodal modular RAG-driven generative AI took us deep into
    the cutting edge of AI. Building a complex system was good preparation for real-life
    AI projects, which often require implementing multisource, multimodal, and unstructured
    data, leading to modular, complex systems. Thanks to transparent access to the
    source of a response, the complexity of RAG can be harnessed, controlled, and
    improved. We will see how we can leverage the transparency of the sources of a
    response to introduce human feedback to improve AI. The next chapter will take
    us further into transparency and precision in AI.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模块化RAG驱动的生成式AI之旅使我们深入到AI的前沿。构建一个复杂的系统是现实生活中的AI项目的良好准备，这些项目通常需要实现多源、多模态和非结构化数据，从而导致模块化和复杂的系统。得益于对响应源的透明访问，可以利用、控制和改进RAG的复杂性。我们将看到如何利用响应源的透明度来引入人类反馈以改进AI。下一章将带我们进一步探讨AI中的透明度和精确度。
- en: Questions
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions with *Yes* or *No*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 用*是*或*否*回答以下问题：
- en: Does multimodal modular RAG handle different types of data, such as text and
    images?
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多模态模块化RAG是否处理不同类型的数据，如文本和图像？
- en: Are drones used solely for agricultural monitoring and aerial photography?
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无人机是否仅用于农业监测和航空摄影？
- en: Is the Deep Lake VisDrone dataset used in this chapter for textual data only?
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中使用的Deep Lake VisDrone数据集是否仅用于文本数据？
- en: Can bounding boxes be added to drone images to identify objects such as trucks
    and pedestrians?
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是否可以在无人机图像中添加边界框以识别如卡车和行人等物体？
- en: Does the modular system retrieve both text and image data for query responses?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模块化系统是否为查询响应检索文本和图像数据？
- en: Is building a vector index necessary for querying the multimodal VisDrone dataset?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建向量索引是否对于查询多模态VisDrone数据集是必要的？
- en: Are the retrieved images processed without adding any labels or bounding boxes?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索的图像是否在添加任何标签或边界框的情况下进行处理？
- en: Is the multimodal modular RAG performance metric based only on textual responses?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多模态模块化RAG的性能指标是否仅基于文本响应？
- en: Can a multimodal system such as the one described in this chapter handle only
    drone-related data?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章所述的多模态系统是否只能处理与无人机相关的数据？
- en: Is evaluating images as easy as evaluating text in multimodal RAG?
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多模态 RAG 中评估图像是否像评估文本一样容易？
- en: References
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'LlamaIndex: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LlamaIndex: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)'
- en: 'Activeloop Deep Lake: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Activeloop Deep Lake: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
- en: 'OpenAI: [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI: [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)'
- en: Further reading
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Retrieval-Augmented Multimodal Language Modeling, Yasunaga et al. (2023), [https://arxiv.org/pdf/2211.12561](https://arxiv.org/pdf/2211.12561)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Retrieval-Augmented Multimodal Language Modeling, Yasunaga 等人 (2023), [https://arxiv.org/pdf/2211.12561](https://arxiv.org/pdf/2211.12561)
- en: Join our community on Discord
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code50409000288080484.png)'
