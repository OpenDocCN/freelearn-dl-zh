- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Getting Started with Next-Generation Artificial Intelligence through Reinforcement
    Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过强化学习开始学习下一代人工智能
- en: Next-generation AI compels us to realize that machines do indeed think. Although
    machines do not think like us, their thought process has proven its efficiency
    in many areas. In the past, the belief was that AI would reproduce human thinking
    processes. Only neuromorphic computing (see *Chapter 18*, *Neuromorphic Computing*),
    remains set on this goal. Most AI has now gone beyond the way humans think, as
    we will see in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下一代人工智能迫使我们意识到机器确实会思考。虽然机器的思维方式与我们不同，但它们的思维过程已经在许多领域证明了其效率。过去，人们认为人工智能会复制人类的思维过程。只有神经形态计算（见*第18章*，*神经形态计算*）仍然致力于这个目标。大多数人工智能现在已经超越了人类的思维方式，正如我们将在本章中看到的那样。
- en: The **Markov decision process** (**MDP**), a **reinforcement learning** (**RL**)
    algorithm, perfectly illustrates how machines have become intelligent in their
    own unique way. Humans build their decision process on experience. MDPs are memoryless.
    Humans use logic and reasoning to think problems through. MDPs apply random decisions
    100% of the time. Humans think in words, labeling everything they perceive. MDPs
    have an unsupervised approach that uses no labels or training data. MDPs boost
    the machine thought process of self-driving cars (SDCs), translation tools, scheduling
    software, and more. This memoryless, random, and unlabeled machine thought process
    marks a historical change in the way a former human problem was solved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**），一种**强化学习**（**RL**）算法，完美地说明了机器是如何以其独特的方式变得智能的。人类根据经验构建决策过程。MDP是无记忆的。人类通过逻辑和推理思考问题。MDP则100%时间使用随机决策。人类用语言思考，把他们感知到的每个事物都打上标签。MDP采用无监督方法，不使用标签或训练数据。MDP推动了自动驾驶汽车（SDC）、翻译工具、调度软件等机器思维过程的发展。这种无记忆、随机且无标签的机器思维过程标志着解决过去人类问题方式的历史性变化。'
- en: 'With this realization comes a yet more mind-blowing fact. AI algorithms and
    hybrid solutions built on IoT, for example, have begun to surpass humans in strategic
    areas. Although AI cannot replace humans in every field, AI combined with classical
    automation now occupies key domains: banking, marketing, supply chain management,
    scheduling, and many other critical areas.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这一认识的到来，还有一个更令人大开眼界的事实。以物联网为基础的人工智能算法和混合解决方案已经开始在战略领域超越人类。尽管人工智能无法在每个领域取代人类，但人工智能与传统自动化结合，现在已占据了关键领域：银行、市场营销、供应链管理、调度以及许多其他关键领域。
- en: As you will see, starting with this chapter, you can occupy a central role in
    this new world as an adaptive thinker. You can design AI solutions and implement
    them. There is no time to waste. In this chapter, we are going to dive quickly
    and directly into reinforcement learning through the MDP.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将看到的，从本章开始，你可以作为一个适应性思维者，在这个新世界中占据中心地位。你可以设计人工智能解决方案并实施它们。没有时间浪费。在本章中，我们将迅速直接地通过MDP深入了解强化学习。
- en: Today, AI is essentially mathematics translated into source code, which makes
    it difficult to learn for traditional developers. However, we will tackle this
    approach pragmatically.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，人工智能本质上是数学被转化为源代码，这使得传统开发人员很难学习。然而，我们将以务实的态度来处理这种方法。
- en: The goal here is not to take the easy route. We're striving to break complexity
    into understandable parts and confront them with reality. You are going to find
    out right from the outset how to apply an adaptive thinker's process that will
    lead you from an idea to a solution in reinforcement learning, and right into
    the center of gravity of the next generation of AI.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标不是走捷径。我们努力将复杂性分解成可理解的部分，并与现实进行对比。你将从一开始就了解到如何应用适应性思维者的过程，这将引导你从一个想法到强化学习中的解决方案，并直接进入下一代人工智能的重心。
- en: Reinforcement learning concepts
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习概念
- en: 'AI is constantly evolving. The classical approach states that:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能在不断发展。经典方法表明：
- en: AI covers all domains
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能涵盖所有领域
- en: Machine learning is a subset of AI, with clustering, classification, regression,
    and reinforcement learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是人工智能的一个子集，包括聚类、分类、回归和强化学习。
- en: Deep learning is a subset of machine learning that involves neural networks
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，涉及神经网络
- en: However, these domains often overlap and it's difficult to fit neuromorphic
    computing, for example, with its sub-symbolic approach, into these categories
    (see *Chapter 18*, *Neuromorphic Computing*).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些领域往往存在重叠，因此很难将神经形态计算，例如其子符号方法，纳入这些类别（见*第18章*，*神经形态计算*）。
- en: 'In this chapter, RL clearly fits into machine learning. Let''s have a brief
    look into the scientific foundations of the MDP, the RL algorithm we are going
    to explore. The main concepts to keep in mind are the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，强化学习（RL）显然适用于机器学习。我们将简要了解MDP（马尔科夫决策过程）的科学基础，这是我们将要探索的强化学习算法。需要牢记的主要概念如下：
- en: '**Optimal transport**: In 1781, Gaspard Monge defined transport optimizing
    from one location to another using the shortest and most cost-effective path;
    for example, mining coal and then using the most cost-effective path to a factory.
    This was subsequently generalized to any form of path from point A to point B.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最优运输**：1781年，贾斯帕·蒙奇定义了从一个位置到另一个位置的运输优化，使用最短和最具成本效益的路径；例如，开采煤矿，然后用最具成本效益的路径运送到工厂。此后，这一理论被推广到从A点到B点的任何路径形式。'
- en: '**Boltzmann equation and constant**: In the late 19th century, Ludwig Boltzmann
    changed our vision of the world with his probabilistic distribution of particles
    beautifully summed up in his entropy formula:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玻尔兹曼方程和常数**：19世纪末，路德维希·玻尔兹曼通过他的粒子概率分布彻底改变了我们对世界的认知，他的熵公式完美地总结了这一点：'
- en: '*S* = *k* * log *W*'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* = *k* * log *W*'
- en: '*S* represents the entropy (energy, disorder) of a system expressed. *k* is the Boltzmann
    constant, and *W* represents the number of microstates. We will explore Boltzmann''s
    ideas further in *Chapter 14*, *Preparing the Input of Chatbots with Restricted
    Boltzmann Machines (RBMs) and Principal Component Analysis (PCA)*.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*S* 代表系统的熵（能量、无序）。*k* 是玻尔兹曼常数，*W* 代表微观状态的数量。我们将在*第14章*，*使用限制玻尔兹曼机（RBMs）和主成分分析（PCA）准备聊天机器人的输入*中进一步探讨玻尔兹曼的思想。'
- en: '**Probabilistic distributions advanced further**: Josiah Willard Gibbs took
    the probabilistic distributions of large numbers of particles a step further.
    At that point, probabilistic information theory was advancing quickly. At the
    turn of the 19th century, Andrey Markov applied probabilistic algorithms to language,
    among other areas. A modern era of information theory was born.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率分布的进一步发展**：乔西亚·威拉德·吉布斯将大量粒子的概率分布推进了一步。此时，概率信息理论正在迅速发展。在19世纪末，安德烈·马尔可夫将概率算法应用于语言等多个领域，信息理论的现代时代由此诞生。'
- en: '**When Boltzmann and optimal transport meet**: 2011 Fields Medal winner, Cédric
    Villani, brought Boltzmann''s equation to yet another level. Villani then went
    on to unify optimal transport and Boltzmann. Cédric Villani proved something that
    was somewhat intuitively known to 19th century mathematicians but required proof.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当玻尔兹曼与最优运输相遇**：2011年菲尔兹奖得主塞德里克·维拉尼将玻尔兹曼方程提升到了一个新的层次。维拉尼随后将最优运输与玻尔兹曼理论结合起来。他证明了一个19世纪数学家直觉上已知但需要证明的命题。'
- en: Let's take all of the preceding concepts and materialize them in a real-world
    example that will explain why reinforcement learning using the MDP, for example,
    is so innovative.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把前面提到的所有概念在现实世界的一个例子中具象化，这将解释为什么使用MDP的强化学习，例如，是如此创新。
- en: 'Analyzing the following cup of tea will take you right into the next generation
    of AI:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 分析以下这杯茶将带你进入下一代人工智能的世界：
- en: '![](img/B15438_01_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_01_01.png)'
- en: 'Figure 1.1: Consider a cup of tea'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：考虑一杯茶
- en: 'You can look at this cup of tea in two different ways:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从两种不同的角度看待这杯茶：
- en: '**Macrostates**: You look at the cup and content. You can see the volume of
    tea in the cup and you could feel the temperature when holding the cup in your hand.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**宏观状态**：你看着这杯茶和茶水。你可以看到杯中茶的体积，并且当你拿着杯子时可以感觉到温度。'
- en: '**Microstates**: But can you tell how many molecules are in the tea, which
    ones are hot, warm, or cold, their velocity and directions? Impossible right?'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微观状态**：但你能说出茶水中的分子有多少吗？哪些是热的，温暖的，还是冷的？它们的速度和方向如何？这几乎不可能对吧？'
- en: Now, imagine, the tea contains 2,000,000,000+ Facebook accounts, or 100,000,000+
    Amazon Prime users with millions of deliveries per year. At this level, we simply
    abandon the idea of controlling every item. We work on trends and probabilities.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，这杯茶包含了超过20亿个Facebook账户，或者超过1亿个亚马逊Prime用户，每年进行数百万次配送。在这个层面上，我们根本放弃了控制每个单独项目的想法。我们着眼于趋势和概率。
- en: Boltzmann provides a probabilistic approach to the evaluation of the features
    of our real world. Materializing Boltzmann in logistics through optimal transport
    means that the temperature could be the ranking of a product, the velocity can
    be linked to the distance to delivery, and the direction could be the itineraries
    we will study in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 玻尔兹曼提供了一种通过最优传输评估我们现实世界特征的概率方法。通过最优传输在物流中的应用，玻尔兹曼意味着温度可以是产品的排名，速度可以与交货距离相关，方向则是我们将在本章中研究的路径。
- en: Markov picked up the ripe fruits of microstate probabilistic descriptions and
    applied it to his MDP. Reinforcement learning takes the huge volume of elements
    (particles in a cup of tea, delivery locations, social network accounts) and defines
    the probable paths they take.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫采纳了微观状态概率描述的成熟成果，并将其应用到他的 MDP 中。强化学习将大量元素（如茶杯中的粒子、交货地点、社交网络账户）进行定义，并确定它们可能的路径。
- en: The turning point of human thought occurred when we simply could not analyze
    the state and path of the huge volumes facing our globalized world, which generates
    images, sounds, words, and numbers that exceed traditional software approaches.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 人类思维的转折点出现在我们无法分析全球化世界面临的庞大数据状态和路径时，这些数据产生的图像、声音、文字和数字超过了传统软件方法的处理能力。
- en: With this in mind, we can start exploring the MDP.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个前提，我们可以开始探索 MDP。
- en: How to adapt to machine thinking and become an adaptive thinker
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何适应机器思维并成为一个自适应思维者
- en: Reinforcement learning, one of the foundations of machine learning, supposes
    learning through trial and error by interacting with an environment. This sounds
    familiar, doesn't it? That is what we humans do all our lives—in pain! Try things,
    evaluate, and then continue; or try something else.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习，作为机器学习的基础之一，假设通过与环境的互动，通过试错进行学习。这听起来很熟悉，不是吗？这正是我们人类一生所做的——痛苦中不断尝试！做事情、评估，然后继续；或者尝试别的东西。
- en: In real life, you are the agent of your thought process. In reinforcement learning,
    the agent is the function calculating randomly through this trial-and-error process.
    This thought process function in machine learning is the MDP agent. This form
    of empirical learning is sometimes called Q-learning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，你是你思维过程的代理。在强化学习中，代理是通过这种试错过程随机计算的函数。机器学习中的这种思维过程函数就是 MDP 代理。这种经验学习的形式有时称为
    Q 学习。
- en: Mastering the theory and implementation of an MDP through a three-step method
    is a prerequisite.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过三步法掌握马尔可夫决策过程（MDP）的理论和实施是前提。
- en: 'This chapter will detail the three-step approach that will turn you into an
    AI expert, in general terms:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍将你转变为 AI 专家的三步法，概括来说：
- en: Starting by describing a problem to solve with real-life cases
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过描述解决实际问题的案例来开始
- en: Then, building a mathematical model that considers real-life limitations
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，建立一个考虑现实生活限制的数学模型
- en: Then, writing source code or using a cloud platform solution
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，编写源代码或使用云平台解决方案
- en: This is a way for you to approach any project with an adaptive attitude from
    the outset. This shows that a human will always be at the center of AI by explaining
    how we can build the inputs, run an algorithm, and use the results of our code.
    Let's consider this three-step process and put it into action.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是让你从一开始就以自适应的态度接近任何项目的一种方式。它表明，通过解释我们如何构建输入、运行算法并使用代码结果，AI 始终以人为中心。让我们考虑这三步法并付诸实践。
- en: Overcoming real-life issues using the three-step approach
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用三步法克服现实生活中的问题
- en: The key point of this chapter is to avoid writing code that will never be used.
    First, begin by understanding the subject as a subject matter expert. Then, write
    the analysis with words and mathematics to make sure your reasoning reflects the
    subject and, most of all, that the program will make sense in real life. Finally,
    in step 3, only write the code when you are sure about the whole project.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的关键是避免编写永远不会被使用的代码。首先，作为领域专家开始理解主题。然后，用文字和数学进行分析，确保你的推理符合主题，最重要的是，程序在现实生活中是有意义的。最后，在第三步，只有在你对整个项目有把握时才编写代码。
- en: Too many developers start writing code without stopping to think about how the
    results of that code are going to manifest themselves within real-life situations.
    You could spend weeks developing the perfect code for a problem, only to find
    out that an external factor has rendered your solution useless. For instance,
    what if you coded a solar-powered robot to clear snow from the yard, only to discover
    that during winter, there isn't enough sunlight to power the robot!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 太多开发者在编写代码时没有停下来思考代码的结果如何在实际生活中体现出来。你可能花费数周的时间开发一个完美的解决方案，结果却发现一个外部因素使你的方案变得毫无用处。例如，如果你编写了一个太阳能驱动的机器人用来清理院子里的雪，但却发现冬季阳光不足以为机器人提供动力，这怎么办？
- en: 'In this chapter, we are going to tackle the MDP (Q function) and apply it to
    reinforcement learning with the Bellman equation. We are going to approach it
    a little differently to most, however. We''ll be thinking about practical application,
    not simply code execution. You can find tons of source code and examples on the
    web. The problem is, much like our snow robot, such source code rarely considers
    the complications that come about in real-life situations. Let''s say you find
    a program that finds the optimal path for a drone delivery. There''s an issue,
    though; it has many limits that need to be overcome due to the fact that the code
    has not been written with real-life practicality in mind. You, as an adaptive
    thinker, are going to ask some questions:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论MDP（Q函数），并将其应用于强化学习中的贝尔曼方程。不过，我们将以与大多数人不同的方式来接近这个问题。我们将关注实际应用，而不仅仅是代码执行。网上有大量的源代码和示例，但问题是，就像我们的雪地机器人一样，这些源代码很少考虑到现实生活中的复杂情况。假设你找到一个程序，可以为无人机配送找到最优路径。然而，问题是，它有很多限制需要克服，因为代码并没有考虑到现实生活中的实用性。你作为一个适应性思维者，将会提出一些问题：
- en: What if there are 5,000 drones over a major city at the same time? What happens
    if they try to move in straight lines and bump into each other?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在一个大城市上空有5000架无人机同时飞行怎么办？如果它们试图直线飞行并碰撞到一起，会发生什么？
- en: Is a drone-jam legal? What about the noise over the city? What about tourism?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无人机干扰合法吗？城市上空的噪音呢？旅游业怎么办？
- en: What about the weather? Weather forecasts are difficult to make, so how is this
    scheduled?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气怎么样？天气预报很难做，那这个计划是怎么安排的？
- en: How can we resolve the problem of coordinating the use of charging and parking
    stations?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何解决协调充电和停车站使用的问题？
- en: In just a few minutes, you will be at the center of attention among theoreticians
    who know more than you, on one hand, and angry managers who want solutions they
    cannot get on the other. Your real-life approach will solve these problems. To
    do that, you must take the following three steps into account, starting with really
    getting involved in the real-life subject.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 只需几分钟，你就会成为那些比你更懂理论的学者和那些需要解决方案却得不到的愤怒经理们之间的焦点。你的现实生活方法将解决这些问题。为了做到这一点，你必须考虑以下三个步骤，首先要真正参与到现实生活的主题中。
- en: 'In order to successfully implement our real-life approach, comprised of the
    three steps outlined in the previous section, there are a few prerequisites:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功实施我们现实生活中的方法，按照前一节中概述的三步骤，有几个先决条件：
- en: '**Be a subject matter expert (SME)**: First, you have to be an SME. If a theoretician
    geek comes up with a hundred TensorFlow functions to solve a drone trajectory
    problem, you now know it is going to be a tough ride in which real-life parameters
    are constraining the algorithm. An SME knows the subject and thus can quickly
    identify the critical factors of a given field. AI often requires finding a solution
    to a complex problem that even an expert in a given field cannot express mathematically.
    Machine learning sometimes means finding a solution to a problem that humans do
    not know how to explain. Deep learning, involving complex networks, solves even
    more difficult problems.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成为主题专家（SME）**：首先，你必须成为主题专家。如果一个理论极客提出了一百个TensorFlow函数来解决无人机轨迹问题，你现在就知道，这将是一次艰难的旅程，现实生活中的参数将限制算法的效果。SME了解该主题，因此可以快速识别某一领域的关键因素。人工智能通常需要找到一个复杂问题的解决方案，甚至是该领域的专家也无法用数学表达出来。机器学习有时意味着找到一个问题的解决方案，而人类甚至不知道如何解释它。深度学习，涉及复杂的网络，解决了更为困难的问题。'
- en: '**Have enough mathematical knowledge to understand AI concepts**: Once you
    have the proper natural language analysis, you need to build your abstract representation
    quickly. The best way is to look around and find an everyday life example and
    make a mathematical model of it. Mathematics is not an option in AI, but a prerequisite.
    The effort is worthwhile. Then, you can start writing a solid piece of source
    code or start implementing a cloud platform ML solution.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具备足够的数学知识来理解AI概念**：一旦你有了正确的自然语言分析，你需要迅速构建你的抽象表示。最好的方法是环顾四周，找到一个日常生活中的例子，并为其建立一个数学模型。在AI中，数学不是可选项，而是前提条件。付出的努力是值得的。然后，你可以开始编写一段扎实的源代码或开始实施一个云平台的机器学习解决方案。'
- en: '**Know what source code is about as well as its potential and limits**: MDP
    is an excellent way to go and start working on the three dimensions that will
    make you adaptive: describing what is around you in detail in words, translating
    that into mathematical representations, and then implementing the result in your
    source code.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解源代码的内容及其潜力与局限性**：MDP是一种极好的方法，可以开始在三个维度上工作，使你具有适应性：用文字详细描述你周围的事物，将其转化为数学表示，然后将结果实现到你的源代码中。'
- en: With those prerequisites in mind, let's look at how you can become a problem-solving
    AI expert by following our practical three-step process. Unsurprisingly, we'll begin
    at step 1.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些前提条件，让我们看看如何通过遵循我们实际的三步法成为一个问题解决型AI专家。不出所料，我们将从第一步开始。
- en: 'Step 1 – describing a problem to solve: MDP in natural language'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步——描述要解决的问题：MDP的自然语言表达
- en: Step 1 of any AI problem is to go as far as you can to understand the subject
    you are asked to represent. If it's a medical subject, don't just look at data;
    go to a hospital or a research center. If it's a private security application,
    go to the places where they will need to use it. If it's for social media, make
    sure to talk to many users directly. The key concept to bear in mind is that you
    have to get a "feel" for the subject, as if you were the real "user."
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 任何AI问题的第一步是尽可能深入理解你要表示的主题。如果是医学主题，不要仅仅看数据；去医院或研究中心。如果是私人安全应用，去那些需要使用它的地方。如果是社交媒体，确保直接与许多用户交谈。需要记住的关键概念是，你必须对该主题有一个“感觉”，就像你是真正的“用户”一样。
- en: For example, transpose it into something you know in your everyday life (work
    or personal), something you are an SME in. If you have a driver's license, then
    you are an SME of driving. You are certified. This is a fairly common certification,
    so let's use this as our subject matter in the example that will follow. If you
    do not have a driver's license or never drive, you can easily replace moving around
    in a car by imagining you are moving around on foot; you are an SME of getting
    from one place to another, regardless of what means of transport that might involve.
    However, bear in mind that a real-life project would involve additional technical
    aspects, such as traffic regulations for each country, so our imaginary SME does
    have its limits.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将它转化为你日常生活中所知道的某些事物（工作或个人生活），是你在其中的专家。如果你有驾驶执照，那么你是驾驶方面的专家。你已获得认证。这是一个相当常见的认证，因此我们将其作为接下来示例的主题。如果你没有驾驶执照或从未驾驶过，你可以通过想象自己是步行出行来轻松替代开车；你是从一个地方到另一个地方的专家，不论所使用的交通工具是什么。然而，请记住，实际项目会涉及其他技术方面，比如每个国家的交通法规，因此我们的假想专家是有限的。
- en: 'Getting into the example, let''s say you are an e-commerce business driver
    delivering a package in a location you are unfamiliar with. You are the operator
    of a self-driving vehicle. For the time being, you''re driving manually. You have
    a GPS with a nice color map on it. The locations around you are represented by
    the letters **A** to **F**, as shown in the simplified map in the following diagram.
    You are presently at **F**. Your goal is to reach location **C**. You are happy,
    listening to the radio. Everything is going smoothly, and it looks like you are
    going to be there on time. The following diagram represents the locations and
    routes that you can cover:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进入这个例子，假设你是一名电子商务业务员，正在一个你不熟悉的地方送货。你是一个自动驾驶车辆的操作员。目前，你正在手动驾驶。你有一个带有漂亮彩色地图的GPS。你周围的位置由字母**A**到**F**表示，如下图所示。你目前在**F**位置。你的目标是到达**C**位置。你很高兴，正在听广播。一切顺利，看起来你会准时到达。下图表示了你可以覆盖的地点和路线：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_01.png](img/B15438_01_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_01.png](img/B15438_01_02.png)'
- en: 'Figure 1.2: A diagram of delivery routes'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：配送路线图
- en: The guidance system's state indicates the complete path to reach **C**. It is
    telling you that you are going to go from **F** to **B** to **D**, and then to
    **C**. It looks good!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 导航系统的状态显示了到达**C**的完整路径。它告诉你，你将从**F**经过**B**到**D**，然后到达**C**。看起来不错！
- en: 'To break things down further, let''s say:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解析，我们可以假设：
- en: 'The present state is the letter *s*. *s* is a variable, not an actual state.
    It can be one of the locations in *L*, the set of locations:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前状态是字母 *s*。*s* 是一个变量，不是实际的状态。它可以是*L*集合中的任何位置，即：
- en: '*L* = {**A**, **B**, **C**, **D**, **E**, **F**}'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*L* = {**A**, **B**, **C**, **D**, **E**, **F**}'
- en: We say *present state* because there is no sequence in the learning process.
    The memoryless process goes from one present state to another. In the example
    in this chapter, the process starts at location **F**.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们之所以称之为*当前状态*，是因为在学习过程中没有序列。无记忆过程是从一个当前状态跳转到另一个当前状态。在本章的例子中，过程从位置**F**开始。
- en: Your next action is the letter *a* (action). This action *a* is not location
    **A**. The goal of this action is to take us to the next possible location in
    the graph. In this case, only **B** is possible. The goal of *a* is to take us
    from *s* (present state) to *s'* (new state).
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的下一个动作是字母 *a*（动作）。这个动作 *a* 不是位置**A**。这个动作的目标是将我们带到图中的下一个可能位置。在这种情况下，只有**B**是可能的。动作
    *a* 的目标是将我们从 *s*（当前状态）带到 *s'*（新状态）。
- en: The action *a* (not location **A**) is to go to location **B**. You look at
    your guidance system; it tells you there is no traffic, and that to go from your
    present state, **F**, to your next state, **B**, will take you only a few minutes.
    Let's say that the next state **B** is the letter **B**. This next state **B**
    is *s'*.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作 *a*（不是位置**A**）是前往位置**B**。你查看导航系统，它告诉你没有交通，且从你当前的状态**F**到下一个状态**B**只需几分钟。假设下一个状态**B**就是字母**B**。这个下一个状态**B**是
    *s'*。
- en: 'At this point, you are still quite happy, and we can sum up your situation
    with the following sequence of events:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你仍然相当满意，我们可以用以下事件序列总结你的情况：
- en: '*s*, *a*, *s''*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*s*, *a*, *s''*'
- en: The letter *s* is your present state, your present situation. The letter *a*
    is the action you're deciding, which is to go to the next location; there, you
    will be in another state, *s'*. We can say that thanks to the action *a*, you
    will go from *s* to *s'*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 字母 *s* 是你当前的状态，你的当前情况。字母 *a* 是你正在决定的动作，即前往下一个位置；在那里，你将处于另一个状态 *s'*。我们可以说，得益于动作
    *a*，你将从 *s* 转移到 *s'*。
- en: Now, imagine that the driver is not you anymore. You are tired for some reason.
    That is when a self-driving vehicle comes in handy. You set your car to autopilot.
    Now, you are no longer driving; the system is. Let's call that system the **agent**.
    At point **F**, you set your car to autopilot and let the self-driving agent take
    over.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设驾驶员不再是你。因为某些原因，你感到疲惫。这个时候，自驾车就派上了用场。你将车设置为自动驾驶。现在，你不再开车了；系统在操作。我们把这个系统叫做**代理**。在**F**点，你将车设置为自动驾驶，让自驾代理接管。
- en: Watching the MDP agent at work
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察MDP代理的工作
- en: The self-driving AI is now in charge of the vehicle. It is acting as the MDP
    agent. This now sees what you have asked it to do and checks its **mapping environment**,
    which represents all the locations in the previous diagram from **A** to **F**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自驾AI现在掌控着车辆。它充当着MDP代理。此时，它看到你要求它做的事情，并检查其**映射环境**，这个环境代表了之前图表中从**A**到**F**的所有位置。
- en: In the meantime, you are rightly worried. Is the agent going to make it or not?
    You are wondering whether its strategy meets yours. You have your **policy** *P*—your
    way of thinking—which is to take the shortest path possible. Will the agent agree?
    What's going on in its machine mind? You observe and begin to realize things you
    never noticed before.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，你当然感到担忧。代理是否能够成功完成任务？你在想它的策略是否与你的相符。你有自己的**策略** *P*——你的思维方式——那就是尽量走最短的路径。代理会同意吗？它的机器大脑里究竟在想什么？你观察着，开始意识到以前从未注意到的事情。
- en: Since this is the first time you are using this car and guidance system, the
    agent is **memoryless**, which is an MDP feature. The agent doesn't know anything
    about what went on before. It seems to be happy with just calculating from this
    state *s* at location **F**. It will use machine power to run as many calculations
    as necessary to reach its goal.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是你第一次使用这辆车和导航系统，智能体是**无记忆的**，这是MDP的一个特点。智能体不知道之前发生了什么。它似乎只对从**F**位置的状态*s*开始计算感到满意。它将使用机器能力进行尽可能多的计算，以实现其目标。
- en: Another thing you are watching is the total distance from **F** to **C** to
    check whether things are OK. That means that the agent is calculating all the
    states from **F** to **C**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在观察的另一件事是从**F**到**C**的总距离，以检查情况是否正常。这意味着智能体正在计算从**F**到**C**的所有状态。
- en: In this case, state **F** is state 1, which we can simplify by writing *s*[1];
    B is state 2, which we can simplify by writing *s*[2]; D is *s*[3]; and C is *s*[4].
    The agent is calculating all of these possible states to make a decision.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，状态**F**是状态1，我们可以简化为写作*s*[1]；B是状态2，我们可以简化为写作*s*[2]；D是*s*[3]；C是*s*[4]。智能体正在计算所有这些可能的状态以做出决策。
- en: The agent knows that when it reaches **D**, **C** will be better because the
    reward will be higher for going to C than anywhere else. Since it cannot eat a
    piece of cake to reward itself, the agent uses numbers. Our agent is a real number
    cruncher. When it is wrong, it gets a poor reward or nothing in this model. When
    it's right, it gets a reward represented by the letter *R*, which we'll encounter
    during step 2\. This action-value (reward) transition, often named the Q function,
    is the core of many reinforcement learning algorithms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体知道当它到达**D**时，**C**会更好，因为前往C的奖励比其他地方都高。由于它不能通过吃蛋糕来奖励自己，智能体使用数字。我们的智能体是一个真正的数字计算者。当它出错时，它在这个模型中会得到一个差的奖励或什么也得不到。当它做对时，它会得到由字母*R*表示的奖励，我们将在第2步中遇到。这种行动价值（奖励）转移，通常被称为Q函数，是许多强化学习算法的核心。
- en: When our agent goes from one state to another, it performs a *transition* and
    gets a reward. For example, the transition can be from **F** to **B**, state 1
    to state 2, or *s*[1] to *s*[2].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的智能体从一个状态转换到另一个状态时，它执行一次*转移*并获得奖励。例如，转移可以是从**F**到**B**，即从状态1到状态2，或者从*s*[1]到*s*[2]。
- en: You are feeling great and are going to be on time. You are beginning to understand
    how the machine learning agent in your self-driving car is thinking. Suddenly,
    you look up and see that a traffic jam is building up. Location **D** is still
    far away, and now you do not know whether it would be good to go from **D** to
    **C** or **D** to **E**, in order to take another road to **C**, which involves
    less traffic. You are going to see what your agent thinks!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你感觉很好，并且会准时到达。你开始理解自驾车中的机器学习智能体是如何思考的。突然，你抬头看到交通堵塞正在形成。**D**位置仍然很远，现在你不知道从**D**到**C**或从**D**到**E**哪个更好，因为后者可以走另一条交通较少的路到**C**。你准备看看智能体是怎么想的！
- en: The agent takes the traffic jam into account, is stubborn, and increases its
    reward to get to **C** by the shortest way. Its policy is to stick to the initial
    plan. You do not agree. You have another policy.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 该智能体考虑到交通堵塞，固执己见，并增加奖励以通过最短路径到达**C**。它的策略是坚持初始计划。你不同意，你有另一种策略。
- en: You stop the car. You both have to agree before continuing. You have your opinion
    and policy; the agent does not agree. Before continuing, your views need to **converge**.
    **Convergence** is the key to making sure that your calculations are correct,
    and it's a way to evaluate the quality of a calculation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你停下了车。你们都必须在继续之前达成一致。你有自己的观点和策略；智能体不同意。在继续之前，你们的看法需要**收敛**。**收敛**是确保计算正确的关键，它也是评估计算质量的一种方法。
- en: A mathematical representation is the best way to express this whole process
    at this point, which we will describe in the following step.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 数学表示是此时表达整个过程的最佳方式，我们将在接下来的步骤中描述。
- en: 'Step 2 – building a mathematical model: the mathematical representation of
    the Bellman equation and MDP'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步——构建数学模型：贝尔曼方程和马尔可夫决策过程（MDP）的数学表示
- en: Mathematics involves a whole change in your perspective of a problem. You are
    going from words to functions, the pillars of source coding.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数学涉及到你对问题的全新视角。你正在从文字转向函数，这是源编码的支柱。
- en: Expressing problems in mathematical notation does not mean getting lost in academic
    math to the point of never writing a single line of code. Just use mathematics
    to get a job done efficiently. Skipping mathematical representation will fast-track
    a few functions in the early stages of an AI project. However, when the real problems
    that occur in all AI projects surface, solving them with source code alone will
    prove virtually impossible. The goal here is to pick up enough mathematics to
    implement a solution in real-life companies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学符号表达问题并不意味着陷入学术数学的困境，甚至写不出一行代码。只需利用数学有效地完成工作。跳过数学表示可以在AI项目的初期阶段加速某些功能的实现。然而，当所有AI项目中出现的真正问题浮出水面时，单靠源代码解决问题几乎是不可能的。这里的目标是掌握足够的数学知识，以便在现实公司中实施解决方案。
- en: It is necessary to think through a problem by finding something familiar around
    us, such as the itinerary model covered early in this chapter. It is a good thing
    to write it down with some abstract letters and symbols as described before, with
    *a* meaning an action, and *s* meaning a state. Once you have understood the problem
    and expressed it clearly, you can proceed further.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 需要通过找到我们周围熟悉的事物来思考问题，例如本章早些时候提到的行程模型。用一些抽象的字母和符号来写下它是件好事，如前所述，其中 *a* 代表动作，*s*
    代表状态。一旦你理解了问题并清晰表达出来，就可以继续前进。
- en: Now, mathematics will help to clarify the situation by means of shorter descriptions.
    With the main ideas in mind, it is time to convert them into equations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数学将通过更简洁的描述帮助澄清情况。掌握了主要思想后，是时候将它们转化为方程式。
- en: From MDP to the Bellman equation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从MDP到贝尔曼方程
- en: In step 1, the agent went from **F**, or state 1 or *s*, to **B**, which was
    state 2 or *s'*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，智能体从 **F**（状态1或 *s*）转移到 **B**（状态2或 *s'*）。
- en: 'A strategy drove this decision—a policy represented by *P*. One mathematical
    expression contains the MDP state transition function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策背后有一个策略——由 *P* 表示的政策。一个数学表达式包含了MDP状态转移函数：
- en: '*P*[a](*s*, *s''*)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*[a](*s*, *s''*)'
- en: '*P* is the policy, the strategy made by the agent to go from **F** to **B**
    through action *a*. When going from **F** to **B**, this state transition is named
    the **state transition function**:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*P* 是策略，智能体通过动作 *a* 从 **F** 转移到 **B** 的策略。在从 **F** 到 **B** 的过程中，这一状态转移被称为 **状态转移函数**：'
- en: '*a* is the action'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是动作'
- en: '*s* is state 1 (**F**), and *s''* is state 2 (**B**)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* 是状态1（**F**），*s''* 是状态2（**B**）'
- en: 'The reward (right or wrong) matrix follows the same principle:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励（正确或错误）矩阵遵循相同的原则：
- en: '*R*[a](*s*, *s''*)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*[a](*s*, *s''*)'
- en: That means *R* is the reward for the action of going from state *s* to state
    *s'*. Going from one state to another will be a random process. Potentially, all
    states can go to any other state.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *R* 是从状态 *s* 转移到状态 *s'* 的奖励。状态之间的转移是一个随机过程。潜在地，所有状态都可以转移到其他任何状态。
- en: Each line in the matrix in the example represents a letter from **A** to **F**,
    and each column represents a letter from **A** to **F**. All possible states are
    represented. The `1` values represent the nodes (vertices) of the graph. Those
    are the possible locations. For example, line 1 represents the possible moves
    for letter **A**, line 2 for letter **B**, and line 6 for letter **F**. On the
    first line, **A** cannot go to **C** directly, so a `0` value is entered. But,
    it can go to **E**, so a `1` value is added.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 示例中的矩阵每一行表示从 **A** 到 **F** 的一个字母，每一列表示从 **A** 到 **F** 的一个字母。所有可能的状态都在其中表示。`1`
    值代表图的节点（顶点）。这些是可能的位置。例如，第1行表示字母 **A** 的可能移动，第2行表示字母 **B**，第6行表示字母 **F**。在第一行中，**A**
    不能直接转移到 **C**，所以输入 `0` 值。但它可以转移到 **E**，因此加上 `1` 值。
- en: Some models start with `-1` for impossible choices, such as **B** going directly
    to **C**, and `0` values to define the locations. This model starts with `0` and
    `1` values. It sometimes takes weeks to design functions that will create a reward
    matrix (see *Chapter 2*, *Building a Reward Matrix – Designing Your Datasets*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型从 `-1` 开始，表示不可能的选择，例如 **B** 不能直接转移到 **C**，而 `0` 值用于定义位置。这个模型从 `0` 和 `1`
    值开始。有时需要数周时间来设计能够创建奖励矩阵的功能（见 *第2章*，*构建奖励矩阵——设计你的数据集*）。
- en: 'The example we will be working on inputs a reward matrix so that the program
    can choose its best course of action. Then, the agent will go from state to state,
    learning the best trajectories for every possible starting location point. The
    goal of the MDP is to go to **C** (line 3, column 3 in the reward matrix), which
    has a starting value of 100 in the following Python code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要处理的例子输入一个奖励矩阵，这样程序就能选择最佳的行动路线。然后，智能体将从一个状态到另一个状态，学习每个可能的起始位置点的最佳轨迹。MDP的目标是到达**C**（奖励矩阵中的第3行，第3列），其初始值为100，如下Python代码所示：
- en: '[PRE0]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Somebody familiar with Python might wonder why I used `ql` instead of `np`.
    Some might say "convention," "mainstream," "standard." My answer is a question.
    Can somebody define what "standard" AI is in this fast-moving world! My point
    here for the MDP is to use `ql` as an abbreviation of "Q-learning" instead of
    the "standard" abbreviation of NumPy, which is `np`. Naturally, beyond this special
    abbreviation for the MDP programs, I'll use `np`. Just bear in mind that conventions
    are there to break so as to set ourselves free to explore new frontiers. Just
    make sure your program works well!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉Python的人可能会想，为什么我使用`ql`而不是`np`。有人可能会说“惯例”、“主流”、“标准”。我的回答是一个问题。能有人在这个快速发展的世界中定义什么是“标准”的AI吗！我在这里的观点是，对于MDP，我使用`ql`作为“Q-learning”的缩写，而不是NumPy的“标准”缩写`np`。当然，除了这个特殊的MDP程序缩写外，我会使用`np`。只是要记住，惯例是用来打破的，目的是让我们自由地去探索新的领域。只要确保你的程序运行良好！
- en: 'There are several key properties of this decision process, among which there
    is the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策过程有几个关键属性，其中包括以下内容：
- en: '**The Markov property**: The process does not take the past into account. It
    is the memoryless property of this decision process, just as you do in a car with
    a guidance system. You move forward to reach your goal.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫属性**：这个过程不考虑过去。它是这个决策过程的无记忆特性，就像你在汽车中使用导航系统时一样。你向前移动，以达到目标。'
- en: '**Unsupervised learning**: From this memoryless Markov property, it is safe
    to say that the MDP is not supervised learning. Supervised learning would mean
    that we would have all the labels of the reward matrix *R* and learn from them.
    We would know what **A** means and use that property to make a decision. We would,
    in the future, be looking at the past. MDP does not take these labels into account.
    Thus, MDP uses unsupervised learning to train. A decision has to be made in each
    state without knowing the past states or what they signify. It means that the
    car, for example, was on its own at each location, which is represented by each
    of its states.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：从这个无记忆的马尔可夫属性来看，可以安全地说，MDP不是监督学习。监督学习意味着我们将拥有奖励矩阵*R*的所有标签并从中学习。我们会知道**A**是什么意思，并利用这个属性做决策。我们将来会回顾过去。而MDP不考虑这些标签。因此，MDP使用无监督学习进行训练。在每个状态下必须做出决策，而不知道过去的状态或它们意味着什么。这意味着汽车在每个位置上都是独立的，每个状态代表了它的一个位置。'
- en: '**Stochastic process**: In step 1, when state **D** was reached, the agent
    controlling the mapping system and the driver didn''t agree on where to go. A
    random choice could be made in a trial-and-error way, just like a coin toss. It
    is going to be a heads-or-tails process. The agent will toss the coin a significant
    number of times and measure the outcomes. That''s precisely how MDP works and
    how the agent will learn.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机过程**：在步骤1中，当达到状态**D**时，控制映射系统的智能体和驾驶员在去向上没有达成一致。可以通过试错的方式做出随机选择，就像掷硬币一样。这将是一个正反两面的过程。智能体将掷硬币相当多次，并衡量结果。这正是MDP如何工作的，智能体就是通过这种方式学习的。'
- en: '**Reinforcement learning**: Repeating a trial-and-error process with feedback
    from the agent''s environment.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：通过反复的试错过程并获得来自环境的反馈。'
- en: '**Markov chain**: The process of going from state to state with no history
    in a random, stochastic way is called a Markov chain.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫链**：从一个状态到另一个状态的过程，且没有历史，以随机的、随机的方式进行，这叫做马尔可夫链。'
- en: 'To sum it up, we have three tools:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们有三种工具：
- en: '*P*[a](*s*, *s''*): A **policy**, *P*, or strategy to move from one state to
    another'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*[a](*s*, *s''*): 一个**策略**，*P*，或从一个状态到另一个状态的策略'
- en: '*T*[a](*s*, *s''*): A *T*, or stochastic (random) **transition**, function
    to carry out that action'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*T*[a](*s*, *s''*): 一个*T*，或随机（随机的）**转移**，执行该动作的函数'
- en: '*R*[a](*s*, *s''*): An *R*, or **reward**, for that action, which can be negative,
    null, or positive'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*[a](*s*, *s''*): 一个*R*，或该动作的**奖励**，可以是负的、零的或正的'
- en: '*T* is the transition function, which makes the agent decide to go from one
    point to another with a policy. In this case, it will be random. That''s what
    machine power is for, and that is how reinforcement learning is often implemented.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*T*是转移函数，它使得智能体依据策略从一个点转移到另一个点。在这个例子中，它是随机的。这就是机器能力的作用，也是强化学习通常实现的方式。'
- en: Randomness
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机性
- en: Randomness is a key property of MDP, defining it as a stochastic process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 随机性是MDP的一个关键属性，将其定义为一个随机过程。
- en: 'The following code describes the choice the **agent** is going to make:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码描述了**智能体**将做出的选择：
- en: '[PRE1]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code selects a new random action (state) at each episode.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在每一轮（episode）选择一个新的随机动作（状态）。
- en: The Bellman equation
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: The Bellman equation is the road to programming reinforcement learning.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程是编程强化学习的关键。
- en: 'The Bellman equation completes the MDP. To calculate the value of a state,
    let''s use *Q*, for the *Q* action-reward (or value) function. The pseudo source
    code of the Bellman equation can be expressed as follows for one individual state:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程完成了马尔可夫决策过程（MDP）。为了计算一个状态的值，我们使用*Q*，表示*Q*的动作-奖励（或值）函数。贝尔曼方程的伪源代码可以表示如下，针对单个状态：
- en: '![](img/B15438_01_001.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_01_001.png)'
- en: 'The source code then translates the equation into a machine representation,
    as in the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 源代码然后将方程转化为机器表示，如下代码所示：
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The source code variables of the Bellman equation are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 贝尔曼方程的源代码变量如下：
- en: '*Q*(*s*): This is the value calculated for this state—the total reward. In
    step 1, when the agent went from **F** to **B**, the reward was a number such
    as 50 or 100 to show the agent that it''s on the right track.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Q*(*s*)：这是为该状态计算的值——总奖励。在第1步，当智能体从**F**移动到**B**时，奖励是一个数值，如50或100，用来表明智能体走在正确的轨道上。'
- en: '*R*(*s*): This is the sum of the values up to that point. It''s the total reward
    at that point.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*(*s*)：这是直到该点为止的值的总和。它是该点的总奖励。'
- en: '![](img/B15438_01_002.png): This is here to remind us that trial and error
    has a price. We''re wasting time, money, and energy. Furthermore, we don''t even
    know whether the next step is right or wrong since we''re in a trial-and-error
    mode. **Gamma** is often set to 0.8\. What does that mean? Suppose you''re taking
    an exam. You study and study, but you don''t know the outcome. You might have
    80 out of 100 (0.8) chances of clearing it. That''s painful, but that''s life.
    The **gamma** penalty, or learning rate, makes the Bellman equation realistic
    and efficient.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15438_01_002.png)：这是提醒我们试错有代价。我们浪费了时间、金钱和精力。此外，我们甚至不知道下一步是对还是错，因为我们处于试错模式中。**Gamma**通常设置为0.8。那意味着什么呢？假设你在参加考试。你努力学习，但不知道结果如何。你可能有80%的机会（0.8）通过考试。这很痛苦，但这就是生活。**Gamma**惩罚或学习速率使得贝尔曼方程更加现实和高效。'
- en: 'max(*s''*): *s''* is one of the possible states that can be reached with *P*[a](*s*, *s''*);
    max is the highest value on the line of that state (location line in the reward matrix).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'max(*s''*): *s''*是可以通过*P*[a](*s*, *s''*)到达的一个可能状态；max是该状态（奖励矩阵中的位置行）上的最高值。'
- en: 'At this point, you have done two-thirds of the job: understanding the real-life
    (process) and representing it in basic mathematics. You''ve built the mathematical
    model that describes your learning process, and you can implement that solution
    in code. Now, you are ready to code!'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经完成了三分之二的工作：理解现实生活中的（过程）并将其转化为基础数学。你已经构建了描述你学习过程的数学模型，并可以在代码中实现这个解决方案。现在，你准备好编码了！
- en: 'Step 3 – writing source code: implementing the solution in Python'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 编写源代码：在Python中实现解决方案
- en: In step 1, a problem was described in natural language to be able to talk to
    experts and understand what was expected. In step 2, an essential mathematical
    bridge was built between natural language and source coding. Step 3 is the software
    implementation phase.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，用自然语言描述了一个问题，以便能够与专家交流并理解期望的内容。在第2步中，建立了自然语言和源代码之间的关键数学桥梁。第3步是软件实现阶段。
- en: When a problem comes up—and rest assured that one always does—it will be possible
    to go back over the mathematical bridge with the customer or company team, and
    even further back to the natural language process if necessary.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题出现时——放心，它总是会出现——你可以回过头来与客户或公司团队一起回顾数学桥梁，甚至在必要时回到自然语言过程。
- en: 'This method guarantees success for any project. The code in this chapter is
    in Python 3.x. It is a reinforcement learning program using the Q function with
    the following reward matrix:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法保证了任何项目的成功。本章中的代码是 Python 3.x 版本的。它是一个使用 Q 函数的强化学习程序，具有以下奖励矩阵：
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`R` is the reward matrix described in the mathematical analysis.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`R`是数学分析中描述的奖励矩阵。'
- en: '`Q` inherits the same structure as `R`, but all values are set to `0` since
    this is a learning matrix. It will progressively contain the results of the decision
    process. The `gamma` variable is a double reminder that the system is learning
    and that its decisions have only an 80% chance of being correct each time. As
    the following code shows, the system explores the possible actions during the
    process:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`Q`继承了与`R`相同的结构，但所有值都被设置为`0`，因为这是一个学习矩阵。它会逐步包含决策过程的结果。`gamma`变量是对系统学习的双重提醒，它的决策每次只有80%的概率是正确的。正如下面的代码所示，系统在过程中探索可能的动作：'
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The agent starts in state 1, for example. You can start wherever you want because
    it's a random process. Note that the process only takes values > 0 into account.
    They represent possible moves (decisions).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，代理从状态1开始。你可以从任何你想要的地方开始，因为这是一个随机过程。注意，过程只考虑大于0的值，它们代表可能的动作（决策）。
- en: 'The current state goes through an analysis process to find possible actions
    (next possible states). You will note that there is no algorithm in the traditional
    sense with many rules. It''s a pure random calculation, as the following `random.choice`
    function shows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当前状态会经过分析过程，以找出可能的动作（下一个可能的状态）。你会注意到，传统意义上的算法没有很多规则，它只是纯粹的随机计算，如下`random.choice`函数所示：
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now comes the core of the system containing the Bellman equation, translated
    into the following source code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是系统核心部分，包含了贝尔曼方程，已翻译为以下源代码：
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can see that the agent looks for the maximum value of the next possible
    state chosen at random.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，代理在随机选择的下一个可能状态中寻找最大值。
- en: The best way to understand this is to run the program in your Python environment
    and `print()` the intermediate values. I suggest that you open a spreadsheet and
    note the values. This will give you a clear view of the process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这一点的最佳方式是运行程序并在你的 Python 环境中使用`print()`输出中间值。我建议你打开一个电子表格并记录这些值，这样可以清晰地看到整个过程。
- en: 'The last part is simply about running the learning process 50,000 times, just
    to be sure that the system learns everything there is to find. During each iteration,
    the agent will detect its present state, choose a course of action, and update
    the Q function matrix:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分是简单地运行学习过程 50,000 次，仅仅是为了确保系统学会了所有可以学习的内容。在每次迭代中，代理将检测其当前状态，选择一个行动方案，并更新
    Q 函数矩阵：
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The process continues until the learning process is over. Then, the program
    will print the result in `Q` and the normed result. The normed result is the process
    of dividing all values by the sum of the values found. `print(Q/ql.max(Q)*100)`
    norms `Q` by dividing `Q` by `q1.max(Q)*100`. The result comes out as a normed
    percentage.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程将持续进行，直到学习过程结束。然后，程序将打印出`Q`的结果以及标准化结果。标准化结果是通过将所有值除以找到的值的总和来得到的。`print(Q/ql.max(Q)*100)`通过将`Q`除以`q1.max(Q)*100`来标准化`Q`，结果会以标准化的百分比形式输出。
- en: You can run the process with `mdp01.py`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`mdp01.py`来运行这个过程。
- en: The lessons of reinforcement learning
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的经验教训
- en: Unsupervised reinforcement machine learning, such as the MDP-driven Bellman
    equation, is toppling traditional decision-making software location by location.
    Memoryless reinforcement learning requires few to no business rules and, thus,
    doesn't require human knowledge to run.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督强化机器学习，例如基于 MDP 的贝尔曼方程，正在逐步颠覆传统的决策软件。无记忆强化学习几乎不需要业务规则，因此也不需要人类知识来运行。
- en: 'Being an adaptive next-generation AI thinker involves three prerequisites:
    the effort to be an SME, working on mathematical models to think like a machine,
    and understanding your source code''s potential and limits.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 成为一名自适应的下一代 AI 思维者涉及三个前提条件：努力成为领域专家，致力于数学模型来像机器一样思考，以及理解你的源代码的潜力和限制。
- en: 'Machine power and reinforcement learning teach us two important lessons:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 机器智能和强化学习教给我们两条重要的经验：
- en: '**Lesson 1**: Machine learning through reinforcement learning can beat human
    intelligence in many cases. No use fighting! The technology and solutions are
    already here in strategic domains.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**经验教训 1**：通过强化学习的机器学习在许多情况下可以超越人类智能。与其抗争，不如接受！在战略领域，这项技术和解决方案已经到来。'
- en: '**Lesson 2**: A machine has no emotions, but you do. And so do the people around
    you. Human emotions and teamwork are an essential asset. Become an SME for your
    team. Learn how to understand what they''re trying to say intuitively and make
    a mathematical representation of it for them. Your job will never go away, even
    if you''re setting up solutions that don''t require much development, such as
    AutoML. AutoML, or automated machine learning, automates many tasks. AutoML automates
    functions such as the dataset pipeline, hyperparameters, and more. Development
    is partially or totally suppressed. But you still have to make sure the whole
    system is well designed.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二课**：机器没有情感，但你有。而且你身边的人也有。人类的情感和团队合作是至关重要的资产。成为你团队的领域专家（SME）。学会如何直观地理解他们想要表达的意思，并为他们将其转化为数学表达形式。即使你正在设置不需要太多开发的解决方案，如AutoML，你的工作也永远不会消失。AutoML，或称自动化机器学习，自动化了许多任务。AutoML自动化了数据集管道、超参数等功能。开发工作在一定程度上被抑制或完全被替代。但你仍然需要确保整个系统的设计良好。'
- en: Reinforcement learning shows that no human can solve a problem the way a machine
    does. 50,000 iterations with random searching is not an option for a human. The
    number of empirical episodes can be reduced dramatically with a numerical convergence
    form of gradient descent (see *Chapter 3*, *Machine Intelligence – Evaluation
    Functions and Numerical Convergence*).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习表明，没有人类能够像机器那样解决问题。进行50,000次随机搜索对人类来说不是一个选项。经验回合的数量可以通过数值收敛的梯度下降形式显著减少（见*第三章*，*机器智能—评估函数与数值收敛*）。
- en: Humans need to be more intuitive, make a few decisions, and see what happens,
    because humans cannot try thousands of ways of doing something. Reinforcement
    learning marks a new era for human thinking by surpassing human reasoning power
    in strategic fields.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 人类需要更加直觉化，做出几个决策并观察结果，因为人类无法尝试成千上万种做事的方式。强化学习通过超越人类在战略领域的推理能力，标志着人类思维的新时代。
- en: On the other hand, reinforcement learning requires mathematical models to function.
    Humans excel in mathematical abstraction, providing powerful intellectual fuel
    to those powerful machines.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强化学习需要数学模型才能发挥作用。人类在数学抽象方面表现出色，为这些强大的机器提供了强大的智力支持。
- en: The boundaries between humans and machines have changed. Humans' ability to
    build mathematical models and ever-growing cloud platforms will serve online machine
    learning services.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 人类与机器之间的界限已经改变。人类在构建数学模型和日益强大的云平台上的能力，将为在线机器学习服务提供支持。
- en: Finding out how to use the outputs of the reinforcement learning program we
    just studied shows how a human will always remain at the center of AI.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何使用我们刚刚研究的强化学习程序的输出，表明人类始终将处于人工智能的核心。
- en: How to use the outputs
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用输出结果
- en: 'The reinforcement program we studied contains no trace of a specific field,
    as in traditional software. The program contains the Bellman equation with stochastic
    (random) choices based on the reward matrix. The goal is to find a route to **C**
    (line 3, column 3) that has an attractive reward (`100`):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的强化程序不包含传统软件中的任何特定领域的痕迹。该程序包含了贝尔曼方程，带有基于奖励矩阵的随机选择。目标是找到一条通向**C**（第3行，第3列）的路径，具有吸引人的奖励（`100`）：
- en: '[PRE8]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That reward matrix goes through the Bellman equation and produces a result
    in Python:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那个奖励矩阵经过贝尔曼方程处理，并在Python中产生结果：
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The result contains the values of each state produced by the reinforced learning
    process, and also a normed `Q` (the highest value divided by other values).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 结果包含强化学习过程中每个状态的值，并且还有一个归一化的`Q`（最高值与其他值的比值）。
- en: As Python geeks, we are overjoyed! We made something that is rather difficult
    work, namely, reinforcement learning. As mathematical amateurs, we are elated.
    We know what MDP and the Bellman equation mean.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为Python极客，我们无比激动！我们完成了一项相当困难的工作，即强化学习。作为数学爱好者，我们欣喜若狂。我们知道MDP和贝尔曼方程是什么意思。
- en: However, as natural language thinkers, we have made little progress. No customer
    or user can read that data and make sense of it. Furthermore, we cannot explain
    how we implemented an intelligent version of their job in the machine. We didn't.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作为自然语言思维者，我们进展甚微。没有客户或用户能够阅读这些数据并理解其中的含义。此外，我们无法解释我们是如何在机器中实现他们工作的智能版本的。实际上，我们没有做到。
- en: We hardly dare say that reinforcement learning can beat anybody in the company,
    making random choices 50,000 times until the right answer came up.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎不敢说强化学习能够击败公司中的任何人，因为它要进行50,000次随机选择，直到找到正确答案。
- en: Furthermore, we got the program to work, but hardly know what to do with the
    result ourselves. The consultant on the project cannot help because of the matrix
    format of the solution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使得程序运行成功，但我们自己几乎不知道如何处理结果。项目中的顾问由于解决方案采用矩阵格式，无法提供帮助。
- en: Being an adaptive thinker means knowing how to be good in all steps of a project.
    To solve this new problem, let's go back to step 1 with the result. Going back
    to step 1 means that if you have problems either with the results themselves or
    understanding them, it is necessary to go back to the SME level, the real-life
    situation, and see what is going wrong.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 成为一个自适应的思考者意味着知道如何在项目的各个步骤中都做到最好。为了解决这个新问题，我们从结果回到第一步。回到第一步意味着如果你在理解结果或结果本身时遇到问题，就需要回到SME（主题专家）级别、现实生活情境，看看哪里出了问题。
- en: 'By formatting the result in Python, a graphics tool, or a spreadsheet, the
    result can be  displayed as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在Python中格式化结果，使用图形工具或电子表格，结果可以如下显示：
- en: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
- en: '| **A** | - | - | - | - | 258.44 | - |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| **A** | - | - | - | - | 258.44 | - |'
- en: '| **B** | - | - | - | 321.8 | - | 207.752 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **B** | - | - | - | 321.8 | - | 207.752 |'
- en: '| **C** | - | - | 500 | 321.8 | - | - |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **C** | - | - | 500 | 321.8 | - | - |'
- en: '| **D** | - | 258.44 | 401. | - | 258.44 | - |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **D** | - | 258.44 | 401. | - | 258.44 | - |'
- en: '| **E** | 207.752 | - | - | 321.8 | - | - |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **E** | 207.752 | - | - | 321.8 | - | - |'
- en: '| **F** | - | 258.44 | - | - | - | - |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **F** | - | 258.44 | - | - | - | - |'
- en: 'Now, we can start reading the solution:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始阅读解决方案：
- en: Choose a starting state. Take **F**, for example.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个起始状态。以**F**为例。
- en: The **F** line represents the state. Since the maximum value is 258.44 in the
    **B** column, we go to state **B**, the second line.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**线表示状态。由于**B**列中的最大值是258.44，我们转到**B**状态，即第二行。'
- en: The maximum value in state **B** in the second line leads us to the **D** state
    in the  fourth column.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二行中的**B**状态的最大值引导我们到第四列的**D**状态。
- en: The highest maximum of the **D** state (fourth line) leads us to the **C** state.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**D**状态中的最高最大值（第四行）引导我们到**C**状态。'
- en: Note that if you start at the **C** state and decide not to stay at **C**, the
    **D** state becomes the maximum value, which will lead you back to **C**. However,
    the MDP will never do this naturally. You will have to force the system to do
    it.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果你从**C**状态开始，并决定不待在**C**，那么**D**状态将成为最大值，这将导致你回到**C**。然而，MDP（马尔可夫决策过程）永远不会自然发生这种情况，你必须强制系统执行此操作。
- en: 'You have now obtained a sequence: **F**->**B**->**D**->**C**. By choosing other
    points of departure, you can obtain other sequences by simply sorting the table.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经得到了一个序列：**F**->**B**->**D**->**C**。通过选择其他起点，你可以通过简单地排序表格来获得其他序列。
- en: 'A useful way of putting it remains the normalized version in percentages, as
    shown in the following table:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一种有用的表达方式是保留标准化的百分比版本，如下表所示：
- en: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | **A** | **B** | **C** | **D** | **E** | **F** |'
- en: '| **A** | - | - | - | - | 51.68% | - |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **A** | - | - | - | - | 51.68% | - |'
- en: '| **B** | - | - | - | 64.36% | - | 41.55% |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **B** | - | - | - | 64.36% | - | 41.55% |'
- en: '| **C** | - | - | 100% | 64.36% | - | - |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **C** | - | - | 100% | 64.36% | - | - |'
- en: '| **D** | - | 51.68% | 80.2% | - | 51.68% | - |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **D** | - | 51.68% | 80.2% | - | 51.68% | - |'
- en: '| **E** | 41.55% | - | - | 64.36% | - | - |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **E** | 41.55% | - | - | 64.36% | - | - |'
- en: '| **F** | - | 51.68% | - | - | - | - |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| **F** | - | 51.68% | - | - | - | - |'
- en: Now comes the very tricky part. We started the chapter with a trip on the road.
    But I made no mention of it in the results analysis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入非常棘手的部分。我们以一次公路旅行开始本章，但在结果分析中我没有提及它。
- en: An important property of reinforcement learning comes from the fact that we
    are working with a mathematical model that can be applied to anything. No human
    rules are needed. We can use this program for many other subjects without writing
    thousands of lines of code.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的一个重要特性来自于我们正在使用的数学模型，它可以应用于任何事情。无需任何人工规则。我们可以在不编写成千上万行代码的情况下，将此程序应用于其他许多领域。
- en: Possible use cases
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可能的使用案例
- en: There are many cases to which we could adapt our reinforcement learning model
    without having to change any of its details.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多案例，我们可以在不改变模型任何细节的情况下，调整我们的强化学习模型。
- en: 'Case 1: optimizing a delivery for a driver, human or not'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例1：优化司机的配送，无论是否为人类
- en: This model was described in this chapter.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 本模型在本章中进行了描述。
- en: 'Case 2: optimizing warehouse flows'
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例2：优化仓库流程
- en: 'The same reward matrix can apply to go from point **F** to **C** in a warehouse,
    as shown in the following diagram:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的奖励矩阵可以用于从**F**点到**C**点的转移，比如在仓库中，如下图所示：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_02.png](img/B15438_01_03.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/05/B09946_01_02.png](img/B15438_01_03.png)'
- en: 'Figure 1.3: A diagram illustrating a warehouse flow problem'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：一个仓库流动问题的示意图
- en: In this warehouse, the **F**->**B**->**D**->**C** sequence makes visual sense.
    If somebody goes from point **F** to **C**, then this physical path makes sense
    without going through walls.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个仓库中，**F**->**B**->**D**->**C**的顺序具有视觉意义。如果有人从**F**点走到**C**点，那么这个物理路径是有意义的，且无需穿过墙壁。
- en: It can be used for a video game, a factory, or any form of layout.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以应用于视频游戏、工厂或任何形式的布局。
- en: 'Case 3: automated planning and scheduling (APS)'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例3：自动化规划与调度（APS）
- en: By converting the system into a scheduling vector, the whole scenery changes.
    We have left the more comfortable world of physical processing of letters, faces,
    and trips. Though fantastic, those applications are social media's tip of the
    iceberg. The real challenge of AI begins in the abstract universe of human thinking.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将系统转化为调度向量，整个场景发生了变化。我们已离开了更为舒适的物理处理字母、面孔和旅行的世界。虽然这些应用很精彩，但它们只是社交媒体的冰山一角。AI的真正挑战始于人类思维的抽象宇宙。
- en: 'Every single company, person, or system requires automatic planning and scheduling
    (see *Chapter 12*, *AI and the Internet of Things (IoT)*). The six **A** to **F**
    steps in the example of this chapter could well be six tasks to perform in a given
    unknown order represented by the following vector *x*:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每个公司、个人或系统都需要自动化规划和调度（参见*第12章*，*人工智能与物联网（IoT）*）。本章示例中的六个**A**到**F**步骤很可能是执行六个任务的顺序，这些任务由以下向量
    *x* 表示：
- en: '![](img/B15438_01_003.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_01_003.png)'
- en: The reward matrix then reflects the weights of constraints of the tasks of vector
    *x* to perform. For example, in a factory, you cannot assemble the parts of a
    product before manufacturing them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励矩阵反映了向量 *x* 执行任务的约束权重。例如，在一个工厂中，你不能在制造零件之前就将它们组装成产品。
- en: In this case, the sequence obtained represents the schedule of the manufacturing
    process.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，得到的序列代表了制造过程的时间表。
- en: 'Cases 4 and more: your imagination'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例4及以上：你的想象力
- en: By using physical layouts or abstract decision-making vectors, matrices, and
    tensors, you can build a world of solutions in a mathematical reinforcement learning
    model. Naturally, the following chapters will enhance your toolbox with many other
    concepts.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用物理布局或抽象的决策向量、矩阵和张量，你可以在数学强化学习模型中构建一个解决方案的世界。当然，接下来的章节将通过许多其他概念丰富你的工具箱。
- en: Before moving on, you might want to imagine some situations in which you could
    use the A to F letters to express some kind of path.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，你可能想象一些场景，看看你能如何利用A到F字母来表达某种路径。
- en: To help you with these mind experiment simulations, open `mdp02.py` and go to
    line 97, which starts with the following code that enables a simulation tool.
    `nextc` and `nextci` are simply variables to remember where the path begins and
    will end. They are set to `-1` so as to avoid 0, which is a location.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助你进行这些思维实验模拟，打开`mdp02.py`并跳到第97行，代码从以下内容开始，启用了模拟工具。`nextc`和`nextci`只是变量，用来记住路径的起始和结束位置。它们被设置为`-1`，以避免0，0表示一个位置。
- en: 'The primary goal is to focus on the expression "concept code." The locations
    have become any concept you wish. A could be your bedroom, and C your kitchen.
    The path would go from where you wake up to where you have breakfast. A could
    be an idea you have, and F the end of a thinking process. The path would go from
    A (How can I hang this picture on the wall?) to E (I need to drill a hole) and,
    after a few phases, to F (I hung the picture on the wall). You can imagine thousands
    of paths like this as long as you define the reward matrix, the "concept code,"
    and a starting point:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目标是关注“概念代码”这个表达式。位置已变成你所希望的任何概念。A可以是你的卧室，C可以是你的厨房。路径从你醒来的地方到你吃早餐的地方。A可以是你有的一个想法，F是思考过程的终点。路径将从A（我该如何把这张照片挂在墙上？）到E（我需要钻个孔），经过几个阶段，到F（我把照片挂在墙上了）。只要你定义了奖励矩阵、"概念代码"和起始点，你可以想象成千上万条这样的路径。
- en: '[PRE10]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This code takes the result of the calculation, labels the result matrix, and
    accepts an input as shown in the following code snippet:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码接收计算结果，标记结果矩阵，并接受如下代码片段所示的输入：
- en: '[PRE11]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The input only accepts the label numerical code: `A=0`, `B=1` … `F=5`. The
    function then runs a classical calculation on the results to find the best path.
    Let''s takes an example.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输入仅接受标签数字代码：`A=0`，`B=1` … `F=5`。然后，函数根据结果运行经典计算，以找到最佳路径。让我们举个例子。
- en: 'When you are prompted to enter a starting point, enter `5`, for example, as
    follows:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统提示输入起始点时，例如输入`5`，如下所示：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The program will then produce the optimal path based on the output of the MDP
    process, as shown in the following output:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，程序将根据MDP过程的输出生成最佳路径，如下所示：
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Try multiple scenarios and possibilities. Imagine what you could apply this
    to:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试多种场景和可能性。想象一下你可以将其应用于哪些领域：
- en: An e-commerce website flow (visit, cart, checkout, purchase) imagining that
    a user visits the site and then resumes a session at a later time. You can use
    the same reward matrix and "concept code" explored in this chapter. For example,
    a visitor visits a web page at 10 a.m., starting at point A of your website. Satisfied
    with a product, the visitor puts the product in a cart, which is point E of your
    website. Then, the visitor leaves the site before going to the purchase page,
    which is C. D is the critical point. Why didn't the visitor purchase the product?
    What's going on?
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个电子商务网站的流程（访问、购物车、结账、购买），设想一个用户访问网站，然后稍后恢复会话。你可以使用本章中探讨的相同奖励矩阵和“概念代码”。例如，一位访客在早上10点访问一个网页，从你的网站A点开始。对产品满意后，访客将产品加入购物车，这是你网站的E点。然后，访客在到达购买页面C之前离开了网站。D是关键点。为什么访客没有购买产品？发生了什么？
- en: 'You can decide to have an automatic email sent after 24 hours saying: "There
    is a 10% discount on all purchases during the next 48 hours." This way, you will
    target all the visitors stuck at D and push them toward C.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以决定在24小时后自动发送一封电子邮件，内容是：“在接下来的48小时内，所有购买享有10%的折扣。”这样，你就能针对所有停留在D点的访客并推动他们向C点前进。
- en: A sequence of possible words in a sentence (subject, verb, object). Predicting
    letters and words was one of Andrey Markov's first applications 100+ years ago!
    You can imagine that B is the letter "a" of the alphabet. If D is "t," it is much
    more probable than F if F is "o," which is less probable in the English language.
    If an MDP reward matrix is built such as B leads to D or F, B can thus either
    go to D or to F. There are thus two possibilities, D or F. Andrey Markov would
    suppose, for example, that B is a variable that represents the letter "a," D is
    a variable that represents the letter "t" and F is a variable that represents
    the letter "o." After studying the structure of a language closely, he would find
    that the letter "a" would more likely be followed by "t" than by "o" in the English
    language. If one observes the English language, it is more likely to find an "a-t"
    sequence than an "a-o" sequence. In a Markov decision process, a higher probability
    will be awarded to the "a-t" sequence and a lower one to "a-o." If one goes back
    to the variables, the B-D sequence will come out as more probable than the B-F
    sequence.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子中可能的单词序列（主语、动词、宾语）。预测字母和单词是安德烈·马尔科夫100多年前的第一次应用！你可以想象B是字母表中的字母“a”。如果D是“t”，它比F为“o”更有可能，因为“o”在英语中较不常见。如果构建了一个MDP奖励矩阵，B可能会转到D或F，因此B可以走向D或F。这样就有了两种可能性，D或F。安德烈·马尔科夫会假设，例如，B是一个表示字母“a”的变量，D是一个表示字母“t”的变量，而F是一个表示字母“o”的变量。经过深入研究语言的结构后，他会发现字母“a”更可能后跟“t”而不是“o”。如果观察英语语言，你会发现“a-t”序列比“a-o”序列更常见。在马尔科夫决策过程中，“a-t”序列将获得更高的概率，而“a-o”则获得较低的概率。如果回到变量，B-D序列将比B-F序列更有可能出现。
- en: And anything you can find that fits the model that works is great!
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何你能找到的适合有效模型的内容都是很棒的！
- en: Machine learning versus traditional applications
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习与传统应用的比较
- en: Reinforcement learning based on stochastic (random) processes will evolve beyond
    traditional approaches. In the past, we would sit down and listen to future users
    to understand their way of thinking.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 基于随机（随机）过程的强化学习将超越传统方法。过去，我们会坐下来倾听未来用户的想法，以了解他们的思维方式。
- en: We would then go back to our keyboard and try to imitate the human way of thinking.
    Those days are over. We need proper datasets and ML/DL equations to move forward.
    Applied mathematics has taken reinforcement learning to the next level. In my
    opinion, traditional software will soon be in the museum of computer science.
    The complexity of the huge volumes of data we are facing will require AI at some
    point.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将回到键盘，尝试模仿人类的思维方式。那些日子已经过去了。我们需要适当的数据集和ML/DL方程式来前进。应用数学已经将强化学习带到了一个新水平。在我看来，传统软件很快就会进入计算机科学博物馆。我们所面临的大量数据的复杂性在某些时候将需要AI。
- en: An artificial adaptive thinker sees the world through applied mathematics translated
    into machine representations.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人工自适应思维者通过应用数学将世界转化为机器表示。
- en: Use the Python source code example provided in this chapter in different ways.
    Run it and try to change some parameters to see what happens. Play around with
    the number of iterations as well. Lower the number from 50,000 down to where you
    find it fits best. Change the reward matrix a little to see what happens. Design
    your reward matrix trajectory. This can be an itinerary or decision-making process.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以本章提供的Python源代码示例为基础，以不同方式进行尝试。运行代码并尝试修改一些参数，看看会发生什么。也可以尝试调整迭代次数，将次数从50,000减少到你认为最合适的值。稍微调整一下奖励矩阵，看看结果如何。设计你自己的奖励矩阵轨迹。这可以是一个行程安排或决策过程。
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Presently, AI is predominantly a branch of applied mathematics, not of neurosciences.
    You must master the basics of linear algebra and probabilities. That's a difficult
    task for a developer used to intuitive creativity. With that knowledge, you will
    see that humans cannot rival machines that have CPU and mathematical functions.
    You will also understand that machines, contrary to the hype around you, don't
    have emotions; although we can represent them to a scary point in chatbots (see
    *Chapter 16, Improving the Emotional Intelligence Deficiencies of Chatbots*).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，AI主要是应用数学的一个分支，而不是神经科学的一个分支。你必须掌握线性代数和概率的基础知识。这对习惯于直觉创造力的开发者来说是个困难的任务。有了这些知识，你会明白人类无法与拥有CPU和数学功能的机器抗衡。你还会理解机器与周围的炒作不同，它们没有情感；尽管我们可以在聊天机器人中以一种可怕的方式表现出来（参见*第16章，改善聊天机器人的情商不足*）。
- en: A multi-dimensional approach is a prerequisite in an AI/ML/DL project. First,
    talk and write about the project, then make a mathematical representation, and
    finally go for software production (setting up an existing platform or writing
    code). In real life, AI solutions do not just grow spontaneously in companies
    as some hype would have us believe. You need to talk to the teams and work with
    them. That part is the real fulfilling aspect of a project—imagining it first
    and then implementing it with a group of real-life people.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 多维方法是AI/ML/DL项目的前提条件。首先，讨论并书面表达项目，然后进行数学表示，最后进行软件生产（设置现有平台或编写代码）。在现实生活中，AI解决方案并不会像某些炒作所说的那样自发地在公司中产生。你需要与团队沟通并合作。那部分才是项目的真正满足感——先设想它，然后与一群现实生活中的人一起实施。
- en: MDP, a stochastic random action-reward (value) system enhanced by the Bellman
    equation, will provide effective solutions to many AI problems. These mathematical
    tools fit perfectly in corporate environments.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: MDP（马尔可夫决策过程），一个通过贝尔曼方程增强的随机动作-奖励（价值）系统，将为许多AI问题提供有效的解决方案。这些数学工具在企业环境中非常契合。
- en: Reinforcement learning using the Q action-value function is memoryless (no past)
    and unsupervised (the data is not labeled or classified). MDP provides endless
    avenues to solve real-life problems without spending hours trying to invent rules
    to make a system work.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Q动作价值函数的强化学习是无记忆的（没有过去）和无监督的（数据未标记或分类）。MDP为解决现实问题提供了无尽的途径，而无需花费数小时试图发明规则来使系统工作。
- en: Now that you are at the heart of Google's DeepMind approach, it is time to go
    to *Chapter 2*, *Building a Reward Matrix – Designing Your Datasets*, and discover
    how to create the reward matrix in the first place through explanations and source
    code.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经处在Google DeepMind方法的核心，现在是时候阅读*第2章*，*构建奖励矩阵—设计你的数据集*，并通过解释和源代码发现如何首先创建奖励矩阵。
- en: Questions
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The answers to the questions are in *Appendix B*, with further explanations:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的答案在*附录B*中，附有进一步的解释：
- en: Is reinforcement learning memoryless? (Yes | No)
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习是无记忆的吗？（是 | 否）
- en: Does reinforcement learning use stochastic (random) functions? (Yes | No)
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习使用随机（随机）函数吗？（是 | 否）
- en: Is MDP based on a rule base? (Yes | No)
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MDP 是否基于规则库？（是 | 否）
- en: Is the Q function based on the MDP? (Yes | No)
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q 函数是否基于 MDP？（是 | 否）
- en: Is mathematics essential to AI? (Yes | No)
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数学对于 AI 是否至关重要？（是 | 否）
- en: Can the Bellman-MDP process in this chapter apply to many problems? (Yes | No)
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章中的 Bellman-MDP 过程是否可以应用于许多问题？（是 | 否）
- en: Is it impossible for a machine learning program to create another program by
    itself? (Yes | No)
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习程序是否不可能自行创建另一个程序？（是 | 否）
- en: Is a consultant required to enter business rules in a reinforcement learning
    program? (Yes | No)
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习程序是否需要顾问输入业务规则？（是 | 否）
- en: Is reinforcement learning supervised or unsupervised? (Supervised | Unsupervised)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 强化学习是监督学习还是无监督学习？（监督学习 | 无监督学习）
- en: Can Q-learning run without a reward matrix? (Yes | No)
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Q-learning 是否可以在没有奖励矩阵的情况下运行？（是 | 否）
- en: Further reading
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Andrey Markov: [https://www.britannica.com/biography/Andrey-Andreyevich-Markov](https://www.britannica.com/biography/Andrey-Andreyevich-Markov)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安德烈·马尔可夫：[https://www.britannica.com/biography/Andrey-Andreyevich-Markov](https://www.britannica.com/biography/Andrey-Andreyevich-Markov)
- en: 'The Markov process: [https://www.britannica.com/science/Markov-process](https://www.britannica.com/science/Markov-process)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫过程：[https://www.britannica.com/science/Markov-process](https://www.britannica.com/science/Markov-process)
