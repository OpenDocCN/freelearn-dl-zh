- en: Getting Started with Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习入门
- en: This chapter discusses deep learning, a powerful multilayered architecture for
    pattern-recognition, signal-detection, and classification or prediction. Although
    deep learning is not new, it is only in the past decade that it has gained great
    popularity, due in part to advances in computational capacity and new ways of
    more efficiently training models, as well as the availability of ever-increasing
    amounts of data. In this chapter, you will learn what deep learning is, the R
    packages available for training such models, and how to get your system set up
    for analysis. We will briefly discuss **MXNet** and **Keras**, which are the two
    main frameworks that we will use for many of the examples in later chapters to
    actually train and use deep learning models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了深度学习，一种强大的多层架构，用于模式识别、信号检测以及分类或预测。尽管深度学习并不新鲜，但它仅在过去十年才获得了极大的关注，这部分归功于计算能力的提升、更加高效的模型训练方法以及不断增长的数据量。在本章中，你将了解什么是深度学习、可用于训练此类模型的R包，以及如何为分析设置系统。我们将简要讨论**MXNet**和**Keras**，这两种框架将在后续章节的多个示例中用于实际训练和使用深度学习模型。
- en: 'In this chapter, we will explore the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: What is deep learning?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: A conceptual overview of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的概念概览
- en: Setting up your R environment and the deep learning frameworks available in
    R
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置R环境及在R中可用的深度学习框架
- en: GPUs and reproducibility
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU和可重复性
- en: What is deep learning?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是深度学习？
- en: '**Deep learning** is a subfield within machine learning, which in turn is a
    subfield within artificial intelligence. **Artificial intelligence** is the art
    of creating machines that perform functions that require intelligence when performed
    by people. **Machine learning** uses algorithms that learn without being explicitly
    programmed. Deep learning is the subset of machine learning that uses artificial neural
    networks that mimic how the brain works.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是机器学习的一个子领域，而机器学习又是人工智能的一个子领域。**人工智能**是创造能够执行需要人类智能的任务的机器的艺术。**机器学习**使用算法来学习，而无需明确编程。深度学习是机器学习的一个子集，使用模仿大脑工作方式的人工神经网络。'
- en: 'The following diagram shows the relationships between them. For example, self-driving
    cars are an application of artificial intelligence. A critical part of self-driving
    cars is to recognize other road users, cars, pedestrians, cyclists, and so on.
    This requires machine learning because it is not possible to explicitly program
    this. Finally, deep learning may be chosen as the method to implement this machine
    learning task:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了它们之间的关系。例如，自动驾驶汽车是人工智能的一个应用。自动驾驶汽车的一个关键部分是识别其他道路使用者，如汽车、行人、骑车人等。这需要机器学习，因为无法明确编程来实现这一点。最终，深度学习可能被选择作为实现这一机器学习任务的方法：
- en: '![](img/063a237d-d10e-46ab-ae2e-d94e272f22bb.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/063a237d-d10e-46ab-ae2e-d94e272f22bb.png)'
- en: 'Figure 1.1: The relationship between artificial intelligence, machine learning,
    and deep learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：人工智能、机器学习和深度学习之间的关系
- en: Artificial intelligence as a field has existed since the 1940s; the definition
    used in the previous diagram is from Kurzweil, 1990\. It is a broad field that
    encompasses ideas from many different fields, including philosophy, mathematics,
    neuroscience, and computer engineering. Machine learning is a subfield within
    artificial intelligence that is devoted to developing and using algorithms that
    learn from raw data. When the machine learning task has to predict an outcome,
    it is known as **supervised learning**. When the task is to predict from a set
    of possible outcomes, it is a **classification** task, and when the task is to
    predict a numeric value, it is a **regression** task. Some examples of classification
    tasks are whether a particular credit card purchase is fraudulent, or whether
    a given image is of a cat or a dog. An example of a regression task is predicting
    how much money a customer will spend in the next month. There are other types
    of machine learning where the learning does not predict values. This is called
    **unsupervised learning** and includes clustering (segmenting) the data, or creating
    a compressed format of the data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能作为一个领域自20世纪40年代就已经存在；前述图表中的定义来自于1990年**库兹韦尔**。它是一个广泛的领域，涵盖了哲学、数学、神经科学和计算机工程等多个不同领域的思想。机器学习是人工智能中的一个子领域，致力于开发和使用能够从原始数据中学习的算法。当机器学习任务需要预测一个结果时，这被称为**监督学习**。当任务是从一组可能的结果中进行预测时，这就是**分类**任务；当任务是预测一个数值时，这就是**回归**任务。一些分类任务的例子包括判断某一信用卡购买是否欺诈，或者某张图片是猫还是狗。回归任务的一个例子是预测客户下个月将花费多少钱。还有其他类型的机器学习，其中学习并不预测值。这被称为**无监督学习**，包括对数据进行聚类（分段）或创建数据的压缩格式。
- en: Deep learning is a subfield within machine learning. It is called **deep** because
    it uses multiple layers to map the relationship between input and output. A **layer**
    is a collection of neurons that perform a mathematical operation on its input.
    This will be explained in more detail in the next section, *Conceptual overview
    of neural networks*. This deep architecture means the model is large enough to
    handle many variables and that it is sufficiently flexible to approximate the
    patterns in the data. Deep learning can also generate features as part of the
    overall learning algorithm, rather than feature-creation being a prerequisite
    step. Deep learning has proven particularly effective in the fields of image-recognition
    (including handwriting as well as photo- or object-classification) , speech recognition
    and natural-language. It has completely transformed how to use image, text, and
    speech data for prediction in the past few years, replacing previous methods of
    working with these types of data. It has also opened up these fields to a lot
    more people because it automates a lot of the feature-generation, which required
    specialist skills.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习中的一个子领域。之所以称其为**深度**，是因为它使用多层结构来映射输入和输出之间的关系。**层**是一个神经元集合，负责对其输入进行数学运算。这个内容将在下一部分的*神经网络概念概述*中详细解释。这个深度结构意味着模型足够大，可以处理许多变量，并且足够灵活，可以近似数据中的模式。深度学习还可以作为整体学习算法的一部分生成特征，而不是将特征创建作为一个前提步骤。深度学习在图像识别（包括手写识别、照片或物体分类）、语音识别和自然语言处理领域表现尤为有效。它在过去几年彻底改变了如何使用图像、文本和语音数据进行预测，取代了之前处理这些数据的方式。它还使这些领域向更多人开放，因为它自动化了大量特征生成工作，这些工作原本需要专业技能。
- en: Deep learning is not the only technique available in machine learning. There
    are other types of machine learning algorithms; the most popular include regression,
    decision trees, random forest, and naive bayes. For many use cases, one of these
    algorithms could be a better choice. Some examples of use cases where deep learning
    may not be the best choice include when interpretability is an essential requirement,
    the dataset size is small, or you have limited resources (time and/or hardware)
    to develop a model. It is important to realize that despite, the industry hype,
    most machine learning in industry does not use deep learning. Having said that,
    this book covers deep learning algorithms, so we will move on. The next sections
    will discuss neural networks and deep neural networks in more depth.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习并不是机器学习中唯一的技术。还有其他类型的机器学习算法；最常见的包括回归、决策树、随机森林和朴素贝叶斯。对于许多应用场景，这些算法中的某些可能是更好的选择。深度学习可能不是最佳选择的一些例子包括当可解释性是必需的要求、数据集较小，或者你在开发模型时资源（时间和/或硬件）有限。重要的是要认识到，尽管行业存在过度炒作，但在行业中大多数机器学习并不使用深度学习。话虽如此，本书涵盖了深度学习算法，因此我们将继续讲解。接下来的章节将更深入地讨论神经网络和深度神经网络。
- en: A conceptual overview of neural networks
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的概念概览
- en: It can be difficult to understand why neural networks work so well. This introduction
    will look at them from two viewpoints. If you have an understanding of how linear
    regression works, the first viewpoint should be useful. The second viewpoint is
    more intuitive and less technical, but equally valid. I encourage you to read
    both and spend some time contemplating both overviews.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么神经网络如此有效可能是困难的。这个介绍将从两个角度来观察它们。如果你了解线性回归的工作原理，第一个角度应该很有帮助。第二个角度则更直观且技术性较少，但同样有效。我鼓励你阅读这两个角度，并花些时间思考这两种概述。
- en: Neural networks as an extension of linear regression
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络作为线性回归的扩展
- en: 'One of the simplest and oldest prediction models is **regression**. It predicts
    a continuous value (that is, a number) based on another value. The linear regression
    function is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单、最古老的预测模型之一是 **回归**。它基于另一个值预测一个连续值（即数字）。线性回归函数是：
- en: '*y=mx+b*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*y=mx+b*'
- en: 'Where *y* is the value you want to predict and *x* is your input variable.
    The linear regression coefficients (or parameters) are *m* (the slope of the line)
    and *b* (the intercept). The following R code creates a line with the *y= 1.4x
    -2* function and plots it:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *y* 是你要预测的值，而 *x* 是你的输入变量。线性回归系数（或参数）为 *m*（直线的斜率）和 *b*（截距）。以下 R 代码创建了一个 *y=
    1.4x -2* 的函数，并绘制它：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The *o* or *x* points are the values to be predicted given a value on the *x* axis
    and the line is the ground truth. Some random noise is added, so that the points
    are not exactly on the line. This code produces the following output:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*o* 或 *x* 点是给定 *x* 轴值时需要预测的值，直线是实际的真实值。添加了一些随机噪声，因此点并不完全在直线上。此代码生成以下输出：'
- en: '![](img/58780258-bafc-45a5-92a6-2883a14be268.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58780258-bafc-45a5-92a6-2883a14be268.png)'
- en: 'Figure 1.2: Example of a regression line fitted to data (that is, predict *y*
    from *x*)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：回归线拟合数据的示例（即从 *x* 预测 *y*）
- en: In a regression task, you are given some *x* and corresponding *y* values, but
    are not given the underlying function to map *x* to *y*. The purpose of a supervised
    machine learning task is that given some previous examples of *x* and *y*, can
    we predict the *y* values for new data where we only have *x* and not *y.* An
    example might be to predict house prices based on the number of bedrooms in the
    house. So far, we have only considered a single input variable, *x*, but we can
    easily extend the example to handle multiple input variables. For the house example,
    we would use the number of bedrooms and square footage to predict the price of
    the house. Our code can accommodate this by changing the input, *x*, from a vector
    (one-dimensional array) into a matrix (two-dimensional array).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归任务中，你会得到一些 *x* 和对应的 *y* 值，但并未给出将 *x* 映射到 *y* 的底层函数。监督式机器学习任务的目的是，在给定一些之前的
    *x* 和 *y* 样本的情况下，我们能否预测出仅知道 *x* 而不知道 *y* 的新数据的 *y* 值。一个例子可能是根据房屋卧室数量预测房价。到目前为止，我们只考虑了一个输入变量
    *x*，但我们可以轻松扩展这个例子来处理多个输入变量。对于房屋的例子，我们会使用卧室数量和房屋面积来预测房价。我们的代码可以通过将输入 *x* 从向量（一维数组）更改为矩阵（二阶数组）来适应这一点。
- en: 'If we consider our model for predicting house prices, linear regression has
    a serious limitation: it can only estimate linear functions. If the mapping from
    *x* to *y* is not linear, it will not predict *y* very well. The function always
    results in a straight line for one variable and a hyperplane if multiple *x* predictor
    values are used. This means that linear regression models may not be accurate
    at the low and high extent of the data.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑用我们的模型来预测房价，线性回归有一个严重的限制：它只能估计线性函数。如果 *x* 到 *y* 的映射不是线性的，它就无法很好地预测 *y*。这个函数总是为一个变量生成一条直线，如果使用多个
    *x* 预测变量，它会生成一个超平面。这意味着线性回归模型在数据的低端和高端可能不准确。
- en: 'A simple trick to make the model fit nonlinear relationships is to add polynomial
    terms to the function. This is known as **polynomial regression**. For example,
    by adding a polynomial of degree 4, our function changes to:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让模型拟合非线性关系的一个简单方法是向函数中添加多项式项。这就是所谓的 **多项式回归**。例如，通过添加一个四次的多项式，我们的函数变为：
- en: '*y= m[1]x⁴+ m[2]x³+ m[3]x²+ m[4]x+b*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = m[1]x⁴ + m[2]x³ + m[3]x² + m[4]x + b*'
- en: 'By adding these extra terms, the line (or decision boundary) is no longer linear.
    The following code demonstrates this – we create some sample data and we create
    three regression models to fit this data. The first model has no polynomial terms,
    the model is a straight line and fits the data very poorly. The second model (blue
    circles) has polynomials up to degree 3, that is, *X*, *X²*, and *X³*. The last
    model has polynomials up to degree 12, that is, *X*, *X**²*,....., *X**^(12)*.
    The first model (straight line) underfits the data and the last line overfits
    the data. Overfitting refers to situations where the model is too complex and
    ends up memorizing the data. This means that the model does not generalize well
    and will perform poorly on unseen data. The following code generates the data
    and creates three models with increasing levels of polynomial:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加这些额外的项，直线（或决策边界）不再是线性的。以下代码演示了这一点——我们创建了一些示例数据，并创建了三个回归模型来拟合这些数据。第一个模型没有多项式项，模型是一个直线，拟合效果很差。第二个模型（蓝色圆圈）包含多项式，最高到3次方，即
    *X*、*X²* 和 *X³*。最后一个模型包含最高12次方的多项式，即 *X*、*X²*、.....、*X^(12)*。第一个模型（直线）欠拟合数据，而最后一个模型则过拟合数据。过拟合是指模型过于复杂，最终记住了数据。这意味着该模型的泛化能力差，在未见过的数据上表现不好。以下代码生成数据并创建了三个随着多项式阶数增加的模型：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we were selecting one of these models to use, we should select the middle
    model, even though the third model has a lower **MSE** (**mean-squared error**).
    In the following screenshot; the best model is the curved line from the top left
    corner:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要选择一个模型使用，应该选择中间的模型，即使第三个模型的 **MSE**（**均方误差**）更低。在下面的截图中；最佳模型是左上角的弯曲线：
- en: '![](img/1ba16713-0827-4935-a408-c93da07f8b68.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ba16713-0827-4935-a408-c93da07f8b68.png)'
- en: 'Figure 1.3: Polynomial regression'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：多项式回归
- en: If we look at the three models and see how they handle the extreme left and
    right points, we see why overfitting can lead to poor results on unseen data.
    On the right side of the plot, the last series of points (plus signs) have a local
    linear relationship. However, the polynomial regression line with degree 12 (green
    triangles) puts too much emphasis on the last point, which is extra noise and
    the line moves down sharply. This would cause the model to predict extreme negative
    values for *y* as *x* increases, which is not justified if we look at the data. Overfitting
    is an important issue that we will look at in more detail in later chapters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看这三个模型，并观察它们如何处理极端的左右点，我们会明白为什么过拟合可能导致在未见过的数据上的不良结果。在图的右侧，最后一组点（加号）具有局部线性关系。然而，12次方的多项式回归线（绿色三角形）过度强调最后一个点，这是额外的噪声，导致曲线急剧下降。这会导致模型在
    *x* 增加时对 *y* 预测极端的负值，而如果我们查看数据，这是不合理的。过拟合是一个重要的问题，我们将在后续章节中更详细地讨论。
- en: By adding square, cube, and more polynomial terms, the model can fit more complex
    data than if we just used linear functions on the input data. Neural networks
    use a similar concept, except that, instead of taking polynomial terms of the
    input variable, they chain multiple regression functions together with nonlinear
    terms between them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加平方、立方和更多的多项式项，模型能够拟合比仅使用线性函数在输入数据上更复杂的数据。神经网络使用类似的概念，区别在于，它们不是使用输入变量的多项式项，而是将多个回归函数通过非线性项链接在一起。
- en: 'The following is an example of a neural network architecture. The circles are
    nodes and the lines are the connections between nodes. If a connection exists
    between two nodes, the output from the node on the left is the input for the next
    node. The output value from a node is a matrix operation on the input values to
    the node and the weights of the node:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个神经网络架构的示例。圆圈代表节点，线条是节点之间的连接。如果两个节点之间有连接，左侧节点的输出就是下一个节点的输入。一个节点的输出值是该节点输入值和权重的矩阵运算结果：
- en: '![](img/8ab9b6b0-1813-40f9-8f58-807061664ff4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ab9b6b0-1813-40f9-8f58-807061664ff4.png)'
- en: 'Figure 1.4: An example neural network'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：一个神经网络示例
- en: Before the output values from a node are passed to the next node as input values,
    a function is applied to the values to change the overall function to a non-linear
    function. These are known as **activation functions** and they perform the same
    role as the polynomial terms.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个节点的输出值被传递给下一个节点作为输入值之前，会对这些值应用一个函数，将整体函数转换为非线性函数。这些函数被称为**激活函数**，它们的作用与多项式项相同。
- en: This idea of creating a machine learning model by combining multiple small functions
    together is a very common paradigm in machine learning. It is used in random forests,
    where many small independent decision trees *vote* for the result. It is also
    used in boosting algorithms, where the misclassified instances from one function
    are given more prominence in the next function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将多个小函数组合在一起来创建机器学习模型，这种思路在机器学习中非常常见。它被广泛应用于随机森林中，在那里，多个独立的决策树*投票*决定结果。它也被应用于提升算法中，其中一个函数中的错误分类实例在下一个函数中被赋予更多的权重。
- en: By including many layers of nodes, the neural network model can approximate
    almost any function. It does make training the model more difficult, so we'll
    give a brief explanation of how to train a neural network. Each node is assigned
    a set of random weights initially. For the first pass, these weights are used
    to calculate and pass (or propagate) values from the input layer to the hidden
    layers and finally to the output layer. This is known as **forward-propagation**.
    Because the weights were set randomly, the final (prediction) values at the output
    layer will not be accurate compared to the actual values, so we need a method
    of calculating how different the predicted values are from the actual values.
    This is calculated using a **cost function**, which gives a measure of how accurate
    the model is during training. We then need to adjust the weights in the nodes
    from the output layer backward to get us nearer to the target values. This is
    done using **backward-propagation**; we move from right to left, updating the
    weights of the nodes in each layer very slightly to get us very slightly closer
    to the actual values. The cycle of forward-propagation and backward-propagation
    continues until the error value from the loss function stops getting smaller;
    this may require hundreds, or thousands of iterations, or epochs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过包含多个层的节点，神经网络模型可以逼近几乎任何函数。虽然这确实使得训练模型变得更加困难，但我们会简要解释如何训练神经网络。每个节点最初会被分配一组随机权重。在第一次传递时，这些权重会用于计算并将值从输入层传递（或传播）到隐藏层，最终到达输出层。这被称为**前向传播**。由于权重是随机设置的，输出层的最终（预测）值与实际值相比将不准确，因此我们需要一种方法来计算预测值与实际值之间的差异。这是通过使用**成本函数**来计算的，成本函数提供了模型在训练过程中的准确度衡量标准。然后，我们需要调整从输出层到各层节点的权重，使其更接近目标值。这是通过**反向传播**实现的；我们从右到左移动，稍微更新每一层节点的权重，使其逐步接近实际值。前向传播和反向传播的循环会持续进行，直到损失函数的误差值不再减小；这可能需要几百次甚至几千次的迭代，或称为“周期”。
- en: To update the node weights correctly, we need to know that the change will get
    us nearer to the target, which is to minimize the result from the cost function.
    We are able to do this because of a clever trick, we use activation functions
    that have derivative functions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确地更新节点权重，我们需要知道这个变化会让我们更接近目标，目标是最小化成本函数的结果。我们之所以能够做到这一点，是因为一个巧妙的技巧——我们使用具有导数函数的激活函数。
- en: 'If your knowledge of calculus is limited, it can be difficult to get an understanding
    of derivatives initially. But in simple terms, a function may have a derivative
    formula that tells us how to change the *input* of a function so that the *output*
    of the function moves in a positive or negative manner. This derivative/formula
    enables the algorithm to minimize the cost function, which is a measurement of
    error. In more technical terms, the derivative of the function measures the rate
    of change in the function as the input changes. If we know the rate of change
    of a function as the input changes, and more importantly what direction it changes
    in, then we can use this to get nearer to minimizing that function. An example
    that you may have seen before is the following diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对微积分的知识有限，刚开始理解导数可能会有些困难。但简单来说，一个函数可能有一个导数公式，告诉我们如何改变一个函数的 *输入*，从而使函数的 *输出*
    发生正向或负向的变化。这个导数/公式使得算法能够最小化代价函数，代价函数是对误差的衡量。用更专业的术语来说，函数的导数衡量了随着输入变化，函数变化的速率。如果我们知道了一个函数在输入变化时的变化速率，更重要的是知道它朝哪个方向变化，那么我们就能利用这些信息逐步逼近最小化该函数的目标。你可能见过的一个示例是以下图示：
- en: '![](img/e255ca19-9fd7-4730-acd6-246e68c0134f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e255ca19-9fd7-4730-acd6-246e68c0134f.png)'
- en: 'Figure 1.5: A function (curved) line and its derivative at a point'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：一个函数（曲线）及其在某点的导数
- en: In this diagram, the curved line is a mathematical function we want to minimize
    over *y*, that is, we want to get to the lowest point (which is marked by the
    arrow). We are currently at the point in the red circle, and the derivative at
    that point is the slope of the tangent. The derivative function indicates the
    direction we need to move in to get there. The derivative value changes as we
    get nearer the target (the arrow), so we cannot make the move in one big step.
    Therefore, the algorithm moves in small steps and re-calculates the derivative
    after each step, but if we choose too small a step, it will take very long to
    **converge** (that is, get near the minimum). If we take too big a step, we run
    the risk of overshooting the minimum value. How big a step you take is known as
    the **learning rate**, and it effectively decides how long it takes the algorithm
    to train.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，曲线是我们希望在 *y* 上最小化的数学函数，也就是说，我们想要到达最低点（由箭头标示）。我们当前位于红色圆圈中的点，且该点的导数是切线的斜率。导数函数表示了我们需要朝哪个方向移动才能到达最低点。随着我们接近目标（箭头所在的点），导数值会发生变化，因此我们不能一步到位。因此，算法会以小步伐前进，每走一步都重新计算导数，但如果我们选择步伐太小，算法收敛的时间会非常长（即到达最小值的时间）。如果步伐太大，则有可能会错过最小值。你选择的步长大小被称为
    **学习率**，它实际上决定了算法训练所需的时间。
- en: This might seem a bit abstract, so an analogy should make it somewhat clearer.
    This analogy may be over-simplified, but it explains derivatives, learning rates,
    and cost functions. Imagine a simple model of driving a car, where the speed must
    be set to a value that is suitable for the conditions and the speed limit. The
    difference between your current speed and the target speed is the error rate and
    this is calculated using a cost function (just simple subtraction, in this case).
    To change your speed, you apply the gas pedal to speed up or the brake pedal to
    slow down. The acceleration/deceleration (that is, the rate of change of the speed)
    is the derivative of the speed. The amount of force that is applied to the pedals
    changes how fast the acceleration/deceleration occurs, the force is similar to
    the learning rate in a machine learning algorithm. It controls how long it takes
    to get to the target value. If only a small change is applied to the pedals, you
    will eventually get to your target speed, but it will take much longer. However,
    you usually don't want to apply maximum force to the pedals, to do so may be dangerous
    (if you slam on the brakes) or a waste of fuel (if you accelerate too hard). There
    is a happy medium where you apply the change and get to the target speed safely
    and quickly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些抽象，所以用一个类比可能会更清楚一些。这个类比可能有些过于简化，但它解释了导数、学习率和成本函数。假设一个简单的驾驶汽车模型，车速必须设置为适合条件和限速的值。你当前的速度和目标速度之间的差异就是误差率，这是通过成本函数计算的（在这种情况下只是简单的减法）。为了改变车速，你可以踩油门加速，或者踩刹车减速。加速度/减速度（即速度的变化率）就是速度的导数。施加到踏板上的力量决定了加速/减速发生的快慢，这个力量类似于机器学习算法中的学习率。它控制了达到目标值所需的时间。如果只对踏板施加小的变化，你最终会达到目标速度，但会花费更长的时间。然而，你通常不希望对踏板施加最大力量，因为这样做可能会很危险（如果猛踩刹车）或浪费燃料（如果加速过猛）。存在一个最佳的平衡点，在这个点上，你可以安全且迅速地达到目标速度。
- en: Neural networks as a network of memory cells
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络作为记忆单元网络
- en: Another way to consider neural networks is to compare them to how humans think.
    As their name suggests, neural networks draw inspiration from neural processes
    and neurons in the mind. Neural networks contain a series of neurons, or nodes,
    which are interconnected and process input. The neurons have weights that are
    learned from previous observations (data). The output of a neuron is a function
    of its input and its weights. The activation of some final neuron(s) is the prediction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种考虑神经网络的方式是将其与人类思维进行比较。顾名思义，神经网络的灵感来自大脑中的神经过程和神经元。神经网络包含一系列互相连接的神经元或节点，用于处理输入。神经元具有从先前观察（数据）中学习到的权重。神经元的输出是其输入和权重的函数。最终神经元的激活即为预测结果。
- en: We will consider a hypothetical case where a small part of the brain is responsible
    for matching basic shapes, such as squares and circles. In this scenario, some
    neurons at the basic level fire for horizontal lines, another set of neurons fires
    for vertical lines, and yet another set of neurons fire for curved segments. These
    neurons feed into higher-order process that combines the input so that it recognizes
    more complex objects, for example, a square when the horizontal and vertical neurons
    both are activated simultaneously.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑一个假设的案例，其中大脑的一个小部分负责匹配基本形状，例如方形和圆形。在这种情况下，一些神经元在基本层次上对水平线发火，另一组神经元对垂直线发火，另一组神经元则对弯曲的线段发火。这些神经元将输入信息传递到更高层次的处理过程，从而识别更复杂的物体，例如，当水平和垂直神经元同时被激活时，识别为一个方形。
- en: 'In the following diagram, the input data is represented as squares. These could
    be pixels in an image. The next layer of hidden neurons consists of neurons that
    recognize basic features, such as horizontal lines, vertical lines, or curved
    lines. Finally, the output may be a neuron that is activated by the simultaneous
    activation of two of the hidden neurons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，输入数据以方形表示。这些可以是图像中的像素。下一层的隐藏神经元由识别基本特征的神经元组成，例如水平线、垂直线或曲线。最后，输出可能是一个通过两个隐藏神经元同时激活而被激活的神经元：
- en: '![](img/90fb64ee-3be2-40ae-bbbc-110c66d1d002.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90fb64ee-3be2-40ae-bbbc-110c66d1d002.png)'
- en: 'Figure 1.6: Neural networks as a network of memory cells'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：神经网络作为一种记忆单元网络
- en: In this example, the first node in the hidden layer is good at matching horizontal
    lines, while the second node in the hidden layer is good at matching vertical
    lines. These nodes *remember* what these objects are. If these nodes combine,
    more sophisticated objects can be detected. For example, if the hidden layer recognizes horizontal
    lines and vertical lines, the object is more likely to be a square than a circle.
    This is similar to how convolutional neural networks work, which we will cover
    in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),* Image Classification
    Using Convolutional Neural Networks*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，隐藏层中的第一个节点擅长匹配水平线，而第二个节点则擅长匹配垂直线。这些节点*记住*了这些物体是什么。如果这些节点结合在一起，可以检测到更复杂的物体。例如，如果隐藏层识别出水平线和垂直线，那么这个物体更可能是一个正方形而不是圆形。这与卷积神经网络的工作原理类似，我们将在[第五章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中讨论，*使用卷积神经网络进行图像分类*。
- en: 'We have covered the theory behind neural networks very superficially here as
    we do not want to overwhelm you in the first chapter! In future chapters, we will
    cover some of these issues in more depth, but in the meantime, if you wish to
    get a deeper understanding of the theory behind neural networks, the following
    resources are recommended:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里对神经网络的理论进行了非常浅显的介绍，因为我们不希望在第一章就让你感到不知所措！在接下来的章节中，我们将更深入地讨论一些这些问题，但同时，如果你希望更深入理解神经网络背后的理论，以下资源值得推荐：
- en: Chapter 6 of *Goodfellow-et-al* (2016)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Goodfellow等人*（2016年）第六章'
- en: Chapter 11 of *Hastie**,* *T.,* *Tibshirani,* R., and *Friedman,* *J.* (2009),
    which is freely available at [https://web.stanford.edu/~hastie/Papers/ESLII.pdf](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hastie, T.*, *Tibshirani, R.*, 和 *Friedman, J.*（2009年）第11章，免费提供于[https://web.stanford.edu/~hastie/Papers/ESLII.pdf](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)'
- en: Chapter 16 of *Murphy,* *K.* *P.* (2012)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Murphy, K. P.*（2012年）第16章'
- en: Next, we will turn to a brief introduction to deep neural networks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将简要介绍深度神经网络。
- en: Deep neural networks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: A **deep neural network** (**DNN**) is a neural network with multiple hidden
    layers. We cannot achieve good results by just increasing the number of nodes
    in a neural network with a small number of layers (a shallow neural network).
    A DNN can fit data more accurately with fewer parameters than a shallow **neural
    network** (**NN**), because more layers (each with fewer neurons) give a more
    efficient and accurate representation. Using multiple hidden layers allows a more
    sophisticated build-up from simple elements to more complex ones. In the previous
    example, we considered a neural network that could recognize basic shapes, such
    as a circle or a square. In a deep neural network, many circles and squares could
    be combined to form other, more advanced shapes. A shallow neural network cannot
    build more advanced shapes from basic pieces. The disadvantage of a DNN is that
    these models are harder to train and prone to overfitting.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度神经网络**（**DNN**）是一种具有多个隐藏层的神经网络。仅仅增加少量层的神经网络中的节点数量（即浅层神经网络）并不能获得好的结果。DNN能够在参数较少的情况下更准确地拟合数据，因为更多的层（每层有更少的神经元）提供了更高效和更准确的表示。使用多个隐藏层使得从简单元素到更复杂元素的构建更加精细。在前面的例子中，我们考虑了一个可以识别基本形状的神经网络，例如圆形或正方形。在深度神经网络中，许多圆形和正方形可以组合形成其他更复杂的形状。浅层神经网络无法从基本元素构建出更复杂的形状。DNN的缺点是这些模型更难训练，并且容易出现过拟合。'
- en: If we consider trying to recognize handwritten text from image data, then the
    raw data is pixel values from an image. The first layer captures simple shapes,
    such as lines and curves. The next layer uses these simple shapes and recognizes
    higher abstractions, such as corners and circles. The second layer does not have
    to directly learn from the pixels, which are noisy and complex. In contrast, a
    shallow architecture may require far more parameters, as each hidden neuron would
    have to be capable of going directly from pixels in the image to the target value.
    It would also not be able to combine features, so for example, if the image data
    were in a different location (for example, not centered), it would fail to recognize
    the text.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑从图像数据中识别手写文本，那么原始数据就是来自图像的像素值。第一层捕捉简单的形状，如直线和曲线。下一层利用这些简单形状识别更高层次的抽象，例如角落和圆形。第二层不必直接从像素中学习，因为像素是嘈杂和复杂的。相比之下，浅层架构可能需要更多的参数，因为每个隐藏神经元都必须能够直接从图像的像素到达目标值。而且它还无法结合特征，所以例如，如果图像数据的位置不同（例如，没有居中），它将无法识别文本。
- en: One of the challenges in training deep neural networks is how to efficiently
    learn the weights. The models are complex with a huge number of parameters to
    train. One of the major advancements in deep learning occurred in 2006, when it
    was shown that **deep belief networks** (**DBNs**) could be trained one layer
    at a time (See *Hinton,* G. E., *Osindero,* S., and *Teh, Y. W.* (2006)). A DBN
    is a type of deep neural network with multiple hidden layers and connections between
    (but not within) layers (that is, a neuron in layer 1 may be connected to a neuron
    in layer 2, but may not be connected to another neuron in layer 1). The restriction
    of no connections within a layer allows for much faster training algorithms to
    be used, such as the **contrastive divergence algorithm**. Essentially, the DBN
    can then be trained layer by layer; the first hidden layer is trained and used
    to transform raw data into hidden neurons, which are then treated as a new set
    of input in the next hidden layer, and the process is repeated until all the layers
    have been trained.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度神经网络的挑战之一是如何高效地学习权重。模型非常复杂，参数数量庞大。深度学习领域的一个重大进展发生在2006年，当时研究表明**深度置信网络**（**DBNs**）可以逐层训练（见
    *Hinton,* G. E., *Osindero,* S., 和 *Teh, Y. W.* (2006)）。DBN是一种具有多个隐藏层的深度神经网络，层与层之间存在连接（但同一层内的神经元之间没有连接）（即，第一层的一个神经元可以与第二层的神经元连接，但不能与第一层的另一个神经元连接）。没有层内连接的限制使得可以使用更快速的训练算法，例如**对比散度算法**。本质上，DBN可以逐层训练；首先训练第一隐藏层，并用其将原始数据转化为隐藏神经元，随后将这些隐藏神经元作为下一隐藏层的新输入，直到所有层都被训练完毕。
- en: The benefits of the realization that DBNs could be trained one layer at a time
    extend beyond just DBNs. DBNs are sometimes used as a pre-training stage for a
    deep neural network. This allows comparatively fast, greedy, layer-by­-layer training
    to be used to provide good initial estimates, which are then refined in the deep
    neural network using other, less efficient, training algorithms, such as back-propagation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到深度置信网络（DBNs）可以逐层训练的好处，不仅仅局限于DBNs。DBNs有时被用作深度神经网络的预训练阶段。这使得可以使用相对快速的、贪婪的逐层训练方法来提供较好的初始估计，然后使用其他效率较低的训练算法（如反向传播）在深度神经网络中进行优化。
- en: 'So far we have primarily focused on feed-forward neural networks, where the
    results from one layer and neuron feed forward to the next. Before closing this
    section, two specific kinds of deep neural network that have grown in popularity
    are worth mentioning. The first is a **recurrent neural network** (**RNN**), where
    neurons send feedback signals to each other. These feedback loops allow RNNs to
    work well with sequences. An example of an application of RNNs is to automatically
    generate click-bait, such as *Top 10 reasons to visit Los Angeles:* #6 *will shock
    you!* or *One trick great hair salons don''t want you to know*. RNNs work well
    for such jobs as they can be seeded from a large initial pool of a few words (even
    just trending search terms or names) and then predict/generate what the next word
    should be. This process can be repeated a few times until a short phrase is generated,
    the click-bait. We will see examples of RNNs in [Chapter 7](03f666ab-60ce-485a-8090-c158b29ef306.xhtml),
    *Natural Language Processing using Deep Learning*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要集中在前馈神经网络上，其中一层和神经元的结果会传递到下一层。在结束这一节之前，值得提到两种在深度神经网络中越来越流行的特定类型。第一种是**递归神经网络**（**RNN**），在这种网络中，神经元会互相发送反馈信号。这些反馈循环使得RNNs能够很好地处理序列。RNN的一个应用实例是自动生成吸引点击的标题，比如*前往洛杉矶的十大理由：第六条*会让你震惊！*或者*一招让你知道大牌发廊不想让你知道的秘密*。RNN在这类任务中表现优异，因为它们可以从一个较大的初始词汇池（甚至只是流行的搜索词或名字）中获取种子，然后预测/生成下一个词。这个过程可以重复几次，直到生成一个短语，即吸引点击的标题。我们将在[第7章](03f666ab-60ce-485a-8090-c158b29ef306.xhtml)，*使用深度学习进行自然语言处理*中看到RNN的例子。
- en: The second type is a **convolutional neural network **(**CNN**). CNNs are most
    commonly used in image-recognition. CNNs work by having each neuron respond to
    overlapping subregions of an image. The benefits of CNNs are that they require
    comparatively minimal preprocessing but still do not require too many parameters
    through weight-sharing (for example, across subregions of an image). This is particularly
    valuable for images as they are often not consistent. For example, imagine ten
    different people taking a picture of the same desk. Some may be closer or farther
    away or at positions resulting in essentially the same image having different
    heights, widths, and the amount of image captured around the focal object. We
    will cover CNNs in depth in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml),* Image
    Classification Using Convolutional Neural Networks*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型是**卷积神经网络**（**CNN**）。CNNs最常用于图像识别。CNN的工作原理是让每个神经元响应图像的重叠子区域。CNN的优势在于，它们需要较少的预处理，且通过权重共享（例如，跨图像的子区域）仍然不需要太多参数。这对图像特别有价值，因为图像通常不一致。例如，想象十个不同的人拍摄同一张桌子的照片。有些可能离得更近或更远，或者处于导致基本相同的图像却有不同高度、宽度和焦点对象周围捕获图像数量的位置。我们将在[第5章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)，*使用卷积神经网络进行图像分类*中深入探讨CNN。
- en: This description only provides the briefest of overviews as to what deep neural
    networks are and some of the use cases to which they can be applied. The seminal
    reference for deep learning is *Goodfellow-et-al* (2016).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本描述仅提供了深度神经网络的简要概述，以及它们的一些应用场景。深度学习的奠基性参考文献是*Goodfellow等人*（2016）。
- en: Some common myths about deep learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些关于深度学习的常见迷思
- en: 'There are many misconceptions, half-truths, and downright misleading opinions
    on deep learning. Here are some common mis-conceptions regarding deep learning:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习，有许多误解、半真半假的说法和误导性观点。以下是一些关于深度学习的常见误解：
- en: Artificial intelligence means deep learning and replaces all other techniques
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能意味着深度学习，并取代所有其他技术
- en: Deep learning requires a PhD-level understanding of mathematics
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习需要博士级别的数学理解
- en: Deep learning is hard to train, almost an art form
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习难以训练，几乎是一种艺术形式
- en: Deep learning requires lots of data
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习需要大量数据
- en: Deep learning has poor interpretability
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的可解释性差
- en: Deep learning needs GPUs
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习需要GPU
- en: The following paragraphs discuss these statements, one by one.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的段落将逐一讨论这些说法。
- en: Deep learning is not artificial intelligence and does not replace all other
    machine learning algorithms. It is only one family of algorithms in machine learning.
    Despite the hype, deep learning probably accounts for less than 1% of the machine
    learning projects in production right now. Most of the recommendation engines
    and online adverts that you encounter when you browse the net are not powered
    by deep learning. Most models used internally by companies to manage their subscribers,
    for example *churn analysis*, are not deep learning models. The models used by
    credit institutions to decide who gets credit do not use deep learning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不是人工智能，也并不取代所有其他机器学习算法。它只是机器学习中一类算法。尽管有炒作，深度学习可能只占当前生产环境中机器学习项目的不到1%。当你在浏览网络时，遇到的大多数推荐引擎和在线广告并不是由深度学习驱动的。许多公司内部用来管理其订阅者的模型，例如*流失分析*，也不是深度学习模型。用于决定谁能获得信用的信用机构模型也并不使用深度学习。
- en: Deep learning does not require a deep understanding of mathematics unless your
    interest is in researching new deep learning algorithms and specialized architectures.
    Most practitioners use existing deep learning techniques on their data by taking
    an existing architecture and modifying it for their work. This does not require
    a deep mathematical foundation, the mathematics used in deep learning are taught
    at high school level throughout the world. In fact, we demonstrate this in [Chapter
    3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),* Deep Learning Fundamentals*,
    where we build an entire neural network from basic code in less than 70 lines
    of code!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习并不要求对数学有深入理解，除非你有兴趣研究新的深度学习算法和专门的架构。大多数从业者使用现有的深度学习技术处理他们的数据，采用现有架构并根据自己的工作需求进行修改。这并不需要深厚的数学基础，深度学习中使用的数学知识在世界各地的高中阶段就已经教授。事实上，我们在[第三章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)《深度学习基础》中展示了这一点，在不到70行代码的情况下，我们从零开始构建了一个完整的神经网络！
- en: Training deep learning models is difficult but it is not an art form. It does
    require practice, but the same problems occur over and over again. Even better,
    there is often a prescribed fix for that problem, for example, if your model is
    overfitting, add regularization, if your model is not training well, build a more
    complex model and/or use *data augmentation*. We will look at this in more depth
    in [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml), *Tuning and Optimizing
    Models*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型是困难的，但它并不是一种艺术形式。它确实需要练习，但相同的问题会一遍又一遍地出现。更好的是，通常会有针对这些问题的解决方法，例如，如果模型过拟合，可以添加正则化；如果模型训练不佳，可以构建更复杂的模型和/或使用*数据增强*。我们将在[第六章](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml)《调优与优化模型》中深入探讨这一点。
- en: There is a lot of truth to the statement that deep learning requires lots of
    data. However, you may still be able to apply deep learning to the problem by
    using a pre-trained network, or creating more training data from existing data
    (data augmentation). We will look at these in later [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml),
    Tuning and Optimizing Models and [Chapter 11](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml),
    *The Next Level in Deep Learning*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习需要大量数据这一说法是有一定道理的。然而，你仍然可以通过使用预训练网络或从现有数据中创建更多训练数据（数据增强）来将深度学习应用于问题。我们将在后续的[第六章](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml)《调优与优化模型》和[第十一章](94299ae0-c3fc-4f1d-97a8-5e8b85b260e9.xhtml)《深度学习的下一个层次》中探讨这些内容。
- en: Deep learning models are difficult to interpret. By this, we mean being able
    to explain how the models came to their decision. This is a problem in many machine
    learning algorithms, not just deep learning. In machine learning, generally there
    is an inverse relationship between accuracy and interpretation – the more accurate
    the model needs to be, the less interpretable it is. For some tasks, for example,
    online advertising, interpretability is not important and there is little cost
    from being wrong, so the most powerful algorithm is preferred. In some cases,
    for example, credit scoring, interpretability may be required by law; people could
    demand an explanation of why they were denied credit. In other cases, such as
    medical diagnoses, interpretability may be important for a doctor to see why the
    model decided someone had a disease.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型难以解释。我们的意思是能够解释模型是如何做出决策的。这是许多机器学习算法中的问题，不仅仅是深度学习。在机器学习中，通常准确性和可解释性之间存在反比关系——模型需要越准确，它的可解释性就越差。对于某些任务，例如在线广告，可解释性并不重要，而且错误的成本也很小，所以通常选择最强大的算法。而在某些情况下，例如信用评分，法律可能要求可解释性；人们可能会要求解释为什么他们被拒绝了信用。在其他情况下，如医学诊断，可解释性可能对医生理解模型为何认为某人患有疾病至关重要。
- en: 'If interpretability is important, some methods can be applied to machine learning
    models to get an understanding of why they predicted the output for an instance.
    Some of them work by perturbing the data (that is, making slight changes to it)
    and trying to find what variables are most influential in the model coming to
    its decision. One such algorithm is called **LIME **(**Local Interpretable Model-Agnostic
    Explanations**). (*Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. Why
    should I trust you?: Explaining the predictions of any classifier. Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining. ACM, 2016.*) This has been implemented in many languages including R;
    there is a package called `lime`. We will use this package in [Chapter 6](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml),
    *Tuning and Optimizing Models*.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可解释性很重要，可以对机器学习模型应用一些方法，以便了解它们为何对某个实例预测了这个结果。某些方法通过扰动数据（即，对数据进行轻微的修改）来工作，试图找出哪些变量在模型做出决策时最具影响力。一个这样的算法叫做**LIME**（**局部可解释模型无关解释**）。(*Ribeiro,
    Marco Tulio, Sameer Singh, 和 Carlos Guestrin. 为什么我应该相信你？：解释任何分类器的预测。第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集。ACM,
    2016.*) 这个算法已在包括R在内的多种语言中实现；有一个叫做`lime`的包。我们将在[第6章](13e9a742-84df-48e5-bbfd-ade33dcdd01a.xhtml)，*调优与优化模型*中使用这个包。
- en: Finally, while deep learning models can run on CPUs, the truth is that any real
    work requires a workstation with a GPU. This does not mean that you need to go
    out and purchase one, as you can use cloud-computing to train your models. In
    [Chapter 10](2ea4d422-70f7-47af-a330-f0901f6f5fd3.xhtml), *Running Deep Learning
    Models in the Cloud*, will look at using AWS, Azure, and Google Cloud to train
    deep learning models.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虽然深度学习模型可以在CPU上运行，但实际上，任何实际工作都需要配备GPU的工作站。这并不意味着你需要购买一个GPU，因为你可以使用云计算来训练模型。在[第10章](2ea4d422-70f7-47af-a330-f0901f6f5fd3.xhtml)，*在云中运行深度学习模型*，我们将讨论使用AWS、Azure和Google
    Cloud来训练深度学习模型。
- en: Setting up your R environment
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置你的R环境
- en: 'Before you begin your deep learning journey, the first step is to install R,
    which is available at [https://cran.r-project.org/](https://cran.r-project.org/).
    When you download R and use it, only a few core packages are installed by default,
    but new packages can be added by selecting from a menu option or by a single line
    of code. We will not go into detail on how to install R or how to add packages,
    we assume that most readers are proficient in these skills. A good **integrated
    development environment** (**IDE**) for working with R is essential. By far the
    most popular IDE, and my recommendation, is RStudio, which can be downloaded from
    [https://www.rstudio.com/](https://www.rstudio.com/). Another option is **Emacs**.
    An advantage of both Emacs and RStudio is that they are available on all major
    platforms (Windows, macOS, and Linux), so even if you switch computers, you can
    have a consistent IDE experience. The following is a screenshot of the RStudio
    IDE:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始你的深度学习之旅之前，第一步是安装 R，R 可在 [https://cran.r-project.org/](https://cran.r-project.org/)
    下载。当你下载并使用 R 时，默认只安装了一些核心包，但可以通过选择菜单选项或一行代码来添加新包。我们不会详细讨论如何安装 R 或如何添加包，因为我们假设大多数读者已经熟练掌握这些技能。一个好的**集成开发环境**（**IDE**）对于使用
    R 至关重要。到目前为止，最受欢迎的 IDE，也是我推荐的，是 RStudio，可以从 [https://www.rstudio.com/](https://www.rstudio.com/)
    下载。另一个选择是 **Emacs**。Emacs 和 RStudio 的一个优点是它们都可以在所有主要平台（Windows、macOS 和 Linux）上使用，因此即使你更换电脑，也能保持一致的
    IDE 体验。以下是 RStudio IDE 的截图：
- en: '![](img/922367db-1bcf-4da6-add0-13979facce86.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/922367db-1bcf-4da6-add0-13979facce86.png)'
- en: Figure 1.7 RStudio IDE
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 RStudio IDE
- en: Using RStudio is a major improvement over the R GUI in Windows. There are a
    number of panes in RStudio that provide different perspectives on your work. The
    top-left pane shows the code, the bottom-left pane shows the console (results
    of running the code). The top-right pane shows the list of variables and their
    current values, the bottom-right pane shows the plots created by the code. All
    of these panes have further tabs to explore further perspectives.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RStudio 相比于 Windows 上的 R 图形用户界面（GUI）有了显著的提升。在 RStudio 中有多个面板，可以从不同的角度查看你的工作。左上方的面板显示代码，左下方的面板显示控制台（运行代码的结果）。右上方的面板显示变量列表及其当前值，右下方的面板显示代码生成的图表。这些面板还包含更多的标签页，可以进一步探索不同的视角。
- en: 'As well as an IDE, RStudio (the company) have either developed or heavily supported
    other tools and packages for the R environment. We will use some of these tools,
    including the R Markdown and R Shiny applications. R Markdown is similar to Jupyter
    or IPython notebooks; it allows you to combine code, output (for example, plots),
    and documentation in one script. R Markdown was used to create sections of this
    book where code and descriptive text are interwoven. R Markdown is a very good
    tool to ensure that your data science experiments are documented correctly. By
    embedding the documentation within the analysis, they are more likely to stay
    synchronized. R Markdown can output to HTML, Word, or PDF. The following is an
    example of an R Markdown script on the left and the output on the right:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 IDE，RStudio（公司）还开发或大力支持了 R 环境中的其他工具和包。我们将使用其中一些工具，包括 R Markdown 和 R Shiny
    应用程序。R Markdown 类似于 Jupyter 或 IPython 笔记本；它允许你将代码、输出（例如图表）和文档结合在一个脚本中。R Markdown
    被用来创建本书中的部分章节，其中代码和描述性文本交织在一起。R Markdown 是一个非常好的工具，可以确保你的数据科学实验得到正确的文档记录。通过将文档嵌入到分析中，它们更有可能保持同步。R
    Markdown 可以输出为 HTML、Word 或 PDF。以下是一个 R Markdown 脚本的示例，左侧是代码，右侧是输出：
- en: '![](img/3757d7ee-368f-4d7a-87e3-63754c8b01c0.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3757d7ee-368f-4d7a-87e3-63754c8b01c0.png)'
- en: 'Figure 1.8: R Markdown example; on the left is a mixture of R code and text
    information. The output on the right is HTML generated from the source script.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8：R Markdown 示例；左侧是 R 代码和文本信息的混合，右侧是从源脚本生成的 HTML 输出。
- en: 'We will also use R Shiny to create web applications using R. This is an excellent
    method to create interactive applications to demonstrate key functionality. The
    following screenshot is an example of an R Shiny web application, which we will
    see in [Chapter 5](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml), *Image Classification
    Using Convolutional Neural Networks*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用 R Shiny 创建基于 R 的网页应用程序。这是一种创建交互式应用程序以展示关键功能的极佳方法。以下截图是一个 R Shiny 网页应用程序的示例，我们将在[第
    5 章](1c0b9897-b0cc-4a8f-9ce8-e6409c347f4f.xhtml)中看到，*卷积神经网络的图像分类*：
- en: '![](img/a6dee86b-bfb0-4424-898a-4d2471640ca6.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6dee86b-bfb0-4424-898a-4d2471640ca6.png)'
- en: 'Figure 1.9: An example of an R Shiny web application'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.9：一个 R Shiny 网页应用程序的示例
- en: Once you have R installed, you can look at adding packages that can fit basic
    neural networks. The `nnet` package is one package and it can fit feed-forward
    neural networks with one hidden layer, such as the one shown in *Figure* 1.6\.
    For more details on the `nnet` package, see *Venables, W.* *N.* and *Ripley, B.*
    D. (2002). The `neuralnet` package fits neural networks with multiple hidden layers
    and can train them using back-propagation. It also allows custom error and neuron-activation
    functions. We will also use the `RSNNS` package, which is an R wrapper of the
    **Stuttgart Neural Network Simulator** (**SNNS**). The SNNS was originally written
    in C, but was ported to C++. The `RSNNS` package makes many model components from
    SNNS available, making it possible to train a wide variety of models. For more
    details on the `RSNNS` package, see *Bergmeir,* C., and *Benitez,* *J.* *M.* (2012).
    We will see examples of how to use these models in [Chapter 2](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml),
    *Training a Prediction Model*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 安装R后，你可以添加适用于基本神经网络的包。`nnet`包就是其中之一，它可以拟合具有一个隐藏层的前馈神经网络，例如*图*1.6所示的网络。关于`nnet`包的更多细节，请参见*Venables,
    W.* *N.* 和 *Ripley, B.* D.（2002）。`neuralnet`包可以拟合具有多个隐藏层的神经网络，并使用反向传播进行训练。它还允许自定义误差和神经元激活函数。我们还将使用`RSNNS`包，这是**斯图加特神经网络模拟器**（**SNNS**）的R包装器。SNNS最初是用C语言编写的，后来移植到C++。`RSNNS`包使许多SNNS的模型组件得以使用，从而可以训练多种类型的模型。有关`RSNNS`包的更多详细信息，请参见*Bergmeir,*
    C. 和 *Benitez,* *J.* *M.*（2012）。我们将在[第二章](cb00118a-2bba-4e43-ba55-c4552c508b7e.xhtml)
    *训练预测模型*中看到如何使用这些模型的示例。
- en: The `deepnet` package provides a number of tools for deep learning in R. Specifically,
    it can train RBMs and use these as part of DBNs to generate initial values to
    train deep neural networks. The `deepnet` package also allows for different activation
    functions, and the use of dropout for regularization.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`deepnet`包提供了多种深度学习工具，特别是它可以训练RBM，并将其用作DBN的一部分，生成初始值以训练深度神经网络。`deepnet`包还允许使用不同的激活函数，并支持使用dropout进行正则化。'
- en: Deep learning frameworks for R
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R的深度学习框架
- en: There are a number of R packages available for neural networks, but few options
    for deep learning. When the first edition of this book came out, it used the deep
    learning functions in h2o ([https://www.h2o.ai/](https://www.h2o.ai/)). This is
    an excellent, general machine learning framework written in Java, and has an API
    that allows you to use it from R. I recommend you look at it, especially for large
    datasets. However, most deep learning practitioners had a preference preferred
    other deep learning libraries, such as TensorFlow, CNTK, and MXNet, which were
    not supported in R when the first edition of this book was written. Today, there
    is a good choice of deep learning libraries that are supported in R—MXNet and
    Keras. Keras is actually a frontend abstraction for other deep learning libraries,
    and can use TensorFlow in the background. We will use MXNet, Keras, and TensorFlow in
    this book.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多R包可用于神经网络，但深度学习的选择较少。当本书的第一版发布时，它使用了h2o中的深度学习功能（[https://www.h2o.ai/](https://www.h2o.ai/)）。这是一个由Java编写的优秀通用机器学习框架，且有一个API可以从R中使用。我推荐你去看一下，特别是当你处理大型数据集时。然而，大多数深度学习从业者更倾向于使用其他深度学习库，如TensorFlow、CNTK和MXNet，而这些库在本书第一版编写时并未在R中得到支持。如今，R中已经有了很多支持的深度学习库——MXNet和Keras。Keras实际上是其他深度学习库的前端抽象，并可以在后台使用TensorFlow。本书中，我们将使用MXNet、Keras和TensorFlow。
- en: MXNet
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MXNet
- en: MXNet is a deep learning library developed by Amazon. It can run on CPUs and
    GPUs. For this chapter, running on CPUs will suffice.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet是由Amazon开发的深度学习库，支持在CPU和GPU上运行。本章中，只在CPU上运行即可。
- en: Apache MXNet is a flexible and scalable deep learning framework that supports
    **convolutional neural networks** (**CNNs**) and **long short-term memory networks**
    (**LSTMs**). It can be distributed across multiple processors/machines and achieves
    almost linear scale on multiple GPUs/CPUs. It is easy to install on R and it supports
    a good range of deep learning functionality for R. It is an excellent choice for
    writing our first deep learning model for image-classification.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Apache MXNet是一个灵活且可扩展的深度学习框架，支持**卷积神经网络**（**CNNs**）和**长短期记忆网络**（**LSTMs**）。它可以在多个处理器/机器上分布式运行，并在多个GPU/CPU上几乎实现线性扩展。它可以轻松安装到R中，并支持R的广泛深度学习功能。它是编写我们第一个深度学习图像分类模型的绝佳选择。
- en: MXNet originated at *Carnegie Mellon University* and is heavily supported by
    Amazon; they chose it as their default deep learning library in 2016\. In 2017,
    MXNet was accepted as the *Apache Incubator* project, ensuring that it would remain
    open source software. It has a higher-level programming model similar to Keras,
    but the reported performance is better. MXNet is very scalable as additional GPUs
    are added.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 起源于 *卡内基梅隆大学*，并且得到了 Amazon 的大力支持；他们在 2016 年将其选为默认的深度学习库。2017 年，MXNet 被接受为
    *Apache Incubator* 项目，确保其保持开源软件。它有一个类似于 Keras 的更高层次编程模型，但报告的性能更好。随着增加更多 GPU，MXNet
    具有很好的可扩展性。
- en: 'To install the MXNet package for Windows, run the following code from an R
    session:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 Windows 安装 MXNet 包，请在 R 会话中运行以下代码：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This installs the CPU version; for the GPU version, you need to change the
    second line to:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装 CPU 版本；如果需要 GPU 版本，你需要将第二行更改为：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You have to change `cu92` to `cu80`, `cu90` or `cu91` based on the version of CUDA
    installed on your machine. For other operating systems (and in case the this does
    not work, as things change very fast in deep learning), you can get further instructions
    at [https://mxnet.incubator.apache.org/install/index.html](https://mxnet.incubator.apache.org/install/index.html).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须根据你机器上安装的 CUDA 版本将 `cu92` 更改为 `cu80`、`cu90` 或 `cu91`。对于其他操作系统（以及万一此方法不工作，因为深度学习领域变化很快），你可以参考
    [https://mxnet.incubator.apache.org/install/index.html](https://mxnet.incubator.apache.org/install/index.html)
    获取进一步的安装说明。
- en: Keras
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: '**Keras** is a high-level, open source, deep learning framework created by
    Francois Chollet from Google that emphasizes iterative and fast development; it
    is generally regarded as one of the best options to use to learn deep learning.
    Keras has a choice of backend lower-level frameworks: TensorFlow, Theano, or CNTK,
    but it is most commonly used with TensorFlow. Keras models can be deployed on
    practically any environment, for example, a web server, iOS, Android, a browser,
    or the Raspberry Pi.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**Keras** 是一个由 Google 的 François Chollet 创建的高层次开源深度学习框架，强调迭代和快速开发；它通常被认为是学习深度学习的最佳选择之一。Keras
    可以选择后端低层次的框架：TensorFlow、Theano 或 CNTK，但它最常与 TensorFlow 一起使用。Keras 模型几乎可以部署到任何环境中，例如，Web
    服务器、iOS、Android、浏览器或 Raspberry Pi。'
- en: 'To learn more about Keras, go to [https://keras.io/](https://keras.io/). To
    learn more about using Keras in R, go to [https://keras.rstudio.com](https://keras.rstudio.com);
    this link will also has more examples of R and Keras, as well as a handy Keras
    cheat sheet that gives a thorough reference to all of the functionality of the
    R Keras package. To install the `keras` package for R, run the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Keras 的信息，请访问 [https://keras.io/](https://keras.io/)。要了解如何在 R 中使用 Keras，请访问
    [https://keras.rstudio.com](https://keras.rstudio.com)；该链接还提供了更多关于 R 和 Keras 的示例，并且有一张方便的
    Keras 备忘单，详细介绍了 R Keras 包的所有功能。要为 R 安装 `keras` 包，请运行以下代码：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will install the CPU-based package of Keras and TensorFlow. If your machine
    has a suitable GPU, you can refer to the documentation for `install_keras()` to
    find out how to install it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装基于 CPU 的 Keras 和 TensorFlow 包。如果你的机器有合适的 GPU，可以参考 `install_keras()` 的文档，了解如何安装它。
- en: Do I need a GPU (and what is it, anyway)?
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我需要 GPU 吗（它到底是什么）？
- en: 'Probably the two biggest reasons for the exponential growth in deep learning
    are:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习指数级增长的两个最大原因可能是：
- en: The ability to accumulate, store, and process large datasets of all types
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够累积、存储和处理各种类型的大型数据集
- en: The ability to use GPUs to train deep learning models
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPU 训练深度学习模型的能力
- en: So what exactly are GPUs and why are they so important to deep learning? Probably
    the best place to start is by actually looking at the CPU and why this is not
    optimal for training deep learning models. The CPU in a modern PC is one of the
    pinnacles of human design and engineering. Even the chip in a mobile phone is
    more powerful now than the entire computer systems of the first space shuttles.
    However, because they are designed to be good at all tasks, they may not be the
    best option for niche tasks. One such task is high-end graphics.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，GPU 到底是什么？它们为什么对深度学习如此重要？可能最好的起点是实际看看 CPU，并分析为什么它不适合训练深度学习模型。现代 PC 中的 CPU
    是人类设计与工程的巅峰之一。即使是手机中的芯片，现在也比第一代航天飞机的整个计算机系统还要强大。然而，由于 CPU 是为处理所有任务而设计的，它们可能不是处理某些特殊任务的最佳选择。一个这样的任务就是高端图形处理。
- en: If we take a step back to the mid-1990s, most games were 2D, for example, platform
    games where the character in the game jumps between platforms and/or avoids obstacles.
    Today, almost all computer games utilize 3D space. Modern consoles and PCs have
    co-processors that take the load of modelling 3D space onto a 2D screen. These co-processors
    are known as **GPUs**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到上世纪90年代中期，大多数游戏都是2D的，例如平台游戏，在这些游戏中，角色要在各种平台之间跳跃和/或避开障碍物。今天，几乎所有的计算机游戏都利用了3D空间。现代主机和个人电脑都有协处理器，负责将3D空间建模到2D屏幕上。这些协处理器被称为**GPU**。
- en: 'GPUs are actually far simpler than CPUs. They are built to just do one task:
    massively parallel matrix operations. CPUs and GPUs both have *cores*, where the
    actual computation takes place. A PC with an Intel i7 CPU has four physical cores
    and eight virtual cores by using *Hyper Threading*. The NVIDIA TITAN Xp GPU card
    has 3,840 CUDA® cores. These cores are not directly comparable; a core in a CPU
    is much more powerful than a core in a GPU. But if the workload requires a large
    amount of matrix operations that can be done independently, a chip with lots of
    simple cores is much quicker.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: GPU实际上比CPU简单得多。它们被设计为只做一件事：大规模并行矩阵运算。CPU和GPU都有*核心*，实际计算发生在这些核心中。一台配有Intel i7
    CPU的PC有四个物理核心和通过*超线程*使用的八个虚拟核心。NVIDIA TITAN Xp GPU卡有3840个CUDA®核心。这些核心不是直接可比的；CPU中的核心比GPU中的核心更强大。但是，如果工作负载需要大量可以独立完成的矩阵操作，具有许多简单核心的芯片会快得多。
- en: 'Before deep learning was even a concept, researchers in neural networks realized
    that doing high-end graphics and training neural networks both involved workloads:
    large amounts of matrix multiplication that could be done in parallel. They realized
    that training the models on the GPU rather than the CPU would allow them to create
    much more complicated models.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习甚至成为一个概念之前，神经网络研究人员意识到，进行高端图形和训练神经网络都涉及到工作负载：大量可以并行进行的矩阵乘法。他们意识到，在GPU上训练模型而不是CPU将允许他们创建更复杂的模型。
- en: Today, all deep learning frameworks run on GPUs as well as CPUs. In fact, if
    you want to train models from scratch and/or have a large amount of data, you
    almost certainly need a GPU. The GPU must be an NVIDIA GPU and you also need to
    install the CUDA® Toolkit, NVIDIA drivers, and cuDNN. These allow you to interface
    with the GPU and *hijack* its use from a graphics card to a maths co-processor.
    Installing these is not always easy, you have to ensure that the versions of CUDA,
    cuDNN and the deep learning libraries you use are compatible. Some people advise
    you need to use Unix rather than Windows, but support on Windows has improved
    greatly. This code on this book was developed on a Windows workstation. Forget
    about using a macOS, because they don't support NVIDIA cards.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，所有深度学习框架都在GPU和CPU上运行。事实上，如果你想要从头开始训练模型和/或有大量数据，你几乎肯定需要一块GPU。GPU必须是NVIDIA
    GPU，你还需要安装CUDA® Toolkit、NVIDIA驱动程序和cuDNN。这些允许你与GPU进行接口交互，*劫持*它的使用，从一个图形卡转变为数学协处理器。安装这些并不总是容易，你必须确保你使用的CUDA、cuDNN版本和深度学习库是兼容的。有些人建议你需要使用Unix而不是Windows，但是Windows上的支持已经大大改善。这本书的代码是在Windows工作站上开发的。忘记在macOS上使用，因为它们不支持NVIDIA卡。
- en: That was the bad news. The good news is that you can learn everything about
    deep learning if you don't have a suitable GPU. The examples in the early chapters
    of this book will run perfectly fine on a modern PC. When we need to scale up,
    the book will explain how to use cloud resources, such as AWS and Google Cloud,
    to train large deep learning models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是坏消息。好消息是，即使没有合适的GPU，你也可以学会关于深度学习的一切。本书早期章节的例子在现代PC上运行得很好。当我们需要扩展时，本书将解释如何利用云资源，如AWS和Google
    Cloud，来训练大型深度学习模型。
- en: Setting up reproducible results
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置可复现的结果
- en: Software for data science is advancing and changing rapidly. Although this is wonderful
    for progress, it can make reproducing someone else's results a challenge. Even
    your own code may not work when you go back to it a few months later. This is
    one of the biggest issues in scientific research today, across all fields, not
    just artificial intelligence and machine learning. If you work in research or
    academia and you want to publish your results in scientific journals, this is
    something you need to be concerned about. The first edition of this book partially
    addressed this problem by using the R checkpoint package provided by Revolution
    Analytics. This makes a record of what versions of software were used and ensures
    there is a snapshot of them available.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学软件正在快速发展和变化。虽然这对于进步是令人兴奋的，但也可能使得重现他人的结果变得具有挑战性。即使是您自己的代码，在几个月后回过头来看，也可能无法正常工作。这是当今科学研究中的一个重大问题，涉及所有领域，不仅仅是人工智能和机器学习。如果您从事科研或学术工作，并且想要在科学期刊上发布您的研究成果，那么这是您需要关注的问题。本书的第一版部分解决了这个问题，使用了Revolution
    Analytics提供的R checkpoint包。该包记录了使用过的软件版本，并确保可以获取其快照。
- en: 'For the second edition, we will not use this package for a number of reasons:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二版，我们将不再使用此包，原因有多个：
- en: Most readers are probably not publishing their work and are more interested
    in other concerns (maximizing accuracy, interpretability, and so on).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数读者可能并不会发布自己的作品，而是更关注其他问题（如最大化准确性、可解释性等）。
- en: Deep learning requires large datasets. When you have a large amount of data,
    it should mean that, while we may not get precisely the same result each time,
    it will be very close (fractions of percentages).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习需要大规模的数据集。当您拥有大量数据时，意味着虽然每次结果可能不完全相同，但差距非常小（仅为百分比的几分之一）。
- en: In production systems, there is more to reproducibility than software. You also
    have to consider data pipelines and random seed-generation.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产系统中，重现性不仅仅是软件的问题。您还必须考虑数据管道和随机种子生成。
- en: In order to ensure reproducibility, the libraries used must stay frozen. New
    versions of deep learning APIs are released constantly and may contain enhancements.
    If we limited ourselves to old versions, we would get poor results.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了确保可重现性，所使用的库必须保持冻结状态。深度学习API的新版不断发布，并可能包含改进。如果我们仅限于使用旧版本，我们可能会得到较差的结果。
- en: If you are interested in learning more about the `checkpoint` package, you can
    read the online vignette for the package at [https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html](https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣了解更多关于`checkpoint`包的信息，可以阅读该包的在线手册：[https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html](https://cran.r-project.org/web/packages/checkpoint/vignettes/checkpoint.html)。
- en: This book was written using R version 3.5 on Windows 10 Professional x64, which
    is the latest version of R at the time of writing. The code was run on a machine
    with an Intel i5 processor and 32 GB RAM; it should run on an Intel i3 processor with
    8 GB RAM.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是使用Windows 10 Professional x64版的R 3.5编写的，这是本书编写时R的最新版本。代码是在搭载Intel i5处理器和32
    GB内存的计算机上运行的；它也应能在搭载Intel i3处理器和8 GB内存的计算机上运行。
- en: You can download the example code files for this book from your account at [http://www.packtpub.com/](http://www.packtpub.com/).
    If you purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从您的帐户中下载本书的示例代码文件：[http://www.packtpub.com/](http://www.packtpub.com/)。如果您在其他地方购买了本书，可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册，将文件直接通过电子邮件发送给您。
- en: 'You can download the code files by following these steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照以下步骤下载代码文件：
- en: Log in or register to our website using your email address and password.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您的电子邮件地址和密码登录或注册我们的网站。
- en: Hover the mouse pointer on the **SUPPORT** tab at the top.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将鼠标指针悬停在顶部的**支持**选项卡上。
- en: Click on **Code Downloads & Errata**.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**代码下载与勘误**。
- en: Enter the name of the book in the **Search box**.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**搜索框**中输入书名。
- en: Select the book for which you're looking to download the code files.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您希望下载代码文件的书籍。
- en: Choose from the drop-down menu where you purchased this book from.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从下拉菜单中选择您购买本书的地方。
- en: Click on **Code Download**.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**代码下载**。
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 文件下载后，请确保使用最新版本的以下工具解压或提取文件夹：
- en: WinRAR I 7-Zip for Windows
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WinRAR I 7-Zip for Windows
- en: Zipeg I iZip I UnRarX for Mac
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zipeg I iZip I UnRarX for Mac
- en: 7-Zip I PeaZip for Linux
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7-Zip I PeaZip for Linux
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter presented a brief introduction to neural networks and deep neural
    networks. Using multiple hidden layers, deep neural networks have been a revolution
    in machine learning. They consistently outperform other machine learning tasks,
    especially in areas such as computer vision, natural-language processing, and
    speech-recognition.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要介绍了神经网络和深度神经网络。通过使用多个隐藏层，深度神经网络在机器学习中引发了一场革命。它们在各种机器学习任务中始终表现优于其他方法，特别是在计算机视觉、自然语言处理和语音识别等领域。
- en: The chapter also looked at some of the theory behind neural networks, the difference
    between shallow neural networks and deep neural networks, and some of the misconceptions
    that currently exist concerning deep learning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还探讨了神经网络背后的部分理论，浅层神经网络与深层神经网络的区别，以及当前关于深度学习的一些误解。
- en: We closed this chapter with a discussion on how to set up R and the importance
    of using a GUI (RStudio). This section discussed the deep learning libraries available
    in R (MXNet, Keras, and TensorFlow), GPUs, and reproducibility.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章结束时讨论了如何设置R以及使用GUI（RStudio）的重要性。本节讨论了R中可用的深度学习库（MXNet、Keras和TensorFlow）、GPU以及可重复性。
- en: In the next chapter, we will begin to train neural networks and generate our
    own predictions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始训练神经网络并生成我们自己的预测。
