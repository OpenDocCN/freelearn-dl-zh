- en: Recurrent Neural Networks
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: This chapter introduces recurrent neural networks, starting with the basic model
    and moving on to *newer* recurrent layers that are able to handle internal memory
    learning to remember, or forget, certain patterns found in datasets. We will begin
    by showing that recurrent networks are powerful in the case of inferring patterns
    that are temporal or sequential, and then we will introduce an improvement on
    the traditional paradigm for a model that has internal memory, which can be applied
    in both directions in the temporal space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了循环神经网络，从基本模型开始，逐步介绍能够处理内部记忆学习的*新型*循环层，这些层能够记住或忘记数据集中的某些模式。我们将首先展示，循环网络在推断时间序列或顺序模式时非常强大，然后我们将介绍对传统范式的改进，提出具有内部记忆的模型，这个模型可以在时间空间中双向应用。
- en: We will approach the learning task by looking at a sentiment analysis problem
    as a sequence-to-vector application, and then we will focus on an autoencoder
    as a vector-to-sequence and sequence-to-sequence model at the same time. By the
    end of this chapter, you will be able to explain why a long short-term memory
    model is better than the traditional dense approach. You will be able to describe
    how a bi-directional long short-term memory model might represent an advantage
    over the single directional approach. You will be able to implement your own recurrent
    networks and apply them to NLP problems or to image-related applications, including
    sequence-to-vector, vector-to-sequence, and sequence-to-sequence modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将情感分析问题作为序列到向量的应用来接近学习任务，然后我们将专注于自动编码器，同时作为向量到序列和序列到序列模型。到本章结束时，你将能够解释为什么长短期记忆模型比传统的密集方法更优。你将能够描述双向长短期记忆模型如何在单向方法上可能具有优势。你将能够实现自己的循环神经网络，并将其应用于自然语言处理问题或图像相关的应用，包括序列到向量、向量到序列和序列到序列建模。
- en: 'This chapter is organized as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构如下：
- en: Introduction to recurrent neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络简介
- en: Long short-term memory models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆模型
- en: Sequence-to-vector models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到向量模型
- en: Vector-to-sequence models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量到序列模型
- en: Sequence-to-sequence models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: Ethical implications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伦理学影响
- en: Introduction to recurrent neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络简介
- en: '**Recurrent neural networks** (**RNNs**) are based on the early work of Rumelhart
    (Rumelhart, D. E., et al. (1986)), who was a psychologist who worked closely with
    Hinton, whom we have already mentioned here several times. The concept is simple,
    but revolutionary in the area of pattern recognition that uses sequences of data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）基于Rumelhart的早期工作（Rumelhart, D. E.等人，1986），他是一位心理学家，与我们之前提到的Hinton密切合作。这个概念很简单，但在使用数据序列进行模式识别的领域却具有革命性。'
- en: A **sequence of data** is any piece of data that has high correlation in either
    time or space. Examples include audio sequences and images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据序列**是任何在时间或空间上具有高度相关性的数据片段。例如，音频序列和图像。'
- en: 'The concept of recurrence in RNNs can be illustrated as shown in the following
    diagram. If you think of a dense layer of neural units, these can be stimulated
    using some input at different time steps, ![](img/8e859b39-e2bf-4310-b7df-7d1aa72d46ca.png).
    *Figures 13.1 (b)* and *(c)* show an RNN with five time steps, ![](img/9c53c6b3-e449-4a6f-9c95-12ff4cb30010.png).
    We can see in *Figures 13.1 (b)* and *(c)* how the input is accessible to the
    different time steps, but more importantly, the output of the neural units is
    also available to the next layer of neurons:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: RNN中的递归概念可以通过以下图示来说明。如果你认为神经单元的密集层，这些层可以在不同的时间步上通过某些输入来激活，![](img/8e859b39-e2bf-4310-b7df-7d1aa72d46ca.png)。*图13.1
    (b)*和*(c)*展示了一个具有五个时间步的RNN，![](img/9c53c6b3-e449-4a6f-9c95-12ff4cb30010.png)。我们可以在*图13.1
    (b)*和*(c)*中看到，输入如何在不同的时间步之间可访问，但更重要的是，神经单元的输出也可以提供给下一层神经元：
- en: '![](img/1f337f3e-cee6-404b-a74b-db34d983cdfa.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f337f3e-cee6-404b-a74b-db34d983cdfa.png)'
- en: 'Figure 13.1\. Different representations of recurrent layers: (a) will be the
    preferred use in this book; (b) depicts the neural units and the feedback loop;
    and (c) is the expanded version of (b), showing what really happens during training'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1\. 循环层的不同表示：（a）是本书中优选的使用方式；（b）展示了神经单元和反馈回路；（c）是（b）的扩展版本，展示了训练过程中实际发生的情况。
- en: 'The ability of an RNN to see how the previous layer of neurons is stimulated
    helps the network to interpret sequences much better than without that additional
    piece of information. However, this comes at a cost: there will be more parameters
    to be calculated in comparison to a traditional dense layer due to the fact that
    there are weights associated with the input ![](img/37624e33-af8b-4b40-8359-c74bd9dec3cd.png) and
    the previous output ![](img/5869c1cc-4186-404b-802e-caadae5f3bbe.png).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 能够看到前一层神经元的激活情况，这有助于网络更好地理解序列，而不是没有这些附加信息。然而，这也带来了成本：与传统的密集层相比，计算的参数会更多，因为输入
    ![](img/37624e33-af8b-4b40-8359-c74bd9dec3cd.png) 和前一个输出 ![](img/5869c1cc-4186-404b-802e-caadae5f3bbe.png)
    都需要有权重。
- en: Simple RNNs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单 RNN
- en: 'In Keras, we can create a simple RNN with **five time steps** and **10 neural
    units** (see *Figure 13.1*) as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，我们可以创建一个简单的 RNN，具有**五个时间步长**和**10 个神经单元**（参见*图 13.1*），代码如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This gives the following summary:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下总结：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding sample code assumes that the number of **features in the input**
    would be just **two**; for example, we can have sequential data in two dimensions.
    These types of RNNs are called *simple* because they resemble the simplicity of
    dense networks with `tanh` activations and a recurrence aspect to it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例代码假设**输入的特征数量**只有**两个**；例如，我们可以在二维中拥有顺序数据。这类 RNN 被称为*简单*的，因为它们类似于具有 `tanh`
    激活函数和递归特性的密集网络。
- en: RNNs are usually tied to embedding layers, which we discuss next.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通常与嵌入层配合使用，接下来我们将讨论这一点。
- en: Embedding layers
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入层
- en: An embedding layer is usually paired with RNNs when there are sequences that
    require additional processing in order to make RNNs more robust. Consider the
    case when you have the sentence *"This is a small vector"*, and you want to train
    an RNN to detect when sentences are correctly written or poorly written. You can
    train an RNN with all the sentences of length five that you can think of, including *"This
    is a small vector".* For this, you will have to figure out a way to transform
    a sentence into something that the RNN can understand. Embedding layers come to
    the rescue.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层通常与 RNN 配合使用，尤其是在需要额外处理的序列中，以增强 RNN 的鲁棒性。考虑以下情境，当你有一句话 *"This is a small
    vector"*，并且你想训练一个 RNN 来检测句子是否写得正确或写得不好。你可以使用所有你能想到的长度为五的句子来训练 RNN，包括 *"This is
    a small vector"*。为此，你需要找到一种方法将句子转化为 RNN 可以理解的形式。嵌入层就派上了用场。
- en: 'There is a technique called **word embedding**, which is tasked with converting
    a word into a vector. There are several successful approaches out there, such
    as Word2Vec (Mikolov, T., et al. (2013)) or GloVe (Pennington, J., et al. (2014)).
    However, we will focus on a simple technique that is readily available. We will
    do this in steps:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种叫做**词嵌入**的技术，它的任务是将一个词转换成一个向量。现在有几种成功的实现方法，例如 Word2Vec（Mikolov, T., 等人（2013））或
    GloVe（Pennington, J., 等人（2014））。然而，我们将重点介绍一种简单且易于使用的技术。我们将分步进行：
- en: Determine the length of the sentences you want to work on. This will become
    the dimensionality of the input for the RNN layer. This step is not necessary
    for the design of the embedding layer, but you will need it for the RNN layer
    very soon, and it is important that you decide this early on.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定你想要处理的句子的长度。这将成为 RNN 层输入的维度。虽然这一步对设计嵌入层不是必需的，但你很快就会需要它，并且在早期决定这一点非常重要。
- en: 'Determine the number of different words in your dataset and assign a number
    to them, creating a dictionary: word-to-index. This is known as a vocabulary.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定数据集中不同单词的数量，并为它们分配一个数字，创建一个字典：词到索引。这被称为词汇表。
- en: Most people will determine the vocabulary and then calculate the frequency of
    each word to rank the words in the vocabulary so as to have the index 0 corresponding
    to the most common word in the dataset, and the last index corresponding to the
    most uncommon word. This can be helpful if you want to ignore the most common
    words or the most uncommon words, for example.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人会先确定词汇表，然后计算每个单词的频率，将词汇表中的单词按频率排序，使得索引 0 对应数据集中最常见的单词，最后一个索引对应最不常见的单词。如果你希望忽略最常见或最不常见的单词，这种方法可能会很有帮助。
- en: Substitute the words in all the sentences of the dataset with their corresponding
    index.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用对应的索引替换数据集中所有句子中的单词。
- en: Determine the dimensionality of the word embedding and train an embedding layer
    to map from the numerical index into a real-valued vector with the desired dimensions.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定词嵌入的维度，并训练一个嵌入层，将数字索引映射到具有所需维度的实值向量。
- en: Look at the example in *Figure 13.2*. If we take the word *This*, whose given
    index is 7, some trained embedding layer can map that number into a vector of
    size 10, as you can see in *Figure 13.2 (b)*. That is the word embedding process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看看*图13.2*中的例子。如果我们取单词*This*，它的索引是7，一些训练过的嵌入层可以将这个数字映射到一个大小为10的向量，如你在*图13.2 (b)*中看到的那样。这就是词嵌入过程。
- en: You can repeat this process for the complete sentence *"This is a small vector"*,
    which can be mapped to a **sequence** of indices [7, 0, 6, 1, 28], and it will
    produce for you a **sequence** of vectors; see *Figure 13.2 (c)*. In other words,
    it will produce a **sequence of word embeddings**. The RNN can easily process
    these sequences and determine whether the sentence that these sequences represent
    is a correct sentence.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对完整的句子*"This is a small vector"*重复这个过程，它可以映射到一个**索引序列** [7, 0, 6, 1, 28]，并为你生成一个**词向量序列**；见*图13.2
    (c)*。换句话说，它会生成一个**词嵌入序列**。RNN可以轻松处理这些序列，并判断这些序列所代表的句子是否正确。
- en: 'However, we must say that determining whether a sentence is correct is a challenging
    and interesting problem (Rivas, P. et al. (2019)):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须说，确定一个句子是否正确是一个具有挑战性且有趣的问题（Rivas, P.等人，2019年）：
- en: '![](img/d91edafe-3594-4b85-890f-9cb37ef14142.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d91edafe-3594-4b85-890f-9cb37ef14142.png)'
- en: 'Figure 13.2\. Embedding layer: (a) will be the preferred use in this book;
    (b) shows an example of a word embedding; and (c) shows a sequence of words and
    its corresponding matrix of word embeddings'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2. 嵌入层：(a) 是本书中推荐使用的形式；(b) 展示了一个词嵌入的例子；(c) 展示了一个词序列及其对应的词嵌入矩阵。
- en: 'Based on the model shown in *Figure 13.2*, an embedding layer in Keras can
    be created as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于*图13.2*中展示的模型，可以如下创建一个Keras中的嵌入层：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This produces the following summary:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下总结：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note, however, that the vocabulary size is usually in the order of thousands
    for typical NLP tasks in most common languages. Just think of your good old-fashioned
    dictionary ... How many entries does it have? Several thousand, usually.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而请注意，词汇表的大小通常在几千个左右，适用于大多数常见语言中的典型NLP任务。想想你那本传统的字典……它有多少条目？通常是几千个。
- en: Similarly, sentences are usually longer than five words, so you should expect
    to have longer sequences than in the preceding example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，句子通常会长于五个单词，因此你应该预期拥有比前面例子更长的序列。
- en: Finally, the embedding dimension depends on how rich you want your model to
    be in the embedding space, or on your model space constraints. If you want a smaller
    model, consider having embeddings of 50 dimensions for example. But if space is
    not a problem and you have an excellent dataset with millions of entries, and
    you have unlimited GPU power, you should try embedding dimensions of 500, 700,
    or even 1000+ dimensions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，嵌入维度取决于你希望模型在嵌入空间中的丰富程度，或者取决于模型空间的约束。如果你希望模型较小，可以考虑使用例如50维的嵌入。但如果空间不成问题，且你拥有一个包含数百万条数据的优秀数据集，并且有无限的GPU计算能力，那么你可以尝试使用500、700甚至1000+维度的嵌入。
- en: Now, let's try to put the pieces together with a real-life example.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个现实生活中的例子来将这些部分结合起来。
- en: Word embedding and RNNs on IMDb
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IMDb上的词嵌入和RNN
- en: The IMDb dataset was explained in previous chapters, but to keep things brief,
    we will say that it has movie reviews based on text and a positive (1) or negative
    (0) review associated with every entry.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: IMDb数据集在前面的章节中已经解释过了，但为了简洁起见，我们会说它包含基于文本的电影评论，并且每个条目都与一个正面（1）或负面（0）的评价相关联。
- en: Keras lets you have access to this dataset and gives a couple of nice features
    to optimize time when designing a model. For example, the dataset is already processed
    according to the frequency of each word such that the smallest index is associated
    with frequent words and vice versa. With this in mind, you can also exclude the
    most common words in the English language, say 10 or 20\. And you can even limit
    the size of the vocabulary to, say, 5,000 or 10,000 words.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 让你可以访问这个数据集，并提供了一些优化时间的不错功能，当你在设计模型时。例如，数据集已根据每个单词的频率进行处理，因此最小的索引会与频繁出现的单词相关联，反之亦然。考虑到这一点，你还可以排除英语中最常见的单词，比如说10个或20个。而且你甚至可以将词汇表的大小限制为比如5,000或10,000个单词。
- en: 'Before we go further, we will have to justify some things you are about see:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们需要解释一些你将要看到的内容：
- en: A vocabulary size of 10,000\. We can make an argument in favor of keeping a
    vocabulary size of 10,000 since the task here is to determine whether a review
    is positive or negative. That is, we do not need an overly complex vocabulary
    to determine this.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇量为10,000。我们可以提出一个观点，支持保持10,000的词汇量，因为这里的任务是确定评论是正面还是负面的。也就是说，我们不需要过于复杂的词汇表来进行这种判断。
- en: Eliminating the top 20 words. The most common words in English include words
    such as "a" or "the"; words like these are probably not very important in determining
    whether a movie review is positive or negative. So, eliminating the 20 most common
    should be OK.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除前20个单词。英语中最常见的单词包括诸如 "a" 或 "the" 的单词；这些单词可能在确定电影评论是正面还是负面时并不重要。因此，消除最常见的20个单词应该是可以接受的。
- en: Sentence length of 128 words. Having smaller sentences, such as 5-word sentences,
    might be lacking enough content, and it would not make a lot of sense having longer
    sentences, such as 300-word sentences, since we can probably sense the tone of
    a review in fewer words than that. The choice of 128 words is completely arbitrary,
    but justified in the sense explained.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子长度为128个单词。较小的句子，如5个单词的句子，可能缺乏足够的内容，而较长的句子，如300个单词的句子，则可能在少于这些单词的情况下就能感知评论的语调。选择128个单词完全是任意的，但在前面解释的意义上是合理的。
- en: 'With such considerations, we can easily load the dataset as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些因素，我们可以轻松地按以下方式加载数据集：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also print some data for verification purposes like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以像这样打印一些数据以进行验证：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will output the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The first part of the preceding code shows how to load the dataset split into
    training and test sets, `x_train` and `y_train`, `x_test` and `y_test`, respectively.
    The remaining part is simply to display the shape of the dataset (dimensionality)
    for purposes of verification, and also for verification, we can print out sample
    #7 in its original form (the indices) and also its corresponding word. Such a
    portion of the code is a little bit strange if you have not used IMDb before.
    But the major points are that we need to reserve certain indices for special tokens:
    beginning of the sentence `<START>`, unused index `<UNUSED>`, unknown word index
    `<UNK>`, and zero padding index `<PAD>`. One we have made a special allocation
    for these tokens, we can easily map from the indices back to words. These indices
    will be learned by the RNN, and it will learn how to handle them, either by ignoring
    those, or by giving specific weights to them.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码的第一部分展示了如何加载数据集并将其分为训练集和测试集，分别为 `x_train` 和 `y_train`，`x_test` 和 `y_test`。剩余部分只是为了显示数据集的形状（维度）以进行验证，并且也可以打印出其原始形式中的第7个样本（索引）及其相应的单词。如果你以前没有使用过
    IMDb，这部分代码可能有些奇怪。但主要的点是我们需要为特殊标记保留某些索引：句子开头 `<START>`，未使用的索引 `<UNUSED>`，未知词索引
    `<UNK>`，以及零填充索引 `<PAD>`。一旦我们为这些标记做了特殊分配，我们就可以轻松地从索引映射回单词。这些索引将由 RNN 学习，并且它将学会如何处理它们，无论是通过忽略它们还是给予它们特定的权重。
- en: 'Now, let''s implement the architecture shown in the following diagram, which
    uses all the layers explained previously:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现如下图表所示的架构，其中包括前面解释过的所有层次：
- en: '![](img/50bfc3af-30bc-4ba2-8e63-699a4b32fbec.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/50bfc3af-30bc-4ba2-8e63-699a4b32fbec.png)'
- en: Figure 13.3\. An RNN architecture for the IMDb dataset
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3\. IMDb 数据集的 RNN 架构
- en: 'The diagram shows the same example (#7 from the training set) that is associated
    with a negative review. The architecture depicted in the diagram along with the
    code that loads the data is the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了与负面评论相关联的训练集中的示例（第7个）。图表中描绘的架构以及加载数据的代码如下所示：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The layers of the model are defined as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的各层定义如下：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This model uses the standard loss and optimizer that we have used before, and
    the summary produced is the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型使用了之前使用过的标准损失和优化器，生成的摘要如下：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we can train the network using the callbacks that we have used before:
    a) early stopping, and b) automatic learning rate reduction. The learning can
    be executed as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用之前使用过的回调函数来训练网络：a）早停止，和 b）自动学习率降低。学习过程可以如下执行：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we save the model and display the loss like so:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们保存模型并显示损失如下所示：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code produces the plot shown in the following diagram, which
    indicates that the network starts to overfit after epoch #3:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下图表，该图表显示网络在第3轮之后开始过拟合：
- en: '![](img/42c839cf-deb9-4108-879b-f8df79fe75cf.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42c839cf-deb9-4108-879b-f8df79fe75cf.png)'
- en: Figure 13.4\. RNN loss during training
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4. 训练过程中RNN的损失
- en: Overfitting is quite common in recurrent networks and you should not be surprised
    by this behavior. As of today, with the current algorithms, this happens a lot.
    However, one interesting fact about RNNs is that they also converge really fast
    compared to other traditional models. As you can see, convergence after three
    epochs is not too bad.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合在递归网络中相当常见，你不应对这种行为感到惊讶。至今，使用当前的算法，这种现象非常普遍。然而，关于RNN的一个有趣事实是，与其他传统模型相比，它们的收敛速度非常快。如你所见，经过三次迭代后，收敛情况已经不错。
- en: 'Next, we must examine the actual classification performance by looking at the
    balanced accuracy, the confusion matrix, and the **area under the ROC curve**
    (**AUC**). We will do this only in the test set as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须通过查看平衡准确率、混淆矩阵和**ROC曲线下面积**（**AUC**）来检查实际的分类性能。我们只会在测试集上进行如下操作：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, let''s analyze the plot produced here, which is shown in *Figure 13.5*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们分析这里生成的图表，它显示在*图13.5*中：
- en: '![](img/a2deb947-03bf-45c9-a981-eb58bf955efb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2deb947-03bf-45c9-a981-eb58bf955efb.png)'
- en: Figure 13.5\. ROC and AUC of the RNN model calculated in the test set
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5. 测试集上计算的RNN模型的ROC和AUC
- en: 'The diagram shows a good combination of **True Positive Rates** (**TPR**) and
    **False Positive Rates** (**FPR**), although it is not ideal: we would like to
    see a sharper step-like curve. The AUC is 0.92, which again is good, but the ideal
    would be an AUC of 1.0.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了**真阳性率**（**TPR**）和**假阳性率**（**FPR**）的良好组合，尽管并不理想：我们希望看到更陡峭的阶梯状曲线。AUC值为0.92，这再次表明表现不错，但理想情况下，AUC应该为1.0。
- en: 'Similarly, the code produces the balanced accuracy and confusion matrix, which
    would look something like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，代码生成了平衡准确率和混淆矩阵，结果大致如下所示：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First of all, we calculate here the optimal threshold value as a function of
    the TPR and FPR. We want to choose the threshold that will give us the maximum
    TPR and minimum FPR. The threshold and results shown here **will vary** depending
    on the initial state of the network; however, the accuracy should typically be
    around a very similar value.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在这里计算作为TPR和FPR函数的最优阈值。我们希望选择一个阈值，使其能给我们带来最大的TPR和最小的FPR。这里显示的阈值和结果**会变化**，具体取决于网络的初始状态；然而，准确率通常应该接近一个非常相似的值。
- en: Once the optimal threshold is calculated, we can use NumPy's `np.where()` method
    to threshold the entire predictions, mapping them to {0, 1}. After this, the balanced
    accuracy is calculated to be 83.82%, which again is not too bad, but also not
    ideal.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出最优阈值，我们可以使用NumPy的`np.where()`方法对整个预测结果进行阈值处理，将其映射为{0, 1}。之后，计算得出的平衡准确率为83.82%，这再次表明表现不错，但也不算理想。
- en: One of the possible ways to improve on the RNN model shown in *Figure 13.3*
    would be to somehow give the recurrent layer the ability to *remember* or *forget *specific
    words across layers and have them continue to stimulate neural units across the
    sequence. The next section will introduce a type of RNN with such capabilities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 改进图13.3中显示的RNN模型的一个可能方法是，某种方式使循环层具有在各层间“记住”或“忘记”特定单词的能力，并且继续在序列中激活神经元。下一节将介绍具有这种能力的RNN类型。
- en: Long short-term memory models
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆模型
- en: 'Initially proposed by Hochreiter, **Long Short-Term Memory Models** (**LSTMs**)
    gained traction as an improved version of recurrent models [Hochreiter, S., *et
    al.* (1997)]. LSTMs promised to alleviate the following problems associated with
    traditional RNNs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最初由Hochreiter提出，**长短期记忆模型**（**LSTM**）作为递归模型的改进版本获得了广泛关注[Hochreiter, S., *et
    al.* (1997)]。LSTM承诺解决与传统RNN相关的以下问题：
- en: Vanishing gradients
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度消失
- en: Exploding gradients
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度爆炸
- en: The inability to remember or forget certain aspects of the input sequences
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法记住或忘记输入序列的某些方面
- en: 'The following diagram shows a very simplified version of an LSTM. In *(b)*,
    we can see the additional self-loop that is attached to some memory, and in *(c)*,
    we can observe what the network looks like when unfolded or expanded:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个非常简化的LSTM版本。在*(b)*中，我们可以看到附加到某些记忆上的自循环，而在*(c)*中，我们可以观察到网络展开或展开后的样子：
- en: '![](img/a4f0eb33-ca40-4135-b0b9-9a854216321a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f0eb33-ca40-4135-b0b9-9a854216321a.png)'
- en: Figure 13.6\. Simplified representation of an LSTM
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6. 简化版LSTM的表示
- en: There is much more to the model, but the most essential elements are shown in
    *Figure 13.6*. Observe how an LSTM layer receives from the previous time step
    not only the previous output, but also something called **state**, which acts
    as a type of memory. In the diagram, you can see that while the current output
    and state are available to the next layer, these are also available to use at
    any point if they are needed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的内容远不止这些，但最核心的部分在*图13.6*中展示了。可以观察到，LSTM层不仅从前一个时间步接收前一个输出，还接收一个叫做**状态**的信息，这个状态充当了一种记忆。在图中你可以看到，尽管当前输出和状态可以传递到下一层，但它们也可以在任何需要的地方使用。
- en: 'Some of the things that we are not showing in *Figure 13.6* include the mechanisms
    by which the LSTM remembers or forgets. These can be complex to explain in this
    book for beginners. However, all you need to know at this point is that there
    are three major mechanisms:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图13.6*中没有显示的一些内容包括LSTM记忆或遗忘的机制。由于这些对于初学者来说可能比较复杂，书中没有进行详细说明。然而，到目前为止你只需要知道，有三种主要机制：
- en: '**Output control**: How much an output neuron is stimulated by the previous
    output and the current state'
  id: totrans-97
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出控制**：输出神经元受前一个输出和当前状态的刺激程度'
- en: '**Memory control**: How much of the previous state will be forgotten in the
    current state'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆控制**：当前状态中将遗忘多少前一个状态的信息'
- en: '**Input control**: How much of the previous output and new state (memory) will
    be considered to determine the new current state'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入控制**：前一个输出和新状态（记忆）在确定当前状态时的考虑程度'
- en: These mechanisms are trainable and optimized for each and every single dataset
    of sequences. But to show the advantages of using an LSTM as our recurrent layer,
    we will repeat the exact same code as before, only changing the RNN by an LSTM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制是可训练的，并且针对每一个单独的序列数据集进行优化。但为了展示使用LSTM作为我们递归层的优势，我们将重复之前的相同代码，只是将RNN替换为LSTM。
- en: 'The code to load the dataset and build the model is the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集并构建模型的代码如下：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model can be specified as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按照以下方式指定模型：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This essentially replicates the model shown in the following diagram:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上复制了下图所示的模型：
- en: '![](img/33464f77-9425-4bb9-8172-8e17ad37ce68.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33464f77-9425-4bb9-8172-8e17ad37ce68.png)'
- en: Figure 13.7\. LSTM-based neural architecture for the IMDb dataset
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7\. 基于LSTM的IMDb数据集神经架构
- en: Notice that this model has nearly 10,000 more parameters than the simple RNN
    approach. However, the premise is that this increase in parameters should also
    result in an increase in performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型的参数几乎比简单的RNN模型多了10,000个。然而，前提是这些参数的增加应当也带来性能的提升。
- en: 'We then train our model the same as before, like so:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们像之前一样训练模型，代码如下：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next we save the model and display its performance as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们保存模型并展示其性能，代码如下：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This code will produce the plot shown in the following diagram:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将生成下图所示的图表：
- en: '![](img/e37d404e-9f38-4e4b-8609-84255ffe6f5d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e37d404e-9f38-4e4b-8609-84255ffe6f5d.png)'
- en: Figure 13.8\. Loss across epochs of training an LSTM
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8\. 训练LSTM过程中各个epoch的损失变化
- en: 'Notice from the diagram that the model begins to overfit after **one epoch**.
    Using the trained model at the best point, we can calculate the actual performance
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，模型在**一个epoch**后开始过拟合。使用在最佳点训练好的模型，我们可以如下计算实际性能：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This produces the ROC shown in the following diagram:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生下图所示的ROC曲线：
- en: '![](img/f2baf0a7-1cf9-4b01-b57e-38169a66ec94.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2baf0a7-1cf9-4b01-b57e-38169a66ec94.png)'
- en: Figure 13.9\. ROC curve of an LSTM-based architecture
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9\. 基于LSTM架构的ROC曲线
- en: From the plot, we can see that there is a slight gain in the model, producing
    an AUC of 0.93 when the simple RNN model had an AUC of 0.92.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中我们可以看到，模型略有提升，生成了AUC为0.93的结果，而简单的RNN模型的AUC为0.92。
- en: 'When looking at the balanced accuracy and the confusion matrix, which was produced
    by the preceding code, this shows numbers like these:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 查看由前述代码生成的平衡准确率和混淆矩阵，我们可以看到类似这样的数字：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we can appreciate that the accuracy was of 85.44%, which is a gain of
    about 2% over the simple RNN. We undertook this experiment simply to show that
    by switching the RNN models, we can easily see improvements. Of course there are
    other ways to improve the models, such as the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到准确率为85.44%，比简单的RNN提高了大约2%。我们进行这个实验只是为了展示，通过更换RNN模型，我们可以轻松地看到性能提升。当然，也有其他方法可以提高模型，例如：
- en: Increase/reduce the vocabulary size
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少词汇表的大小
- en: Increase/reduce the sequence length
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少序列长度
- en: Increase/reduce the embedding dimension
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少嵌入维度
- en: Increase/reduce the neural units in recurrent layers
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加/减少递归层中的神经元单元
- en: And there may be others besides.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可能还有其他类型的模型。
- en: 'So far, you have seen how to take text representations (movie reviews), which
    is a common NLP task, and find a way to represent those in a space where you can
    classify them into negative or positive reviews. We did this through embedding
    and LSTM layers, but at the end of this, there is a dense layer with one neuron
    that gives the final output. We can think of this as mapping from the text space
    into a one-dimensional space where we can perform classification. We say this
    because there are three main ways in which to consider these mappings:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到如何将文本表示（电影评论）进行处理，这是一种常见的NLP任务，并且找到一种方式将其表示为一个空间，在这个空间中你可以将评论分类为负面或正面。我们通过嵌入和LSTM层完成了这一过程，但在最后，这里有一个包含一个神经元的全连接层，给出最终的输出。我们可以将其看作是从文本空间映射到一个一维空间，在这个空间中我们可以进行分类。我们这么说是因为考虑这些映射时，有三种主要的方式：
- en: '**Sequence-to-vector**: Just like the example covered here, mapping sequences
    to an *n-*dimensional space.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到向量**：就像这里讨论的例子，将序列映射到一个*n*维的空间。'
- en: '**Vector-to-sequence**: This goes the opposite way, from an *n*-dimensional
    space to a sequence.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量到序列**：这与上面相反，从一个*n*维的空间到一个序列。'
- en: '**Sequence-to-sequence**: This maps from a sequence to a sequence, usually
    going through an *n*-dimensional mapping in the middle.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列到序列**：这将一个序列映射到另一个序列，通常中间会经过一个*n*维的映射。'
- en: To exemplify these things, we will use an autoencoder architecture and MNIST
    in the next sections.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明这些概念，我们将在接下来的章节中使用自编码器架构和MNIST。
- en: Sequence-to-vector models
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到向量模型
- en: In the previous section, you *technically *saw a sequence-to-vector model, which
    took a sequence (of numbers representing words) and mapped to a vector (of one
    dimension corresponding to a movie review). However, to appreciate these models
    further, we will move back to MNIST as the source of input to build a model that
    will take one MNIST numeral and map it to a latent vector.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你*从技术上*看到了一个序列到向量的模型，它将一个序列（代表单词的数字）映射到一个向量（一个维度对应于电影评论）。然而，为了更进一步理解这些模型，我们将回到MNIST作为输入源，构建一个将一个MNIST数字映射到潜在向量的模型。
- en: Unsupervised model
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督模型
- en: 'Let''s work in the autoencoder architecture shown in the following diagram.
    We have studied autoencoders before and now we will use them again since we learned
    that they are powerful in finding vectorial representations (latent spaces) that
    are robust and driven by unsupervised learning:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下图所示的自编码器架构中进行工作。我们之前研究过自编码器，现在我们将再次使用它们，因为我们了解到它们在找到稳健的、由无监督学习驱动的向量表示（潜在空间）方面非常强大：
- en: '![](img/6af9a8ff-db2e-4cdc-b0c7-2ba2350e2ca2.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6af9a8ff-db2e-4cdc-b0c7-2ba2350e2ca2.png)'
- en: Figure 13.10\. LSTM-based autoencoder architecture for MNIST
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10. 基于LSTM的MNIST自编码器架构
- en: 'The goal here is to take an image and find its latent representation, which,
    in the example of *Figure 13.10*, would be two dimensions. However, you might
    be wondering: how can an image be a sequence?'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标是获取一张图像并找到其潜在表示，在*图13.10*的例子中，这个表示是二维的。然而，你可能会想：图像怎么会是一个序列呢？
- en: We can interpret an image as a sequence of rows or as a sequence of columns.
    Let's say that we interpret a two-dimensional image, 28x28 pixels, as a sequence
    of rows; we can look at every row from top to bottom as a sequence of 28 vectors
    whose dimensions are each 1x28\. In this way, we can use an LSTM to process those
    sequences, taking advantage of the LSTM's ability to understand temporal relationships
    in sequences. By this, we mean that, for example in the case of MNIST, the chances
    that a particular row in an image will look like the previous or next row are
    very high.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一张图像解释为一系列行，或者一系列列。假设我们将一个二维图像（28x28像素）解释为一系列行；我们可以将每一行从上到下看作是28个向量的序列，每个向量的维度为1x28。这样，我们就可以使用LSTM来处理这些序列，利用LSTM理解序列中时间关系的能力。通过这种方式，我们的意思是，比如在MNIST的例子中，某一行图像与前一行或下一行相似的可能性非常高。
- en: Notice further that the model proposed in *Figure 13.10* does not require an
    embedding layer as we did before when processing text. Recall that when processing
    text, we need to embed (vectorize) every single word into a sequence of vectors.
    However, with images, they already are sequences of vectors, which eliminates
    the need for an embedding layer.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步注意到，*图13.10*中提出的模型不需要像我们之前处理文本时那样使用嵌入层。回想一下，在处理文本时，我们需要将每个单词嵌入（向量化）成一个向量序列。然而，对于图像，它们本身就是向量的序列，因此不再需要嵌入层。
- en: 'The code that we will show here has nothing new to show except for two useful
    data manipulation tools:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的代码没有什么新内容，除了两个有用的数据操作工具：
- en: '`RepeatVector()`: This will allow us to arbitrarily repeat a vector. It helps
    in the decoder (see *Figure 13.10*) to go from a vector to a sequence.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RepeatVector()`：这将允许我们任意重复一个向量。它有助于解码器（见*图13.10*）将一个向量转换为一个序列。'
- en: '`TimeDistributed()`: This will allow us to assign a specific type of layer
    to every element of a sequence.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TimeDistributed()`：这将允许我们将特定类型的层分配给序列的每个元素。'
- en: 'These two are part of the `tensorflow.keras.layers` collection. These are implemented
    in the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者是`tensorflow.keras.layers`集合的一部分。它们在以下代码中实现：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After loading the data we can define the encoder part of the model as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据后，我们可以如下定义模型的编码器部分：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next we can define the decoder part of the model as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以如下定义模型的解码器部分：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally we compile and train the model like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们像这样编译并训练模型：
- en: '[PRE24]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The code should print the following output, corresponding to the dimensions
    of the dataset, a summary of the model parameters, followed by the training steps,
    which we omitted in order to save space:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 代码应打印以下输出，显示数据集的维度，模型参数的总结，以及训练步骤，我们为了节省空间省略了训练步骤：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The model will eventually converge to a valley where it is stopped automatically
    by the callback. After this, we can simply invoke the `encoder` model to literally
    convert any valid sequence (for example, MNIST images) into a vector, which we
    will do next.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 模型最终会收敛到一个谷底，随后通过回调自动停止。之后，我们可以直接调用`encoder`模型，将任何有效的序列（例如MNIST图像）转换为向量，接下来我们将执行这个操作。
- en: Results
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'We can invoke the `encoder` model to convert any valid sequence into a vector
    like so:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调用`encoder`模型将任何有效的序列转换为一个向量，方法如下：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will produce a two-dimensional vector with values corresponding to a vectorial
    representation of the sequence `x_test[0]`, which is the first image of the test
    set of MNIST. It might look something like this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个二维向量，其中的值对应于序列`x_test[0]`的向量表示，`x_test[0]`是MNIST测试集中的第一张图像。它可能看起来像这样：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: However, remember that this model was trained without supervision, hence, the
    numbers shown here will be different for sure! The encoder model is literally
    our sequence-to-vector model. The rest of the autoencoder model is meant to do
    the reconstruction.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，这个模型是在没有监督的情况下训练的，因此，这里显示的数字肯定会有所不同！编码器模型实际上就是我们的序列到向量的模型。其余的自编码器模型用于进行重建。
- en: 'If you are curious about how the autoencoder model is able to reconstruct a
    28x28 image from a vector of just two values, or if you are curious about how
    the entire test set of MNIST would look when projected in the learned two-dimensional
    space, you can run the following code:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对自编码器模型如何从仅有两个值的向量重建28x28图像感到好奇，或者对整个MNIST测试集在学习到的二维空间中投影后的样子感兴趣，你可以运行以下代码：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Which displays samples of the original digits, as shown in Figure 11.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示原始数字的样本，如图11所示。
- en: '![](img/2ee1636e-8781-4b4a-bbfd-b288336795e6.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ee1636e-8781-4b4a-bbfd-b288336795e6.png)'
- en: Figure 11\. MNIST original digits 0-9
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. 原始MNIST数字0-9
- en: 'The following code produces samples of the reconstructed digits:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成重建数字的样本：
- en: '[PRE29]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The reconstructed digits appear as shown in *Figure 12:*
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重建的数字如下所示，如*图12*：
- en: '![](img/4cce3195-d9fc-46ef-83bf-5cee6a876b8d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cce3195-d9fc-46ef-83bf-5cee6a876b8d.png)'
- en: Figure 12\. MNIST reconstructed digits 0-9 using an LSTM-based autoencoder
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图12\. 使用基于LSTM的自编码器重建的MNIST数字0-9
- en: 'The next piece of code will display a scatter plot of the original data projected
    into the latent space, which is shown in *Figure 13*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段代码将显示原始数据投影到潜在空间中的散点图，如*图13*所示：
- en: '[PRE30]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Recall that these results may vary due to the unsupervised nature of the autoencoder.
    Similarly, the learned space can be visually conceived to look like the one shown
    in *Figure 13*, where every dot corresponds to a sequence (MNIST digit) that was
    made a vector of two dimensions:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，由于自编码器的无监督性质，这些结果可能会有所不同。同样，学习到的空间可以被直观地认为是像*图 13*中所示的那样，其中每个点对应一个序列（MNIST
    数字），它被转化为二维向量：
- en: '![](img/0424aed3-9b57-4bf1-89ec-b2578136e808.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0424aed3-9b57-4bf1-89ec-b2578136e808.png)'
- en: Figure 13\. Learned vector space based on the MNIST dataset
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13. 基于 MNIST 数据集的学习向量空间
- en: From *Figure 13*, we can see that the sequence-to-vector model is working decently
    even when the reconstruction was based only in two-dimensional vectors. We will
    see larger representations in the next section. However, you need to know that
    sequence-to-vector models have been very useful in the last few years [Zhang,
    Z., *et al.* (2017)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 13*中，我们可以看到，即使重建仅基于二维向量，序列到向量模型也能有效工作。我们将在下一节看到更大的表示。然而，你需要知道，序列到向量模型在过去几年里非常有用
    [Zhang, Z., *等* (2017)]。
- en: Another useful strategy is to create vector-to-sequence models, which is going
    from a vectorial representation to a sequential representation. In an autoencoder,
    this would correspond to the decoder part. Let's go ahead and discuss this next.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的策略是创建向量到序列模型，即从向量表示转换到序列表示。在自编码器中，这对应于解码器部分。接下来，我们将讨论这个话题。
- en: Vector-to-sequence models
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量到序列模型
- en: If you look back at *Figure 10*, the vector-to-sequence model would correspond
    to the decoder funnel shape. The major philosophy is that most models usually
    can go from large inputs down to rich representations with no problems. However,
    it is only recently that the machine learning community regained traction in producing
    sequences from vectors very successfully (Goodfellow, I., et al. (2016)).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回顾*图 10*，向量到序列模型将对应于解码器漏斗形状。主要理念是，大多数模型通常能够从大量输入到丰富表示之间无缝转换。然而，直到最近，机器学习社区才重新获得动力，在从向量成功生成序列方面取得了显著进展（Goodfellow,
    I., 等 (2016)）。
- en: You can think of *Figure 10* again and the model represented there, which will
    produce a sequence back from an original sequence. In this section, we will focus
    on that second part, the decoder, and use it as a vector-to-sequence model. However,
    before we go there, we will introduce another version of an RNN, a bi-directional
    LSTM.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以再次思考*图 10*以及其中表示的模型，它将从原始序列中生成一个序列。在这一节中，我们将专注于第二部分——解码器，并将其用作向量到序列模型。然而，在进入这一部分之前，我们将介绍另一种
    RNN 的版本——双向 LSTM。
- en: Bi-directional LSTM
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向 LSTM
- en: 'A **Bi-directional LSTM** (**BiLSTM**), simply put, is an LSTM that analyzes
    a sequence going forward and backward, as shown in *Figure 14*:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**双向 LSTM**（**BiLSTM**）简单来说，就是一个能够同时向前和向后分析序列的 LSTM，如*图 14*所示：'
- en: '![](img/65dedd87-5a7e-4f19-8f0b-00c6aa7dede3.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65dedd87-5a7e-4f19-8f0b-00c6aa7dede3.png)'
- en: Figure 14\. A bi-directional LSTM representation
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14. 双向 LSTM 表示
- en: 'Consider the following examples of sequences analyzed going forward and backward:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下示例，序列被分析为向前和向后：
- en: An audio sequence that is analyzed in natural sound, and then going backward
    (some people do this to look for *subliminal *messages).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一段音频序列，在自然声音中分析，然后再反向分析（有些人这样做是为了寻找*潜意识*信息）。
- en: A text sequence, like a sentence, that is analyzed for good style going forward,
    and also going backward since some patterns (at least in the English and Spanish
    languages) make reference backward; for example, a verb that makes reference to
    a subject that appears at the beginning of the sentence.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一段文本序列，如一句话，既可以向前分析其风格，也可以向后分析，因为某些模式（至少在英语和西班牙语中）会向后引用；例如，一个动词引用的是在句首出现的主语。
- en: An image that has peculiar shapes going from top to bottom, or bottom to top,
    or from side to side and backwards; if you think of the number 9, going from top
    to bottom, a traditional LSTM might forget the round part at the top and remember
    the slim part at the bottom, but a BiLSTM might be able to recall both important
    aspects of the number by going top to bottom and bottom to top.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张图像，从上到下、从下到上、从一侧到另一侧或反向都有独特的形状；如果你想到数字 9，从上到下，传统的 LSTM 可能会忘记顶部的圆形部分而记住底部的细长部分，而
    BiLSTM 可能能够通过从上到下和从下到上的方式同时回忆起数字的两个重要部分。
- en: From *Figure 14 (b)*, we can also observe that the state and output of both
    the forward and backward pass are available at any point in the sequence.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 14 (b)*中，我们也可以观察到，正向和反向传递的状态和输出在序列中的任何位置都是可用的。
- en: 'We can implement the bi-directional LSTM by simply invoking the `Bidirectional()`
    wrapper around a simple LSTM layer. We will then take the architecture in *Figure
    10* and modify it to have the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地在一个普通的LSTM层外面调用`Bidirectional()`包装器来实现双向LSTM。然后，我们将采用*图 10*中的架构，并对其进行修改，得到如下结果：
- en: 100 dimensions in the latent space
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 潜在空间中的100维
- en: A BiLSTM replacing LSTM layers
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用双向LSTM替换LSTM层
- en: An additional dropout layer going from the latent space into the decoder
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从潜在空间到解码器的额外丢弃层
- en: 'The new architecture will look like *Figure 15*:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 新的架构将如下所示：*图 15*
- en: '![](img/66ead033-612e-4d8c-be82-593830cb24ab.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66ead033-612e-4d8c-be82-593830cb24ab.png)'
- en: Figure 15\. Implementing BiLSTMs with a view to building a vector-to-sequence
    model
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. 实现双向LSTM以构建向量到序列模型
- en: Recall that the most important point here is to make the latent space (the input
    to the vector-to-sequence model) as rich as possible in order to generate better
    sequences. We are trying to achieve this by increasing the latent space dimensionality
    and adding BiLSTMS. Let's go ahead and implement this and look a the results.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这里最重要的一点是尽可能让潜在空间（向量到序列模型的输入）变得更加丰富，从而生成更好的序列。我们试图通过增加潜在空间的维度并添加双向LSTM来实现这一点。让我们继续实现这一点，并查看结果。
- en: Implementation and results
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现与结果
- en: 'The code to implement the architecture in *Figure 15* is the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 实现*图 15*中架构的代码如下：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We define the encoder portion of the model as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义模型的编码器部分如下：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The decoder portion of the model can be defined as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的解码器部分可以定义如下：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next we compile the autoencoder and train it:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编译自编码器并训练它：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There is nothing new here, except for the `Bidirectional()` wrapper used that
    has been explained previously. The output should produce a summary of the full
    autoencoder model and the full training operation and will look something like
    this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么新东西，除了之前解释过的`Bidirectional()`包装器。输出应该会生成完整自编码器模型的总结以及完整的训练操作，结果看起来会是这样：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, after a number of epochs of unsupervised learning, the training will stop
    automatically and we can use the `decoder` model as our vector-to-sequence model.
    But before we do that, we might want to quickly check the quality of the reconstructions
    by running the same code as before to produce the images shown in the following
    diagram:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，经过若干轮无监督学习，训练会自动停止，我们可以将`decoder`模型作为我们的向量到序列模型。但在此之前，我们可能想快速检查重构的质量，可以通过运行与之前相同的代码生成以下图示中的图像：
- en: '![](img/43b0a05b-9283-44a1-84cd-12274c6959e5.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43b0a05b-9283-44a1-84cd-12274c6959e5.png)'
- en: Figure 16\. MNIST digits reconstructed with a BiLSTM autoencoder
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 使用双向LSTM自编码器重构的MNIST数字
- en: If you compare *Figure 11* with *Figure 16*, you will notice that the reconstructions
    are much better and the level of detail is better when compared to the previous
    model reconstructions in *Figure 12*.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将*图 11*与*图 16*进行比较，你会注意到重构效果要好得多，而且相比于之前模型在*图 12*中的重构，细节程度也得到了改善。
- en: 'Now we can call our vector-to-sequence model directly with any compatible vector
    as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以直接调用我们的向量到序列模型，输入任何兼容的向量，方法如下：
- en: '[PRE36]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This produces the following output and the plot in *Figure 17*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成如下输出和*图 17*中的图表：
- en: '[PRE37]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/bda7e5f5-ec39-4f70-8ac7-4e566aae3894.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bda7e5f5-ec39-4f70-8ac7-4e566aae3894.png)'
- en: Figure 17\. Sequence produced by a model from a random vector
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. 由模型从随机向量生成的序列
- en: You can generate as many random vectors as you wish and test your vector-to-sequence
    model. And another interesting thing to observe is a sequence-to-sequence model,
    which we will cover next.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以生成任意数量的随机向量并测试你的向量到序列模型。另一个有趣的观察点是序列到序列模型，我们将在接下来的部分介绍。
- en: Sequence-to-sequence models
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: 'A Google Brain scientist (Vinyals, O., et al. (2015)) wrote the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 一位Google Brain的科学家（Vinyals, O. 等，2015）写道：
- en: '"Sequences have become first-class citizens in supervised learning thanks to
    the resurgence of recurrent neural networks. Many complex tasks that require mapping
    from or to a sequence of observations can now be formulated with the **sequence-to-sequence**
    (**seq2seq**) framework, which employs the chain rule to efficiently represent
    the joint probability of sequences."'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: “序列已成为监督学习中的一等公民，这要归功于循环神经网络的复兴。许多需要从一个序列映射到另一个序列的复杂任务现在可以通过**序列到序列**（**seq2seq**）框架来公式化，该框架利用链式法则有效地表示序列的联合概率。”
- en: 'This is astoundingly correct because now the applications have grown. Just
    think about the following sequence-to-sequence project ideas:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常正确，因为现在这些应用已经扩展。只需想一下以下的序列到序列项目想法：
- en: 'Document summarization. Input sequence: a document. Output sequence: an abstract.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档摘要。输入序列：一份文档。输出序列：摘要。
- en: 'Image super resolution. Input sequence: a low-resolution image. Output sequence:
    a high-resolution image.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像超分辨率。输入序列：低分辨率图像。输出序列：高分辨率图像。
- en: 'Video subtitles. Input sequence: video. Output sequence: text captions.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频字幕。输入序列：视频。输出序列：文本字幕。
- en: 'Machine translation. Input sequence: text in source language. Output sequence:
    text in a target language.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译。输入序列：源语言的文本。输出序列：目标语言的文本。
- en: These are exciting and extremely challenging applications. If you have used
    online translators, chances are you have used some type of sequence-to-sequence
    model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是令人兴奋且极具挑战性的应用。如果你使用过在线翻译工具，那么很可能你已经使用过某种类型的序列到序列模型。
- en: 'In this section, to keep it simple, we will continue using the autoencoder
    in *Figure 15* as our main focus, but just to make sure we are all on the same
    page with respect to the generality of sequence-to-sequence models, we will point
    out the following notes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，为了简单起见，我们将继续使用*图15*中的自编码器作为我们的主要关注点，但为了确保我们都对序列到序列模型的一般性有共同的理解，我们将指出以下几点：
- en: Sequence-to-sequence models can map across domains; for example, video to text
    or text to audio.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型可以跨领域映射；例如，从视频到文本或从文本到音频。
- en: Sequence-to-sequence models can map in different dimensions; for example, a
    low-res image to high-res or vice versa for compression.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型可以在不同维度中进行映射；例如，将低分辨率图像转换为高分辨率图像，或反之，用于压缩。
- en: Sequence-to-sequence models can use many different tools, such as dense layers,
    convolutional layers, and recurrent layers.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列到序列模型可以使用许多不同的工具，例如稠密层、卷积层和循环层。
- en: With this in mind, you can pretty much build a sequence-to-sequence model depending
    of your application. For now, we will come back to the model in *Figure 15* and
    show that the autoencoder is a sequence-to-sequence model in the sense that it
    takes a sequence of rows of an image and produces a sequence of rows of another
    image. Since this is an autoencoder, the input and output dimensions must match.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个概念，你几乎可以根据你的应用构建一个序列到序列模型。现在，我们将回到*图15*中的模型，并展示自编码器是一个序列到序列模型，因为它接受图像的行序列并产生另一幅图像的行序列。由于这是一个自编码器，输入和输出的维度必须匹配。
- en: 'We will limit our showcase of the previously trained sequence-to-sequence model
    (autoencoder) to the following short code snippet, which builds up from the code
    in the previous section:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把展示之前训练过的序列到序列模型（自编码器）限制在以下简短的代码片段中，这段代码是从前一节的代码发展而来的：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let''s explain some of these steps. In *(a)*, we calculate the average sequence
    for every single number; this is in response to the question: what can we use
    as our input sequence since doing random is so easy? Well, using the average sequences
    to form the test set sounds interesting enough.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下这些步骤。在*(a)*中，我们为每个数字计算平均序列；这是对以下问题的回答：既然随机做很简单，我们可以使用什么作为输入序列？好吧，使用平均序列来构建测试集听起来足够有趣。
- en: 'Next, *(b)* is simply to make the input compatible with the encoder input dimensions.
    Then, *(c)* takes the average sequence and makes a vector out of it. Finally,
    *(d)* uses that vector to recreate the sequence, producing the plot shown in the
    following diagram:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，*(b)* 只是为了使输入与编码器输入维度兼容。然后，*(c)* 获取平均序列并将其转换为一个向量。最后，*(d)* 使用该向量重新创建序列，产生以下图示所示的图表：
- en: '![](img/c00afefc-9376-43b2-845f-c1b8b8883fa3.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c00afefc-9376-43b2-845f-c1b8b8883fa3.png)'
- en: Figure 18\. Sequence-to-sequence example outputs
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图18。序列到序列示例输出
- en: From the diagram, you can easily observe well-defined patterns consistent with
    handwritten numbers, which are generated as sequences of rows by bi-directional
    LSTMs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中，你可以轻松观察到与手写数字一致的明确定义的模式，这些模式是由双向LSTM生成的行序列。
- en: Before we finish this, let's have a word on the ethical implications of some
    of these models.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束之前，让我们谈一下这些模型的一些伦理影响。
- en: Ethical implications
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伦理影响
- en: With the resurgence of recurrent models and their applicability in capturing
    temporal information in sequences, there is a risk of finding latent spaces that
    are not properly being fairly distributed. This can be of higher risk in unsupervised
    models that operate in data that is not properly curated. If you think about it,
    the model does not care about the relationships that it finds; it only cares about
    minimizing a loss function, and therefore if it is trained with magazines or newspapers
    from the 1950s, it may find spaces where the word "women" may be close (in terms
    of Euclidean distance) to home labor words such as "broom", "dishes", and "cooking",
    while the word "man" may be close to all other labor such as "driving", "teaching",
    "doctor", and "scientist". This is an example of a bias that has been introduced
    into the latent space (Shin, S.,et al. (2020)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 随着递归模型的复兴及其在捕捉序列中的时间信息方面的适用性，存在发现潜在空间未被公平分配的风险。在操作未经妥善整理数据的无监督模型中，这种风险可能更大。如果你考虑到，模型并不关心它发现的关系；它只关心最小化损失函数，因此如果它是用1950年代的杂志或报纸进行训练的，它可能会发现“女性”一词与家务劳动的词汇（如“扫帚”、“碗碟”和“烹饪”）在欧几里得距离上接近，而“男性”一词则可能与所有其他劳动（如“驾驶”、“教学”、“医生”和“科学家”）接近。这就是潜在空间中引入偏见的一个例子（Shin,
    S.,et al. (2020)）。
- en: The risk here is that the vector-to-sequence or sequence-to-sequence models
    will find it much easier to associate a doctor with a man than with a woman, and
    cooking with a woman than with a man, just to name a couple of examples. You can
    take this to images of faces as well and find that certain people with certain
    features might be associated incorrectly. This is why it is so important to undertake
    the type of analysis we are doing here, trying to visualize the latent space whenever
    possible, trying to look at what the model outputs, and so on.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的风险在于，向量到序列或序列到序列模型会更容易将医生与男性而非女性联系起来，将烹饪与女性而非男性联系起来，仅举几个例子。你可以将这种情况扩展到面部图像，发现某些具有特定特征的人可能会被错误地关联起来。这就是为什么进行我们所做的这种分析如此重要——尽可能地尝试可视化潜在空间，查看模型输出，等等。
- en: The key takeaway here is that, while the models discussed here are extremely
    interesting and powerful, they also carry the risk of learning things about our
    societies that are particularly perceived as unwanted. If the risk exists and
    goes undetected, it might cause bias (Amini, A., et al. (2019)). And if bias goes
    undetected, it might lead to several forms of discrimination. Please always be
    careful about these things as well as things beyond your own societal context.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键要点是，虽然这里讨论的模型非常有趣且强大，但它们也带来了学习我们社会中被认为是“不可接受”的内容的风险。如果这种风险存在并且未被发现，可能会导致偏见（Amini,
    A., et al. (2019)）。如果偏见没有被及时发现，它可能会导致多种形式的歧视。因此，请始终对这些问题以及超出你自己社会背景的事项保持谨慎。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This advanced chapter showed you how to create RNNs. You learned about LSTMs
    and its bi-directional implementation, which is one of the most powerful approaches
    for sequences that can have distant temporal correlations. You also learned to
    create an LSTM-based sentiment analysis model for the classification of movie
    reviews. You designed an autoencoder to learn a latent space for MNIST using simple
    and bi-directional LSTMs and used it both as a vector-to-sequence model and as
    a sequence-to-sequence model.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本高级章节向你展示了如何创建RNN。你了解了LSTM及其双向实现，这是处理具有远程时间相关性的序列最强大的方法之一。你还学习了如何创建基于LSTM的情感分析模型，用于电影评论的分类。你设计了一个自编码器，使用简单的和双向的LSTM来学习MNIST的潜在空间，并将其既用作向量到序列模型，也用作序列到序列模型。
- en: At this point, you should feel confident explaining the motivation behind memory
    in RNNs founded in the need for more robust models. You should feel comfortable
    coding your own recurrent network using Keras/TensorFlow. Furthermore, you should
    feel confident implementing both supervised and unsupervised recurrent networks.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你应该能够自信地解释RNN中记忆的动机，这种动机源于对更强大模型的需求。你应该能够舒适地使用Keras/TensorFlow编写自己的递归网络。此外，你还应该自信地实现监督式和无监督式的递归网络。
- en: 'LSTMs are great in encoding highly correlated spatial information, such as
    images, or audio, or text, just like CNNs. However, both CNNs and LSTMs learn
    very specific latent spaces that may lack diversity. This can cause a problem
    if there is a malicious hacker that is trying to break your system; if your model
    is very specific to your data, it may create certain sensitivity to variations,
    leading to disastrous consequences in your outputs. Autoencoders solve this by
    using a generative approach called the variational autoencoder, which learns the
    distribution of the data rather than the data itself. However, the question remains:
    How can we implement this idea of generative approaches in other types of networks
    that are not necessarily autoencoders? To find out the answer, you cannot miss
    the next chapter, [Chapter 14](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml), *Generative
    Neural Networks*. The next chapter will present a way of overcoming the fragility
    of neural networks by attacking them and teaching them to be more robust. But
    before you go, quiz yourself with the following questions.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM在编码高度相关的空间信息（如图像、音频或文本）方面表现出色，就像CNN一样。然而，CNN和LSTM都学习非常特定的潜在空间，这些空间可能缺乏多样性。如果有恶意黑客试图破解你的系统，这可能会成为问题；如果你的模型非常特定于你的数据，它可能对变化非常敏感，从而导致输出结果出现灾难性后果。自编码器通过使用一种生成方法——变分自编码器，来解决这个问题，变分自编码器学习的是数据的分布，而不是数据本身。然而，问题依然存在：如何将这种生成方法的理念应用到其他类型的网络中，这些网络不一定是自编码器？想要找到答案，你不能错过下一章，[第14章](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml)，*生成对抗神经网络*。下一章将介绍一种通过攻击神经网络并教会它们变得更强大的方法。但在你继续之前，请先通过以下问题来测试自己。
- en: Questions and answers
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题与答案
- en: '**If both CNNs and LSTMs can model spatially correlated data, what makes LSTMs
    particularly better?**'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如果CNN和LSTM都能建模空间相关的数据，是什么让LSTM更优？**'
- en: Nothing in general, other than the fact that LSTMs have memory. But in certain
    applications, such as NLP, where a sentence is discovered sequentially as you
    go forward and backward, there are references to certain words at the beginning,
    middle, and end, and multiples at a time. It is easier for BiLSTMs to model that
    behavior faster than a CNN. A CNN may learn to do that, but it may take longer
    to do so in comparison.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说没有什么特别的，除了LSTM具有记忆功能这一点。但在某些应用中，比如自然语言处理（NLP），其中句子是顺序发现的，并且有时会前后引用某些词语，且可能在开始、中间和结尾处都有多次引用。BiLSTM比CNN更容易快速建模这种行为。CNN可能也能学会这样做，但相比之下，它可能需要更长的时间。
- en: '**Does adding more recurrent layers make the network better? **'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**增加更多递归层能使网络变得更好吗？**'
- en: No. It can make things worse. It is recommended to keep it simple to no more
    than three layers, unless you are a scientist and are experimenting with something
    new. Otherwise, there should be no more than three recurrent layers in a row in
    an encoder model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 不，增加更多的层可能会使情况变得更糟。建议保持简单，最多不超过三层，除非你是科学家并在进行某些新实验。否则，在编码器模型中，不应连续有超过三层的递归层。
- en: '**What other applications are there for LSTMs?**'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LSTM还有哪些其他应用？**'
- en: Audio processing and classification; image denoising; image super-resolution;
    text summarization and other text-processing and classification tasks; word completion;
    chatbots; text completion; text generation; audio generation; image generation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 音频处理与分类；图像去噪；图像超分辨率；文本摘要与其他文本处理和分类任务；词语补全；聊天机器人；文本补全；文本生成；音频生成；图像生成。
- en: '**It seems like LSTMs and CNNs haver similar applications. What makes you choose
    one over the other?**'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LSTM和CNN似乎有相似的应用，是什么让你选择一个而不是另一个？**'
- en: LSTMs are faster to converge; thus, if time is a factor, LSTMs are better. CNNs
    are more stable than LSTMs; thus, if your input is very unpredictable, chances
    are an LSTM might carry the problem across recurrent layers, making it worse every
    time, in which case CNNs could alleviate that with pooling. On a personal level,
    I usually try CNNs first for image-related applications, and LSTMs first for NLP
    applications.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM比其他模型更快收敛，因此，如果时间是一个因素，LSTM更好。CNN比LSTM更稳定，因此，如果你的输入非常不可预测，LSTM可能会将问题带入递归层，每次使其变得更糟，这时CNN可以通过池化来缓解问题。在个人层面上，我通常在与图像相关的应用中首先尝试CNN，而在NLP应用中则优先尝试LSTM。
- en: References
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). *Learning representations
    by backpropagating errors*. *Nature*, 323(6088), 533-536.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和 Williams, R. J. (1986). *通过反向传播误差学习表示*. *Nature*,
    323(6088), 533-536.
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). *Distributed
    representations of words and phrases and their compositionality*. In *Advances
    in neural information processing systems* (pp. 3111-3119).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., 和 Dean, J. (2013). *词语和短语的分布式表示及其组合性*.
    见于 *神经信息处理系统进展* (第3111-3119页).
- en: 'Pennington, J., Socher, R., and Manning, C. D. (October 2014). *Glove: Global
    vectors for word representation*. In *Proceedings of the 2014 conference on empirical
    methods in natural language processing* (EMNLP) (pp. 1532-1543).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington, J., Socher, R., 和 Manning, C. D. (2014年10月). *Glove：用于词表示的全局向量*.
    见于 *2014年自然语言处理实证方法会议论文集* (EMNLP) (第1532-1543页).
- en: Rivas, P., and Zimmermann, M. (December 2019). *Empirical Study of Sentence
    Embeddings for English Sentences Quality Assessment*. In *2019 International Conference
    on Computational Science and Computational Intelligence* (CSCI) (pp. 331-336).
    IEEE.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rivas, P., 和 Zimmermann, M. (2019年12月). *关于英语句子质量评估的句子嵌入的实证研究*. 见于 *2019年国际计算科学与计算智能会议*
    (CSCI) (第331-336页). IEEE.
- en: Hochreiter, S., and Schmidhuber, J. (1997). *Long short-term memory*. *Neural
    computation*, 9(8), 1735-1780.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter, S., 和 Schmidhuber, J. (1997). *长短期记忆网络*. *神经计算*, 9(8), 1735-1780.
- en: Zhang, Z., Liu, D., Han, J., and Schuller, B. (2017). *Learning audio sequence
    representations for acoustic event classification*. *arXiv preprint* arXiv:1707.08729.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, Z., Liu, D., Han, J., 和 Schuller, B. (2017). *学习音频序列表示用于声学事件分类*. *arXiv预印本*
    arXiv:1707.08729.
- en: 'Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Sequence modeling: Recurrent
    and recursive nets*. *Deep learning*, 367-415.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow, I., Bengio, Y., 和 Courville, A. (2016). *序列建模：递归与递归网络*. *深度学习*,
    367-415.
- en: 'Vinyals, O., Bengio, S., and Kudlur, M. (2015). *Order matters: Sequence to
    sequence for sets*. *arXiv preprint* arXiv:1511.06391.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals, O., Bengio, S., 和 Kudlur, M. (2015). *顺序重要：集合的序列到序列*. *arXiv预印本* arXiv:1511.06391.
- en: Shin, S., Song, K., Jang, J., Kim, H., Joo, W., and Moon, I. C. (2020). *Neutralizing
    Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation*.
    *arXiv preprint* arXiv:2004.03133.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin, S., Song, K., Jang, J., Kim, H., Joo, W., 和 Moon, I. C. (2020). *通过潜在解耦和反事实生成中和词嵌入中的性别偏见*.
    *arXiv预印本* arXiv:2004.03133.
- en: Amini, A., Soleimany, A. P., Schwarting, W., Bhatia, S. N., and Rus, D. (January
    2019). *Uncovering and mitigating algorithmic bias through learned latent structure*.
    In *Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society* (pp.
    289-295).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini, A., Soleimany, A. P., Schwarting, W., Bhatia, S. N., 和 Rus, D. (2019年1月).
    *通过学习潜在结构揭示和缓解算法偏见*. 见于 *2019年AAAI/ACM人工智能、伦理与社会会议论文集* (第289-295页).
